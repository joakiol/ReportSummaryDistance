2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 172?181,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsApples to Oranges: Evaluating Image Annotations from Natural LanguageProcessing SystemsRebecca Mason and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown University, Providence, RI 02912{rebecca,ec}@cs.brown.eduAbstractWe examine evaluation methods for systemsthat automatically annotate images using co-occurring text.
We compare previous datasetsfor this task using a series of baseline mea-sures inspired by those used in information re-trieval, computer vision, and extractive sum-marization.
Some of our baselines match orexceed the best published scores for thosedatasets.
These results illuminate incorrect as-sumptions and improper practices regardingpreprocessing, evaluation metrics, and the col-lection of gold image annotations.
We con-clude with a list of recommended practices forfuture research combining language and vi-sion processing techniques.1 IntroductionAutomatic image annotation is an important areawith many applications such as tagging, generat-ing captions, and indexing and retrieval on the web.Given an input image, the goal is to generate rel-evant descriptive keywords that describe the visualcontent of the image.
The Computer Vision (CV)literature contains countless approaches to this task,using a wide range of learning techniques and visualfeatures to identify aspects such as objects, people,scenes, and events.Text processing is computationally less expensivethan image processing and easily provides informa-tion that is difficult to learn visually.
For this reason,most commerical image search websites identify thesemantic content of images using co-occurring textexclusively.
But co-occurring text is also a noisysource for candidate annotations, since not all of thetext is visually relevant.
Techniques from NaturalLanguage Processing help align descriptive wordsand images.
Some examples of previous researchuse named-entity recognition to identify people inimages (Deschacht and Moens, 2007); term associa-tion to estimate the ?visualness?
of candidate anno-tations (Boiy et al, 2008; Leong et al, 2010); andtopic models to annotate images given both visualand textual features (Feng and Lapata, 2010b).Image annotation using NLP is still an emergingarea with many different tasks, datasets, and eval-uation methods, making it impossible to comparemany recent systems to each other.
Although there issome effort being made towards establishing sharedtasks1, it is not yet clear which kinds of tasks anddatasets will provide interesting research questionsand practical applications in the long term.
Untilthen, establishing general ?best practices?
for NLPimage annotation will help advance and legitimitizethis work.
In this paper, we propose some good prac-tices and demonstrate why they are important.2 Image Annotation Evaluation in CV andNLPIn this section, we first review related work in im-age annotation evaluation in computer vision, spe-cific challenges, and proposed solutions.
We thenrelate these challenges to the NLP image annotationtask and some of the specific problems we proposeto address.1http://imageclef.org/1722.1 Related Work in Computer VisionThe work of Mu?ller et al (2002) is one of the firstto address the issue of evaluation for image annota-tion systems.
While using the exact same annotationsystem, dataset, and evaluation metric, they dramati-cally improve the apparent performance of their sys-tem by using dataset pruning heuristics.Others have criticized commonly-used CVdatasets for being too ?easy?
?
images with thesame keywords are extremely similar in low-levelfeatures such as orientation, lighting, and color;while differences between images with differentkeywords are very clear (Westerveld and de Vries,2003; Ponce et al, 2006; Herve?
and Boujemaa,2007; Tang and Lewis, 2007).
These features areunwittingly exploited by certain algorithms andobscure the benefits of using more complex tech-niques (Ponce et al, 2006).
The problem is furtherexacerbated by evaluation metrics which essentiallyprefer precision over recall and are biased towardscertain keywords.
Annotations in test data mightnot include all of the ?correct?
keywords, andevaluation metrics need to account for the fact thatfrequent keywords in the corpus are safer guessesthan keywords that appear less frequently (Monayand Gatica-Perez, 2003).New baseline techniques, evaluation metrics, anddatasets for image annotation have been developedin response to these problems.
Makadia et al (2008;2010) define a basic set of low-level features, andpropose new baselines for more complex systems toevaluate against.
Barnard et al (2003) present a nor-malized loss function to address the preference to-wards precision in evaluation metrics.
New datasetsare larger and provide more diverse images, and it isnow easy to obtain multiple human-annotations perimage thanks to distributed services such as Ama-zon?s Mechanical Turk, and the ESP game (vonAhn and Dabbish, 2004).
Hanbury (2008) providesan overview of popular CV annotation datasets andmethods used for building them.2.2 Image Annotation using Natural LanguageProcessingMany of the problems from CV image annotationare also applicable to NLP image annotation, andbringing NLP to the task brings new challenges aswell.
One of these challenges is whether to allowinfrequent words to be pruned.
In CV annotation itis typical to remove infrequent terms from both thekeyword vocabulary and the evaluation data becauseCV algorithms typically need a large number of ex-amples to train on.
However, using NLP systemsand baselines one can correctly annotate using key-words that did not appear in the training set.
Remov-ing ?unlearnable?
keywords from evaluation data, asdone in (Boiy et al, 2008; Feng and Lapata, 2010b),artificially inflates performance against simple base-lines such as term frequency.Nearly all NLP annotation datasets use naturally-occurring sources of images and text.
A particu-larly popular source is news images alongside cap-tions or articles, which are collected online fromsources such as Yahoo!
News (Berg et al, 2004; De-schacht and Moens, 2007).
There are also domain-specific databases with images and descriptions suchas the art, antiques, and flowers corpora used in Boiyet al (2008).
Wikipedia has also been used as asource of images and associated text (Tsikrika et al,2011).
These sources typically offer well-writtenand cleanly formatted text but introduce the problemof converting text into annotations, and the annota-tions may not meet the requirements of the new task(as shown in Section 3.1).
Obtaining data via imagesearch engines is a common practice in CV (Fei-Feiet al, 2004; Berg and Forsyth, 2006) and can alsobe used to provide more challenging and diverse in-stances of images and co-occurring text.
The addi-tional challenge for NLP is that text content on manywebsites is written to improve their rank in searchengines, using techniques such as listing dozens ofpopular keywords.
Co-occurring text for retrievedimages on popular queries may not be representativeof the task to be performed.3 DatasetsIn this paper, we examine two established image an-notation datasets: the BBC News Dataset of Fengand Lapata (2008) (henceforth referred to as BBC),and the general web dataset of Leong et al (2010)(henceforth referred to as UNT).
These datasetswere both built to evaluate image annotation systemsthat use longer co-occurring text such as a news ar-ticle or a webpage, but they use data from differ-173Dataset BBC UNTdata instances article, image, and caption from anews storyimage and text from a webpagesource of data scraped from BBC News website Google Image Search resultscandidate keywords orcollocations for anno-tationdescriptive unigram words fromtraining datan ?
7-grams extracted from co-occurring text; collocations must ap-pear as article names on Wikipediagold annotations descriptive words from held-out im-age captionsmultiple human-authored annota-tions for each imageevaluation metric precision and recall against gold an-notationsmetrics adapted from evaluation oflexical substitutions (SemEval)number of train in-stances3121 instances of related news arti-cle, image, and captionnone (train using cross-validation)number of test in-stances240 instances of news article and re-lated image300 instances of webpage with textand imagepreprocessing proce-durelemmatize tokens, remove fromdataset al words that are not descrip-tive or that appear fewer than fivetimes in training articlesstem all tokensaverage number oftext tokens afterpreprocessing169 word tokens per article, 4.5 percaption278 word tokens per webpageaverage document titlelength4 word tokens 6 word tokenstotal vocabulary afterpreprocessing10479 word types 8409 word typesTable 1: Comparison of the BBC and UNT image annotation datasets.ent domains, different sources of gold image anno-tations, different preprocessing procedures, and dif-ferent evaluation measures.Table 1 provides an overview of the datasets;while this section covers the source of the datsetsand their gold annotations in more detail.3.1 BBCThe BBC Dataset (Feng and Lapata, 2008)2 containsnews articles, image captions, and images takenfrom the BBC News website.
Training instancesconsist of a news article, image, and image captionfrom the same news story.
Test instances are just theimage and the article, and hold-out the caption as asource of gold image annotations.Using news image captions as annotations has2Downloaded from http://homepages.inuf.ed.ac.uk/s0677528/data.htmlthe disadvantage that captions often describe back-ground information or relate the photo to the story,rather than listing important entities in the image.It also fails to capture variation in how humans de-scribe images, since it is limited to one caption perimage.3 However, captions are a cheap source ofdata; BBC has ten times as many images as UNT.To address the problem of converting natural lan-guage into annotations, a large amount of prepro-cessing is performed.
The established preprocessingprocedure for this dataset is to lemmatize and POS-tag using TreeTagger (Schmid, 1994) then removeall but the ?descriptive?
words (defined as nouns, ad-jectives, and certain classes of verbs).
This leavesa total text vocabulary of about 32K words, which3The Pascal Sentences dataset (vision.cs.uiuc.edu/pascal-sentences) provides multiple captions per image,but they are not naturally-occurring.174is further reduced by removing words that appearfewer than five times in the training set articles.
Ta-ble 1 shows the number of word tokens and typesafter performing these steps.43.2 UNTThe UNT Dataset (Leong et al, 2010)5 consistsof images and co-occurring text from webpages.The webpages are found by querying Google ImageSearch with frequent English words, and randomlyselecting from the results.Each image in UNT is annotated by five peoplevia Mechanical Turk.
In order to make human andsystem results comparable, human annotators are re-quired to only select words and collocations that aredirectly extracted from the text, and the gold anno-tations are the count of how many times each key-word or collocation is selected.
The human annota-tors write keywords into a text box; while the col-locations are presented as a list of candidates andannotators mark which ones are relevant.
Humanannotators tend to select subsets of collocations inaddition to the entire collocation.
For example, thegold annotation for one image has ?university oftexas?, ?university of texas at dallas?, ?the univer-sity of texas?, and ?the university of texas at dal-las?, each selected by at least four of the five an-notators.
Additionally, annotators can select mul-tiple forms of the same word (such as ?tank?
and?tanks?).
Gold annotations are stemmed after theyare collected, and keywords with the same stem havetheir counts merged.
For this reason, many key-words have a higher count than the number of an-notators.4 We are unable to reproduce work from Feng & Lapata(2008; 2010a; 2010b) and Feng (2011).
Specifically, our vocab-ulary counts after preprocessing (as in Table 1) are much higherthan reported counts, although the number of tokens per arti-cle/caption they report is higher than ours.
We have contactedthe authors, who confirmed that they took additional steps to re-duce the size of the vocabulary, but were unable to tell us exactlywhat those steps are.
Therefore, all system and baseline scorespresented on their dataset are of our own implementation, anddo not match those reported in previous publications.5Downloaded from http://lit.csci.unt.edu/index.php?P=research/downloads4 BaselinesWe run several baselines on the datasets.
Term fre-quency, tf*idf, and corpus frequency are featuresthat are often used in annotation systems, so it is im-portant to test them on their own.
Document Titleand tf*idf are both baselines that were used in theoriginal papers where these datasets came from.Sentence extraction is a new baseline that we pro-pose specifically for the BBC dataset, in order see ifwe can exploit certain properties of the gold annota-tions, which are also derived from sentences.4.1 Term FrequencyTerm frequency has been shown to be a power-ful feature in summarization (Nenkova and Vander-wende, 2005).
Words that appear frequently areconsidered more meaningful than infrequent words.Term frequency is the number of times a term (ex-cluding function words) appears in a document, di-vided by the total number of terms in that document.On the UNT dataset we use the stopword list in-cluded with the MALLET6 toolkit, while the BBCdataset doesn?t matter because the function wordshave already been removed.4.2 tf*idfWhile term frequency baseline requires the use of anad hoc function word list, tf*idf adjusts the weightsof different words depending on how important theyare in the corpus.
It is a standard baseline used forinformation retrieval tasks, based on the intuitionthat a word that appears in a smaller number of doc-uments is more likely to be meaningful than a wordthat appears in many documents.tf*idf is the product of term frequency and inversedocument frequency ?
idf(ti) = log Nni where N isthe number of documents in the corpus, and ni isthe number of documents that contain the term ti.For the BBC Dataset, we base the idf weights onthe document frequency of the training articles.
ForUNT, we use the reported tf*idf score which uses theBritish National Corpus to calculate the idf scores.76mallet.cs.umass.edu7We also ran tf*idf where for each document we recalcu-late idf using the other 299, but it didn?t make any meaningfuldifference.1754.3 Corpus FrequencyImage annotations in both NLP and CV tend to bedistributed with a relatively small number of fre-quently occuring keywords, and a long tail of key-words that only appear a few times.
For UNT, weuse the total keyword frequency of all the gold an-notations, except for the one document that we arecurrently scoring.
For BBC, we only measure thefrequency of keywords in the training set captions,since we are specifically interested in the frequencyof terms in captions.4.4 Document TitleFor BBC, the news article headline, and for UNT,the title of the webpage.4.5 Sentence ExtractionOur baseline extracts the most central sentence fromthe co-occurring text and uses descriptive wordsfrom that sentence as the image annotation.
Un-like sentence extraction techniques from Feng andLapata (2010a), we determine which sentence to ex-tract using the term frequency distribution directly.We extract the sentence with the minimum KL-divergence to the entire document.85 BBC Dataset Experiments5.1 System ComparisonIn addition to the baselines, we compare against theMix LDA system from Feng and Lapata (2010b).
InMix LDA, each instance is represented as a bag oftextual features (unigrams) and visual features (SIFTfeatures quantized to discrete ?image words?
usingk-means).
A Latent Dirichlet Allocation topic modelis trained on articles, images, and captions from thetraining set.
Keywords are generated for an unseenimage and article pair by estimating the distributionof topics that generates the test instance, then multi-plying them with the word distributions in each topicto find the probability of textual keywords for theimage.
Text LDA is is the same model but only us-ing words and not image features.8One could also think of this as a version of the KLSumsummarization system (Haghighi and Vanderwende, 2009) thatstops after one sentence.5.2 EvaluationThe evaluation metric and the source of gold anno-tations is described in Table 1.
For the baselines 4.1,4.2, 4.3 and the Mix LDA system, the generated an-notation for each test image is its ten most likelykeywords.
We also run all baselines and the MixLDA system on an unpruned version of the dataset,where infrequent terms are not removed from train-ing data, test data, or the gold annotations.
The pur-pose of this evaluation is to see if candidate key-words deemd ?unlearnable?
by the Mix LDA systemcan be learned by the baselines.5.3 ResultsThe evaluation results for the BBC Dataset areshown in Table 2.
Clearly, term frequency is astronger baseline than tf*idf by a large margin.
Thereason for this is simple: since nearly all of BBC?sfunction words are removed during preprocessing,the only words downweighted by the idf score arecommon ?
but meaningful ?
words such as police orgovernment.
This is worth pointing out because, inmany cases, the choice of using a term frequency ortf*idf baseline is made based on what was used inprevious work.
As we show here and in Section 6.3,the choice of frequency baseline should be based onthe data and processing techniques being used.We use the corpus frequency baseline to illus-trate the difference between standard and include-infrequent evaluations.
Since including infrequentwords doesn?t change which are most frequent inthe dataset, precision for corpus frequency doesn?tchange.
But since infrequent words are now in-cluded in the evaluation data, we see a 0.5% drop inrecall (since corpus frequency won?t capture infre-quent words).
Compared to the other baselines, thisis not a large difference.
Other baselines see a largerdrop in recall because they have both more gold key-words to estimate and more candidate keywords toconsider.
tf*idf is the most affected by this, becauseidf overly favors very infrequent keywords, despitetheir low term frequency.
In comparison, the termfrequency baseline is not as negatively affected andeven improves in precision because there are somecases where a word is very important to an articlein the test set but just didn?t appear very often in thetraining set (see Table 3 for examples).
But the base-176Standard Include-infrequentPrecision Recall F1 Precision Recall F1Term Frequency 13.13 27.84 17.84 13.62 25.71 17.81tf * idf 9.21 19.97 12.61 7.25 13.52 9.44Doc Title 17.23 13.70 15.26 15.91 11.86 13.59Corpus Frequency 3.17 6.52 4.26 3.17 6.02 4.15Sentence Extraction 16.67 15.61 16.13 18.62 16.83 17.68Mix LDA 7.30 16.16 10.06 7.50 13.98 9.76Text LDA 8.38 17.46 11.32 7.79 14.52 10.14Table 2: Image annotation results for previous systems and our proposed baselines on the BBC Dataset.Cadbury increasecontaminationtesting levelmalaria parasitespread mosquitoTable 3: Examples of gold annotations from the test sec-tion of the BBC Dataset.
The bolded words are the onesthat appear five or more times in the training set; the un-bolded words appear fewer than five times and would beremoved from both the candidate and gold keywords inthe standard BBC evaluation.lines with the best precision are the Doc Title andSentence Extraction baselines, which do not need togenerate ten keywords for every image.While sentence extraction has a lower recall thanterm frequency, it is the only baseline or systemthat has improved recall when including infrequentwords.
This is unexpected because our baseline se-lects a sentence based on the term frequency of thedocument, and the recall for term frequency fell.One possible explanation is that extraction implic-itly uses correlations between keywords.
Probabili-ties of objects appearing together in an image are notindependent; and the accuracy of annotations can beimproved by generating annotation keywords as aset (Moran and Lavrenko, 2011).
Recent works inimage captioning also use these correlations: explic-itly, using graphical models (Kulkarni et al, 2011;Yang et al, 2011); and implicitly, using languagemodels (Feng and Lapata, 2010a).
In comparison,sentence extraction is very implicit.Unsurprisingly, the Text LDA and Mix LDA sys-tems do worse on the include-infrequent evaluationthan they do on the standard, because words thatdo not appear in the training set will not have highprobability in the trained topic models.
We were un-able to reproduce the reported scores for Mix LDAfrom Feng and Lapata (2010b) where Mix LDA?sscores were double the scores of Text LDA (seeFootnote 4).
We were also unable to reproduce re-ported scores for tf*idf and Doc Title (Feng and Lap-ata, 2008).
However, we have three reasons why webelieve our results are correct.
First, BBC has morekeywords, and fewer images, than typically seen inCV datasets.
The BBC dataset is simply not suitedfor learning from visual data.
Second, a single SIFTdescriptor describes which way edges are orientedat a certain point in an image (Lowe, 1999).
Whilecertain types of edges may correlate to visual objectsalso described in the text, we do not expect SIFT fea-tures to be as informative as textual features for thistask.
Third, we refer to the best system scores re-ported by Leong et al (2010), who evaluate their textmining system (see section 6.1) on the standard BBCdataset.9 While their f1 score is slightly worse thanour term frequency baseline, they do 4.86% betterthan tf*idf.
But, using the baselines reported in Fengand Lapata (2008), their improvement over tf*idf is12.06%.
Next, we compare their system against fre-quency baselines using the 10 keyword generationtask on the UNT dataset (the oot normal scores intable 5).
Their best system performs 4.45% better9Combined model; precision: 13.38, recall: 25.17, f1:17.47.
Crucially, they do not reimplement previous systems orbaselines, but use scores reported from Feng and Lapata (2008).177than term frequency, and 0.55% worse than tf*idf.10Although it is difficult to compare different datasetsand evaluation metrics, our baselines for BBC seemmore reasonable than the reported baselines, giventheir relative performance to Leong et als system.6 UNT Dataset Experiments6.1 System ComparisonWe evaluate against the text mining system from(Leong et al, 2010).
Their system generates imagekeywords by extracting text from the co-occurringtext of an image.
It uses three features for select-ing keywords.
Flickr Picturability queries the FlickrAPI with words from the text in order to find re-lated image tags.
Retrieved tags that appear as sur-face forms in the text are rewarded proportional totheir frequency in the text.
Wikipedia Salience as-signs scores to words based on a graph-based mea-sure of importance that considers each term?s docu-ment frequency in Wikipedia.
Pachinko AllocationModel is a topic model that captures correlations be-tween topics (Li and McCallum, 2006).
PAM inferssubtopics and supertopics for the text, then retrievestop words from the top topics as annotations.
Thereis also a combined model of these features using anSVM with 10-fold cross-validation.6.2 EvaluationEvaluation on UNT uses a framework originally de-veloped for the SemEval lexical substitution task(McCarthy and Navigli, 2007).
This frameworkaccounts for disagreement between annotators byweighting each generated keyword by the number ofhuman annotators who also selected that keyword.The scoring framework consists of four evaluationmeasures: best normal, best mode, oot (out-of-ten)normal, and oot mode.11The two best evaluations find the accuracy of asingle ?best?
keyword generated by the system12.10And as we stated earlier, the relative performance of termfrequency vs tf*idf is different from dataset to dataset.11Both the original framework and its adaptation by Leonget al (2010) give precision and recall for each of the evaluationmeasures.
However, precision and recall are identical for allbaselines and systems, and only slightly different on the upperbound (human) scores.
To preserve space, we only present themetric and scores for precision.12In contrast to the original SemEval task, where systems canBest normal measures the accuracy for each systemannotation aj as the number of times aj appears inthe Rj , the multi-set union of human tags, and aver-ages over all the test images.Bestnormal =?ij?I|aj?Rj ||Rj ||I|In oot normal, up to ten unordered guesses can bemade without penalty.ootnormal =?ij?I?aj?Aj|aj?Rj ||Rj ||I|where Aj is the set of ten system annotations forimage ij .The best mode and oot mode metrics are the sameas the normal metrics except they only evaluate sys-tem annotations for images where Rj contains a sin-gle most frequent tag.
We use the scoring softwareprovided by SemEval13 with the gold annotation fileprovided in the UNT Dataset.6.3 ResultsThe results of the lexical substitution evaluation onthe UNT Dataset are shown in Table 5.
The resultsfrom the normal show support for our earlier ideathat the relative performance of term frequency vstf*idf depends on the dataset.
Although the term fre-quency baseline uses a stopword list, there are otherwords that appear frequently enough to suggest theyare not meaningful to the document ?
such as copy-right disclaimers.Recall that the mode evaluation is only measuredon data instances where the gold annotations havea single most frequent keyword.
While runningthe evaluation script on the gold annotation file thatcame with the UNT dataset, we discover that Se-mEval only identifies 28 of the 300 instances as hav-ing a single mode annotation, and that for 21 ofthose 28 instances, the mode keyword is ?cartoon?.Those 21/28 images correspond to the 75% bestmode score obtained by Corpus Frequency baseline.Given the small number of instances that actuallymake from zero to many ?best?
guesses, penalized by the totalnumber of guesses made.13http://nlp.cs.swarthmore.edu/semeval/tasks/task10/data.shtml178cartoon(6), market(5), market share(5),declin(3), imag(3), share(3), pictur(1),illustr(1), cartoonstock(1), origin(1),artist(1), meet(1), jfa0417(1), meeting-copyright(1)cartoon(6), bill gate(5), gate(4), monop-oli(4), pearli gate(4), bill(3), imag(3),caricatur(2), pictur(2), illustr(1), copy-right(1), artist(1), own(1), pearli(1)lift index(5), gener(3), index(3), con-dit(2), comput(2), comput gener(2),unstabl(2), zone(2), area(1), field(1),between(1), stabl(1), encyclopedia(1),thunderstorm(1), lift(1), free encyclope-dia(1), wikipedia(1)Table 4: Examples of gold annotations from the UNT Dataset.Best Out-of-ten (oot)Normal Mode Normal ModeTerm Frequency 5.67 14.29 33.40 89.29tf * idf 5.94 14.29 38.40 78.57Doc Title 6.40 7.14 35.19 92.86Corpus Frequency 2.54 75.00 8.22 82.14Flickr Picturability 6.32 78.57 35.61 92.86Wikipedia Salience 6.40 7.14 35.19 92.86Topic Model (PAM) 5.99 42.86 37.13 85.71Combined (SVM) 6.87 67.49 37.85 100.00Table 5: Image annotation results for our proposed baselines, the text mining systems from (Leong et al, 2010)count towards these metrics, we conclude that modeevaluation is not a meaningful way to compare im-age annotation systems on the UNT dataset.That said, the number of cartoons in the datasetdoes seem to be strikingly high.
Looking at thesource of the images, we find that 45 of the 300images were collected from a single online cartoonlibrary.
Predictably, we find that the co-occurringtext to these images contains a long list of keywords,and little other text that is relevant to the image.
Welooked at a small sample of the rest of the datasetand found that many of the other text documents inUNT also have keyword lists.Including this types of text in a general web cor-pus is not necessarily a problem, but it?s difficult tomeasure the benefits of using complex techniqueslike topic modeling and graph similarily to find andextract annotations when in so many cases the anno-tations have already been found and extracted.
Thisis shown in the normal evaluation results, where thecombined system is only slightly better at selectingthe single best keyword, and no better than tf*idf forthe out-of-ten measure.7 ConclusionThe intent of this paper is not to teach researchershow to inflate their own results, but to encourage bet-ter practices.
With that purpose in mind, we makethe following suggestions regarding future work inthis area:179Get to know your data.
The ability to quicklyand cheaply collect very large ?
but very noisy ?
col-lections of data from the internet is a great advancefor both NLP and CV research.
However, there stillneeds to be an appopriate match betwen the task be-ing performed, the system being proposed, and thedataset being used; and large noisy datasets can hideunintended features or incorrect assumptions aboutthe data.Use relevant gold annotations.
Do not convertother sources of data into annotations.
When collect-ing human annotations, avoid postprocessing stepssuch as merging or deleting keywords that changethe annotators?
original intent.
Keep an open di-alogue with annotators about issues that they findconfusing, since that is a sign of an ill-formed task.Preprocessing should be simple and reprodu-cable.
The use of different preprocessing proce-dures affects the apparent performance of systemsand sometimes has unintended consequences.Use strong baselines and compare to other workonly when appropriate.
Systems developed for dif-ferent tasks or datasets can make for misleadingcomparisons if they don?t use all features available.Strong baselines explicitly exploit low-level featuresthat are implicitly exploited by proposed systems, aswell as low-level features of the dataset.Don?t remove keywords from gold annotations.Just because keywords are impossible for one sys-tem to learn, does not mean they are impossible forall systems to learn.
Removing evaluation data arti-ficially inflates system scores and limits comparisonto related work.If a proposed system is to learn associations be-tween visual and textual features, then it is neces-sary to use larger datasets.
In general, global an-notations, such as scenes, is easiest; identifying spe-cific objects is more difficult; and identification ofevents, activities, and other abstract qualities has avery low success rate (Fluhr et al, 2006).
Alter-nately, use simpler image features that are knownto have a high sucess rate.
For example, Deschachtand Moens (2007) used a face detector to determinethe number of faces in an image, and then used NLPto determine the names of those people from associ-ated text.ReferencesK.
Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D.M.Blei, and M.I.
Jordan.
2003.
Matching words andpictures.
The Journal of Machine Learning Research,3:1107?1135.Tamara L. Berg and David A. Forsyth.
2006.
Animals onthe web.
In Proceedings of the 2006 IEEE ComputerSociety Conference on Computer Vision and PatternRecognition - Volume 2, CVPR ?06, pages 1463?1470,Washington, DC, USA.
IEEE Computer Society.T.L.
Berg, A.C. Berg, J. Edwards, M. Maire, R. White,Yee-Whye Teh, E. Learned-Miller, and D.A.
Forsyth.2004.
Names and faces in the news.
In Computer Vi-sion and Pattern Recognition, 2004.
CVPR 2004.
Pro-ceedings of the 2004 IEEE Computer Society Confer-ence on, volume 2, pages II?848 ?
II?854 Vol.2, june-2july.E.
Boiy, K. Deschacht, and M.-F. Moens.
2008.
Learn-ing visual entities and their visual attributes from textcorpora.
In Database and Expert Systems Applica-tion, 2008.
DEXA ?08.
19th International Workshopon, pages 48 ?53, sept.Koen Deschacht and Marie-Francine Moens.
2007.
Textanalysis for automatic image annotation.
In ACL, vol-ume 45, page 1000.Li Fei-Fei, Rob Fergus, and Pietro Perona.
2004.
Learn-ing generative visual models from few training ex-amples: An incremental bayesian approach tested on101 object categories.
In Proceedings of the 2004Conference on Computer Vision and Pattern Recog-nition Workshop (CVPRW?04) Volume 12 - Volume12, CVPRW ?04, pages 178?, Washington, DC, USA.IEEE Computer Society.Yansong Feng and Mirella Lapata.
2008.
Automatic im-age annotation using auxiliary text information.
Pro-ceedings of ACL-08: HLT, pages 272?280.Yansong Feng and Mirella Lapata.
2010a.
How manywords is a picture worth?
automatic caption gener-ation for news images.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1239?1249, Stroudsburg,PA, USA.
Association for Computational Linguistics.Yansong Feng and Mirella Lapata.
2010b.
Topic mod-els for image annotation and text illustration.
In HLT-NAACL, pages 831?839.Yansong Feng.
2011.
Automatic caption generation fornews images.
Ph.D. thesis, University of Edinburgh.Christian Fluhr, Pierre-Alain Mollic, and Patrick Hde.2006.
Usage-oriented multimedia information re-trieval technological evaluation.
In James Ze Wang,Nozha Boujemaa, and Yixin Chen, editors, Multime-dia Information Retrieval, pages 301?306.
ACM.180Aria Haghighi and Lucy Vanderwende.
2009.
Exploringcontent models for multi-document summarization.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 362?370, Boulder, Colorado, June.
Associationfor Computational Linguistics.Allan Hanbury.
2008.
A survey of methods for imageannotation.
J. Vis.
Lang.
Comput., 19:617?627, Octo-ber.Nicolas Herve?
and Nozha Boujemaa.
2007.
Image an-notation: which approach for realistic databases?
InProceedings of the 6th ACM international conferenceon Image and video retrieval, CIVR ?07, pages 170?177, New York, NY, USA.
ACM.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, and Tamara L.Berg.
2011.
Baby talk: Understanding and generat-ing simple image descriptions.
In CVPR, pages 1601?1608.Chee Wee Leong, Rada Mihalcea, and Samer Hassan.2010.
Text mining for automatic image tagging.
InCOLING, pages 647?655.Wei Li and Andrew McCallum.
2006.
Pachinko alloca-tion: Dag-structured mixture models of topic correla-tions.
In Proceedings of the 23rd international confer-ence on Machine learning, ICML ?06, pages 577?584,New York, NY, USA.
ACM.D.G.
Lowe.
1999.
Object recognition from local scale-invariant features.
In Computer Vision, 1999.
The Pro-ceedings of the Seventh IEEE International Confer-ence on, volume 2, pages 1150 ?1157 vol.2.Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Kumar.2008.
A new baseline for image annotation.
In Pro-ceedings of the 10th European Conference on Com-puter Vision: Part III, ECCV ?08, pages 316?329,Berlin, Heidelberg.
Springer-Verlag.A.
Makadia, V. Pavlovic, and S. Kumar.
2010.
Baselinesfor image annotation.
International Journal of Com-puter Vision, 90(1):88?105.D.
McCarthy and R. Navigli.
2007.
Semeval-2007 task10: English lexical substitution task.
In Proceedingsof the 4th International Workshop on Semantic Evalu-ations (SemEval-2007), pages 48?53.Florent Monay and Daniel Gatica-Perez.
2003.
On im-age auto-annotation with latent space models.
In Pro-ceedings of the eleventh ACM international conferenceon Multimedia, Multimedia ?03, pages 275?278, NewYork, NY, USA.
ACM.S.
Moran and V. Lavrenko.
2011.
Optimal tag sets forautomatic image.Henning Mu?ller, Ste?phane Marchand-Maillet, andThierry Pun.
2002.
The truth about corel - evaluationin image retrieval.
In Proceedings of the InternationalConference on Image and Video Retrieval, CIVR ?02,pages 38?49, London, UK, UK.
Springer-Verlag.Ani Nenkova and Lucy Vanderwende.
2005.
The im-pact of frequency on summarization.
Technical report,Microsoft Research.J.
Ponce, T. Berg, M. Everingham, D. Forsyth, M. Hebert,S.
Lazebnik, M. Marszalek, C. Schmid, B. Russell,A.
Torralba, C. Williams, J. Zhang, and A. Zisser-man.
2006.
Dataset issues in object recognition.In Jean Ponce, Martial Hebert, Cordelia Schmid, andAndrew Zisserman, editors, Toward Category-LevelObject Recognition, volume 4170 of Lecture Notesin Computer Science, pages 29?48.
Springer Berlin /Heidelberg.
10.1007/11957959 2.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of internationalconference on new methods in language processing,volume 12, pages 44?49.
Manchester, UK.Jiayu Tang and Paul Lewis.
2007.
A study of qualityissues for image auto-annotation with the corel data-set.
IEEE Transactions on Circuits and Systems forVideo Technology, Vol.
1(NO.
3):384?389, March.T.
Tsikrika, A. Popescu, and J. Kludas.
2011.
Overviewof the wikipedia image retrieval task at imageclef2011.
In CLEF (Notebook Papers/LABs/Workshops):CLEF.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proceedings of theSIGCHI conference on Human factors in computingsystems, CHI ?04, pages 319?326, New York, NY,USA.
ACM.Thijs Westerveld and Arjen P. de Vries.
2003.
Ex-perimental evaluation of a generative probabilistic im-age retrieval model on ?easy?
data.
In In Proceedingsof the SIGIR Multimedia Information Retrieval Work-shop 2003, Aug.Yezhou Yang, Ching Lik Teo, Hal Daume?
III, and Yian-nis Aloimonos.
2011.
Corpus-guided sentence gen-eration of natural images.
In Empirical Methods inNatural Language Processing (EMNLP), Edinburgh,Scotland.181
