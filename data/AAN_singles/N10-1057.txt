Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 369?372,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsPredicting Human-Targeted Translation Edit Ratevia Untrained Human AnnotatorsOmar F. Zaidan  and  Chris Callison-BurchDept.
of Computer Science, Johns Hopkins UniversityBaltimore, MD 21218, USA{ozaidan,ccb}@cs.jhu.eduAbstractIn the field of machine translation, automaticmetrics have proven quite valuable in systemdevelopment for tracking progress and meas-uring the impact of incremental changes.However, human judgment still plays a largerole in the context of evaluating MT systems.For example, the GALE project uses human-targeted translation edit rate (HTER), whereinthe MT output is scored against a post-editedversion of itself (as opposed to being scoredagainst an existing human reference).
Thisposes a problem for MT researchers, sinceHTER is not an easy metric to calculate, andwould require hiring and training human an-notators to perform the editing task.
In thiswork, we explore soliciting those edits fromuntrained human annotators, via the onlineservice Amazon Mechanical Turk.
We showthat the collected data allows us to predictHTER-ranking of documents at a significantlyhigher level than the ranking obtained usingautomatic metrics.1 IntroductionIn the early days of machine translation (MT), itwas typical to evaluate MT output by solicitingjudgments from human subjects, such as evaluat-ing the fluency and adequacy of MT output (LDC,2005).
While this approach was appropriate (in-deed desired) for evaluating a system, it was not apractical means of tracking the progress of a sys-tem during its development, since collecting hu-man judgments is both costly and time-consuming.The introduction of automatic metrics like BLEUcontributed greatly to MT research, for instanceallowing researchers to measure and evaluate theimpact of small modifications to an MT system.However, manual evaluation remains a corecomponent of system evaluation.
Teams on theGALE project, a DARPA-sponsored MT researchprogram, are evaluated using the HTER metric,which is a version of TER whereby the output isscored against a post-edited version of itself, in-stead of a preexisting reference.
Moreover, empha-sis is placed on performing well across alldocuments and across all genres.
Therefore, it isimportant for a research team to be able to evaluatetheir system using HTER, or at least determine theranking of the documents according to HTER, forpurposes of error analysis.
Instead of hiring ahuman translator and training them, we proposemoving the task to the virtual world of Amazon?sMechanical Turk (AMT), hiring workers to edit theMT output and predict HTER from those edits.
Weshow that edits collected this way are better atpredicting document ranking than automaticmetrics, and furthermore that it can be done at alow cost, both in terms of time and money.The paper is organized as follows.
We firstdiscuss options available to predict HTER, such asautomatic metrics.
We then discuss the possibilityof relying on human annotators, and the inherentdifficulty in training them, before discussing theconcept of soliciting edits over AMT.
We detailthe task given to the workers and summarize thedata that we collected, then show how we cancombine their data to obtain significanly betterrank predictions of documents.2 Human-Targeted TERTranslation edit rate (TER) measures the numberof edits required to transform a hypothesis into anappropriate sentence in terms of grammaticalityand meaning (Snover et al, 2006).
While TERusually scores a hypothesis against an existing ref-erence sentence, human-targeted TER scores ahypothesis against a post-edited version of itself.369While HTER has been shown to correlate quitewell with human judgment of MT quality, it isquite challenging to obtain HTER scores for MToutput, since this would require hiring and traininghuman subjects to perform the editing task.
There-fore, other metrics such as BLEU or TER are usedas proxies for HTER.2.1 Amazon?s Mechanical TurkThe high cost associated with hiring and training ahuman editor makes it difficult to imagine an alter-native to automatic metrics.
However, we proposesoliciting edits from workers on Amazon?s Me-chanical Turk (AMT).
AMT is a virtual market-place where ?requesters?
can post tasks to becompleted by ?workers?
(aka Turkers) around theworld.
Two main advantages of AMT are the pre-existing infrastructure, and the low cost of com-pleting tasks, both in terms of time and money.Data collected over AMT has already been used inseveral papers such as Snow et al (2008) and Cal-lison-Burch (2009).When a requester creates a task to be completedover AMT, it is typical to have completed by morethan one worker.
The reason is that the use ofAMT for data collection has an inherent problemwith data quality.
A requester has fewer tools attheir disposal to ensure workers are doing the taskproperly (via training, feedback, etc) when com-pared to hiring annotators in the ?real?
world.Those redundant annotations are therefore col-lected to increase the likelihood of at least onesubmission from a faithful (and competent)worker.2.2 AMT for HTERThe main idea it to mimic the real-world HTERsetup by supplying workers with the original MToutput that needs to be edited.
The worker is alsogiven a human reference, produced independentlyfrom the MT output.
The instructions ask theworker to modify the MT output, using as few ed-its as possible, to match the human reference inmeaning and grammaticality.The submitted edited hypothesis can then beused as the reference for calculating HTER.
Theidea is that, with this setup, a competent workerwould be able to closely match the editing behav-ior of the professionally trained editor.3 The DatasetsWe solicited edits of the output from one ofGALE?s teams on the Arabic-to-English task.
ThisMT output was submitted by this team and HTER-scored by LDC-hired human translators.
Therefore,we already had the edits produced by aprofessional translator.
These edits were used asthe ?gold-standard?
to evaluate the edits solicitedfrom AMT and to evaluate our methods ofcombining Turkers?
submissions.The MT output is a translation of more than2,153 Arabic segments spread across 195 docu-ments in 4 different genres: broadcast conversa-tions (BC), broadcast news (BN), newswire (NW),and blogs (WB).
Table 1 gives a summary of eachgenre?s dataset.Genre # docs Segs/doc Words/segBC 40 15.8 28.3BN 48 9.6 36.1NW 54 8.7 39.5WB 53 11.1 31.6Table 1:  The 4 genres of the dataset.For each of the 2,153 MT output segments, wecollected edits from 5 distinct workers on AMT,for a total of 10,765 post-edited segments by a totalof about 500 distinct workers.1 The segments werepresented in 1,210 groups of up to 15 segmentseach, with a reward of $0.25 per group.
Hence thetotal rewards to workers was around $300, at a rateof 36 post-edited segments per dollar (or 2.8 pen-nies per segment).4 What are we measuring?We are interested in predicting the ranking thedocuments according to HTER, not necessarilypredicting the HTER itself (though of course at-tempting to predict the latter accurately is the cor-nerstone of our approach to predict the former).
Tomeasure the quality of a predicted ranking, we useSpearman?s rank correlation coefficient, ?, wherewe first convert the raw scores into ranks and thenuse the following formula to measure correlation:)1())()((61),( 212??
?=?=nnyrankxrankYXniii?1 Data available at http://cs.jhu.edu/~ozaidan/hter.370where n is the number of documents, and each of Xand Y is a vector of n HTER scores.Notice that values for ?
range from ?1 to 1, with+1 indicating perfect rank correlation, ?1 perfectinverse correlation, and 0 no correlation.
That is,for a fixed X, the best-correlated Y is that for which),( YX?
is highest.5 Combining Tukers?
EditsOnce we have collected edits from the humanworkers, how should we attempt to predict HTERfrom them?
If we could assume that all Turkers aredoing the task faithfully (and doing it adequately),we should use the annotations of the worker per-forming the least amount of editing, since thatwould mirror the real-life scenario.However, data collected from AMT should betreated with caution, since a non-trivial portion ofthe collected data is of poor quality.
Note that thisdoes not necessarily indicate a ?cheating?
worker,for even if a worker is acting in good faith, theymight not be able to perform the task adequately,due to misunderstanding the task, or neglecting toattempt to use a small number of edits.And so we need to combine the redundant edits inan intelligent manner.
Recall that, given a segment,we collected edits from multiple workers.
Somebaseline methods include taking the minimum overthe edits, taking the median, and taking the aver-age.Once we start thinking of averages, we shouldconsider taking a weighted average of the edits fora segment.
The weight associated with a workershould reflect our confidence in the quality of thatworker?s edits.
But how can we evaluate a workerin the first place?5.1 Self Verification of TurkersWe have available ?gold-standard?
editing behav-ior for the segments, and we treat a small portionof the segments edited by a Turker as a verificationdataset.
On that portion, we evaluate how closelythe Turker matches the LDC editor, and weightthem accordingly when predicting the number ofedits of the rest of that group?s segments.
Specifi-cally, the Turker?s weight is the absolute differencebetween the Turker?s edit count and the profes-sional editor?s edit count.Notice that we are not simply interested in aworker whose edited submission closely matchesthe edited submission of the professional transla-tor.
Rather, we are interested in mirroring the pro-fessional translator?s edit rate.
That is, the closer aTurker?s edit rate is to the LDC editor?s, the morewe should prefer the worker.
This is a subtle point,but it is indeed possible for a Turker to have simi-lar edit rate as the LDC editor but still require alarge number of edits to get the LDC editor?s sub-mission itself.6 ExperimentsWe examine the effectiveness of any of the abovemethods by comparing the resulting documentranking versus the desired ranking by HTER.
Inaddition to the above methods, we use a baseline aranking predicted by TER to a human reference.
(For clarity, we omit discussion with other metricssuch as BLEU and (TER?BLEU)/2, since thosebaselines are not as strong as the TER baseline.6.1 Experimental SetupWe examine each genre individually, since genresvary quite a bit in difficulty, and, more impor-tantly, we care about the internal ranking withineach genre, to mirror the GALE evaluation proce-dure.We examine the effect of varying the amount ofdata by which we judge a Turker?s data quality.The amount of this ?verification?
data is varied asa percentage of the total available segments.
Thosesegments are chosen at random, and we perform100 trials for each point.6.2 Experimental ResultsFigure 1 shows the rank correlations for variousmethods across different sizes of verification sub-sets.
Notice that some methods, such as the TERbaseline, have horizontal lines, since these do notrate a Turker based on a verification subset.It is worth noting that the oracle performs verywell.
This is an indication that predicting HTERaccurately is mostly a matter of identifying the bestworker.
While oracle scenarios usually representunachievable upper bounds, keep in mind thatthere are only a very small number of editors persegment (five, as opposed to oracle scenarios deal-ing with 100-best lists, etc).371Other than that, in general, it is possible toachieve very high rank correlation using Turkers?data, significantly outperforming the TER ranking,even with a small verification subset.
The genresdo vary quite a bit in difficulty for Turkers, withBC and especially NW being quite difficult,though in the case of NW for instance, this is dueto the human reference doing quite well to beginwith, rather than Turkers performing poorly.7 Conclusions and Future WorkWe proposed soliciting edits of MT output viaAmazon?s Mechanical Turk and showed we canpredict ranking significantly better than an auto-matic metric.
The next step is to explicitly identifyundesired worker behavior, such as not editing theMT output at all, or using the human reference asis instead of editing the MT output.
This can bedetected by not limiting our verification to compar-ing behavior to the professional editor?s, but alsoby comparing submitted edits to the MT outputitself and to the human reference.
In other words, aworker?s submission could be characterized interms of its distance to the MT output and to thehuman reference, thus building a complete ?pro-file?
of the worker, and adding another componentto guard against poor data quality and to rewardthe desired behavior.AcknowledgmentsThis work was supported by the EuroMatrixPlusProject (funded by the European Commission), andby the DARPA GALE program under Contract No.HR0011-06-2-0001.
The views and findings arethe authors' alone.ReferencesChris Callison-Burch.
2009.
Fast, Cheap, and Creative:Evaluating Translation Quality Using Amazon's Me-chanical Turk.
In Proceedings of EMNLP.LDC.
2005.
Linguistic data annotation specification:Assessment of fluency and adequacy in translations.Revision 1.5.Matthew Snover, Bonnie J. Dorr, Richard Schwartz.2006.
A Study of Translation Edit Rate with TargetedHuman Annotation.
Proceedings of AMTA.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast ?
but is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of EMNLP.Figure 1: Rank correlation between predicted rank-ing and HTER ranking for different predictionschemes, across the four genres, and across varioussizes of the worker verification set.372
