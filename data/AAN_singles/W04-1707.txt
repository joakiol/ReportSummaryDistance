Towards Deeper Understanding and Personalisation in CALLGalia Angelova, Albena Strupchanska, Ognyan Kalaydjiev, Milena YankovaInstitute for Parallel Processing, Bulgarian Academy of Sciences, Sofia, Bulgaria{galia, albena, ogi, myankova}@lml.bas.bgSvetla Boytcheva, Irena VitanovaSofia University ?St.
Kliment Ohridski?, Sofia, Bulgariasvetla@fmi.uni-sofia.bg, itv@gmx.co.ukPreslav NakovUniversity of California at Berkeley, USA, nakov@eecs.berkeley.eduAbstractWe consider in depth the semantic analysis inlearning systems as well as some informationretrieval techniques applied for measuring thedocument similarity in eLearning.
Theseresults are obtained in a CALL project, whichended by extensive user evaluation.
Afterseveral years spent in the development ofCALL modules and prototypes, we think thatmuch closer cooperation with real teachingexperts is necessary, to find the properlearning niches and suitable wrappings of thelanguage technologies, which could give birthto useful eLearning solutions.1 IntroductionThe tendency to develop natural interfaces for allusers implies man-machine interaction in a naturalway, including natural language too, both asspeech and as free text.
Many recent eLearningresearch prototypes try to cope with theunrestricted text input as it is considered old-fashioned and even obsolete to offer interfacesbased on menu-buttons and mouse-clickingcommunication only.
On the other hand, theavailable eLearning platforms such as WebCT [1],CISCO [2], and the freeware HotPotatoes [3], arefar from the application of advanced languagetechnologies that might provide interfaces based onspeech and language processing.
They representcomplex communication environments and/orempty shells where the teacher uploads trainingmaterials, drills, etc.
using specialised authoringtools.
Recently on-line voice communicationbetween teachers and students has been madeavailable as well, via fast Internet in virtualclassrooms, but no speech or language processinghas been considered.
So there is a deep, principlegap between the advanced research on tutoringsystems and the typical market eLearningenvironments addressing primarily thecommunication needs of the mass user.In what follows we will concentrate on researchprototypes integrating language technologies ineLearning environments.
In general, suchprototypes might be called Intelligent TutoringSystems (ITS) and we will stick to this notion here.Most of the systems discussed below addressComputer-Aided Language Learning (CALL) butlanguage technologies are applied for automaticanalysis of user utterances in other domains too.
Areview of forty Intelligent CALL systems (Gam-per, 2002) summarises the current trends to embed?intelligence?
in CALL.
What we developed (andreport here) might be considered intelligentbecause of the integration of reasoning and theorientation to adaptivity and personalisation.This paper is structured as follows.
In section 2we consider the task of semantic analysis of thelearner's input, which is an obligatory elementwhen the student is given the opportunity to type infreely in response to ITS's questions and/or drills.Section 3 deals with Information Retrieval (IR)approaches for measuring document similarity,which are integrated in ITS as techniques for e.g.assessing the content of student essays or choosingthe most relevant text to be shown to the learner.Section 4 discusses how the language technologiesin question can provide some adaptivity of the ITS,as a step towards personalisation.
In section 5 wesummarise the current results regarding theevaluation of our prototypes with real users.Section 6 contains the conclusion.2 Semantic Analysis in ITSAlthough the automatic analysis of userutterances is a hot research topic, it achieved onlypartial success so far.
The review (Nerbonne,2002) shows that Natural Language Processing(NLP) is often integrated in CALL, as the domainof language learning is the first ?candidate?
for theapplication of computational linguistics tools.Different language technologies are applied in?programs designed to help people learn foreignlanguages?
: morphology and lemmatisation,syntax, corpus-based language acquisition, speechprocessing, etc.
Attempts to implement automaticsemantic analysis of free text input are relativelyrare, due to the sophisticated paradigm and thedefault assumption that it will have a very limitedsuccess (i.e.
will be the next failure).The famous collection of papers (Holland, 1995)presents several systems, which integrate NLPmodules in different ways.
The most advanced oneregarding semantic analysis is MILT (Dorr, 1995),where the correctness as well as theappropriateness of the student?s answer arechecked by matching them against expectations.This is performed in the context of a question-answering session, where expected answers arepredefined by the foreign language tutor.
Thelanguage-independent internal semanticrepresentation is based on lexical conceptualstructures, which (following Jackendoff) havetypes, with primitives and propositionaldescriptions along different dimensions and fieldsetc.
Consider as an example that the teacher hasspecified that ?John ran to the house?
is a correctanswer.
This sentence is processed by the systemand the following lexical conceptual structure isobtained:[Event GO Loc([Thing JOHN],[Path TO Loc ([Position AT Loc ([Thing JOHN],[Property HOUSE])])],[Manner RUNNINGLY])]which is stored by the tutoring system and latermatched against the student?s answer.
If thestudent types ?John went to the house?, the systemmust determine whether this matches the teacher-specified answer.
The student?s sentence isprocessed and respresented as:[Event GO Loc([Thing JOHN],[Path TO Loc ([Position AT Loc ([Thing JOHN],[Property HOUSE])])])]The matcher compares the two lexicalconceptual structures and produces the output:Missing: MANNER RUNNINGLYINCORRECT ANSWERPut another way, the comparison of internalrepresentations helps in the diagnostics of semanticerrors and appropriateness, which are twodifferent notions.
For instance ?John loves Marry?is a semantically correct sentence, but it is not anappropriate answer when the system expects ?Johnran to the house?.
Further discussions in (Dorr,1995) show that the matching scenario is veryuseful in question-answering lessons, which areformulated as sets of free response questionsassociated with a picture or text in the targetlanguage.
In an authoring session, the lessondesigner enters the texts, the questions and asample appropriate answer to each question.
Atlesson time, the questions are presented to thestudent who answers them.
If the predefinedanswers are general enough, the system willflexibly recognise a set of possible answers.
Forinstance, the student might answer:Juan died   or Carlos killed Juan   orCarlos murdered Juanto the question ?What happened to Juan?, whichchecks the comprehension of a simple newspaperarticle.
The matching technique can be extended tocheck whether the translations of sentences into thetarget language are correct etc.
Even as an earlierimplementation at the ?concept demonstrationstage?, this prototype identifies possible solutionsfor the integration of semantic analysis in CALL.A recent system Why2-Atlas (VanLehn, 2002),based on deep syntactic analysis and compositionalsemantics, aims at the understanding of studentessays in the domain of physics.
Why2-Atlas isdeveloped within the project Why2 where severaldifferent NL processing techniques are compared(Rose, 2002).
The sentence-level understanderconverts each sentence of the student's essay into aset of propositions.
For instance, the sentence?Should the arrow have been drawn to pointdown?
?is to be (roughly speaking) converted toeevents, vvectors, sdraw(e,s,v) & tense(e, past)&mood(e,interrog)&direction(v,down).As the authors note in (VanLehn, 2002), this isjust an approximation of the real output, whichillustrates the challenge of converting words intothe appropriate domain-specific predicates.
Theleft-corner parser LCFlex copes withungrammatical input by skipping words, insertingmissing categories and relaxing grammaticalconstraints as necessary in order to parse thesentence.
For instance, ?Should the arrow havebeen drawn point down??
would parse.
In case oftoo many analyses, the parser uses statisticalinformation about the word roots frequency andthe grammatical analyses in order to determine themost likely parse.
If no complete analysis can beproduced, a fragmentary analysis will be passed forfurther processing.
The fragments present?domain-specific predicates that are looking forargument fillers, and domain-specific typedvariables that are looking for arguments to fill?.
Ifthe symbolic approach for input analysis vialogical forms fails, a probabilistic one will be usedas an alternative.What is particularly interesting for us here, is thediscourse-level understander (VanLehn, 2002)which, given logical forms, outputs a proof.Topologically, this is a forest of interwoven trees,where the leaves are facts from the problemstatement or assumptions made during the proofconstruction.
The roots (conclusions) are student?spropositions.
Consider the example:Question: Suppose you are in a free-fallingelevator and you hold your keys motionless infront of your face and then let go.
What willhappen to them?
Explain.Answer: The keys will fall parallel to theperson face because of the constant accelerationcaused by gravity but later the keys may go overyour head because the mass of the keys is less.The essay answer will be translated into fourpropositions, which will be passed to the discourseunderstander.
The first one (keys fall parallel to theperson's face) is correct and becomes the root ofthe proof.
The second one (gravitationacceleration is constant) corresponds to facts fromthe knowledge base.
The third proposition (keys goover the person's head) is based on the commonmisconception that heavier objects fall faster,which is pre-stored in the knowledge base as well,it becomes the root of the proof.
The last one (themass of the keys is less) corresponds to a node ofthe interior of the proof of the third proposition.Once a proof has been constructed, a tutorialstrategist performs an analysis in order to findflaws and to discuss them.
Here the major one isthe misconception ?heavier objects fall faster?.
Thetutoring goals have priorities as follows: fixmisconceptions, then fix self-contradictions, errorsand incorrect assumptions, and lastly elicit missingmandatory points.
The Why2 project in general,and Why2-Atlas in particular, illustrate the recenttrends in the ITS development:(i) mixture of symbolic and stochastic appro-aches in order to cope with the free NL input;(ii) application of shallow and partial analysis asan alternative to the deep understanding;(iii) integration of AI techniques (esp.
reasoningand personalisation);(iv) organisation of bigger projects withconsiderable duration to attack the wholespectre of problems together (incl.development of authoring tools, systematicuser evaluation at all stages, severaldevelopment cycles and so on).We are experienced in the application ofsemantic analysis to CALL in two scenarios.
Thefirst one1, in 1999-2002, deals with deepunderstanding of the correct sentences and provingthe domain correctness and the appropriateness ofthe logical form of each one.
The second onefocuses on the integration of  shallow analysis andpartial understanding in CALL (Boytcheva, 2004).1 In Larflast, a Copernicus Joint Research Project.The system described in (Angelova, 2002) is alearning environment for teaching Englishfinancial terminology to adults, foreigners, withintermediate level of English proficiency.
Theprototype is a Web-based learning environmentwhere the student accomplishes three basic tasks:(i) reading teaching materials, (ii) performing testexercises and (iii) discussing his/her own learnermodel with the system.
The project is oriented tolearners who need English language competence aswell as expertise in correct usage of Englishfinancial terms.
This ambitiously formulatedparadigm required the integration of some formaltechniques for NL understanding, allowing foranalysis of the user?s answers to drills where thestudent is given the opportunity to enter freenatural language text (normally short discourse of2-3 sentences).
The morphological, syntax andsemantic analysis is performed by the systemParasite (Ramsay, 2000), developed in UMIST.After the logical form has been produced for eachcorrect sentence, the CALL environment has todetermine whether the student?s utterance matchesthe expected appropriate answer in the currentlearning situation.
A special prover has beendeveloped, which checks whether the logical formof the answer is ?between?
the minimum andmaximum predefined answers (Angelova, 2002).Unlike MILT (Dorr, 1995), we think that thecorrect answer has to be subsumed by themaximum expected one, i.e.
there is not only alower but also an upper limit on the correctness.Table 1 lists examples for all diagnostic cases fromuser?s perspective, by sentences in naturallanguage.
Please note that nowadays the deductiveapproach can be relatively efficient, as our prover(in Sicstus Prolog) works on-line, integrated in aWeb-based environment, in real time with severalhundred meaning postulates.
Proofs are certainlybased on a predefined ontology of the domainterms, which in this case is a lexical one since theterms are treated as words with special lexicalmeaning encoded in the meaning postulates thusforming a hidden hierarchy of meanings.
Theconceptual and lexical hierarchy of meanings arefurther discussed in (Angelova, 2004).However, we discovered that deep semanticanalysis is difficult to integrate in CALL.
First, thisrequires enormous amount of efforts for themeaning postulates acquisition.
While hierarchy ofterms is reusable, as it is in fact the domain model,the propositions, which encode the lexicalsemantics are somewhat application and domainspecific and therefore difficult to reuse or totransfer  to another domain (moreover they arebound to the domain words).
Implementing theprover and testing the definitions and the inferenceTable 1: Decisions about erroneous answers according to the configuration of the logical forms of thepredefined minimal, maximal and the current learner?s answer (see also Angelova, 2002).procedures with several hundred predicatesrequired approximately one man-year for an AIexpert who worked closely with domain experts.Second, the result is not perfect from theperspective of the user who has to answer withcorrect and full sentences (see section 5 fordetails).
Thus our recent work (Boytcheva, 2004)is directed towards integration of shallow anddeep semantic techniques in CALL systems.
Weuse shallow parsing, which allows for theprocessing of both syntactically incorrect andincomplete answers.
However, during the user?sCase Sample of learner?s utterance DiscussionKernel (predefinedminimum answer)Primary market is a financial market thatoperates with newly issued debt instrumentsand securities.The logical form is pre-stored in thesystem as a Kernel.Cover (predefinedmaximum answer)Primary market is a financial market thatoperates with newly issued debt instrumentsand securities and provides new investmentsand its goal is to raise capital.The logical form is pre-stored in thesystem as a Cover.1.Correct answer Primary market is a financial market thatoperates with newly issued debt instrumentsand securities and provides new investments.This logical form is between theKernel and the Cover.2a) IncompleteanswerPrimary market is a financial market thatoperates with newly issued securities.Missing Kernel term: debtinstruments.2b) Specialisationof concepts fromthe definitionPrimary market is a financial market thatoperates with newly issued bonds.Bond is a specialisation of security;Missing: debt instruments.2c) Paraphraseusing the conceptdefinitionPrimary market is a financial market thatoperates with new emissions of stocks, bondsand other financial assets.New emissions = newly issued;stocks, bonds and other financialassets = debt instruments andsecurities.3a) Partially correct Primary market is a financial market thatoperates with newly issued debt instrumentsand securities for instant delivery.Wrong: for instant delivery.3b) Generalisationof concepts fromthe definitionPrimary market is a market that operateswith newly issued financial instruments.Market is a generalisation offinancial market; Financialinstruments are generalisation of debtinstruments and securities.4.
Partially correct Primary market is a financial market thatoperates with newly issued securities forinstant delivery and provides newinvestments.Wrong: for instant delivery;Missing: debt instruments.5.
Wrong answer Primary market is an organisation in whichthe total worth is divided into commercialpapers.Wrong: an organisation in which thetotal worth is divided intocommercial papers;Missing: financial market thatoperates with newly issued debtinstruments and securities.6.
Wrong answer Primary market provides new investments forinstant delivery.Wrong: for instant delivery;Missing: financial market thatoperates with newly issued debtinstruments and securities;7.
Partially correct Primary market is a financial market thatoperates with newly issued securities andprovides new investments.Missing: debt instruments.8.
Wrong answer Primary market provides new investments.
Missing: financial market thatoperates with newly issued debtinstruments and securities.utterances evaluation we use deep semanticanalysis concerning the concepts and the relationsthat are important for the domain only.
Users?answers are represented as logical forms,convenient for the inference mechanism, whichtakes into account the type hierarchy and iselaborated in domain-specific points only.
Thusthe combination of shallow and deep techniquesgives the users more freedom in answering, i.e.various utterances to express themselves withoutimpeding the evaluation process.
The idea toapply the shallow NLP techniques in CALL wasinspired by their successful application in IE fortemplate filling.
The assessment of userknowledge in a specific domain can be viewed asa kind of  template filling, where the templatescorrespond to concepts and relations relevant tothe tested domain.3 Exploiting Document Proximity in ITSThere is a huge demand for intelligent systemsthat can handle free texts produced by the learnersin eLearning mode.
As most of the courses beingtaught are represented as texts, the challenge is tocompare one text to another.
Since the phrasingwill not be the same in both texts, the comparisonneeds to be performed at the semantic level.
Onesolution is sketched above: translate the student?stext to a set of logical forms and then applysymbolic approaches for their assessment.Unfortunately, there are only few researchprototypes that address the problem from thisperspective, which are very expensive and havedelivered only partially applicable results so far.Another option is to try to exploit  the IRtechniques we have at hand in order to check forinstance whether the student?s answer contains the?right words?
(in which case it would be a goodwriting, since it would be similar to theexpectation).
A natural choice for assessing theusage of the ?right words?
is the so-called LatentSemantic Analysis (LSA) as it reveals the latentlinks between the words and phrases, especiallywhen it is trained with enough samples.
Below webriefly overview the application of LSA ineLearning and our experiments in this direction.The classical LSA method, as proposed in(Deerwester, 1990) is a bag-of-words technique,which represents the text semantics by assigningvectors to words and texts (or text fragments).Indeed, knowing how words are combined toencode the document knowledge is a kind ofsemantic representation of the word meaning andtext semantics.
The underlying idea is that wordsare semantically similar, if they appear in similartexts, and texts are semantically similar, if theycontain similar words.
This mutual word-textdependency is investigated by building a word-text matrix, where each cell contains the numberof occurrences of word X in document Y, afterwhich the original matrix is submitted to SingularValue Decomposition ?
a transformation that ismeant to reveal the hidden (latent) similaritybetween words and texts.
This produces a vectorof low dimensionality (the claim is that 300 isnear optimal) for each word and for each text.
Thesimilarity between two words, two texts, or aword and a text, is given by the cosine of theangle between their corresponding vectors (thecosine is the most popular similarity measure).Therefore, the similarity between two words ortwo sets of words is a number between ?1 (lowestsimilarity) and 1 (highest similarity).
Withoutmorphology and grammar rules, syntacticalanalysis, and manually encoded implicitknowledge, LSA is considered successful invarious experiments including assessment ofstudent essays.For the purposes of assessment, usually a high-dimensional space is computed from textsdescribing the domain (most often the availableelectronic version of the course).
Each word fromthe domain as well as the student?s essay arejuxtaposed a vector, usually a 300-dimensionalone.
The student gets as feedback an assessmentscore and/or an indication of the topics/aspectsthat are not covered well by the essay.
TheIntelligent Essay Assessor (IEA) (Foltz 1999a,Foltz 1999b) is based on reference texts (manuallypre-graded essays) and assigns a holistic scoreand a gold standard score.
The former iscomputed by seeking the closest pre-graded essayand returning its grade (i.e.
the current one isscored as the closest pre-graded one), while thelatter is based on a standard essay written by anexpert.
It returns the proximity between thestudent?s essay and the expert?s one.
Anexperiment with 188 student essays showed acorrelation of 0.80 between the IEA scores andteacher?s ones, which is a very high similarity.However, IEA outputs no comments or adviceregarding the student essay.
The Apex system(Lemaire, 2001) performs a semantic comparisonbetween the essay and the parts of the coursepreviously marked as relevant by the teacher.
Thewhole student essay is to be compared to each ofthese text fragments.
For instance, if the studenthas to write an answer to the question ?What werethe consequences of the financial crash of 1929?
?,the essay is compared to the following sections ofthe teaching course: The political consequences inEurope, Unemployment and poverty, Theeconomical effects, The consequences until 1940.An experiment with 31 student essays in thedomain of Sociology of Education exhibited  acorrelation of 0.51 between Apex grades andteacher?s ones, which is close to the correlationagreement between two human graders in thisliterary domain.
Select-a-Kibitzer (Wiemer-Hastings, 2000) aims at the assessment of essaycomposition.
Students are required to write ontopics like: ?If you could change something aboutschool, what would you change??.
The assessmentmodule is based on reference sentences of whatstudents usually discuss about school (food,teachers, school hours, etc.).
Several kinds offeedback are delivered to the student, concerningthe text coherence, the kind of sentences or thetopic of the composition.
For example, the adviceregarding coherence can be: ?I couldn?t quiteunderstand the connection between the firstsentence and the second one.
Could you make it abit clearer?
Or maybe make a new paragraph.?
(Here the underlying idea is that totally newwords in the subsequent sentence normallyconcern another topic, i.e.
this fits to a newparagraph).
A principled criticism of these threerecent systems is that the bag-of-words modeldoes not take into consideration the grammarcorrectness and the discourse structure, i.e.
twoessays with the same sentences structured in adifferent order would be scored identically (whichis a funny idea from an NLP perspective).
Afurther example illustrates attempts to combinethe strengths of the bag-of-words and thesymbolic approaches, while trying to avoid someof their weaknesses.
CarmelTC (Rose, 2002), arecent system which analyses essay answers toqualitative physics questions, learns to classifyunits of text based on features extracted from asyntactic analysis of that text.
The system wasdeveloped inside the Why2-Atlas conceptualphysics tutoring environment for the purpose ofgrading short essays written in response toquestions such as ?Suppose you are running in astraight line at constant speed.
You throw apumpkin straight up.
Where will it land?Explain?.
CarmelTC?s goal is not to assign a lettergrade to student essays, but to tally which set of?correct answer?
aspects are present in studentessays (e.g.
a satisfactory answer to the examplequestion above should include a detailedexplanation of how the Newton's 1st law applies tothis scenario.
Then the student should infer thatthe pumpkin and the man will continue at thesame constant horizontal velocity that they bothhad before the release.
Thus, they will alwayshave the same displacement from the point ofrelease.
Therefore, after the pumpkin rises andfalls, it will land back in the man's hands.
The?presence?
of certain sentences is checked byword classification).
The evaluation shows thatthe hybrid CarmelTC approach achieves 90%precision, 80% recall and 85% F-measure, andthus outperforms the pure bag-of-words run ofLSA, which scores 93% precision, 54% recalland 70% F-measure  (Rose, 2002).Our experiments with LSA (Angelova, 2002)were focused on finding financial texts, which areappropriate to be shown as teaching materials in aparticular learning situation.
Given a set of key-words, agents retrieve texts from well-knownfinancial sites and store them to the servers of ourenvironment for further assignment ofappropriateness.
We implemented the classicalLSA scenario and applied it as a filteringprocedure, which assigns off-line a similarityscore to each new text.
The text archive consistedof 800 most relevant readings, which representHTML-pages with textual information (elementssignaling prevailing technical content, e.g.
tables,have been excluded).
These texts are offered assuggested readings but are also used for buildingdynamic concordances, which show samples ofterms usages to the learner.
The latter may bedisplayed in cases of language errors to drillswhere the student makes linguistic mistakes.Choosing this option (view samples) is up to thestudent.
The dynamic nature of the text collectionensures the appearance of new samples, whichmakes the browsing interesting at every run.4 PersonalisationOur learning environment supportspersonalisation as follows: as a step towards instructional as well as contentplanning: a planner (the so-called pedagogicalagent) plans the next learner?s moves across thehypertext pages which, technically, constitutethe Web-site of our tutoring system; these movesare between (i) performing drills and (ii) choicesfor suggestion of readings, which may be eithertexts from Internet or especially generated Web-pages.
The pedagogical agent deals with bothpresentational and educational issues.
The localplanning strategy aims at creating a completeview of the learner?s knowledge of the currentconcept.
It supports movements between drillswith increasing complexity, when the studentanswers correctly.
The global planning strategydetermines movements between drills testingdifferent concepts, going from the simple andgeneral concepts to the more specific andcomplex notions. as a step towards personalised IR: an LSA-filterassigns proximity score to constantly updatedtexts, which are stored as suggested readings.This allows for constant update of the system?stext archive and, following the practice at themain financial sites, provides up-to-date newsand readings, which may be used as texts fordifferent teaching purposes.
As key words forinitial collection of texts, the not_known andwrongly_known terms from the learner?smodels are chosen, so the CALL system alwaysstores the proper relevant text for each student.The adaptivity is provided using an ontology offinancial terms as a backbone of all system?sresources.
No matter whether these are conceptual(e.g.
knowledge base), linguistic (e.g.
lexicons,meaning postulates, etc) or pedagogical resources(e.g.
set of preliminary given drills or learnermodel, which is dynamically constructed at run-time), the ontology always represents the unifyingskeleton as all chunks of knowledge are organisedaround the terms?labels.
In addition to the is-apartition, we support in the knowledge baseexplicit declarations of the perspectives orviewpoints.
E.g., the isa_kind/4 clause:isa_kind(security, [bond, hybrid_security, stock],[exhaustive, disjoint],?status of security holder: creditor or owner?
)means that the securities are disjoint andexhaustively classified into bonds, stocks andhybrid securities depending on the status of theirowner.
These comments provide nice visualisation(Angelova, 2004).5 User Study and User EvaluationLarflast started with a user study of howforeigners ?
adults acquire domain terminology intheir second language.
In fact the acquisition isclosely related to the elicitation of domainknowledge, especially in a relatively new domain(students have to learn simultaneously a subjectwith its terminology and its specific languageutterances).
Mistakes are linguistically-motivatedbut wrong domain conceptualisations contributeto the erroneous answers as well.
Erroneousanswers appear in terminology learning due to thefollowing reasons: Language errors (spelling, morphology,syntax); Question misunderstanding, which causeswrong answer; Correct question understanding, butabsent knowledge of the correct term,which implies usage of paraphrases andgeneralisation instead of the expectedanswer; Correct question understanding, butabsent domain knowledge, which impliesspecialisation, partially correct answers,incomplete answers and wrong answers.This classification influenced considerably thedesign of the prover?s algorithms, i.e.
the decisionhow to check of the appropriateness of the studentanswer.
The diagnostics shown in Table 1 followsclosely the four reasons above.Our learning prototype was tested by (i) twogroups of university students in finance withintermediate knowledge of English, (ii) theiruniversity lecturers in English, and (iii) a group ofstudents of English philology.
The system wasevaluated as a CALL-tool for self-tuition andother autonomous classroom activities, i.e.
as anintegral part of a course in ?English for SpecialPurposes?.
The learners could test theirknowledge through the specially designedexercises, compare their answers to the correctones using the generated feedback (immediate,concrete and time-saving, it comes in summaryform, which is crucial in order to accomplish thesystem?s use autonomously) and extract additionalinformation from the suggested readings andconcordancers.The users liked the feedback after performingdrills, immediately after they prompted erroneousanswers to exercises where this term appears.They evaluated positively the visualisation of thehierarchy as well as the surrounding context oftexts and terms usages organised in aconcordancer, which is dynamically built andcentred on the terms discussed at the particularlearning situation.
The teachers were very pleasedto have concordancers with contiguously updatedterm usages; they would gladly see such alanguage resource integrated in a further authoringtool, because searching suitable texts in Internet isa difficult and time-consuming task.Unfortunately the learners were not veryenthusiastic regarding the free NL input, as itpermits relatively restricted simple answers anddoes not go beyond the human capacity of theteacher.
The main disappointment of both learnersand teachers is the system?s inability to answerwhy, i.e.
while the formal semantics and reasoningtools provide extremely comprehensive diagnosticabout the error type, they tell nothing about thereason.
Fortunately, all users liked the fact thatthere were numerous examples of terms usagesfrom real texts whenever morphological or syntaxerrors were encountered in the free NL input.Thus we conclude with a certain pessimismconcerning the appropriateness of today?s formalsemantic approaches in ITS and much optimismthat data-driven corpus techniques, if properlyapplied, fit quite well to the adaptive ITS.
What isstill desirable regarding the filtering module is torestrict the genre of the suggested readings, sincethe current texts are freely collected from theInternet and some of them should be used asteaching materials (LSA cannot recognise the texteducational appropriateness since it considers theterms?
occurrences only; other supervisedtechniques such as text categorisation mightimprove the filtering, if properly integrated).As a possible improvement of the currentparadigm for formal analysis, we turned recentlyto partial analysis, which gives more flexibility tothe students to enter phrases instead of fullsentences (Boytcheva, 2004).6 ConclusionThe conclusion is that teachers as well aslearners like CALL systems that are easy tointegrate in the typical educational tasks, i.e.
thearea of language learning has well-establishedtraditions and the experimental software is well-accepted, only if it is really useful and facilitatesthe learning process.
Our feeling is that allattempts to integrate language technologies inCALL should be closely related to testing thelaboratory software with real students.
At thesame time, cooperation with teachers is anobligatory condition as the necessary pedagogicalbackground is often missing in the researchenvironments where normally the NLPapplications and language resources appear.Language technologies have a long way to go,until they find the proper wrappings forintegration of advanced applications and thenecessary resources into useful CALL systems.References[1] WebCT, http://www.webct.com/[2] CISCO, http://cisco.netacad.net/[3] HotPotatoes, http://web.uvic.ca/hrd/hotpot/Angelova G., Boytcheva, Sv., Kalaydjiev, O.Trausan-Matu, St., Nakov, P. and A.Strupchanska.
2002.
Adaptivity in Web-basedCALL In Proc.
of ECAI?02, the 15th EuropeanConference on AI, IOS Press, pp.
445-449.Angelova G., Strupchanska, A., Kalaydjiev, O.,Boytcheva, Sv.
and I. Vitanova.
2004.Terminological Grid and Free Text Repositoriesin Computer-Aided Teaching of ForeignLanguage Terminology.
Proc.
"LanguageResources: Integration & Development in e-learning & in Teaching ComputationalLinguistics", Workshop at LREC 2004, 35-40.Boytcheva Sv., Vitanova, I., Strupchanska, A.,Yankova, M. and G. Angelova.
2004.
Towardsthe assessment of free learner's utterances inCALL.
Proc.
"NLP and Speech Technologies inAdvanced Language Learning Systems",InSTIL/ICALL Symposium, Venice,17-19 June.Deerwester S., Dumais S.T., Furnas G.W.,Landauer T.K., and Harshman R. 1990.Indexing by latent semantic analysis, Journal ofthe American Society for Information Science,41(6), pp.
391?407.Dorr, B., Hendler, J., Blanksteen, S. and B.Migdaloff.
1995.
On Beyond Syntax: Use ofLexical Conceptual Structure for IntelligentTutoring.
In (Holland, 1995), pp.
289-311.Foltz P.W., Laham D., and Landauer T.K.
1999.Automated essay scoring: Applications toeducational technology, In Proceedings of theED-MEDIA Conference, Seattle.Foltz P.W., Laham D., and Landauer T.K.
1999.The intelligent essay assessor: Applications toeducational technology, Interactive MultimediaElectronic Journal of Computer-EnhancedLearning, 1(2).Gamper, J. and J. Knapp.
2002. Review ofintelligent CALL systems.
Computer AssistedLanguage Learning 15/4, pp.
329-342.Holland, M., Kaplan, J. and R. Sams (eds.)
1995.Intelligent Language Tutors: Theory ShapingTechnology.
Lawrence Erlbaum Associates, Inc.Lemaire B. and Dessus P. 2001.
A system toassess the semantic content of student essays, J.of Educ.
Computing Research, 24(3), 305?320.Nerbonne, J.
2002.
Computer-Assisted LanguageLearning and Natural Language Processing.
In:R. Mitkov (Ed.)
Handbook of ComputationalLinguistics, Oxford Univ.
Press, pp.
670-698.Ramsay, A. and H. Seville.
2000.
What did hemean by that?
Proc.
Int.
Conf.
AIMSA-2000,Springer, LNAI 1904, pp.
199-209.Rose, C.P., Bhembe, D., Roque, A., Siler, S.,Srivastava, R. and K. van Lehn.
2002.
A hybridlanguage understanding approach for robustselection of tutoring goals.
.
In Proc.
of the Int.Conf.
Intelligent Tutoring Systems, Springer,LNCS, 2363: 552-561VanLehn, K., Jordan, P., Rose, C., Bhembe, D.,Boettner, M., Gaydos, A., Makatchev, M.,Pappuswamy, U., Rindenberg, M., Roque, A.,Siler, A. and Srivastava, R. 2002.
TheArchitecture of Why2-Atlas: A Coach forQualitative Physics Essay Writing.
In Proc.
ofthe Int.
Conf.
Intelligent Tutoring Systems,Springer, Lecture Notes in CS, 2363: 158-162.Wiemer-Hastings P. and Graesser A.
2000.
Select-a-kibitzer: A computer tool that givesmeaningful feedback on student compositions,Interactive Learning Environments, 8(2), pp.149?169.
