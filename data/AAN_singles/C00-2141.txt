Local context templates for Chinese constituent boundarypredictionQiang ZhouThe State Key Laboratory of Intelligent Technology and SystemsDept.
o1' Computer Science and technology,Tsinghua University, Beijing 100084zhouq @ s 1000e.cs.t singhu a.ed u.cnAbstract:in this paper, we proposed a shallowsyntactic knowledge description:constituent boundary representation a d itssimple and efficient prediction algorithm,based on different local context templateslearned fiom the annotated corpus.
An opentest on 2780 Chinese real text sentencesshowed the satisfying results: 94%(92%)precision for the words with multiple(single) boundary tag output.llt simplified the complex constituent levels inparse trees and only kept the boundaryinformation of every word in differentconstituents.
Then, we developed a simple andefficient constituent boundary predictionalgorithm, based on different local contexttemplates learned flom the annotated corpus.
Anopen test on 2780 Chinese real text sentencesshowed the satisfying results: 94%(92%)precision for lhe words with multiple (single)boundary lag output.I.
IntroductionResearch on syntactic parsing has been a focusin natural anguage processing for a long lime.
Asthe developlnent of corpus linguistics, manystatistics-based parsers were proposed, such asMagerman(1995)'s statistical decision tree parser,Collins(1996)'s bigram dependency model parser,1;/atnaparkhi(1997)'s maximum entropy modelparser.
All of lhem fried to get the complete parsetrees of the input sentences, based on thestatistical data extracted l'rom an annotated corpus.The besl parsing accuracy of these parsers wasabout 87%.Realizing the difficulties o1' complete parsing,many researches turned to explore the partialparsing techniques.
Church(1988) proposed asilnple stochastic technique for lecognizing thenon-recursive base noun phrases in English.\;outilaimen(1993) designed an English nounphrase recognition tool --~ NPTbol.
Abney(1997)applied both rule-based and statistics-basedapproaches for parsing chunks in English.
Due tothe advantages of simplicity and robustness, thesesystems can be acted as good preprocessors forthe further colnplete parsing.In tiffs paper, we will introduce our partialparsing aPl)roach for the Chinese language.
Wefirst proposed a shallow syntactic knowledgedescription: constituent boundary representation.2.
Constituent boundary descriptionThe constituent boundary representationcomes fl'om the simplification of the completeparse Irees of the senlences.
It omits theconstituenfl levels in parse trees and only keepsthe boundary information of every word indifferent constituents, i.e.
it is at the left boundary,right boundary or middle position of aconstituent.l~,vidently, if the input sentence has only oneparse lree, i.e.
without syntactic ambiguity, theconstituent boundary position of every word inthe sentence is clear and definite.
In the sense, theconstituent boundary tag indicates the basicsyntactic structure information in the sentence.Separating them frolll the constituent smlcturetree and assigning them Io every word in thesentence, we can form a special syntactic unit:word botmdao, block (WBB).Definition: A word boumla O, block is lhecombination o1' the word(including part-of-speechinformation) and its constituent boundary tag, i.e.wbb~=<%, b~>, where % is the ith word in thesentence, b~ can value 0,1,2, which means % is atI llereafler, 'constiluent' represents all internal or rootnodes in a parse tree, i.e.
phrase oF sentence tags.
Inour syslem, each consliluen( must consist of two ormore words{leaf node in parser tree).975the middle, left-most, or right--most position of aconstituent respectively.In the view of syntactic description capability,the WBBs defined above, the chunks defined byAbney(1991) and the phrases(i.e, constituents)defined in a parse tree have the followingtealtions: WBBs < chunks < phrasesHere is an example:?
The input sentence (10 words):(My brother gives him a book.)?
Its parse tree representation (7 phrases):1,,, \[,,~ \[,,~ @~ t'('J ~ '}  \] \[,,4 l,,~ ~(t T \]41J~ \[1'6 \[,'~ - -  $ \] -I~ \]1\] O \]?
l t s  chunk representation (5 chunks):?
Its constituent boundary tepresentation(10 WBBs): <~f~,l> <l'l<J,0> <~'~'A\],2> <)(\],1> <T,2> <41g,0> <- ,1> </l<,2> <:1~,2> <o ,2>The goal of the constituent boundaryprediction is to assign a suitable boundary tag forevery word in the sentence.
It can provide basicinformation for further syntactic parsing research.The following lists some application examples:?
To develop a statistics-based Chineseparser(Zhou 1997) based on tile bracketmatching principle(Zhou and Huang, 1997).?
To develop a Chinese maxinmm nounphrase identifier(Zhou,Sun a d Huang, 1999).?
The automatic inference of Chineseprobabilistic context-five grammar(PCFG)(Zhou and throng 1998).3.
Loca l  context  templatesThe linguistic intuitions tell us that many localcontexts may be useful for constituent boundaryprediction.
For example, many function words inChinese have their certain constituent boundaryposition in the sentences, such as, mostprepositions are at the left boundaries, and theaspectual particles ("le", "zhe", "guo") ate at theright boundaries.
Moreover, seine content wordsalso show their pteferential constituent boundarypositions in a special ocal context, such as mostadjectives arc at the right boundary in localcontext: "adverb + adjective".A tentative idea is how to use such simplelocal context information(including the part-of-speech(POS) tags and the number of Chinesecharacters(CN)) todevelop an elTicient automaticboundary prediction algorithm.
Therefore, wedefined the following local context templates(LCTs):1) Unigram POS template: t~, BPFL,2) Bigram POS templates:, Left restriction: t~_, t~, BI'I;L~?
Right restriction:L t~+,, BPFL~3) Trigram POS template: t~_~t~t~+~, BPFL,4) Trigram POS+CN template: t~_~+cn~_~t~+cn,~ t~+~+cn~+~, BPFL~In tile above LCTs, t~ is tile POS tag of the ithword in the sentence, cn~ is its character number,and BPFL~ is the frequency distribution list of itsdiffetent BP(boundary prediction) value(0,1,2)under the local context restrictions(I.CR)(the leftand right word).Table 1 Some examples of the local contexttemplatesI ~  TokenUnigram p,39 849 476. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.bigram a n,(left) 5 164 2007bigram a n,(right) 4 2012 160MeaningA preposition is prior toat the constituent leftboundary in Chinese.A noun is prior to at theright boundary if itsprevious word is anadjective.An adjective is prior toat llle left boundary ifits nexl word is a noun.Trigranl n n u.Jl)l'~, A noun is priorlo at tilePOS 1 18 1496 right boundary if itsprevious word is a normand its next one is a. .
.
.
.
.
.
.
.
.
/ra_r_tial(De).Table shows some examples of LCTs.
Allthese templates can be easily acquired fiom theChinese treebanks or (.hme:e ~ " s corpus annotatedwith constituent boundary tags.Among these templates, some special oneshave the following properties:a) TIV~ = ~ BPI;L, \[bl),\] >(z,b) 3bl),e 10,21, P(bPALCR)=BPFL, \[1%11 7T, > f3where the total frequency threshokl o~ and the BPprobability threshold f~ are set to 3 and 0.95,respectively.
They are called the proiectcdtemplates (PTs) (i.e.
the local context templatewith a projecting BP wflue).Based on the different PTs, we can design athtee-stage training procedure to overcome tileproblem of data sparseness:976Slage 1 ?
I,carn the unigram and bigramtClllplales Oil the whole inslallCOS in annolated COl|mS.Slago 2 : I,carn lhc Irigraln P()S lonlphltos Oil thenonq~rojocled unigraili and bigraiil illSlallCCs (see i1o?lsection for illOl'O dclailod).Slago 3: \[,carll lho lfigfaill PO,q-I-CN Ioinphilesoil Iho non-projcclt;d Irigrani P()~ iliSlanccs.Therefore, only the useful trigranl templatesCall be lear|led.4.
Automat ic  p red ic t ion  a lgor i t lnnAfter getting lhc LCTs, the auloillaticprediction algorithm becomes very simple: 1) tosot the protecting BPs based on the projectedLCTs, 2) to select he best l:IPs based on tile lion-pro|coted LCTs.
Sonic detailed inl~rmation willbe discussed in the 12fllowing sections.4.1 Set tile project ing l ipsIn this stage, tile refercllce seqlielice lo theLCTs is : unigfanl ~ I)igralil ~ higranl  POStiigranl I>OS+CN, i.e.
l:ronl the rough roslriclionLcrrs to tile tight restriction L(\]Ts.
This sequenceis same with the LCT training procedure.The detailed algoritlnn is as follows:Input: the position of the/ill word in the sentence.Background: the LCTs learned |'1o111 corpus.Output: the pro|coting BP of the word - if' fo/lnd;- 1 - otherwise.Procedure:?
Gel lhc local context of the iih word.Ill I f  its unigran\] tonal)late is a PT, thor| returnits projecting BP.?
If its left and right bigram template satisfythe following conditions:> rE,+ TF,~- Z SmFL, Ijl +Z BPFL, Ijl > a> p0,/,,I Lc#O : (BPFL, Ijl + nS'FL.
Ijl) /(77:, + 77=,< ) > I~thor  returl l  this combined  pro jec t ing  l~P(l)l@?
If its trigram POS template is a PT, thenretul'u its protecting BP.?
If its trigram POS+CN template is a PT,then return its projecting BP.4.2 Select the best liPsIn this stage, tile reference sequence to theLCTs is : trigram POS+CN --> trigram POS "->bigram ---7 unigram.
It's a backing-off model(K.atz, 1987), just like the approach of Collins andBrooks(1995) for the prepositional phraseallachineilt plot)loin in English.
The detailedalger|finn is as follows:Input: the position of the ith word in lho sentence.Background: lho LCTs learned from corpus.Output: tile best BP of the word.Procedure:?
Get the local context of tile ith word.o For tile kth nlatched lrigram POS+CNtonlplatos, i f  77,'x > CZ, lhen rolHrll SelectBestl l l  >UHJFL,).?
|Sor the ruth niatchod loft bigranl and nthmatched righl: bigrain,)~ Gel lho Combined BI 'FL = Blqq,  + IHq<L,,lJ" TFc<,,,,I,i,,,',l ,<',,,H ..... > 0, then rol!.lrilSelectlJestBP(C<mibined Blqq O.I For lhe kth nlatched unigram templates, if7"/~ > 0, lhen relurn SelectlJestBl~(17PlrLk).I 17,olurn l(dol\]nilt is at the loft t)oundary).The internal function SelectBeslBP() tries toselect the best BP based on the fi'equencydistribution list of different BP wllue in LCTs.
Ithas two output modes: I) single-output mode:only output the best t3t > with the highestfiequency in the LCT; 2) nmltiple-outlmt mode:outpul the BPs salisfying the conditions:\[/',,,,,-Pt,,.,,\[ < 7, where 7 = 0.25.
Exper imental  results5.1 Training and test dataThe training data were extracted fl'Olll twoditTerent parts of annotated Chinese corpus:1) The small Chinese treebank developedin Peking University(Zhou, 1996b), whichconsists of the sentences extracted fiomtwo parts of Chinese texts: (a) test set forChinese-Englisla machine transhltionsystems, (b) Singapore priiaary schooltextbooks.2) The test suite treebank beingdeveloped in Tsinghua University(Zhouand Sun, 1999), which consists of about10,000 representative Chinese sentencesextracted from a large-scale Chinesebahmced corpus with about 2,000,000Chinese characters.The test data were extracted from the articlesof People's Daily and manually annotated with977correct constituent boundary tags.
It was alsodivided into two parts:1) The ordinary sentences.2) The sentences with keywords forconjunction structures (such as theconjunctions or special punctuation'DunHao').
They can be used to test theperformance of our prediction algorithmon complex conjunction structures.Table 2 shows some basic statistics of thesetraining and test data.
Only the sentences withmore than one word were used for training andtesting.Table 2 The basic statistics of training andtest data.
(ASL = Average sentence length)TrainlTrain2TestlTest2Sent.
Word Char.
ASLNum.
Num.
Num.
(w/s)5573 64426 89492 11.567774 108542 173334 13.962780 68986 108218 24.821071 32358 51169 30.215.2 The learned templatesAfter the three-stage l arning procedure, wegot four kinds of local context emplates.
Table 3shows their different distribution data, where thesection 'Type' lists the distribution of differentkinds of LCTs and the section 'Token' lists thedistribution of total words(i.e, tokens) covered bythe LCTs.
In the colmnn 'PTs' and 'Ratio', theslash '/' was used to separate the PTs with totalfrequency threshold 0 and 3.More than 66% words in the training corpuscan be covered by the unigram and bigram POSprojected templates.
Then only about 1/3 tokenswill be used for training the trigram templates.Although the type distribution of the trigramtemplates hows the tendency of data sparseness(more than 70% trigram projected templates withtotal fi'equency less than 3), the useful trigramtemplates (TF>3) still covers about 70% tokenslearned.
Therefore, we can expect hat them canplay an important role during constituentboundary prediction in open test set.5.3 Prediction resultsIn order to evaluate the performance of theconstituent boundary prediction algorithm, thefollowiug measures were used:1) The cost time(CT) of the kernalfunctions(CPU: Celeron TM 366, RAM: 64M).2) Prediction precision(PP) =number of words with correct BPs(CortBP)total word number (TWN)For the words with single BP output, thecorrect condition is:Annotated BP = Predicted BPFor the words with nmltiple BP outputs, thecorrect conditiou is:Annotated BP ~ Predicted BP setTile prediction results of the two test sets wereshown in Table 4 and Table 5, whose firstcolumns list the different emplate combinationsusing in the algorithm.
In the columns 'CortBP'and 'PP', the slash '/' was used to list thedifferent results of the single and multiple BPoutputs.After analyzing the experimental results, wefound:1) The POS information in local context isvery important for constituent boundaryprediction.
After using the bigram and trigramPOS templates, the prediction accuracy wasincreased by about 9% and 3% respectively.But the chmacter number information showslower boundary restriction capability.
Theirapplication only results in a slight increase ofprecision in single-output mode but a slightdecrease in lnultiple-output lnode.Table 3 Distribution data of different learned LCTsLCTsl -gram2-gram(Left)2-gram(Right)3-gram (POS)3-graln(P+CN)Total591448TypePTs(c~=0/3)241030/591Ratio(tz=0/3)40.6871.13/40.81Total171705171705TokenPTs(o~=0/3)5393287027/86339Ratio0x=0/3)31.4150.68 / 50.281440 1008/567 70.00/39.38 171705 99443/98754 57.92/57.513105 2324 !
713 74.85 / 22.96 50333 24280 / 21982 48.24 / 43.072553 1677 / 287 65.69 / I1.24 19098 5978 / 4079 31.30 / 21.36978Table 4 Experimental results of the test set 1Set the Projecting BPs templatesusedI -gram 22408+2-gram 46167TWN \[ CortBP22143452855396955866PP(%)98.8298.0997.5697.40TWN CortBP46555 32876/2437422796 16188/1767813642 9946/1098611603 8168/8955+3-gram 55321POS+3-gram 57360P+CNSelect he best BPsPP(%)70.62/73.8471.01/77.5572.90/80.5370.40/77.18TWNTotalCortBP PP(%) CT68963689636896355019/5651761473/6296363915/6495579.78/ 14/1681.9589.14/ 11/1591.3092.68/ 13/I 194.1968963 64034/ 92.85/64821 93.9911/142) Most of the prediction errors can beattributed to the special structures in thesentences, uch as col!iunction structures (CSs)or collocation structures.
I)ue to the longdistance dependencies among them, it's verydifficult to assign the conect botmdary lags tothe words in these structures only according tothe local context emplates.
The lower overallprecision of the test set 2 (about 2% lowerthan tesl set 1) also indicates the boundaryprediction difficulties of the conjunctionstructures, because there are more CSs in testset 2 than in test set I.3) The accuracy of the multiple outlmlresults is about 2% better than the singleOUtlmt results.
But the words with multipleboundary tags constitute only about 10% ofthe tolal words predicted.
Therefore, themultil)le-output mode shows a good trade-offbetween precision and redundancy.
It can beused as the best preprocessing data for thefurther syntactic parser.4) The maximal ratio of the words set byprqjected templates can reach 80%.
Itguarantees the higher overall pl'ecisioiLTable 5 Experimental results of the test set 25) Tile algoritlml shows high efficiency.
Itcan process about 6,000 words per second(CPU: Celeron TM 366, RAM: 64M).5.4 Compare with other workZhou(1996) proposed a constituent boundaryprediction algorithm based on hidden Marcovmodel(HMM).
The Viterbi algorithm was used tofind the best boundary path B':B' = arg max P (W,  T \[ B )P (B)II= arg  max I'(CT, Ib,)P(t, iII,,-,)i - Iwhere the local POS probability l ' (C~ \[ b) wascomputed by backing-off model and the bigramparameters:./(/~, t, , b) and.l(b~ , t,, \[i+l)"To compare its 19erformance with ouralgorithm, the trigram (POS and POS+CN)information was added up to its backing-offmodel.
Table 6 and Table 7 show the predictionresults of lhc HMM-based algorithm, based on thesame parameters learned from training set 1 and2.Table 6.
Prediction results of the HMM-based_ _ ITemplates Set tile Projecti~Used L TWN CoriBPI -gram 9873+2-gram 20454+3-D'am 24079POS+3-gram 24866P+CNPP(%)98.5797.0096.42Select ile best BPsfWN CortgP I )P(%)II22342 15737/1 70.44/6593 74.2711273 7856/ 70.44/8607 74.277384 5225/ 70.76/5777 78.246519 4525/ 69.40/4958 76.0596.23TotalTWN32358323583235832358CortBP I25610/2646628310/2906129304/2985629390/29824PP(%)79. t 5/81.7987.49/89.8190.56/92.2790.83/92.17\[\[ CT6/54/43/68/6979algorithm(test set 1)TWN CortBP PP(%) Ctime2-gram 68963 60908 88.32 144+ 3g-POS 68963 63397 91.93 138+3g-P+CN 68963 63649 92.29 139Table 7.
Prediction results of the HMM-basedalgorithm(test set 2)+2-gram+3g-POS+3g-P+CNTWN CortBP PP(%)32358 27792 85.8932358 28918 89.3732358 29030 89.72Ctime687O68The performance of the LCT-based algorithmsurpassed the HMM-based algorithm inaccuracy(about 1%) and efficiency (about 10times).Another similar work is Sun(1999).
Thedifference lies in the definition of the constituentboundary tags: he defined them between wordpair: w; /_?
w;~;, not for the word.
By using theHMM and Viterbi model, his algorithm showedthe similar performance with Zhou(1996) (usingbigram POS parameters):?
Training data : 3051 sentences extractedfrom People's Daily.?
Test data: I000 sentences.?
Best precision:86.3%6.
ConclusionsThe paper proposed a constituent boundaryprediction algorithm based on local contexttemplates.
Its characteristics can be summarizedas follows:?
The simple definition of the local contexttemplates made the training procedure veryeasy.?
The three-stage training procedureguarantees that only the useful trigramtemplates can be learned.
Thus, the datasparseness problem was partially overcome.?
The high coverage of different ypes ofprojected templates assures a higher overallprediction accuracy.?
The multiple output mode provides thepossibility to describe different boundaryambiguities.?
The algorithm runs very fast, surpassesthe HMM-based algorithm in accuracy andefficiency.There are a few possible improvement whichmay raise performance flwther.
Firstly, somelexical-based templates, such as prepositions asleft restriction, may improve performance further- this needs to be investigated.
The introductionof the automatic identifiers for some specialstructures, such as conjunction structures orcollocation structures, may reduce the predictionerrors due to the long distance dependencyproblem.
Finally, more training data is ahnostcertain to improve results.AcknowledgementsThe research was supported by NationalNatural Science Foundation of China (NSFC)(Grant No.
69903007).ReferencesAbney S. (1991).
"Parsing by Chunks", In RobertBerwick, Steven Abney and Carol Tenny (eds.
)Principle-Based Parsing, Kluwer AcademicPublishers.Abney S. (1997).
"Part-of-speech Tagging and PartialParsing", Ill Young S. Bloothooft G.
(eds.)
Corpus-based ntethods in language and speech processings,118-136.Collins M. and Brooks J.
(1995) "Prepositional PhraseAttachment through a Backing-OIT Model", InDavid Yarowsky & Ken Church(eds.)
Proceedingsof the third workshop on very large corpora, MIT.27-38.Church K. (1988).
"A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text."
In:Proceedings of Second Conference on AppliedNatural Language Processing, Austin, Texas, 136-143.Collins M. J.
(1996).
"A New Statistical Parser Basedon Bigram Lexical Dependencies."
In Proc.
of ACL-34, 184-191.Katz S. (1987).
"Estimation of Probabilities fromsparse data for the language model component of aspeech recogniser".
IEEE Transactions on ASSP,Vol .35, No.
3.Magerman.
D.M.
(1995).
"Statistical Decision-TreeModels for Parsing", In Proc.
o1' ACL-95,276-303.Ratnaparkhi A.(1997).
'% linear observed timestatistical parser based on maximum entropymodels".
In Claire Cardie and RalphWeischedel(eds.
), Second Conference on EmpMcalMethods in Natural Language Proeessing(EMNLP-2), Somerset, New Jersey, ACL.Sun H. L., Lu Q. and Yu S. W.(1999).
"Two-levelshallow parser for t, nrestricted Chinese text", In980Changning Huang and Zhendong l)ong (eds.
)Proceedings of Computalional linguistics, Beijing:Tsinghua University press, 280-286.Votllilanmn A.
(1993).
"NPTool, a delector of EnglishNoun Phrases."
In: Ken Church (ed.)
t'roceedings oflhe Workshop on Very La,ge Corpora: Academicand lnduslrial Perspeelives.
Columbus, Ohio, USA,48-57.Zhou Q.
(1996a).
'% Model for Automalic Predictionof (;hinese l'hrase Boundary I,ocation",Zhou Q.
(1996b).
Phrase Bracketing and Annotatingon Chinese l,anguage Corpus .
l~h.l), l)issertalion,Peking University.Zhou Q.
(1997) "A Statistics-Based Chinese Parser",In l;roc, of lhe Fiflh Wol'kshol~ on Very I,argeCorpora, 4-15.Zhou Q. and l\]uang C.N.
(1997) "A Chincse syntaclicparser based on bracket malehing principle",Communication f COIdPS, 7(2), #97008.Zhotl Q. alld Htlang C.N.
(1998).
"An hll'crenceApproach for Chinese Probabilistic Con/exl-FreeGramnmr", Chinese .Iournal of Computers, 21(5),385-392.Zhou Q. and Sun M.S.
(1999).
"P, uiM a ChineseTrecbank as the lest suite for Chinese parser", InKey-Sun Choi & Young-Soog Chae(cds.
)Proceedings of the workshop MAI,'99, Beijing.
32-37.Zhou (,)., Sun M.S.
and ltuallg C.N.
(1999)"Attlonmlically Identify Chinese Maximal NounPhrases", Technical Report 99001, Slate Key l~ab.
o1'Intelligent Technology and Syslcm,% l)epl, ofComlmter Science and Technology, TsinghuaUniversity.981
