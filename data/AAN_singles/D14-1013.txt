Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 121?130,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsCombining Punctuation and Disfluency Prediction: An Empirical StudyXuancong Wang1,3Khe Chai Sim2Hwee Tou Ng1,21NUS Graduate School for Integrative Sciences and Engineering2Department of Computer Science, National University of Singapore3Human Language Technology, Institute for Infocomm Research, Singaporexuancong84@gmail.com, {simkc, nght}@comp.nus.edu.sgAbstractPunctuation prediction and disfluency pre-diction can improve downstream naturallanguage processing tasks such as ma-chine translation and information extrac-tion.
Combining the two tasks can poten-tially improve the efficiency of the over-all pipeline system and reduce error prop-agation.
In this work1, we compare var-ious methods for combining punctuationprediction (PU) and disfluency prediction(DF) on the Switchboard corpus.
We com-pare an isolated prediction approach witha cascade approach, a rescoring approach,and three joint model approaches.
Forthe cascade approach, we show that thesoft cascade method is better than the hardcascade method.
We also use the cas-cade models to generate an n-best list, usethe bi-directional cascade models to per-form rescoring, and compare that with theresults of the cascade models.
For thejoint model approach, we compare mixed-label Linear-chain Conditional RandomField (LCRF), cross-product LCRF and 2-layer Factorial Conditional Random Field(FCRF) with soft-cascade LCRF.
Our re-sults show that the various methods link-ing the two tasks are not significantly dif-ferent from one another, although theyperform better than the isolated predic-tion method by 0.5?1.5% in the F1 score.Moreover, the clique order of features alsoshows a marked difference.1 IntroductionThe raw output from automatic speech recogni-tion (ASR) systems does not have sentence bound-1The research reported in this paper was carried out aspart of the PhD thesis research of Xuancong Wang at the NUSGraduate School for Integrated Sciences and Engineering.aries or punctuation symbols.
Spontaneous speechalso contains a significant proportion of disflu-ency.
Researchers have shown that splitting inputsequences into sentences and adding in punctua-tion symbols improve machine translation (Favreet al., 2008; Lu and Ng, 2010).
Moreover, dis-fluencies in speech also introduce noise in down-stream tasks like machine translation and informa-tion extraction (Wang et al., 2010).
Thus, punc-tuation prediction (PU) and disfluency prediction(DF) are two important post-processing tasks forautomatic speech recognition because they im-prove not only the readability of ASR output, butalso the performance of downstream Natural Lan-guage Processing (NLP) tasks.The task of punctuation prediction is to insertpunctuation symbols into conversational speechtexts.
Punctuation prediction on long, unseg-mented texts also achieves the purpose of sentenceboundary prediction, because sentence boundariesare identified by sentence-end punctuation sym-bols: periods, question marks, and exclamationmarks.
Consider the following example,How do you feel about the Viet Nam War ?
Yeah ,I saw that as well .The question mark splits the sequence into twosentences.
This paper deals with this task which ismore challenging than that on text that has alreadybeen split into sentences.The task of disfluency prediction is to identifyword tokens that are spoken incorrectly due tospeech disfluency.
There are two main types ofdisfluencies: filler words and edit words.
Fillerwords mainly include filled pauses (e.g., ?uh?,?um?)
and discourse markers (e.g., ?I mean?, ?youknow?).
As they are insertions in spontaneousspeech to indicate pauses or mark boundaries indiscourse, they do not convey useful content in-formation.
Edit words are words that are spokenwrongly and then corrected by the speaker.
Forexample, consider the following utterance:121I want a flightEdit?
??
?to BostonFiller?
??
?uh I meanRepair?
??
?to DenverThe phrase ?to Boston?
forms the edit region to bereplaced by ?to Denver?.
The words ?uh I mean?are filler words that serve to cue the listener aboutthe error and subsequent corrections.The motivation of combining the two tasks canbe illustrated by the following two utterances:I am uh I am not going with you .I am sorry .
I am not going with you .Notice that the bi-gram ?I am?
is repeated inboth sentences.
For the first utterance, if punctu-ation prediction is performed first, it might breakthe utterance both before and after ?uh?
so that thesecond-stage disfluency prediction will treat thewhole utterance as three sentences, and thus maynot be able to detect any disfluency because eachone of the three sentences is legitimate on its own.On the other hand, for the second utterance, if dis-fluency prediction is performed first, it might mark?I am sorry?
as disfluent in the first place and re-move it before passing into the second-stage punc-tuation prediction.
Therefore, no matter whichtask is performed first, certain utterances can al-ways cause confusion.There are many ways to combine the two tasks.For example, we can perform one task first fol-lowed by another, which is called the cascade ap-proach.
We can also mix the labels, or take thecross-product of the labels, or use joint predictionmodels.
In this paper, we study the mutual influ-ence between the two tasks and compare a varietyof common state-of-the-art joint prediction tech-niques on this joint task.In Section 2, we briefly introduce previous workon the two tasks.
In Section 3, we describe ourbaseline system which performs punctuation anddisfluency prediction separately (i.e., in isolation).In Section 4, we compare the soft cascade ap-proach with the hard cascade approach.
We alsoexamine the effect of task order, i.e., performingwhich task first benefits more.
In Section 5, wecompare the cascade approach with bi-directionaln-best rescoring.
In Section 6, we compare the 2-layer Factorial CRF (Sutton et al., 2007) with thecross-product LCRF (Ng and Low, 2004), mixed-label LCRF (Stolcke et al., 1998), the cascade ap-proach, and the baseline isolated prediction.
Sec-tion 7 gives a summary of our overall findings.Section 8 gives the conclusion.2 Previous WorkThere were many works on punctuation predictionor disfluency prediction as an isolated task.
Forpunctuation prediction, Huang and Zweig (2002)used maximum entropy model; Christensen et al.
(2001) used finite state and multi-layer perceptronmethod; Liu et al.
(2005) used conditional ran-dom fields; Lu and Ng (2010) proposed using dy-namic conditional random fields for joint sentenceboundary type and punctuation prediction; Wanget al.
(2012) has added prosodic features for thedynamic conditional random field approach andZhang et al.
(2013) used transition-based parsing.For disfluency prediction, Shriberg et al.
(1997)uses purely prosodic features to perform the task.Johnson and Charniak (2004) proposed a TAG-based (Tree-Adjoining Grammar) noisy channelmodel.
Maskey et al.
(2006) proposed a phrase-level machine translation approach for this task.Georgila (2009) used integer linear programming(ILP) which can incorporate local and global con-straints.
Zwarts and Johnson (2011) has inves-tigated the effect of using extra language mod-els as features in the reranking stage.
Qian andLiu (2013) proposed using weighted Max-marginMarkov Networks (M3N) to balance precision andrecall to further improve the F1-score.
Wang et al.
(2014) proposed a beam-search decoder which in-tegrates M3N and achieved further improvements.There were also some works that addressed bothtasks.
Liu et al.
(2006) and Baron et al.
(1998)carried out sentence unit (SU) and disfluency pre-diction as separate tasks.
The difference betweenSU prediction and punctuation prediction is onlyin the non-sentence-end punctuation symbols suchas commas.
Stolcke et al.
(1998) mixed sen-tence boundary labels with disfluency labels sothat they do not predict punctuation on disfluenttokens.
Kim (2004) performed joint SU and In-terruption Point (IP) prediction, deriving edit andfiller word regions from predicted IPs using a rule-based system as a separate step.In this paper, we treat punctuation predictionand disfluency prediction as a joint prediction task,and compare various state-of-the-art joint predic-tion methods on this task.1223 The Baseline System3.1 Experimental SetupWe use the Switchboard corpus (LDC99T42) inour experiment with the same train/develop/testsplit as (Qian and Liu, 2013) and (Johnson andCharniak, 2004).
The corpus statistics are shownin Table 1.
Since the proportion of exclamationmarks and incomplete SU boundaries is too small,we convert all exclamation marks to periods andremove all incomplete SU boundaries (treat as nopunctuation).
In the Switchboard corpus, the ut-terances of each speaker have already been seg-mented into short sentences when used in (Qianand Liu, 2013; Johnson and Charniak, 2004).
Inour work, we concatenate the utterances of eachspeaker to form one long sequence of words foruse as the input to punctuation prediction and dis-fluency prediction.
This form of input where,utterances are not pre-segmented into short sen-tences, better reflects the real-world scenarios andprovides a more realistic test setting for punctu-ation and disfluency prediction.
Punctuation pre-diction also gives rise to sentence segmentation inthis setting.Data set train develop test# of tokens 1.3M 85.9K 65.5K# of sentences 173.7K 10.1K 7.9K# of sequences* 1854 174 134# of edit words 63.6K 4.7K 3.7K# of filler words 137.1K 9.6K 7.3K# of Commas 52.7K 1.8K 2.1K# of Periods 97.6K 6.5K 4.5K# of Questions 6.8K 363 407# of Exclamations 67 4 1# of Incomplete 189 2 0Table 1: Corpus statistics for all the experiments.
*: each conversation produces two long/sentence-joined sequences, one from each speaker.Our baseline system uses M3N (Taskar et al.,2004), one M3N for punctuation prediction andthe other for disfluency prediction.
We use thesame set of punctuation and disfluency labels (asshown in Table 2) throughout this paper.
To com-pare the various isolated, cascade, and joint pre-diction models, we use the same feature templatesfor both tasks as listed in Table 3.
Since some ofthe feature templates require predicted filler labelsand part-of-speech (POS) tags, we have trained aPOS tagger and a filler predictor both using CRF(i.e., using the same approach as that in Qian andLiu (2013)).
The same predicted POS tags andfillers are used for feature extraction in all theexperiments in this paper for a fair comparison.The degradation on disfluency prediction due tothe concatenation of utterances of each speakeris shown in Table 4.
The pause duration fea-tures are extracted by running forced alignmenton the corresponding Switchboard speech corpus(LDC97S62).Task Label MeaningDisfluencypredictionE edit wordF filler wordO otherwise / fluentPunctuationpredictionComma commaPeriod full-stopQMark question markNone no punctuationTable 2: Labels for punctuation prediction and dis-fluency prediction.3.2 FeaturesWe use the standard NLP features such asF (w?1w0=?so that?
), i.e., the word tokens atthe previous and current node position are?so?
and ?that?
respectively.
Each feature isassociated with a clique order.
For example,since the clique order of this feature templateis 2 (see Table 3), its feature functions can bef(w?1w0=?so that?, y0=?F?, y?1=?O?, t).
Theexample has a value of 1 only when the wordsat node t ?
1 and t are ?so that?, and the labelsat node t and t ?
1 are ?F?
and ?O?
respectively.The maximum length of the y history is called theclique order of the feature (in this feature func-tion, it is 2 since only y0and y?1are covered).The feature templates are listed in Table 3. wirefers to the word at the ithposition relative tothe current node; window size is the maximumspan of words centered at the current word thatthe template covers, e.g., w?1w0with a windowsize of 9 means w?4w?3, w?3w?2, ..., w3w4; pirefers to the POS tag at the ithposition relativeto the current node; wi?jrefers to any wordfrom the ithposition to the jthposition relative tothe current node, this template can capture wordpairs which can potentially indicate a repair, e.g.,?was ... is ...?, the speaker may have spoken any123word(s) in between and it is very difficult for thestandard n-gram features to capture all possiblevariations; wi, 6=Frefers to the ithnon-filler wordwith respect to the current position, this templatecan extract n-gram features skipping filler words;the multi-pair comparison function I(a, b, c, ...)indicates whether each pair (a and b, b and c, andso on) is identical, for example, if a = b 6= c = d,it will output ?101?
(?1?
for being equal, ?0?for being unequal), this feature template cancapture consecutive word/POS repetitions whichcan further improve upon the standard repetitionfeatures; and ngram-score features are the nat-ural logarithm of the following 8 probabilities:P (w?3, w?2, w?1, w0), P (w0|w?3, w?2, w?1),P (w?3, w?2, w?1), P (?/s?|w?3, w?2, w?1),P (w?3), P (w?2), P (w?1) and P (w0) (where??/s??
denotes sentence-end).Feature TemplateWindowSizeCliqueOrderw09 1w?1w09 2w?2w?1w09 2p09 1p?1p09 2p?2p?1p09 2w0w?6?
?1, w0w1?61 1I(wi, wj) 21 2I(wi, wj, wi+1, wj+1) 21 2I(wi, wj)(wiif wi=wj) 21 2I(pi, pj) 21 3I(pi, pj, pi+1, pj+1) 21 3I(pi, pj)(piif pi=pj) 21 3p?1w05 2w?1p05 2w?2, 6=Fw?1, 6=F1 2w?3, 6=Fw?2, 6=Fw?1, 6=F1 2p?2, 6=Fp?1, 6=F1 2p?3, 6=Fp?2, 6=Fp?1, 6=F1 2ngram-score features 1 3pause duration before w01 3pause duration after w01 3transitions 1 3Table 3: Feature templates for disfluency predic-tion, or punctuation prediction, or joint predictionfor all the experiments in this paper.The performance of the system can be fur-ther improved by adding additional prosodic fea-tures (Savova and Bachenko, 2003; Shriberg et al.,1998; Christensen et al., 2001) apart from pausedurations.
However, since in this work we focuson model-level comparison, we do not use otherprosodic features for simplicity.3.3 Evaluation and ResultsExperimentF1(PU)F1(DF)Short sentences, with preci-sion/recall balancing, clique or-der of features up to 3, and la-bels {E,F,O}N.A.
84.7Short sentences, with preci-sion/recall balancing, clique or-der of features up to 3, and la-bels {E,O}N.A.
84.3Join utterances into long sen-tences71.1 79.2Join utterances into long sen-tences + remove precision/recallbalancing71.1 78.2Join utterances into long sen-tences + remove precision/recallbalancing + reduce clique orderof all features68.5 76.4Table 4: Baseline results showing the degradationby joining utterances into long sentences, remov-ing precision/recall balancing, and reducing theclique order of features.
All models are trainedusing M3N.We use the standard F1 score as our evaluationmetric and this is similar to that in Qian and Liu(2013).
For training, we set the frequency prun-ing threshold to 5 to control the number of pa-rameters.
The regularization parameter is tunedon the development set.
Since the toolkits usedto run different experiments have slightly differ-ent limitations, in order to make fair comparisonsacross different toolkits, we do not use weightingto balance precision and recall when training M3Nand we have reduced the clique order of transi-tion features to two and all the other features toone in some of our experiments.
Since the per-formance of filler word prediction on this datasetis already very high, (>97%), we only focus onthe F1 score of edit word prediction in this pa-per when reporting the performance of disfluencyprediction.
Table 4 shows our baseline results.Our preliminary study shows the following gen-124eral trends: (i) for disfluency prediction: joiningutterances into long sentences will cause a 5?6%drop in F1 score; removing precision/recall bal-ance in M3N will cause about 1% drop in F1 score;and reducing the clique order in Table 3 will causeabout 1?2% drop in F1 score; and (ii) for punctua-tion prediction: removing precision/recall balancein M3N will cause negligible drop in F1 score; andreducing clique order will cause about 2?3% dropin F1 score.
Conventionally, the degradation fromreducing the clique orders can be mostly compen-sated by using the BIES (Begin, Inside, End, andSingle) labeling scheme.
In this work, for con-sistency and comparability across various experi-ments, we will stick to the same set of labels inTable 2.4 The Cascade ApproachInstead of decomposing the joint prediction ofpunctuation and disfluency into two independenttasks, the cascade approach considers one task tobe conditionally dependent on the other task suchthat the predictions are performed in sequence,where the results from the first step is used in thesecond step.
In this paper, we compare two typesof cascade: hard cascade versus soft cascade.4.1 Hard CascadeFor the hard cascade, we use the output from thefirst step to modify the input sequence before ex-tracting features for the second step.
For PU?DF(PUnctuation prediction followed by DisFluencyprediction), we split the input sequence into sen-tences according to the sentence-end punctuationsymbols predicted by the first step, and then per-form the DF prediction on the short/sentence-splitsequences in the second step.
For DF?PU, weremove the edit and filler words predicted by thefirst step, and then predict the punctuations usingthe cleaned-up input sequence.
The hard cascademethod may be helpful because the disfluency pre-diction on short/sentence-split sequences is betterthan on long/sentence-joined sequences (see thesecond and third rows in Table 4).
On the otherhand, the punctuation prediction on fluent text ismore accurate than that on non-fluent text basedon our preliminary study.For this experiment, four models are trainedusing M3N without balancing precision/recall.For the first step, two models are trained onlong/sentence-joined sequences with disfluent to-kens - one for PU prediction and the other for DFprediction.
These are simply the isolated base-line systems.
For the second step, the DF predic-tion model is trained on the short/sentence-split se-quences with disfluent tokens while the PU predic-tion model is trained on the long/sentence-joinedsequences with disfluent tokens removed.
Notethat in the second step of DF?PU, punctuation la-bels are predicted only for the fluent tokens sincethe disfluent tokens predicted by the first step hasalready been removed.
Therefore, during evalua-tion, if the first step makes a false positive by pre-dicting a fluent token as an edit or filler, we set itspunctuation label to the neutral label, None.
Allthe four models are trained using the same featuretemplates as shown in Table 3.
The regularizationparameter is tuned on the development set.4.2 Soft CascadeFor the soft cascade method, we use the labels pre-dicted from the first step as additional features forthe second step.
For PU?DF, we model the jointprobability as:P (DF,PU|x) = P (PU|x)?
P (DF|PU,x) (1)Likewise, we model the joint probability forDF?PU as:P (DF,PU|x) = P (DF|x)?
P (PU|DF,x) (2)For this experiment, four models are trained us-ing M3N without balancing precision/recall.
Aswith the case of hard cascade, the two modelsused in the first step are simply the isolated base-line systems.
For the second step, in addition tothe feature templates in Table 3, we also pass onthe labels (at the previous, current and next posi-tion) predicted by the first step as three third-order-clique features.
We also tune the regularization pa-rameter on the development set to obtain the bestmodel.4.3 Experimental ResultsTable 5 compares the performance of the hardand soft cascade methods with the isolated base-line systems.
In addition, we have also includedthe results of using the true labels in place ofthe labels predicted by the first step to indicatethe upper-bound performance of the cascade ap-proaches.
The results show that both the hard andsoft cascade methods outperform the baseline sys-tems, with the latter giving a better performance125Experiment F1 for PU F1 for DFisolated baseline 71.1 78.2hard cascade 71.2 79.1hard cascade(using true labels)72.6 83.5soft cascade 71.6 79.6soft cascade(using true labels)72.1 82.7Table 5: Performance comparison between thehard cascade method and the soft cascade methodwith respect to the baseline isolated prediction.All models are trained using M3N without balanc-ing precision and recall.
(statistical significance at p=0.01).
However, hardcascade has a higher upper-bound than soft cas-cade.
This observation can be explained as fol-lows.For hard cascade, the input sequence is modi-fied prior to feature extraction.
Therefore, manyof the features generated by the feature templatesgiven in Table 3 will be affected by these modi-fications.
So, provided that the modifications arebased on the correct information, the resulting fea-tures will not contain unwanted artefacts causedby the absence of the sentence boundary informa-tion for the presence of disfluencies.
For exam-ple, in ?do you do you feel that it was worthy?,the punctuation prediction system tends to insert asentence-end punctuation after the first ?do you?because the speaker restarts the sentence.If the disfluency was correctly predicted in thefirst step, then the hard cascade method wouldhave removed the first ?do you?
and eliminatedthe confusion.
Similarly, in ?I ?m sorry .
I ?m notgoing with you tomorrow .
?, the first ?I ?m?
islikely to be incorrectly detected as disfluent tokenssince consecutive repetitions are a strong indica-tion of disfluency.
In the case of hard cascade,PU?DF, the input sequence would have been splitinto sentences and the repetition feature would notbe activated.
However, since the hard cascademethod has a greater influence on the features forthe second step, it is also more sensitive to the pre-diction errors from the first step.Another observation from Table 5 is that theimprovement of the soft cascade over the isolatebaseline is much larger on DF (1.4% absolute)than that on PU (only 0.5% absolute).
The sameholds true for the hard cascade, despite the factthat there are more DF labels than PU labels in thiscorpus (see Table 1) and the first step prediction ismore accurate on DF than on PU.
This suggeststhat their mutual influence is not symmetrical, inthe way that the output from punctuation predic-tion provides more useful information for disflu-ency prediction than the other way round.5 The Rescoring ApproachIn Section 4, we have described that the two taskscan be cascaded in either order, i.e., PU?DF andDF?PU.
However, the performance of the sec-ond step greatly depends on that of the first step.In order to reduce sensitivity to the errors madein the first step, one simple approach is to prop-agate multiple hypotheses from the first step tothe second step to obtain a list of joint hypothe-ses (with both the DF and PU labels).
We thenrerank these hypotheses based on the joint proba-bility and pick the best.
We call this the rescoringapproach.
From (1) and (2), the joint probabilitiescan be expressed in terms of the probabilities gen-erated by four models: P (PU|x), P (DF|PU,x),P (DF|x), and P (PU|DF,x).
We can combine thefour models to form the following joint probabilityfunction for rescoring:P (DF,PU|x) = P (DF|x)?1?
P (PU|DF,x)?2?
P (PU|x)?1?
P (DF|PU,x)?2where ?1, ?2, ?1, and ?2are used to weightthe relative importance between (1) and (2); andbetween the first and second steps.
In practice,the probabilities are computed in the log domainwhere the above expression becomes a weightedsum of the log probabilities.
A similar rescoringapproach using two models is described in Shi andWang (2007).The experimental framework is shown in Fig-ure 1.
For PU?DF, we first use P (PU|x) to gen-erate an n-best list.
Then, for each hypothesis inthe n-best list, we use P (DF|PU,x) to obtain an-other n-best list.
So we have n2-best joint hy-potheses.
We do the same for DF?PU to ob-tain another n2-best joint hypotheses.
We rescorethe 2n2-best list using the four models.
The fourweights ?1, ?2, ?1, and ?2are tuned to opti-mize the overall F1 score on the development set.We used the MERT (minimum-error-rate training,(Och, 2003)) algorithm to tune the weights.
Wealso vary the size of n.126P(PU|x)P(DF|PU,x)InputSequence P(PU|DF,x)P(DF|x)PU-hypo-1PU-hypo-n?DF-hypo-1DF-hypo-n?2 n-bestjoint-hypo-1?
?joint-hypo-12 n2-bestjoint-hypo-2joint-hypo-2?
?Rescore using:?1 ?
log?
PU|x+?2 ?
log?
DF|PU, x+?1 ?
log?
DF|x+?2 ?
log?
PU|DF, xFigure 1: Illustration of the rescoring pipeline framework using the four M3N models used in the soft-cascade method: P (PU|x), P (DF|PU,x), P (DF|x) and P (PU|DF,x)The results shown in Table 6 suggest that therescoring method does not improve over the soft-cascade baseline.
This can be due to the fact thatwe are using the same four models for the soft-cascade and the rescoring methods.
It may bepossible that the information contained in the twomodels for the soft-cascade PU?DF mostly over-laps with the information contained in the othertwo models for the soft-cascade DF?PU since allthe four models are trained using the same fea-tures.
Thus, no additional information is gainedby combining the four models.6 The Joint ApproachIn this section, we compare 2-layer FCRF (Lu andNg, 2010) with mixed-label LCRF (Stolcke et al.,1998) and cross-product LCRF on the joint predic-tion task.
For the 2-layer FCRF, we use punctua-tion labels for the first layer and disfluency labelsfor the second layer (see Table 2).
For the mixed-label LCRF, we split the neutral label {O} into{Comma, Period, QMark, None} so that we havesix labels in total, {E, F, Comma, Period, QMark,None}.
In this approach, disfluent tokens do nothave punctuation labels because in real applica-tions, if we just want to get the cleaned-up/fluenttext with punctuations, we do not need to predictpunctuations on disfluent tokens as they will beremoved during the clean-up process.
Since thisapproach does not predict punctuation labels ondisfluent tokens, its punctuation F1 score is onlyevaluated on those fluent tokens.
For the cross-product LCRF, we compose each of the three dis-fluency labels with the four punctuation labels toget 12 PU-DF-joint labels (Ng and Low, 2004).Figure 2 shows a comparison of these three modelsin the joint prediction of punctuation and disflu-ency.
All the LCRF and FCRF models are trainedusing the GRMM toolkit (Sutton, 2006).
We usethe same feature templates (Table 3) to generateall the features for the toolkit.
However, to reducethe training time, we have set clique order to 2 forthe transitions and 1 for all other features.
We tunethe Gaussian prior variance on the development setfor all the experiments to obtain the best model fortesting.Table 7 shows the comparison of results.
OnDF alone, the improvement of the cross-productLCRF over the mixed-label LCRF, and the im-provement of the mixed-label LCRF over theisolated baseline are not statistically significant.However, if we test the statistical significance onthe overall performance of both PU and DF, boththe 2-layer FCRF and the cross-product LCRFperform better than the mixed-label LCRF.
Andwe also obtain the same conclusion as Stolckeet al.
(1998) that mixed-label LCRF performsbetter than isolated prediction.
However, for thecomparison between the 2-layer FCRF and thecross-product LCRF, although the 2-layer FCRFperforms better than the cross-product LCRF ondisfluency prediction, it does worse on punctua-tion prediction.
Overall, the two methods performabout the same, their difference is not statisticallysignificant.
In addition, both the 2-layer FCRFand the cross-product LCRF slightly outperformthe soft cascade method (statistical significance atp=0.04).127Experiment F1 for PU F1 for DFisolated baseline 71.1 78.2soft-cascade 71.6 79.6rescore n=1 71.5 (72.5) 79.3 (81.1)rescore n=2 71.2 (73.0) 79.3 (81.8)rescore n=3 71.2 (73.3) 79.9 (82.6)rescore n=4 71.2 (73.6) 79.8 (82.8)rescore n=5 71.2 (73.9) 79.4 (83.3)rescore n=6 71.1 (74.0) 79.6 (83.5)rescore n=8 71.2 (74.2) 79.8 (84.0)rescore n=10 * 71.2 (74.4) 79.8 (84.3)rescore n=12 71.1 (74.5) 79.7 (84.6)rescore n=15 71.2 (74.8) 79.8 (84.9)rescore n=18 71.1 (74.9) 79.7 (85.1)rescore n=25 70.7 (75.2) 79.3 (85.5)Table 6: Performance comparison between therescoring method and the soft-cascade methodwith respect to the baseline isolated prediction.The rescoring is done on 2n2hypotheses.
Allmodels are trained using M3N without balancingprecision and recall.
Figures in the bracket are theoracle F1 scores of the 2n2hypotheses.
*:on thedevelopment set, the best overall result is obtainedat n = 10.7 DiscussionIn this section, we will summarise our observa-tions based on the empirical studies that we haveconducted in this paper.Firstly, punctuation prediction and disfluencyprediction do influence each other.
The outputfrom one task does provide useful information thatcan improve the other task.
All the approachesstudied in this work, which link the two taskstogether, perform better than their correspondingExperiment F1 for PU F1 for DFisolated baseline 68.7 77.0soft cascade 69.0 77.5mixed-label LCRF 69.0 77.2cross-product LCRF 69.9 77.32-layer FCRF 69.2 77.8Table 7: Performance comparison among 2-layer FCRF, mixed-label LCRF and cross-productLCRF, with respect to the soft-cascade and the iso-lated prediction baseline.
All models are trainedusing GRMM (Sutton, 2006), with reduced cliqueorders.ERef: it was n?t , you know , it was never announced .Token: it was n?t you know it was never announcedPU: None  None  Comma  None  Comma  None  None  None  Perio dDF: E  E  E  F  F  O  O  O  O(a)Mixed-labelLCRF(b)Cross-productLCRF(c)2-layerFCRFE  Period  E  F  F  O  O  ONoneEPeriodONoneEC ommaENoneFC ommaFNoneONoneONoneONone  Period  None  Comma  None  C omma  None  None  NoneE  O  E  E  F  F  O  O  Oedit  fillerx 1  x 2 x 9  x 3  x 4  x 5  x 6  x 7  x 8x 1  x 2 x 9  x 3  x 4  x 5  x 6  x 7  x 8x 1  x 2 x 9  x 3  x 4  x 5  x 6  x 7  x 8Figure 2: Illustration using (a) mixed-label LCRF;(b) cross-product LCRF; and (c) 2-layer FCRF, forjoint punctuation (PU) and disfluency (DF) predic-tion.
Shaded nodes are observations and unshadednodes are variables to be predicted.isolated prediction baseline.Secondly, as compared to the soft cascade, thehard cascade passes more information from thefirst step into the second step, and thus is muchmore sensitive to errors in the first step.
In prac-tice, unless the first step has very high accuracy,soft cascade is expected to do better than hard cas-cade.Thirdly, if we train a model using a fine-grainedlabel set but test it on the same coarse-grained la-bel set, we are very likely to get improvement.
Forexample:?
The edit word F1 for mixed edit and filler pre-diction using {E, F, O} is better than that foredit prediction using {E, O} (see the secondand third rows in Table 4).
This is because theformer actually splits the O in the latter intoF and O.
Thus, it has a finer label granularity.?
Disfluency prediction using mixed-labelLCRF (using label set {E, F, Comma, Pe-riod, Question, None}) performs better thanthat using isolated LCRF (using label set {E,F, O}) (see the second and fourth rows inTable 7).
This is because the former dis-128tinguishes between different punctuations forfluent tokens and thus has a finer label granu-larity.?
Both the cross-product LCRF and 2-layerFCRF perform better than mixed-label LCRFbecause the former two distinguish betweendifferent punctuations for edit, filler and flu-ent tokens while the latter distinguishes be-tween different punctuations only for fluenttokens.
Thus, the former has a much finer la-bel granularity.From the above comparisons, we can see thatincreasing the label granularity can greatly im-prove the accuracy of a model.
However, thismay also increase the model complexity dramat-ically, especially when higher clique order is used.Although the joint approach (2-layer FCRF andcross-product LCRF) are better than the soft-cascade approach, they cannot be easily scaled upto using higher order cliques, which greatly limitstheir potential.
In practice, the soft cascade ap-proach offers a simpler and more efficient way toachieve a joint prediction of punctuations and dis-fluencies.8 ConclusionIn general, punctuation prediction and disfluencyprediction can improve downstream NLP tasks.Combining the two tasks can potentially improvethe efficiency of the overall framework and mini-mize error propagation.
In this work, we have car-ried out an empirical study on the various methodsfor combining the two tasks.
Our results show thatthe various methods linking the two tasks performbetter than the isolated prediction.
This meansthat punctuation prediction and disfluency predic-tion do influence each other, and the predictionoutcome in one task can provide useful informa-tion that helps to improve the other task.
Specifi-cally, we compare the cascade models and the jointprediction models.
For the cascade approach, weshow that soft cascade is less sensitive to predic-tion errors in the first step, and thus performs bet-ter than hard cascade.
For joint model approach,we show that, when clique order of one is used, allthe three joint model approaches perform signifi-cantly better than the isolated prediction baseline.Moreover, the 2-layer FCRF and the cross-productLCRF perform slightly better than the mix-labelLCRF and the soft-cascade approach, suggestingthat modelling at a finer label granularity is po-tentially beneficial.
However, the soft cascade ap-proach is more efficient than the joint approachwhen a higher clique order is used.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.ReferencesDon Baron, Elizabeth Shriberg, and Andreas Stolcke.2002.
Automatic punctuation and disfluency detec-tion in multi-party meetings using prosodic and lex-ical cues.
In Proc.
of ICSLP.Heidi Christensen, Yoshihiko Gotoh, and Steve Re-nals.
2001.
Punctuation annotation using statisti-cal prosody models.
In ISCA Tutorial and ResearchWorkshop (ITRW) on Prosody in Speech Recognitionand Understanding.Benoit Favre, Ralph Grishman, Dustin Hillard, HengJi, Dilek Hakkani-Tur, and Mari Ostendorf.
2008.Punctuating speech for information extraction.
InProc.
of ICASSP.Kallirroi Georgila.
2009.
Using integer linear pro-gramming for detecting speech disfluencies.
InProc.
of NAACL.Jing Huang and Geoffrey Zweig.
2002.
Maximum en-tropy model for punctuation annotation from speech.In Proc.
of INTERSPEECH.Mark Johnson and Eugene Charniak.
2004.
A TAG-based noisy-channel model of speech repairs.
InProc.
of ACL.Joungbum Kim.
2004.
Automatic detection of sen-tence boundaries, disfluencies, and conversationalfillers in spontaneous speech.
Master dissertation ofUniversity of Washington.Yang Liu, Andreas Stolcke, Elizabeth Shriberg, andMary Harper.
2005.
Using conditional randomfields for sentence boundary detection in speech.
InProc.
of ACL.Yang Liu, Elizabeth Shriberg, Andreas Stolcke, DustinHillard, Mari Ostendorf, and Mary Harper.
2006.Enriching speech recognition with automatic detec-tion of sentence boundaries and disfluencies.
IEEETransactions on Audio, Speech, and Language Pro-cessing, 14(5):1526?1540.Wei Lu and Hwee Tou Ng.
2010.
Better punctuationprediction with dynamic conditional random fields.In Proc.
of EMNLP.129Sameer Maskey, Bowen Zhou, and Yuqing Gao.
2006.A phrase-level machine translation approach for dis-fluency detection using weighted finite state trans-ducers.
In Proc.
of INTERSPEECH.Hwee Tou Ng and Jin Kiat Low.
2004.
Chi-nese part-of-speech tagging: One-at-a-time or all-at-once?
Word-based or character-based?
In Proc.
ofEMNLP.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL.Xian Qian and Yang Liu.
2013.
Disfluency detec-tion using multi-step stacked learning.
In Proc.
ofNAACL.Guergana Savova, Joan Bachenko.
2003.
Prosodic fea-tures of four types of disfluencies.
In ISCA Tuto-rial and Research Workshop on Disfluency in Spon-taneous Speech.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layerCRFs based joint decoding method for cascaded seg-mentation and labeling tasks.
In Proc.
of IJCAI.Elizabeth Shriberg, Rebecca Bates and Andreas Stol-cke.
1997.
A prosody-only decision-tree model fordisfluency detection.
In Proc.
of Eurospeech.Elizabeth Shriberg, Andreas Stolcke, Daniel Jurafsky,Noah Coccaro, Marie Meteer, Rebecca Bates, PaulTaylor, Klaus Ries, Rachel Martin, and Carol VanEss-Dykema.
1998.
Can prosody aid the auto-matic classification of dialog acts in conversationalspeech?
In Language and speech 41, no.
3-4: 443-492.Andreas Stolcke, Elizabeth Shriberg, Rebecca A.Bates, Mari Ostendorf, Dilek Hakkani, MadelainePlauche, Gokhan Tur, and Yu Lu.
1998.
Auto-matic detection of sentence boundaries and disfluen-cies based on recognized words.
In Proc.
of ICSLP.Charles Sutton.
2006.
GRMM: GRaphical Models inMallet.
http://mallet.cs.umass.edu/grmm/Charles Sutton, Andrew McCallum, and KhashayarRohanimanesh.
2007.
Dynamic conditional randomfields: factorized probabilistic models for labelingand segmenting sequence data.
In Journal of Ma-chine Learning Research, 8: 693?723.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004.Max-margin Markov networks.
In Proc.
of NIPS.Wen Wang, Gokhan Tur, Jing Zheng, and Necip FazilAyan.
2010.
Automatic disfluency removal for im-proving spoken language translation.
In Proc.
ofICASSP.Xuancong Wang, Hwee Tou Ng, and Khe Chai Sim.2012.
Dynamic conditional random fields for jointsentence boundary and punctuation prediction.
InProc.
of Interspeech.Xuancong Wang, Hwee Tou Ng, and Khe Chai Sim.2014.
A beam-search decoder for disfluency detec-tion.
In Proc.
of COLING.Dongdong Zhang, Shuangzhi Wu, Nan Yang, and MuLi.
2013.
Punctuation prediction with transition-based parsing.
In Proc.
of ACL.Simon Zwarts and Mark Johnson.
2011.
The impactof language models and loss functions on repair dis-fluency detection.
In Proc.
of ACL.130
