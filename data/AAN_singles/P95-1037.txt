Statistical Decision-Tree Models for Parsing*Dav id  M.  MagermanBolt  Beranek and Newman Inc.70 Fawcett  Street,  Room 15/148Cambr idge,  MA 02138, USAmagerman@bbn, comAbst ractSyntactic natural language parsers haveshown themselves tobe inadequate for pro-cessing highly-ambiguous large-vocabularytext, as is evidenced by their poor per-formance on domains like the Wall StreetJournal, and by the movement awayfrom parsing-based approaches to text-processing in general.
In this paper, I de-scribe SPATTER, a statistical parser basedon decision-tree l arning techniques whichconstructs a complete parse for every sen-tence and achieves accuracy rates far bet-ter than any published result.
This workis based on the following premises: (1)grammars are too complex and detailed todevelop manually for most interesting do-mains; (2) parsing models must rely heav-ily on lexical and contextual informationto analyze sentences accurately; and (3)existing n-gram modeling techniques areinadequate for parsing models.
In exper-iments comparing SPATTER with IBM'scomputer manuals parser, SPATTER sig-nificantly outperforms the grammar-basedparser.
Evaluating SPATTER against hePenn Treebank Wall Street Journal corpususing the PARSEVAL measures, SPAT-TER achieves 86% precision, 86% recall,and 1.3 crossing brackets per sentence forsentences of 40 words or less, and 91% pre-cision, 90% recall, and 0.5 crossing bracketsfor sentences between 10 and 20 words inlength.This work was sponsored by the Advanced ResearchProjects Agency, contract DABT63-94-C-0062.
It doesnot reflect the position or the policy of the U.S. Gov-ernment, and no official endorsement should be inferred.Thanks to the members of the IBM Speech RecognitionGroup for their significant contributions to this work.1 In t roduct ionParsing a natural anguage sentence can be viewed asmaking a sequence of disambiguation decisions: de-termining the part-of-speech of the words, choosingbetween possible constituent structures, and select-ing labels for the constituents.
Traditionally, disam-biguation problems in parsing have been addressedby enumerating possibilities and explicitly declaringknowledge which might aid the disambiguation pro-cess.
However, these approaches have proved toobrittle for most interesting natural language prob-lems.This work addresses the problem of automaticallydiscovering the disambiguation criteria for all of thedecisions made during the parsing process, given theset of possible features which can act as disambigua-tors.
The candidate disambiguators are the words inthe sentence, relationships among the words, and re-lationships among constituents already constructedin the parsing process.Since most natural language rules are not abso-lute, the disambiguation criteria discovered in thiswork are never applied deterministically.
Instead, alldecisions are pursued non-deterministically accord-ing to the probability of each choice.
These proba-bilities are estimated using statistical decision treemodels.
The probability of a complete parse tree(T) of a sentence (S) is the product of each decision(dl) conditioned on all previous decisions:P(T\[S) = H P(dildi-ldi-2""dlS)"diETEach decision sequence constructs a unique parse,and the parser selects the parse whose decision se-quence yields the highest cumulative probability.
Bycombining a stack decoder search with a breadth-first algorithm with probabilistic pruning, it is pos-sible to identify the highest-probability parse for anysentence using a reasonable amount of memory andtime.276The claim of this work is that statistics froma large corpus of parsed sentences combined withinformation-theoretic classification and training al-gorithms can produce an accurate natural languageparser without the aid of a complicated knowl-edge base or grammar.
This claim is justified byconstructing a parser, called SPATTER (StatisticalPATTErn Recognizer), based on very limited lin-gnistic information, and comparing its performanceto a state-of-the-art grammar-based parser on acommon task.
It remains to be shown that an accu-rate broad-coverage parser can improve the perfor-mance of a text processing application.
This will bethe subject of future experiments.One of the important points of this work is thatstatistical models of natural language should notbe restricted to simple, context-insensitive models.In a problem like parsing, where long-distance l x-ical information is crucial to disambiguate inter-pretations accurately, local models like probabilisticcontext-free grammars are inadequate.
This workillustrates that existing decision-tree t chnology canbe used to construct and estimate models which se-lectively choose elements of the context which con-tribute to disambignation decisions, and which havefew enough parameters to be trained using existingresources.I begin by describing decision-tree modeling,showing that decision-tree models are equivalent tointerpolated n-gram models.
Then I briefly describethe training and parsing procedures used in SPAT-TER.
Finally, I present some results of experimentscomparing SPATTER with a grammarian's rule-based statistical parser, along with more recent re-suits showing SPATTER applied to the Wall StreetJournal domain.2 Decision-Tree ModelingMuch of the work in this paper depends on replac-ing human decision-making skills with automaticdecision-making algorithms.
The decisions underconsideration i volve identifying constituents andconstituent labels in natural language sentences.Grammarians, the human decision-makers in pars-ing, solve this problem by enumerating the featuresof a sentence which affect he disambiguation deci-sions and indicating which parse to select based onthe feature values.
The grammarian is accomplish-ing two critical tasks: identifying the features whichare relevant o each decision, and deciding whichchoice to select based on the values of the relevantfeatures.Decision-tree classification algorithms account forboth of these tasks, and they also accomplish athird task which grammarians classically find dif-ficult.
By assigning a probability distribution to thepossible choices, decision trees provide a ranking sys-tem which not only specifies the order of preferencefor the possible choices, but also gives a measure ofthe relative likelihood that each choice is the onewhich should be selected.2.1 What  is a Decision Tree?A decision tree is a decision-making device whichassigns a probability to each of the possible choicesbased on the context of the decision: P(flh), wheref is an element of the future vocabulary (the set ofchoices) and h is a history (the context of the de-cision).
This probability P(flh) is determined byasking a sequence of questions ql q2 ... qn about thecontext, where the ith question asked is uniquely de-termined by the answers to the i - 1 previous ques-tions.For instance, consider the part-of-speech taggingproblem.
The first question a decision tree mightask is:1.
What is the word being tagged?If the answer is the, then the decision tree needsto ask no more questions; it is clear that the deci-sion tree should assign the tag f = determiner withprobability 1.
If, instead, the answer to question 1isbear, the decision tree might next ask the question:2.
What is the tag of the previous word?If the answer to question 2 is determiner, the de-cision tree might stop asking questions and assignthe tag f = noun with very high probability, andthe tag f = verb with much lower probability.
How-ever, if the answer to question 2is noun, the decisiontree would need to ask still more questions to get agood estimate of the probability of the tagging deci-sion.
The decision tree described in this paragraphis shown in Figure 1.Each question asked by the decision tree is repre-sented by a tree node (an oval in the figure) and thepossible answers to this question are associated withbranches emanating from the node.
Each node de-fines a probability distribution on the space of pos-sible decisions.
A node at which the decision treestops asking questions i a leaf node.
The leaf nodesrepresent the unique states in the decision-makingproblem, i.e.
all contexts which lead to the sameleaf node have the same probability distribution forthe decision.2.2 Decision Trees vs. n-graxnsA decision-tree model is not really very differentfrom an interpolated n-gram model.
In fact, they277I II P(aoun I bear, determiner)f0.8P(vo~ I bear, determiner)--0.2 I -"Figure I: Partially-grown decision tree for part-of-speech tagging.are equivalent in representational power.
The maindifferences between the two modeling techniques arehow the models are parameterized and how the pa-rameters are estimated.2.2.1 Model  Parameter i za t ionFirst, let's be very clear on what we mean by ann-gram model.
Usually, an n-gram model refers to aMarkov process where the probability of a particulartoken being generating is dependent on the valuesof the previous n - 1 tokens generated by the sameprocess.
By this definition, an n-gram model hasIWI" parameters, where IWI is the number of uniquetokens generated by the process.However, here let's define an n-gram model moreloosely as a model which defines a probability distri-bution on a random variable given the values of n -  1random variables, P(flhlh2... hn-1).
There is noassumption i the definition that any of the randomvariables F or Hi range over the same vocabulary.The number of parameters in this n-gram model isIFI I'\[ IH, I.Using this definition, an n-gram model can berepresented by a decision-tree model with n - 1questions.
For instance, the part-of-speech taggingmodel P(tilwiti_lti_2) can be interpreted as a 4-gram model, where HI is the variable denoting theword being tagged, Ha is the variable denoting thetag of the previous word, and Ha is the variable de-noting the tag of the word two words back.
Hence,this 4-gram tagging model is the same as a decision-tree model which always asks the sequence of 3 ques-tions:1.
What is the word being tagged?2.
What is the tag of the previous word?3.
What is the tag of the word two words back?But can a decision-tree model be represented byan n-gram model?
No, but it can be representedby an interpolated n-gram model.
The proof of thisassertion is given in the next section.2.2.2 Mode l  Es t imat ionThe standard approach to estimating an n-grammodel is a two step process.
The first step is to countthe number of occurrences of each n-gram from atraining corpus.
This process determines the empir-ical distribution,Count(hlhz .. .
hn-lf)P(flhlh2... hn-1)= Count(hlh2.. .
hn-1)The second step is smoothing the empirical distri-bution using a separate, held-out corpus.
This stepimproves the empirical distribution by finding statis-tically unreliable parameter stimates and adjustingthem based on more reliable information.A commonly-used technique for smoothing isdeleted interpolation.
Deleted interpolation es-timates a model P(f\[hlh2... hn-1) by us-ing a linear combination of empirical modelsP(f\]hklhk=...
hk.,), where m < n andk,-x < ki < n for all i < m. For example, a model\[~(fihlh2h3) might be interpolated as follows:P(.flhl h2hs ) =AI (hi h2hs)P(.fJhl h2h3) +:~2(h~h2h3)P(flhlh2) + As(hlh2h3)P(Ylhzh3) +)~(hlhuha)P(flh2hs) + As(hzhshs)P(f\]hlh2) +)~ (hi h2h3)P(.flhl) + A~ (hi h2ha)P(.flh2) +AS (hlh2hs)P(flh3)where ~'~)q(hlh2h3) = 1 for all histories hlhshs.The optimal values for the A~ functions can beestimated using the forward-backward algorithm(Baum, 1972).A decision-tree model can be represented by aninterpolated n-gram model as follows.
A leaf node ina decision tree can be represented by the sequence ofquestion answers, or history values, which leads thedecision tree to that leaf.
Thus, a leaf node definesa probability distribution based on values of thosequestions: P(flhklhk2 ...
ha.,), where m < n andki-1 < ki < n, and where hk~ is the answer to oneof the questions asked on the path from the root tothe leaf.
~ But this is the same as one of the termsin the interpolated n-gram model.
So, a decision1Note that in a decision tree, the leaf distribution isnot affected by the order in which questions are asked.Asking about hi followed by h2 yields the same futuredistribution as asking about h2 followed by hi.278tree can be defined as an interpolated n-gram modelwhere the At function is defined as:1 if hk~hk2.. ,  h~.
is aleaf,Ai(hk~hk2... hk,) = 0 otherwise.2.3 Decision-Tree Algor i thmsThe point of showing the equivalence between -gram models and decision-tree models is to makeclear that the power of decision-tree models is notin their expressiveness, but instead in how they canbe automatically acquired for very large modelingproblems.
As n grows, the parameter space for ann-gram model grows exponentially, and it quicklybecomes computationally infeasible to estimate thesmoothed model using deleted interpolation.
Also,as n grows large, the likelihood that the deleted in-terpolation process will converge to an optimal oreven near-optimal parameter setting becomes van-ishingly small.On the other hand, the decision-tree l arning al-gorithm increases the size of a model only as thetraining data allows.
Thus, it can consider very largehistory spaces, i.e.
n-gram models with very large n.Regardless of the value of n, the number of param-eters in the resulting model will remain relativelyconstant, depending mostly on the number of train-ing examples.The leaf distributions in decision trees are empiri-cal estimates, i.e.
relative-frequency counts from thetraining data.
Unfortunately, they assign probabil-ity zero to events which can possibly occur.
There-fore, just as it is necessary to smooth empirical n-gram models, it is also necessary tosmooth empiricaldecision-tree models.The decision-tree l arning algorithms used in thiswork were developed over the past 15 years bythe IBM Speech Recognition group (Bahl et al,1989).
The growing algorithm is an adaptation ofthe CART algorithm in (Breiman et al, 1984).
Fordetailed escriptions and discussions of the decision-tree algorithms used in this work, see (Magerman,1994).An important point which has been omitted fromthis discussion of decision trees is the fact that onlybinary questions are used in these decision trees.
Aquestion which has k values is decomposed into a se-quence of binary questions using a classification treeon those k values.
For example, a question about aword is represented as 30 binary questions.
These30 questions are determined by growing a classifi-cation tree on the word vocabulary as described in(Brown et al, 1992).
The 30 questions represent 30different binary partitions of the word vocabulary,and these questions are defined such that it is possi-ble to identify each word by asking all 30 questions.For more discussion of the use of binary decision-treequestions, ee (Magerman, 1994).3 SPATTER Pars ingThe SPATTER parsing algorithm is based on inter-preting parsing as a statistical pattern recognitionprocess.
A parse tree for a sentence is constructedby starting with the sentence's words as leaves ofa tree structure, and labeling and extending nodesthese nodes until a single-rooted, labeled tree is con-structed.
This pattern recognition process is drivenby the decision-tree models described in the previoussection.3.1 SPATTER Representat ionA parse tree can be viewed as an n-ary branchingtree, with each node in a tree labeled by either anon-terminal label or a part-of-speech label.
If aparse tree is interpreted as a geometric pattern, aconstituent is no more than a set of edges whichmeet at the same tree node.
For instance, the nounphrase, "a brown cow," consists of an edge extendingto the right from "a," an edge extending to the leftfrom "cow," and an edge extending straight up from"brown".Figure 2: Representation of constituent and labelingof extensions in SPATTER.In SPATTER, a parse tree is encoded in termsof four elementary components, or features: words,tags, labels, and extensions.
Each feature has a fixedvocabulary, with each element of a given feature vo-cabulary having a unique representation.
The wordfeature can take on any value of any word.
The tagfeature can take on any value in the part-of-speechtag set.
The label feature can take on any value inthe non-terminal set.
The extension can take on anyof the following five values:r ight  - the node is the first child of a constituent;left  - the node is the last child of a constituent;up - the node is neither the first nor the last childof a constituent;unary  - the node is a child of a unary constituent;279root - the node is the root of the tree.For an n word sentence, a parse tree has n leafnodes, where the word feature value of the ith leafnode is the ith word in the sentence.
The word fea-ture value of the internal nodes is intended to con-tain the lexical head of the node's constituent.
Adeterministic lookup table based on the label of theinternal node and the labels of the children is usedto approximate his linguistic notion.The SPATTER representation f the sentence(S (N Each_DD1 code_NN1(Tn used_VVN(P by_II  (N the_AT PC_NN1))))(V is_VBZ listed_VVN))is shown in Figure 3.
The nodes are constructedbottom-up from left-to-right, with the constraintthat no constituent ode is constructed until all of itschildren have been constructed.
The order in whichthe nodes of the example sentence are constructedis indicated in the figure.1410Each| 4 t2,~i~4 l~tOdmind ~?
tho PC ~- I i~odFigure 3: Treebank analysis encoded using featurevalues.3.2 Training SPATTER's modelsSPATTER consists of three main decision-treemodels: a part-of-speech tagging model, a node-extension model, and a node-labeling model.Each of these decision-tree models are grown usingthe following questions, where X is one of word, tag,label, or extension, and Y is either left and right:?
What is the X at the current node??
What is the X at the node to the Y??
What is the X at the node two nodes to the Y??
What is the X at the current node's first childfrom the Y??
What is the X at the current node's secondchild from the Y?For each of the nodes listed above, the decision treecould also ask about he number of children and spanof the node.
For the tagging model, the values of theprevious two words and their tags are also asked,since they might differ from the head words of theprevious two constituents.The training algorithm proceeds as follows.
Thetraining corpus is divided into two sets, approx-imately 90% for tree growing and 10% for treesmoothing.
For each parsed sentence in the treegrowing corpus, the correct state sequence is tra-versed.
Each state transition from si to 8i+1 is anevent; the history is made up of the answers to all ofthe questions at state sl and the future is the valueof the action taken from state si to state Si+l.
Eachevent is used as a training example for the decision-tree growing process for the appropriate feature'stree (e.g.
each tagging event is used for growingthe tagging tree, etc.).
After the decision trees aregrown, they are smoothed using the tree smoothingcorpus using a variation of the deleted interpolationalgorithm described in (Magerman, 1994).3.3 Parsing with SPATTERThe parsing procedure is a search for the highestprobability parse tree.
The probability of a parseis just the product of the probability of each of theactions made in constructing the parse, according tothe decision-tree models.Because of the size of the search space, (roughlyO(ITI"INJ"), where \[TJ is the number of part-of-speech tags, n is the number of words in the sen-tence, and \[NJ is the number of non-terminal labels),it is not possible to compute the probability of everyparse.
However, the specific search algorithm usedis not very important, so long as there are no searcherrors.
A search error occurs when the the high-est probability parse found by the parser is not thehighest probability parse in the space of all parses.SPATTER's search procedure uses a two phaseapproach to identify the highest probability parse of280a sentence.
First, the parser uses a stack decodingalgorithm to quickly find a complete parse for thesentence.
Once the stack decoder has found a com-plete parse of reasonable probability (> 10-5), itswitches to a breadth-first mode to pursue all of thepartial parses which have not been explored by thestack decoder.
In this second mode, it can safelydiscard any partial parse which has a probabilitylower than the probability of the highest probabil-ity completed parse.
Using these two search modes,SPATTER guarantees that it will find the highestprobability parse.
The only limitation of this searchtechnique is that, for sentences which are modeledpoorly, the search might exhaust he available mem-ory before completing both phases.
However, thesesearch errors conveniently occur on sentences whichSPATTER is likely to get wrong anyway, so thereisn't much performance lossed due to the search er-rors.
Experimentally, the search algorithm guaran-tees the highest probability parse is found for over96% of the sentences parsed.4 Experiment ResultsIn the absence of an NL system, SPATTER can beevaluated by comparing its top-ranking parse withthe treebank analysis for each test sentence.
Theparser was applied to two different domains, IBMComputer Manuals and the Wall Street Journal.4.1 IBM Computer  Manua lsThe first experiment uses the IBM Computer Man-uals domain, which consists of sentences extractedfrom IBM computer manuals.
The training and testsentences were annotated by the University of Lan-caster.
The Lancaster treebank uses 195 part-of-speech tags and 19 non-terminal labels.
This tree-bank is described in great detail in (Black et al,1993).The main reason for applying SPATTER to thisdomain is that IBM had spent the previous tenyears developing a rule-based, unification-style prob-abilistic context-free grammar for parsing this do-main.
The purpose of the experiment was to esti-mate SPATTER's ability to learn the syntax for thisdomain directly from a treebank, instead of depend-ing on the interpretive xpertise of a grammarian.The parser was trained on the first 30,800 sen-tences from the Lancaster treebank.
The test setincluded 1,473 new sentences, whose lengths rangefrom 3 to 30 words, with a mean length of 13.7words.
These sentences are the same test sentencesused in the experiments reported for IBM's parserin (Black et al, 1993).
In (Black et al, 1993),IBM's parser was evaluated using the 0-crossing-brackets measure, which represents the percentageof sentences for which none of the constituents inthe parser's parse violates the constituent bound-aries of any constituent in the correct parse.
Afterover ten years of grammar development, he IBMparser achieved a 0-crossing-brackets score of 69%.On this same test set, SPATTER scored 76%.4.2 Wal l  S t reet  Journa lThe experiment is intended to illustrate SPATTER'sability to accurately parse a highly-ambiguous,large-vocabulary domain.
These experiments usethe Wall Street Journal domain, as annotated in thePenn Treebank, version 2.
The Penn Treebank uses46 part-of-speech tags and 27 non-terminal labels.
2The WSJ portion of the Penn Treebank is dividedinto 25 sections, numbered 00 - 24.
In these exper-iments, SPATTER was trained on sections 02 - 21,which contains approximately 40,000 sentences.
Thetest results reported here are from section 00, whichcontains 1920 sentences, s Sections 01, 22, 23, and24 will be used as test data in future experiments.The Penn Treebank is already tokenized and sen-tence detected by human annotators, and thus thetest results reported here reflect this.
SPATTERparses word sequences, not tag sequences.
Further-more, SPATTER does not simply pre-tag the sen-tences and use only the best tag sequence in parsing.Instead, it uses a probabilistic model to assign tagsto the words, and considers all possible tag sequencesaccording to the probability they are assigned by themodel.
No information about the legal tags for aword are extracted from the test corpus.
In fact, noinformation other than the words is used from thetest corpus.For the sake of efficiency, only the sentences of 40words or fewer are included in these experiments.
4For this test set, SPATTER takes on average 122This treebank also contains coreference information,predicate-argument relations, and trace information in-dicating movement; however, none of this additional in-formation was used in these parsing experiments.SFor an independent research project on coreference,sections 00 and 01 have been annotated with detailedcoreference information.
A portion of these sections isbeing used as a development test set.
Training SPAT-TER on them would improve parsing accuracy signifi-cantly and skew these experiments in favor of parsing-based approaches to coreference.
Thus, these two sec-tions have been excluded from the training set and re-served as test sentences.4SPATTER returns acomplete parse for all sentencesof fewer then 50 words in the test set, but the sentencesof 41 - 50 words required much more computation thanthe shorter sentences, and so they have been excluded.281seconds per sentence on an SGI R4400 with 160megabytes of RAM.To evaluate SPATTER's  performance on this do-main, I am using the PARSEVAL measures, as de-fined in (Black et al, 1991):Precisionno.
of correct constituents in SPATTER parseno.
of constituents in SPATTER parseRecallno.
of correct constituents in SPATTER parseno.
of constituents in treebank parseCrossing Brackets no.
of constituents which vio-late constituent boundaries with a constituentin the treebank parse.The precision and recall measures do not considerconstituent labels in their evaluation of a parse, sincethe treebank label set will not necessarily coincidewith the labels used by a given grammar.
SinceSPATTER uses the same syntactic label set as thePenn Treebank, it makes sense to report labelledprecision and labelled recall.
These measures arecomputed by considering a constituent to be correctif and only if it's label matches the label in the tree-bank.Table 1 shows the results of SPATTER evaluatedagainst he Penn Treebank on the Wall Street Jour-nal section 00.ComparisonsAvg.
Sent.
LengthTreebank ConstituentsParse ConstituentsTagging AccuracyCrossings Per SentenceSent.
with 0 CrossingsSent.
with 1 CrossingSent.
with 2 CrossingsPrecisionRecallLabelled PrecisionLabelled Recall1759 1114 65322.3 16.8 15.617.58 13.21 12.1017.48 13.13 12.0396.5% 96.6% 96.5%1.33 0.63 0.4955.4% 69.8% 73.8%69.2% 83.8% 86.8%80.2% 92.1% 95.1%86.3% 89.8% 90.8%85.8% 89.3% 90.3%84.5% 88.1% 89.0%84.0% 87.6% 88.5%Table 1: Results from the WSJ Penn Treebank ex-periments.Figures 5, 6, and 7 illustrate the performance ofSPATTER as a function of sentence l ngth.
SPAT-TER's performance d grades slowly for sentences upto around 28 words, and performs more poorly andmore erratically as sentences get longer.
Figure 4 in-dicates the frequency of each sentence l ngth in thetest corpus.807080SO403020100iii4 ?
II 10 12  14 l id 18 20  2 |  24  2i l  28 :10 :12  34  :i l l  38  40Senbmce LengthFigure 4: Frequency in the test corpus as a functionof sentence length for Wall Street Journal experi-ments.3.5$2.521.S10.60t l ........................................................................................$ 8 10  12 14 18 15 20  22  24  28  ~Zll 'lO $2 :14  ~ l  ~8 40Sentence LengthFigure 5: Number of crossings per sentence as afunction of sentence l ngth for Wall Street Journalexperiments.5 Conc lus ionRegardless of what techniques are used for parsingdisambiguation, one thing is clear: if a particularpiece of information is necessary for solving a dis-ambiguation problem, it must be made available tothe disambiguation mechanism.
The words in thesentence are clearly necessary to make parsing de-cisions, and in some cases long-distance structuralinformation is also needed.
Statistical models for282100%90%80%70%60%50%40%30%20%10%0%.
.
.
.
.
.. '.
: ', '.
: : '.
: ', ~ ~ ~ I ~ ~ : : : : : : : : : : ', '.
', ~ : : : : : : :I II; il 1012141118 |0  2= J4  te  20  30  5 t  $4  ~lll ~18 40Sentence L~gthFigure 6: Percentage of sentence with 0, 1, and 2crossings as a function of sentence length for WallStreet Journal experiments.100%96%90%85%00%76%- .
-ememonI 8 lO  1 |  14  1(1 18 s*O | |  |4  |$  18  =0 S |  S4 =e $8 40Sentence LengthFigure 7: Precision and recall as a function of sen-tence length for Wall Street Journal experiments.parsing need to consider many more features of asentence than can be managed by n-gram modelingtechniques and many more examples than a humancan keep track of.
The SPATTER parser illustrateshow large amounts of contextual information can beincorporated into a statistical model for parsing byapplying decision-tree l arning algorithms to a largeannotated corpus.Re ferencesL.
R. Bahl, P. F. Brown, P. V. deSouza, and R. L.Mercer.
1989.
A tree-based statistical languagemodel for natural language speech recognition.IEEE ~Pransactions onAcoustics, Speech, and Sig-nal Processing, Vol.
36, No.
7, pages 1001-1008.L.
E. Baum.
1972.
An inequality and associatedmaximization technique in statistical estimationof probabilistic functions of markov processes.
In-equalities, Vol.
3, pages 1-8.E.
Black and et al 1991.
A procedure for quanti-tatively comparing the syntactic overage of en-glish grammars.
Proceedings o/ the February 1991DARPA Speech and Natural Language Workshop,pages 306-311.E.
Black, R. Garside, and G. Leech.
1993.Statistically-driven computer grammars of english:the ibm/lancaster approach.
Rodopi, Atlanta,Georgia.L.
Breiman, J. H. Friedman, R. A. Olshen, and C. J.Stone.
1984.
Ci~ssi\]ication a d Regression Trees.Wadsworth and Brooks, Pacific Grove, California.P.
F. Brown, V. Della Pietra, P. V. deSouza,J.
C. Lai, and R. L. Mercer.
1992.
"Class-basedn-gram models of natural language."
Computa-tional Linguistics, 18(4), pages 467-479.D.
M. Magerman.
1994.
Natural Language Pars-ing as Statistical Pattern Recognition.
Doctoraldissertation.
Stanford University, Stanford, Cali-fornia.283
