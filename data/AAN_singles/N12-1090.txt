2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 720?730,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsTranslation-Based Projection for Multilingual Coreference ResolutionAltaf Rahman and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{altaf,vince}@hlt.utdallas.eduAbstractTo build a coreference resolver for a newlanguage, the typical approach is to firstcoreference-annotate documents from this tar-get language and then train a resolver on theseannotated documents using supervised learn-ing techniques.
However, the high cost asso-ciated with manually coreference-annotatingdocuments needed by a supervised approachmakes it difficult to deploy coreference tech-nologies across a large number of natural lan-guages.
To alleviate this corpus annotationbottleneck, we examine a translation-basedprojection approach to multilingual corefer-ence resolution.
Experimental results on twotarget languages demonstrate the promise ofour approach.1 IntroductionNoun phrase (NP) coreference resolution is thetask of determining which NPs (or mentions) referto each real-world entity in a document.
Recentyears have witnessed a surge of interest in multilin-gual coreference resolution.
For instance, the ACE2004/2005 evaluations and SemEval-2010 SharedTask 1 have both involved coreference resolution inmultiple languages.
As evidenced by the partici-pants in these evaluations, the most common ap-proach to building a resolver for a new languageis supervised, which involves training a resolveron coreference-annotated documents from the tar-get language.
Although supervised approaches workreasonably well, they present a challenge to deploy-ing coreference technologies across a large numberof natural languages.
Specifically, for each new lan-guage of interest, one has to hire native speakers ofthe language to go through the labor-intensive, time-consuming process of hand-annotating a potentiallylarge number of documents with coreference anno-tation before a supervised resolver can be trained.One may argue that a potential solution to thiscorpus annotation bottleneck is to employ an unsu-pervised or heuristic approach to coreference resolu-tion, especially in light of the fact that they have re-cently started to rival their supervised counterparts.However, by adopting these approaches, we are sim-ply replacing the corpus annotation bottleneck byanother, possibly equally serious, bottleneck, theknowledge acquisition bottleneck.
Specifically, inthese approaches, one has to employ knowledge ofthe target language to design coreference rules (e.g.,Mitkov (1999), Poon and Domingos (2008), Raghu-nathan et al (2010)) or sophisticated generativemodels (e.g., Haghighi and Klein (2007,2010), Ng(2008)) to combine the available knowledge sources.One could argue that designing coreferencerules and generative models may not be as time-consuming as annotating a large coreference corpus.This may be true for a well-studied language likeEnglish, where we can easily compose a rule thatdisallows coreference between two mentions if theydisagree in number and gender, for instance.
How-ever, computing these features may not be as simpleas we hope for a language like Chinese: the lack ofmorphology complicates the determination of num-ber information, and the fact that most Chinese firstnames are used by both genders makes gender deter-mination difficult.
The difficulty in accurately com-puting features translates to difficulties in compos-ing coreference rules: for example, the aforemen-tioned rule involving gender and number agreement,as well as rules that implement traditional linguistic720constraints on coreference, may no longer be accu-rate and desirable to have if the features involvedcannot be accurately computed.
Consequently, webelieve that research in multilingual coreference res-olution will continue to be dominated by supervisedapproaches.Given the high cost of annotating data with coref-erence chains, it is crucial to explore methods forobtaining annotated data in a cost-effective manner.Motivated in part by this observation, we examineone such method that has recently shown promisefor a variety of NLP tasks, translation-based projec-tion, which is composed of three steps.
To coref-erence annotate a text in the target language, we(1) machine-translate it to a resource-rich language(henceforth the source language); (2) automaticallyproduce the desired linguistic annotations (which inour case are coreference annotations) on the trans-lated text using the linguistic tool developed for thesource language (which in our case is a coreferenceresolver) ; and (3) project the annotations from thesource language to the target language.Unlike supervised approaches, this projection ap-proach does not require any coreference-annotateddata from the target language.
Equally importantly,unlike its unsupervised counterparts, this approachdoes not require that we have any linguistic knowl-edge of the target language.
In fact, we have noknowledge of the target languages we employ in ourevaluation.
One of our goals is to examine the fea-sibility of building a coreference resolver for a lan-guage for which we have no coreference-annotateddata and no linguistic knowledge of the language.Recall that we view projection as an approachfor alleviating the corpus annotation bottleneck, notas a solution to the multilingual coreference resolu-tion problem.
In fact, though rarely emphasized inprevious work on applying projection, we note thatprojection alone cannot be used to solve multilin-gual NLP problems, including coreference resolu-tion.
The reason is that every language has its ownidiosyncrasies with respect to linguistic properties,and projection simply cannot produce annotationscapturing those properties that are specific to the tar-get language.
Our goal in this paper is to explore theextent to which projection, which does not requirethat we have any knowledge of the target language,can push the limits of multilingual coreference res-olution.
If our results indicate that projection is apromising approach, then the automatic coreferenceannotations it produces can be used to augment themanual annotations that capture the properties spe-cific to the target language, thus alleviating the cor-pus annotation bottleneck.2 Related Work on ProjectionThe idea of projecting annotations from a resource-rich language to a resource-scarce language wasoriginally proposed by Yarowsky and Ngai (2001)and subsequently developed by others (e.g., Resnik(2004), Hwa et al (2005)).
These projection al-gorithms assume as input a parallel corpus for thesource language and the target language.
Given therecent availability of machine translation (MT) ser-vices on the Web, researchers have focused moreon translated-based projection rather than acquiringa parallel corpus themselves.
MT-based projectionhas been applied to various NLP tasks, such as part-of-speech tagging (e.g., Das and Petrov (2011)),mention detection (e.g., Zitouni and Florian (2008)),and sentiment analysis (e.g., Mihalcea et al (2007)).There have been two initial attempts to apply pro-jection to create coreference-annotated data for aresource-poor language, both of which involve pro-jecting hand-annotated coreference data from En-glish to Romanian via a parallel corpus.
Specifically,Harabagiu and Maiorano (2000) create an English-Romanian corpus by manually translating the MUC-6 corpus into Romanian and manually project theEnglish annotations to Romanian.
On the otherhand, Postolache et al (2006) apply a word align-ment algorithm to project the hand-annotated En-glish coreference chains and then manually fix theprojection errors on the Romanian side.
Hence,their goal is different from ours in at least two re-spects.
First, while they employ significant knowl-edge of the target language to create a clean corefer-ence corpus, we examine the quality of coreference-annotated data created via an entirely automatic pro-cess, determining quality by the performance of theresolver trained on the data.
Second, unlike ours,neither of these attempts is at the level of defininga technology for projection annotations that can po-tentially be deployed across a large number of lan-guages without coreference-annotated data.7213 Translation-Based ProjectionRecall that our MT-based projection approach tocoreference resolution is composed of three steps.Given a text in the target language, we (1) machine-translate the text to the source language; (2) au-tomatically produce coreference annotations on thetranslated text using a coreference resolver devel-oped for the source language; and (3) project theannotations from the source language to the targetlanguage.
In this section, we employ our approachin three settings, which differ in terms of the ex-tent to which linguistic taggers (e.g., chunkers andnamed entity (NE) recognizers) for the target lan-guage are available.
The goal is to examine whetherthese linguistic taggers can be profitably exploited toimprove the performance of the projection approach.Below we assume that English and French are oursource and target languages, respectively.3.1 Setting 1: No French Taggers AvailableIn this setting, we assume that we do not have accessto any French tagger that we can exploit to improveprojection.
Hence, all we can do is to employ thethree steps involved in the projection approach asdescribed at the beginning of this section to createcoreference-annotated data for French.
Specifically,we translate a French text to an English text usingGoogleTranslate1 , and create coreference chains forthe translated English text using Reconcile2 (Stoy-anov et al, 2010).
To project mentions from En-glish to French, we first align the English and Frenchwords in each pair of parallel sentences, and thenproject the English mentions onto the French text us-ing the alignment.
However, since the alignment isnoisy, the French words to which the words in theEnglish mention are aligned may not form a con-tiguous text span.
To fix this problem, we followYarowsky and Ngai (2001) and use the smallest textspan that covers all the aligned French words to cre-ate the French mention.3 We process the Englishmentions in the text in a left-to-right manner, asprocessing the mentions sequentially enables us toensure that an English mention is not mapped to a1See http://translate.google.com.2See http://www.cs.utah.edu/nlp/reconcile.We use the resolver pre-trained on the Wolverhampton corpus.3Other methods for projecting mentions can be found in Pos-tolache et al (2006), for example.French text span that has already been mapped to bya previously-processed English mention.4To align English and French words, we traineda word alignment model using GIZA++5 (Och andNey, 2000) on a parallel corpus comprising theEnglish-French section of Europarl6 (Koehn, 2005)as well as all the French texts (and their translatedEnglish counterparts) for which we want to auto-matically create coreference chains.
Following com-mon practice, we stemmed the parallel corpus us-ing the Porter stemmer (Porter, 1980) in order toreduce data sparseness.
However, even with stem-ming, we found that many English words were notaligned to any French words by the resulting align-ment model.
This would prevent many English men-tions from being projected to the French side, poten-tially harming the recall of the French coreferenceannotations.
To improve alignment coverage, we re-trained the alignment model by supplying GIZA++with an English-French bilingual dictionary that weassembled using three online dictionary databases:OmegaWiki, Wiktionary, and Universal Dictionary.Furthermore, if a word w appears in both the Englishside and the French side in a pair of parallel sen-tences, we assume that it has the same orthographicform in both languages and hence we augment thebilingual dictionary with the entry (w, w).Note that the use of a supervised resolver likeReconcile does not render our approach supervised,since we can replace it with any resolver, be it super-vised, heuristic, or unsupervised.
In other words, wetreat the resolver built for the source language as ablack box that can produce coreference annotations.3.2 Setting 2: Mention Extractor AvailableNext, we consider a comparatively less resource-scarce setting where a French mention extractor isavailable for identifying mentions in a French text7,and describe how we can modify the projection ap-proach to exploit this French mention extractor.Given a French text we want to coreference-4While we chose to process the mentions in a left-to-rightmanner, any order of processing the mentions would work.5See http://code.google.com/p/giza-pp/.6See http://www.statmt.org/europarl/.7Mention extraction is a term used in Automatic ContentEvaluation to refer to the task of determining the NPs that acoreference system should consider in the resolution process.722annotate, we first translate it to English usingGoogleTranslate and align the French and Englishwords using a French-to-English word alignmentalgorithm.
Next, we identify the mentions in theFrench text using the given mention extractor, andproject them onto the English text using the NP pro-jection algorithm described in Setting 1.
Finally, werun Reconcile on the resulting English mentions togenerate coreference chains for the translated text,and project these chains back to the French text.As explained before, the performance of thismethod is sensitive to the accuracy of the NP projec-tion algorithm in recovering the English mentions,which in turn depends on the accuracy of the wordalignment algorithm.
To make this method more ro-bust to noisy word alignment, we make a modifica-tion to it.
Rather than running Reconcile on the men-tions produced by the NP projection algorithm, weuse Reconcile to identify the mentions directly fromthe translated text.
After that, we create a mappingbetween the English mentions produced by the NPprojection algorithm and those produced by Recon-cile using a small set of heuristics.Specifically, let MP be the set of mentions identi-fied by the NP projection algorithm and MR be theset of mentions identified by Reconcile.
For eachmention mP in MP , we map it to a mention in MRthat shares the same right boundary.
If this fails, wemap it to a mention that covers its entire text span.
Ifthis fails again, we map it to a mention that has a par-tial overlap with it.
If this still fails, we assume thatmP is not found by Reconcile and simply add mP toMR.
As before, we process the mentions in MP ina left-to-right manner in order to ensure that no twomentions in MP are mapped to the same Reconcilemention.
Finally, we discard all mentions in MR thatare not mapped by any mention in MP , and presentMR to Reconcile for coreference resolution.
Sincewe now have a 1-to-1 mapping between the Recon-cile mentions and the French mentions, projectingthe coreference results back to French is trivial.It may not be immediately clear why the exploita-tion of the mention extractor in this setting may yieldbetter coreference annotations than those producedin Setting 1.
To see the reason, recall that one sourceof errors inherent in a projection approach is wordalignment errors.
In Setting 1, when we tried toproject English mentions to the French text, wordalignment errors would adversely affect the abilityof the NP projection algorithm to correctly definethe boundaries of the French mentions.
Since coref-erence performance depends crucially on the abil-ity to correctly identify mentions (Stoyanov et al,2009), the presence of word alignment errors im-plies that the resulting French coreference annota-tions could score poorly even if the English coref-erence annotations produced by Reconcile were ofhigh quality.
In the current setting, on the otherhand, we reduce the sensitivity of coreference per-formance to word alignment errors via the use of theFrench mention extractor to produce more accurateFrench mention boundaries.3.3 Setting 3: Additional Taggers AvailableFinally, we consider a setting that is the leastresource-scarce of the three.
We assume that in ad-dition to a French mention extractor, we have accessto other French linguistic taggers (e.g., syntactic andsemantic parsers) that will allow us to generate thelinguistic features needed to train a French resolveron the projected coreference annotations.Specifically, assume that Test is a set of Frenchtexts we want to coreference-annotate, and Trainingis a set of French texts that is disjoint from Test but isdrawn from the same domain as Test.8 To annotatethe Test texts, we perform the following steps.
First,we employ the French mention extractor in combi-nation with the method described in Setting 2 to au-tomatically coreference-annotate the Training texts.Next, motivated by Kobdani et al (2011), we traina French coreference resolver on the automaticallycoreference-annotated training texts, using the fea-tures provided by the available linguistic taggers.
Fi-nally, we apply the resolver to generate coreferencechains for each Test text.Two questions arise.
First, is this method neces-sarily better than the one described in Setting 2?
Wehypothesize that the answer is affirmative: not onlycan this method exploit the knowledge about the tar-get language provided by the additional linguistictaggers, but the resulting coreference resolver mayallow us to generalize from the (noisily labeled) dataand make this method more robust to the noise in-8We assume that it is easy to assemble the Training set, sinceunlabeled texts are typically easy to collect in practice.723herent in the projected coreference annotations thanthe previously-described methods.
Second, is thismethod necessarily better than projection via a par-allel corpus?
Like the first question, this is also anempirical question.
Nevertheless, one reason whythis method is intuitively better is that it ensures thatthe training and test documents are drawn from thesame domain.
On the other hand, when project-ing annotations via a parallel corpus, we may en-counter a domain mismatch problem if the parallelcorpus and the test documents come from differentdomains, and the coreference resolver may not workwell if it is trained and tested on different domains.4 Coreference Resolution SystemTo train the coreference resolver employed in Set-ting 3 in the previous section, we need to derivelinguistic features from the documents in the targetlanguage.
In our experiments, we employ the coref-erence data sets produced as part of the SemEval-2010 shared task on Coreference Resolution in Mul-tiple Languages.
The shared task organizers havemade publicly available six data sets that corre-spond to six European languages.
Each data setcomprises not only training and test documents thatare coreference-annotated, but also a number ofword-based linguistic features from which we derivemention-based linguistic features for training a re-solver.
In this section, we will describe how this re-solver is trained and then applied to generate coref-erence chains for unseen documents.Training the coreference classifier.
As our coref-erence model, we train a mention-pair model, whichis a classifier that determines whether two mentionsare co-referring or not (e.g., Soon et al (2001), Ngand Cardie (2002)).9 Each instance i(mj ,mk) cor-responds to mj (a candidate antecedent) and mk (themention to be resolved), and is represented by a setof 23 features shown in Table 1.
As we can see, eachfeature is either relational, capturing the relation be-tween mj and mk, or non-relational, capturing thelinguistic property of mk.
The possible values ofa relational feature (except LEXICAL) are C (com-patible), I (incompatible), and NA (the comparison9Note that any supervised coreference model can be used,such as an entity-mention model (e.g., Luo et al (2004), Yanget al (2008)) or a ranking model (e.g., Denis and Baldridge(2008), Rahman and Ng (2009)).cannot be made due to missing data).
For a non-relational feature, we refer the reader to the data setsfor the list of possible values.10We follow Soon et al?s (2001) method for creat-ing training instances.
Specifically, we create (1) apositive instance for each anaphoric mention mk andits closest antecedent mj; and (2) a negative instancefor mk paired with each of the intervening mentions,mj+1,mj+2, .
.
.
,mk?1.
The classification associ-ated with a training instance is either positive or neg-ative, depending on whether the two mentions arecoreferent in the associated text.
To train the classi-fier, we use SVMlight (Joachims, 1999).Applying the classifier to a test text.
After train-ing, the classifier is used to identify an antecedentfor a mention in a test text.
Specifically, each men-tion, mk, is compared to each preceding mention,mj , from right to left, and mj is selected as the an-tecedent of mk if the pair is classified as coreferent.The process terminates as soon as an antecedent isfound for mk or the beginning of the text is reached.5 EvaluationWe evaluate our MT-based projection approach foreach of the three settings described in Section 3.5.1 Experimental SetupData sets.
We use the Spanish and Italian data setsfrom the SemEval-2010 shared task on CoreferenceResolution in Multiple Languages.11 Each data setis composed of a training set and a test set.
Statisticsof these data sets are shown in Table 2.Spanish ItalianTraining Set Statisticsnumber of mentions 78779 24853number of non-singleton clusters 48681 18376number of singleton clusters 37336 15984Test Set Statisticsnumber of mentions 14133 13394number of non-singleton clusters 8789 9520number singleton clusters 6737 8288Table 2: Statistics of the data sets.10The data sets can be downloaded from http://stel.ub.edu/semeval2010-coref/datasets.11Note, however, that our approach is equally applicable toother languages evaluated in the shared task.724Features describing mk, the mention to be resolved1 NUM WORDS the number of words in mk2 COARSE POS the coarse POS of mk (see ?PoS?
in Recasens et al (2010))3 FINE POS the fine-grained POS of mk (see ?PoS type?
in Recasens et al (2010))4 NE the named entity tag of mk if mk is a named entity; else NA5 SR the semantic role of mk6 GRAMROLE the grammatical role of mk7 NUMBER the number of mk8 GENDER the gender of mk9 PERSON the person of mk (e.g., first, second, third) if it is pronominal; else NAFeatures describing the relationship between mj , a candidate antecedent and mk, the mention to be resolved10 CS STR MATCH determines whether the mentions are the same string11 CI STR MATCH same as feature 10, except that case differences are ignored12 CS SUBSTR MATCH determines whether one mention is a substring of the other13 CI SUBSTR MATCH same as feature 12, except that case differences are ignored14 NUMBER MATCH determines whether the mentions agree in number15 GENDER MATCH determines whether the mentions agree in gender16 COARSE POS MATCH determines whether the mentions have the same coarse POS tag17 FINE POS MATCH determines whether the mentions have the same fine-grained POS tag18 ROLE MATCH determines whether the mentions have the same grammatical role19 NE MATCH determines whether both are NEs and have the same NE type20 SR MATCH determines whether the mentions have the same semantic role21 ALIAS determines whether one mention is an abbreviation or an acronym of the other22 PERSON MATCH determines whether both mentions are pronominal and have the same person23 LEXICAL the concatenation of the heads of the two mentionsTable 1: Feature set for coreference resolution.Scoring programs.
To score the output of a coref-erence resolver, we employ four scoring programs,MUC (Vilain et al, 1995), B3 (Bagga and Baldwin,1998), ?3-CEAF (Luo, 2005), and BLANC (Re-casens and Hovy, 2011), which were downloadedfrom the shared task website (see Footnote 10).Gold-standard versus regular settings.
The for-mat of each data set follows that of a typical CoNLLshared task data set.
In other words, each row cor-responds to a word in a document; moreover, all butthe last column contain the linguistic features com-puted for the words, and the last column stores thecoreference information.
Some of the features werecomputed via automatic means, but some were ex-tracted from human annotations.
Given this distinc-tion, the shared task organizers defined two evalua-tion settings: in the regular setting, only the columnsthat were computed automatically can be used to de-rive coreference features for classifier training, andresults should be reported on system mentions; onthe other hand, in the gold-standard setting, onlythe columns that were extracted from human annota-tions can be used to derive coreference features, andresults should be reported on true mentions.
We willpresent results corresponding to both settings.
Notethat these two settings should not be confused withthe three settings described in Section 3.Mention extraction.
Recall that Settings 2 and 3both assume the availability of a mention extrac-tor for extracting mentions in the target language.In our experiments, we extract mentions using twomethods.
First, we assume the availability of anoracle mention extractor that will enable us to ex-tract true mentions (i.e., gold-standard mentions) di-rectly from the test texts.
Second, we employ simpleheuristics to automatically extract system mentions.Since coreference performance is sensitive to theaccuracy of mention extraction (Stoyanov et al,2009), we experiment with several heuristic meth-ods for extracting system mentions for both Span-ish and Italian.
According to our cross-validationexperiments on the training data, the best heuris-tic for extracting Spanish mentions is different fromthat for extracting Italian mentions.
Specifically, for725Spanish, the best heuristic method operates as fol-lows.
First, it extracts all the syntactic heads (i.e.,the word tokens whose gold dependency labels areSUBJ, PRED, or GMOD).
Second, for each syntac-tic head, it identifies the smallest text span contain-ing the head and all of its dependents, and creates amention from this text span.
For Italian, on the otherhand, the best heuristic simply involves creating onemention for each gold NE.
The reason why this sim-ple heuristic works well is that most of the Italianmentions are NEs, owing to the fact that abstractNPs and pronouns are also annotated as NEs in theItalian data set.
When evaluated on the test set, theheuristic-based mention extractor achieves F-scoresof 80.2 (78.4 recall, 82.1 precision) for Spanish and92.3 (85.9 recall, 99.6 precision) for Italian.5.2 Results and Discussion5.2.1 Supervised ResultsOur supervised systems.
While our MT-basedprojection approach is unsupervised (i.e., it does notrely on any coreference annotations from the targetlanguage), it would be informative to see the perfor-mance of the supervised resolvers, since their perfor-mance can be viewed as a crude upper bound on theperformance of our unsupervised systems.
Specif-ically, we train a mention-pair model on the train-ing set using the 23 features shown in Table 1 andSVMlight as the underlying learning algorithm12,and apply the resulting model in combination withSoon et al?s clustering algorithm (see Section 4) togenerate coreference chains for the test texts.Results on the test sets, reported in terms of re-call (R), precision (P), and F-score (F) computed bythe four coreference scorers, are shown in the firsttwo rows of Table 3 (Spanish) and Table 4 (Italian).For convenience, we summarize a system?s perfor-mance using a single number, which is shown in thelast column (Average) and is obtained by taking asimple average of the F-scores of the four scorers.More specifically, row 1, which is marked with a?G?, and row 2, which is marked with a ?R?, showthe results obtained under the gold-standard settingand the regular setting, respectively.As we can see, under the gold-standard setting,12All SVM learning parameters in this and other experimentsin this paper are set to their default values.the supervised resolver achieves an average F-scoreof 66.1 (Spanish) and 65.9 (Italian).
Not surpris-ingly, under the regular setting, its average F-scoredrops statistically significantly13 to 54.6 (Spanish)and 63.4 (Italian).14Best systems in the shared task.
To determinewhether the upper bounds established by our su-pervised systems are reasonable, we show the re-sults of the best-performing resolvers participatingin the shared task for both languages under the gold-standard and regular settings in rows 3 and 4 of Ta-bles 3 and 4.
Since none of the participating systemsachieved the best score over all four scorers, we re-port the performance of the system that has the high-est average F-score.
According to the shared taskwebsite, TANL-1 (Attardi et al, 2010) achieved thebest average F-score in the regular setting for Span-ish, whereas SUCRE (Kobdani and Schu?tze, 2010)outperformed others in the remaining settings.Comparing these best shared task results with oursupervised results in rows 1 and 2, we see that ouraverage F-score for Spanish/Gold is worse than itsshared task counterpart by 0.7 points, but otherwiseour system outperforms in other settings w.r.t.
av-erage F-score, specifically by 5.0 points for Span-ish/Regular (due to a better MUC F-score), by 3.4?4.7 points for Italian (due to better CEAF, B3, andBLANC scores).
Overall, these results suggest thatthe scores achieved by our systems are at least ascompetitive as the best shared task scores.5.2.2 Unsupervised ResultsNext, we evaluate our projection algorithm.Setting 1.
Results of our approach, when appliedin Setting 1, are shown in row 5 of Tables 3 and 4.Given that it has to operate under the severe condi-tion where no linguistic taggers are available for thetarget language, it is perhaps not surprising to seethat its performance is significantly worse than thatof its supervised counterparts.Setting 2.
Recall that this setting is less resource-scarce than Setting 1 in that a mention extractor for13All significance test results in this paper are obtained usingone-way ANOVA, with p set to 0.05.14Separately, we determined whether the performance dropin the regular setting is due to the use of automatically computedfeatures or the use of system mentions, and found that the latterwas almost entirely responsible for the drop.726CEAF MUC B3 BLANC AverageApproach R P F R P F R P F R P F F1 Supervised (G) 68.8 68.8 68.8 58.2 52.6 55.3 76.5 75.1 75.8 62.9 66.1 64.3 66.12 Supervised (R) 57.4 60.1 58.8 41.0 46.3 43.5 57.6 64.8 61.0 53.9 65.0 55.2 54.63 Shared task best (G) 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5 66.84 Shared task best (R) 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4 49.65 Setting 1 35.9 52.9 42.8 10.8 48.7 17.7 30.5 63.9 41.3 51.2 72.6 48.7 37.66 Setting 2 (True) 65.6 65.6 65.6 16.8 64.7 26.7 64.3 96.9 77.3 52.8 78.8 54.6 56.17 Setting 2 (System) 53.2 55.7 54.4 13.4 58.5 21.8 49.8 79.7 61.3 50.7 75.5 49.5 46.88 Setting 3 (G) 65.9 65.9 65.9 48.1 45.2 46.6 72.3 72.6 72.5 60.1 61.4 60.7 61.49 Setting 3 (R) 55.3 55.3 55.3 34.1 41.6 37.5 55.1 63.6 59.0 53.8 62.1 54.9 51.7Table 3: Results for SpanishCEAF MUC B3 BLANC AverageApproach R P F R P F R P F R P F F1 Supervised (G) 74.5 74.5 74.5 31.8 67.4 43.2 74.4 93.6 82.9 58.4 79.6 62.9 65.92 Supervised (R) 73.7 74.3 74.0 31.9 68.0 43.4 60.8 92.5 73.3 58.4 79.6 62.9 63.43 Shared task best (G) 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9 61.24 Shared task best (R) 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7 60.05 Setting 1 17.0 26.0 20.6 8.1 28.5 12.6 14.1 30.5 19.3 50.1 62.9 32.9 21.46 Setting 2 (True) 73.3 73.3 73.3 14.2 60.6 23.0 72.9 96.8 83.2 51.9 77.9 53.2 58.27 Setting 2 (System) 60.4 70.1 64.9 17.2 68.2 27.5 59.3 97.1 73.6 52.0 82.9 53.4 54.98 Setting 3 (G) 64.3 64.3 64.3 28.3 63.3 39.1 65.3 87.4 74.8 55.1 74.7 57.5 58.99 Setting 3 (R) 61.1 62.9 61.9 29.5 63.2 40.2 60.3 84.1 70.2 55.3 72.9 58.3 57.7Table 4: Results for Italianthe target language is available.
Results of our al-gorithm, when operating under Setting 2 using truementions and system mentions, are shown in rows6 and 7 of Tables 3 and 4, respectively.
In com-parison to the results for Setting 1, we see that theF-scores obtained under Setting 2 increase signifi-cantly, regardless of (1) the scoring programs and(2) whether true mentions or system mentions areused.
These results provide evidence for our earlierhypothesis that our projection algorithm can prof-itably exploit the linguistic knowledge about the tar-get language that is available to it.
In particular, themention extractor helps make our approach less sen-sitive to word alignment and NP projection errors.In comparison to our supervised results in rows 1and 2, our algorithm still lags behind by about 8?10points in average F-score.
However, this should notbe surprising, since our algorithm is unsupervised.Looking closer at the results, we can see that theperformance lag by our approach can be attributedto its lower recall: in general, the lag in MUC recallappears to be more acute than that in B3 and CEAFrecall.
Since MUC only scores non-singleton clus-ters wheres B3 and CEAF score both singleton andnon-singleton clusters, these results suggest that ourapproach is better at identifying singleton clustersthan recovering coreference links.Setting 3.
Finally, we evaluate our approach in asetting where it has access to all the informationavailable to our supervised resolvers, except for thegold-standard coreference annotations on the train-ing sets.
Specifically, our approach uses projectedcoreference annotations to train a resolver on thetraining texts, whereas the supervised resolvers doso using gold-standard annotations.Comparing Settings 2 and 3 with respect to truementions (rows 6 and 8 of Tables 3 and 4), we seemixed results.
According to MUC and BLANC, theresolvers in Setting 3 are significantly better thanthose in Setting 2 for both languages.
According toB3, the resolvers in Setting 2 are significantly betterthan those in Setting 3 for both languages.
Accord-ing to CEAF, the Spanish resolvers in Setting 3 aresignificantly better than their counterparts in Setting2, but the opposite is true for the Italian resolvers.To understand these somewhat contradictory per-formance trends, let us first note that the dramatic in-crease in the MUC F-score can be attributed to large727gains in MUC recall.
This suggests that the clas-sifiers being trained in Setting 3 have enabled thediscovery of additional coreference links.
In otherwords, there are benefits to be obtained just by learn-ing over noisy coreference annotations, a result thatwe believe is quite interesting.
However, not all ofthese newly discovered coreference links are correct.The fact that some scoring programs (e.g., B3) aremore sensitive to spurious coreference links than theothers (e.g., MUC) explains these mixed results.Nevertheless, according to average F-score, theresolvers in Setting 3 perform significantly betterthan those in Setting 2 for both languages: F-scoreincreases by 5.3 points for Spanish and 0.7 points forItalian.
Similar trends can be observed when com-paring the two settings w.r.t.
system mentions (rows7 and 9 of Tables 3 and 4): F-score increases by 4.9points for Spanish and 2.8 points for Italian.While our Setting 3 results still underperform thesupervised results in rows 1 and 2, we can see thatthey achieve 93?94% of the average F-scores of thesupervised Spanish resolvers and 89?91% of the av-erage F-scores of the supervised Italian resolvers.Importantly, recall that our approach achieves thislevel of performance without relying on any gold-standard coreference annotations in Spanish andItalian, and we believe that these results demonstratethe promise of our MT-based projection approach.Since these results suggest that our approach can-not be successfully applied without MT services, aparallel corpus for learning a word alignment model,and a mention extractor for the target language, anatural question is: to what extent do these require-ments limit the applicability of our approach?
Whileit is the case that our approach cannot be applied toa truly resource-scarce language, it can be applied tothe numerous Indian and East European languagesfor which the aforementioned requirements are sat-isfied but coreference-annotated data is not readilyavailable.6 Conclusions and Future WorkWe explored the under-investigated yet challengingtask of performing coreference resolution for a lan-guage for which we have no coreference-annotateddata and no linguistic knowledge of the language.Our translation-based projection approach has theflexibility to exploit any available knowledge aboutthe target language.
In experiments with Spanishand Italian, we obtained promising results: our ap-proach achieved around 90% of the performance ofa supervised resolver when only a mention extrac-tor for the target language was available.
We believethat this approach has the potential to allow coref-erence technologies to be deployed across a largernumber of languages than is currently possible, andthat this is just the beginning of a new line of work.To gain additional insights into our approach,we plan to pursue several directions.
First, wewill isolate the impact of each factor that ad-versely affects its performance, including errorsin projection, translation, and coreference resolu-tion in the resource-rich language.
Second, wewill perform an empirical comparison of two ap-proaches to projecting coreference annotations, ourtranslation-based approach and Camargo de Souzaand Orasan?s (2011) approach, where annotationsare projected via a parallel corpus.
Third, rather thantranslate from the target to the source language, wewill examine whether it is better to translate all thecoreference-annotated data available in the sourcelanguage to the target language, and train a coref-erence model for the target language on the trans-lated data.
Fourth, since the success of our pro-jection approach depends heavily on the accuraciesof machine translation as well as coreference res-olution in the source language, we will determinewhether their accuracies can be improved via an en-semble approach, where we employ multiple MTengines and multiple coreference resolvers.
Finally,we plan to employ our approach to alleviate thecorpus-annotation bottleneck, specifically by usingthe annotated data it produces to augment the man-ual coreference annotations that capture the specificproperties of the target language.AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlier draftof the paper.
This work was supported in part byNSF Grants IIS-0812261 and IIS-1147644.
Anyopinions, findings, or conclusions expressed in thispaper are those of the authors and do not necessarilyreflect the views or official policies of NSF.728ReferencesGiuseppe Attardi, Maria Simi, and Stefano Dei Rossi.2010.
TANL-1: Coreference resolution by parse anal-ysis and similarity clustering.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 108?111.Amit Bagga and Breck Baldwin.
1998.
Entity-basedcross-document coreferencing using the vector spacemodel.
In Proceedings of the 36th Annual Meeting ofthe Association for Computational Linguistics and the17th International Conference on Computational Lin-guistics, pages 79?85.Jennifer Camargo de Souza and Constantine Orasan.2011.
Can projected chains in parallel corpora helpcoreference resolution?
In Anaphora Processing andApplications, pages 59?69.
Springer.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 600?609.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 660?669.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics, pages848?855.Aria Haghighi and Dan Klein.
2010.
Coreference res-olution in a modular, entity-centered model.
In Pro-ceedings of Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages385?393.Sanda Harabagiu and Steven Maiorano.
2000.
Multi-lingual coreference resolution.
In Proceedings of theSixth Applied Natural Language Processing Confer-ence, pages 142?149.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(3):311?325.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Scho?lkopf, Christo-pher Burges, and Alexander Smola, editors, Advancesin Kernel Methods ?
Support Vector Learning, pages44?56.
MIT Press, Cambridge, MA.Hamidreza Kobdani and Hinrich Schu?tze.
2010.
SU-CRE: A modular system for coreference resolution.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, pages 92?95.Hamidreza Kobdani, Hinrich Schu?tze, MichaelSchiehlen, and Hans Kamp.
2011.
Bootstrap-ping coreference resolution using word associations.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 783?792.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of MTSummit X.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the Bell tree.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguis-tics, pages 135?142.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of Human LanguageTechnology Conference and Conference on EmpiricalMethods in Natural Language Processing, pages 25?32.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007.Learning multilingual subjective language via cross-lingual projections.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguis-tics, pages 976?983..Ruslan Mitkov.
1999.
Multilingual anaphora resolution.Machine Translation, 14(3?4):281?299.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 104?111.Vincent Ng.
2008.
Unsupervised models for coreferenceresolution.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 640?649.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics.Hoifung Poon and Pedro Domingos.
2008.
Joint un-supervised coreference resolution with Markov Logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 650?659.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Oana Postolache, Dan Cristea, and Constantin Orasan.2006.
Transferring coreference chains through wordalignment.
In Proceedings of the Fifth InternationalConference on Language Resources and Evaluation,pages 889?892.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-sky, and Christopher Manning.
2010.
A multi-pass729sieve for coreference resolution.
In Proceedings ofthe 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 492?501.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 968?977.Marta Recasens and Eduard Hovy.
2011.
BLANC: Im-plementing the Rand Index for coreference evaluation.Natural Language Engineering, 17(4):485?510.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena,M.
Anto`nia Mart?
?, Mariona Taule?, Ve?roniqueHoste, Massimo Poesio, and Yannick Versley.
2010.Semeval-2010 task 1: Coreference resolution in multi-ple languages.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 1?8.Philip Resnik.
2004.
Exploiting hidden meanings: Us-ing bilingual text for monolingual annotation.
In Pro-ceedings of the 5th International Conference on Com-putational Linguistics and Intelligent Text Processing,pages 283?299.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrase coref-erence resolution: Making sense of the state-of-the-art.
In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 656?664.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.
Coref-erence resolution with reconcile.
In Proceedings of theACL 2010 Conference Short Papers, pages 156?161.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the Sixth Message Understanding Conference,pages 45?52.Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, andSheng Li.
2008.
An entity-mention model for coref-erence resolution with inductive logic programming.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 843?851.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedings ofthe Second Meeting of the North American Chapter ofthe Association for Computational Linguistics, pages200?207.Imed Zitouni and Radu Florian.
2008.
Mention detec-tion crossing the language barrier.
In Proceedings ofthe 2008 Conference on Empirical Methods in NaturalLanguage Processing, pages 600?609.730
