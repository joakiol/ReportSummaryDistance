Proceedings of the First Workshop on Argumentation Mining, pages 79?87,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsMining Arguments From 19th Century Philosophical Texts Using TopicBased ModellingJohn Lawrence and Chris ReedSchool of Computing,University of Dundee, UKColin AllenDept of History & Philosophy of Science,Indiana University, USASimon McAlister and Andrew RavenscroftCass School of Education & Communities,University of East London, UKDavid BourgetCentre for Digital Philosophy,University of Western Ontario, CanadaAbstractIn this paper we look at the manual anal-ysis of arguments and how this comparesto the current state of automatic argumentanalysis.
These considerations are used todevelop a new approach combining a ma-chine learning algorithm to extract propo-sitions from text, with a topic model to de-termine argument structure.
The resultsof this method are compared to a manualanalysis.1 IntroductionAutomatic extraction of meaningful informationfrom natural text remains a major challenge fac-ing computer science and AI.
As research on spe-cific tasks in text mining has matured, it has beenpicked up commercially and enjoyed rapid suc-cess.
Existing text mining techniques struggle,however, to identify more complex structures indiscourse, particularly when they are marked by acomplex interplay of surface features rather thansimple lexeme choice.The difficulties in automatically identifyingcomplex structure perhaps suggest why there hasbeen, to date, relatively little work done in the areaof argument mining.
This stands in contrast to thelarge number of tools and techniques developedfor manual argument analysis.In this paper we look at the work which hasbeen done to automate argument analysis, as wellas considering a range of manual methods.
Wethen apply some of the lessons learnt from thesemanual approaches to a new argument extractiontechnique, described in section 3.
This techniqueis applied to a small sample of text extracted fromthree chapters of ?THE ANIMAL MIND: A Text-Book of Comparative Psychology?
by MargaretFloy Washburn, and compared to a high level man-ual analysis of the same text.
We show that de-spite the small volumes of data considered, thisapproach can be used to produce, at least, an ap-proximation of the argument structure in a pieceof text.2 Existing Approaches to ExtractingArgument from Text2.1 Manual Argument AnalysisIn most cases, manual argument analysis can besplit into four distinct stages as shown in Figure 1.Text segmentationArgument /Non-ArgumentSimple StructureRefined StructureFigure 1: Steps in argument analysisText segmentation This involves selecting frag-ments of text from the original piece thatwill form the parts of the resulting argumentstructure.
This can often be as simple as high-lighting the section of text required, for ex-ample in OVA (Bex et al., 2013).
Though insome cases, such as the AnalysisWall1, this isa separate step carried out by a different user.1http://arg.dundee.ac.uk/analysiswall79Argument / Non-Argument This step involvesdetermining which of the segments previ-ously identified are part of the argument be-ing presented and which are not.
For mostmanual analysis tools this step is performedas an integral part of segmentation: the an-alyst simply avoids segmenting any parts ofthe text that are not relevant to the argument.This step can also be performed after deter-mining the argument structure by discardingany segments left unlinked to the rest.Simple Structure Once the elements of the argu-ment have been determined, the next step is toexamine the links between them.
This couldbe as simple as noting segments that are re-lated, but usually includes determining sup-port/attack relations.Refined Structure Having determined the basicargument structure, some analysis tools al-low this to be refined further by adding de-tails such as the argumentation scheme.2.2 Automatic Argument AnalysisOne of the first approaches to argument mining,and perhaps still the most developed, is the workcarried out by Moens et al.
beginning with (Moenset al., 2007), which attempts to detect the argu-mentative parts of a text by first splitting the textinto sentences and then using features of these sen-tences to classify each as either ?Argument?
or?Non-Argument?.
This approach was built uponin (Palau and Moens, 2009) where an additionalmachine learning technique was implemented toclassify each Argument sentence as either premiseor conclusion.Although this approach produces reasonableresults, with a best accuracy of 76.35% forArgument/Non-Argument classification and f-measures of 68.12% and 74.07% for classifica-tion as premise or conclusion, the nature of thetechnique restricts its usage in a broader context.For example, in general it is possible that a sen-tence which is not part of an argument in one sit-uation may well be in another.
Similarly, a sen-tence which is a conclusion in one case is often apremise in another.Another issue with this approach is the originaldecision to split the text into sentences.
While thismay work for certain datasets, the problem here isthat, in general, multiple propositions often occurwithin the same sentence and some parts of a sen-tence may be part of the argument while others arenot.The work of Moens et al.
focused on the firstthree steps of analysis as mentioned in section 2.1,and this was further developed in (Feng and Hirst,2011), which looks at fitting one of the top fivemost common argumentation schemes to an argu-ment that has already undergone successful extrac-tion of conclusions and premises, achieving accu-racies of 63-91% for one-against-others classifica-tion and 80-94% for pairwise classification.Despite the limited work carried out on argu-ment mining, there has been significant progressin the related field of opinion mining (Pang andLee, 2008).
This is often performed at the doc-ument level, for example to determine whether aproduct review is positive or negative.
Phrase-level sentiment analysis has been performed in asmall number of cases, for example (Wilson et al.,2005) where expressions are classified as neutralor polar before determining the polarity of the po-lar expressions.Whilst it is clear that sentiment analysis alonecannot give us anything close to the results of man-ual argument analysis, it is certainly possible thatthe ability to determine the sentiment of a givenexpression may help to fine-tune any discoveredargument structure.Another closely related area is ArgumentativeZoning (Teufel et al., 1999), where scientific pa-pers are annotated at the sentence level with labelsindicating the rhetorical role of the sentence (criti-cism or support for previous work, comparison ofmethods, results or goals, etc.).
Again, this infor-mation could assist in determining structure, andindeed shares some similarities to the topic mod-elling approach as described in section 3.2 .3 Methodology3.1 Text SegmentationMany existing argument mining approaches, suchas (Moens et al., 2007), take a simple approachto text segmentation, for example, simply splittingthe input text into sentences, which, as discussed,can lead to problems when generally applied.There have been some more refined attemptsto segment text, combining the segmentation stepwith Argument/Non-Argument classification.
Forexample, (Madnani et al., 2012) uses three meth-ods: a rule-based system; a supervised probabilis-80tic sequence model; and a principled hybrid ver-sion of the two, to separate argumentative dis-course into language used to express claims andevidence, and language used to organise them(?shell?).
Whilst this approach is instructive, itdoes not necessarily identify the atomic parts ofthe argument required for later structural analysis.The approach that we present here does not con-sider whether a piece of text is part of an argu-ment, but instead simply aims to split the text intopropositions.
Proposition segmentation is carriedout using a machine learning algorithm to identifyboundaries, classifying each word as either the be-ginning or end of a proposition.
Two Naive Bayesclassifiers, one to determine the first word of aproposition and one to determine the last, are gen-erated using a set of manually annotated trainingdata.
The text given is first split into words and alist of features calculated for each word.
The fea-tures used are given below:word The word itself.length Length of the word.before The word before.after The word after.
Punctuation is treated as aseparate word so, for example, the last wordin a sentence may have an after feature of ?.
?.pos Part of speech as identified by the PythonNatural Language Toolkit POS tagger2.Once the classifiers have been trained, thesesame features can then be determined for eachword in the test data and each word can be clas-sified as either ?start?
or ?end?.
Once the classi-fication has taken place, we run through the textand when a ?start?
is reached we mark a proposi-tion until the next ?end?.3.2 Structure identificationHaving extracted propositions from the text wenext look at determining the simple structure ofthe argument being made and attempt to establishlinks between propositions.
We avoid distinguish-ing between Argument and Non-Argument seg-ments at this stage, instead assuming that any seg-ments left unconnected are after the structure hasbeen identified are Non-Argument.2http://www.nltk.org/In order to establish these links, we first con-sider that in many cases an argument can be repre-sented as a tree.
This assumption is supported byaround 95% of the argument analyses contained inAIFdb (Lawrence et al., 2012) as well as the factthat many manual analysis tools including Arau-caria (Reed and Rowe, 2004), iLogos3, Rationale(Van Gelder, 2007) and Carneades (Gordon et al.,2007), limit the user to a tree format.Furthermore, we assume that the argument treeis generated depth first, specifically that the con-clusion is presented first and then a single lineof supporting points is followed as far as possi-ble before working back up through the pointsmade.
The assumption is grounded in work incomputational linguistics that has striven to pro-duce natural-seeming argument structures (Reedand Long, 1997).
We aim to be able to constructthis tree structure from the text by looking at thetopic of each proposition.
The idea of relatingchanges in topic to argument structure is supportedby (Cardoso et al., 2013), however, our approachhere is the reverse, using changes in topic to de-duce the structure, rather than using the structureto find topic boundaries.Based on these assumptions, we can determinestructure by first computing the similarity of eachproposition to the others using a Latent Dirich-let Allocation (LDA) model.
LDA is a genera-tive model which conforms to a Bayesian infer-ence about the distributions of words in the docu-ments being modelled.
Each ?topic?
in the modelis a probability distribution across a set of wordsfrom the documents.To perform the structure identification, a topicmodel is first generated for the text to be stud-ied and then each proposition identified in the testdata is compared to the model, giving a similar-ity score for each topic.
The propositions are thenprocessed in the order in which they appear in thetest data.
Firstly, the distance between the propo-sition and its predecessor is calculated as the Eu-clidean distance between the topic scores.
If thisis below a set threshold, the proposition is linkedto its predecessor.
If the threshold is exceeded, thedistance is then calculated between the propositionand all the propositions that have come before, ifthe closest of these is then within a certain dis-tance, an edge is added.
If neither of these criteria3http://www.phil.cmu.edu/projects/argument_mapping/81is met, the proposition is considered unrelated toanything that has gone before.By adjusting the threshold required to join aproposition to its predecessor we can change howlinear the structure is.
A higher threshold will in-crease the chance that a proposition will instead beconnected higher up the tree and therefore reducelinearity.
The second threshold can be used to alterthe connectedness of the resultant structure, witha higher threshold giving more unconnected sec-tions.It should be noted that the edges obtained donot have any direction, and there is no further de-tail generated at this stage about the nature of therelation between two linked propositions.4 Manual AnalysisIn order to train and test our automatic analysis ap-proach, we first required some material to be man-ually analysed.
The manual analysis was carriedout by an analyst who was familiar with manualanalysis techniques, but unaware of the automaticapproach that we would be using.
In this way weavoided any possibility of fitting the data to thetechnique.
He also chose areas of texts that wereestablished as ?rich?
in particular topics in animalpsychology through the application of the mod-elling techniques above, the assumption being thatthese selections would also contain relevant argu-ments.The material chosen to be analysed was takenfrom ?THE ANIMAL MIND: A TextBook ofComparative Psychology by Margaret Floy Wash-burn, 1908?
made available to us through the HathiTrust.The analyst began with several selected pas-sages from this book and in each case generated ananalysis using OVA4, an application which linksblocks of text using argument nodes.
OVA pro-vides a drag-and-drop interface for analysing tex-tual arguments.
It is reminiscent of a simplifiedAraucaria, except that it is designed to work in anonline environment, running as an HTML5 canvasapplication in a browser.The analyst was instructed only to capture theargument being made in the text as well as theycould.
Arguments can be mapped at different lev-els depending upon the choices the analyst priori-tises.
This is particularly true of volumes suchas those analysed here, where, in some cases, the4http://ova.computing.dundee.ac.uksame topic is pursued for a complete chapter andso there are opportunities to map the extended ar-gument.In this case the analyst chose to identify discretesemantic passages corresponding to a proposition,albeit one that may be compound.
An example isshown in Figure 2.
A section of contiguous textfrom the volume has been segmented and markedup using OVA, where each text box corresponds tosuch a passage.
It is a problem of the era in whichthe chosen volume is written that there is a ver-bosity and indirectness of language, so a passagemay stretch across several sentences.
The contentof each box was then edited to contain only ar-gumentative content and a simple structure pro-posed by linking supporting boxes towards con-cluding or sub-concluding boxes.
Some fifteenOVA maps were constructed to represent the argu-ments concerned with animal consciousness andwith anthropomorphism.In brief, this analysis approach used OVA as aformal modelling tool, or lens, to characterise andbetter understand the nature of argument withinthe texts that were considered, as well as produc-ing a large set of argument maps.
Therefore, itrepresented a data-driven and empirically authen-tic approach and set of data against which the au-tomated techniques could be considered and com-pared.5 Automatic Analysis ResultsAs discussed in section 4, the manual analysis isat a higher level of abstraction than is carried outin typical approaches to critical thinking and argu-ment analysis (Walton, 2006; Walton et al., 2008),largely because such analysis is very rarely ex-tended to arguments presented at monograph scale(see (Finocchiaro, 1980) for an exception).
Themanual analysis still, however, represents an idealto which automatic processing might aspire.
In or-der to train the machine learning algorithms, how-ever, a large dataset of marked propositions is re-quired.
To this end, the manual analysis conductedat the higher level is complemented by a more fine-grained analysis of the same text which marks onlypropositions (and not inter-proposition structure).In this case a proposition was considered to cor-respond to the smallest span of text containing asingle piece of information.
It is this detailed anal-ysis of the text which is used as training data fortext segmentation.82Figure 2: Sample argument map from OVA5.1 Text segmentationAn obvious place to start, then, is to assess the per-formance of the proposition identification ?
that is,using discourse indicators and other surface fea-tures as described in section 3.1, to what extent dospans of text automatically extracted match up tospans annotated manually described in section 4?There are four different datasets upon which thealgorithms were trained, with each dataset com-prising extracted propositions from: (i) raw datadirectly from Hathi Trust taken only from Chap-ter 1 ; (ii) cleaned data (with these errors manuallycorrected) taken only from Chapter 1; (iii) cleaneddata from Chapters 1 and 2; and (iv) cleaned datafrom Chapters 1, 2 and 4.
All the test data is takenfrom Chapter 1, and in each case the test data wasnot included in the training dataset.It is important to establish a base line using theraw text, but it is expected that performance willbe poor since randomly interspersed formatting ar-tifacts (such as the title of the chapter as a run-ning header occurring in the middle of a sentencethat runs across pages) have a major impact on thesurface profile of text spans used by the machinelearning algorithms.The first result to note is the degree of corre-spondence between the fine-grained propositionalanalysis (which yielded, in total, around 1,000propositions) and the corresponding higher levelanalysis.
As is to be expected, the atomic argu-ment components in the abstract analysis typicallycover more than one proposition in the less ab-stract analysis.
In total, however, 88.5% of thepropositions marked by the more detailed anal-ysis also appear in the more abstract.
That isto say, almost nine-tenths of the material markedas argumentatively relevant in the detailed analy-sis was also marked as argumentatively relevantin the abstract analysis.
This result not onlylends confidence to the claim that the two lev-els are indeed examining the same linguistic phe-nomena, but also establishes a ?gold standard?
forthe machine learning ?
given that manual analysisachieves 88.5% correspondence, and it is this anal-ysis which provides the training data, we wouldnot expect the automatic algorithms to be able toperform at a higher level.Perhaps unsurprisingly, only 11.6% of thepropositions automatically extracted from the raw,uncleaned text exactly match spans identified aspropositions in the manual analysis.
By runningthe processing on cleaned data, this figure is im-proved somewhat to 20.0% using training datafrom Chapter 1 alone.
Running the algorithmstrained on additional data beyond Chapter 1 yieldsperformance of 17.6% (for Chapters 1 and 2) and13.9% (for 1, 2 and 4).
This dropping off is quitesurprising, and points to a lack of homogeneity in83the book as a whole ?
that is, Chapters 1, 2 and4 do not provide a strong predictive model for asmall subset.
This is an important observation, asit suggests the need for careful subsampling fortraining data.
That is, establishing data sets uponwhich machine learning algorithms can be trainedis a highly labour-intensive task.
It is vital, there-fore, to focus that effort where it will have the mosteffect.
The tailing-off effect witnessed on thisdataset suggests that it is more important to sub-sample ?horizontally?
across a volume (or set ofvolumes), taking small extracts from each chapter,rather than subsampling ?vertically,?
taking larger,more in-depth extracts from fewer places acrossthe volume.This first set of results is determined usingstrong matching criteria: that individual proposi-tions must match exactly between automatic andmanual analyses.
In practice, however, artefactsof the text, including formatting and punctuation,may mean that although a proposition has indeedbeen identified automatically in the correct way,it is marked as a failure because it is includingor excluding a punctuation mark, connective wordor other non-propositional material.
To allow forthis, results were also calculated on the basis of atolerance of ?3 words (i.e.
space-delimited char-acter strings).
On this basis, performance with un-formatted text was 17.4% ?
again, rather poor as isto be expected.
With cleaned text, the match ratebetween manually and artificially marked propo-sition boundaries was 32.5% for Chapter 1 textalone.
Again, performance drops over a largertraining dataset (reinforcing the observation aboveregarding the need for horizontal subsampling), to26.5% for Chapters 1 and 2, and 25.0% for Chap-ters 1, 2 and 4.A further liberal step is to assess automaticproposition identification in terms of argument rel-evance ?
i.e.
to review the proportion of automat-ically delimited propositions that are included atall in manual analysis.
This then stands in directcomparison to the 88.5% figure mentioned above,representing the proportion of manually identi-fied propositions at a fine-grained level of analy-sis that are present in amongst the propositions atthe coarse-grained level.
With unformatted text,the figure is still low at 27.3%, but with cleanedup text, results are much better: for just the text ofChapter 1, the proportion of automatically identi-fied propositions which are included in the man-ual, coarse-grained analysis is 63.6%, though thisdrops to 44.4% and 50.0% for training datasetscorresponding to Chapters 1 and 2, and to Chap-ters 1, 2 and 4, respectively.
These figures com-pare favourably with the 88.5% result for humananalysis: that is, automatic analysis is relativelygood at identifying text spans with argumentativeroles.These results are summarised in Table 1, below.For each of the four datasets, the table lists theproportion of automatically analysed propositionsthat are identical to those in the (fine-grained level)manual analysis, the proportion that are withinthree words of the (fine-grained level) manualanalysis, and the proportion that are general sub-strings of the (coarse-grained level) manual analy-sis (i.e.
a measure of argument relevance).Identical ?3Words SubstringUnformated 11.6 17.4 27.3Ch.
1 20.0 32.5 63.6Ch.
1&2 17.6 26.5 44.4Ch.
1,2&4 13.9 25.0 50.0Table 1: Results of automatic proposition process-ing5.2 Structure identificationClearly, identifying the atoms from which argu-ment ?molecules?
are constructed is only part ofthe problem: it is also important to recognise thestructural relations.
Equally clearly, the resultsdescribed in section 5.1 have plenty of room forimprovement in future work.
They are, however,strong enough to support further investigation ofautomatic recognition of structural features (i.e.,specifically, features relating to argument struc-ture).In order to tease out both false positives andfalse negatives, our analysis here separates preci-sion and recall.
Furthermore, all results are givenwith respect to the coarse-grained analysis of sec-tion 4, as no manual structure identification wasperformed on the fine-grained analysis.As described in section 3.2, the automatic struc-ture identification currently returns connectedness,not direction (that is, it indicates two argumentatoms that are related together in an argumentstructure, but do not indicate which is premiseand which conclusion).
The system uses propo-sitional boundaries as input, so can run equally onmanually segmented propositions (those used as84training data in section 5.1) or automatically seg-mented propositions (the results for which weredescribed in Table 1).
In the results which follow,we compare performance between manually an-notated and automatically extracted propositions.Figures 3 and 4 show sample extracts from the au-tomatic structure recognition algorithms runningon manually segmented and automatically seg-mented propositions respectively.For all those pairs of (manually or automati-cally) analysed propositions which the automaticstructure recognition algorithms class as beingconnected, we examine in the manual structuralanalysis connectedness between propositions inwhich the text of the analysed propositions ap-pears.
Thus, for example, if our analysed propo-sitions are the strings xxx and yyy, and the auto-matic structure recognition system classes them asconnected, we first identify the two propositions(P1 and P2) in the manual analysis which includeamongst the text with which they are associatedthe strings xxx and yyy.
Then we check to see if P1and P2 are (immediately) structurally related.
Forautomatically segmented propositions, precision is33.3% and recall 50.0%, whilst for manually seg-mented propositions, precision is 33.3% and re-call 18.2%.
For automatically extracted proposi-tions, the overlap with the coarse-grained analy-sis was small ?
just four propositions ?
so the re-sults should be treated with some caution.
Preci-sion and recall for the manually extracted proposi-tions however is based on a larger dataset (n=26),so the results are disappointing.
One reason is thatwith the manual analysis at a significantly morecoarse-grained level, propositions that were identi-fied as being structurally connected were quite of-ten in the same atomic unit in the manual analysis,thus being rejected as a false positive by the anal-ysis engine.
As a result, we also consider a moreliberal definition of a correctly identified link be-tween propositions, in which success is recordedif either:(a) for any two manually or automatically anal-ysed propositions (p1, p2) that the automatic struc-ture recognition indicates as connected, there is astructural connection between manually analysedpropositions (P1, P2) where p1 is included in P1and p2 included in P2or(b) for any two manually or automatically anal-ysed propositions (p1, p2) that the automatic struc-ture recognition indicates as connected, there is asingle manually analysed propositions (P1) wherep1 and p2 are both included in P1Under this rubric, automatic structure recog-nition with automatically segmented propositionshas precision of 66.6% and recall of 100% (butagain, only on a dataset of n=4), and more signif-icantly, automatic structure recognition with man-ually segmented propositions has precision 72.2%and recall 76.5% These results are summarised inTable 2.AutomaticallysegmentedpropositionsManually seg-mented propo-sitionsIn separatepropositionsn=4, P=33.3%,R=50.0%n=26,P=33.3%,R=18.2%In separateor the samepropositionn=4, P=66.6%,R=100.0%n=26,P=72.2%,R=76.5%Table 2: Results of automatic structure generationThe results are encouraging, but larger scaleanalysis is required to further test the reliability ofthe extant algorithms.6 ConclusionWith fewer than one hundred atomic argumentcomponents analysed at the coarse-grained level,and barely 1,000 propositions at the fine-grainedlevel, the availability of training data is a ma-jor hurdle.
Developing these training sets is de-manding and extremely labour intensive.
Onepossibility is to increasingly make available andreuse datasets between projects.
Infrastructureefforts such as aifdb.org make this morerealistic, with around 15,000 analysed propo-sitions in around 1,200 arguments, though asscale increases, quality management (e.g.
overcrowdsourced contributions) becomes an increas-ing challenge.With sustained scholarly input, however, in con-junction with crossproject import and export, wewould expect these datasets to increase 10 to 100fold over the next year or two, which will sup-port rapid expansion in training and test data setsfor the next generation of argument mining algo-rithms.Despite the lack of training data currently avail-able, we have shown that automatic segmentationof propositions in a text on the basis of relativelysimple features at the surface and syntactic levels85Figure 3: Example of automated structure recognition using manually identified propositionsFigure 4: Example of automated structure recognition using automatically identified propositionsis feasible, though generalisation between chap-ters, volumes and, ultimately, genres, is extremelydemanding.Automatic identification of at least some struc-tural features of argument is surprisingly robust,even at this early stage, though more sophisticatedstructure such as determining the inferential direc-tionality and inferential type is likely to be muchmore challenging.We have also shown that automatic segmenta-tion and automatic structure recognition can beconnected to determine at least an approximationof the argument structure in a piece of text, thoughmuch more data is required to test its applicabilityat scale.6.1 Future WorkSignificantly expanded datasets are crucial to fur-ther development of these techniques.
This willrequire collaboration amongst analysts as well asthe further development of tools for collaboratingon and sharing analyses.Propositional segmentation results could be im-proved by making more thorough use of syntacticinformation such as clausal completeness.
Com-bining a range of techniques to determine proposi-tions would counteract weaknesses that each mayface individually.With a significant foundation for argumentstructure analysis, it is hoped that future work canfocus on extending and refining sets of algorithmsand heuristics based on both statistical and deeplearning mechanisms for exploiting not just topi-cal information, but also the logical, semantic, in-ferential and dialogical structures latent in argu-mentative text.7 AcknowledgementsThe authors would like to thank the Digging IntoData challenge funded by JISC in the UK andNEH in the US under project CIINN01, ?DiggingBy Debating?
which in part supported the researchreported here.86ReferencesF.
Bex, J. Lawrence, M. Snaith, and C.A.
Reed.
2013.Implementing the argument web.
Communicationsof the ACM, 56(10):56?73.P.C.
Cardoso, M. Taboada, and T.A.
Pardo.
2013.On the contribution of discourse structure to topicsegmentation.
In Proceedings of the Special Inter-est Group on Discourse and Dialogue (SIGDIAL),pages 92?96.
Association for Computational Lin-guistics.V.W.
Feng and G. Hirst.
2011.
Classifying argu-ments by scheme.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies-Volume 1,pages 987?996.
Association for Computational Lin-guistics.Maurice A. Finocchiaro.
1980.
Galileo and the art ofreasoning.
rhetorical foundations of logic and scien-tific method.
Boston Studies in the Philosophy ofScience New York, NY, 61.Thomas F Gordon, Henry Prakken, and DouglasWalton.
2007.
The carneades model of argu-ment and burden of proof.
Artificial Intelligence,171(10):875?896.John Lawrence, Floris Bex, Chris Reed, and MarkSnaith.
2012.
Aifdb: Infrastructure for the argu-ment web.
In COMMA, pages 515?516.N.
Madnani, M. Heilman, J. Tetreault, andM.
Chodorow.
2012.
Identifying high-levelorganizational elements in argumentative discourse.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 20?28.
Association for ComputationalLinguistics.M.F.
Moens, E. Boiy, R.M.
Palau, and C. Reed.
2007.Automatic detection of arguments in legal texts.
InProceedings of the 11th international conferenceon Artificial intelligence and law, pages 225?230.ACM.R.M.
Palau and M.F.
Moens.
2009.
Argumentationmining: the detection, classification and structure ofarguments in text.
In Proceedings of the 12th in-ternational conference on artificial intelligence andlaw, pages 98?107.
ACM.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Now Pub.Chris Reed and Derek Long.
1997.
Content orderingin the generation of persuasive discourse.
In IJCAI(2), pages 1022?1029.
Morgan Kaufmann.Chris Reed and Glenn Rowe.
2004.
Araucaria: Soft-ware for argument analysis, diagramming and repre-sentation.
International Journal on Artificial Intelli-gence Tools, 13(04):961?979.S.
Teufel, J. Carletta, and M. Moens.
1999.
An anno-tation scheme for discourse-level argumentation inresearch articles.
In Proceedings of the ninth con-ference on European chapter of the Association forComputational Linguistics, pages 110?117.
Associ-ation for Computational Linguistics.Tim Van Gelder.
2007.
The rationale for rationale.Law, probability and risk, 6(1-4):23?42.D Walton, C Reed, and F Macagno.
2008.
Argumenta-tion Schemes.
Cambridge University Press.D Walton.
2006.
Fundamentals of critical argumenta-tion.
Cambridge University Press.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on human language technology and empiri-cal methods in natural language processing, pages347?354.
Association for Computational Linguis-tics.87
