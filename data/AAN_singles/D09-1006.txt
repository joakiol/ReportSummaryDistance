Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 52?61,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPFeasibility of Human-in-the-loop Minimum Error Rate TrainingOmar F. Zaidan and Chris Callison-BurchDept.
of Computer Science, Johns Hopkins UniversityBaltimore, MD 21218, USA{ozaidan,ccb}@cs.jhu.eduAbstractMinimum error rate training (MERT) in-volves choosing parameter values for amachine translation (MT) system thatmaximize performance on a tuning set asmeasured by an automatic evaluation met-ric, such as BLEU.
The method is bestwhen the system will eventually be eval-uated using the same metric, but in reality,most MT evaluations have a human-basedcomponent.
Although performing MERTwith a human-based metric seems like adaunting task, we describe a new metric,RYPT, which takes human judgments intoaccount, but only requires human input tobuild a database that can be reused overand over again, hence eliminating the needfor human input at tuning time.
In thisinvestigative study, we analyze the diver-sity (or lack thereof) of the candidates pro-duced during MERT, we describe how thisredundancy can be used to our advantage,and show that RYPT is a better predictor oftranslation quality than BLEU.1 IntroductionMany state-of-the-art machine translation (MT)systems over the past few years (Och and Ney,2002; Koehn et al, 2003; Chiang, 2007; Koehnet al, 2007; Li et al, 2009) rely on several mod-els to evaluate the ?goodness?
of a given candidatetranslation in the target language.
The MT systemproceeds by searching for the highest-scoring can-didate translation, as scored by the different modelcomponents, and returns that candidate as the hy-pothesis translation.
Each of these models neednot be a probabilistic model, and instead corre-sponds to a feature that is a function of a (can-didate translation,foreign sentence) pair.Treated as a log-linear model, we need to as-sign a weight for each of the features.
Och (2003)shows that setting those weights should take intoaccount the evaluation metric by which the MTsystem will eventually be judged.
This is achievedby choosing the weights so as to maximize the per-formance of the MT system on a development set,as measured by that evaluation metric.
The otherinsight of Och?s work is that there exists an ef-ficient algorithm to find such weights.
This pro-cess has come to be known as the MERT phase(for Minimum Error Rate Training) in trainingpipelines of MT systems.A problem arises if the performance of the sys-tem is not judged by an automatic evaluation met-ric such as BLEU or TER, but instead throughan evaluation process involving a human.
TheGALE evaluation, for instance, judges the qualityof systems as measured by human-targeted TER(HTER), which computes the edit distance be-tween the system?s output and a version of theoutput post-edited by a human.
The IWSLT andWMT workshops also have a manual evaluationcomponent, as does the NIST Evaluation, in theform of adequacy and fluency (LDC, 2005).In theory, one could imagine trying to optimizea metric like HTER during the MERT phase, butthat would require the availability of an HTER au-tomatic scorer, which, by definition, does not ex-ist.
If done manually, the scoring of thousands ofcandidates produced during MERT would literallytake weeks, and cost a large sum of money.
Forthese reasons, researchers resort to optimizing anautomatic metric (almost always BLEU) as a proxyfor human judgment.As daunting as such a task seems for anyhuman-based metric, we describe a new metric,RYPT, that takes human judgment into accoutwhen scoring candidates, but takes advantage ofthe redundancy in the candidates produced dur-ing MERT.
In this investigative study, we describehow this redundancy can be used to our advantageto eliminate the need to involve a human at any52time except when building a database of reusablejudgments, and furthermore show that RYPT is abetter predictor of translation quality than BLEU,making it an excellent candidate for MERT tun-ing.The paper is organized as follows.
We start bydescribing the core idea of MERT before intro-ducing our new metric, RYPT, and describing thedata collection effort we undertook to collect theneeded human judgments.
We analyze a MERTrun optimizing BLEU to quantify the level of re-dundancy in the candidate set, and also providean extensive analysis of the collected judgments,before describing a set of experiments showingRYPT is a better predictor of translation qualitythan BLEU.
Following a discussion of our findings,we briefly review related work, before pointing outfuture directions and summarizing.2 Och?s Line Search MethodA common approach to translating a source sen-tence f in a foreign language is to select the can-didate translation e that maximizes the posteriorprobability:Pr(e | f)def=exp(s?
(e, f))?e?exp(s?
(e?, f)).This defines Pr(e | f) using a log-linear modelthat associates a sentence pair (e, f) with a fea-ture vector ?
(e, f) = {?1(e, f), ..., ?M(e, f)},and assigns a scores?
(e, f)def= ?
?
?
(e, f) =M?m=1?m?m(e, f)for that sentence pair, with the feature weights?
= {?1, ..., ?M} being the parameters of themodel.
Therefore, the system selects the transla-tion e?:e?
= argmaxePr(e | f) = argmaxes?
(e, f).
(1)Och (2003) provides evidence that ?
should bechosen by optimizing an objective function basdon the evaluation metric of interest, rather thanlikelihood.
Since the error surface is not smooth,and a grid search is too expensive, Och suggests analternative, efficient, line optimization approach.Assume we are performing a line optimiza-tion along the dthdimension.
Consider a for-eign sentence f , and let the candidate set for fbe {e1, ..., eK}.
Recall from (1) that the 1-bestcandidate at a given ?
is the one with maxi-mum?Mm=1?m?m(ek, f).
We can rewrite thesum as ?d?d(ek, f) +?m 6=d?m?m(ek, f).
Thesecond term is constant with respect to ?d, andso is ?d(ek, f).
Renaming those two quantitiesoffest?
(ek) and slope(ek), we gets?
(ek, f) = slope(ek)?d+ offset?
(ek).Therefore, if we plot the score for a candidatetranslation vs. ?d, that candidate will be repre-sented by a line.
If we plot the lines for all candi-dates (Figure 1), then the upper envelope of theselines indicates the best candidate at any value for?d.Therefore, the objective function is piece-wiselinear across any of the M dimensions1, mean-ing we only need to evaluate it at the ?critical?points corresponding to line intersection points.Furthermore, we only need to calculate the suffi-cient statistics once, at the smallest critical point,and then simply adjust the sufficient statistics toreflect changes in the set of 1-best candidates.2.1 The BLEU MetricThe metric most often used with MERT is BLEU(Papineni et al, 2002), where the score of a candi-date c against a reference translation r is:BLEU = BP (len(c), len(r))?exp(4?n=114log pn),where pnis the n-gram precision2and BP is abrevity penalty meant to penalize short outputs, todiscourage improving precision at the expense ofrecall.There are several compelling reasons to opti-mize to BLEU.
It is the most widely reported met-ric in MT research, and has been shown to cor-relate well with human judgment (Papineni et al,2002; Coughlin, 2003).
But BLEU is also partic-ularly suitable for MERT, because it can be com-puted quite efficiently, and its sufficient statisticsare decomposable, as required by MERT.3,41Or, in fact, along any linear combination of the M di-mensions.2Modifed precision, to be precise, based on clipped n-gram counts.3Note that for the sufficient statistics to be decomposable,the metric itself need not be ?
this is in fact the case withBLEU.4Strictly speaking, the sufficient statistics need not be de-53e1 e21 2e1 e21 4e1 e22 4e1 e23 1e1 2e2 1e1 3e1 1 e1 4 e2 2e2 3e2 41:[6,10]2:[1,10]3:[3,10]4:[4,10]e1 e1 e1 e11:[3,15]2:[8,15]3:[9,15]4:[2,15]e2 e2 e2 e2e1 e23 3score(e,f1) TERscore(e,f2)0.48[12,25]0.24[6,25]0.56[14,25] 0.32[8,25]0.12[3,25]?
d ?
d?
d?
??
?+TERsuff.stats forcandidates.TheSSformean6editsareneededtomatcha10-wordreference.e1 1Och?smethodappliedtoasetoftwoforeignsentences.
Candidatescorrespondtolines,andenvelopesof top-most linescorrespondtoargmaxinEq.
1.
Thesetof1-bestcandidatesandtheerrormetric(TER)changeonlyatfourcritical?
dvalues.Numbers()insquarebracketsaretheoverallsufficient statistics(SS)forTER,andarethesumof SSfor individual1-bestcandidates().Thissumisonlydoneoncetoobtain[14,25],andthensimplyadjustedappropriatelytoreflectchange(s) in1-bestcandidates.Figure 1: Och?s method applied to a set of two foreign sentences.
This figure is essentially a visualizationof equation (1).
We show here sufficient statistics for TER for simplicity, since there are only 2 of them,but the metric optimized in MERT is usually BLEU.In spite of these advantages, recent work haspointed out a number of problematic aspects ofBLEU that should cause one to pause and recon-sider the reliance on it.
Chiang et al (2008) in-vestigate several weaknesses in BLEU and showthere are realistic scenraios where the BLEU scoreshould not be trusted, and in fact behaves in acounter-intuitive manner.
Furthermore, Callison-Burch et al (2006) point out that it is not alwaysappropriate to use BLEU to compare systems toeach other.
In particular, the quality of rule-basedsystems is usually underestimated by BLEU.All this raises doubts regarding BLEU?s ade-quacy as a proxy for human judgment, which isa particularly important issue in the context of set-ting parameters during the MERT phase.
But whatis the alternative?2.2 (Non-)Applicability of Och?s Method toHuman MetricsIn principle, MERT is applicable to any evalua-tion metric, including HTER, as long as its suffi-cient statistics are decomposable.4In practice, ofcourse, the method requires the evaluation of thou-sands of candidate translations.
Whereas this iscomposable in MERT, as they can be recalculated at each crit-ical point.
However, this would slow down the optimizationprocess quite a bit, since one cannot traverse the dimensionby simply adjusting the sufficient statistics to reflect changesin 1-best candidates.not a problem with a metric like BLEU, for whichautomatic (and fast) scorers are available, such anevaluation with a human metric would require alarge amount of effort and money, meaning thata single MERT run would take weeks to com-plete, and would cost thousands of dollars.
As-sume a single candidate string takes 10 secondsto post-edit, at a cost of $0.10.
Even with suchan (overly) optimistic estimate, scoring 100 candi-dates for each of 1000 sentences would take 35 8-hour work days and cost $10,000.
The cost wouldfurther grow linearly with the number of MERT it-erations and the n-best list size.
On the other hand,optimizing for BLEU takes on the order of minutesper iteration, and costs nothing.2.3 The RYPT MetricWe suggest here a new metric that combines thebest of both worlds, in that it is based on humanjudgment, but that is a viable metric to be used inthe MERT phase.
The key to the feasiblity is thereliance on a database of human judgment ratherthan immendiate feedback for each candidate, andso human feedback is only needed once, and thecollected human judgments can be reused over andover again by an automatic scorer.The basic idea is to reward syntactic con-stituents in the source sentence that get algnedto ?acceptable?
translations in the candidate sen-54S{0-11}S{0-10} X{11-11}S{0-7} X{8-10}X{8-8}X{10-10}X{0-7}X{4-6}X{0-2}X{1-1} X{4-4}official forecasts    are  based on only 3 per cent reported   ,   bloomberg    .offizielle prognosen sind von nur 3 prozent ausgegangen  ,  meldete bloomberg .YNYYY Y YNNY Y YNYROOTY NYYLabel Y indicatesforecasts deemedacceptable translationof prognosen.Range 0-2 indicatescoverage of the first3 words in thesource sentence.Figure 2: The source parse tree (top) and the can-didate derivation tree (bottom).
Nodes in the parsetree with a thick border correspond to the frontiernode set with maxLen = 4.
The human annota-tor only sees the portion surrounded by the dashedrectangle, including the highlighting (though ex-cluding the word alignment links).tence, and penalize constituents that do not.
Forinstance, consider the source-candidate sentencepair of Figure 2.
To evaluate the candidate transla-tion, the source parse tree is first obtained (Dubey,2005), and each subtree is matched with a sub-string in the candidate string.
If the source sub-string covered by this subtree is translated into anacceptable substring in the candidate, that nodegets a YES label.
Otherwise, the node gets a NOlabel.The metric we propose is taken to be the ratio ofYES nodes in the parse tree (or RYPT).
The candi-date in Figure 2, for instance, would get a RYPTscore of 13/18 = 0.72.To justify its use as a proxy for HTER-like met-rics, we need to demonstrate that this metric corre-lates well with human judgment.
But it is also im-portant to show that we can obtain the YES/NO la-bel assignments in an efficient and affordable man-ner.
At first glance, this seems to require a humanto provide judgments for each candidate, muchlike with HTER.
But we describe in the next sec-tion strategies that minimize the number of judg-ments we need to actually collect.3 Collecting Human JudgmentsThe first assumption we make to minimize thenumber of human judgments, is that once wehave a judgment for a source-candidate substringpair, that same judgment can be used across allcandidates for this source sentence.
In otherwords, we build a database for each source sen-tence, which consists of <source substring,targetsubstring,judgment> entries.
For a given sourcesubstring, multiple entries exist, each with a dif-ferent target candidate substring.
The judgmentfield is one of YES, NO, and NOT SURE.Note that the entries do not store the full can-didate string, since we reuse a judgment acrossall the candidates of that source sentence.
For in-stance, if we collect the judgment:<der patient,the patient,YES>from the sentence pair:der patient wurde isoliert .the patient was isolated .then this would apply to any candidate translationof this source sentence.
And so all of the followingsubstrings are labeled YES as well:the patient isolated .the patient was in isolation .the patient has been isolated .Similarly, if we collect the judgment:<der patient,of the patient,NO>from the sentence pair:der patient wurde isoliert .of the patient was isolated .then this would apply to any candidate translationof the source, and the following substrings are la-beled NO as well:of the patient isolated .of the patient was in isolation .of the patient has been isolated .The strategy of using judgments across candi-dates reduces the amount of labels we need to col-lect, but evaluating a candidate translation for thesource sentence of Figure 2 would still require ob-taining 18 labels, one for each node in the parsetree.
Instead of querying a human for each one55of those nodes, it is quite reasonable to percolateexisting labels up and down the parse tree: if anode is labeled NO, this likely means that all itsancestors would also be labeled NO, and if a nodeis labeled YES, this likely means that all its de-scendents whould also be labeled YES.While those two strategies (using judgmentsacross candidates, and percolating labels up anddown the tree) are only approximations for the truelabels, employing them considerably reduces theamount of data we need to collect.3.1 Obtaining Source-to-CandidateAlignmentsHow do we determine which segment of the can-didate sentence aligns to a given source segment?Given a word alignment between the source andthe candidate, we take the target substring to con-tain any word aligned with at least one word inthe source segment.
One could run an aligner (e.g.GIZA++) on the two sentences to obtain the wordalignment, but we take a different approach.We use Joshua (Li et al, 2009), in our experi-ments.
Joshua is a hierarchical parsing-based MTsystem, and it can be instructed to produce deriva-tion trees instead of the candidate sentence stringitself.
Furthermore, each node in the derivationtree is associated with the two indices in the sourcesentence that indicate the segment correspondingto this derivation subtree (the numbers indicatedin curly brackets in Figure 2).Using this information, we are able to recovermost of the phrasal alignments.
There are otherphrasal alignments that can be deduced fromthe structure of the tree indirectly, by system-atically discarding source words that are partof another phrasal alignment.
For instance,in Figure 2, one can observe the alignment(offizielle,prognosen,sind)?
(official,forecasts,are)and the alignment (prognosen)?
(forecasts) todeduce (offizielle,sind)?
(official,are).Although some of the phrasal alignment areone-to-one mappings, many of them are many-to-many.
By construction, any deduced many-to-many mapping has occurred in the training paral-lel corpus at least once.
And so we recover theindividual word alignments by consulting the par-allel corpus from which the grammar rules wereextracted (which requires maintaining the wordalignments obtained prior to rule extraction).55We incorporated our implementation of the source-We emphasize here that our recovery of wordalignment from phrasal alignment is independentfrom the hierarchical and parsing-based nature ofthe Joshua system.
And so the alignment approachwe suggest here can be applied to a different MTsystem as well, as long as that system providesphrasal alignment along with the output.
In partic-ular, a phrase-based system such as Moses can bemodified in a straightforward manner to providephrasal alignments, and then apply our method.4 Data CollectionWe chose the WMT08 German-English newsdataset to work with, and since this is an investiga-tive study of a novel approach, we collected judg-ments for a subset of 250 source sentences fromthe development set for the set of candidate sen-tences produced in the last iteration of a MERTrun optimizing BLEU on the full 2051-sentence de-velopment set.
The MT system we used is Joshua(Li et al, 2009), a software package that comescomplete with a grammar extraction module and aMERT module, in addition to the decoder itself.What segments of the source should be chosento be judged?
We already indicated that we limitourselves, by definition of RYPT, to segments thatare covered exactly by a subtree in the source parsetree.
This has a couple of nice advantages: it al-lows us to present an annotator with a high num-ber of alternatives judged simulataneously (sincethe annotator is shown a source segment and sev-eral candidates, not just one), and this probablyalso makes judging them easier ?
it is reasonableto assume that strings corresponding to syntacticconstituents are easier to process by a human.Our query selection strategy attempts to max-imize the amount of YES/NO percolation thatwould take place.
We therefore ensure that for any2 queries, the corresponding source segments donot overlap: such overlap indicates that one sub-tree is completely contained within the other.
Hav-ing both queries (in the same batch) might be re-dundant if we use the above percolation procedure.The idea is to select source segments so thatthey fully cover the entire source sentence, buthave no overlap amongst them.
In one extreme,each query would correspond to an entire parsetree.
This is not ideal since the overwhelming ma-jority of the judgments will most likely be NO,candidate aligner into the Joshua software as a newaligner package.56which does not help identify where the problemis.
In the other extreme, each query would corre-spond to a subtree rooted at a preterminal.
This isalso not ideal, since it would place too much em-phasis on translations of unigrams.So we need a middle ground.
We select amaximum-source-length maxLen to indicate howlong we?re willing to let source segments be.
Thenwe start at the root of the parse tree, and prop-agate a ?frontier?
node set down the parse tree,to end up with a set of nodes that fully cover thesource sentence, have no overlap amongst them,and with each covering no more than maxLensource words.
For instance, with maxLen set to4, the frontier set of Figure 2 are the nodes witha thick border.
An algorithmic description is pro-vided in Algorithm 1.Algorithm 1 Constructing the frontier node set fora parse tree.Input: A source parse tree T rooted at ROOT, anda maximum source length maxLen.Return: A nonempty set frontierSet, con-taining a subset of the nodes in T .1.
Initialize frontierSet to the empty set.2.
Initialize currNodes to {ROOT}.3. while currNodes is not empty do4.
Initialize newNodes to the empty set.5.
for each node N in currNodes do6.
if N covers ?
maxLen source wordsthen7.
Add N to frontierSet.8.
else9.
Add children of N to newNodes.10.
end if11.
end for12.
Set currNodes = newNodes13.
end while14.
Return frontierSet.This would ensure that our queries cover be-tween 1 and maxLen source words, and ensuresthey do not overlap, which would allow us to takefull advantage of the downward-YES and upward-NO percolation.
We set maxLen = 4 based on apilot study of 10 source sentences and their candi-dates, having observed that longer segments tendto always be labeled as NO, and shorter segmentstend to be so deep down the parse tree.4.1 Amazon Mechanical TurkWe use the infrastructure of Amazon?s Mechan-ical Turk (AMT)6to collect the labels.
AMT isa virtual marketplace that allows ?requesters?
tocreate and post tasks to be completed by ?work-ers?
around the world.
To create the tasks (calledHuman Intelligence Tasks, or HITs), a requestersupplies an HTML template along with a comma-separated-values database, and AMT automati-cally creates the HITs and makes them available toworkers.
The queries are displayed as an HTMLpage (based on the provided HTML template),with the user indicating the label (YES, NO, or NOTSURE) by selecting the appropriate radio button.The instructions read, in part:7You are shown a ?source?
Germansentence with a highlighted segment,followed by several candidate trans-lations with corresponding highlightedsegments.
Your task is to decide if eachhighlighted English segment is an ac-ceptable translation of the highlightedGerman segment.In each HIT, the worker is shown up to 10 al-ternative translations of a highlighted source seg-ment, with each itself highlighted within a fullcandidate string in which it appears.
To aid theworker in the task, they are also shown the ref-erence translation, with a highlighted portion thatcorresponds to the source segment, deduced usingword alignments obtained with GIZA++.84.2 Cost of Data CollectionThe total number of HITs created was 3873,with the reward for completing a HIT depend-ing on how many alternative translations are beingjudged.
On average, each HIT cost 2.1 cents andinvolved judging 3.39 alternatives.
115 distinctworkers put in a total of 30.82 hours over a pe-riod of about 4 days.
On average, a label required8.4 seconds to determine (i.e.
at a rate of 426 la-bels per hour).
The total cost was $81.44: $21.43for Amazon?s commission, $53.47 for wages, and6AMT?s website: http://www.mturk.com.7Template and full instructions can be viewed at http://cs.jhu.edu/?ozaidan/hmert.8These alignments are not always precise, and we do notethat fact in the instructions.
We also deliberately highlight thereference substring in a different color to make it clear thatworkers should judge a candidate substring primarily basedon the source substring, not the reference substring.57$6.54 for bonuses9, for a cost per label of 0.62cents (i.e.
at a rate of 161.32 labels per dol-lar).
Excluding Amazon?s commission, the effec-tive hourly ?wage?
was $1.95.5 Experimental Results and AnalysisBy limiting our queries to source segments corre-sponding to frontier nodes with maxLen = 4, weobtain a total of 3601 subtrees across the 250 sen-tences, for an average of 14.4 per sentence.
Onaverage, each subtree has 3.65 alternative trans-lations.
Only about 4.8% of the judgments werereturned as NOT SURE (or, occasionally, blank),with the rest split into 35.1% YES judgments and60.1% NO judgments.The coverage we get before percolating labelsup and down the trees is 39.4% of the nodes, in-creasing to a coverage of 72.9% after percolation.This is quite good, considering we only do a sin-gle data collection pass, and considering that about10% of the subtrees do not align to candidate sub-strings to begin with (e.g.
single source words thatlack a word alignment into the candidate string).The main question, of course, is whether or notthose labels allow us to calculate a RYPT scorethat is reliably correlated with human judgment.We designed an experiment to compare the predic-tive power of RYPT vs. BLEU.
Given the candidateset of a source sentence, we rerank the candidateset according to RYPT and extract the top-1 can-didate, and we rerank the candidate set accordingto BLEU, and extract the top-1 candidate.
We thenpresent the two candidates to human judges, andask them to choose the one that is a more adequatetranslation.
For reliability, we collect 3 judgmentsper sentence pair comparison, instead of just 1.The results show that RYPT significantly outper-forms BLEU when it comes to predicting humanpreference, with its choice prevailing in 46.1%of judgments vs. 36.0% for BLEU, with 17.9%judged to be of equal quality (left half of Ta-ble 1).
This advantage is especially true when thejudgments are grouped by sentence, and we ex-amine cases of strong agreement among the threeannotators (Table 2): whereas BLEU?s candidateis strongly preferred in 32 of the candidate pairs(bottom 2 rows), RYPT?s candidate is strongly pre-ferred in about double that number: 60 candidate9We would review the collected labels and give a 20%reward for good workers to encourage them to come backand complete more HITs.pairs (top 2 rows).This is quite a remarkable result, given thatBLEU, by definition, selects a candidate that hassignificant overlap with the reference shown to theannotators to aid in their decision-making.
Thismeans that BLEU has an inherent advantage incomparisons where both candidates are more orless of equal quality, since annotators are encour-aged (in the instructions) to make a choice even ifthe two candidates seem of be of equal quality atfirst glance.
Pressed to make such a choice, theannotator is likely to select the candidate that su-perficially ?looks?
more like the reference to be the?better?
of the two candidates.
That candidate willmost likely be the BLEU-selected one.To test this hypothesis, we repeated the experi-ment without showing the annotators the referencetranslations, and limited data collection to work-ers living in Germany, making judgments basedonly on the source sentences.
(We only collectedone judgment per source sentence, since Germanworkers on AMT are in short supply.
)As expected, the difference is even more pro-nounced: human judges prefer the RYPT-selectedcandidate 45.2% of the time, while BLEU?s can-didate is preferred only 29.2% of the time, with25.6% judged to be of equal quality (right halfof Table 1).
Our hypothesis is further supportedby the fact that most of the gain of the ?equal-quality?
category comes from BLEU, which loses6.8 percentage points, whereas RYPT?s share re-mains largely intact, losing less than a single per-centage point.5.1 Analysis of Data CollectionRecall that we minimize data collection by per-forming label percolation and by employing afrontier node set selection strategy.
While the re-sults just presented indicate those strategies pro-vide a good approximation of some ?true?
RYPTscore, label percolation was a strategy based pri-marily on intuition, and choosing maxLen = 4for frontier set construction was based on examin-ing a limited amount of preliminary data.Therefore, and in addition to encouraging em-pricial results, we felt a more rigorous quantitativeanalysis was in order, especially with future, moreambitious annotation projects on the horizon.
Tothis end, we collected a complete set of judgmentsfor 50 source sentences and their candidates.
Thatis, we generated a query for each and every node58References shown; References not shown;unrestricted restricted to DE workersPreferred candidate # judgments % judgments # judgments % judgmentsTop-1 by RYPT 346 46.1 113 45.2Top-1 by BLEU 270 36.0 73 29.2Neither 134 17.9 64 25.6Total 750 100.0 250 100.0Table 1: Ranking comparison results.
The left half corresponds to the experiment (open to all workers)where the English reference was shown, whereas the right half corresponds to the experiment (open onlyto workers living in Germany) where the English reference was not shown.Aggregate # sentences % sentences Aggregate # sentences % sentencesRYPT +3 45 18.0RYPT +2 15 6.0 RYPT +any 120 48.0RYPT +1 60 24.0?
0 42 16.8 ?
0 42 16.8BLEU +1 55 22.0BLEU +2 5 2.0 BLEU +any 88 35.2BLEU +3 28 11.2Total 250 100.0 Total 250 100.0Table 2: Ranking comparison results, grouped by sentence.
This table corresponds to the left half ofTable 1.
3 judgments were collected for each comparison, with the ?aggregate?
for a comparison calcu-lated from these 3 judgments.
For instance, an aggregate of ?RYPT +3?
means all 3 judgments favoredRYPT?s choice, and ?RYPT +1?
means one more judgment favored RYPT than did BLEU.in the source parse tree, instead of limiting our-selves to a frontier node set.
(Though we did limitthe length of a source segment to be ?
7 words.
)This would allow us to judge the validity of labelpercolation, and under different maxLen values.Furthermore, we collected multiple judgmentsfor each query in order to minimize the effet ofbad/random annotations.
For each of 5580 gen-erated queries, we collected five judgments, for atotal of 27,900 judgments.10As before, the anno-tator would pick one of YES, NO, and NOT SURE.First, collecting multiple judgments allowed usto investigate inter-annotator agreement.
In 68.9%of the queries, at least 4 of the 5 annotators chosethe same label, signifying a high degree of inter-annotator agreement.
This is especially encourag-ing considering that we identified about 15% ofthe HITs as being of poor quality, and blocked therespective annotators from doing further HITs.11We then examine the applicability and validity10For a given query, the five collected judgments are fromfive different annotators, since AMT ensures an annotator isnever shown the same HIT twice.11It is especially easy to identify (and then block) such an-notators when they submit a relatively large number of HITs,since inspecting some of their annotations would indicatethey are answering randomly and/or inconsistently.of label percolation.
For each of 7 different valuesfor Algorithm 1?s maxLen, we ignore all but la-bels that would be requested under that maxLenvalue, and percolate the labels up and down thetree.
In Figure 3 we plot the coverage before andafter percolation (middle two curves), and observeexpansion in coverage across different values ofmaxLen, peaking at about +33% for maxLen= 4and 5, with most of the benefit coming from YESpercolation (bottom two curves).0%10%20%30%40%50%60%70%80%90%0 1 2 3 4 5 6 7maxLenCoverage before percolationCoverage after percolationPercolation accuracy?
due to perc.
of YES?
due to perc.
of NOFigure 3: Label percolation under differentmaxLen values.
The bottom two curves are thebreakdown of the difference between the middletwo.
Accuracy is measured against majority votes.59We also measure the accuracy of labels deducedfrom percolation (top curve of Figure 3).
We de-fine a percolated label to be correct if it matchesthe label given by a majority vote over the col-lected labels for that particular node.
We find thataccuracy at low maxLen values is significantlylower than at higer values (e.g.
72.6% vs. 84.1%for 1 vs. 4).
This means a middle value such as 3or 4 is optimal.
Higher values could be suitable ifwe wish to emphasize translation fluency.6 Related WorkNie?en et al (2000) is an early work that also con-structs a database of translations and judgments.There, a source sentence is stored along with allthe translations that have already been manuallyjudged, along with their scores.
They utilize thisdatabase to carry out ?semi-automatic?
evaluationin a fast and convenient fashion thanks to tool theydeveloped with a user-friendly GUI.In their annual evaluation, the WMT work-shop has effectively conducted manual evaluationof submitted systems over the past few years bydistributing the work across tens of volunteers,though they relied on a self-designed online por-tal.
On the other hand, Snow et al (2008) illus-trate how AMT can be used to collect data in a?fast and cheap?
fashion, for a number of NLPtasks, such as word sense disambiguation.
Theygo a step further and model the behavior of theirannotators to reduce annotator bias.
This was pos-sible as they collect multiple judgments for eachquery from multiple annotators.The question of how to design an automaticmetric that best approximates human judgmenthas received a lot of attention lately.
NIST startedorganizing the Metrics for Machine TranslationChallenge (MetricsMATR) in 2008, with the aimof developing automatic evaluation metrics thatcorrelate highly with human judgment of transla-tion quality.
The latest WMT workshop (Callison-Burch et al, 2009) also conducted a full assess-ment of how well a suite of automatic metrics cor-relate with human judgment.7 Future WorkThis pilot study has demonstrated the feasibilityof collecting a large number of human judgments,and has shown that the RYPT metric is better thanBLEU at picking out the best translation.
Thenext step is to run a complete MERT run.
Thiswill involve collecting data for thousands of al-ternative translations for several hundreds sourcesentences.
Based on our analysis, this it shouldbe cost-effective to solicit these judgments usingAMT.
After training MERT using RYPT as an ob-jective function the, the next logical step would beto compare two outputs of a system.
One outputwould have parameters optimized to BLEU and theother to RYPT.
The hope is that the RYPT-trainedsystem would be better under the final HTER eval-uation than the BLEU-trained system.We are also investigating a probabilistic ap-proach to percolating the labels up and down thetree, whereby the label of a node is treated as arandom variable, and inference is performed basedon values of the other observed nodes, as well asproperties of the source/candidate segment.
Castthis way, a probabilistic approach is actually quiteappealing, and one could use collected data totrain a prediction model (such as a Markov ran-dom field).8 SummaryWe propose a human-based metric, RYPT, that isquite feasible to optimize using MERT, relying onthe redundancy in the candidate set, and collect-ing judgments using Amazon?s Mechanical Turkinfrastructure.
We show this could be done in aquite cost-effective manner, and produces data ofgood quality.
We show the effectiveness of themetric by illustrating that it is a better predictor ofhuman judgment of translation quality than BLEU,the most commonly used metric in MT.
We showthis is the case even with a modest amount of datathat does not cover the entirety of all parse trees,on which the metric is dependent.
The collecteddata represents a database that can be reused overand over again, hence limiting human feedback tothe initial phase only.AcknowledgmentsThis research was supported by the EuroMatrix-Plus project funded by the European Commission(7th Framework Programme), by the Defense Ad-vanced Research Projects Agency?s GALE pro-gram under Contract No.
HR0011-06-2-0001, andthe US National Science Foundation under grantIIS-0713448.
The views and findings are the au-thors?
alone.60ReferencesChris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of Bleu in ma-chine translation research.
In Proceedings of EACL,pages 249?256.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.David Chiang, Steve DeNeefe, Yee Seng Chan, andHwee Tou Ng.
2008.
Decomposability of trans-lation metrics for improved evaluation and efficientalgorithms.
In Proceedings of EMNLP, pages 610?619.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Deborah Coughlin.
2003.
Correlating automated andhuman assessments of machine translation quality.In Proceedings of MT Summit IX.Amit Dubey.
2005.
What to do when lexicaliza-tion fails: parsing German with suffix analysis andsmoothing.
In Proceedings of ACL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL, pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of ACL, Demo and Poster Ses-sions, pages 177?180.LDC.
2005.
Linguistic data annotation specification:Assessment of fluency and adequacy in translations.Revision 1.5.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenThornton, Jonathan Weese, and Omar F. Zaidan.2009.
Joshua: An open source toolkit for parsing-based machine translation.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, pages 135?139.Sonja Nie?en, Franz Josef Och, Gregor Leusch, andHermann Ney.
2000.
An evaluation tool for ma-chine translation: Fast evaluation for mt research.In Proceedings of LREC.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of ACL,pages 295?302.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167.Kishore Papineni, Salim Poukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast ?
but is itgood?
evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of EMNLP, pages254?263.61
