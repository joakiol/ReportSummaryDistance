Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731?736,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLow-Rank Tensors for Verbs in Compositional Distributional SemanticsDaniel Fried, Tamara Polajnar, and Stephen ClarkUniversity of CambridgeComputer Laboratory{df345,tp366,sc609}@cam.ac.ukAbstractSeveral compositional distributional se-mantic methods use tensors to modelmulti-way interactions between vectors.Unfortunately, the size of the tensors canmake their use impractical in large-scaleimplementations.
In this paper, we inves-tigate whether we can match the perfor-mance of full tensors with low-rank ap-proximations that use a fraction of theoriginal number of parameters.
We in-vestigate the effect of low-rank tensors onthe transitive verb construction where theverb is a third-order tensor.
The resultsshow that, while the low-rank tensors re-quire about two orders of magnitude fewerparameters per verb, they achieve perfor-mance comparable to, and occasionallysurpassing, the unconstrained-rank tensorson sentence similarity and verb disam-biguation tasks.1 IntroductionDistributional semantic methods represent wordmeanings by their contextual distributions, for ex-ample by computing word-context co-ocurrencestatistics (Sch?utze, 1998; Turney and Pantel, 2010)or by learning vector representations for wordsas part of a context prediction model (Bengio etal., 2003; Collobert et al., 2011; Mikolov et al.,2013).
Recent research has also focused on com-positional distributional semantics (CDS): com-bining the distributional representations for words,often in a syntax-driven fashion, to produce distri-butional representations of phrases and sentences(Mitchell and Lapata, 2008; Baroni and Zam-parelli, 2010; Socher et al., 2012; Zanzotto andDell?Arciprete, 2012).One method for CDS is the Categorial frame-work (Coecke et al., 2011; Baroni et al., 2014),where each word is represented by a tensor whoseorder is determined by the Categorial Grammartype of the word.
For example, nouns are anatomic type represented by a vector, and adjec-tives are matrices that act as functions transform-ing a noun vector into another noun vector (Baroniand Zamparelli, 2010).
A transitive verb is a third-order tensor that takes the noun vectors represent-ing the subject and object and returns a vector inthe sentence space (Polajnar et al., 2014).However, a concrete implementation of the Cat-egorial framework requires setting and storing thevalues, or parameters, defining these matrices andtensors.
These parameters can be quite numerousfor even low-dimensional sentence spaces.
For ex-ample, a third-order tensor for a given transitiveverb, mapping two 100-dimensional noun spacesto a 100-dimensional sentence space, would have1003parameters in its full form.
All of themore complex types have corresponding tensors ofhigher order, and therefore a barrier to the practi-cal implementation of this framework is the largenumber of parameters required to represent an ex-tended vocabulary and a variety of grammaticalconstructions.We aim to reduce the size of the models bydemonstrating that reduced-rank tensors, whichcan be represented in a form requiring fewer pa-rameters, can capture the semantics of complextypes as well as the full-rank tensors do.
We baseour experiments on the transitive verb constructionfor which there are established tasks and datasets(Grefenstette and Sadrzadeh, 2011; Kartsaklis andSadrzadeh, 2014).Previous work on the transitive verb construc-tion within the Categorial framework includes atwo-step linear-regression method for the con-struction of the full verb tensors (Grefenstette etal., 2013) and a multi-linear regression methodcombined with a two-dimensional plausibilityspace (Polajnar et al., 2014).
Polajnar et al.
(2014)731also introduce several alternative ways of reducingthe number of tensor parameters by using matri-ces.
The best performing method uses two matri-ces, one representing the subject-verb interactionsand the other the verb-object interactions.
Someinteraction between the subject and the object isre-introduced through a softmax layer.
A similarmethod is presented in Paperno et al.
(2014).
Mi-lajevs et al.
(2014) use vectors generated by a neu-ral language model to construct verb matrices andseveral different composition operators to generatethe composed subject-verb-object sentence repre-sentation.In this paper, we use tensor rank decomposi-tion (Kolda and Bader, 2009) to represent eachverb?s tensor as a sum of tensor products of vec-tors.
We learn the component vectors and applythe composition without ever constructing the fulltensors and thus we are able to improve on bothmemory usage and efficiency.
This approach fol-lows recent work on using low-rank tensors to pa-rameterize models for dependency parsing (Lei etal., 2014) and semantic role labelling (Lei et al.,2015).
Our work applies the same tensor rankdecompositions, and similar optimization algo-rithms, to the task of constructing a syntax-drivenmodel for CDS.
Although we focus on the Cat-egorial framework, the low-rank decompositionmethods are also applicable to other tensor-basedsemantic models including Van de Cruys (2010),Smolensky and Legendre (2006), and Blacoe et al.
(2013).2 ModelTensor Models for Verbs We model each tran-sitive verb as a bilinear function mapping subjectand object noun vectors, each of dimensionalityN , to a single sentence vector of dimensionality S(Coecke et al., 2011; Maillard et al., 2014) repre-senting the composed subject-verb-object (SVO)triple.
Each transitive verb has its own third-order tensor, which defines this bilinear function.Consider a verb V with associated tensor V ?RS?N?N, and vectors s ?
RN, o ?
RNforsubject and object nouns, respectively.
Then thecompositional representation for the subject, verb,and object is a vector V (s,o) ?
RS, produced byapplying tensor contraction (the higher-order ana-logue of matrix multiplication) to the verb tensorand two noun vectors.
The lthcomponent of thevector for the SVO triple is given byV (s,o)l=?j,kVljkoksj(1)We aim to learn distributional vectors s and ofor subjects and objects, and tensors V for verbs,such that the output vectors V (s,o) are distri-butional representations of the entire SVO triple.While there are several possible definitions ofthe sentence space (Clark, 2013; Baroni et al.,2014), we follow previous work (Grefenstette etal., 2013) by using a contextual sentence spaceconsisting of content words that occur within thesame sentences as the SVO triple.Low-Rank Tensor Representations FollowingLei et al.
(2014), we represent each verb?s tensorusing a low-rank canonical polyadic (CP) decom-position to reduce the numbers of parameters thatmust be learned during training.
As a higher-orderanalogue to singular value decomposition for ma-trices, CP decomposition factors a tensor into asum of R tensor products of vectors.1Given athird-order tensor V ?
RS?N?N, the CP decom-position of V is:V =R?r=1Pr?Qr?Rr(2)where P ?
RR?S,Q ?
RR?N,R ?
RR?Nareparameter matrices, Prgives the rth row of matrixP, and ?
is the tensor product.The smallest R that allows the tensor to be ex-pressed as this sum of outer products is the rankof the tensor (Kolda and Bader, 2009).
By fixing avalue for R that is sufficiently small compared toS and N (forcing the verb tensor to have rank ofat mostR), and directly learning the parameters ofthe low-rank approximation using gradient-basedoptimization, we learn a low-rank tensor requiringfewer parameters without ever having to store thefull tensor.In addition to reducing the number of parame-ters, representing tensors in this form allows us toformulate the verb tensor?s action on noun vectorsas matrix multiplication.
For a tensor in the formof Eq.
(2), the output SVO vector is given byV (s,o) = P>(QsRo) (3)where  is the elementwise vector product.1However, unlike matrix singular value decomposition,the component vectors in the CP decomposition are not nec-essarily orthonormal.7323 TrainingWe train the compositional model for verbs inthree steps: extracting transitive verbs and theirsubject and object nouns from corpus data, pro-ducing distributional vectors for the nouns and theSVO triples, and then learning parameters of theverb functions, which map the nouns to the SVOtriple vectors.Corpus Data We extract SVO triples from anOctober 2013 download of Wikipedia, tokenizedusing Stanford CoreNLP (Manning et al., 2014),lemmatized with the Morpha lemmatizer (Minnenet al., 2001), and parsed using the C&C parser(Curran et al., 2007).
We filter the SVO triplesto a set containing 345 distinct verbs: the verbsfrom our test datasets, along with some additionalhigh-frequency verbs included to produce morerepresentative sentence spaces.
For each verb, weselected up to 600 triples which occurred morethan once and contained subject and object nounsthat occurred at least 100 times (to allow suffi-cient context to produce a distributional represen-tation for the triple).
This resulted in approxi-mately 150,000 SVO triples overall.Distributional Vectors We produce two typesof distributional vectors for nouns and SVO triplesusing the Wikipedia corpus.
Since these methodsfor producing distributional vectors for the SVOtriples require that the triples occur in a corpus oftext, the methods are not a replacement for a com-positional framework that can produce representa-tions for previously unseen expressions.
However,they can be used to generate data to train such amodel, as we will describe.1) Count vectors (SVD): we count the num-ber of times each noun or SVO triple co-occurswith each of the 10,000 most frequent words (ex-cluding stopwords) in the Wikipedia corpus, usingsentences as context boundaries.
If the verb in theSVO triple is itself a content word, we do not in-clude it as context for the triple.
This produces oneset of context vectors for nouns and another forSVO triples.
We weight entries in these vectorsusing the t-test weighting scheme (Curran, 2004;Polajnar and Clark, 2014), and then reduce thevectors to 100 dimensions via singular value de-composition (SVD), decomposing the noun vec-tors and SVO vectors separately.2) Prediction vectors (PV): we train vectorembeddings for nouns and SVO triples by adapt-ing the Paragraph Vector distributed bag of wordsmethod of Le and Mikolov (2014), an extension ofthe skip-gram model of Mikolov et al.
(2013).
Inour experiments, given an SVO triple, the modelmust predict contextual words sampled from allsentences containing that triple.
In the process, themodel learns vector embeddings for both the SVOtriples and for the words in the sentences such thatSVO vectors have a high dot product with theircontextual word vectors.
While previous work(Milajevs et al., 2014) has used prediction-basedvectors for words in a tensor-based CDS model,ours uses prediction-based vectors for both wordsand phrases to train a tensor regression model.We learn 100-dimensional vectors for nounsand SVO triples with a modified version ofword2vec,2using the hierarchical samplingmethod with the default hyperparameters and 20iterations through the training data.Training Methods We learn the tensor V of pa-rameters for a given verb V using multi-linear re-gression, treating the noun vectors s and o as in-put and the composed SVO triple vector V (s,o)as the regression output.
Let MVbe the num-ber of training instances for V , where the ithin-stance is a triple of vectors(s(i),o(i), t(i)), whichare the distributional vectors for the subject noun,object noun, and the SVO triple, respectively.
Weaim to learn a verb tensor V (either in full or indecomposed, low-rank form) that minimizes themean of the squared residuals between the pre-dicted SVO vectors V (s(i),o(i)) and those vec-tors obtained distributionally from the corpus, t(i).Specifically, we attempt to minimize the followingloss function:L(V ) =1MVMV?i=1||V (s(i),o(i))?
t(i)||22(4)V (s,o) is given by Eq.
(1) for full tensors, and byEq.
(3) for tensors represented in low-rank form.In both the low-rank and full-rank tensor learn-ing, we use mini-batch ADADELTA optimization(Zeiler, 2012) up to a maximum of 500 iterationsthrough the training data, which we found to besufficient for convergence for every verb.
Ratherthan placing a regularization penalty on the ten-sor parameters, we use early stopping if the loss2https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ733increases on a validation set consisting of 10% ofthe available SVO triples for each verb.For low-rank tensors, we compare seven differ-ent maximal ranks: R=1, 5, 10, 20, 30, 40 and 50.To learn the parameters of the low-rank tensors,we use an alternating optimization method (Koldaand Bader, 2009; Lei et al., 2014): performing gra-dient descent on one of the parameter matrices (forexample P) to minimize the loss function whileholding the other two fixed (Q and R), then re-peating for the other parameter matrices in turn.The parameter matrices are randomly initialized.34 EvaluationWe compare the performance of the low-rank ten-sors against full tensors on two tasks.
Both tasksrequire the model to rank pairs of sentences eachconsisting of a subject, transitive verb, and objectby the semantic similarity of the sentences in thepair.
The gold standard ranking is given by sim-ilarity scores provided by human evaluators andthe scores are not averaged among the annotators.The model ranking is evaluated against the rank-ing from the gold standard similarity judgementsusing Spearman?s ?.The verb disambiguation task (GS11) (Grefen-stette and Sadrzadeh, 2011) involves distinguish-ing between senses of an ambiguous verb, givensubject and object nouns as context.
The datasetconsists of 200 sentence pairs, where the two sen-tences in each pair have the same subject and ob-ject but differ in the verb.
Each of these pairs wasranked by human evaluators on a 1-7 similarityscale so that properly disambiguated pairs (e.g.
au-thor write book ?
author publish book) have highersimilarity scores than improperly disambiguatedpairs (e.g.
author write book ?
author spell book).The transitive sentence similarity dataset (Kart-saklis and Sadrzadeh, 2014) consists of 72 subject-verb-object sentences arranged into 108 sentencepairs.
As in GS11, each pair has a gold standardsemantic similarity score on a 1-7 scale.
For ex-ample, the pair medication achieve result ?
drugproduce effect has a high similarity rating, whileauthor write book ?
delegate buy land has a lowrating.
In this dataset, however, the two sentencesin each pair have no lexical overlap: neither sub-jects, objects, nor verbs are shared.3Since the low-rank tensor loss is non-convex, we suspectthat parameter initialization may produce better results.GS11 KS14 # tensorSVD PV SVD PV params.Add.
0.13 0.14 0.55 0.56 ?Mult.
0.13 0.14 0.09 0.27 ?R=1 0.10 0.05 0.18 0.30 300R=5 0.26 0.30 0.28 0.40 1.5KR=10 0.29 0.32 0.26 0.45 3KR=20 0.31 0.34 0.39 0.44 6KR=30 0.28 0.33 0.32 0.46 9KR=40 0.32 0.30 0.31 0.52 12KR=50 0.34 0.32 0.42 0.51 15KFull 0.29 0.36 0.41 0.52 1MTable 1: Model performance on the verb disam-biguation (GS11) and sentence similarity (KS14)tasks, given by Spearman?s ?, and the number ofparameters needed to represent each verb?s tensor.We show the highest tensor result for each task andvector set in bold (and also bold the baseline whenit outperforms the tensor method).5 ResultsTable 1 displays correlations between the systems?scores and human SVO similarity judgements onthe verb disambiguation (GS11) and sentence sim-ilarity (KS14) tasks, for both the count (SVD) andprediction vectors (PV).
We also give results forsimple composition of word vectors using elemen-twise addition and multiplication (Mitchell andLapata, 2008) (using verb vectors produced in thesame manner as for nouns).
As is consistent withprior work, the tensor-based models are surpassedby vector addition on the KS14 dataset (Milajevset al., 2014), but perform better than both additionand multiplication on the GS11 dataset.4Unsurprisingly, the rank-1 tensor has lowestperformance for both tasks and vector sets, andperformance generally increases as we increasethe maximal rank R. The full tensor achievesthe best, or tied for the best, performance on bothtasks when using the PV vectors.
However, for theSVD vectors, low-rank tensors surpass the perfor-mance of the full-rank tensor for R=40 and R=504The results in this table are not directly comparable withMilajevs et al.
(2014), who compare against averaged annota-tor scores.
Comparing against averaged annotator scores, ourbest result on GS11 is 0.47 for the full-rank tensor with PVvectors, and our best non-addition result on KS14 is 0.68 forthe K=40 tensor with PV vectors (the best result is additionwith PV vectors, which achieves 0.71).
These results exceedthe scores reported for tensor-based models by Milajevs et al.
(2014).734on GS11, and R=50 on KS14.On GS11, the SVD and PV vectors have vary-ing but mostly comparable performance, with PVhaving higher performance on 5 out of 8 models.However, on KS14, the PV vectors have better per-formance than the SVD vectors for every modelby at least 0.05 points, which is consistent withprior work comparing count and predict vectors onthese datasets (Milajevs et al., 2014).The low-rank tensor models are also at leasttwice as fast to train as the full tensors: on a singlecore, training a rank-1 tensor takes about 5 sec-onds for each verb on average, ranks 5-50 eachtake between 1 and 2 minutes, and the full tensorseach take about 4 minutes.
Since a separate tensoris trained for each verb, this allows a substantialamount of time to be saved even when using theconstrained vocabulary of 345 verbs.6 ConclusionWe find that low-rank tensors for verbs achievecomparable or better performance than full-ranktensors on both verb disambiguation and sentencesimilarity tasks, while reducing the number of pa-rameters that must be learned and stored for eachverb by at least two orders of magnitude, and cut-ting training time in half.While in our experiments the prediction-basedvectors outperform the count-based vectors onboth tasks for most models, Levy et al.
(2015) in-dicate that tuning hyperparameters of the count-based vectors may be able to produce compara-ble performance.
Regardless, we show that thelow-rank tensors are able to achieve performancecomparable to the full rank for both types of vec-tors.
This is important for extending the modelto many more grammatical types (including thosewith corresponding tensors of higher order than in-vestigated here) to build a wide-coverage tensor-based semantic system using, for example, theCCG parser of Curran et al.
(2007).AcknowledgmentsDaniel Fried is supported by a Churchill Schol-arship.
Tamara Polajnar is supported by ERCStarting Grant DisCoTex (306920).
Stephen Clarkis supported by ERC Starting Grant DisCoTex(306920) and EPSRC grant EP/I037512/1.
Wewould like to thank Laura Rimell and the anony-mous reviewers for their comments.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2010), Cambridge, Massachusetts.Marco Baroni, Raffaela Bernardi, and Roberto Zam-parelli.
2014.
Frege in space: A program of compo-sitional distributional semantics.
Linguistic Issuesin Language Technology, 9.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, (3):1137?1155.William Blacoe, Elham Kashefi, and Mirella Lapata.2013.
A quantum-theoretic approach to distribu-tional semantics.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (NAACL-HLT 2013), Atlanta,Georgia.Stephen Clark.
2013.
Type-driven syntax and se-mantics for composing meaning vectors.
Quan-tum Physics and Linguistics: A Compositional, Dia-grammatic Discourse, pages 359?377.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2011.
Mathematical foundations for a com-positional distributional model of meaning.
Linguis-tic Analysis, 36(1-4):345?384.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.James R Curran, Stephen Clark, and Johan Bos.
2007.Linguistically motivated large-scale NLP with C&Cand Boxer.
In Proceedings of the DemonstrationSession of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2007),Prague, Czech Republic.James R. Curran.
2004.
From distributional to seman-tic similarity.
Ph.D. thesis, University of Edinburgh.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimenting with transitive verbs in a DisCoCat.In Proceedings of the 2011 Workshop on Geometri-cal Models of Natural Language Semantics (GEMS2011), Edinburgh, Scotland.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for composi-tional distributional semantics.
In Proceedings ofthe 10th International Conference on ComputationalSemantics (IWCS 2013), Pottsdam, Germany.735Dimitri Kartsaklis and Mehrnoosh Sadrzadeh.
2014.
Astudy of entanglement in a categorical framework ofnatural language.
In Proceedings of the 11th Work-shop on Quantum Physics and Logic (QPL 2014),Kyoto, Japan, June.Tamara G Kolda and Brett W Bader.
2009.
Ten-sor decompositions and applications.
SIAM Review,51(3):455?500.Quoc V. Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
In Pro-ceedings of the 31st International Conference onMachine Learning (ICML 2014), Beijing, China.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (ACL 2014), Baltimore, Mary-land.Tao Lei, Yuan Zhang, Lluis Marquez, Alessandro Mos-chitti, and Regina Barzilay.
2015.
High-order low-rank tensors for semantic role labeling.
In Proceed-ings of the 2015 Conference of the North AmericanChapter of the Association for Computational Lin-guistics ?
Human Language Technologies (NAACL-HLT 2015), Denver, Colorado.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
Transactions of the Associ-ation for Computational Linguistics, 3:211?225.Jean Maillard, Stephen Clark, and Edward Grefen-stette.
2014.
A type-driven tensor-based semanticsfor CCG.
In Proceedings of the EACL 2014 TypeTheory and Natural Language Semantics Workshop(TTNLS), Gothenburg, Sweden.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics (ACL 2014): System Demonstra-tions, pages 55?60, Baltimore, Maryland.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed repre-sentations of words and phrases and their composi-tionality.
In Neural Information Processing Systems(NIPS 2013).Dmitrijs Milajevs, Dimitri Kartsaklis, MehrnooshSadrzadeh, and Matthew Purver.
2014.
Evaluatingneural word representations in tensor-based compo-sitional settings.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2014).Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(03):207?223.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofthe 46th Annual Meeting of the Assocation for Com-putational Linguistics: Human Language Technolo-gies (ACL-08: HLT), Columbus, Ohio.Denis Paperno, Nghia The Pham, and Marco Baroni.2014.
A practical and linguistically-motivated ap-proach to compositional distributional semantics.In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (ACL2014), Baltimore, Maryland.Tamara Polajnar and Stephen Clark.
2014.
Improv-ing distributional semantic vectors through contextselection and normalisation.
In Proceedings of the14th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL2014), Gothenburg, Sweden.Tamara Polajnar, Luana Fagarasan, and Stephen Clark.2014.
Reducing dimensions of tensors in type-driven distributional semantics.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2014), Doha,Qatar.Hinrich Sch?utze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?124.Paul Smolensky and Geraldine Legendre.
2006.The Harmonic Mind: from neural computation tooptimality-theoretic grammar.
MIT Press, Cam-bridge, MA.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Composi-tionality Through Recursive Matrix-Vector Spaces.In Proceedings of the 2012 Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL 2012), Jeju Island, Korea.Peter D Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Tim Van de Cruys.
2010.
A non-negative tensor fac-torization model for selectional preference induc-tion.
Journal of Natural Language Engineering,16(4):417?437.Fabio M Zanzotto and Lorenzo Dell?Arciprete.
2012.Distributed tree kernels.
In Proceedings of the29th International Conference on Machine Learning(ICML 2012), Edinburgh, Scotland.Matthew D Zeiler.
2012.
ADADELTA: anadaptive learning rate method.
arXiv preprintarXiv:1212.5701.736
