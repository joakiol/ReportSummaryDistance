Automatic Annotation for All Semantic Layers in FrameNetRichard Johansson and Pierre NuguesDepartment of Computer Science, Lund UniversityBox 118SE-221 00 Lund, Sweden{richard, pierre}@cs.lth.seAbstractWe describe a system for automatic an-notation of English text in the FrameNetstandard.
In addition to the conventionalannotation of frame elements and their se-mantic roles, we annotate additional se-mantic information such as support verbsand prepositions, aspectual markers, cop-ular verbs, null arguments, and slot fillers.As far as we are aware, this is the first sys-tem that finds this information automati-cally.1 IntroductionShallow semantic parsing has been an active areaof research during the last few years.
Seman-tic parsers, which are typically based on theFrameNet (Baker et al, 1998) or PropBank for-malisms, have proven useful in a number of NLPprojects, such as information extraction and ques-tion answering.
The main reason for their popular-ity is that they can produce a flat layer of semanticstructure with a fair degree of robustness.Building English semantic parsers for theFrameNet standard has been studied widely(Gildea and Jurafsky, 2002; Litkowski, 2004).These systems typically address the task of identi-fying and classifying Frame Elements (FEs), thatis semantic arguments of predicates, for a giventarget word (predicate).Although the FE layer is arguably the most cen-tral, the FrameNet annotation standard defines anumber of additional semantic layers, which con-tain information about support expressions (verbsand prepositions), copulas, null arguments, slot-fillers, and aspectual particles.
This informationcan for example be used in a semantic parser torefine the meaning of a predicate, to link predi-cates in a sentence together, or possibly to improvedetection and classification of FEs.
The task ofautomatic reconstruction of the additional seman-tic layers has not been addressed by any previoussystem.
In this work, we describe a system that au-tomatically identifies the entities in those layers.2 Introduction to FrameNetFrameNet (Baker et al, 1998; Johnson et al,2003) is a comprehensive lexical database thatlists descriptions of words in the frame-semanticparadigm (Fillmore, 1976).
The core concept isthe frame, which is conceptual structure that rep-resents a type of situation, object, or event, cou-pled with a semantic valence description that de-scribes what kinds of semantic arguments (frameelements) are allowed or required for that partic-ular frame.
The frames are arranged in an ontol-ogy using relations such as inheritance (such as therelation between COMMUNICATION and COM-MUNICATION_NOISE) and causative-of (such asKILLING and DEATH).For each frame, FrameNet lists a set of lemmasor lexical units (mostly nouns, verbs, and adjec-tives, but also a few prepositions and adverbs).When such a word occurs in a sentence, it is calleda target word that evokes the frame.
FrameNetcomes with a large set of manually annotated ex-ample sentences, which is typically used by sta-tistical systems for training and testing.
Figure 1shows an example of such a sentence.
Here,the target word eat evokes the INGESTION frame.Three FEs are present: INGESTOR, INGESTIBLES,and PLACE.135Often [an informal group]INGESTOR will eat[lunch]INGESTIBLES [near a machine or otherwork station]PLACE, even though a canteen isavailable.Figure 1: A sentence from the FrameNet examplecorpus, with FEs bracketed and the target word initalics.3 Semantic Entities in FrameNetThe semantic annotation in FrameNet consists ofa set of layers.
One of the layers defines the tar-get, and the other layers provide additional infor-mation with respect to the target.
The followinglayers are used:?
The FE layer, which defines the spans and se-mantic roles of the arguments of the predi-cate.?
A part-of-speech-specific layer, which con-tains aspectual information for verbs; andcopulas, support expressions, and slot fillinginformation for nouns and adjectives.?
The ?Other?
layer, containing special casessuch as null arguments.The semantic entities that we consider in thisarticle are defined in the second and third of theselayers.3.1 Support ExpressionsSome noun targets, typically denoting events, areoften constructed using support verbs.
In this case,the noun carries most of the semantics (that is, itevokes the frame), while the verb allows the slotsof the frame to be filled.
Thus, the dependentsof a support verb are annotated as FEs, just likefor a verb target.
Support verbs are annotated us-ing the SUPP label on the Noun or Adjective layer.In the following sentence, there is a support verb(underwent) for the noun target (operation).
[Frances Patterson]PATIENT underwent an op-eration at RMH today and is expected to be hos-pitalized for a week or more.The support verbs do not change the core se-mantics of the noun target (that is, they bear no re-lation to the frame).
However, they may determinethe relation between the FEs and the target (?point-of-view supports?, such as ?undergo an operation?or ?perform an operation?)
or provide aspectualinformation (such as ?start an operation?
).The following sentence shows an examplewhere a governing verb is not a support verb of thenoun target.
An automatic system must be able todistinguish support verbs from other verbs.A senior nurse observed the operation.Although a large majority of the support expres-sions are verbs, there are additionally some casesof support prepositions, such as the following ex-ample:Secret agents of this ilk are at work all the time.3.2 CopulasCopular verbs, typically be, may be seen as a spe-cial kind of support verb.
They are marked us-ing the COP label on the Noun or Adjective layer.There are several uses of copulas:?
Class membership: John is a sailor.?
Qualities: Your literary masterpiece was delicious.?
Location: This was inside a desk drawer.?
Identity: Smithers is the vice-president of the arm-chair division.In FrameNet annotation, these uses of the cop-ular verb are not distinguished.3.3 Null ArgumentsThere are constructions that require special argu-ments to be syntactically valid, but where these ar-guments have no relation to the semantics of thesentence.
In the example below, it is an exampleof this phenomenon.I hate it when you do that.Other common cases include existential con-stuctions (?there are?)
and subject requirement ofzero-place predicates (?it rains?).
These null argu-ments are tagged as NULL on the Other layer.3.4 Aspectual ParticlesVerb particles that indicate aspectual informationare marked using the ASPECT label.
These parti-cles must be distinguished from particles that areparts of multiword units, such as carry out.They just moan on and on about Fergie this andFergie that and I ?ve simply had enough.1363.5 Slot Fillers: GOV and XFrameNet annotation contains some informationabout the relation of predicates in the same sen-tence when one predicate is a slot filler (that is,an argument) of the other.
This is most commonfor noun target words, typically referring to natu-ral kinds or artifacts.In the following example, the target wordfingertips evokes the OBSERVABLE_BODYPARTSframe, involving two FEs: POSSESSOR (?his?
)and BODY_PART (?fingertips?).
This noun phraseis also a slot filler (that is, an argument) of anotherpredicate in the sentence: cling on.
In FrameNet,such predicates are annotated using the GOV la-bel.
The constituent that contains the slot filler inquestion is called (for lack of a better name) X.Shares will boom and John Major will[cling on]GOV [by [his]POSSESSOR[fingertips]BODY_PART ]X.If GOV and X are present, all FEs must becontained in the span of the X node, such asBODY_PART and POSSESSOR above.
This maybe of use for automatic FE identifiers.4 Identifying Semantic EntitiesTo find the semantic entities in the text, we usedthe method that has previously been used forFE detection: classification of nodes in a parsetree.
We divide the identification process into twostages:?
The first stage finds SUPP, COP, and GOV.?
The second stage finds NULL, ASP, and X.The reason for this division is that we expectthat the knowledge of the presence of SUPP, COP,and GOV, which are almost always verbs, is use-ful when detecting the other entities.
The secondstage makes use of the information found in thefirst stage.
Above all, it is necessary to have infor-mation about GOV to be able to detect X.To train the classifiers, we selected the 150 mostcommon frames and divided the annotated exam-ple sentences for those frames into a training setof 100,000 sentences and a test set of 8,000 sen-tences.The classifiers used the Support Vector learningmethod using the LIBSVM package (Chang andLin, 2001).
The features used by the classifiers arelisted in Table 1.
Apart from the features used byFeatures for first and second stageTarget lemmaTarget POSVoiceAvailable semantic role labelsPosition (before or after target)Head word and POSPhrase typeParse tree path from target to nodeFeatures for second stage onlyHas SUPPHas COPHas GOVParse tree path from SUPP to nodeParse tree path from COP to nodeParse tree path from GOV to nodeTable 1: Features used by the classifiers.Stage 2, most of them are well-known from pre-vious literature on FE identification and labeling(Gildea and Jurafsky, 2002; Litkowski, 2004).
Forall path features, we used both the traditional con-stituent parse tree path (as by Gildea and Jurafsky(2002)) and a dependency tree path (as by Ahn etal.
(2004)).
We produced the parse trees using theparser of Collins (1999).5 EvaluationWe applied the system to a test set consisting ofapproximately 8,000 sentences.Because of inconsistent annotation, we did notevaluate the performance of detection of the EX-IST tag used in existential constructions.
Prelim-inary experiments indicated that the performancewas very poor.The results, with confidence intervals at the95% level, are shown in Table 2.
They demon-strate that the classical approach for FE identifica-tion, that is classification of nodes in the parse tree,is as well a viable method for detection of otherkinds of semantic information.
The detection ofX shows the poorest performance.
This is to beexpected, since it is very dependent on a GOV tohave been detected in the first stage.The results for detection of aspectual particlesis not very reliable (the confidence interval was?0.17 for precision and ?0.19 for recall), sincetest corpus contained just 25 of these particles.137P R F?=1SUPP 0.85 ?
0.046 0.64 ?
0.054 0.73COP 0.90 ?
0.027 0.87 ?
0.030 0.88NULL 0.76 ?
0.082 0.80 ?
0.080 0.78ASP 0.83 ?
0.17 0.6 ?
0.19 0.70GOV 0.79 ?
0.029 0.64 ?
0.030 0.71X 0.59 ?
0.035 0.49 ?
0.032 0.54Table 2: Results with 95% confidence intervals onthe test set.6 Conclusion and Future WorkWe have described a system that reconstructs allsemantic layers in FrameNet: in addition to thetraditional task of building the FE layer, it marksup support expressions, aspectual particles, cop-ulas, null arguments, and slot filling information(GOV/X).
As far as we know, no previous systemhas addressed these tasks.In the future, we would like to study how theinformation provided by the additional layers in-fluence the performance of the traditional task fora semantic parser.
FE identification, especiallyfor noun and adjective target words, may be madeeasier by knowledge of the additional layers.
Asmentioned above, if a support verb is present, itsdependents are arguments of the predicate.
Thesame holds for copular verbs.
GOV/X nodes alsorestrict where FEs may occur.
In addition, supportverbs (such as ?perform?
or ?undergo?
an opera-tion) may be beneficial when determining the re-lationship between the FE and the predicate, thatis when assigning semantic roles.ReferencesDavid Ahn, Sisay Fissaha, Valentin Jijkoun, andMaarten de Rijke.
2004.
The university of Amster-dam at Senseval-3: Semantic roles and logic forms.In Proceedings of SENSEVAL-3.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of COLING-ACL?98, pages 86?90, Montr?al,Canada.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM:a library for support vector machines.Michael J. Collins.
1999.
Head-driven statistical mod-els for natural language parsing.
Ph.D. thesis, Uni-versity of Pennsylvania, Philadelphia.Charles J. Fillmore.
1976.
Frame semantics andthe nature of language.
Annals of the New YorkAcademy of Sciences: Conference on the Origin andDevelopment of Language, 280:20?32.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Christopher Johnson, Miriam Petruck, Collin Baker,Michael Ellsworth, Josef Ruppenhofer, and CharlesFillmore.
2003.
FrameNet: Theory and Practice.Ken Litkowski.
2004.
Senseval-3 task: Automaticlabeling of semantic roles.
In Rada Mihalcea andPhil Edmonds, editors, Senseval-3: Third Interna-tionalWorkshop on the Evaluation of Systems for theSemantic Analysis of Text, pages 9?12, Barcelona,Spain, July.
Association for Computational Linguis-tics.138
