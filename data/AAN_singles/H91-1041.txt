Session 7: Natura l  Language IISalim RoukosIBM Research Division, Thomas J. Watson Resea.rch Center,Yorktown Heights, NY 10598Context-free grammars (CFG) describe the possiblederivations of a Non-Terminal (NT), denoted by A, inde-pendently of the context (or tree) in which this NT (also re-ferred to as constituent) occurs.
Typically there are severalrewrite rules for a particular NT A.
One straight-forwardmethod for using a probabilistic model is to define a prob-abilistic CFG (PCFG) which assigns probabilities to therewrite rules of a NT A that sum to one.
This simplemodel therefore assumes a probabilistic independence as-sumption that the choice of the rule to expand A is inde-pendent of the context (parse tree) in which A occurs.
Ashas transpired in the discussion period, this strong assump-tion was found to be objectionable by many.
Several in theaudience suggested using other grammatical formalisms toput probabilities on, where the independence assumptionmay be more acceptable.
While this may be an approach,I suspect hat there is a gold mine in using PCFG in amanner that is slightly more sophisticated than the aboveapproach.
For example, as Magerman and Marcus  (andreference 3 in their paper) suggest, one may assign that therewrite rule probability depend on the parent rule and thetrigram of part-of-speech categories centered on the wordthat is the left corner of the rewrite rule.
Other condi-tionings are possible and care should be taken that theparameters of the resulting probabilistic model can be es-timated reliably from the training corpus.
I think whetherPCFGs or more sophisticated probabilistic grammars areneeded for language processing would be best answered byusing empirical experiments using standardized tests to al-low for meaningful comparisons.
To foster such a researchprogram several ingredients are needed:?
Grammars (whether context free or not) that have alarge enough coverage with a reasonable number ofparses (say that the correct parse is with probability99% in the top N parses where N is some agreedupon number.)?
Standard blind test sets annotated with the correctparse.
See the paper by Black et alin these proceed-ings where a common marking is proposed.?
Standard training sets to allow parameter estimationand grammar development.has a strong independence assumption that has not yetshown to be a drastic barrier to improved performance.Four of the five papers in this session address issueswith PCFGs.
Paper #2 by DeMor i  and Kuhn extendsthe algorithm to compute the probability that a sequenceof words is an initial (prefix) substring of a sentence (seereference 8 in DeMori and Kuhn) to handle the case of anisland: the probability that a string of words is an island(i.e., occur somewhere in a sentence.)
They point out thatthe resulting computation is impractical.
But they identifythe special case where the gap length to the left of the is-land is known.
Extending the gap length by one is only cu-bic.
(In Paper #2, a dynamic cache language model usinga tri-part-of-speech model is also described.)
Paper #4 byKoch lnan  and Kup in  presents an algorithm that com-putes the probability that an LR parser for a PCFG willcomplete (accept a sentence.)
Thereby, deriving the prefixprobability (involves a matrix inversion) and then derivingupdate rules to compute the joint probability of the parserstack and input substring.
Paper #5 by Kup iec  presentsa new organization on how to carry the probability compu-tations (for parameter estimation of PCFGs) based on anextension that uses several trellises (a la tbrward-backwardalgorithm).
The.
algorithm does not require the grammarto be in Chomsky Normal Form as required by the Inside-Outside algorithm but rather uses a recursive transitionnetwork representation f the grammar.
As discussed ear-lier, Paper #3 by Magerman and Marcus  makes thecase that rule probabilities should depend on more context.Ilowever, they quickly abandon the probabilistic approachin favor of a heuristic score citing shortcomings of the inde-pendence assumption and unreliable probability estimates.1 suspect that we will hear more on this subject as moreempirical work is done to determine how best to deal withthese issues.
Finally, Paper #1 by Bobrow proposes asearch strategy that uses several agenda to fill a chart inorder to get an "acceptable" parse (a parse that leads toexecutable database access commands.)
He introduces theuse of the rule success probability which is estimated byfrequency counts that the rule introduces terms that be-long to the "acceptable" parse.
Using heuristic weigthlngof rule success probabilities yields a speedup by 1.8 com-pared to a full search CYK algorithm.In trying to assess the value of PCFGs, one cannot for-get the analogy to speech research where the HMM model221
