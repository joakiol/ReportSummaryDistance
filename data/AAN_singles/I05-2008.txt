A System to Solve Language Tests for Second Grade StudentsManami SaitoNagaoka University of Technologysaito@nlp.nagaokaut.ac.jpKazuhide YamamotoNagaoka University of Technologyyamamoto@fw.ipsj.or.jpSatoshi SekineNew York UniversityLanguage Craftsekine@cs.nyu.eduHitoshi IsaharaNational Institute of Information and Com-munications Technologyisahara@nict.go.jpAbstractThis paper describes a system whichsolves language tests for second gradestudents (7 years old).
In Japan, thereare materials for students to measureunderstanding of what they studied,just like SAT for high school studentsin US.
We use textbooks for the stu-dents as the target material of this study.Questions in the materials are classifiedinto four types: questions about Chi-nese character (Kanji), about wordknowledge, reading comprehension,and composition.
This program doesn?tresolve the composition and some otherquestions which are not easy to be im-plemented in text forms.
We built asubsystem for each finer type of ques-tions.
As a result, we achieved 55% -83% accuracy in answering questionsin unseen materials.1 IntroductionThis paper describes a system which solves lan-guage tests for second grade students (7 yearsold).
We have the following two objections.First, we aim to realize the NLP technologiesinto the form which can be easily observed byordinary people.
It is difficult to evaluate NLPtechnology clearly by ordinary people.
Thus, weset the target to answer second grade Japaneselanguage test, as an example of intelligible ap-plication to ordinary people.
The ability of thisprogram will be shown by scores which are fa-miliar to ordinary people.Second aim is to observe the problems of theNLP technologies by degrading the level of tar-get materials.
Those of the current NLP researchare usually difficult, such as newspapers or tech-nological texts.
They require high accuracy lan-guage processing, complex world knowledge orsemantic processing.
The NLP problems wouldbecome more apparent when we degrade thetarget materials.
Although questions for secondgrade students also require world knowledge, itis expected that the questions become simplerand are resolved without tangled techniques.2 Related WorksHirschman et al (1999) and Charniak et al(2000) proposed systems to solve ?ReadingComprehension.?
Hirschman et al (1999) de-veloped ?Deep Read,?
which is a system to se-lect sentences in the text which include answersto a question.
In their experiments, the types ofquestions are limited to ?When,?
?Who?
and soon.
The system is basically an information re-trieval system which selects a sentence, insteadof a document, based on the bag-of-wordsmethod.
That system retrieves the sentence con-taining the answer at 30-40% of the time on thetests of third to sixth grade materials.
In short,Deep Read is very restricted compared to oursystem.
Charniak et al (2000) built a systemimproved over the Deep Read by giving moreweights for verb and subject, and introducedheuristic rules for ?Why?
question.
Though, theessential target and method are the same as thatof Deep Read.433 Question ClassificationFirst, we bought five language test books forsecond grade students and one of them, pub-lished by KUMON, was used as a training textto develop our system.
The other four books arereferred occasionally.
Second, we classified thequestions in the training text into four types:questions about Chinese character (Kanji), ques-tions on word knowledge, reading comprehen-sion, and composition.
We will call these typesas major types.
Each of the ?major types?
isclassified into several ?minor types.?
Table 1shows four major types and their minor types.
Inpractice, each minor type farther has differentstyle of questions; such as description question,choice question, and true-false question.
Thequestions can be classified into approximately100 categories.
We observed that some ques-tions in other books are mostly similar; howeverthere are several questions which are not cov-ered by the categories.Major type Minor typeKanji Reading, Writing, Radical, Theorder of writing, ClassificationWord knowl-edgeKatakana, How to use Kana, Ap-propriate Noun, To fill blanks forVerb, Adjective, and Adjunct,Synonym, Antonym, Particle,Conjunction, Onomatopoeia, Po-lite Expression, Punctuation markReading com-prehensionWho, What, When, Where, How,Why question, Extract specificphrases, Progress order of a story,Prose and VerseComposition Constructing sentence, How towrite compositionTable 1.
Question types4 Answering questions and evaluationresultIn this section, we describe the programs tosolve the questions for each of the 100 catego-ries and these evaluation results.
First we classi-fied questions.
Some questions are difficult tocover by the system such as the stroke order ofwriting Kanji.
For about 90% of all the questiontypes other than such questions, we created pro-grams for basically one or two categories ofquestions.
There are 47 programs for the catego-ries found in the training data.In each section of 4.1 to 4.3, we describehow to solve questions of typical types and theevaluation results.
The evaluation results of thetotal system will be reported in the followingsection.Table 2 to 4 show the distributions of minortypes in each major type, ?Kanji,?
?Wordknowledge,?
and ?Reading comprehension,?
inthe training data and the evaluation results.Training and evaluation data have no overlap.4.1 Kanji questionsMost of the Kanji questions are categorized into?reading?
and ?writing.?
Morphological analysisis used in the questions of both reading and writ-ing Kanji; we found that large corpus is effec-tive to choose the answer from Kanji candidatesgiven from dictionary.
Table 2 shows it in detail.This system cannot answer to the questionswhich are asking the order of writing Kanji, be-cause it is difficult to put it into digital format.The system made 7 errors out of 334 ques-tions.
The most of the questions are the errors inreading Kanji by morphological analysis.In particular, morphological analysis is theonly effective method to answer questions onthis type.
It would be very helpful, if we had alarge corpus considering reading information,but there is no such corpus.TrainingdataTest data Ques-tion typeThe rateof Q intrainingdata[%]The usedknowledgeand tools Correctans.
(total)Correct ans.
(known typeQ., total)Reading 27 Kanji dictionary,Morphologicalanalysis96(100)6(8,8)Writing 61 Word diction-ary, Largecorpus220(222)63(66,66)Order ofwriting6 -0(20)0(0,2)Combi-nationof Kanjiparts3 -0(10)0(0,0)ClassifyKanji3 Word diction-ary, Thesaurus11(12)0(0,0)Total 100 - 327(364) 69(74,76)Table 2.
Question types for Kanji4.2  Word knowledge questionsThe word knowledge question dealt with vo-cabulary, different kinds of words and the struc-ture of sentence.
These don?t include Kanjiquestions and reading comprehension.
Table 3shows different types of questions in this type.44For the questions on antonym, the system cananswer correctly by choosing most relevant an-swer candidate using the large corpus out ofmultiple candidates found in the antonym dic-tionary.The questions about synonyms ask relationsof priority/inferiority between words and choos-ing the word in a different group.
These ques-tions can usually be answered using thesaurus.Ex.1 shows a question about particle, Japa-nese postposition, which asks to select the mostappropriate particle for the sentence.The system produces all possible sentenceswith the particle choices, and finds most likelysentences in a corpus.
In Ex.1, all combinationsare not in a corpus, therefore shorter parts of thesentence are used to find in the corpus (e.g.
????
(1) ???
?, ????
(2) ????).
In thiscase, the most frequent particle for (1) is ???
ina corpus, so this system outputs incorrect answer.Ex.1 [?/?/?]????()????????????
?Select particle which fits the sentence from{wo,to,ni}[1] ???
(1) ???
(2) ??
?apple-(1) orange-(2) buy(1) correct=?
(to) system=?
(wo)(2) correct=?
(wo) system=?
(wo)The questions of Katakana can be answeredmostly by the dictionary.
The accuracy of thistype is not so high, found in Table 2, becausethere are questions asking the origin of thewords, most Katakana words in Japanese has afew origins: borrowed words, onomatopoeia,and others.
Because we don?t have such knowl-edge, we could not answer those questions.The questions of onomatopoeia include thoseshown in Ex.2.
The system uses co-occurrenceof words in the given sentence and each answercandidate to choose the correct answer in Ex.2,?????.?
However, it was not chosen be-cause the co-occurrence frequency of ?????,?
the word in the sentence, and ????
?,?incorrect answer, is higher.Ex.2 ???
????
????
????
[ ] ??
???????
??????
?Choose the most appropriate onomatopoeia(1) ???
???
????
????????
(A large object is rowing slowly)[??????????????
]The questions of word knowledge are classi-fied into 29 types.
We made a subsystem foreach type.
As there are possibly more types inother books, making a subsystem for each typeis very costly.
One of the future directions ofthis study is to solve this problem.TrainingdataTest data QuestiontypeThe rateof Q intrainingdata[%]The usedknowledge andtools Correctans.
(total)Correct ans.
(known typeQ., total)Anonym 18 Antonymdictionary,Large corpus26(27) 12(15,21)Synonym 11 Thesaurus 14(17) 34(44,83)Particle 19 Large corpus 25(28) 16(17,17)Katakana 25 Word diction-ary, Morpho-logical analysis18(37) 19(22,52)Onomato-poeia19 Large corpus,Morphologicalanalysis18(29) 16(20,31)Structureof sen-tence5 Morphologicalanalysis7(7) 20(22,22)How touse kana2 - 0(3) 0(0,19)Dictationof verb2 - 0(3) 0(0,0)Total 100 - 108(151) 117(140,245)Table 3.
Question types for Word knowledge4.3 Reading comprehension questionsThe reading comprehension questions need toread a story and answer questions on the story.We will describe five typical techniques that areused at different types of questions, shown inTable 4.Pattern matching (a) is used in questions tofill blanks in an expression which describes apart of the story.
In general, the sequence ofword used in the matching is the entire expres-sion, but if no match was found, smaller por-tions are used in the matching.Ex.3 Fill blanks in the expressionStory?partial???????
??????
??
?????????
??????
????
??????
(In a few days, the flower withers and gradu-ally changes its color to black.)Expression????
(1)?
(2) ??
?????
(The flower (1) and change its color to (2).)Answer?
(1) ????
(withers)(2) ????
(black)The effectiveness of this technique is foundin this example.
The other methods will beneeded when questions will be more difficult.
Atthe time, this technique is very useful to solvemany questions in reading comprehension.45When the question is ?Why?
question, key-words such as ????
(thus)?
and ????(because)?
are used.For example, when questions start with?When (??)?
and ?Where (??),?
we canrestrict the type of answer word to time or loca-tion, respectively.
If the question includes theexpression of ?????
(???
is a particle toindicate direction/specification), the answer isalso likely to be expressed with ?ni?
right afterthe location in the story.
(The kind of NE(Named Entity) and particle right after word(b))For the questions asking the time or locationabout the entire story, this system outputs theappropriate type of the word which appearedfirst in the story.
Although there are mistakesdue to morphological analysis and NE extraction,this technique is also consistently very useful.The technique which is partial matchingwith keywords (c) is used to seek an answerfrom story for ?how,?
?why?
or ?of what?
ques-tions.
Keywords from the question are used tolocate the answer in the story.Ex.4 ????????
??????????
??
?Frequency in the large corpus is used tofind the appropriate sentence conjunction.
(d)Answer is chosen by comparing the mutual in-formation of the candidate conjunctions and thelast basic block of the previous sentence.
How-ever, this technique turns out to be not effective.Discourse analysis considering wider context isrequired to solve this question.The technique which uses distance betweenkeywords in question and answers (e) is sup-plementary to the abovementioned methods.
Ifmultiple answers are found, the answer candi-date that is the closest in the story text to thekeywords in questions is generated.
These key-words are content words and unknown words inthe text.
This technique is found very effective.In Table 4, the column ?Used method?
showsthe techniques used to solve the type of ques-tions, in the order of priority.
?f?
in the tabledenotes means that we use a method which wasnot mentioned above.????
(How big are chicks whenthey hatched?)Text?partial???????????
??????????????
???????
(The size of chicks when they hatched is aboutthe size of your thumb.)Answer??????
????
(size ofthumb)The rate of Training data Test dataquestions in Used corrent wns.
corrent ans.training data [%] methods (total) (known type Q, total)Who said 5 b,a,fThe others 0 b,e,fLike what c,b,eOf what c,f,eWhat doing c,a,fWhat is a,eWhat do A say b,a,f,eWhole story bPart of story -Whole story bPart of story b,f,c16 c,f 11(18) 0(1, 1)10 c 8(11) 0(0, 1)2 b,c,f 1(2) 0(0, 0)10 a 10(12) 4(9, 9)4 - 0(5) 0(0, 0)2 d 1(2) 1(3, 3)10 f 8(11) 0(0, 0)10 f 7(12) 3(3, 3)1 - 0(1) 0(0, 6)100 - 74(116) 10(22, 34)WhyHowHow long, how often, how largeTotalParagraphThe othersTo fill blanksNot have interrogative pronounConjunctionProgress order of a storyWhere 4 3(5) 0(1When 4 3(5) 0(0WhoQuestion type17(26) 1(1, 6)What 225(6) 1(4, 4), 1), 0)Table 4.
Question types for Reading comprehension465 EvaluationWe collected questions for the test from differ-ent books of the training data.
The propositionof the number of questions for different sectionsis not the same as that of the training data.
Table2 to 4 show the evaluation results in the test datafor each type.
Table 5 shows the summary of theevaluation result.
In the test, we use only thequestions of the type in training data.
The tablesalso show the total number of questions, thenumber of questions which are solved correctly,and the number of questions which are not oneof the types the system targeted (not a type inthe training data).The ratio of the questions covered by the sys-tem, questions in test data which have the sametype in the training data are 97.4% in Kanji,57.1% in word knowledge, and 64.7% in read-ing comprehension.
It indicates that about a halfof the questions in word knowledge isn?t cov-ered.
As the result, accuracy on the questions ofthe covered types in word knowledge is 83.6%,but it drops to 47.8% for the entire questions.
Itis because our system classified the questionsinto many small types and builds a subsystemfor each type.The accuracy for the questions of coveredtype is 83.4%.
In particular, for the questions ofKanji and word knowledge, the scores in the testdata are nearly the same as those in the trainingdata.
It presents that the accuracy of the systemis provided that the question is in the coveredtype.
However, the score of reading comprehen-sion is lower in the test data.
We believe thatthis is mainly due to the small test data of read-ing comprehension (only 34) and that the accu-racy for ?Who?
questions and the questions tofill blanks in the test data are quite difficult com-pared to the training data.Num.
Num.
Num.
RCA * RCA* RCA*of of of (known (total?
in totalall Q known corrent type Q) [%] of knowntype Q ans.
[%] type Q [%]Kanji 76 74 69 93.2 90.8 89.8Word knowledge 245 140 117 83.6 47.8 71.5ReadingComprehensionTotal 355 235 196 83.4 55.2 80.345.5 29.4 63.834 22 10Table 5.
Evaluations at test data* Rate of Correct Answer6 DiscussionsWe will discuss the problems and future direc-tions found by the experiments.6.1 Recognition and Classification of Ques-tionsIn order to solve a question in language test,students have to recognize the type of the ques-tion.
The current system skips this process.
Inthis system, we set up about 100 types of ques-tions referring the training data and a subpro-gram solves questions corresponding to eachtype.
There are two problems to be solved.
First,we have to design the appropriate classificationand avoid unknown types in the test data.
Fromthe experiment, we found that the current typesare not enough to solve this problem.
Second,the program has to classify the questions auto-matically.
We are building this system and areforecasting it quite optimistically once a goodformat is provided.6.2 Effectiveness of Large CorpusThe large corpus of newspapers and the Web areused effectively in many different cases.
Wewill describe several examples.In Japanese, there are different Kanji for thesame reading.
For example, Kanji for ???
(au:to see, to solve correctly) are ???
(to see)?
or???
(to solve correctly)?
for ?????
(to seepeople)?
and ??????
(to solve an answercorrectly),?
respectively.
This type of questionscan be solved by counting the expressions withKanji in the corpus.
It is similar to word sensedisambiguation.In the questions of particle complement, suchas ???
(umbrella) ??/?/?
(locative-, con-junctive-, and objective particles) ??
(home)??/?/??????
(to left) ?
(Intentionalsentence is ?I left the umbrella at home?
)?, it canbe solved by counting the expressions with eachparticle in a corpus.
This method is mentioned inMatsui?2004?but the evaluation result wasnot reported.
When the answer is not found forthe entire expression, the answer is searched bydeleting some contexts.
Most questions of fillingblank types, similar strategy is helpful to findthe correct answer.In summary, the experiments showed that thelarge corpus is quite useful in several types of47questions.
We believe it would be quite difficultto achieve the same accuracy by compiledknowledge, such as a dictionary of verbs, anto-nyms, synonyms, and relation words, and a the-saurus.6.3 World KnowledgeThe questions sometimes need various types ofworld knowledge.
For example, ?A student en-ters junior high school after graduated fromelementary school.?
And ?People become happy,if he receives something nice from someone.?
Itis a difficult problem how to describe and howuse that knowledge.
Another type of worldknowledge includes origin of words, such asforeign borrowed word or onomatopoeia.
As faras we know, there is no comprehensive knowl-edge of such in electronic form.
It is required todesign attributes of world knowledge and to usethem flexibly when applying then to solve thequestions.6.4 Difference between Reading Compre-hension and Question AnsweringThe current QA systems identify the NE type ofquestions and seek the answer candidate of thetype.
However, the questions in the readingcomprehension don?t limit the answer types toperson and organization, even if the question is?Who?
type question.
For example, ?raccoondog behind our house?
or ?the moon?
can be theanswer.
Also, the answer is not always a nounphrase, but can be a clause, for example, ?thetime when new leaves growing on a branch?
forquestions asking time.
There are different kindsof questions, which are asking not the time ofspecific event but the time or season of the en-tire story.
For example ?When is this storyabout??
In this case, the question can?t be an-swered by just extracting a noun phrase.However, at the moment, we can?t concludeif the question can or cannot be answered with-out really understanding it.
Sometime, we canfind a correct answer without reading the storydown the line or understanding the story per-fectly.
It is one of the future works.6.5 Other techniques: discourse andanaphoraSome techniques other than morphologicalanalysis, frequency of appearance in a corpus,and question answering methods are used in oursystem.
We raise two issues.
One of those is thediscourse analysis.
It is required in the questionsto assign the order of paragraphs, and to selectappropriate sentence conjunction.
The other isanaphora analysis, which is very important, notonly to indicate the antecedent, but also to findthe link of mentions of entities.7 Conclusionseveral inter-esting NLP problems were found.HiComprehension system?.Cher-based Language UnderstandingK.ofK.atural Language Processing, 2004,We develop a system to solve questions of sec-ond grade language tests.
Our objectives are todemonstrate the NLP technologies to ordinarypeople, and to observe the problems of NLP bydegrading the level of target materials.
Weachieved 55% - 83% accuracy andReferencesrschman, L., Light, M., Breck, E. and Burger, J. D.?Deep READ: a ReadingACL, 1999, pp 325-332.arniak et al, ?Reading Comprehension Programsin a Statistical-language-Processing Class?.
Work-shop on Reading Comprehension Tests as Evalua-tion for ComputSystems.
2000.Matsui: ?Search Technologies on WWW whichutilize search engines?.
(In Japanese) JournalJapanese Language, February, 2004, pp 34-43.Yoshihira, Y. Takeda, S. Sekine: ?KWIC Systemon WEB documents?, (In Japanese) 10th AnnualMeeting of Npp 137-139.F(http://languagecraft.jp/dennou/)igure 1.
A Snapshot of the system48
