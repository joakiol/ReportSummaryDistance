Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1291?1301, Dublin, Ireland, August 23-29 2014.Multilingual Semantic Parsing :Parsing Multiple Languages into Semantic RepresentationsZhanming JieUniversity of Electronic Science& Technology of Chinaallanmcgrady@gmail.comWei LuInformation Systems Technology and DesignSingapore University of Technology and Designluwei@sutd.edu.sgAbstractWe consider multilingual semantic parsing ?
the task of simultaneously parsing semanticallyequivalent sentences from multiple different languages into their corresponding formal semanticrepresentations.
Our model is built on top of the hybrid tree semantic parsing framework, wherenatural language sentences and their corresponding semantics are assumed to be generated jointlyfrom an underlying generative process.
We first introduce a variant of the joint generative pro-cess, which essentially gives us a new semantic parsing model within the framework.
Based onthe different models that can be developed within the framework, we then investigate several ap-proaches for performing the multilingual semantic parsing task.
We present our evaluations on astandard dataset annotated with sentences in multiple languages coming from different languagefamilies.1 IntroductionSemantic parsing, the task of parsing natural language sentences into their formal semantic representa-tions (Mooney, 2007) is one of the most important tasks in the field of natural language processing andartificial intelligence.
This area of research recently has received a significant amount of attention (Zettle-moyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones etal., 2012b).
Consider these example sentence-semantics pairs:English: Which states have points that are higher than the highest point in Texas ?Semantics: answer(state(loc1(place(higher2(highest(place(loc2(stateid(?TX?
)))))))))English: What rivers do not run through Tennessee ?Semantics: answer(exclude(river(all), traverse2(stateid(?TN?
))))In the typical setting, the semantic parser learns from a collection of such sentence-semantics pairs amodel that can parse novel input sentences into their respective semantic representations.
Such semanticrepresentations can then be used to interact with certain downstream components to perform interestingtasks.
For example, retrieving of answers from an underlying database, or performing certain actionsbased on the generated executable semantic instructions.Note that in the training data, although complete sentence-semantics pairs are given, specific word-level semantic information is not explicitly provided.
The model therefore needs to automatically learnsuch latent mappings between natural language words/phrases and semantic units.One natural assumption is that the semantics exhibit certain restricted structures, such as the recursivetree structures.
Under such an assumption, one can convert the second semantics appeared above as thetree structure illustrated in Figure 1.
More details about such tree structured representations will be givenin Section 2.1.Currently, researchers only focused on the semantic parsing task under a single language setting wherethe input is a sentence from one particular language.
However, natural language is highly ambiguous,and identifying the correct semantics associated with words with limited background information is aThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1291QUERY : answer(RIVER)RIVER : exclude(RIVER, RIVER)RIVER : traverse(STATE)STATE : stateid(STATENAME)STATENAME : (?tn?
)RIVER : river(all)What rivers do not run through Tennessee ????????????
?Welche Fl?usse flie?en nicht durch Tennessee ?Figure 1: An example tree-structured semantic representation (above) and its corresponding natural language sentences (inEnglish, Chinese and German).challenging task.
Researchers resorted to performing context-dependent semantic parsing to alleviatesuch an issue (Zettlemoyer and Collins, 2009).On the other hand, researchers have successfully exploited parallel texts for improved word-level se-mantic processing (Chan and Ng, 2005).
This is because words from different languages that conveythe same semantics can be used to disambiguate each other?s semantics.
In fact, texts from differentlanguages that convey the same semantic information becomes increasingly available nowadays.
Webcrawlers such as Google and Yahoo!
are able to rapidly aggregate a large volume of news stories ev-ery day.
One crucial fact is that many such news articles written in different languages are actually alldiscussing the same underlying story and therefore convey similar or identical semantic information.
Tobuild better automatic systems for improved natural language understanding, it is therefore helpful todevelop algorithms that can simultaneously process the underlying semantic information associated withall these documents coming from different language sources together.
For example, consider the fol-lowing example taken from the multilingual version of the dataset, which shows semantically equivalentsentences from three different languages and their corresponding semantics:English: What rivers do not run through Tennessee ?Chinese: ??????????
?German: Welche Fl?usse flie?en nicht durch Tennessee ?Semantics: answer(exclude(river(all), traverse2(stateid(?TN?
))))As a step towards the above-mentioned goal, this work focuses on the development of an automatedsystem that is capable of simultaneously parsing semantically equivalent natural language texts in differ-ent languages into their underlying semantics.Specifically, in this work, we first introduce a new variant of a semantic parsing model under anexisting framework.
This new variant can be used together with other models for jointly making semanticparsing predictions, leading to an improved multilingual semantic parsing system.
We demonstrate theeffectiveness of this new variant through experiments.
Although bilingual parsing has been extensivelystudied in fields such as statistical machine translation (Wu, 1997; Chiang, 2007), to the best of ourknowledge, bilingual or multilingual semantic parsing that focuses on parsing sentences from multipledifferent languages into their formal semantic representations has not yet been studied.
We present thevery first work on performing multilingual semantic parsing that simultaneously parses semanticallyequivalent sentences from multiple different languages into their semantics.
We believe this line of workcan potentially lead to further developments and advancements in areas such as multilingual semanticprocessing and semantics-based machine translations (Jones et al., 2012a).2 Background2.1 SemanticsResearchers have focused on various semantic formalisms for semantic parsing.
Popular examplesinclude the tree-structured semantic representations (Wong and Mooney, 2006; Kate and Mooney,1292QUERY : answer(RIVER)?RIVER : exclude(RIVER, RIVER)RIVER : traverse(STATE)STATE : stateid(STATENAME)STATENAME : (?TN?
)Tennesseerun throughdo notRIVER : river(all)riversWhatFigure 2: An example hybrid tree.
Such a hybrid tree is generated from the generative process, and captures the correspon-dences between natural language words and semantic units.2006), the lambda calculus expressions (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007),and dependency-based semantic representations (DCS) (Liang et al., 2013).
In this work, we specificallyfocus on the tree-structured representations for semantics.Each semantic representation consists of semantic units as its tree nodes, where each semantic unit isof the following form:ma?
?a: p?(?b?)
(1)Heremais used to denote a complete semantic unit, which consists of its semantic type ?a, its functionsymbol p?, as well as a list of types for argument semantic units ?b?
(here ?
means 0, 1, or 2; we assumethere are at most two arguments for each semantic unit).
In other words, each semantic unit can beregarded as a function which takes in other semantic representations of specific types as arguments, andreturns a new semantic representation of a particular type.
For example, in Figure 1, the semantic unit atthe root has a type QUERY, a function name answer, and a single argument type RIVER.2.2 Related WorkSubstantial research efforts have focused on building monolingual semantic parsing systems.
We surveyin this section several of them.WASP (Wong and Mooney, 2006) is a model motivated by statistical synchronous parsing-based ma-chine translation (Chiang, 2007), which essentially casts the semantic parsing problem as a phrase-basedtranslation problem (Koehn et al., 2003).
KRISP (Kate and Mooney, 2006) makes use of Support VectorMachines with string kernels (Lodhi et al., 2002) to recursively map contiguous word sequences intosemantic units to construct a tree structure.
The SCISSOR model (Ge and Mooney, 2005) performs in-tegrated semantic and syntactic parsing.
The model parses natural language sentences into semanticallyaugmented parse trees whose nodes consist of both semantic and syntactic labels and then builds seman-tic representations based on such augmented trees.
The hybrid tree model (Lu et al., 2008; Lu et al.,2009), whose code is publicly available, makes the assumption that there exists an underlying generativeprocess for jointly producing both the language and semantics.
The model employs efficient dynamicprogramming algorithms for learning a distribution over the latent hybrid trees which jointly encode bothlanguage and semantics.
An example hybrid tree representation is shown in Figure 2.
Jones et al.
(2012b)recently proposed a framework that performs semantic parsing with tree transducers.
The model learnsrepresentations that are similar to the hybrid tree structures using a generative process under a Bayesianframework.Besides these approaches, recently there are also several works that take alternative learning ap-proaches for semantic parsing which do not require annotated semantic representations (Poon andDomingos, 2009; Clarke et al., 2010; Goldwasser et al., 2011; Liang et al., 2013; Artzi and Zettle-moyer, 2013).
Most of such approaches rely on either weak supervision or certain forms of indirectsupervision.
Some of these works also focus on optimizing specific downstream tasks rather than thesemantic parsing task itself.1293maw9w10mbw7w8mdw6w3w4w5mcw1w2mambw10mdw8w9w7mcw6w4w5w1w2w3Figure 3: Two example hybrid trees.
Their leaves are natural language words, and the internal nodes are semantic units.
Bothhybrid trees correspond to the same n-m pair ?w1w2w3w4w5w6w7w8w9w10,ma(mb(mc,md))?.
Thus they can beviewed as two different ways of generating such a pair from the joint generative process.We note there also exist various multilingual or cross-lingual semantic processing works.
Most ofsuch works focus on semantic role labeling(SRL), the task of recovery of shallow meaning.
Examplesinclude multilingual semantic role labeling (Bj?orkelund et al., 2009), multilingual joint syntactic and se-mantic dependency parsing (Henderson et al., 2013), and cross-lingual transfer of semantic role labelingmodels (Kozhevnikov and Titov, 2013).
Researchers also looked into exploiting semantic informationfor bilingual processing such as machine translations (Chan et al., 2007; Carpuat and Wu, 2007; Jones etal., 2012a).In this work, we focus on the task of multilingual semantic parsing under the setting where the in-put consists of semantically equivalent sentences from multiple different languages, and the outputsare formal semantic representations.
We specifically focus on the hybrid tree model, a state-of-the-artframework for semantic parsing.
We first make an extension to the model, and investigate methods forperforming such a multilingual semantic parsing task by aggregating a few variants of the models undersuch a framework.3 ApproachIn this section, we first discuss the hybrid tree model of Lu et al.
(2008), and introduce a novel extension.Next we discuss the approach used for multilingual semantic parsing.3.1 The Hybrid Tree ModelFor a given n-m pair (where n is a complete natural language sentence, and m is a complete semanticrepresentation), the hybrid tree model assumes that both n and m are generated from an underlyinggenerative process in a top-down, left-to-right, level-by-level, recursive manner.
The joint generativeprocess for the pair results in a new tree-structured representation called a hybrid tree, which consists ofnatural language words as leaves, and semantic units as internal nodes.There are three types of model parameters involved in the generative process.
The meaning repre-sentation model parameters (?)
are used for generating one semantic unit from its parent semantic unit.The hybrid pattern parameters (?)
are used for deciding how natural language words and semantic unitsare organized together to form the next level of the nodes of the hybrid tree structure.
The emissionparameters (?)
are used for generating natural language words from its corresponding semantic unit.For a given n-m pair, there are multiple possible hybrid trees that can jointly represent such a pair.See Figure 3 for two possible hybrid trees that contain the same n-m pair.
Consider the first examplehybrid tree illustrated there.
The probability of generating such a hybrid tree h (i.e., jointly generatingboth the natural language sentence n and the semantics m) is:P (n,m,h) = ?(ma)?
?(Xw|ma)?
?(X|ma,?)?
?(w9|ma,?)?
?(w10|ma,?)??
(mb|ma, arg = 1)?
?(XwYw|mb)?
?(X|mb,?)?
?(w3|mb,?)??(w4|mb,?)?
?(w5|mb,?)?
?(Y|mb,?)?
?(w7|mb,?)?
?(w8|mb,?)??
(mc|mb, arg = 1)?
?(w|mc)?
?(w1|mc,?)?
?(w2|mc,?)??
(md|mb, arg = 2)?
?(w|md)?
?(w6|md,?)
(2)1294Note that Xw refers to a pattern which says the next level of the hybrid tree is expected to consistof the first child semantic unit, followed by a contiguous sequence of natural language words.
Similardefinitions can be given to the patterns XwYw and w, where X and Y refer to the first and secondchild semantic unit, respectively.
The symbols X and Y appear in emission parameters are used todenote placeholders for the first and second child semantic unit, respectively.The hybrid tree model then focuses on the learning of these model parameters from the training datausing maximum likelihood estimation.
In other words, the model tries to maximize:?ilogP (ni,mi; ?, ?, ?)
=?ilog?hP (ni,mi,h; ?, ?, ?)
(3)Since the correct hybrid tree associated with n-m pair is unknown, we marginalize over the hiddenvariable h. The model parameters will then be estimated using the Expectation-Maximization (EM)algorithm (Dempster et al., 1977).
Specifically, an inside-outside style algorithm (Baker, 2005) is usedwhere an additional layer of dynamic programming algorithms are used for efficient inference (Lu etal., 2008).
The complexity of the inference algorithm is O(mn3), where m is the size of the semanticrepresentation (number of semantic units), and n is the number of words in the input sentence.Note that the generation of natural language words involves the context ?.
Specifically, if the contextis empty, the model is regarded as the unigram model.
If the context is the previously generated word, themodel is called a bigram model.
For example, consider the generation of the natural language word w4inthe left hybrid tree in Figure 2.
The probability for generating this word is ?
(w4|mb) and ?
(w4|mb, w3),under the unigram and the bigram model, respectively.
In Lu et al.
(2008), the mixgram model (aninterpolation between the unigram model and the bigram model) was also considered when parsingnovel sentences, which yielded a better performance.Once the model parameters are learned, we will be able to use them to parse novel sentences.
Specifi-cally, for each novel input sentence, we first find the most probable hybrid tree that contains the sentencen, and then extract its internal nodes to form the semantic representation.
Efficient dynamic program-ming algorithms similar to the ones used for training can also be employed here.
In addition, the algo-rithm can also be extended to support exact top-k decoding, which will be useful later for combiningmultiple lists of outputs with rank aggregation (to be discussed in Sec.
3.3).3.2 The Backward Bigram ModelOne assumption associated with the original hybrid tree model is that nodes at each level of the hybridtree are generated from the left to the right.
An alternative assumption would be that the nodes at eachlevel are generated in the reverse order ?
from the right to the left.
While this alternative assumptionwill not introduce any difference in the unigram model (since each node is generated from its respectiveparent semantic unit only, regardless of its context), such a new assumption will lead to a completelynew generative process under the bigram assumption.To see this, again consider the emission probability for generating the word w4in the hybrid tree onthe left of Figure 3.
Under the assumption of our new model, the probability of generating this word is?
(w4|mb, w5), since now the context ?
becomes the word to the right of the current word.
The parameterestimation and parsing (decoding) procedures are largely similar to those of the original bigram model,where similar efficient dynamic programming algorithms can be employed.3.3 Multilingual Semantic ParsingIn multilingual semantic parsing, the input consists of multiple semantically equivalent sentences, eachof which is from a different language.
One approach for building such a multilingual semantic parsingsystem is to develop a joint generative process from which both the semantic representations and thesentences in different languages are generated simultaneously.
However, building such a joint modelis non-trivial.
Typically, sentences from different languages exhibit very different syntactic structuresand word orderings.
It is also non-trivial to design efficient dynamic programming algorithms for thiscase where multiple languages are involved in the joint generative process.
Furthermore, the difficultyof building such a joint generative model becomes higher as the number of input languages increases.1295Previous research efforts show that it can be beneficial to learn individual models independently, andthen combine the learned models only during the inference stage (Punyakanok et al., 2005; Chang et al.,2012).
Motivated by this, we take the approach that learns a separate semantic parser for each differentlanguage first.
Next, we combine these semantic parsers for different languages into a single multilingualsemantic parser only during the inference stage.One common approach for combining different outputs from different systems is to perform majorityvoting based on optimal predictions from each parser.
We first obtain the best output semantic represen-tation from each individual semantic parser, and then count the number of occurrences for each possibleoutput.
The most frequent output semantic representation is returned as the final output of our system.Naturally, this approach is only applicable when there are at least three systems/models.An alternative approach is to allow each system to produce a ranked list of k most probable outputs,each is associated with a score.
Our system then aggregates these ranked lists to select the best output.This problem is known as rank aggregation and has been extensively studied in fields such as data miningand information retrieval (Dwork et al., 2001; Gleich and Lim, 2011; Li, 2011).
For our task, we firstlet each semantic parser (for each language) generate a ranked list of the top-k most probable outputs(hybrid trees) for the given input.
Next, based these hybrid trees we find a ranked list of most probablesemantic representations.
Each such semantic representation is also associated with a score, which isthe log-likelihood of the hybrid tree, i.e., logP (n,m,h).
Note that for each semantic representation,we only consider the score associated with the most probable hybrid tree that contains such a semanticrepresentation.
We use the standard approach for combining two ranked lists with scores.
Consider aranked list from the j-th model/system that consists of n distinct items.
Let?s use s(j)ito denote theoriginal score associated with the i-th semantic representation in the j-th ranked list.
We normalize thescore s(j)iin the following way to obtain the new score s?
(j)i(normalized score, divided by the standarddeviation associated with the sample):s?(j)i=s(j)in?(j)?
(j)where ?
(j)=1n?nk=1s(j)k, ?(j)=????1n?
1n?k=1(s(j)k?
?
(j))2Such new scores will then be used for aggregating the results to form a new ranked list.
How do wefind the best output from multiple lists?
Two useful sources of information that we may use include: 1)the number of times each output appears in these lists; 2) the combined score?js?
(j)for each output s.We believe the more frequent an output appears in these lists (i.e., more systems/models predict such anoutput in their top-k lists), the more likely it can be a good candidate.
Therefore we first find the set ofmost frequent outputs, next from such a set we select the output with the highest overall score?js?
(j)as the final output of our system.4 Experiments4.1 Data and SetupWe conducted our experiments on the multilingual GEOQUERY dataset released by Jones et al.
(2012b).This dataset consists of 880 instances of natural language queries related to US geography facts.
Eachquery is coupled with its corresponding semantic representation originally written in Prolog.
The origi-nal GEOQUERY dataset (Wong and Mooney, 2006; Kate and Mooney, 2006) contains natural languagequeries in English only.
Additional Chinese annotations were provided by Lu and Ng (2011) when per-forming a natural language generation task.
Jones et al.
(2012b) further provided the following threeadditional language annotations to this dataset: German, Greek and Thai.
Thus, this dataset is now fullyannotated with five different languages, two of which (Chinese, Thai) are Sino-Tibetan languages, andthe rest are all Indo-European languages.Following previous works on semantic parsing (Kwiatkowski et al., 2010; Jones et al., 2012b), wesplit the dataset into two portions.
The training set consists of 600 instances, and we report evaluationresults on the portion consisting of the remaining 280 instances.
We used the identical split provided byJones et al.
(2012b) for all the experiments.
Following previous works, we used the standard approach for1296EN DE EL TH CNUnigram 70.0 59.6 70.0 68.9 68.9Bigram 75.4 56.1 65.4 70.7 68.9Bigram (inv) 74.3 57.1 65.4 71.1 66.8Mixgram 76.1 62.5 69.3 73.2 70.7Voting (u,b,m) 76.1 61.1 70.4 73.6 70.0Voting (u,b,bi) 76.4 61.4 71.8 74.3 72.1Aggregation 78.6 60.0 72.1 71.4 73.2Table 1: Monolingual semantic parsing results on all five languages (EN:English, DE:German, EL:Greek, TH:Thai,CN:Chinese.).
We report accuracy percentages in this table.ENDE ENEL ENTH ENCN DEEL DETH DECN ELTH ELCN THCNUnigram 74.6 76.1 76.4 75.0 76.8 72.1 74.3 80.4 79.6 74.0Bigram 80.0 77.9 87.1 78.2 72.1 75.0 76.4 81.4 76.8 79.6Bigram (inv) 78.2 76.8 86.4 75.7 72.5 75.7 76.1 82.1 75.7 79.3Mixgram 77.9 76.4 82.5 81.1 76.1 75.7 74.3 81.1 80.7 77.9Voting (u,b) 80.0 79.6 83.6 82.1 77.1 74.6 74.6 82.1 78.6 79.6Voting (u,b,bi) 82.1 79.3 86.4 82.1 76.8 77.1 76.4 85.4 78.9 80.7Aggregation 78.9 82.1 85.7 83.6 76.4 73.6 76.8 83.9 81.4 79.3Table 2: Semantic parsing results when two different input languages are considered (for example, the column ENDE gives theresults when each input to our system consists of a pair of semantically equivalent sentences written in English and German.
).Scores are accuracy percentages.evaluation on the multilingual GEOQUERY dataset.
Specifically, we first let our semantic parsers producesemantic representations from multilingual input sentences.
The resulting semantic representations arethen converted into Prolog queries in a deterministic manner, which can be used to interact with theunderlying knowledge base to retrieve answers.
A predicted semantic representation is considered correctif and only if it retrieves identical results as the correct reference semantic representation when both areused for retrieving answers from the underlying database.4.2 Results and DiscussionsWe performed experiments on the conventional monolingual semantic parsing task first.
We report accu-racy scores, which are defined as the number of correctly parsed inputs (i.e., the total number of correctsemantic representations) divided by the total number of input sentences.
Baseline results for unigram,bigram, and mixgram models, which are originally introduced in Lu et al.
(2008) are reported under?Unigram?, ?Bigram?, and ?Mixgram?
respectively in Table 1.
The results for backward bigram modelsare reported under ?Bigram(inv)?.To assess the effectiveness of our methods for combining different outputs, we first conducted ex-periments on voting over the outputs from the three models originally introduced in the work of Lu etal.
(2008) (Voting(u,b,m)).
Next we performed voting over outputs from unigram model, bigram model,as well as the backward bigram model introduced in this paper (Voting(u,b,bi)).
These voting-basedapproaches yielded better results over the first voting-based approach.
Specifically, we compared thisnew voting-based approach against the previous best model reported in Lu et al.
(2008) ?
mixgrammodel, which was also based on a combination of unigram and bigram models.
We used the pairedt-test to assess the significance of the overall improvements across different languages when using ournew method.
When comparing the approach ?Voting(u,b,m)?
over ?Mixgram?, we obtained a one-tailedp-value of 0.40.
When comparing the approach ?Voting(u,b,bi)?
over ?Mixgram?, we obtained a one-tailed p-value of 0.11.
We also investigated the effectiveness of the aggregation-based approach.
Thisapproach is based on aggregating the two top-100 lists generated by unigram, bigram and backwardbigram models.
When comparing this approach over ?Mixgram?, we obtained a one-tailed p-value of1297ENDE ENDE ENDE ENEL ENEL ENTH DEEL DEEL DETH ELTHEL TH CN TH CN CN TH CN CN CNUnigram 79.6 78.2 79.3 83.2 83.2 79.3 81.8 79.6 77.1 81.4Bigram 82.1 85.7 81.8 87.5 81.8 86.4 82.5 80.7 79.6 83.6Bigram (inv) 82.9 85.4 79.6 86.8 81.1 85.4 82.1 80.4 78.9 83.2Mixgram 81.4 83.2 81.8 85.0 83.2 84.3 82.9 80.7 79.3 82.9Voting (u,b) 83.2 85.0 84.3 87.9 84.0 85.0 84.0 83.6 81.1 84.6Voting (u,b,bi) 84.0 86.1 85.4 89.6 84.3 86.8 85.0 82.5 81.1 84.6Aggregation 83.6 85.0 85.4 88.9 87.1 85.7 82.5 82.5 80.0 85.4Table 3: Semantic parsing results when three different input languages are considered (for example, the column ENDEEL givesthe results when each input to our system consists of three semantically equivalent sentences, which are written in English,German and Greek, respectively.).
Scores are accuracy percentages.ENDE ENDE ENDE ENEL DEEL ENDEELELTH ELCN THCN THCN THCN THCNUnigram 82.9 82.1 81.1 85.0 82.1 84.0Bigram 86.1 83.6 84.3 87.1 85.0 86.1Bigram (inv) 86.4 82.5 84.0 86.8 85.4 85.0Mixgram 84.0 82.1 83.2 86.4 84.0 85.7Voting (u,b) 87.5 86.1 86.4 89.6 86.4 89.3Voting (u,b,bi) 88.6 86.8 87.1 90.0 85.7 89.6Aggregation 87.1 87.1 86.1 88.9 86.1 88.6Table 4: Semantic parsing results when four or five different input languages are considered (for example, the columnENDEELTH gives the results when each input to our system consists of four semantically equivalent sentences, which arewritten in English, German, Greek, and Thai respectively.).
Scores are accuracy percentages.0.29 under the paired t-test.
These results indicate that the approach based on voting over the unigram,bigram and backward bigram models gives the most promising results for monolingual semantic parsing,demonstrating the usefulness of our proposed backward bigram model.Next we move to the multilingual setting where we would like to simultaneously process more thantwo languages.
Specifically, we considered multilingual semantic parsing where there are two, three,four and five input languages.
Table 2, Table 3, and Table 4 summarize these results.
Table 2 shows theresults for bilingual semantic parsing where we have two different input languages.
The results reportedunder ?Unigram?
are based on the aggregation approach over unigram models.
Similarly for ?Bigram?,?Bigram(inv)?, and ?Mixgram?
(we also tried the voting-based approach for combining such baselinesystems, which yielded slightly worse results).
From this table we can see that generally speaking byconsidering two different languages as the input, our system is able to do better semantic parsing.
Wecompared the voting-based approaches against the baseline approaches.
For the approach ?Voting(u,b)?
(we excluded mixgram models in voting since now we have four models, two from each language, whichare sufficient for voting, and preliminary results show that the inclusion of the mixgram models is nothelpful), it does not outperform the bigram baseline approach (which is the most competitive amongstall baseline approaches) significantly (p = 0.19).
When comparing the aggregation approach against thebigram baseline approach, we obtain p = 0.04.
In contrast, the approach ?Voting(u,b,bi)?
outperformsall the baseline systems significantly (p < 0.005).
These results again demonstrate the effectiveness ofour newly proposed backward bigram model.We can see from the results presented in Table 3 and Table 4 that, in general, the performance of themultilingual semantic parser tends to improve as the number of input languages increases.
However thisis not always the case.
For example, consider the final system where we use all five languages as the input(refer to the results in the column of ENDEELTHCN in Table 4); interestingly, when we remove German(DE) from the inputs, we are able to build a better system in terms of accuracy (refer to the results in thecolumn of ENELTHCN).
We believe this is partly due to the fact that the monolingual semantic parsing1298task with German as the input language (see DE in Table 1) is relatively more challenging.
Nevertheless,when all the languages are considered, the overall system is able to obtain an accuracy of 89.6% with thevoting-based approach where our proposed backward bigram model is incorporated.
This is significantlyhigher than any other monolingual system?s performance reported in the literature.
According to Joneset al.
(2012b), the results of state-of-the-art monolingual semantic parsing systems on four of these fivelanguages considered here are: 82.1%(EN), 75.0%(DE), 75.4%(EL), and 78.2%(TH).
Note that to date,no single system reported in the literature can dominate all other systems across all these languages onthis dataset in terms of accuracy performance.
We hypothesize that this is because semantic informationconveyed by the sentences from a single language tends to be highly ambiguous, and various linguis-tic phenomenons can be difficult to capture under a monolingual setting for any existing monolingualsemantic parsing system.
The multilingual semantic parsing system introduced in this work, in con-trast, can exploit richer information from multiple languages to successfully disambiguate the semanticsassociated with the inputs for improved semantic parsing.5 ConclusionsIn this work, we focused on multilingual semantic parsing, the task of simultaneously parsing sentencesfrom various different languages into their corresponding formal semantic representations.
Our work isbuilt on top of the hybrid tree framework where different generative process can be developed for jointlymodelling the generation of both language and semantics.
We first introduced a variant of the generativeprocess, leading to a new semantic parsing model.
Next we presented methods for combining and ag-gregating outputs from different models within the framework to build our multilingual semantic parsingsystem.
Our results demonstrate the effectiveness of our approaches for such a task.
To the best of ourknowledge, this is the first work that tackles such a multilingual semantic parsing task which simulta-neously parses sentences from multiple languages into formal semantic representations.
Future workinclude explorations on applications of our system in areas such as multilingual semantic processing,cross-lingual semantic processing, and semantics-based machine translations (Jones et al., 2012a).AcknowledgementsWe would like to thank the anonymous reviewers for comments.
This work was conducted during thefirst author?s internship at SUTD.
This work was supported by SUTD grant SRG ISTD 2013 064.ReferencesYoav Artzi and Luke Zettlemoyer.
2013.
Weakly supervised learning of semantic parsers for mapping instructionsto actions.
TACL.James K Baker.
2005.
Trainable grammars for speech recognition.
The Journal of the Acoustical Society ofAmerica, 65(S1):S132?S132.Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009.
Multilingual semantic role labeling.
In Proceedingsof CONLL?09, pages 43?48.Marine Carpuat and Dekai Wu.
2007.
Improving statistical machine translation using word sense disambiguation.In Proceedings of EMNLP-CoNLL ?07, pages 61?72.Yee Seng Chan and Hwee Tou Ng.
2005.
Scaling up word sense disambiguation via parallel texts.
In Proceedingsof AAAI ?05, pages 1037?1042.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.
Word sense disambiguation improves statistical machinetranslation.
In Proceedings of ACL ?07.Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2012.
Structured learning with constrained conditional models.Machine learning, 88(3):399?431.David Chiang.
2007.
Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.1299James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth.
2010.
Driving semantic parsing from the world?sresponse.
In Proceedings of CONLL ?10, pages 18?27.Arthur P Dempster, Nan M Laird, Donald B Rubin, et al.
1977.
Maximum likelihood from incomplete data viathe em algorithm.
Journal of the Royal statistical Society, 39(1):1?38.Cynthia Dwork, Ravi Kumar, Moni Naor, and Dandapani Sivakumar.
2001.
Rank aggregation methods for theweb.
In Proceedings of WWW ?01, pages 613?622.Ruifang Ge and Raymond J. Mooney.
2005.
A statistical semantic parser that integrates syntax and semantics.
InProceedings of CONLL ?05, pages 9?16.David F Gleich and Lek-Heng Lim.
2011.
Rank aggregation via nuclear norm minimization.
In Proceedings ofKDD ?11, pages 60?68.Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth.
2011.
Confidence driven unsupervised semanticparsing.
In Proceedings of ACL ?11, pages 1486?1495.James Henderson, Paola Merlo, Ivan Titov, and Gabriele Musillo.
2013.
Multi-lingual joint parsing of syntacticand semantic dependencies with a latent variable model.
Computational Linguistics, 39(4).Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight.
2012a.
Semantics-basedmachine translation with hyperedge replacement grammars.
In Proceedings of COLING ?12, pages 1359?1376.Bevan Keeley Jones, Mark Johnson, and Sharon Goldwater.
2012b.
Semantic parsing with bayesian tree transduc-ers.
In Proceedings of ACL ?12, pages 488?496.Rohit J. Kate and Raymond J. Mooney.
2006.
Using string-kernels for learning semantic parsers.
In Proceedingsof COLING/ACL ?06, pages 913?920.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Statistical phrase-based translation.
In Proceedings ofNAACL ?03, pages 48?54.Mikhail Kozhevnikov and Ivan Titov.
2013.
Cross-lingual transfer of semantic role labeling models.
In Proceed-ings of ACL ?13.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman.
2010.
Inducing probabilistic ccggrammars from logical form with higher-order unification.
In Proceedings of EMNLP?10, pages 1223?1233.Hang Li.
2011.
Learning to rank for information retrieval and natural language processing.
Synthesis Lectures onHuman Language Technologies, 4(1):1?113.Percy Liang, Michael I Jordan, and Dan Klein.
2013.
Learning dependency-based compositional semantics.Computational Linguistics, 39(2):389?446.Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins.
2002.
Text classificationusing string kernels.
The Journal of Machine Learning Research, 2:419?444.Wei Lu and Hwee Tou Ng.
2011.
A probabilistic forest-to-string model for language generation from typedlambda calculus expressions.
In Proceedings of EMNLP ?11, pages 1611?1622.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer.
2008.
A generative model for parsing naturallanguage to meaning representations.
In Proceedings of EMNLP ?08, pages 783?792.Wei Lu, Hwee Tou Ng, and Wee Sun Lee.
2009.
Natural language generation with tree conditional random fields.In Proceedings of EMNLP ?09, pages 400?409.Raymond J. Mooney.
2007.
Learning for semantic parsing.
In Computational Linguistics and Intelligent TextProcessing, pages 311?324.
Springer.Hoifung Poon and Pedro Domingos.
2009.
Unsupervised semantic parsing.
In Proceedings of EMNLP ?09, pages1?10.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak.
2005.
Learning and inference over constrainedoutput.
In Proceedings of IJCAI ?05, pages 1124?1129.Yuk Wah Wong and Raymond J. Mooney.
2006.
Learning for semantic parsing with statistical machine translation.In Proceedings of HLT/NAACL ?06, pages 439?446.1300Yuk Wah Wong and Raymond J. Mooney.
2007.
Learning synchronous grammars for semantic parsing withlambda calculus.
In Proceedings of ACL ?07.Dekai Wu.
1997.
Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.
Computa-tional Linguistics, 23(3):377?403.Luke S Zettlemoyer and Michael Collins.
2005.
Learning to map sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Proceedings of UAI ?05.Luke S Zettlemoyer and Michael Collins.
2009.
Learning context-dependent mappings from sentences to logicalform.
In Proceedings of ACL/IJCNLP ?09, pages 976?984.1301
