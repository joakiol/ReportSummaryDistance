Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 17?24, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSearch Engine Statistics Beyond the n-gram:Application to Noun Compound BracketingPreslav NakovEECS, Computer Science DivisionUniversity of California, BerkeleyBerkeley, CA 94720nakov@cs.berkeley.eduMarti HearstSIMSUniversity of California, BerkeleyBerkeley, CA 94720hearst@sims.berkeley.eduAbstractIn order to achieve the long-range goalof semantic interpretation of noun com-pounds, it is often necessary to first de-termine their syntactic structure.
This pa-per describes an unsupervised method fornoun compound bracketing which extractsstatistics from Web search engines using a?2 measure, a new set of surface features,and paraphrases.
On a gold standard, thesystem achieves results of 89.34% (base-line 66.80%), which is a sizable improve-ment over the state of the art (80.70%).1 IntroductionAn important but understudied language analy-sis problem is that of noun compound bracketing,which is generally viewed as a necessary step to-wards noun compound interpretation.
Consider thefollowing contrastive pair of noun compounds:(1) liver cell antibody(2) liver cell lineIn example (1) an antibody targets a liver cell, while(2) refers to a cell line which is derived from theliver.
In order to make these semantic distinctionsaccurately, it can be useful to begin with the cor-rect grouping of terms, since choosing a particularsyntactic structure limits the options left for seman-tics.
Although equivalent at the part of speech (POS)level, these two noun compounds have different syn-tactic trees.
The distinction can be represented as abinary tree or, equivalently, as a binary bracketing:(1b) [ [ liver cell ] antibody ] (left bracketing)(2b) [ liver [cell line] ] (right bracketing)In this paper, we describe a highly accurate un-supervised method for making bracketing decisionsfor noun compounds (NCs).
We improve on the cur-rent standard approach of using bigram estimates tocompute adjacency and dependency scores by intro-ducing the use of the ?2 measure for this problem.We also introduce a new set of surface features forquerying Web search engines which prove highly ef-fective.
Finally, we experiment with paraphrases forimproving prediction statistics.
We have evaluatedthe application of combinations of these features topredict NC bracketing on two distinct collections,one consisting of terms drawn from encyclopediatext, and another drawn from bioscience text.The remainder of this paper describes relatedwork, the word association models, the surface fea-tures, the paraphrase features and the results.2 Related WorkThe syntax and semantics of NCs is an active area ofresearch; the Journal of Computer Speech and Lan-guage has an upcoming special issue on MultiwordExpressions.The best known early work on automated un-supervised NC bracketing is that of Lauer (1995)who introduces the probabilistic dependency modelfor the syntactic disambiguation of NCs and arguesagainst the adjacency model, proposed by Marcus(1980), Pustejovsky et al (1993) and Resnik (1993).Lauer collects n-gram statistics from Grolier?s en-cyclopedia, containing about 8 million words.
To17overcome data sparsity problems, he estimates prob-abilities over conceptual categories in a taxonomy(Roget?s thesaurus) rather than for individual words.Lauer evaluated his models on a set of 244 unam-biguous NCs derived from the same encyclopedia(inter-annotator agreement 81.50%) and achieved77.50% for the dependency model above (baseline66.80%).
Adding POS and further tuning allowedhim to achieve the state-of-the-art result of 80.70%.More recently, Keller and Lapata (2003) evalu-ate the utility of using Web search engines for ob-taining frequencies for unseen bigrams.
They thenlater propose using Web counts as a baseline unsu-pervised method for many NLP tasks (Lapata andKeller, 2004).
They apply this idea to six NLP tasks,including the syntactic and semantic disambigua-tion of NCs following Lauer (1995), and show thatvariations on bigram counts perform nearly as wellas more elaborate methods.
They do not use tax-onomies and work with the word n-grams directly,achieving 78.68% with a much simpler version ofthe dependency model.Girju et al (2005) propose a supervised model(decision tree) for NC bracketing in context, basedon five semantic features (requiring the correctWordNet sense to be given): the top three Word-Net semantic classes for each noun, derivationallyrelated forms and whether the noun is a nominaliza-tion.
The algorithm achieves accuracy of 83.10%.3 Models and Features3.1 Adjacency and Dependency ModelsIn related work, a distinction is often made betweenwhat is called the dependency model and the adja-cency model.
The main idea is as follows.
For agiven 3-word NC w1w2w3, there are two reasons itmay take on right bracketing, [w1[w2w3]].
Either (a)w2w3 is a compound (modified byw1), or (b)w1 andw2 independently modify w3.
This distinction canbe seen in the examples home health care (healthcare is a compound modified by home) versus adultmale rat (adult and male independently modify rat).The adjacency model checks (a), whether w2w3is a compound (i.e., how strongly w2 modifies w3as opposed to w1w2 being a compound) to decidewhether or not to predict a right bracketing.
Thedependency model checks (b), does w1 modify w3(as opposed to w1 modifying w2).Left bracketing is a bit different since there is onlymodificational choice for a 3-word NC.
If w1 modi-fies w2, this implies that w1w2 is a compound whichin turn modifies w3, as in law enforcement agent.Thus the usefulness of the adjacency model vs.the dependency model can depend in part on the mixof left and right bracketing.
Below we show that thedependency model works better than the adjacenymodel, confirming other results in the literature.
Thenext subsections describe several different ways tocompute these measures.3.2 Using FrequenciesThe most straightforward way to compute adjacencyand dependency scores is to simply count the cor-responding frequencies.
Lapata and Keller (2004)achieved their best accuracy (78.68%) with the de-pendency model and the simple symmetric score#(wi, wj).13.3 Computing ProbabilitiesLauer (1995) assumes that adjacency and depen-dency should be computed via probabilities.
Sincethey are relatively simple to compute, we investigatethem in our experiments.Consider the dependency model, as introducedabove, and the NC w1w2w3.
Let Pr(wi ?
wj |wj)be the probability that the word wi precedes agiven fixed word wj .
Assuming that the distincthead-modifier relations are independent, we obtainPr(right) = Pr(w1 ?
w3|w3)Pr(w2 ?
w3|w3)and Pr(left) = Pr(w1 ?
w2|w2)Pr(w2 ?
w3|w3).To choose the more likely structure, we can dropthe shared factor and compare Pr(w1 ?
w3|w3) toPr(w1 ?
w2|w2).The alternative adjacency model comparesPr(w2 ?
w3|w3) to Pr(w1 ?
w2|w2), i.e.
theassociation strength between the last two words vs.that between the first two.
If the first probability islarger than the second, the model predicts right.The probability Pr(w1 ?
w2|w2) can be esti-mated as #(w1, w2)/#(w2), where #(w1, w2) and#(w2) are the corresponding bigram and unigram1This score worked best on training, when Keller&Lapatawere doing model selection.
On testing, Pr (with the depen-dency model) worked better and achieved accuracy of 80.32%,but this result was ignored, as Pr did worse on training.18frequencies.
They can be approximated as the num-ber of pages returned by a search engine in responseto queries for the exact phrase ?w1 w2?
and for theword w2.
In our experiments below we smoothed2each of these frequencies by adding 0.5 to avoidproblems caused by nonexistent n-grams.Unless some particular probabilistic interpreta-tion is needed,3 there is no reason why for a givenordered pair of words (wi, wj), we should usePr(wi ?
wj |wj) rather than Pr(wj ?
wi|wi),i < j.
This is confirmed by the adjacency modelexperiments in (Lapata and Keller, 2004) on Lauer?sNC set.
Their results show that both ways ofcomputing the probabilities make sense: using Al-tavista queries, the former achieves a higher accu-racy (70.49% vs. 68.85%), but the latter is better onthe British National Corpus (65.57% vs. 63.11%).3.4 Other Measures of AssociationIn both models, the probability Pr(wi ?
wj |wj)can be replaced by some (possibly symmetric) mea-sure of association between wi and wj , such as Chisquared (?2).
To calculate ?2(wi, wj), we need:(A) #(wi, wj);(B) #(wi, wj), the number of bigrams in which thefirst word is wi, followed by a word other thanwj ;(C) #(wi, wj), the number of bigrams, ending inwj , whose first word is other than wi;(D) #(wi, wj), the number of bigrams in which thefirst word is not wi and the second is not wj .They are combined in the following formula:?2 =N(AD ?BC)2(A+ C)(B +D)(A+B)(C +D)(1)Here N = A + B + C + D is the total num-ber of bigrams, B = #(wi) ?
#(wi, wj) and C =#(wj) ?#(wi, wj).
While it is hard to estimate D2Zero counts sometimes happen for #(w1, w3), but are rarefor unigrams and bigrams on the Web, and there is no need fora more sophisticated smoothing.3For example, as used by Lauer to introduce a prior for left-right bracketing preference.
The best Lauer model does notwork with words directly, but uses a taxonomy and further needsa probabilistic interpretation so that the hidden taxonomy vari-ables can be summed out.
Because of that summation, the termPr(w2 ?
w3|w3) does not cancel in his dependency model.directly, we can calculate it asD = N?A?B?C.Finally, we estimate N as the total number of in-dexed bigrams on the Web.
They are estimated as 8trillion, since Google indexes about 8 billion pagesand each contains about 1,000 words on average.Other measures of word association are possible,such as mutual information (MI), which we can usewith the dependency and the adjacency models, sim-ilarly to #, ?2 or Pr.
However, in our experiments,?2 worked better than other methods; this is not sur-prising, as ?2 is known to outperform MI as a mea-sure of association (Yang and Pedersen, 1997).3.5 Web-Derived Surface FeaturesAuthors sometimes (consciously or not) disam-biguate the words they write by using surface-levelmarkers to suggest the correct meaning.
We havefound that exploiting these markers, when they oc-cur, can prove to be very helpful for making brack-eting predictions.
The enormous size of Web searchengine indexes facilitates finding such markers fre-quently enough to make them useful.One very productive feature is the dash (hyphen).Starting with the term cell cycle analysis, if we canfind a version of it in which a dash occurs betweenthe first two words: cell-cycle, this suggests a leftbracketing for the full NC.
Similarly, the dash indonor T-cell favors a right bracketing.
The right-hand dashes are less reliable though, as their scopeis ambiguous.
In fiber optics-system, the hyphen in-dicates that the noun compound fiber optics modifiessystem.
There are also cases with multiple hyphens,as in t-cell-depletion, which preclude their use.The genitive ending, or possessive marker is an-other useful indicator.
The phrase brain?s stemcells suggests a right bracketing for brain stem cells,while brain stem?s cells favors a left bracketing.4Another highly reliable source is related to inter-nal capitalization.
For example Plasmodium vivaxMalaria suggests left bracketing, while brain Stemcells would favor a right one.
(We disable this fea-ture on Roman digits and single-letter words to pre-vent problems with terms like vitamin D deficiency,where the capitalization is just a convention as op-posed to a special mark to make the reader think thatthe last two terms should go together.
)4Features can also occur combined, e.g.
brain?s stem-cells.19We can also make use of embedded slashes.
Forexample in leukemia/lymphoma cell, the slash pre-dicts a right bracketing since the first word is an al-ternative and cannot be a modifier of the second one.In some cases we can find instances of the NCin which one or more words are enclosed in paren-theses, e.g., growth factor (beta) or (growth fac-tor) beta, both of which indicate a left structure, or(brain) stem cells, which suggests a right bracketing.Even a comma, a dot or a colon (or any spe-cial character) can act as indicators.
For example,?health care, provider?
or ?lung cancer: patients?are weak predictors of a left bracketing, showingthat the author chose to keep two of the words to-gether, separating out the third one.We can also exploit dashes to words external tothe target NC, as in mouse-brain stem cells, whichis a weak indicator of right bracketing.Unfortunately, Web search engines ignore punc-tuation characters, thus preventing querying directlyfor terms containing hyphens, brackets, apostrophes,etc.
We collect them indirectly by issuing querieswith the NC as an exact phrase and then post-processing the resulting summaries, looking for thesurface features of interest.
Search engines typicallyallow the user to explore up to 1000 results.
We col-lect all results and summary texts that are availablefor the target NC and then search for the surface pat-terns using regular expressions over the text.
Eachmatch increases the score for left or right bracket-ing, depending on which the pattern favors.While some of the above features are clearlymore reliable than others, we do not try to weightthem.
For a given NC, we post-process the returnedWeb summaries, then we find the number of left-predicting surface feature instances (regardless oftheir type) and compare it to the number of right-predicting ones to make a bracketing decision.53.6 Other Web-Derived FeaturesSome features can be obtained by using the over-all counts returned by the search engine.
As thesecounts are derived from the entire Web, as opposedto a set of up to 1,000 summaries, they are of differ-ent magnitude, and we did not want to simply addthem to the surface features above.
They appear as5This appears as Surface features (sum) in Tables 1 and 2.independent models in Tables 1 and 2.First, in some cases, we can query for possessivemarkers directly: although search engines drop theapostrophe, they keep the s, so we can query for?brain?s?
(but not for ?brains?
?).
We then com-pare the number of times the possessive marker ap-peared on the second vs. the first word, to make abracketing decision.Abbreviations are another important feature.
Forexample, ?tumor necrosis factor (NF)?
suggests aright bracketing, while ?tumor necrosis (TN) fac-tor?
would favor left.
We would like to issue exactphrase queries for the two patterns and see whichone is more frequent.
Unfortunately, the search en-gines drop the brackets and ignore the capitalization,so we issue queries with the parentheses removed, asin ?tumor necrosis factor nf?.
This produces highlyaccurate results, although errors occur when the ab-breviation is an existing word (e.g., me), a Romandigit (e.g., IV), a state (e.g., CA), etc.Another reliable feature is concatenation.
Con-sider the NC health care reform, which is left-bracketed.
Now, consider the bigram ?health care?.At the time of writing, Google estimates 80,900,000pages for it as an exact term.
Now, if we try theword healthcare we get 80,500,000 hits.
At thesame time, carereform returns just 109.
This sug-gests that authors sometimes concatenate words thatact as compounds.
We find below that comparingthe frequency of the concatenation of the left bigramto that of the right (adjacency model for concatena-tions) often yields accurate results.
We also tried thedependency model for concatenations, as well as theconcatenations of two words in the context of thethird one (i.e., compare frequencies of ?healthcarereform?
and ?health carereform?
).We also used Google?s support for ?
*?, which al-lows a single word wildcard, to see how often two ofthe words are present but separated from the third bysome other word(s).
This implicitly tries to captureparaphrases involving the two sub-concepts makingup the whole.
For example, we compared the fre-quency of ?health care * reform?
to that of ?health* care reform?.
We also used 2 and 3 stars andswitched the word group order (indicated with rev.in Tables 1 and 2), e.g., ?care reform * * health?.We also tried a simple reorder without insertingstars, i.e., compare the frequency of ?reform health20care?
to that of ?care reform health?.
For exam-ple, when analyzing myosin heavy chain we see thatheavy chain myosin is very frequent, which providesevidence against grouping heavy and chain togetheras they can commute.Further, we tried to look inside the internal inflec-tion variability.
The idea is that if ?tyrosine kinaseactivation?
is left-bracketed, then the first two wordsprobably make a whole and thus the second wordcan be found inflected elsewhere but the first wordcannot, e.g., ?tyrosine kinases activation?.
Alterna-tively, if we find different internal inflections of thefirst word, this would favor a right bracketing.Finally, we tried switching the word order of thefirst two words.
If they independently modify thethird one (which implies a right bracketing), then wecould expect to see also a form with the first twowords switched, e.g., if we are given ?adult malerat?, we would also expect ?male adult rat?.3.7 ParaphrasesWarren (1978) proposes that the semantics of the re-lations between words in a noun compound are of-ten made overt by paraphrase.
As an example ofprepositional paraphrase, an author describing theconcept of brain stem cells may choose to write itin a more expanded manner, such as stem cells inthe brain.
This contrast can be helpful for syntacticbracketing, suggesting that the full NC takes on rightbracketing, since stem and cells are kept together inthe expanded version.
However, this NC is ambigu-ous, and can also be paraphrased as cells from thebrain stem, implying a left bracketing.Some NCs?
meaning cannot be readily expressedwith a prepositional paraphrase (Warren, 1978).
Analternative is the copula paraphrase, as in officebuilding that/which is a skyscraper (right bracket-ing), or a verbal paraphrase such as pain associatedwith arthritis migraine (left).Other researchers have used prepositional para-phrases as a proxy for determining the semantic rela-tions that hold between nouns in a compound (Lauer,1995; Keller and Lapata, 2003; Girju et al, 2005).Since most NCs have a prepositional paraphrase,Lauer builds a model trying to choose between themost likely candidate prepositions: of, for, in, at,on, from, with and about (excluding like which ismentioned by Warren).
This could be problematicthough, since as a study by Downing (1977) shows,when no context is provided, people often come upwith incompatible interpretations.In contrast, we use paraphrases in order to makesyntactic bracketing assignments.
Instead of tryingto manually decide the correct paraphrases, we canissue queries using paraphrase patterns and find outhow often each occurs in the corpus.
We then addup the number of hits predicting a left versus a rightbracketing and compare the counts.Unfortunately, search engines lack linguistic an-notations, making general verbal paraphrases too ex-pensive.
Instead we used a small set of hand-chosenparaphrases: associated with, caused by, containedin, derived from, focusing on, found in, involved in,located at/in, made of, performed by, preventing,related to and used by/in/for.
It is however feasi-ble to generate queries predicting left/right brack-eting with/without a determiner for every preposi-tion.6 For the copula paraphrases we combine twoverb forms is and was, and three complementizersthat, which and who.
These are optionally combinedwith a preposition or a verb form, e.g.
themes thatare used in science fiction.4 Evaluation4.1 Lauer?s DatasetWe experimented with the dataset from (Lauer,1995), in order to produce results comparable tothose of Lauer and Keller & Lapata.
The set consistsof 244 unambiguous 3-noun NCs extracted fromGrolier?s encyclopedia; however, only 216 of theseNCs are unique.Lauer (1995) derived n-gram frequencies fromthe Grolier?s corpus and tested the dependency andthe adjacency models using this text.
To help combatdata sparseness issues he also incorporated a taxon-omy and some additional information (see RelatedWork section above).
Lapata and Keller (2004) de-rived their statistics from the Web and achieved re-sults close to Lauer?s using simple lexical models.4.2 Biomedical DatasetWe constructed a new set of noun compounds fromthe biomedical literature.
Using the Open NLP6In addition to the articles (a, an, the), we also used quanti-fiers (e.g.
some, every) and pronouns (e.g.
this, his).21tools,7 we sentence splitted, tokenized, POS taggedand shallow parsed a set of 1.4 million MEDLINEabstracts (citations between 1994 and 2003).
Thenwe extracted all 3-noun sequences falling in the lastthree positions of noun phrases (NPs) found in theshallow parse.
If the NP contained other nouns, thesequence was discarded.
This allows for NCs whichare modified by adjectives, determiners, and so on,but prevents extracting 3-noun NCs that are part oflonger NCs.
For details, see (Nakov et al, 2005).This procedure resulted in 418,678 different NCtypes.
We manually investigated the most frequentones, removing those that had errors in tokeniza-tion (e.g., containing words like transplan or tation),POS tagging (e.g., acute lung injury, where acutewas wrongly tagged as a noun) or shallow parsing(e.g., situ hybridization, that misses in).
We had toconsider the first 843 examples in order to obtain500 good ones, which suggests an extraction accu-racy of 59%.
This number is low mainly because thetokenizer handles dash-connected words as a singletoken (e.g.
factor-alpha) and many tokens containedother special characters (e.g.
cd4+), which cannotbe used in a query against a search engine and hadto be discarded.The 500 NCs were annotated independently bytwo judges, one of which has a biomedical back-ground; the other one was one of the authors.
Theproblematic cases were reconsidered by the twojudges and after agreement was reached, the set con-tained: 361 left bracketed, 69 right bracketed and70 ambiguous NCs.
The latter group was excludedfrom the experiments.8We calculated the inter-annotator agreement onthe 430 cases that were marked as unambiguousafter agreement.
Using the original annotator?schoices, we obtained an agreement of 88% or 82%,depending on whether we consider the annotations,that were initially marked as ambiguous by one ofthe judges to be correct.
The corresponding valuesfor the kappa statistics were .606 (substantial agree-ment) and .442 (moderate agreement).7http://opennlp.sourceforge.net/8Two NCs can appear more than once but with a differentinflection or with a different word variant, e.g,.
colon cancercells and colon carcinoma cells.4.3 ExperimentsThe n-grams, surface features, and paraphrasecounts were collected by issuing exact phrasequeries, limiting the pages to English and request-ing filtering of similar results.9 For each NC, wegenerated all possible word inflections (e.g., tumorand tumors) and alternative word variants (e.g., tu-mor and tumour).
For the biomedical dataset theywere automatically obtained from the UMLS Spe-cialist lexicon.10 For Lauer?s set we used Carroll?smorphological tools.11 For bigrams, we inflect onlythe second word.
Similarly, for a prepositional para-phrase we generate all possible inflected forms forthe two parts, before and after the preposition.4.4 Results and DiscussionThe results are shown in Tables 1 and 2.
As NCsare left-bracketed at least 2/3rds of the time (Lauer,1995), a straightforward baseline is to always as-sign a left bracketing.
Tables 1 and 2 suggest thatthe surface features perform best.
The paraphrasesare equally good on the biomedical dataset, but onLauer?s set their performance is lower and is compa-rable to that of the dependency model.The dependency model clearly outperforms theadjacency one (as other researchers have found) onLauer?s set, but not on the biomedical set, where itis equally good.
?2 barely outperforms #, but on thebiomedical set ?2 is a clear winner (by about 1.5%)on both dependency and adjacency models.The frequencies (#) outperform or at least rival theprobabilities on both sets and for both models.
Thisis not surprising, given the previous results by Lap-ata and Keller (2004).
Frequencies also outperformPr on the biomedical set.
This may be due to theabundance of single-letter words in that set (becauseof terms like T cell, B cell, vitamin D etc.
; similarproblems are caused by Roman digits like ii, iii etc.
),whose Web frequencies are rather unreliable, as theyare used by Pr but not by frequencies.
Single-letterwords cause potential problems for the paraphrases9In our experiments we used MSN Search statistics for then-grams and the paraphrases (unless the pattern contained a?*?
), and Google for the surface features.
MSN always re-turned exact numbers, while Google and Yahoo rounded theirpage hits, which generally leads to lower accuracy (Yahoo wasbetter than Google for these estimates).10http://www.nlm.nih.gov/pubs/factsheets/umlslex.html11http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html22Model??
?
P(%) C(%)# adjacency 183 61 0 75.00 100.00Pr adjacency 180 64 0 73.77 100.00MI adjacency 182 62 0 74.59 100.00?2 adjacency 184 60 0 75.41 100.00# dependency 193 50 1 79.42 99.59Pr dependency 194 50 0 79.51 100.00MI dependency 194 50 0 79.51 100.00?2 dependency 195 50 0 79.92 100.00# adjacency (*) 152 41 51 78.76 79.10# adjacency (**) 162 43 39 79.02 84.02# adjacency (***) 150 51 43 74.63 82.38# adjacency (*, rev.)
163 48 33 77.25 86.47# adjacency (**, rev.)
165 51 28 76.39 88.52# adjacency (***, rev.)
156 57 31 73.24 87.30Concatenation adj.
175 48 21 78.48 91.39Concatenation dep.
167 41 36 80.29 85.25Concatenation triples 76 3 165 96.20 32.38Inflection Variability 69 36 139 65.71 43.03Swap first two words 66 38 140 63.46 42.62Reorder 112 40 92 73.68 62.30Abbreviations 21 3 220 87.50 9.84Possessives 32 4 208 88.89 14.75Paraphrases 174 38 32 82.08 86.89Surface features (sum) 183 31 30 85.51 87.70Majority vote 210 22 12 90.52 95.08Majority vote?
left 218 26 0 89.34 100.00Baseline (choose left) 163 81 0 66.80 100.00Table 1: Lauer Set.
Shown are the numbers for cor-rect (?
), incorrect (?
), and no prediction (?
), fol-lowed by precision (P, calculated over?
and?
only)and coverage (C, % examples with prediction).
Weuse ???
for back-off to another model in case of ?.as well, by returning too many false positives, butthey work very well with concatenations and dashes:e.g., T cell is often written as Tcell.As Table 4 shows, most of the surface featuresthat we predicted to be right-bracketing actually in-dicated left.
Overall, the surface features were verygood at predicting left bracketing, but unreliable forright-bracketed examples.
This is probably in partdue to the fact that they look for adjacent words, i.e.,they act as a kind of adjacency model.We obtained our best overall results by combiningthe most reliable models, marked in bold in Tables1, 2 and 4.
As they have independent errors, we useda majority vote combination.Table 3 compares our results to those of Lauer(1995) and of Lapata and Keller (2004).
It is impor-tant to note though, that our results are directly com-parable to those of Lauer, while the Keller&Lapata?sare not, since they used half of the Lauer set for de-Model??
?
P(%) C(%)# adjacency 374 56 0 86.98 100.00Pr adjacency 353 77 0 82.09 100.00MI adjacency 372 58 0 86.51 100.00?2 adjacency 379 51 0 88.14 100.00# dependency 374 56 0 86.98 100.00Pr dependency 369 61 0 85.81 100.00MI dependency 369 61 0 85.81 100.00?2 dependency 380 50 0 88.37 100.00# adjacency (*) 373 57 0 86.74 100.00# adjacency (**) 358 72 0 83.26 100.00# adjacency (***) 334 88 8 79.15 98.14# adjacency (*, rev.)
370 59 1 86.25 99.77# adjacency (**, rev.)
367 62 1 85.55 99.77# adjacency (***, rev.)
351 79 0 81.63 100.00Concatenation adj.
370 47 13 88.73 96.98Concatenation dep.
366 43 21 89.49 95.12Concatenation triple 238 37 155 86.55 63.95Inflection Variability 198 49 183 80.16 57.44Swap first two words 90 18 322 83.33 25.12Reorder 320 78 32 80.40 92.56Abbreviations 133 23 274 85.25 36.27Possessives 48 7 375 87.27 12.79Paraphrases 383 44 3 89.70 99.30Surface features (sum) 382 48 0 88.84 100.00Majority vote 403 17 10 95.95 97.67Majority vote?
right 410 20 0 95.35 100.00Baseline (choose left) 361 69 0 83.95 100.00Table 2: Biomedical Set.velopment and the other half for testing.12 We, fol-lowing Lauer, used everything for testing.
Lapata &Keller also used the AltaVista search engine, whichno longer exists in its earlier form.
The table doesnot contain the results of Girju et al (2005), whoachieved 83.10% accuracy, but used a supervised al-gorithm and targeted bracketing in context.
Theyfurther ?shuffled?
the Lauer?s set, mixing it with ad-ditional data, thus making their results even harderto compare to these in the table.Note that using page hits as a proxy for n-gramfrequencies can produce some counter-intuitive re-sults.
Consider the bigrams w1w4, w2w4 and w3w4and a page that contains each bigram exactly once.A search engine will contribute a page count of 1 forw4 instead of a frequency of 3; thus the page hitsfor w4 can be smaller than the page hits for the sumof the individual bigrams.
See Keller and Lapata(2003) for more issues.12In fact, the differences are negligible; their system achievespretty much the same result on the half split as well as on thewhole set (personal communication).23Model Acc.
%LEFT (baseline) 66.80Lauer adjacency 68.90Lauer dependency 77.50Our ?2 dependency 79.92Lauer tuned 80.70?Upper bound?
(humans - Lauer) 81.50Our majority vote?
left 89.34Keller&Lapata: LEFT (baseline) 63.93Keller&Lapata: best BNC 68.03Keller&Lapata: best AltaVista 78.68Table 3: Comparison to previous unsupervisedresults on Lauer?s set.
The results of Keller & La-pata are on half of Lauer?s set and thus are only in-directly comparable (note the different baseline).5 Conclusions and Future WorkWe have extended and improved upon the state-of-the-art approaches to NC bracketing using an un-supervised method that is more robust than Lauer(1995) and more accurate than Lapata and Keller(2004).
Future work will include testing on NCsconsisting of more than 3 nouns, recognizing theambiguous cases, and bracketing NPs that includedeterminers and modifiers.
We plan to test this ap-proach on other important NLP problems.As mentioned above, NC bracketing should behelpful for semantic interpretation.
Another possi-ble application is the refinement of parser output.Currently, NPs in the Penn TreeBank are flat, with-out internal structure.
Absent any other information,probabilistic parsers typically assume right bracket-ing, which is incorrect about 2/3rds of the time for3-noun NCs.
It may be useful to augment the PennTreeBank with dependencies inside the currently flatNPs, which may improve their performance overall.Acknowledgements We thank Dan Klein, FrankKeller and Mirella Lapata for valuable comments,Janice Hamer for the annotations, and Mark Lauerfor his dataset.
This research was supported by NSFDBI-0317510, and a gift from Genentech.ReferencesPamela Downing.
1977.
On the creation and use of englishcompound nouns.
Language, (53):810?842.R.
Girju, D. Moldovan, M. Tatu, and D. Antohe.
2005.
On thesemantics of noun compounds.
Journal of Computer Speechand Language - Special Issue on Multiword Expressions.Example Predicts Accuracy Coveragebrain-stem cells left 88.22 92.79brain stem?s cells left 91.43 16.28(brain stem) cells left 96.55 6.74brain stem (cells) left 100.00 1.63brain stem, cells left 96.13 42.09brain stem: cells left 97.53 18.84brain stem cells-death left 80.69 60.23brain stem cells/tissues left 83.59 45.35brain stem Cells left 90.32 36.04brain stem/cells left 100.00 7.21brain.
stem cells left 97.58 38.37brain stem-cells right 25.35 50.47brain?s stem cells right 55.88 7.90(brain) stem cells right 46.67 3.49brain (stem cells) right 0.00 0.23brain, stem cells right 54.84 14.42brain: stem cells right 44.44 6.28rat-brain stem cells right 17.97 68.60neural/brain stem cells right 16.36 51.16brain Stem cells right 24.69 18.84brain/stem cells right 53.33 3.49brain stem.
cells right 39.34 14.19Table 4: Surface features analysis (%s), run overthe biomedical set.Frank Keller and Mirella Lapata.
2003.
Using the Web toobtain frequencies for unseen bigrams.
Computational Lin-guistics, 29:459?484.Mirella Lapata and Frank Keller.
2004.
The Web as a base-line: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks.
In Proceedings ofHLT-NAACL, pages 121?128, Boston.Mark Lauer.
1995.
Designing Statistical Language Learners:Experiments on Noun Compounds.
Ph.D. thesis, Departmentof Computing Macquarie University NSW 2109 Australia.Mitchell Marcus.
1980.
A Theory of Syntactic Recognition forNatural Language.
MIT Press.Preslav Nakov, Ariel Schwartz, Brian Wolf, and Marti Hearst.2005.
Scaling up BioNLP: Application of a text annotationarchitecture to noun compound bracketing.
In Proceedingsof SIG BioLINK.James Pustejovsky, Peter Anick, and Sabine Bergler.
1993.Lexical semantic techniques for corpus analysis.
Compu-tational Linguistics, 19(2):331?358.Philip Resnik.
1993.
Selection and information: a class-basedapproach to lexical relationships.
Ph.D. thesis, Universityof Pennsylvania, UMI Order No.
GAX94-13894.Beatrice Warren.
1978.
Semantic patterns of noun-noun com-pounds.
In Gothenburg Studies in English 41, Goteburg,Acta Universtatis Gothoburgensis.Y.
Yang and J. Pedersen.
1997.
A comparative study on featureselection in text categorization.
In Proceedings of ICML?97),pages 412?420.24
