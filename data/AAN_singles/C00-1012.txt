The effects of analys ing cohes ion  on document  summar isat ionBran imi r  K. Boguraev  and  Mary  S. Nef fIBM ZJ.
Watson Research Centel, P.O.
Box 704, Yorktown Heights, NY  10598, USAbkb,nef f@watson ,  ibm.
comAbstractWe argue that in general, the analysis of lexical co-hesion factors in a document can drive a summarizer,as well as enable other content characterization tasks.More narrowly, this paper focuses on how one particularcohesion factol~simple l xical repetition--can enhancean existing sentence xtraction summarizer, by enablingstrategies for overcoming some particularly jarring end-user effects in the summaries, typically due to coher-ence degradation, readability deterioration, and topicalunder-representation.
Lexical repetition is instrumentalto, among other things, the topical make-up of a text, andin our framework a lexical repetition-based model of dis-course segmentation, capable of detecting topic shifts, isintegrated with a linguistically-aware summarizer utiliz-ing notions of salience and dynamically-adjustable sum-mary size.
We show that even by leveraging lexical rep-etition alone, summaries are of comparable, and undercertain conditions bette~, quality than the ones deliveredby a state-of-the-art summarizer.
This is encouraging fora broad research platform focusing on the recognitionand use of cohesive devices in text for a range of contentcharacterisation a d document management tasks.1 IntroductionThis paper add resses aparticular class of problems inher-ent to summaries derived by sentence xtraction, namelythe related issues of coherence degradation, readability de-terioration, and topical under-representation.
Fundamen-tally, these problems arise from unconstrained deletionof arbitrary amount of source material between two sen-tences which end up adjacent in the summary; this hasunpredictable effects on the amount of potentially essen-tial information which may be lost in that deletion.
Ex-amples like 'dangling' anaphors (with lost antecedents)have been cited often enougk and strategies like includ-ing the immediately preceding sentence in the summarylmve some effect.
While intuitively plausible, these arestill simple strategies, prone to misfiring; moreover, othereffects like the reversal of a cote premise in an argument,or the introduction, and subsequent elaboration, of a newtopic, are not easily handled by similar heuristics.We seek to leverage a mechanism for assessing the de-gree of cohesion between individual sentences in the sourcedocument, as well as having a notion of how these maponto the underlying themes in the document.
Informally,cohesion--and lexical cohesion in particular--is manifestin the ways in which the words, or word patterns, of asentence connect hat sentence to certain of its predeces-sors and successors.
The intuition is that identifying, andpreserving, some of these connections in the summarywould improve its coherence.1.1 Lexical  cohesion and summarizationDocuments are coherent because of the continuity oftheir discourse.
A number of rhetorical devices helpachieve cohesion between related document fragments.Analysing such devices--or at the very least being sen-sitive to their manifestation and interplay---can bring amoderately refined degree of discourse awareness intothe summarization process.
In the absence of deep textunderstanding, this boils down to making extensive useof a formalized notion of lexical cohesion.Linguists have studied extensively how various cohe-sive devices operate, and interact, in order to account forcertain properties of the overall organization ofa text dis-course.
For (Halliday and Hasan, 1976), the organizationof text derives from a variety of relationships (cohesiveties) among discourse ntities.
More recently, (Wintel;1979) has focused on the devices that enforce lexical re-lationships and connect a discourse fragment with otherdiscourse fragments.
The underlying theme here is thatcohesion can be best explained in terms of how repetitionis manifested across pairs of sentences.
Repetition car-ries informational va lue- -  it provides a reference pointfor interpreting what has changed, and thus, what is atthe focus of attention of the discourse--and thus clearlygoes well beyond the simple notion that discourse frag-ments with shared content will also share vocabulary.
As(Phillips, 1985) points out, the lexical inventory of a textis tightly organized in terms of collocation; this makesit possible to get a handle on the overall organization oftext, in general, and on the identification of topic introduc-tion and topic closure, in particular.A variety of linguistic devices act as vehicles for rep-etition: viewed at the level of interplay between wordsand phrases in the text, these include lexical repetition, tex-tual substitution and the use of a range of lexical relations,co-re~',rence and ellipsis, paraphrasing, colqunetion, and soforth.
Analysing these would enable the identification ofstrong cohesive ties pulling together a chain of sentenceswhich focus on (aspects of) the same discourse ntity orevent; this would require carrying out, for instance, in-depth co-reference and ellipsis resolution, as well as lexi-cal relation determination.At the other end of the spectrum, just a lexical chainingprocedure (like the one described in (Morris and I-1irst,1.991)) could be used to determine the degree of cohe-sion between adjacent pairs of sentences.
Indeed, this hasbeen the basis for an operational definition of linear dis-course segmentation, where segments in a document aredefined to be contiguous blocks of text, roughly 'aboutthe same thing', with segment boundaries indicative oftopic shifts.The research reported here is just one aspect of a largerstudy into the recognition and use of cohesive devicesfor content characterisation tasks.
It presupposes fine-grained methods for the identification of cohesive ties76between (sentence) units in a text; describing the com-putational basis for developing such methods is outsideof the scope of this paper (howeveb see (Kennedy andBoguraev, 1996), (Fellbaum, 1999), (Kelleb 1994)), as isthe complete framework for lexical cohesion analysis wehave developed.
Instead, in focusing on the effects of lex-ical cohesion on summarization, we limit ourselves hereon the phenomenon of simple lexical repetition; it turnsout that even this can be beneficially applied to enhanc-ing summarizatkm quality.Recent work (Barzilay and Elhadad, 1999) makes ex-plicit this intuition.
"Lexical chains" are constructed bygrouping together items related by repetition and cer-tain lexical relations derived via the WOI',DNET lexicaldatabase (Fellbaum, 1999).
A sequence of items in a chainhighlights a discussion focused on topic related to (an)ite, m(s) in the chain; a metric for scoring chains picks top-ically prominent ones; these are then taken as the basis ofsentence xtractkm heuristics.
A positive result of thatwork is that in an intrinsic evaluation against human-constructed summaries, the system outperformed at leastone commercial summarizer.
This highlights the poten-tial of a purely lexical chains-based appnmch; still, Barzi-lay and Elhadad remain frustrated by the high degree ofpolysemy in WORDNET (not to mention its limited cov-erage with respect o more specialized omains); fortu-nately, this does not concern us here.1.2 Discourse segmentation and summarizationUnlike Barzilay and Elhadad, we start with a sentence-based summarizeb and are specifically seeking to im-prove upon what is already (by some measure; seeSection 4.1 below) a good performance, judged in adiscipline-wide evaluation initiative (Mani et al, 1999).This places certain constraints on how lexical cohesionanalysis results, and in particular the identification oftopically coherent segments, can be incorporated in theexisting strategies and nmchanisms h~r sentence selec-tion, already deployed by the summarizer.
Making cer-tain that a summary incorporates sentences from eachsegment intuitively seeks to ensure uniform representa-tion of all sub-stories ina document; he notion here is toavoid having inordinately large gaps between adjacentsummary sentences, which would tend to lose essentialinhmnation.
Moreove,, a mechanism which would pickthe sentence(s) in a segment most representative its maintopic, would also carry over into the summary 'traces' ofall the main topics in the original document.This is more than just an intuition.
In the process ofdeveloping, and training, our base summarizer (see Sec-tion 2.2 below), an analysis was carried out to determinethe causes of a certain class of failure.
It turns out that30.7% of the failures could be prevented by a heuris-tic sensitive to the logical structure of documents, whichwould enforce that each (topical) section gets representedin the summary.
Additional 15.2% of failures could alsobe avoided if the summarizer was capable of detectingsub-stories within a single section, leading/trailing oise(see below), and so forth.
Thus ahnost half of the errors(in a certain summarization regime, at least) could havebeen avoided by using a segmentation component.This exemplifies how a document-wide analysis of asingle lexical cohesion factor (simple repetition) can im-prove upon an existing sentence selection strategy-~eveuif such a strategy has been devised without prior knowl-edge of additional enhancements to come.
The specificapproaches to being sensitive to foci of attention withina segment, and topic shifts between segments, may vary;as we discuss this below (see Section 3.1), these will de-pend on other environment settings for the summarizer.Still, in the right operational environment even very sim-ple heuristics--take the first sentence from each segment,for instance--have r markably noticeable impact.We thus argue that a lexical repetition-based model oflinear segmentation ffers effective schemes for derivingsentence-based summaries with certain discourse prop-erties, enhancing their quality.What follows is organized in three main sections.
Weoutline some linguistic functions of the summarizel, andgive details of the summarization a d segmentation com-ponents.
We focus specifically on how higher level con-tent analysis uses lower level shallow linguistic process-ing, both to obtain a richer model of the document do-main, and to leverage cohesion analysis for sub-storyidentification.
Next we discuss some strategies for op-timal ttse of discourse segments and topic shifts for sum-marization.
We sketch our evaluation testbed environ-ment, and present experimental results comparing thet)erformance of sunnnarization alone to segmentation-enhanced summarizatkm.
We conclude with an assess-ment of the overall utility of 'cheap' approximations tolexical cohesion measures, pecifically from the point ofview of enhancing a fully operational summarizer.2 Techno logy  baseAs an integral component of an infrastructure for docu-ment analysis with a number of intercoimccted and mu-tually enabling linguistic filters, the summarization sys-teln discussed here makes use of'shallow' linguistic func-tions.
The infrastructure is designed from the ground upto perform a variety of linguistic feature xtraction func-tions, ranging fl'om single pass tokenisation, lexical took-up and morphological analysis, to coinplex aggrega-tion of representative (salient) phrasal units across multi-doculnent collections.
Given such a document processingenvironment, the design of our summarizer is based onsentence selection mechanisms utlilizing salience rankingof phrasal traits in individual documents, when viewedagainst a background of the distribution of phrasal vo-cabulary across a large multi-document collectkm.2.1 Linguistic filtersIn essence, we have a robust ext analysis ystem for iden-tification of proper nanms and technical terms, since theseare most likely to carry the bulk of the semantic loadin a document.
Howeveb in addition to simple iden-tification of certain phrasal types, capabilities also ex-ist for identifying their variants (contractions, abbrevia-tions, colloquial uses, etc.)
in individual documents ina multi-document collection.
A collection vocabulary ofcanonical forms and variants, with statistical informationabout their distribution behaviom; are used in the sum-marizer's alience calculation.
Salience, in turn, is a ma-jor component of the sentence-level score that selects thesentences for extraction (see 2.2 below).As a frequency-based system, our summarizer is ide-ally positioned to exploit linguistic analysis, filtering, and77normalization functions.
Morphological processing al-lows us to link multiple variants of the same word, bynormalizing to lemma forms.
Proper name identificationis enhanced with context disambiguation, amed entitytyping, and variant normalisation; asa result he system'sfrequency analysis is more precise, and less sensitive tonoise; ultimately, this leads to more robust salience cal-culation.
Normalisation of different variants of the sameconcept o a canonical form is further facilitated by pro-cesses of abbreviations unscrambling, resolution of def-inite noun phrase anaphora, and aggregation across theentire document collection.
The set of potentially salientphrases is enriched by the identification and extractionof technical terms; this enables the recognition of certainmulti-word concepts mentioned in the document, withdiscourse properties indicative of high topicality value,which is also directly relevant to salience determination.Each document in a collection is analyzed individually.All 'content' (non-stop) words, as well as all phrasal unitsidentified by the linguistic filters, are deemed to be vo-cabulary items, indexed via their canonical forms.
Witha view to future extensions of the base summarizationfunction (see Section 5), these retain complete contextualinformation about the variants they have been encoun--tered in, as well as the local context of each occurrence.The vocabulary items are counted and aggregated acrossdocuments to form the collection w)cabulary.
In additionto all the canonical forms and variants, the collection vo-cabulary contains the composite frequency of each canoni-cal form, and its information quotient, a statistical measureof the distribution of a vocabulary item in the collection.Aggregating together similar items from different docu-ments (cross-document co-reference) is far from straight-forward for multi-word items; howeveb being able tocarry out a process of cross-document coreference r solu-tion is clearly a further enabling capability for obtainingmore precise collection statistics.
A pronominal anaphoraresolution function further contributes to the quality ofthe collection statistics.In addition to the domain vocabulary, the summarizeralso has access to document structure information.
A hi-erarchical representation f the document separates con-tent and layout metadata, and makes the latter explicitin a document structure tree.
Encoded are data includ-ing: appearance and layout ags; document title; abstract,and other front matter; (sub-)section, etc.
headings; para-graphs, themselves composed of sentences; 'floating' ob-jects like tables, figures, captions; side-bars and other textextraneous to the main document narrative; etc.
Doc-ument structure is constructed by 'shadowing' markupparsing, as markup tags are used to construct he doc-ument structure tree; for documents without markup,structure determination is carried out on the basis of pagelayout cues.
The document structure records additionaldiscourse-level annotations, uch as cue phrases mark-ing rhetorical relations, quoted speech, and so forth.
Allof these elements both contribute directly to the summa-rizer's set of heuristics, as well as inform the discoursesegmentation process.2.2 Sa l ience-dr iven  summar izat ionWith its set of linguistic filters, our frequency-based sum-marizer can exploit linguistic dimensions beyond singleword analysis; this is not unlike the approach of (Aoneet al, 1997).
Due to the sophistication and integration ofthe filters (see Section 2.1), we are able to exploit a richersource of domain knowledge than most other frequency-based systems.Frequency alone is poor indicator of salience, evenwhen ignoring stop words.
Unlike early frequency-basedtechniques for sentence selection, we utilize the moreindicative inverse document frequency measure, adaptedfrom information retrieval, in which the relative fre-quency of an item in a document is compared with its rel-ative frequency in a background collection.
The trade-off,however, for more precise term salience is the summa-rizer's dependence on background collection statistics;we return to this issue below.Sentence selection is driven by the notion of salience;the summary is constructed by extracting the most salientsentences in the full document.
The salience score of asentence is derived partly from the salience of vocabu-lary items in the document and partly from its positionin the document structure (e.g.
section-initial, paragraph-internal, and so forth) and the salience of the surround-ing sentences.
The calculation of inverse document fre-quency for a w)cabulary item t compares its relative fre-quency in the document with its relative frequency in thecollection.
We define the item's salience score to be thisinverse document frequency measure (in the formula be-low, No'oft and Nlgo,., refer to, respectively, to the numberof items in the collection, and document).Salience(t) = log2((N~:ott/fre~q(i,)c~oll)/(NDo~/J'rexl(t)D,,~:))Salient items are items occurring more than once in thedocument, whose salience score is above an experimen-tally determined cutoff, or items appearing in a strategicposition in the document structure (e.g.
title, headings,etc.
; see Section 2.1).
All others are assigned zero salience.The score for a sentence is made up of two components.The salience component is the sum of the salience scoresof the items in the sentence.
The structure componentreflects the sentence's proximity to the beginning of theparagraph, and its paragraph's proximity to the begin-ning and/or  end of the document.
Structure score is sec-ondary to salience score; sentences with no salient itemsget no structure score.A set of heuristics address some of the coherence-related problems discussed earlier (see 1).
For example,under certain conditions, a sentence might be selected forinclusion in the summary, even if it has low, or even zero,score: sentences immediately preceding higher scoringones in a paragraph may get promoted by virtue of an'agglomeration rule'.
Agglomeration is an inexpensiveway of preventing dangling anaphors without havingto resolve them.
Another problem for sentence-basedsummarizers, that of thematic under-representation ( r,loosely speaking, coverage; see 1), is addressed by an'empty section' rule, which is of particular interest for thispaper.
Longer documents with multiple sections, or newsdigests containing several stories, may be unevenly rep-resented in a sentence-extracted summary.
The 'emptysection' rule aims to ensure that each section is repre-sented in the summary by forcing inclusion of its high-est scoring sentences, ob if all sentence scores are zero, itsfirst sentence.As a general purpose summarize~, ours makes ex-tensive use of small scale linguistic information (termphrasal patterns) and large scale statistical information78(term distribution patterns).
With the exception of tileheuristic rules outlined earlier in this section, tile summa-rizer is operating without any focused analysis of cohe-sion factors in tile input text.
I lence the departure pointfor this work, as already discussed (in Section 1): can thesummarizer's performance be improved, if we take intoaccount lexical cohesion in the source?We address this question by making the summarizeraware of certain discourse-level f atures of the document,and in particular, by leveraging tile topic shifts in it; tothis end, the infrastructure has been augmented with afunction for linear discourse segmentation.2.3 Linear discourse segmentationSegmentation is a document analysis function which di-rectly exploits one of tile core text cohesion factors, pat-terras of h'xicaI repetition (see Section 1.1), for" identifyingsome baseline data concerning tile distribution of topicsin a text.
In particulal, discourse segmentation is drivenby tile determination f points in the narrative where per-ceptible discontinuities in the text cohesion are detected.Such discontinuities are indicative of topic shifts, t%llow-ing the original idea of lexical chains (Morris and l lirst,1991), subsequently developed specifically for the pur-poses of segmentation f expository text (Hearst, 1994),we have adapted an algorithm for discourse segmenta-tion to our document processing environment.
In par-ticulab while remaining sensitive to tile distribution of"terms" across tile docunlent, and calculating similaritybetween adjacent ext blocks by a cosine measure, ourprocedure differs from that in (Hearst, 1994) in severalways.We only take into account content words (as opposedto all terms yielded by a tnkenizatkm step).
These arenormalized to lemma forms.
"Termhood" is addition-ally refined to take into account multi-word sequences(pcnper names, technical terms, and so forth, as discussedin Section 2.1 above), as well as a notion of co-reference,where different name variants get "aggregated" into thesame canonical form.
The cohesioi~ calculation (tll\]C-tion is biased towards different ypes of possible breakpoints: thus certain cue phrases ("llowever", "On lhe olherham/") unambiguously signal a topic shift; documentstructure lements--such as sentence beginnings, para-graph openers, and section heads--are exploited for their'pre-disposition' to act as likely segment boundaries; andso forth (see Section 2.1).
The function is also adjusted toreduce the noise from block comparisons where the blockbnundary--and thus a potential topic shift--falls at un-natural break points (such as tile middle of  a sentence).By making segmentation a other component withinour  document processing environment, we are able touse, transparently, the results of processes such as lexicalmM morl;hohNical lookup, docmnent structure identification,and cue I#trase detection.
Likewise, segmentation resultsare naturally incorporated in an annotation superstruc-ture which records the various levels of document anal-ysis: discourse segments are just another type of a 'span'(annotation) over: a number of sentences, logically akin toa paragraph (Bird and Liberman, 1999).Apart from the adjustments and modifications out-lined above, we use essentially tearst's formnla for com-puting lexical similarity between adjacent blocks of textbl and b2 (t denotes a discourse lement term identifiedas such by prior processing, ranging over tim text spanof the currently analyzed block; Wt,l,N is the normalizedfrequency of occurrence of the term in block b~\,):sim(bl,b2) :~ > tWl,btwt,b~Unlike most applications ofsegmentatkm to date, whichare concerned with the identification of segment bound-aries, we are primarily interested ira Ieveraging the con-tent of the segments, to the extent hat it is indicativeof the focus of attention, and (indirectl3; at least) pointsat tile topical shifts to be utilized for surnmary genera-tion.
We use tile segmentation results (together with thename and term identificatkm and salience calculation de-livered by other functions) in order to ensure that all thebase data for inferring the topic stamps, and topic shifts,ix available to the user.3 Segmentation-assisted summariesWhat is tile relationsldp between segmentation a d sum-marizatkm: is segmentation a strictly "under the covers"function for tile summarizer, or might segmentation re-suits be of any interest, and use, to tile end nser?
WefOCLIS ()l 1.
SOIlle strategies for incorporating segmentationresults in tile summary generation process.
1tmvever, un-like (Kan et al, 11998) (whose work also seeks to lever-age linear segmentation forthe explicit purposes of docu-ment summarization), we further take tile view that withan appropriate interface metaphor where the user hasan overview of the relationships between a sunnnary sen-tence, the key salient phrases within it, and its enclosingdiscourse segment--a sequence of visually demarkatedsegments can impart a lot of information directly leadingto in-depth perception of the summary, as it relates to thefull docun~ent (Boguraev and Neff, 2000).3.1 Strategies for utilizing segments(_~ol'ln'llOll intuitions uggest a number of strategies forleveraging the results of linear discourse segmentationfor enhancing stunmavizaLion.
As topic shift points intile text are 'published' into the document structure (seeSection 2.3), by defining a segment as an additional typeof document span (akin to sentence, paragraph, section,and so forth), the summarizer t ansparently, and imme-diately, becomes aware of the segmentation results.
Wealso make arrangements for a mechanism whereby cer-tain strategies for incorporating segmentation results intothe SUlnnlarization process were easy to cast in summa-rizer terms.Thus, for instance, a heuristic requMng that each seg-ment is represented in the summary can be naturally ex-pressed by treating segments as sections, and strictly en-forcing the 'empty section' rule (see 2.2).
The selectionof a segment-initial sentence for tile summary can be era-forced simply by boosting the salience score for that sen-tence above a known threshold.
A decision to drop ananecdotal (or otherwise peripheral; see below) segmentfrom consideration i  summary generation would be re-alised by setting, as a last step prior to sumlnary genera-tion, the sentence salience scores for all sentences in thesegment to zeros.3.2 Other benefits of segmentationSuch strategies are discussed in mnre detail atch as theynaturally belong with their evaluation, llere we highlight79a few observations concerning the overall benefits thatsegmentation brings to summarization.
Thus, in additionto facilitating sentence-based summaries with certain dis-course and rhetorical properties, it turns out that undercertain conditions the summarizer can operate very ef-fectively without a need for background corpus statistics.This is a better solution than the highly genre-sensitiveapproach of supplying a 'generic' background collection,against which summaries could be generated even fordocuments which are not a priori part of the collection.Note that the derivation of a background collection andstatistics for it might be impractical for a variety of rea-sons: lack of access to a sufficiently large and represen-tative data sample; no time for processing; sparse stor-age resources; and so forth.
Clearly, being able to oper-ate without such statistics i an operational bonus for thesummarizer.Another use for segmentation is for optimising the useof source input, as well as possibly maximising its re-use.Occasionally, the document contains 'noise'--possibly inthe form of anecdotal leads, closing remarks tangential to themain points of the story, side-bars, and so forth--whichare inappropriate sources for summary sentences.
Lin-ear segmentation sensitive to topic shifts and documentstructure would identify such source fragments and re-move them from consideration by the summarizer.
Con-versely, in certain news reporting enres a whole docu-ment fragment (typically towards the beginning or theend of the document) functions as a summary of thestory: we would like to be able to use this fragment;clearly identifying it as a segment would help.We also use segmentation to handle long documentsmore effectively.
While the collection-based saliencedetermination works reasonably well for the average-length news story, it has some disadvantages.
Forlonger documents, with requisite longer summaries, thenotion of salience degenerates, and the summary be-comes just an incoherent collection of sentences.
(Evenif paragraphs, rather than sentences, are used to com-pose the summary--see .g.
(Mitra et al, 1997)--the sameproblems of coherence degradation and topical under-representation, remain.)
We use segmentation to iden-tify contiguous ub-stories in long documents, which arethen individually passed on to the summarizer; the re-sults of sub-story summaries are 'glued' together.4 Evaluat ionFor evaluating the effect of various strategies upon sum-marizer output quality, we used as baseline an evalua-tion corpus of full-length articles and their 'digests', fromThe New York Times.
There are advantages, and disadvan-tages, to this approach.
Setting aside whether task-basedevaluation is appropriate for testing strictly the effect ofone technology on another (see Section 4.1 below), sucha decision ties us to a particular set of data.
On the pos-itive side, this offers a realistic baseline against which tocompare strategies and heuristics; on the negative side, ifa certain type of data is missing from the evaluation cor-pus, there is little hard evidence for judging the effects ofstrategies and heuristics on such data.The remainder of this section describes our evalua-tion environment, and then looks at the results for small-to-average size documents (the collection comprises justover 800 texts, less than half of which are over 10K,and virtually none are over 20K; the byte count includesHTML markup tags; in terms of number of sentences perdocument, very few of these longer documents are over100 sentences long).4.1 Summar izat ion  eva luat iontes tbedEvaluating summarization results is not trivial, at leastbecause there is no such thing as the best, or correct,summary--especial ly when the summary is constructedas an extract.
The purposes of such extracts vary; so dohuman extractors.
Sentence xtraction systems may beevaluated by comparing the extract with sentences e-lected by human subjects (Edmundson, 1969).
This isa (superficial) objective measure that clearly ignores thepossibility of multiple right answers.
Another objectivemeasure compares summaries with pre-existing abstractsusing a suitable method for mapping a sentence in theabstract o its counterpart in the document.
Subjectivemeasures, even though still less satisfying, can also be de-vised: for instance, summary acceptability has been pro-posed as one such measure.
Other evaluation protocolsshare the primary feature of being task-based, even thoughdetails may vary.
Thus performance may be measuredby comparing browsing and search time as summary ab-stracts and fulMength originals are being used (Miike etal., 1994); other measures look at recall and precision indocument retrieval (Brandow et al, 1995); or recall, pre-cision, and time required in document categorization (i.e.assessing whether a document has been correctly judgedto be relevant or not, on the basis of its summary alone)(Mani et al, 1999).We built an environment for baseline summarizer eval-uation, as part of its development/training cycle.
Thiswas also used in analyzing the impact of discourse seg-mentation on the summarizer's performance.
A back-ground collection vocabulary statistics was derived fromanalyzing 2334 New York Times news stories.
Sentencesin digests for 808 stories and feature articles were auto-matically matched with their corresponding sentences inthe full-length documents.
Digests range in length from1 to 4 sentences.
Since we were particularly interestedin longer stories, as well as stories in which the first sen-tence in the document did not appear in the digest, theirrepresentation in the test set, 38%, is larger than their dis-tribution in the newspaper.Since digests are inherently short, this evaluation strat-egy is somewhat limited in its capability of fully assess-ing segmentation effects on summarization of long doc-uments.
Nonetheless, a number of comparative analysescan be carried out against his baseline collection, whichare indicative of the interplay of the various control op-tions, environment settings, and linguistic filters used.One parameter, in particular, isquite instrumental in tun-ing the summarizer's performance, to a large extent be-cause it is directly related to length of the original docu-ment: size of thesummary, expressed either as number ofsentences, or as percentage of the full length of the origi-nal.
In addition to a clear intuition (namely that the sizeof the summary ought to be related to the size of the orig-inal), varying the length of the summary offers both theability to measure the summarizer's performance againstbaseline summaries (i.e.
our collection of digests), and thepotential of dynamically adjusting the derived summarysize to optimally represent the full document content, de-80pending on the size of that document.Our experiments vary tile granularity of summarysize.
In principle, the performance of a system whichdoes absolute sentence ranking, and systematically picksthe N 'best' sentences for tlle summary, should not de-pend on the summary size.
In our case, the additionalheuristics for improving the coherence, readability, andrepresentativeness of tile summary (see Section 2.2) in-troduce variations in overall summary quality, depend-ing on the compaction factor applied to the original doc-ument size.
A representative spectrum for tlle test corpuswe use is given by data points at: diqest size (i.e.
sum-mary exactly the size, expressed as number of sentences,of tile digest); 4 sentem:es; I0% of the size of the full lengthdocument; and 20% of the document.
Not surprisingly(for a salience-based system), the sumnlarization ftmc-tion alone, without discourse segmentation, benefits fromlarger summary size.
Although tlle recall rate is higherstill for longer summaries, it is not a measure of the over-all quality of tile summary because of tile inherently shortlength of tile digest.4.2 Segmentat ion effects on summar izat ionOur experiments compare the base summarization pro-cedure, which calculates object salience with respect o abackground ocument collection (Section 2.2), with en-hanced procedures incorporating different strategies us-ing the notions of discourse segments and topic shifts.These elaborate tile intuitions underlying our ap-proach to leveraging lexical cohesion effects (see Sec-tion 1.2).
The experiments fall in either of two categories.In an environment where a background collection, andstatistics, cannot be assumed, a summarization proce-dure was defined to take selected (typically initial) sen-tences from each segment; this appeals to the intuitionthat segment-initial sentences would be good topic in-dicators for their respective segments, qhe other cate-gory of experiment focused on enriching the base sum-marization procedure with a sentence selection mecha-nism which is informed by segment botmdary identifica-tion and topic shift detection.In combining different sentence selection mechanisms,several variables need adjustment to account for relativecontributions of the different document analysis meth-ods, especially where summaries can be specified to beof different lengths.
Given the additional sentence selec-tion factors interacting with abso\]ute sentence ranking,we again set the granularity of summary size at three dis-crete steps, mirroring the evaluation of the original sum-marizer: summaries can be requested to be precisely 4sentences long, or to reflect source compaction factor of10% or 20% (Sectkm 4.1).We experimented with two broad strategies for incor-porating topical informatkm into the sunnnary.
One ap-proach aimed to bring 'topic openers' into the summars;by adding segment-initial sentences to those already se-lected via salience calculation.
The other was to exertfiner control over the number of sentences elected viasalience, and 'pad' the summary to its requested size withsentences selected from segments by invoking the 'emptysegment' (aka 'empty sectkm', see 2.2) rule.
Special pro-visions accounted for the fact that segmentatkm wouldnaturally always select he document-initial sentence.It turns out that the differences between a range of re-alisations of the above two strategies are not statisticallysignificant over our test corpus; we thus use the label"SUM+SEG" to denote a 'composite' strategy and to rep-resent he whole family of variations.
In contrast, "SUM"refers to the base smnmarization component, and "SEG"represents summarization by segmentation alone.
Table 1below shows the recall rates for the three major summa-rization regimes defined by different summary granular-ities.
Since segmentation effects are clearly very differ-ent across different sizes of source document, our experi-ments were additionally conducted at sampling the doc-ument collection at different sizes of the originals: thecorpus was split into four sections, grouping togetherdocuments less than 7.5K characters long, 7.5-\]0K, 10-19K, and over 19K; for brevity, the table encapsulates a'composite' result (denoted by the label "All documents").\[\[ 4 sents 10% 20?/,,All documentsSEG 54.74 54.74 56.09SUM 46.85 49.71 66.47SUM+SEG 56.52 56.30 58.37All documents with > 1 digest sentenceSEG 45.13 45.13 46.78SUM 36.34 39.84 58.66SUM+SEG 41.64 46.75 51.65All documents whose 1st sentence not in target digestSliG 31.12 32.73 33.99SUM 29.93 39.96 61.71SUM+SEG 32.53 41.45 47.96qhble 1: Sumn~ary data for segmentation effectslb  get a better sense for tile effects of different strategymixes, we also show results for tile same summarizationregimes, on subsets of the test corpus.
"All documents with> I di,?esl senh'm:t'" represents documents whose digestsare longer than a single sentence; "All documetHs whose1st sent is not in target d~qest" extracts a document set forwhich a baseline strategy automatically picking a repre-sentative sentence for inclusion in the summary would beinappropriate.
These subset selection criteria explain thedeterioration of overall results; howeveb what is moreinteresting to observe in the table is the relative perfor-mance of tile three summarization regimes.Overall, leveraging some of the segmentation a alysisis positively beneficial to summarization; the effects areparticularly strong where short summaries are required.In addition, summarization driven by segmentation dataalone shows recall rates comparable to, and in certain sit-uatkms even higher than, tlle baseline: this suggests thatsuch a procedure is certainly usable in situations wherebackground collection-based salience calculation is im-possible, or impractical.Finally, we emphasise a note of particular interest here:the complete set of data from these experiments makes itpossible, for any given document, to select dynamicallytile summarization strategy appropriate to its size, in or-der to get an optimal summary for it, in any given infor-mation compaction regime.5 ConclusionStarting from a class of problems inherent o summariza-tion by sentence xtraction, we have proposed a strat-81egy for alleviating some of the particularly jarring end-user effects in the summaries, which are due to coher-ence degradation, readability deterioration, and topicalunder-representation.
Our approach is to aim for morecohesive summaries, by leveraging the lexical cohesionfactors in the source document exts.
As an initial ex-periment, we have looked at one particular facto1; lcxicalrepetition, and have developed a framework for integrabing a discourse segmentation component capable of de-tecting shifts in topic, with a linguistically-aware summa-rizer which utilizes notions of salience and dynamically-adjustable size of the resulting summaries.
By analyz-ing cohesion indicators in the discourse, segmentationidentifies points in the narrative where sub-stories alter-nate; the summarization function uses the resulting setof discourse segments to derive more complete, informa-tive and faithful summaries than ones extracted solely onthe basis of sentence salience (calculated with respect oa background ocument collection).A comparative evaluation of summarization with, andwithout, segmentation analysis shows that under cer-tain conditions, segmentation-enhanced summarizationis better than the base segmentation technology: Someof these conditions can be expressed as a function ofthe original document length, and the document-to-summary ratio; thus, of particular interest is the fact thatoptimal strategy for combining the two technologies canbe selected 'on the fly', depending on the type of input tobe summarized.Furthemore, having access to a segmentation compo-nent makes it possible to alleviate a serious shortcom-ing of summarizers like ours, which crucially dependon the statistics of a background collection: in situa-tions where background collection-based salience calcu-latkm is impossible, or impractical, it is realistic to de-liver summaries--of  comparable quality, yet consider-ably cheaper to generate--derived by access to discoursesegmentation i formation alone.The research reported here is part of a larger effort fo-cused on leveraging elements of the discourse structurefor a variety of content characterisation tasks.
Overall,we aim to build an infrastructure for recognizing and us-ing a broad range of cohesive devices in text.
Documentsummarization is just one application in the larger spaceof document content management; our long term goal isto develop a framework where summarization and otherapplications would be enabled by a rich substrate of lin-guistic analysis of lexical cohesion.ReferencesChinatsu Aone, Mary Ellen Okurowski, James Gorlinsky,and Bjornar Larsen.
1997.
A scalable summarizationsystem using robust NLP.
In Intelligent Scalable TextSummarizathm, Proceedings ofWorkshop Sponsored by theAssochltion fi," Computational Linguistics, pages 66-73.Regina Barzilay and Michael Elhadad.
1999.
Using lex-ical chains for text summarization.
In Inderjeet Maniand Mark T. Maybury, editors, Advances in automatictext summarization, pages 111-121.
MIT Press, Cam-bridge, MA.Steven Bird and Mark Liberman.
:1999.
Annotationgraphs as a framework for multidimensional linguis-tic data analysis.
In Proceedings ofa Workshop, "TowaMsStandards and Tools fin" Discourse Tagging", 37th AmmalMeeting of the AssochTtion for Computational Linguistics,pages 1-10, Baltimore, MD.Branimir Boguraev and Mary Neff.
2000.
Lexical co-hesion, discourse segmentation and document sum-marization.
In Proceedings ofRIAO-2000, Content-BasedMultimedia Infi~rmation Access, Paris, France.R.
Brandow, K. Mitze, and L. Rau.
1995.
Automatic on-densation of electronic publications by sentence selec-tion.
Informathm Processing & Management, 31(5).H.P.
Edmundson.
1969.
New methods in automatic ab-stracting.
Journal of the ACM, 16(2):264-285.Christiana Fellbaum, editor.
1999.
WORDNET: all elec-tronic lexical database and some of its applications.
M1TPress, Cambridge, MA.M.A.K.
Hall iday and R. Hasan.
1976.
Cohesion i  English.Longman, London.Marti Hearst.
1994.
Multi-paragraph segmentation f ex-pository text.
In 32nd Ammal Meeting of the Associationfi~r Computational Linguistics, Las Cruces, New Mexico.Min-Yen Kan, Judith L. Klavans, and Kathleen R. McK-eown.
1998.
Linear segmentation and segment signif-icance, in Eugene Charniak, editob Proceedings oftheSixth Workshop on Very Large Corpora, pages 197-205,Montreal, Canada, August.
Sponsored by ACL andACI 's  SIGDAT.Andrew Keller.
1994.
Common topics and coherent situ-ations: interpreting ellipsis in the context of discourseinference.
In Proceedinqs ofthe 32nd Annual Meeting ~the Association for Comlmtational Linguistics, pages 50-57, Las Cruces, NM.Christopher Kennedy and Branimir Bogurae\: 1996.Anaphora for everyone: Pronominal anaphora reso-lution without a parser.
In Proceedings ofCOLING-96(161h International Conference on Computathmal Linguis-tics), Copenhagen, DK.lnderjeet Mani, Therese Firmin, and Beth Sundheim.1999.
The TIPSTER SUMMAC text summarization eval-uation.
In Proceedings ofthe Ninth CoJ~rence of the Euro-wan Chapter of the ACL, pages 77-85, Bergen, Norway,June.
Association for Computational Linguistics.Seije Miike, Etsuo ltho, Kenji Ono, and Kazuo Sumita.1994.
A full text retrieval system with a dynamic ab-stract generation function.
In Proceedings ofthe 17th An-m~al International ACM SIGIR Conference on Research andDevelopment i  Information Retrieval, pages 152-161.Mandar Mitra, Amit Singhal, and Chris Buckley.
:1997.Automatic text summarisation by paragraph extrac-tion.
In Inderjeet Mani and Mark T. Maybury, editors,Proceedings ~a Worksh W on Intelligent Scalable text Sum-marization, pages 39-46, Madrid, Spain.
Sponsored bythe Association for Computational Linguistics.Jane Morris and Graeme Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as an indicator of thestructure of text.
Computational Linguistics, 17:21-48.M.
Phillips.
1985.
Aspects of text structure: an investigationof the lexical organization oftext.
North Holland, Ams-terdam.E.O.
Winter 1979.
Replacement as a fundamental func-tion of the sentence in context.
Forum Linguistieum,4(2):95-133.82
