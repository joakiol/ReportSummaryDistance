Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 515?522,Sydney, July 2006. c?2006 Association for Computational LinguisticsParsing and Subcategorization DataJianguo Li and Chris BrewDepartment of LinguisticsThe Ohio State UniversityColumbus, OH, USA{jianguo|cbrew}@ling.ohio-state.eduAbstractIn this paper, we compare the per-formance of a state-of-the-art statisticalparser (Bikel, 2004) in parsing written andspoken language and in generating sub-categorization cues from written and spo-ken language.
Although Bikel?s parserachieves a higher accuracy for parsingwritten language, it achieves a higher ac-curacy when extracting subcategorizationcues from spoken language.
Our exper-iments also show that current technologyfor extracting subcategorization framesinitially designed for written texts worksequally well for spoken language.
Addi-tionally, we explore the utility of punctu-ation in helping parsing and extraction ofsubcategorization cues.
Our experimentsshow that punctuation is of little help inparsing spoken language and extractingsubcategorization cues from spoken lan-guage.
This indicates that there is no needto add punctuation in transcribing spokencorpora simply in order to help parsers.1 IntroductionRobust statistical syntactic parsers, made possi-ble by new statistical techniques (Collins, 1999;Charniak, 2000; Bikel, 2004) and by the avail-ability of large, hand-annotated training corporasuch as WSJ (Marcus et al, 1993) and Switch-board (Godefrey et al, 1992), have had a majorimpact on the field of natural language process-ing.
There are many ways to make use of parsers?output.
One particular form of data that can be ex-tracted from parses is information about subcate-gorization.
Subcategorization data comes in twoforms: subcategorization frame (SCF) and sub-categorization cue (SCC).
SCFs differ from SCCsin that SCFs contain only arguments while SCCscontain both arguments and adjuncts.
Both SCFsand SCCs have been crucial to NLP tasks.
For ex-ample, SCFs have been used for verb disambigua-tion and classification (Schulte im Walde, 2000;Merlo and Stevenson, 2001; Lapata and Brew,2004; Merlo et al, 2005) and SCCs for semanticrole labeling (Xue and Palmer, 2004; Punyakanoket al, 2005).Current technology for automatically acquiringsubcategorization data from corpora usually relieson statistical parsers to generate SCCs.
Whilegreat efforts have been made in parsing writtentexts and extracting subcategorization data fromwritten texts, spoken corpora have received littleattention.
This is understandable given that spokenlanguage poses several challenges that are absentin written texts, including disfluency, uncertaintyabout utterance segmentation and lack of punctu-ation.
Roland and Jurafsky (1998) have suggestedthat there are substantial subcategorization differ-ences between written corpora and spoken cor-pora.
For example, while written corpora show amuch higher percentage of passive structures, spo-ken corpora usually have a higher percentage ofzero-anaphora constructions.
We believe that sub-categorization data derived from spoken language,if of acceptable quality, would be of more value toNLP tasks involving a syntactic analysis of spokenlanguage.
We do not show this here.The goals of this study are as follows:1.
Test the performance of Bikel?s parser inparsing written and spoken language.2.
Compare the accuracy level of SCCs gen-erated from parsed written and spoken lan-515guage.
We hope that such a comparison willshed some light on the feasibility of acquiringsubcategorization data from spoken languageusing the current SCF acquisition technologyinitially designed for written language.3.
Apply our SCF extraction system (Li andBrew, 2005) to spoken and written lan-guage separately and compare the accuracyachieved for the acquired SCFs from spokenand written language.4.
Explore the utility of punctuation1 in pars-ing and extraction of SCCs.
It is gen-erally recognized that punctuation helps inparsing written texts.
For example, Roark(2001) finds that removing punctuation fromboth training and test data (WSJ) decreaseshis parser?s accuracy from 86.4%/86.8%(LR/LP) to 83.4%/84.1%.
However, spo-ken language does not come with punctua-tion.
Even when punctuation is added in theprocess of transcription, its utility in help-ing parsing is slight.
Both Roark (2001)and Engel et al (2002) report that removingpunctuation from both training and test data(Switchboard) results in only 1% decrease intheir parser?s accuracy.2 Experiment DesignThree models will be investigated for parsing andextracting SCCs from the parser?s output:1. punc: leaving punctuation in both trainingand test data.2.
no-punc: removing punctuation from bothtraining and test data.3.
punc-no-punc: removing punctuation fromonly the test data.Following the convention in the parsing com-munity, for written language, we selected sections02-21 of WSJ as training data and section 23 astest data (Collins, 1999).
For spoken language, wedesignated section 2 and 3 of Switchboard as train-ing data and files of sw4004 to sw4135 of section 4as test data (Roark, 2001).
Since we are also inter-ested in extracting SCCs from the parser?s output,1We use punctuation to refer to sentence-internal punctu-ation unless otherwise specified.label clause type desired SCCsgerundive (NP)-GERUNDS small clause NP-NP, (NP)-ADJPcontrol (NP)-INF-tocontrol (NP)-INF-wh-toSBAR with a complementizer (NP)-S-wh, (NP)-S-thatwithout a complementizer (NP)-S-thatTable 1: SCCs for different clauseswe eliminated from the two test corpora all sen-tences that do not contain verbs.
Our experimentsproceed in the following three steps:1.
Tag test data using the POS-tagger describedin Ratnaparkhi (1996).2.
Parse the POS-tagged data using Bikel?sparser.3.
Extract SCCs from the parser?s output.
Theextractor we built first locates each verb in theparser?s output and then identifies the syntac-tic categories of all its sisters and combinesthem into an SCC.
However, there are caseswhere the extractor has more work to do.?
Finite and Infinite Clauses: In the PennTreebank, S and SBAR are used to labeldifferent types of clauses, obscuring toomuch detail about the internal structureof each clause.
Our extractor is designedto identify the internal structure of dif-ferent types of clause, as shown in Table1.?
Passive Structures: As noted above,Roland and Jurafsky (Roland and Juraf-sky, 1998) have noticed that written lan-guage tends to have a much higher per-centage of passive structures than spo-ken language.
Our extractor is alsodesigned to identify passive structuresfrom the parser?s output.3 Experiment Results3.1 Parsing and SCCsWe used EVALB measures Labeled Recall (LR)and Labeled Precision (LP) to compare the pars-ing performance of different models.
To comparethe accuracy of SCCs proposed from the parser?soutput, we calculated SCC Recall (SR) and SCCPrecision (SP).
SR and SP are defined as follows:SR = number of correct cues from the parser?s outputnumber of cues from treebank parse (1)516WSJmodel LR/LP SR/SPpunc 87.92%/88.29% 76.93%/77.70%no-punc 86.25%/86.91% 76.96%/76.47%punc-no-punc 82.31%/83.70% 74.62%/74.88%Switchboardmodel LR/LP SR/SPpunc 83.14%/83.80% 79.04%/78.62%no-punc 82.42%/83.74% 78.81%/78.37%punc-no-punc 78.62%/80.68% 75.51%/75.02%Table 2: Results of parsing and extraction of SCCsSP = number of correct cues from the parser?s outputnumber of cues from the parser?s output (2)SCC Balanced F-measure = 2 ?
SR ?
SPSR + SP (3)The results for parsing WSJ and Switchboardand extracting SCCs are summarized in Table 2.The LR/LP figures show the following trends:1.
Roark (2001) showed LR/LP of86.4%/86.8% for punctuated writtenlanguage, 83.4%/84.1% for unpunctuatedwritten language.
We achieve a higheraccuracy in both punctuated and unpunctu-ated written language, and the decrease ifpunctuation is removed is less2.
For spoken language, Roark (2001) showedLR/LP of 85.2%/85.6% for punctuated spo-ken language, 84.0%/84.6% for unpunctu-ated spoken language.
We achieve a loweraccuracy in both punctuated and unpunctu-ated spoken language, and the decrease ifpunctuation is removed is less.
The trends in(1) and (2) may be due to parser differences,or to the removal of sentences lacking verbs.3.
Unsurprisingly, if the test data is unpunctu-ated, but the models have been trained onpunctuated language, performance decreasessharply.In terms of the accuracy of extraction of SCCs,the results follow a similar pattern.
However, theutility of punctuation turns out to be even smaller.Removing punctuation from both the training andtest data results in a 0.8% drop in the accuracy ofSCC extraction for written language and a 0.3%drop for spoken language.Figure 1 exhibits the relation between the ac-curacy of parsing and that of extracting SCCs.If we consider WSJ and Switchboard individu-ally, there seems to exist a positive correlation be-tween the accuracy of parsing and that of extract-ing SCCs.
In other words, higher LR/LP indicatespunc no?punc punc?no?punc747678808284868890ModelsF?measure(%)WSJ parsingSwitchboard parsingWSJ SCCSwitchboard SCCFigure 1: F-measure for parsing and extraction ofSCCshigher SR/SP.
However, Figure 1 also shows thatalthough the parser achieves a higher F-measurevalue for paring WSJ, it achieves a higher F-measure value for generating SCCs from Switch-board.The fact that the parser achieves a higher ac-curacy of extracting SCCs from Switchboard thanWSJ merits further discussion.
Intuitively, itseems to be true that the shorter an SCC is, themore likely that the parser is to get it right.
Thisintuition is confirmed by the data shown in Fig-ure 2.
Figure 2 plots the accuracy level of extract-ing SCCs by SCC?s length.
It is clear from Fig-ure 2 that as SCCs get longer, the F-measure valuedrops progressively for both WSJ and Switch-board.
Again, Roland and Jurafsky (1998) havesuggested that one major subcategorization differ-ence between written and spoken corpora is thatspoken corpora have a much higher percentage ofthe zero-anaphora construction.
We then exam-ined the distribution of SCCs of different length inWSJ and Switchboard.
Figure 3 shows that SCCsof length 02 account for a much higher percentagein Switchboard than WSJ, but it is always the otherway around for SCCs of non-zero length.
Thisobservation led us to believe that the better per-formance that Bikel?s parser achieves in extractingSCCs from Switchboard may be attributed to thefollowing two factors:1.
Switchboard has a much higher percentage ofSCCs of length 0.2.
The parser is very accurate in extractingshorter SCCs.2Verbs have a length-0 SCC if they are intransitive andhave no modifiers.5170 1 2 3 4102030405060708090Length of SCCF?measure(%)WSJSwitchboardFigure 2: F-measure for SCCs of different length0 1 2 3 40102030405060Length of SCCsPercentage(%)WSJSwitchboardFigure 3: Distribution of SCCs by length3.2 Extraction of DependentsIn order to estimate the effects of SCCs of length0, we examined the parser?s performance in re-trieving dependents of verbs.
Every constituent(whether an argument or adjunct) in an SCC gen-erated by the parser is considered a dependent ofthat verb.
SCCs of length 0 will be discounted be-cause verbs that do not take any arguments or ad-juncts have no dependents3 .
In addition, this wayof evaluating the extraction of SCCs also matchesthe practice in some NLP tasks such as semanticrole labeling (Xue and Palmer, 2004).
For the taskof semantic role labeling, the total number of de-pendents correctly retrieved from the parser?s out-put affects the accuracy level of the task.To do this, we calculated the number of depen-dents shared by between each SCC proposed fromthe parser?s output and its corresponding SCC pro-3We are aware that subjects are typically also consid-ered dependents, but we did not include subjects in ourexperimentsshared-dependents[i.j] = MAX(shared-dependents[i-1,j],shared-dependents[i-1,j-1]+1 if target[i] = source[j],shared-dependents[i-1,j-1] if target[i] != source[j],shared-dependents[i,j-1])Table 3: The algorithm for computing shared de-pendentsINF #5 1 1 2 3ADVP #4 1 1 2 2PP-in #3 1 1 2 2NP #2 1 1 1 1NP #1 1 1 1 1#0 #1 #2 #3 #4NP S-that PP-in INFTable 4: An example of computing the number ofshared dependentsposed from Penn Treebank.
We based our cal-culation on a modified version of Minimum EditDistance Algorithm.
Our algorithm works by cre-ating a shared-dependents matrix with one col-umn for each constituent in the target sequence(SCCs proposed from Penn Treebank) and onerow for each constituent in the source sequence(SCCs proposed from the parser?s output).
Eachcell shared-dependent[i,j] contains the number ofconstituents shared between the first i constituentsof the target sequence and the first j constituents ofthe source sequence.
Each cell can then be com-puted as a simple function of the three possiblepaths through the matrix that arrive there.
The al-gorithm is illustrated in Table 3.Table 4 shows an example of how the algo-rithm works with NP-S-that-PP-in-INF as the tar-get sequence and NP-NP-PP-in-ADVP-INF as thesource sequence.
The algorithm returns 3 as thenumber of dependents shared by two SCCs.We compared the performance of Bikel?s parserin retrieving dependents from written and spo-ken language over all three models using De-pendency Recall (DR) and Dependency Precision(DP).
These metrics are defined as follows:DR = number of correct dependents from parser?s outputnumber of dependents from treebank parse(4)DP = number of correct dependents from parser?s outputnumber of dependents from parser?s output(5)Dependency F-measure = 2 ?DR ?DPDR +DP (6)518punc no?punc punc?no?punc7880828486ModelsF?measure(%)WSJSwitchboardFigure 4: F-measure for extracting dependentsThe results of Bikel?s parser in retrieving depen-dents are summarized in Figure 4.
Overall, theparser achieves a better performance for WSJ overall three models, just the opposite of what havebeen observed for SCC extraction.
Interestingly,removing punctuation from both the training andtest data actually slightly improves the F-measure.This holds true for both WSJ and Switchboard.This Dependency F-measure differs in detail fromsimilar measures in Xue and Palmer (2004).
Forpresent purposes all that matters is the relativevalue for WSJ and Switchboard.4 Extraction of SCFs from SpokenLanguageOur experiments indicate that the SCCs generatedby the parser from spoken language are as accurateas those generated from written texts.
Hence, wewould expect that the current technology for ex-tracting SCFs, initially designed for written texts,should work equally well for spoken language.We previously built a system for automatically ex-tracting SCFs from spoken BNC, and reported ac-curacy comparable to previous systems that workwith only written texts (Li and Brew, 2005).
How-ever, Korhonen (2002) has shown that a directcomparison of different systems is very difficult tointerpret because of the variations in the numberof targeted SCFs, test verbs, gold standards and inthe size of the test data.
For this reason, we applyour SCF acquisition system separately to a writtenand spoken corpus of similar size from BNC andcompare the accuracy of acquired SCF sets.4.1 OverviewAs noted above, previous studies on automatic ex-traction of SCFs from corpora usually proceed intwo steps and we adopt this approach.1.
Hypothesis Generation: Identify all SCCsfrom the corpus data.2.
Hypothesis Selection: Determine which SCCis a valid SCF for a particular verb.4.2 SCF Extraction SystemWe briefly outline our SCF extraction systemfor automatically extracting SCFs from corpora,which was based on the design proposed inBriscoe and Carroll (1997).1.
A Statistical Parser: Bikel?s parser is usedto parse input sentences.2.
An SCF Extractor: An extractor is use toextract SCCs from the parser?s output.3.
An English Lemmatizer: MORPHA (Min-nen et al, 2000) is used to lemmatize eachverb.4.
An SCF Evaluator: An evaluator is usedto filter out false SCCs based on their like-lihood.An SCC generated by the parser and extractormay be a correct SCC, or it may contain an ad-junct, or it may simply be wrong due to tagging orparsing errors.
We therefore need an SCF evalua-tor capable of filtering out false cues.
Our evalu-ator has two parts: the Binomial Hypothesis Test(Brent, 1993) and a back-off algorithm (Sarkar andZeman, 2000).1.
The Binomial Hypothesis Test (BHT): Letp be the probability that an scfi occurs withverbj that is not supposed to take scfi.
If averb occurs n times and m of those times itco-occurs with scfi, then the scfi cues arefalse cues is estimated by the summation ofthe binomial distribution for m ?
k ?
n:P (m+, n, p) =nXk=mn!k!(n?
k)!pk(1?
p)(n?k) (7)If the value of P (m+, n, p) is less than orequal to a small threshold value, then the nullhypothesis that verbj does not take scfi is ex-tremely unlikely to be true.
Hence, scfi isvery likely to be a valid SCF for verbj .
The519SCCs SCFsNP-PP-beforeNP-S-when NPNP-PP-at-S-beforeNP-PP-to-S-whenNP-PP-to-PP-at NP-PP-toNP-PP-to-S-because-ADVPTable 5: SCCs and correct SCFs for introducecorpus WC SCnumber of verb tokens 115,524 109,678number of verb types 5,234 4,789verb types seen more than 10 times 1,102 998number of acquired SCFs 2,688 1,984average number of SCFs per verb 2.43 1.99Table 6: Training data for WC and SCvalue of m and n can be directly computedfrom the extractor?s output, but the value ofp is not easy to obtain.
Following Manning(1993), we empirically determined the valueof p. It was between 0.005 to 0.4 depend-ing on the likelihood of an SCC being a validSCF.2.
Back-off Algorithm: Many SCCs generatedby the parser and extractor tend to containsome adjuncts.
However, for many SCCs,one of its subsets is likely to be the correctSCF.
Table 5 shows some SCCs generated bythe extractor and the corresponding SCFs.The Back-off Algorithm always starts withthe longest SCC for each verb.
Assume thatthis SCC fails the BHT.
The evaluator theneliminates the last constituent from the re-jected cue, transfers its frequency to its suc-cessor and submits the successor to the BHTagain.
In this way, frequency can accumulateand more valid frames survive the BHT.4.3 Results and DiscussionWe evaluated our SCF extraction system on writ-ten and spoken BNC.
We chose one million wordwritten corpus (WC) and a comparable spokencorpus (SC) from BNC.
Table 6 provides relevantinformation on the two corpora.
We only keep theverbs that occur at least 10 times in our trainingdata.To compare the performance of our system onWC and SC, we calculated the type precision, typegold standard COMLEX Manually Constructedcorpus WC SC WC SCtype precision 93.1% 92.9% 93.1% 92.9%type recall 49.2% 47.7% 56.5% 57.6%F-measure 64.4% 63.1% 70.3% 71.1%Table 7: Type precision and recall and F-measurerecall and F-measure.
Type precision is the per-centage of SCF types that our system proposeswhich are correct according some gold standardand type recall is the percentage of correct SCFtypes proposed by our system that are listed in thegold standard.
We used the 14 verbs 4 selectedby Briscoe and Carroll (1997) and evaluated ourresults of these verbs against the SCF entries intwo gold standards: COMLEX (Grishman et al,1994) and a manually constructed SCF set fromthe training data.
It makes sense to use a manuallyconstructed SCF set while calculating type preci-sion and recall because some of the SCFs in a syn-tax dictionary such as COMLEX might not occurin the training data at all.
We constructed separateSCF sets for the written and spoken BNC.The results are summarized in Table 7.
Asshown in Table 7, the accuracy achieved for WCand SC are very comparable: Our system achievesa slightly better result for WC when using COM-LEX as the gold standard and for SC when usingmanually constructed SCF set as gold standard,suggesting that it is feasible to apply the currenttechnology for automatically extracting SCFs tospoken language.5 Conclusions and Future Work5.1 Use of Parser?s OutputIn this paper, we have shown that it is not nec-essarily true that statistical parsers always per-form worse when dealing with spoken language.The conventional accuracy metrics for parsing(LR/LP) should not be taken as the only metricsin determining the feasibility of applying statisti-cal parsers to spoken language.
It is necessary toconsider what information we want to extract outof parsers?
output and make use of.1.
Extraction of SCFs from Corpora: This tasktakes SCCs generated by the parser and ex-tractor as input.
Our experiments show that4The 14 verbs used in Briscoe and Carroll (1997) are ask,begin, believe, cause, expect, find, give, help, like, move, pro-duce, provide, seem and sway.
We replaced sway with showbecause sway occurs less than 10 times in our training data.520the SCCs generated for spoken language areas accurate as those generated for written lan-guage.
We have also shown that it is feasibleto apply the current SCF extraction technol-ogy to spoken language.2.
Semantic Role Labeling: This task usuallyoperates on parsers?
output and the numberof dependents of each verb that are correctlyretrieved by the parser clearly affects the ac-curacy of the task.
Our experiments showthat the parser achieves a much lower accu-racy in retrieving dependents from the spokenlanguage than written language.
This seemsto suggest that a lower accuracy is likely tobe achieved for a semantic role labeling taskperformed on spoken language.
We are notaware that this has yet been tried.5.2 Punctuation and Speech TranscriptionPracticeBoth our experiments and Roark?s experimentsshow that parsing accuracy measured by LR/LPexperiences a sharper decrease for WSJ thanSwitchboard after we removed punctuation fromtraining and test data.
In spoken language, com-mas are largely used to delimit disfluency ele-ments.
As noted in Engel et al (2002), statis-tical parsers usually condition the probability ofa constituent on the types of its neighboring con-stituents.
The way that commas are used in speechtranscription seems to have the effect of increasingthe range of neighboring constituents, thus frag-menting the data and making it less reliable.
Onthe other hand, in written texts, commas serve asmore reliable cues for parsers to identify phrasaland clausal boundaries.In addition, our experiment demonstrates thatpunctuation does not help much with extraction ofSCCs from spoken language.
Removing punctu-ation from both the training and test data resultsin rougly a 0.3% decrease in SR/SP.
Furthermore,removing punctuation from both training and testdata actually slightly improves the performanceof Bikel?s parser in retrieving dependents fromspoken language.
All these results seem to sug-gest that adding punctuation in speech transcrip-tion is of little help to statistical parsers includ-ing at least three state-of-the-art statistical parsers(Collins, 1999; Charniak, 2000; Bikel, 2004).
As aresult, there may be other good reasons why some-one who wants to build a Switchboard-like corpusshould choose to provide punctuation, but there isno need to do so simply in order to help parsers.However, segmenting utterances into individualunits is necessary because statistical parsers re-quire sentence boundaries to be clearly delimited.Current statistical parsers are unable to handle aninput string consisting of two sentences.
For ex-ample, when presented with an input string as in(1) and (2), if the two sentences are separated by aperiod (1), Bikel?s parser wrongly treats the sec-ond sentence as a sentential complement of themain verb like in the first sentence.
As a result, theextractor generates an SCC NP-S for like, which isincorrect.
The parser returns the same parse afterwe removed the period (2) and let the parser parseit again.
(1) I like the long hair.
It was back in highschool.
(2) I like the long hair It was back in high school.Hence, while adding punctuation in transcribinga Switchboard-like corpus is not of much help tostatistical parsers, segmenting utterances into in-dividual units is crucial for statistical parsers.
Infuture work, we plan to develop a system capa-ble of automatically segmenting speech utterancesinto individual units.6 AcknowledgmentsThis study was supported by NSF grant 0347799.Our thanks go to Eric Fosler-Lussier, Mike Whiteand three anonymous reviewers for their valuablecomments.ReferencesD.
Bikel.
2004.
Intricacies of Collin?s parsing models.Computational Linguistics, 30(2):479?511.M.
Brent.
1993.
From grammar to lexicon: Unsu-pervised learning of lexical syntax.
ComputationalLinguistics, 19(3):243?262.T.
Briscoe and J. Carroll.
1997.
Automatic extractionof subcategorization from corpora.
In Proceedingsof the 5th ACL Conference on Applied Natural Lan-guage Processing, pages 356?363.E.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 2000 Conference ofthe North American Chapter of the Association forComputation Linguistics, pages 132?139.M.
Collins.
1999.
Head-driven statistical models fornatural language parsing.
Ph.D. thesis, Universityof Pennsylvania.521D.
Engel, E. Charniak, and M. Johnson.
2002.
Parsingand disfluency placement.
In Proceedings of 2002Conference on Empirical Methods of Natural Lan-guage Processing, pages 49?54.J.
Godefrey, E. Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus forresearch and development.
In Proceedings ofICASSP-92, pages 517?520.R.
Grishman, C. Macleod, and A. Meryers.
1994.Comlex syntax: Building a computational lexicon.In Proceedings of the 1994 International Conferenceof Computational Linguistics, pages 268?272.A.
Korhonen.
2002.
Subcategorization Acquisition.Ph.D.
thesis, Cambridge University.M.
Lapata and C. Brew.
2004.
Verb class disambigua-tion using informative priors.
Computational Lin-guistics, 30(1):45?73.J.
Li and C. Brew.
2005.
Automatic extraction of sub-categorization frames from spoken corpora.
In Pro-ceedings of the Interdisciplinary Workshop on theIdentification and Representation of Verb Featuresand Verb Classes, Saarbracken, Germany.C.
Manning.
1993.
Automatic extraction of a largesubcategorization dictionary from corpora.
In Pro-ceedings of 31st Annual Meeting of the Associationfor Computational Linguistics, pages 235?242.M.
Marcus, G. Kim, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English:the Penn Treebank.
Computational Linguistics,19(2):313?330.P.
Merlo and S. Stevenson.
2001.
Automaticverb classification based on statistical distributionof argument structure.
Computational Linguistics,27(3):373?408.P.
Merlo, E. Joanis, and J. Henderson.
2005.
Unsuper-vised verb class disambiguation based on diathesisalternations.
manuscripts.G.
Minnen, J. Carroll, and D. Pearce.
2000.
Appliedmorphological processing of English.
Natural Lan-guage Engineering, 7(3):207?223.V.
Punyakanok, D. Roth, and W. Yih.
2005.
The neces-sity of syntactic parsing for semantic role labeling.In Proceedings of the 2nd Midwest ComputationalLinguistics Colloquium, pages 15?22.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of the Con-ference on Empirical Methods of Natural LanguageProcessing, pages 133?142.B.
Roark.
2001.
Robust Probabilistic PredictiveProcessing: Motivation, Models, and Applications.Ph.D.
thesis, Brown University.D.
Roland and D. Jurafsky.
1998.
How verb sub-categorization frequency is affected by the corpuschoice.
In Proceedings of the 17th InternationalConference on Computational Linguistics, pages1122?1128.A.
Sarkar and D. Zeman.
2000.
Automatic extractionof subcategorization frames for Czech.
In Proceed-ings of the 19th International Conference on Com-putational Linguistics, pages 691?697.S.
Schulte im Walde.
2000.
Clustering verbs semanti-cally according to alternation behavior.
In Proceed-ings of the 18th International Conference on Com-putational Linguistics, pages 747?753.N.
Xue and M. Palmer.
2004.
Calibrating features forsemantic role labeling.
In Proceedings of 2004 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 88?94.522
