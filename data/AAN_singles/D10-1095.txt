Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 971?981,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsConfidence in Structured-Prediction using Confidence-Weighted ModelsAvihai MejerDepartment of Computer ScienceTechnion-Israel Institute of TechnologyHaifa 32000, Israelamejer@tx.technion.ac.ilKoby CrammerDepartment of Electrical EngineeringTechnion-Israel Institute of TechnologyHaifa 32000, Israelkoby@ee.technion.ac.ilAbstractConfidence-Weighted linear classifiers (CW)and its successors were shown to performwell on binary and multiclass NLP prob-lems.
In this paper we extend the CW ap-proach for sequence learning and show that itachieves state-of-the-art performance on fournoun phrase chucking and named entity recog-nition tasks.
We then derive few algorith-mic approaches to estimate the prediction?scorrectness of each label in the output se-quence.
We show that our approach providesa reliable relative correctness information asit outperforms other alternatives in rankinglabel-predictions according to their error.
Wealso show empirically that our methods outputclose to absolute estimation of error.
Finally,we show how to use this information to im-prove active learning.1 IntroductionIn the past decade structured classification has seenmuch interest by the machine learning community.After the introduction of conditional random fields(CRFs) (Lafferty et al, 2001), and maximum mar-gin Markov networks (Taskar et al, 2003), whichare batch algorithms, new online method were in-troduced.
For example the passive-aggressive algo-rithm was adapted to chunking (Shimizu and Haas,2006), parsing (McDonald et al, 2005b), learningpreferences (Wick et al, 2009) and text segmenta-tion (McDonald et al, 2005a).
These new onlinealgorithms are fast to train and simple to implement,yet they generate models that output merely a pre-diction with no additional information, as opposedto probabilistic models like CRFs or HMMs.In this work we fill this gap proposing few al-ternatives to compute confidence in the output ofdiscriminative non-probabilistic algorithms.
As be-fore, our algorithms output the highest-scoring la-beling.
However, they also compute additional la-belings, that are used to compute the per word con-fidence in its labelings.
We build on the recentlyintroduced confidence-weighted learning (Dredze etal., 2008; Crammer et al, 2009b) and induce a dis-tribution over labelings from the distribution main-tained over weight-vectors.We show how to compute confidence estimatesin the label predicted per word, such that the con-fidence reflects the probability that the label is notcorrect.
We then use this confidence informationto rank all labeled words (in all sentences).
Thiscan be thought of as a retrieval of the erroneouswords, which can than be passed to human anno-tator for an examination, either to correct these mis-takes or as a quality control component.
Next, weshow how to apply our techniques to active learningover sequences.
We evaluate our methods on fourNP chunking and NER datasets and demonstrate theusefulness of our methods.
Finally, we report theperformance of obtained by CW-like adapted to se-quence prediction, which are comparable with cur-rent state-of-the-art algorithms.2 Confidence-Weighted LearningConsider the following online binary classificationproblem that proceeds in rounds.
On the ith roundthe online algorithm receives an input xi ?
Rd and971applies its current rule to make a prediction y?i ?
Y ,for the binary set Y = {?1,+1}.
It then receivesthe correct label yi ?
Y and suffers a loss `(yi, y?i).At this point, the algorithm updates its predictionrule with the pair (xi, yi) and proceeds to the nextround.
A summary of online algorithms can befound in (Cesa-Bianchi and Lugosi, 2006).Online confidence-weighted (CW) learning(Dredze et al, 2008; Crammer et al, 2008),generalized the passive-aggressive (PA) updateprinciple to multivariate Gaussian distributionsover the weight vectors - N (?,?)
- for binaryclassification.
The mean ?
?
Rd contains thecurrent estimate for the best weight vector, whereasthe Gaussian covariance matrix ?
?
Rd?d capturesthe confidence in this estimate.
More precisely,the diagonal elements ?p,p, capture the confidencein the value of the corresponding weight ?p ; thesmaller the value of ?p,p, is, the more confidentis the model in the value of ?p.
The off-diagonalelements ?p,q for p 6= q capture the correlationbetween the values of ?p and ?q.
When the datais of large dimension, such as in natural languageprocessing, a model that maintains a full covariancematrix is not feasible and we back-off to diagonalcovariance matrices.CW classifiers are trained according to a PA rulethat is modified to track differences in Gaussian dis-tributions.
At each round, the new mean and co-variance of the weight vector distribution is chosento be the solucion of an optimization problem (see(Crammer et al, 2008) for details).
This particu-lar CW rule may over-fit by construction.
A morerecent alternative scheme called AROW (adaptiveregularization of weight-vectors) (Crammer et al,2009b) replaces the guaranteed prediction at eachround with the a more relaxed objective (see (Cram-mer et al, 2009b)).
AROW has been shown toperform well in practice, especially for noisy datawhere CW severely overfits.The solution for the updates of CW and AROWshare the same general form,?i+1 =?i+?i?iyixi ; ?
?1i+1 =?
?1i+1+?ixix>i , (1)where the difference between CW and AROW is thespecific instance-dependent rule used to set the val-ues of ?i and ?i.Algorithm 1 Sequence Labeling CW/AROWInput: Joint feature mapping ?
(x,y) ?
RdInitial variance a > 0Tradeoff Parameter r > 0 (AROW)or Confidence parameter ?
(CW)Initialize: ?0 = 0 , ?0 = aIfor i = 1, 2 .
.
.
doGet xi ?
XPredict best labelingy?i = arg maxz ?i?1 ??
(xi, z)Get correct labeling yi ?
Y|xi|Define ?i,y,y?
= ?(x,yi)??
(x, y?i)Compute ?i and ?i (Eq.
(3) for CW ;Eqs.
(4),?i = 1/r) for AROW)Set ?i = ?i?1 + ?i?i?1?i,y,y?Set ?
?1i = ?
?1i?1 + ?i?i,y,y?
?>i,y,y?end for3 Sequence LabelingIn the sequence labeling setting, instances x be-long to a general input space X and conceptually arecomposed of a finite number n of components, suchas words of a sentence.
The number of componentsn = |x| varies between instances.
Each part of aninstance is labelled from a finite set Y , |Y| = K.That is, a labeling of an entire instance belongs tothe product set y ?
Y ?
Y .
.
.Y (n times).We employ a general approach (Collins, 2002;Crammer et al, 2009a) to generalize binary clas-sification and use a joined feature mapping of aninstance x and a labeling y into a common vectorspace, ?
(x,y) ?
Rd.Given an input instance x and a model ?
?
Rdwe predict the labeling with the highest score, y?
=arg maxz ?
??
(x, z).
A brute-force approach eval-uates the value of the score ?
??
(x, z) for each pos-sible labeling z ?
Yn, which is not feasible for largevalues of n. Instead, we follow standard factoriza-tion and restrict the joint mapping to be of the form,?
(x,y) =?np=1 ?
(x, yp)+?nq=2 ?
(x, yq, yq?1).That is, the mapping is a sum of mappings, each tak-ing into consideration only a label of a single part, ortwo consecutive parts.
The time required to computethe max operator is linear in n and quadratic in Kusing the dynamic-programming Viterbi algorithm.After the algorithm made a prediction, it uses972the current labeled instance (xi,yi) to update themodel.
We now define the update rule both for aversion of CW and for AROW for strucutred learn-ing, staring with CW.
Given the input parameter ?of CW we denote by ??
= 1 + ?2/2, ???
= 1 + ?2.We follow a similar argument as in the single up-date of (Crammer et al, 2009a, sec.
5.1) to se-quence labeling by a reduction to binary classifica-tion.
We first define the difference between the fea-ture vector associated with the current labeling yiand the feature vector associated with some label-ing z to be, ?i,y,z = ?
(x,yi) ?
?
(x, z) , andin particular, when we use the prediction y?i we get,?i,y,y?
= ?(x,yi)??
(x, y?i) .
The CW update is,?i = ?i?1 + ?i?i?1?i,y,y??
?1i = ?
?1i?1 + ?i?i,y,y??>i,y,y?
, (2)where the two scalars ?i and ?i are set using theupdate rule defined by (Crammer et al, 2008) forbinary classification,vi = ?>i,y,y??i?1?i,y,y?
, mi = ?i?1 ??i,y,y?
(3)?i = max{0,1vi???(?mi??
+?m2i?44+ vi?2???
)}?i =?i?
?v+i, v+i =14(??ivi?+?
?2i v2i ?2 + 4vi)2We turn our attention and describe a mod-ification of AROW for sequence prediction.Replacing the binary-hinge loss in (Crammeret al, 2009b, Eqs.
(1,2)) the first one with thecorresponding multi-class hinge loss for structuredproblems we obtain, 12 (?i??)>?
?1i (?i??)
+12r (max {0,maxz 6=y {d(y, z)?
?
?
(?i,y,z)}})2,where, d(y, z) =?|x|q=1 1yq 6=zq , is the hammingdistance between the two label sequences y and z.The last equation is hard to optimize since the maxoperator is enumerating over exponential number ofpossible labellings z.
We thus approximate the enu-meration over all possible z with the predicted labelsequence y?i and get, 12 (?i??)>?
?1i (?i??)
+12r(max{0, d(yi, y?i)?
?
?(?i,y,y?)})2.
Com-puting the optimal value of the last equation we getan update of the form of the first equation of Eq.
(2)where?i =max{0, d(yi, y?i)?
?i?1 ?(?i,y,y?
)}r + ?>i,y,y??i?1?i,y,y?.
(4)Dataset Sentences Words FeaturesNP chunking 11K 259K 1.35MNER English 17.5K 250K 1.76MNER Spanish 10.2K 317.6K 1.85MNER Dutch 21K 271.5K 1.76MTable 1: Properties of datasets.AROW CW 5-best PA PerceptronNP chunking 0.946 0.947 0.946 **0.944NER English 0.878 0.877 * 0.870 * 0.862NER Dutch 0.791 0.787 0.784 * 0.761NER Spanish 0.775 0.774 0.773 * 0.756Table 2: Averaged F-measure of methods.
Statistical sig-nificance (t-test) are with respect to AROW, where * in-dicates 0.001 and ** indicates 0.01We proceed with the confidence paramters in(Crammer et al, 2009b, Eqs.
(1,2)), which takes intoconsidiration the change of confidence due to the up-date.
The effective features vector that is used toupdate the mean parameters is ?i,y,y?, and thus thestructured update is, 12 log(det ?idet ?)+12Tr(??1i?1?)+12r?>i,y,y???i,y,y?
.
Solving the above equation weget an update of the form of the second term ofEq.
(2) where ?i = 1r .
The pseudo-code of CW andAROW for sequence problems appears in Alg.
1.4 EvaluationFor the experiments described in this paper we usedfour large sequential classification datasets takenfrom the CoNLL-2000, 2002 and 2003 shared tasks:noun-phrase (NP) chunking (Kim et al, 2000),and named-entity recognition (NER) in Spanish,Dutch (Tjong and Sang, 2002) and English (Tjonget al, 2003).
The properties of the four datasetsare summarized in Table 1.
We followed the featuregeneration process of (Sha and Pereira, 2003).Although our primary goal is estimating confi-dence in prediction and not the actual performanceitself, we first report the results of using AROW andCW for sequence learning.
We compared the perfor-mance CW and AROW of Alg.
1 with two standardonline baseline algorithms: Averaged-Perceptron al-gorithm and 5-best PA (the value of five was shownto be optimal for various tasks (Crammer et al,2005)).
The update rule described in Alg.
1 assumesa full covariance matrix, which is not feasible in our9730.934 0.936 0.938 0.94 0.942 0.9440.9360.9380.940.9420.9440.9460.948RecallPrecisionPerceptronPACWAROW(a) NP Chunking0.81 0.82 0.83 0.84 0.85 0.860.820.830.840.850.860.870.880.89RecallPrecisionPerceptronPACWAROW(b) NER English0.68 0.7 0.72 0.74 0.760.70.720.740.760.780.8RecallPrecisionPerceptronPACWAROW(c) NER Dutch0.7 0.72 0.74 0.760.70.720.740.760.78RecallPrecisionPerceptronPACWAROW(d) NER SpanishFigure 1: Precision and Recall on four datasets (four panels).
Each connected set of ten points corresponds to the performance ofa specific algorithm after each of the 10 iterations, increasing from bottom-left to top-right.Prec Recall F-meas % ErrCW 0.945 0.942 0.943 2.34%NP chunkingCRF 0.938 0.934 0.936 2.66%CW 0.838 0.826 0.832 3.38%NER EnglishCRF 0.823 0.820 0.822 3.53%CW 0.803 0.755 0.778 2.05%NER DutchCRF 0.775 0.753 0.764 2.09%CW 0.738 0.720 0.729 4.09%NER SpanishCRF 0.751 0.730 0.740 2.05%Table 3: Precision, Recall, F-measure and percentage ofmislabeled words results of CW vs. CRFsetting.
Three options are possible: compute a full ?and then take its diagonal elements; compute a fullinverse ?, take its diagonal elements and then com-pute its inverse; assume that ?
is diagonal and com-pute the optimal update for this choice.
We foundthe first method to work best, and thus employ itfrom now on.The hyper parameters (r for AROW, ?
for CW, Cfor PA) were tuned for each task by a single run overa random split of the data into a three-fourths train-ing set and a one-fourth test set.
We used parameteraveraging with all methods.For each of the four datasets we used 10-foldcross validation.
All algorithms (Perceptron, PA,CW and AROW) are online, and as mentioned abovework in rounds.
For each of the ten folds, each of thefour algorithm performed ten (10) iterations over thetraining set and the performance (Recall, Precisionand F-measure) was evaluated on the test set aftereach iteration.The F-measure of the four algorithms after 10 it-erations over the four datasets is summarized in Ta-ble 2.
The general trend is that AROW slightly out-performs CW, which is better than PA that is bet-ter than the Perceptron.
The difference betweenAROW and the Perceptron is significant, and be-tween AROW and PA is significant in two datasets.The difference between AROW and CW is not sig-nificant although it is consistent.We further investigate the convergence propertiesof the algorithms in Fig.
1.
The figure shows the re-call and precision results after each training roundaveraged across the 10 folds.
Each panel summa-rizes the results on a single dataset, and in each panela single set of connected points corresponds to onealgorithm.
Points in the left-bottom of the plot cor-respond to early iterations and points in the right-topcorrespond to later iterations.
Long segments indi-cate a big improvement in performance between twoconsecutive iterations.Few points are in order.
First, high (in the y-axis)values indicate better precision and right (in the x-axis) values indicate better recall.
Second, the per-formance of all algorithms is converging in about 10iterations as indicated by the fact the points in thetop-right of the plot are close to each other.
Third,the long segments in the bottom-left for the Percep-tron algorithm indicate that this algorithm benefitsmore from more than one pass compared with theother.
Fourth, on the three NER datasets after 10 it-erations AROW gets slightly higher precision valuesthan CW, while CW gets slightly higher recall val-ues than AROW.
This is indicated by the fact thatthe top-right red square is left and above to the top-right blue circle.
Finally, in two datasets, PA getslightly better recall than CW and AROW, but pay-ing in terms of precision and overall F-measure per-formance.In addition to online algorithms we also com-pared the performance of CW with the CRF algo-974NP chunking NER English NER Spanish NER Dutch00.050.10.150.20.250.30.350.40.450.50.55CRFKD?Fixed (K=50)KD?PC (K=50)DeltaWKBV (K=30)KBV (K=30)Random(a) AvgP CW & CRFNP chunking NER English NER Spanish NER Dutch00.020.040.060.080.1Root Mean Squared Errorin ConfidenceCRFKD?Fixed (K=50)KD?PC (K=50)WKBV (K=30)(b) RMSE CW & CRFNP chunking NER English NER Spanish NER Dutch00.050.10.150.20.250.30.350.40.450.50.55KD?Fixed (K=50)DeltaWKBV (K=30)KBV (K=30)Random(c) AvgP PANP chunking NER English NER Spanish NER Dutch00.020.040.060.080.1Root Mean Squared Errorin ConfidenceKD?Fixed (K=50)WKBV (K=30)(d) RMSE PAFigure 2: Two left panels: average precision of rankings ofthe words of the test-set according to confidence in the predic-tion of seven methods (left to right bars in each group): CRF,KD-Fixed, KD-PC, Delta, WKBV, KBV and random ordering,when training with the CW algorithm (top) and the PA algo-rithm (bottom).
Two right panels: The root-mean-squared-errorof four methods that output absolute valued confidence: CRF,KD-Fixed, KD-PC and WKBV.rithm which is a batch algorithm.
We used Mal-let toolkit (McCallum, 2002) for CRF implementa-tion.
For feature generation we used a combinationof standard methods provided with Mallet toolkit(called pipes).
We chose a combination yielding afeature set that is close as possible to the featureset we used in our system but it was not a perfectmatch, CRF generated about 20% fewer features inall datasets.
Nevertheless, any other combination ofpipes we tried only hurt CRF performance.
The pre-cision, recall, F-measure and percentage of misla-beled words of CW algorithm compared with CRFmeasured over a single split of the data into a three-fourths training set and a one-fourth test set is sum-marized in Table 3.
We see that in three of the fourdatasets CW outperforms CRF and in one datasetCRF performs better.
Some of the performance dif-ferences may be due to the differences in features.5 Confidence in the PredictionMost large-margin-based training algorithms outputmodels that their prediction is a single labeling ofthe input, with no additional confidence informationabout the correctness of that prediction.
This situ-ation is acceptable when the output of the systemis used anyway, irrespectively of its quality.
Thissituation is not acceptable when the output of thesystem is used as an input of another system that issensitive to correctness of the specific prediction orthat integrates various input sources.
In such cases,additional confidence information about the correct-ness of these feeds for specific input can be usedto improve the total output quality.
Another casewhere such information is useful, is when there isadditional agent that is validating the output of thesystem.
The confidence information can be usedto direct the check into small number of suspectedpredictions as opposed to random check, which maymiss errors if their rate is small.Some methods only provide relative confidenceinformation.
This information can be used to rankall predictions according to their confidence score,which can be used to direct a quality control com-ponent to detect errors in the prediction.
Note,the confidence score is meaningless by itself andin fact, any monotonic transformation of the con-fidence scores yield equivalent confidence informa-tion.
Other methods are providing confidence in thepredicted output as an absolute information, that is,the probability of a prediction to be correct.
We re-fer to these probabilistic outputs in a frequentists ap-proach.
When taking a large set of events (predic-tions) with similar probability confidence value ?
ofbeing correct, we expect that about ?
fraction of thepredictions in the group will be correct.Algorithms: All of our methods to evaluate confi-dence, except two (Delta and CRF below), share thesame conceptual approach and work in two stages.First, a method generates a set of K possible label-ings for the input sentence (instead of a single pre-diction).
Then, the confidence in a predicted label-ing for a specific word is defined to be the proportionof labelings which are consistent with the predictedlabel.
Formally, let z(i) for i = 1 .
.
.K be the Klabelings for some input x, and let y?
be the actualprediction for the input.
(We do not assume thaty?
= z(i) for some i).
The confidence in the labely?p of word p = 1 .
.
.
|x| is defined to be?p = |{i : y?p = z(i)p }|/K .
(5)9751000 2000 3000 4000 500002004006008001000Word IndexNo.
of Words Classified IncorrectlyCRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom(a) NP Chunking1000 2000 3000 4000 500002004006008001000120014001600Word IndexNo.
of Words Classified IncorrectlyCRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom(b) NER English1000 2000 3000 4000 50000200400600800100012001400Word IndexNo.
of Words Classified IncorrectlyCRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom(c) NER Dutch1000 2000 3000 4000 5000050010001500Word IndexNo.
of Words Classified IncorrectlyCRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom(d) NER Spanish1000 2000 3000 4000 5000?100?50050100150200250Word IndexNo.
of Words Classified IncorrectlyCRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom(e) NP Chunking1000 2000 3000 4000 5000?100?50050100150200250300Word IndexNo.
of Words Classified IncorrectlyCRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom(f) NER English1000 2000 3000 4000 5000?100?50050100Word IndexNo.
of Words Classified IncorrectlyCRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom(g) NER Dutch1000 2000 3000 4000 5000?100?50050100150200Word IndexNo.
of Words Classified IncorrectlyCRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom(h) NER SpanishFigure 3: Total number of detected erroneous words vs. the number of ranked words (top panels), and relative to the Delta method(bottom panels).
In other words, the lines in the bottom panels are the number of additional erroneous words detected compared toDelta method.
All methods builds on the same weight-vector except CRF (see text).We tried four approaches to generate the set of Kpossible labelings.
The first method is valid onlyfor methods that induce a probability distributionover predicted labels.
In this case, we draw K la-belings from this distribution.
Specifically, we ex-ploit the Gaussian distribution over weight vectorsw ?
N (?,?)
maintained by AROW and CW, byinducing a distribution over labelings given an in-put.
The algorithm samples K weight vectors ac-cording to this Gaussian distribution and outputs thebest labeling with respect to each weight vector.
For-mally, we define the set Z = {z(i) : z(i) =arg maxzw ??
(x, z) where w ?
N (?,?
)}The predictions of algorithms that use the meanweight vector y?
= arg maxz ?
??
(x, z) are invari-ant to the value of the input ?
(as noted by (Cram-mer et al, 2008)).
However for the purpose of con-fidence estimation the specific value of ?
has a hugeaffect.
Small eigenvalue of ?
yield that all the ele-ments of Z will be the same, while large values yieldrandom elements in the set, ignoring the input.One possible simple option is to run the algorithmfew times, with few possible initializations of ?
andchoose one using the training set.
However since theactual predictions of all these versions is the same(invariance to scaling, see (Crammer et al, 2008))in practice we run the algorithm once initializing?
= I .
Then, after the training is completed, wetry few scalings of the final covariance s?
for somepositive scalar s, and choose the best value s usingthe training set.
We refer to this method as KD-PCfor K-Draws by Parameters Confidence.The second method to estimate confidence fol-lows the same conceptual steps, except that we usedan isotropic covariance matrix, ?
= sI for somepositive scale information s. As before, the valueof s was tuned on the training set.
We denote thismethod KD-Fixed for K Draws by Fixed Stan-dard Deviation.
This method is especially appeal-ing, since it can be used in combination with trainingalgorithms that do not maintain confidence informa-tion, such as the Perceptron or PA.Our third and fourth methods are deterministicand do not involve a stochastic process.
We mod-ified the Viterbi algorithm to output the K distinctlabelings with highest score (computed using themean weight vector in case of CW or AROW).
Thethird method assigns uniform importance to eachof the K labelings ignoring the actual score val-ues.
We call this method KBV, for K-best Viterbi.We thus propose the fourth method in which we de-fine an importance weight ?i to each labeling z(i)and evaluate confidence using the weights, ?p =(?i s.t.
y?p=z(i)p?i)/ (?i ?i) , where we set theweights to be their score value clipped at zero frombelow ?i = max{0,?
?
?
(x, z(i))}.
(In practice,976top score was always positive.)
We call this methodWKBV for weighted K-best Viterbi.In addition to these four methods we propose afifth method that is based on the margin and doesnot share the same conceptual structure of the previ-ous methods.
This method provide confidence scorethat is only relative and not absolute, namely its out-put can be used to compare the confidence in twolabelings, yet there is no semantics defined over thescores.
Given an input sentence to be labeledx and amodel we define the confidence in the prediction as-sociated with the pthword to be the difference in thehighest score and the closest score, where we set thelabel of that word to anything but the label with thehighest score.
Formally, as before we define the bestlabeling y?
= arg maxz ?
?
?
(x, z), then the scoreof word p is defined to be, ???
(x, y?
)?maxu6=y?p ???
(x, z|zp=u) , where we define the labeling z|zp=uto be the labeling that agrees with z on all words,except the pth word, where we define its label tobe u.
We refer to this method as Delta where theconfidence information is a difference, aka as delta,between two score values.Finally, as an additional baseline, we used a sixthmethod based on the confidence values for singlewords produced by CRF model.
We considered themarginal probability of the word p to be assigned thepredicted label y?p to be the confide value, this prob-ability is calculated using the forward-backwards al-gorithm.
This method is close in spirit to the Deltamethod as the later can be thought of computingmarginals (in score, rather than probability).
It alsoclose to the K-Draws methods, as both CRF and K-Draws induce a distribution over labels.
For CRF wecan compute the marginals explicitly, while for theGaussian models generated by CW (or AROW) themarginals can not be computed expliclity, and thus asample based estimation (K-Draws) is used.Experimental Setting: We evaluate the abovemethods as follows.
We trained a classifier usingthe CW algorithm running for ten (10) iterations onthree-fourth of the data and applied it to the remain-ing one-fourth to get a labeling of the test set.
Thereare between 49K ?
54K words to be labeled inall tasks, except NER Dutch where there are about74K words.
The fraction of words for which thetrained model makes a mistake ranges between 2%(for NER Dutch) to 4.1% for NER Spanish.We set the value of the hyper parameter ?
to itsoptimal value obtained in the experiments reportedin the previous section.
The size ofK of the numberof labelings used in the four first methods (KD-PC,KD-Fixed, KBV, WKBV) and the weighting scalars used in KD-PC and KD-Fixed were tuned for eachdataset on a single evaluation on subset of the train-ing set according to the best measured average pre-cision.
For the parameter s we tried about 20 valuesin the range 0.01 to 1.0, and for the number of labelsK we tried the values in 10, 20 .
.
.
80.
The optimalvalues are K = 50 for KD-PC and KD-Fixed, andK = 30 for KBV and WKBV.
We noticed that KD-PC and KD-Fixed were robust to larger values of K,while the performance of KBV and WKBV was de-graded significantly for large values of K.We also trained CRF on the same training sets andapplied it to label and assign confidence values toall the words in the test sets.
The fraction of mis-labeled words produced by the CRF model and theCW model is summarized in Table 3.Relative Confidence: For each of the datasets,we first trained a model using the CW algorithm andapplied each of the confidence methods on the out-put, ranking from low to high all the words of thetest set according to the confidence in the predictionassociated with them.
Ideally, the top ranked wordsare the ones for which the classifier made a mistakeon.
This task can be thought of as a retrieval task ofthe erroneous words.The average precision is the average of the pre-cision values computed at all ranks of erroneouswords.
The average precision for ranking the wordsof the test-set according the confidence in the predic-tion of seven methods appears in the top-left panel ofFig.
2.
(left to right bars in each group : CRF, KD-Fixed, KD-PC, Delta, WKBV, KBV and random or-dering.)
We see that when ordering the words ran-domly, the average precision is about the frequencyof erroneous word, which is the lowest average pre-cision.
Next are the two methods based on the bestViterbi labelings, where the weighted approach out-performing the non-weighted version.
Thus, takingthe actual score value into consideration improvesthe ability to detect erroneous words.
Next in per-formance is Delta, the margin-induced method.
The9770 0.2 0.4 0.6 0.8 100.20.40.60.81Expected Accuracy (bin center)Actual AccuracyWKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF(a) NP Chunking0 0.2 0.4 0.6 0.8 100.20.40.60.81Expected Accuracy (bin center)Actual AccuracyWKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF(b) NER English0 0.2 0.4 0.6 0.8 100.20.40.60.81Expected Accuracy (bin center)Actual AccuracyWKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF(c) NER Dutch0 0.2 0.4 0.6 0.8 100.20.40.60.81Expected Accuracy (bin center)Actual AccuracyWKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF(d) NER SpanishFigure 4: Predicted error in each bin vs. the actual frequency of mistakes in each bin.
Best performance is obtained by methodsclose to the line y = x (black line) for four tasks.
Four methods are compared: weighted K-Viterbi (WKBV), K-draws PC(KD-PC) and K-draws fixed covariance (KD-Fixed) and CRF.two best performing among the CW based methodsare KD-Fixed and KD-PC, where the former is bet-ter in three out of four datasets.
When comparedto CRF we see that in two cases CRF outperformsthe K-Draws based methods and in the other twocases it performs equally.
We found the relative suc-cess of KD-Fixed compared to KD-PC surprising,as KD-Fixed does not take into consideration the ac-tual uncertainty in the parameters learned by CW,and in fact replaced it with a fixed value across allfeatures.
Since this method does not need to as-sume a confidence-based learning approach we re-peated the experiment, training a model with thepassive-aggressive algorithm, rather than CW.
Allconfidence estimation methods can be used exceptthe KD-PC, which does take the confidence infor-mation into consideration.
The results appear inthe bottom-left panel of Fig.
2, and basically tellthe same story, KD-Fixed outperform the marginbased method (Delta), and the Viterbi based meth-ods (KBV, WKBV).To better understand the behavior of the variousmethods we plot the total number of detected erro-neous words vs. the number of ranked words (first5, 000 ranked words) in the top panels of Fig.
3.
Thebottom panels show the relative additional numberof words each methods detects on top of the margin-based Delta method.
Clearly, KD-Fixed and KD-PC detect erroneous words better than the other CWbased methods, finding about 100 more words thanDelta (when ranking 5, 000 words) which is about8% of the total number of erroneous words.Regarding CRF, it outperforms the K-Drawsmethods in NER English and NP chunking datasets,finding about 150 more words, CRF performedequally for NER Dutch, and performed worse forNER Spanish finding about 80 less words.
We em-phasize that all methods except CRF were based onthe same exact weight vector, ranking the same pre-dations, while CRF used an alternative weight vectorthat yields different number of erroneous words.In details, we observe some correlation betweenthe percentage or erroneous words in the entire setand the number of erroneous words detected amongthe first 5, 000 ranked words.
For NP chunkingand NER English datasets, CRF has more erroneouswords compared to CW and it detects more erro-neous words compared to K-Draws.
For NER Dutchdataset CRF and CW have almost same number oferroneous words and almost same number of erro-neous words detected, and finally in NER Spanishdataset CRF has fewer erroneous words and it de-tected less erroneous words.
In other words, wherethere are more erroneous words to find (e.g.
CRF inNP chunking), the task of ranking erroneous wordsis easier, and vice-versa.We hypothesize that part of the performance dif-ferences we see between the K-Draws and CRFmethods is due to the difference in the number oferroneous words in the ranked set.This ranking view can be thought of marking sus-pected words to be evaluated manually by a humanannotator.
Although in general it may be hard for ahuman to annotate a single word with no need to an-notate its close neighbor, this is not the case here.
Asthe neighbor words are already labeled, and prettyreliably, as mentioned above.Absolute Confidence: Our next goal is to eval-uate how reliable are the absolute confidence val-ues output by the proposed methods.
As before, theconfidence estimation methods (KD-PC, KD-Fixed,978KBV, WKBV and CRF) were applied on the entireset of predicted labels.
(Delta method is omitted asthe confidence score it produces is not in [0, 1]).For each of the four datasets and the five algo-rithms we grouped the words according to the valueof their confidence.
Specifically, we used twenty(20) bins dividing uniformly the confidence rangeinto intervals of size 0.05.
For each bin, we com-puted the fraction of words predicted correctly fromthe words assigned to that bin.
Ultimately, the valueof the computed frequency should be about the cen-ter value of the interval of the bin.
Formally, binindexed j contains words with confidence value inthe range [(j ?
1)/20, j/20) for j = 1 .
.
.
20.
Let bjbe the center value of bin j, that is bj = j/20?1/40.The frequency of correct words in bin j, denotedby cj is the fraction of words with confidence ?
?
[(j?1)/20, j/20) that their assigned label is correct.Ultimately, these two values should be the same,bj = cj , meaning that the confidence informationis a good estimator of the frequency of correct la-bels.
Methods for which cj > bj are too pessimistic,predicting too high frequency of erroneous labels,while methods for which cj < bj are too optimistic,predicting too low frequency of erroneous words.The results are summarized in Fig 4, one panelper dataset, where we plot the value of the center-of-bin bj vs. the frequency of correct prediction cj ,connecting the points associated with a single algo-rithm.
Four algorithms are shown: KD-PC, KD-Fixed, WKBV and CRF.
We omit the results of theKBV approach - they were substantially inferior toall other methods.
Best performance is obtainedwhen the resulting line is close to the line y = x.From the plots we observe that WKBV is too pes-simistic as its corresponding line (blue square) isabove the line y = x. CRF method is too optimistic,its corresponding line is below the line y = x.The KD-Fixed method is too pessimistic on NER-Dutch and too optimistic on NER-English.
The bestmethod is KD-PC which, surprisingly, tracks the linex = y pretty closely.
We hypothesis that its superi-ority is because it makes use of the uncertainty infor-mation captured in the covariance matrix ?
which ispart of the Gaussian distribution.Finally, these bins plots does not reflect the factthat different bins were not populated uniformly, thebins with higher values were more heavily popu-lated.
We thus plot in the top-right of Fig.
2 theroot mean-square error in predicting the bin centervalue given by?
(?j nj(bj ?
cj)2)/(?j nj),where nj is the number of words in the jth bin.We observed a similar trend to the one appeared inthe previous figure.
WKBV is the least-performingmethod, then KD-Fixed and CRF, and then KD-PCwhich achieved lowest RMSE in all four datasets.Similar plot but when using PA for training appearin the bottom-right panel of Fig.
2.
In this case wealso see that KD-Fixed is better than WKBV, eventhough both methods were not trained with an algo-rithm that takes uncertainty information into consid-eration, like CW.The success of KD-PC and KD-Fixed in evaluat-ing confidence led us to experiment with using sim-ilar techniques for inference.
Given an input sen-tence, the inference algorithm samplesK times fromthe Gaussian distribution and output the best label-ing according to each sampled weight vector.
Thenthe algorithm predicts for each word the most fre-quent label.
We found this method inferior to infer-ence with the mean parameters.
This approach dif-fers from the one used by (Crammer et al, 2009a),as they output the most frequent labeling in a set,while the predicted label of our algorithm may noteven belong to the set of predictions.6 Active LearningEncouraged by the success of the KD-PC and KD-Fixed algorithms in estimating the confidence in theprediction we apply these methods to the task of ac-tive learning.
In active learning, the algorithm isgiven a large set of unlabeled data and a small setof labeled data and works in iterations.
On each it-eration, the overall labeled data at this point is usedto build a model, which is then used to choose newsubset of examples to be annotated.In our setting, we have a large set of unlabeledsentences and start with a small set of 50 annotatedsentences.
The active learning algorithm is then us-ing the CW algorithm to build a model, which in turnis used to rank sentences.
The new data items arethen annotated and accumulated to the set of labeleddata points, ready for the next round.
Many activelearning algorithms are first computing a predictionfor each of the unlabeled-data examples, which is979then used to choose new examples to be labeled.
Inour case the goal is to label sentences, which areexpensive to label.
We thus applied the followingsetting.
First, we chose a subset of 9K sentencesas unlabeled training set, and another subset of size3K for evaluation.
After obtaining a model, the al-gorithm labels random 1, 000 sentences and chose asubset of 10 sentences using the active learning rule,which we will define shortly.
After repeating thisprocess 10 times we then evaluate the current modelusing the test data and proceed to choose new un-labeled examples to be labeled.
Each method wasapplied to pick 5, 000 sentences to be labeled.In the previous section, we used the confidenceestimation algorithms to choose individual words tobe annotated by a human.
This setting is realisticsince most words in each sentence were already clas-sified (correctly).
However, when moving to activelearning, the situation changes.
Now, all the wordsin a sentence are not labeled, thus a human may needto label additional words than the one in target, in or-der to label the target word.
We thus experimentedwith the following protocol.
On each iteration, thealgorithm defines the score of an entire sentence tobe the score of the least confident word in the sen-tence.
Then the algorithm chooses the least confi-dent sentence, breaking ties by favoring shorter sen-tences (assuming they contain relatively more infor-mative words to be labeled than long sentences).We evaluated five methods, KD-PC and KD-Fixed mentioned above.
The method that ranksa sentence by the difference in score between thetop- and second-best labeling, averaged over thelength of sentence, denoted by MinMargin (Tongand Koller, 2001).
A similar approach, motivatedby (Dredze and Crammer, 2008), normalizes Min-Margin score using the confidence information ex-tracted from the Gaussian covariance matrix, we callthis method MinConfMargin.
Finally, We also eval-uated an approach that picks random sentences to belabeled, denoted by RandAvg (averaged 5 times).The averaged cumulative F-measure vs. num-ber of words labeled is presented in Figs.
5,6.
Wecan see that for short horizon (small number of sen-tences) the MinMargin is worse (in three out of fourdata sets), while MinConfMargin is worse in NPChunking.
Then there is no clear winner, but theKD-Fixed seems to be the best most of the time.
The2000 4000 6000 8000 100000.840.850.860.870.880.890.9Total No.
of Labeled WordsF?MeasureKD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg(a) NP Chunking2000 4000 6000 8000 100000.350.40.450.50.550.6Total No.
of Labeled WordsF?MeasureKD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg(b) NER English1040.90.9050.910.9150.920.9250.93Total No.
of Labeled WordsF?MeasureKD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg(c) NP Chunking1040.620.640.660.680.70.720.740.76Total No.
of Labeled WordsF?MeasureKD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg(d) NER EnglishFigure 5: Averaged cumulative F-score vs. total number ofwords labeled.
The top panels show the results for up to 10, 000labeled words, while the bottom panels show the results formore than 10k labeled words.bottom panels show the results for more than 10ktraining words.
Here, the random method perform-ing the worst, while KD-PC and KD-Fixed are thebest, and as shown in (Dredze and Crammer, 2008),MinConfMargin outperforming MinMargin.Related Work: Most previous work has fo-cused on confidence estimation for an entire exam-ple or some fields of an entry (Culotta and McCal-lum, 2004) using CRFs.
(Kristjansson et al, 2004)show the utility of confidence estimation is extractedfields of an interactive information extraction systemby high-lighting low confidence fields for the user.
(Scheffer et al, 2001) estimate confidence of sin-gle token label in HMM based information extrac-tion system by a method similar to the Delta methodwe used.
(Ueffing and Ney, 2007) propose severalmethods for word level confidence estimation for thetask of machine translation.
One of the methods theyuse is very similar to the weighted and non-weightedK-best Viterbi methods we used with the proper ad-justments to the machine translation task.AcknowledgmentsThe resrach is supported in part by German-IsraeliFoundation grant GIF-2209-1912.
KC is a HorevFellow, supported by the Taub Foundations.
The re-viewers thanked for their constructive comments.9802000 4000 6000 8000 100000.30.350.40.450.50.55Total No.
of Labeled WordsF?MeasureKD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg(a) NER Dutch2000 4000 6000 8000 100000.320.340.360.380.40.420.440.460.48Total No.
of Labeled WordsF?MeasureKD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg(b) NER Spanish1040.580.60.620.640.660.680.70.72Total No.
of Labeled WordsF?MeasureKD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg(c) NER Dutch104 1050.480.50.520.540.560.580.60.620.64Total No.
of Labeled WordsF?MeasureKD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg(d) NER SpanishFigure 6: See Fig.
5References[Cesa-Bianchi and Lugosi2006] N. Cesa-Bianchi andG.
Lugosi.
2006.
Prediction, Learning, and Games.Cambridge University Press, New York, NY, USA.
[Collins2002] M. Collins.
2002.
Discriminative trainingmethods for hidden markov models: Theory and ex-periments with perceptron algorithms.
In EMNLP.
[Crammer et al2005] K. Crammer, R. Mcdonald, andF.
Pereira.
2005.
Scalable large-margin online learn-ing for structured classification.
Tech.
report, Dept.
ofCIS, U. of Penn.
[Crammer et al2008] K. Crammer, M. Dredze, andF.
Pereira.
2008.
Exact confidence-weighted learning.In NIPS 22.
[Crammer et al2009a] K. Crammer, M. Dredze, andA.
Kulesza.
2009a.
Multi-class confidence weightedalgorithms.
In EMNLP.
[Crammer et al2009b] K. Crammer, A. Kulesza, andM.
Dredze.
2009b.
Adaptive regularization ofweighted vectors.
In NIPS 23.
[Culotta and McCallum2004] A. Culotta and A. McCal-lum.
2004.
Confidence estimation for information ex-traction.
In HLT-NAACL, pages 109?112.
[Dredze and Crammer2008] M. Dredze and K. Crammer.2008.
Active learning with confidence.
In ACL.
[Dredze et al2008] M. Dredze, K. Crammer, andF.
Pereira.
2008.
Confidence-weighted linearclassification.
In ICML.
[Kim et al2000] E.F. Tjong Kim, S. Buchholz, andK.
Sang.
2000.
Introduction to the conll-2000 sharedtask: Chunking.
[Kristjansson et al2004] T. Kristjansson, A. Culotta,P.
Viola, and A. McCallum.
2004.
Interactive infor-mation extraction with constrained conditional randomfields.
In AAAI, pages 412?418.
[Lafferty et al2001] J. Lafferty, A. McCallum, andF.
Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
[McCallum2002] Andrew McCallum.
2002.
MALLET:A machine learning for language toolkit.
http://mallet.cs.umass.edu.
[McDonald et al2005a] R.T. McDonald, K. Crammer,and F. Pereira.
2005a.
Flexible text segmentation withstructured multilabel classification.
In HLT/EMNLP.
[McDonald et al2005b] Ryan T. McDonald, Koby Cram-mer, and Fernando C. N. Pereira.
2005b.
Online large-margin training of dependency parsers.
In ACL.
[Scheffer et al2001] Tobias Scheffer, Christian Deco-main, and Stefan Wrobel.
2001.
Active hiddenmarkov models for information extraction.
In IDA,pages 309?318, London, UK.
Springer-Verlag.
[Sha and Pereira2003] Fei Sha and Fernando Pereira.2003.
Shallow parsing with conditional random fields.In Proc.
of HLT-NAACL, pages 213?220.
[Shimizu and Haas2006] N. Shimizu and A. Haas.
2006.Exact decoding for jointly labeling and chunking se-quences.
In COLING/ACL, pages 763?770.
[Taskar et al2003] B. Taskar, C. Guestrin, and D. Koller.2003.
Max-margin markov networks.
In nips.
[Tjong and Sang2002] Erik F. Tjong and K. Sang.
2002.Introduction to the conll-2002 shared task: Language-independent named entity recognition.
In CoNLL.
[Tjong et al2003] E.F. Tjong, K. Sang, and F. De Meul-der.
2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InCoNLL, pages 142?147.
[Tong and Koller2001] S. Tong and D. Koller.
2001.Support vector machine active learning with applica-tions to text classification.
In JMLR, pages 999?1006.
[Ueffing and Ney2007] Nicola Ueffing and Hermann Ney.2007.
Word-level confidence estimation for machinetranslation.
Comput.
Linguist., 33(1):9?40.
[Wick et al2009] M. Wick, K. Rohanimanesh, A. Cu-lotta, and A. McCallum.
2009.
Samplerank: Learningpreferences from atomic gradients.
In NIPS Workshopon Advances in Ranking.981
