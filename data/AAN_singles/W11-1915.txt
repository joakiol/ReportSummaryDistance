Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 97?101,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsPoly-co: a multilayer perceptron approach for coreference detectionEric ChartonE?cole Polytechnique de Montre?al2500, chemin de PolytechniqueMontre?al (Que?bec), H3T 1J4eric.charton@polymtl.caMichel GagnonE?cole Polytechnique de Montre?al2500, chemin de PolytechniqueMontre?al (Que?bec), H3T 1J4michel.gagnon@polymtl.caAbstractThis paper presents the coreference resolutionsystem Poly-co submitted to the closed trackof the CoNLL-2011 Shared Task.
Our sys-tem integrates a multilayer perceptron classi-fier in a pipeline approach.
We describe theheuristic used to select the pairs of corefer-ence candidates that are feeded to the networkfor training, and our feature selection method.The features used in our approach are based onsimilarity and identity measures, filtering in-formations, like gender and number, and othersyntactic information.1 IntroductionCoreference resolution is the process of determiningwhether two expressions in natural language refer tothe same entity in the world.
It is an important sub-task in natural language processing systems.
In thispaper, we present a learning approach to coreferenceresolution of named entities (NE), pronouns (PRP),noun phrases (NP) in unrestricted text according tothe CoNLL-2011 shared task (Pradhan et al, 2011).This system have been used in the context of closedtrack.2 Previous propositionsMany learning-based systems have been proposed tosolve coreference resolution task, and Soon?s (Soonet al, 2001) architecture is one of the most pop-ular ones.
In this proposition, all possible men-tions in a training document are determined by apipeline of natural language processing (NLP) mod-ules.
Then, training examples are generated as fea-ture vectors.
Each feature vector represents a pairof mentions that can potentially corefer.
Those vec-tors are used as training examples given to build aC5 classifier.
To determine the coreference chainsin a new document, all potential pairs of corefer-ring mentions are presented to the classifier, whichdecides whether the two mentions actually core-fer.
Since then, this dominant architecture has beenwidely implemented.
As it is a very flexible propo-sition, many families of classifiers have been used,trained with various configurations of feature vec-tors.
Good results are obtained with SVM classi-fiers, like described in (Versley et al, 2008).
Somepropositions keep only the principle of feature vec-tors, associated with more complex coreference de-tection algorithms.
A constraint-based graph parti-tioning system has been experimented by (Sapena etal., 2010) and a coreference detection system basedon Markov logic networks (MLNs) has been pro-posed by (Poon and Domingos, 2008).3 Architecture of the proposed systemA considerable engineering effort is needed toachieve the coreference resolution task.
A signif-icant part of this effort concerns feature engineer-ing.
We decided to keep the well established archi-tecture of (Soon et al, 2001) with a pre-processingNLP pipeline used to prepare pairs of coreferencefeatures.
The features are then submitted to the clas-sifier for pairing validation.
We tested various clas-sifiers on our feature model (see table 2) and fi-nally selected a multilayer perceptron (MLP) clas-sifier to make decision.
Since the Ontonotes layersprovide syntactic information (Pradhan et al, 2007),97Gender and number detectionTraining features vectors generationPerceptron training Features vectors generationPerceptron classificationNamed entities alias detectionCandidate mentionsdetection moduleSimilarity measuresModelNumber and GenderdatasCo-reference selectionTest CorpusCandidate mentions extraction moduleTraining corpusLabeled corpusFigure 1: The pipeline architecture of the Poly-co system.we could concentrate our efforts on the introductionof some complementary high level properties (likemention similarities or gender compatibility) usedin the feature vectors given to the classifiers.
Theglobal architecture, presented in figure 1, includestwo pipelines.
One configured for training purposesand the other one for coreference resolution.3.1 Architecture componentsOntonotes corpus includes part-of-speech tagging,noun phrases identification and named entity labels.We introduce complementary modules to detect gen-der and number, and evaluate mentions aliasing andsimilarity.
The detection task is composed of 4 mod-ules:?
Candidate mentions detection module, basedon extraction rules, using Ontonotes layers.?
Named entities alias detection module, basedon the previous version of Poly-co, describedin (Charton et al, 2010).
The purpose of thismodule is to identify variations in names ofthe same entity by examination of their surfaceform.?
Similarity calculation module, used to evalu-ate the similarity of two mentions according toa comparison of their string.?
Gender and number detection module,which determines gender and number for anycandidate mention.In the training pipeline, the candidate mentionsdetection module and the alias detection moduleare replaced by a unique candidate mentions ex-traction module.
This module collects from thetraining corpus the labeled mentions and their refer-ence numbers and use them to generate aliases andmentions values required to build training features.As we will see later, similarity calculation andgender and number detection all result in a value thatis integrated to the feature vector used to train andapply the classifier.
We give below a more detaileddescription of each module.3.1.1 Candidate mentions detection moduleIt is mandatory for coreference resolution to firstget al the potential mentions from the input text.To determine the mentions, this module explores thetext corpus and extracts a candidate mentions list.This list includes, for each mention, its position inthe document, its word content and its syntactic cat-egory.
This module uses simple detection rules tocollect the mentions according to their part of speech(POS) and their text content, their syntactic bound-aries and their named entity type labels.When used in classification mode, the detectionprocess is followed by a filtering process, whererules are used to remove mentions that have a verylow probability of being involved in coreference.These rules are based on simple word sequence pat-terns.
For example, pronoun it is filtered out whenimmediately followed by verb to be and relative pro-noun that within the next 6 following words.3.1.2 Alias detection moduleThis module implements an algorithm that clus-ters entities by comparing the form of their names.Entities are put in a list, ordered according to theirchronological apparition in the text.
At the begin-ning of the process, the first entity in the list is re-moved and constitutes the first item of a cluster.
Thisentity is compared sequentially, by using similarityand logical rules (i.e, a PERSON can?t be an alias ofa LOC ), with every other entities contained in the98list.
When there is a match, the entity is removedfrom the list and transferred to the currently instan-tiated cluster.
This operation is repeated until the listis empty.At the end of this process, an entity in a cluster isconsidered to be an alias of every other entity in thesame cluster.The TIME and DATE alias detection is donethrough a specific heuristic set.
Each TIME entityrepresentation is converted in a standardized format(Hour/Minutes).
Dates are normalized as a relativeamount of days (?today?
is 1, ?last month?
is -30,etc) or a formal date (Year/Month/Day).3.1.3 Similarity calculation moduleThe similarity module is applied on named enti-ties (excepted TIME and DATE ) and NP of thecandidate mentions list.
It consists in a text com-parison function which returns the number of com-mon words between two mentions.
After executionof this module, we obtain a square matrix containinga similarity measure for every pair of mentions.3.1.4 Gender and number detection moduleGender and number are associated with each entryof the candidate mentions list, including PRP andNP.
First, this module tries to detect the gender usingthe gender data provided1.
Then a set of less than10 very simple rules is used to avoid anomaly (i.e aPERSON entity associated with the neutral gender).Another set of rules using plural markers of wordsand POS is used to validate the number.4 Features definition and productionThe feature vector of the Poly-co system (see ta-ble 1) consists of a 22 features set, described below.This vector is based on two extracted mentions, Aand B, where B is the potential antecedent and A isthe anaphor.Four features are common to A and B (section Aand B properties of table 1):?
IsAlias : this value is binary (yes or no) andprovided by the alias module.
The value is yesif A and B have been identified as describingthe same entity.1The list allowed by the Shared Task definition and availableat http://www.clsp.jhu.edu/ sbergsma/Gender/Feature Name Value valueA and B propertiesIsAlias yes/no 1/0IsSimilar real 0.00 /1.00Distance int 0/const(b)Sent int 0/xReference AISNE yes/no 1/0ISPRP yes/no 1/0ISNP yes/no 1/0NE SEMANTIC TYPE null / EN 0 / 1-18PRP NAME null / PRP 0 / 1-30NP NAME null / DT 0 / 1-15NP TYPE null / TYPE 0 / 1-3GENDER M/F/N/U 1/2/3/0NUMBER S/P/U 1/2/0Reference BSame as Reference ATable 1: Feature parameters?
IsSimilar : this value is the similarity measureprovided by the similarity module.?
Distance : this indicates the offset distance (interms of number of items in the candidate men-tions list) between A and B.?
Sent : this indicates the amount of sentencesmarker (like .
!
?)
separating the mentions Aand B.For each candidate A and B, a set containing ninefeatures is added to the vector (in table 1, only prop-erties for A are presented).
First, 3 flags determineif mention is a named entity (IsNE), a personal pro-noun (IsPRP) or a noun phrase (IsNP).
The next sixflags define the characteristics of the mention :?
NE SEMANTIC TYPE is one of the 18 availableNE types (PERSON, ORG, TIME, etc)?
PRP NAME is a value representing 30 possiblewords (like my, she, it, etc) for a PRP.?
NP NAME is a value indicating the DT used bya NP (like the, this, these, etc).?
NP TYPE specifies if NP is demonstrative, def-inite, or a quantifier.?
GENDER and NUMBER flags indicate whetherthe mention gender (Male, Female or Neutral)99Poly-co Score Mentions B3 CEAF MUCR P F R P F R P F R P FMultilayer perceptron (MLP) 65.91 64.84 65.37 66.61 62.09 64.27 50.18 50.18 50.18 54.47 50.86 52.60SVM 65.06 66.11 65.58 65.28 57.68 61.24 46.31 46.31 46.31 53.30 50.00 51.60Tree J48 66.06 64.57 65.31 66.53 62.27 64.33 50.59 50.59 50.59 54.24 50.60 52.36Table 2: System results obtained with scorer v4 on gold dev-set applying various classifiers on same features vectors.Poly-co Score Mentions B3 CEAF MUCMultilayer perceptron (MLP) 64.53 63.42 63.97 66.07 61.65 63.79 49.12 49.12 49.12 52.70 49.22 50.90Table 3: System results obtained with scorer v4 on predicted dev-set using our system.and number (Singular or Plural) are known ornot (if not, U is the value for the flag).A null value (0) is used when a flag doesn?t haveto be defined (i.e PRP flag if the mention is a NE).5 Classifier training and useFor training, we use an algorithm that selects themore relevant pairs or mentions.
Suppose thatthe candidate mentions list contains k mentionsM1,M2, .
.
.
,Mk, in this order in the document.
Thealgorithm starts with the last mention in the docu-ment, that is, Mk.
It compares Mk sequentially withpreceding mentions, going backward until a core-ferring mention Mc is reached, or a maximum of nmentions have been visited (the value of n is fixed to10 in our experiments).
When a coreferring mentionMc has been found, a vector is constructed for everypair of mentions ?Mk,Mi?, where Mi is a mentionthat has been visited, including the coreferring one.These vectors are added to the training set, Mc beinga positive instance, and all the others ones being neg-ative instances.
The process is repeated with Mk?1,and so on, until every mention has been processed.If none of the n precedent mentions are coreferentto M1, all the n pairs are rejected and not used astraining instance.During the coreference detection process, a sim-ilar algorithm is used.
Starting from mention Mk,we compare it with n preceding mentions, until wefind one for which the multilayer perceptron classi-fier gives a coreference probability higher than 0.52.If none is found within the limit of n mentions, Mk2Note that in comparison tests, displayed in table 2, SVMprovides a binary decision and J48 a probability value.
Theyare used as the multilayer perceptron ones.is considered as a non coreferring mention.
Whenthis has been done for every mention in the docu-ment, the detected coreferences are used to constructthe coreference chains.6 ResultsThe results presented on table 2 are obtained on thedev-set of the Ontonotes corpus.
To evaluate the po-tential of our features model, we trained our sys-tem with MLP, SVM and J48 Tree classifiers.
Wefinally chose the MLP models for the test evalua-tion due to its better performance on the predicteddev-set.
However, according to the small differencebetween MLP and J48 Tree, it?s difficult to defineclearly wich one is the best choice.7 ConclusionsWe presented Poly-co, a system for coreference res-olution in English easy to adapt to other languages.The first version of Poly-co was built to detect onlycoreferences of persons.
As the dataset provided forCoNLL is much more complex, it was an interstingopportunity to evaluate our mention detection algo-rithms in the perspective of a full task, including dif-ficult coreferences mentions beetween named enti-ties, noun phrases and prepositions.
Our comparisonof various classifier results on dev-sets have shownthat our proposition to use a multilayer perceptron ascoreference chain builder can be an intersting solu-tion, but does not introduce an important differenceof performance with previously experimented clas-sifiers.100ReferencesEric Charton, Michel Gagnon, and Benoit Ozell.
2010.Poly-co : an unsupervised co-reference detection sys-tem.
In INLG 2010-GREC, Dublin.
ACL SIGGEN.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov logic.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing - EMNLP ?08, page650, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Unre-stricted coreference: Identifying entities and events inOntoNotes.
In International Conference on SemanticComputing, 2007.
ICSC 2007., pages 446?453.
IEEE.Sameer Pradhan, Lance Ramshaw., Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Xue Nianwen.2011.
CoNLL-2011 Shared Task: Modeling Unre-stricted Coreference in OntoNotes.
In Proceedingsof the Fifteenth Conference on Computational NaturalLanguage Learning (CoNLL 2011), Portland, Oregon.Emili Sapena, L.
Padro?, and Jordi Turmo.
2010.
Relax-Cor: A global relaxation labeling approach to coref-erence resolution.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, number July,pages 88?91.
Association for Computational Linguis-tics.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A Machine Learning Approach to Coref-erence Resolution of Noun Phrases.
ComputationalLinguistics, 27(4):521?544, December.Yannick Versley, S.P.
Ponzetto, Massimo Poesio,Vladimir Eidelman, Alan Jern, Jason Smith, XiaofengYang, and Alessandro Moschitti.
2008.
BART:A modular toolkit for coreference resolution.
InProceedings of the Sixth International Language Re-sources and Evaluation (LREC?08), number 2006,pages 9?12, Marrakech.
European Language Re-sources Association (ELRA).101
