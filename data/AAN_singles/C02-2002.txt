Dynamic Lexical Acquisition in Chinese Sentence AnalysisAndi WuMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAandiwu@microsoft.comJoseph PentheroudakisMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAjosephp@microsoft.comZixin JiangMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAjiangz@microsoft.comAbstractDynamic lexical acquisition is a procedurewhere the lexicon of an NLP system isupdated automatically during sentenceanalysis.
In our system, new words and newattributes are proposed online according tothe context of each sentence, and then getaccepted or rejected during syntactic analysis.The accepted lexical information is stored inan auxiliary lexicon which can be used inconjunction with the existing dictionary insubsequent processing.
In this way, we areable to process sentences with an incompletelexicon and fill in the missing info withoutthe need of human editing.
As the auxiliarylexicons are corpus-based, domain-specificdictionaries can be created automatically bycombining the existing dictionary withdifferent auxiliary lexicons.
Evaluationshows that this mechanism significantlyimproves the coverage of our parser.IntroductionThe quality of many NLP systems dependsheavily on the completeness of the dictionarythey use.
However, no dictionary can ever becomplete since new words are being coinedconstantly and the properties of existing wordscan change over time.
In addition, a dictionarycan be relatively complete for a given domain butmassively incomplete for a different domain.The traditional way to make a dictionary morecomplete is to edit the dictionary itself, either byhand or through batch updates using dataobtained from other sources.
This approach isundesirable because(1) it can be very expensive due to theamount of hand work required;(2) the job will never be complete since newwords and new usages of words willcontinue to appear.
(3) certain words and usages of words decayafter a while or  only exist in a certaindomain, and it is inappropriate to makethem a permanent part of the dictionary.This paper discusses an alternative approachwhere, instead of editing a static dictionary, weacquire lexical information dynamically duringsentence analysis.
This approach is currentlyimplemented in our Chinese system and Chineseexamples will be used to illustrate the process.
InSection 1, we will discuss how the new lexicalinformation is discovered.
Section 2 discusseshow such information is filtered, lexicalized, andused in future processing.
Section 3 is devoted toevaluation.1 Proposing words and attributesTwo major types of lexical information are beingacquired dynamically in our current Chinesesystem: new words and new grammaticalattributes such as parts of speech (POS) andsub-categorization frames.
The acquisitionassumes the availability of an existing dictionarywhich is relatively mature though incomplete inmany ways.
In our case, we have a lexicon of88,000 entries with grammatical attributes inmost of them.
Our assumption is that, once adictionary has reached this scale, we should haveenough information to predict the missinginformation in the context of sentence analysis.We can then stop hand-editing the staticdictionary and let dynamic lexical acquisitiontake over.In most cases, the grammatical properties of aword define the syntactic context in which thisword may appear.
Therefore, it is often possibleto detect the grammatical properties of a word bylooking at the surrounding context of this word ina sentence.
In fact, this is one of the main criteriaused by lexicographers, who often apply aconscious or subconscious contextual ?template?for each grammatical property they assign.
Wehave coded those templates in our system so thata computer can make similar judgments.1  Whena word is found to fit into a template for a givenproperty but we do not have that property in thedictionary yet, we can make a guess and proposeto add that property.
Our current Chinese systemhas 29 such templates, 14 for detecting newwords and 15 for detecting new grammaticalattributes for new or existing words.1.1 Proposing new wordsTwo types of unlisted words exist in Chinese:(1) single-character bound morphemes usedas words;(2) new combinations of characters as words.An example of Type (1) is ?.
This is a boundmorpheme in our dictionary, appearing only as apart in words like ???
(have a good chat).However, like many other bound morphemes inChinese, it can occasionally be used as anindependent word, as in the following sentence:?
?
?
?
?
?
?
?
?
?he  at  I  home  chat LE  two CL  hourHe chatted for two hours at my house.The usual response to this problem is to treat it asa lexical gap and edit the entry of ?
to make it averb in the dictionary.
This is undesirable for atleast two reasons.
First of all, many boundmorphemes in Chinese can be occasionally usedas words and making all of them independentwords will introduce a lot of noise in sentenceanalysis.
Secondly, it will be a difficult task forlexicographers, not just because it takes time, butbecause the lexicographers will often be unableto make the decision unless they see sentenceswhere a given bound morpheme is used as aword.In our system, we leave the existing dictionaryuntouched.
Instead, we ?promote?
a bound1Currently these templates are hand-coded heuristicsbased on linguists?
intuition.
We are planning to usemachine learning techniques to acquire thosetemplates automatically.morpheme to be a word dynamically when itappears in certain contextual templates.
Thetemplate that promotes ?
to be a verb mayinclude conditions such as:?
not subsumed by a longer word, such as????;?
being part of an existing multiple-characterverb, such as ?
in ????;?
followed by an aspect marker, such as ;?
etc.Currently we have 4 such templates, promotingmorphemes to nouns, verbs, adjectives andadverbs respectively.Examples of Type (2) are found all the time andadding them all to the existing dictionary will bea never-ending job.
Here is an example:?
?
??
??
?
??
??
?
?
?not need again start then can  dock  or  undock??
?
?easy-to-carry  computerYou can dock and undock your laptop withoutrestarting.?
?
(dock),  ?
?
(undock) and ?
?
(easy-to-carry) are not entries in our dictionary.Instead of adding them to the dictionary, we usetemplates to recognize them online.
Thetemplate that combines two individual charactersto form a verb may include conditions such as:?
none of the characters is subsumed by alonger word;?
the joint probability of the characters beingindependent words in text is low;?
the internal structure of the new wordconforms to the word formation rules ofChinese?
the component characters have similarbehavior in existing words?
etc.The details can be found in Wu & Jiang (2000).Currently we have 10 such templates, which arecapable of identifying nouns, verbs, adjectivesand adverbs of various lengths.1.2.
Proposing grammatical attributesPOS and sub-categorization information iscrucial for the success of sentence analysis.However, there is no guarantee that every word inthe existing dictionary will have the correct POSand sub-categorization information.
Besides,words can behave differently in differentdomains or develop new properties over time.Take the Chinese word (synchronize) forexample.
It is an intransitive verb in ourdictionary, but it is now often used as a transitiveverb, especially in the computer domain.
Forinstance:MADC ?
??
?
??
Exchange ?
?can easily DE synchronize           accountMADC (Microsoft Active Directory Connector)can easily synchronize Exchange accounts.We may want to change the existing dictionary tomake words like?
?transitive verbs, but thatmay not be appropriate lexicographically, at leastin the general domain, not to mention the humanlabor involved in such an undertaking.
However,the sentence above cannot get a spanning parseunless?
?is a transitive verb.
To overcome thisdifficulty, our system can dynamically create atransitive verb in certain contexts.
An obviouscontext would be ?followed by an NP?, forexample.
This way we are able to parse thesentence without changing the dictionary.A similar approach is taken in cases where a wordis used in a part of speech other than the one(s)specified in the dictionary.
In the followingsentence, for example, the noun ??
(cluster) isused as a verb instead:?
??
??
32 ?
??
?you can cluster 32 CL serverYou can cluster 32 servers.Rather than edit the dictionary to permanentlyadd the verb POS to nouns like ?
?, we turnthem into verbs dynamically during sentenceanalysis if they fit into the verb template.
Theconditions in the verb template may include:?
preceded by an modal or auxiliary verb?
followed by aspectual markers such as ?, ?and ??
preceded by adverbials?
etc.Such templates are in effect very similar to POStaggers, though we use them exclusively to createnew POS instead of choosing from existing POS.2 Harvesting new words and attributesProposing of new words and attributes asdescribed in the previous section is only intendedto be intelligent guesses, which can be wrongsometimes.
For example, although transitiveverbs tend to be followed by NPs, not all verbsthat precede NPs are transitive verbs.
To makesure that (1) the wrong guesses do not introducetoo much noise into the analysis and (2) only thecorrect guesses are accepted as true lexicalinformation, we take the following steps to filterout the errors that result from over-guessing.2.1 Set up the competitionThe proposed words and attributes are assignedlower probability in our system.
This isstraightforward for new words.
We simplyassign them low scores when we add them (asnew terminal nodes) to the parsing chart2.
Fornew attributes on existing words, we make a newnode which is a copy of the original node andassign the new attributes and a lower probabilityto this node.
As a result, the chart will containtwo nodes for the same word, one with the newattributes and one without.
The overall effect isthat the newly proposed nodes will compete withother nodes to get into a parse, though with adisadvantage.
The sub-trees built with the newnodes will have lower scores and will not be inthe preferred analysis unless there is no other wayto get a spanning parse.
Therefore, if the guessesare wrong and the sentence can be successfullyparsed without the additional nodes, the bestparse (the parse with the highest score) will notcontain those nodes and the guesses arepractically ignored.
On the other hand, if theguesses are right and we cannot get anysuccessful parse unless we use them, then theywill end up in the top parse3 in spite of their low2See Jensen et al(1993) and Heidorn (2000) for ageneral description of how chart parsing works in oursystem.
A Chinese-specific description of the systemcan be found in Wu & Jiang (1998).3Our system can produce more than one parse for agiven sentence and the top parse is the one with theprobability.2.2 Keep the winnersFor each sentence, we pick the top parse andcheck it to see if  there are any terminal nodes thatare  new words or nodes containing newattributes.
If so, we know that these nodes arenecessary at least to make the current sentenceanalyzable.
The fact that they are able to beattheir competitors despite their disadvantagesuggests that they probably represent lexicalinformation that is missing in the existingdictionary.
We therefore collect suchinformation and store it away in a separatelexicon.
This auxiliary lexicon contains entriesfor the new words and the new attributes ofexisting words.
Each entry in this lexicon carriesa frequency count which records the number oftimes a given new word or new attribute hasappeared in good parses during the processing ofcertain texts.
The content of this lexicon dependson the corpora, of course, and different lexiconscan be built for different domains.
Whenprocessing future sentences, the entries in thoselexicons can be dynamically merged with theentries in the main lexicon, so that we do not haveto make the same guesses again.2.3 Use the fittestThe information lexicalized in those auxiliarylexicons, though good in general, is notguaranteed to be correct.
While being necessaryfor a successful parse is strong evidence for itsvalidity, that is not a sufficient condition for thecorrectness of such information.
Consequently,there can be some noise in those lexicons.However, a real linguistic property is likely to befound consistently whereas mistakes tend to berandom.
To prevent the use of wronglylexicalized entries, we may require a frequencythreshold during the merging process: only thoseentries that have been encountered more than ntimes in the corpora are allowed to be mergedwith the main lexicon and used in future analysis.If a given new word or linguistic property isfound to occur repeatedly across differentdomains, we may even consider physicallyhighest score.merging it into the main dictionary, as it may be apiece of information that is worth addingpermanently.3 EvaluationThe system described above has been evaluatedin terms of the contribution it makes in parsing.The corpus parsed in the evaluation consists of121,863 sentences from Microsoft technicalmanuals.
The choice is based on theconsideration that this is a typicaldomain-specific text where there are manyunlisted words and many novel usages of words.4To tease apart the effects of online guessing andlexicalization, we did two separate tests, one withonline guessing only and one with lexicalizationas well.
When lexicalization is switched on, thenew words and attributes that are stored in theauxiliary lexicon are used in subsequentprocessing.
Once a new word or attribute hasbeen recognized in n sentences, it will act as if itwere an entry in the main dictionary and can beused in the analysis of any other sentence withnormal probability.3.1 Online guessing onlyIn this test, we parsed the corpus twice, once withguessing and once without.
Then we picked outall the sentences that had different analyses in thetwo passes and compared their parses to see ifthey became better when lexical guessing is on.Since comparing the parses requires humaninspection and is therefore very time consuming,we randomly selected 10,000 sentences out of the121,863 and used only those sentences in the test.It turns out that 1,459 of those 10,000 sentencesgot different parses when lexical guessing isswitched on.
Human comparison of thosedifferences shows that, of the 1,459, the guessingmade 1,153 better, 82 worse, and 224 stay thesame (different parses but equally good or bad).The net gain is 1,071.
In other words, 10.71% ofthe sentences became better when lexicalguessing is used.4The novel usages are mainly due to the fact that thetext is translated from English.More detailed analysis shows that 48% of theimprovements are due to the recognition of newwords and 52% to the addition of newgrammatical attributes.
Of the 82 sentences thatbecame worse, 6 failed because of the lack ofstorage during processing caused by theadditional resources required by the guessingalgorithm.
The rest are due to over-guessing, ormore precisely, the failure to rule out theover-guesses in sentence analysis.
The guessingcomponent is designed to over-guess, since thegoal there is recall rather than precision.
Thelatter is achieved by the filtering effect of theparser.3.2 Additional gain with lexicalizationIn this second test, we evaluated the effect oflexicalization on new word recognition5.
Weparsed all the 121,863 sentences twice, once withlexicalization and once without.
The number ofunique new words recognized in this corpus is9226.
Notice that this number does not changebetween the two processes.
Using the lexiconcreated by dynamic lexicalization will increasethe instances of those words being recognized,but will not change the number of unique words,since the entries in the auxiliary lexicon can alsobe recognized online.
However, the numbers ofinstances are different in the two cases.
Whenlexicalization is turned off, we are able to get5963 instances of those 922 new words in 5239sentences.
When lexicalization is on, however,we are able to get 6464 instances in 5608sentences.
In other words, we can increase therecognition rate by 8.4% and potentially save 369additional sentences in parsing.
The reason forthis improvement is that, without lexicalization,we may fail to identify the new words in certainsentences because there were not enough goodcontexts in those sentences for the identification.Once those words are lexicalized, we no longerhave to depend on context-based guessing and5We would like to look at the effect on grammaticalattributes as well, but the evaluation is not asstraightforward there and much moretime-consuming.6The total number of unique words used in this corpusis 17,110.
So at least 5% of the words are missing inthe original dictionary.those sentences can benefit from what we havelearned from other sentences.
Here is a concreteexample for illustration:?
??
?
??
????
?
??
?He master LE undock  laptop   DE technologyHe mastered the technology of undocking alaptop.In this sentence, we do not have enough contextto identify the new word ??
because ??
is aword in Chinese (Remember there are no spacesbetween words in Chinese!).
This destroys thecondition that none of the characters in the newword should be subsumed by a longer word.However, if ??
has been recognized in someother sentences, such as the one we saw inSection 1.1, and has been lexicalized, we cansimply look up this word in the dictionary anduse it right away.
In short, lexicalization enableswhat is learned locally to be available globally.ConclusionIn this paper, we have demonstrated a mechanismfor dynamic dictionary update.
This methodreduces human effort in dictionary maintenanceand facilitates domain-switching in sentenceanalysis.
Evaluation shows that this mechanismmakes a significant contribution to parsing,especially the parsing of large, domain-specificcorpora.ReferencesHeidorn G. E. (2000) Intelligent writing assistance,.In "A Handbook of Natural Language Processing:Techniques and Applications for the Processing ofLanguage as Text ", Dale R., Moisl H., and SomersH.
eds., Marcel Dekker, New York, pp.
181-207.Jenson K., Heidorn G. and Richardson S.  (1993)Natural Language Processing: the PLNLPApproach?
Boston, KluwerWu A. and Jiang Z.
(1998)  Word Segmentation inSentence Analysis.
In "Proceedings of the 1998International Conference on Chinese InformationProcessing", Beijing, China.Wu A. and Jiang Z.
(2000)  Statistically-EnhancedNew Word Identification in a Rule-based ChineseSystem.
In "Proceedings of the Second ACLChinese Processing Workshop", HKUST, HongKong.
