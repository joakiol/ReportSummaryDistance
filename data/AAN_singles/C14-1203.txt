Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2151?2160, Dublin, Ireland, August 23-29 2014.Data-driven Measurement of Child Language Developmentwith Simple Syntactic TemplatesShannon LubetichPomona CollegeClaremont, CA 91711shannon.lubetich@pomona.eduKenji SagaeInstitute for Creative TechnologiesUniversity of Southern CaliforniaLos Angeles, CA 90089sagae@ict.usc.eduAbstractWhen assessing child language development, researchers have traditionally had to choose be-tween easily computable metrics focused on superficial aspects of language, and more expressivemetrics that are carefully designed to cover specific syntactic structures and require substantialand tedious labor.
Recent work has shown that existing expressive metrics for child languagedevelopment can be automated and produce accurate results.
We go a step further and pro-pose that measurement of syntactic development can be performed automatically in a completelydata-driven way without the need for definition of language-specific inventories of grammaticalstructures.
As a crucial step in that direction, we show that four simple feature templates areas expressive of language development as a carefully crafted standard inventory of grammaticalstructures that is commonly used and has been validated empirically.1 IntroductionAlthough child language has been the focus of much study, our understanding of first language acquisi-tion is still limited.
In attempts to measure child language development over time, several metrics havebeen proposed.
The most commonly used metric is Mean Length of Utterance, or MLU (Brown, 1973),which is based on the number of morphemes per utterance.
The main appeal of MLU is that it can beeasily computed automatically, given machine-readable transcripts.
Although MLU values may not bemeaningful across languages, the general approach is suitable for analysis within different languages.However, MLU?s ability to track language development from age four has been questioned (Klee andFitzgerald, 1985; Scarborough, 1990), and its usefulness is still the subject of debate (Rice et al., 2010).Several metrics based on the usage of grammatical structure have been proposed as more sensitive tochanges in language over a wider range of ages (Scarborough, 1990; Lee and Canter, 1971; Fletcher andGarman, 1988).
These metrics continue to show score increases where MLU plateaus, but their increasedexpressivity is typically associated with two severe drawbacks.
The first is that their use for computa-tion of language development scores involves identification of several specific grammatical structures inchild language transcripts, a process that requires linguistic expertise and is both time-consuming anderror-prone.
This issue has been addressed by recent work that shows that current natural language pro-cessing techniques can be applied to automate the computation of these metrics, removing the bottleneckof manual labor (Sagae et al., 2005; Roark et al., 2007; Sahakian and Snyder, 2012).
The second draw-back is that these measures are language-specific, and development of a measure for a specific languagerequires deep expertise and careful design of an inventory of grammatical structures that researchers be-lieve to be indicative of language development.
Going beyond previous work, which addressed the firstdrawback of traditional metrics for child language development, we address the second, paving the wayfor a language-independent methodology for tracking child language development that is as expressiveas current language-specific alternatives, but without the need for carefully constructed inventories ofgrammatical structures.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2151The specific hypothesis we address in this paper is whether a fully-data driven approach that uses onlya few simple feature templates applied to syntactic dependency trees can capture the same information asthe well-known Index of Productive Syntax, or IPSyn (Scarborough, 1990).
In contrast to previous workthat showed that the computation of IPSyn scores can be performed automatically by encoding each ofthe 60 language structures in a language-specific inventory (e.g.
wh-questions with auxiliary inversion,propositional complements, conjoined sentences) as complex patterns over parse trees, we propose thatchild language development can instead be measured automatically in a way that is fully data-driven andcan be applied to many languages for which accurate dependency parsers are available, without relyingon carefully constructed lists of grammatical structures or complex syntactic patterns in each language.Specifically, we examine two hypotheses: (1) counts of features extracted from syntactic parse treesusing only simple templates are at least as expressive of changes in language development as the Indexof Productive Syntax (Scarborough, 1990), an empirically validated metric based on an inventory ofgrammatical structures derived from the child language literature; and (2) these parse tree features canbe used to model language development without the use of an inventory of specific structures, assumingonly the knowledge that in typically developing children the level of language development is correlatedwith age.
We emphasize that the goal of this work is not to develop yet one more way to compute IPSynscores automatically, but to show empirically that lists of grammatical structures such as those used tocompute IPSyn are not essential to measure syntactic development in children.In this paper, we start by reviewing IPSyn and previous work on automatic IPSyn scoring based onmanually crafted syntactic patterns in section 2.
Using a similar approach, we validate the languagedevelopment curves observed by Scarborough (1990) in the original IPSyn study.
In section 3 we showhow IPSyn scores can be computed in an entirely different, fully data-driven way, using a support vectorregression.
In section 4 we examine how this data-driven framework can be used to track languagedevelopment in the absence of a metric such as IPSyn, which allows for application of this approach tolanguages other than English.
We discuss related work in section 5, and conclude in section 6.2 Index of Productive Syntax (IPSyn)The Index of Productive Syntax (Scarborough, 1990) evaluates a child?s linguistic development by ana-lyzing a transcript of utterances and awarding points when certain syntactic and morphological structuresare encountered.
The end result is a number score ranging from 0 to 120, with a higher score correspond-ing to the presence of more complex grammatical structures, and thus further linguistic development.IPSyn was designed to be more sensitive to language changes after age 3 than the more common MeanLength of Utterance (MLU) (Brown, 1973), which fails to account for the fact that children?s speechincreases in complexity even after utterances stop increasing in length.IPSyn scores are calculated by analyzing a transcript of 100 utterances of a child?s speech, and award-ing points to specific language structures encountered.
There are 60 forms in total from four categoriesof noun phrases, verb phrases, questions and negations, and sentence structures.
Each form is awarded0 points if not encountered, 1 point if found once in a transcript, and 2 points if found at least twice.This sums to a total ranging between 0 and 120 points.
Scarborough (1990) motivates the use of thisspecific inventory of 60 forms by stating that they ?have been shown to occur in preschool languageproduction in innumerable studies of language acquisition during the past 25 years,?
highlighting thatthe task of generating such an inventory and performing empirical validation for additional languagesrequires considerable expertise and is far from trivial.2.1 Automating IPSynIn support of empirical testing of our first hypothesis?that features extracted from parse trees using onlysimple feature templates are as expressive of child language development as the carefully constructedinventory of grammatical structures in IPSyn?we first implemented an automated version of IPSyn fol-lowing Sagae et al.
(2005), who showed that this task can be performed nearly at the level of trainedhuman experts.
This allows us to generate IPSyn scores for a large set of child language transcripts.
Ourimplementation differs from previous work mainly in that it uses only the tools provided in the CLAN2152software suite (MacWhinney, 2000), which were designed specifically for analysis of child languagetranscripts, instead of the Charniak (2000) parser, which was used by Sagae et al.
and later by Hassanaliet al.
(2014) in a more recent implementation of the same general approach.We evaluated our implementation using the set of 20 manually scored transcripts described by Sagaeet al.
as Set A, and subsequently used to evaluate the implementation of Hassanali et al.
Three transcriptswere used as development data, following Sagae et al.
The mean absolute difference between manuallygenerated and automatically generated scores was 3.6, which is very similar to what has been reportedby Hassanali et al.
and by Sagae et al.
(3.05 and 3.7, respectively) for the same set of transcripts.Given the possible score differences in manual scoring reported by Scarborough (1990) and the smallnumber of transcripts used for testing, the differences observed among the automatic systems are notmeaningful.
In fact, in examining our development data, we found multiple errors in the manual coding,causing point discrepancies when our system produced correct results.
This highlights the difficulty ofperforming this scoring task manually, and raises the question of whether automatic scoring has in factsurpassed the reliability of manual scoring.
That three different implementations of IPSyn appear toperform comparably suggests this might be the case.
We leave an empirical investigation of this questionto future work.3 From automatic IPSyn to data-driven IPSynThe fully automatic way of computing IPSyn scores described above in section 2.1, paired with a suffi-ciently large amount of child language transcript data, gives us a way to test the hypothesis mentioned inthe beginning of section 2.1, that simple features of parse trees are as expressive as the hand-crafted IPSynlanguage structure inventory.
We did this by first creating several 100-utterance transcripts from existingchild language transcripts, then automatically assigning them IPSyn scores, and using these scores astargets to be learned from features extracted from the corresponding 100-utterance transcripts.
Details ofthe data and learning approach used for this experiment, as well as empirical results, are described in theremainder of this section.3.1 Generating IPSyn dataTo obtain enough child language transcripts in a wide range of ages to test our hypothesis, we turnedto the CHILDES database.
To generate training and development sets for our experiments, we usedtranscripts from CHILDES of 14 different children with ages ranging from 1 year 1 month to 8 years.Because each application of IPSyn requires only 100 child utterances, transcripts were split, producinga total of 593 transcripts, each containing 100 utterances.
The 14 children in our dataset came from thefollowing CHILDES corpora: Brown, MacWhinney, Sachs and Warren.
The reason for choosing thesecorpora is that they were quickly identified as containing spontaneous natural interactions, as opposedto reading or specific games and activities designed to elicit a certain kind of language production.
It islikely that other corpora in CHILDES would also suit our purposes, but the data in these four corpora wassufficient for our experiments.
Each of the 593 transcripts was assigned an IPSyn score automatically.From the Brown, MacWhinney and Sachs corpora, we used transcripts from a total of four children(Adam from Brown, Mark and Ross from MacWhinney, and Naomi from Sachs), from whom languagedata was collected over several years.
Transcripts from these three corpora, 572 in total, served as ourtraining set.
The Warren corpus includes data from ten children with ages ranging from 1;6 to 6;2 (thatis, 1 year and 6 months to 6 years and 2 months, using the commonly accepted age notation for this typeof data), from which we created 21 transcripts that served as our development set.The complete set of 593 transcripts with IPSyn scores gives us the opportunity to verify whether thelanguage development curves observed by Scarborough (1990) averaged over 75 transcripts in the orig-inal IPSyn study matches curves produced from averaging results from 593 transcripts from entirelydifferent subjects.
Figure 1 shows a side-by-side comparison between the original figure from (Scarbor-ough, 1990) and a corresponding figure generated with our automatically scored transcripts.
Althoughnot identical, the two figures are remarkably similar, reflecting that aspects of the emergence of grammarin child language development are shared across children, and that IPSyn captures some of these aspects.2153(a) Original IPSyn study.
(b) Automatically generated.Figure 1: Comparison between the IPSyn development curves for the four subscales in (a) the 75 tran-scripts in the original IPSyn study (reproduced from (Scarborough, 1990)), and (b) our set of 593 tran-scripts scored automatically.Finally, we used the Garvey corpus to generate a test set.
This corpus includes data from 48 differentchildren with ages ranging from 2;10 to 5;7, from which we extracted 60 transcripts covering all 48children and the full range of ages in the corpus.
No data from the 48 children in the Garvey corpus,which we used as a test set, were used for training or development of the models used in our experiments.3.2 A regression model for IPSynGiven 593 pairs of transcript and IPSyn score, we approached the task of learning a data-driven modelfor IPSyn scoring as one of regression.
For each transcript, a set of features is extracted, and the IPSynscore is associated with that feature vector.
The features extracted from the transcripts followed fourtemplates, described in the next subsection.
If an accurate function for predicting IPSyn scores fromthese feature vectors can be learned, our hypothesis that these features are at least expressive enough totrack child language development as well as the inventory of IPSyn structures is confirmed.
To learn ourmodel, we used the SVM Light1implementation of support vector regression (Drucker et al., 1997).3.3 FeaturesAn important step in learning a regression model for IPSyn is choosing what features to use.
To supportour goal of language independence, we decided not to consider language specific features that have beenshown to be useful in this task but are language dependent2, and opted instead to see whether the use ofonly simple parse tree features would be sufficient.
The only prerequisite for extraction of our feature setis that each transcript must be parsed to produce a syntactic dependency tree.
We used the CLAN toolsfor morphology analysis (MOR), part-of-speech tagging (POST) and parsing (MEGRASP)3, since it isstraightforward to process CHILDES transcripts using those, and they provide high-accuracy analysesfor child language transcripts.
The accuracy of the MEGRASP dependency parser for child utterances inEnglish is estimated to be close to 93% (Sagae et al., 2010).All of the features used in our model are extracted from parse trees according to four simple classesthat target the following information:1http://svmlight.joachims.org/2This is in contrast to, for example, the related work of Sahakian and Snyder (2012), which we discuss in section 5.3Models for MOR and POST are available for a wide variety of languages.
Models for MEGRASP are available only forEnglish and Japanese, but our data-driven approach is not tied to any specific tagger or parser.2154n:prop v n nEve eat dust mop.SUBJOBJMODFigure 2: A dependency tree generated with part-of-speech and grammatical relation information.Part-of-speech tags: The first type of feature we used is simply the part-of-speech tag of each word.
Thiscan be thought of as a bag of part-of-speech tags.
We intentionally avoided the commonly used bag-of-words, because our goal is to obtain a model that tracks changes in syntax structure, not content.Although it is highly likely that lexical features would be very informative in this learning task, theywould be useful for the wrong reason: our model is intended to target the emergence of syntax, andnot what children talk about at different ages.
We note, however, that as with the Penn Treebanktagset, the tags used by MOR and POST also reflect morphology, so that information is accountedfor.
The full tag set is listed in (MacWhinney, 2000).Grammatical relations: The second feature class in our model is a bag of dependency labels, where eachlabel correspond to a grammatical relation that holds between two words in the dependency tree (thehead word and the dependent word).
The full set of grammatical relations is listed in (Sagae et al.,2010).Head-dependent part-of-speech pairs: Our third feature class is based on pairs of part-of-speech tags,where each pair corresponds to a bilexical dependency relation in the parse tree, and one of the tagscomes from the head in the dependency, and the other tag comes from the dependent.Head-relation-dependent triples: The last feature class is similar to the head-dependent pairs describedabove, but also including the dependency label that indicates the grammatical relation that holdsbetween the head and dependent words.
Features in this class are then triples composed of a headpart-of-speech tag, a dependent part-of-speech tag, and a dependency label.As an example, given the parse tree shown in Figure 2, the following features would be extracted:n:prop v n nSUBJ OBJ MODv_n:prop v_n n_nv_n:prop_SUBJ v_n_OBJ n_n_MODFeatures are extracted for every tree in each transcript.
Because our goal is to measure grammaticaldevelopment in child language, these four feature templates were designed to capture the grammaticalrelations represented in dependency trees, while leaving out the content reflected in specific lexical items.While the content of what is said may be related to language development, our features are intended tofocus on syntactic information, covering exactly each of the labeled arcs and the part-of-speech tags ina dependency tree (Figure 2) with the words removed.
We also experimented with part-of-speech tagbigrams (pairs of adjacent part-of-speech tags), and dependency chains formed by two dependency arcs.The final choice of the four templates described above was based on results obtained on developmentdata.3.4 Data-driven IPSyn evaluationWe trained a support vector regression model using our training set of 572 transcripts, using a polynomialkernel and tuning the degree d and the regularization metaparameter C on the development set.
Whilethe default C and d values resulted in a mean absolute error of 6.6 points in the score predictions in thedevelopment set, setting C = 1 and d = 3 resulted in a mean absolute error of 4.1 on the developmentset.
We used these values for the rest of our experiments.
The mean absolute error obtained on our2155test set of 48 children (60 transcripts) not used in training or tuning of the system was 3.9.
Whenapplying our regression model to the manually scored set of 20 transcripts used by Sagae et al.
(2005),the mean absolute difference was 4.2 from the scores computed automatically using the approach insection 2.1, and 5.4 from the manually computed scores, which we consider our gold standard target.Compared to these manually computed scores, the absolute difference of 5.4 is higher than what weobtained using carefully designed templates based on the IPSyn inventory, but still within the range ofvariability expected for trained human scorers (Scarborough, 1990).
It is important to keep in mind thatthe goal of this experiment was not to improve on the accuracy of previous automatic scoring programs,which work quite differently by listing manually crafted patterns over parse trees, but to show that ascoring function can be learned in a data-driven way, without manually crafted patterns.
The resultsobtained with our regression model do confirm our hypothesis that simple features extracted from parsetrees are enough for tracking child language development in the same way as the much more complexpatterns included in IPSyn.4 Age predictionGiven the ability of our data-driven approach to approximate IPSyn scores, confirming that a regressionapproach with parse tree features is capable of capturing the progression of language development, wenow turn to the question of whether the same type of data-driven framework can be used to track childlanguage development without the need for a metric such as IPSyn.Assuming only that language acquisition progresses monotonically over time, we can apply the samedata-driven regression approach to predict a child?s age given a language sample.
This task was ap-proached recently by Sahakian and Snyder (2012), who used an ensemble of existing metrics with a fewadditional features.
Unlike in our approach, Sahakian and Snyder do include lexical features and hand-selected patterns in the form of an existing metric (D-level).
They make the reasonable argument thatthe task of age prediction is child-dependent, and that prediction across children would not make sensedue to individual variation in the rate of language development.
Following Sahakian and Snyder, we firstapproach age prediction as a child-specific task, but then discuss the application of our regression modelsfor other children than those used for training.4.1 Child-specific age predictionTo determine whether our data-driven regression approach can model the development of individualchildren at the level where accurate age predictions can be made, we used the same feature templatesdescribed in section 3.3, but trained a regression model to predict age in months, rather than IPSyn scores.Because this is a child-specific prediction task, we train separate regression models for each child.
Wetested our age predictions using 10-fold cross-validation for three children from three different CHILDEScorpora (Adam from Brown, Ross from MacWhinney and Naomi from Sachs) for whom enough datawas available over a wide enough range of ages.
In each case the regression approach performed well.Table 1 shows the mean absolute error in months for each child, and the Pearson r for the correlationbetween predicted age and actual age.Child (corpus) Mean Abs Err Pearson (r)Adam (Brown) 2.5 0.93Ross (MacWhinney) 3.7 0.84Naomi (Sachs) 3.1 0.91Table 1: Regression results for single corpus age prediction (p < 0.0001 for all r values.
)Perhaps more interesting than the strong correlations between actual age and predicted age for each ofthe individual corpora is a comparison of these correlations to correlations between age and MLU, andage and IPSyn score.
One main general criticism of MLU is that it fails to correlate well with age forolder children (around three to four years old).
More detailed metrics such as IPSyn are believed to havebetter correlation with age after that point.
We do observe this situation in our data.
Interestingly, our2156predicted age scores have much stronger correlations to actual age for older children, which suggests thatour regression approach with simple syntactic features is more expressive in tracking syntactic develop-ment in older children than either MLU or IPSyn.
This is shown in Table 2, which contains Pearson rcorrelation coefficients for age and MLU, age and IPSyn, and age and predicted age using our regressionapproach.Child (corpus) MLU r IPSyn r Regression rAdam (Brown) 0.37?0.53?0.85?Ross (MacW) 0.19 0.34?0.79?Naomi (Sachs) 0.27 0.52 0.82?Table 2: Pearson correlation coefficients between actual age and MLU, actual age and IPSyn score,and actual age and predicted age, for children at least three years and four months old.
?p < 0.0001.?p < 0.05.The results shown in Table 2 confirm that features extracted from parse trees alone can offer sub-stantially better prediction of age for individual children than MLU or even IPSyn scores.
This is notsurprising, given that weights for these features are optimized to predict age using data from the specificchild and discriminative learning, but it does show that these features offer enough resolution to tracksyntactic development in child language, confirming our second hypothesis.4.2 Pilot experiment with Japanese language dataA great advantage of using a data-driven framework based on simple feature templates rather than atraditional approach for measuring syntactic development with manually crafted lists of grammaticalstructures is that the data-driven approach is, in principle, language-independent.
The same features de-scribed in section 2.1 could be extracted from dependency parse trees in any language, assuming only thatthese dependency trees can be produced automatically.
Syntactic dependency parsers and treebanks arein fact available for a variety of languages (Buchholz and Marsi, 2006; Nivre et al., 2007).
Although theavailability of treebanks that include child language samples is certainly desirable, it is not clear whetherit is strictly required in order to generate the syntactic structures used in our approach.
While Sagae etal.
(2005) and Hassanali et al.
(2014) obtained high levels of accuracy in IPSyn scoring using the Char-niak (2000) parser with a model trained on the Wall Street Journal portion of the Penn Treebank (Marcuset al., 1993), we have not verified the effects of parser errors in our data-driven approach.
Of course, thelanguage independence claim applies only to the ability to measure syntactic development within differ-ent languages, and direct numerical comparisons across languages are not meaningful, since the availablesyntactic annotations for different languages follow different conventions and syntactic theories.Although a full empirical validation of our regression approach in other languages is left as futurework, we performed a pilot experiment with a single Japanese child that suggests our findings may berobust across languages.
We used transcripts from the child Ryo, from the Miyata corpus of the Japanesesection of the CHILDES database4.
We extracted 80 transcripts of 100 utterances each, covering ages1;10 (22 months) to 3;0 (36 months).
These transcripts were analyzed with the Japanese version ofthe MEGRASP parser for CHILDES transcripts at an estimated accuracy of 93% (Miyata et al., 2013).Using the exact same experimental settings and feature templates as for English, we performed a 10-foldcross-validation for age prediction using the Japanese data.
We obtained a strong correlation betweenpredicted age and actual age, with r = 0.82 (p < 0.0001).
Although this value is slightly lower than thevalues in Table 1 for English, the range of target values (age in months) is more compressed.
Althoughthis experiment included only one child, it does suggest that our approach may work well for Japanese.5 Related workWithin the literature on assessment of child language development, the metric most closely related toour work is the Index of Productive Syntax (Scarborough, 1990), which we discussed in more detail in4http://childes.psy.cmu.edu/data/EastAsian/Japanese/Miyata/2157section 2, and used as a target for data-driven learning.
Other traditional metrics include DevelopmentalSentence Scoring (Lee and Canter, 1971), Language Assessment Remediation and Screening Proce-dure (Fletcher and Garman, 1988), and D-level (Parisse and Le Normand, 1987) all of which share withIPSyn the reliance on a hand-crafted inventory of grammatical structures meant to be identified manuallyin transcribed child language samples.Each of these metrics for child language development, along with the Mean Length of Utter-ance (Brown, 1973), can be computed semi-automatically using the Computerized Profiling sys-tem (Long et al., 2004).
Although fully automatic computation with Computerized Profiling produceslevels of reliability lower than that of manual scoring, the system can be used with human intervention toproduce results of higher quality.
More closely related is the work of Sagae et al.
(2005) on automatingIPSyn using patterns extracted from automatic parse trees.
The work we describe in section 2.1 is closelybased on that of Sagae et al., which we use as a way to validate our data-driven approach.Roark et al.
(2007) examined the ability of several automatically computed syntactic complexity met-rics to discriminate between healthy and language impaired subjects.
Among other metrics, Roark et al.used Frazier scoring (Frazier, 1985) and Yngve scoring (Yngve, 1960), which are more commonly asso-ciated with processing difficulty than with emergence of syntax in child language development, but arerelated to our approach in that they are based on straightforward generic features of parse trees (depth,node count), like our counts of grammatical relation labels.
Finally, Sahakian and Snyder (Sahakianand Snyder, 2012) have also approached the problem of learning automatic metrics for child languagedevelopment using a regression approach.
Their focus, however, was on the combination of the existingmetrics MLU, mean depth of tree (similar to Yngve scoring mentioned above) and D-level, along with afew hand-picked features (counts of certain closed-class words, ratio of function words to content words,and average word frequency), to achieve better discrimination than any of these metrics or features alone.A key difference between our approach and that of Sahakian and Snyder is that their approach builds onand assumes the existence of a metric such as D-level, which, like IPSyn, includes a carefully designedlanguage-dependent inventory of language structures, while we use only simple feature templates appliedto parse trees.
In addition, they include vocabulary-centric features, while we explicitly avoid vocabu-lary features, focusing on structural features.
It is possible that Sahakian and Snyder?s approach wouldbenefit from the parse tree features of our approach, either by using the features directly, or by taking ascore obtained by our approach as an additional feature in theirs.6 Conclusion and future workWe presented a framework for assessment of syntactic development in child language that is com-pletely data-driven, and unlike traditional metrics such as IPSyn, LARSP and D-level, does not relyon a language-dependent inventory of language structures chosen specifically for the task.
Instead, ourapproach is based on the application of support vector regression with simple features extracted fromsyntactic parse trees.
In our experiments we used dependency parses produced by the MEGRASP parserfor CHILDES transcripts, but it is likely that other modern dependency and constituent parsers wouldprovide similar results.
We showed that our framework is capable of learning IPSyn scores, and that forindividual children it can model syntactic development well after MLU and IPSyn scores fail to correlatewith age.Having shown that the feature templates described in section 2.1 are as expressive as the inventoryof grammatical structures in IPSyn at tracking language development, and that syntactic development ofindividual children can be modeled using our data-driven framework in complete absence of an existingmetric such as IPSyn, it is interesting to consider the applicability of this framework to different lan-guages for which child language development metrics have not been developed or are not widely used.One possible way to do this is to train several age regression models representing different developmentprofiles.
In most practical scenarios, the child?s age is known and would not need to be predicted by amodel.
By predicting age with several different models and selecting the one that most closely predictsthe child?s actual age, a language development profile matching the child can be found.
This could beused, for example, in diagnosis of language impairment.
In this paper we established only the expressive2158power of regression using simple syntactic features, and the application of this approach to practical tasksis left as an interesting direction for future work.A related direction for future work is the application of this method for assessment of syntacticdevelopment in languages other than English.
Given the availability of child language data in vari-ous languages (MacWhinney, 2000) and recent progress in syntactic analysis for many of these lan-guages (Buchholz and Marsi, 2006; Nivre et al., 2007), we are optimistic about the applicability of ourapproach to other languages.
Preliminary results using data from one Japanese child suggest that thesame set of simple feature templates can be used to track language development in Japanese.AcknowledgmentsWe thank the anonymous reviewers for insightful suggestions.
This work was partly supported by theNational Science Foundation under grants 1263386 and 1 219253 and by the U.S. Army.
Any opinion,content or information presented does not necessarily reflect the position or the policy of the UnitedStates Government, and no official endorsement should be inferred.ReferencesRoger Brown.
1973.
A first language: The early stages.
George Allen & Unwin.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x shared task on multilingual dependency parsing.
In Proceedingsof the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149?164, New YorkCity, June.
Association for Computational Linguistics.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the first conference on NorthAmerican chapter of the Association for Computational Linguistics, pages 132?139, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Harris Drucker, Chris, Burges L. Kaufman, Alex Smola, and Vladimir Vapnik.
1997.
Support vector regressionmachines.
In Advances in Neural Information Processing Systems 9, volume 9, pages 155?161.Paul Fletcher and Michael Garman.
1988.
LARSPing by numbers.
British Journal of Disorders of Communica-tion, 23(3):309?321.L.
Frazier.
1985.
Syntactic complexity.
In D. R. Dowty, L. Karttunen, and A. M. Zwicky, editors, NaturalLanguage Parsing: Psychological, Computational, and Theoretical Perspectives, pages 129?189.
CambridgeUniversity Press, Cambridge.Khairun-Nisa Hassanali, Yang Liu, Aquiles Iglesias, Thamar Solorio, and Christine Dollaghan.
2014.
Auto-matic generation of the index of productive syntax for child language transcripts.
Behavior Research Methods,46:254?262.Thomas Klee and Martha Deitz Fitzgerald.
1985.
The relation between grammatical development and mean lengthof utterance in morphemes.
Journal of Child Language, 12:251?269, 6.Laura L. Lee and Susan M. Canter.
1971.
Developmental sentence scoring: A clinical procedure for estimatingsyntactic development in children?s spontaneous speech.
Journal of Speech and Hearing Disorders, 36(3):315?340.Steven H. Long, Marc E. Fey, and Ron W. Channell.
2004.
Computerized profiling (version 9.6.0).Brian MacWhinney.
2000.
The CHILDES Project: Tools for Analyzing Talk.
Lawrence Erlbaum Associates, 3rdedition edition.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.
1993.
Building a large annotated corpus ofenglish: The penn treebank.
Comput.
Linguist., 19(2):313?330, June.Susanne Miyata, Kenji Sagae, and Brian MacWhinney.
2013.
The syntax parser GRASP for CHILDES (inJapanese).
Journal of Health and Medical Science, 3:45?62.Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task Ses-sion of EMNLP-CoNLL 2007, pages 915?932, Prague, Czech Republic, June.
Association for ComputationalLinguistics.2159Christophe Parisse and Marie-Thrse Le Normand.
1987.
Indicators of linguistic competence in the peer groupconversational behavior of mildly retarded adults.
Applied Psycholinguistics, 8:19?32.Mabel L. Rice, Filip Smolik, Denise Perpich, Travis Thompson, Nathan Rytting, and Megan Blossom.
2010.Mean length of utterance levels in 6-month intervals for children 3 to 9 years with and without language impair-ments.
Journal of Speech, Language, and Hearing Research, 53:1?17.Brian Roark, Margaret Mitchell, and Kristy Hollingshead.
2007.
Syntactic complexity measures for detectingmild cognitive impairment.
In Proceedings of the Workshop on BioNLP 2007: Biological, Translational, andClinical Language Processing, BioNLP ?07, pages 1?8, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005.
Automatic measurement of syntactic development inchild language.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL?05, pages 197?204, Stroudsburg, PA, USA.
Association for Computational Linguistics.Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhinney, and Shuly Wintner.
2010.
Morphosyntactic annotationof CHILDES transcripts.
Journal of Child Language, 37:705?729.Sam Sahakian and Benjamin Snyder.
2012.
Automatically learning measures of child language development.
InACL (2), pages 95?99.
The Association for Computer Linguistics.Hollis S. Scarborough.
1990.
Index of productive syntax.
Applied Psycholinguistics, 11:1?22, 3.Victor H. Yngve.
1960.
A model and an hypothesis for language structure.
Proceedings of the American Philo-sophical Society, 104:444?466.2160
