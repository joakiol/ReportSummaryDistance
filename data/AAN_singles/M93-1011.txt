GE-CMU: DESCRIPTION OF THE SHOGUN SYSTEM USEDFOR MUC- 5Paul S. Jacobs, George Krupka, and Lisa Ra uInformation Technology LaboratoryGE Research and Developmen tSchenectady, NY 12301 USAMichael L .
Mauldin, Teruko Mitamura, and Tsuyoshi Kitan iCenter for Machine TranslationCarnegie Mellon UniversityPittsburgh, PA 15213 USAIra Sider and Lois ChildsManagement Data System sMartin Mariett aPhiladelphia, PA 19101 USAAbstractThis paper describes the GE-CMU TIPSTER/SHOGUN system as configured for the TIP-STER 24-month (MUC-5) benchmark, and gives details of the system's performance on the se-lected Japanese and English texts.
The SHOGUN system is a distillation of some of the key ideasthat emerged from previous benchmarks and experiments, emphasizing a simple architecture inwhich the focus is on detailed corpus-based knowledge .
This design allowed the project to meet itsgoal of achieving advances in coverage and accuracy while showing consistently good performanc eacross languages and domains .INTRODUCTIONThe GE-CMU TIPSTER/SHOGUN system is the result of a two-year research effort, part of the A1tPA -sponsored TIPSTER: data extraction program.
The project's main goals were : (1) to develop algorithm sthat would advance the state of the art in coverage and accuracy in data extraction, and (2) to demonstrat ehigh performance across languages and domains and to develop methods for easing the adaptation of thesystem to new languages and domains .The system as used in MUC-5 represents a considerable shift from those used in earlier stages of th eprogram and in previous MUC ' s .
The original SHOGUN design integrated several different approaches b ycombining different knowledge sources, such as syntax, semantics, phrasal rules, and domain knowledge, a trun-time .
This allowed the system to achieve a good level of performance very quickly, and made it easy t otest different modules and methods ; however, it proved very difficult to make all the changes necessary t oimprove the system, especially across languages, when system knowledge was so distributed at run-time .As a result, the team adopted a new approach, relying heavily on finite-state approximation .
This methodcombines several earlier previous of work, including Pereira's research on grammar approximation [4], som eof the original ideas on parser compilation from Tomita [5], and GE 's representation of the dynamic lexico n[3, 1] .
Like Pereira 's model, the system uses a finite-state grammar as a loose version of a context fre e1 This research was sponsored (in part) by the Advanced Research Project Agency (DOD) and other government agencies .The views and conclusions contained in this document are those of the authors and should not be interpreted as representin gthe official policies, either expressed or implied, of the Advanced Research Project Agency or the US Government .109grammar, under the assumption that the finite state grammar will cover all the inputs that the genera lgrammar would recognize but perhaps be more tolerant .
However, the system also includes methods fo rcompiling different knowledge sources into the finite state model, particularly emphasizing lexical knowledg eand domain knowledge as reflected in a corpus .This model, in which knowledge is combined at development time to be used by a finite-state patter nsnatching engine at run-time, makes it easier to tune the system to a new language or domain withou tsacrificing the benefit of having general linguistic and conceptual knowledge in the system .While the GE systems, and more recently, the GE-CMU systems, have done well in all the MUC evalua-tions, our rate of progress has never been so great as it has been in the period before MUC-5 .
This is in spit eof the fact that, the team's diagnostic and debugging efforts had to be divided across languages and domain s(handling Japanese, for example, presented a significant overhead in simply being able to follow the rule sand analyze the results) .
We attribute this progress to the current focus on facilitating and automating th eknowledge acquisition process, especially on the use of a corpus .This paper will give a very brief overview of the configuration of the system, followed by the analysis o fthe examples, and some conclusions about the results .SYSTEM OVERVIEWThe TIPSTER/SHOGUN system as configured for the 24-month/ MUC-5 benchmark has roughly thesame components as earlier versions of the system, but the system now performs linguistic analysis entirel yusing a finite-state pattern matcher, instead of LR parsing or chart-style parsing, both of which were par tof the configuration in MUC-4 .Figure 1 shows the basic components of the SHOGUN system, using our own names for modules, wher eapplicable, along with the labels used in Jerry Hobbs ' paper "The Generic Information Extraction System" .The core components of SHOGUN are a subset of the modules that Hobbs describes .
However, the syste mdiffers from other current extraction systems in the use of the finite-state analyzer and the way that corpus-based knowledge is integrated into the lexico-syntactic rules .Finite?state Sentence Analysi s(MUC?5 System )("parser")("lexical disambiguation")PM3Syntactic Parsing(MUC?4 System)Post?processingTRUM PImmiiLR Parse rtext structure (" zoner " )NLlex ( "preprocessor ")PM1 ("filter") (English )statistical filter (ME)PM2 ("preparser ")TRUMPET(" fragment combiner")"semantic interpreter ""discourse processing""template generator "Core lexicons and grammarsFigure I : SHOGUN configuration in MUC- 5Because many of the MUC-5 systems now perform much the same type of pre-processing, name recog-nition, and post.
processing that SIIOGUN has, we will concentrate here on linguistic analysis, includin g110parsing and lexical disambiguation, which were the main research areas of our work on SIIOGUN .About half of the MUC-5 systems still use linguistic analysis driven by "traditional" phrase structur erules, traditional in the sense that there is a clearly separable syntactic component whose knowledge consistsmainly of rules for recognizing grammatical constituents based on word categories (like noun, verb) and wor dorder .
SHOGUN differs from all these systems in that it no longer has any purely syntactic component, an duses finite state rules in place of phrase structure rules .The remaining systems divide roughly into those that emphasize pattern matching and those that empha-size fragment parsing.
The fragment parsing systems, notably BBN's, work fairly close to the way our MU( 1- 4system did, taking advantage of partial parses by using a combination of syntactic and domain knowledg eto guide the combination of syntactic chunks .
The difference between this approach and SHOGUN's curren tprocessing is that fragment parsing is still a largely syntax-first method, while pattern matching tends t ointroduce specialized domain and corpus knowledge by combining this knowledge with syntactic knowledg ein the system's declarative representation .By this coarse characterization, the "pattern matching " group of systems includes, for example, SRI an dUnisys as well as GE-CMU.
We also consider UMass to be in this category, because their linguistic analysi semphasizes lexical and conceptual knowledge rather than constituent structure .Among these approaches, we believe the main differentiator is not in the basic processing algorithms but ,in the way that knowledge ends up getting assigned to various system components .
If there is one noteworth ytrend among the MUC systems as they have evolved over time, it is that they have become more knowledge -based, especially emphasizing more corpus-based and lexical knowledge as well as automated knowledgeacquisition methods.
Within the emerging "generic" model, the main difference among systems is thus i nthe content of their knowledge bases .
Here, the distinguishing characteristic of SHOGUN is probably th edegree to which the system still includes sentence-level knowledge, assigning linguistic and conceptual role smuch the way the TRUMP/TRUMPET combination did but using more detailed, lexically-driven knowledge .Many of the sentence-level rules, for example, include groupings like start a facility and organization nou nphrase, which combine traditional syntactic phrases with lexical or domain knowledge .As systems continue to become still broader in scope and more accurate, it is likely that the way knowledg eis acquired will become the main differentiator .The rest of this paper will discuss the overall results of SHOGUN on MUC-5 and describe how the syste mhandles some of the system walkthrough examples .
The analysis of the examples will highlight some of thes echaracteristics and demonstrate the system's actions in various stages of processing .OVERALL RESULTSThe SHOGUN system did very well on MUC-5 .
The team ' s specific goals were to achieve results on the MUC -5/TIPSTER tasks that were above the level of the simpler MUC-4 task, to attain comparable performanceacross languages and domains, and to reduce customization time as much as possible .
In addition, the aimwas to produce near-human accuracy at a throughput orders of magnitude faster than human beings .
Thesegoals seemed rather ambitious, but SHOGUN reached all of them .The following is a summary of SHOGUN 's performance on all the official metrics .
We put error rat efirst and F-measure last in this table because these are the only ones that can be used for overall syste mcomparison (the goal being low error rate and high F-measure) .Error UND OVG SUB Min-err Max-err Text Rec Pre F-measEJV 61 30 39 19 0 .8784 0.9026 96/92 57 49 52 .
8JJV 54 36 27 12 0 .6624 0.6794 99/98 57 64 60 .
1EME 65 37 41 19 0 .8354 0.8724 95/81 50 48 49 .
2JME 58 30 38 14 0 .7756 0.8152 97/86 60 53 56 .3Figure 2 : SHOGUN Scores for MUC- 5The overall results here are better, on average, than SHOGUN's scores on the MUC-4 benchmark .
Whil e11 1it is very difficult to compare results across domains across languages, it is clear that this shows substantia lprogress, as the MUC-5 tasks are certainly much harder and more detailed than MUC-4 .
In addition, theaverage improvement between the TIPSTER 18 month benchmark and the current point was over 20%, an dthere is certainly more room for further improvement .
Thus we are confident that our current methods an dalgorithms support continued progress toward high accuracy .While it seems that there is substantial variation among the scores on the different language-domain pairs ,this variation is reasonable given the differences among the task and the variations on the test samples .
TheEME result is worse than the others, but the EME MUC-5 test set seemed to be a very difficult one for ou rsystem .
In fact, the system on a blind test using the same configuration scored 9 error rate points better i nEME than on the test reported above .
We are not sure what accounts for this variability in EME, which i smuch greater than on the other domain-language pairs .With respect to achieving human performance, it is not clear where good human perform falls on thesescales, but we are close .
At the TIPSTER 12-month test, a study of trained human analysts placed individua lanalysts between 70 and 80 in F-measure.
However, this test used a somewhat more generous scorin galgorithm than the current one (there have been a number of important changes to the scoring since th e12-month point), and did not separate the analysts work from the preparation of the "ideal" answers?it i simportant in a blind test that the human subject have no impact on the answer key, because there are man ytexts that involve fine-grained interpretation .The results on Japanese are, on average, somewhat higher than the English results .
This is consistent withour tests.
We attribute this to the fact that the Japanese tests are considerably easier than the English ( afactor that is somewhat difficult to weight, given that none of our system developers know Japanese) .
Someof the influences that make the Japanese easier are greater homogeneity in the text sources (for example ,In ;ME includes very different sources from EJV, while JJV and JME are quite consistent in style), shorte rstories with fewer distinct events in Japanese, far fewer new joint venture companies in Japanese, and a nemphasis in Japanese on research and sales rather than production (production activities are more difficul tto assign to codes in the template design) .In addition to the SHOGUN system, the GE-CMU team ran the Japanese benchmarks only using a syste mcalled TEXTRACT, which was developed in parallel to SHOGUN by Tsuyoshi Kitani, a visiting researche rat CMU from NTT Data .
TEXTRACT, like SHOGUN, emphasizes lexically-driven pattern matching ,and the two systems share a Japanese tagging/segmentation program from NTT Data, called MAJESTY .While there is little else that is directly shared between the two system's, additions to TEXTRACT ' sknowledge base were incrementally adapted, in functionality, to SHOGUN 's knowledge base in JJV, thusit it, not surprising that the systems had similar performance on this set .
TEXTRACT generally had abetter performance on company name recognition than SHOGUN, and a somewhat more effective metho dof splitting events .
SHOGUN had better coverage of industry types and products (based, we think, on theheavy use of statistically-based training), and had higher recall (but lower precision) in JME .Figure 3 shows the results of both systems on the recall/precision scale on the various MUC-5 sets .ANALYSIS OF WALKTHROUGH MESSAGESOverview of Example sThe examples are in many ways typical of the TIPSTER-SHOGUN system.
These are relatively easymessages, but the problems the system encountered are illustrative.
In the English message, the systemmade a few minor mistakes, some of which may even have been matters of fine-grained interpretation, andhad an error rate of 15 for EJV0592 .
This is much better than the average message ; on the whole, the EJVperformance is pulled down by "tangled tie-up" messages in which the system has a great deal of difficultydetermining who is doing what with whom ..IJ V0002 was much harder, because it requires information to be split across two tie-ups .
The systemcorrectly determined that there were two tie-ups (which it did not do when it ran this message at the 12 -nronth point), but, it failed to recognize "Toukyou kaijou" as an alias for "Toukyou kaijou kasai hoken " , andas a result ended up getting a whole bunch of aliases and entity pointers wrong .
In addition, SHOGUN mad ethe very typical mistake of almost getting the product service information but losing most of the points ,anyway.
In this case, the Japanese text says that the tie-up will be selling a new product called "hyu-man " .1127060 ?JJ VJME n JJVn JM En GE?CMU50n nSHOGUNEME EJVL GE?CMU "optional"(Textract)4030 ?Precision20 ?
.0I 	 I	 IIIII10203040506070 Recal lFigure 3 : GE-CMU Results for MUC-5/TIPSTER 24-month benchmar kSHOGUN correctly spots this and assumes that whatever "hyu-man" is will be wholesale sales with code 50 .The analyst infers from the context that "hyu-man" is an insurance product, so the actual industry type i s"finance" rather than "sales" .
Finally, the answer key contains an error in the string fill, so SHOGUN getsscored completely wrong on this object .We emphasize these minor mistakes because it helps to show, for one thing, how hard it is to get extremelyhigh accuracy, and, for another, the relative effects of easy and hard objects .
SHOGUN was, by far, the mostaccurate system in determining industry information, probably because our efforts on automated knowledg eacquisition used this object as a test case for both English and Japanese .
However, the net effect of th eindustry object in SHOGUN was a reduction in error of .2 in English and 1 .2 in Japanese over what th esystem would have produced by leaving the product service slot blank .
This is because potentially spuriou sinformation on hard objects and slots dilutes the good scores produced on the easier objects and slots .
Henceit is very difficult to show improvement by getting more information ; the easiest improvements are to ge thigher and higher performance on the "critical" slots and objects .In addition, the system made many technical errors with the location and alias slots, some of which ar eillustrated here .
Often these were due to bugs, but there are many other problems .
The location slot(s )proved much more difficult than expected, because many forms of subtle inferences often affect locatio ninformation, such as inferring that one site subsumes another or inferring location by process of eliminatio n(especially in Japanese) .We will now show, very briefly, the results of each stage in processing of SHOGUN on the EJV and JJ Vexamples .Pre-processingPre-processing identifies names, dates, locations, and other special phrases, and handles certain morphologi -cal rules in Japanese.
For example, the following gives some of the results of pre-processing on one sentencefrom each example :EJV0592 Sentence 0 :[CNAME{1} : BRIDGESTONE SPORTS CO . ]
SAID FRIDAY IT HAS SET UP A JOINT VENTURE113[IGNORE{41} : IN TAIWAN ] WITH A LOCAL CONCERN AND A JAPANESE TRADING HOUS ETO PRODUCE GOLF CLUBS TO BE SHIPPED TO JAPAN .JJV0002 Sentence 0 :[CNAME{24} : V_Ek ` M ]4A 73, 6* U IE L [MORPH{8} :LZ ]L ` ~~t lie1 [MORPH{4} : {eiJc ]flJpa f x ?
v j[MORPH{5} :EB L fc ]Where a company name is marked in pre-processing, this means that the name is "learned" rather tha nrecognized as a known name .
In JJV0002, Daiwashouken ()C*IIIA) is a known name, so it is not markedabove .Linguistic analysi sLinguistic analysis uses the same pattern matcher and same knowledge base notation as pre-processing, bu trelies on a mixture of syntactic and lexical information to perform sentence-level interpretation .
For example ,the following is one rule for marking verb phrases with activity information in English :44 : :; ; JV ACTIVITY-VP; ; ACTIVITY{< ?START-TIME=*date* * >}[ $jventure?ENTITY=(and org-name (not *partner* *venture*) )< ?VENTURE=*venture* {< (member *apostrophe-s* *apostrophe*) rights >} >< (and *venture-org-np* (not $ventureterm)) {$loc} >it$facilityphr ]{$np-postmod}{which}{$helperphr }$verb-premod*{to}[ ?ACTIVITY-TEXT =< ?TIE-UP-ACTIVITY=$actverb< *comma* ?TIE-UP-ACTIVITY=$actverb >*{< {*comma*} and ?TIE-UP-ACTIVITY=$actverb>}$ps-text-list >< ?ACTIVITY-TEXT=< ?TIE-UP-ACTIVITY=$actverb $ps-text-list >?ACTIVITY-TEXT=< *comma* ?TIE-UP-ACTIVITY=$actverb $ps-text-list >*{*comma*} and?ACTIVITY-TEXT=< ?TIE-UP-ACTIVITY=$actverb $ps-text-list > > ]{< (not *date*)* $loc > }{< (not *date*)* ?START-TIME=*date* {$loc} > }<=> (mark-jv-activator c-joint-venture-template) ;In linguistic analysis, the pattern matcher annotates the text, much like it does during pre-processing ,but these annotations can be very close to the roles that portions of text will play in the template .
Forexample, where pre-processing finds company names and organization descriptions, sentence analysis wil loften find partners and ventures .The following are exarnples of this analysis from the walkthrong hEJV0592 Sentence 0 :114[C-JOINT-VENTURE-TEMPLATE{45,44,16,2,0} ?CONJ=<?ENTITY=?PARTNER=?HEAD=BRIDGESTONE SPORTS CO .
SAID FRIDAY IT ?HEAD=HA S?HEAD=SET UP [C-JOINT-VENTURE-TEMPLATE{36,13}?HEAD=A ?HEAD=JOIN TVENTURE IN ?LOCATION=TAIWAN WITH A LOCAL ?PARTNER=CONCERN AND AJAPANESE ?ACTIVITY-TEXT=< ?TIE-UP-ACTIVITY=TRADIN G?TIE-UP-PRODSERV=?PS-TEXT=?PARTNER=HOUSE >=?ACTIVITY-TEXT >=?CON J{45}] ?ACTIVITY-TEXT=< TO ?ACTIVITY-TEXT= <?TIE-UP-ACTIVITY=?HEAD=PRODUCE ?PS-TEXT=< GOL F?TIE-UP-PRODSERV=?TIE-UP-ACTIVITY=CLUBS >=?PS-TEXT >=?ACTIVITY-TEX T{44,36,16,13,2,0}]TO BE SHIPPED TO JAPAN .JJV0002 Sentence 1 :[C-JOINT-VENTURE-TEMPLATE{12,0} .
?PARTNER=?HEAD= pgk( E , ?PARTNER= Ms TtI XiLorME?PARTNER=MIv"C [C-JOINT-VENTURE-TEMPLATE{9} ?PS-TEXT= <?ACTIVITY-TEXT=< ?HEAD= *mil spa >=?PS-TEXT <?TIE-UP-ACTIVITY= ESYj 6 >=?ACTIVITY-TEXT {12,9}]MAW.
[C-JOINT-VENTURE-TEMPLATE{1} ?PARTNER=?HEAD=* o{1,01] [C-JOINT-VENTURE-TEMPLATE{8}?HEAD= Onn 5 < It ?PS-TEXT= ?a{8}]Each set of annotations from sentence-level analysis goes through semantic interpretation, top-dow nanalysis (using TRUMPET), and discourse processing, just as full parses and fragment parses were used i nTRUMP and the LR parser .
The input to TRUMPET now, however, is a set of annotations instead of ful lor partial syntactic trees .Calling Trumpet with SENSE Interpretation :(C-JOINT-VENTURE-TEMPLATE (R-TIE-UP-ACTIVITY (PRODUCE) )(R-LOCATION (TAIWAN (R-NAME TAIWAN)) )(R-PARTNER(CNAME_BRIDGESTONE-SPORTS-001 (R-NAME BRIDGESTONE-SPORTS-CO )(R-PART (C-ENTITY))) )(R-PARTNER (CONCERN)) (R-PARTNER (HOUSE)) )Calling Trumpet with SENSE Interpretation :(C-CAP-TEMPLAT E(R-CAP(C-MONEY (R-QUANTITY (C-NUMBER (R-VALUE 1200000001))) (R-UNIT (DOLLAR))) )(R-OWNED(CNAME_BRIDGESTONE-SPORTS-TAIWAN-001 (R-NAME BRIDGESTONE-SPORTS-TAIWAN-CO )(R-PART (C-ENTITY)))) ); ; Top-down processingLinking (special) C-CAP-TEMPLATE as filler for R-OWNERSHIP of C-JOINT-VENTURE-TEMPLAT ECreating objects in sentence 3 for C-OWN-PERCENT-TEMPLATE marker{17} wit h(?OWNER ?PERCENT) variablesCalling Trumpet with SENSE Interpretation :(C-OWN-PERCENT-TEMPLATE(R-OWNER (CNAME_TAGA-CO1 (R-NAME TAGA-CO) (R-PART (C-ENTITY))) )115(Ft-PERCENT (REMAINDER)) )TRUMPET then takes these pieces of semantic interpretation and tries to map them onto a final template ,applying domain constraints, reference resolution, and heuristics for merging and splitting information fro mmultiple sentences and paragraphs .Discourse Processin gBefore producing the final template, SHOGUN must take all the references to objects and events and try toresolve them.
Often the resolution of object references affects the resolution of event references, because th eobjects become the only tie-in from one description of an event to the next .The discourse processing knowledge of the system is considerably more developed in English than i nJapanese .
This is a case where it was difficult to do all the experiments we would have liked because ou rdevelopers were not bilingual, and discourse cues in Japanese are often fairly subtle .In EJV 0592, the system correctly resolves most of the event and object references, but still does badl yon the location and activity site slots because it assumes that the location of the joint venture company i sthe location of the production activity, and it fails to guess that "Kaohsiung" is in Taiwan .
In addition ,there is a very subtle inference here that the production of clubs in Japan is not an additional location fo rthe production of clubs by the Taiwan unit ; SHOGUN treats both Japan and Taiwan as production bases .
; ; Removing nations (TAIWAN) which conflict with the organizations; ; Replacing Ft-VENTURE references (COMPANY) with (CNAME_BRIDGESTONE-SPORTS-TAIWAN-001 ); ; Removing references (CONCERN HOUSE) from It-PARTNER (CNAME_TAGA-CO 1CNAME_UNION-PRECISION-CASTING-CO1 CNAME_BRIDGESTONE-SPORTS-001 )Creating ACTIVITY template with 1 industrie sResolving "THE TAIWAN UNIT" to PARTNER CNAME_UNION-PRECISION-CASTING-CO 1for nationality "Taiwan (COUNTRY)" using locationResolving "THE JAPANESE SPORTS GOODS MAKER" to PARTNERCNAME_BRIDGESTONE-SPORTS-CO1 for nationality "Japan (COUNTRY) "TEXTRACT "OPTIONAL" SYSTE MIn order to process Japanese, the SHOGUN system uses a morphological analyzer called MAJESTY de-veloped at NTT Data.
As part of our early efforts in the Joint Venture domain, Tusyoshi Kitani of NT TData (who was then a visiting scientist at Carnegie Mellon) wrote several AWK scripts to identify Japanes ecompany names in the segmented output .
Later, rules for identifying other kinds of text fields includin gproper names, locations, numbers and times were added .
This year, he has extended this set of finite-stat erules and augmented it with other modules to perform the entire TIPSTER task on Japanese texts .
For theMUC-5 evaluation, we have submitted TEXTRACT's results on the JJV and JME texts as optional scores .These were officially scored by the government, and the results appear in the table .Error UND OVG SUB Min-err Max-err Text Rec Pre F-measJJVJME49 .9 958 .643 2432 3281 2120 .58770 .67280 .602 80 .707299/9996/856 051686363 .8456 .35Figure 4 : Official TEXTRACT Scores for MUC- 5TEXTRACT : OverviewI'I?X'I'Ii .A('1' is comprised of four major components : preprocessing, pattern snatching, discourse processin gand template generation .
Although only the first of these modules is shared with the SIHO( .7UN system ,,, ,; ; ;; ; ;11 6both systems share the basic method of using finite-state pattern matching instead of full natural languageparsing .In the preprocessor, Japanese text is segmented into primitive words and they are tagged parts of speechby a Japanese segmenter called MAJESTY.
Then, proper nouns, monetary, numeric and temporal expressionsare identified by the proper noun recognizer .
The comprising segments are grouped together to provid emeaningful sets of segments to the succeeding processes [2] .
The pattern matcher searches all possibl epatterns of interest in a sentence that match defined patterns such as tie-up relationships and economi cactivities.
In the discourse processor, company names are identified uniquely throughout a text, allowin grecognition of company relationships and correct merging of information within a text .
Finally, the templategenerator puts extracted information together to create the required template format .The JJV configuration of TEXTRACT has been under development since the Spring, and during th eTIPSTER 18 month evaluation it achieved a recall of 29 and a precision of 70 (for an F-measure of 40 .9) .With 5 months of additional work, TEXTRACT now has a recall of 60 and a precision of 68, giving anF-measure of 63 .8 .The JJV Textract system was ported to the microelectronics domain in three (3) weeks by one person .This was possible because most of the system's modules were shared across both domains (and becauseidentifying company names is a key element to performance in both domains) .
Most of the developmenttime was spent identifying key expressions from the corpus .
The JME configuration of the TEXTRAC Tsystem performed about the same as the base SHOGUN system on JME, but had higher precision compare dto the higher recall of SHOGUN .Our experience with TEXTRACT confirms that finite-state pattern matching allows for very rapid de-velopment of high performance text extraction for new domains .TEXTRACT : Company name identification throughout a text .Unifying multiple references to the same company throughout a text is key to achieving a high performanc ein the template structure of joint venture .
A notion Of "topic companies," which are the main concern i nthe sentence, was introduced.
Topic companies are identified where subject case markers such as " h ;" and" (? "
appear .
When a subject is missing in a sentence, which is often the case in Japanese, the subject i sautomatically assumed as the topic companies taken from the previous sentence .Company aliases are identified by applying a substring matching algorithm called the longest commo nsubsequence (LCS) method .
References of three kinds of company name pronouns, " Ha" (dousha ; thecompany), " (jisha; the company itself), and "MI" (ryousha; both companies) are also identifie dusing the topic companies and some heuristic rules .Every company name in the text, including company aliases and pronouns, is given a unique numbe rby the discourse process .
Using topic companies and the unique number, individual pieces of informatio nidentified by the preprocessor and the pattern matcher are merged together to generate a relevant templat estructure .TEXTRACT: Analysis of a Walkthrough Messag eIn JJV0002, all five entities were correctly identified by the preprocessor .
The pattern matcher also recognize dtwo tie-ups correctly, although the pattern selected from four matched patterns was incorrect in Sentence 2as shown in the traces below.
TEXTRACT found one tie-up from Sentence 2, only because it cannot identifymultiple tie-ups in a sentence with the current design .Sentence no .
= 1@CNAME_PARTNER_SUBJ : 0 .97 :_~~~kktdefined = ti@CNAME_PARTNER_WITH :0 .94 : =36MM*defined =@SKIP =defined = J ,LZL moo) >G i'ifi A.tcgOal re .
?Q'jwLt~o117Sentence no .
= 2@CNAME_PARTNER_WITH1 :0 .50 : = t-cOAX _ E, n Aj$ _o')f J$:E6 IJI?u1defined =@CNAME_PARTNER_WITH2 :0 .50 : = Iv t`' ~ fl inp< , Mrct Z-c .
A5goUlfA#?defined =defined = 0)@SKIP =defined =Oar-i< h f ?
X Lf i~~i Z 1n xa o0002 ryosha = rJfkk#_H, category = 1, distance = 30002 ryosha = Qgfkk~, category = 2, distance = 6An alias " 3krt ?"
(Toukyou kaijou) was found by the LCS method as a substring of the entity name(Toukyou Kaijou Kasai Hoken) .
References of "ME" (ryousha; both companies) werecorrectly resolved as " H AXiLE " (Nisshin Kasai Kaijou Hoken) and " M[fkX' _E%M" (DouwaKasai Kaijou Hoken) .
After the discourse processing, entities were given unique numbers (uniqueid) a sfollows :gid = 1, unique_id = 1, partner 1, string = WTP-:0?AXfOlgid = 4, unique_id = 4, partner 1, string = [1 'ugid = 5, unique_id = 5, partner 2, string = bIVAMN.Egid = 7, unique_id = 7, partner 2, string = n AXitEgid = 9, unique_id = 9, partner 2, string = W?Mlfgid = 12, unique_id = 1, partner 0, string = A-A Egid = 14, unique_id = 1, partner 0, string = W .Egid = 15, unique_id = 4, partner 0, string = Mug*Industry objects and the product service slot were completely wrong due to the following reasons : (1)TEXTRACT did not find the Product/Servicel string, and (2) although it did spot the Product/Service 2string, it gave a wrong pointer to Activityl due to a system bug .
Another observation regarding theindustry object was that TEXTRACT gave the industry type "sales" with SIC 50 to Product/Service 2as the SHOGUN system did .COMBINING SYSTEMS : SHOGUN + TEXTRACTFor the Japanese Microelectronics domain, the SHOGUN system scored the highest recall, while the TEX-TRACT system scored the highest precision .
The F-measure and error scores were almost exactly the same .We developed a statistical technique to combine these systems in a way to improve the F-measure, and as aby-product we determined the theoretical limits of combining the output of the two systems .The combining algorithm works as follows : both SHOGUN and TEXTRACT are run on an input text ,and the output templates are given as input to the combiner .
The following methods were examined :SHOGUN this row just shows the scores for the SHOGUN system .TEXTRACT this row shows the scores for the TEXTRACT system .Theoretical max this row shows the scores for a system which chooses perfectly whether SHOGUN o rTEXTRACT has the better answer for a particular text .Entity weight D=T this row shows the results of using total entity weight to select the output template ,using TEXTRACT output in case of ties .Entity weight D=S same as above, but uses SHOGUN output to break ties .Most names D=S this method chooses the output template with the most entity names .Avg Entity weight D=T similar to entity weight, but the average is used instead of the total weight .11 8SHO + TEX this method uses SHOG UN's output unless it is empty, in which case TEXTRACT's outpu tis used .TEX + SHO this method uses TEXTRACT's output unless it is empty, it which case SHOGUN's outpu tis used .Avg Entity weight D=S average entity weight with SHOGUN output used in case of a tic .Single capability D=T this method chooses the output with the number of capabilities closest to one ,and chooses TEXTRACT's output in case of a tie .Method RecallPrecision F-Measur eSHOGUN 60.030653 .0254 56 .311 0TEXTRACT 50.649863 .4988 56 .351 1Theoretical max 61 .033063 .4371 62 .211 8Entity weight D=T 56 .020858 .9768 57 .460 8Entity weight D=S 60 .146753 .1031 56 .405 8Most names D=S 61 .766551 .5824 56 .217 0Avg Entity weight D=T 53 .820358 .7784 56 .190 2SHO + TEX 60 .703451 .4782 55 .711 5TEX + SHO 52 .347658 .6007 55 .297 9Avg Entity weight D=S 55 .229455 .0946 55 .1619Single capability D=T 53 .025757 .0724 54 .9747Figure 5 : Combining Two MUC-5 Systems : TableFigure 5 gives the numeric values for the various combining methods, and Figure 6 shows the recall -precision performance of each method graphically ..TEXTRACT.
Theoretical ma xEntity weight D=TAvg entity weight D=TTEX + SH OSingle capability D=TAvg entity weight D=S.SHOGUNMost names D=SSHO + TEXRecall65Figure 6 : Combining Two MUC-5 Systems : Graph65C0y0a505011 9Note that the best performing method was the total entity weight, which used statistics from the de-velopment corpus for the entity-name slot to determine which output template had more commonly foun dcompany names .
Intuitively, if the output template had more companies that were associated with correc tkeys from the development corpus, that template is more likely to be correct .
Note also that no knowledge-free combining method gave a better F-measure than either of the two systems alone .SUMMARY AND CONCLUSIO NThe examples and the analysis here are illustrative of the performance of the TIPSTER/SHOGUN syste mon MUC-5 .
While the system has done well and continued to improve significantly, there are still quit ea number of problems that could be fixed to achieve better accuracy .
On the other hand, the stead yimprovement of the system and the high performance across languages are very gratifying, and the fact tha twe already seem close to human performance seems to bode well for the deployment of this technology.While research up to this point has emphasized interpretation and control issues, we see corpus analysi sand knowledge acquisition algorithms as being the key topics for further research and further progress .
Inthis way, MUC-5 may represent a turning point from matters of structure to matters of scale, with most o fthe necessary work on this type of task being broadening scope and scale .
At the same time, we expect tha tsimple but very challenging tasks will emerge that test some of the key algorithms that are required for dat aextraction .References[ 1 ] P. S .
Jacobs and L .
F. Rau .
Innovations in text interpretation .
Artificial Intelligence (Special Issue o nNatural Language Processing), 48, To Appear 1993 .
[2] T. Kitani and T .
Mitamura .
A Japanese Preprocessor for Syntactic and Semantic Parsing .
In NinthIEEE Conference on Artificial Intelligence for Applications .
IEEE, 1993 .
[3] Susan McRoy .
Using multiple knowledge sources for word sense discrimination .
Computational Linguis-tics, 18(1), March 1992 .
[4] Fernando Pereira.
Finite-state approximations of grammars .
In DARPA Speech and Natural Languag eWorkshop, pages 20-25, Hidden Valley, PA, 1990 .
[5] M .
Tomita .
Efficient Parsing for Natural Language .
Kluwer Academic Publishers, Hingham, Mas-sachusetts, 1986 .120
