Syntax-based Alignment of Multiple Translations: Extracting Paraphrasesand Generating New SentencesBo PangDepartment of Computer ScienceCornell UniversityIthaca, NY 14853 USApabo@cs.cornell.eduKevin Knight and Daniel MarcuInformation Sciences InstituteUniversity of Southern CaliforniaMarina Del Rey, CA 90292 USA{knight,marcu}@isi.eduAbstractWe describe a syntax-based algorithm that au-tomatically builds Finite State Automata (wordlattices) from semantically equivalent transla-tion sets.
These FSAs are good representa-tions of paraphrases.
They can be used to ex-tract lexical and syntactic paraphrase pairs andto generate new, unseen sentences that expressthe same meaning as the sentences in the inputsets.
Our FSAs can also predict the correctnessof alternative semantic renderings, which maybe used to evaluate the quality of translations.1 IntroductionIn the past, paraphrases have come under the scrutinyof many research communities.
Information retrieval re-searchers have used paraphrasing techniques for query re-formulation in order to increase the recall of informationretrieval engines (Sparck Jones and Tait, 1984).
Naturallanguage generation researchers have used paraphrasingto increase the expressive power of generation systems(Iordanskaja et al, 1991; Lenke, 1994; Stede, 1999).And researchers in multi-document text summarization(Barzilay et al, 1999), information extraction (Shinyamaet al, 2002), and question answering (Lin and Pantel,2001; Hermjakob et al, 2002) have focused on identi-fying and exploiting paraphrases in the context of recog-nizing redundancies, alternative formulations of the samemeaning, and improving the performance of question an-swering systems.In previous work (Barzilay and McKeown, 2001; Linand Pantel, 2001; Shinyama et al, 2002), paraphrasesare represented as sets or pairs of semantically equiva-lent words, phrases, and patterns.
Although this is ade-quate in the context of some applications, it is clearly tooweak from a generative perspective.
Assume, for exam-ple, that we know that text pairs (stock market rose, stockmarket gained) and (stock market rose, stock prices rose)have the same meaning.
If we memorized only these twopairs, it would be impossible to infer that, in fact, con-sistent with our intuition, any of the following sets ofphrases are also semantically equivalent: {stock marketrose, stock market gained, stock prices rose, stock pricesgained } and {stock market, stock prices } in the con-text of rose or gained; {market rose }, {market gained}, {prices rose } and {prices gained } in the context ofstock; and so on.In this paper, we propose solutions for two problems:the problem of paraphrase representation and the problemof paraphrase induction.
We propose a new, finite-state-based representation of paraphrases that enables one toencode compactly large numbers of paraphrases.
We alsopropose algorithms that automatically derive such repre-sentations from inputs that are now routinely released inconjunction with large scale machine translation evalu-ations (DARPA, 2002): multiple English translations ofmany foreign language texts.
For instance, when givenas input the 11 semantically equivalent English transla-tions in Figure 1, our algorithm automatically induces theFSA in Figure 2, which represents compactly 49 distinctrenderings of the same semantic meaning.
Our FSAscapture both lexical paraphrases, such as {fighting, bat-tle}, {died, were killed} and structural paraphrases suchas {last week?s fighting, the battle of last week}.
Thecontexts in which these are correct paraphrases are alsoconveniently captured in the representation.In previous work, Langkilde and Knight (1998) usedword lattices for language generation, but their methodinvolved hand-crafted rules.
Bangalore et al (2001) andBarzilay and Lee (2002) both applied the technique ofmulti-sequence alignment (MSA) to align parallel cor-pora and produced similar FSAs.
For their purposes,they mainly need to ensure the correctness of consensusamong different translations, so that different constituentorderings in input sentences do not pose a serious prob-Edmonton, May-June 2003Main Papers , pp.
102-109Proceedings of HLT-NAACL 20031.
At least 12 people were killed in the battle last week.
2.
At least 12 people lost their lives in last week?s fighting.3.
Last week?s fight took at least 12 lives.
4.
The fighting last week killed at least 12.5.
The battle of last week killed at least 12 persons.
6.
At least 12 persons died in the fighting last week.7.
At least 12 died in the battle last week.
8.
At least 12 people were killed in the fighting last week.9.
During last week?s fighting, at least 12 people died.
10.
Last week at least twelve people died in the fighting.11.
Last week?s fighting took the lives of twelve people.Figure 1: Sample Sentence Group from the Chinese-English DARPA Evaluation Corpus: 11 English translations ofthe same Chinese sentence.atduringlasttheleastlastweekbattlefightingwerediedlostkilledintheir12persons*e*peoplethelastweekbattlefightinglivesinlastweekfighting?sat?sleastfightingfightdiedinpeopletwelvetheatleastdiedweekfighting?speople12killedtooktheatoflastweeklivesleastoftwelve people12livespersons*e*Figure 2: FSA produced by our syntax-based alignment algorithm from the input in Figure 1.
*e*theduring*e*fightingbattlelast*e*weekweeksfightfighting*e*killedof took*e*theatlivesleastoftwelve12peoplepersons*e*livesdied*e*in*e*the*e*battlefighting*e*lastweeksweekfighting*e**e*peoplelostweretheirkilledFigure 3: FSA produced by a Multi-Sequence Alignment algorithm from the input in Figure 1.lem.
In contrast, we want to ensure the correctness ofall paths represented by the FSAs, and direct applicationof MSA in the presence of different constituent orderingscan be problematic.
For example, when given as input thesame sentences in Figure 1, one instantiation of the MSAalgorithm produces the FSA in Figure 3, which containsmany ?bad?
paths such as the battle of last week?s fight-ing took at least 12 people lost their people died in thefighting last week?s fighting (See Section 4.2.2 for a morequantitative analysis.).
It?s still possible to use MSA if,for example, the input is pre-clustered to have the sameconstituent ordering (Barzilay and Lee (2003)).
But wechose to approach this problem from another direction.As a result, we propose a new syntax-based algorithm toproduce FSAs.In this paper, we first introduce the multiple transla-tion corpus that we use in our experiments (see Section2).
We then present the algorithms that we developed toinduce finite-state paraphrase representations from suchdata (see Section 3).
An important part of the paper isdedicated to evaluating the quality of the finite-state rep-resentations that we derive (see Section 4).
Since our rep-resentations encode thousands and sometimes millions ofequivalent verbalizations of the same meaning, we useboth manual and automatic evaluation techniques.
Someof the automatic evaluations we perform are novel aswell.2 DataThe data we use in this work is the LDC-availableMultiple-Translation Chinese (MTC) Corpus1 developedfor machine translation evaluation, which contains 105news stories (993 sentences) from three sources of jour-nalistic Mandarin Chinese text.
These stories were inde-pendently translated into English by 11 translation agen-cies.
Each sentence group, which consists of 11 semanti-cally equivalent translations, is a rich source for learninglexical and structural paraphrases.
In our experiments,we use 899 of the sentence groups ?
the sentence groupswith sentences longer than 45 words were dropped.3 A Syntax-Based Alignment AlgorithmOur syntax-based alignment algorithm, whose pseu-docode is shown in Figure 4, works in three steps.
In thefirst step (lines 1-5 in Figure 4), we parse every sentencein a sentence group and merge all resulting parse treesinto a parse forest.
In the second step (line 6), we extract1Linguistic Data Consortium (LDC) Catalog NumberLDC2002T01, ISBN 1-58563-217-1.1.
ParseForest = 2.
foreach s ?
SentenceGroup3.
t = parseTree(s);4.
ParseForest = Merge(ParseForest, t);5. endfor6.
Extract FSA from ParseForest;7.
Squeeze FSA;Figure 4: The Syntax-Based Alignment Algorithm.an FSA from the parse forest and then we compact it fur-ther using a limited form of bottom-up alignment, whichwe call squeezing (line 7).
In what follows, we describeeach step in turn.Top-down merging.
Given a sentence group, we passeach of the 11 sentences to Charniak?s (2000) parser toget 11 parse trees.
The first step in the algorithm is tomerge these parse trees into one parse-forest-like struc-ture using a top-down process.Let?s consider a simple case in which the parse for-est contains one single tree, Tree 1 in Figure 5, and weare adding Tree 2 to it.
Since the two trees correspondto sentences that have the same meaning and since bothtrees expand an S node into an NP and a V P , it is rea-sonable to assume that NP1 is a paraphrase of NP2 andV P1 is a paraphrase of V P2.
We merge NP1 with NP2and V P1 with V P2 and continue the merging process oneach of the subtrees recursively, until we either reach theleaves of the trees or the two nodes that we examine areexpanded using different syntactic rules.When we apply this process to the trees in Figure 5,the NP nodes are merged all the way down to the leaves,and we get ?12?
as a paraphrase of ?twelve?
and ?people?as a paraphrase of ?persons?
; in contrast, the two V P sare expanded in different ways, so no merging is donebeyond this level, and we are left with the informationthat ?were killed?
is a paraphrase of ?died?.We repeat this top-down merging procedure with eachof the 11 parse trees in a sentence group.
So far, onlyconstituents with same syntactic type are treated as para-phrases.
However, later we shall see that we can matchword spans whose syntactic types differ.Keyword checking.
The matching process describedabove appears quite strict ?
the expansions must matchexactly for two nodes to be merged.
But consider the fol-lowing parse trees:1.
(S (NP1 people)(V P1 were killed in this battle))2.
(S (NP2 this battle)(V P2 killed people))If we applied the algorithm described above, we wouldmistakenly align NP1 with NP2 and V P1 with V P2 ?the algorithm described so far makes no use of lexical12twelvepeoplepersons were killeddiedMergeLinearizationTree 1 Tree 2Parse ForestFSA / Word LatticeBEG END+SNP VPCD12 NNpersons AUXwere VPVBkilledSNP VPCDtwelve NNpeople VBdiedNP VPCD NN AUX VPVB12twelvepeoplepersons...were...killed...diedFigure 5: Top-down merging of parse trees and FSA ex-traction.information.To prevent such erroneous alignments, we also imple-ment a simple keyword checking procedure.
We notethat since the word ?battle?
appears in both V P1 andNP2, this can serve as an evidence against the merging of(NP1, NP2) and (V P1, V P2).
A similar argument canbe constructed for the word ?people?.
So in this exam-ple we actually have double evidence against merging; ingeneral, one such clue suffices to stop the merging.Our keyword checking procedure acts as a filter.
A listof keywords is maintained for each node in a syntactictree.
This list contains all the nouns, verbs, and adjectivesthat are spanned by a syntactic node.
Before merging twonodes, we check to see whether the keyword lists asso-ciated with them share words with other nodes.
That is,supposed we just merged nodes A and B, and they are ex-panded with the same syntactic rule into A1A2...An andB1B2...Bn respectively; before we merge each Ai withBi, we check for each Bi if its keyword list shares com-mon words with any Aj (j 6= i).
If they do not, we con-tinue the top-down merging process; otherwise we stop.detroitabuildingdetroitdetroitabuildingbuildingin?sbuildingbuildingreducedtorubbleflattenedrazedwasblastedleveledrazedrazedleveledintotodetroitbuildingto downthe groundashesgroundthe groundlevelledtoin detroit grounda.
Before squeezingdetroita*e*?s*e*buildingbuildingreduced*e*wasflattenedblastedleveledlevelledtorazedleveled*e*intototorubblein detroitdown ashesthe*e*groundb.
After squeezingFigure 6: Squeezing effectIn our current implementation, a pair of synonyms cannot stop an otherwise legitimate merging, but it?s possi-ble to extend our keyword checking process with the helpof lexical resources such as WordNet in future work.Mapping Parse Forests into Finite State Automata.The process of mapping Parse Forests into Finite StateAutomata is simple.
We simply traverse the parse foresttop-down and create alternative paths for every mergednode.
For example, the parse forest in Figure 5 is mappedinto the FSA shown at the bottom of the same figure.
Inthe FSA, there is a word associated with each edge.
Dif-ferent paths between any two nodes are assumed to beparaphrases of each other.
Each path that starts from theBEGIN node and ends at the END node correspondsto either an original input sentence or a paraphrase sen-tence.Squeezing.
Since we adopted a very strict matchingcriterion in top-down merging, a small difference in thesyntactic structure of two trees prevents some legitimatemergings from taking place.
This behavior is also exacer-bated by errors in syntactic parsing.
Hence, for instance,three edges labeled detroit at the leftmost of the top FSAin Figure 6 were kept apart.
To compensate for this ef-fect, our algorithm implements an additional step, whichwe call squeezing.
If two different edges that go into (orout of) the same node in an FSA are labeled with the sameword, the nodes on the other end of the edges are merged.We apply this operation exhaustively over the FSAs pro-duced by the top-down merging procedure.
Figure 6 il-lustrates the effect of this operation: the FSA at the topof this figure is compressed into the more compact FSAshown at the bottom of it.
Note that in addition to reduc-ing the redundant edges, this also gives us paraphrasesnot available in the FSA before squeezing (e.g.
{reducedto rubble, blasted to ground}).
Therefore, the squeezingoperation, which implements a limited form of lexicallydriven alignment similar to that exploited by MSA algo-rithms, leads to FSAs that have a larger number of pathsand paraphrases.4 EvaluationThe evaluation for our finite state representations and al-gorithm requires careful examination.
Obviously, whatcounts as a good result largely depends on the applica-tion one has in mind.
If we are extracting paraphrases forquestion-reformulation, it doesn?t really matter if we out-put a few syntactically incorrect paraphrases, as long aswe produce a large number of semantically correct ones.If we want to use the FSA for MT evaluation (for exam-ple, comparing a sentence to be evaluated with the pos-sible paths in FSA), we would want all paths to be rela-tively good (which we will focus on in this paper), whilein some other applications, we may only care about thequality of the best path (not addressed in this paper).
Sec-tion 4.1 concentrates on evaluating the paraphrase pairsthat can be extracted from the FSAs built by our system,while Section 4.2 is dedicated to evaluating the FSAs di-rectly.4.1 Evaluating paraphrase pairs4.1.1 Human-based evaluation of paraphrasesBy construction, different paths between any twonodes in the FSA representations that we derive are para-phrases (in the context in which the nodes occur).
Toevaluate our algorithm, we extract paraphrases from ourFSAs and ask human judges to evaluate their correctness.We compare the paraphrases we collect with paraphrasesthat are derivable from the same corpus using a co-training-based paraphrase extraction algorithm (Barzilayand McKeown, 2001).
To the best of our knowledge, thisis the most relevant work to compare against since it aimsat extracting paraphrase pairs from parallel corpus.
Un-like our syntax-based algorithm which treats a sentenceas a tree structure and uses this hierarchical structural in-formation to guide the merging process, their algorithmtreats a sentence as a sequence of phrases with surround-ing contexts (no hierarchical structure involved) and co-trains classifiers to detect paraphrases and contexts forparaphrases.
It would be interesting to compare the re-sults from two algorithms so different from each other.For the purpose of this experiment, we randomly se-lected 300 paraphrase pairs (Ssyn) from the FSAs pro-duced by our system.
Since the co-training-based al-gorithm of Barzilay and McKeown (2001) takes paral-lel corpus as input, we created out of the MTC corpus55 ?
993 sentence pairs (Each equivalent translation setof cardinality 11 was mapped into(112)equivalent trans-lation pairs.).
Regina Barzilay kindly provided us the listof paraphrases extracted by their algorithm from this par-allel corpus, from which we randomly selected anotherset of 300 paraphrases (Scotr).Correct Partial IncorrectSsyn 85% 12% 3%Judge 1 Scotr 68% 13% 19%Ssyn 80% 13% 7%Judge 2 Scotr 63% 13% 24%Ssyn 81% 5% 13%Judge 3 Scotr 68% 3% 29%Ssyn 77% 17% 5%Judge 4 Scotr 68% 16% 16%Average of Ssyn 81% 12% 7%All Judges Scotr 66% 11% 22%Table 1: A comparison of the correctness of the para-phrases produced by the syntax-based alignment (Ssyn)and co-training-based (Scotr) algorithms.The resulting 600 paraphrase pairs were mixed andpresented in random order to four human judges.
Eachjudge was asked to assess the correctness of 150 para-phrase pairs (75 pairs from each system) based on thecontext, i.e., the sentence group, from which the para-phrase pair was extracted.
Judges were given threechoices: ?Correct?, for perfect paraphrases, ?Partiallycorrect?, for paraphrases in which there is only a par-tial overlap between the meaning of two paraphrases (e.g.while {saving set, aid package} is a correct paraphrasepair in the given context, {set, aide package} is consid-ered partially correct), and ?Incorrect?.
The results of theevaluation are presented in Table 1.Although the four evaluators were judging four differ-ent sets, each clearly rated a higher percentage of the out-puts produced by the syntax-based alignment algorithmas ?Correct?.
We should note that there are parametersspecific to the co-training algorithm that we did not tuneto work for this particular corpus.
In addition, the co-training algorithm recovered more paraphrase pairs: thesyntax-based algorithm extracted 8666 pairs in total with1051 of them extracted at least twice (i.e.
more or lessreliable), while the numbers for the co-training algorithmis 2934 out of a total of 16993 pairs.
This means we arenot comparing the accuracy on the same recall level.Aside from evaluating the correctness of the para-phrases, we are also interested in the degree of overlapbetween the paraphrase pairs discovered by the two algo-rithms so different from each other.
We find that out ofthe 1051 paraphrase pairs that were extracted from morethan one sentence group by the syntax-based algorithm,62.3% were also extracted by the co-training algorithm;and out of the 2934 paraphrase pairs from the results ofco-training algorithm, 33.4% were also extracted by thesyntax-based algorithm.
This shows that in spite of thevery different cues the two different algorithms rely on,range of ASL 1-10 10-20 20-30 30-45recall 30.7% 16.3% 7.8% 3.8%Table 2: Recall of WordNet-consistent synonyms.they do discover a lot of common pairs.4.1.2 WordNet-based analysis of paraphrasesIn order to (roughly) estimate the recall (of lexical syn-onyms) of our algorithm, we use the synonymy relationin WordNet to extract all the synonym pairs present inour corpus.
This extraction process yields the list of allWordNet-consistent synonym pairs that are present in ourdata.
(Note that some of the pairs identified as synonymsby WordNet, like ?follow/be?, are not really synonyms inthe contexts defined in our data set, which may lead toartificial deflation of our recall estimate.)
Once we havethe list of WordNet-consistent paraphrases, we can checkhow many of them are recovered by our method.
Table 2gives the percentage of pairs recovered for each range ofaverage sentence length (ASL) in the group.Not surprisingly, we get higher recall with shorter sen-tences, since long sentences tend to differ in their syn-tactic structures fairly high up in the parse trees, whichleads to fewer mergings at the lexical level.
The recallon the task of extracting lexical synonyms, as definedby WordNet, is not high.
But after all, this is not whatour algorithm has been designed for.
It?s worth notic-ing that the syntax-based algorithm also picks up manyparaphrases that are not identified as synonyms in Word-Net.
Out of 3217 lexical paraphrases that are learned byour system, only 493 (15.3%) are WordNet synonyms,which suggests that paraphrasing is a much richer andlooser relation than synonymy.
However, the WordNet-based recall figures suggest that WordNet can be used asan additional source of information to be exploited by ouralgorithm.4.2 Evaluating the FSA directlyWe noted before that apart from being a natural represen-tation of paraphrases, the FSAs that we build have theirown merit and deserve to be evaluated directly.
Since ourFSAs contain large numbers of paths, we design auto-matic evaluation metrics to assess their qualities.4.2.1 Language Model-based evaluationIf we take our claims seriously, each path in our FSAsthat connects the start and end nodes should correspond toa well-formed sentence.
We are interested in both quan-tity (how many sentences our automata are able to pro-duce) and quality (how good these sentences are).
To an-swer the first question, we simply count the number ofpaths produced by our FSAs.average N (# of paths) logNlength max ave max ave1 - 10 22749 775 10.0 5.210 - 20 172386 4468 12.1 6.220 - 30 3479544 29202 15.1 5.830 - 45 684589 4135 13.4 4.5Table 3: Statistics on Number of Paths in FSAsrandom variable mean std.
devent(FSA)?
ent(SG) ?0.11586 1.25162ent(MTS)?
ent(SG) 1.74259 1.05749Table 4: Quality judged by LMTable 3 gives the statistics on the number of paths pro-duced by our FSAs, reported by the average length ofsentences in the input sentence groups.
For example, thesentence groups that have between 10 and 20 words pro-duce, on average, automata that can yield 4468 alterna-tive, semantically equivalent formulations.Note that if we always get the same degree of mergingper word across all sentence groups, the number of pathswould tend to increase with the sentence length.
This isnot the case here.
Apparently we are getting less merg-ing with longer sentences.
But still, given 11 sentences,we are capable of generating hundreds, thousands, and insome cases even millions of sentences.Obviously, we should not get too happy with our abil-ity to boost the number of equivalent meanings if they areincorrect.
To assess the quality of the FSAs generated byour algorithm, we use a language model-based metric.We train a 4-gram model over one year of the WallStreet Journal using the CMU-Cambridge Statistical Lan-guage Modeling toolkit (v2).
For each sentence groupSG, we use this language model to estimate the aver-age entropy of the 11 original sentences in that group(ent(SG)).
We also compute the average entropy ofall the sentences in the corresponding FSA built by oursyntax-based algorithm (ent(FSA)).
As the statistics inTable 4 show, there is little difference between the av-erage entropy of the original sentences and the averageentropy of the paraphrase sentences we produce.
To bet-ter calibrate this result, we compare it with the averageentropy of 6 corresponding machine translation outputs(ent(MTS)), which were also made available by LDCin conjunction with the same corpus.
As one can see, thedifference between the average entropy of the machineproduced output and the average entropy of the origi-nal 11 sentences is much higher than the difference be-tween the average entropy of the FSA-produced outputsand the average entropy of the original 11 sentences.
Ob-viously, this does not mean that our FSAs only producewell-formed sentences.
But it does mean that our FSAsproduce sentences that look more like human producedsentences than machine produced ones according to a lan-guage model.4.2.2 Word repetition analysisNot surprisingly, the language model we used in Sec-tion 4.2.1 is far from being a perfect judge of sentencequality.
Recall the example of ?bad?
path we gave in Sec-tion 1: the battle of last week?s fighting took at least 12people lost their people died in the fighting last week?sfighting.
Our 4-gram based language model will not findany fault with this sentence.
Notice, however, that somewords (such as ?fighting?
and ?people?)
appear at leasttwice in this path, although they are not repeated in anyof the source sentences.
These erroneous repetitions in-dicate mis-alignment.
By measuring the frequency ofwords that are mistakenly repeated, we can now examinequantitatively whether a direct application of the MSAalgorithm suffers from different constituent orderings aswe expected.For each sentence group, we get a list of words thatnever appear more than once in any sentence in thisgroup.
Given a word from this list and the FSA builtfrom this group, we count the total number of paths thatcontain this word (C) and the number of paths in whichthis word appears at least twice (Cr, i.e.
number of er-roneous repetitions).
We define the repetition ratio tobe Cr/C, which is the proportion of ?bad?
paths in thisFSA according to this word.
If we compute this ra-tio for all the words in the lists of the first 499 groups2and the corresponding FSAs produced by an instantia-tion of the MSA algorithm3, the average repetition ra-tio is 0.0304992 (14.76% of the words have a non-zerorepetition ratio, and the average ratio for these words is0.206671).
In comparison, the average repetition ratio forour algorithm is 0.0035074 (2.16% of the words have anon-zero repetition ratio4, and the average ratio for thesewords is 0.162309).
The presence of different constituentorderings does pose a more serious problem to the MSAalgorithm.4.2.3 MT-based evaluationRecently, Papineni et al (2002) have proposed an au-tomatic MT system evaluation technique (the BLEUscore).
Given an MT system output and a set of refer-2MSA runs very slow for longer sentences, and we believeusing the first 499 groups should be enough to make our point.3We thank Regina Barzilay for providing us this set of re-sults4Note that FSAs produced right after keyword checking willnot yield any non-zero repetition ratio.
However, if there aremis-alignment not prevented by keyword checking in an FSA,it may contain paths with erroneous repetition of words aftersqueezing.range 0-1 1-2 2-3 3-4 4-5count 546 256 80 15 2Table 5: Statistics for edgainence translations, one can estimate the ?goodness?
of theMT output by measuring the n-gram overlap between theoutput and the reference set.
The higher the overlap, i.e.,the closer an output string is to a set of reference transla-tions, the better a translation it is.We hypothesize that our FSAs provide a better repre-sentation against which the outputs of MT systems canbe evaluated because they encode not just a few but thou-sands of equivalent semantic formulations of the desiredmeaning.
Ideally, if the FSAs we build accept all andonly the correct renderings of a given meaning, we canjust give a test sentence to the reference FSA and see ifit is accepted by it.
Since this is not a realistic expecta-tion, we measure the edit distance between a string andan FSA instead: the smaller this distance is, the closer itis to the meaning represented by the FSA.To assess whether our FSAs are more appropriate rep-resentations for evaluating the output of MT systems, weperform the following experiment.
For each sentencegroup, we hold out one sentence as test sentence, and tryto evaluate how much of it can be predicted from the other10 sentences.
We compare two different ways of estimat-ing the predictive power.
(a) we compute the edit distancebetween the test sentence and the other 10 sentences inthe set.
The minimum of this distance is ed(input).
(b)we use dynamic programming to efficiently compute theminimum distance (ed(FSA)) between the test sentenceand all the paths in the FSA built from the other 10 sen-tences.
The smaller the edit distance is, the better weare predicting a test sentence.
Mathematically, the differ-ence between these two measures ed(input)?
ed(FSA)characterizes how much is gained in predictive power bybuilding the FSA.We carry out the experiment described above in a?leave-one-out?
fashion (i.e.
each sentence serves asa test sentence once).
Now let edgain be the averageof ed(input) ?
ed(FSA) over the 11 runs for a givengroup.
We compute this for all 899 groups and find themean for edgain to be 0.91 (std.
dev = 0.78).
Table 5gives the count for groups whose edgain falls into thespecified range.
We can see that the majority of edgainfalls under 2.We are also interested in the relation between the pre-dictive power of the FSAs and the number of referencetranslations they are derived from.
For a given group, werandomly order the sentences in it, set the last one as thetest sentence, and try to predict it with the first 1, 2, 3,... 10 sentences.
We investigate whether more sentencesed(FSAn) ed(inputn)?ed(FSA10) ?ed(FSAn)n mean std.
dev mean std.
dev1 5.65 3.86 0 02 3.66 3.02 0.19 0.603 2.71 2.55 0.33 0.764 2.10 2.33 0.46 0.905 1.56 2.01 0.56 0.956 1.18 1.79 0.65 1.027 0.79 1.48 0.75 1.098 0.49 1.10 0.81 1.119 0.21 0.74 0.89 1.1610 0 0 0.93 1.21Table 6: Effect of monotonically increasing the numberof reference sentencesyield an increase in the predictive power.Let ed(FSAn) be the edit distance from the test sen-tence to the FSA built on the first n sentences; similarly,let ed(inputn) be the minimum edit distance from thetest sentence to an input set that consists of only the firstn sentences.
Table 6 reports the effect of using differ-ent number of reference translations.
The first columnshows that each translation is contributing to the predic-tive power of our FSA.
Even when we add the tenth trans-lation to our FSA, we still improve its predictive power.The second column shows that the more sentences we addto the FSA the larger the difference between its predic-tive power and that of a simple set.
The results in Table 6suggest that our FSA may be used in order to refine theBLEU metric (Papineni et al, 2002).5 Conclusion & Future WorkIn this paper, we presented a new syntax-based algorithmthat learns paraphrases from a newly available dataset.The multiple translation corpus that we use in this paperis the first instance in a series of similar corpora that arebuilt and made publicly available by LDC in the contextof a series of DARPA-sponsored MT evaluations.
Thealgorithm we proposed constructs finite state represen-tations of paraphrases that are useful in many contexts:to induce large lists of lexical and structural paraphrases;to generate semantically equivalent renderings of a givenmeaning; and to estimate the quality of machine transla-tion systems.
More experiments need to be carried outin order to assess extrinsically whether the FSAs we pro-duce can be used to yield higher agreement scores be-tween human and automatic assessments of translationquality.In our future work, we wish to experiment with moreflexible merging algorithms and to integrate better thetop-down and bottom-up processes that are used to in-duce FSAs.
We also wish to extract more abstract para-phrase patterns from the current representation.
Such pat-terns are more likely to get reused ?
which would help usget reliable statistics for them in the extraction phase, andalso have a better chance of being applicable to unseendata.AcknowledgmentsWe thank Hal Daume?
III, Ulrich Germann, and Ulf Herm-jakob for help and discussions; Eric Breck, Hubert Chen,Stephen Chong, Dan Kifer, and Kevin O?Neill for par-ticipating in the human evaluation; and the Cornell NLPgroup and the reviewers for their comments on this pa-per.
We especially want to thank Regina Barzilay andLillian Lee for many valuable suggestions and help at var-ious stages of this work.
Portions of this work were donewhile the first author was visiting Information SciencesInstitute.
This work was supported by the AdvancedResearch and Development Activity (ARDA)?s AdvanceQuestion Answering for Intelligence (AQUAINT) Pro-gram under contract number MDA908-02-C-0007, theNational Science Foundation under ITR/IM grant IIS-0081334 and a Sloan Research Fellowship to Lillian Lee.Any opinions, findings, and conclusions or recommen-dations expressed above are those of the authors and donot necessarily reflect the views of the National ScienceFoundation or the Sloan Foundation.ReferencesSrinivas Bangalore, German Bordel, and Giuseppe Ric-cardi.
2001.
Computing consensus translation frommultiple machine translation systems.
In Workshop onAutomatic Speech Recognition and Understanding.Regina Barzilay and Lillian Lee.
2002.
Bootstrap-ping lexical choice via multiple-sequence alignment.In Proceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 164?171.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT/NAACL.Regina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceedingsof the ACL/EACL, pages 50?57.Regina Barzilay, Kathleen McKeown, and Michael El-hadad.
1999.
Information fusion in the context ofmulti-document summarization.
In Proceedings of theACL, pages 550?557.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the NAACL.DARPA.
2002.
In DARPA IAO Machine TranslationWorkshop, Santa Monica, CA, July 22-23.Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural language based reformulationresource and web exploitation for question answer-ing.
In Proceedings of the Text Retrieval Conference(TREC?2002).
November.Lidija Iordanskaja, Richard Kittredge, and Alain Polge?re.1991.
Lexical selection and paraphrase in a meaning-text generation model.
In Ce?cile L. Paris, William R.Swartout, and William C. Mann, editors, Natural Lan-guage Generation in Artificial Intelligence and Com-putational Linguistics, pages 293?312.
Kluwer Aca-demic Publisher.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProceedings of of ACL/COLING.Nils Lenke.
1994.
Anticipating the reader?s problemsand the automatic generation of paraphrases.
In Pro-ceedings of the 15th International Conference on Com-putational Linguistics, volume 1, pages 319?323, Ky-oto, Japan, August 5?9.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
In Proceedingsof ACM SIGKDD Conference on Knowledge Discov-ery and Data Mining 2001, pages 323?328.Kishore Papineni, Salim Roukos, Todd Ward, John Hen-derson, and Florence Reeder.
2002.
Corpus-basedcomprehensive and diagnostic MT evaluation: InitialArabic, Chinese, French, and Spanish results.
In Pro-ceedings of the Human Language Technology Confer-ence, pages 124?127, San Diego, CA, March 24-27.Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, andRalph Grishman.
2002.
Automatic paraphrase acqui-sition from news articles.
In Proceedings of the Hu-man Language Technology Conference (HLT?02), SanDiego, CA, March 24-27.
Poster presentation.Karen Sparck Jones and John I. Tait.
1984.
Automaticsearch term variant generation.
Journal of Documen-tation, 40(1):50?66.Manfred Stede.
1999.
Lexical Semantics andKnowledge Representation in Multilingual TextGeneration.
Kluwer Academic Publishers,Boston/Dordrecht/London.
