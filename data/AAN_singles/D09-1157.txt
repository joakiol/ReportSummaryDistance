Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513?1522,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPClassifying Relations for Biomedical Named Entity DisambiguationXinglong Wang?
?Jun?ichi Tsujii??
?Sophia Ananiadou??
?School of Computer Science, University of Manchester, UK?National Centre for Text Mining, UK?Department of Computer Science, University of Tokyo, Japan{xinglong.wang,j.tsujii,sophia.ananiadou}@manchester.ac.ukAbstractNamed entity disambiguation concernslinking a potentially ambiguous mentionof named entity in text to an unambigu-ous identifier in a standard database.
Oneapproach to this task is supervised classifi-cation.
However, the availability of train-ing data is often limited, and the avail-able data sets tend to be imbalanced and,in some cases, heterogeneous.
We pro-pose a new method that distinguishes anamed entity by finding the informativekeywords in its surrounding context, andthen trains a model to predict whether eachkeyword indicates the semantic class ofthe entity.
While maintaining a compara-ble performance to supervised classifica-tion, this method avoids using expensivemanually annotated data for each new do-main, and thus achieves better portability.1 IntroductionWhile technology on named entity recognition(NER) matures, many researchers in the field ofinformation extraction (IE) gradually shifted theirfocus to more complex tasks such as named en-tity disambiguation and relation extraction.
Bothtasks are particularly important for biomedical textmining, which concerns automatically extractingfacts from the exponentially growing biomedicalliterature (Hunter and Cohen, 2006).
One type offacts is relations between biomedical named en-tities, such as disease-drug relation, gene-diseaserelation, protein-protein interaction (PPI), etc.
Toautomatically extract these facts, advanced natu-ral language processing techniques such as parsinghave been adopted to analyse the syntactic and se-mantic structure of text.
The idea is that linguisticstructures between the interacting biological enti-ties may have common characteristics that can beexploited by similarity measures or machine learn-ing algorithms.
For example, Erkan et al (2007)used the shortest path between two genes accord-ing to edit distance in a dependency tree to de-fine a kernel function for extracting gene interac-tions.
Miwa et al (2008) comparably evaluated anumber of kernels for incorporating syntactic fea-tures, including the bag-of-word kernel, the subsettree kernel (Moschitti, 2006) and the graph ker-nel (Airola et al, 2008), and they concluded thatcombining all kernels achieved better results thanusing any individual one.
Miyao et al (2008)used syntactic paths as one of the features to traina support vector machines (SVM) model for PPIsand also discussed how different parsers and out-put representations affected the end results.Another crucial IE task is named entity disam-biguation, which concerns grounding mentions ofnamed entities in text to unambiguous concepts asdefined in some standard dictionary or database.For instance, given a search term Python, usersmay like to see the results grouped into the fol-lowing categories: a type of snake, a programminglanguage, or a film (Bunescu and Pas?ca, 2006).One approach to such lexical disambiguation tasksis supervised classification.
However, such tech-niques suffer from the knowledge acquisition bot-tleneck, meaning that manually annotating train-ing data is costly and can never satisfy the need bythe machine learning algorithms.
In addition, su-pervised techniques may not yield reliable resultswhen the distributions of the semantic classes aredifferent in the training and test datasets (Agirreand Martinez, 2004; Koeling et al, 2005).
For ex-ample, on the task of word sense disambiguation,a model trained on a dataset where the predom-inant sense of the word star is ?heavenly body?,may not work well on text mainly composed ofentertainment news.
Such problems are also ma-jor concerns when developing a system to disam-biguate biomedical named entities (e.g., protein,1513gene, and disease), for which some researchersrely on hand-crafted rules in addition to a smallamount of training data (Morgan and Hirschman,2007; Hakenberg et al, 2008).This paper proposes a new disambiguationmethod that, instead of classifying each individualoccurrence of an entity, it classifies pair-wise re-lations between the entity mention in question andthe ?cue words?
in its adjacent context, where eachcue word is assumed to bear a semantic class.
Wethen select the cue word that has a positive rela-tion with the entity, and pass its semantic tag to it.While an individual entity mention may belong toa large number of semantic classes, a relation canonly take one of two values: positive or negative,hence transforming a complex multi-classificationproblem into a less complicated binary classifica-tion task.
The remainder of the paper is organisedas follows: Section 2 proposes the disambigua-tion method and Section 3 introduces the task ofdisambiguating the model organisms of biomedi-cal named entities.
Section 4 describes in detailour proposed method and also a number of base-line systems for comparison purposes.
Section 5shows the evaluation results and discusses the ad-vantages and drawback of our system, and we fi-nally conclude in Section 6.2 Disambiguation as RelationClassificationThe named entity disambiguation task is definedas follows: given a mention of a named entity intext, we automatically assign a semantic tag d toit, where d ?
D, and D is a pre-compiled dic-tionary with |D| entries.
When |D| is small, theproblem can be approached by supervised classi-fication.
For example, to determine whether anoccurrence of an entity is a protein, a gene or anRNA, Hatzivassiloglou et al (2001) comparedperformance of 3 supervised classification meth-ods and reported results near the human agree-ment rate.
Nevertheless, when |D| is large (e.g.,> 100), the performance of classification may de-crease, especially when the distribution of d intraining dataset differs from that in the test set.
Inother words, when |D| is large, named entity dis-ambiguation becomes a multi-class classificationtask on heterogeneous and imbalanced datasets,which is challenging for a machine learning modelto learn to discriminate enough between the se-mantic classes (Japkowicz, 2000).We propose an alternative method for namedentity disambiguation.
Intuitively, in the surround-ing context of an ambiguous entity, one can of-ten find ?cue words?
that are informative indica-tors of the entity?s semantic category.
These cuewords are provided by authors to remind readersthe semantic identity of a named entity.
For ex-ample, in an article about protein p53, phrase ?hu-man protein p53?
may be mentioned, where bothhuman and protein contain semantic informationregarding p53: human indicates the model organ-ism of p53, and protein suggests the type of thisentity.
Such cue words may occur infrequently inthe training data, making it difficult for machinelearning classifiers to capture.Our method exploits this observation.
Given asentence, let E be the set of ?target?
entities (e.g.,p53) and W of the ?cue?
words (e.g., human) thatco-occur in a sentence, we define a relation as apair r = ?e, w?, where e ?
E and w ?
W , andr is a positive relation if e belongs to the semanticclass indicated by w, and is a negative one if not.Then we can disambiguate e by accomplishing thefollowing steps: 1) identify W and build a setof relations R = {?e, wi?|wi?W, i = 1, 2, .., n},where n is the size of W ; and 2) classify everyr ?
R and assign the semantic tag of wjto e suchthat rj= ?e, wj?
is positive.
The first task can betackled by a dictionary lookup, or by an NER sys-tem, if manually annotated data is available.
Thesecond is essentially a binary relation classifica-tion task, and in this work, we use an SVM modelexploiting bag-of-word and syntactic features.3 Species DisambiguationWe show the performance of the proposed methodon a task of resolving one major source of am-biguity in protein and gene entities: model or-ganisms.
Model organisms are species studied tounderstand particular biological phenomena.
Bi-ological experiments are often conducted on onespecies, with the expectation that the discover-ies will provide insight into the workings of oth-ers, including humans, which are more difficultto study directly.
From viruses, prokaryotes, toplants and animals, there are dozens of organ-isms commonly used in biological studies, suchas E. coli, Drosophila, Homo sapiens, and hun-dreds more are frequently mentioned in biologi-cal research papers.
In biomedical articles, entitiesof different species are commonly referred to us-1514ing the same name, causing great ambiguity.
Forexample, searching a protein sequence database,RefSeq1with query ?tumor protein p53?
resultedin over 100 proteins, as the name is shared bymany organisms.The importance of distinguishing model organ-isms has been recognised by the community ofbiomedical text mining.
Chen et al (2005) col-lected gene names from various source databasesand calculated intra- and inter-species ambigui-ties.
Overall, only 25 (0.02%) official symbolswere ambiguous within the organisms.
However,when official symbols from 21 organisms werecombined, the ambiguity increased substantiallyto 21, 279 (14.2%) symbols.
Hakenberg et al(2008) showed that species disambiguation is oneof the most important steps for term normalisa-tion and identification, which concerns automat-ically associating mentions of biomedical enti-ties in text to unique database identifiers (Mor-gan et al, 2008).
Also, the task of extractingPPIs in the recent BioCreative Challenge II work-shop (Hirschman et al, 2007) requires proteinpairs to be recognised and normalised, which in-evitably involves species disambiguation.More specifically, given a text, in which men-tions of biomedical named entities are annotated,a species disambiguation system automatically as-signs a species identifier, as in a standard databaseof model organisms, to every entity mention.
Thetypes of biomedical named entities concerned inthis study are protein, gene, protein complex andmRNA/cDNA, and we used identifiers from theNCBI Taxonomy of model organisms.2The workfocuses on species disambiguation and assumesthat the entities are already identified.
In practice,an automated named entity recogniser (e.g., AB-NER (Settles, 2005)) should be used before apply-ing the systems.4 ApproachesThis section describes a number of approaches tospecies disambiguation, highlighting the relationclassification method proposed in Section 2.4.1 Heuristics BaselinesThe cue words for species are words denotingnames of model organisms (e.g., mouse as in1http://www.ncbi.nlm.nih.gov/RefSeq2http://www.ncbi.nlm.nih.gov/sites/entrez?db=taxonomyphrase ?mouse p53?).
Another clue is the pres-ence of the species-indicating prefixes in gene andprotein names.
For instance, prefix ?h?
in en-tity ?hSos-1?
suggests that it is a human protein.Throughout this paper, we refer to such cue words(e.g., mouse, hSos-1) as ?species words?.
Notethat a species ?word?
may contain multiple tokens(e.g., E. Coli).We encoded this knowledge in a rule-basedspecies tagging system (Wang and Grover, 2008).The system takes a 2-step approach.
First, it marksup species words in the document using a species-word detection program,3which searches everyword in a dictionary of model organisms and as-signs a species ID to the word if a match is found.The dictionary was built using the NCBI taxon-omy4and the UniProt controlled vocabulary ofspecies,5and in total it contains 420,224 specieswords for 324,157 species IDs.
When specieswords are identified, we disambiguate an entitymention using one of the following rules:1. previous species word: If the word preceding an entityis a species word, assign the species ID indicated bythat word to the entity.2.
species word in the same sentence: If a species wordand an entity appear in the same sentence, assign itsspecies ID to the entity.
When more than one speciesword co-occurs in the sentence, priority is given to thespecies word to the entity?s left with the smallest dis-tance.
If all species words occur to the right of the en-tity, take the nearest one.3.
majority vote: assign the most frequently occurringspecies ID in the document to all entity mentions.It is expected that the first rule would producegood precision.
However, it can only disam-biguate the fraction of entities that happen to havea species word to their immediate left.
The secondrule relaxes the first by allowing an entity to takethe species indicated by its nearest species wordin the same sentence, which should increase recallbut decrease precision.
Statistics from our dataset(see Section 5.1) show that only 5.68% entities canpotentially be resolved by rule 1 and 22.16% byrule 2, while the majority rule can tackle every en-tity mention in the dataset.3The species word detector identifies the cue words andwas used in all the systems studied in this paper.
We couldnot properly evaluate the detector due to the lack of man-ually annotated data.
Its performance, however, would notaffect the comparative evaluation results, and improvementto species word detection should increase the performance ofthese disambiguation systems.4ftp://ftp.ncbi.nih.gov/pub/taxonomy/5http://www.expasy.ch/cgi-bin/speclist15154.2 Supervised ClassificationThe disambiguation problem can be approached asa classification task.
Given an entity mention andits surrounding context, a machine learning modelclassifies the entity into one of the classes, whereeach class corresponds to a species ID.
We car-ried out experiments with two classification meth-ods: multi-class classification and one-class clas-sification, where a maximum entropy model6wasused for the former and SVM-light7for the lat-ter.
In one-class classification, we trained a se-ries of binary SVM classifiers, each constructinga separating hyperplane that maximises the mar-gin between the instances of one specific species(i.e., the target class) and a set of randomly se-lected instances of other species (i.e., the outlierclass).
We used equal numbers of instances forboth classes in training.
The following types offeatures were used in both multi-class and one-class experiments, where the values of n wereset empirically by cross-validation on the trainingdata:?
leftContext The n word lemmas to the left of the entity(n = 200).?
rightContext The n word lemmas to the right of theentity (n = 200).?
leftSpeciesIDs The n species IDs to the left of the entity(with order, n = 5).?
rightSpeciesIDs The n species IDs to the right of theentity (with order, n = 5).?
leftNouns The n nouns to the left of the entity (withorder, n = 2).?
leftAdjs The n adjectives to the left of the entity (withorder, n = 2).?
leftSpeciesWords The n species word forms to the leftof the entity (n = 5).?
rightSpeciesWords The n species word forms to theright of the entity (n = 5).?
firstLetter The first character of the entity itself (e.g.,?h?
in hP53).?
documentSpeciesIDs All species IDs that occur in thedocument in question.?
useStopWords filter out function words.?
useStopPattern filter out words consisting only of digitsand punctuation characters.Feature selection was also carried out for theone-class classification experiments.
We com-pared two feature selection methods that report-edly work well on the task of text classification:information gain (IG) (Yang and Pedersen, 1997)6http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html7http://svmlight.joachims.org/The ARG1 ARG1 ARG2ARG1 ARG2ARG1Drosophila Kip3 isorthologue of Klp67A.Figure 1: Predicate argument structure (PAS).and Bi-Normal separation (BNS) (Forman, 2003).IG measures the decrease in entropy when thefeature is given vs. absent, and is defined as:IG(Y |X) = H(Y ) ?
H(Y |X) where H(Y ) isthe uncertainty about the value of Y (i.e., Y ?s en-tropy), and H(Y |X) is Y ?s conditional entropygiven X .
The BNS is defined as: |F?1(x) ?F?1(y)|, where F?1is the standard Normal distri-bution?s inverse cumulative probability function,namely, z-score; x is the ratio between the numberof positive cases containing the feature in ques-tion, and the total number of positive cases; and yis the ratio between the number of negative casescontaining the feature, and the total number ofnegative cases.We computed a weight for each feature and thenranked the features according to their weight, withrespect to each feature selection method.
The top10% features were used in training.
Given a testinstance, the one-class classification method firstcounts the species words in the document that theinstance appears in, and then applies in sequencethe binary models of each occurring species, start-ing from the most frequent one.
For example, ifa document contains 5 occurrences of human and3 mouse, we first apply the human species modelto judge whether an entity mention is of humanspecies, and only if not, the mouse model was ap-plied.
The most-frequent species in the documentwas used as backup when none of the binary mod-els gives positive answers.4.3 Relation Classification4.3.1 OverviewAs for the proposed relation classification method,in the training phase, we first selected the sen-tences in which an entity mention and a speciesword co-occur, and constructed pair-wise entity-species relations.
We then assigned each relation abinary label: a relation is positive if the species IDinferred from the species word matches the gold-standard species annotation on the entity, and isnegative otherwise.
For example, for the sentenceshown in Figure 1, where Drosophila is a speciesword, and Kip3 and Klp67A are proteins, relation?Kip3, Drosophila?
is a negative instance and the1516pair ?Klp67A, Drosophila?
is a positive one.8For each relation, a vector of features were ex-tracted.
We followed the PPI extraction methoddescribed in (Miyao et al, 2008), where two typesof features were used for a SVM classifier.
Thefirst was bag-of-word features, i.e., the words be-fore, between and after the pair of entities, wherethe words were lemmatised.
We added an ad-ditional feature of the distance between the en-tity and the cue word.
The other type was syn-tactic features obtained from parsers.
For bag-of-word features, a linear kernel was used, andfor syntactic ones, a subset tree kernel (Mos-chitti, 2006) was adopted.
The syntactic featureswere represented in a flat tree format.
Figure 2shows such a feature for the negative instance?Kip3, Drosophila?
from Figure 1.
Note that allspecies words (e.g., Drosophila) were normalisedto ?SPECIESWORD?, and entities (e.g., Kip3) to?ENTITY?, which not only reduces the noise inthe feature set, but also makes the model morespecies-generic.
From the training dataset (seeSection 5.1), 25, 413 relations were extracted, ofwhich 63.3% were positive.
(ENJU(noun arg1(SPECIESWORD orthologue))(prep arg12(of orthologue))(prep arg12(of ENTITY)))Figure 2: A syntactic feature obtained from the ENJUparser.To identify the species of an entity in unseentext, we first parsed the sentence, and then listedall pairs of species words and entities as relations.Having extracted the bag-of-word and syntacticfeatures from the instance, the trained model wasapplied to judge whether each species-entity rela-tion was positive.
The entity mention in a positiverelation would be tagged with the ID indicated bythe species word, while the mentions in negativerelations would be left untagged.
The next sectiondescribes in detail how we extracted the syntacticfeatures from text.4.3.2 Syntactic FeaturesGiven a sentence, a natural language parser au-tomatically recognises its syntactic structure andoutputs a parse tree, in which nodes representwords or syntactic constituents.
A path between8Orthologues are genes/proteins in different species buthave similar sequences.
In this example it implies thatKlp67A is a Drosophila protein but Kip3 is not.Parser Input OutputC&C POS-tagged GRENJU POS-tagged PASENJU-Genia POS-tagged PASMinipar Sentence-detected MiniparRASP Tokenised GRStanford POS-tagged SDStanford-Genia POS-tagged SDTable 1: Parsers and their input and output formata pair of nodes can be interpreted as a syntactic re-lation between sentence units, which was proveduseful to infer biological relations (e.g., Airola etal., 2008; Miwa et al, 2008).We experimented with the following parsers(summarised in Table 1):?
Dependency parsers identify one word as the headof a sentence and all other words are either a depen-dent of that word, or else dependent on some otherword that connects to the headword through a sequenceof dependencies.
We used Minipar (Lin, 1998) andRASP (Briscoe et al, 2006) for the experiments;?
Constituent-structured parsers split a sentence intosyntactic constituents such as noun phrases or verbphrases.
We used the Stanford parser (Klein and Man-ning, 2003), and also a variant of the Stanford parser(i.e., Stanford-Genia), which was trained on the GE-NIA treebank (Tateisi et al, 2005) for biomedical text;?
Deep parsers aim to compute in-depth syntactic andsemantic structures based on syntactic theories such asHPSG (Pollard and Sag, 1994) and CCG (Steedman,2000).
We used the C&C parser (Clark and Curran,2007), ENJU (Miyao and Tsujii, 2008), and a variantof ENJU (Hara et al, 2007) adapted for the biomedicaldomain (i.e., ENJU-Genia);There were a number of practical issues to con-sider when using parsers for this task.
Firstly, be-fore parsing, the text needs to be linguistically pre-processed, and the quality of this process has a sig-nificant impact on parsers?
performance.
The pre-processing steps include sentence boundary detec-tion, tokenisation and part-of-speech (POS) tag-ging, all of which can be tricky especially whenapplied to biomedical text (Grover et al, 2003).To avoid the noise that can be introduced in thepre-processing steps and to concentrate on evalu-ating the performance of the parsers, we used thesame pre-processing tools (Alex et al, 2008a)9whenever possible.
The middle column in Ta-ble 1 shows how the input text was linguisti-cally pre-processed with respect to each parser.A POS-tagged text implies that it was also sen-tence boundary detected and tokenised Except for9These particular tools were chosen because they wereadopted to pre-process the ITI-TXM dataset, which we usedin our study.1517RASP and Minipar, all parsers took POS-taggedtext as input.
RASP requires POS tags and punctu-ation labels that were derived from the CLAWS-7tagset,10whereas our dataset uses POS labels fromthe Penn Treebank tagset (Marcus et al, 1994).As RASP does not recognise the Penn tagset, weused its build-in POS tagger.
Minipar, on the otherhand, does not support input of tokenised or POS-tagged text, and therefore took split sentences asinput.Secondly, the output representations of theparsers are different and we preferred a formatthat depicts relations between words instead ofsyntactic constituents.
In total, 4 representationswere used: grammatical relation (GR) (Briscoe etal., 2006), Stanford typed dependency (SD) (deMarneffe et al, 2006), Minipar?s own representa-tion (Lin, 1998), and ENJU?s predicate-argumentstructure (PAS).
All the above representations de-fine relations of words in triples, where a depen-dency triple (i.e., GR, SD and Minipar) consistsof head, dependent and relation, and a PAS triplecontains predicate, argument, and relation.
Fig-ure 1 shows a sentence parsed by ENJU in PASrepresentation.
The right-most column in Table 1lists the output representation of each parser.
Asyntactic path between an entity and a speciesword was represented by a sequence of triples,each following the order of head-dependent orpredicate-argument.
These paths were used assyntactic features for the SVM classifier.4.4 Spreading StrategiesExcept for the majority vote rule, the approachesdescribed in Sections 4.1 and 4.3 were expectedto yield low recall, because they can only detectintra-sentential relations, and therefore only be ap-plied to the entities having at least one speciesword appearing in the same sentence.Since our aim is to disambiguate as many entitymentions as possible, we would like to ?spread?the decisions from the disambiguated mentions totheir ?relatives?
in the same document.
We definean entity mention e?
as another mention e?s rela-tive under either of the following conditions: a)if e?
has the same surface form with e; or, b) ife?
is an abbreviation or an antecedent of e, whereabbreviation/antecedent pairs were detected usingthe algorithm described in (Schwartz and Hearst,10http://ucrel.lancs.ac.uk/claws7tags.html2003).
Given the set of disambiguated mentions,we then ?spread?
their species IDs to their rela-tives in the same document.
After this process, thementions that do not have any disambiguated rela-tives would still be missed by the system.
In suchcases, we used a ?default?
species, as determinedby the rule of majority vote (see Section 4.1).5 Evaluation5.1 Data and OntologyThe species disambiguation experiments wereconducted using the ITI-TXM corpus (Alex et al,2008b), a collection of full-length biomedical re-search articles manually annotated with linguisticand biomedical information for developing auto-matic information extraction systems.
The cor-pus contains two datasets covering slightly dif-ferent domains: enriched protein-protein interac-tion (EPPI) and tissue expression (TE).
When-ever possible, protein, protein complex, gene, andmRNA/cDNA entities were tagged with NCBITaxonomy IDs, denoting their species, and it wasthe species annotation that this study used.The EPPI and TE datasets have different distri-butions of species.
The entities in EPPI belong to118 species with human being the most frequent at51.98%.
In TE, the entities are across 67 speciesand mouse is the most frequent at 44.67%.11Theinter-annotator agreement of species annotation onEPPI and TE are 86.45% and 95.11%, respectively.The species disambiguation systems were de-veloped on the training portions of the EPPI andTE corpora, each containing 221 articles, and eval-uated on a dataset combining the developmenttest (DEVTEST) datasets of EPPI and TE, contain-ing 58 and 48 articles, respectively.
The com-bined training dataset contains 96, 992 entity men-tions belonging to 138 model organisms, while theDEVTEST dataset contains 23, 118 entities of 54species.
The diversity of model organisms in thiscorpus highlights the fact that a primary consid-eration when developing a species disambiguationsystem is its ability to distinguish a wide range ofspecies with minimal additional manual effort.5.2 Results5.2.1 Evaluation MetricsThe evaluation was carried out on the DEVTESTdataset, and the systems are compared using av-11These figures were obtained from the training split of thedatasets.1518micro-avg.
macro-avg.Maxent 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85SVM 62.24 / 59.35 / 60.76 14.70 / 17.11 / 15.01SVM (IG) 65.20 / 61.06 / 63.06 14.90 / 19.53 / 16.09SVM (BNS) 43.61 / 42.63 / 43.11 11.99 / 10.05 / 9.34Table 2: Evaluation results of the classification systems onDEVTEST (precision/recall/F1-score, in %)eraged precision, recall and F1 scores over allspecies.
In more detail, for each model organismthat appears in the DEVTEST dataset, we collecttwo lists of entity mentions of that species: onefrom the gold-standard DEVTEST dataset, and theother from the output of a disambiguation system.Then the list of system output is compared againstthe gold-standard list to obtain precision, recalland F1 score.
For each system, the scores ob-tained from all species are averaged using micro-average and macro-average.
The micro-average isthe mean of the summation of contingency metricsfor all model organisms, so that scores of the morefrequent species influence the mean more thanthose of less frequent ones.
The macro-average isthe mean of precision, recall, or F1 over all labels,thus attributing equal weights to each species, andmeasuring a system?s adaptability across differentmodel organisms.5.2.2 Evaluation ResultsFirst of all, Table 2 shows the results of the clas-sification methods described in Section 4.2.
Themulti-classification system using a maximum en-tropy model (Maxent) yielded the highest overallmicro-averaged F1.
Among the SVM-based sys-tems, the one using IG feature selection achievedbetter performance.
In particular, it outperformedthe Maxent model in term of macro-averages.
Theperformance of the SVM model with BNS featureselection is disappointing, perhaps because the oc-currences of a feature in each instance are not nor-mally distributed.
As the Maxent system obtainedbetter results, it was used to compare with otherdisambiguation systems.Table 3 shows the results of a number of meth-ods described in the previous sections.
The meth-ods are categorised into 4 groups: rule-basedbaseline systems, a Maxent classification model,relation-classification methods, and a hybrid sys-tem.
The difference between the relation classifi-cation systems is the features adopted.
Rel-Contextwas trained on only bag-of-word and distance fea-tures, whereas each other system also used syn-tactic features provided by a specific parser.
Forexample, the Rel-RASP system identifies an entity?sspecies by finding positive relations between theentity and its neighbouring species words, usingfeatures including bag-of-word, distance, and de-pendency paths generated by RASP.
The hybridsystem (Hbrd) ran the Rel-ENJU-Genia system on topof the outcome of Maxent.
When a conflict oc-curs, the species ID is chosen by Rel-ENJU-Genia.The idea is that the relation classification systemis more accurate than Maxent when it is applica-ble, and hence would improve precision on dis-ambiguating the species with few or no traininginstances.Without spreading (shown in the ?NO SPRD?columns of Table 3), most of the rule-based and re-lation classification systems only work on a subsetof DEVTEST, resulting in low recall: Rule-Sp workson the small proportion of entities (5.68%) with apreceding species word, while the other systemsonly work on the collection of sentences contain-ing at least one species word and one entity, whichcovers 4.60% sentences and 22.16% entity men-tions.
Rule-Majority, Maxent, and Hbrd, on the otherhand, apply to all entity mentions, and thereforethey are only compared against the others whenspreading was applied.The results shown in the ?NO SPRD?
columnscan be viewed as a comparative evaluation ofthe usefulness of the syntactic features suppliedby the parsers on this particular task.
The rule-based systems set high baselines: Rule-Sp pro-duced good precision and Rule-SpSent achieved thehighest micro-averaged F1, thanks to its highcoverage, which is also an upperbound of recallfor the relation classification systems.
Neverthe-less, it is encouraging that the relation classifica-tion systems obtained higher precision than Rule-SpSent, which is important, considering the de-cisions will be transfered to the untagged entitymentions across the document.
Indeed, as shownin the SPRD columns in Table 3, most relationclassification systems outperformed the Rule-SpSentbaseline when spreading was used.
The scoresof the systems using different parser outputs onlyvary slightly.
Rel-Context, on the other hand, sur-passed others in terms of micro-averaged preci-sion, while sacrificing micro-averaged recall andmacro-averaged scores.Next, the SPRD columns in Table 3 show the re-sults when the spreading rules were applied, which1519METHOD NO SPRD (micro-avg) NO SPRD (macro-avg) SPRD (micro-avg) SPRD (macro-avg)Rule-Majority N/A N/A 66.14 / 61.99 / 64.00 16.76 / 21.75 / 18.08Rule-Sp 88.96 / 5.02 / 9.51 33.77 / 8.55 / 10.18 66.96 / 63.41 / 65.13 28.25 / 30.65 / 27.00Rule-SpSent 80.82 / 16.88 / 27.93 43.16 / 28.85 / 24.73 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10Maxent N/A N/A 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85Rel-Context 90.04 / 3.71 / 6.13 15.23 / 4.45 / 4.90 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10Rel-C&C 82.79 / 16.14 / 27.02 43.97 / 29.56 / 25.60 66.59 / 63.64 / 65.08 32.29 / 33.20 / 29.14Rel-ENJU 83.39 / 15.87 / 26.66 46.89 / 29.88 / 25.95 68.28 / 65.02 / 66.61 31.82 / 34.08 / 29.67Rel-ENJU-Genia 83.54 / 15.74 / 26.49 44.13 / 29.93 / 25.78 68.91 / 65.45 / 67.13 32.00 / 34.87 / 30.21Rel-Minipar 81.82 / 16.27 / 27.14 43.63 / 27.88 / 24.15 67.98 / 63.77 / 65.81 31.83 / 33.93 / 29.44Rel-RASP 81.67 / 16.10 / 26.90 43.95 / 28.92 / 25.03 66.62 / 64.08 / 65.33 32.66 / 33.54 / 29.80Rel-Stanford 82.75 / 16.10 / 26.95 44.05 / 29.49 / 25.92 66.81 / 63.81 / 65.28 32.67 / 33.03 / 29.45Rel-Stanford-Genia 82.22 / 16.04 / 26.84 43.37 / 29.40 / 25.22 66.85 / 63.64 / 65.21 32.72 / 32.29 / 28.64Hbrd N/A N/A 74.15 / 73.26 / 73.70 43.98 / 37.47 / 31.80Table 3: Evaluation results of the species disambiguation systems on DEVTEST (precision/recall/F1-score, in %)effectively improved recall (see Section 5.2.3for discussion on statistical significance tests onthe results).
The Maxent system achieved verygood micro-averaged precision, but low macro-averaged scores.
In fact, as shown in Table 4, Max-ent can only disambiguate 7 species (out of a totalof 54) that have relatively large amount of train-ing instances,12and failed completely on otherspecies.
This suggests that Maxent may not be ableto generate good micro-averaged scores when ap-plied to a dataset where the dominant species aredifferent from those in the training set.
On theother hand, the relation-classification approacheshave a clear advantage over Maxent as measuredby macro-averaged scores.
As shown in Table 4,Rel-ENJU-Genia worked well on most of the species,displaying its good adaptability, while achievingcomparable micro-averaged F1 to Maxent.
Over-all, Hbrd, which combines the strengths of relationclassification and the Maxent classification model,obtained the highest points as measured by everymetric.5.2.3 Statistical SignificanceTo see whether our methods significantly im-proved the baseline systems, we performed ran-domisation tests (Noreen, 1989; Yeh, 2000) onsome of the results shown in Table 3.
The in-tuition of randomisation test is as follows: whencomparing two systems (e.g., A and B), we erasethe labels ?output of A?
or ?output of B?
from allobservations.
The null hypothesis is that there isno difference between A and B, and thus any re-sponse produced by one of the systems could haveas likely come from the other.
We shuffle these re-12The following 7 species occur most frequently in thetraining set: H. sapiens (43.25%), M. musculus (27.05%),R. norvegicus (5.35%), S. cerevisiae (3.98%), X. tropicalis(3.56%), D. melanogaster (3.33%) and C. elegans (0.94%).Species Name Pct Mxt Rel HbrdH.
sapiens 50.13% 76.25 65.33 79.51M.
musculus 13.99% 66.41 58.29 68.27X.
tropicalis 7.35% 64.80 77.72 71.39D.
melanogaster 6.34% 93.17 78.46 95.15S.
cerevisiae 4.79% 90.12 83.32 87.68R.
norvegicus 2.97% 44.04 38.69 51.77T.
aestivum 2.62% 0.00 89.68 23.35P.
americana 2.27% 0.00 98.50 7.76C.
elegans 2.08% 96.83 95.88 97.50H.
herpesvirus 5 1.58% 0.00 54.46 4.27R.
virus 1.45% 0.00 28.54 6.45H.
spumaretrovirus 1.17% 0.00 99.37 2.49... ... ... ... ...Macro-average 9.85 30.21 31.80Micro-average 70.48 67.13 73.70Table 4: The micro-averaged F1 scores (%) of Maxent(Mxt), Rel-ENJU-Genia with spreading (Rel), and Hbrd withrespect to each of the most frequent 12 species in DEVTEST.sponses R times, reassign each response to A orB and see how likely such a shuffle produces adifference in the metric of interest that is at leastas large as the difference observed when using Aand B on the test data.
Let r denote the numberof times that such a difference occurred, then asR ?
?,r+1R+1approaches the significance level.In our case, the metrics tested were micro- andmacro-averaged precision, recall and F1.Following this procedure, we tested whether theimprovements made by a relation classificationbased system (i.e., Rel-ENJU-Genia with SPRD) andthe hybrid system (i.e., Hbrd) over the baseline sys-tems were statistically significant.
We carried outapproximate randomisation with 10,000 shufflesand the test results are shown in Table 5.
The nu-merical figures in the cells are differences in pre-cision, recall and F1 between a pair of systems.The significance levels (i.e., p-values) are indi-cated by superscript marks, whose correspond-ing values are displayed in Table 6.
For exam-1520Rule-Majority Rule-Sp Rule-SpSent MaxentRelmicro-avg 2.77?/3.46?/3.13?1.95?/2.04?/2.00?1.57?/2.22?/1.92?-1.57?/ -5.02?/ -3.35?macro-avg 15.24?/13.12?/12.13?3.75a/4.21a/3.20a9.35?/8.44?/7.10?21.92?/24.87?/20.35?Hbrdmicro-avg 8.01?/11.27?/9.70?7.19?/9.85?/8.57?6.81?/10.04?/8.49?3.67?/2.78?/2.82bmacro-avg 27.22?/15.72c/13.72d15.73?/6.82e/4.80f21.33?/11.05g/ 8.70h33.91i/27.47?/21.95?Table 5: Results of paired randomisation tests on whether Rel-ENJU-Genia with SPRD (Rel) and Hbrd significantly im-proved the baseline systems.
The numerical figures in the cells show the differences between the two systems as measured byprecision/recall/F1 in percentage.
The superscript marks indicate the significance levels and are explained in Table 6.ple, the difference in micro-averaged precision be-tween Rel-ENJU-Genia and Rule-Majority on the testdata was 2.77%, and in 10,000 approximate ran-domisation trials, there was zero times13that Rel-ENJU-Genia?s micro-averaged precision is greaterthan Rule-Majority?s by at least 2.77% (p < 0.0001).MARK VALUE MARK VALUE* p < 0.0001 a p < 0.06b p < 0.002 c p < 0.0003d p < 0.0002 e p < 0.03f p < 0.05 g p < 0.003h p < 0.005 i p < 0.07Table 6: p-values.The test results confirmed that, the improve-ments made by Hbrd are statistically significantwith at least 95% confidence as measured by allmetrics except for macro-averaged precision.
Therelation classification approach achieved signifi-cantly lower performance than Maxent in terms ofmicro-averaged scores (hence the ?-?
sign in thecorresponding cell in Table 5), but in all othercases it can reject the null hypothesis with veryhigh confidence (i.e., p < 0.0001).6 Conclusions and Future WorkThis paper proposes a method that tackles a com-plex disambiguation problem by breaking it intotwo cascaded simpler tasks of cue word discov-ery and binary relation classification.
We evalu-ated the method on the task of disambiguating themodel organisms of biomedical named entities,along with a number of other approaches.
As mea-sured by micro-averaged F1 score, a supervisedclassification approach (Maxent) yielded the secondbest result.
However, it can only disambiguatea small number of species that have abundanttraining instances.
With spreading rules, a rela-tion classification system (Rel-ENJU-Genia) trainedon word and syntactic features from ENJU-Geniaalso obtained good micro-averaged F1, while sur-13The numbers of times are not shown in Table5 forbrevity.passing Maxent significantly in terms of macro-averaged scores.
Combining these two systemsachieved the best overall performance.
Neverthe-less, we combined the two methods in a rathercrude way, leaving ample room for exploring bet-ter strategies in the future.One drawback of the relation classification sys-tems is that they can not cover all entity mentionsbut only the ones with informative keywords co-occurring in the same sentence.
We overcame thedrawback by using spreading rules.
For some ap-plications, however, it may be sufficient to makepredictions exclusively for cases where the sys-tems are applicable.
Also, the predictions withhigh confidence can be used as seed training ma-terial for automatically harvesting more trainingdata.AcknowledgmentsThe work reported in this paper is funded by PfizerLtd..
The UK National Centre for Text Mining isfunded by JISC.
The ITI-TXM corpus used in theexperiments was developed at School of Informat-ics, University of Edinburgh, in the TXM project,which was funded by ITI Life Sciences, Scotland.ReferencesE.
Agirre and D. Martinez.
2004.
Unsupervised WSD basedon automatically retrieved examples: The importance ofbias.
In Proceedings of EMNLP.A.
Airola, S. Pyysalo, J. Bj?orne, T. Pahikkala, F. Ginter, andT.
Salakoski.
2008.
A graph kernel for protein-proteininteraction extraction.
In Proceedings of BioNLP.B.
Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,M.
Matthews, S. Roebuck, R. Tobin, and X. Wang.
2008a.Assisted curation: does text mining really help?
In Pro-ceedings of the Pacific Symposium on Biocomputing.B.
Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,M.
Matthews, S. Roebuck, R. Tobin, and X. Wang.
2008b.The ITI TXM corpus: Tissue expression and protein-protein interactions.
In Proceedings of the Workshop onBuilding and Evaluating Resources for Biomedical TextMining at LREC.E.
Briscoe, J. Carroll, and R. Watson.
2006.
The secondrelease of the RASP system.
In Proceedings of the COL-ING/ACL Interactive Presentation Sessions.1521R.
Bunescu and M. Pas?ca.
2006.
Using encyclopedic knowl-edge for named entity disambiguation.
In Proceedings ofEACL.L.
Chen, H. Liu, and C. Friedman.
2005.
Gene nameambiguity of eukaryotic nomenclatures.
Bioinformatics,21(2):248?256.S.
Clark and J. R. Curran.
2007.
Wide-coverage efficientstatistical parsing with CCG and log-linear models.
Com-putational Linguistics, 33(4).M-C de Marneffe, B. MacCartney, and C. D. Manning.
2006.Generating typed dependency parses from phrase struc-ture.
In Proceedings of LREC.G.
Erkan, A. Ozgur, and D. R. Radev.
2007.
Semi-supervised classification for extracting protein interactionsentences using dependency parsing.
In Proceedings ofthe Joint Conference of EMNLP and CoNLL.G.
Forman.
2003.
An extensive empirical study of feature se-lection metrics for text classification.
Journal of MachineLearning Research, 3:1289?1305.C.
Grover, M. Lapata, and A. Ascarides.
2003.
A compar-ison of parsing technologies for the biomedical domain.Natural Language Engineering, 1(1):1?38.J.
Hakenberg, C. Plake, R. Leaman, M. Schroeder, andG.
Gonzalez.
2008.
Inter-species normalization of genementions with GNAT.
Bioinformatics, 24(16).T.
Hara, Y. Miyao, and J. Tsujii.
2007.
Evaluating impactof re-training a lexical disambiguation model on domainadaptation of an HPSG parser.
In Proceedings of the 10thInternational Conference on Parsing Technology.V.
Hatzivassiloglou, PA Dubou?e, and A. Rzhetsky.
2001.Disambiguating proteins, genes, and RNA in text: a ma-chine learning approach.
Bioinformatics, 17(Suppl 1).L.
Hirschman, M. Krallinger, J. Wilbur, and A. Valencia, ed-itors.
2007.
The BioCreative II - Critical Assessmentfor Information Extraction in Biology Challenge, volume9(Suppl 2).
Genome Biology.L.
Hunter and K. B. Cohen.
2006.
Biomedical languageprocessing: what?s beyond PubMed.
Molecular Cell,21(5):589?594.N.
Japkowicz.
2000.
Learning from imbalanced data sets: acomparison of various strategies.
In Proceedings of AAAIWorkshop on Learning from Imbalanced Data Sets.D.
Klein and C. D. Manning.
2003.
Accurate unlexicalizedparsing.
In Proceedings of ACL.R.
Koeling, D. McCarthy, and J. Carroll.
2005.
Domain-specific sense distributions and predominant sense acqui-sition.
In Proceedings of HLT/EMNLP.D.
Lin.
1998.
Dependency-based evaluation of MINIPAR.In Proceedings of Workshop on the Evaluation of ParsingSystems.M.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.
1994.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313?330.M.
Miwa, R. Satre, Y. Miyao, T. Ohta, and J. Tsujii.
2008.Combining multiple layers of syntactic information forprotein-protein interaction extraction.
In Proceedings ofSMBM.Y.
Miyao and J. Tsujii.
2008.
Feature forest models for prob-abilistic HPSG parsing.
Computational Linguistics, 34(1).Y.
Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsujii.2008.
Task-oriented evaluation of syntactic parsers andtheir representations.
In Proceedings of ACL-08: HLT.A.
A. Morgan and L. Hirschman.
2007.
Overview ofBioCreAtIvE II gene normalisation.
In Proceedings of theBioCreAtIvE II Workshop, Madrid.A.
A. Morgan, Z. Lu, X. Wang, A. M. Cohen, J. Fluck,P.
Ruch, A. Divoli, K. Fundel, R. Leaman, J. Haken-berg, C. Sun, H. Liu, R. Torres, M. Krauthammer, W. W.Lau, H. Liu, C. Hsu, M. Schuemie, K. B. Cohen, andL.
Hirschman.
2008.
Overview of BioCreAtIvE II genenormalization.
Genome Biology, 9(Suppl 2).A.
Moschitti.
2006.
Making tree kernels practical for naturallanguage learning.
In Proceedings of EACL.E.
W. Noreen.
1989.
Computer Intensive Methods for Test-ing Hypothesis.
John Wiley & Sons.C.
Pollard and I.
A.
Sag.
1994.
Head-Driven Phrase Struc-ture Grammar.
University of Chicago Press, Chicago.A.
S. Schwartz and M. A. Hearst.
2003.
Identifying abbrevi-ation definitions in biomedical text.
In Proceedings of thePacific Symposium on Biocomputing.B.
Settles.
2005.
ABNER: An open source tool for automat-ically tagging genes, proteins, and other entity names intext.
Bioinformatics, 21(14):3191?3192.M.
Steedman.
2000.
The Syntactic Process.
The MIT Press,Cambridge, MA.Y.
Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii.
2005.
Syn-tax annotation for the GENIA corpus.
In Proceedings ofIJCNLP.X.
Wang and C. Grover.
2008.
Learning the species ofbiomedical named entities from annotated corpora.
InProceedings of LREC.Y.
Yang and J. Pedersen.
1997.
A comparative study onfeature selection in text categorization.
In Proceedings ofICML.A.
Yeh.
2000.
More accurate tests for the statistical signifi-cance of result differences.
In Proceedings of COLING.1522
