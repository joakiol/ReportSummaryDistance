Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 822?827,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsHow much do word embeddings encode about syntax?Jacob Andreas and Dan KleinComputer Science DivisionUniversity of California, Berkeley{jda,klein}@cs.berkeley.eduAbstractDo continuous word embeddings encodeany useful information for constituencyparsing?
We isolate three ways in whichword embeddings might augment a state-of-the-art statistical parser: by connectingout-of-vocabulary words to known ones,by encouraging common behavior amongrelated in-vocabulary words, and by di-rectly providing features for the lexicon.We test each of these hypotheses with atargeted change to a state-of-the-art base-line.
Despite small gains on extremelysmall supervised training sets, we findthat extra information from embeddingsappears to make little or no differenceto a parser with adequate training data.Our results support an overall hypothe-sis that word embeddings import syntac-tic information that is ultimately redun-dant with distinctions learned from tree-banks in other ways.1 IntroductionThis paper investigates a variety of ways inwhich word embeddings might augment a con-stituency parser with a discrete state space.
Wordembeddings?representations of lexical items aspoints in a real vector space?have a long historyin natural language processing, going back at leastas far as work on latent semantic analysis (LSA)for information retrieval (Deerwester et al, 1990).While word embeddings can be constructed di-rectly from surface distributional statistics, as inLSA, more sophisticated tools for unsupervisedextraction of word representations have recentlygained popularity (Collobert et al, 2011; Mikolovet al, 2013a).
Semi-supervised and unsupervisedmodels for a variety of core NLP tasks, includ-ing named-entity recognition (Freitag, 2004), part-of-speech tagging (Sch?utze, 1995), and chunking(Turian et al, 2010) have been shown to benefitfrom the inclusion of word embeddings as fea-tures.
In the other direction, access to a syntac-tic parse has been shown to be useful for con-structing word embeddings for phrases composi-tionally (Hermann and Blunsom, 2013; Andreasand Ghahramani, 2013).
Dependency parsers haveseen gains from distributional statistics in the formof discrete word clusters (Koo et al, 2008), and re-cent work (Bansal et al, 2014) suggests that simi-lar gains can be derived from embeddings like theones used in this paper.It has been less clear how (and indeed whether)word embeddings in and of themselves are use-ful for constituency parsing.
There certainly existcompetitive parsers that internally represent lexi-cal items as real-valued vectors, such as the neuralnetwork-based parser of Henderson (2004), andeven parsers which use pre-trained word embed-dings to represent the lexicon, such as Socher etal.
(2013).
In these parsers, however, use of wordvectors is a structural choice, rather than an addedfeature, and it is difficult to disentangle whethervector-space lexicons are actually more powerfulthan their discrete analogs?perhaps the perfor-mance of neural network parsers comes entirelyfrom the model?s extra-lexical syntactic structure.In order to isolate the contribution from word em-beddings, it is useful to demonstrate improvementover a parser that already achieves state-of-the-artperformance without vector representations.The fundamental question we want to exploreis whether embeddings provide any informationbeyond what a conventional parser is able to in-duce from labeled parse trees.
It could be thatthe distinctions between lexical items that embed-dings capture are already modeled by parsers inother ways and therefore provide no further bene-fit.
In this paper, we investigate this question em-pirically, by isolating three potential mechanismsfor improvement from pre-trained word embed-8220 0.1 0.2 0.3 0.4 0.5 0.6?0.4?0.200.20.40.60.8athethisthatmostfeweacheveryFigure 1: Word representations of English de-terminers, projected onto their first two principalcomponents.
Embeddings from Collobert et al(2011).dings.
Our result is mostly negative.
With ex-tremely limited training data, parser extensions us-ing word embeddings give modest improvementsin accuracy (relative error reduction on the orderof 1.5%).
However, with reasonably-sized trainingcorpora, performance does not improve even whena wide variety of embedding methods, parser mod-ifications, and parameter settings are considered.The fact that word embedding features resultin nontrivial gains for discriminative dependencyparsing (Bansal et al, 2014), but do not appear tobe effective for constituency parsing, points to aninteresting structural difference between the twotasks.
We hypothesize that dependency parsersbenefit from the introduction of features (like clus-ters and embeddings) that provide syntactic ab-stractions; but that constituency parsers alreadyhave access to such abstractions in the form of su-pervised preterminal tags.2 Three possible benefits of wordembeddingsWe are interested in the question of whethera state-of-the-art discrete-variable constituencyparser can be improved with word embeddings,and, more precisely, what aspect (or aspects) ofthe parser can be altered to make effective use ofembeddings.It seems clear that word embeddings exhibitsome syntactic structure.
Consider Figure 1,which shows embeddings for a variety of Englishdeterminers, projected onto their first two princi-pal components.
We can see that the quantifierseach and every cluster together, as do few andmost.
These are precisely the kinds of distinc-tions between determiners that state-splitting inthe Berkeley parser has shown to be useful (Petrovand Klein, 2007), and existing work (Mikolov etal., 2013b) has observed that such regular em-bedding structure extends to many other parts ofspeech.
But we don?t know how prevalent orimportant such ?syntactic axes?
are in practice.Thus we have two questions: Are such groupings(learned on large data sets but from less syntacti-cally rich models) better than the ones the parserfinds on its own?
How much data is needed tolearn them without word embeddings?We consider three general hypotheses abouthow embeddings might interact with a parser:1.
Vocabulary expansion hypothesis: Wordembeddings are useful for handling out-of-vocabulary words, because they automati-cally ensure that unknown words are treatedthe same way as known words with similarrepresentations.
Example: the infrequently-occurring treebank tag UH dominates greet-ings (among other interjections).
Upon en-countering the unknown word hey, the parserassigns a low posterior probability of hav-ing been generated from UH.
But its distri-butional representation is very close to theknown word hello, and a model capable ofmapping hey to its neighbor should be able toassign the right tag.2.
Statistic sharing hypothesis: Word embed-dings are useful for handling in-vocabularywords, by making it possible to pool statisticsfor related words.
Example: individual firstnames are also rare in the treebank, but tendto cluster together in distributional represen-tations.
A parser which exploited this effectcould use this to acquire a robust model ofname behavior by sharing statistics from allfirst names together, preventing low countsfrom producing noisy models of names.3.
Embedding structure hypothesis: Thestructure of the space used for the embed-dings directly encodes syntactic informationin its coordinate axes.
Example: with theexception of a, the vertical axis in Figure 1823seems to group words by definiteness.
Wewould expect a feature corresponding to aword?s position along this axis to be a usefulfeature in a feature-based lexicon.Note that these hypotheses are not all mutuallyexclusive, and two or all of them might provide in-dependent gains.
Our first task is thus to design aset of orthogonal experiments which make it pos-sible to test each of the three hypotheses in isola-tion.
It is also possible that other mechanisms areat play that are not covered by these three hypothe-ses, but we consider these three to be likely centraleffects.3 Parser extensionsFor the experiments in this paper, we will usethe Berkeley parser (Petrov and Klein, 2007) andthe related Maryland parser (Huang and Harper,2011).
The Berkeley parser induces a latent, state-split PCFG in which each symbol V of the (ob-served) X-bar grammar is refined into a set ofmore specific symbols {V1, V2, .
.
.}
which cap-ture more detailed grammatical behavior.
Thisallows the parser to distinguish between wordswhich share the same tag but exhibit very differ-ent syntactic behavior?for example, between ar-ticles and demonstrative pronouns.
The Marylandparser builds on the state-splitting parser, replac-ing its basic word emission model with a feature-rich, log-linear representation of the lexicon.The choice of this parser family has two moti-vations.
First, these parsers are among the best inthe literature, with a test performance of 90.7 F1for the baseline Berkeley parser on the Wall StreetJournal corpus (compared to 90.4 for Socher et al(2013) and 90.1 for Henderson (2004)).
Second,and more importantly, the fact that they use nocontinuous state representations internally makesit easy to design experiments that isolate the con-tributions of word vectors, without worrying abouteffects from real-valued operators higher up in themodel.
We consider the following extensions:Vocabulary expansion ?
OOV modelTo evaluate the vocabulary expansion hypothe-sis, we introduce a simple but targeted out-of-vocabulary (OOV) model in which every unknownword is simply replaced by its nearest neighbor inthe training set.
For OOV words which are not inthe dictionary of embeddings, we back off to theunknown word model for the underlying parser.Statistic sharing ?
Lexicon pooling modelTo evaluate the statistic sharing hypothesis, wepropose a novel smoothing technique.
The Berke-ley lexicon stores, for each latent (tag, word) pair,the probability p(w|t) directly in a lookup ta-ble.
If we want to encourage similarly-embeddedwords to exhibit similar behavior in the generativemodel, we need to ensure that the are preferen-tially mapped onto the same latent preterminal tag.In order to do this, we replace this direct lookupwith a smoothed, kernelized lexicon, where:p(w|t) =1Z?w??t,w?e??||?(w)??(w?
)||2(1)with Z a normalizing constant to ensure that p(?|t)sums to one over the entire vocabulary.
?
(w) is thevector representation of the word w, ?t,ware per-basis weights, and ?
is an inverse radius parame-ter which determines the strength of the smooth-ing.
Each ?t,wis learned in the same way asits corresponding probability in the original parsermodel?during each M step of the training proce-dure, ?w,tis set to the expected number of timesthe word w appears under the refined tag t. Intu-itively, as ?
grows small groups of related wordswill be assigned increasingly similar probabilitiesof being generated from the same tag (in the limitwhere ?
= 0, Equation 1 is a uniform distribu-tion over the entire vocabulary).
As ?
grows largewords become more independent (and in the limitwhere ?
= ?, each summand in Equation 1 iszero except where w?= w, and we recover theoriginal direct-lookup model).There are computational concerns associatedwith this approach: the original scoring procedurefor a (word, tag) pair was a single (constant-time)lookup; here it might take time linear in the sizeof the vocabulary.
This causes parsing to becomeunacceptably slow, so an approximation is neces-sary.
Luckily, the exponential decay of the kernelensures that each word shares most of its weightwith a small number of close neighbors, and al-most none with words farther away.
To exploitthis, we pre-compute the k-nearest-neighbor graphof points in the embedding space, and take the sumin Equation 1 only over this set of nearest neigh-bors.
Empirically, taking k = 20 gives adequateperformance, and increasing it does not seem toalter the behavior of the parser.As in the OOV model, we also need to worryabout how to handle words for which we have no824vector representation.
In these cases, we simplytreat the words as if their vectors were so far awayfrom everything else they had no influence, andreport their weights as p(w|t) = ?w.
This ensuresthat our model continues to include the originalBerkeley parser model as a limiting case.Embedding structure ?
embedding featuresTo evaluate the embedding structure hypothesis,we take the Maryland featured parser, and replacethe set of lexical template features used by thatparser with a set of indicator features on a dis-cretized version of the embedding.
For each di-mension i, we create an indicator feature corre-sponding to the linearly-bucketed value of the fea-ture at that index.
In order to focus specificallyon the effect of word embeddings, we remove themorphological features from the parser, but retainindicators on the identity of each lexical item.The extensions we propose are certainly notthe only way to target the hypotheses describedabove, but they have the advantage of being min-imal and straightforwardly interpretable, and eachcan be reasonably expected to improve parser per-formance if its corresponding hypothesis is true.4 Experimental setupWe use the Maryland implementation of theBerkeley parser as our baseline for the kernel-smoothed lexicon, and the Maryland featuredparser as our baseline for the embedding-featuredlexicon.1For all experiments, we use 50-dimensional word embeddings.
Embeddings la-beled C&W are from Collobert et al (2011); em-beddings labeled CBOW are from Mikolov et al(2013a), trained with a context window of size 2.Experiments are conducted on the Wall StreetJournal portion of the English Penn Treebank.
Weprepare three training sets: the complete trainingset of 39,832 sentences from the treebank (sec-tions 2 through 21), a smaller training set, consist-ing of the first 3000 sentences, and an even smallerset of the first 300.Per-corpus-size settings of the parameter ?
areset by searching over several possible settings onthe development set.
For each training corpus sizewe also choose a different setting of the number ofsplitting iterations over which the Berkeley parseris run; for 300 sentences this is two splits, and for1Both downloaded from https://code.google.com/p/umd-featured-parser/Model 300 3000 FullBaseline 71.88 84.70 91.13OOV (C&W) 72.20 84.77 91.22OOV (CBOW) 72.20 84.78 91.22Pooling (C&W) 72.21 84.55 91.11Pooling (CBOW) 71.61 84.73 91.15Features (ident) 67.27 82.77 90.65Features (C&W) 70.32 83.78 91.08Features (CBOW) 69.87 84.46 90.86Table 1: Contributions from OOV, lexical poolingand featured models, for two kinds of embeddings(C&W and CBOW).
For both choices of embed-ding, the pooling and OOV models provide smallgains with very little training data, but no gainson the full training set.
The featured model neverachieves scores higher than the generative base-line.Model 300 3000 FullBaseline 72.02 84.09 90.70Pool + OOV (C&W) 72.43?84.36?90.11Table 2: Test set experiments with the best com-bination of models (based on development exper-iments).
Again, we observe small gains with re-stricted training sets but no gains on the full train-ing set.
Entries marked?are statistically signifi-cant (p < 0.05) under a paired bootstrap resam-pling test.3000 four splits.
This is necessary to avoid over-fitting on smaller training sets.
Consistent with theexisting literature, we stop at six splits when usingthe full training corpus.5 ResultsVarious model-specific experiments are shown inTable 1.
We begin by investigating the OOVmodel.
As can be seen, this model alone achievessmall gains over the baseline for a 300-word train-ing corpus, but these gains become statistically in-significant with more training data.
This behavioris almost completely insensitive to the choice ofembedding.Next we consider the lexicon pooling model.We began by searching over exponentially-spacedvalues of ?
to determine an optimal setting for825Experiment WSJ ?
Brown FrenchBaseline 86.36 74.84Pool + OOV 86.42 75.18Table 3: Experiments for other corpora, using thesame combined model (lexicon pooling and OOV)as in Table 2.
Again, we observe no significantgains over the baseline.each training set size; as expected, for small set-tings of ?
(corresponding to aggressive smooth-ing) performance decreased; as we increased theparameter, performance increased slightly beforetapering off to baseline parser performance.
Thefirst block in Table 1 shows the best settings of ?for each corpus size; as can be seen, this also givesa small improvement on the 300-sentence trainingcorpus, but no discernible once the system has ac-cess to a few thousand labeled sentences.Last we consider a model with a featured lex-icon, as described in Huang and Harper (2011).A baseline featured model (?ident?)
contains onlyindicator features on word identity (and performsconsiderably worse than its generative counter-part on small data sets).
As described above, thefull featured model adds indicator features on thebucketed value of each dimension of the word em-bedding.
Here, the trend observed in the other twomodels is even more prominent?embedding fea-tures lead to improvements over the featured base-line, but in no case outperform the standard base-line with a generative lexicon.We take the best-performing combination of allof these models (based on development experi-ments, a combination of the lexical pooling modelwith ?
= 0.3, and OOV, both using C&W wordembeddings), and evaluate this on the WSJ testset (Table 2).
We observe very small (but statis-tically significant) gains with 300 and 3000 trainsentences, but a decrease in performance on thefull corpus.To investigate the possibility that improvementsfrom embeddings are exceptionally difficult toachieve on the Wall Street Journal corpus, or onEnglish generally, we perform (1) a domain adap-tation experiment, in which we use the OOV andlexicon pooling models to train on WSJ and teston the first 4000 sentences of the Brown corpus(the ?WSJ ?
Brown?
column in Table 3), and (2)a multilingual experiment, in which we train andtest on the French treebank (the ?French?
column).Apparent gains from the OOV and lexicon poolingmodels remain so small as to be statistically indis-tinguishable.6 ConclusionWith the goal of exploring how much useful syn-tactic information is provided by unsupervisedword embeddings, we have presented three vari-ations on a state-of-the-art parsing model, withextensions to the out-of-vocabulary model, lexi-con, and feature set.
Evaluation of these modi-fied parsers revealed modest gains on extremelysmall training sets, which quickly vanish as train-ing set size increases.
Thus, at least restricted tophenomena which can be explained by the exper-iments described here, our results are consistentwith two claims: (1) unsupervised word embed-dings do contain some syntactically useful infor-mation, but (2) this information is redundant withwhat the model is able to determine for itself fromonly a small amount of labeled training data.It is important to emphasize that these resultsdo not argue against the use of continuous repre-sentations in a parser?s state space, nor argue moregenerally that constituency parsers cannot possi-bly benefit from word embeddings.
However, thefailure to uncover gains when searching across avariety of possible mechanisms for improvement,training procedures for embeddings, hyperparam-eter settings, tasks, and resource scenarios sug-gests that these gains (if they do exist) are ex-tremely sensitive to these training conditions, andnot nearly as accessible as they seem to be in de-pendency parsers.
Indeed, our results suggest ahypothesis that word embeddings are useful fordependency parsing (and perhaps other tasks) be-cause they provide a level of syntactic abstrac-tion which is explicitly annotated in constituencyparses.
We leave explicit investigation of this hy-pothesis for future work.AcknowledgmentsThis work was partially supported by BBN underDARPA contract HR0011-12-C-0014.
The firstauthor is supported by a National Science Foun-dation Graduate Research Fellowship.826ReferencesJacob Andreas and Zoubin Ghahramani.
2013.
A gen-erative model of vector space semantics.
In Pro-ceedings of the ACL Workshop on Continuous Vec-tor Space Models and their Compositionality, Sofia,Bulgaria.Mohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-dauer, GeorgeW.
Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Dayne Freitag.
2004.
Trained named entity recog-nition using distributional clusters.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.James Henderson.
2004.
Discriminative training of aneural network statistical parser.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, page 95.
Association for Com-putational Linguistics.Karl Moritz Hermann and Phil Blunsom.
2013.
Therole of syntax in vector space models of compo-sitional semantics.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics, pages 894?904, Sofia, Bulgaria, August.Zhongqiang Huang and Mary P. Harper.
2011.Feature-rich log-linear lexical model for latent vari-able pcfg grammars.
In Proceedings of the Interna-tional Joint Conference on Natural Language Pro-cessing, pages 219?227.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of the Annual Meeting of the Asso-ciation for Computational Linguistics, pages 595?603.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013a.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 746?751.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics.
Assocation for ComputationalLinguistics.Hinrich Sch?utze.
1995.
Distributional part-of-speechtagging.
In Proceedings of the European Associa-tion for Computational Linguistics, pages 141?148.Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013.
Parsing with composi-tional vector grammars.
In Proceedings of the An-nual Meeting of the Association for ComputationalLinguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Proceedings ofthe Annual Meeting of the Association for Compu-tational Linguistics.827
