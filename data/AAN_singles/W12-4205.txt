Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39?48,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsUsing Parallel Features in Parsing of Machine-Translated Sentences forCorrection of Grammatical Errors ?Rudolf Rosa, Ondr?ej Dus?ek, David Marec?ek, and Martin PopelCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied Linguistics{rosa,odusek,marecek,popel}@ufal.mff.cuni.czAbstractIn this paper, we present two dependencyparser training methods appropriate for pars-ing outputs of statistical machine transla-tion (SMT), which pose problems to standardparsers due to their frequent ungrammatical-ity.
We adapt the MST parser by exploitingadditional features from the source language,and by introducing artificial grammatical er-rors in the parser training data, so that thetraining sentences resemble SMT output.We evaluate the modified parser on DEP-FIX, a system that improves English-CzechSMT outputs using automatic rule-based cor-rections of grammatical mistakes which re-quires parsed SMT output sentences as its in-put.
Both parser modifications led to im-provements in BLEU score; their combina-tion was evaluated manually, showing a sta-tistically significant improvement of the trans-lation quality.1 IntroductionThe machine translation (MT) quality is on a steadyrise, with mostly statistical systems (SMT) dominat-ing the area (Callison-Burch et al, 2010; Callison-Burch et al, 2011).
Most MT systems do not employstructural linguistic knowledge and even the state-of-the-art MT solutions are unable to avoid makingserious grammatical errors in the output, which of-ten leads to unintelligibility or to a risk of misinter-pretations of the text by a reader.
?This research has been supported by the EU SeventhFramework Programme under grant agreement n?
247762(Faust), and by the grants GAUK116310 and GA201/09/H057.This problem is particularly apparent in target lan-guages with rich morphological inflection, such asCzech.
As Czech often conveys the relations be-tween individual words using morphological agree-ment instead of word order, together with the wordorder itself being relatively free, choosing the cor-rect inflection becomes crucial.Since the output of phrase-based SMT shows fre-quent inflection errors (even in adjacent words) dueto each word belonging to a different phrase, apossible way to address the grammaticality prob-lem is a combination of statistical and structural ap-proach, such as SMT output post-editing (Stymneand Ahrenberg, 2010; Marec?ek et al, 2011).In this paper, we focus on improving SMT outputparsing quality, as rule-based post-editing systemsrely heavily on the quality of SMT output analy-sis.
Parsers trained on gold standard parse trees of-ten fail to produce the expected result when appliedto SMT output with grammatical errors.
This ispartly caused by the fact that when parsing highly in-flected free word-order languages the parsers have torely on morphological agreement, which, as statedabove, is often erroneous in SMT output.Training a parser specifically by creating a man-ually annotated treebank of MT systems?
outputswould be very expensive, and the application of suchtreebank to other MT systems than the ones usedfor its generation would be problematic.
We addressthis issue by two methods of increasing the qualityof SMT output parsing:?
a different application of previous works onbitext parsing ?
exploiting additional featuresfrom the source language (Section 3), and39?
introducing artificial grammatical errors in thetarget language parser training data, so that thesentences resemble the SMT output in someways (Section 4).
This technique is, to ourknowledge, novel with regards to its applica-tion to SMT and the statistical error model.We test these two techniques on English-CzechMT outputs using our own reimplementation of theMST parser (McDonald et al, 2005) named RUR1parser.
and evaluate their contribution to the SMTpost-editing quality of the DEPFIX system (Marec?eket al, 2011), which we outline in Section 5.
Wedescribe the experiments carried out and present themost important results in Section 6.
Section 7 thenconcludes the paper and indicates more possibilitiesof further improvements.2 Related WorkOur approach to parsing with parallel features issimilar to various works which seek to improve theparsing accuracy on parallel texts (?bitexts?)
by us-ing information from both languages.
Huang etal.
(2009) employ ?bilingual constraints?
in shift-reduce parsing to disambiguate difficult syntac-tic constructions and resolve shift-reduce conflicts.Chen et al (2010) use similar subtree constraints toimprove parser accuracy in a dependency scenario.Chen et al (2011) then improve the method by ob-taining a training parallel treebank via SMT.
In re-cent work, Haulrich (2012) experiments with a setupvery similar to ours: adding alignment-projectedfeatures to an originally monolingual parser.However, the main aim of all these works is to im-prove the parsing accuracy on correct parallel texts,i.e.
human-translated.
This paper applies similarmethods, but with a different objective in mind ?
in-creasing the ability of the parser to process ungram-matical SMT output sentences and, ultimately, im-prove rule-based SMT post-editing.Xiong et al (2010) use SMT parsing in translationquality assessment, providing syntactic features to aclassifier detecting erroneous words in SMT output,yet they do not concentrate on improving parsing ac-curacy ?
they employ a link grammar parser, which1The abbreviation ?RUR?
parser stands for ?Rudolph?s Uni-versal Robust?
parser.is robust, but not tuned specifically to process un-grammatical input.There is also another related direction of researchin parsing of parallel texts, which is targeted on pars-ing under-resourced languages, e.g.
the works byHwa et al (2005), Zeman and Resnik (2008), andMcDonald et al (2011).
They address the fact thatparsers for the language of interest are of low qual-ity or even non-existent, whereas there are high-quality parsers for the other language.
They ex-ploit common properties of both languages and de-lexicalization.
Zhao et al (2009) uses informationfrom word-by-word translated treebank to obtain ad-ditional training data and boost parser accuracy.This is different from our situation, as there ex-ist high performance parsers for Czech (Buchholzand Marsi, 2006; Nivre et al, 2007; Hajic?
et al,2009).
Boosting accuracy on correct sentences isnot our primary goal and we do not intend to re-place the Czech parser by an English parser; instead,we aim to increase the robustness of an already ex-isting Czech parser by adding knowledge from thecorresponding English source, parsed by an Englishparser.Other works in bilingual parsing aim to parse theparallel sentences directly using a grammar formal-ism fit for this purpose, such as Inversion Trans-duction Grammars (ITG) (Wu, 1997).
Burkett etal.
(2010) further include ITG parsing with word-alignment in a joint scenario.
We concentrate hereon using dependency parsers because of tools andtraining data availability for the examined languagepair.Regarding treebank adaptation for parser robust-ness, Foster et al (2008) introduce various kinds ofartificial errors into the training data to make the fi-nal parser less sensitive to grammar errors.
How-ever, their approach concentrates on mistakes madeby humans (such as misspellings, word repetition oromission etc.)
and the error models used are hand-crafted.
Our work focuses on morphology errors of-ten encountered in SMT output and introduces sta-tistical error modelling.3 Parsing with Parallel FeaturesThis section describes our SMT output parsing setupwith features from analyzed source sentences.
We40explain our motivation for the inclusion of parallelfeatures in Section 3.1, then provide an account ofthe parsers used (including our RUR parser) in Sec-tion 3.2, and finally list all the monolingual and par-allel features included in the parser training (in Sec-tions 3.3 and 3.4, respectively).3.1 MotivationAn advantage of SMT output parsing over generaldependency parsing is that one can also make use ofsource ?
English sentences in our case.
Moreover,although SMT output is often in many ways ungram-matical, source is usually grammatical and thereforeeasier to process (in our case especially to tag andparse).
This was already noticed in Marec?ek et al(2011), who use the analysis of source sentence toprovide additional information for the DEPFIX rules,claiming it to be more reliable than the analysis ofSMT output sentence.We have carried this idea further by having de-vised a simple way of making use of this informationin parsing of the SMT output sentences: We parsethe source sentence first and include features com-puted over the parsed source sentence in the set offeatures used for parsing SMT output.
We first alignthe source and SMT output sentences on the wordlevel and then use alignment-wise local features ?i.e.
for each SMT output word, we add features com-puted over its aligned source word, if applicable (cf.Section 3.4 for a listing).3.2 Parsers UsedWe have reimplemented the MST parser (McDonaldet al, 2005) in order to provide for a simple insertionof the parallel features into the models.We also used the original implementation of theMST parser by McDonald et al (2006) for com-parison in our experiments.
To distinguish the twovariants used, we denote the original MST parseras MCD parser,2 and the new reimplementation asRUR parser.We trained RUR parser in a first-order non-projective setting with single-best MIRA.
Depen-dency labels are assigned in a second stage by a2MCD uses k-best MIRA, does first- and second-orderparsing, both projectively and non-projectively, and can beobtained from http://sourceforge.net/projects/mstparser.MIRA-based labeler, which has been implementedaccording to McDonald (2006) and Gimpel and Co-hen (2007).We used the Prague Czech-English DependencyTreebank3 (PCEDT) 2.0 (Bojar et al, 2012) as thetraining data for RUR parser ?
a parallel treebankcreated from the Penn Treebank (Marcus et al,1993) and its translation into Czech by human trans-lators.
The dependency trees on the English sidewere converted from the manually annotated phrase-structure trees in Penn Treebank, the Czech treeswere created automatically using MCD.
Words ofthe Czech and English sentences were aligned byGIZA++ (Och and Ney, 2003).We apply RUR parser only for SMT output pars-ing; for source parsing, we use MCD parser trainedon the English CoNLL 2007 data (Nivre et al,2007), as the performance of this parser is sufficientfor this task.3.3 Monolingual FeaturesThe set of monolingual features used in RUR parserfollows those described by McDonald et al (2005).For parsing, we use the features described below.The individual features are computed for both theparent node and the child node of an edge and con-joined in various ways.
The coarse morphologicaltag and lemma are provided by the Morc?e tagger(Spoustova?
et al, 2007).?
coarse morphological tag ?
Czech two-lettercoarse morphological tag, as described in(Collins et al, 1999),4?
lemma ?
morphological lemma,?
context features: preceding coarse morpholog-ical tag, following coarse morphological tag?
coarse morphological tag of a neighboringnode,?
coarse morphological tags in between ?
bag ofcoarse morphological tags of nodes positionedbetween the parent node and the child node,3http://ufal.mff.cuni.cz/pcedt4The first letter is the main POS (12 possible values), thesecond letter is either the morphological case field if the mainPOS displays case (i.e.
for nouns, adjectives, pronouns, numer-als and prepositions; 7 possible values), or the detailed POS ifit does not (22 possible values).41?
distance ?
signed bucketed distance of the par-ent and the child node in the sentence (in # ofwords), using buckets 1, 2, 3, 4, 5 and 11.To assign dependency labels, we use the sameset as described above, plus the following features(called ?non-local?
by McDonald (2006)), whichmake use of the knowledge of the tree structure.?
is first child, is last child ?
a boolean indicatingwhether the node appears in the sentence as thefirst/last one among all the child nodes of itsparent node,?
child number ?
the number of syntactic chil-dren of the current node.3.4 Parallel FeaturesFigure 1: Example sentence for parallel features illustra-tion (see Table 1).In RUR parser we use three types of parallel fea-tures, computed for the parent and child node of anedge, which make use of the source English nodesaligned to the parent and child node.?
aligned tag: morphological tag following thePenn Treebank Tagset (Marcus et al, 1993) ofthe English node aligned to the Czech nodeFeature Feature value onparent node child nodeword form jel Martinaligned tag VBD NNPaligned dep.
label Pred Sbaligned edge existence trueword form jel autemaligned tag VBD NNaligned dep.
label Pred Advaligned edge existence falseword form do zahranic??
?aligned tag ?
RBaligned dep.
label ?
Advaligned edge existence ?word form #root# .aligned tag #root# .aligned dep.
label AuxS AuxKaligned edge existence trueTable 1: Parallel features for several edges in Figure 1.?
aligned dependency label: dependency label ofthe English node aligned to the Czech node inquestion, according to the PCEDT 2.0 label set(Bojar et al, 2012)?
aligned edge existence: a boolean indicatingwhether the English node aligned to the Czechparent node is also the parent of the Englishnode aligned to the Czech child nodeThe parallel features are conjoined with themonolingual coarse morphological tag and lemmafeatures in various ways.If there is no source node aligned to the parentor child node, the respective feature cannot be com-puted and is skipped.An example of a pair of parallel sentences is givenin Figure 1 with the corresponding values of parallelfeatures for several edges in Table 1.4 Worsening Treebanks to Simulate Someof the SMT Frequent ErrorsAddressing the issue of great differences betweenthe gold standard parser training data and the actualanalysis input (SMT output), we introduced artificialinconsistencies into the training treebanks, in orderto make the parsers more robust in the face of gram-mar errors made by SMT systems.
We have concen-42trated solely on modeling incorrect word flection,i.e.
the dependency trees retained their original cor-rect structures and word lemmas remained fixed, butthe individual inflected word forms have been modi-fied according to an error model trained on real SMToutput.
We simulate thus, with respect to morphol-ogy, a treebank of parsed MT output sentences.In Section 4.1 we describe the steps we take toprepare the worsened parser training data.
Sec-tion 4.2 contains a description of our monolingualgreedy alignment tool which is needed during theprocess to map SMT output to reference transla-tions.4.1 Creating the Worsened Parser TrainingDataThe whole process of treebank worsening consistsof five steps:1.
We translated the English side of PCEDT5 toCzech using SMT (we chose the Moses sys-tem (Koehn et al, 2007) for our experiments)and tagged the resulting translations using theMorc?e tagger (Spoustova?
et al, 2007).2.
We aligned the Czech side of PCEDT, nowserving as a reference translation, to the SMToutput using our Monolingual Greedy Aligner(see Section 4.2).3.
Collecting the counts of individual errors, weestimated the Maximum Likelihood probabili-ties of changing a correct fine-grained morpho-logical tag (of a word from the reference) intoa possibly incorrect fine-grained morphologicaltag of the aligned word (from the SMT output).4.
The tags on the Czech side of PCEDT wererandomly sampled according to the estimated?fine-grained morphological tag error model?.In those positions where fine-grained morpho-logical tags were changed, new word formswere generated using the Czech morphologicalgenerator by Hajic?
(2004).65This approach is not conditioned by availability of paralleltreebanks.
Alternatively, we might translate any text for whichreference translations are at hand.
The model learned in thethird step would then be applied (in the fourth step) to a differenttext for which parse trees are available.6According to the ?fine-grained morphological tag errorWe use the resulting ?worsened?
treebank to trainour parser described in Section 3.2.4.2 The Monolingual Greedy AlignerOur monolingual alignment tool, used in treebankworsening to tie reference translations to MT out-put (see Section 4.1), scores all possible alignmentlinks and then greedily chooses the currently highestscoring one, creating the respective alignment linkfrom word A (in the reference) to word B (in theSMT output) and deleting all scores of links from Aor to B, so that one-to-one alignments are enforced.The process is terminated when no links with a scorehigher than a given threshold are available; somewords may thus remain unaligned.The score is computed as a linear combination ofthe following four features:?
word form (or lemma if available) similar-ity based on Jaro-Winkler distance (Winkler,1990),?
fine-grained morphological tag similarity,?
similarity of the relative position in the sen-tence,?
and an indication whether the word following(or preceding) A was already aligned to theword following (or preceding) B.Unlike bilingual word aligners, this tool needs notraining except for setting weights of the four fea-tures and the threshold.75 The DEPFIX SystemThe DEPFIX system (Marec?ek et al, 2011) appliesvarious rule-based corrections to Czech-EnglishSMT output sentences, especially of morphologicalagreement.
It also employs the parsed source sen-tences, which must be provided on the input togetherwith the SMT output sentences.The corrections follow the rules of Czech gram-mar, e.g.
requiring that the clause subject be in themodel?, about 20% of fine-grained morphological tags werechanged.
In 4% of cases, no word form existed for the newfine-grained morphological tag and thus it was not changed.7The threshold and weights were set manually using just tensentence pairs.
The resulting alignment quality was found suf-ficient, so no additional weights tuning was performed.43nominative case or enforcing subject-predicate andnoun-attribute agreements in morphological gender,number and case, where applicable.
Morphologicalproperties found violating the rules are corrected andthe corresponding word forms regenerated.The source sentence parse, word-aligned to theSMT output using GIZA++ (Och and Ney, 2003),is used as a source of morpho-syntactic informationfor the correction rules.
An example of a correctionrule application is given in Figure 2.SomepeoplecamelaterAtrSbPredAdvplpl.AuxKp?i?liPredplN?kte?
?lid?p?i?elpozd?jiAtrSbPredAdvsg, mpl.AuxKFigure 2: Example of fixing subject-predicate agreement.The Czech word pr?is?el [he came] has a wrong morpho-logical number and gender.
Adapted from Marec?ek et al(2011).The system is implemented within theTectoMT/Treex NLP framework (Popel andZ?abokrtsky?, 2010).
Marec?ek et al (2011) feed theDEPFIX system with analyses by the MCD parsertrained on gold-standard treebanks for parsing ofEnglish source sentences as well as Czech SMToutput.6 Experiments and ResultsWe evaluate RUR parser indirectly by using it in theDEPFIX system and measuring the performance ofthe whole system.
This approach has been choseninstead of direct evaluation of the SMT output parsetrees, as the task of finding a correct parse tree ofa possibly grammatically incorrect sentence is notwell defined and considerably difficult to do.We used WMT10, WMT11 and WMT12 En-glish to Czech translation test sets, newssyscomb-test2010, newssyscombtest2011 and news-test2012,8 (denoted as WMT10, WMT11 and8http://www.statmt.org/wmt10,WMT12) for the automatic evaluation.
The data setsinclude the source (English) text, its reference trans-lation and translations produced by several MT sys-tems.
We used the outputs of three SMT systems:GOOGLE,9 UEDIN (Koehn et al, 2007) and BOJAR(Bojar and Kos, 2010).For the manual evaluation, two sets of 1000 ran-domly selected sentences from WMT11 and fromWMT12 translated by GOOGLE were used.6.1 Automatic EvaluationTable 2 shows BLEU scores (Papineni et al, 2002)for the following setups of DEPFIX:?
SMT output: output of an SMT system withoutapplying DEPFIX?
MCD: parsing with MCD?
RUR: parsing with RUR (Section 3.2)?
RUR+PARA: parsing with RUR using parallelfeatures (Section 3.4)?
RUR+WORS: parsing with RUR trained onworsened treebank (Section 4)?
RUR+WORS+PARA: parsing with RURtrained on worsened treebank and usingparallel featuresIt can be seen that both of the proposed ways ofadapting the parser to parsing of SMT output of-ten lead to higher BLEU scores of translations post-processed by DEPFIX, which suggests that they bothimprove the parsing accuracy.We have computed 95% confidence intervalson 1000 bootstrap samples, which showed thatthe BLEU score of RUR+WORS+PARA was sig-nificantly higher than that of MCD and RURparser in 4 and 3 cases, respectively (resultswhere RUR+WORS+PARA achieved a significantlyhigher score are marked with ?*?).
On the otherhand, the score of neither RUR+WORS+PARA norRUR+WORS and RUR+PARA was ever signifi-cantly lower than the score of MCD or RUR parser.This leads us to believe that the two proposed meth-ods are able to produce slightly better SMT outputparsing results.http://www.statmt.org/wmt11,http://www.statmt.org/wmt129http://translate.google.com44Test set WMT10 WMT11 WMT12SMT system BOJAR GOOGLE UEDIN BOJAR GOOGLE UEDIN BOJAR GOOGLE UEDINSMT output *15.85 *16.57 *15.91 *16.88 *20.26 *17.80 14.36 16.25 *15.54MCD 16.09 16.95 *16.35 *17.02 20.45 *18.12 14.35 16.32 *15.65RUR 16.08 *16.85 *16.29 17.03 20.42 *18.09 14.37 16.31 15.66RUR+PARA 16.13 *16.90 *16.35 17.05 20.47 18.19 14.35 16.31 15.72RUR+WORS 16.12 16.96 *16.45 17.06 20.53 18.21 14.40 16.31 15.71RUR+WORS+PARA 16.13 17.03 16.54 17.12 20.53 18.25 14.39 16.30 15.74Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE andUEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups andprocessed by DEPFIX.
The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scoresmarked with ?*?
on the same data.6.2 Manual EvaluationPerformance of RUR+WORS+PARA setup was man-ually evaluated by doing a pairwise comparison withother setups ?
SMT output, MCD and RUR parser.The evaluation was performed on both the WMT11(Table 4) and WMT12 (Table 5) test set.
1000 sen-tences from the output of the GOOGLE system wererandomly selected and processed by DEPFIX, usingthe aforementioned SMT output parsers.
The anno-tators then compared the translation quality of theindividual variants in differing sentences, selectingthe better variant from a pair or declaring two vari-ants ?same quality?
(indefinite).
They were also pro-vided with the source sentence and a reference trans-lation.
The evaluation was done as a blind test, withthe sentences randomly shuffled.The WMT11 test set was evaluated by two inde-pendent annotators.
(The WMT12 test set was eval-uated by one annotator only.)
The inter-annotatoragreement and Cohen?s kappa coefficient (Cohenand others, 1960), shown in Table 3, were computedboth including all annotations (?with indefs?
), anddisregarding sentences where at least one of the an-notators marked the difference as indefinite (?with-out indefs?)
?
we believe a disagreement in choos-ing the better translation to be more severe than adisagreement in deciding whether the difference inquality of the translations allows to mark one as be-ing better.For both of the test sets, RUR+WORS+PARA sig-nificantly outperforms both MCD and RUR base-line, confirming that a combination of the proposedmodifications of the parser lead to its better perfor-mance.
Statistical significance of the results wasRUR+WORS+PARA with indefs without indefscompared to IAA Kappa IAA KappaSMT output 77% 0.54 92% 0.74MCD 79% 0.66 95% 0.90RUR 75% 0.60 94% 0.85Table 3: Inter-annotator agreement on WMT11 data settranslated by GOOGLEconfirmed by a one-sided pairwise t-test, with thefollowing differences ranking: RUR+WORS+PARAbetter = 1, baseline better = -1, indefinite = 0.6.3 Inspection of Parser Modification BenefitsFor a better understanding of the benefits of usingour modified parser, we inspected a small number ofparse trees, produced by RUR+WORS+PARA, andcompared them to those produced by RUR.In many cases, the changes introduced byRUR+WORS+PARA were clearly positive.
Weprovide two representative examples below.Subject IdentificationCzech grammar requires the subject to be in nom-inative case, but this constraint is often violated inSMT output and a parser typically fails to identifythe subject correctly in such situations.
By wors-ening the training data, we make the parser more ro-bust in this respect, as the worsening often switchesthe case of the subject; by including parallel fea-tures, especially the aligned dependency label fea-ture, RUR+WORS+PARA parser can often identifythe subject as the node aligned to the source subject.45Out of the differing sentencesAnnotator Baseline Differing sentences RUR+WORS+PARA better baseline better indefinitecount percent count percent count percentSMT output 422 301 71% 79 19% 42 10%A MCD 211 120 57% 65 31% 26 12%RUR 217 123 57% 64 29% 30 14%SMT output 422 284 67% 69 16% 69 16%B MCD 211 107 51% 56 26% 48 23%RUR 217 118 54% 53 24% 46 21%Table 4: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT11 data settranslated by GOOGLE, evaluated by two independent annotators.Out of the differing sentencesAnnotator Baseline Differing sentences RUR+WORS+PARA better baseline better indefinitecount percent count percent count percentSMT output 420 270 64% 88 21% 62 15%A MCD 188 86 45% 64 34% 38 20%RUR 187 96 51% 57 30% 34 18%Table 5: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT12 data settranslated by GOOGLE.Governing Noun IdentificationA parser for Czech typically relies on morpho-logical agreement between an adjective and its gov-erning noun (in morphological number, gender andcase), which is often violated in SMT output.
Again,RUR+WORS+PARA is more robust in this respect,aligned edge existence now being the crucial featurefor the correct identification of this relation.7 Conclusions and Future WorkWe have studied two methods of improving the pars-ing quality of Machine Translation outputs by pro-viding additional information to the parser.In Section 3, we propose a method of integrat-ing additional information known at runtime, i.e.the knowledge of the source sentence (source), fromwhich the sentence being parsed (SMT output) hasbeen translated.
This knowledge is provided byextending the parser feature set with new featuresfrom the source sentence, projected through word-alignment.In Section 4, we introduce a method of utilizingadditional information known in the training phase,namely the knowledge of the ways in which SMToutput differs from correct sentences.
We providethis knowledge to the parser by adjusting its trainingdata to model some of the errors frequently encoun-tered in SMT output, i.e.
incorrect inflection forms.We have evaluated the usefulness of these twomethods by integrating them into the DEPFIX rule-based MT output post-processing system (Marec?eket al, 2011), as MT output parsing is crucial for theoperation of this system.
When used with our im-proved parsing, the DEPFIX system showed betterperformance both in automatic and manual evalua-tion on outputs of several, including state-of-the-art,MT systems.We believe that the proposed methods of improv-ing MT output parsing can be extended beyond theircurrent state.
The parallel features used in our setupare very few and very simple; it thus remains tobe examined whether more elaborate features couldhelp utilize the additional information contained inthe source sentence to a greater extent.
Modelingother types of SMT output inconsistencies in parsertraining data is another possible step.We also believe that the methods could be adaptedfor use in other applications, e.g.
automatic classifi-cation of translation errors, confidence estimation ormultilingual question answering.46ReferencesOndr?ej Bojar and Kamil Kos.
2010.
2010 Failures inEnglish-Czech Phrase-Based MT.
In Proceedings ofthe Joint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 60?66, Uppsala, Swe-den, July.
Association for Computational Linguistics.Ondr?ej Bojar, Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?,Petr Sgall, Silvie Cinkova?, Eva Fuc??
?kova?, MarieMikulova?, Petr Pajas, Jan Popelka, Jir???
Semecky?,Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?kaUres?ova?, and Zdene?k Z?abokrtsky?.
2012.
Announc-ing Prague Czech-English Dependency Treebank 2.0.In Proceedings of LREC 2012, Istanbul, Turkey, May.ELRA, European Language Resources Association.In print.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, pages 149?164.Association for Computational Linguistics.David Burkett, John Blitzer, and Dan Klein.
2010.Joint parsing and alignment with weakly synchronizedgrammars.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 127?135.
Association for Computational Lin-guistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Uppsala, Sweden, July.
Association forComputational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion, pages 22?64, Edinburgh, Scotland, July.
Associ-ation for Computational Linguistics.Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.2010.
Bitext dependency parsing with bilingual sub-tree constraints.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 21?29.
Association for Computational Lin-guistics.Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshi-masa Tsuruoka, Yujie Zhang, Yiou Wang, KentaroTorisawa, and Haizhou Li.
2011.
SMT helps bitextdependency parsing.
In EMNLP, pages 73?83.
ACL.Jacob Cohen et al 1960.
A coefficient of agreement fornominal scales.
Educational and psychological mea-surement, 20(1):37?46.Michael Collins, Lance Ramshaw, Jan Hajic?, andChristoph Tillmann.
1999.
A statistical parser forCzech.
In Proceedings of the 37th annual meetingof the Association for Computational Linguistics onComputational Linguistics, ACL ?99, pages 505?512,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Jennifer Foster, Joachim Wagner, and Josef Van Gen-abith.
2008.
Adapting a WSJ-trained parser to gram-matically noisy text.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics on Human Language Technologies: ShortPapers, pages 221?224.
Association for Computa-tional Linguistics.Kevin Gimpel and Shay Cohen.
2007.
Discriminativeonline algorithms for sequence labeling- a comparativestudy.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, et al 2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?18.
Associa-tion for Computational Linguistics.Jan Hajic?.
2004.
Disambiguation of rich inflection: com-putational morphology of Czech.
Karolinum.Martin Haulrich.
2012.
Data-Driven Bitext DependencyParsing and Alignment.
Ph.D. thesis.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing:Volume 3-Volume 3, pages 1222?1231.
Association forComputational Linguistics.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Nat.
Lang.
Eng., 11:311?325, September.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In ACL2007, Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics CompanionVolume Proceedings of the Demo and Poster Sessions,pages 177?180, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.47Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
Comput.
Lin-guist., 19:313?330, June.David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, andOndr?ej Bojar.
2011.
Two-step translation with gram-matical post-processing.
In Chris Callison-Burch,Philipp Koehn, Christof Monz, and Omar Zaidan, edi-tors, Proceedings of the Sixth Workshop on StatisticalMachine Translation, pages 426?432, Edinburgh, UK.Association for Computational Linguistics.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In HLT ?05: Proceed-ings of the conference on Human Language Technol-ogy and Empirical Methods in Natural Language Pro-cessing, pages 523?530, Vancouver, British Columbia,Canada.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of theTenth Conference on Computational Natural Lan-guage Learning, CoNLL-X ?06, pages 216?220,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages62?72.
Association for Computational Linguistics.Ryan McDonald.
2006.
Discriminative learning andspanning tree algorithms for dependency parsing.Ph.D.
thesis, Philadelphia, PA, USA.
AAI3225503.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLL 2007Shared Task.
Joint Conf.
on Empirical Methods in Nat-ural Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), June.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In ACL 2002, Proceed-ings of the 40th Annual Meeting of the Association forComputational Linguistics, pages 311?318, Philadel-phia, Pennsylvania.Martin Popel and Zdene?k Z?abokrtsky?.
2010.
TectoMT:modular NLP framework.
In Proceedings of the 7thinternational conference on Advances in natural lan-guage processing, IceTAL?10, pages 293?304, Berlin,Heidelberg.
Springer-Verlag.Drahom?
?ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-bec, and Pavel Kve?ton?.
2007.
The best of two worlds:Cooperation of statistical and rule-based taggers forCzech.
In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL 2007,pages 67?74, Praha.Sara Stymne and Lars Ahrenberg.
2010.
Using a gram-mar checker for evaluation and postprocessing of sta-tistical machine translation.
In Proceedings of LREC,pages 2175?2181.William E. Winkler.
1990.
String comparator met-rics and enhanced decision rules in the Fellegi-Suntermodel of record linkage.
In Proceedings of the Sectionon Survey Research Methods (American Statistical As-sociation), pages 354?359.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Deyi Xiong, Min Zhang, and Haizhou Li.
2010.
Er-ror detection for statistical machine translation usinglinguistic features.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 604?611.
Association for ComputationalLinguistics.Daniel Zeman and Philip Resnik.
2008.
Cross-languageparser adaptation between related languages.
NLP forLess Privileged Languages, page 35.Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.2009.
Cross language dependency parsing using abilingual lexicon.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 1-Volume 1, pages55?63.
Association for Computational Linguistics.48
