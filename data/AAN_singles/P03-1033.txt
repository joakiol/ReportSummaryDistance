Flexible Guidance Generation usingUser Model in Spoken Dialogue SystemsKazunori Komatani Shinichi Ueno Tatsuya Kawahara Hiroshi G. OkunoGraduate School of InformaticsKyoto UniversityYoshida-Hommachi, Sakyo, Kyoto 606-8501, Japanfkomatani,ueno,kawahara,okunog@kuis.kyoto-u.ac.jpAbstractWe address appropriate user modeling inorder to generate cooperative responses toeach user in spoken dialogue systems.
Un-like previous studies that focus on user?sknowledge or typical kinds of users, theuser model we propose is more compre-hensive.
Specifically, we set up three di-mensions of user models: skill level tothe system, knowledge level on the tar-get domain and the degree of hastiness.Moreover, the models are automaticallyderived by decision tree learning usingreal dialogue data collected by the sys-tem.
We obtained reasonable classifica-tion accuracy for all dimensions.
Dia-logue strategies based on the user model-ing are implemented in Kyoto city bus in-formation system that has been developedat our laboratory.
Experimental evalua-tion shows that the cooperative responsesadaptive to individual users serve as goodguidance for novice users without increas-ing the dialogue duration for skilled users.1 IntroductionA spoken dialogue system is one of the promisingapplications of the speech recognition and naturallanguage understanding technologies.
A typical taskof spoken dialogue systems is database retrieval.Some IVR (interactive voice response) systems us-ing the speech recognition technology are being putinto practical use as its simplest form.
According tothe spread of cellular phones, spoken dialogue sys-tems via telephone enable us to obtain informationfrom various places without any other special appa-ratuses.However, the speech interface involves two in-evitable problems: one is speech recognition er-rors, and the other is that much information can-not be conveyed at once in speech communications.Therefore, the dialogue strategies, which determinewhen to make guidance and what the system shouldtell to the user, are the essential factors.
To copewith speech recognition errors, several confirma-tion strategies have been proposed: confirmationmanagement methods based on confidence measuresof speech recognition results (Komatani and Kawa-hara, 2000; Hazen et al, 2000) and implicit con-firmation that includes previous recognition resultsinto system?s prompts (Sturm et al, 1999).
In termsof determining what to say to the user, several stud-ies have been done not only to output answers cor-responding to user?s questions but also to generatecooperative responses (Sadek, 1999).
Furthermore,methods have also been proposed to change the di-alogue initiative based on various cues (Litman andPan, 2000; Chu-Carroll, 2000; Lamel et al, 1999).Nevertheless, whether a particular response is co-operative or not depends on individual user?s char-acteristics.
For example, when a user says nothing,the appropriate response should be different whetherhe/she is not accustomed to using the spoken dia-logue systems or he/she does not know much aboutthe target domain.
Unless we detect the cause of thesilence, the system may fall into the same situationrepeatedly.In order to adapt the system?s behavior to individ-ual users, it is necessary to model the user?s patterns(Kass and Finin, 1988).
Most of conventional stud-ies on user models have focused on the knowledgeof users.
Others tried to infer and utilize user?s goalsto generate responses adapted to the user (van Beek,1987; Paris, 1988).
Elzer et al (2000) proposed amethod to generate adaptive suggestions accordingto users?
preferences.However, these studies depend on knowledge ofthe target domain greatly, and therefore the usermodels need to be deliberated manually to be ap-plied to new domains.
Moreover, they assumed thatthe input is text only, which does not contain errors.On the other hand, spoken utterances include variousinformation such as the interval between utterances,the presence of barge-in and so on, which can beutilized to judge the user?s character.
These featuresalso possess generality in spoken dialogue systemsbecause they are not dependent on domain-specificknowledge.We propose more comprehensive user models togenerate user-adapted responses in spoken dialoguesystems taking account of all available informationspecific to spoken dialogue.
The models changeboth the dialogue initiative and the generated re-sponse.
In (Eckert et al, 1997), typical users?
be-haviors are defined to evaluate spoken dialogue sys-tems by simulation, and stereotypes of users are as-sumed such as patient, submissive and experienced.We introduce user models not for defining users?
be-haviors beforehand, but for detecting users?
patternsin real-time interaction.We define three dimensions in the user models:?skill level to the system?, ?knowledge level on thetarget domain?
and ?degree of hastiness?.
The for-mer two are related to the strategies in manage-ment of the initiative and the response generation.These two enable the system to adaptively gener-ate dialogue management information and domain-specific information, respectively.
The last one isused to manage the situation when users are in hurry.Namely, it controls generation of the additive con-tents based on the former two user models.
Handlingsuch a situation becomes more crucial in speechcommunications using cellular phones.The user models are trained by decision treeSys: Please tell me your current bus stop, your destinationor the specific bus route.User: Shijo-Kawaramachi.Sys: Do you take a bus from Shijo-Kawaramachi?User: Yes.Sys: Where will you get off the bus?User: Arashiyama.Sys: Do you go from Shijo-Kawaramachi to Arashiyama?User: Yes.Sys: Bus number 11 bound for Arashiyama has departedSanjo-Keihanmae, two bus stops away.Figure 1: Example dialogue of the bus systemlearning algorithm using real data collected from theKyoto city bus information system.
Then, we imple-ment the user models and adaptive dialogue strate-gies on the system and evaluate them using data col-lected with 20 novice users.2 Kyoto City Bus Information SystemWe have developed the Kyoto City Bus InformationSystem, which locates the bus a user wants to take,and tells him/her how long it will take before itsarrival.
The system can be accessed via telephoneincluding cellular phones1.
From any places, userscan easily get the bus information that changes ev-ery minute.
Users are requested to input the bus stopto get on, the destination, or the bus route numberby speech, and get the corresponding bus informa-tion.
The bus stops can be specified by the name offamous places or public facilities nearby.
Figure 1shows a simple example of the dialogue.Figure 2 shows an overview of the system.The system operates by generating VoiceXMLscripts dynamically.
The real-time bus informationdatabase is provided on the Web, and can be ac-cessed via Internet.
Then, we explain the modulesin the following.VWS (Voice Web Server)The Voice Web Server drives the speech recog-nition engine and the TTS (Text-To-Speech)module according to the specifications by thegenerated VoiceXML.Speech RecognizerThe speech recognizer decodes user utterances1+81-75-326-3116VWS(Voice Web Server)responsesentencesrecognition results(only language info.
)recognition results(including features otherthan language info.)
VoiceXMLuserTTS speechrecognizerVoiceXMLgeneratordialoguemanageruserprofilesreal businformationuser modelidentifierCGIthe system except forproposed user modelsFigure 2: Overview of the bus system with usermodelsbased on specified grammar rules and vocabu-lary, which are defined by VoiceXML at eachdialogue state.Dialogue ManagerThe dialogue manager generates response sen-tences based on speech recognition results (busstop names or a route number) received fromthe VWS.
If sufficient information to locate abus is obtained, it retrieves the correspondinginformation from the real-time bus informationdatabase.VoiceXML GeneratorThis module dynamically generates VoiceXMLfiles that contain response sentences and spec-ifications of speech recognition grammars,which are given by the dialogue manager.User Model IdentifierThis module classifies user?s characters basedon the user models using features specific tospoken dialogue as well as semantic attributes.The obtained user profiles are sent to the dia-logue manager, and are utilized in the dialoguemanagement and response generation.3 Response Generation using User Models3.1 Classification of User ModelsWe define three dimensions as user models listed be-low. Skill level to the system Knowledge level on the target domain Degree of hastinessSkill Level to the SystemSince spoken dialogue systems are notwidespread yet, there arises a difference in theskill level of users in operating the systems.
Itis desirable that the system changes its behaviorincluding response generation and initiative man-agement in accordance with the skill level of theuser.
In conventional systems, a system-initiatedguidance has been invoked on the spur of themoment either when the user says nothing orwhen speech recognition is not successful.
In ourframework, by modeling the skill level as the user?sproperty, we address a radical solution for theunskilled users.Knowledge Level on the Target DomainThere also exists a difference in the knowledgelevel on the target domain among users.
Thus, it isnecessary for the system to change information topresent to users.
For example, it is not cooperativeto tell too detailed information to strangers.
On theother hand, for inhabitants, it is useful to omit tooobvious information and to output additive informa-tion.
Therefore, we introduce a dimension that rep-resents the knowledge level on the target domain.Degree of HastinessIn speech communications, it is more importantto present information promptly and concisely com-pared with the other communication modes such asbrowsing.
Especially in the bus system, the concise-ness is preferred because the bus information is ur-gent to most users.
Therefore, we also take accountof degree of hastiness of the user, and accordinglychange the system?s responses.3.2 Response Generation Strategy using UserModelsNext, we describe the response generation strategiesadapted to individual users based on the proposeduser models: skill level, knowledge level and hasti-ness.
Basic design of dialogue management is basedon mixed-initiative dialogue, in which the systemmakes follow-up questions and guidance if neces-sary while allowing a user to utter freely.
It is in-vestigated to add various contents to the system re-sponses as cooperative responses in (Sadek, 1999).Such additive information is usually cooperative, butsome people may feel such a response redundant.Thus, we introduce the user models and controlthe generation of additive information.
By introduc-ing the proposed user models, the system changesgenerated responses by the following two aspects:dialogue procedure and contents of responses.Dialogue ProcedureThe dialogue procedure is changed based on theskill level and the hastiness.
If a user is identified ashaving the high skill level, the dialogue managementis carried out in a user-initiated manner; namely, thesystem generates only open-ended prompts.
On theother hand, when user?s skill level is detected as low,the system takes an initiative and prompts necessaryitems in order.When the degree of hastiness is low, the systemmakes confirmation on the input contents.
Con-versely, when the hastiness is detected as high, sucha confirmation procedure is omitted.Contents of ResponsesInformation that should be included in the sys-tem response can be classified into the following twoitems.1.
Dialogue management information2.
Domain-specific informationThe dialogue management information specifieshow to carry out the dialogue including the instruc-tion on user?s expression like ?Please reply with ei-ther yes or no.?
and the explanation about the fol-lowing dialogue procedure like ?Now I will ask inorder.?
This dialogue management information isdetermined by the user?s skill level to the system,58.8>=the maximum number of filled slotsdialogue stateinitial state otherwisepresense of barge-inrate of no input0.07>30 1 2average ofrecognition score58.8<skill levelhighskill levelhighskill levellowskill levellowFigure 3: Decision tree for the skill leveland is added to system responses when the skill levelis considered as low.The domain-specific information is generated ac-cording to the user?s knowledge level on the targetdomain.
Namely, for users unacquainted with thelocal information, the system adds the explanationabout the nearest bus stop, and omits complicatedcontents such as a proposal of another route.The contents described above are also controlledby the hastiness.
For users who are not in hurry, thesystem generates the additional contents as cooper-ative responses.
On the other hand, for hasty users,the contents are omitted in order to prevent the dia-logue from being redundant.3.3 Classification of User based on DecisionTreeIn order to implement the proposed user models as aclassifier, we adopt a decision tree.
It is constructedby decision tree learning algorithm C5.0 (Quinlan,1993) with data collected by our dialogue system.Figure 3 shows the derived decision tree for the skilllevel.We use the features listed in Figure 4.
They in-clude not only semantic information contained in theutterances but also information specific to spokendialogue systems such as the silence duration priorto the utterance and the presence of barge-in.
Ex-cept for the last category of Figure 4 including ?at-tribute of specified bus stops?, most of the featuresare domain-independent.The classification of each dimension is done forevery user utterance except for knowledge level.
Themodel of a user can change during a dialogue.
Fea-tures extracted from utterances are accumulated ashistory information during the session.Figure 5 shows an example of the system behav- features obtained from a single utterance?
dialogue state (defined by already filled slots)?
presence of barge-in?
lapsed time of the current utterance?
recognition result (something recognized / un-certain / no input)?
score of speech recognizer?
the number of filled slots by the current utter-ance features obtained from the session?
the number of utterances?
dialogue state of the previous utterance?
lapsed time from the beginning of the session?
the number of repetitions of a same question?
the average number of repetitions of a samequestion?
ratio of the total time of user utterances inwhole elapsed time?
ratio of the occurrence of barge-in out of thewhole number of utterances?
recognition result of the previous utterance?
ratio of something recognized?
ratio of getting uncertain results?
ratio of no input?
the number of barge-in?
the number of something recognized?
the number of getting uncertain results?
the number of no input?
average of recognition scores?
the maximum number of slots filled by a singleutterance features obtained from the session (used only in clas-sification of knowledge level)?
attribute of specified bus stops?
the way to specify bus stops (whether a bus stopis specified by its correct name or not)?
ratio of each attribute of specified bus stops?
ratio of each way of specifying bus stopsFigure 4: Features to classify the user modelsior with the proposed user models.
The skill level isclassified as being low by the decision tree, becausethe first user?s utterance includes only one contentword.
Then, dialogue procedure is changed to thesystem-initiated one.
Similarly, the hastiness is clas-sified as being low by the decision tree, and the sys-tem includes the explanation on the dialogue pro-cedure and instruction on the expression in the re-sponses.
They are omitted if the hastiness is identi-fied as high.3.4 Decision Tree Learning for User ModelsWe train and evaluate the decision tree for the usermodels using dialogue data collected by our system.The data was collected from December 10th 2001 tocase: [skill level: low][knowledge level: low][hastiness: low]Sys: Please tell me your current bus stop, your destinationor the specific bus route number.User: Maruyama Park.Sys: Let me confirm one by one.
( The explanation is added by [skill level: low].
)Will you get on at Maruyama Park?
( The content is confirmed by [hastiness: low].
)Please reply with yes or no.
( The instruction is added by [skill level: low].
)User: Yes.Sys: Where will you get off the bus?
Please specify by abus stop name or a well-known landmark.
( The explanation is added by [skill level: low].
)User: Tofukuji temple.Sys: Will you get off the bus at Tofukuji temple?Please reply with yes or no.User: Yes.Sys: The following buses are located.
( The explanation is added by [skill level: low].
)The nearest bus stop to Maruyama Park is Gion.
( The domain information is added by [knowl-edge level: low].
)Bus number 202 bound for Nishioji-Kujo has de-parted Higashiyama-Sanjo, which is two stops away....Figure 5: An example dialogue with the proposeduser modelslow indeterminable high totalskill level 743 253 496 1492knowledge level 275 808 409 1492hastiness 421 932 139 1492Table 1: Number of manually labeled items for de-cision tree learningMay 10th 2002.
The number of the sessions (tele-phone calls) is 215, and the total number of utter-ances included in the sessions is 1492.
We anno-tated the subjective labels by hand.
The annotatorjudges the user models for every utterances basedon recorded speech data and logs.
The labels weregiven to the three dimensions described in section3.3 among ?high?, ?indeterminable?
or ?low?.
It ispossible that annotated models of a user change dur-ing a dialogue, especially from ?indeterminable?
to?low?
or ?high?.
The number of labeled utterances isshown in Table 1.Using the labeled data, we evaluated the classi-fication accuracy of the proposed user models.
Allthe experiments were carried out by the method of10-fold cross validation.
The process, in which onetenth of all data is used as the test data and the re-mainder is used as the training data, is repeated tentimes, and the average of the accuracy is computed.The result is shown in Table 2.
The conditions #1,#2 and #3 in Table 2 are described as follows.#1: The 10-fold cross validation is carried out perutterance.#2: The 10-fold cross validation is carried out persession (call).#3: We calculate the accuracy under more realis-tic condition.
The accuracy is calculated notin three classes (high / indeterminable / low)but in two classes that actually affect the dia-logue strategies.
For example, the accuracy forthe skill level is calculated for the two classes:low and the others.
As to the classification ofknowledge level, the accuracy is calculated fordialogue sessions because the features such asthe attribute of a specified bus stop are not ob-tained in every utterance.
Moreover, in orderto smooth unbalanced distribution of the train-ing data, a cost corresponding to the reciprocalratio of the number of samples in each class isintroduced.
By the cost, the chance rate of twoclasses becomes 50%.The difference between condition #1 and #2 is thatthe training was carried out in a speaker-closed orspeaker-open manner.
The former shows better per-formance.The result in condition #3 shows useful accuracyin the skill level.
The following features play im-portant part in the decision tree for the skill level:the number of filled slots by the current utterance,presence of barge-in and ratio of no input.
For theknowledge level, recognition result (something rec-ognized / uncertain / no input), ratio of no input andthe way to specify bus stops (whether a bus stop isspecified by its exact name or not) are effective.
Thehastiness is classified mainly by the three features:presence of barge-in, ratio of no input and lapsedtime of the current utterance.condition #1 #2 #3skill level 80.8% 75.3% 85.6%knowledge level 73.9% 63.7% 78.2%hastiness 74.9% 73.7% 78.6%Table 2: Classification accuracy of the proposed usermodels4 Experimental Evaluation of the Systemwith User ModelsWe evaluated the system with the proposed usermodels using 20 novice subjects who had not usedthe system.
The experiment was performed in thelaboratory under adequate control.
For the speechinput, the headset microphone was used.4.1 Experiment ProcedureFirst, we explained the outline of the system to sub-jects and gave the document in which experimentconditions and the scenarios were described.
Weprepared two sets of eight scenarios.
Subjects wererequested to acquire the bus information using thesystem with/without the user models.
In the sce-narios, neither the concrete names of bus stops northe bus number were given.
For example, one ofthe scenarios was as follows: ?You are in Kyotofor sightseeing.
After visiting the Ginkakuji temple,you go to Maruyama Park.
Supposing such a situa-tion, please get information on the bus.?
We also setthe constraint in order to vary the subjects?
hastinesssuch as ?Please hurry as much as possible in orderto save the charge of your cellular phone.
?The subjects were also told to look over question-naire items before the experiment, and filled in themafter using each system.
This aims to reduce the sub-ject?s cognitive load and possible confusion due toswitching the systems (Over, 1999).
The question-naire consisted of eight items, for example, ?Whenthe dialogue did not go well, did the system guide in-telligibly??
We set seven steps for evaluation abouteach item, and the subject selected one of them.Furthermore, subjects were asked to write downthe obtained information: the name of the bus stopto get on, the bus number and how much time ittakes before the bus arrives.
With this procedure,we planned to make the experiment condition closeto the realistic one.duration (sec.)
# turngroup 1 with UM 51.9 4.03(with UM w/o UM) w/o UM 47.1 4.18group 2 w/o UM 85.4 8.23(w/o UM with UM) with UM 46.7 4.08UM: User ModelTable 3: Duration and the number of turns in dia-logueThe subjects were divided into two groups; a half(group 1) used the system in the order of ?withuser models  without user models?, the other half(group 2) used in the reverse order.The dialogue management in the system withoutuser models is also based on the mixed-initiative di-alogue.
The system generates follow-up questionsand guidance if necessary, but behaves in a fixedmanner.
Namely, additive cooperative contents cor-responding to skill level described in section 3.2 arenot generated and the dialogue procedure is changedonly after recognition errors occur.
The system with-out user models behaves equivalently to the initialstate of the user models: the hastiness is low, theknowledge level is low and the skill level is high.4.2 ResultsAll of the subjects successfully completed the giventask, although they had been allowed to give up if thesystem did not work well.
Namely, the task successrate is 100%.Average dialogue duration and the number ofturns in respective cases are shown in Table 3.Though the users had not experienced the system atall, they got accustomed to the system very rapidly.Therefore, as shown in Table 3, both the durationand the number of turns were decreased obviouslyin the latter half of the experiment in either group.However, in the initial half of the experiment, thegroup 1 completed with significantly shorter dia-logue than group 2.
This means that the incorpora-tion of the user models is effective for novice users.Table 4 shows a ratio of utterances for which theskill level was identified as high.
The ratio is calcu-lated by dividing the number of utterances that werejudged as high skill level by the number of all utter-ances in the eight sessions.
The ratio is much largerfor group 1 who initially used the system with usergroup 1 with UM 0.72(with UM  w/o UM) w/o UM 0.70group 2 w/o UM 0.41(w/o UM  with UM) with UM 0.63Table 4: Ratio of utterances for which the skill levelwas judged as highmodels.
This fact means that novice users got ac-customed to the system more rapidly with the usermodels, because they were instructed on the usageby cooperative responses generated when the skilllevel is low.
The results demonstrate that coopera-tive responses generated according to the proposeduser models can serve as good guidance for noviceusers.In the latter half of the experiment, the dialogueduration and the number of turns were almost samebetween the two groups.
This result shows that theproposed models prevent the dialogue from becom-ing redundant for skilled users, although generatingcooperative responses for all users made the dia-logue verbose in general.
It suggests that the pro-posed user models appropriately control the genera-tion of cooperative responses by detecting charactersof individual users.5 ConclusionsWe have proposed and evaluated user models forgenerating cooperative responses adaptively to in-dividual users.
The proposed user models consistof the three dimensions: skill level to the system,knowledge level on the target domain and the de-gree of hastiness.
The user models are identified us-ing features specific to spoken dialogue systems aswell as semantic attributes.
They are automaticallyderived by decision tree learning, and all featuresused for skill level and hastiness are independent ofdomain-specific knowledge.
So, it is expected thatthe derived user models can be used in other do-mains generally.The experimental evaluation with 20 novice usersshows that the skill level of novice users was im-proved more rapidly by incorporating the user mod-els, and accordingly the dialogue duration becomesshorter more immediately.
The result is achievedby the generated cooperative responses based on theproposed user models.
The proposed user modelsalso suppress the redundancy by changing the dia-logue procedure and selecting contents of responses.Thus, they realize user-adaptive dialogue strategies,in which the generated cooperative responses serveas good guidance for novice users without increas-ing the dialogue duration for skilled users.ReferencesJennifer Chu-Carroll.
2000.
MIMIC: An adaptivemixed initiative spoken dialogue system for informa-tion queries.
In Proc.
of the 6th Conf.
on applied Nat-ural Language Processing, pages 97?104.Wieland Eckert, Esther Levin, and Roberto Pieraccini.1997.
User modeling for spoken dialogue system eval-uation.
In Proc.
IEEE Workshop on Automatic SpeechRecognition and Understanding, pages 80?87.Stephanie Elzer, Jennifer Chu-Carroll, and Sandra Car-berry.
2000.
Recognizing and utilizing user prefer-ences in collaborative consultation dialogues.
In Proc.of the 4th Int?l Conf.
on User Modeling, pages 19?24.Timothy J. Hazen, Theresa Burianek, Joseph Polifroni,and Stephanie Seneff.
2000.
Integrating recognitionconfidence scoring with language understanding anddialogue modeling.
In Proc.
ICSLP.Robert Kass and Tim Finin.
1988.
Modeling the user innatural language systems.
Computational Linguistics,14(3):5?22.Kazunori Komatani and Tatsuya Kawahara.
2000.Flexible mixed-initiative dialogue management usingconcept-level confidence measures of speech recog-nizer output.
In Proc.
Int?l Conf.
Computational Lin-guistics (COLING), pages 467?473.Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and SamirBennacef.
1999.
The LIMSI ARISE system fortrain travel information.
In IEEE Int?l Conf.
Acoust.,Speech & Signal Process.Diane J. Litman and Shimei Pan.
2000.
Predicting andadapting to poor speech recognition in a spoken dia-logue system.
In Proc.
of the 17th National Confer-ence on Artificial Intelligence (AAAI2000).Paul Over.
1999.
Trec-7 interactive track report.
In Proc.of the 7th Text REtrieval Conference (TREC7).Cecile L. Paris.
1988.
Tailoring object descriptions toa user?s level of expertise.
Computational Linguistics,14(3):64?78.J.
Ross Quinlan.
1993.
C4.5: Programs for Ma-chine Learning.
Morgan Kaufmann, San Mateo, CA.http://www.rulequest.com/see5-info.html.David Sadek.
1999.
Design considerations on dia-logue systems: From theory to technology -the caseof artimis-.
In Proc.
ESCA workshop on InteractiveDialogue in Multi-Modal Systems.Janienke Sturm, Els den Os, and Lou Boves.
1999.
Is-sues in spoken dialogue systems: Experiences with theDutch ARISE system.
In Proc.
ESCA workshop on In-teractive Dialogue in Multi-Modal Systems.Peter van Beek.
1987.
A model for generating betterexplanations.
In Proc.
of the 25th Annual Meeting ofthe Association for Computational Linguistics (ACL-87), pages 215?220.
