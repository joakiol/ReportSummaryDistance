Tagging English by Path Voting ConstraintsGhkhan Tfir and Kemal OflazerDepartment of Computer Engineering and Information ScienceBilkent University, Bilkent, Ankara, TR-06533, TURKEY{tur, ko }@cs.
bilkent, edu.
trAbstract: We describe a constraint-basedtagging approach where individual constraintrules vote on sequences of matching tokens andtags.
Disambiguation of all tokens in a sentenceis performed at the very end by selecting tagsthat appear on the path that receives the high-est vote.
This constraint application paradigmmakes the outcome of the disambiguation i -dependent of the rule sequence, and hence re-lieves the rule developer from worrying aboutpotentially conflicting rule sequencing.
The ap-proach can also combine statistically and manu-ally obtained constraints, and incorporate neg-ative constraint rules to rule out certain pat-terns.
We have applied this approach to taggingEnglish text from the Wall Street Journal andthe Brown Corpora.
Our results from the WallStreet Journal Corpus indicate that with 400statistically derived constraint rules and about800 hand-crafted constraint rules, we can attainan average accuracy of 9Z89~ on the trainingcorpus and an average accuracy of g7.50~ onthe testing corpus.
We can also relax the singletag per token limitation and allow ambiguoustagging which lets us trade recall and precision.1 In t roduct ionPart-of-speech tagging is one of the preliminarysteps in many natural anguage processing sys-tems in which the proper part-of-speech tag ofthe tokens comprising the sentences are disam-biguated using either statistical or symbolic lo-cal contextual information.
Tagging systemshave used either a statistical approach wherea large corpora is employed to train a proba-bilistic model which then is used to tag unseentext, (e.g., Church (1988), Cutting et al (1992),DeRose (1988)), or a constraint-based approachwhich employs a large number of hand-craftedlinguistic constraints that are used to eliminateimpossible sequences or morphological parsesfor a given word in a given context, recentlymost prominently exemplified by the ConstraintGrammar work (Karlsson et al, 1995; Vouti-lainen, 1995b; Voutilainen et al, 1992; Vouti-lainen and Tapanainen, 1993).
BriU (1992;1994; 1995) has presented a transformation-based learning approach.This paper extends a novel approach toconstraint-based tagging first applied for Turk-ish (Oflazer and Tiir, 1997), which relieves therule developer from worrying about conflictingrule ordering requirements and constraints.
Theapproach depends on assigning votes to con-straints via statistical and/or manual means,and then letting constraints vote on match-ing sequences on tokens, as depicted in Figure1.
This approach does not reflect the outcomeof matching constraints to the set of morpho-logical parses immediately as usually done inconstraint-based systems.
Only after all appli-cable rules are applied to a sentence, tokens aredisambiguated in parallel.
Thus, the outcome ofthe rule applications i independent of the orderof rule applications.W1 W2 W3 W4 Wn TokensR1  R3  R2 --" Rm voting RulesFigure 1: Voting Constraint Rules1277?
( can, HD) (can, l ~)(I.PRP) ~ ~ -  _ (the,DT)( cart o HD |Figure 2: Representing sentences with a directed acyclic graph2 Tagg ing  by  Path  Vot ingConst ra in tsWe assume that sentences are delineated andthat each token is assigned all possible tags by alexicon or by a morphological nalyzer.
We rep-resent each sentence as a standard chart usinga directed acyclic graph where nodes representtoken boundaries and arcs are labeled with am-biguous interpretations of tokens.
For instance,the sentence I can can the  can.
would berepresented as shown in Figure 2, where boldarcs denote the correct tags.We describe constraints on token sequencesusing rules of the sort R = (C1,C2,-.
",Cn; V),where the Ci are, in general, feature constraintson a sequence of the ambiguous parses, and Vis an integer denoting the vote of the rule.
ForEnglish, the features that we use are: (1) LEX:the lexical form, and (2) TAG: the tag.
It iscertainly possible to extend the set of featuresused, by including features uch as initial lettercapitalization, any derivational information,etc.
(see (Oflazer and Tiir, 1997)).
For in-stance, (\[ThG=MD\], [ThG=RB\], \[ThGfVB\] ; 100)is a rule with a high vote to promote modalfollowed by a verb with an intervening adverb.The rule (\[TAG=DT,LEX=that\], \[ThG=NNS\] ;-100) demotes a singular determiner ead-ing of that  before a plural noun, while( \[ThG=DT, LEX=each\], \[TAG=J J ,  LEX=o'cher\] ;100) is a rule with a high vote that captures acollocation (Santorini, 1995).The constraints apply to a sentence in thefollowing manner: Assume for a moment hatall possible paths from the start node to theend node of a sentence graph are explicitly enu-merated, and that after the enumeration, eachpath is augmented by a vote component.
Foreach path at hand, we apply each constraintto all possible sequences of token parses.
LetR = (C1 ,C2, ' " ,C ,~;V)  be a constraint andlet w i ,w i+l , - ' - ,  wi+,~-i be a sequence of tokenparses labeling sequential arcs of the path.
Wesay rule R matches this sequence of parses, ifwj, i _< j < i + n - 1 is subsumed by the corre-sponding constraint Cj-i+l.
When such a matchoccurs, the vote of the path is incremented byV.
When all constraints are applied to all pos-sible sequences in all paths, we select the pathwith the maximum vote.
If there are multiplepaths with the same maximum vote, the tokenswhose parses are different in those paths are as-sumed to be left ambiguous.Given that each token has on the averagemore than 2 possible tags, the procedural de-scription above is very inefficient for all but veryshort sentences.
However, the observation thatour constraints are localized to a window of asmall number of tokens (say at most 5 tokensin a sequence), suggests a more efficient schemeoriginally used by Church (1988).
Assume ourconstraint windows are allowed to look at a win-dow of at most size k sequential parses.
Letus take the first k tokens of a sentence andgenerate all possible paths of k arcs (spanningk + 1 nodes), and apply all constraints to these"short" paths.
Now, if we discard the first to-ken and consider the (k + 1) st token, we needto consider and extend only those paths thathave accumulated the maximum vote among thepaths whose last k - 1 parses are the same.
Thereason is that since the first token is now outof the context window, it can not influence theapplication of any rules.
Hence only the high-est scoring (partial) paths need to be extended,as lower scoring paths can not later accumu-late votes to surpass the current highest scoringpaths.In Figure 3 we describe the procedure in amore formal way where wl, w2, "  ", ws denotesa sequence of tokens in a sentence, amb(wi)  de-notes the number of ambiguous tags for tokenwi, and k denotes the maximum context win-dow size (determined at run time).12781.
P = { all I-I~_--: arnb(wj) paths of the first k -  1tokens }2. i=k3.
while i < s4.
begin4.1) Create amb(wi) copies of each path in Pand extend each such copy with one of thedistinct tags for token wl.4'.2) Apply all constraints to the last k tokensof every path in P, updating path votesaccordingly.4.3) Remove from P any path p if there is someother path p' such that vote(p') > vote(p)and the last k - 1 tags of path p are sameas the last k - 1 tags of p'.4.4) i= i+1endFigure 3: Procedure for fast constraint apphca-tion3 Resu l ts  f rom Tagg ing  Eng l i shWe evaluated our approach using l 1-fold crossvalidation on the Wall Street Journal Corpusand 10-fold cross validation on a portion of theBrown Corpus from the Penn Treebank CD.We used two classes of constraints: (i) we ex-tracted a set of tag k-grams from a trainingcorpus and used them as constraint rules withvotes assigned as described below, and (ii) wehand-crafted a set rules mainly incorporatingnegative constraints (demoting impossible orunlikely situations), or lezicalized positive con-straints.
These were constructed by observingthe failures of the statistical constraints on thetraining corpus.Ru les  der ived  f rom the  t ra in ing  corpusFor the statistical constraint rules, we extracttag k-grams from the tagged training corpusfor k = 2, and k = 3.
For each tagk-gram, we compute a vote which is essen-tially very similar to the rule strength usedby Tzoukermann et al (1995) except thatwe do not use their notion of genotypes ex-actly in the same way.
Given a tag k-gramt l ,t2,.
.
.tk, let n = count(t1 E Tags(wi),t2 ETags(wi+l),...,tk E Tags(wi+k-1)) for all pos-sible i's in the training corpus, be the numberof possible places the tags sequence can possi-bly occur, footnoteTags(wi) is the set of tagsassociated with the token wi.
Let f be the num-ber of times the tag sequence t l , t2 , .
.
.
tk  ac-tually occurs in the tagged text, that is, f =count(tl,t~,...tk).
We smooth f in  by defining/+0.5 so that neither p nor 1 -p  is zero.
The P" -  n+luncertainty of p is then given as ~/p(1 -  p)/n(Tzoukermann et al, 1995).
We then computedthe vote for this k-gram asVote(tl,t2,...tk) = (p -  ~fp(1 - p)/n) ?
100.This formulation thus gives high votes to k-grams which are selected most of the time theyare "selectable."
And, among the k-gramswhich are equally good (same f /n) ,  those witha higher n (hence less uncertainty) are givenhigher votes.After extracting the k-grams as describedabove for k = 2 and k = 3, we ordered eachgroup by decreasing votes and conducted an ini-tim set of experiments to select a small groupof constraints performing satisfactorily.
We se-lected the first 200 (with highest votes) of the 2-gram and the first 200 of the 3-gram constraints,as the set of statistical constraints.
It should benoted that the constraints obtained this way arepurely constraints on tag sequences and do notuse any lexical or genotype information.Hand-c ra f ted  ru les In addition to thesestatistical constraint rules, we introduced 824hand-crafted constraint rules.
Most of thehand-crafted constraints imposed negative con-straints (with large negative votes) to rule outcertain tag sequences that we encountered inthe Wall Street Journal Corpus.
Another setof rules were lexicahzed rules involving the to-kens as well as the tags.
A third set of rules foridiomatic constructs and collocations was alsoused.
The votes for negative and positive hand-crafted constraints are selected to override anyvote a statisticM constraint may have.Init ia l  Votes  To reflect the impact of lexicalfrequencies we initialize the totM vote of eachpath with the sum of the lexical votes for thetoken and tag combinations on it.
These lexicalvotes for the parse ti,j of token wi are obtainedfrom the training corpus in the usuM way, i.e.,as count(wi,ti,j)/count(w~), and then are nor-mahzed to between 0 and 100.Exper iments  on  WSJ  and  Brown CorporaWe tested our approach on two English Corpora1279from the Penn Treebank CD.
We divided a 5500sentence portion of the Wall Street Journal Cor-pus into 11 different sets of training texts (withabout 118,500 words on the average), and corre-sponding testing texts (with about 11,800 wordson the average), and then tagged these textsusing the statistical rules and hand-crafted con-straints.
The hand-crafted rules were obtainedfrom only one of the training text portions, andnot from all, but for each experiment the 400statistical rules were obtained from the respec-tive training set.We also performed a similar experiment witha portion of the Brown Corpus.
We used 4000sentences (about 100,000 words) with 10-foldcross validation.
Again we extracted the statis-tical rules from the respective training sets, butthe hand-crafted rules were the ones developedfrom the Wall Street Journal training set.
Foreach case we measured the accuracy by countingthe correctly disambiguated tokens.
The man-ual rules used for Brown Corpus were the rulesderived the from Wall Street Journal data.
Theresults of these experiments are shown in Table1.WSJ  BrownConst.
Tra.
Test Tra.
TestSet Acc.
Acc.
Acc.
Acc.1 95.59 94.54 95.75 94.251+2 96.47 95.68 96.78 95.761+3 96.39 95.37 96.50 95.101+2+3 96.66 95.96 96.91 96.021+4 97.21 96.70 96.27 95.531+2+4 97.85 97.43 97.13 96.511+3+4 97.60 97.08 96.80 96.091+2+3+4 97.89 97.50 97.18 96.67(I) Lexical Votes (2) 200 2-grams(3) 200 3-grams (4) 824 Manual Constr.Table 1: Results from tagging the WSJ andBrown Corpora.We feel that the results in the last row ofTable 1 are quite satisfactory and warrant fur-ther extensive investigation.
On the Wall StreetJournal Corpus, our tagging approach is on paror even better than stochastic taggers makingclosed vocabulary assumption.
Weischedel t al.
(1993) report a 96.7% accuracy with 1,000,000words of training corpus.
The performance ofP0.990.980.970.960.950.940.930.920.91RecallTest SetPrecis ion Ambigu i ty97.94 96.70 1.01298.27 95.29 1.03198.48 93.63 1.05298.65 91.63 1.07698.78 90.21 1.09598.98 88.92 1.11399.03 88.05 1.12499.10 87.19 1.13699.13 86.68 1.143Table 2: Recall and precision results on a WSJtest set with some tokens left ambiguousour system with Brown corpus is very closeto that of Brill's transformation-based tagger,which can reach 97.2% accuracy with closed vo-cabulary assumption and 96.5% accuracy withopen vocabulary assumption with no ambiguity(Brill, 1995).
Our tagging speed is also quitehigh.
With over 1000 constraint rules (longestspanning 5 tokens) loaded, we can tag at about1600 tokens/sec on a Ultrasparc 140, or a Pen-tium 200.It is also possible for our approach to allowfor some ambiguity.
In the procedure given ear-lier, in line 4.3, if one selects all (partial) pathswhose accumulated vote is within p (0 < p <__ 1)of the (partial) path with the largest vote, thena certain amount of ambiguity can be intro-duced, at the expense of a slowdown in taggingspeed and an increase in memory requirements.In such a case, instead of accuracy, one needsto use ambiguity, recall, and precision (Vouti-lainen, 1995a).
Table 2 presents the recall, pre-cision and ambiguity results from tagging .one ofthe Wall Street Journal test sets using the sameset of constraints but with p ranging from 0.91to 0.99.
These compare quite favorably withthe k-best results of Brill(1995), but reductionin tagging speed is quite noticeable, especiallyfor lower p's.
Any improvements in single tagper token tagging (by additional hand craftedconstraints) will certainly be reflected to theseresults also.4 Conc lus ionsWe have presented an approach to constraint-based tagging that relies on constraint rules vot-1280ing on sequences of tokens and tags.
This ap-proach can combine both statistically and man-ually derived constrMnts, and relieves the ruledeveloper f om worrying about rule ordering, asremoval of tags is not immediately committedbut only after all rules have a say.
Using posi-tive or negative votes, we can promote meaning-ful sequences of tags or collocations, or demoteimpossible sequences.
Our approach is quitegeneral and is applicable to any language.
Ourresults from the Wall Street Journal Corpus in-dicate that with 400 statistically derived con-straint rules and about 800 hand-crafted con-straint rules, we can attain an average accuracyof 9Z89~ on the training corpus and an averageaccuracy of 9Z50~ on the testing corpus.
Ourfuture work involves extending to open vocabu-lary case and evaluating unknown word perfor-mance.5 AcknowledgmentsA portion of the first author's work was donewhile he was visiting Johns Hopkins University,Department of Computer Science with a NATOVisiting Student Scholarship.
This research wasin part supported by a NATO Science for Stabil-ity Program Project Grant - TU-LANGUAGE.ReferencesEric Brill.
1992.
A simple-rule based part-of-speech tagger.
In Proceedings of the ThirdConference on Applied Natural LanguageProcessing, Trento, Italy.Eric Brill.
1994.
Some advances in rule-basedpart of speech tagging.
In Proceedings of theTwelfth National Conference on Articial In-telligence (AAAI-94), Seattle, Washinton.Eric Brill.
1995.
Transformation-based error-driven learning and natural language pro-cessing: A case study in part-of-speech tag-ging.
Computational Linguistics, 21(4):543-566, December.Kenneth W. Church.
1988.
A stochastic partsprogram and a noun phrase parser for un-restricted text.
In Proceedings of the Sec-ond Conference on Applied Natural LanguageProcessing, Austin, Texas.Doug Cutting, Julian Kupiec, Jan Pedersen,and Penelope Sibun.
1992.
A practicalpart-of-speech tagger.
In Proceedings of theThird Conference on Applied Natural Lan-guage Processing, Trento, Italy.Steven J. DeRose.
1988.
Grammatical cate-gory disambiguation by statistical optimiza-tion.
Computational Linguistics, 14(1):31-39.Fred Karlsson, Atro Voutilainen, Juhatteikkil~, and Arto Anttila.
1995.
Con-straint Grammar-A Language-IndependentSystem for Parsing Unrestricted Text.?
Mouton de Gruyter.Kemal Oflazer and GSkhan Tiir.
1997.Morphological disambiguation by vot-ing constraints.
In Proceedings ofACL '97/EACL '97, The 35th Annual Meet-ing of the Association for ComputationalLinguistics, June.Beatrice Santorini.
1995.
Part-of-speech tag-ging guidelines fro the penn treebank project.Available at h t tp  ://www.
ldc.upenn,  edu/.3rd Revision, 2rid Printing.Evelyne Tzoukermann, Dragomir R. Radev,and William A. Gale.
1995.
Combining lin-guistic knowledge and statistical learning infrench part-of-speech tagging.
In Proceedingsof the ACL SIGDAT Workshop From Textsto Tags: Issues in Multilingual LanguageAnalysis, pages 51-57.Atro Voutilainen and Pasi Tapanainen.
1993.Ambiguity resolution in a reductionisticparser.
In Proceedings of EACL'93, Utrecht,Holland.Atro Voutilainen, Juha Heikkil~, and ArtoAnttila.
1992.
Constraint Grammar of En-glish.
University of Helsinki.Atro Voutilainen.
1995a.
Morphological dis-ambiguation.
In Fred Karlsson, Atro Vouti-lainen, Juha Heikkil~, and Arto Anttila,editors, Constraint Grammar-A Language-Independent System for Parsing UnrestrictedText, chapter 5.
Mouton de Gruyter.Atro Voutilainen.
1995b.
A syntax-based part-of-speech analyzer.
In Proceedings of the Sev-enth Conference of the European Chapter ofthe Association of Computational Linguistics,Dublin, Ireland.Ralph Weischedel, Marie Meteer, RichardSchwartz, Lance Ramshaw, and Jeff Pal-mucci.
1993.
Coping with ambiguity and un-known words through probabilistic models.Computational Linguistics, 19(2):359-382.1281
