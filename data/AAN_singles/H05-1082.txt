Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 652?659, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Methodology for Extrinsically Evaluating Information ExtractionPerformanceMichael Crystal, Alex Baron, Katherine Godfrey, Linnea Micciulla, Yvette Tenney, andRalph WeischedelBBN Technologies10 Moulton St.Cambridge, MA 02138-1119mcrystal@bbn.comAbstractThis paper reports a preliminary studyaddressing two challenges in measuringthe effectiveness of information extrac-tion (IE) technology:?
Developing a methodology for ex-trinsic evaluation of IE; and,?
Estimating the impact of improvingIE technology on the ability to per-form an application task.The methodology described can be em-ployed for further controlled experi-ments regarding information extraction.1 IntroductionIntrinsic evaluations of information extraction(IE) have a history dating back to the Third Mes-sage Understanding Conference1 (MUC-3) andcontinuing today in the Automatic Content Ex-traction (ACE) evaluations.2  Extrinsic evalua-tions of IE, measuring the utility of IE in a task,are lacking and needed (Jones, 2005).In this paper, we investigate an extrinsicevaluation of IE where the task is question an-swering (QA) given extracted information.
Inaddition, we propose a novel method for explor-ing hypothetical performance questions, e.g., ifIE accuracy were x% closer to human accuracy,how would speed and accuracy in a task, e.g.,QA, improve?1 For more information on the MUC conferences, seehttp://www.itl.nist.gov/iad/894.02/related_projects/muc/.2 For an overview of ACE evaluations seehttp://www.itl.nist.gov/iad/894.01/tests/ace/.We plot QA accuracy and time-to-completegiven eight extracted data accuracy levels rang-ing from the output of SERIF, BBN?s state-of-the-art IE system, to manually extracted data.2 MethodologyFigure 1 gives an overview of the methodol-ogy.
The left portion of the figure shows sourcedocuments provided both to a system and a hu-man to produce two extraction databases, onecorresponding to SERIF?s automated perform-ance and one corresponding to double-annotated, human accuracy.
By merging por-tions of those two sources in varying degrees(?blends?
), one can derive several extracted da-tabases ranging from machine quality, throughvarying percentages of improved performance,up to human accuracy.
This method of blendingdatabases provides a means of answering hypo-thetical questions, i.e., what if the state-of-the-art were x% closer to human accuracy, with asingle set of answer keys.A person using a given extraction databaseperforms a task, in our case, QA.
The measuresof effectiveness in our study were time to com-plete the task and percent of questions answeredcorrectly.
An extrinsic measure of the value ofimproved IE technology performance is realizedby rotating users through different extractiondatabases and questions sets.In our preliminary study, databases of fullyautomated IE and manual annotation (the goldstandard) were populated with entities, relation-ships, and co-reference links from 946 docu-ments.
The two initial databases representingmachine extraction and human extraction re-spectively were then blended to produce a con-tinuum of database qualities from machine to652human performance.
ACE Value Scores3 weremeasured for each database.
Pilot studies wereconducted to develop questions for a QA task.Each participant answered four sets of questions,each with a different extraction database repre-senting a different level of IE accuracy.
An an-swer capture tool recorded the time to answereach question and additional data to confirm thatthe participant followed the study protocol.
Theanswers were then evaluated for accuracy andthe relationship between QA performance andIE quality was established.Each experiment used four databases.
The first ex-periment used databases spanning the range fromsolely machine extraction to solely human extraction.Based on the results of this experiment, two furtherexperiments focused on smaller ranges in databasequality to study the relationship between IE and QAperformance.2.1 Source Document Selection, Annota-tion, and ExtractionSource documents were selected based on theavailability of manual annotation.
We identified946 broadcast news and newswire articles fromrecent ACE efforts, all annotated by the LDCaccording to the ACE guidelines for the relevantyear (2002, 2003, 2004).
Entities, relations, andwithin-document co-reference were marked.Inter-document co-reference annotation wasadded by BBN.
The 946 news articles com-prised 363 articles (187,720 words) from news-wire and 583 (122,216 words) from broadcastnews.
With some corrections to deal with errorsand changes in guidelines, the annotations wereloaded as the human (DB-quality 100) database.3 The 2004 ACE evaluation plan, available athttp://www.nist.gov/speech/tests/ace/ace04/doc/ace04-evalplan-v7.pdf, contains a full description of the scoring metric used in theevaluation.
Entity type weights were 1 and the level weights wereNAM=1.0, NOM=0.5, and PRO=0.1.SERIF, BBN?s automatic IE system based on itspredecessor, SIFT (Miller, 2000), was run on the946 ACE documents to create the machine (DB-quality 0) database.
SERIF is a statisticallytrained software system that automatically per-forms entity, co-reference, and relationship in-formation extraction.Intermediate IE performance was simulatedby blending the human and automatically gener-ated databases in various degrees using an inter-polation algorithm developed specifically forthis study.
To create a blended database, DB-quality n, all of the entities, relationships, andco-reference links common to the human andautomatically generated databases are copiedinto a new one.
Then, n% of the entity mentionsin the human database (100), but not in theautomatic IE system output (0), are copied; and,(100 ?
n)% of the entity mentions in the auto-matically generated database, but not in the hu-man database, are copied.
Next, the relationshipsfor which both of the constituent entity mentionshave been copied are also copied to the blendeddatabase.
Finally, co-reference links and entitiesfor the already copied entity mentions are copiedinto the blended database.For the first experiment, two intermediate ex-traction databases were created: DB-qualities 33and 67.
For the second experiment, two addi-tional databases were created: 16.5 and 50.
Thefirst intermediate databases were both createdusing the 0 and 100 databases as seeds.
The 16.5database was created by mixing the 0 and the 33databases in a 50% blend.
The 50 database wascreated by doing the same with the 33 and 67databases.
For Experiment 3, 41 and 58 data-bases were created by mixing the 33 and 50, and50 and 67 databases respectively.100010067330<<IE Tool>>AnnotatorsSourcedocs+++0-BlendedExtractionAccuracyMeasure QAAccuracy &SpeedQA TaskStudy establishescurve+++0-10067330Imputed AccuracyRequirementA Priori UtilityThresholdQAPerformanceIE Accuracy (% human annotation)Figure 1: Study Overview653DB Blend0(Machine) 16.5 33 41 50 58 67100(Human)Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent RelRecall 64 33 70 40 74 45 76 48 79 54 82 58 86 65 100 100Pre.
74 50 77 62 79 67 80 70 83 75 85 78 89 82 100 100Value 60 29 67 37 71 42 73 45 77 51 80 56 84 63 100 100Table 1: Precision, Recall and Value Scores for Entities and Relations for each DB Blend0(Machine) 16.5 33 41 50 58 67100(Human)Entities 17,117 18,269 18,942 19,398 19,594 19,589 19,440 18,687Relations 6,684 6,675 6,905 7,091 7,435 7,808 8,406 11,032Descriptions 18,666 18,817 19,135 19,350 19,475 19,639 19,752 20,376Table 2: Entity, Relation and Description Counts for each DB BlendTo validate the interpolation algorithm andblending procedure, we applied NIST?s 2004ACE Scorer to the eight extraction databases.Polynomial approximations were fitted againstboth the entity and relation extraction curves.Entity performance was found to vary linearlywith DB blend (R2 = .9853) and relation per-formance was found to vary with the square ofDB blend (R2 = .9961).
Table 1 shows the scoresfor each blend, and Table 2 shows the counts ofentities, relationships, and descriptions.2.2 Question Answering TaskExtraction effectiveness was measured by howwell a person could answer questions given adatabase of facts, entities, and documents.
Par-ticipants answered four sets of questions usingfour databases.
They accessed the database usingBBN?s FactBrowser (Miller, 2001) and recordedtheir answers and source citations in a separatetool developed for this study, AnswerPad.Each database represented a different data-base quality.
In some databases, facts were miss-ing, or incorrect facts were recorded.Consequently, answers were more accessible insome databases than in others, and participantshad to vary their question answering strategydepending on the database.Participants were given five minutes to an-swer each question.
To ensure that they had ac-tually located the answer rather than relied onworld knowledge, they were required to providesource citations for every answer.
The instruc-tions emphasized that the investigation was atest of the system, and not of their world knowl-edge or web search skills.
Compliance withthese instructions was high.
Users resorted toknowledge-based proper noun searches only onepercent of the time.
In addition, keyword searchwas disabled to force participants to rely on thedatabase features.2.3 ParticipantsStudy participants were recruited through localweb lists and at local colleges and universities.Participants were restricted to college studentsand recent graduates with PC (not Mac) experi-ence, without reading disabilities, for whomEnglish was their native language.
No otherscreening was necessary because the designcalled for each participant to serve as his or herown control, and because opportunities to useworld knowledge in answering the questionswere minimized through the interface and pro-cedures.During the first two months of the study 23participants were used to help develop questions,participant criteria, and the overall test proce-dure.
Then, experiments were conducted com-paring the 0, 33, 67, and 100 database blends(Experiment 1, 20 subjects); the 0, 16.5, 33, and50 database blends (Experiment 2, 20 subjects),and the 33, 41, 50, and 58 database blends (Ex-periment 3, 24 subjects).6542.4 Question Selection and ValidationQuestions were developed over two months ofpilot studies.
The goal was to find a set of ques-tions that would be differentially supported bythe 0, 33, 67, and 100 databases.
We exploredboth ?random?
and ?engineered?
approaches.The random approach called for creating ques-tions using only the documents, without refer-ence to the kind of information extracted.
Usinga list of keywords, one person generated 86questions involving relationships and entitiespertaining to politics and the military by scan-ning the 946 ACE documents to find referencesto each keyword and devising questions basedon the information she found.The alternative, engineered approach involvedeliminating questions that were not supported bythe types of information extracted by SERIF,and generating additional questions to fit thedesired pattern of increasing support with in-creased human annotation.
This approach en-sured that the question sets reflected thestructural differences that are assumed to exist inthe database, and produced psychophysical datathat link degree of QA support to human per-formance parameters.
The IE results from fourof the databases (0, 33, 67 and 100) were used todevelop questions that received differential sup-port from the different quality databases.
Forexample, such a question could be answered us-ing the automatically extracted results, but mightbe more straightforwardly answered given hu-man annotation.Sixty-four questions, plus an additional tenpractice questions, were created using the engi-neering approach.
Additional criteria that werefollowed in creating the question sets were: 1)Questions had to contain at least one reasonableentry hook into all four databases, e.g., the termsU.S.
and America were considered too broad tobe reasonable; and, 2) For ease of scoring, list-type questions had to specify the number of an-swers required.
Alternative criteria were consid-ered but rejected because they correlated withthe aforementioned set.
The following are ex-amples of engineered questions.?
Identify eight current or former U.S. StateDepartment workers.?
In what two West Bank towns does Fatahhave an office??
Name two countries where Osama binLaden has been.?
Were Lebanese women allowed to vote inmunicipal elections between two Shiitegroups in the year 1998?Two question lists, one with 86 questionsgenerated by the random procedure and one with64 questions generated by the engineered proce-dure, were analyzed with respect to the degree ofsupport afforded by each of the four databases asviewed through FactBrowser.
Four a priori cri-teria were established to assess degree of support?
or its opposite, the degree of expected diffi-culty ?
for each question in each of the four da-tabases.
Ranked from easiest to hardest, they arelisted in Table 3.The question can be answered?1.
Directly with fact or description (answeris highlighted in FactBrowser citation)2.
Indirectly with fact or description (an-swer is not highlighted)3.
With name mentioned in question (longlist of mentions without context)4.
Via database crawlingTable 3: A Priori Question Difficulty Character-istics, listed from easiest to hardestTable 4 shows the question difficulty levelsfor both question types, for each of four data-bases.
Analysis of the engineered set was doneon all 64 questions.
Analysis for randomly gen-erated questions was done on a random sampleof 44 of the 86 questions.
Fifteen questions didnot meet the question criteria, leaving 29.The randomly generated questions showed astatistically significant, but small, variation inexpected difficulty, in part due to the number ofunanswerable questions.
While the questionswere made up with respect to information foundin the documents, the process did not considerthe types of extracted entities and relations.
Thisproblem might have been mitigated by limitingthe search to questions involving entities andrelations that were part of the extraction task.By contrast, the engineered question setshowed a highly significant decrease in expecteddifficulty as the percentage of human annotationin the database increased (P < 0.0001 for chi-square analysis).
This result is not surprising,given that the questions were constructed withreference to the list of entities in the four data-655bases.
The analysis confirms that the experimen-tal manipulation of different degrees of supportprovided by the four databases was achieved forthis question set.Random Question GenerationDifficultyLevel(easiest tohardest)0%Human33%Human67%Human100%Human1 Fact-Highlight7 10 13 152 Fact-Indirect14 10 8 103 Mention 3 5 2 14 Web Crawl 5 4 6 3Total 29 29 29 29Engineered Question GenerationDifficultyLevel(from easiestto hardest)0%Human33%Human67Human100%Human1 Fact-Highlight16 25 35 492 Fact-Indirect23 20 18 143 Mention 7 14 11 14 Web Crawl 18 5 0 0Total 64 64 64 64Table 4: Anticipated Difficulty of Questions as aFunction of Database QualityPreliminary human testing with both questionsets suggested that the a priori difficulty indica-tors predict human question answering perform-ance.
Experiments with the randomly generatedquestions, therefore, were unlikely to revealmuch about the databases or about human ques-tion answering performance.
On the other hand,an examination of how different levels of data-base quality affect human performance, in a psy-chophysical experiment where structure is variedsystematically, promised to address the questionof how much support is needed for good per-formance.Based on the question difficulties, and pilotstudy timing and performance results, the 64questions were grouped into four, 16-questionbalanced sets.2.5 ProcedureParticipants were tested individually at our site,in sessions lasting roughly four hours.
Trainingprior to the test lasted for approximately a halfhour.
Training consisted of a walk-through ofthe interface features followed by guided prac-tice with sample questions.
The test consisted offour question sets, each with a different data-base.
Participants were informed that theywould be using a different database for eachquestion set and that some might be easier to usethan others.Questions were automatically presented andresponses were captured in AnswerPad, a soft-ware tool designed for the study.
AnswerPad isshown in Figure 2.Key features of the tool include:?
Limiting view to current question set ?disallowing participants to view previousquestion sets?
Automatically connecting to correct db?
Logging time spent on each question?
Enforcing five-minute limit per question?
Enforcing requirement that all answers in-clude a citationFigure 2: AnswerPad Question Presentation andAnswer Capture InterfaceParticipants were given written documenta-tion as part of their training.
The participantswere instructed to cut-and-paste question an-swers and document citations from sourcedocuments into AnswerPad.Extracted facts and entities, and source docu-ments were accessed through FactBrowser.FactBrowser, shown in Figure 3, is web-browserbased and is invoked via a button in AnswerPad.FactBrowser allows one to enter a string, which656is matched against the database of entity men-tions.
The list of entities that have at least onemention partially matching the string are re-turned (e.g., ?Laura Bush?)
along with an iconindicating the type of the entity and the numberof documents in which the entity appears.Clicking on the entity in the left panel causes thetop right panel to display all of the descriptions,facts, and mentions for the entity.
Selecting oneof these displays citations in which the descrip-tion, fact, or mention occurs.
Clicking on thecitation opens up a document view in the lowerright corner of the screen and highlights the ex-tracted information in the text.
When a docu-ment is displayed, all of the entities detected inthe document are listed down the left side of thedocument viewer.Figure 3: Browsing Tool InterfaceThe browsing tool was instrumented to recordcommand invocations so that the path a partici-pant took to answer a question could be recre-ated, and the participant?s adherence to protocolcould be verified.
Furthermore, the find function(Ctrl-F) was disabled to prevent users from per-forming ad hoc searches of the documents in-stead of using the extracted data.The order of question sets and the order of da-tabase conditions were counterbalanced acrossparticipants, so that, for every four participants,every question set and database appeared once inevery ordinal position, and every question setwas paired once with every database.
Thisavoided carryover effects from question order.2.6 Data CollectedBased on the initial results from Experiment 1, a70% target effectiveness threshold was identi-fied to occur between the 33 and 67 databaseblends.
To refine and verify this finding, Ex-periment 2 examined the 0, 16.5, 33, and 50 da-tabase blends.
Experiment 3 examined the 33,41, 50, and 58 database blends.AnswerPad collected participant-provided an-swers to questions and the corresponding cita-tions.
In addition, AnswerPad recorded the timespent answering the questions.
A limit of fiveminutes was imposed based on pilot study re-sults.
The browsing tool logged commands in-voked while the user searched the fact-base forquestion answers.
Questions were manuallyscored based on the answers in the providedcorpus.
No partial credit was given.
The maxi-mum score, for each database condition, was 16,for a total maximum score of 64.3 ResultsFigure 4 shows the question answer scoresand times for each of the three individual ex-periments, and for Experiments 1 and 2 com-bined.
Database quality affects both task speed(downward-sloping line) and task accuracy (up-ward-sloping line) in the expected direction.
Alogistic fit, as for a binary-response curve, wasused to fit the relationship between blend per-centage and accuracy in each experiment.
Thelogistic fit Goodman-Theil quasi-R2 was .9973for Experiment 1, .9594 for Experiment 2, .8936for Experiment 3, and .9959 for Experiments 1and 2 combined.For the target accuracy of 70%, the 95% con-fidence interval for the required blend is (35,56)around a predicted 46% blend for Experiment 1,and (41,56) around a predicted 49% for Experi-ments 1 and 2 combined.657Experiment 1 Performance and Time vs DB Blend56687582202174152140505560657075808590951000.0 16.7 33.3 50.0 66.7 83.3 100.0DB Blend (% Human)Performance(%Correct)100125150175200225Time(seconds)Experiment 2 Performance and Time vs DB Blend52616470210187183166505560657075808590951000.0 16.7 33.3 50.0 66.7 83.3 100.0DB Blend (% Human)Performance(%Correct)100125150175200225Time(seconds)Experiment 3 Performance and Time vs DB Blend616368 67173171164178505560657075808590951000.0 16.7 33.3 50.0 66.7 83.3 100.0DB Blend (% Human)Performance(%Correct)100125150175200Time(seconds)Experiments  1 & 2 Performance and Time vs DB Blend546166707582206187179166152140505560657075808590951000.0 16.7 33.3 50.0 66.7 83.3 100.0DB Blend (% Human)Performance(%Correct)100125150175200225Time(seconds)% Correct Blend Lower Bound Logistic Fit for BlendBlend Upper Bound TimeFigure 4 QA Performance (upward-sloping) and QA Time (downward-sloping) vs.
Extraction BlendError Bars are Plus/Minus Standard Error of Mean (SEM) Within Each BlendUpper and Lower Bounds Are Approximate 95% Confidence Intervals Based on the Logistic FitFor the Blend (X) to Produce a Given Performance (Y)(Read these bounds horizontally, as bounds on X, with the upper bound to the right of the lower bound.
)The downward-sloping line in each graphdisplays the average time to answer a questionas a function of the extraction blend.
For thisanalysis we used strict time, the time it took theparticipant to answer the question if he or sheanswered correctly, or the full 5 minutes allowedfor any incorrectly answered question.
This ad-dresses the situation where a person quickly an-swers all of the questions incorrectly.
Theaverage question-answer time drops 32% as onemoves from a machine generated extraction da-tabase to a human generated database.
Astraight-line fit to the Experiment 1 and 2 com-bined data predicts a drop of 6.5 seconds as thehuman proportion of the database increases by10 percentage points.A one-way repeated measures analysis ofvariance (ANOVA) was performed for Experi-ment 1 (0-33-67-100), Experiment 2 (0-16.5-33-50), and Experiment 3 (33-41-50-58).
Table 5summarizes the results.
In Experiments 1 and 2the impact of database quality on QA perform-ance and on QA time were highly significant (P< 0.0001), but not for the narrower range of da-tabases in Experiment 3.
Other ANOVAsshowed that the impact of trial order and ques-tion set on QA performance were both non-significant (P > 0.05).658Experiment QAPerformanceStrict Time1 F(3,57) = 30.98,P < .0001F(3, 57) = 28.36P < .00012 F(3,57)= 19.32,P < .0001F(3, 57) =  15.37,P < .00013 F(3,69)= 2.023,P = .1187F(3,69)= 1.053,P = .3747Table 5: ANOVA Analyses for QA PerformanceExpt.
1 used db blends of 0, 33, 67, and 100%Expt.
2 used db blends of 0, 16.5, 33, and 50%Expt.
3 used db blends of 33, 41, 50, and 58%In Experiment 1, Newman-Keuls contrastsindicate that the 0, 33, 67, and 100 databasesdiffer significantly (P < .05) on their impact onQA quality.
For Experiment 2, however, the16.5 and 33 database qualities were not shown tobe different, nor were any of the database blendsin Experiment 3.
The data suggest that nearlyhalf the improvement in QA quality from 0 to100 occurs by the 33 database blend, and morethan half the improvement in QA quality from 0to 50 occurs by the 16.5 blend: a little ?human?goes a long way.
Experiment 3 suggests thatsmall differences in data blends make no practi-cal difference in the results.
Alternatively, theremight be real differences that are small enoughsuch that a larger number of participants wouldbe required to detect them.
Experiment 3 alsohad two participants with atypical patterns ofQA against blend, which might account for thefailure to detect a difference between the 33 and50 or 58 blends as suggested by the results fromExperiment 2.
Furthermore, larger experimentscould reveal whether the atypical participantswere representatives of a subpopulation, or sim-ply outliers.
Bearing the possibility of outliers inmind, we used the combination of Experiments1 and 2 for the combined logistic analysis.4 ConclusionsWe presented a methodology for assessing in-formation extraction effectiveness using an ex-trinsic study.
In addition, we demonstrated howa novel database blending (merging) strategyallows interpolating extraction quality fromautomated performance up through human accu-racy, thereby decreasing the resources requiredto conduct effectiveness evaluations.Experiments showed QA accuracy and speedincreased with higher IE performance, and thatthe database blend percentage was a good proxyfor ACE value scores.
We emphasize that thestudy was not to show that IE supports QA bet-ter than other technologies, rather to isolate util-ity gains due to IE performance improvements.QA performance was plotted against human-machine IE blend and, for example, 70% QAperformance was achieved with a database blendbetween 41% and 46% machine extraction.
Thiscorresponded to entity and relationship valuescores of roughly 74 and 47 respectively.The logistic dose-response model provided agood fit and allowed for computation of confi-dence bounds for the IE associated with a par-ticular level of performance.
The constraintsimposed by AnswerPad and FactBrowser en-sured that world knowledge was neutralized, andthe repeated-measures design (using participantsas their own controls across multiple levels ofdatabase quality) excluded inter-participant vari-ability from experimental error, increasing theability to detect differences with relatively smallsample sizes.AcknowledgementThis material is based upon work supported inpart by the Department of the Interior underContract No.
NBCHC030014.
Any opinions,findings and conclusions or recommendationsexpressed in this material are those of the au-thors and do not necessarily reflect the views ofthe Department of the Interior.ReferencesS.
Miller, H. Fox, L. Ramshaw, and R. Weischedel,"A Novel Use of Statistical Parsing to Extract In-formation from Text", in Proceedings of 1st Meet-ing of the North American Chapter of the ACL,Seattle, WA., pp.226-233, 2000.S.
Miller, S. Bratus, L. Ramshaw, R. Weischedel, andA.
Zamanian.
"FactBrowser Demonstration", Hu-man Language Technology Conference, SanDiego, 2001.D.
Jones and E. Walton, ?Measuring the Utility ofHuman Language Technology for IntelligenceAnalysis,?
2005 International Conference on Intel-ligence Applications, McLean, VA May, 2005.659
