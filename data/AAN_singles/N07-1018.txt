Proceedings of NAACL HLT 2007, pages 139?146,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsBayesian Inference for PCFGs via Markov chain Monte CarloMark JohnsonCognitive and Linguistic SciencesBrown UniversityMark Johnson@brown.eduThomas L. GriffithsDepartment of PsychologyUniversity of California, BerkeleyTom Griffiths@berkeley.eduSharon GoldwaterDepartment of LinguisticsStanford Universitysgwater@stanford.eduAbstractThis paper presents two Markov chainMonte Carlo (MCMC) algorithms forBayesian inference of probabilistic con-text free grammars (PCFGs) from ter-minal strings, providing an alternativeto maximum-likelihood estimation usingthe Inside-Outside algorithm.
We illus-trate these methods by estimating a sparsegrammar describing the morphology ofthe Bantu language Sesotho, demonstrat-ing that with suitable priors Bayesiantechniques can infer linguistic structurein situations where maximum likelihoodmethods such as the Inside-Outside algo-rithm only produce a trivial grammar.1 IntroductionThe standard methods for inferring the parameters ofprobabilistic models in computational linguistics arebased on the principle of maximum-likelihood esti-mation; for example, the parameters of ProbabilisticContext-Free Grammars (PCFGs) are typically es-timated from strings of terminals using the Inside-Outside (IO) algorithm, an instance of the Ex-pectation Maximization (EM) procedure (Lari andYoung, 1990).
However, much recent work in ma-chine learning and statistics has turned away frommaximum-likelihood in favor of Bayesian methods,and there is increasing interest in Bayesian methodsin computational linguistics as well (Finkel et al,2006).
This paper presents two Markov chain MonteCarlo (MCMC) algorithms for inferring PCFGs andtheir parses from strings alone.
These can be viewedas Bayesian alternatives to the IO algorithm.The goal of Bayesian inference is to compute adistribution over plausible parameter values.
This?posterior?
distribution is obtained by combining thelikelihood with a ?prior?
distribution P(?)
over pa-rameter values ?.
In the case of PCFG inference ?
isthe vector of rule probabilities, and the prior mightassert a preference for a sparse grammar (see be-low).
The posterior probability of each value of ?is given by Bayes?
rule:P(?|D) ?
P(D|?)P(?).
(1)In principle Equation 1 defines the posterior prob-ability of any value of ?, but computing this maynot be tractable analytically or numerically.
For thisreason a variety of methods have been developed tosupport approximate Bayesian inference.
One of themost popular methods is Markov chain Monte Carlo(MCMC), in which a Markov chain is used to sam-ple from the posterior distribution.This paper presents two new MCMC algorithmsfor inferring the posterior distribution over parsesand rule probabilities given a corpus of strings.
Thefirst algorithm is a component-wise Gibbs samplerwhich is very similar in spirit to the EM algo-rithm, drawing parse trees conditioned on the cur-rent parameter values and then sampling the param-eters conditioned on the current set of parse trees.The second algorithm is a component-wise Hastingssampler that ?collapses?
the probabilistic model, in-tegrating over the rule probabilities of the PCFG,with the goal of speeding convergence.
Both algo-139rithms use an efficient dynamic programming tech-nique to sample parse trees.Given their usefulness in other disciplines, webelieve that Bayesian methods like these are likelyto be of general utility in computational linguis-tics as well.
As a simple illustrative example, weuse these methods to infer morphological parses forverbs from Sesotho, a southern Bantu language withagglutinating morphology.
Our results illustrate thatBayesian inference using a prior that favors sparsitycan produce linguistically reasonable analyses in sit-uations in which EM does not.The rest of this paper is structured as follows.The next section introduces the background for ourpaper, summarizing the key ideas behind PCFGs,Bayesian inference, and MCMC.
Section 3 intro-duces our first MCMC algorithm, a Gibbs samplerfor PCFGs.
Section 4 describes an algorithm forsampling trees from the distribution over trees de-fined by a PCFG.
Section 5 shows how to integrateout the rule weight parameters ?
in a PCFG, allow-ing us to sample directly from the posterior distribu-tion over parses for a corpus of strings.
Finally, Sec-tion 6 illustrates these methods in learning Sesothomorphology.2 Background2.1 Probabilistic context-free grammarsLet G = (T,N, S,R) be a Context-Free Grammarin Chomsky normal form with no useless produc-tions, where T is a finite set of terminal symbols, Nis a finite set of nonterminal symbols (disjoint fromT ), S ?
N is a distinguished nonterminal called thestart symbol, and R is a finite set of productions ofthe form A ?
BC or A ?
w, where A,B,C ?
Nand w ?
T .
In what follows we use ?
as a variableranging over (N ?N) ?
T .A Probabilistic Context-Free Grammar (G, ?)
isa pair consisting of a context-free grammar G anda real-valued vector ?
of length |R| indexed by pro-ductions, where ?A??
is the production probabilityassociated with the production A ?
?
?
R. Werequire that ?A??
?
0 and that for all nonterminalsA ?
N , ?A??
?R ?A??
= 1.A PCFG (G, ?)
defines a probability distributionover trees t as follows:PG(t|?)
=?r?R?fr(t)rwhere t is generated by G and fr(t) is the numberof times the production r = A ?
?
?
R is usedin the derivation of t. If G does not generate t letPG(t|?)
= 0.
The yield y(t) of a parse tree t isthe sequence of terminals labeling its leaves.
Theprobability of a string w ?
T+ of terminals is thesum of the probability of all trees with yield w, i.e.:PG(w|?)
=?t:y(t)=wPG(t|?
).2.2 Bayesian inference for PCFGsGiven a corpus of strings w = (w1, .
.
.
, wn), whereeach wi is a string of terminals generated by a knownCFG G, we would like to be able to infer the pro-duction probabilities ?
that best describe that corpus.Taking w to be our data, we can apply Bayes?
rule(Equation 1) to obtain:P(?|w) ?
PG(w|?)P(?
), wherePG(w|?)
=n?i=1PG(wi|?
).Using t to denote a sequence of parse trees for w,we can compute the joint posterior distribution overt and ?, and then marginalize over t, with P(?|w) =?t P(t, ?|w).
The joint posterior distribution on tand ?
is given by:P(t, ?|w) ?
P(w|t)P(t|?)P(?
)=( n?i=1P(wi|ti)P(ti|?))P(?
)with P(wi|ti) = 1 if y(ti) = wi, and 0 otherwise.2.3 Dirichlet priorsThe first step towards computing the posterior dis-tribution is to define a prior on ?.
We take P(?)
tobe a product of Dirichlet distributions, with one dis-tribution for each non-terminal A ?
N .
The prioris parameterized by a positive real valued vector ?indexed by productions R, so each production prob-ability ?A??
has a corresponding Dirichlet param-eter ?A??.
Let RA be the set of productions in R140with left-hand side A, and let ?A and ?A refer tothe component subvectors of ?
and ?
respectivelyindexed by productions in RA.
The Dirichlet priorPD(?|?)
is:PD(?|?)
=?A?NPD(?A|?A), wherePD(?A|?A) =1C(?A)?r?RA?
?r?1r andC(?A) =?r?RA ?(?r)?
(?r?RA ?r)(2)where ?
is the generalized factorial function andC(?)
is a normalization constant that does not de-pend on ?A.Dirichlet priors are useful because they are con-jugate to the distribution over trees defined by aPCFG.
This means that the posterior distributionon ?
given a set of parse trees, P(?|t, ?
), is also aDirichlet distribution.
Applying Bayes?
rule,PG(?|t, ?)
?
PG(t|?)
PD(?|?)?(?r?R?fr(t)r)(?r?R?
?r?1r)=?r?R?fr(t)+?r?1rwhich is a Dirichlet distribution with parametersf(t) + ?, where f(t) is the vector of productioncounts in t indexed by r ?
R. We can thus write:PG(?|t, ?)
= PD(?|f(t) + ?
)which makes it clear that the production counts com-bine directly with the parameters of the prior.2.4 Markov chain Monte CarloHaving defined a prior on ?, the posterior distribu-tion over t and ?
is fully determined by a corpusw.
Unfortunately, computing the posterior probabil-ity of even a single choice of t and ?
is intractable,as evaluating the normalizing constant for this dis-tribution requires summing over all possible parsesfor the entire corpus and all sets of production prob-abilities.
Nonetheless, it is possible to define al-gorithms that sample from this distribution usingMarkov chain Monte Carlo (MCMC).MCMC algorithms construct a Markov chainwhose states s ?
S are the objects we wish to sam-ple.
The state space S is typically astronomicallylarge ?
in our case, the state space includes all pos-sible parses of the entire training corpus w ?
andthe transition probabilities P(s?|s) are specified via ascheme guaranteed to converge to the desired distri-bution ?
(s) (in our case, the posterior distribution).We ?run?
the Markov chain (i.e., starting in initialstate s0, sample a state s1 from P(s?|s0), then sam-ple state s2 from P(s?|s1), and so on), with the prob-ability that the Markov chain is in a particular state,P(si), converging to ?
(si) as i ?
?.After the chain has run long enough for it to ap-proach its stationary distribution, the expectationE?
[f ] of any function f(s) of the state s will beapproximated by the average of that function overthe set of sample states produced by the algorithm.For example, in our case, given samples (ti, ?i) fori = 1, .
.
.
, ?
produced by an MCMC algorithm, wecan estimate ?
asE?[?]
?1??
?i=1?iThe remainder of this paper presents two MCMCalgorithms for PCFGs.
Both algorithms proceed bysetting the initial state of the Markov chain to a guessfor (t, ?)
and then sampling successive states usinga particular transition matrix.
The key difference be-twen the two algorithms is the form of the transitionmatrix they assume.3 A Gibbs sampler for P(t, ?|w, ?
)The Gibbs sampler (Geman and Geman, 1984) isone of the simplest MCMC methods, in which tran-sitions between states of the Markov chain resultfrom sampling each component of the state condi-tioned on the current value of all other variables.
Inour case, this means alternating between samplingfrom two distributions:P(t|?,w, ?)
=n?i=1P(ti|wi, ?
), andP(?|t,w, ?)
= PD(?|f(t) + ?
)=?A?NPD(?A|fA(t) + ?A).Thus every two steps we generate a new sample oft and ?.
This alternation between parsing and up-dating ?
is reminiscent of the EM algorithm, with141tit1 tnw1 wi wn?Aj.
.
.
?A1 .
.
.
?A|N|?A1 .
.
.. .
.
?Aj ?A|N|.
.
.. .
.. .
.. .
.Figure 1: A Bayes net representation of dependen-cies among the variables in a PCFG.the Expectation step replaced by sampling t and theMaximization step replaced by sampling ?.The dependencies among variables in a PCFG aredepicted graphically in Figure 1, which makes clearthat the Gibbs sampler is highly parallelizable (justlike the EM algorithm).
Specifically, the parses tiare independent given ?
and so can be sampled inparallel from the following distribution as describedin the next section.PG(ti|wi, ?)
=PG(ti|?)PG(wi|?
)We make use of the fact that the posterior is aproduct of independent Dirichlet distributions in or-der to sample ?
from PD(?|t, ?).
The productionprobabilities ?A for each nonterminal A ?
N aresampled from a Dirchlet distibution with parameters?
?A = fA(t) + ?A.
There are several methods forsampling ?
= (?1, .
.
.
, ?m) from a Dirichlet distri-bution with parameters ?
= (?1, .
.
.
, ?m), with thesimplest being sampling xj from a Gamma(?j) dis-tribution for j = 1, .
.
.
,m and then setting ?j =xj/?mk=1 xk (Gentle, 2003).4 Efficiently sampling from P(t|w, ?
)This section completes the description of the Gibbssampler for (t, ?)
by describing a dynamic program-ming algorithm for sampling trees from the set ofparses for a string generated by a PCFG.
This al-gorithm appears fairly widely known: it was de-scribed by Goodman (1998) and Finkel et al(2006)and used by Ding et al(2005), and is very simi-lar to other dynamic programming algorithms forCFGs, so we only summarize it here.
The algo-rithm consists of two steps.
The first step con-structs a standard ?inside?
table or chart, as used inthe Inside-Outside algorithm for PCFGs (Lari andYoung, 1990).
The second step involves a recursionfrom larger to smaller strings, sampling from theproductions that expand each string and construct-ing the corresponding tree in a top-down fashion.In this section we take w to be a string of terminalsymbols w = (w1, .
.
.
, wn) where each wi ?
T ,and define wi,k = (wi+1, .
.
.
, wk) (i.e., the sub-string from wi+1 up to wk).
Further, let GA =(T,N,A,R), i.e., a CFG just like G except that thestart symbol has been replaced with A, so, PGA(t|?
)is the probability of a tree t whose root node is la-beled A and PGA(w|?)
is the sum of the probabili-ties of all trees whose root nodes are labeled A withyield w.The Inside algorithm takes as input a PCFG(G, ?)
and a string w = w0,n and constructs a ta-ble with entries pA,i,k for each A ?
N and 0 ?i < k ?
n, where pA,i,k = PGA(wi,k|?
), i.e., theprobability of A rewriting to wi,k.
The table entriesare recursively defined below, and computed by enu-merating all feasible i, k and A in any order such thatall smaller values of k?i are enumerated before anylarger values.pA,k?1,k = ?A?wkpA,i,k =?A?B C?R?i<j<k?A?B C pB,i,j pC,j,kfor all A,B,C ?
N and 0 ?
i < j < k ?
n. At theend of the Inside algorithm, PG(w|?)
= pS,0,n.The second step of the sampling algorithm usesthe function SAMPLE, which returns a sample fromPG(t|w, ?)
given the PCFG (G, ?)
and the insidetable pA,i,k.
SAMPLE takes as arguments a non-terminal A ?
N and a pair of string positions0 ?
i < k ?
n and returns a tree drawn fromPGA(t|wi,k, ?).
It functions in a top-down fashion,selecting the production A ?
BC to expand the A,and then recursively calling itself to expand B andC respectively.function SAMPLE(A, i, k) :if k ?
i = 1 then return TREE(A,wk)(j,B,C) = MULTI(A, i, k)return TREE(A, SAMPLE(B, i, j), SAMPLE(C, j, k))In this pseudo-code, TREE is a function that con-structs unary or binary tree nodes respectively, and142MULTI is a function that produces samples froma multinomial distribution over the possible ?split?positions j and nonterminal children B and C ,where:P(j,B,C) = ?A?BC PGB (wi,j|?)
PGC (wj,k|?)PGA(wi,k|?
)5 A Hastings sampler for P(t|w, ?
)The Gibbs sampler described in Section 3 hasthe disadvantage that each sample of ?
re-quires reparsing the training corpus w. Inthis section, we describe a component-wiseHastings algorithm for sampling directly fromP(t|w, ?
), marginalizing over the produc-tion probabilities ?.
Transitions betweenstates are produced by sampling parses ti fromP(ti|wi, t?i, ?)
for each string wi in turn, wheret?i = (t1, .
.
.
, ti?1, ti+1, .
.
.
, tn) is the current setof parses for w?i = (w1, .
.
.
, wi?1, wi+1, .
.
.
, wn).Marginalizing over ?
effectively means that theproduction probabilities are updated after eachsentence is parsed, so it is reasonable to expectthat this algorithm will converge faster than theGibbs sampler described earlier.
While the samplerdoes not explicitly provide samples of ?, the resultsoutlined in Sections 2.3 and 3 can be used to samplethe posterior distribution over ?
for each sample oft if required.Let PD(?|?)
be a Dirichlet product prior, and let?
be the probability simplex for ?.
Then by inte-grating over the posterior Dirichlet distributions wehave:P(t|?)
=??PG(t|?)PD(?|?
)d?=?A?NC(?A + fA(t))C(?A)(3)where C was defined in Equation 2.
Because weare marginalizing over ?, the trees ti become depen-dent upon one another.
Intuitively, this is becausewi may provide information about ?
that influenceshow some other string wj should be parsed.We can use Equation 3 to compute the conditionalprobability P(ti|t?i, ?)
as follows:P(ti|t?i, ?)
=P(t|?)P(t?i|?
)=?A?NC(?A + fA(t))C(?A + fA(t?i))Now, if we could sample fromP(ti|wi, t?i, ?)
=P(wi|ti)P(ti|t?i, ?
)P(wi|t?i, ?
)we could construct a Gibbs sampler whose stateswere the parse trees t. Unfortunately, we don?t evenknow if there is an efficient algorithm for calculat-ing P(wi|t?i, ?
), let alne an efficient sampling al-gorithm for this distribution.Fortunately, this difficulty is not fatal.
A Hast-ings sampler for a probability distribution ?
(s) isan MCMC algorithm that makes use of a proposaldistribution Q(s?|s) from which it draws samples,and uses an acceptance/rejection scheme to define atransition kernel with the desired distribution ?
(s).Specifically, given the current state s, a sample s?
6=s drawn from Q(s?|s) is accepted as the next statewith probabilityA(s, s?)
= min{1, ?(s?)Q(s|s?)?
(s)Q(s?|s)}and with probability 1 ?A(s, s?)
the proposal is re-jected and the next state is the current state s.We use a component-wise proposal distribution,generating new proposed values for ti, where i ischosen at random.
Our proposal distribution is theposterior distribution over parse trees generated bythe PCFG with grammar G and production proba-bilities ?
?, where ??
is chosen based on the currentt?i as described below.
Each step of our Hastingssampler is as follows.
First, we compute ??
fromt?i as described below.
Then we sample t?i fromP(ti|wi, ??)
using the algorithm described in Sec-tion 4.
Finally, we accept the proposal t?i given theold parse ti for wi with probability:A(ti, t?i) = min{1, P(t?i|wi, t?i, ?
)P(ti|wi, ??
)P(ti|wi, t?i, ?
)P(t?i|wi, ??
)}= min{1, P(t?i|t?i, ?
)P(ti|wi, ??
)P(ti|t?i, ?
)P(t?i|wi, ??
)}The key advantage of the Hastings sampler over theGibbs sampler here is that because the acceptanceprobability is a ratio of probabilities, the difficult to143compute P(wi|t?i, ?)
is a common factor of boththe numerator and denominator, and hence is not re-quired.
The P (wi|ti) term also disappears, being 1for both the numerator and the denominator sinceour proposal distribution can only generate trees forwhich wi is the yield.All that remains is to specify the production prob-abilities ??
of the proposal distribution P(t?i|wi, ??
).While the acceptance rule used in the Hastingsalgorithm ensures that it produces samples fromP(ti|wi, t?i, ?)
with any proposal grammar ??
inwhich all productions have nonzero probability, thealgorithm is more efficient (i.e., fewer proposals arerejected) if the proposal distribution is close to thedistribution to be sampled.Given the observations above about the corre-spondence between terms in P(ti|t?i, ?)
and therelative frequency of the corresponding productionsin t?i, we set ??
to the expected value E[?|t?i, ?]
of?
given t?i and ?
as follows:?
?r =fr(t?i) + ?r?r?
?RA fr?
(t?i) + ?r?6 Inferring sparse grammarsAs stated in the introduction, the primary contribu-tion of this paper is introducing MCMC methodsfor Bayesian inference to computational linguistics.Bayesian inference using MCMC is a technique ofgeneric utility, much like Expectation-Maximizationand other general inference techniques, and we be-lieve that it belongs in every computational linguist?stoolbox alongside these other techniques.Inferring a PCFG to describe the syntac-tic structure of a natural language is an obvi-ous application of grammar inference techniques,and it is well-known that PCFG inference us-ing maximum-likelihood techniques such as theInside-Outside (IO) algorithm, a dynamic program-ming Expectation-Maximization (EM) algorithm forPCFGs, performs extremely poorly on such tasks.We have applied the Bayesian MCMC methods de-scribed here to such problems and obtain resultsvery similar to those produced using IO.
We be-lieve that the primary reason why both IO and theBayesian methods perform so poorly on this taskis that simple PCFGs are not accurate models ofEnglish syntactic structure.
We know that PCFGs?
= (0.1, 1.0)?
= (0.5, 1.0)?
= (1.0, 1.0)Binomial parameter ?1P(?1|?
)10.80.60.40.20543210Figure 2: A Dirichlet prior ?
on a binomial parame-ter ?1.
As ?1 ?
0, P(?1|?)
is increasingly concen-trated around 0.that represent only major phrasal categories ignorea wide variety of lexical and syntactic dependen-cies in natural language.
State-of-the-art systemsfor unsupervised syntactic structure induction sys-tem uses models that are very different to these kindsof PCFGs (Klein and Manning, 2004; Smith andEisner, 2006).1Our goal in this section is modest: we aim merelyto provide an illustrative example of Bayesian infer-ence using MCMC.
As Figure 2 shows, when theDirichlet prior parameter ?r approaches 0 the priorprobability PD(?r|?)
becomes increasingly concen-trated around 0.
This ability to bias the samplertoward sparse grammars (i.e., grammars in whichmany productions have probabilities close to 0) isuseful when we attempt to identify relevant produc-tions from a much larger set of possible productionsvia parameter estimation.The Bantu language Sesotho is a richly agglutina-tive language, in which verbs consist of a sequenceof morphemes, including optional Subject Markers(SM), Tense (T), Object Markers (OM), Mood (M)and derivational affixes as well as the obligatoryVerb stem (V), as shown in the following example:reSM-aT-diOM-bonV-aM?We see them?1It is easy to demonstrate that the poor quality of the PCFGmodels is the cause of these problems rather than search or otheralgorithmic issues.
If one initializes either the IO or Bayesianestimation procedures with treebank parses and then runs theprocedure using the yields alone, the accuracy of the parses uni-formly decreases while the (posterior) likelihood uniformly in-creases with each iteration, demonstrating that improving the(posterior) likelihood of such models does not improve parseaccuracy.144We used an implementation of the Hastings samplerdescribed in Section 5 to infer morphological parsest for a corpus w of 2,283 unsegmented Sesothoverb types extracted from the Sesotho corpus avail-able from CHILDES (MacWhinney and Snow, 1985;Demuth, 1992).
We chose this corpus because thewords have been morphologically segmented manu-ally, making it possible for us to evaluate the mor-phological parses produced by our system.
We con-structed a CFG G containing the following produc-tionsWord ?
VWord ?
V MWord ?
SM V MWord ?
SM T V MWord ?
SM T OM V Mtogether with productions expanding the pretermi-nals SM,T,OM,V and M to each of the 16,350 dis-tinct substrings occuring anywhere in the corpus,producting a grammar with 81,755 productions inall.
In effect, G encodes the basic morphologi-cal structure of the Sesotho verb (ignoring factorssuch as derivation morphology and irregular forms),but provides no information about the phonologicalidentity of the morphemes.Note that G actually generates a finite language.However, G parameterizes the probability distribu-tion over the strings it generates in a manner thatwould be difficult to succintly characterize exceptin terms of the productions given above.
Moreover,with approximately 20 times more productions thantraining strings, each string is highly ambiguous andestimation is highly underconstrained, so it providesan excellent test-bed for sparse priors.We estimated the morphological parses t in twoways.
First, we ran the IO algorithm initializedwith a uniform initial estimate ?0 for ?
to producean estimate of the MLE ?
?, and then computed theViterbi parses t?
of the training corpus w with respectto the PCFG (G, ??).
Second, we ran the Hastingssampler initialized with trees sampled from (G, ?0)with several different values for the parameters ofthe prior.
We experimented with a number of tech-niques for speeding convergence of both the IO andHastings algorithms, and two of these were particu-larly effective on this problem.
Annealing, i.e., us-ing P(t|w)1/?
in place of P(t|w) where ?
is a ?tem-perature?
parameter starting around 5 and slowly ad-justed toward 1, sped the convergence of both algo-rithms.
We ran both algorithms for several thousanditerations over the corpus, and both seemed to con-verge fairly quickly once ?
was set to 1.
?Jittering?the initial estimate of ?
used in the IO algorithm alsosped its convergence.The IO algorithm converges to a solution where?Word?
V = 1, and every string w ?
w is analysedas a single morpheme V. (In fact, in this grammarP(wi|?)
is the empirical probability of wi, and it iseasy to prove that this ?
is the MLE).The samples t produced by the Hastings algo-rithm depend on the parameters of the Dirichletprior.
We set ?r to a single value ?
for all pro-ductions r. We found that for ?
> 10?2 the sam-ples produced by the Hastings algorithm were thesame trivial analyses as those produced by the IOalgorithm, but as ?
was reduced below this t be-gan to exhibit nontrivial structure.
We evaluatedthe quality of the segmentations in the morpholog-ical analyses t in terms of unlabeled precision, re-call, f-score and exact match (the fraction of wordscorrectly segmented into morphemes; we ignoredmorpheme labels because the manual morphologicalanalyses contain many morpheme labels that we didnot include in G).
Figure 3 contains a plot of howthese quantities vary with ?
; obtaining an f-score of0.75 and an exact word match accuracy of 0.54 at?
= 10?5 (the corresponding values for the MLE ?
?are both 0).
Note that we obtained good results as ?was varied over several orders of magnitude, so theactual value of ?
is not critical.
Thus in this appli-cation the ability to prefer sparse grammars enablesus to find linguistically meaningful analyses.
Thisability to find linguistically meaningful structure isrelatively rare in our experience with unsupervisedPCFG induction.We also experimented with a version of IO modi-fied to perform Bayesian MAP estimation, where theMaximization step of the IO procedure is replacedwith Bayesian inference using a Dirichlet prior, i.e.,where the rule probabilities ?
(k) at iteration k are es-timated using:?
(k)r ?
max(0,E[fr|w, ?
(k?1)] + ??
1).Clearly such an approach is very closely related tothe Bayesian procedures presented in this article,145ExactRecallPrecisionF-scoreDirichlet prior parameter ?r1 0.01 1e-04 1e-06 1e-08 1e-1010.750.50.250Figure 3: Accuracy of morphological segmentationsof Sesotho verbs proposed by the Hastings algo-rithms as a function of Dirichlet prior parameter?.
F-score, precision and recall are unlabeled mor-pheme scores, while Exact is the fraction of wordscorrectly segmented.and in some circumstances this may be a usefulestimator.
However, in our experiments with theSesotho data above we found that for the small val-ues of ?
necessary to obtain a sparse solution,theexpected rule count E[fr] for many rules r was lessthan 1??.
Thus on the next iteration ?r = 0, result-ing in there being no parse whatsoever for many ofthe strings in the training data.
Variational Bayesiantechniques offer a systematic way of dealing withthese problems, but we leave this for further work.7 ConclusionThis paper has described basic algorithms for per-forming Bayesian inference over PCFGs given ter-minal strings.
We presented two Markov chainMonte Carlo algorithms (a Gibbs and a Hastingssampling algorithm) for sampling from the posteriordistribution over parse trees given a corpus of theiryields and a Dirichlet product prior over the produc-tion probabilities.
As a component of these algo-rithms we described an efficient dynamic program-ming algorithm for sampling trees from a PCFGwhich is useful in its own right.
We used thesesampling algorithms to infer morphological analy-ses of Sesotho verbs given their strings (a task onwhich the standard Maximum Likelihood estimatorreturns a trivial and linguistically uninteresting so-lution), achieving 0.75 unlabeled morpheme f-scoreand 0.54 exact word match accuracy.
Thus thisis one of the few cases we are aware of in whicha PCFG estimation procedure returns linguisticallymeaningful structure.
We attribute this to the abilityof the Bayesian prior to prefer sparse grammars.We expect that these algorithms will be of inter-est to the computational linguistics community bothbecause a Bayesian approach to PCFG estimation ismore flexible than the Maximum Likelihood meth-ods that currently dominate the field (c.f., the useof a prior as a bias towards sparse solutions), andbecause these techniques provide essential buildingblocks for more complex models.ReferencesKatherine Demuth.
1992.
Acquisition of Sesotho.
In DanSlobin, editor, The Cross-Linguistic Study of Language Ac-quisition, volume 3, pages 557?638.
Lawrence Erlbaum As-sociates, Hillsdale, N.J.Ye Ding, Chi Yu Chan, and Charles E. Lawrence.
2005.
RNAsecondary structure prediction by centroids in a Boltzmannweighted ensemble.
RNA, 11:1157?1166.Jenny Rose Finkel, Christopher D. Manning, and Andrew Y.Ng.
2006.
Solving the problem of cascading errors:Approximate Bayesian inference for linguistic annotationpipelines.
In Proceedings of the 2006 Conference on Empir-ical Methods in Natural Language Processing, pages 618?626, Sydney, Australia.
Association for Computational Lin-guistics.Stuart Geman and Donald Geman.
1984.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration of images.IEEE Transactions on Pattern Analysis and Machine Intelli-gence, 6:721?741.James E. Gentle.
2003.
Random number generation and MonteCarlo methods.
Springer, New York, 2nd edition.Joshua Goodman.
1998.
Parsing inside-out.Ph.D.
thesis, Harvard University.
available fromhttp://research.microsoft.com/?joshuago/.Dan Klein and Chris Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency and con-stituency.
In Proceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics, pages 478?485.K.
Lari and S.J.
Young.
1990.
The estimation of StochasticContext-Free Grammars using the Inside-Outside algorithm.Computer Speech and Language, 4(35-56).Brian MacWhinney and Catherine Snow.
1985.
The child lan-guage data exchange system.
Journal of Child Language,12:271?296.Noah A. Smith and Jason Eisner.
2006.
Annealing structuralbias in multilingual weighted grammar induction.
In Pro-ceedings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 569?576, Sydney,Australia.
Association for Computational Linguistics.146
