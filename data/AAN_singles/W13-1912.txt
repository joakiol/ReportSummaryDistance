Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 98?101,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAdapting a parser to clinical text by simple pre-processing rulesMaria SkeppstedtDept.
of Computer and Systems Sciences (DSV)Stockholm University, Forum 100, 164 40 Kista, Swedenmariask@dsv.su.seAbstractSentence types typical to Swedish clini-cal text were extracted by comparing sen-tence part-of-speech tag sequences in clin-ical and in standard Swedish text.
Parsingsby a syntactic dependency parser, trainedon standard Swedish, were manually ana-lysed for the 33 sentence types most typ-ical to clinical text.
This analysis re-sulted in the identification of eight errortypes, and for two of these error types, pre-processing rules were constructed to im-prove the performance of the parser.
Forall but one of the ten sentence types af-fected by these two rules, the parsing wasimproved by pre-processing.1 IntroductionInput speed is often prioritised over completenessand grammatical correctness in health record nar-ratives.
This has the effect that lower results areachieved when parsers trained on standard text areapplied on clinical text (Hassel et al 2011).Syntactic annotations to use for training a parseron clinical text are, however, expensive (Albrightet al 2013) and treebanking large clinical corporais therefore not always an option for smaller lan-guages (Haverinen et al 2009).
There are studieson adaptation of standard parsers to the biomedicaldomain, focusing on overcoming difficulties dueto different vocabulary use (Candito et al 2011).How to overcome difficulties due to syntactic dif-ferences between standard and clinical languageis, however, less studied.
The aim of this studywas therefore to explore syntactic differences be-tween clinical language and standard language andto analyse errors made by the parser on sentencetypes typical to the clinical domain.
To exemplifyhow this knowledge can be used, two simple pre-processing rules for improving parser performanceon these typical sentences were developed.2 MethodTo find sentence types typical to the clinical do-main, a comparison to standard text was con-ducted.
The used clinical corpus was: free-textentries from assessment sections, thus mostly con-taining diagnostic reasoning, that were randomlyselected from the Stockholm EPR corpus1 (Dalia-nis et al 2009); and the used standard corpuswas: La?kartidningen (Kokkinakis, 2012), a jour-nal from the Swedish Medical Association.The comparison was carried out on part-of-speech sequences on a sentence level.
The part-of-speech tagger Granska (Carlberger and Kann,1999), having an accuracy of 92% on clinical text(Hassel et al 2011), was applied on both cor-pora, and the proportion of each sentence tag se-quence was calculated.
?Sentence tag sequence?refers here to the parts-of-speech corresponding toeach token in the sentence, combined to one unit,e.g.
?dt nn vb nn mad?
for the sentence ?The pa-tient has headache.?.
Pronouns, nouns and propernames were collapsed into one class, as they oftenplay the same role in the sentence, and as termsspecific to the clinical domain are tagged inconsis-tently as either nouns or proper names (Hassel etal., 2011).
As sentences from La?kartidningen notending with a full stop or a question mark are lesslikely to be full sentences, they were not included,in order to obtain a more contrasting corpus.A 95% confidence interval for the proportion ofeach sentence combination was computed usingthe Wilson score interval, and the difference be-tween the minimum frequency in the clinical cor-pus and the maximum frequency in the standardlanguage corpus was calculated.
Thereby, statis-tics for the minimum difference between the twodomains was achieved.1This research has been approved by the Regional EthicalReview Board in Stockholm (Etikpro?vningsna?mnden i Stock-holm), permission number 2012/834-31/5.98A total of 458 436 sentence types were foundin the clinical corpus.
Of these, there were 1 736types significantly more frequent in the clinicalcorpus than in the standard corpus, not havingoverlapping confidence interval for the propor-tions.
33 sentence types, to which 10% of the sen-tences in the corpus belonged, had more than 0.1percentage points difference between minimumfrequency in the clinical corpus and maximum fre-quency in the standard language corpus.
For eachof these 33 sentence types, 30 sentences were ran-domly extracted and the dependency parser Malt-Parser (Nivre et al 2009), pre-trained on Tal-banken (Nivre et al 2006) using the algorithmstacklazy (Nivre et al 2009), was applied to thesepart-of-speech tagged sentences.
Error categorieswere manually identified, using MaltEval (Nilssonand Nivre, 2008) for visualisation.Given the identified error categories, two pre-processing rules were constructed.
These werethen evaluated by applying the same pre-trainedparser model on pre-processed sentences as onoriginal sentences.
A manual analysis was per-formed on a subset of the sentences that were dif-ferently parsed after pre-processing.3 ResultsAlthough only one sentence type was a full sen-tence (nn vb pp nn mad), most sentences werecorrectly parsed.
Omitted words could be inferredfrom context, and therefore also the intended syn-tax.
Eight error types, to which most errors be-longed, were identified: 1) Abbreviated wordsending with a full stop interpreted as the last wordin a sentence, resulting in an incorrect sentencesplitting.
2) Abbreviations incorrectly labelled asnouns by Granska, resulting in sentences exclu-sively containing nouns.
3) Adjectives not recog-nised as such (often because they were abbrevi-ated), resulting in AT relations being labelled asDT relations.
4) A general adverbial relation in-correctly assigned an adverb of place or time rela-tion or vice versa.
5) The first word in compoundexpressions parsed as a determiner to the second.6) nn pp nn pp nn mad sentences for which apreposition had been incorrectly attributed.
7) Thesentence type nn jj (noun adjective), for whichmost evaluated sentences were incorrectly parsed.8) An omitted initial subject, resulting in the ob-ject incorrectly being parsed as the subject of thesentence.Pre-processing rules were constructed for errortypes 7) and 8).
As a verb in the middle of nnjj-sentences (in most cases copula) was left out,the first pre-processing rule added copula in themiddle of these sentences.
The second rule addedthe pronoun I as the first word in sentences startingwith a verb, as this was the most frequently leftout subject, along with the slightly less frequentomission, patient.
The rules were not applied onsentences ending with a question mark.10 (out of 33) sentence types were affected bythe two rules.
The proportion of those receiving adifferent parsing after pre-processing is shown inthe column Changed in Table 1.
A subset of thesesentences, for which the parsing was changed, wasmanually classified as either incorrect (= contain-ing at least one parsing or labelling error) or com-pletely correct.For sentences classified as incorrect, a moregranular comparison between the original and themodified parsing was carried out.
For these sen-tences, the difference in average unlabelled (UAS)and labelled (LAS) attachment score between thepre-processed and the original parsing was com-puted.
A positive value indicates that although thepre-processing resulted in some incorrectly parsedsentences, these sentences were improved by pre-processing.
The sentence types vb pp nn nn madand vb pp nn pp nn mad were thus slightly im-proved by the pre-processing, although they had alow proportion of correctly parsed sentences.A negative value for attachment score differ-ence, on the other hand, indicates that parsing forthe incorrectly parsed sentences was impaired bypre-processing.
As these figures only apply to sen-tences incorrectly parsed after pre-processing, thismeans that although e.g.
the type vb ab nn madhas negative UAS and LAS difference, this onlyapplies to the 3 sentences that were incorrectlyparsed by the pre-processed version.With one important exception, sentences modi-fied by pre-processing, were either a) given a com-pletely correct parsing and labelling in between64% and 100% of the cases, or were b) slightlyimproved by pre-processing.
A reasonable sim-plification in this case is that there can only beone correct parsing of a sentence, as althoughthere might be occurrences of syntactically am-biguous sentences, it is unlikely that their inter-pretation is not given by the context in the closeddomain of language used for diagnostic reasoning.99Given this simplification, this means that a sen-tence was transformed from an incorrectly parsedsentence to a correctly parsed sentence in 64% ormore of the cases, when pre-processing was ap-plied.
The difference in attachment score showsthat the parsing is not drastically degraded for therest of the sentences, although it mostly changedto a worse parsing.
The overall effect of apply-ing pre-processing is therefore positive.
Sentencesof the type vb nn pp nn mad is the important ex-ception to this positive effect, important as 54% ofthe sentences belonging to this type received a dif-ferent parsing after pre-processing and as 0.39%of the sentences in the corpus belong to this type.Only 61% of the pre-processed sentences of thistype had a correct unlabelled parsing and only32% had a correct labelled parsing.
Many of thesesentences were similar to Writes a prescription ofTrombyl, for which of Trombyl incorrectly is giventhe word write as the head after pre-processing.Almost all of the sentences of the type nn jj madwere correctly parsed when a copula was insertedbetween the noun and the adjective.
Of the othertypes of sentences that improved, many improvedby an incorrectly labelled subject relation beingchanged to an object relation.
There were, how-ever, also improvements because some adverbs ofplace and time were correctly labelled after thepre-processing rules had been applied.4 DiscussionEven if quantitative data is given in Table 1, thecore of this study has been to use a qualitative ap-proach: searching for different categories of errorsrather than determining accuracy figures, and in-vestigating whether pre-processing has a positiveeffect, rather than determining the final accuracy.The next step is to apply the findings of thisstudy for developing a small treebank of clinicaltext.
A possible method for facilitating syntacticannotation is to present pre-annotated data to theannotator (Brants and Plaehn, 2000) for correctionor for selection among several alternatives.
As theoverall effect of applying pre-processing were im-proved parsings, the pre-annotation could be car-ried out by applying a model trained on standardlanguage and improve it with the pre-processingrules investigated here.
The other identified errortypes also give suggestions of how to improve theparser, improvements that should be attempted be-fore using a parser trained on standard languagefor pre-annotation.
Error types 1), 2) and partly3) were due to abbreviations negatively affect-ing part-of-speech tagging and sentence splitting.Therefore, abbreviation expansion would be a pos-sible way of improving the parser.
That availablemedical vocabularies also could be useful is shownby error type 5), which was due to the parser fail-ing to recognise compound expressions.Of the sentences in the corpus, only 10%belonged to the analysed sentence types, andeven fewer were affected by the evaluated pre-processing rules.
It is, however, likely that the twodeveloped pre-processing rules have effects on allsentence types lacking a verb or starting with averb, thus effecting more sentence type than thoseincluded in this study.
This is worth studying,as is also syntactic differences for shorter part-of-speech sequences than sentence level sequences.Another possible method for domain adaptationwould be to adapt the training data to construct amodel more suitable for parsing clinical text.
In-stead of applying pre-processing, sentences in thetraining data could be modified to more closely re-semble sentences in clinical text, e.g.
by removingwords in the treebank corpus to achieve the incom-plete sentences typical to clinical text.
Differencesin vocabulary has not been included in this study,but methods from previous studies for bridgingdifferences in vocabulary between the general andmedical domain could also be applied for improv-ing parser performance.For supplementing a treebank to also includesentences typical to clinical text, some of themethods investigated here for extracting such sen-tence types, could be employed5 ConclusionSentence types typical to clinical text were ex-tracted, and eight categories of error types wereidentified.
For two of these error types, pre-processing rules were devised and evaluated.For four additional error types, techniques fortext-normalisation were suggested.
As the pre-processing rules had an overall positive effect onthe parser performance, it was suggested that amodel for syntactic pre-annotation of clinical textshould employ the evaluated text pre-processing.AcknowledgementsMany thanks to Joakim Nivre and to the four re-viewers for their many valuable comments.100Sentence # % # % Correct # Incorrect pp UAS(LAS)type In test Changed Manually unlabelled unlabelled difference am-classified (labelled) (labelled) ong incorrecta) vb nn mad 1181 30% 40 100 (100)% 0 (0)vb jj nn mad 317 13% 32 100 (94) % 0 (2)nn jj mad 316 100% 200 94 (94) % 12 (12)vb ab nn mad 256 33% 31 90 (90) % 3 (3) -25 (-25) ppvb pp nn mad 674 5% 27 100 (85) % 0 (4) (-19) ppvb ab pp nn mad 222 21% 30 100 (70) % 0 (9) (+7) ppvb pp jj nn mad 207 7% 14 100 (64) % 0 (5) (-16) ppb) vb pp nn nn mad 197 5% 9 22 (11) % 7 (8) 0 (+10) ppvb pp nn pp nn mad 232 5% 12 75 (4) % 3 (12) 0 (+2) ppc) vb nn pp nn mad 813 54% 28 61 (32) % 11 (19) -20 (-15) ppTable 1: In test: Number of sentences in test set of this type.
Changed: Proportion of sentences thatreceived a different parsing after pre-processing had been applied.
Manually classified: Number of man-ually classified sentences.
Correct: Proportion of sentences that were correctly parsed (and labelled)after pre-processing had been applied.
# Incorrect: Number of incorrectly parsed (and labelled) sen-tences after pre-processing.
UAS (LAS) difference: For these incorrect sentences: The difference in UAS,unlabelled attachment score, (and LAS, labelled attachment score) before and after pre-processing.
(Forsentence types with more than 90% correct sentences, this difference was not calculated.
)ReferencesDaniel Albright, Arrick Lanfranchi, Anwen Fredrik-sen, William F Styler, 4th, Colin Warner, Jena DHwang, Jinho D Choi, Dmitriy Dligach, Rod-ney D Nielsen, James Martin, Wayne Ward, MarthaPalmer, and Guergana K Savova.
2013.
Towardscomprehensive syntactic and semantic annotationsof the clinical narrative.
J Am Med Inform Assoc,Jan.Thorsten Brants and Oliver Plaehn.
2000.
Interactivecorpus annotation.
In LREC.
European LanguageResources Association.Marie Candito, Enrique H. Anguiano, and Djame?
Sed-dah.
2011.
A Word Clustering Approach to DomainAdaptation: Effective Parsing of Biomedical Texts.In Proceedings of the 12th International Conferenceon Parsing Technologies, pages 37?42, Dublin, Ire-land, October.
Association for Computational Lin-guistics.Johan Carlberger and Viggo Kann.
1999.
Implement-ing an efficient part-of-speech tagger.
Software?Practice and Experience, 29:815?832.Hercules Dalianis, Martin Hassel, and SumithraVelupillai.
2009.
The Stockholm EPR Corpus -Characteristics and Some Initial Findings.
In Pro-ceedings of ISHIMR 2009, Evaluation and imple-mentation of e-health and health information initia-tives: international perspectives.
14th InternationalSymposium for Health Information Management Re-search, Kalmar, Sweden, pages 243?249.Martin Hassel, Aron Henriksson, and SumithraVelupillai.
2011.
Something Old, Something New- Applying a Pre-trained Parsing Model to Clini-cal Swedish.
In Proceedings of NODALIDA?11 -18th Nordic Conference on Computational Linguis-tics, Riga, Latvia, May 11-13.Katri Haverinen, Filip Ginter, Veronika Laippala, andTapio Salakoski.
2009.
Parsing Clinical Finnish:Experiments with Rule-Based and Statistical De-pendency Parsers.
In Kristiina Jokinen and Eck-hard Bick, editors, Proceedings of NODALIDA?09,Odense, Denmark, pages 65?72.Dimitrios Kokkinakis.
2012.
The journal of theSwedish medical association - a corpus resource forbiomedical text mining in Swedish.
In The ThirdWorkshop on Building and Evaluating Resources forBiomedical Text Mining (BioTxtM), an LREC Work-shop.
Turkey.Jens Nilsson and Joakim Nivre.
2008.
Malteval:An evaluation and visualization tool for dependencyparsing.
In Proceedings of the Sixth InternationalLanguage Resources and Evaluation.
LREC, pages161?166.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.
Tal-banken05: A Swedish treebank with phrase struc-ture and dependency annotation.
In Proceedings ofthe fifth international conference on Language Re-sources and Evaluation (LREC 2006), pages 24?26.Joakim Nivre, Marco Kuhlmann, and Johan Hall.2009.
An improved oracle for dependency parsingwith online reordering.
In Proceedings of the 11thInternational Conference on Parsing Technologies,IWPT ?09, pages 73?76, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.101
