Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 229?232, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsApplying spelling error correction techniquesfor improving semantic role labellingErik Tjong Kim SangInformatics InstituteUniversity of Amsterdam, Kruislaan 403NL-1098 SJ Amsterdam, The Netherlandserikt@science.uva.nlSander Canisius, Antal van den Bosch, Toine BogersILK / Computational Linguistics and AITilburg University, P.O.
Box 90153,NL-5000 LE Tilburg, The Netherlands{S.V.M.Canisius,Antal.vdnBosch,A.M.Bogers}@uvt.nl1 IntroductionThis paper describes our approach to the CoNLL-2005 shared task: semantic role labelling.
We domany of the obvious things that can be found in theother submissions as well.
We use syntactic treesfor deriving instances, partly at the constituent leveland partly at the word level.
On both levels we editthe data down to only the predicted positive casesof verb-constituent or verb-word pairs exhibiting averb-argument relation, and we train two next-levelclassifiers that assign the appropriate labels to thepositively classified cases.
Each classifier is trainedon data in which the features have been selected tooptimize generalization performance on the particu-lar task.
We apply different machine learning algo-rithms and combine their predictions.As a novel addition, we designed an automaticallytrained post-processing module that attempts to cor-rect some of the errors made by the base system.To this purpose we borrowed Levenshtein-distance-based correction, a method from spelling error cor-rection to repair mistakes in sequences of labels.
Weadapted the method to our needs and applied it forimproving semantic role labelling output.
This pa-per presents the results of our approach.2 Data and featuresThe CoNLL-2005 shared task data sets provide sen-tences in which predicate?argument relations havebeen annotated, as well as a number of extra anno-tations like named entities and full syntactic parses(Carreras and Ma`rquez, 2005).
We have used theparses for generating machine learning instances forpairs of predicates and syntactic phrases.
In princi-ple each phrase can have a relation with each verbin the same sentence.
However, in order to keepthe number of instances at a reasonable number, wehave only built instances for verb?phrase pairs whenthe phrase parent is an ancestor of the verb (400,128training instances).
A reasonable number of ar-guments are individual words; these do not matchwith phrase boundaries.
In order to be able to labelthese, we have also generated instances for all pairsof verbs and individual words using the same con-straint (another 542,217 instances).
The parent nodeconstraint makes certain that embedded arguments,which do not occur in these data sets, cannot be pre-dicted by our approach.Instances which are associated with verb?argument pairs receive the label of the argument asclass while others in principle receive a NULL class.In an estimated 10% of the cases, the phrase bound-aries assigned by the parser are different from thosein the argument annotation.
In case of a mismatch,we have always used the argument label of the firstword of a phrase as the class of the correspondinginstance.
By doing this we attempt to keep the posi-tional information of the lost argument in the train-ing data.
Both the parser phrase boundary errors aswell as the parent node constraint restrict the num-ber of phrases we can identify.
The maximum recallscore attainable with our phrases is 84.64% for thedevelopment data set.We have experimentally evaluated 30 featuresbased on the previous work in semantic role la-belling (Gildea and Jurafsky, 2002; Pradhan et al,2004; Xue and Palmer, 2004):?
Lexical features (5): predicate (verb), firstphrase word, last phrase word and words im-mediately before and after the phrase.?
Syntactic features (14): part-of-speech tags(POS) of: first phrase word, last phrase word,229word immediately before phrase and word im-mediately after phrase; syntactic paths fromword to verb: all paths, only paths for wordsbefore verb and only paths for words after verb;phrase label, label of phrase parent, subcate-gorisation of verb parent, predicate frame fromPropBank, voice, head preposition for preposi-tional phrases and same parents flag.?
Semantic features (2): named entity tag forfirst phrase word and last phrase word.?
Positional features (3): position of the phrasewith respect to the verb: left/right, distance inwords and distance in parent nodes.?
Combination features (6): predicate + phraselabel, predicate + first phrase word, predicate+ last phrase word, predicate + first phrasePOS, predicate + last phrase POS and voice +left/right.The output of two parsers was available.
We havebriefly experimented with the Collins parses includ-ing the available punctuation corrections but foundthat our approach reached a better performance withthe Charniak parses.
We report only on the resultsobtained with the Charniak parses.3 ApproachThis section gives a brief overview of the three maincomponents of our approach: machine learning, au-tomatic feature selection and post-processing by anovel procedure designed to clean up the classifieroutput by correcting obvious misclassifications.3.1 Machine learningThe core machine learning technique employed, ismemory-based learning, a supervised inductive al-gorithm for learning classification tasks based on thek-nn algorithm.
We use the TiMBL system (Daele-mans et al, 2003), version 5.0.0, patch-2 with uni-form feature weighting and random tiebreaking (op-tions: -w 0 -R 911).
We have also evaluated two al-ternative learning techniques.
First, Maximum En-tropy Models, for which we employed Zhang Le?sMaximum Entropy Toolkit, version 20041229 withdefault parameters.
Second, Support Vector Ma-chines for which we used Taku Kudo?s YamCha(Kudo and Matsumoto, 2003), with one-versus-allvoting and option -V which enabled us to ignore pre-dicted classes with negative distances.3.2 Feature selectionIn previous research, we have found that memory-based learning is rather sensitive to the chosen fea-tures.
In particular, irrelevant or redundant fea-tures may lead to reduced performance.
In orderto minimise the effects of this sensitivity, we haveemployed bi-directional hill-climbing (Caruana andFreitag, 1994) for finding the features that were mostsuited for this task.
This process starts with an emptyfeature set, examines the effect of adding or remov-ing one feature and then starts a new iteration withthe set associated with the best performance.3.3 Automatic post-processingCertain misclassifications by the semantic role-labelling system described so far lead to unlikely andimpossible relation assignments, such as assigningtwo indirect objects to a verb where only one is pos-sible.
Our proposed classifier has no mechanism todetect these errors.
One solution is to devise a post-processing step that transforms the resulting role as-signments until they meet certain basic constraints,such as the rule that each verb may have only sin-gle instances of the different roles assigned in onesentence (Van den Bosch et al, 2004).We propose an alternative automatically-trainedpost-processing method which corrects unlikely roleassignments either by deleting them or by replacingthem with a more likely one.
We do not do this byknowledge-based constraint satisfaction, but ratherby adopting a method for error correction based onLevenshtein distance (Levenshtein, 1965), or editdistance, as used commonly in spelling error correc-tion.
Levenshtein distance is a dynamically com-puted distance between two strings, accounting forthe number of deletions, insertions, and substitu-tions needed to transform the one string into theother.
Levenshtein-based error correction typicallymatches a new, possibly incorrect, string to a trustedlexicon of assumedly correct strings, finds the lex-icon string with the smallest Levenshtein distanceto the new string, and replaces the new string withthe lexicon string as its likely correction.
We imple-mented a roughly similar procedure.
First, we gener-ated a lexicon of semantic role labelling patterns ofA0?A5 arguments of verbs on the basis of the entiretraining corpus and the PropBank verb frames.
This230lexicon contains entries such as abandon A0 V A1,and categorize A1 V A2 ?
a total of 43,033 variable-length role labelling patterns.Next, given a new test sentence, we consider allof its verbs and their respective predicted role la-bellings, and compare each with the lexicon, search-ing the role labelling pattern with the same verb atthe smallest Levenshtein distance (in case of an un-known verb we search in the entire lexicon).
Forexample, in a test sentence the pattern emphasize A0V A1 A0 is predicted.
One closest lexicon item isfound at Levenshtein distance 1, namely emphasizeA0 V A1, representing a deletion of the final A0.
Wethen use the nearest-neighbour pattern in the lexiconto correct the likely error, and apply all deletionsand substitutions needed to correct the current pat-tern according to the nearest-neighbour pattern fromthe trusted lexicon.
We do not apply insertions, sincethe post-processor module does not have the infor-mation to decide which constituent or word wouldreceive the inserted label.
In case of multiple possi-ble deletions (e.g.
in deleting one out of two A1s inemphasize A0 V A1 A1), we always delete the argu-ment furthest from the verb.4 ResultsIn order to perform the optimisation of the seman-tic role labelling process in a reasonable amount oftime, we have divided it in four separate tasks: prun-ing the data for individual words and the data forphrases, and labelling of these two data sets.
Prun-ing amounts to deciding which instances correspondwith verb-argument pairs and which do not.
Thisresulted in a considerable reduction of the two datasets: 47% for the phrase data and 80% for the worddata.
The remaining instances are assumed to de-fine verb-argument pairs and the labelling tasks as-sign labels to them.
We have performed a sepa-rate feature selection process in combination withthe memory-based learner for each of the four tasks.First we selected the best feature set based on taskaccuracy.
As soon as a working module for each ofthe tasks was available, we performed an extra fea-ture selection process for each of the modules, opti-mising overall system F?=1 while keeping the otherthree modules fixed.The effect of the features on the overall perfor-Words PhrasesFeatures prune label prune labelpredicate -0.04 +0.05 -0.25 -0.52first word +0.38 +0.16 -0.17 +1.14last word ?
?
-0.01 +1.12previous word -0.06 +0.02 -0.05 +0.74next word -0.04 -0.08 +0.44 -0.16part-of-speech first word -0.01 -0.02 -0.07 -0.11part-of-speech last word ?
?
-0.14 -0.45previous part-of-speech -0.12 -0.06 +0.22 -1.14next part-of-speech -0.08 -0.12 -0.01 -0.21all paths +0.42 +0.10 +0.84 +0.75path before verb +0.00 -0.02 +0.00 +0.27path after verb -0.01 -0.01 -0.01 -0.06phrase label -0.01 -0.02 +0.13 -0.02parent label +0.03 -0.02 -0.03 +0.00voice +0.02 -0.04 -0.04 +1.85subcategorisation -0.01 +0.00 -0.02 +0.03PropBank frame -0.12 -0.03 -0.16 +1.04PP head +0.00 +0.00 -0.06 +0.08same parents -0.02 -0.01 +0.03 -0.05named entity first word +0.00 +0.00 +0.05 -0.11named entity last word ?
?
-0.04 -0.12absolute position +0.00 +0.00 +0.00 -0.02distance in words +0.34 +0.04 +0.16 -0.96distance in parents -0.02 -0.02 +0.06 -0.04predicate + label -0.05 -0.07 -0.22 -0.47predicate + first word -0.05 +0.00 +0.13 +0.97predicate + last word ?
?
-0.03 +0.08predicate + first POS -0.05 -0.06 -0.20 -0.50predicate + last POS ?
?
-0.13 -0.40voice + position +0.02 -0.04 -0.05 -0.04Table 1: Effect of adding a feature to the best featuresets when memory-based learning is applied to thedevelopment set (overall F?=1).
The process con-sisted of four tasks: pruning data sets for individualwords and phrases, and labelling these two data sets.Selected features are shown in bold.
Unfortunately,we have not been able to use all promising features.mance can be found in Table 1.
One feature (syntac-tic path) was selected in all four tasks but in generaldifferent features were required for optimal perfor-mance in the four tasks.
Changing the feature sethad the largest effect when labelling the phrase data.We have applied the two other learners, MaximumEntropy Models and Support Vector Machines to thetwo labelling tasks, while using the same features asthe memory-based learner.
The performance of thethree systems on the development data can be foundin Table 3.
Since the systems performed differentlywe have also evaluated the performance of a com-bined system which always chose the majority classassigned to an instance and the class of the strongestsystem (SVM) in case of a three-way tie.
The com-bined system performed slightly better than the best231Precision Recall F?=1Development 76.79% 70.01% 73.24Test WSJ 79.03% 72.03% 75.37Test Brown 70.45% 60.13% 64.88Test WSJ+Brown 77.94% 70.44% 74.00Test WSJ Precision Recall F?=1Overall 79.03% 72.03% 75.37A0 85.65% 81.73% 83.64A1 76.97% 71.89% 74.34A2 71.07% 58.20% 63.99A3 69.29% 50.87% 58.67A4 75.56% 66.67% 70.83A5 100.00% 40.00% 57.14AM-ADV 64.36% 51.38% 57.14AM-CAU 75.56% 46.58% 57.63AM-DIR 48.98% 28.24% 35.82AM-DIS 81.88% 79.06% 80.45AM-EXT 87.50% 43.75% 58.33AM-LOC 62.50% 50.96% 56.15AM-MNR 64.52% 52.33% 57.78AM-MOD 96.76% 97.64% 97.20AM-NEG 97.38% 96.96% 97.17AM-PNC 45.98% 34.78% 39.60AM-PRD 50.00% 20.00% 28.57AM-REC 0.00% 0.00% 0.00AM-TMP 80.52% 70.75% 75.32R-A0 81.47% 84.38% 82.89R-A1 74.00% 71.15% 72.55R-A2 60.00% 37.50% 46.15R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 100.00% 25.00% 40.00R-AM-EXT 100.00% 100.00% 100.00R-AM-LOC 86.67% 61.90% 72.22R-AM-MNR 33.33% 33.33% 33.33R-AM-TMP 64.41% 73.08% 68.47V 97.36% 97.36% 97.36Table 2: Overall results (top) and detailed results onthe WSJ test (bottom).individual system.5 ConclusionWe have presented a machine learning approach tosemantic role labelling based on full parses.
Wehave split the process in four separate tasks: prun-ing the data bases of word-based and phrase-basedexamples down to only the positive verb-argumentcases, and labelling the two positively classified datasets.
A novel automatic post-processing procedurebased on spelling correction, comparing to a trustedlexicon of verb-argument patterns from the trainingmaterial, was able to achieve a performance increaseby correcting unlikely role assignments.Learning algorithm Precision Recall F?=1without post-processing:Maximum Entropy Models 70.78% 70.03% 70.40Memory-Based Learning 70.70% 69.85% 70.27Support Vector Machines 75.07% 69.15% 71.98including post-processing:Maximum Entropy Models 74.06% 69.84% 71.89Memory-Based Learning 73.84% 69.88% 71.80Support Vector Machines 77.75% 69.11% 73.17Combination 76.79% 70.01% 73.24Table 3: Effect of the choice of machine learningalgorithm, the application of Levenshtein-distance-based post-processing and the use of system combi-nation on the performance obtained for the develop-ment data set.AcknowledgementsThis research was funded by NWO, the NetherlandsOrganisation for Scientific Research, and by Senter-Novem IOP-MMI.ReferencesX.
Carreras and L. Ma`rquez.
2005.
Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedingsof CoNLL-2005.
Ann Arbor, MI, USA.R.
Caruana and D. Freitag.
1994.
Greedy attribute selection.In Proceedings of the Eleventh International Conference onMachine Learning, pages 28?36, New Brunswick, NJ, USA.Morgan Kaufman.W.
Daelemans, J. Zavrel, K. van der Sloot, and A. van denBosch.
2003.
TiMBL: Tilburg memory based learner, ver-sion 5.0, reference guide.
ILK Technical Report 03-10,Tilburg University.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of seman-tic roles.
Computational Linguistics, 28(3):245?288.T.
Kudo and Y. Matsumoto.
2003.
Fast methods for kernel-based text analysis.
In Proceedings of ACL-2003.
Sapporo,Japan.V.
Levenshtein.
1965.
Binary codes capable of correctingdeletions, insertions and reversals.
Doklady Akademii NaukSSSR, 163(4):845?848.S.
Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky.2004.
Shallow semantic parsing using support vector ma-chines.
In Proceedings of the HLT/NAACL 2004.
Boston,MA.A.
van den Bosch, S. Canisius, W. Daelemans, I Hendrickx,and E. Tjong Kim Sang.
2004.
Memory-based semanticrole labeling: Optimizing features, algorithm, and output.
InProceedings of the CoNLL-2004, Boston, MA, USA.N.
Xue and M. Palmer.
2004.
Calibrating features for semanticrole labeling.
In Proceedings of EMNLP-2004.
Barcelona,Spain.232
