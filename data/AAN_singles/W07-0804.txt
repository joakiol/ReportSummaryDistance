Proceedings of the 5th Workshop on Important Unresolved Matters, pages 25?32,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsArabic Cross-Document Person Name NormalizationWalid Magdy, Kareem Darwish, Ossama Emam, and Hany HassanHuman Language Technologies GroupIBM Cairo Technology Development CenterP.O.
Box 166 El-Ahram, Giza, Egypt{wmagdy, darwishk, emam, hanyh}@eg.ibm.comAbstractThis paper presents a machine learningapproach based on an SVM classifiercoupled with preprocessing rules for cross-document named entity normalization.
Theclassifier uses lexical, orthographic,phonetic, and morphological features.
Theprocess involves disambiguating differententities with shared name mentions andnormalizing identical entities with differentname mentions.
In evaluating the quality ofthe clusters, the reported approach achievesa cluster F-measure of 0.93.
The approachis significantly better than the two baselineapproaches in which none of the entities arenormalized or entities with exact namementions are normalized.
The two baselineapproaches achieve cluster F-measures of0.62 and 0.74 respectively.
The classifierproperly normalizes the vast majority ofentities that are misnormalized by thebaseline system.1.
Introduction:Much recent attention has focused on theextraction of salient information from unstructuredtext.
One of the enabling technologies forinformation extraction is Named EntityRecognition (NER), which is concerned withidentifying the names of persons, organizations,locations, expressions of times, quantities, ...
etc.
(Chinchor, 1999; Maynard et al, 2001;  Sekine,2004; Joachims, 2002).
The NER task ischallenging due to the ambiguity of naturallanguage and to the lack of uniformity in writingstyles and vocabulary used across documents(Solorio, 2004).Beyond NER, considerable work has focusedon the tracking and normalization of entities thatcould be mentioned using different names (e.g.George Bush, Bush) or nominals (e.g.
thepresident, Mr., the son) (Florian et al, 2004).Most of the named entity tracking work hasfocused on intra-document normalization withvery limited work on cross-documentsnormalization.Recognizing and tracking entities of type?Person Name?
are particularly important forinformation extraction.
Yet they pose interestingchallenges that require special attention.
Theproblems can result from:1.
A Person?s name having many variant spellings(especially when it is transliterated into aforeign language).
These variations aretypically limited in the same document, but arevery common across different documents fromdifferent sources (e.g.
Mahmoud Abbas =Mahmod Abas, Mohamed El-Baradei =Muhammad AlBaradey ?
etc).2.
A person having more than one name (e.g.Mahmoud Abbas = Abu Mazen).3.
Some names having very similar or identicalnames but refer to completely different persons(George H. W. Bush ?
George W. Bush).4.
Single token names (e.g.
Bill Clinton = Clinton?
Hillary Clinton).This paper will focus on Arabic cross-documentnormalization of named entities of type ?personname,?
which would involve resolving theaforementioned problems.
As illustrated in Figure1, the task involves normalizing a set of personentities into a set of classes each of which is25formed of at least one entity.
For N input entities,the output of normalization process will be Mclasses, where M ?
N.  Each class would refer toonly one person and each class would contain allentities referring to that person.For this work, intra-document normalization isassumed and an entity refers to a normalized set ofname mentions and nominals referring to a singleperson in a single document.
Florian et al (2004)were kind enough to provide the authors access toan updated version of their state-of-the-art NamedEntity Recognition and Tracking (NERT) system,which achieves an F-measure of 0.77 for NER,and an F-measure of 0.88 for intra-documentnormalization assuming perfect NER.
Althoughthe NERT systems is efficient for relatively shortdocuments, it is computational impractical forlarge documents, which precludes using the NERTsystem for cross-document normalization throughcombining the documents into one largedocument.
The main challenges of this work stemfrom large variations in the spelling oftransliterated foreign names and the presence ofmany common Arabic names (such asMuhammad, Abdullah, Ahmed ?etc.
), whichincreases the ambiguity in identifying the personreferred to by the mentioned name.
Further, theNERT system output system contains many NERerrors and intra-document normalization errors.In this paper, cross-document normalizationsystem employs a two-step approach.
In the firststep, preprocessing rules are used to remove errantnamed entities.
In the second step, a supportvector machine (SVM) classifier is used todetermine if two entities from two differentdocuments need to be normalized.
The classifieris trained on lexical, orthographic, phonetic, andmorphological features.The paper is organized as follows: Section 2provides a background on cross-document NEnormalization; Section 3 describes thepreprocessing steps and data used for training andtesting;  Section 4 describes the normalizationmethodology; Section 5 describes theexperimental setup;  Section 6 reports anddiscusses experimental results;  and Section 7concludes the paper and provides possible futuredirections.2.
BackgroundWhile considerable work has focused on namedentity normalization within a single document,little work has focused on the challengesassociated with resolving person name referencesacross multiple documents.
Most of the workdone in cross-document normalization focused onthe problem of determining if two instances withthe same name from different documents referringto the same person (Fleischman and Hovy, 2004).Fleischman and Hovy (2004) focused ondistinguishing between individuals havingidentical names, but they did not extendnormalization to different names referring to thesame individual.
Their task is a subtask of what isexamined in this paper.
They used a large numberof features to accomplish their work, dependingmostly on language specific dictionaries andwordnet.
Some these resources are not availablefor Arabic and many other languages.
Mann andYarowsky (Mann and Yarowsky, 2003) examinedthe same problem but they treated it as a clusteringtask.
They focused on information extraction tobuild biographical profiles (date of birth, place ofbirth, etc.
), and they wanted to disambiguatebiographies belonging to different authors withidentical names.Dozier and Zielund (Dozier and Zielund, 2004)reported on cross-document person namenormalization in the legal domain.
They used aFigure 1 Normalization ModelE1E3E7 E5E2E4 E6E8NormalizationE1E4 E8E2E3 E7E5E626finite state machine that identifies paragraphs in adocument containing the names of attorneys,judges, or experts and a semantic parser thatextracts from the paragraphs template informationabout each named individual.
They relied onreliable biographies for each individual.
Abiography would typically contain a person?s firstname, middle name, last name, firm, city, state,court, and other information.
They used aBayesian network to match the name mentions tothe biographical records.Bhattacharya and Getoor (Bhattacharya andGetoor, 2006) introduced a collective decisionalgorithm for author name entity resolution, wheredecisions are not considered on an independentpairwise basis.
They focused on using relationallinks among the references and co-authorrelationships to infer collaboration groups, whichwould disambiguate entity names.
Such explicitlinks between co-authors can be extracted directly.However, implicit links can be useful whenlooking at completely unstructured text.
Otherwork has extended beyond entities of type ?personname?
to include the normalization of locationnames (Li et al, 2002) and organizations (Ji andGrishman.
2004).3.
Preprocessing  and the Data SetFor this work, a set of 7,184 person name entitieswas constructed.
Building new training and testsets is warranted, because the task at hand issufficiently different from previously reportedtasks in the literature.
The entities wererecognized from 2,931 topically related documents(relating to the situation in the Gaza and Lebanonduring July of 2006) from different Arabic newssources (obtained from searching the Arabicversion of news.google.com).
The entities wererecognized and normalized (within document)using the NERT system of Florian et al(2004).As shown in Figure 2, each entity is composed ofa set of name mentions (one or more) and a set ofnominal mentions (zero or more).The NERT system achieves an F-score of 0.77with precision of 0.82 and recall of 0.73 for personname mention and nominal recognition and an F-score of 0.88 for tracking (assuming 100%recognition accuracy).
The produced entities maysuffer from the following:1.
Errant name mentions: Two name mentionsreferring to two different entities areconcatenated into an errant name mention (e.g.
?Bush Blair?, ?Ahmadinejad Bush?).
Thesetypes of errors stem from phrases such as ?Themeeting of Bush Blair?
and generally due tolack of sufficient punctuation marks.2.
NE misrecognitions: Regular words arerecognized as person name mentions and areembedded into person entities (e.g.
Bush =George Bush = said).3.
Errant entity tracking: name mentions ofdifferent entities are recognized as differentmentions of the same entity (e.g.
Bush =Clinton = Ahmadinejad).4.
Lack of nominal mentions: Many entities donot contain any nominal mentions, whichincreases the entity ambiguity (especiallywhen there is only one name mentioncomposed of a single token).To overcome these problems, entities werepreprocessed as follows:1.
Errant name mentions such as ?Bush Blair?were automatically removed.
In this step, adictionary of person name mentions was builtfrom the 2,931 documents collection fromwhich the entities were recognized andnormalized along with the frequency ofappearance in the collection.
For each entity,all its name mentions are checked in thedictionary and their frequencies are comparedto each other.
Any name mention with afrequency less than 1/30 of the frequency ofthe name mention with the highest frequencyis automatically removed (1/30 was pickedbased on manual examination of the trainingset).
Figure 2 Entity Description272.
Name mentions formed of a single tokenconsisting of less than 3 characters areremoved.
Such names are almost alwaysmisrecognized name entities.3.
Name entities with 10 or more different namementions are automatically removed.
TheNERT system often produces entities thatinclude many different name mentionsreferring to different persons as one.
Suchentities are errant because they over normalizename mentions.
Persons are referred to usinga limited number of name mentions.4.
Nominal mentions are stemmed using acontext sensitive Arabic stemmer (Lee et al2003) to overcome the morphologicalcomplexity of Arabic.
For example, ?JKL??
=?president?, ?
O?JKLQ ?
= ?the president?,?
O?
?JKLQ ?
= ?and the president?, ?
SKL?TU ?
= ?itspresidents?
?
etc are stemmed to ?JKL??
=?president?.Cross-document entities are compared in apairwise manner and binary decision is taken onwhether they are the same.
Therefore, theavailable 7,184 entities lead to nearly 26 millionpairwise comparisons (For N entities, the numberof pair wise comparisons = 2)1( ?NN ).Entity pairs were chosen to be included in thetraining set if they match any of the followingcriteria:1.
Both entities have one shared name mention.2.
Both entities have shared nominal mentions.3.
A name mention in one of the entities is asubstring of a name mention in the otherentity.4.
Both entities have nearly identical namementions (small edit distance between bothmentions).The resulting set was composed of 19,825pairs, which were manually judged to determine ifthey should be normalized or not.
These criteriaskew the selection of pairs towards moreambiguous cases, which would be bettercandidates to train the intended SVM classifier,where the items near the boundary dividing thehyperplane are the most important.
For thetraining set, 18,503 pairs were normalized, and1,322 pairs were judged as different.Unfortunately, the training set selection criteriaskewed the distribution of training examplesheavily in favor of positive examples.
It wouldinteresting to examine other training sets wherethe distribution of positives and negatives isbalanced or skewed in favor of negatives.The test set was composed of 470 entities thatwere manually normalized into 253 classes, ofwhich 304 entities were normalized to 87 classesand 166 entities remained unnormalized (formingsingle-entity classes).
Using 470 entities leads to110,215 pairwise comparisons.
The test set, whichwas distinct from the training set, was chosenusing the same criteria as the training set.
Further,all duplicate (identical) entities were removedfrom the test set.
The selection criteria insure thatthe test set is skewed more towards ambiguouscases.
Randomly choosing entities would havemade the normalization too easy.4.
Normalization MethodologySVMLight, an SVM classifier (Joachims, 2002),was used for classification with a linear kernel anddefault parameters.
The following trainingfeatures were employed:1.
The percentage of shared name mentionsbetween two entities calculated as:Name Commonality =?
?
?>< ???????
?namescommon jijiffff2211 ,minwhere f1i is the frequency of the shared namemention in first entity, and f2i is the frequencyof the shared name mention in the secondentity.
?
f1i is the number of name mentionsappearing in the entity.2.
The maximum number of tokens in the sharedname mentions, i.e.
if there exists more thanone shared name mention then this feature isthe number of tokens in the longest sharedname mention.3.
The percentage of shared nominal mentionsbetween two entities, and it is calculated as thename commonality but for nominal mentions.4.
The smallest minimum edit distance(Levenshtein distance with uniform weights)between any two name mentions in bothentities (Cohen et al, 2003) and this feature isonly enabled when name commonalitybetween both entities equals to zero.285.
Phonetic edit distance, which is similar to editdistance except that phonetically similarcharacters, namely {(?
?
t, ?
?
T), (?
?
k, ?
?q),(?
?
d, ?
?
D),(?
?
v, ?
?
s, ?
?
S), (?
?
*,?
?
z, ?
?
Z),(?
?
j, ?
?
g),(i?
?
p, k?
?
h),(?
?
<,n ?
|, ?
,< ?
?
?
A)1}, are normalized, vowels areremoved, and spaces between tokens areremoved.6.
The number of tokens in the pair of namementions that lead to the minimum editdistance.Some of the features might seem duplicative.However, the edit distance and phonetic editdistance are often necessary when names aretransliterated into Arabic and hence may havedifferent spellings and consequently no sharedname mentions.
Conversely, given a shared namemention between a pair of entities will lead to zeroedit distance, but the name commonality may alsobe very low indicating two different persons mayhave a shared name mention.
For example?Abdullah the second?
and ?Abdullah binHussein?
have the shared name mention?Abdullah?
that leads to zero edit distance, butthey are in fact two different persons.
In this case,the name commonality feature can be indicative ofthe difference.
Further, nominals are important indifferentiating between identical name mentionsthat in fact refer to different persons (Fleischmanand Hovy, 2004).
The number of tokens featureindicates the importance of the presence ofsimilarity between two name mentions, as thesimilarity between name mentions formed of onetoken cannot be indicative for similarity when thenumber of tokens is more than one.Further, it is assumed that entities are transitiveand are not available all at once, but rather thesystem has to normalize entities incrementally asthey appear.
Therefore, for a given set of entitypairs, if the classifier deems that Entityi = Entityjand Entityj = Entityk, then Entityi is set to equalEntityk even if the classifier indicates that Entityi ?Entityk, and all entities (i, j, and k) are merged intoone class.1 Buckwalter transliteration scheme is used throughoutthe paper5.
Experimental SetupTwo baselines were established for thenormalization process.
In the first, no entities arenormalized, which produces single entity classes(?no normalization?
condition).
In the second, anytwo entities having two identical name mentions incommon are normalized (?surface normalization?condition).
For the rest of the experiments, focuswas given to two main issues:1.
Determining the effect of the different featuresused for classification.2.
Determining the effect of varying the numberof training examples.To determine the effect of different features,multiple classifiers were trained using differentfeatures, namely:?
All features: all the features mentioned aboveare used,?
Edit distance removed:  edit distance features(features 4, 5, and 6) are removed,?
Number of tokens per name mention removed:the number of shared tokens and the numberof tokens leading to the least edit distance(features 2 and 6) are removed.To determine the effect of training examples,the classifier was trained using all features butwith a varying number of training example pairs,namely all 19,825 pairs, a set of randomly picked5,000 pairs, and a set of randomly picked 2,000pairs.For evaluation, 470 entities in test set werenormalized into set of classes with differentthresholds for the SVM classifier.
The quality ofthe clusters was evaluated using purity, entropy,and Cluster F-measure (CF-measure) in themanner suggested by Rosell et al (2004).
For thecluster quality measures, given cluster i (formedusing automatic normalization) and each cluster j(reference normalization formed manually), clusterprecision (p) and recall (r) are computed asfollows:iijij nnp = , andjijij nnr = , where ni number ofentities in cluster i, nj number of entities in clusterj, and nij number of shared entities between clusteri and j.The CF-measure for an automatic cluster iagainst a manually formed reference cluster j is:29ijijijijij prprCF +?
?= 2 , and the CF-measure for areference cluster j is:}{max ijij CFCF = .The final CF-measure is computed over all thereference clusters as follows: ?= j jij CFnnCF .Purity of (?i) of an automatically producedcluster i is the maximum cluster precision obtainedwhen comparing it with all the reference clustersas follows: }{max ijji p=?
, and the weightedaverage purity over all clusters is:?= i iiijnn ??
, where n is the total number ofentities in the set to be normalized (470 in thiscase).As for entropy of a cluster, it is calculated as:?
?= j ijiji ppE log , and the average entropyas:?= i iii EnnE .The CF-measure captures both precision andrecall while purity and entropy are precisionoriented measures (Rosell et al, 2004).6.
Results and DiscussionFigure 3 shows the purity and CF-measure for thetwo baseline conditions (no normalization, andsurface normalization) and for the normalizationsystem with different SVM thresholds.
Sincepurity is a precision measure, purity is 100% whenno normalization is done.
The CF-measure is 62%and 74% for baseline runs with no normalizationand surface normalization respectively.
As can beseen from the results, the baseline run based onexact matching of name mentions in entitiesachieves low CF-measure and low purity.
LowCF-measure values stem from the inability tomatch identical entities with different namementions, and the low purity value stems from notdisambiguating different entities with shared namementions.
Some notable examples where thesurface normalization baseline failed include:1.
The normalization of the different entitiesreferring to the Israeli soldier who isimprisoned in Gaza with different Arabicspellings for his name, namely ?tKuv ?Twux?
(jlEAd $lyT), ?tKOTv ?Twux?
(jlEAd $AlyT),?zKuv ?|}~O??
(the soldier $lyt), and so forth.2.
The separation between ??T?O??
?
|??
?u?O??
(King Abdullah the Second) and ?
??
??
|??
?u?O????wO?
|???
(King Abdullah ibn Abdul-Aziz)that have a shared name mention ???|??
?u?O??
(King Abdullah).3.
The normalization of the different entitiesrepresenting the president of PalestinianAuthority with different name mentions,namely ???T?
????
(Abu Mazen) and ?
??????T???
(Mahmoud Abbas).The proposed normalization techniqueproperly normalized the aforementioned examples.Given different SVM thresholds, Figure 3 showsthat the purity of resultant classes increases as theSVM threshold increases since the number ofnormalized entities decreases as the thresholdincreases.
The best CF-measure of 93.1% isobtained at a threshold of 1.4 and as show in Table1 the corresponding purity and entropy are 97.2%and 0.056 respectively.
The results confirm thesuccess of the approach.Table 1 highlights the effect of removingdifferent training feature and the highest CF-measures (at different SVM thresholds) as a result.The table shows that using all 6 features producedthe best results and the removal of the sharednames and tokens (features 2 and 6) had the mostadverse effect on normalization effectiveness.
Theadverse effect is reasonable especially given thatsome single token names such as ?Muhammad?and ?Abdullah?
are very common and matchingone of these names across entities is an insufficientindicator that they are the same.
Meanwhile, theexclusion of edit distance features (features 4, 5,and 6) had a lesser but significant adverse impacton normalization effectiveness.
Table 1 reportsthe best results obtained using different thresholds.Perhaps, a separate development set should beused for ascertaining the best threshold.Table 2 shows that decreasing the number oftraining examples (all six features are used) has anoticeable but less pronounced effect onnormalization effectiveness compared to removingtraining features.30Table 1 Quality of clusters as measured by purity (higher values are better), entropy (lower values arebetter), and CF-measure (higher values are better) for different feature sets.
Values are shown for maxCF-measure.
Thresholds were tuned for max CF-measure for each feature configuration separatelyTraining Data Purity Maximum CF-Measure Entropy ThresholdNo Normalization 100.0% 62.6% 0.000 -Baseline 83.4% 74.7% 0.151 -All Features 97.2% 93.1% 0.056 1.4Edit Distance removed 99.4% 85.5% 0.010 1.0# of tokens/name removed 96.6% 77.8% 0.071 1.5Normalization Evaluation60%65%70%75%80%85%90%95%100%NoNormalizationBaseline1.01.11.21.31.41.51.61.71.81.92.0SVM ThresholdPurityCF-MeasureFigure 3 Purity and cluster F-measure versus SVM ThresholdTable 2 Effect of number of training examples on normalization effectivenessTraining Data Purity Maximum CF-Measure Entropy Threshold20k training pairs 97.2% 93.1% 0.056 1.45k training pairs 97.4% 90.5% 0.053 1.52k training pairs 98.5% 90.3% 0.031 1.67.
Conclusion:This paper presented a two-step approach to cross-document named entity normalization.
In the firststep, preprocessing rules are used to remove errantnamed entities.
In the second step, a machinelearning approach based on an SVM classifier todisambiguate different entities with matchingname mentions and to normalize identical entitieswith different name mentions.
The classifier wastrained on features that capture name mentions andnominals overlap between entities, edit distance,and phonetic similarity.
In evaluating the qualityof the clusters, the reported approach achieved acluster F-measure of 0.93.
The approachoutperformed that two baseline approaches inwhich no normalization was done or normalizationwas done when two entities had matching name31mentions.
The two approaches achieved cluster F-measures of 0.62 and 0.74 respectively.For future work, implicit links between entitiesin the text can serve as the relational links thatwould enable the use of entity attributes inconjunction with relationships between entities.An important problem that has not beensufficiently explored is cross-lingual cross-document normalization.
This problem wouldpose unique and interesting challenges.
Thedescribed approach could be generalized toperform normalization of entities of different typesacross multilingual documents.
Also, thenormalization problem was treated as aclassification problem.
Examining the problem asa clustering (or alternatively an incrementalclustering) problem might prove useful.
Lastly,the effect of cross-document normalization shouldbe examined on applications such as informationextraction, information retrieval, and relationshipand social network visualization.References:Bhattacharya I. and Getoor L. ?A Latent DirichletAllocation Model for Entity Resolution.?
6th SIAMConference on Data Mining (SDM), Bethesda, USA,April 2006.Chinchor N., Brown E., Ferro L., and Robinson P.?Named Entity Recognition Task Definition.
?MITRE, 1999.Cohen W., Ravikumar P., and Fienberg S. E. ?AComparison of String Distance Metrics for Name-Matching Tasks.?
In Proceedings of theInternational Joint Conference on ArtificialIntelligence, 2003.Dozier C. and Zielund T. ?Cross-document Co-Reference Resolution Applications for People in theLegal Domain.?
In 42nd Annual Meeting of theAssociation for Computational Linguistics,Reference Resolution Workshop, Barcelona, Spain.July 2004.Fleischman M. B. and Hovy E. ?Multi-DocumentPerson Name Resolution.?
In 42nd Annual Meetingof the Association for Computational Linguistics,Reference Resolution Workshop, Barcelona, Spain.July 2004.Ji H. and Grishman R. ?Applying Coreference toImprove Name Recognition?.
In 42nd AnnualMeeting of the Association for ComputationalLinguistics, Reference Resolution Workshop,Barcelona, Spain.
July (2004).Ji H. and Grishman R. "Improving Name Tagging byReference Resolution and Relation Detection."
ACL2005Joachims T. ?Learning to Classify Text Using SupportVector Machines.?
Ph.D. Dissertation, Kluwer,(2002).Joachims T. ?Optimizing Search Engines Using Click-through Data.?
Proceedings of the ACM Conferenceon Knowledge Discovery and Data Mining (KDD),(2002).Lee Y. S., Papineni K., Roukos S., Emam O., HassanH.
?Language Model Based Arabic WordSegmentation.?
In ACL 2003, pp.
399-406, (2003).Li H., Srihari R. K., Niu C., and Li W. ?LocationNormalization for Information Extraction.
?Proceedings of the 19th international conference onComputational linguistics, pp.
1-7, 2002Li H., Srihari R. K., Niu C., and Li W. ?LocationNormalization for Information Extraction.
?Proceedings of the sixth conference on appliednatural language processing, 2000. pp.
247 ?
254.Mann G. S. and Yarowsky D. ?Unsupervised PersonalName Disambiguation.?
Proceedings of the seventhconference on Natural language learning at HLT-NAACL 2003.  pp.
33-40.Maynard D., Tablan V., Ursu C., Cunningham H., andWilks Y.
?Named Entity Recognition from DiverseText Types.?
Recent Advances in Natural LanguageProcessing Conference, (2001).Palmer D. D. and Day D. S. ?A statistical Profile of theNamed Entity Task?.
Proceedings of the fifthconference on Applied natural language processing,pp.
190-193, (1997).R.
Florian R., Hassan H., Ittycheriah A., Jing H.,Kambhatla N., Luo X., Nicolov N., and Roukos S.?A Statistical Model for Multilingual EntityDetection and Tracking.?
In HLT-NAACL, 2004.Rosell M., Kann V., and Litton J. E.  ?ComparingComparisons: Document Clustering EvaluationUsing Two Manual Classifications.?
In ICON 2004Sekine S. ?Named Entity: History and Future?.
Projectnotes, New York University, (2004).Solorio T. ?Improvement of Named Entity Tagging byMachine Learning.?
Ph.D. thesis, National Instituteof Astrophysics, Optics and Electronics, Puebla,Mexico, September 2005.32
