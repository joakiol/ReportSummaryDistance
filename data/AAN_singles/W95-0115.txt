Automat ic  Eva luat ion and Un i fo rm Fi l ter  Cascadesfor Induc ing  N-Best  Translat ion Lex iconsI.
Dan MelamedDepartment  of Computer  and Informat ion ScienceUniversity of PennsylvaniaPhi ladelphia, PA, 19104U.S.A.melamed~unagi, cis.
upenn, eduAbstractThis paper shows how to induce an N-best translat ion lexicon from a bil ingualtext corpus using statistical properties of the corpus together with four externalknowledge sources.
The knowledge sources are cast as filters, so that  any subset ofthem can be cascaded in a uniform framework.
A new objective valuation measureis used to compare the quality of lexicons induced with different filter cascades.
Thebest filter cascades improve lexicon quality by up to 137% over the plain vanillastatist ical method,  and approach human performance.
Drastical ly reducing thesize of the training corpus has a much smaller impact on lexicon quality when theseknowledge sources are used.
This makes it practical to train on small hand-bui l tcorpora for language pairs where large bilingual corpora are unavailable.
Moreover,three of the four filters prove useful even when used with large training corpora.1 INTRODUCTIONA machine translation system must be able to choose among possible translations based on context.To do this, it usuMly relies on a translation lexicon that contains a number of possible translationsfor each word.
N-best  translation lexicons contain up to N candidate translations for each word, or-dered from most probable to least probable, sometimes specifying a priori probabilities or likelihoodscores.Existing automatic methods for constructing N-best translation lexicons rely on the availabilityof large training corpora of parallel texts in the source and target languages.
For some methods, thecorpora must also be aligned by sentence \[Bro93, Ga191a\].
Unfortunately, such training corpora areavailable for only a handful of language pairs, and the cost to create enough training data manuallyfor new language pairs is very high.This paper presents1.
a new automatic evaluation method for N-best translation lexicons,2.
a filter-based approach for enhancing statistical translation models with non-statistical sourcesof information,3.
four sources of information that can drastically reduce the necessary amount of trainingmaterial.184IThe evaluation method uses a simple objective criterion rather than relying on subjective humanjudges.
It allows many experiments to be run without concern about the cost, availability andreliability of human evaluators.The filter-based approach is designed to identify likely (source word, target word) 1 pairs, using astatistical decision procedure.
Candidate word pairs are drawn from a corpus of aligned sentences:(S, T) is a candidate iff T appears in the translation of a sentence containing S. In the simplest case,the decision procedure considers M1 candidates for inclusion in the lexicon; but the new frameworkallows a cascade of non-statistical filters to remove inappropriate pairs fl'om consideration.Each filter is based on a particular knowledge source, and can be placed into the cascadeindependently of the others.
The knowledge sources investigated here are:?
part of speech information,?
machine-readable bilingual dictionaries (MRBDs),?
cognate heuristics, and?
word alignment heuristics.\[Bro94\] investigated the statistical use of MRBDs, though not as filters.
The other three knowledgesources have not previously been used for the task of inducing translation lexicons.The filter-based framework, together with the fully automatic evaluation method, allows easyinvestigation o$ the relative efficacy of cascades of each of the subsets of these four filters.
Aswill be shown below, some filter cascades ift candidate word pMrs so well that training corporasmall enough tO be hand-built can be used to induce more accurate translation lexicons than thoseinduced from a much larger training corpus without such filters.
In one evaluation, a trainingcorpus of 500 sentence pairs processed with these knowledge sources achieved a precision of 0.54,while a training corpus of 100,000 training pairs alone achieved a precision of only 0.45.
Suchimprovements Could not be previously obtained, because?
These knowledge sources have not been used together for this task before.?
There was no way to uniformly combine the different kinds of filters.?
There was no way to objectively judge lexicon precision.Table 1 provides a qualitative demonstration of how a lexicon entry gradually improves as more.Efilters are applied.
The table contains actual entries for the French source word "premier," from 7-best lexicons that were induced from 5000 pairs of training sentences, using different filter cascades.The baseline lexicon, induced with no filters, contains correct translations only in the first and sixthpositions.
The Cognate Filter disallows all candidate translations of French "premier" whenever theEnglish cognate "premier" appears in the target English sentence.
This causes English "premier"to move up to second position.
The Part-of-Speech Filter realizes that "premier" can only be anadjective in French, whereas in the English Hansards it is mostly used as a noun.
So, it throws outthat pairing, along with several other English noun candidates, allowing "first" to move up to thirdposition.
The POS and Cognate filters reduce noise better together than separately.
More of theincorrect translations are filtered out in the "POS & COG" column, making room for "foremost.
"Finally, the MRBD Filter narrows the list down to just the three translations of French "premier"that are appropriate in the Hansard sublanguage.1Punctuation, numbers, etc.
also count as words.185Table 1: entries for French "premier" in 7-best lexiconsEntry # No Filtersprimeministerp remierdirectquestionf irstSpeakerCOG Filter POS Filterenerated usingCOG & POSpr imepremierdirectSpeakerMr.myfirstprimedirectf irstsupplementaryformerfriendlyreaffirmpr imedirectf irstformerfriendlyfo remostechodifferent cascades of filtersCOG, POS & MRBDpr imef irstfo remost2 EXPERIMENTAL  FRAMEWORKAll translation lexicons discussed in this paper were created and evaluated using the procedure inFigure 1.
First, candidate translations were generated for each pair of aligned training sentences,by taking a simple cross-product of the words.
Next, the candidate translations from each pair oftraining sentences were passed through a cascade of filters.
The remaining candidate translationsfrom all training sentence pairs were pooled together and fed into a fixed decision procedure.
Theoutput of the decision procedure was a model of word correspondences between the two halves ofthe training corpus - -  a translation lexicon.
Each filter combination resulted in a different model.All the models were compared in terms of how well they represented a held-out test set.
Theevaluation was performed objectively and automatically using Bitext-Based Lexicon Evaluation(BIBLE, described below).
BIBLE assigned a score for each model, and these scores were used tocompare the effectiveness of various filter cascades.As shown in Figure 1, the only independent variable in the framework is the cascade of filtersused on the translation candidates generated by each sentence pair, while the only dependentvariable is a numerical score.
Since the filters only serve to remove certain translation candidates,any number of filters can be used in sequence.
This arrangement allows for fair comparison ofdifferent filter combinations.3 B ITEXT-BASED LEX ICON EVALUATION (BIBLE)Translation lexicon quality has traditionally been measured on two axes: precision and recall.
Recallis the fraction of the source language's vocabulary that appears in the lexicon.
Precision is thefraction of lexicon entries that are correct.
While the true size of the source vocabulary is usuallyunknown, recall can be estimated using a representative t xt sample by computing the fractionof words in the text that also appear in the lexicon.
Measuring precision is much more difficult,because it is unclear what a "correct" lexicon entry is - -  different translations are appropriate fordifferent contexts, and, in most cases, more than one translation is correct.
This is why evaluationof translation has eluded automation efforts until now.The large number of quantitative lexicon evaluations required for the present study made itinfeasible to rely on evaluation by human judges.
The only existing automatic lexicon evaluationmethod that I am aware of is the perplexity comparisons used by Brown et al in the fl'ameworkof their Model 1 \[Bro93\].
Lexicon perplexity indicates how "sure" a translation lexicon is about itscontents.
It does not, however, directly measure the quality of those contents.186IndependentVariableI J DataProcessI Aligned TrainingSentences 1Target Word PairsTranslation Candidatesi Translation \] LexiconI Aligned Test ~ ~SentencesDependent I Score \]VariableFigure 1: Uniform Framework for Data FiltersBIBLE is a family of algorithms, based on the observation that translation pairs 2 tend toappear in corresponding sentences in an aligned bilingual text corpus (a bitext).
Given a test setof aligned sentences, a better translation lexicon will contain a higher fraction of the (source word,target word) pairs in those sentences.
This fraction can be computed either by token or by type,depending on ghe application.
If only the words in the lexicon are considered, BIBLE gives anestimate of precision.
If all the words in the text are considered, then BIBLE measures percentcorrect.
The greater the overlap between the vocabulary of the test bitext and the vocabulary ofthe lexicon being evaluated, the more confidence can be placed in the BIBLE score.The BIBLE approach is suitable for many different evaluation tasks.
Besides comparing differentlexicons on different scales, BIBLE can be used to compare different parts of one lexicon that hasbeen partitione d using some characteristic of its entries.
For example, the quality of a lexicon'snoun entries can be compared to the quality of its adjective entries; the quality of its entries forfrequent words can be compared to the quality of its entries for rare words.
Likewise, separateevaluations canl be performed for each k, 1 < k < N, in N-best lexicons.Figure 2 shows the outline of a BIBLE algorithm for evaluating precision of N-best translationlexicons.
The kth cumulative hit rate for a source word S is the fraction of test sentences containingS whose translations contain one of the k best translations of S in the lexicon.
For each k, the kth2A " t rans la t ion  pair"  is a source word and a target  word that  are t rans la t ions  of each other .187Figure 2: A Bitext-Based Lexicon Evaluation (BIBLE) algorithm for precision of N-best lexicons- -  Percent correct can be evaluated instead of precision by switching lines 3 and 4.Input:1. translation lexicon with up to N translations for each word2.
aligned test bitextAlgorithm:1 for each pair  of al igned test sentences2 LOOP2: for each word S in the source sentence3 if S is in the lexicon4 frq(S) += 15 for k = I to N6 if S's kth translat ion T is in the target sentence7 delete T from target sentence8 HitCount\[S,k\]  += 19 next LOOP2;10 for each word S in the source vocabularyII if frq(S) > 012 for k = I to N13 HitRate\[k\] += HitCount\[S,k\]  / frq(S)14 Cumulat iveHitRate\[O\]  = 015 for k = 1 to N16 Cumulat iveHi tRate \[k\] = Cumulat iveHitRate \[k-l\] + HitRate \[k\]Output :Cumulat  i veH i tRat  e \[I.. N\]188cumulative hit rates are averaged over all the source words in the lexicon, counting words by type.This yields N average cumulative hit rates for the lexicon as a whole.In this study, the average is computed by type and not by token, because translations for themost frequent words are easy to estimate using any reasonable statistical decision procedure, evenwithout any extra information.
Token-based evaluation scores would be misleadingly inflated withvery little variation.
Computing hit rates for each word separately and then taking an unweightedaverage ensures that a correct translation of a common source word does not contribute more tothe score than :correct ranslations of rare words.
The evaluation is uniform over the whole lexicon.BIBLE evaluation is quite harsh, because many translations are not word for word in real bitexts.To put BIBLE scores reported here into proper perspective, human performance was evaluated ona similar task.
The 1994 ARPA-sponsored machine translation evaluation effort generated twoindependent English translations of one hundred French newspaper texts \[Whi93\].
I hand-alignedeach pair of translations by paragraph; most paragraphs contained between one and four sentences.For each pair of translations, the fl'action of times (by type) that identical words were used incorresponding :paragraphs was computed.
The average of these 100 fl'actions was 0.6182 witha standard deViation of 0.0647.
This is a liberal estimate of the upper bound on the internalconsistency of :BIBLE test sets.
Scores for sentence-based comparisons will always be lower thanscores for paragraph-based comparisons, because there will be fewer spurious "hits."
To confirmthis, an independent second translation of 50 French Hansard sentences was commissioned.
Thetranslation scored 0.57 on this test.4 EXPERIMENTSA bilingual teXt corpus of Canadian parliamentary proceedings ("Hansards") was aligned by sen-tence using the method presented in \[Gal91b\].
From the resulting aligned corpus, this study usedonly sentence pairs that were aligned one to one, and then only when they were less than 16 wordslong and aligned with high confidence.
Morphological variants in these sentences were stemmed toa canonical form.
Fifteen thousand sentence pairs were randomly selected and reserved for testing;one hundred thousand were used for training.The independent variable in the experiments was a varying combination of four different filters,used with six different sizes of training corpora.
These four filters fall into three categories: predicatefilters, oracle filters and alignment filters.
A predicate .filter is one where the candidate translationpair (S, T) must satisfy some predicate in order to pass the filter.
Various predicate filters arediscussed in \[Wu94\].
An oracle filter is useful when a list of likely translation pairs is availablea priori.
Then i if the translation pair (S, T) occurs in this oracle list, it is reasonable to filter outall other translation pairs involving S or T in the same sentence pair.
An alignment filter is basedon the relative positions of S and T in their respective texts\[Dag93\].The decision procedure used to select lexicon entries from the multiset of candidate translationpairs is a variation of the method presented in \[Gal91a\].
\[Dun93\] found binomial log-likelihoodratios to be relatively accurate when dealing with rare tokens.
This statistic was used to estimatedependencies between all co-occuring (source word, target word) pairs.
For each source word S,target words were ranked by their dependence with S. The top N target words in the rank-orderingfor S formed the entry for S in the N-best lexicon.
In other words, the relative magnitude ofdependence between S and its candidate translations was used as a maximum likelihood estimatorof the translations of S.1894.1 Part of Speech FilterThe POS Filter is a predicate filter.
It is based on the idea that word pairs that are good translationsof each other are likely to be the same parts of speech in their respective languages.
For example,a noun in one language is very unlikely to be translated as a verb in another language.
Therefore,candidate translation pairs involving different parts of speech should be filtered out.This heuristic should not be taken too far, however, in light of the imperfection of today'stagging technology.
For instance, particles are often confused with prepositions and adjectives withpast participles.
These considerations are further complicated by the differences in the tag setsused by taggers for different languages.
To maximize the filter's effectiveness, tag sets must beremapped to a more general common tag set, which ignores many of the language-specific details.Otherwise, correct ranslation pairs would be filtered out because of superficial differences like tenseand capitalization.The different ways to remap different ag sets into a more general common tag set representa number of design decisions.
Fortunately, BIBLE provided an objective criterion for tag setdesign, and a fast evaluation method.
The English half of the corpus was tagged using Brill'stransformation-based tagger \[Bri92\].
The French half was kindly tagged by George Foster of CITI.Then, BIBLE was used to select among several possible generalizations of the two tag sets.
Theresulting optimal tag set is shown in Table 2.TagCDCJDEOPEOSINJNNPPRSCMUHVVBGVBNTable 2: optimal common tag set for POS FilterMeaningnumberconjunctiondeterminerend of phrase marker (",", ";", etc.
)end of sentence marker (".
", "~", etc.
)preposition or particleadjectiveMatchesCDCJDEOPEOSINJ, VBG, VBNnoun (including "$")proper nounpronounadverbsubordinate clause marker (quotes, brackets, etc.
)interjectionverbpresent participlepast participleN, NPNP, NPRSCMUHVVBG, J, VBNVBN, J, VBG4.2 Machine-Readable Bilingual Dictionary (MRBD)An oracle list of 53363 one-to-one translation pairs was extracted from the Collins French-EnglishMRBD \[Cou91\].
whenever a candidate translation pair (S,T) appeared in the list of translationsextracted from the MRBD, the filter removed all word pairs (S, not T) and (not S, T) that occurredin the same sentence pair.190The MRBD Filter is an oracle filter.
It is based on the assumption that if a candidate translationpair (S,T) appears in an oracle list of likely translations, then T is the correct translation of S intheir sentence i pair, and there are no other translations of S or T in that sentence pair.
Thisassumption is stronger than the one made by Brown et al \[Bro94\], where the MRBD was treatedas data and not as an oracle.
Brown et al allowed the training data to override inibrmation gleanedfrom the MRBD.
The attitude of the present study is "Don't guess when you know."
This attitudemay be less appropriate when there is less of an overlap between the vocabulary of the MRBDand the vocabulary of the training bitext, as when dealing with technical text or with a very smallMRBD.The presented framework can be used as a method of enhancing an MRBD.
Merging an MRBDwith an N-best translation lexicon induced using the MRBD Filter will result in an MRBD withmore entries that are relevant to the sublanguage of the training bitext.
All the relevant entrieswill be rank orldered for appropriateness.4.3 Cognate FilterA Cognate Filt~er is another kind of oracle filter.
It is based on the simple heuristic that if a sourceword S is a cognate of some target word T, then T is the correct translation of SS in their sentencepair, and there are no other translations of S or T in that sentence pair.
Of course, identical wordscan mean different things in different languages.
The cognate heuristic fails when dealing withsuch faux amis \[Mac94\].
Fortunately, between French and English, true cognates occur far morefrequently than faux amis.There are many possible notions of what a cognate is.
Simard et al used the criterion that thefirst four characters must be identical for alphabetic tokens to be considered cognates \[Sim92\].
Un-fortunately, this criterion produces false negatives for pairs like "government" and "gouvernement",and false positives for words with a great difference in length, like "conseil" and "conservative."
Iused an appro~mate string matching algorithm to capture a more general notion of cognateness.Whether a pair of words is considered a cognate pair depends on the ratio of the length of theirlongest (not necessarily contiguous) common subsequence to the length of the longer word.
Thisis called the Longest Common Subsequence Ratio (LCSR).
For example, "gouvernement," whichis 12 letters long, has 10 letters that appear in the same order in "government."
So, the LCSR forthese two words is 10/12.
On the other hand, the LCSR for "conseil" and "conservative" is only6/12.
The only: remaining question was what minimum LCSR value should indicate that two wordsare cognates.
This question was easy to answer using BIBLE.
BIBLE scores were maximized forlexicons using t!he Cognate Filter when a LCSR cut-off of 0.58 was used.
The Wilcoxon signed rankstest found the difference between BIBLE scores for lexicons produced with this LCSR cut-off andfor lexicons produced with the criterion used in \[Sim92\] to be statistically significant at o?
= 0.01.The longest common subsequence between two words can be computed as a special case of theiredit distance, in time proportional to the product of their lengths\[Wag74\].
34.4 Word Al ignment FilterLanguages with a similar syntax tend to express ideas in similar order.
The translation of aword occurring: at the end of a French sentence is likely to occur towards the end of the Englishtranslation.
In general, lines drawn between corresponding lexemes in a French sentence and its3Due to time constraints, I report results for a greedy approximation of the LCSR.
A proper implementationmight perform even better.191Les neo-democrates ont aussi paris de General Motors dans ce contexte.The NDP Members also mentioned General Motors in this context .Figure 3: Word Alignment Filter - -  Partitioning loci marked "d" are translation pairs found inthe MRBD, while those marked "c" are cognates.
The remaining uncertainties are marked withdashed lines.
The Word Alignment Filter removes from consideration candidate translation pairslike (ont, mentioned) which would cross the partition created by (aussi, also).ABCDEFG/ \ \ \,.~abcdefgh iFigure 4: One of the heuristics used in the Word Alignment Filter - -  Crossing partitions areminimized by aligning D with g rather than with c.English translation will be mostly parallel.
This idea of translation alignment was central to themachine translation method pioneered at IBM \[Bro93\].The Word Alignment Filter exploits this observation, as illustrated in Figure 3.
If word T ina target sentence is the translation of word S in the corresponding source sentence, then wordsoccurring before S in the source sentence will likely correspond to words occurring before T in thetarget sentence.
Likewise, words occurring after S in the source sentence will likely translate towords occurring after T in the target sentence.
So S and T can be used as loci for partitioningthe source and target sentences into two shorter pairs of corresponding word strings.
Each suchpartition reduces the number of candidate translations from each sentence pair by approximatelya factor of two - -  an excellent noise filter for the decision procedure.The Word Alignment Filter is particularly useful when oracle lists are available to identify alarge number of translation pairs that can be used to partition sentences.
Using a LCSR cut-offof 0.58 (optimized using BIBLE, of course), cognates were found for 23% of the source tokens inthe training corpus (counting punctuation).
47% of the source tokens were found in the MRBD.Although there was some overlap, an average of 63% of the words in each sentence were paired upwith a cognate or with a translation found in the MRBD, leaving few candidate translations forthe remaining 37%.The oracles lists often supplied more than one match per word.
For instance, several determinersor prepositions in the French sentence often matched the same word in the English sentence.
When192this happened, the current implementation of the Word Alignment Filter used several heuristics tochoose at most one partitioning locus per word.
For example, one heuristic says that the order ofideas in a sentence is not likely to change during translation.
So, it aimed to minimize crossingpartitions, as shown in Figure 4.
If word A matches word e, and word D matches words c andg, then D is paired with g, so that when the sentences are written one above the other, the linesconnecting the matching words do not cross.
Between French and English, this heuristic worksquite well, except when it comes to the order between ouns and adjectives.L4.5 Eva luat ionTable 1 is unusual: It is atypical for more than two of the filters studied here to incrementallyimprove one lexicon entry.
Most lexicon entries are improved by just one or two filters, after whichmore filtering .gives no significant benefit.
However, each filter improves a large number of differententries.
Two more examples of the benefits of different filter cascades are given in Tables 3 and 4.Table 3: lexicon entries for French "grand" in 7-best lexicons generated with different filters - -The baseline lexicon has correct entries only for the most likely translation and for the second mostlikely translation.
The POS Filter throws out nouns and pronouns, and makes room for "high"and "vast."
The Word Alignment Filter removes enough noise to capture "high, .... vast," "giant,"and "extensive" all at once.~ Entry #1234567No Filters(baseline)greatlargecorporation'SmoreonedevelopmentalPOS FiltergreatlargehighdevelopmentalhumbleundeniablevastCognate & POS Filterswith Word Alignmentgreatlargehighvasthumblegiantextens iveTable 4: lexicon entries for French "parti" in 7-best lexicons generated with different filters - -Only the most  likely translation and the fourth most likely translation in the baseline lexicon areappropriate.
The Cognate Filter allows the fourth item, a cognate, to percolate up to second place,and makes room for "two-party" in sixth place.Entry # No Filters Cognate Filter1 Par ty2 Liberal3 Democratic4 par ty5 Conservative6 new7 thePar typar tystretchhandbookespousetwo-par tybetweenFigures 5 and 6 show mean BIBLE scores for precision of the best translations in lexicons inducedwith various cascades of the four filters discussed.
Assuming that BIBLE scores are normally1930.6.mO,f -  vc0CL20ooW.
.
.
Jt~00.58 ........................................................................................................................................................................................human performance0 .56 ................................................................................................................................................................................ i  -,=0.54 ............................................................................................................................................................................ ~- -o .
.
.
.
.
.
.
.
i0.52  ........... ~ ........................ - , : - : - : : : : :~-1 .
: :~-~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- , zzz :~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.\ [ \ ]  ............ i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~._ i ............................... ~ ............... i ?0.5 .......... i ............................................................... : :=- : : : .
.
: : : : .~: .~:u .
i i i~ .  "
.................................. i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.0.48 .......... ~ ................ =.=-==.-~::~:~-::~ ~.
..................... ; ......................................................................... _~ LL:~.C':c:X-0 .46 .................................................................................................................................................................................... i .0.57  'POS,  Cognate  &i MRBD Filters wth  Wo~d A gnment  -~- - i0 .44  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
P .OS,C .~gnate  & .ME IBD E It~s...:+::.
:.i..POS & MRBD Filters .-~.-.
!MRBD Filter Only -x---.~0 .42 i I500  1000 2000 5000 10000pairs of training sentencesFigure 5: The large MRBD resulted in the most useful filter for this pair of languages.
The scoresfor the cascade of all the filters (the highest curve) are close to the human performance of 0.57..m0vC.9oO.0o u~IL lO3c?00E0.420.40 .380.360 .340 .320 .30.280.260 .240 .22. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
~.:.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.~:~::.....-..::.
- -  ~ : /  :-: ;..--~-~ .
.
.
.
.
.
.
.
.
.
~.
._\[ ........... .
.
\ [3  o~.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
: :>~'~"~:"~"~:::  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
!.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ :~-.......... ~: : : : : : : : : : : : : :~ : : : : :~ : :$ :~S : :~f  ..................... i .............................................. i .. ~ :7~ ......................i l j~  i. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i...~1 : , .~ :~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1.~.
~ i.
.~-~ ~ POS & Cognate  Filters -~---i........................ i~:7,~:c ........ ~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
!..
POS.F i l ter  .~3~,,.!..-.
.
.
.
.
.
.
i C.Ognate Filter --x--.-i~--~ i basel ine (no filters) -~---~I I I I500 1000 2000 5000 10000pairs of training sentencesFigure 6: Each filter contributes to an improvement in BIBLE scores.1940.62  i_ .
\ [~.
_ - -  :0.6  .................................................................. " .............................................. ~ .................... :----~=: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: : .
.
- "  ....... x0.58 : " : .....c ............... i.................................................. : i .................. :::.i ................. :::/ii......~...::~i~ ............................................. i .. ..... ._o ;0.56o.
0 .542 i / "  .." ........ ~ /  i !o 10.52.
J  , .
/ ,,: , ,  ,,.
/L~ 0 .5  .................................. ; ....... ~ '~ ..................................................................................................................... ?
.." /," ....-/'"/ C0gnate  & MRBD F i l te rs  w i th  Word  A l ignment  o"'" ."
' MRBD F i l te r  --F-~10.48 ................... ::,-:-..---i.:z;!/ .................................................................................................. C6gr ia .
teF i l te i "  :~: : ;  .......', d" ...-.S/ : :basel ine (no  f i l te rs )  ...x..-....i ..... , POS F i l te r  -~-+0 .46  : " "  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
; .
.
.
/~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
: . '
t: i0.44 i IBest  T rans la t ion  Best  o f  2 Best  o f  3 Best  o f  4Figure 7: Some useful filter cascades for training corpora as large as 100000 sentence pairs.
TheCognate Filter !by itself achieves the best precision for the best-of-N translations, when N > 2.The POS Filter~ only degrades precision for large training corpora.idistributed, 95% confidence intervals were estimated for each score, using ten mutually exclusivetraining sets of each size.
All the confidence intervals were narrower than one percentage pointat 500 pairs of!training sentences, and narrower than half of one percentage point at 2000 pairs.bTherefore, BIBLE score differences displayed in Figures 5 and 6 are quite reliable.The upper bound on performance for this task is plotted at 0.57 (see end of Section 3).
Thebetter filter cashade produce lexicons whose precision comes close to this mark.
The best cascadesare up to 137% :more precise than the baseline model.
The large MRBD resulted in the most usefulfilter for this pair of languages.
Future research will look into why the MRBD's contribution tolexicon precision decreases with more training data.Figure 7 shows the relative performance of selected filters when the entire training set of onehundred thousand sentences i used.
All the presented filters, except the POS Filter, improveperformance even when a large training corpus is available.
Evidently, some information that isuseful for inducing translation lexicons cannot be inferred from any amount of training data usingonly simple statistical methods.
The best precision for the single best translation is achieved by acascade of the MRBD, Cognate and Word Alignment Filters.
To maximize precision for the bestof three or more translations, only the Cognate Filter should be used.19500o~0LUmm40383634323028262422i.......................... i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i ...........................................................................................i : i ~ i ..--bFi / /  i.
/  : !.
.
.
.
.
: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
.
.
.
~<~.
.
.
.
.
.d  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
; -i j *.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
S .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: /i / ,, lexicons Cascaded in order of precision 0 i~ ,," baseline lexicon induced without any filters -?-.~...... 7?
.......................................................................................................................................................................... 7 i /  !
"r" iI I500 1000 2000 5000 10000 100000pairs of training sentencesFigure 8: Percent Correct by token - Data filters can improve these scores by more than 35%.5 APPL ICAT ION TO MACHINE-ASS ISTED TRANSLAT IONA machine translation system should not only translate with high precision, but it should alsohave good coverage of the source language.
So, the product of recall and precision, percent correct,is a good indication of a lexicon's suitability for use with such a system.
This statistic actuallyrepresents the percentage of words in the target test corpus that would be correctly translated fromthe source, if the lexicon were used as a simple map, Therefore, if the lexicon is to be used as part ofa machine-assisted translation system, then the percent correct score will be inversely proportionalto the required post-editing time.A simple strategy was adopted to demonstrate the practical utility of filters presented in thispaper.
First, the most precise filter cascade was selected by looking at Figure 5.
Translationswere found for all words in the test source text that had entries in the lexicon induced using thatcascade.
Then the second most precise filter cascade was selected.
Words that the most preciselexicon "didn't know about," which were found in the second most precise lexicon, were translatednext.
All the other available lexicons were cascaded this way, in the order of their apparentprecision, down to the baseline lexicon.
This "cascaded back-off" strategy maintained the recallof the baseline lexicon, while taking advantage of the higher precision produced by various filtercascades.Although more sophisticated translation strategies are certainly possible, BIBLE percent correctscores for cascaded lexicons suffice to test the utility of data filters for machine translation.
Theresults in Figure 8 indicate that the filters described in this paper can be used to improve theperformance of lexical transfer models by more than 35%.1966 CONCLUSIONSThe research presented here makes several contributions to research in machine translation andrelated fields:?
a uniform framework for combining various data filters with statistical methods tbr illducingN-best translation lexicons,?
an automatic evaluation method for translation lexicons which obviates the need for labor-intensive subjective valuation by human judges,?
four different ways to improve statisticM translation models,?
a demonstration of how tiny training corpora can be enhanced with non-statistical knowledgesources to induce better lexicons than unenhanced training corpora many times the size.The effectiveness of different data filters for inducing translation lexicons crucially depends onthe particular pair of languages under consideration.
Cognates are more common, and thereforemore useful, in languages which are more closely related.
For example, one would expect to findmore cognates between Russian and Ukrainian than between French and English.
The implemen-tation of a part of speech filter for a given pair of languages depends on the availability of partof speech taggers for both languages, where the two taggers have a small common tag set.
Theeffectiveness of oracle filters based on MRBDs will depend o11 the extent to which the vocabularyof the MRBD intersects with the vocabulary of the training text.
This, in turn, depends partlyon the size of the MRBD.
Filters based on word alignment patterns will only be as good as themodel of typical word alignments between the pair of languages in question.
For languages withvery similar syntax, a linear model will suffice.
Higher order models will be required for a pair oflanguages like English and Japanese.For the case of French and English, each of the presented filters makes a significant improve-ment over the baseline model.
Taken together, the filters produce models which approach humanperformance.
These conclusions could not have been drawn without a uniform framework for filtercomparison or without a technique for automatic evaluation.
An automatic ewluation techniquesuch as BIBLE should be used to gauge the effectiveness of any MT system which has a lexicaltransfer component.
BiBLE's objective criterion is quite simple, with the drawback that it gives noindication of what kinds of errors exist in the lexicon being evaluated.
Even so, given a test corpusof a reasonable size, it can detect very small differences in quality between two N-best translationlexicons.
For example, BIBLE evaluations were used to find the precise optimum value for theLCSR cut-off in the Cognate Filter.
BIBLE also helped to select the optimum tag set for the POSFilter.
This kind of automatic quality control is indispensable for an engineering approach to bettermachine translation.7 ACKNOWLEDGEMENTSI am deeply grateful to George Foster for POS-tagging the French half of my text corpus, toMatthew Stone for providing a second translation of some Hansard text, and to the followingpeople for valuable advice and discussions: Ken Church, Michael Collins, Jason Eisner, GeorgeFoster, Mark Liberman, Mitch Marcus, Adwait Ratnaparkhi, Jeff Reynar, Henry Thompson, DavidYarowsky, and four anonymous reviewers.
This research was partially supported by ARO ContractDAAL03-89-C0031 and by ARPA Contract N6600194-c6043.197References\[Bri92\] E. Brill, "A Simple Rule-Based Part of Speech Tagger," Proceedings of the 3rd Conferenceon Applied Natural Language PTvcessing, pp.
152-155, 1992.\[Bro94\] P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, M. J. Goldsmith, J. Hajic, R. L. Mercer& S. Mohanty, "But Dictionaries are Data Too," in Proceedings of the ARPA HLT Workshop,Princeton, N J, 1993.\[Bro93\] P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & R. L. Mercer, "The Mathematicsof Statistical Machine Translation: Parameter Estimation," Computational Linguistics 19(2),1993.\[Cou91\] P. H. Cousin, L. Sinclair, J. F. Allain & C. E. Love, The Collins Paperback French Dic-tionary, Harper Collins Publishers, Glasgow, 1991.\[Dag93\] I. Dagan, K. Church, ~ W. Gale, "Robust Word Alignment for Machine AidedTranslation," Proceedings of the Workshop on Very Large Corpora: Academic and IndustrialPerspectives, available from the ACL, pp.
1-8, 1993.\[Dun93\] T. Dunning, "Accurate Methods for the Statistics of Surprise and Coincidence," Compu-tational Linguistics 19(1), 1993.\[Gal91a\] W. Gale & K. W. Church, "Identifying Word Correspondences in Parallel Texts," Pro-ceedings of the DARPA SNL Workshop, 1991.\[Gal91b\] W. Gale, & K. W. Church, "A Program for Aligning Sentences in Bilingual Corpora"Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,Berkeley, Ca., 1991.\[Mac94\] E. Macklovitch, "Using Bi-textual alignment for Translation Validation: the TransCheckSystem," Proceedings of the First Conference of the Association for Machine Translation inthe Americas, Columbia, MD, 1994.\[Sim92\] M. Simard, G.F. Foster & P. Isabelle, "Using Cognates to Align Sentences in BilingualCorpora," in Proceedings of the Fourth International Conference on Theoretical and Method-ological Issues in Machine Translation, Montreal, Canada, 1992.\[Wag74\] R. A. Wagner & M. J. Fischer, "The String-to-String Correction Problem," Journal of theACM 21(1), pp.
168-173, 1974.\[Whi93\] J. S. White & T. A. O'Connell, "Evaluation of Machine Translation," in Proceedings ofthe ARPA HLT Workshop, Princeton, NJ, 1993.\[Wu94\] D. Wu & P. Fung, "Improving Chinese Tokenization with Linguistic Filters on StatisticalLexical Acquisition," Proceedings of the Conference on Applied Natural Language Processing,1994.198
