Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 449?456,Sydney, July 2006. c?2006 Association for Computational LinguisticsDirect Word Sense Matching for Lexical SubstitutionIdo Dagan1, Oren Glickman1, Alfio Gliozzo2, Efrat Marmorshtein1, Carlo Strapparava21Department of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel2ITC-Irst, via Sommarive, I-38050, Trento, ItalyAbstractThis paper investigates conceptually andempirically the novel sense matching task,which requires to recognize whether thesenses of two synonymous words match incontext.
We suggest direct approaches tothe problem, which avoid the intermediatestep of explicit word sense disambigua-tion, and demonstrate their appealing ad-vantages and stimulating potential for fu-ture research.1 IntroductionIn many language processing settings it is neededto recognize that a given word or term may be sub-stituted by a synonymous one.
In a typical in-formation seeking scenario, an information needis specified by some given source words.
Whenlooking for texts that match the specified need thesource words might be substituted with synony-mous target words.
For example, given the sourceword ?weapon?
a system may substitute it with thetarget synonym ?arm?.This scenario, which is generally referred hereas lexical substitution, is a common techniquefor increasing recall in Natural Language Process-ing (NLP) applications.
In Information Retrieval(IR) and Question Answering (QA) it is typicallytermed query/question expansion (Moldovan andMihalcea, 2000; Negri, 2004).
Lexical Substi-tution is also commonly applied to identify syn-onyms in text summarization, for paraphrasing intext generation, or is integrated into the features ofsupervised tasks such as Text Categorization andInformation Extraction.
Naturally, lexical substi-tution is a very common first step in textual en-tailment recognition, which models semantic in-ference between a pair of texts in a generalized ap-plication independent setting (Dagan et al, 2005).To perform lexical substitution NLP applica-tions typically utilize a knowledge source of syn-onymous word pairs.
The most commonly usedresource for lexical substitution is the manuallyconstructed WordNet (Fellbaum, 1998).
Anotheroption is to use statistical word similarities, suchas in the database constructed by Dekang Lin (Lin,1998).
We generically refer to such resources assubstitution lexicons.When using a substitution lexicon it is assumedthat there are some contexts in which the givensynonymous words share the same meaning.
Yet,due to polysemy, it is needed to verify that thesenses of the two words do indeed match in a givencontext.
For example, there are contexts in whichthe source word ?weapon?
may be substituted bythe target word ?arm?
; however one should recog-nize that ?arm?
has a different sense than ?weapon?in sentences such as ?repetitive movements couldcause injuries to hands, wrists and arms.
?A commonly proposed approach to addresssense matching in lexical substitution is applyingWord Sense Disambiguation (WSD) to identifythe senses of the source and target words.
Then,substitution is applied only if the words have thesame sense (or synset, in WordNet terminology).In settings in which the source is given as a sin-gle term without context, sense disambiguationis performed only for the target word; substitu-tion is then applied only if the target word?s sensematches at least one of the possible senses of thesource word.One might observe that such application of WSDaddresses the task at hand in a somewhat indi-rect manner.
In fact, lexical substitution only re-quires knowing that the source and target senses449do match, but it does not require that the match-ing senses will be explicitly identified.
Selectingexplicitly the right sense in context, which is thenfollowed by verifying the desired matching, mightbe solving a harder intermediate problem than re-quired.
Instead, we can define the sense match-ing problem directly as a binary classification taskfor a pair of synonymous source and target words.This task requires to decide whether the senses ofthe two words do or do not match in a given con-text (but it does not require to identify explicitlythe identity of the matching senses).A highly related task was proposed in (Mc-Carthy, 2002).
McCarthy?s proposal was to asksystems to suggest possible ?semantically similarreplacements?
of a target word in context, wherealternative replacements should be grouped to-gether.
While this task is somewhat more com-plicated as an evaluation setting than our binaryrecognition task, it was motivated by similar ob-servations and applied goals.
From another per-spective, sense matching may be viewed as a lex-ical sub-case of the general textual entailmentrecognition setting, where we need to recognizewhether the meaning of the target word ?entails?the meaning of the source word in a given context.This paper provides a first investigation of thesense matching problem.
To allow comparisonwith the classical WSD setting we derived anevaluation dataset for the new problem from theSenseval-3 English lexical sample dataset (Mihal-cea and Edmonds, 2004).
We then evaluated alter-native supervised and unsupervised methods thatperform sense matching either indirectly or di-rectly (i.e.
with or without the intermediate senseidentification step).
Our findings suggest that inthe supervised setting the results of the direct andindirect approaches are comparable.
However, ad-dressing directly the binary classification task haspractical advantages and can yield high precisionvalues, as desired in precision-oriented applica-tions such as IR and QA.More importantly, direct sense matching setsthe ground for implicit unsupervised approachesthat may utilize practically unlimited volumesof unlabeled training data.
Furthermore, suchapproaches circumvent the sisyphean need forspecifying explicitly a set of stipulated senses.We present an initial implementation of such anapproach using a one-class classifier, which istrained on unlabeled occurrences of the sourceword and applied to occurrences of the targetword.
Our current results outperform the unsuper-vised baseline and put forth a whole new directionfor future research.2 WSD and Lexical ExpansionDespite certain initial skepticism about the useful-ness of WSD in practical tasks (Voorhees, 1993;Sanderson, 1994), there is some evidence thatWSD can improve performance in typical NLPtasks such as IR and QA.
For example, (Shu?tzeand Pederson, 1995) gives clear indication of thepotential for WSD to improve the precision of an IRsystem.
They tested the use of WSD on a standardIR test collection (TREC-1B), improving precisionby more than 4%.The use of WSD has produced successful exper-iments for query expansion techniques.
In partic-ular, some attempts exploited WordNet to enrichqueries with semantically-related terms.
For in-stance, (Voorhees, 1994) manually expanded 50queries over the TREC-1 collection using syn-onymy and other WordNet relations.
She foundthat the expansion was useful with short and in-complete queries, leaving the task of proper auto-matic expansion as an open problem.
(Gonzalo et al, 1998) demonstrates an incre-ment in performance over an IR test collection us-ing the sense data contained in SemCor over apurely term based model.
In practice, they ex-perimented searching SemCor with disambiguatedand expanded queries.
Their work shows thata WSD system, even if not performing perfectly,combined with synonymy enrichment increasesretrieval performance.
(Moldovan and Mihalcea, 2000) introduces theidea of using WordNet to extend Web searchesbased on semantic similarity.
Their results showedthat WSD-based query expansion actually im-proves retrieval performance in a Web scenario.Recently (Negri, 2004) proposed a sense-basedrelevance feedback scheme for query enrichmentin a QA scenario (TREC-2003 and ACQUAINT),demonstrating improvement in retrieval perfor-mance.While all these works clearly show the potentialusefulness of WSD in practical tasks, nonethelessthey do not necessarily justify the efforts for refin-ing fine-grained sense repositories and for build-ing large sense-tagged corpora.
We suggest thatthe sense matching task, as presented in the intro-450duction, may relieve major drawbacks of applyingWSD in practical scenarios.3 Problem Setting and DatasetTo investigate the direct sense matching problemit is necessary to obtain an appropriate dataset ofexamples for this binary classification task, alongwith gold standard annotation.
While there isno such standard (application independent) datasetavailable it is possible to derive it automaticallyfrom existing WSD evaluation datasets, as de-scribed below.
This methodology also allowscomparing direct approaches for sense matchingwith classical indirect approaches, which apply anintermediate step of identifying the most likelyWordNet sense.We derived our dataset from the Senseval-3 En-glish lexical sample dataset (Mihalcea and Ed-monds, 2004), taking all 25 nouns, adjectives andadverbs in this sample.
Verbs were excluded sincetheir sense annotation in Senseval-3 is not basedon WordNet senses.
The Senseval dataset includesa set of example occurrences in context for eachword, split to training and test sets, where each ex-ample is manually annotated with the correspond-ing WordNet synset.For the sense matching setting we need exam-ples of pairs of source-target synonymous words,where at least one of these words should occur ina given context.
Following an applicative moti-vation, we mimic an IR setting in which a sin-gle source word query is expanded (substituted)by a synonymous target word.
Then, it is neededto identify contexts in which the target word ap-pears in a sense that matches the source word.
Ac-cordingly, we considered each of the 25 words inthe Senseval sample as a target word for the sensematching task.
Next, we had to pick for each targetword a corresponding synonym to play the role ofthe source word.
This was done by creating a listof all WordNet synonyms of the target word, underall its possible senses, and picking randomly oneof the synonyms as the source word.
For example,the word ?disc?
is one of the words in the Sense-val lexical sample.
For this target word the syn-onym ?record?
was picked, which matches ?disc?in its musical sense.
Overall, 59% of all possiblesynsets of our target words included an additionalsynonym, which could play the role of the sourceword (that is, 41% of the synsets consisted of thetarget word only).
Similarly, 62% of the test exam-ples of the target words were annotated by a synsetthat included an additional synonym.While creating source-target synonym pairs itwas evident that many WordNet synonyms corre-spond to very infrequent senses or word usages,such as the WordNet synonyms germ and source.Such source synonyms are useless for evaluat-ing sense matching with the target word since thesenses of the two words would rarely match in per-ceivable contexts.
In fact, considering our motiva-tion for lexical substitution, it is usually desired toexclude such obscure synonym pairs from substi-tution lexicons in practical applications, since theywould mostly introduce noise to the system.
Toavoid this problem the list of WordNet synonymsfor each target word was filtered by a lexicogra-pher, who excluded manually obscure synonymsthat seemed worthless in practice.
The source syn-onym for each target word was then picked ran-domly from the filtered list.
Table 1 shows the 25source-target pairs created for our experiments.
Infuture work it may be possible to apply automaticmethods for filtering infrequent sense correspon-dences in the dataset, by adopting algorithms suchas in (McCarthy et al, 2004).Having source-target synonym pairs, a classifi-cation instance for the sense matching task is cre-ated from each example occurrence of the targetword in the Senseval dataset.
A classification in-stance is thus defined by a pair of source and targetwords and a given occurrence of the target word incontext.
The instance should be classified as pos-itive if the sense of the target word in the givencontext matches one of the possible senses of thesource word, and as negative otherwise.
Table 2illustrates positive and negative example instancesfor the source-target synonym pair ?record-disc?,where only occurrences of ?disc?
in the musicalsense are considered positive.The gold standard annotation for the binarysense matching task can be derived automaticallyfrom the Senseval annotations and the correspond-ing WordNet synsets.
An example occurrence ofthe target word is considered positive if the an-notated synset for that example includes also thesource word, and Negative otherwise.
Notice thatdifferent positive examples might correspond todifferent senses of the target word.
This happenswhen the source and target share several senses,and hence they appear together in several synsets.Finally, since in Senseval an example may be an-451source-target source-target source-target source-target source-targetstatement-argument subdivision-arm atm-atmosphere hearing-audience camber-banklevel-degree deviation-difference dissimilar-different trouble-difficulty record-discraging-hot ikon-image crucial-important sake-interest bare-simpleopinion-judgment arrangement-organization newspaper-paper company-party substantial-solidexecution-performance design-plan protection-shelter variety-sort root-sourceTable 1: Source and target pairssentence annotationThis is anyway a stunning disc, thanks to the playing of the Moscow Virtuosi with Spivakov.
positiveHe said computer networks would not be affected and copies of information should be made onfloppy discs.negativeBefore the dead soldier was placed in the ditch his personal possessions were removed, leavingone disc on the body for identification purposesnegativeTable 2: positive and negative examples for the source-target synonym pair ?record-disc?notated with more than one sense, it was consid-ered positive if any of the annotated synsets for thetarget word includes the source word.Using this procedure we derived gold standardannotations for all the examples in the Senseval-3 training section for our 25 target words.
For thetest set we took up to 40 test examples for each tar-get word (some words had fewer test examples),yielding 913 test examples in total, out of which239 were positive.
This test set was used to eval-uate the sense matching methods described in thenext section.4 Investigated MethodsAs explained in the introduction, the sense match-ing task may be addressed by two general ap-proaches.
The traditional indirect approach wouldfirst disambiguate the target word relative to a pre-defined set of senses, using standard WSD meth-ods, and would then verify that the selected sensematches the source word.
On the other hand, adirect approach would address the binary sensematching task directly, without selecting explicitlya concrete sense for the target word.
This sectiondescribes the alternative methods we investigatedunder supervised and unsupervised settings.
Thesupervised methods utilize manual sense annota-tions for the given source and target words whileunsupervised methods do not require any anno-tated sense examples.
For the indirect approachwe assume the standard WordNet sense repositoryand corresponding annotations of the target wordswith WordNet synsets.4.1 Feature set and classifierAs a vehicle for investigating different classifica-tion approaches we implemented a ?vanilla?
stateof the art architecture for WSD.
Following com-mon practice in feature extraction (e.g.
(Yarowsky,1994)), and using the mxpost1 part of speech tag-ger and WordNet?s lemmatization, the followingfeature set was used: bag of word lemmas for thecontext words in the preceding, current and fol-lowing sentence; unigrams of lemmas and partsof speech in a window of +/- three words, whereeach position provides a distinct feature; and bi-grams of lemmas in the same window.
The SVM-Light (Joachims, 1999) classifier was used in thesupervised settings with its default parameters.
Toobtain a multi-class classifier we used a standardone-vs-all approach of training a binary SVM foreach possible sense and then selecting the highestscoring sense for a test example.To verify that our implementation provides areasonable replication of state of the art WSD weapplied it to the standard Senseval-3 Lexical Sam-ple WSD task.
The obtained accuracy2 was 66.7%,which compares reasonably with the mid-range ofsystems in the Senseval-3 benchmark (Mihalceaand Edmonds, 2004).
This figure is just a fewpercent lower than the (quite complicated) bestSenseval-3 system, which achieved about 73% ac-curacy, and it is much higher than the standardSenseval baselines.
We thus regard our classifieras a fair vehicle for comparing the alternative ap-proaches for sense matching on equal grounds.1ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz2The standard classification accuracy measure equals pre-cision and recall as defined in the Senseval terminology whenthe system classifies all examples, with no abstentions.4524.2 Supervised Methods4.2.1 Indirect approachThe indirect approach for sense matching fol-lows the traditional scheme of performing WSDfor lexical substitution.
First, the WSD classifierdescribed above was trained for the target wordsof our dataset, using the Senseval-3 sense anno-tated training data for these words.
Then, the clas-sifier was applied to the test examples of the targetwords, selecting the most likely sense for each ex-ample.
Finally, an example was classified as pos-itive if the selected synset for the target word in-cludes the source word, and as negative otherwise.4.2.2 Direct approachAs explained above, the direct approach ad-dresses the binary sense matching task directly,without selecting explicitly a sense for the targetword.
In the supervised setting it is easy to ob-tain such a binary classifier using the annotationscheme described in Section 3.
Under this schemean example was annotated as positive (for the bi-nary sense matching task) if the source word isincluded in the Senseval gold standard synset ofthe target word.
We trained the classifier using theset of Senseval-3 training examples for each tar-get word, considering their derived binary anno-tations.
Finally, the trained classifier was appliedto the test examples of the target words, yieldingdirectly a binary positive-negative classification.4.3 Unsupervised MethodsIt is well known that obtaining annotated trainingexamples for WSD tasks is very expensive, andis often considered infeasible in unrestricted do-mains.
Therefore, many researchers investigatedunsupervised methods, which do not require an-notated examples.
Unsupervised approaches haveusually been investigated within Senseval usingthe ?All Words?
dataset, which does not includetraining examples.
In this paper we preferred us-ing the same test set which was used for the super-vised setting (created from the Senseval-3 ?Lexi-cal Sample?
dataset, as described above), in orderto enable comparison between the two settings.Naturally, in the unsupervised setting the sense la-bels in the training set were not utilized.4.3.1 Indirect approachState-of-the-art unsupervised WSD systems arequite complex and they are not easy to be repli-cated.
Thus, we implemented the unsupervisedversion of the Lesk algorithm (Lesk, 1986) as areference system, since it is considered a standardsimple baseline for unsupervised approaches.
TheLesk algorithm is one of the first algorithms de-veloped for semantic disambiguation of all-wordsin unrestricted text.
In its original unsupervisedversion, the only resource required by the algo-rithm is a machine readable dictionary with onedefinition for each possible word sense.
The algo-rithm looks for words in the sense definitions thatoverlap with context words in the given sentence,and chooses the sense that yields maximal wordoverlap.
We implemented a version of this algo-rithm using WordNet sense-definitions with con-text length of ?10 words before and after the tar-get word.4.3.2 The direct approach: one-class learningThe unsupervised settings for the direct methodare more problematic because most of unsuper-vised WSD algorithms (such as the Lesk algo-rithm) rely on dictionary definitions.
For this rea-son, standard unsupervised techniques cannot beapplied in a direct approach for sense matching, inwhich the only external information is a substitu-tion lexicon.In this subsection we present a direct unsuper-vised method for sense matching.
It is based onthe assumption that typical contexts in which boththe source and target words appear correspond totheir matching senses.
Unlabeled occurrences ofthe source word can then be used to provide evi-dence for lexical substitution because they allowus to recognize whether the sense of the targetword matches that of the source.
Our strategy isto represent in a learning model the typical con-texts of the source word in unlabeled training data.Then, we exploit such model to match the contextsof the target word, providing a decision criterionfor sense matching.
In other words, we expect thatunder a matching sense the target word would oc-cur in prototypical contexts of the source word.To implement such approach we need a learningtechnique that does not rely on the availability ofnegative evidence, that is, a one-class learning al-gorithm.
In general, the classification performanceof one-class approaches is usually quite poor, ifcompared to supervised approaches for the sametasks.
However, in many practical settings one-class learning is the only available solution.For our experiments we adopted the one-classSVM learning algorithm (Scho?lkopf et al, 2001)453implemented in the LIBSVM package,3 and repre-sented the unlabeled training examples by adopt-ing the feature set described in Subsection 4.1.Roughly speaking, a one-class SVM estimates thesmallest hypersphere enclosing most of the train-ing data.
New test instances are then classifiedpositively if they lie inside the sphere, while out-liers are regarded as negatives.
The ratio betweenthe width of the enclosed region and the numberof misclassified training examples can be variedby setting the parameter ?
?
(0, 1).
Smaller val-ues of ?
will produce larger positive regions, withthe effect of increasing recall.The appealing advantage of adopting one-classlearning for sense matching is that it allows us todefine a very elegant learning scenario, in which itis possible to train ?off-line?
a different classifierfor each (source) word in the lexicon.
Such a clas-sifier can then be used to match the sense of anypossible target word for the source which is givenin the substitution lexicon.
This is in contrast tothe direct supervised method proposed in Subsec-tion 4.2, where a different classifier for each pairof source - target words has to be defined.5 Evaluation5.1 Evaluation measures and baselinesIn the lexical substitution (and expansion) set-ting, the standard WSD metrics (Mihalcea and Ed-monds, 2004) are not suitable, because we are in-terested in the binary decision of whether the tar-get word matches the sense of a given source word.In analogy to IR, we are more interested in positiveassignments, while the opposite case (i.e.
when thetwo words cannot be substituted) is less interest-ing.
Accordingly, we utilize the standard defini-tions of precision, recall and F1 typically used inIR benchmarks.
In the rest of this section we willreport micro averages for these measures on thetest set described in Section 3.Following the Senseval methodology, we evalu-ated two different baselines for unsupervised andsupervised methods.
The random baseline, usedfor the unsupervised algorithms, was obtained bychoosing either the positive or the negative classat random resulting in P = 0.262, R = 0.5,F1 = 0.344.
The Most Frequent baseline hasbeen used for the supervised algorithms and is ob-tained by assigning the positive class when the3Freely available from www.csie.ntu.edu.tw//?cjlin/libsvm.percentage of positive examples in the training setis above 50%, resulting in P = 0.65, R = 0.41,F1 = 0.51.5.2 Supervised MethodsBoth the indirect and the direct supervised meth-ods presented in Subsection 4.2 have been testedand compared to the most frequent baseline.Indirect.
For the indirect methodology wetrained the supervised WSD system for each tar-get word on the sense-tagged training sample.
Asdescribed in Subsection 4.2, we implemented asimple SVM-based WSD system (see Section 4.2)and applied it to the sense-matching task.
Resultsare reported in Table 3.
The direct strategy sur-passes the most frequent baseline F1 score, but theachieved precision is still below it.
We note that inthis multi-class setting it is less straightforward totradeoff recall for precision, as all senses competewith each other.Direct.
In the direct supervised setting, sensematching is performed by training a binary clas-sifier, as described in Subsection 4.2.The advantage of adopting a binary classifica-tion strategy is that the precision/recall tradeoffcan be tuned in a meaningful way.
In SVM learn-ing, such tuning is achieved by varying the param-eter J , that allows us to modify the cost functionof the SVM learning algorithm.
If J = 1 (default),the weight for the positive examples is equal to theweight for the negatives.
When J > 1, negativeexamples are penalized (increasing recall), while,whenever 0 < J < 1, positive examples are penal-ized (increasing precision).
Results obtained byvarying this parameter are reported in Figure 1.Figure 1: Direct supervised results varying J454Supervised P R F1 Unsupervised P R F1Most Frequent Baseline 0.65 0.41 0.51 Random Baseline 0.26 0.50 0.34Multiclass SVM Indirect 0.59 0.63 0.61 Lesk Indirect 0.24 0.19 0.21Binary SVM (J = 0.5) Direct 0.80 0.26 0.39 One-Class ?
= 0.3 Direct 0.26 0.72 0.39Binary SVM (J = 1) Direct 0.76 0.46 0.57 One-Class ?
= 0.5 Direct 0.29 0.56 0.38Binary SVM (J = 2) Direct 0.68 0.53 0.60 One-Class ?
= 0.7 Direct 0.28 0.36 0.32Binary SVM (J = 3) Direct 0.69 0.55 0.61 One-Class ?
= 0.9 Direct 0.23 0.10 0.14Table 3: Classification results on the sense matching taskAdopting the standard parameter settings (i.e.J = 1, see Table 3), the F1 of the systemis slightly lower than for the indirect approach,while it reaches the indirect figures when J in-creases.
More importantly, reducing J allows usto boost precision towards 100%.
This feature isof great interest for lexical substitution, particu-larly in precision oriented applications like IR andQA, for filtering irrelevant candidate answers ordocuments.5.3 Unsupervised methodsIndirect.
To evaluate the indirect unsupervisedsettings we implemented the Lesk algorithm, de-scribed in Subsection 4.3.1, and evaluated it onthe sense matching task.
The obtained figures,reported in Table 3, are clearly below the base-line, suggesting that simple unsupervised indirectstrategies cannot be used for this task.
In fact, theerror of the first step, due to low WSD accuracyof the unsupervised technique, is propagated inthe second step, producing poor sense matching.Unfortunately, state-of-the-art unsupervised sys-tems are actually not much better than Lesk on all-words task (Mihalcea and Edmonds, 2004), dis-couraging the use of unsupervised indirect meth-ods for the sense matching task.Direct.
Conceptually, the most appealing solu-tion for the sense matching task is the one-classapproach proposed for the direct method (Section4.3.2).
To perform our experiments, we trained adifferent one-class SVM for each source word, us-ing a sample of its unlabeled occurrences in theBNC corpus as training set.
To avoid huge train-ing sets and to speed up the learning process, wefixed the maximum number of training examplesto 10000 occurrences per word, collecting on av-erage about 6500 occurrences per word.For each target word in the test sample, we ap-plied the classifier of the corresponding sourceword.
Results for different values of ?
are reportedin Figure 2 and summarized in Table 3.Figure 2: One-class evaluation varying ?While the results are somewhat above the base-line, just small improvements in precision are re-ported, and recall is higher than the baseline for?
< 0.6.
Such small improvements may suggestthat we are following a relevant direction, eventhough they may not be useful yet for an appliedsense-matching setting.Further analysis of the classification results foreach word revealed that optimal F1 values are ob-tained by adopting different values of ?
for differ-ent words.
In the optimal (in retrospect) param-eter settings for each word, performance for thetest set is noticeably boosted, achieving P = 0.40,R = 0.85 and F1 = 0.54.
Finding a principled un-supervised way to automatically tune the ?
param-eter is thus a promising direction for future work.Investigating further the results per word, wefound that the correlation coefficient between theoptimal ?
values and the degree of polysemy ofthe corresponding source words is 0.35.
More in-terestingly, we noticed a negative correlation (r= -0.30) between the achieved F1 and the degreeof polysemy of the word, suggesting that polyse-mous source words provide poor training modelsfor sense matching.
This can be explained by ob-serving that polysemous source words can be sub-stituted with the target words only for a strict sub-455set of their senses.
On the other hand, our one-class algorithm was trained on all the examplesof the source word, which include irrelevant ex-amples that yield noisy training sets.
A possiblesolution may be obtained using clustering-basedword sense discrimination methods (Pedersen andBruce, 1997; Schu?tze, 1998), in order to train dif-ferent one-class models from different sense clus-ters.
Overall, the analysis suggests that future re-search may obtain better binary classifiers basedjust on unlabeled examples of the source word.6 ConclusionThis paper investigated the sense matching task,which captures directly the polysemy problem inlexical substitution.
We proposed a direct ap-proach for the task, suggesting the advantages ofnatural control of precision/recall tradeoff, avoid-ing the need in an explicitly defined sense reposi-tory, and, most appealing, the potential for novelcompletely unsupervised learning schemes.
Wespeculate that there is a great potential for suchapproaches, and suggest that sense matching maybecome an appealing problem and possible trackin lexical semantic evaluations.AcknowledgmentsThis work was partly developed under the collab-oration ITC-irst/University of Haifa.ReferencesIdo Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailmentchallenge.
Proceedings of the PASCAL ChallengesWorkshop on Recognising Textual Entailment.C.
Fellbaum.
1998.
WordNet.
An Electronic LexicalDatabase.
MIT Press.J.
Gonzalo, F. Verdejo, I. Chugur, and J. Cigarran.1998.
Indexing with wordnet synsets can improvetext retrieval.
In ACL, Montreal, Canada.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In B. Scho?lkopf, C. Burges, and A. Smola,editors, Advances in kernel methods: support vectorlearning, chapter 11, pages 169 ?
184.
MIT Press.M.
Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of theACM-SIGDOC Conference, Toronto, Canada.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Proceedings of the 17thinternational conference on Computational linguis-tics, pages 768?774, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Automatic identification of infre-quent word senses.
In Proceedings of COLING,pages 1220?1226.Diana McCarthy.
2002.
Lexical substitution as a taskfor wsd evaluation.
In Proceedings of the ACL-02 workshop on Word sense disambiguation, pages109?115, Morristown, NJ, USA.
Association forComputational Linguistics.R.
Mihalcea and P. Edmonds, editors.
2004.
Proceed-ings of SENSEVAL-3: Third International Workshopon the Evaluation of Systems for the Semantic Anal-ysis of Text, Barcelona, Spain, July.D.
Moldovan and R. Mihalcea.
2000.
Using wordnetand lexical operators to improve internet searches.IEEE Internet Computing, 4(1):34?43, January.M.
Negri.
2004.
Sense-based blind relevance feedbackfor question answering.
In SIGIR-2004 Workshopon Information Retrieval For Question Answering(IR4QA), Sheffield, UK, July.T.
Pedersen and R. Bruce.
1997.
Distinguishing wordsense in untagged text.
In EMNLP, Providence, Au-gust.M.
Sanderson.
1994.
Word sense disambiguation andinformation retrieval.
In SIGIR, Dublin, Ireland,June.B.
Scho?lkopf, J. Platt, J. Shawe-Taylor, A. J. Smola,and R. C. Williamson.
2001.
Estimating the supportof a high-dimensional distribution.
Neural Compu-tation, 13:1443?1471.H.
Schu?tze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1).H.
Shu?tze and J. Pederson.
1995.
Information retrievalbased on word senses.
In Proceedings of the 4thAnnual Symposium on Document Analysis and In-formation Retrieval, Las Vegas.E.
Voorhees.
1993.
Using WordNet to disambiguateword sense for text retrieval.
In SIGIR, Pittsburgh,PA.E.
Voorhees.
1994.
Query expansion using lexical-semantic relations.
In Proceedings of the 17th ACMSIGIR Conference, Dublin, Ireland, June.D.
Yarowsky.
1994.
Decision lists for lexical ambi-guity resolution: Application to accent restorationin spanish and french.
In ACL, pages 88?95, LasCruces, New Mexico.456
