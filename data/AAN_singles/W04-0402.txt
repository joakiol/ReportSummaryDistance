Paraphrasing of Japanese Light-verb ConstructionsBased on Lexical Conceptual StructureAtsushi Fujita?
Kentaro Furihata?
Kentaro Inui?
Yuji Matsumoto?
Koichi Takeuchi?
?Graduate School of Information Science,Nara Institute of Science and Technology{atsush-f,kenta-f,inui,matsu}@is.naist.jp?Department of Information Technology,Okayama Universitykoichi@it.okayama-u.ac.jpAbstractSome particular classes of lexical paraphrases suchas verb alteration and compound noun decomposi-tion can be handled by a handful of general rulesand lexical semantic knowledge.
In this paper, weattempt to capture the regularity underlying theseclasses of paraphrases, focusing on the paraphras-ing of Japanese light-verb constructions (LVCs).We propose a paraphrasing model for LVCs thatis based on transforming the Lexical ConceptualStructures (LCSs) of verbal elements.
We also pro-pose a refinement of an existing LCS dictionary.
Ex-perimental results show that our LCS-based para-phrasing model characterizes some of the semanticfeatures of those verbs required for generating para-phrases, such as the direction of an action and therelationship between arguments and surface cases.1 IntroductionAutomatic paraphrase generation technology offersthe potential to bridge gaps between the authors andreaders of documents.
For example, a system thatis capable of simplifying a given text, or showingthe user several alternative expressions conveyingthe same content, would be useful for assisting areader (Carroll et al, 1999; Inui et al, 2003).In Japanese, like other languages, there are sev-eral classes of paraphrasing that exhibit a degreeof regularity that allows them to be explained bya handful of sophisticated general rules and lexicalsemantic knowledge.
For example, paraphrases as-sociated with voice alteration, verb/case alteration,compounds, and lexical derivations all fall into suchclasses.
In this paper, we focus our discussion onanother useful class of paraphrases, namely, theparaphrasing of light-verb constructions (LVCs),and propose a computational model for generatingparaphrases of this class.Sentence (1s) is an example of an LVC1.
An LVCis a verb phrase (?kandou-o ataeta (made an impres-sion)?
in (1s)) that consists of a light-verb (?ataeta(give-PAST)?)
that grammatically governs a nomi-1For each example, s denotes an input and t denotes its para-phrase.nalized verb (?kandou (an impression)?)
(also seeFigure 1 in Section 2.2).
A paraphrase of (1s) is sen-tence (1t), in which the nominalized verb functionsas the main verb with its verbal form (?kandou-s-ase-ta (be impressed-CAU, PAST)?).
(1) s. Eiga-ga kare-ni kandou-o ataeta.film-NOM him-DAT impression-ACC give-PASTThe film made an impression on him.t.
Eiga-ga kare-o kandou-s-ase-ta.film-NOM him-ACC be impressed-CAUSATIVE, PASTThe film impressed him.To generate this type of paraphrase, we need a com-putational model that is capable of the followingtwo classes of choice (also see Section 2.2):Selection of the voice: The model needs to be ableto choose the voice of the target sentence fromactive, passive, causative, etc.
In example (1),the causative voice is chosen, which is indi-cated by the auxiliary verb ?ase (causative)?.Reassignment of the cases: The model needs tobe able to reassign a case marker to each ar-gument of the main verb.
In (1), the gram-matical case of ?kare (him),?
which was orig-inally assigned the dative case, is changed toaccusative.The task is not as simple as it may seem, becauseboth decisions depend not only on the syntactic andsemantic attributes of the light-verb, but also onthose of the nominalized verb (Muraki, 1991).In this paper, we propose a novel lexicalsemantics-based account of the LVC paraphrasing,which uses the theory of Lexical Conceptual Struc-ture (LCS) of Japanese verbs (Kageyama, 1996;Takeuchi et al, 2001).
The theory of LCS offersan advantage as the basis of lexical resources forparaphrasing, because it has been developed to ex-plain varieties of linguistic phenomena includinglexical derivations, the construction of compounds,and verb alteration (Levin, 1993; Dorr et al, 1995;Kageyama, 1996; Takeuchi et al, 2001), all ofwhich are associated with the systematic paraphras-ing we mentioned above.The paraphrasing associated with LVCs is not id-iosyncratic to Japanese but also appears commonlySecond ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp.
9-16in other languages such as English (Mel?c?uk andPolgue`re, 1987; Iordanskaja et al, 1991; Dras,1999, etc.
), as indicated by the following examples.
(2) s. Steven made an attempt to stop playing.t.
Steven attempted to stop playing.
(3) s. It had a noticeable effect on the trade.t.
It noticeably affected the trade.Our approach raises the interesting issue of whetherthe paraphrasing of LVCs can be modeled in ananalogous way across languages.Our aim in this paper are: (i) exploring the reg-ularity of the LVC paraphrasing based a lexicalsemantics-based account, and (ii) assessing the im-mature Japanese semantic typology through a prac-tical task.The following sections describe our motiva-tion, target, and related work on LVC paraphras-ing (Section 2), the basics of LCS and the refine-ments we made (Section 3), our paraphrasing model(Section 4), and our experiments (Section 5).
Fi-nally, we conclude this paper with a brief of descrip-tion of work to be done in the future (Section 6).2 Motivation, target, and related work2.1 MotivationOne of the critical issues that we face in para-phrase generation is how to develop and maintainknowledge resources that covers a sufficiently widerange of paraphrasing patterns such as those in-dicating that ?to make an attempt?
can be para-phrased into ?to attempt,?
and that ?potential?
canbe paraphrased into ?possibility.?
Several attemptshave been made to develop such resources manually(Sato, 1999; Dras, 1999; Inui and Nogami, 2001);those work have, however, tended to restrict theirscope to specific classes of paraphrases, and cannotbe used to construct a sufficiently comprehensive re-source for practical applications.There is another trend in the research in this field,namely, the automatic acquisition of paraphrase pat-terns from parallel or comparable corpora (Barzilayand McKeown, 2001; Lin and Pantel, 2001; Pang etal., 2003; Shinyama and Sekine, 2003, etc.).
Thistype of approach may be able to reduce the costof resource development.
There are problems thatmust be overcome, however, before they can workpractically.
First, automatically acquired patternstend to be complex.
For example, from the para-phrase of (4s) into (4t), we can naively obtain thepattern: ?X is purchased by Y ?
Y buys X.?
(4) s. This car was purchased by him.t.
He bought this car.This could also, however, be regarded as a combi-nation of a simpler pattern of lexical paraphrasing(?purchase ?
buy?)
and a voice activization (?XAdjectiveNoun + Case particle"no"(GEN)Nominalized verb + Case particleNoun + Case ParticleAdverbLight-verb (+suffixes)Embedded clauseLVCTarget of this paper(a)(b)(c)(d)(e)Figure 1: Dependency structure showing the rangewhich the LVC paraphrasing affects.be VERB-PP by Y ?
Y VERB X?).
If we were touse an acquisition scheme that is not capable of de-composing such complex paraphrases correctly, wewould have to collect a combinatorial number ofparaphrases to gain the required coverage.
Second,the results of automatic acquisition would likely in-clude many inappropriate patterns, which would re-quire manual correction.
Manual correction, how-ever, would be impractical if we were collecting acombinatorial number of patterns.Our approach to this dilemma is as follows: first,we manually develop the resources needed to coverthose paraphrases that appear regularly, and then de-compose and automatically refine the acquired para-phrasing patterns using those resources.
The workreported in this paper is aimed at this resource de-velopment.2.2 Target structure and required operationsFigure 1 shows the range which the LVC para-phrasing affects, where the solid boxes denoteJapanese base-chunk so-called ?bunsetsu.
?2 Beinginvolved in the paraphrasing, the modifiers of theLVC need the following operations:Change of the dependence: The dependences ofthe elements (a) and (b) need to be changedbecause the original modifiee, the light-verb,is eliminated by the paraphrasing.Re-conjugation: The conjugation form of the ele-ments (d), (e), and occasionally (c) need to bechanged according to the category change oftheir modifiee, the nominalized verb.Reassignment of the cases: As described in theprevious section, the case markers of the ele-ments (b) and often (c) need to be reassigned.Selection of the voice: The voice of the nominal-ized verb needs to be chosen according to thecombination of the nominalized verb, the light-verb, and the original voice.The first two operations are trivial in the field oftext generation.
Moreover, they can be done inde-pendently of the LVC paraphrasing.
The most deli-cate operation is for the element (c) because it actseither as an adverb or as a case, relying on the con-2The modifiee of the LVC is not affected because the part-of-speech of the light-verb and main verb are the same.Table 1: Examples of LCSVerb LCS for verb Verb phrasemove [y MOVE TO z] My sister (Theme) moves to a neighboring town (Goal).transmit [x CONTROL [y MOVE TO z]] The enzyme (Agent) transmits messages (Theme) to the muscles (Goal).locate [y BE AT z] The school (Theme) locates near the river (Goal).maintain [x CONTROL [y BE AT z]] He (Agent) maintains a machine (Theme) in good condition (Goal).text.
In the former case, it needs the second opera-tion.
In the latter case, it needs the third operationas well as the element (b).In this paper, we take into account only the ele-ment (b), namely, the sibling cases of the nominal-ized verb.2.3 Related workBased on the Meaning-Text Theory (Mel?c?uk andPolgue`re, 1987), Iordanskaja et al (1991) pro-poses a set of paraphrasing rules including one forLVC paraphrasing.
Their rule heavily relies on whatare called lexical functions, by which they virtuallyspecify all the choices relevant to LVC paraphras-ing for every combination of nominalized verb andlight-verb individually.
Our approach is to employlexical semantics to provide a general account ofthose classes of choices.On the other hand, Kaji and Kurohashi (2004)proposes a paraphrasing model which bases on anordinary dictionary.
Given an input LVC, theirmodel paraphrases it using the gloss of both thenominalized verb and the light-verb with the seman-tic feature of the light-verb.
Their model looks ro-bust because of the availability of an ordinary dic-tionary.
However, their model fails to explain thedifference in the voice selection between examples(5) and (6) since it selects the voice based onlyon the light-verb ?
in their approach, the light-verb ?ukeru (to receive)?
always maps to the passivevoice irrespective of the nominalized verb.
(5) s. Enkai-eno shoutai-o uketa.party-GEN invitation-ACC receive-PASTI received an invitation to the party.t.
Enkai-ni shoutai-s-are-ta.party-DAT invite-PAS, PASTI was invited to the party.
(6) s. Kare-no hanashi-nihis-GEN talk-DATkandou-o uketa.impression-ACC receive-PASTI was given a good impression by his talk.t.
Kare-no hanashi-ni kandou-shi-ta.his-GEN talk-DAT be impressed-ACT, PASTI was impressed by his talk.In (Kaji and Kurohashi, 2004), the target expres-sion is restricted only to the LVC itself (also seeFigure 1).
Hence, their model is unable to reassignthe cases as we saw in example (1).3 Lexical Conceptual Structure3.1 Basic framework of LCSThe theory of Lexical Conceptual Structure(LCS) associates a verb with a semantic struc-ture as exemplified by Table 1.
An LCS consistsof semantic predicates (?CONTROL,?
?BE AT,?etc.)
and their argument slots (x, y, z).
Argumentslots x, y, and z correspond to the semantic roles?Agent,?
?Theme,?
and ?Goal,?
respectively.
Tak-ing the LCS of the verb ?transmit?
as an example,[y MOVE TO z] denotes the state of affairs that thestate of the ?Theme?
changes to the ?Goal,?
and[x CONTROL .
.
.]
denotes that the ?Agent?
causes thestate change.3.2 RefinementsWe make use of the TLCS dictionary, a Japaneseverb LCS dictionary developed by Takeuchi et al(2001), because it offers the following advantages:?
It is based on solid linguistic work, as in(Kageyama, 1996).?
Its scale is considerably larger than any otherexisting collections of verb LCS entries.?
It provides a set of concrete rules for LCS as-signment, which ensures the reliability of thedictionary.In spite of these advantages, our preliminary ex-amination of the dictionary revealed that further re-finements were needed.
To refine the typology ofTLCS, we collected the following sets of words:Nominalized verbs: We regard ?sahen-nouns?4and nominal forms of verbs as nominalizedverbs.
We retrieved 1,210 nominalized verbsfrom the TLCS dictionary.Light-verbs: Since a verb takes different meaningswhen it is a part of LVCs with different caseparticles, we collected pairs ?c, v?
of case par-ticle c and verb v in the following way:Step 1.
We collected 876,101 types of triplets?n, c, v?
of nominalized verb n, case par-ticle c, and base form of verb v from theparsed5 sentences of newspaper articles6.4A sahen-noun is a verbal noun in Japanese, which acts asa verb in the form of ?sahen-noun + suru?.5We used the statistical Japanese dependency parserCaboCha (Kudo and Matsumoto, 2002) for parsing.http://chasen.naist.jp/?taku/software/cabocha/6Excerpts from 9 years of the Mainichi Shinbun and 10years of the Nihon Keizai Shinbun, giving a total of 25,061,504sentences, were used.Table 2: Extensions of LCSVerb Verb phrase and its LCS representationExt.1 hankou-suru [[Ken]y BE AGAINST [parents]z](resist) Ken-ga oya-ni hankou-suru.Ken-NOM parents-DAT resist-PRES (Ken resists his parents.
)Ext.2 ukeru [BECOME [[salesclerk]z BE WITH [[complaint]y MOVE FROM [customer]x TO [salesclerk]z]]](receive) Ten?in-ga kyaku-kara kujo-o ukeru.salesclerk-NOM customer-ABL complaint-ACC receive-PRES(The salesclerk receives a complaint from a customer.
)Ext.3 motomeru [[Ken]x CONTROL [[apology]y MOVE FROM [George]z TO [FILLED]]]3(ask) Ken-ga George-ni shazai-o motomeru.Ken-NOM George-DAT apology-ACC ask-PRES (Ken asks George for an apology.
)Ext.4 kandou-suru [BECOME [[Ken]z BE WITH [[FILLED]y MOVE FROM [music]x TO [Ken]z]]](be impressed) Ken-ga ongaku-ni kandou-suru.Ken-NOM music-DAT be impressed-PRES (Ken is impressed by the music.
)Step 2.
For each of the 50 most frequent ?c, v?tuples, we extracted the 10 most frequent?n, c, v?.Step 3.
Each ?n, c, v?
was manually evaluatedto determine whether it was an LVC.
Ifany of 10 triplets was determined to bean LVC, ?c, v?
was merged into the list oflight-verbs.
As a result, we collected 40types of ?c, v?
for light-verbs.Through investigating the above 1,210 nominal-ized verbs and 40 light-verbs, we extended the ty-pology of TLCS as shown below (also see Table 2).Ext.
1.
Treatment of ?Partner?
: The dative caseof ?hankou-suru (resist)?
and ?eikyo-suru (af-fect)?
does not indicate the ?Goal?
of the ac-tion but the ?Partner.?Ext.
2.
Verbs of obtaining (Levin, 1993): In con-trast with ?ataeru (give),?
the nominative caseof ?ukeru (receive)?
and ?eru (acquire)?
is the?Goal?
of the ?Theme,?
while the ablative caseindicates ?Source.?Ext.
3.
Require verb: ?motomeru (ask)?
and?yokyu-suru (require)?
denote the existence ofthe external ?Agent?
who controls the action ofthe other ?Agent?
or ?Theme.?Ext.
4.
Verbs of psychological state (Levin,1993): ?kandou-suru (be impressed)?
and ?os-oreru (fear)?
indicate the change of psycholog-ical state of the ?Agent.?
The ascriptive part ofthe change has to be described.Consequently, we defined a new LCS typologyconsisting of 16 types.
Note that more than one LCScan be assigned to a verb if it has a polysemy.
Forconvenience, we refer to the extended dictionary asthe LCSdic7.6The predicate ?FILLED?
represents an implicit argumentof the verb and the verb assigned this LCS cannot take thisargument.
Taking the LCS of the verb ?sign?
as an example,?FILLED?
in [x CONTROL [BECOME [[FILLED]y BE ATz]]] denotes the name of ?Agent.
?7The latest version of the LCSdic is available fromhttp://cl.it.okayama-u.ac.jp/rsc/lcs/4 Paraphrasing modelIn this section, we describe how we generate para-phrases of LVCs.
Figure 2 illustrates how our modelparaphrases the LVC of example (7).
(7) s. Ken-ga eiga-ni shigeki-o uketa.Ken-NOM film-DAT inspiration-ACC receive-PASTKen received inspiration from the film.t.
Ken-ga eiga-ni shigeki-s-are-ta.Ken-NOM film-DAT inspire-PAS, PASTKen was inspired by the film.The idea is to exploit the LCS representation as asemantic representation and to model the LVC para-phrasing by the transformation of the LCS represen-tation.
The process consists of the following threesteps:Step 1.
Semantic analysis: The model first ana-lyzes a given input sentence including an LVCto obtain its semantic structure in terms of theLCS representation.
In Figure 2, this step pro-duces LCSV 1.Step 2.
Semantic transformation (LCS transfor-mation): The model then transfers the ob-tained semantic structure to another semanticstructure so that the target structure consists ofthe LCS of the nominalized verb of the input.In our example, this step generates LCSN1to-gether with the supplement [BECOME [.
.
.
]].Step 3.
Surface generation: Having obtained thetarget LCS representation, the model finallylexicalizes it to generate the output sentence.So, the key issue is how to control the second step,namely, the transformation of the LCS representa-tion.The rest of this section elaborates on each step,using different symbols to denote arguments; x, y,and z for LCSV, and x?, y?, and z?
for LCSN.4.1 Semantic analysisGiven an input sentence, which we assume to be asimple clause with an LVC, we first look up the LCStemplate LCSV 0for the given light-verb, and thenapply the case assignment rule, below (Takeuchi et(2) LCS transformation(3) Surface generationLCS dictionary[x?
ACT ON y?
][BECOME [z BE WITH [y MOVE FROM x TO z]]]Paraphrased sentenceInput sentenceukeru (receive)shigeki-suru (inspire)Ken-ga (Ken-NOM) eiga-ni (film-DAT)shigeki-o (inspiration-ACC) uketa (receive-PAST).[[film]x?
ACT ON [Ken]y?
][BECOME [[Ken]z BE WITH[[inspiration]y MOVE FROM [film]x TO [Ken]z]]][BECOME [[Ken]z BE WITH]] +LCSV0LCSV1LCSN0LCSN1Ken-ga (Ken-NOM) eiga-ni (film-DAT)shigeki-s-are-ta (inspire-PAS, PAST).
(1) Semantic analysisFigure 2: The LCS-based paraphrasing model.al., 2001), to obtain its LCS representation LCSV 1:Case assignment rule:?
In the case of the LCSV 0having argument x,fill the leftmost argument of the LCSV 0withthe nominative case of the input, the secondleftmost with the accusative, and the rest withthe dative.?
Otherwise, fill arguments y and z of the LCSV 0with the nominative and the dative, respec-tively.In the example shown in Figure 2, the nominative?Ken?
fills the leftmost argument z. Accordingly,the accusative ?shigeki (inspiration)?
and the dative?eiga (film)?
fill y and x, respectively.
(8) s. Ken-ga eiga-ni shigeki-o uketa.Ken-NOM film-DAT inspiration-ACC receive-PASTKen received inspiration from the film.LCSV 0[BECOME [z BE WITH [y MOVE FROM xTO z]]]LCSV 1[BECOME [[Ken]z BE WITH [[inspiration]yMOVE FROM [film]x TO [Ken]z]]]4.2 LCS transformationThe second step of our paraphrasing modelmatches the resultant LCS representation (LCSV 1in Figure 2) with the LCS of the nominalized verb(LCSN0) to generate the target LCS representation(LCSN1).
Figure 3 shows a more detailed view ofthis process for the example shown in Figure 2.4.2.1 Predicate matchingThe first step is to determine the predicate inLCSV 1that should be matched with the predicatein LCSN0.
Assuming that only the agentivity is rel-evant to the selection of the voice in the paraphras-ing of LVC, which is our primary concern, we clas-sify the semantic predicates into the following twoclasses:Agentive predicates: ?CONTROL,?
?ACT ON,??ACT,?
?BE AGAINST,?
and ?MOVE FROMTO.?[[film]x?
ACT ON [Ken]y?
][BECOME [[Ken]z BE WITH]] +[BECOME [[Ken]z BE WITH[[inspiration]y MOVE FROM [film]x TO [Ken]z]]][x?
ACT ON y?
](b) Argument matching(a) Predicate matching(c) Attaching the remaining structureLCSN0LCSN1LCSV1Figure 3: LCS transformation.State of affair predicates: ?MOVE TO,?
?BEAT,?
and ?BE WITH.
?Aspectual predicates: ?BECOME.
?We also assume that any pair of predicates ofthe same class is allowed to match, and that theaspectual predicates are ignored.
In our example,?MOVE FROM TO?
matches ?ACT ON,?
as shownin Figure 3.LCS representations have right-branching (orright-embedding) structures.
Since inner-embeddedpredicates denote the state of affairs, they take pri-ority in the matching.
In other words, the matchingproceeds from the rightmost inner predicates to theouter predicates.Having matched the predicates, we then fill eachargument slot in LCSN0with its corresponding ar-gument in LCSV 1.
In Figure 3, argument z ismatched with y?, and x with x?.
As a result, ?Ken?comes to the y?
slot and ?eiga (film)?
comes to thex?
slot8.This process is repeated until the leftmost predi-cate in LCSN0or that in LCSV 1is matched.4.2.2 Treatment of non-transfered predicatesIf LCSV 1has any non-transfered predicates whenthe predicate matching has been completed, theyrepresent the semantic content that is not covered byLCSN1and which needs to be lexicalized by aux-iliary linguistic devices such as voice auxiliaries.In the case of Figure 3, [BECOME [[Ken]z BE WITH]]in LCSV 1remains non-transfered.
In such a case,we attach the non-transfered predicates to LCSN0,which are then lexicalized by auxiliaries in the nextstep, the surface generation.4.3 Surface generationWe again apply the aforementioned case assignmentrule to generate a sentence from the resultant LCSrepresentation.
In this process, the model makes thefinal decisions on the selection of the voice and thereassignment of the cases, according to the follow-ing decision list:8When an argument is filled with another LCS, argumentswithin the inner LCS are also matched.
Likewise, with regardto an assumption that the input sentences are periphrastic, weintroduced some exceptional rules.
That is, arguments filledwith the implicit filler represented by ?FILLED?
or the targetnominalized verb N are never matched, and ?Goal?
in LCSV 1can be matched to ?Theme?
in LCSN0.1.
If the attached predicate is filled with the sameargument as the leftmost argument in LCSN1,the ?active?
voice is selected and the casestructure is left as is.2.
If the argument of the attached predicate hasthe same value as either z?
or y?
in LCSN1,lexicalization is performed to make the argu-ment a subject.
Therefore, the ?passive?
voiceis selected and case alternation (passivization)is applied.3.
If the attached predicate is ?BE WITH?
and itsargument has the same value as x?
in LCSN1,the ?causative?
voice is selected and case alter-nation (causativizaton) is applied.4.
If the attached predicate is an agentive predi-cate, and its argument is filled with a value dif-ferent from those of the other arguments, thenthe ?causative?
voice is selected and case alter-nation (causativization) is applied.5.
Otherwise, no modification is applied.Since the example in Figure 2 satisfies the secondcondition, the model chooses ?s-are-ru (passive)?and passivizes the sentence so that ?Ken?
fills thenominative case.
(9) LCSN1[BECOME [[Ken]z BE WITH]]+ [[film]x?
ACT ON [Ken]y?]t.
Ken-ga eiga-ni shigeki-s-are-ta.Ken-NOM film-DAT inspire-PAS, PASTKen was inspired by the film.5 Experiment5.1 Paraphrase generation and evaluationTo empirically evaluate our paraphrasing model andthe LCSdic, and to clarify the remaining problems,we analyzed a set of automatically generated para-phrase candidates.
The sentences used in the exper-iment were collected in the following way:Step 1.
From the 876,101 types of triplet ?n, c, v?collected in Section 3.2, 23,608 types of?n, c, v?
were extracted, whose components, nand ?c, v?, are listed in the LCSdic.Step 2.
For each of the 245 most frequent ?n, c, v?,the 3 most frequent simple clauses includ-ing the ?n, c, v?
were extracted from the cor-pus from which ?n, c, v?
s were extracted inSection 3.2.
As a result, we collected 735 sen-tences.Step 3.
We input these 735 sentences into our para-phrasing model, and then automatically gener-ated paraphrase candidates.
When more thanone LCS is assigned to a verb in the LCSdicdue to its polysemy or ergative verb such as?kaifuku-suru (recover),?
our model generatesall the possible paraphrase candidates.
As a re-sult, 825 paraphrase candidates, that is, at leastone for each input, were generated.Table 3: Error sourcesCorrect candidates 621 (75.8%)Erroneous candidates 198 (24.2%)Definition of LCS 30LCS for light-verb 24LCS for nominalized verb 6Paraphrasing model 61LCS transformation algorithm 59Treatment of ?suru (to do)?
2Ambiguity 107Ambiguous thematic role of dative 78Recognition of LVC 24Selection of transitive/intransitive 5We manually classified the resultant 825 para-phrase candidates into 621 correct and 198 erro-neous candidates.
The remaining 6 candidates werenot classified.
The precision of the paraphrase gen-eration was 75.8% (621 / 819).5.2 Error analysisTo clarify the cause of the erroneous paraphrases,we manually classified 198 erroneous paraphrasecandidates.
Table 3 lists the error sources.5.2.1 LCS transformation algorithmThe experiment came close to confirming that theright-first matching algorithm in our paraphrasingmodel operates correctly.
Unfortunately, the match-ing rules produced some erroneous paraphrases inLCS transformation.Errors in predicate matching: To paraphrase(10s) below, ?CONTROL?
in LCSV 1must bematched with ?CONTROL?
in LCSN0, and x to x?.However, our model first matched ?CONTROL?
inLCSV 1with ?MOVE FROM TO?
in LCSN0.
Thus,x was incorrectly matched with z?
and x?
remainedempty.
The desired form of LCSN1is shown in(11).
(10) s. kacho-ga buka-nisection-chief-NOM subordinate-DATshiji-o dasu.order-ACC issue-PRESThe section chief issues orders to his subordinates.
(N=?order?, V =?issue?
)LCSV 1[[chief ]x CONTROL [BECOME [[order]yBE AT [subordinate]z]]]LCSN0[x?
CONTROL [y?
MOVE FROM z?
TO[FILLED]]]LCSN1?[x?
CONTROL [[subordinate]y?
MOVEFROM [chief ]z?
TO [FILLED]]](11) LCSN1[[chief ]x?
CONTROL [y?
MOVE FROM[subordinate] TO [FILLED]]]This error was caused by the mis-matching of?CONTROL?
with ?MOVE FROM TO.?
Althoughwe regard some predicates as being in the sameclasses as those described in Section 4.2.1, theseneed to be considered carefully.
In particular?MOVE FROM TO?
needs further investigation be-cause it causes many errors whenever it has the?FILLED?
argument.Errors in argument matching: Even if all thepredicates are matched properly, there would stillbe a chance of errors being caused by incorrect ar-gument matching.
With the present algorithm, zcan be matched with y?
if and only if z?
contains?FILLED.?
In the case of (12), however, z has tobe matched with y?, even though z?
is empty.
Thedesired form of LCSN1is shown in (13).
(12) s. Jikan-ni seigen-ga aru.time-DAT limitation-NOM exist-PRESThere is a time limitation.
(N=?limitation?, V =?exist?
)LCSV 1[BECOME [[limitation]y BE AT [time]z]]LCSN0[x?
CONTROL [BECOME [y?
BE AT z?]]]LCSN1?[x?
CONTROL [BECOME [y?
BE AT[time]z?
]]](13) LCSN1[x?
CONTROL [BECOME [[timey]y?
BE ATz?
]]]5.2.2 Ambiguous thematic role of dativeIn contrast to dative cases in English, in Japanese,the dative case has ambiguity.
That is, it can be acomplement to the verb or an adjunct9.
However,since LCS is not capable of determining whether thecase is a complement or an adjunct, z is occasion-ally incorrectly filled with an adjunct.
For exam-ple, ?medo-ni?
in (14s) should not fill z, because itacts as an adverb, even though it consists of a noun,?medo (prospect)?
and a case particle for the dative.We found that 78 erroneous candidates constitutethis most dominant type of errors.
(14) s. Kin?you-o medo-ni sagyo-o susumeru.Friday-NOM by-DAT work-ACC carry on-PRESI plan to finish the work by Friday.
(N=?work?, V =?carry?
)LCSV 0[x CONTROL [BECOME [y BE AT z]]]LCSV 1?
[x CONTROL [BECOME [[work]y BE AT[by]z]]]The ambiguity of dative cases in Japanese hasbeen discussed in the literature of linguistics andsome natural language processing tasks (Muraki,1991).
To date, however, a practical compli-ment/adjunct classifier has not been established.
Weplan to address this topic in our future research.Preliminary investigation revealed that only cer-tain groups of nouns can constitute both compli-ments and adjuncts according to the governing verb.Therefore, generally whether a word acts as a com-plement is determined without combining it with theverb.9(Muraki, 1991) classifies dative cases into 11 thematicroles that can be regarded as complements.
In contrast, thereis no typology of dative cases that act as adjuncts.5.2.3 Recognition of LVCIn our model, we assume that a triplet ?n, c, v?
con-sisting of a nominalized verb n and a light-verb tu-ple ?c, v?
from our vocabulary lists (see Section 3.2)always act as an LVC.
However, not only the tripletitself but also its context sometimes affects whetherthe given triplet can be paraphrased.
For exam-ple, we regard ?imi-ga aru?
as an LVC, because thenominalized verb ?imi?
and the tuple ?
?ga?, ?aru?
?appear in the vocabulary lists.
However, the ?n, c, v?in (15s) does not act as an LVC, while the sametriplet in (16s) does.
(15) s. Sanka-suru-koto-ni imi-ga aru.to participate-DAT meaning-NOM exist-PRESThere is meaning in participating.t.
?Sanka-suru-koto-o imi-suru.to participate-ACC mean-ACT, PRES?It means to participate in it.
(16) s. ?kennel?-niwa inugoya-toiu?kennel?-TOP doghouse-OFimi-ga aru.meaning-NOM exist-PRES?kennel?
has the meaning of doghouse.t.
?kennel?-wa inugoya-o imi-suru.
?kennel?-TOP doghouse-ACC mean-ACT, PRES?kennel?
means doghouse.The above difference is caused by the polysemyof the nominalized verb ?imi?
that denotes ?worth?in the context of (15s), but ?meaning?
in (16s).Although incorporating word sense disambiguationusing contextual clues complicates our model, infact only a limited number of nominalized verbs arepolysemous.
We therefore expect that we can listthem up and use this as a trigger for making a deci-sion as to whether we need to take the context intoaccount.
Namely, given a ?n, c, v?, we would beable to classify it into (a) a main verb phrase, (b) adelicate case in terms of the dependence of its con-text, and (c) an LVC.We can adopt a different approach to avoidingincorrect paraphrase generation.
As described inSection 5.1, our model generates all the possibleparaphrase candidates when more than one LCS isassigned to a verb.
Similarly, our approach can beextended to (i) over-generate paraphrase candidatesby considering the polysemy of not only assignedLCS types, but also that of nominalized verbs (see(15s) and (16s)) and whether the given ?n, c, v?
isan LVC, and (ii) revise or reject the incorrect candi-dates by using handcrafted solid rules or statisticallanguage models.6 Conclusion and future workIn this paper, we presented an LCS-based para-phrasing model for LVCs and an extension of an ex-isting LCS dictionary.
Our model achieved an accu-racy of 75.8% in selecting the voice and reassigningthe cases.To make our paraphrasing model more accurate,further analysis is needed, especially for the LCStransformation stage described in Section 4.2.
Sim-ilarly, several levels of disambiguation should alsobe solved.
The Japanese LCS typology has to berefined from the theoretical point of view.
For ex-ample, since extensions are no more than human in-tuition, we must discuss how we can assign LCSsfor given verbs based on explicit language tests, asdescribed in (Takeuchi et al, 2001).In future research, we will also extend our LCS-based approach to other classes of paraphrases thatexhibit some regularity, such as verb alteration andcompound noun decomposition as shown in (17)and (18), below.
LCS has been discussed as ameans of explaining the difference between transi-tive/intransitive verbs, and the construction of com-pounds.
Therefore, our next goal is to show the ap-plicability of LCS through practical tasks, namely,paraphrasing.
(17) s. Jishin-ga building-o kowashita.earthquake-NOM building-DAT destroy-PASTThe earthquake destroyed the building.t.
Jishin-de building-ga kowareta.earthquake-LOC building-NOM be destroyed-PASTThe building was destroyed in the earthquake.
(18) s. Kare-wa kikaihe-TOP machine-sousa-ga jouzu-da.operation-NOM good-COPULAHe is good at operating the machine.t.
Kare-wa kikai-o jouzu-ni sousa-suru.he-TOP machine-DAT well-ADV operate-PRESHe operates machines well.ReferencesR.
Barzilay and K. R. McKeown.
2001.
Extracting para-phrases from a parallel corpus.
In Proceedings of the39th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 50?57.J.
Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-vlin, and J. Tait.
1999.
Simplifying text for language-impaired readers.
In Proceedings of the 9th Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL), pages 269?270.B.
J. Dorr, J. Garman, and A. Weinberg.
1995.
Fromsyntactic encodings to thematic roles: building lexi-cal entries for interlingual MT.
Machine Translation,9(3):71?100.M.
Dras.
1999.
Tree adjoining grammar and the reluc-tant paraphrasing of text.
Ph.D. thesis, Department ofComputing, Macquarie University.K.
Inui and M. Nogami.
2001.
A paraphrase-basedexploration of cohesiveness criteria.
In Proceedingsof the 8th European Workshop on Natulal LanguageGeneration (EWNLG), pages 101?110.K.
Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.2003.
Text simplification for reading assistance: aproject note.
In Proceedings of the 2nd InternationalWorkshop on Paraphrasing: Paraphrase Acquisitionand Applications (IWP), pages 9?16.L.
Iordanskaja, R. Kittredge, and A. Polgue`re.
1991.Lexical selection and paraphrase in a meaning-textgeneration model.
In Paris et al (Eds.)
Natural Lan-guage Generation in Artificial Intelligence and Com-putational Linguistics, pages 293?312.
Kluwer Aca-demic Publishers.T.
Kageyama, editor.
1996.
Verb semantics.
KuroshioPublishers.
(in Japanese).N.
Kaji and S. Kurohashi.
2004.
Recognition and para-phrasing of periphrastic and overlapping verb phrases.In Proceedings of the 4th International Conference onLanguage Resources and Evaluation (LREC) Work-shop on Methodologies and Evaluation of MultiwordUnits in Real-world Application.T.
Kudo and Y. Matsumoto.
2002.
Japanese depen-dency analysis using cascaded chunking.
In Proceed-ings of 6th Conference on Natural Language Learning(CoNLL), pages 63?69.B.
Levin.
1993.
English verb classes and alternations:a preliminary investigation.
Chicago Press.D.
Lin and P. Pantel.
2001.
Discovery of inference rulesfor question answering.
Natural Language Engineer-ing, 7(4):343?360.I.
Mel?c?uk and A. Polgue`re.
1987.
A formal lexicon inmeaning-text theory (or how to do lexica with words).Computational Linguistics, 13(3-4):261?275.S.
Muraki.
1991.
Various aspects of Japanese verbs.Hitsuji Syobo.
(in Japanese).B.
Pang, K. Knight, and D. Marcu.
2003.
Syntax-basedalignment of multiple translations: extracting para-phrases and generating new sentences.
In Proceed-ings of the 2003 Human Language Technology Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics (HLT-NAACL),pages 102?109.S.
Sato.
1999.
Automatic paraphrase of technical pa-pers?
titles.
Journal of Information Processing Societyof Japan, 40(7):2937?2945.
(in Japanese).Y.
Shinyama and S. Sekine.
2003.
Paraphrase acquisi-tion for information extraction.
In Proceedings of the2nd International Workshop on Paraphrasing: Para-phrase Acquisition and Applications (IWP), pages 65?71.K.
Takeuchi, K. Uchiyama, S. Yoshioka, K. Kageura,and T. Koyama.
2001.
Categorising deverbal nounsbased on lexical conceptual structure for analysingJapanese compounds.
In Proceedings of IEEE Sys-tem, Man, and Cybernetics Conference, pages 904?909.
