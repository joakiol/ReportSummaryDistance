Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 66?71,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSyntactic Tree-based Relation Extraction Using a Generalization ofCollins and Duffy Convolution Tree KernelMahdy Khayyamian Seyed AbolghasemMirroshandelHassan AbolhassaniSharif University of Technology Sharif University of Technology Sharif University of Technologykhayyamian@ce.sharif.edu mirroshandel@ce.sharif.edu abolhassani@sharif.eduAbstractRelation extraction is a challenging task innatural language processing.
Syntacticfeatures are recently shown to be quiteeffective for relation extraction.
In thispaper, we generalize the state of the artsyntactic convolution tree kernelintroduced by Collins and Duffy.
Theproposed generalized kernel is moreflexible and customizable, and can beconveniently utilized for systematicgeneration of more effective applicationspecific syntactic sub-kernels.
Using thegeneralized kernel, we will also propose anumber of novel syntactic sub-kernels forrelation extraction.
These kernels show aremarkable performance improvement overthe original Collins and Duffy kernel in theextraction of ACE-2005 relation types.1 IntroductionOne of the contemporary demanding NLP tasks isinformation extraction, which is the procedure ofextracting structured information such as entities,relations, and events from free text documents.
Asan information extraction sub-task, semanticrelation extraction is the procedure of findingpredefined semantic relations between textualentity mentions.
For instance, assuming a semanticrelation with type Physical and subtype Locatedbetween an entity of type Person and anotherentity of type Location, the sentence "Policearrested Mark at the airport last week."
conveystwo mentions of this relation between "Mark" and"airport" and also between "police" and "airport"that can be shown in the following format.Phys.Located(Mark, airport)Phys.Located(police, airport)Relation extraction is a key step towardsquestion answering systems by which vitalstructured data is acquired from underlying freetext resources.
Detection of protein interactions inbiomedical corpora (Li et al, 2008) is anothervaluable application of relation extraction.Relation extraction can be approached by astandard classification learning method.
Weparticularly use SVM (Boser et al, 1992; Cortesand Vapnik, 1995) and kernel functions as ourclassification method.
A kernel is a function thatcalculates the inner product of two transformedvectors of a high dimensional feature space usingthe original feature vectors as shown in eq.
1.)().
(),( jiji XXXXK ?
?=  (1)Kernel functions can implicitly capture a largeamount of features efficiently; thus, they have beenwidely used in various NLP tasks.Various types of features have been exploited sofar for relation extraction.
In (Bunescu andMooney, 2005b) sequence of words features areutilized using a sub-sequence kernel.
In (Bunescuand Mooney, 2005a) dependency graph featuresare exploited, and in (Zhang et al, 2006a) syntacticfeatures are employed for relation extraction.Although in order to achieve the best performance,it is necessary to use a proper combination of thesefeatures (Zhou et al, 2005), in this paper, we willconcentrate on how to better capture the syntacticfeatures for relation extraction.66In CD?01 (Collins and Duffy, 2001) aconvolution syntactic tree kernel is proposed thatgenerally measures the syntactic similaritybetween parse trees.
In this paper, a generalizedversion of CD?01 convolution tree kernel isproposed by associating generic weights to thenodes and sub-trees of the parse tree.
Theseweights can be used to incorporate domainknowledge into the kernel and make it moreflexible and customizable.
The generalized kernelcan be conveniently used to generate a variety ofsyntactic sub-kernels (including the original CD?01kernel), by adopting appropriate weightingmechanisms.As a result, in this paper, novel syntactic sub-kernels are generated from the generalized kernelfor the task of relation extraction.
Evaluationsdemonstrate that these kernels outperform theoriginal CD?01 kernel in the extraction of ACE-2005 main relation typesThe remainder of this paper is structured asfollows.
In section 2, the most related works arebriefly reviewed.
In section 3, CD?01 tree kernel isdescribed.
The proposed generalized convolutiontree kernel is explained in section 4 and itsproduced sub-kernels for relation extraction areillustrated in section 5.
The experimental resultsare discussed in section 6.
Our work is concludedin section 7 and some possible future works arepresented in section 8.2 Related WorkIn (Collins and Duffy, 2001), a convolution parsetree kernel has been introduced.
This kernel isgenerally designed to measure syntactic similaritybetween parse trees and is especially exploited forparsing English sentences in their paper.
Sincethen, the kernel has been widely used in differentapplications such as semantic role labeling(Moschitti, 2006b) and relation extraction (Zhanget al, 2006a; Zhang et al, 2006b; Zhou et al,2007; Li et al 2008).For the first time, in (Zhang et al, 2006a), thisconvolution tree kernel was used for relationextraction.
Since the whole syntactic parse tree ofthe sentence that holds the relation argumentscontains a plenty of misleading features, severalparse tree portions are studied to find the mostfeature-rich portion of the syntactic tree forrelation extraction, and Path-Enclosed Tree (PT) isfinally found to be the best performing treeportion.
PT is a portion of parse tree that isenclosed by the shortest path between the tworelation arguments.
Moreover, this tree kernel iscombined with an entity kernel to form areportedly high quality composite kernel in (Zhanget al, 2006b).3 CD?01 Convolution Tree KernelIn (Collins and Duffy, 2001), a convolution treekernel has been introduced that measures thesyntactic similarity between parse trees.
Thiskernel computes the inner products of thefollowing feature vector.10))(#),...,(#),...,(#()(22121?<=???
?TsubTreeTsubTreeTsubTreeTHnsizeisizesizeni(2)Each feature of this vector is the occurrence countof a sub-tree type in the parse tree decayedexponentially by the parameter ?
.
Without thisdecaying mechanism used to retain the kernelvalues within a fairly small range, the value of thekernel for identical trees becomes far higher thanits value for different trees.
Term isize  is definedto be the number of rules or internal nodes of the ithsub-tree type.
Samples of such sub-trees are shownin Fig.
1 for a simple parse tree.
Since the numberof sub-trees of a tree is exponential in its size(Collins and Duffy, 2001), direct inner productcalculation is computationally infeasible.Consequently, Collins and Duffy (2001) proposedan ingenious kernel function that implicitlycalculates the inner product in )( 21 NNO ?
timeon the trees of size 1N  and 2N .4 A Generalized Convolution TreeKernelIn order to describe the kernel, a feature vectorover the syntactic parse tree is firstly defined in eq.
(3), in which the ith feature equals the weightedsum of the number of instances of sub-tree type ithin the tree.Function )(nIisubtreeis an indicator function thatreturns 1 if the isubtree  occurs with its root atnode n and 0 otherwise.
As described in eq.
(4),67function tw(T) (which stands for "tree weight")assigns a weight to a tree T which is equal to theproduct of the weights of all its nodes.
)))](()([,...,))](()([))],...,(()([()( 11?????????=TnmsubtreeTnisubtreeTnsubtreensubtreetwnInsubtreetwnInsubtreetwnITHmi(3)????
?=)()()()()(TdesExternalNonTdesInternalNonnenwninwTtw(4)Figure 1.
Samples of sub-trees used in convolution treekernel calculation.Since each node of the whole syntactic tree caneither happen as an internal node or as an externalnode of a supposed sub-tree (presuming itsexistence in the sub-tree), two types of weights arerespectively associated to each node by thefunctions )(ninw  and )(nenw  (which respectivelystand for "internal node weight" and "external nodeweight").
For instance, in Fig.
1, the node withlabel PP is an external node for sub-trees (1) and(7) while it is an internal node of sub-trees (3) and(4).????
???
??
??
???=???=??
?=><=11 2211 222211),())](())(()()([)]])(()([]))(()([[)(),(),(21212122112121Tn TngciiTn Tn isubtreesubtreeTnisubtreei TnisubtreennCnsubTreetwnsubTreetwnInInsubTreetwnInsubTreetwnITHTHTTKiiii(5)As shown in eq.
(5), A similar procedure to(Collins and Duffy, 2001) can be employed todevelop a kernel function for the calculation of dotproducts on H(T) vectors.
According to eq.
(5) thecalculation of the kernel finally leads to the sum ofa ),( 21 nnCgc  function over all tree node pairs of T1and T2.
Function ),( 21 nnCgc  is the weighted sum ofthe common sub-trees rooted at 1n  and n2, and canbe recursively computed in a similar way tofunction ),( 21 nnC  of (Collins and Duffy, 2001) asfollows.
(1) if the production rules of nodes n1 and n2 aredifferent then 0),( 21 =nnCgc(2) else if n1 and n2 are the same pre-terminals (thesame part of speeches) then))(()())(()(),(221121nchildenwninwnchildenwninwnnCgc??
?=(3) else if both n1 and n2 have the same productionrules then))](),(())(())(([)()(),(21212121nchildnchildCnchildenwnchildenwninwninwnnCiigciiigc?
+??
?=In the first case, when the two nodes representdifferent production rules they can't accordinglyhave any sub-trees in common.
In the second case,there is exactly one common sub-tree of size two.It should be noted that all the leaf nodes of the tree(or words of the sentence) are considered identicalin the calculation of the tree kernel.
The value ofthe function in this case is the weight of thiscommon sub-tree.
In the third case, when the nodesgenerally represent the same production rules theweighted sum of the common sub-trees arecalculated recursively.
The equation holds becausethe existence of common sub-trees rooted at n1 andn2 implies the existence of common sub-treesrooted at their corresponding children, which canbe combined multiplicatively to form their parents'common sub-trees.Due to the equivalent procedure of kernelcalculation, this generalized version of the treekernel preserves the nice )( 21 NNO ?
timecomplexity property of the original kernel.
It isworthy of note that in (Moschitti, 2006b) a sortingbased method is proposed for the fastimplementation of such tree kernels that reducesthe average running time to )( 21 NNO + .The generalized kernel can be converted toCD?01 kernel by defining ?=)(ninw  and1)( =nenw .
Likewise, other definitions can beutilized to produce other useful sub-kernels.DTtheairportNNNPNP PPNPNPNP PPIN NNPMarkairportNPNP PPIN NPDT NNNNPMark atthePPairportIN NPDT NNatthe(4) (5)(6) (7)DTthe airportNPNNNPNP PPNNP(1) (2) (3)685 Kernels for Relation ExtractionIn this section, three sub-kernels of the generalizedconvolution tree kernel will be proposed forrelation extraction.
Using the embedded weights ofthe generalized kernel, these sub-kernelsdifferentiate among sub-trees based on theirexpected relevance to semantic relations.
Morespecifically, the sub-trees are weighted accordingto how their nodes interact to the arguments of therelation.5.1 Argument Ancestor Path Kernel (AAP)Definition of weighting functions is shown in eq.
(6) and (7).
Parameter 10 ?< ?
is a decayingparameter similar to ?
.?????=otherwiseitonnodeaofchilddirectaorpathancestorumenttheonisnifninw0arg)(?(6)????
?=otherwiseitonnodeaofchilddirectaorpathancestorumenttheonisnifnenw0arg1)((7)This weighting method is equivalent to applyingCD?01 tree kernel (by setting 2??
= ) on a portionof the parse tree that exclusively includes thearguments ancestor nodes and their direct children.5.2 Argument Ancestor Path Distance Kernel(AAPD)DISTMAXnAAPDistnAAPDistMinninw _))arg,(),arg,(( 21)( ?=(8)DISTMAXnAAPDistnAAPDistMinnenw _))arg,(),arg,(( 21)( ?=(9)Definition of weighting functions is shown in eq.
(8) and (9).
Both functions have identicaldefinitions for this kernel.Function AAPDist(n,arg) calculates the distance ofthe node n from the argument arg on the parse treeas illustrated by Fig.
2.
MAX_DIST is used fornormalization, and is the maximum ofAAPDist(n,arg) in the whole tree.
In this way, thecloser a tree node is to one of the argumentsancestor path, the less it is decayed by thisweighting method.5.3 Threshold Sensitive Argument AncestorPath Distance Kernel (TSAAPD)This kernel is intuitively similar to the previouskernel but uses a rough threshold based decayingtechnique instead of a smooth one.
The definitionof weighting functions is shown in eq.
(10) and(11).
Both functions are again identical in this case.????
?=ThresholdnAAPDistThresholdnAAPDistninw )()(1)(?
(10)????
?=ThresholdnAAPDistThresholdnAAPDistnenw )()(1)(?
(11)6 Experiments6.1 Experiments SettingThe proposed kernels are evaluated on ACE-2005multilingual corpus (Walker et al, 2006).
In orderto avoid parsing problems, the more formal partsof the corpus in "news wire" and "broadcast news"sections are used for evaluation as in (Zhang et al,2006b).AAPDist(airport, NP)=1SNNairportNP VPNNPPoliceVBNarrestedNPNP PPIN NPDT NNNNPMark attheNPJJlast weekFigure 2.
The syntactic parse tree of the sentence"Police arrested Mark at the airport last week" thatconveys a Phys.Located(Mark, airport) relation.
Theancestor path of the argument "airport" (dashedcurve) and the distance of the node NP of "Mark"from it (dotted curve) is shown.69PER-SOC ART GEN-AFF ORG-AFF PART-WHOLE PHYSCD?01 0.62 0.51 0.09 0.43 0.30 0.32AAP 0.58 0.49 0.10 0.43 0.28 0.36AAPD 0.70 0.50 0.12 0.43 0.29 0.29TSAAPD-0 0.63 0.48 0.11 0.43 0.30 0.33TSAAPD-1 0.73 0.47 0.11 0.45 0.28 0.33Table 1: The F1-Measure value is shown for every kernel on each ACE-2005 main relation type.
For every relationtype the best result is shown in bold font.We have used LIBSVM (Chang and Lin 2001)java source for the SVM classification andStanford NLP package1 for tokenization, sentencesegmentation and parsing.Following [Bunescu and Mooney, 2007], everypair of entities within a sentence is regarded as anegative relation instance unless it is annotated as apositive relation in the corpus.
The total number ofnegative training instances, constructed in thisway, is about 20 times more than the number ofannotated positive instances.
Thus, we alsoimposed the restriction of maximum argumentdistance of 10 words.
This constraint eliminateshalf of the negative constructed instances whileslightly decreases positive instances.
Nevertheless,since the resulted training set is still unbalanced,we used LIBSVM weighting mechanism.Precisely, if there are P positive and N negativeinstances in the training set, a weight value ofPN /  is used for positive instances while thedefault weight value of 1 is used for negative ones.A binary SVM is trained for every relation typeseparately, and type compatible annotated andconstructed relation instances are used to train it.For each relation type, only type compatiblerelation instances are exploited for training.
Forexample to learn an ORG-AFF relation (whichapplies to (PER, ORG) or (ORG, ORG) argumenttypes) it is meaningless to use a relation instancebetween two entities of type PERSON.
Moreover,the total number of training instances used fortraining every relation type is restricted to 5000instances to shorten the duration of the evaluationprocess.
The reported results are achieved using a5-fold cross validation method.The kernels AAP, AAPD and TSAAPD-0(TSAAPD with threshold = 0) and TSAAPD-1(TSAAPD with threshold = 1) are compared withCD?01 convolution tree kernel.
All the kernels1 http://nlp.stanford.edu/software/index.shtmlexcept for AAP are computed on the PT portiondescribed in section 2.
AAP is computed over theMCT tree portion which is also proposed by(Zhang et al, 2006a) and is the sub-tree rooted atthe first common ancestor of relation arguments.For the proposed kernels ?
is set to 0.44 whichis tuned on a development set that contained 5000instances of type PHYS.
The ?
parameter ofCD?01 kernel is set to 0.4 according to (Zhang etal., 2006a).
The C parameter of SVM classificationis set to 2.4 for all the kernels after tuning itindividually for each kernel on the mentioneddevelopment set.6.2 Experiments ResultsThe results of the experiments are shown in Table1.
The proposed kernels outperform the originalCD?01 kernel in four of the six relation types.
Theperformance of TSAAPD-1 is especiallyremarkable because it is the best kernel in ORG-AFF and PER-SOC relations.
It particularlyperforms very well in the extraction of PER-SOCrelation with an F1-measure of 0.73.
It should benoted that the general low performance of all thekernels on the GEN-AFF type is because of itsextremely small number of annotated instances inthe training set (40 in 5000).
The AAPD kernel hasthe best performance with a remarkableimprovement over the Collins kernel in GEN-AFFrelation type.The results clearly demonstrate that the nodescloser to the ancestor path of relation argumentscontain the most useful syntactic features forrelation extraction7 ConclusionIn this paper, we proposed a generalizedconvolution tree kernel that can generate varioussyntactic sub-kernels including the CD?01 kernel.KernelRelation70The kernel is generalized by assigning weights tothe sub-trees.
The weight of a sub-tree is theproduct of the weights assigned to its nodes by twotypes of weighting functions.
In this way, impactsof the tree nodes on the kernel value can bediscriminated purposely based on the application.Context information can also be injected to thekernel via context sensitive weighting mechanisms.Using the generalized kernel, various sub-kernels can be produced by different definitions ofthe two weighting functions.
We consequentlyused the generalized kernel for systematicgeneration of useful kernels in relation extraction.In these kernels, the closer a node is to the relationarguments ancestor paths, the less it is decayed bythe weighting functions.
Evaluation on the ACE-2005 main relation types demonstrates theeffectiveness of the proposed kernels.
They showremarkable performance improvement over CD?01kernel.8 Future WorkAlthough the path-enclosed tree portion (PT)(Zhang et al, 2006a) seems to be an appropriateportion of the syntactic tree for relation extraction,it only takes into account the syntactic informationbetween the relation arguments, and discards manyuseful features (before and after the argumentsfeatures).
It seems that the generalized kernel canbe used with larger tree portions that containsyntactic features before and after the arguments,because it can be more easily targeted to relatedfeatures.Currently, the proposed weighting mechanismsare solely based on the location of the tree nodes inthe parse tree; however other useful informationsuch as labels of nodes can also be used inweighting.Another future work can be utilizing thegeneralized kernel for other applicable NLP taskssuch as co-reference resolution.AcknowledgementThis work is supported by Iran TelecommunicationResearch Centre under contract No.
500-7725.ReferencesBoser B. E., Guyon I., and Vapnik V. 1992.
A trainingalgorithm for optimal margin classifiers.
InProceedings of the Fifth Annual Workshop onComputational Learning Theory, pages 144-152.ACM Press.Bunescu R. C. and Mooney R. J.
2005a.
A Shortest PathDependency Kernel for Relation Extraction.EMNLP-2005Bunescu R. C. and Mooney R. J.
2005b.
Subsequencekernels for relation extraction.
NIPS-2005.Bunescu R. C. and Mooney R. J.
2007.
Learning forInformation Extraction: From Named EntityRecognition and Disambiguation to RelationExtraction, Ph.D. Thesis.
Department of ComputerSciences, University of Texas at Austin.Chang, C.-C. and C.-J.
Lin 2001.
LIBSVM: a library forsupport vector machines.
Software available athttp://www.csie.ntu.edu.tw/~cjlin/libsvm.Collins M. and Duffy N. 2001.
Convolution Kernels forNatural Language.
NIPS-2001Cortes C. and Vapnik V. 1995.
Support-vector network.Machine Learning.
20, 273-297.Li J., Zhang Z., Li X. and Chen H. 2008.
Kernel-basedlearning for biomedical relation extraction.
J. Am.Soc.
Inf.
Sci.
Technol.
59, 5, 756?769.Moschitti A.
2006a.
Making tree kernels practical fornatural language learning.
EACL-2006.Moschitti A.
2006b.
Syntactic kernels for naturallanguage learning: the semantic role labeling case.HLT-NAACL-2006 (short paper)Walker, C., Strassel, S., Medero J. and Maeda, K. 2006.ACE 2005 Multilingual Training Corpus.
LinguisticData Consortium, Philadelphia.Zhang M., Zhang J. and SU j.
2006a.
Exploringsyntactic features for relation extraction using aconvolution tree kernel.
HLT-NAACL-2006.Zhang M., Zhang J., Su J. and Zhou G.D. 2006b.
AComposite Kernel to Extract Relations betweenEntities with both Flat and StructuredCOLINGACL-2006: 825-832.Zhou G.D., Su J, Zhang J. and Zhang M. 2005.Exploring Various Knowledge in RelationExtraction.
ACL-2005Zhou G.D., Zhang M., Ji D.H. and Zhu Q.M.
2007.
TreeKernel-based Relation Extraction with Context-Sensitive Structured Parse Tree Information.EMNLP-CoNLL-200771
