Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 74?79,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTowards Using Structural Events To Assess Non-native SpeechLei Chen, Joel Tetreault, Xiaoming XiEducational Testing Service (ETS)Princeton, NJ 08540, USA{LChen,JTetreault,XXi}@ets.orgAbstractWe investigated using structural events, e.g.,clause and disfluency structure, from tran-scriptions of spontaneous non-native speech,to compute features for measuring speakingproficiency.
Using a set of transcribed au-dio files collected from the TOEFL PracticeTest Online (TPO), we conducted a sophisti-cated annotation of structural events, includ-ing clause boundaries and types, as well asdisfluencies.
Based on words and the anno-tated structural events, we extracted featuresrelated to syntactic complexity, e.g., the meanlength of clause (MLC) and dependent clausefrequency (DEPC), and a feature related todisfluencies, the interruption point frequencyper clause (IPC).
Among these features, theIPC shows the highest correlation with holis-tic scores (r = ?0.344).
Furthermore, we in-creased the correlation with human scores bynormalizing IPC by (1) MLC (r = ?0.386),(2) DEPC (r = ?0.429), and (3) both (r =?0.462).
In this research, the features derivedfrom structural events of speech transcriptionsare found to predict holistic scores measuringspeaking proficiency.
This suggests that struc-tural events estimated on speech word stringsprovide a potential way for assessing non-native speech.1 IntroductionIn the last decade, a breakthrough in speech pro-cessing is the emergence of a lot of active researchwork on automatic estimation of structural events,e.g., sentence structure and disfluencies, on sponta-neous speech (Shriberg et al, 2000; Liu, 2004; Os-tendorf et al, 2008).
The detected structural eventshave been successfully used in many natural lan-guage processing (NLP) applications (Ostendorf etal., 2008).However, the structural events in speech datahaven?t been largely utilized by the research on us-ing automatic speech recognition (ASR) technologyto assess speech proficiency (Neumeyer et al, 2000;Zechner et al, 2007), which mainly used cues de-rived at the word level, such as timing informationof spoken words.
The information beyond the wordlevel, e.g., clause/sentence structure of utterancesand disfluency structure, has not been or is poorlyrepresented.
For example, in Zechner et al (2007),only special words for filled pauses such as um anduh were obtained from ASR results to represent dis-fluencies.Given the successful usage of structural eventson a wide range of NLP applications and the factthat the usage of these events is missing in the auto-matic speech assessment research, a research ques-tion emerges: Can we use structural events of spon-taneous speech to assess non-native speech profi-ciency?We will address this question in this paper.
Thepaper is organized as follows: Section 2 reviewsprevious research.
Section 3 describes our annota-tion convention.
Section 4 reports on the data col-lection, annotation, and quality control.
Section 5reports on features based on structural event anno-tations.
Section 6 reports on our experiments.
Sec-tion 7 discusses our findings and plans for future re-search work.742 Previous WorkIn the last decade, a large amount of research (Os-tendorf et al, 2008) has been conducted on detectionof structural events, e.g., sentence structure and dis-fluency structure, in spontaneous speech.
In theseresearch works, the structural events were detectedwith a quite high accuracy.
Furthermore, the de-tected sentence and disfluency structures have beenfound to help many of the following NLP tasks,e.g., speech parsing, information retrieval, machinetranslation, and extractive speech summary (Osten-dorf et al, 2008).In the second language acquisition (SLA) andchild language development research fields, the lan-guage development is measured according to flu-ency, accuracy, and complexity (Iwashita, 2006).The syntactic complexity of learners?
writing datahas been extensively studied in the SLA commu-nity (Ortega, 2003).
Recently, this study has beenextended to the learner?s speaking data (Iwashita,2006).
Typical metrics for examining syntactic com-plexity include: length of production unit (e.g., T-unit, which is defined as essentially a main clauseplus any other clauses which are dependent uponit (Hunt, 1970), clauses, verb phrases, and sen-tences), amount of embedding, subordination andcoordination, range of structural types, and structuresophistication.Iwashita (2006) investigated several measures forsyntactic complexity on the data from learners ofJapanese.
The author reported that some measure-ments, e.g., T-unit length, the number of clauses perT-unit, and the number of independent clauses per T-Unit, were good at predicting learners?
proficiencylevels.In addition, some previous studies used measure-ments related to disfluencies to assess speaking pro-ficiency.
For example, Lennon (1990) used a dozenfeatures related to speed, pauses, and several dis-fluency markers, such as filler pauses per T-unit,to measure four German-speaking women?s Englishimprovement during a half year study in England.He found a significant change in filled pauses perT-unit during the studying process.The features related to syntactic complexity andthe features related to ?smoothness?
(disfluency) ofspeech were jointly used in some previous stud-ies.
For example, Mizera (2006) used fluency fac-tors related to speed, voiced smoothness (frequen-cies of repetitions or self-corrections), pauses, syn-tactic complexity (mean length of T-units), andaccuracy, to measure speaking proficiency on 20non-native English speakers.
In this experiment,disfluency-related factors, such as total voiced dis-fluencies, had a high correlation with fluency (r =?0.45).
However, the syntactic complexity factoronly showed a moderate correlation (r = 0.310).Yoon (2009) implemented an automated disfluencydetection method and found that the disfluency-related features lead to the moderate improvementin the automated speech proficiency scoring.There were limitations on using the features re-ported in these SLA studies on standard languagetests.
For example, only a very limited number ofsubjects (from 20 to 30 speakers) were used in thesestudies.
Second, the speaking content was narra-tions of picture books or cartoon videos rather thanstandard test questions.
Therefore, we conducted astudy using a much larger data set obtained from realspeech tests to address these limitations.3 Structural Event Annotation ConventionTo annotate structural events of speech content, wehave developed a convention based on previous stud-ies and our observations on the TOEFL Practice On-line (TPO) test data.
Defining clauses is a relativelysimple task; however, defining clause boundariesand specifying which elements fall within a particu-lar clause is a much more challenging task for spo-ken discourse, due to the presence of grammaticalerrors, fragments, repetitions, self corrections, andconversation fillers.Foster et al (Foster et al, 2000) review variousunits for analyzing spoken language, including syn-tactic, semantic and intonational units, and proposea new analysis of speech unit (AS-Unit) that theyclaim is appropriate for many different purposes.
Inthis study, we focused on clauses given the charac-teristics of spontaneous speech.
Also, we definedclause types based on grammar books such as (Azar,2003).
The following clause types were defined:?
Simple sentence (SS) contains a subject and averb, and expresses a complete thought.75?
Independent clause (I) is the main clause thatcan stand along syntactically as a complete sen-tence.
It consists minimally a subject and a fi-nite verb (a verb that shows tense, person, orsingular/plural, e.g., he goes, I went, and I was).?
Subordinate clause is a clause in a complexsentence that cannot stand alone as a completesentence and that functions within the sentenceas a noun, a verb complement, an adjective oran adverb.
There are three types of subordi-nate clauses: noun clause (NC), relative clausethat functions as an adjective (ADJ), adverbialclause that functions as an adverb (ADV).?
Coordinate clause (CC) is a clause in a com-pound sentence that is grammatically equiva-lent to the main clause and that performs thesame grammatical function.?
Adverbial phrase (ADVP) is a separate clausefrom the main clause that contains a non-finiteverb (a verb that does not show tense, person,or singular/plural).The clause boundaries and clause types were an-notated on the word transcriptions.
Round brack-ets were used to indicate the beginning and end of aclause.
Then, the abbreviations described above forclause types were added.
Also, if a specific bound-ary serves as the boundaries for both the local andglobal clause, the abbreviation of the local clausewas followed by that of the global.
Some examplesof clause boundaries and types are reported in Ta-ble 1.In our annotation manual, a speech disfluencycontains three parts:?
Reparandum: the speech portion that will berepeated, corrected, or even abandoned.
Theend of the reparandum is called the interruptionpoint (IP), which indicates the stop of a normalfluent speech stream.?
Editing phrase: optional inserted words, e.g.,um.?
Correction: the speech portion that repeats,corrects, or even starts new content.In our annotation manual, the reparandum was en-closed by ?
*?, the editing phrase was enclosed by?%?, and the correction was enclosed by ?$?.
Forexample, in the following utterance, ?He is a * verymad * % er % $ very bad $ cop?, ?very mad?
wascorrected by ?very bad?
and an editing phrase, er,was inserted.4 Data Collection and Annotation4.1 Audio data collection and scoringAbout 1300 speech responses from the TPO testwere collected and transcribed.
Each item wasscored by two experienced human raters indepen-dently using a 4-point holistic score based on thescoring rubrics designed for the test.In the TPO test, some tasks required test-takers toprovide information or opinions on familiar topicsbased on their personal experience or backgroundknowledge.
Others required them to summarize andsynthesize information presented in listening and/orreading materials.
Each test-taker was required tofinish six items in one test session.
Each item has a45 or 60 seconds response time.4.2 Annotation procedureTwo annotators (who were not the human ratersmentioned above) with a linguistics background andpast linguistics annotation experience were first pre-sented with a draft of the annotation convention.After reading through it, the annotators, as well asthe second and third author completed four iterativeloops of rating 4 or 5 responses per meeting.
All fourdiscussed differences in annotations and the conven-tion was refined as needed.
After the final iterationof comparisons, the raters seemed to have very fewdisagreement and thus began annotating sets of re-sponses.
Each set consisted of roughly 50-75 re-sponses and then a kappa set of 30-50 responseswhich both annotators completed.
Accordingly, be-tween the two annotators, a set comprised roughly130 to 200 responses.
Each response takes roughly3-8 minutes to annotate.
The annotators were in-structed to listen to the corresponding audio file ifthey needed the prosodic information to annotate aparticular speech disfluency event.76Clause type ExampleSS (That?s right |SS)I (He turned away |I) as soon as he saw me |ADV)NC ((What he did |NC) shocked me |I)ADJ (She is the woman (I told you about |ADJ)|I)ADV (As soon as he saw me |ADV) (he turned away |I)CC (I will go home |I) (and he will go to work |CC)ADVP (While walking to class |ADVP) (I ran into a friend |I)Table 1: Examples of clause boundary and type annotation4.3 Evaluation of annotationTo evaluate the quality of structural event anno-tation, we measured the inter-rater agreement onclause boundary (CB) annotation and interruptionpoint (IP) of disfluencies1.We used Cohen?s ?
to calculate the annotatoragreement on each kappa set.
?
is calculated on theabsence or presence of a boundary marker (either aclause boundary (CB) or an interruption point (IP)between consecutive words).
For each consecutivepair of words, we check for the existence of one ormore boundaries, and collapse the set into one term?boundary?
and then compute the agreement on thisreduced annotation.In Table 2, we list the annotator agreement forboth boundary events over 4 kappa sets.
The secondcolumn refers to the number of speech responses inthe kappa set, the next two columns refer to the an-notator agreement using the Cohen?s ?
value on CBand IP annotation results.Set N ?
CB ?
IPSet1 54 0.886 0.626Set2 71 0.847 0.687Set3 35 0.855 0.695Set4 34 0.899 0.833Table 2: Between-rater agreement of structural event an-notationIn general, a ?
of 0.8-1.0 represents excellentagreement, 0.6-0.8 represents good agreement, andso forth.
Over each kappa set, ?
for CB annota-tions ranges between 0.8 and 0.9, which is an ex-1Measurement on CBs and IPs can provide a rough qual-ity measurement of annotations.
In addition, doing so is moreimportant to us since automatic detection of these two types ofevents will be investigated in future.cellent agreement; ?
for IP annotation ranges be-tween 0.6 and 0.8, which is a good agreement.
Com-pared to annotating clauses, marking disfluencies ismore challenging.
As a result, a lower between-rateragreement is expected.5 Features Derived On Structural EventsBased on the structural event annotations, includingclause boundaries and their types, as well as disflu-encies, some features measuring syntactic complex-ity and disfluency profile were derived.Since simple sentence (SS), independent clause(I), and conjunct clause (CC) represent a completeidea, we treat them as an approximate to a T-unit (T).The clauses that have no complete idea, are depen-dent clauses (DEP), including noun clauses (N), rel-ative clauses that function as adjective (ADJ), adver-bial clauses (ADV), and adverbial phrases (ADVP).The total number of clauses is a summation of thenumber of T-units (T), dependent clauses (DEP), andfragments2 (denoted as F).
Therefore,NT = NSS +NI +NCCNDEP = NNC +NADJ +NADV +NADV PNC = NT +NDEP +NFAssuming Nw is the total number of words inthe speech response (without pruning speech re-pairs), the following features, including mean lengthof clause (MLC), dependent clauses per clause(DEPC), and interruption points per clause (IPC),are derived:MLC = Nw/NC2It is either a subordinate clause that does not have a cor-responding independent clause or a string of words without asubject or a verb that does not express a complete thought.77DEPC = NDEP /NCIPC = NIP /NCFurthermore, we elaborated the IPC feature.
Dis-fluency is a complex behavior and is influenced bya variety of factors, such as proficiency level, speak-ing rate, and familiarity with speaking content.
Thecomplexity of utterances is also an important fac-tor on the disfluency pattern.
For example, Rollet al (Roll et al, 2007) found that complexity ofexpression computed based on the language?s pars-ing tree structure influenced the frequency of disflu-encies in their experiment on Swedish.
Therefore,since disfluency frequency was not only influencedby the test-takers?
speaking proficiency but also bythe utterance?s syntactic structure?s difficulty, we re-duced the impact from the syntactic structure so thatwe can focus on speakers?
ability.
For this purpose,we normalized IPC by dividing by some features re-lated to syntactic-structure?s complexity, includingMLC, DEPC, and both.
Therefore, the followingelaborated disfluency-related features were derived:IPCn1 = IPC/MLCIPCn2 = IPC/DEPCIPCn3 = IPC/MLC/DEPC6 ExperimentFor each item, two human raters rated it separatelywith a score from 1 to 4.
If these two scores areconsistent (the difference between two scores is ei-ther zero or one), we put this item in an item-pool.Finally, a total of 1, 257 audio items were includedin the pool.
Following the score-handling protocolused in the TPO test, we used the first human rater?sscore as the item score.
From the obtained item-pool, we selected speakers with more than threeitems so that the averaged score per speaker can beestimated on several items to achieve a robust scorecomputation3.
As a result, 175 speakers4 were se-lected.3The mean holistic score of these speakers is 2.786, whichis close to the mean holistic score of the selected item-pool(2.785), indicating that score distribution was kept after focus-ing on speakers with more than three items.4If a speaker was assigned in a Kappa set in the annotationas described in Section 4, this speaker would have as many as 12annotated items.
Therefore, the minimum number of speakersfrom the item-pool was about 105 (1257/12).For each speaker, his or her annotations of wordsand structural events were used to extract the fea-tures described in Section 5.
Then, we computedthe Pearson correlation among the obtained featureswith the averaged holistic scores per speaker.Feature rMLC 0.211DEPC 0.284IPC -0.344IPCn1 -0.386IPCn2 -0.429IPCn3 -0.462Table 3: Correlation coefficients (rs) between the fea-tures derived from structural events with human scoresaveraged on test takersTable 3 reports on the correlation coefficient(r) between the proposed features derived fromstructural events with holistic scores.
Relyingon three simple structural event annotations, i.e.,clause boundaries, dependent clauses, and interrup-tion points in speech disfluencies, some promisingcorrelations between features with holistic scoreswere found.
Between the two syntactic complex-ity features, the DEPC has a higher correlation withholistic scores than the MLC (0.284 > 0.211).
It ap-pears that a measurement about clauses?
embeddingprofile is more informative about a speaker?s profi-ciency level.
Second, compared to features measur-ing syntactic complexity, the feature measuring thedisfluency profile is better to predict human holis-tic scores on this non-native data set.
For example,IPC has a r of ?0.344, which is better than the fea-tures about clause lengths or embedding.
Finally, byjointly using the structural events related to clausesand disfluencies, we can further achieve a furtherimproved r. Compared to IPC, IPCn3 has a relative34.30% correlation increase.
This is consistent withour idea of reducing utterance-complexity?s impacton disfluency-related features.7 DiscussionIn most current automatic speech assessment sys-tems, features derived from recognized words, suchas delivery features about speaking rate, pause infor-mation, and accuracy related to word identities, havebeen widely used to assess non-native speech from78fluency and accuracy points of view.
However, in-formation beyond recognized words, e.g., the struc-ture of clauses and disfluencies, has only receivedlimited attention.
Although several previous SLAstudies used features derived from structural eventsto measure speaking proficiency, these studies werelimited and the findings from them were difficult todirectly apply to on large-scale standard tests.In this paper, using a large-sized data set col-lected in the TPO speaking test, we conducted ansophisticated annotation of structural events, includ-ing boundaries and types of clauses and disfluen-cies, from transcriptions of spontaneous speech testresponses.
A series of features were derived fromthese structural event annotations and were eval-uated according to their correlations with holisticscores.
We found that disfluency-related featureshave higher correlations to human holistic scoresthan features about syntactic complexity, which con-firms the result reported in (Mizera, 2006).
In spon-taneous speech utterances, simple syntactic structuretends to be utilized by speakers.
This is in contrast tosophisticated syntactic structure appearing in writ-ing.
This may cause that complexity-related featuresare poor at predicting fluency scores.
On the otherhand, disfluencies, a pattern unique to spontaneousspeech, were found to play a more important role inindicating speaking proficiency levels.Although syntactic complexity features were nothighly indicative of holistic scores, they were usefulto further improve disfluency-related features?
corre-lation with holistic scores.
By normalizing IPC us-ing measurements representing syntactic complex-ity, we can highlight contributions from speakers?proficiency levels.
Therefore, in our experiment,IPCn3 shows a 34.30% relative improvement in itscorrelation coefficient with human holistic scoresover the original IPC.The study reported in this paper suggests promisethat structural events beyond speech recognition re-sults can be utilized to measure non-native speakerproficiency levels.
Recently, in the NLP researchfield, an increasing amount of effort has beenmade on structural event detection in spontaneousspeech (Ostendorf et al, 2008).
Therefore, suchprogress can benefit the study of automatic estima-tion of structural events on non-native speech data.For our future research plan, first, we will inves-tigate automatically detecting these structural eventsfrom speech transcriptions and recognition hypothe-ses.
Second, the features derived from the obtainedstructural events will be used to augment the featuresin automatic speech assessment research to providea wider construct coverage than fluency and pronun-ciation features do.ReferencesB.
Azar.
2003.
Fundamentals of English grammar.Pearson Longman, White Plains, NY, 3rd edition.P.
Foster, A. Tonkyn, and G. Wigglesworth.
2000.
Mea-suring spoken language: A unit for all reasons.
Ap-plied Linguistics, 21(3):354.K.
W. Hunt.
1970.
Syntactic maturity in school chil-dren and adults.
In Monographs of the Society for Re-search in Child Development.
University of ChicagoPress, Chicago, IL.N.
Iwashita.
2006.
Syntactic complexity measures andtheir relation to oral proficiency in Japanese as a for-eign language.
Language Assessment Quarterly: AnInternational Journal, 3(2):151?169.P.
Lennon.
1990.
Investigating fluency in EFL: A quanti-tative approach.
Language Learning, 40(3):387?417.Y.
Liu.
2004.
Structural Event Detection for Rich Tran-scription of Speech.
Ph.D. thesis, Purdue University.G.
J. Mizera.
2006.
Working memory and L2 oral flu-ency.
Ph.D. thesis, University of Pittsburgh.L.
Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.2000.
Automatic Scoring of Pronunciation Quality.Speech Communication, 30:83?93.L.
Ortega.
2003.
Syntactic complexity measures andtheir relationship to L2 proficiency: A research syn-thesis of college-level L2 writing.
Applied Linguistics,24(4):492.M.
Ostendorf et al 2008.
Speech segmentation and spo-ken document processing.
Signal Processing Maga-zine, IEEE, 25(3):59?69, May.M.
Roll, J. Frid, and M. Horne.
2007.
Measuring syntac-tic complexity in spontaneous spoken Swedish.
Lan-guage and Speech, 50(2):227.E.
Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur.2000.
Prosody-based automatic segmentation ofspeech into sentences and topics.
Speech Communi-cation, 32(1-2):127?154.S.
Yoon.
2009.
Automated assessment of speech fluencyfor L2 English learners.
Ph.D. thesis, University ofIllinois at Urbana-Champaign.K.
Zechner, D. Higgins, and Xiaoming Xi.
2007.SpeechRater: A Construct-Driven Approach to Scor-ing Spontaneous Non-Native Speech.
In Proc.
SLaTE.79
