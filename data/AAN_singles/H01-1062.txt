The RWTH System for Statistical Translation of SpokenDialoguesH.
Ney, F. J. Och, S. VogelLehrstuhl fu?r Informatik VI, Computer Science DepartmentRWTH Aachen, University of TechnologyD-52056 Aachen, GermanyABSTRACTThis paper gives an overview of our work on statistical ma-chine translation of spoken dialogues, in particular in theframework of the Verbmobil project.
The goal of theVerbmobil project is the translation of spoken dialoguesin the domains of appointment scheduling and travel plan-ning.
Starting with the Bayes decision rule as in speechrecognition, we show how the required probability distri-butions can be structured into three parts: the languagemodel, the alignment model and the lexicon model.
Wedescribe the components of the system and report resultson the Verbmobil task.
The experience obtained in theVerbmobil project, in particular a large-scale end-to-endevaluation, showed that the statistical approach resulted insignificantly lower error rates than three competing transla-tion approaches: the sentence error rate was 29% in compar-ison with 52% to 62% for the other translation approaches.1.
INTRODUCTIONIn comparison with written language, speech and espe-cially spontaneous speech poses additional difficulties forthe task of automatic translation.
Typically, these difficul-ties are caused by errors of the recognition process, which iscarried out before the translation process.
As a result, thesentence to be translated is not necessarily well-formed froma syntactic point-of-view.
Even without recognition errors,speech translation has to cope with a lack of conventionalsyntactic structures because the structures of spontaneousspeech differ from that of written language.The statistical approach shows the potential to tacklethese problems for the following reasons.
First, the statisti-cal approach is able to avoid hard decisions at any level ofthe translation process.
Second, for any source sentence, atranslated sentence in the target language is guaranteed tobe generated.
In most cases, this will be hopefully a syn-tactically perfect sentence in the target language; but evenif this is not the case, in most cases, the translated sentencewill convey the meaning of the spoken sentence..Whereas statistical modelling is widely used in speechrecognition, there are so far only a few research groups thatapply statistical modelling to language translation.
The pre-sentation here is based on work carried out in the frameworkof the EuTrans project [8] and the Verbmobil project [25].2.
STATISTICAL DECISION THEORYAND LINGUISTICS2.1 The Statistical ApproachThe use of statistics in computational linguistics has beenextremely controversial for more than three decades.
Thecontroversy is very well summarized by the statement ofChomsky in 1969 [6]:?It must be recognized that the notion of a ?probabilityof a sentence?
is an entirely useless one, under anyinterpretation of this term?.This statement was considered to be true by the major-ity of experts from artificial intelligence and computationallinguistics, and the concept of statistics was banned fromcomputational linguistics for many years.What is overlooked in this statement is the fact that, in anautomatic system for speech recognition or text translation,we are faced with the problem of taking decisions.
It isexactly here where statistical decision theory comes in.
Inspeech recognition, the success of the statistical approach isbased on the equation:Speech Recognition = Acoustic?Linguistic Modelling+ Statistical Decision TheorySimilarly, for machine translation, the statistical approachis expressed by the equation:Machine Translation = Linguistic Modelling+ Statistical Decision TheoryFor the ?low-level?
description of speech and image signals,it is widely accepted that the statistical framework allowsan efficient coupling between the observations and the mod-els, which is often described by the buzz word ?subsymbolicprocessing?.
But there is another advantage in using prob-ability distributions in that they offer an explicit formalismfor expressing and combining hypothesis scores:?
The probabilities are directly used as scores: Thesescores are normalized, which is a desirable property:when increasing the score for a certain element in theset of all hypotheses, there must be one or several otherelements whose scores are reduced at the same time.?
It is straightforward to combine scores: dependingon the task, the probabilities are either multiplied oradded.?
Weak and vague dependencies can be modelled eas-ily.
Especially in spoken and written natural language,there are nuances and shades that require ?grey levels?between 0 and 1.2.2 Bayes Decision Rule andSystem ArchitectureIn machine translation, the goal is the translation of atext given in a source language into a target language.
Weare given a source string fJ1 = f1...fj ...fJ , which is to betranslated into a target string eI1 = e1...ei...eI .
In this arti-cle, the term word always refers to a full-form word.
Amongall possible target strings, we will choose the string with thehighest probability which is given by Bayes decision rule [5]:e?I1 = argmaxeI1{Pr(eI1|fJ1 )}= argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)} .Here, Pr(eI1) is the language model of the target language,and Pr(fJ1 |eI1) is the string translation model which will bedecomposed into lexicon and alignment models.
The argmaxoperation denotes the search problem, i.e.
the generationof the output sentence in the target language.
The overallarchitecture of the statistical translation approach is sum-marized in Figure 1.In general, as shown in this figure, there may be additionaltransformations to make the translation task simpler for thealgorithm.
The transformations may range from the cate-gorization of single words and word groups to more complexpreprocessing steps that require some parsing of the sourcestring.
We have to keep in mind that in the search procedureboth the language and the translation model are applied af-ter the text transformation steps.
However, to keep thenotation simple, we will not make this explicit distinction inthe subsequent exposition.3.
ALIGNMENT MODELLING3.1 ConceptA key issue in modelling the string translation probabil-ity Pr(fJ1 |eI1) is the question of how we define the corre-spondence between the words of the target sentence and thewords of the source sentence.
In typical cases, we can as-sume a sort of pairwise dependence by considering all wordpairs (fj , ei) for a given sentence pair (fJ1 ; eI1).
Here, we willfurther constrain this model by assigning each source wordto exactly one target word.
Later, this requirement will berelaxed.
Models describing these types of dependencies arereferred to as alignment models [5, 24].When aligning the words in parallel texts, we typicallyobserve a strong localization effect.
Figure 2 illustrates thiseffect for the language pair German?English.
In many cases,although not always, there is an additional property: overlarge portions of the source string, the alignment is mono-tone.Source Language TextTransformationLexicon ModelLanguage ModelGlobal Search:Target Language TextoverPr(f1  J  |e1I )Pr(   e1I )Pr(f1  J  |e1I )   Pr(   e1I )e1If1 Jmaximize  Alignment ModelTransformationFigure 1: Architecture of the translation approachbased on Bayes decision rule.3.2 Basic ModelsTo arrive at a quantitative specification, we define thealignment mapping: j ?
i = aj , which assigns a word fjin position j to a word ei in position i = aj .
We rewritethe probability for the translation model by introducing the?hidden?
alignments aJ1 := a1...aj ...aJ for each sentence pair(fJ1 ; eI1).
To structure this probability distribution, we fac-torize it over the positions in the source sentence and limitthe alignment dependencies to a first-order dependence:Pr(fJ1 |eI1) = p(J |I) ?XaJ1JYj=1[p(aj |aj?1, I, J) ?
p(fj |eaj )] .Here, we have the following probability distributions:?
the sentence length probability: p(J |I), which is in-cluded here for completeness, but can be omitted with-out loss of performance;?
the lexicon probability: p(f |e);?
the alignment probability: p(aj |aj?1, I, J).By making the alignment probability p(aj |aj?1, I, J) depen-dent on the jump width aj ?
aj?1 instead of the absolutepositions aj , we obtain the so-called homogeneous hiddenMarkov model, for short HMM [24].We can also use a zero-order model p(aj |j, I, J), wherethere is only a dependence on the absolute position index jof the source string.
This is the so-called model IBM-2 [5].Assuming a uniform alignment probability p(aj |j, I, J) =1/I, we arrive at the so-called model IBM-1.These models can be extended to allow for source wordshaving no counterpart in the translation.
Formally, thisis incorporated into the alignment models by adding a so-called ?empty word?
at position i = 0 to the target sentenceand aligning all source words without a direct translation tothis empty word.wellIthinkifwecanmakeitateightonbothdaysja ich denkewenn wir dashinkriegen anbeidenTagenacht UhrFigure 2: Word-to-word alignment.In [5], more refined alignment models are introduced byusing the concept of fertility.
The idea is that often a wordin the target language may be aligned to several words inthe source language.
This is the so-called model IBM-3.
Us-ing, in addition, first-order alignment probabilities along thepositions of the source string leads us to model IBM-4.
Al-though these models take one-to-many alignments explicitlyinto account, the lexicon probabilities p(f |e) are still basedon single words in each of the two languages.In systematic experiments, it was found that the qual-ity of the alignments determined from the bilingual trainingcorpus has a direct effect on the translation quality [14].3.3 Alignment Template ApproachA general shortcoming of the baseline alignment modelsis that they are mainly designed to model the lexicon de-pendences between single words.
Therefore, we extend theapproach to handle word groups or phrases rather than sin-gle words as the basis for the alignment models [15].
Inother words, a whole group of adjacent words in the sourcesentence may be aligned with a whole group of adjacentwords in the target language.
As a result, the context ofwords tends to be explicitly taken into account, and thedifferences in local word orders between source and targetlanguages can be learned explicitly.
Figure 3 shows some ofthe extracted alignment templates for a sentence pair fromthe Verbmobil training corpus.
The training algorithm forthe alignment templates extracts all phrase pairs which arealigned in the training corpus up to a maximum length of 7words.
To improve the generalization capability of the align-ment templates, the templates are determined for bilingualword classes rather than words directly.
These word classesare determined by an automatic clustering procedure [13].4.
SEARCHThe task of the search algorithm is to generate the mostlikely target sentence eI1 of unknown length I for an observedsource sentence fJ1 .
The search must make use of all threeknowledge sources as illustrated by Figure 4: the alignmentmodel, the lexicon model and the language model.
All threeokay,howaboutthenineteenthatmaybe,twoo?clockintheafternoon?okay ,wiesieht es amneunzehnten aus ,vielleicht umzwei Uhrnachmittags ?Figure 3: Example of a word alignment and of ex-tracted alignment templates.of them must contribute in the final decision about the wordsin the target language.To illustrate the specific details of the search problem, weslightly change the definitions of the alignments:?
we use inverted alignments as in the model IBM-4 [5]which define a mapping from target to source positionsrather the other way round.?
we allow several positions in the source language to becovered, i.e.
we consider mappings B of the form:B : i ?
Bi ?
{1, ...j, ...J}We replace the sum over all alignments by the bestalignment, which is referred to as maximum approxima-tion in speech recognition.
Using a trigram language modelp(ei|, ei?2, ei?1), we obtain the following search criterion:maxBI1 ,eI1IYi=124[p(ei|ei?1i?2) ?
p(Bi|Bi?1, I, J) ?Yj?Bip(fj |ei)]35Considering this criterion, we can see that we can buildup hypotheses of partial target sentences in a bottom-to-top strategy over the positions i of the target sentence ei1as illustrated in Figure 5.
An important constraint for thealignment is that all positions of the source sentence shouldbe covered exactly once.
This constraint is similar to thatof the travelling salesman problem where each city has tobe visited exactly once.
Details on various search strategiescan be found in [4, 9, 12, 21].In order to take long context dependences into account,we use a class-based five-gram language model with backing-off.
Beam-search is used to handle the huge search space.
Tonormalize the costs of partial hypotheses covering differentparts of the input sentence, an (optimistic) estimation of theremaining cost is added to the current accumulated cost asfollows.
For each word in the source sentence, a lower boundon its translation cost is determined beforehand.
Using thisSENTENCE INSOURCE LANGUAGETRANSFORMATIONSENTENCE GENERATEDIN TARGET LANGUAGESENTENCEKNOWLEDGE SOURCESSEARCH: INTERACTION OFKNOWLEDGE SOURCESWORD + POSITIONALIGNMENTLANGUAGE MODELBILINGUAL LEXICONALIGNMENTMODELWORD RE-ORDERINGSYNTACTIC ANDSEMANTIC ANALYSISLEXICAL CHOICEHYPOTHESESHYPOTHESESHYPOTHESESTRANSFORMATIONFigure 4: Illustration of search in statistical trans-lation.lower bound, it is possible to achieve an efficient estimationof the remaining cost.5.
EXPERIMENTAL RESULTS5.1 The Task and the CorpusWithin the Verbmobil project, spoken dialogues wererecorded.
These dialogues were manually transcribed andlater manually translated by Verbmobil partners (Hildes-heim for Phase I and Tu?bingen for Phase II).
Since differenthuman translators were involved, there is great variabilityin the translations.Each of these so-called dialogues turns may consist of sev-eral sentences spoken by the same speaker and is sometimesSOURCE POSITIONTARGETPOSITIONii-1jFigure 5: Illustration of bottom-to-top search.rather long.
As a result, there is no one-to-one correspon-dence between source and target sentences.
To achieve aone-to-one correspondence, the dialogue turns are split intoshorter segments using punctuation marks as potential splitpoints.
Since the punctuation marks in source and targetsentences are not necessarily identical, a dynamic program-ming approach is used to find the optimal segmentationpoints.
The number of segments in the source sentence andin the test sentence can be different.
The segmentation isscored using a word-based alignment model, and the seg-mentation with the best score is selected.
This segmentedcorpus is the starting point for the training of translationand language models.
Alignment models of increasing com-plexity are trained on this bilingual corpus [14].A standard vocabulary had been defined for the variousspeech recognizers used in Verbmobil.
However, not allwords of this vocabulary were observed in the training cor-pus.
Therefore, the translation vocabulary was extendedsemi-automatically by adding about 13 000 German?Englishword pairs from an online bilingual lexicon available on theweb.
The resulting lexicon contained not only word-wordentries, but also multi-word translations, especially for thelarge number of German compound words.
To counteractthe sparseness of the training data, a couple of straightfor-ward rule-based preprocessing steps were applied before anyother type of processing:?
categorization of proper names for persons and cities,?
normalization of:?
numbers,?
time and date phrases,?
spelling: don?t ?
do not,...?
splitting ofGerman compound words.Table 1 gives the characteristics of the training corpusand the lexicon.
The 58 000 sentence pairs comprise abouthalf a million running words for each language of the bilin-gual training corpus.
The vocabulary size is the number ofdistinct full-form words seen in the training corpus.
Punctu-ation marks are treated as regular words in the translationapproach.
Notice the large number of word singletons, i. e.words seen only once.
The extended vocabulary is the vo-cabulary after adding the manual bilingual lexicon.5.2 Offline ResultsDuring the progress of the Verbmobil project, differentvariants of statistical translation were implemented, and ex-Table 1: Bilingual training corpus, recognition lex-icon and translation lexicon (PM = punctuationmark).German EnglishTraining Text Sentences 58 332Words (+PMs) 519 523 549 921Vocabulary 7 940 4 673Singletons 44.8% 37.6%Recognition Vocabulary 10 157 6 871Translation Manual Pairs 12 779Ext.
Vocab.
11 501 6 867perimental tests were performed for both text and speechinput.
To summarize these experimental tests, we brieflyreport experimental offline results for the following transla-tion approaches:?
single-word based approach [20];?
alignment template approach [15];?
cascaded transducer approach [23]:unlike the other two-approaches, this approach re-quires a semi-automatic training procedure, in whichthe structure of the finite state transducers is designedmanually.
For more details, see [23].The offline tests were performed on text input for the trans-lation direction from German to English.
The test set con-sisted of 251 sentences, which comprised 2197 words and 430punctuation marks.
The results are shown in Table 2.
Tojudge and compare the quality of different translation ap-proaches in offline tests, we typically use the following errormeasures [11]:?
mWER (multi-reference word error rate):For each test sentence sk in the source language, thereare several reference translationsRk = {rk1, .
.
.
, rknk}in the target language.
For each translation of the testsentence sk, the edit distances (number of substitu-tions, deletions and insertions as in speech recognition)to all sentences in Rk are calculated, and the smallestdistance is selected and used as error measure.?
SSER (subjective sentence error rate):Each translated sentence is judged by a human exam-iner according to an error scale from 0.0 (semanticallyand syntactically correct) to 1.0 (completely wrong).Both error measures are reported in Table 2.
Althoughthe experiments with the cascaded transducers [23] were notfully optimized yet, the preliminary results indicated thatthis semi-automatic approach does not generalize as wellas the other two fully automatic approaches.
Among thesetwo, the alignment template approach was found to workconsistently better across different test sets (and also tasksdifferent from Verbmobil).
Therefore, the alignment tem-plate approach was used in the final Verbmobil prototypesystem.5.3 Disambiguation ExamplesIn the statistical translation approach as we have pre-sented it, no explicit word sense disambiguation is per-formed.
However, a kind of implicit disambiguation is pos-sible due to the context information of the alignment tem-plates and the language model as shown by the examplesin Table 3.
The first two groups of sentences contain theTable 2: Comparison of three statistical translationapproaches (test on text input: 251 sentences =2197 words + 430 punctuation marks).Translation mWER SSERApproach [%] [%]Single-Word Based 38.2 35.7Alignment Template 36.0 29.0Cascaded Transducers >40.0 >40.0verbs ?gehen?
and ?annehmen?
which have different transla-tions, some of which are rather collocational.
The correcttranslation is only possible by taking the whole sentenceinto account.
Some improvement can be achieved by ap-plying morpho-syntactic analysis, e.g handling of the sepa-rated verb prefixes in German [10].
The last two sentencesshow the implicit disambiguation of the temporal and spa-tial sense for the German preposition ?vor?.
Although thesystem has not been tailored to handle such types of disam-biguation, the translated sentences are all acceptable, apartfrom the sentence: The meeting is to five.5.4 Integration into the Verbmobil PrototypeSystemThe statistical approach to machine translation is em-bodied in the stattrans module which is integrated into theVerbmobil prototype system.
We briefly review those as-pects of it that are relevant for the statistical translation ap-proach.
The implementation supports the translation direc-tions from German to English and from English to German.In regular processing mode, the stattrans module receivesits input from the repair module [18].
At that time, theword lattices and best hypotheses from the speech recogni-tion systems have already been prosodically annotated, i.e.information about prosodic segment boundaries, sentencemode and accentuated syllables are added to each edge inthe word lattice [2].
The translation is performed on thesingle best sentence hypothesis of the recognizer.The prosodic boundaries and the sentence mode informa-tion are utilized by the stattrans module as follows.
If thereis a major phrase boundary, a full stop or question mark isinserted into the word sequence, depending on the sentencemode as indicated by the prosody module.
Additional com-mas are inserted for other types of segment boundaries.
Theprosody module calculates probabilities for segment bound-aries, and thresholds are used to decide if the sentence marksare to be inserted.
These thresholds have been selected insuch a way that, on the average, for each dialogue turn, agood segmentation is obtained.
The segment boundaries re-strict possible word reordering between source and targetlanguage.
This not only improves translation quality, butalso restricts the search space and thereby speeds up thetranslation process.5.5 Large-Scale End-to-End EvaluationWhereas the offline tests reported above were importantfor the optimization and tuning of the system, the mostimportant evaluation was the final evaluation of the Verb-mobil prototype in spring 2000.
This end-to-end evaluationof the Verbmobil system was performed at the Universityof Hamburg [19].
In each session of this evaluation, twonative speakers conducted a dialogue.
They did not haveany direct contact and could only interact by speaking andlistening to the Verbmobil system.Three other translation approaches had been integratedinto the Verbmobil prototype system:?
a classical transfer approach [3, 7, 22],which is based on a manually designed analysis gram-mar, a set of transfer rules, and a generation grammar,?
a dialogue act based approach [16],which amounts to a sort of slot filling by classifyingTable 3: Disambiguation examples (?
: using morpho-syntactic analysis).Ambiguous Word Text Input Translationgehen Wir gehen ins Theater.
We will go to the theater.Mir geht es gut.
I am fine.Es geht um Geld.
It is about money.Geht es bei Ihnen am Montag?
Is it possible for you on Monday?Das Treffen geht bis 5 Uhr.
The meeting is to five.annehmen Wir sollten das Angebot annehmen.
We should accept that offer.Ich nehme das Schlimmste an.
I will assume the worst.
?vor Wir treffen uns vor dem Fru?hstu?ck.
We meet before the breakfast.Wir treffen uns vor dem Hotel.
We will meet in front of the hotel.each sentence into one out of a small number of possi-ble sentence patterns and filling in the slot values,?
an example-based approach [1],where a sort of nearest neighbour concept is applied tothe set of bilingual training sentence pairs after suit-able preprocessing.In the final end-to-end evaluation, human evaluatorsjudged the translation quality for each of the four trans-lation results using the following criterion:Is the sentence approximatively correct: yes/no?The evaluators were asked to pay particular attention tothe semantic information (e.g.
date and place of meeting,participants etc) contained in the translation.
A missingtranslation as it may happen for the transfer approach orother approaches was counted as wrong translation.
Theevaluation was based on 5069 dialogue turns for the trans-lation from German to English and on 4136 dialogue turnsfor the translation from English to German.
The speechrecognizers used had a word error rate of about 25%.
Theoverall sentence error rates, i.e.
resulting from recognitionand translation, are summarized in Table 4.
As we can see,the error rates for the statistical approach are smaller by afactor of about 2 in comparison with the other approaches.In agreement with other evaluation experiments, these ex-periments show that the statistical modelling approach maybe comparable to or better than the conventional rule-basedapproach.
In particular, the statistical approach seems tohave the advantage if robustness is important, e.g.
whenthe input string is not grammatically correct or when it iscorrupted by recognition errors.Although both text and speech input are translated withgood quality on the average by the statistical approach,Table 4: Sentence error rates of end-to-end evalua-tion (speech recognizer with WER=25%; corpus of5069 and 4136 dialogue turns for translation Ger-man to English and English to German, respec-tively).Translation Method Error [%]Semantic Transfer 62Dialogue Act Based 60Example Based 52Statistical 29there are examples where the syntactic structure of the pro-duced sentence is not correct.
Some of these syntactic errorsare related to long range dependencies and syntactic struc-tures that are not captured by the m-gram language modelused.
To cope with these problems, morpho-syntactic anal-ysis [10] and grammar-based language models [17] are cur-rently being studied.6.
SUMMARYIn this paper, we have given an overview of the statisticalapproach to machine translation and especially its imple-mentation in the Verbmobil prototype system.
The sta-tistical system has been trained on about 500 000 runningwords from a bilingual German?English corpus.
Transla-tions are performed for both directions, i.e.
from Germanto English and from English to German.
Comparative eval-uations with other translation approaches of the Verbmo-bil prototype system show that the statistical translationis superior, especially in the presence of speech input andungrammatical input.AcknowledgmentThe work reported here was supported partly by the Verb-mobil project (contract number 01 IV 701 T4) by the Ger-man Federal Ministry of Education, Science, Research andTechnology and as part of the EuTrans project (ESPRITproject number 30268) by the European Community.Training ToolkitIn a follow-up project of the statistical machine translationproject during the 1999 Johns Hopkins University workshop,we have developped a publically available toolkit for thetraining of different alignment models, including the modelsIBM-1 to IBM-5 [5] and an HMM alignment model [14, 24].The software can be downloaded athttp://www-i6.Informatik.RWTH-Aachen.DE/~och/software/GIZA++.html.7.
REFERENCES[1] M. Auerswald: Example-based machine translationwith templates.
In [25], pp.
418?427.
[2] A. Batliner, J. Buckow, H. Niemann, E. No?th,V.
Warnke: The prosody module.
In [25], pp.
106?121.
[3] T. Becker, A. Kilger, P. Lopez, P. Poller: TheVerbmobil generation component VM-GECO.
In [25],pp.
481?496.
[4] A. L. Berger, P. F. Brown, J. Cocke, S. A. Della Pietra,V.
J. Della Pietra, J. R. Gillett, J. D. Lafferty,R.
L. Mercer, H. Printz,L.
Ures: The Candide Sys-tem for Machine Translation.
ARPA Human Lan-guage Technology Workshop, Plainsboro, NJ, MorganKaufmann Publishers, pp.
152-157, San Mateo, CA,March 1994.
[5] P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,R.
L. Mercer: The mathematics of statistical ma-chine translation: Parameter estimation.
Computa-tional Linguistics, Vol.
19, No.
2, pp.
263?311, 1993.
[6] N. Chomsky: ?Quine?s Empirical Assumptions?, inD.
Davidson, J. Hintikka (eds.
): Words and objections.Essays on the work of W. V. Quine, Reidel, Dordrecht,The Netherlands, 1969.
[7] M. C. Emele, M. Dorna, A. Lu?deling, H. Zinsmeister,C.
Rohrer: Semantic-based transfer.
In [25], pp.
359?376.
[8] EuTrans Project; Instituto Tecnolo?gico de Informa?tica(ITI, Spain), Fondazione Ugo Bordoni (FUB, Italy),RWTH Aachen, Lehrstuhl f. Informatik VI (Ger-many), Zeres GmbH Bochum (Germany): Example-Based Language Translation Systems.
Final report ofthe EuTrans project (EU project number 30268), July2000.
[9] H. Ney, S. Nie?en, F. J. Och, H. Sawaf, C. Tillmann,S.
Vogel: Algorithms for statistical translation of spo-ken language.
IEEE Trans.
on Speech and Audio Pro-cessing Vol.
8, No.
1, pp.
24?36, Jan.
2000.
[10] S. Nie?en, H. Ney: Improving SMT quality withmorpho-syntactic analysis.
18th Int.
Conf.
on Compu-tational Linguistics, pp.
1081-1085, Saarbru?cken, Ger-many, July 2000.
[11] S. Nie?en, F.-J.
Och, G. Leusch, H. Ney: An evalua-tion tool for machine translation: Fast evaluation forMT research.
2nd Int.
Conf.
on Language Resourcesand Evaluation, pp.39?45, Athens, Greece, May 2000.
[12] S. Nie?en, S. Vogel, H. Ney, C. Tillmann: A DPbased search algorithm for statistical machine transla-tion.
COLING?ACL ?98: 36th Annual Meeting of theAssociation for Computational Linguistics and 17thInt.
Conf.
on Computational Linguistics, pp.
960?967,Montreal, Canada, Aug.
1998.
[13] F. J. Och: An efficient method to determine bilingualword classes.
9th Conf.
of the European Chapter of theAssociation for Computational Linguistics, pp.
71?76,Bergen, Norway, June 1999.
[14] F. J. Och, H. Ney: A comparison of alignmentmodels for statistical machine translation.
18th Int.Conf.
on Computational Linguistics, pp.
1086-1090,Saarbru?cken, Germany, July 2000.
[15] F. J. Och, C. Tillmann, H. Ney: Improved alignmentmodels for statistical machine translation.
Joint SIG-DAT Conf.
on Empirical Methods in Natural LanguageProcessing and Very Large Corpora, 20?28, Universityof Maryland, College Park, MD, June 1999.
[16] N. Reithinger, R. Engel: Robust content extraction fortranslation and dialog processing.
In [25], pp.
428?437.
[17] H. Sawaf, K. Schu?tz, H. Ney: On the use of grammarbased language models for statistical machine trans-lation.
6th Int.
Workshop on Parsing Technologies,pp.
231?241, Trento, Italy, Feb.
2000.
[18] J. Spilker, M. Klarner, G. Go?rz: Processing self-corrections in a speech-to-speech system.
In [25],pp.
131?140.
[19] L. Tessiore, W. v. Hahn: Functional validation ofa machine translation system: Verbmobil.
In [25],pp.
611?631.
[20] C. Tillmann, H. Ney: Word re-ordering in a DP-basedapproach to statistical MT.
18th Int.
Conf.
on Com-putational Linguistics 2000, Saarbru?cken, Germany,pp.
850-856, Aug.
2000.
[21] C. Tillmann, S. Vogel, H. Ney, A. Zubiaga: A DP-based search using monotone alignments in statisti-cal translation.
35th Annual Conf.
of the Associationfor Computational Linguistics, pp.
289?296, Madrid,Spain, July 1997.
[22] H. Uszkoreit, D. Flickinger, W. Kasper, I.
A.
Sag:Deep linguistic analysis with HPSG.
In [25], pp.
216?263.
[23] S. Vogel, H. Ney: Translation with Cascaded Finite-State Transducers.
ACL Conf.
(Assoc.
for Comput.Linguistics), Hongkong, pp.
23-30, Oct.
2000.
[24] S. Vogel, H. Ney, C. Tillmann: HMM-based wordalignment in statistical translation.
16th Int.
Conf.
onComputational Linguistics, pp.
836?841, Copenhagen,Denmark, August 1996.
[25] W. Wahlster (Ed.
): Verbmobil: Foundations of speech-to-speech translations.
Springer-Verlag, Berlin, Ger-many, 2000.
