Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 44?52,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsDeveloping Robust Models for Favourability AnalysisDaoud Clarke Peter LaneSchool of Computer ScienceUniversity of HertfordshireHatfield, UKdaoud@metrica.netpeter.lane@bcs.org.ukPaul HenderMetricaLondon, UKpaul@metrica.netAbstractLocating documents carrying positive or neg-ative favourability is an important applicationwithin media analysis.
This paper presentssome empirical results on the challenges fac-ing a machine-learning approach to this kindof opinion mining.
Some of the challenges in-clude: the often considerable imbalance in thedistribution of positive and negative samples;changes in the documents over time; and ef-fective training and quantification proceduresfor reporting results.
This paper begins withthree datasets generated by a media-analysiscompany, classifying documents in two ways:detecting the presence of favourability, and as-sessing negative vs. positive favourability.
Wethen evaluate a machine-learning approach toautomate the classification process.
We ex-plore the effect of using five different types offeatures, the robustness of the models whentested on data taken from a later time period,and the effect of balancing the input data byundersampling.
We find varying choices forthe optimum classifier, feature set and trainingstrategy depending on the task and dataset.1 IntroductionMedia analysis is a discipline closely related to con-tent analysis (Krippendorff, 2004), with an emphasison analysing content with respect to:Favourability how favourable an article is withrespect to an entity.
This will typically be on a fivepoint scale: very negative, negative, neutral, positiveor very positive.Key messages topics or areas that a client is inter-ested in.
This allows the client to gain feedback onthe success of particular public relations campaigns,for example.Media analysis has traditionally been done manu-ally, however the explosion of content on the world-wide web, in particular social media, has led to theintroduction of automatic techniques for performingmedia analysis, e.g.
Tatzl and Waldhauser (2010).In this paper, we discuss our recent findings in ap-plying machine learning techniques to favourabilityanalysis.
The work is part of a two-year collabo-ration between Gorkana Group, which includes oneof the foremost media analysis companies, Metrica,and the University of Hertfordshire.
The goal is todevelop ways of automating media analysis, espe-cially for social media.
The data used are from tra-ditional media (newspapers and magazines) since atthe time of starting the experiment there was moremanually analysed data available.
We discuss thetypical problems that arise in this kind of text min-ing, and the practical results we have found.The documents are supplied by Durrants, the me-dia monitoring company within the Gorkana Group,and consist of text from newspaper and magazinearticles in electronic form.
Each document is anal-ysed by trained human analysts, given scores forfavourability, as well as other characteristics whichthe client has requested.
This dataset is used to pro-vide feedback to the clients about how they are por-trayed in the media, and is summarised by Metricafor clients?
monthly reports.Favourability analysis is very closely related tosentiment analysis, with the following distinction:44sentiment analysis generally focuses on a (subjec-tive) sentiment implying an opinion of the author,for example:1(1) Microsoft is the greattteesssst at EVERY-THINGexpresses the author?s opinion (which others maynot share) whereas favourability analysis, whilstalso taking into account sentiment, also measuresfavourable objective mentions of entities.
For ex-ample:2(2) Halloween Eve Was The Biggest InstagramDay Ever, Doubling Its Trafficis an objective statement (no one can doubt that thetraffic doubled) that is favourable with respect to theorganisation, Instagram.
Since the task is so simi-lar to that of sentiment analysis, we hypothesise thatsimilar techniques will be useful.The contributions of this paper are as follows: (1)whilst automated sentiment analysis has received alot of attention in the academic literature, favourabil-ity analysis has so far not benefited from an in-depthanalysis.
(2) We provide results on a wide variety ofdifferent classifiers, whereas previous work on sen-timent analysis typically considers at most two orthree different classifiers.
(3) We discuss the prob-lem of imbalanced data, looking at how this impactson the training and evaluation techniques.
(4) Weshow that both attribute selection and balancing theclassifier?s training set can improve performance.2 BackgroundThere is a very large body of literature on both sen-timent analysis and machine learning; for space rea-sons, we will mention only a small sample.2.1 Favourability AnalysisThe most closely related task to ours is arguablyopinion mining, i.e.
determining sentiment with re-spect to a particular target.
Balahur et al (2010)examine this task for newspaper articles.
Theyshow that separating out the objective favourabil-ity from the expressed sentiment led to an increase1Actually, this is an ironic comment on a blog post atTechCrunch.2A headline from TechCrunchin inter-annotator agreement, which they report as81%, after implementing improvements to the pro-cess.
Melville et al (2009) report on an automatedsystem for opinion mining applied to blogs, whichachieves between 64% and 91% accuracy, depend-ing on the domain, while Godbole et al (2007) de-scribe a system applied to news and blogs.Pang et al (2002) introduced machine learning toperform sentiment analysis.
They used na?
?ve bayes,support vector machines (SVMs) and maximum en-tropy on the movie review domain, and report ac-curacies between 77% and 83% depending on thefeature set, which included unigrams, bigrams, andpart-of-speech tagged unigrams.
More recent workalong these lines is described in (Pang and Lee,2008; Prabowo and Thelwall, 2009).One approach to sentiment analysis is to buildup a lexicon of sentiment carrying words.
Turney(2002) described a way to automatically build such alexicon based on looking at co-occurrences of wordswith other words whose sentiment is known.
Thisidea was extended by Gamon et al (2005) who alsoconsidered the lack of co-occurrence as useful infor-mation.Koppel and Schler (2006) show that it is impor-tant to distinguish the two tasks of determining neu-tral from non-neutral sentiment, and positive versusnegative sentiment, and that doing so can signifi-cantly improve the accuracy of automated systems.2.2 Machine Learning ApproachesDocument classification is an ideal domain for ma-chine learning, because the raw data, the text, areeasily manipulated, and often large amounts of textcan be obtained, making the problems amenable tostatistical analysis.A classification model is essentially a mapping,from a document described as a set of feature valuesto a class label.
In most cases, this class label is asimple yes-no choice, such as whether the documentis favourable or not.
In the experimental section ofthis paper we describe results from applying a rangeof different classification algorithms.In general, two issues that affect machine-learning approaches are the selection of features,and the presence of imbalanced data.452.2.1 FeaturesUseful features for constructing classificationmodels from text documents include sets of uni-grams, bigrams or trigrams, dependency relation-ships or selected words: we review these features inthe next section.
From a machine-learning perspec-tive, it is useful for the features to include only rele-vant information, and also to be independent of eachother.
This feature-selection problem has been tack-led by several authors in different ways, e.g.
(Blumand Langley, 1997; Forman, 2003; Green et al,2010; Mladenic?, 1998; Rogati and Yang, 2002).
Inour experiments, we evaluate a technique to reducethe number of features using attribute selection.Alternative approaches to understanding the sen-timent of text attempt to go beyond the simple la-belling of the presence of a word.
Some authorshave described experiments augmenting the abovefeature sets with additional information.
Mullen andCollier (2004), for example, uses WordNet to add in-formation about words found within text, and conse-quently reports improved classification performancein a sentiment analysis task.2.3 Imbalanced DataOur datasets, as is usual in many real-world applica-tions, present varying degrees of imbalance betweenthe two classes.
Imbalanced data must be dealt withat two parts of the process: during training, to ensurethe model is capable of working with both classes,and in evaluation, to ensure a model with the bestperformance is selected for use on novel data.
Thesetwo elements are often treated together, but need tobe considered separately.
In particular, the appropri-ate training method to handle imbalanced data canvary between algorithm and domain.First considering evaluation, the standard mea-sure of accuracy (proportion of correctly classifiedexamples) is inappropriate if 90% of the documentsare within one class.
A simple ZeroR classifier (se-lecting the majority class) will score highly, but itwill never get any examples of the minority classcorrect.
A better evaluation technique uses a combi-nation of the separate accuracy measures on the twoclasses (a1 and a2), where ai denotes the proportionof instances from class i that were judged correctly.For example, the geometric mean, as proposed byKubat et al (1998), computes ?a1 ?
a2.
This hasthe property that it strongly penalises poor perfor-mance in any one class: if either a1 or a2 is zero thenthe geometric mean will be zero.
This characteristicis important for our purposes, since it is ?easy?
toget high accuracy on the majority class, the measurewill favour classifiers that perform well on the mi-nority class without significant loss of accuracy inthe majority class.
In addition, the geometric meandoes not give preference to any one class, unlike, forexample, the F-measure.
Measures such as the av-erage precision and recall, or F-measure, may alsoprove useful, especially if preference is being givento one class.Second considering the training process.
An im-balanced training set can lead to bias in the construc-tion of a machine-learning model.
Such effects arewell-known in the literature, and various approacheshave been proposed to address this problem, such asbalancing the training set using under or over sam-pling, and altering the weighting of the classifierbased on the proportion of the expected class.
In ourexperiments we used undersampling (where a ran-dom sample is taken from the majority class to bal-ance the size of the minority class); this techniquehas the disadvantage of discarding training data.
Incontrast, the SMOTE (Chawla et al, 2004) algo-rithm is a technique for creating new instances of theminority class, to balance the number in the major-ity class.
We also used geometric-mean as the eval-uation measure for algorithms such as SVMs, whenselecting parameters.3 Our Approach3.1 Description of DataThe source documents have been tagged by analystsfor favourability and unfavourability, both of whichare given a non-negative score that is indicative bothof the number of favourable/unfavourable mentionsof the organisation and the degree of favourabil-ity/unfavourability.
Neutral documents are assigneda score of zero for both favourability and unfavoura-bility.
We assign each document a class based on itsfavourability f and unfavourability u scores.
Docu-ments are categorised as follows:46Dataset Mixed V. Neg.
Negative Neutral Positive V. Pos.A 472 86 138 1610 1506 1664C 7 0 5 2824 852 50S 522 94 344 9580 2057 937Table 1: Number of documents in each class for the datasets A, C and S.Dataset Neutral Non-neutralA 1610 3866C 2824 914S 9580 3954Table 2: Class distributions for pseudo-subjectivity taskf > 0 and u > 0: mixedf = 0 and u > 1: very negativef = 0 and u = 1: negativef = 0 and u = 0: neutralf = 1 and u = 0: positivef > 1 and u = 0: very positiveTable 1 shows the number of documents in eachcategory for three datasets A, C and S, which areanonymised to protect Metrica?s clients?
privacy.
Aand S are datasets for high-tech companies, whereasC is for a charity.
This is reflected in the low oc-curence of negative favourability with dataset C.Datasets A and C contain only articles that are rele-vant to the client, whereas S contains articles for theclient?s competitors.
We only make use of favoura-bility judgments with respect to the client, however,so those that are irrelevant to the client we simplytreat as neutral.
This explains the overwhelming biastowards neutral sentiment in dataset S.In our experiments, we consider only those doc-uments which have been manually analysed and forwhich the raw text is available.
Duplicates were re-moved from the dataset.
Duplicate detection wasperformed using a modified version of Ferret (Laneet al, 2006) which compares occurrences of charac-ter trigrams between documents.
We considered twodocuments to be duplicates if they had a similarityscore higher than 0.75.This paper describes experiments for two tasks:Pseudo-subjectivity ?
detecting the presence or ab-sence of favourability.
This is thus a two-class prob-lem with neutral documents in one class, and allother documents in the other.Dataset Positive NegativeA 3170 224C 902 5S 2994 438Table 3: Class distributions for pseudo-sentiment taskPseudo-sentiment ?
distinguishing between docu-ments with generally positive and negative favoura-bility.
In our experiments, we treat this as a two classproblem, with negative and very negative docu-ments in one class and positive and very positivedocuments in the other (ignoring mixed sentiment).3.2 MethodWe follow a similar approach to Pang et al (2002):we generate features from the article text, and traina classifier using the manually analysed data.We sorted the documents by time, and then se-lected the earliest two thirds as a training set, andkept the remainder as a held out test set.
This al-lows us to get an idea of how the system will per-form when it is in use, since the system will neces-sarily be trained on documents from an earlier timeperiod.
We performed cross validation on the ran-domised training set, giving us an upper bound onthe performance of the system, and we also mea-sured the accuracy of every system on the held outdataset.
We hypothesised that new topics would bediscussed in the later time frame, and thus the accu-racy would be lower, since the system would not betrained on data for these topics.We also experimented with balancing the inputdata to the classifiers; each system was run twice,once with all the input data, and once with datawhich had been undersampled so that the numberof documents in each class was the same.
And alsowe experimented with attribute selection: reducingthe number of features used to describe the dataset.47Type Relation Termgovernor det thegovernor rcmod suedgovernor nn leaderdependent poss conferencedependent nsubj bullishdependent dep beatTable 4: Example dependency relations extracted fromthe data.
?Type?
indicates whether the term referring tothe organisation is the governor or the dependent in theexpression.3.2.1 Features for documentsWe used five types of features:Unigrams, bigrams and trigrams: produced usingthe WEKA tokenizer with the standard settings.3EntityWords: unigrams of words occurring withina sentence containing a mention of the organisationin question.
Mentions of the organisation were de-tected using manually constructed regular expres-sions, based on datasets for organisations collectedelsewhere in the company.
Sentence boundary de-tection was performed using an OpenNLP4 tool.Dependencies: we extract dependencies using theStanford dependency parser.
For the purpose ofthis experiment, we only considered dependenciesdirectly connecting the term relating to the organ-isation.
Table 4 gives example dependencies ex-tracted from the data.
For example, the phrase?.
.
.
prompted [organisation name] to be bullish.
.
.
?led to the extraction of the term bullish, where theorganisation name is the subject of the verb and theorganisation name is a dependent of the verb bullish.For each dependency, all this information is com-bined into a single feature.3.3 Classification AlgorithmsWe used the following classifiers in our experiments:na?
?ve Bayes, Support Vector Machines (SVMs), k-nearest neighbours with k = 1 and k = 5, radialbasis function (RBF) networks, Bayesian networks,decision trees (J48) and a propositional rule learner,Repeated Incremental Pruning to Produce Error Re-duction (JRip).
We also included two baseline clas-3We used the StringToWordVectorClass constructed with anargument of 5,000.4http://opennlp.sourceforge.netsifiers, ZeroR, which simply chooses the most fre-quent class in the training set, and Random, whichchooses classes at random based on their frequenciesin the training set.These are taken from the WEKA toolkit (Wittenand Frank, 2005), with the exception of SVMs, forwhich we used the LibSVM implementation, na?
?veBayes (since the Weka implementation does not ap-pear to treat the value occurring with a feature asa frequency) and Random, both of which we imple-mented ourselves.
We used WEKA?s default settingsfor classifiers where appropriate.3.3.1 Parameter search for SVMsWe used a radial-basis kernel for our SVM algo-rithm which requires two parameters to be optimisedexperimentally.
This was done for each fold of crossvalidation.
Each fold was further divided, and three-fold cross validation was performed for each param-eter combination.
We varied the gamma parameterexponentially between 10?5 and 105 in multiples of100, and varied cost between 1 and 15 in incrementsof 2.
We used the geometric mean of the accuracieson the two classes to choose the best combination ofparameters; using the geometric mean enables us totrain and evaluate the SVM from either balanced orimbalanced datasets.3.3.2 Attribute SelectionBecause of the long training time of many ofthe classifiers with numbers of features, we alsolooked at whether reducing the dimensionality of thedata before training by performing attribute selec-tion would enhance or hinder performance.
The at-tribute selection was done by ranking the featuresusing the Chi-squared measure and taking the top250 with the most correlation with the class.
The ex-ception to this was k-nearest neighbours, for whichwe used random projections with 250 dimensions.For the RBF network we tried both attribute selec-tion and random projections, and na?
?ve Bayes wasrun both with and without attribute selection.3.4 ResultsTables 5 and 6 show the best classifier on the cross-validation evaluation for each dataset and featureset for the pseudo-subjectivity and pseudo-sentimenttasks respectively, together with the Random clas-48DatasetFeatures Best Classifier Att.Sel.BalanceCross val.
acc.
Held out acc.S Random 0.465 ?
0.008 0.461 ?
0.007S EntityWords SVM X 0.912 ?
0.002 0.952 ?
0.001S Unigrams JRip X X 0.907 ?
0.002 0.952 ?
0.002S Bigrams SVM X X 0.875 ?
0.007 0.885 ?
0.004S Trigrams Na?
?ve Bayes 0.791 ?
0.003 0.759 ?
0.003S Dependencies RBFNet X 0.853 ?
0.005 0.766 ?
0.054C Random 0.417 ?
0.017 0.419 ?
0.027C EntityWords Na?
?ve Bayes X 0.704 ?
0.011 0.640 ?
0.018C Unigrams Na?
?ve Bayes X 0.735 ?
0.007 0.659 ?
0.032C Bigrams Na?
?ve Bayes 0.756 ?
0.012 0.640 ?
0.014C Trigrams Na?
?ve Bayes 0.757 ?
0.004 0.679 ?
0.017A Random 0.453 ?
0.004 0.453 ?
0.017A EntityWords BayesNet X 0.691 ?
0.008 0.625 ?
0.019A Unigrams SVM X X 0.696 ?
0.005 0.619 ?
0.010A Bigrams SVM X X 0.680 ?
0.012 0.609 ?
0.026A Trigrams Na?
?ve Bayes X 0.610 ?
0.011 0.536 ?
0.019Table 5: Results for the pseudo-subjectivity task, distinguishing documents neutral with respect to favourability fromthose which are not neutral.
The accuracy was computed as the geometric mean of accuracy on the neutral documentsand the accuracy on the non-neutral documents.
The best-performing classifier on cross-validation is shown for eachfeature set, along with the Random classifier as a baseline.
An indication is given of whether the best-performingsystem used attribute selection and/or balancing on the input data.DatasetFeatures Best Classifier BalanceCross val.
acc.
Held out acc.S Random 0.332 ?
0.023 0.365 ?
0.03S EntityWords Na?
?ve Bayes X 0.738 ?
0.008 0.552 ?
0.033S Unigrams Na?
?ve Bayes X 0.718 ?
0.017 0.650 ?
0.024S Bigrams Na?
?ve Bayes X 0.748 ?
0.013 0.682 ?
0.023S Trigrams Na?
?ve Bayes X 0.766 ?
0.014 0.716 ?
0.038S Dependencies Na?
?ve Bayes 0.566 ?
0.014 0.523 ?
0.060A Random 0.253 ?
0.026 0.111 ?
0.072A EntityWords Na?
?ve Bayes X 0.737 ?
0.016 0.656 ?
0.067A Unigrams Na?
?ve Bayes X 0.769 ?
0.008 0.756 ?
0.031A Bigrams Na?
?ve Bayes 0.755 ?
0.009 0.618 ?
0.157A Trigrams Na?
?ve Bayes 0.800 ?
0.02 0.739 ?
0.088Table 6: Results for the pseudo-sentiment task, distinguishing positive and negative favourability.
See the precedingtable for details.
None of the best performing systems used attribute selection on this task.
No data is shown for datasetC since there were not enough negative documents in the test set to compute the accuracies.49sifier baseline.
The accuracies shown were com-puted using the geometric mean of the accuracy onthe two classes.
This was computed for each cross-validation fold; the value shown is the (arithmetic)mean of the accuracies on the five folds, togetherwith an estimate of the error in this mean.
The val-ues for the held out data were computed in the sameway, dividing the data into five, allowing us to esti-mate the error in the accuracy.4 Discussion4.1 Overall accuracyThe most notable difference between the two tasks,pseudo-subjectivity and pseudo-sentiment, is thatthe best classifier for the sentiment task was na?
?veBayes in every case, whereas the best classifiervaries with dataset and feature set for the pseudo-subjectivity task.
This is presumably because the in-dependence assumption on which the na?
?ve Bayesclassifier is based holds very well for the pseudo-sentiment task, at least with our datasets.The level of accuracy we report for the pseudo-sentiment task is lower than that typically reportedfor sentiment analysis, e.g.
Pang et al (2002), butin line with that from other results, such as Melvilleet al (2009).
This could be because favourabilityis harder to determine than sentiment.
For exam-ple it may require world knowledge in addition tolinguistic knowledge, in order to determine whetherthe reporting of a particular event is good news for acompany, even if reported objectively.Accuracy on the held out dataset is up to 10%lower than the cross-validation accuracy on thepseudo-subjectivity task, and up to 6% lower on thepseudo-sentiment task.
This is probably due to achange in topics over time.
This degradation in per-formance could be reduced by techniques such asthose used to improve cross-domain sentiment anal-ysis (Li et al, 2009; Wan, 2009).4.2 FeaturesTrigrams proved the most effective feature type in3 out of the 5 different experiments, with unigramsand entity words proving the best in 1 case each.However, in many cases, there is not a significantdifference between the results for different datasets.Although we only computed dependencies forone dataset, S, we found that they did not providesignificant benefit on their own.
This may be dueto the sparseness of the data, since we only ex-tracted dependencies with respect to the organisa-tion in question.
Dependencies may be useful whencombined with other features, such as unigrams.Attribute selection was not always effectivein improving classification, even with the high-dimensionality of the data.
In the pseudo-sentimenttask, none of the best classifiers used attribute se-lection.
In the pseudo-subjectivity task, 8 out of 13results showed a benefit in using attribute selection.This issue deserves further exploration, not least be-cause reducing the number of attributes can consid-erably speed-up the training process.4.3 ImbalanceFinally, we look at our results considering the im-balanced data problem.
Within some of the algo-rithms, balance is actively taken account during thetraining process: e.g.
na?
?ve Bayes has a weightingon its class output to compensate for different fre-quencies, and the SVM training process uses geo-metric mean for computing performance, which en-courages a good performance on imbalanced data.In addition, we have presented results on the differ-ence between training with balanced and unbalanceddatasets.
Better results are obtained in 5 out of the13 results for the pseudo-subjectivity task (Table 5),and in 6 out of 9 results for the pseudo-sentimenttask (Table 6), suggesting that balancing the trainingdata is a useful technique in most cases.However, a surprising result is found in Table 7,which shows selected pseudo-subjectivity results fordataset S with and without balanced input data.
Thisdataset has an approximately 70:30 imbalance inthe class distribution.
Interestingly, balancing thedata shows mixed results for this dataset.
In par-ticular, the accuracy of the Bayesian network, andsometimes the na?
?ve Bayes classifier, are severelyreduced.
We found similar behaviour with datasetC (with a 75:25 imbalance), however, as shown inTable 8, we found the converse on dataset A (witha 30:70 imbalance): nearly every classifier per-formed better with balanced data.
Further, Table 6shows that balancing data has proven effective forthe na?
?ve Bayes classifiers in the pseudo-sentimenttask, where the imbalance is more severe (94:6 for50Unbalanced BalancedFeatures Classifier Neut.
Non.
Cross val.
acc.
Neut.
Non.
Cross val.
acc.EntityWords SVM 0.962 0.864 0.912 ?
0.003 0.959 0.864 0.911 ?
0.002EntityWords Na?
?ve Bayes 0.969 0.850 0.908 ?
0.003 1 0 0 ?
0Unigrams SVM 0.959 0.857 0.907 ?
0.002 0.954 0.859 0.905 ?
0.002Unigrams Na?
?ve Bayes 0.774 0.789 0.781 ?
0.006 0.910 0.581 0.727 ?
0.008Bigrams SVM 0.747 0.933 0.835 ?
0.006 0.849 0.901 0.875 ?
0.007Bigrams Na?
?ve Bayes 0.883 0.716 0.795 ?
0.004 0.947 0.569 0.734 ?
0.005Trigrams BayesNet 0.620 0.883 0.739 ?
0.009 0.975 0.118 0.289 ?
0.086Trigrams J48 0.356 0.964 0.586 ?
0.012 0.441 0.942 0.644 ?
0.008Trigrams JRip 0.422 0.963 0.637 ?
0.003 0.388 0.963 0.605 ?
0.042Trigrams SVM 0.575 0.921 0.728 ?
0.008 0.604 0.909 0.740 ?
0.009Trigrams Na?
?ve Bayes 0.810 0.758 0.784 ?
0.003 0.922 0.593 0.739 ?
0.005Trigrams RBFNet 0.459 0.949 0.659 ?
0.010 0.478 0.934 0.667 ?
0.013Table 7: Selected balanced versus unbalanced cross validation accuracies (geometric mean) for dataset S, pseudo-subjectivity task, together with the accuracies on the individual classes, neutral and non-neutral.
For consistency, onlyresults where attribute selection was performed are shown.Unbalanced BalancedFeatures Classifier Neut.
Non.
Cross val.
acc.
Neut.
Non.
Cross val.
acc.EntityWords SVM 0.872 0.394 0.587 ?
0.006 0.575 0.812 0.683 ?
0.007EntityWords Na?
?ve Bayes 0.972 0.111 0.326 ?
0.021 0.944 0.192 0.426 ?
0.015Unigrams SVM 0.837 0.464 0.622 ?
0.011 0.694 0.698 0.696 ?
0.005Unigrams Na?
?ve Bayes 0.896 0.318 0.531 ?
0.018 0.736 0.582 0.652 ?
0.012Bigrams SVM 0.852 0.36 0.553 ?
0.006 0.58 0.8 0.68 ?
0.012Bigrams Na?
?ve Bayes 0.959 0.203 0.439 ?
0.017 0.86 0.433 0.605 ?
0.024Trigrams SVM 0.935 0.173 0.401 ?
0.018 0.407 0.851 0.588 ?
0.009Trigrams Na?
?ve Bayes 0.938 0.249 0.481 ?
0.013 0.84 0.446 0.61 ?
0.011Table 8: Selected balanced versus unbalanced cross validation accuracies (geometric mean) for dataset A, pseudo-subjectivity task (see the preceding table for details).A, and 88:12 for S).Given these results, we suggest that balancing thetraining datasets is usually an effective strategy, al-though sometimes the benefits are small if accountof balancing is also part of the parameter-selectionprocess for your learning algorithm.5 Conclusion and Further WorkWe have empirically analysed a range of machine-learning techniques for developing favourabilityclassifiers in a commercial context.
These tech-niques include different classification algorithms,use of attribute selection to reduce the feature sets,and treatment of the imbalanced data problem.
Also,we used five different types of feature set to createthe datasets from the raw text.
We have found a widevariation, from less than 0.7 to over 0.9 geometricmean of accuracy, depending on the particular setof data analysed.
We have shown how balancingthe class distribution in training data can be benefi-cial in improving performance, but some algorithms(i.e.
na?
?ve Bayes) can be adversely affected.
In fu-ture work we will apply these techniques to largervolumes of social media, and further explore thequestions of balancing datasets, other features andfeature selection, as well as embedding these algo-rithms within the workflow of the company.51ReferencesA.
Balahur, R. Steinberger, M. Kabadjov, V. Zavarella,E.
Van Der Goot, M. Halkia, B. Pouliquen, andJ.
Belyaeva.
2010.
Sentiment analysis in the news.In Proceedings of LREC.A.L.
Blum and P. Langley.
1997.
Selection of relevantfeatures and examples in machine learning.
Artificialintelligence, 97:245?271.N.V.
Chawla, N. Japkowicz, and A. Kotcz.
2004.
Edi-torial: special issue on learning from imbalanced datasets.
ACM SIGKDD Explorations Newsletter, 6:1?6.G.
Forman.
2003.
An extensive empirical study of fea-ture selection metrics for text classification.
The Jour-nal of Machine Learning Research, 3:1289?1305.M.
Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.2005.
Pulse: Mining customer opinions from free text.Advances in Intelligent Data Analysis VI, pages 121?132.N.
Godbole, M. Srinivasaiah, and S. Skiena.
2007.Large-scale sentiment analysis for news and blogs.
InProceedings of the International Conference on We-blogs and Social Media (ICWSM).P.D.
Green, P.C.R.
Lane, A.W.
Rainer, and S. Scholz.2010.
Selecting measures in origin analysis.
In Pro-ceedings of AI-2010, The Thirtieth SGAI InternationalConference on Innovative Techniques and Applica-tions of Artificial Intelligence, pages 379?392.M.
Koppel and J. Schler.
2006.
The importance of neu-tral examples for learning sentiment.
ComputationalIntelligence, 22:100?109.K.
Krippendorff.
2004.
Content analysis: An introduc-tion to its methodology.
Sage Publications, Inc.M.
Kubat, R.C.
Holte, and S. Matwin.
1998.
Machinelearning for the detection of oil spills in satellite radarimages.
Machine Learning, 30:195?215.P.C.R.
Lane, C. Lyon, and J.A.
Malcolm.
2006.
Demon-stration of the Ferret plagiarism detector.
In Proceed-ings of the 2nd International Plagiarism Conference.T.
Li, V. Sindhwani, C. Ding, and Y. Zhang.
2009.Knowledge transformation for cross-domain senti-ment classification.
In Proceedings of the 32nd in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 716?717.ACM.P.
Melville, W. Gryc, and R. D. Lawrence.
2009.Sentiment analysis of blogs by combining lexicalknowledge with text classification.
In Proceedingsof the 15th ACM SIGKDD international conferenceon Knowledge discovery and data mining, KDD ?09,pages 1275?1284, New York, NY, USA.
ACM.D.
Mladenic?.
1998.
Feature subset selection in text-learning.
Machine Learning: ECML-98, pages 95?100.T.
Mullen and N. Collier.
2004.
Sentiment analysis us-ing support vector machines with diverse informationsources.
In Proceedings of EMNLP, volume 4, pages412?418.B.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: sentiment classification using machine learningtechniques.
In Proceedings of the ACL-02 conferenceon Empirical methods in natural language processing-Volume 10, pages 79?86.
Association for Computa-tional Linguistics.R.
Prabowo and M. Thelwall.
2009.
Sentiment analy-sis: A combined approach.
Journal of Informetrics,3:143?157.M.
Rogati and Y. Yang.
2002.
High-performing featureselection for text classification.
In Proceedings of theeleventh international conference on Information andknowledge management, pages 659?661.
ACM.G.
Tatzl and C. Waldhauser.
2010.
Aggregating opin-ions: Explorations into Graphs and Media ContentAnalysis.
ACL 2010, page 93.P.D.
Turney.
2002.
Thumbs up or thumbs down?
: Se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 417?424.
Association for ComputationalLinguistics.X.
Wan.
2009.
Co-training for cross-lingual sentimentclassification.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 1-Volume 1, pages235?243.
Association for Computational Linguistics.I.
H. Witten and E. Frank.
2005.
Data Mining: Practi-cal Machine Learning Tools and Techniques.
MorganKaufmann.52
