Workshop on TextGraphs, at HLT-NAACL 2006, pages 17?24,New York City, June 2006. c?2006 Association for Computational LinguisticsGraph-Based Text Representation for Novelty DetectionMichael GamonMicrosoft ResearchRedmond, WA 98052mgamon@microsoft.comAbstractWe discuss several feature sets fornovelty detection at the sentence level,using the data and procedure establishedin task 2 of the TREC 2004 novelty track.In particular, we investigate feature setsderived from graph representations ofsentences and sets of sentences.
We showthat a highly connected graph producedby using sentence-level term distancesand pointwise mutual information canserve as a source to extract features fornovelty detection.
We compare severalfeature sets based on such a graphrepresentation.
These feature sets allow usto increase the accuracy of an initialnovelty classifier which is based on a bag-of-word representation and KLdivergence.
The final result ties with thebest system at TREC 2004.1 IntroductionNovelty detection is the task of identifying novelinformation given a set of already accumulatedbackground information.
Potential applications ofnovelty detection systems are abundant, given the?information overload?
in email, web content etc.Gabrilovich et al(2004), for example, describe ascenario in which a newsfeed is personalized basedon a measure of information novelty: the user canbe presented with pieces of information that arenovel, given the documents that have already beenreviewed.
This will spare the user the task ofsifting through vast amounts of duplicate andredundant information on a topic to find bits andpieces of information that are of interest.In 2002 TREC introduced a novelty track(Harman 2002), which continued ?
with majorchanges ?
in 2003 (Soboroff and Harman 2003)and 2004 (Voorhees 2004).
In 2002 the task was toidentify the set of relevant and novel sentencesfrom an ordered set of documents within a TRECtopic.
Novelty was defined as ?providing newinformation that has not been found in anypreviously picked sentences?.
Relevance wasdefined as ?relevant to the question or requestmade in the description section of the topic?.
Inter-annotator agreement was low (Harman 2002).There were 50 topics for the novelty task in 2002.For the 2003 novelty track a number of majorchanges were made.
Relevance and noveltydetection were separated into different tasks,allowing a separate evaluation of relevancedetection and novelty detection.
In the 2002 track,the data proved to be problematic since thepercentage of relevant sentences in the documentswas small.
This, in turn, led to a very highpercentage of relevant sentences being novel,given that amongst the small set of relevantsentences there was little redundancy.
50 newtopics were created for the 2003 task, with a betterbalance of relevant and novel sentences.
Slightlymore than half of the topics dealt with ?events,?the rest with ?opinions.
?The 2004 track used the same tasks, the samenumber of topics and the same split between eventand opinion topics as the 2003 track.For the purpose of this paper, we are onlyconcerned with novelty detection, specifically withtask 2 of the 2004 novelty track, as described inmore detail in the following section.17The question that we investigate here is: what isa meaningful feature set for text representation fornovelty detection?
This is obviously a far-reachingand loaded question.
Possibilities range fromsimple bag-of-word features to features derivedfrom sophisticated linguistic representations.Ultimately, the question is open-ended since therewill always be another feature or featurecombination that could/should be exploited.
Forour experiments, we have decided to focus morenarrowly on the usefulness of features derivedfrom graph representations and we have restrictedourselves to representations that do not requirelinguistic analysis.
Simple bag-of-word metricslike KL divergence establish a baseline forclassifier performance.
More sophisticated metricscan be defined on the basis of graphrepresentations.
Graph representations of text canbe constructed without performing linguisticanalysis, by using term distances in sentences andpointwise mutual information between terms toform edges between term-vertices.
A term-distancebased representation has been used successfully fora variety of tasks in Mihalcea (2004) and Mihalceaand Tarau (2004).2 Previous workThere were 13 participants and 54 submitted runsfor the 2004 TREC novelty track task 2.
Eachparticipant submitted up to five runs with differentsystem configurations.
Metrics and approachesvaried widely, from purely string based approachesto systems that used sophisticated linguisticcomponents for synonymy resolution, coreferenceresolution and named entity recognition.
Manysystems employed a thresholding approach to thetask, defining a novelty metric and thendetermining a sentence to be novel if the thresholdis exceeded (e.g.
Blott et al 2004, Zhang et al2004, Abdul-Jaleel et al 2004, Eichmann et al2004, Erkan 2004).
Thresholds are eitherdetermined on the 2003 data, are based on a notionof mean score, or are determined in an ad hocmanner1.
Tomiyama et al(2004), similar to ourapproach, use an SVM classifier to make thebinary classification of a sentence as novel or not.The baseline result for the 2004 task 2 was anaverage F-measure of 0.577.
This baseline is1Unfortunately, some of the system descriptions are unclear about the exactrationale for choosing a particular threshold.achieved if all relevant sentences are categorizedas novel.
The difficulty of the novelty detectiontask is evident from the relatively low scoreachieved by even the best systems.
The five best-performing runs were:1.
Blott et al (2004) (Dublin CityUniversity): using a tf.idf based metric of?importance value?
at an ad hoc threshold:0.622.2.
Tomiyama et al (2004) (Meiji University):using an SVM classifier trained on 2003data, features based on conceptual fuzzysets derived from a background corpus:0.619.3.
Abdul-Jaleel et al (2004) (UMass): usingnamed entity recognition, using cosinesimilarity as a metric and thresholdsderived from the 2003 data set: 0.618.4.
Schiffman and McKeown (2004)(Columbia): using a combination of testsbased on weights (derived from abackground corpus) for previously unseenwords with parameters trained on the 2003data set, and taking into account thenovelty status of the previous sentence:0.617.5.
Tomiyama et al(2004) (Meiji University):slight variation of the system describedabove, with one of the features (scarcitymeasure) eliminated: 0.617.As this list shows, there was no clear tendencyof any particular kind of approach outperformingothers.
Among the above four systems and fiveruns, there are thresholding and classificationapproaches, systems that use background corporaand conceptual analysis and systems that do not.3 Experimental setup3.1 The taskTask 2 of the 2004 novelty track is formulated asfollows:Task 2: Given the relevant sentences in thecomplete document set (for a given topic),identify all novel sentences.The procedure is sequential on an ordered list ofsentences per topic.
For each Sentence Si thedetermination needs to be made whether it is novelgiven the previously seen sentences S1 through Si-1.18The evaluation metric for the novelty track is F1-measure, averaged over all 50 topics.3.2 Novelty detection as classificationFor the purpose of this paper we view noveltydetection as a supervised classification task.
Whilethe supervised approach has its limitations in real-life scenarios where annotated data are hard tocome by, it can serve as a testing ground for thequestion we are interested in: the evaluation offeature sets and text representations.At training time, a feature vector is created foreach tagged sentence S and the set of sentencesthat comprise the already seen information that S iscompared to.
Features in the vector can be featuresof the tagged sentence, features of the set ofsentences comprising the given backgroundinformation and features that capture a relationbetween the tagged sentence and the set ofbackground sentences.
A classifier is trained on theset of resulting feature vectors.
At evaluation time,a feature vector is extracted from the sentence tobe evaluated and from the set of sentences thatform the background knowledge.
The classifierthen determines whether, given the feature valuesof that vector, the sentence is more likely to benovel or not.We use the TREC 2003 data set for training,since it is close to the 2004 data set in its makeup.We train Support Vector Machines (SVMs) on the2003 data, using the LibSVM tool (Chang and Lin2001).
Following the methodology outlined inChang and Lin 2003, we use radial basis function(RBF) kernels and perform a grid search on two-fold cross validated results on the training set toidentify optimal parameter settings for the penaltyparameter C and the RBF parameter ?.Continuously valued features are scaled to valuesbetween -1 and 1.
The scaling range is determinedon the training set and the same range is applied tothe test set.The text was minimally preprocessed beforeextracting features: stop words were removed,tokens were lowercased and punctuation wasstripped from the strings.4 Text representations and features4.1 KL divergence as a featureTreating sentences as an unordered collection ofterms, the information-theoretic metric of KLdivergence (or relative entropy) has beensuccessfully used to measure ?distance?
betweendocuments by simply comparing the termdistributions in a document compared to anotherdocument or set of documents.
The notions ofdistance and novelty are closely related: if a newdocument is very distant from the collection ofdocuments that has been seen previously, it islikely to contain new, previously unseeninformation.
Gabrilovich et al (2004), forexample, report on a successful use of KLdivergence for novelty detection.
KL divergence isdefined in Equation 1:( )( ) log ( )ddw Rp wp wp w?Equation 1: KL divergence.w belongs to the set of words that are sharedbetween document d and document (set) R. pd andpR are the probability distributions of words in dand R, respectively.
Both pd(w) and pR(w) need tobe non-zero in the equation above.
We used simpleadd-one smoothing to ensure non-zero values.While it is conceivable that KL divergence couldtake into account other features than just bag-of-words information, we restrict ourselves to thisparticular use of the measure since it correspondsto the typical use in novelty detection.4.2 Term distance graphs: from text tograph without linguistic analysisKL divergence as described above treats adocument or sentence as an unordered collection ofwords.
Language obviously provides morestructure than that.
Linguistic resources can imposestructure on a string of words through consultationof linguistic knowledge (either hand-coded orlearned from a tagged corpus).
Even without anyoutside knowledge, however, the order of words ina sentence provides a means to construct a highlyconnected undirected graph with the words asvertices.
The intuition here is:191.
All words in a sentence have somerelationship to all other words in thesentence, modulo a ?window size?outside of which the relationship is nottaken into consideration2.
The closer two words are to each other,the stronger their connection tends tobe2It follows from (2) that weights on the edgeswill be inversely proportional to the distancebetween two words (vertices).
In the remainder ofthe paper we will refer to these graphs as TD (termdistance) graphs.
Of course (1) and (2) are roughgeneralizations with many counterexamples, butwithout the luxury of linguistic analysis this seemsto be a reasonable step to advance beyond simplebag-of-word assumptions.
Multiple sentencegraphs can then be combined into a highlyconnected graph to represent text.
Mihalcea (2004)and Mihalcea and Tarau (2004) have successfullyexplored very similar graph representations forextractive summarization and key word extraction.In addition to distance, we also employpointwise mutual information as defined inEquation 2 between two words/vertices to enterinto the calculation of edge weight3.
Thiscombination of distance and a cooccurrencemeasure such as PMI is reminiscent of decayinglanguage models, as described for IR, for example,in Gao et al (2002)4.
Cooccurrence is counted atthe sentence level, i.e.
( , )P i j  is estimated by thenumber of sentences that contain both terms wi andwj, and ( )P i  and ( )P j  are estimated by countingthe total sentences containing wi and wj,respectively.
As the set of seen sentences growsand cooccurrence between words becomes moreprevalent, PMI becomes more influential on edgeweights, strengthening edges between words thathave high PMI.
( , ) 2( , )log ( ) ( )i jP i jPMIP i P j=Equation 2: Pointwise Mutual Information (PMI)between two terms i and j.2This view is supported by examining dependency structures derived from thePenn Tree Bank and mapping the probability of a dependency to the distancebetween words.
See also Eisner and Smith (2005) who explore thisgeneralization for dependency parsing.3We also computed results from a graph where the edge weight is determinedonly by term distance, without PMI.
These results were consistently worse thanthe ones reported here.4We are grateful to an anonymous reviewer for pointing this out.Formally, the weight wt for each edge in thegraph is defined as in Equation 3, where di,j is thedistance between words wi and wj.and PMI(i,j) isthe pointwise mutual information between wordswi and wj, given the sentences seen so far.
For thepurpose of Equation 3 we ignored negative PMIvalues, i.e.
we treated negative PMI values as 0., 2,1 ( , )i ji jPMI i jwtd+=Equation 3: Assigning weight to an edge betweentwo vertices.We imposed a ?window size?
as a limit on themaximum distance between two words to enter anedge relationship.
Window size was variedbetween 3 and 8; on the training set a window sizeof 6 proved to be optimal.On a TD graph representation, we can calculatevarious features based on the strengths and numberof connections between words.
In noveltydetection, we can model the growing store ofbackground information by adding each?incoming?
sentence graph to the existingbackground graph.
If an ?incoming?
edge alreadyexists in the background graph, the weight of the?incoming?
edge is added to the existing edgeweight.Figure 1 shows a subset of a TD graph for thefirst two sentences of topic N57.
The visualizationis generated by the Pajek tool (Bagatelj andMrvar).Figure 1: A subset of a TD graph of the first twosentences of topic N57.204.3 Graph features4.3.1 Simple Graph featuresIn novelty detection, graph based features allow toassess the change a graph undergoes through theaddition of a new sentence.
The intuition behindthese features is that the more a graph changeswhen a sentence is added, the more likely theadded sentence is to contain novel information.After all, novel information may be conveyed evenif the terms involved are not novel.
Establishing anew relation (i.e.
edge in the graph) between twopreviously seen terms would have exactly thateffect: old terms conveying new information.
KLdivergence or any other measure of distributionalsimilarity is not suited to capture this scenario.
Asan example consider a news story thread about acrime.
The various sentences in the backgroundinformation may mention the victim, multiplesuspects, previous convictions, similar crimes etc.When a new sentence is encountered where onesuspect?s name is mentioned in the same sentencewith the victim, at a close distance, none of thesetwo terms are new.
The fact that suspect and victimare mentioned in one sentence, however, mayindicate a piece of novel information: a closerelationship between the two that did not exist inthe background story.We designed 21 graph based features, based onthe following definitions:?
Background graph: the graph representingthe previously seen sentences.?
G(S): the graph of the sentence that iscurrently being evaluated.?
Reinforced background edge: an edge thatexists both in the background graph andin G(S).?
Added background edge: a new edge inG(S) that connects two vertices thatalready exist in the background graph.?
New edge: an edge in G(S) that connectstwo previously unseen vertices.?
Connecting edge: an edge in G(S) betweena previously unseen vertex and apreviously seen vertex.The 21 features are:?
number of new edges?
number of added background edges?
number of background edges?
number of background vertices?
number of connecting edges?
sum of weights on new edges?
sum of weights on added backgroundedges?
sum of weights on connecting edges?
background connectivity (ratio betweenedges and vertices)?
connectivity added by S?
ratio between added background edges andnew edges?
ratio between new edges and connectingedges?
ratio between added background edges andconnecting edges?
ratio between the sum of weights on newedges and the sum of weights on addedbackground edges?
ratio between the sum of weights on newedges and the sum of weights onconnecting edges?
ratio between the sum of weights on addedbackground edges and the sum of weightson connecting edges?
ratio between sum of weights on addedbackground edges and the sum of pre-existing weights on those edges?
ratio between sum of weights on newedges and sum of weight on backgroundedges?
ratio between sum of weights added toreinforced background edges and sum ofbackground weights?
ratio between number of addedbackground edges and reinforcedbackground edges?
number of background edges leading fromthose background vertices that have beenconnected to new vertices by G(S)We refer to this set of 21 features as simplegraph features, to distinguish them from a secondset of graph-based features that are based onTextRank.4.3.2 TextRank featuresThe TextRank metric, as described in Mihalceaand Tarau (2004) is inspired by the PageRank21metric which is used for web page ranking5.TextRank is designed to work well in text graphrepresentations: it can take edge weights intoaccount and it works on undirected graphs.TextRank calculates a weight for each vertex,based on Equation 4.
( )( )( ) (1 ) * ( )j ik jjii jV NB V jkV NB VwtTR V d d TR Vwt???
??
?= ?
+ ?
??
??
??
?Equation 4: The TextRank metric.where TR(Vi) is the TextRank score for vertex i,NB(Vi) is the set of neighbors of Vi, i.e.
the set ofnodes connected to Vi by a single edge, wtxy is theweight of the edge between vertex x and vertex y,and d is a constant ?dampening factor?, set at0.856.
To calculate TR, an initial score of 1 isassigned to all vertices, and the formula is appliediteratively until the difference in scores betweeniterations falls below a threshold of 0.0001 for allvertices (as in Mihalcea and Tarau 2004).The TextRank score itself is not particularlyenlightening for novelty detection.
It measures the?importance?
rather than the novelty of a vertex -hence its usefulness in keyword extraction.
Wecan, however, derive a number of features from theTextRank scores that measure the change in scoresas a result of adding a sentence to the graph of thebackground information.
The rationale is that themore the TextRank scores are ?disturbed?
by theaddition of a new sentence, the more likely it isthat the new sentence carries novel information.We normalize the TextRank scores by the numberof vertices to obtain a probability distribution.
Thefeatures we define on the basis of the (normalized)TextRank metric are:1. sum of TR scores on the nodes of S,after adding S2.
maximum TR score on any nodes of S3.
maximum TR score on any backgroundnode before adding S4.
delta between 2 and 35. sum of TR scores on the backgroundnodes (after adding S)5Erkan and Radev (2005) introduced LexRank where a graph representation of aset of sentences is derived from the cosine similarity between sentences.Kurland and Lee (2004) derive a graph representation for a set of documents bylinking documents X and Y with edges weighted by the score that a languagemodel trained on X assigns to Y.6Following Mihalcea and Tarau (2004), who in turn base their default setting onBrin and Page (1998).6. delta between 5 and 17. variance of the TR scores before addingS8.
variance of TR scores after adding S9.
delta between 7 and 810. ratio of 1 to 511.
KL divergence between the TR scoresbefore and after adding S5 ResultsTo establish a baseline, we used a simple bag-of-words approach and KL divergence as a feature forclassification.
Employing the protocol describedabove, i.e.
training the classifier on the 2003 dataset, and optimizing the parameters on 2 folds of thetraining data, we achieve a surprisingly high resultof 0.618 average F-measure on the 2004 data.
Thisresult would place the run at a tie for third placewith the UMass system in the 2004 competition.In the tables below, KL refers to the KLdivergence feature, TR to the TextRank basedfeatures and SG to the simple graph based features.Given that the feature sets we investigatepossibly capture orthogonal properties, we werealso interested in using combinations of the threefeature sets.
For the graph based features wedetermined on the training set that results wereoptimal at a ?window size?
of 6, i.e.
if graph edgesare produced only if the distance between terms issix tokens or less.
All results are tabulated in Table1, with the best results boldfaced.Feature set Average F measureKL 0.618TR 0.600SG 0.619KL + SG 0.622KL + SG + TR 0.621SG + TR 0.615TR + KL 0.618Table 1: Performance of the different feature sets.We used the McNemar test to determinepairwise statistical significance levels between thenovelty classifiers based on different feature sets7.The two (boldfaced) best results from Table 1 aresignificantly different from the baseline at 0.999confidence.
Individual sentence level7We could not use the Wilcoxon rank test for our results since we only hadbinary classification results for each sentence, as opposed to individual (classprobability) scores.22classifications from the official 2004 runs were notavailable to us, so we were not able to test forstatistical significance on our results versus TRECresults.6 Summary and ConclusionWe showed that using KL divergence as a featurefor novelty classification establishes a surprisinglygood result at an average F-measure of 0.618,which would top all but 3 of the 54 runs submittedfor task 2 in the TREC novelty track in 2004.
Toimprove on this baseline we computed graphfeatures from a highly connected graph built fromsentence-level term cooccurrences with edgesweighted by distance and pointwise mutualinformation.
A set of 21 ?simple graph features?extracted directly from the graph perform slightlybetter than KL divergence, at 0.619 average F-measure.
We also computed TextRank featuresfrom the same graph representation.
TextRankfeatures by themselves achieve 0.600 average F-measure.
The best result is achieved by combiningfeature sets: Using a combination of KL featuresand simple graph features produces an average F-measure of 0.622.Being able to establish a very high baseline withjust the use of KL divergence as a feature wassurprising to us: it involves a minimal approach tonovelty detection.
We believe that the highbaseline indicates that a classification approach tonovelty detection is promising.
This iscorroborated by the very good performance of theruns from Meiji University which also used aclassifier.The second result, i.e.
the benefit obtained byusing graph based features was in line with ourexpectations.
It is a reasonable assumption that thegraph features would be able to add to theinformation that a feature like KL divergence cancapture.
The gains were statistically significant butvery modest, which poses a number of questions.First, our feature engineering may be less thanoptimal, missing important information from agraph-based representation.
Second, theclassification approach may be suffering frominherent differences between the training data(TREC 2003) and the test data (TREC 2004).
Toexplore this hypothesis, we trained SVMs on theKL + SG feature set with default settings on threerandom folds of the 2003 and 2004 data.
For theseexperiments we simply measured accuracy.
Thebaseline accuracy (predicting the majority classlabel) was 65.77% for the 2003 data and 58.59%for the 2004 data.
Average accuracy for thethreefold crossvalidation on 2003 data was75.72%, on the 2004 data it was 64.88%.
Using theSVMs trained on the 2003 data on the three foldsof the 2004 data performed below baseline at55.07%.
These findings indicate that the 2003 dataare indeed not an ideal fit as training material forthe 2004 task.With these results indicating that graph featurescan be useful for novelty detection, the questionbecomes which graph representation is best suitedto extract these features from.
A highly connectedterm-distance based graph representation, with theaddition of pointwise mutual information, is acomputationally relatively cheap approach.
Thereare at least two alternative graph representationsthat are worth exploring.First, a ?true?
dependency graph that is based onlinguistic analysis would provide a less connectedalternative.
Such a graph would, however, containmore information in the form of directed edges andedge labels (labels of semantic relations) that couldprove useful for novelty detection.
On thedownside, it would necessarily be prone to errorsand domain specificity in the linguistic analysisprocess.Second, one could use the parse matrix of astatistical dependency parser to create the graphrepresentation.
This would yield a dependencygraph that has more edges than those coming froma ?1-best?
dependency parse.
In addition, theweights on the edges could be based ondependency probability estimates, and analysiserrors would not be as detrimental since severalalternative analyses enter into the graphrepresentations.It is beyond the scope of this paper to present athorough comparison between these differentgraph representations.
However, we were able todemonstrate that a computationally simple graphrepresentation, which is based solely on pointwisemutual information and term distance, allows us tosuccessfully extract useful features for noveltydetection.
The results that can be achieved in thismanner only present a modest gain over a simpleapproach using KL divergence as a classificationfeature.
The best achieved result, however, wouldtie for first place in the 2004 TREC novelty track,23in comparison to many systems which relied onrelatively heavy analysis machinery and additionaldata resources.ReferencesNasreen Abdul-Jaleel, James Allan, W. Bruce Croft,Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D.Smucker and Courtney Wade.
2004.
UMass at TREC2004: Novelty and HARD.
NIST Special Publication500-261: The Thirteenth Text REtrieval Conference(TREC 2004).Vladimir Batagelj, Andrej Mrvar: Pajek - Program forLarge Network Analysis.
Home pagehttp://vlado.fmf.uni-lj.si/pub/networks/pajek/.Stephen Blott, Oisin Boydell, Fabrice Camous, PaulFerguson, Georgina Gaughan, Cathal Gurrin, GarethJ.
F. Jones, Noel Murphy, Noel O?Connor, Alan F.Smeaton, Barry Smyth, Peter Wilkins.
2004.Experiments in Terabyte Searching, GenomicRetrieval and Novelty Detection for TREC-2004.NIST Special Publication 500-261: The ThirteenthText REtrieval Conference (TREC 2004).Sergey Brin and Lawrence Page.
1998.
The anatomy ofa large-scale hypertextual web search engine.Computer Networks and ISDN Systems 30: 107-117.Chih-Chung Chang and Chih-Jen Lin.
LIBSVM: alibrary for support vector machines, 2001.
Softwareavailable at http://www.csie.ntu.edu.tw/~cjlin/libsvm.Chih-Chung Chang and Chih-Jen Lin.
2003.
A PracticalGuide to Support Vector Classification.David Eichmann, Yi Zhang, Shannon Bradshaw, XinYing Qiu, Li Zhou, Padmini Srinivasan, AdityaKumar Sehgal and Hudon Wong.
2004.
Novelty,Question Answering and Genomics: The Universityof Iowa Response.
NIST Special Publication 500-261: The Thirteenth Text REtrieval Conference(TREC 2004).Jason Eisner and Noah A. Smith.
2005.
Parsing withSoft and Hard Constraints on Dependency Length.Proceedings of the International Workshop onParsing Technologies (IWPT).G?ne?
Erkan.
2004.
The University of Michigan inNovelty 2004.
NIST Special Publication 500-261:The Thirteenth Text REtrieval Conference (TREC2004).G?ne?
Erkan and Dragomir Radev.
2004.
LexRank:Graph-based Centrality as Salience in TextSummarization.
Journal of Artificial IntelligenceResearch 22, pp.
457-479.Evgeniy Gabrilovich, Susan Dumais and Eric Horvitz.2004.
Newsjunkie: Providing PersonalizedNewsfeeds via Analysis of Information Novelty.WWW13, 2004.Jianfeng Gao, Jian-Yun Nie, Hongzhao He, WeijunChena and Ming Zhou.
2002.
Resolving QueryTranslation Ambiguity using a Decaying Co-occurrence Model and Syntactic DependencyRelations.
Proceedings of SIGIR 2002, 183-190.Donna Harman.
2002.
Overview of the TREC 2002Novelty Track.
NIST Special Publication 500-251:The Eleventh Text REtrieval Conference (TREC2002).Oren Kurland and Lillian Lee.
2004.
PageRank withouthyperlinks: structural re-ranking using links inducedby language models.
Proceedings of SIGIR 2005, pp.306-313.Rada Mihalcea.
2004.
Graph-based Ranking Algorithmsfor Sentence Extraction, Applied to TextSummarization.
Proceedings of the 42nd AnnualMeeting of the Association for ComputationalLinguistics, companion volume (ACL 2004).Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing Order into Texts.
Proceedings of theConference on Empirical Methods in NaturalLanguage Processing (EMNLP 2004).Barry Schiffman and Kathleen R. McKeown.
2004.Columbia University in the Novelty Track at TREC2004.
NIST Special Publication 500-261: TheThirteenth Text REtrieval Conference (TREC 2004).Ian Soboroff and Donna Harman.
2003.
Overview of theTREC 2003 Novelty Task.
NIST Special Publication500-255: The Twelfth Text REtrieval Conference(TREC 2003).Tomoe Tomiyama, Kosuke Karoji, Takeshi Kondo,Yuichi Kakuta and Tomohiro Takagi.
2004.
MeijiUniversity Web, Novelty and Genomics TrackExperiments.
NIST Special Publication 500-261: TheThirteenth Text REtrieval Conference (TREC 2004).Ellen M. Voorhees.
2004.
Overview of TREC 2004.NIST Special Publication 500-261: The ThirteenthText REtrieval Conference (TREC 2004).Hua-Ping Zhang, Hong-Bo Xu, Shuo Bai, Bin Wangand Xue-Qi Cheng.
2004.
Experiments in TREC2004 Novelty Track at CAS-ICT.
NIST SpecialPublication 500-261: The Thirteenth Text REtrievalConference (TREC 2004).24
