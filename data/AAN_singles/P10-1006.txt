Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 50?59,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsStructural Semantic Relatedness: A Knowledge-Based Method toNamed Entity DisambiguationXianpei Han        Jun Zhao?National Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of SciencesBeijing 100190, China{xphan,jzhao}@nlpr.ia.ac.cn?
Corresponding authorAbstractName ambiguity problem has raised urgentdemands for efficient, high-quality named ent-ity disambiguation methods.
In recent years,the increasing availability of large-scale, richsemantic knowledge sources (such as Wikipe-dia and WordNet) creates new opportunities toenhance the named entity disambiguation bydeveloping algorithms which can exploit theseknowledge sources at best.
The problem is thatthese knowledge sources are heterogeneousand most of the semantic knowledge withinthem is embedded in complex structures, suchas graphs and networks.
This paper proposes aknowledge-based method, called StructuralSemantic Relatedness (SSR), which can en-hance the named entity disambiguation bycapturing and leveraging the structural seman-tic knowledge in multiple knowledge sources.Empirical results show that, in comparisonwith the classical BOW based methods andsocial network based methods, our method cansignificantly improve the disambiguation per-formance by respectively 8.7% and 14.7%.1 IntroductionName ambiguity problem is common on the Web.For example, the name ?Michael Jordan?represents more than ten persons in the Googlesearch results.
Some of them are shown below:Michael (Jeffrey) Jordan, Basketball PlayerMichael (I.)
Jordan, Professor of BerkeleyMichael (B.)
Jordan, American ActorThe name ambiguity has raised serious prob-lems in many relevant areas, such as web personsearch, data integration, link analysis and know-ledge base population.
For example, in responseto a person query, search engine returns a long,flat list of results containing web pages aboutseveral namesakes.
The users are then forcedeither to refine their query by adding terms, or tobrowse through the search results to find the per-son they are seeking.
Besides, an ever-increasingnumber of question answering and informationextraction systems are coming to rely on datafrom multi-sources, where name ambiguity willlead to wrong answers and poor results.
For ex-ample, in order to extract the birth date of theBerkeley professor Michael Jordan, a systemmay return the birth date of his popular name-sakes, e.g., the basketball player Michael Jordan.So there is an urgent demand for efficient,high-quality named entity disambiguation me-thods.
Currently, the common methods fornamed entity disambiguation include name ob-servation clustering (Bagga and Baldwin, 1998)and entity linking with knowledge base (McNa-mee and Dang, 2009).
In this paper, we focus onthe method of name observation clustering.
Giv-en a set of observations O = {o1, o2, ?, on} of thetarget name to be disambiguated, a named entitydisambiguation system should group them into aset of clusters C = {c1, c2, ?, cm}, with each re-sulting cluster corresponding to one specific enti-ty.
For example, consider the following four ob-servations of Michael Jordan:1) Michael Jordan is a researcher in ComputerScience.2) Michael Jordan plays basketball in Chicago Bulls.3) Michael Jordan wins NBA MVP.4) Learning in Graphical Models: Michael Jordan.A named entity disambiguation system shouldgroup the 1st and 4th Michael Jordan observationsinto one cluster for they both refer to the Berke-50ley professor Michael Jordan, meanwhile groupthe other two Michael Jordan into another clus-ter as they refer to another person, the BasketballPlayer Michael Jordan.To a human, named entity disambiguation isusually not a difficult task as he can make deci-sions depending on not only contextual clues, butalso the prior background knowledge.
For exam-ple, as shown in Figure 1, with the backgroundknowledge that both Learning and Graphicalmodels are the topics related to Machine learning,while Machine learning is the sub domain ofComputer science, a human can easily determinethat the two Michael Jordan in the 1st and 4th ob-servations represent the same person.
In the sameway, a human can also easily identify that thetwo Michael Jordan in the 2nd and 3rd observa-tions represent the same person.Figure 1.
The exploitation of knowledge in humannamed entity disambiguationThe development of systems which could rep-licate the human disambiguation ability, however,is not a trivial task because it is difficult to cap-ture and leverage the semantic knowledge ashumankind.
Conventionally, the named entitydisambiguation methods measure the similaritybetween name observations using the bag ofwords (BOW) model (Bagga and Baldwin (1998);Mann and Yarowsky (2006); Fleischman andHovy (2004); Pedersen et al (2005)), where aname observation is represented as a feature vec-tor consisting of the contextual terms.
This mod-el measures similarity based on only the co-occurrence statistics of terms, without consider-ing all the semantic relations like social related-ness between named entities, associative related-ness between concepts, and lexical relatedness(e.g., acronyms, synonyms) between key terms.Figure 2.
Part of the link structure of WikipediaFortunately, in recent years, due to the evolu-tion of Web (e.g., the Web 2.0 and the SemanticWeb) and many research efforts for the construc-tion of knowledge bases, there is an increasingavailability of large-scale knowledge sources,such as Wikipedia and WordNet.
These large-scale knowledge sources create new opportuni-ties for knowledge-based named entity disam-biguation methods as they contain rich semanticknowledge.
For example, as shown in Figure 2,the link structure of Wikipedia contains rich se-mantic relations between concepts.
And we be-lieve that the disambiguation performance can begreatly improved by designing algorithms whichcan exploit these knowledge sources at best.The problem of these knowledge sources isthat they are heterogeneous (e.g., they containdifferent types of semantic relations and differenttypes of concepts) and most of the semanticknowledge within them is embedded in complexstructures, such as graphs and networks.
For ex-ample, as shown in Figure 2, the semantic rela-tion between Graphical Model and ComputerScience is embedded in the link structure of theWikipedia.
In recent years, some research hasinvestigated to exploit some specific semanticknowledge, such as the social connection be-tween named entities in the Web (Kalashnikov etal.
(2008), Wan et al (2005) and Lu et al(2007)), the ontology connection in DBLP (Has-sell et al, 2006) and the semantic relations inWikipedia (Cucerzan (2007), Han and Zhao(2009)).
These knowledge-based methods, how-ever, usually are specialized to the knowledgesources they used, so they often have the know-ledge coverage problem.
Furthermore, these me-thods can only exploit the semantic knowledge toa limited extent because they cannot take thestructural semantic knowledge into consideration.To overcome the deficiencies of previous me-thods, this paper proposes a knowledge-basedmethod, called Structural Semantic Relatedness(SSR), which can enhance the named entity dis-ambiguation by capturing and leveraging thestructural semantic knowledge from multipleknowledge sources.
The key point of our methodis a reliable semantic relatedness measure be-tween concepts (including WordNet concepts,NEs and Wikipedia concepts), called StructuralSemantic Relatedness, which can capture boththe explicit semantic relations between conceptsand the implicit semantic knowledge embeddedin graphs and networks.
In particular, we firstextract the semantic relations between two con-cepts from a variety of knowledge sources andComputer ScienceMachine learningStatisticsGraphical model LearningMathematicProbability Theory2) Michael Jordan plays basketball in Chicago Bulls.1) Michael Jordan is a researcher in Computer Science.4) Learning in Graphical Models: Michael Jordan3) Michael Jordan wins NBA MVP.Machine learning51represent them using a graph-based model, se-mantic-graph.
Then based on the principle that?two concepts are semantic related if they areboth semantic related to the neighbor concepts ofeach other?, we construct our Structural Seman-tic Relatedness measure.
In the end, we leveragethe structural semantic relatedness measure fornamed entity disambiguation and evaluate theperformance on the standard WePS data sets.The experimental results show that our SSR me-thod can significantly outperform the traditionalmethods.This paper is organized as follows.
Section 2describes how to construct the structural seman-tic relatedness measure.
Next in Section 3 wedescribe how to leverage the captured knowledgefor named entity disambiguation.
Experimentalresults are demonstrated in Sections 4.
Section 5briefly reviews the related work.
Section 6 con-cludes this paper and discusses the future work.2 The Structural Semantic RelatednessMeasureIn this section, we demonstrate the structural se-mantic relatedness measure, which can capturethe structural semantic knowledge in multipleknowledge sources.
Totally, there are two prob-lems we need to address:1) How to extract and represent the seman-tic relations between concepts, since there aremany types of semantic relations and they mayexist as different patterns (the semantic know-ledge may exist as explicit semantic relations orbe embedded in complex structures).2) How to capture all the extracted seman-tic relations between concepts in our semanticrelatedness measure.To address the above two problems, in follow-ing we first introduce how to extract the semanticrelations from multiple knowledge sources; thenwe represent the extracted semantic relations us-ing the semantic-graph model; finally we buildour structural semantic relatedness measure.2.1 Knowledge SourcesWe extract three types of semantic relations (se-mantic relatedness between Wikipedia concepts,lexical relatedness between WordNet conceptsand social relatedness between NEs) correspon-dingly from three knowledge sources: Wikipedia,WordNet and NE Co-occurrence Corpus.1.
Wikipedia1, a large-scale online encyc-lopedia, its English version includes more than3,000,000 concepts and new articles are addedquickly and up-to-date.
Wikipedia contains richsemantic knowledge in the form of hyperlinksbetween Wikipedia articles, such as Polysemy(disambiguation pages), Synonym (redirect pages)and Associative relation (hyperlinks betweenWikipedia articles).
In this paper, we extract thesemantic relatedness sr between Wikipedia con-cepts using the method described in Milne andWitten(2008):log(max( )) log( )( , ) 1log( ) log(min( , ))A B A Bsr a bW A B?= ?
??
?where a and b are the two concepts of interest, Aand B are the sets of all the concepts that are re-spectively linked to a and b, and W is the entireWikipedia.
For demonstration, we show the se-mantic relatedness between four selected con-cepts in Table 1.Statistics BasketballMachine learning 0.58 0.00MVP 0.00 0.45Table 1.
The semantic relatedness table of four se-lected Wikipedia concepts2.
WordNet 3.02 (Fellbaum et al, 1998), alexical knowledge source includes over 110,000WordNet concepts (word senses about Englishwords).
Various lexical relations are recordedbetween WordNet concepts, such as hyponyms,holonym and synonym.
The lexical relatedness lrbetween two WordNet concepts are measuredusing the Lin (1998)?s WordNet semantic simi-larity measure.
Table 2 shows some examples ofthe lexical relatedness.school scienceuniversity 0.67 0.10research 0.54 0.39Table 2.
The lexical relatedness table of four selectedWordNet concepts3.
NE Co-occurrence Corpus, a corpus ofdocuments for capturing the social relatednessbetween named entities.
According to the fuzzyset theory (Baeza-Yates et al, 1999), the degreeof named entities co-occurrence in a corpus is ameasure of the relatedness between them.
Forexample, in Google search results, the ?ChicagoBulls?
co-occurs with ?NBA?
in more than1 http://www.wikipedia.org/2 http:// wordnet.princeton.edu/527,900,000 web pages, while only co-occurs with?EMNLP?
in less than 1,000 web pages.
So theco-occurrence statistics can be used to measurethe social relatedness between named entities.
Inthis paper, given a NE Co-occurrence Corpus D,the social relatedness scr between two namedentities ne1 and ne2 is measured using the GoogleSimilarity Distance (Cilibrasi and Vitanyi, 2007):1 2 1 21 21 2log(max( , )) log( )( , ) 1log( ) log(min( , ))D D D Dscr ne neD D D?= ?
?
?where D1 and D2 are the document sets corres-pondingly containing ne1 and ne2.
An example ofsocial relatedness is shown in Table 3, which iscomputed using the Web corpus through Google.ACL NBAEMNLP 0.61 0.00Chicago Bulls 0.19 0.55Table 3.
The social relatedness table of four selectednamed entities2.2 The Semantic-Graph ModelIn this section we present a graph-based repre-sentation, called semantic-graph, to model theextracted semantic relations as a graph withinwhich the semantic relations are interconnectedand transitive.
Concretely, the semantic-graph isdefined as follows:A semantic-graph is a weighted graph G = (V,E), where each node represents a distinct con-cept; and each edge between a pair of nodesrepresents the semantic relation between thetwo concepts corresponding to these nodes,with the edge weight indicating the strength ofthe semantic relation.For demonstration, Figure 3 shows a semantic-graph which models the semantic knowledgeextracted from Wikipedia for the Michael Jordanobservations in Section 1.Figure 3.
An example of semantic-graphGiven a set of name observations, the con-struction of semantic-graph takes two steps: con-cept extraction and concept connection.
In thefollowing we respectively describe each step.1) Concept Extraction.
In this step we ex-tract all the concepts in the contexts of name ob-servations and represent them as the nodes in thesemantic-graph.
We first gather all the N-grams(up to 8 words) and identify whether they corres-pond to semantically meaningful concepts: if aN-gram is contained in the WordNet, we identifyit as a WordNet concept, and use its primaryword sense as its semantic meaning; to findwhether a N-gram is a named entity, we match itto the named entity list extracted using the open-Calais API3, which contains more than 30 typesof named entities, such as Person, Organizationand Award; to find whether a N-gram is a Wiki-pedia concept, we match it to the Wikipedia anc-hor dictionary, then find its corresponding Wiki-pedia concept using the method described in(Medelyan et al 2008).
After concept identifica-tion, we filter out all the N-grams which do notcorrespond to the semantic meaningful concepts,such as the N-grams ?learning in?
and ?winsNBA MVP?.
The retained N-grams are identifiedas concepts, corresponding with their semanticmeanings (a concept may have multiple semanticmeaning explanation, e.g., the ?MVP?
has threesemantic meaning, as ?most valuable player,MVP?
in WordNet, as the ?Most Valuable Play-er?
in Wikipedia and as a named entity of Awardtype).2) Concept Connection.
In this step werepresent the semantic relations as the edges be-tween nodes.
That is, for each pair of extractedconcepts, we identify whether there are semanticrelations between them: 1) If there is only onesemantic relation between them, we connectthese two concepts with an edge, where the edgeweight is the strength of the semantic relation; 2)If there is more than one semantic relations be-tween them, we choose the most reliable seman-tic relation, i.e., we choose the semantic relationin the knowledge sources according to the orderof WordNet, Wikipedia and NE Co-concurrencecorpus (Suchanek et al, 2007).
For example, ifboth Wikipedia and WordNet provide the seman-tic relation between MVP and NBA, we choosethe semantic relation provided by WordNet.3 http://www.opencalais.com/Researcher GraphicalModelLearningNBAMVPBasketballChicago BullsComputerScience0.32 0.280.480.410.580.760.450.710.71 0.57532.3 The Structural Semantic RelatednessMeasureIn this section, we describe how to capture thesemantic relations between the concepts in se-mantic-graph using a semantic relatedness meas-ure.
Totally, the semantic knowledge betweenconcepts is modeled in two forms:1) The edges of semantic-graph.
Theedges model the direct semantic relations be-tween concepts.
We call this form of semanticknowledge as explicit semantic knowledge.2) The structure of semantic-graph.
Ex-cept for the edges, the structure of the semantic-graph also models the semantic knowledge ofconcepts.
For example, the neighbors of a con-cept represent all the concepts which are explicit-ly semantic-related to this concept; and the pathsbetween two concepts represent all the explicitand implicit semantic relations between them.We call this form of semantic knowledge asstructural semantic knowledge, or implicit se-mantic knowledge.Therefore, in order to deduce a reliable seman-tic relatedness measure, we must take both theedges and the structure of semantic-graph intoconsideration.
Under the semantic-graph model,the measurement of semantic relatedness be-tween concepts equals to quantifying the similar-ity between nodes in a weighted graph.
To simpl-ify the description, we assign each node in se-mantic-graph an integer index from 1 to |V| anduse this index to represent the node, then we canwrite the adjacency matrix of the semantic-graphG as A, where A[i,j] or Aij is the edge weight be-tween node i and node j.The problem of quantifying the relatedness be-tween nodes in a graph is not a new problem, e.g.,the structural equivalence and structural similar-ity (the SimRank in Jeh and Widom (2002) andthe similarity measure in Leicht et al (2006)).However, these similarity measures are not suit-able for our task, because all of them assume thatthe edges are uniform so that they cannot takeedge weight into consideration.In order to take both the graph structure andthe edge weight into account, we design thestructural semantic relatedness measure by ex-tending the measure introduced in Leicht et al(2006).
The fundamental principle behind ourmeasure is ?a node u is semantically related toanother node v if its immediate neighbors aresemantically related to v?.
This definition is natu-ral, for example, as shown in Figure 3, the con-cept Basketball and its neighbors NBA and Chi-cago Bulls are all semantically related to MVP.This definition is recursive, and the starting pointwe choose is the semantic relatedness in the edge.Thus our structural semantic relatedness has twocomponents: the neighbor term of the previousrecursive phase which captures the graph struc-ture and the semantic relatedness which capturesthe edge information.
Thus, the recursive form ofthe structural semantic relatedness Sij betweenthe node i and the node j can be written as:iilij lj ijl N iAS S Ad?
?
?= +?where ?
and ?
control the relative importanceof the two components andNi={j | Aij > 0} is the set of the immediateneighbors of node i;j Nid Aiji ?
?= is the degree of node i.In order to solve this formula, we introduce thefollowing two notations:T: The relatedness transition matrix, whereT[i,j]=Aij/di, indicating the transition rate of re-latedness from node j to its neighbor i.S: The structural semantic relatedness matrix,where S[i,j]=Sij.Now we can turn our first form of structural se-mantic relatedness into the matrix form:S TS A?
?= +By solving this equation, we can get:1( )S I T A?
?
?= ?where I is the identity matrix.
Since ?
is a pa-rameter which only contributes an overall scalefactor to the relatedness value, we can ignore itand get the final form of the structural semanticrelatedness as:1( )S I T A?
?= ?Because the S is asymmetric, the finally related-ness between node i and node j is the average ofSij and Sji.The meaning of ?
: The last question of ourstructural semantic relatedness measure is how toset the free parameter ?
.
To understand themeaning of ?
, let us expand the similarity as apower series thus:2 2( ... ...)k kS I T T T A?
?
?= + + + + +Noting that the [Tk]ij element is the relatednesstransition rate from node i to node j with pathlength k, we can view the ?
as a penalty factorfor the transition path length: by setting the ?with a value within (0, 1), a longer graph pathwill contribute less to the final relatedness value.The optimal value of ?
is 0.6 through a learning54process shown in Section 4.
For demonstration,Table 4 shows some structural semantic related-ness values of the Semantic-graph in Figure 3(CS represents computer science and GMrepresents Graphical model).
From Table 4, wecan see that the structural semantic relatednesscan successfully capture the semantic knowledgeembedded in the structure of semantic-graph,such as the implicit semantic relation betweenResearcher and Learning.Researcher CS GM LearningResearcher --- 0.50 0.27 0.31CS 0.50 --- 0.62 0.73GM 0.27 0.62 --- 0.80Learning 0.31 0.73 0.80 ---Table 4.
The structural semantic relatedness of thesemantic-graph shown in Figure 33 Named Entity Disambiguation by Le-veraging Semantic KnowledgeIn this section we describe how to leverage thesemantic knowledge captured in the structuralsemantic relatedness measure for named entitydisambiguation.
Because the key problem ofnamed entity disambiguation is to measure thesimilarity between name observations, we inte-grate the structural semantic relatedness in thesimilarity measure, so that it can better reflect theactual similarity between name observations.Concretely, our named entity disambiguationsystem works as follows: 1) Measuring the simi-larity between name observations; 2) Groupingname observations using the clustering algorithm.In the following we describe each step in detail.3.1 Measuring the Similarity between NameObservationsIntuitively, if two observations of the target namerepresent the same entity, it is highly possiblethat the concepts in their contexts are closely re-lated, i.e., the named entities in their contexts aresocially related and the Wikipedia concepts intheir contexts are semantically related.
In con-trast, if two name observations represent differ-ent entities, the concepts within their contextswill not be closely related.
Therefore we canmeasure the similarity between two name obser-vations by summarizing all the semantic related-ness between the concepts in their contexts.To measure the similarity between name ob-servations, we represent each name observationas a weighted vector of concepts (includingnamed entities, Wikipedia concepts and Word-Net concepts), where the concepts are extractedusing the same method described in Section 2.2,so they are just the same concepts within the se-mantic-graph.
Using the same concept index asthe semantic-graph, a name observation oi is thenrepresented as 1 2{ , ,..., }i i i ino w w w= , where wik isthe kth concept?s weight in observation oi, com-puted using the standard TFIDF weight model,where the DF is computed using the GoogleWeb1T 5-gram corpus4.
Given the concept vec-tor representation of two name observations oiand oj, their similarity is computed as:( , )i j il jk lk il jkl k l kSIM o o w w S w w=??
?
?which is the weighted average of all the structur-al semantic relatedness between the concepts inthe contexts of the two name observations.3.2 Grouping Name Observations throughHierarchical Agglomerative ClusteringGiven the computed similarities, name observa-tions are disambiguated by grouping them ac-cording to their represented entities.
In this paper,we group name observations using the hierar-chical agglomerative clustering(HAC) algorithm,which is widely used in prior disambiguationresearch and evaluation task (WePS1 andWePS2).
The HAC produce clusters in a bottom-up way as follows: Initially, each name observa-tion is an individual cluster; then we iterativelymerge the two clusters with the largest similarityvalue to form a new cluster until this similarityvalue is smaller than a preset merging thresholdor all the observations reside in one commoncluster.
The merging threshold can be deter-mined through cross-validation.
We employ thesingle-link method to compute the similarity be-tween two clusters, which has been applied wide-ly in prior research (Bagga and Baldwin (1998);Mann and Yarowsky (2003)).4 ExperimentsTo assess the performance of our method andcompare it with traditional methods, we conducta series of experiments.
In the experiments, weevaluate the proposed SSR method on the task ofpersonal name disambiguation, which is the mostcommon type of named entity disambiguation.
Inthe following, we first explain the general expe-rimental settings in Section 4.1, 4.2 and 4.3; thenevaluate and discuss the performance of our me-thod in Section 4.4.4 www.ldc.upenn.edu/Catalog/docs/LDC2006T13/554.1 Disambiguation Data SetsWe adopted the standard data sets used in theFirst Web People Search Clustering Task(WePS1) (Artiles et al, 2007) and the SecondWeb People Search Clustering Task (WePS2)(Artiles et al, 2009).
The three data sets we usedare WePS1_training data set, WePS1_test dataset, and WePS2_test data set.
Each of the threedata sets consists of a set of ambiguous personalnames (totally 109 personal names); and for eachname, we need to disambiguate its observationsin the web pages of the top N (100 for WePS1and 150 for WePS2) Yahoo!
search results.The experiment made the standard ?one per-son per document?
assumption, which is widelyused in the participated systems in WePS1 andWePS2, i.e., all the observations of the samename in a document are assumed to represent thesame entity.
Based on this assumption, the fea-tures within the entire web page are used to dis-ambiguate personal names.4.2 Knowledge SourcesThere were three knowledge sources we used forour experiments: the WordNet 3.0; the Sep. 9,2007 English version of Wikipedia; and the Webpages of each ambiguous name in WePS datasetsas the NE Co-occurrence Corpus.4.3 Evaluation CriteriaWe adopted the measures used in WePS1 to eva-luate the performance of name disambiguation.These measures are:Purity (Pur): measures the homogeneity ofname observations in the same cluster;Inverse purity (Inv_Pur): measures the com-pleteness of a cluster;F-Measure (F): the harmonic mean of purityand inverse purity.The detailed definitions of these measures canbe found in Amigo, et al (2008).
We use F-measure as the primary measure just likingWePS1 and WePS2.4.4 Experimental ResultsWe compared our method with four baselines: (1)BOW: The first one is the traditional Bag ofWords model (BOW) based methods: hierarchic-al agglomerative clustering (HAC) over termvector similarity, where the features includingsingle words and NEs, and all the features areweighted using TFIDF.
This baseline is also thestate-of-art method in WePS1 and WePS2.
(2)SocialNetwork: The second one is the socialnetwork based methods, which is the same as themethod described in Malin et al (2005): HACover the similarity obtained through randomwalk over the social network built from the webpages of the top N search results.
(3)SSR-NoKnowledge: The third one is used as a base-line for evaluating the efficiency of semanticknowledge: HAC over the similarity computedon semantic-graph with no knowledge integrated,i.e., the similarity is computed as:( , )i j il jl il jkl l kSIM o o w w w w=?
??
(4) SSR-NoStructure: The fourth one is used asa baseline for evaluating the efficiency of thesemantic knowledge embedded in complex struc-tures: HAC over the similarity computed by onlyintegrating the explicit semantic relations, i.e.,the similarity is computed as:( , )i j il jk lk il jkl k l kSIM o o w w A w w=??
?
?4.4.1 Overall PerformanceWe conducted several experiments on all thethree WePS data sets: the four baselines, the pro-posed SSR method and the proposed SSR me-thod with only one special type knowledge added,respectively SSR-NE, SSR-WordNet and SSR-Wikipedia.
All the optimal merging thresholdsused in HAC were selected by applying leave-one-out cross validation.
The overall perfor-mance is shown in Table 5.MethodWePS1_trainingPur Inv_Pur FBOW 0.71 0.88 0.78SocialNetwork 0.66 0.98 0.76SSR-NoKnowledge 0.79 0.89 0.81SSR-NoStructure 0.87 0.83 0.83SSR-NE 0.80 0.86 0.82SSR-WordNet 0.80 0.91 0.83SSR-Wikipedia 0.82 0.90 0.84SSR 0.82 0.92 0.85WePS1_testPur Inv_Pur FBOW 0.74 0.87 0.74SocialNetwork 0.83 0.63 0.65SSR-NoKnowledge 0.80 0.74 0.75SSR-NoStructure 0.80 0.78 0.78SSR-NE 0.73 0.80 0.74SSR-WordNet 0.81 0.77 0.77SSR-Wikipedia 0.88 0.77 0.81SSR 0.85 0.83 0.84WePS2_testPur Inv_Pur FBOW 0.80 0.80 0.77SocialNetwork 0.62 0.93 0.70SSR-NoKnowledge 0.84 0.80 0.80SSR-NoStructure 0.84 0.83 0.81SSR-NE 0.78 0.88 0.80SSR-WordNet 0.85 0.82 0.83SSR-Wikipedia 0.84 0.81 0.82SSR 0.89 0.84 0.86Table 5.
Performance results of baselines and SSRmethods56From the performance results in Table 5, wecan see that:1) The semantic knowledge can greatly im-prove the disambiguation performance: com-pared with the BOW and the SocialNetworkbaselines, SSR respectively gets 8.7% and 14.7%improvement on average on the three data sets.2) By leveraging the semantic knowledgefrom multiple knowledge sources, we can obtaina better named entity disambiguation perfor-mance: compared with the SSR-NE?s 0% im-provement, the SSR-WordNet?s 2.3% improve-ment and the SSR-Wikipedia?s 3.7% improve-ment, the SSR gets 6.3% improvement over theSSR-NoKnowledge baseline, which is larger thanall the SSR methods with only one type of se-mantic knowledge integrated.3) The exploitation of the structural seman-tic knowledge can further improve the disambig-uation performance: compared with SSR-NoStructure, our SSR method achieves 4.3% im-provement.Figure 4.
The F-Measure vs. ?
on three data sets4.4.2 Optimizing ParametersThere is only one parameter ?
needed to be con-figured, which is the penalty factor for the rela-tedness transition path length in the structuralsemantic relatedness measure.
Usually a smaller?
will make the structural semantic knowledgecontribute less in the resulting relatedness value.Figure 4 plots the performance of our methodcorresponding to the special ?
settings.
Asshown in Figure 4, the SSR method is not verysensitive to the ?
and can achieve its best aver-age performance when the value of ?
is 0.6.4.4.3 Detailed AnalysisTo better understand the reasons why our SSRmethod works well and how the exploitation ofstructural semantic knowledge can improve per-formance, we analyze the results in detail.The Exploitation of Semantic Knowledge.
Theprimary advantage of our method is the exploita-tion of semantic knowledge.
Our method exploitsthe semantic knowledge in two directions:1) The Integration of Multiple SemanticKnowledge Sources.
Using the semantic-graphmodel, our method can integrate the semanticknowledge extracted from multiple knowledgesources, while most traditional knowledge-basedmethods are usually specialized to one type ofknowledge.
By integrating multiple semanticknowledge sources, our method can improve thesemantic knowledge coverage.2) The exploitation of Semantic Knowledgeembedded in complex structures.
Using the struc-tural semantic relatedness measure, our methodcan exploit the implicit semantic knowledge em-bedded in complex structures; while traditionalknowledge-based methods usually lack this abili-ty.The Rich Meaningful Features.
One anotheradvantage of our method is the rich meaningfulfeatures, which is brought by the multiple seman-tic knowledge sources.
With more meaningfulfeatures, our method can better describe thename observations with less information loss.Furthermore, unlike the traditional N-gram fea-tures, the features enriched by semantic know-ledge sources are all semantically meaningfulunits themselves, so little noisy features will beadded.
The effect of rich meaningful features canalso be shown in Table 5: by adding these fea-tures, the SSR-NoKnowledge respectivelyachieves 2.3% and 9.7% improvement over theBOW and the SocialNetwork baseline.5 Related WorkIn this section, we briefly review the relatedwork.
Totally, the traditional named entity dis-ambiguation methods can be classified into twocategories: the shallow methods and the know-ledge-based methods.Most of previous named entity disambiguationresearches adopt the shallow methods, which aremostly the natural extension of the bag of words(BOW) model.
Bagga and Baldwin (1998)represented a name as a vector of its contextualwords, then two names were predicted to be thesame entity if their cosine similarity is above athreshold.
Mann and Yarowsky (2003) and Niuet al (2004) extended the vector representationwith extracted biographic facts.
Pedersen et al(2005) employed significant bigrams to represent57a name observation.
Chen and Martin (2007) ex-plored a range of syntactic and semantic features.In recent years some research has investigatedemploying knowledge sources to enhance thenamed entity disambiguation.
Bunescu and Pasca(2006) disambiguated the names using the cate-gory information in Wikipedia.
Cucerzan (2007)disambiguated the names by combining the BOWmodel with the Wikipedia category information.Han and Zhao (2009) leveraged the Wikipediasemantic knowledge for computing the similaritybetween name observations.
Bekkerman andMcCallum (2005) disambiguated names basedon the link structure of the Web pages between aset of socially related persons.
Kalashnikov et al(2008) and Lu et al (2007) used the co-occurrence statistics between named entities inthe Web.
The social network was also exploitedfor named entity disambiguation, where similari-ty is computed through random walking, such asthe work introduced in Malin (2005), Malin andAiroldi (2005), Yang et al(2006) and Minkov etal.
(2006).
Hassell et al (2006) used the relation-ships from DBLP to disambiguate names in re-search domain.6 Conclusions and Future WorksIn this paper we demonstrate how to enhance thenamed entity disambiguation by capturing andexploiting the semantic knowledge existed inmultiple knowledge sources.
In particular, wepropose a semantic relatedness measure, Struc-tural Semantic Relatedness, which can captureboth the explicit semantic relations and the im-plicit structural semantic knowledge.
The expe-rimental results on the WePS data sets demon-strate the efficiency of the proposed method.
Forfuture work, we want to develop a frameworkwhich can uniformly model the semantic know-ledge and the contextual clues for named entitydisambiguation.AcknowledgmentsThe work is supported by the National NaturalScience Foundation of China under Grants no.60875041 and 60673042, and the National HighTechnology Development 863 Program of Chinaunder Grants no.
2006AA01Z144.ReferencesAmigo, E., Gonzalo, J., Artiles, J. and Verdejo, F.2008.
A comparison of extrinsic clustering evalua-tion metrics based on formal constraints.
Informa-tion Retrieval.Artiles, J., Gonzalo, J.
& Sekine, S. 2007.
The Se-mEval-2007 WePS Evaluation: Establishing abenchmark for the Web People Search Task.
InSemEval.Artiles, J., Gonzalo, J. and Sekine, S. 2009.
WePS2Evaluation Campaign: Overview of the WebPeople Search Clustering Task.
In WePS2, WWW2009.Baeza-Yates, R., Ribeiro-Neto, B., et al 1999.
Mod-ern information retrieval.
Addison-Wesley Reading,MA.Bagga, A.
& Baldwin, B.
1998.
Entity-based cross-document coreferencing using the vector spacemodel.
Proceedings of the 17th international confe-rence on Computational linguistics-Volume 1, pp.79-85.Bekkerman, R. & McCallum, A.
2005.
Disambiguat-ing web appearances of people in a social network.Proceedings of the 14th international conference onWorld Wide Web, pp.
463-470.Bunescu, R. & Pasca, M. 2006.
Using encyclopedicknowledge for named entity disambiguation.
Pro-ceedings of EACL, vol.
6.Chen, Y.
& Martin, J.
2007.
Towards robust unsuper-vised personal name disambiguation.
Proceedingsof EMNLP and CoNLL, pp.
190-198.Cilibrasi, R. L., Vitanyi, P. M. & CWI, A.
2007.
Thegoogle similarity distance, IEEE Transactions onknowledge and data engineering, vol.
19, no.
3, pp.370-383.Cucerzan, S. 2007, Large-scale named entity disam-biguation based on Wikipedia data.
Proceedings ofEMNLP-CoNLL, pp.
708-716.Fellbaum, C., et al 1998.
WordNet: An electroniclexical database.
MIT press Cambridge, MA.Fleischman, M. B.
& Hovy, E. 2004.
Multi-documentperson name resolution.
Proceedings of ACL, Ref-erence Resolution Workshop.Han, X.
& Zhao, J.
2009.
Named entity disambigua-tion by leveraging Wikipedia semantic knowledge.Proceeding of the 18th ACM conference on Infor-mation and knowledge management, pp.
215-224.Hassell, J., Aleman-Meza, B.
& Arpinar, I.
2006.
On-tology-driven automatic entity disambiguation inunstructured text.
Proceedings of The 2006 ISWC,pp.
44-57.Jeh, G. & Widom, J.
2002.
SimRank: A measure ofstructural-context similarity, Proceedings of theeighth ACM SIGKDD international conference onKnowledge discovery and data mining, p. 543.58Kalashnikov, D. V., Nuray-Turan, R. & Mehrotra, S.2008.
Towards Breaking the Quality Curse.
AWeb-Querying Approach to Web People Search.
InProc.
of SIGIR.Leicht, E. A.,  Petter Holme,  & M. E. J. Newman.2006.
Vertex similarity in networks.
Physical Re-view E , vol.
73, no.
2, p.
26120.Lin., D. 1998.
An information-theoretic definition ofsimilarity.
In Proc.
of ICML.Lu, Y.
& Nie , Z. et al 2007.
Name DisambiguationUsing Web Connection.
In Proc.
of AAAI.Malin, B.
2005.
Unsupervised name disambiguationvia social network similarity.
SIAM SDM Work-shop on Link Analysis, Counterterrorism and Secu-rity.Malin, B., Airoldi, E. & Carley, K. M. 2005.
A net-work analysis model for disambiguation of namesin lists.
Computational & Mathematical Organiza-tion Theory, vol.
11, no.
2, pp.
119-139.Mann, G. S. & Yarowsky, D. 2003.
Unsupervisedpersonal name disambiguation, Proceedings of theseventh conference on Natural language learning atHLT-NAACL 2003-Volume 4, p. 40.McNamee, P. & Dang, H. Overview of the TAC 2009Knowledge Base Population Track.
In Proceedingsof Text Analysis Conference (TAC-2009), 2009.Medelyan, O., Witten, I. H. & Milne, D. 2008.
Topicindexing with Wikipedia.
Proceedings of the AAAIWikiAI workshop.Milne, D., Medelyan, O.
& Witten, I. H. 2006.
Min-ing domain-specific thesauri from wikipedia: Acase study.
IEEE/WIC/ACM International Confe-rence on Web Intelligence, pp.
442-448.Minkov, E., Cohen, W. W. & Ng, A. Y.
2006.
Con-textual search and name disambiguation in emailusing graphs, Proceedings of the 29th annual inter-national ACM SIGIR conference on Research anddevelopment in information retrieval, pp.
27-34.Niu C., Li W. and Srihari, R. K. 2004.
Weakly Super-vised Learning for Cross-document Person NameDisambiguation Supported by Information Extrac-tion.
Proceedings of ACL, pp.
598-605.Pedersen, T., Purandare, A.
& Kulkarni, A.
2005.Name discrimination by clustering similar contexts.Computational Linguistics and Intelligent TextProcessing, pp.
226-237.Strube, M. & Ponzetto, S. P. 2006.
WikiRelate!
Com-puting semantic relatedness using Wikipedia, Pro-ceedings of the National Conference on ArtificialIntelligence, vol.
21, no.
2, p. 1419.Suchanek, F. M., Kasneci, G. & Weikum, G. 2007.Yago: a core of semantic knowledge, Proceedingsof the 16th international conference on WorldWide Web, p. 706.Wan, X., Gao, J., Li, M. & Ding, B.
2005.
Personresolution in person search results: Webhawk.
Pro-ceedings of the 14th ACM international conferenceon Information and knowledge management, p.170.Witten, D. M. & Milne, D. 2008.
An effective, low-cost measure of semantic relatedness obtained fromWikipedia links.
Proceeding of AAAI Workshopon Wikipedia and Artificial Intelligence: an Evolv-ing Synergy, AAAI Press, Chicago, USA, pp.
25-30.Yang, K. H., Chiou, K. Y., Lee, H. M. & Ho, J. M.2006.
Web appearance disambiguation of personalnames based on network motif.
Proceedings of the2006 IEEE/WIC/ACM International Conference onWeb Intelligence, pp.
386-389.59
