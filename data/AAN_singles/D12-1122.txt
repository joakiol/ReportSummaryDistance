Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1335?1345, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsExtracting Opinion Expressions withsemi-Markov Conditional Random FieldsBishan YangDepartment of Computer ScienceCornell Universitybishan@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell Universitycardie@cs.cornell.eduAbstractExtracting opinion expressions from text isusually formulated as a token-level sequencelabeling task tackled using Conditional Ran-dom Fields (CRFs).
CRFs, however, do notreadily model potentially useful segment-levelinformation like syntactic constituent struc-ture.
Thus, we propose a semi-CRF-based ap-proach to the task that can perform sequencelabeling at the segment level.
We extend theoriginal semi-CRF model (Sarawagi and Co-hen, 2004) to allow the modeling of arbitrar-ily long expressions while accounting for theirlikely syntactic structure when modeling seg-ment boundaries.
We evaluate performance ontwo opinion extraction tasks, and, in contrastto previous sequence labeling approaches tothe task, explore the usefulness of segment-level syntactic parse features.
Experimentalresults demonstrate that our approach outper-forms state-of-the-art methods for both opin-ion expression tasks.1 IntroductionAccurate opinion expression identification is crucialfor tasks that benefit from fine-grained opinion anal-ysis (Wiebe et al 2005): e.g., it is a first stepin characterizing the sentiment and intensity of theopinion; it provides a textual anchor for identifyingthe opinion holder and the target or topic of an opin-ion; and these, in turn, form the basis of opinion-oriented question answering and opinion summa-rization systems.
In this paper, we focus on opin-ion expressions as defined in Wiebe et al(2005) ?subjective expressions that denote emotions, senti-ment, beliefs, opinions, judgments, or other privatestates (Quirk et al 1985) in text.
These includedirect subjective expressions (DSEs): explicit men-tions of private states or speech events expressingprivate states; and expressive subjective expressions(ESEs): expressions that indicate sentiment, emo-tion, etc.
without explicitly conveying them.
Follow-ing are two example sentences labeled with DSEsand ESEs.
(1) The International Committee of theRed Cross, [as usual][ESE], [has refused tomake any statements][DSE].
(2) The Chief Minister [said][DSE] that [thedemon they have reared will eat up theirown vitals][ESE].As a type of information extraction task, opinionexpression extraction has been successfully tackledin the past via sequence tagging methods: Choi etal.
(2006) and Breck et al(2007), for example, ap-ply conditional random fields (CRFs) (Lafferty etal., 2001) using sophisticated token-level features.In token-level sequence labeling, labels are assignedto single tokens, and the label of each token dependson the current token and the label of the previous to-ken (we consider the usual first-order assumption).Segment-based features ?
features that describe aset of related contiguous tokens, e.g., a phrase orconstituent ?
might provide critical information foridentifying opinion expressions; they cannot, how-ever, be readily and naturally represented in the CRFmodel.1335Our goal in this work is to extract opinion ex-pressions at the segment level with semi-Markovconditional random fields (semi-CRFs).
Semi-CRFs(Sarawagi and Cohen, 2004) are more powerful thanCRFs in that they allow one to construct featuresto capture characteristics of the subsequences of asentence.
They are defined on semi-Markov chainswhere labels are attached to segments instead oftokens and label dependencies are modeled at thesegment-level.
Previous work has shown that semi-CRFs outperform CRFs on named entity recog-nition (NER) tasks (Sarawagi and Cohen, 2004;Okanohara et al 2006).
However, to the best ofour knowledge, semi-CRF techniques have not beeninvestigated for opinion expression extraction.The contribution of this paper is a semi-CRF-based approach for opinion expression extractionthat leverages parsing information to provide bettermodeling of opinion expressions.
Specifically, pos-sible segmentations are generated by taking into ac-count likely syntactic structure during learning andinference.
As a result, arbitrarily long expressionscan be modeled and their boundaries can be influ-enced by probable syntactic structure.
We also ex-plore the impact of syntactic features for extractingopinion expressions.We evaluate our model on two opinion extrac-tion tasks: identifying direct subjective expres-sions (DSEs) and expressive subjective expressions(ESEs).
Experimental results show that our ap-proach outperforms the state-of-the-art approach forthe task by a large margin.
We also identify usefulsyntactic features for the task.2 Related WorkPrevious research to extract direct subjective ex-pressions exists, but is mainly focused on single-word expressions (Wiebe et al 2005; Wilson etal., 2005; Munson et al 2005).
More recent stud-ies tackle opinion expression extraction at the ex-pression level.
Breck et al(2007) formulate theproblem as a token-level sequence labeling prob-lem; their CRF-based approach was shown to sig-nificantly outperform two subjectivity-clue-basedbaselines.
Others extend the token-level approachto jointly identify opinion holders (Choi et al2006), and to determine the polarity and inten-sity of the opinion expressions (Choi and Cardie,2010).
Reranking the output of a simple sequencelabeler has been shown to further improve the ex-traction of opinion expressions (Johansson and Mos-chitti, 2010; Johansson and Moschitti, 2011); impor-tantly, their reranking approach relied on featuresthat encoded syntactic structure.
All of the aboveapproaches, however, are based on token-level se-quence labeling, which ignores potentially usefulphrase-level information.Semi-CRFs (Sarawagi and Cohen, 2004) are gen-eral CRFs that relax the Markovian assumptions toallow sequence labeling at the segment level.
Pre-vious work has shown that semi-CRFs are supe-rior to CRFs for NER and Chinese word segmen-tation (Sarawagi and Cohen, 2004; Okanohara et al2006; Andrew, 2006).
The task of opinion expres-sion extraction is known to be harder than traditionalNER since subjective expressions exhibit substantiallexical variation and their recognition requires moreattention to linguistic structure.Parsing has been leveraged to improve perfor-mance for numerous natural language tasks.
In opin-ion mining, numerous studies have shown that syn-tactic parsing features are very helpful for opinionanalysis.
A lot of work uses syntactic features toidentify opinion holders and opinion topics (Bethardet al 2005; Kim and Hovy, 2006; Kobayashi et al2007; Joshi and Carolyn, 2009; Wu et al 2009;Choi et al 2005).
Jakob et al(2010) recentlyemployed dependency path features for the extrac-tion of opinion targets.
Johansson and Moschitti(2010; Johansson and Moschitti (2011) also success-fully employed syntactic features that indicate de-pendency relations between opinion expressions forthe task of opinion expression extraction.
However,as their approach is based on the output of a se-quence labeler, these features cannot be encoded tohelp the learning of the sequence labeler.3 ApproachWe formulate the extraction of opinion expres-sions as a sequence labeling problem.
Unlikeprevious sequence-labeling approaches to the task(e.g., Breck et al(2007)), however, we aim to modelsegment-level, rather than token-level, information.As a result, we explore the use of semi-CRFs, which1336can assign labels to segments instead of tokens;hence, features can be defined at the segment level.For example, features like JX is a verb phraseK canbe easily encoded in the model.
In the followingsubsections, we first introduce standard semi-CRFsand then describe our semi-CRF-based approach foropinion expression extraction.3.1 Semi-CRFsIn semi-CRFs, each observed sentence x is repre-sented as a sequence of consecutive segments s =?s1, ..., sn?, where si is a triple si = (ti, ui, yi), tidenotes the start position of segment si, ui denotesthe end position, and yi denotes the label of the seg-ment.
Segments are restricted to have positive lengthless than or equal to a maximum length of L that hasbeen seen in the corpus (1 ?
ui ?
ti + 1 ?
L).Features in semi-CRFs are defined at the seg-ment level rather than the word level.
The fea-ture function g(i, x, s) is a function of x, the cur-rent segment si, and the label yi?1 of the previ-ous segment si?1 (we consider the usual first-orderMarkovian assumption).
It can also be written asg(x, ti, ui, yi, yi?1).
The conditional probability ofa segmentation s given a sequence x is defined asp(s|x) =1Z(x)exp{?i?k?kgk(i, x, s)}(1)whereZ(x) =?s?
?Sexp{?i?k?kgk(i, x, s?
)}and the set S contains all possible segmentations ob-tained from segment candidates with length rangingfrom 1 to the maximum length L.The correct segmentation s of a sentenceis defined as a sequence of entity segments(i.e., the entities to be extracted) and non-entity segments.
For example, the correctsegmentation of sentence (2) in Section 1 is?
(The,NONE),(Chief,NONE),(Minister,NONE),(said,DSE),(that,NONE),(the demon they havereared will eat up their own vitals,ESE),(.,NONE)?.Here, non-entity segments are represented asunit-length segments.3.2 Semi-CRF-based Approach for OpinionExpression ExtractionIn this section, we present an extended version ofsemi-CRFs in which we can make use of parsing in-formation in learning entity boundaries and labelsfor opinion expression extraction.As discussed in Section 3.1, the maximum entitylength L is fixed during training to generate segmentcandidates in the standard semi-CRFs.
In opinionexpression extraction, L is unbounded since opin-ion expressions may be clauses or whole sentences,which can be arbitrarily long.
Thus, fixing an upperbound on segment length based on the observed en-tities may lead to an incorrect removal of segmentsduring inference.
Also note that possible segmentcandidates are generated based on the length con-straint, which means any span of the text consistingof no more than L words would be considered asa possible segment.
This would lead to the consid-eration of implausible segments, e.g., ?The Chief?in sentence (2) is an incorrect segment within themulti-word expression ?The Chief Minister?.To address these problems, we propose tech-niques to incorporate parsing information into themodeling of segments in semi-CRFs.
More specifi-cally, we construct segment units from the parse treeof each sentence1, and then build up possible seg-ment candidates based on those units.
In the parsetree, each leaf phrase or leaf word is considered to bea segment unit.
Each segment unit performs as thesmallest unit in the model (words within a segmentunit will be automatically assigned the same label).The segment units are highlighted in rectangles inthe parse tree example in Figure 1.
As the segmentunits are not separable, we avoid implausible seg-ments, which truncate multi-word expressions.
Forexample, ?both ridiculous and?, would not be con-sidered a possible segment in our model.To generate segment candidates for the model,we consider meaningful combinations of consecu-tive segment units.
Intuitively, a sentence is madeup of several parts, and each has its own grammati-cal role or meaning.
We define the boundary of theseparts based on the parse tree structure.
Specifically,1We use the Stanford Parser http://nlp.stanford.edu/software/lex-parser.shtml to generate theparse trees.1337NP VP .VBD SBARSrootthat.IN SNP VPPRPI foundDT NNSthe statementsVBP ADJPare DT JJ JJCCboth riddiculous and oddGoG1 G4G6ridiculous and oddFigure 1: A parse tree example.
There are seven segment units in the sentence.
The shaded regions correspond tosegment groups, where Gi represents the segment group starting from segment unit Ui.we consider each segment unit to belong to a mean-ingful group defined by the span of its parent node.Two consecutive segment units are considered to be-long to the same group if the subtrees rooted in theirparent nodes have the same rightmost child.
For ex-ample, in Figure 1, segment units ?are?
and ?bothridiculous and odd?
belong to the same group, while?I?
and ?found?
belong to different groups.Algorithm 1 Construction of segment candidatesInput: A training sentence xOutput: A set of segment candidates S1: Obtain the segment units U = (U1, ..., Um) bypreorder traversal of the parse tree T , each Uicorresponds to a node in T2: for i = 1 to m do3: j ?
i?
14: while j < m?
1 andcommonGroup(Ui, ..., Uj+1) do5: j ?
j + 16: for k = i to j do7: for t = 0 to j ?
k do8: s?
segment(Uk, ..., Uk+t)9: S ?
S ?
s10: Return SFollowing this idea, we generate possible seg-ment candidates by Algorithm 1.
Starting fromeach segment unit Ui, we first find the rightmostsegment unit Uj that belongs to the same groupas Ui.
Function commonGroup(Ui, ..., Uj) re-turns True if Ui, ..., Uj are within the same group(the parent nodes of Ui,...,Uj have the same right-most child in their subtrees), otherwise it returnsFalse.
Then we enumerate all possible combina-tions of segment units Ui, ..., Uk where i ?
k ?j.
segment(Ui, ..., Uj) denotes the segment ob-tained by concatenating words in the consecutivesegment units Ui,...,Uj .
This way, segment can-didates are generated without constraints on lengthand are meaningful for learning entity boundaries.Based on the generated segment candidates, thecorrect segmentation for each training sentence canbe obtained as follows.
For opinion expressionsthat do not match any segment candidate, we breakthem down into smaller segments using a greedymatching process.
Starting from the start positionof the expression, we search for the longest candi-date that is contained in the expression, add it tothe correct segmentation for the sentence, set thestart position to be the next position, and repeat theprocess.
Using this process, the correct segmen-tation of sentence (2) would be s = ?
(The ChiefMinister,NONE),(said,DSE),(that,NONE),(the de-mon they have reared,ESE), (will eat up their ownvitals,ESE),(.)?.
Note that here non-entities corre-spond to segment units instead of single-word seg-ments in the original semi-CRF model.2After obtaining the set of possible segment candi-dates and the correct segmentation s for each train-ing sentence, the semi-CRF model can be trained.The goal of learning is to find the optimal parameter?
by maximizing log-likelihood.
We use the limited-2There are cases where words within a segment unit havedifferent labels.
This may be due to errors by the human anno-tators or the errors in the parser.
In such cases, we consider eachword within the segment unit as a segment.1338memory BFGS algorithm (Liu and Nocedal, 1989)for optimization in our implementation, where thegradient of the log-likelihood L (corresponding toone instance x) is computed:?L?
?k=?igk(x, ti, ui, yi, yi?1)??s??S?y,y?
?jgk(x, t?j , u?j , y, y?
)p(y, y?|x)(2)where S is all possible segmentations consisting ofthe generated segment candidates, p(y, y?|x) is theprobability of having label y for the current segments?j (with boundary (t?j , u?j)) and label y?
for the pre-vious segment s?j?1.We use a forward-backward algorithm to com-pute the marginal distribution p(y, y?|x) and the nor-malization factor Z(x) efficiently.
For inference weseek the best segmentation s?
= argmaxs p(s|x),where p(s|x) is defined by Equation 1.
We im-plement efficient inference using an extension ofViterbi algorithm to segments.
In particular, defineV (j, y) as the largest unnormalized probability ofp(s1:j |x) with label y at the ending position j. Thenwe haveV (j, y) = max(i,j)?s:,jmaxy??
(x, i, j, y, y?
)V (i?
1, y?)where?
(x, i, j, y, y?)
= exp{?k?kgk(x, i, j, y, y?
)}and s:,j denotes the set of the generated segmentcandidates ending at position j.
The best segmen-tation can be obtained from tracing the path ofmaxy V (n, y).3.3 FeaturesHere we described the features used in our model.Very generally, we include CRF-style features thatare segment-level extensions of the token-level fea-tures.
We also include new segment-level featuresthat can be naturally represented in semi-CRFs butnot CRFs.For CRF-style features, we consider the stringrepresentation of the current word, its part-of-speech, and a dictionary-derived feature, which isbased on a subjectivity lexicon provided by Wilsonet al(2005).
The lexicon consists of a set of wordsthat can act as strong or weak cues to subjectivity.If the current word appears as an entry in the lexi-con, then a feature strong or weak will be fired if theentry is of that strength.
These features have beensuccessfully employed in previous work (Breck etal., 2007).
To employ them in our model, we sim-ply extend the feature definition to the segment level.For example, a token-level feature Jx is great K willbe extended to a segment-level feature Js containsgreat K.Previous work on semi-CRFs has explored fea-tures such as the length of the segment, the positionof the segment in the current segmentation (at the be-ginning or at the end), indicators for the start wordand end word within the segment, and indicators forwords before and after the segment.
These featureshave been shown useful for the task of NE recogni-tion (Sarawagi and Cohen, 2004; Okanohara et al2006).
However, we only found the position of thesegment to be helpful for the extraction of opinionexpressions, probably due to the lack of patterns inthe length distribution and word choices of opinionexpressions.Besides the above features, we design newsegment-level syntactic features to capture the syn-tactic patterns of opinion expressions.
Syntactic pat-terns are often used to identify useful information ininformation extraction tasks.
In our task, we foundthat the majority of opinion expressions involve verbphrases.3 For example, ?was encouraged?, ?ex-pressed goodwill?, ?cannot accept?
are all within aVP constituent.
To capture such structural prefer-ences, we define several syntax-based parse featuresfor VP-related constituents.4Let VPROOT denote a VP constituent whose par-ent node is not VP, and let VPLEAF denote a VPconstituent whose children nodes are non-VP.
De-note the head of VPLEAF as the predicate, and itsnext segment unit as the argument.
If a segment con-sists of words in the VP nodes visited by the preorder3The percentages of opinion expressions involvingVP/NP/PP are 64.13%/18.43%/5.92% for DSEs and43.22%/24.99%/11.77% for ESEs in the data set we used.4We also conducted experiments with NP and PP-relatedfeatures, and could not find any performance improvement forthe tasks.1339traversal from a VPROOT to a VPLEAF, then we re-fer to it as a verb-cluster segment.
If a segment con-sists of a verb cluster and the argument in VPLEAF,we consider it as a VP segment.
The following fea-tures are defined for verb-cluster segments and VPsegments.VPcluster: Indicates whether or not the segmentmatches the verb-cluster structure.VPpred: A feature of the syntactic category andthe word of the head of VPLEAF.
The head ofVPLEAF is the predicate of the verb phrase, whichmay encode some intention of opinions in the verbphrase.
For example, if ?warned?
is the head ofVPLEAF rather than ?informed?, the chance of thesegment being an opinion expression increases.VParg: A feature of the syntactic category andthe head word of the argument in VPLEAF.
For ex-ample, the noun phrase ?a negative stand?
is the ar-gument of the predicate ?take?
in the verb phrase?take a negative stand?.
The argument in the verbphrase (could be a noun phrase, adjectival phrase orprepositional phrase) may convey some relevant in-formation for identifying opinion expressions.VPsubj: Whether the verb clusters or the argu-ment in the segment contains an entry from the sub-jectivity lexicon.
For example, the word ?negative?is in the lexicon, so the segment ?take a negativestand?
has a feature ISVPSUBJ.4 ExperimentsFor evaluation, we use the MPQA 1.2 corpus (Wiebeet al 2005)5, a widely used data set for fine-grainedopinion analysis.
It contains 535 news articles, a to-tal of 11,114 sentences with subjectivity-related an-notations at the phrase level.
We focus on the taskof extracting two types of opinion expressions: di-rect subjective expressions (DSEs) and expressivesubjective expressions (ESEs).
Table 1 shows somestatistics of the corpus.
As in prior research thatuses the corpus, we set aside the standard 135 docu-ments as a development set and use 400 documentsas the evaluation set.
All experiments employ 10-fold cross validation on the evaluation set, and theaverage over all runs is reported.5Available at http://www.cs.pitt.edu/mpqa/.DSEs ESEsSentences with opinions(%) 55.89 57.93TotalNum 9746 11730MaxLength 15 40Length ?
1 (%) 43.38 71.65Length ?
4 (%) 9.44 35.01Table 1: Statistics of opinion expressions in the MPQACorpus.4.1 Evaluation MetricsWe use precision, recall, and F-measure to evalu-ate the quality of the model.
Precision is definedas |C?P ||P | and recall, as|C?P ||C| , where C and P arethe sets of correct and predicted expression spans,respectively.
F-measure is computed as 2PRP+R .
Be-cause the boundaries of opinion expressions are hardto define even for human annotators (Wiebe et al2005), previous research mainly focused on soft pre-cision and recall measures for performance evalu-ation.
Breck et al(2007) introduced an overlapmeasure, which considers a predicted expression tobe correct if it overlaps with a correct expression.We refer to this metric as Binary Overlap.
Johans-son and Moschitti (2010) provides a stricter measurethat computes the proportion of overlapping spans:if a correct expression s overlaps with a predictedexpression s?, the overlap contributes value |s?s?||s?| to|C ?
P | instead of value 1.
We refer to this metricas Proportional Overlap.
To compare with previouswork, we present our results according to both met-rics.4.2 Baseline MethodsAs a baseline, we use the token-level CRF-based ap-proach of Breck et al(2007) applied to the MPQAdataset.
We employ a very similar, but not iden-tical set of features: indicators for specific wordsat the current location and neighboring words in a[?4,+4] window, part-of-speech features, and opin-ion lexicon features for tokens that are contained inthe subjectivity lexicon (see Section 3.3).
We do notinclude WordNet, Levin?s verb categorization, andFrameNet features.We also include two variants of standard CRFs asbaselines: segment-CRF and syntactic-CRF.
Theyincorporate segmentation information into standardCRFs without modifying the Markovian assump-1340DSE Extraction ESE ExtractionMethod Precision Recall F-measure Precision Recall F-measureCRF 82.83 49.38 61.87 78.56 43.57 56.05segment-CRF 82.52 51.48 63.41 78.90 44.46 56.88syntactic-CRF 82.48 49.09 61.55 78.41 43.39 55.95semi-CRF 66.67 74.13 70.20 71.21 57.41 63.57new-semi-CRF 67.72??
74.33 70.87?
73.57???
57.63 64.74?
?semi-CRF(w/ syn) 64.86 74.10 69.17 70.68 56.61 62.87new-semi-CRF(w/ syn) 70.12???
74.74?
72.36???
73.61???
59.27???
65.67??
?Table 2: Results for extracting opinion expressions with Binary-Overlap metric.
(w/ syn) indicates the inclusion ofsyntactic parse features VPpre, VParg and VPsubj.
Results of new-semi-CRF that are statistically significantly greaterthan semi-CRF according to a two-tailed t-test are indicated with ?
(p < 0.1), ??
(p < 0.05), ???
(p < 0.005).
T-testresults are also shown for new-semi-CRF(w/ syn) versus semi-CRF(w/ syn).DSE Extraction ESE ExtractionMethod Precision Recall F-measure Precision Recall F-measureCRF 77.91 46.45 58.20 67.72 37.55 48.31segment-CRF 77.86 48.58 59.83 68.03 38.34 49.04syntactic-CRF 77.73 46.27 58.01 67.80 37.60 48.37semi-CRF 60.38 68.34 64.11 57.30 46.20 51.16new-semi-CRF 62.50??
68.59?
65.41?
61.69???
47.44??
53.63??
?semi-CRF(w/ syn) 58.69 67.80 62.92 57.09 45.63 50.72new-semi-CRF(w/ syn) 65.52???
68.91???
67.17???
61.66???
48.77???
54.47??
?Table 3: Results for extracting opinion expressions with Proportional-Overlap metric.
Notation is the same as above.tion.
Segment-CRF treats segment units obtainedfrom the parser as word tokens.
For example, inFigure 1, the segment units the statement and bothridiculous and odd will be treated as word tokens.Syntactic-CRF encodes segment-level syntactic in-formation in a standard token-level CRF as inputfeatures.
We consider the VP-related segment fea-tures introduced in Section 3.3.
VPPRE and VPARGare added to the head word of the corresponding verbphrase, and VPSUBJ and VPCLUSTER are added toeach token within the corresponding segment.Another baseline method is the original semi-CRF model (Sarawagi and Cohen, 2004).
To thebest of our knowledge, our work is the first to ex-plore the use of semi-CRFs on the extraction ofopinion expressions.
They are considered to be morepowerful than CRFs since they allow information tobe represented at the expression level.
The modelrequires an input of the maximum entity length.
Weset it to 15 for DSE and 40 for ESE.
For segment fea-tures, we used the same features as in our approach(see Section 3.3).4.3 ResultsTable 2 and Table 3 show the results of DSE andESE extraction using two different metrics.
Thestandard token-based CRF baseline of Breck et al(2007) is labeled CRF; the original semi-CRF base-line is labeled semi-CRF; and our extended semi-CRF approach is labeled new-semi-CRF.
For semi-CRF and new-semi-CRF, the results were obtainedusing two different settings of features: the basicfeature set includes features described in Section 3.3excluding the segment-level syntactic features.
Inthe second feature setting (labeled as w/ syn in thetables), we further augment the basic features withthe syntactic parse features.Using the basic features, we observe thatsemi-CRF-based approaches significantly outper-form CRF and its two variants segment-CRF andsyntactic-CRF in F-Measure on both DSE and ESEextraction, and new-semi-CRF achieves the best re-sults.
By simply incorporating the segmentationprior into the standard CRF, segment-CRF achievesa slight improvement over standard CRF, but theresults are still worse than those of semi-CRFand new-semi-CRF.
However, adding segment-level1341DSE Extraction ESE ExtractionFeature set Precision Recall F-measure Precision Recall F-measureBasic 67.72 74.33 70.87 73.57 57.63 64.74Basic+VPpre 70.88 71.44 71.16 73.20 58.20 64.85Basic+VParg 70.12 74.03 72.02 73.05 58.20 64.79Basic+VPcluster 70.08 72.94 71.48 73.06 58.45 64.94Basic+VPsubj 70.04 72.34 71.17 73.31 58.53 65.09Basic+VPpre+VPsubj 70.91 72.54 71.72 73.61 58.29 65.07Basic+VParg+VPsubj 70.45 73.53 71.96 74.45 57.80 65.07Basic+VPpre+VParg+VPsubj 70.12 74.74 72.36 73.61 59.27 65.67Basic+VPcluster+VPpre+VParg+VPsubj 70.91 72.54 71.72 72.84 58.45 64.86Table 4: Effect of syntactic features on extracting opinion expressions with Binary-Overlap metricsyntactic features into standard CRF yields slightlyreduced performance.
This is not surprising as en-coding segment-level information into the token-level CRF is not natural.
These experiments in-dicate that simply encoding segmentation informa-tion into standard CRF cannot result in large per-formance gains.
The promising F-measure resultsobtained by semi-CRF and new-semi-CRF confirmthat relaxing the Markovian assumption on segmentsleads to better modeling of opinion expressions.
Wecan also see that new-semi-CRF consistently outper-forms the original semi-CRF model.
This furtherconfirms the benefit of taking into account syntacticparsing information in modeling segments.
In Ta-ble 3, we observe the same general results trend asin Table 2.
The scores are generally lower since themetric Proportional Overlap is stricter than BinaryOverlap.We also study the impact of syntactic parse fea-tures on the semi-Markov CRF models.
Here weconsider the combination of VPPRE, VPARG andVPSUBJ since they turned out to be the most help-ful features for our tasks.
Interestingly, we foundthat after incorporating the syntactic parse features,performance decreases on semi-CRF.
This indicatesthat syntactic information does not help if learningand inference take place on segment candidates gen-erated without accounting for parse information.
Incontrast, our approach incorporates syntactic pars-ing information in modeling segments and meaning-ful segmentations.
We can see in Tables 2 and 3that adding syntactic features successfully boosts theperformance of our approach.To further explore the effect of the syntactic fea-tures, we include the results of our model with dif-ferent configurations of syntactic features in Table 4(here we focus on the Binary Overlap metric asthe results with Proportional Overlap demonstratea similar conclusion).
We can see that using the ba-sic features and the combination of VPPRE, VPARGand VPSUBJ yields the best results for both DSEand ESE extraction.
For DSE extraction, combin-ing these three features improves the precision no-ticeably from 67.72% to 70.12% while the recallslightly improves.
This indicates that VP-relatedstructural information is very helpful for modelingsegments as DSEs.
However, this trend is not soclear for ESE extraction.
This may be due to the factthat DSEs often involve verb phrases while ESEs arerepresented via a variety of syntactic structures.Comparison with previous work.
In Table 5, wecompare our results to the previous work on opinionexpression extraction (here we also focus on the Bi-nary Overlap metric due to the similar trend demon-strated by the Proportional Overlap metric).
Brecket al(2007) presents the state-of-the-art sequencelabeling approach on the tasks of DSE and ESE ex-traction.
Their best results are shown as Breck etal.
Baseline in the table.
Johansson and Mos-chitti (2010) use a reranking technique on the bestk outputs of a sequence labeler to further improvetheir sequence labeling results on the task of ex-tracting DSEs, ESEs and OSEs (Objective SpeechEvents) (we don?t consider OSEs here).
Resultsusing our re-implementation of their approach us-ing SVM struct (Tsochantaridis et al 2004) on theoutput of CRF are labeled CRF+Reranking Base-line in the table.
We use the same features and1342parameter settings as in their approach.
Our ap-proach+Reranking are results obtained by apply-ing the reranking step on the output of our new-semi-CRF approach.We can see that our approach outperforms theBreck et alBaseline on both DSE extraction andESE extraction in spite of the fact that we do notuse their WordNet, Levin?s verb categorization, andFrameNet features.
The CRF+Reranking Baselinedoes provide a performance increase over the thebaseline CRF results, but overall it cannot beat theother methods since the CRF baseline is very low.As one might expect, reranking also succeeds inboosting the performance of new-semi-CRF, achiev-ing the best performance on F-measure for both DSEand ESE extraction.
Note that the interannotatoragreement results for these two tasks are 75% forDSE and 72% for ESE using a similar metric to Bi-nary Overlap.
Our results are much closer to theseinterannotator scores than previous systems espe-cially for DSEs.Task Method F-measureDSE ExtractionBreck et alBaseline 70.65CRF+Reranking Baseline 63.87Our approach 72.36Our approach+Reranking 73.12ESE ExtractionBreck et alBaseline 63.43CRF+Reranking Baseline 58.21Our approach 65.67Our approach+Reranking 67.01Table 5: Comparison of our work with previous work onopinion expression extraction using the Binary-Overlapmetric4.4 DiscussionWe note that our new-semi-CRF approach outper-forms the original semi-CRF w.r.t.
both precisionand recall, but compared to CRF, our approachyields a clear improvement on recall but not on pre-cision.
An error analysis helps explain why.
Wefound that our semi-CRF approach predicted almostthe same number of DSEs as the gold standard la-bels while CRF only predicted half of them (for ESEextraction, the trend is similar).
With more pre-dicted entities, the precision is sacrificed but recall isboosted substantially, and overall we see an increasein F-measure.Looking further into the errors, we found sev-eral mistakes that could potentially be fixed to yieldbetter a precision score.
Some errors were due tothe false prediction of speech events like ?said?
or?told?
as DSEs in cases where they actually just in-troduced statements of fact without expressing anyprivate state.
Adding features to distinguish suchcases should help improve performance.
Other er-rors were due to inadequate modeling of the contextsurrounding the expressions.
For example, ?enjoy arelative advantage?
was falsely predicted as an ESE.If incorporating information about the subject of thisverb phrase which is ?products?, this mistake couldbe avoided since ?products?
cannot hold or expressprivate state.
We also noticed some errors causedby inaccurate parsing and hope to study ways to ac-count for these in our approach as future work.By comparing the extraction results across differ-ent methods, we see that full parsing provides manybenefits for modeling segment boundaries and im-proving the prediction precision for opinion expres-sion extraction.
For example, given the sentence, ?...who are living [a lot better][ESE] ...?, both CRF andthe original semi-CRF extract ?lot better?
as an ESE,while our approach correctly extracts ?a lot better?as an ESE.
And we also found many cases wherethe original semi-CRF cannot extract the opinion ex-pressions while our approach can.
Another benefitof utilizing parsing is to speed up learning and infer-ence.
Although in theory, the computational cost ofparsing is O(g ?
n3) where g is the grammar sizeand n is the sentence length while the cost of semi-CRFs is O(K2 ?
L?
n) where K is the number oflabels and L is the maximum entity length, featureextraction overhead and the potentially large num-ber of learning iterations in parameter optimizationmay lead to a long training time for semi-CRFs.
Inour experiments on the MPQA data set, our machinewith Intel Core 2 Duo CPU and 4GB RAM took 2hours to fully parse 11,114 sentences using the Stan-ford Parser, and also 2 hours to train the standardsemi-CRF.
With the parsing information, our semi-CRF-based approach is able to finish training in 15minutes.
As full parsing would be expensive whenthe average sentence length is very large, it would beinteresting to study how to utilize parsing with lesscost in our task.13435 ConclusionIn this paper we propose a semi-CRF-based ap-proach for extracting opinion expressions that takesinto account during learning and inference the struc-tural information available from syntactic parsing.Our approach allows opinion expressions to be iden-tified at the segment level and their boundaries tobe influenced by their probable syntactic structure.Experimental evaluations show that our model out-performs the best existing approaches on two opin-ion extraction tasks.
In addition, we identify usefulsyntactic parse features for these tasks that have notbeen explored in previous work.
Our error analysisindicates that adding additional features that accountfor subjectivity cues in the local context might fur-ther improve the performance.
In future work, wehope to explore better ways of utilizing parsing in-formation with less cost.
Also, we will apply ourmodel to additional opinion analysis tasks such asfine-grained opinion summarization and relation ex-traction.6 AcknowledgementThis work was supported in part by National ScienceFoundation Grants IIS-1111176 and IIS-0968450,and by a gift from Google.
We thank Nikos Karam-patziakis, Igor Labutov, Veselin Stoyanov, AinurYessenalina and Jason Yosinski for their helpfulcomments.ReferencesGalen Andrew.
2006.
A hybrid Markov/semi-Markovconditional random field for sequence segmentation.In Proceedings of EMNLP ?06.Steven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2005.
Extractingopinion propositions and opinion holders using syn-tactic and lexical cues.
In Shanahan, James G., YanQu, and Janyce Wiebe, editors, Computing Attitudeand Affect in Text: Theory and Applications.Eric Breck, Yejin Choi, and Claire Cardie.
2007.
Identi-fying expressions of opinion in context.
IJCAI?07.Yejin Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying sources of opinionswith conditional random fields and extraction patterns.In Proceedings of HLT ?05.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Jointextraction of entities and relations for opinion recog-nition.
In Proceedings of EMNLP ?06.Yejin Choi and Claire Cardie.
2010.
Hierarchical se-quential learning for extracting opinions and their at-tributes.
In Proceedings of ACL 2010, Short Papers.Richard Johansson and Alessandro Moschitti.
2010.
Syn-tactic and semantic structure for opinion expressiondetection.
In Proceedings of CoNLL ?10.Niklas Jakob and Iryna Gurevych.
Extracting opinion tar-gets in a single- and cross-domain setting with condi-tional random fields.
In Proceedings of EMNLP?
10.Mahesh Joshi and Penstein-Ros?e Carolyn.
2009.
Gen-eralizing dependency features for opinion mining.In Proceedings of ACL/IJCNLP 2009, Short PapersTrack.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL ?03.Soo-Min Kim and Eduard Hovy.
2006.
Extracting opin-ions, opinion holders, and topics expressed in onlinenews media text.
In Proceedings of the ACL Workshopon Sentiment and Subjectivity in Text.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.2007.
Extracting aspect-evaluation and aspect-of rela-tions in opinion mining.
In Proceedings of EMNLP-CoNLL-2007.John D. Lafferty, Andrew McCallum, and Fernando C.N.
Pereira.
2001.
Conditional Random Fields: Proba-bilistic Models for Segmenting and Labeling SequenceData.
In Proceedings of ICML ?01.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming B 45(3): 503-528.M Arthur Munson, Claire Cardie, and Rich Caruana.
Op-timizing to arbitrary NLP metrics using ensemble se-lection.
In HLT-EMNLP05, 2005.Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-ruoka, and Jun?ichi Tsujii.
Improving the scalabilityof semi-Markov conditional random fields for namedentity recognition.
In Proceedings of ACL?06.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
A comprehensive grammar of theEnglish language.
New York: Longman, 1985.Richard Johansson and Alessandro Moschitti.
Extract-ing Opinion Expressions and Their Polarities - Explo-ration of Pipelines and Joint Models.
In Proceedingsof ACL ?11, Short Paper.Ellen Riloff and Janyce M Wiebe.
Learning extractionpatterns for subjective expressions.
In Proceedings ofEMNLP 2003.Sunita Sarawagi and William W. Cohen.
2004.
Semi-Markov Conditional Random Fields for InformationExtraction.
In Proceedings of NIPS 2004.1344Charles Sutton and Andrew McCallum.
An Introduc-tion to Conditional Random Fields.
Foundations andTrends in Machine Learning (FnT ML), 2010.Janyce Wiebe, Theresa Wilson , and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation, vol-ume 39, issue 2-3, pp.
165-210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of HLT ?05.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi, ClaireCardie, Ellen Riloff, and Siddharth Patwardhan.
Opin-ionFinder: A system for subjectivity analysis.
EMNLP2005.
Demo abstract.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.Phrase dependency parsing for opinion mining.
In Pro-ceedings of EMNLP 2009.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemi Altun.
Support Vector Learn-ing for Interdependent and Structured Output Spaces.In Proceedings of ICML 2004.1345
