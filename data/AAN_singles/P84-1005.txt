A STOCHASTIC APPROACH TO SENTENCE PARSINGTetsunosuke FuJisakiScience Institute, IBM Japan, Ltd.No.
36 Kowa Building5-19 Sanbancho,Chiyoda-kuTokyo 102, JapanABSTRACTA description will be given of a procedure to asslgnthe most likely probabilitles to each of the rulesof a given context-free grammar.
The grammar devel-oped by S. Kuno at Harvard University was picked asthe basis and was successfully augmented with ruleprobabilities.
A brief exposition of the methodwith some preliminary results, whenused as a devicefor disamblguatingparsing English texts picked fromnatural corpus, will be given.Z.
INTRODUCTIONTo prepare a grammar which can parse arbitrary sen-tances taken from a natural corpus is a difficulttask.
One of the most serious problems is the poten-tlally unbounded number of ambiguities.
Pure syn-tactic analysis with an imprudent grammar willsometimes result in hundreds of parses.With prepositional phrase attachments and conjunc-tions, for example, it is known that the actualgrowth of ambiguities can be approximated by a Cat-fan number \[Knuth\], the number of ways to insertparentheses into a formula of M terms: 1, 2, 5, 14,42, 132, 469, 1430, 4892, ...
The five ambiguitiesin the following sentence with three ambiguous con-structions can be well explained wlth this number.\[ I saw a man in a park with a scope.
\[I !This Catalan number is essentially exponentlal and\[Martin\] reported a syntactically amblguous sentencewith 455 parses:List the sales of products produced in 1973 IIwith the products produced in 1972.
IOn the other hand, throughout the long history ofnatural language understanding work, semantic andpragmatic constraints are known to be indispensableand are recommended to be represented in some formalway and to be referred to during or after the syntac-tic analysis process.However, to represent semantic and pragmatic con-straints, (which are usually domain sensitive) in awell-formed way is a very difficult and expensivetask.
A lot of effort in that direction has beenexpended, especially in Artificial Intelligence,using semantic networks, frame theory, etc.
Howev-er, to our knowledge no one has ever succeeded inpreparing them except in relatlvely small restricteddomains.
\[Winograd, Sibuya\].Faced with this situation, we propose in this paperto use statistics as a device for reducing ambigui-ties.
In other words, we propose a scheme for gram-matical inference as defined by \[Fu\], a stochasticaugmentatlon of a given grammar; furthermore, wepropose to use the resultant statistics as a devicefor semantic and pragmatic constraints.
Wlthin thisstochastic framework, semantic and pragmatic con-straints are expected to be coded implicitly in thestatistics.
A simple bottom-up parse referring tothe grammar rules as well as the statistics willassign relative probabilities among ambiguous deri-vations.
And these relative probabilities should beuseful for filtering meaningless garbage parsesbecause high probabilities will be asslgned to theparse trees corresponding to meaningful interpreta-tions and iow probabilities, hopefully 0.0, to otherparse trees which are grammatlcally correct but arenot meaningful.Most importantly, stochastic augmentation of a gram-mar will be done automatically by feeding a set ofsentences as samples from the relevant domain inwhich we are interested, while the preparation ofsemantic and pragmatic constraints in the form ofusual semantic network, for example, should be doneby human experts for each specific domain.This paper first introduces the basic ideas of auto-matic training process of statistics from givenexample sentences, and then shows how it works witexperimental results.I I .
GRAMMATICAL INFERENCE OF A STOCHASTIC GRAMMARA.
Estimation of Markov Parameters for sample textsAssume a Markov source model as a collectlon ofstates connected to one another by transitions whichproduce symbols from a finite alphabet.
To eachtransition, t from a state s, is associated a proba-bility q(s , t ) ,  which is the probability that t willbe chosen next when s is reached.When output sentences \ [B( i )}  from this markov modelare observed, we can estimate the transition proba-bilities {q(s , t )}  through an iteration process inthe following way:i.
Make an initial guess of {q(s,t\]}.162.
Parse each output sentence B(1).
Let d(i,j) bea j-th derivation of the i-th output sentenceB( i \ ] .3.4.Then the probability p|d( i , J}} of each deriva-tion d{i , J \ ]  can be defined in the following way:p{d| i , j}}  is the product of probability of allthe transitions q{s,~) which contribute to thatderivation d(~,~) .From this p(d(i ,~}), the Bayes a posterloriestimate of the count c{s , t , i , j ) ,  how many timesthe transition t from state $ is used on the der-ivation d\[i,J}, can be estimated as follows:5.n (s , t , i , j )  x p (d( i , j ) )c(s,t,i,j) =~-p(d( i , j ) )Jwhere n{s,t,i,~} is a number of times the tran-sition t from state s is used in the  derivationd{i,j}.Obviously, c{s,t,i,~} becomes nfs,t,i,J} in anunambiguous case.From this ={a,t,l,j}, new estimate of the proba-billties @{$,t} can be calculated.~-~ c(s,t,i,j)?
jf(s,t) =Y- Y- Y-c(s,t,?,j)i j t6.
Replace {qfs, t}} with this new estimate {@{s,t}}and repeat from step 2.Through this process, asymptotic convergence willhold in the entropy of {q{$,t\]} which is defined as:Zntoropy  = ~- ~ -q (s , t )x log(q(s , t ) )s tand the {q(s,t))  will approach the real transitionprobability \[Baum-1970~1792\].Further optimized versions of this algorlthm can befound in \[Bahl-1983\] and have been successfully usedfor estimating parameters of various Markov modelswhich approximate speech processes \[Bahl - 1978,1980\].B.
Extension to context-free grammar"This procedure for automatically estimating Markovsource parameters can easily be extended to con-text-free grammars in the following manner.Assume that each state in the Markov model corre-sponds to a possible sentential form based on a giv-en context-free grammar.
Then each transitioncorresponds to the application of a context-freeproduction rule to the previous state, i.e.
previ-ous sentential form.
For example, the state NP.
VPcan be reached from the state S by applying a ruleS->NP VP, the state ART.
NOUN.
VP can be reached fromthe state NP.
VP by applying the rule NP->ART NOUN tothe first NP of the state NP.
VP, and so on.Since the derivations correspond to sequences ofstate transitions among the states defined above,parsin E over the set of sentences given as trainingdata will enable us to count how many times eachtransition is fired from the given sample sentences.For example, transitions from the state S to thestate NP.
VP may occur for almost every sentencebecause the correspondin E rule, 'S->NP VP', must beused to derive the most frequent declarative sen-tences; the transition from state ART.
NOUN.
VP to thestats 'every'.NOUN.
VP may happen 103 times; etc.
Ifwe associate each grammar rule with an a prioriprobabillty as an initial guess, then the Bayes aposteriorl estimate of the number of times eachtransition will be traversed can be calculated fromthe initial probabilities and the actual countsobserved as described above.Since each production is expected to occur independ-ently of the context, the new estimate of the proba-billty for a rule will be calculated at eachiteration step by masking the contexts.
That is,the Bayes estimate counts from all of the transi-tions which correspond to a single context freerule; all transitions between states llke xxx.
A. yyyand xxx.
B.C.
yyy correspond to the production rule'A->B C' regardless of the contents of xxx and yyy;are tied together to  get the new probability esti-mate of the corresponding rule.Renewing the  probabilities of the rules with newestimates, the same steps will be repeated untilthey converge.ZZZ.
EXPERIHENTATZONA.
Base GrammarAs the basis of this research, the grammar developedby Prof. S. Kuno in the 1960's for the machine trans-lation project at Harvard University \[Ktmo-1963,1966\] was chosen, with few modifications.
The setof grammar specifications in that grammar, wh lcharein Greibach normal form, were translated into a formwhich is favorable to our method.
2118 rules of theoriginal rules were rewrlttenas 5241 rules in Chom-sky normal form.B.
ParserA bottom-up context - f ree  parser  based on Cocke-Kasa-mi-Yotmg a lgor i thm was developed espec ia l l y  fo r  th i spurpose.
Spec ia l  emphasis  was put  on the  des ign  ofthe parser to get better performance in highlyambiguous cases.
That is, alternative-links, thedotted llnk shown in the figure below, are intro-duced to reduce the number of intermediate substruc-ture as far as possible.A/P17C.
Test CorpusTraining sentences were selected from the magazines,31 articles from Reader's Digest and Datamation, andfrom IBM correspondence.
Among 5528 selected sen-tences from the magazine articles, 3582 sentenceswere successfully parsed with 0.89 seconds of CPUtime ( IBM 3033-UP ) and with 48.5 ambiguities per asentence.
The average word lengths were 10.85 wordsfrom this corpus.From the corpus of IBM cor respondence ,  1001 sen-tences, 12.65 words in length in average, were cho-sen end 624 sentences were successfully parsed with--average of 13.5 ambiguities.D.
Resultant Stochastic Context-free GrammarAfter a certain number of iterations, probabilitieswere successfully associated to all of the grammarrules and the lexlcal rules as shown below:* IT40.98788 HELP0.00931 SEE0.00141 HEAR0.00139 WATCH0.00000 HAVE0.00000 FEEL---(a)---(b)* SE0.28754 PRN VX PD ---(c)0.25530 AAA 4XVX PD ---(d)0.14856 NNNVX PD0.13567 AV1 SE0.04006 PRE NQ SE0.02693 AV4 IX MX PD0.01714 NUM 4XVXPD0.01319 IT1 N2 PD*VE0.16295 VT1 N20.14372 VIl0.11963 AUX BV0.10174 PRE NQ VX0.09460 8E3 PAIn the above llst, (a) means that "HELP" will be gen-erated from part-of-speech "IT4" with the probabili-ty 0.98788, and (b) means that "SEE" will begenerated from part-of-speech "IT4" with the proba-bility 0.00931.
(c) means that the non-terminal "SE(sentence)" will generate the sequence, "PRN (pro-noun)", "VX (predicate)" and "PD (period or postsententlal modifiers followed by period)" with theprobability 0.28754.
(d) means that "SE" will gener-ate the sequence, "AAA(artlcle, adjective, etc.)"
,"4X (subject noun phrase)", "VX" and "PD" with theprobability 0.25530.
The remaining lines are to beinterpreted similarly.E.
Parse Trees with ProbabilitiesParse trees were printed as shown below includingrelative probabilities of each parse.WE DO NOT UTILIZE OUTSIDE ART SERVICES DIRECTLY .
** total ambiguity is : 3*: SENTENCE*: PRONOUN 'we'*: PREDICATE*: AUXILIARY 'do'*: INFINITE VERB PHRASE* ADVERB TYPE1 'not'A: 0.356 INFINITE VERB PHRASEI*: VERB TYPE ITl'utilize'\[*: OBJECT\[ *: NOUN 'outside'\] *: ADJ CLAUSE\[ *: NOUN 'art'\[ *: PRED.
WITH NO OBJECT\[ *: VERB TYPE VT1 'services'B: 0.003 INFINITE VERB PHRASE\[*: VERB TYPE ITl'utillze'\[*: OBJECTI *: PREPOSITION 'outside'\[ *: NOUN OBJECT\[ *: NOUN ' art '\[ *: OBJECT\[ *: NOUN 'services'C: 0.
641 INFINITE VERB PHRASE\[*: VERB TYPE ITl'utilize'\[*: OBJECT\] *: .
NOUN 'outs ide '\[ *: OBJECT MASTER\[ *: NOUN ' ar t '\[ *: OBJECT MASTER\] * NOUN 'services'*: PERIOD*: ADVERB TYPE1 'directly'*: PRD w !This example shows that the sentence 'We do not uti-lize outside art services directly.'
was parsed inthree different ways.
The differences are shown asthe difference of the sub-trees identified by A, Band C in the figure.The numbers following the identifiers are the rela-tive probabilities.
As shown in this case, the cor-rect parse, the third one, got the highest relatlveprobability, as was expected.F.
Result63 ambiguous sentences from magazine corpus and 21ambiguous sentences from IBM correspondence werechosen at random from the sample sentences and theirparse trees with probabilities were manually exam-ined as shown in the table below:18a?b.C.d.e.f.Corpus Magazine63 Number of sentenceschecked manuallyNumber of sentences 4with no correct parse I~umber of sentences 54which got highest prob.on most natural parseNumber of sentences 5which did not get thehighest prob.
on themost natural parseSuccess ratio d/(d+e) .915IBM2118?
947Taking into consideration that the grammar is nottailored for this experiment in any way, the resultis quite satisfactory.The only erroneous case of the IBM corpus is due to agrammar problem.
That is, in this grammar, suchmodifier phrases as TO-infinltives, prepositionalphrases, adverbials, etc.
after the main verb willbe derived from the 'end marker' of the sentence,i.e.
period, rather then from the relevant constitu-ent being modified.
The parse tree in the previousfigure is a typical example, that is, the adverb'DIRECTLY' is derived from the 'PERIOD' rather thenfrom the verb 'UTILIZE '.
This simplified handlingof dependencies will not keep information betweenmodifying and modified phrases end as a result, willcause problems where the dependencies have crucialroles in the analysis.
This error occurred in a sen-tenoe ' ... is going ~o work out', where the twointerpretations for the phrase '%o work' exist:'~0 work' modifies 'period' as:1.
A TO-infinitlve phrase2.
A prepositional phraseIgnoring the relationship to the previous context'Is going', the second interpretation got the higherprobability because prepositionalphrases occur morefrequently then TO-infinltivephrases if the contextis not taken into account.IV .
CONCLUSIONThe result from the trials suggests the strongpotential of this method.
And this also suggestssome application possibility of this method such as:refining, minimizing, and optimizing a given con-text-free grammar.
It will be also useful for giv-ing a dlsamblguation capability to a given ambiguouscontext-free grammar.In this experiment, an existing grammar was pickedwith few modlflcatlons, therefore, only statisticsdue to the syntactic differences' of the sub-strut-tured units were gathered.
Applying this method tothe collection of statistics which relate more tosementlcs should be investigated as the next step ofthis project?
Introduction into the grammar of adependency relationship among sub-structured units,semantically categorized parts-of-speech, head wordinheritance among sub-structured units, etc.
mightbe essential for this purpose.
More investigationshould be done on this direction.V.
ACKNOWLEDGEMENTSThis work was carried out when the author was in theComputer Science Department of the IBM Thomas J.Watson Research Center.The author would llke to thank Dr. John Cocke, Dr. F.Jelinek, Dr. B. Mercer, Dr. L. Bahl of the IBM ThomasJ?
Watson Research Center, end Prof. S. Kuno, ofHarvard University for their encouragement and valu-able technical suggestions.Also the author is indebted to Mr. E. Black, Mr. B.Green end Mr. J Lutz for their assistance end dis-cussions.VIZ.
REFERENCES?
Bahl,L.
,Jelinek,F.
, end Mercer,R.
,A MaximumLikelihood Approarch to Continuous Speech Recog-nition,Vol.
PAMI-5,No.
2, IEEE Trans.
PatternAnalysis end Machine Intelligence,1983?
Bahl,L.
,et.
al.
,Automatic Recognition of Contin-uously Spoken Sentences from a finite stategrammar, Pron.
IEEE Int.
Conf.
Acoust., Speech,Signal Processing, Tulsa, OK, Apr.
1978?
Bahl,L.
,et.
al.
,Further results on the recogni-tion of a continuousl read natural corpus, Pron.IEEE Int.
Conf.
Acoust., Speech,Signal Process-ing, Denver, CO,Apr.
1980?
Baum,L.E.
,A Maximazatlon Technique occurring inthe Statistical Analysis of Probablistlc Func-tlons of Markov Chains, Vol.
41, No.l, TheAnnals of Mathematical Statistlcs, 1970?
Baum,L.E.
,An Inequality and Associated Maximi-zation Technique in Statistical Estimation forProbabllstlc Functions of Markov Processes, Ine-qualities, Vol.
3, Academic Press, 1972?
Fu,K.S.
,Syntactic Methods in Pattern Recogni-tion,Vol 112, Mathematics in science end Engi-neering, Academic Press, 1974?
Knuth,D.
,Fundamental Algorlthms,Vol I. in TheArt of Computer Programming, Addison Wesley,1975?
Kuno,S.
,The Augmented Predictive Analyzer forContext-free Languages-Its Relative Efficiency,Vol.
9, No.
11, CACM, 1966?
Kttno,S.
,Oettinger,A.
G. ,Syntactic Structure andAmbiguity of English, Pron.
FJCC, AFIPS, 1963?
Martln,W.
, at.
al.
,Preliminary Analysis of aBreadth-First Parsing Algorithm: Theoretical andExperimental Results, MIT LCS report TR-261, MIT1981?
Sibuya,M.
,FuJlsakl,T.
end Takao,Y.
,Noun-PhraseModel end Natural Query Language, Vol 22, No5,IBM J. Res.
Dev.
1978?
Winograd,T.
,Understanding Natural Language,Academic Press, 1972?
Woods ,W. ,The Lunar Sciences Natural LanguageInformation System, BBN Report No.
2378, Bolt,Berenek end Newman19
