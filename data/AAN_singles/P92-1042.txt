DOCUMENTATION PARSER TO EXTRACT SOFTWARE TEST CONDITIONSPatricia LutskyBrandeis UniversityDigital Equipment Corporation111 Locke Drive LMO2-1/LllMarlboro, MA 01752OVERVIEWThis project concerns building a documentparser that can be used as a software ngineer-ing tool.
A software tester's task frequentlyinvolves comparing the behavior of a runningsystem with a document describing the behav-ior of the system.
If a problem is found, it mayindicate an update is required to the document,the software system, or both.
A tool to generatetests automatically based on documents wouldbe very useful to software ngineers, but it re-quires a document parser which can identifyand extract estable conditions in the text.This tool would also be useful in reverse n-gineering, or taking existing artifacts of a soft-ware system and using them to write the spec-ification of the system.
Most reverse ngineer-ing tools work only on source code.
However,many systems are described by documents hatcontain valuable information for reverse ngi-neering.
Building a document parser would al-low this information to be harvested as well.Documents describing a large software project(i.e.
user manuals, database dictionaries) areoften semi-formatted text in that they havefixed-format sections and free text sections.The benefits of parsing the fixed-format por-tions have been seen in the CARPER project(Schlimmer, 1991), where information found inthe fixed-format sections of the documents de-scribing the system under test is used to ini-tialize a test system automatically.
The cur-rent project looks at the free text descriptionsto see what useful information can be extractedfrom them.PARSING A DATABASE DICTIONARYThe current focus of this project is on ex-tracting database related testcases from thedatabase dictionary of the XCON/XSEL con-figuration system (XCS) (Barker & O'Connor,2941989).
The CARPER project is aimed at build-ing a self-maintaining database checker for theXCS database.
As part of its processing, it ex-tracts basic information contained in the fixed-format sections of the database dictionary.This project looks at what additional testinginformation can be retrieved from the databasedictionary.
In particular, each attribute de-scription contains a "sanity checks" sectionwhich includes information relevant for test-ing the attribute, such as the format and al-lowable values of the attribute, or informationabout attributes which must or must not beused together.
If this information is extractedusing a text parser, either it will verify the ac-curacy of CARPER's checks, or it will augmentthem.The database checks generated from a docu-ment parser will reflect changes made to thedatabase dictionary automatically.
This willbe particularly useful when new attributes areadded and when changes are made to attributedescriptions.
(Lutsky, 1989) investigated the parsing ofmanuals for system routines to extract themaximum allowed length of the characterstring parameters.
Database dictionary pars-ing represents a new software domain as wellas a more complex type of testable information.SYSTEM ARCHITECTUREThe overall structure of the system is givenin Figure 1.
The input to the parser is a setof system documents and the output is testcaseinformation.
The parser has two main domain-independent components, one a testing knowl-edge module and one a general purpose parser.It also has two domain-specific components: adomain model and a sublanguage grammar ofexpressions for representing testable informa-tion in the domain.Figure 1Document Parser SystemXCS database dictionary which concern thesetest conditions.Input .................................. ~.
Output!Domain Independent !IiI' Testing knowledge i, i 'Parser I i.i *i 1!
Domain Dependenti, , 1i!
Subfanguage grammar I i\]Domain Model 1iL.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
III (Documents)~0 Canonicalsentences0 Additions totest systemFor this to be a successful architecture, thedomain-independent part must be robust enoughto work for multiple domains.
A person work-ing in a new domain should be given the frame-work and have only to fill in the appropriatedomain model and sublanguage grammar.The grammar developed oes not need toparse the attribute descriptions of the inputtext exhaustively.
Instead, it extracts the spe-cific concepts which can be used to test thedatabase.
It looks at the appropriate sectionsof the document on a sentence-by-sentence ba-sis.
If it is able to parse a sentence and de-rive a semantic interpretation for it, it re-turns the corresponding semantic expression.If not, it simply ignores it and moves on tothe next sentence.
This type of partial pars-ing is well suited to this job because any infor-mation parsed and extracted will usefully aug-ment  the test system.
Missed testcases willnot adversely impact he test system.COMBINATION CONDITIONSIn order to evaluate the effectiveness of thedocument parser, a particular type of testablecondition for database tests was chosen: legalcombinations of attributes and classes.
Theseconditions include two or more attributes thatmust or must not be used together, or an at-tribute that must or must not be used for aclass.The following are example sentences from the1.
If BUS-DATA is defined, then BUS mustalso be defined.2.
Must be used if values exist for START-ADDRESS or ADDRESS-PRIORITY attributes.3.
This attribute is appropriate only for classSYNC-COMM.4.
The attribute ABSOLUTE-MAX-PER-BUSmust also be defined.Canonical forms for the sentences were devel-oped and are listed in Figure 2.
Examples ofsentences and their canonical forms are givenin Figure 3.
The canonical form can be used togenerate a logical formula or a representationappropriate for input to the test system.Figure 2Canonical sentencesATTRIBUTE must \[not\] be def ined ifATTRIBUTE is \[not\] defined.ATTRIBUTE must \[not\] be def ined forCLASS.ATTRIBUTE can only be def ined forCLASS.Figure 3Canonical forms of example sentencesSentence:If BUS-DATA is def ined then BUS mustalso be defined.Canonical  form:BUS must be def ined if BUS-DATA isdefined.Sentence:This at t r ibute is appropr iate  onlyfor class SYNC-COMM.Canonical  form:BAUD-RATE can only be def ined forclass SYNC-COMM.THE GRAMMARSince we are only interested in retrieving spe-cific types of information from the documen-tation, the sublanguage grammar only has to295cover the specific ways of expressing that in-formation which are found in the documents.As can be seen in the list of example sentences,the information is expressed either in the formof modal, conditional, or generic sentences.In the XCS database dictionary, sentences de-scribing legal combinations of attributes andclasses use only certain syntactic onstructs,all expressible within context-free grammar.The grammar is able to parse these specifictypes of sentence structure.These sentences also use only a restricted setof semantic oncepts, and the grammar specifi-cally covers only these, which include negation,value phrases Ca value of,") and verbs of def-inition or usage ("is defined," is used").
Theyalso use the concepts of attribute and class asfound in the domain model.
Two specific lex-ical concepts which were relevant were thosefor "only," which implies that other things areexcluded from the relation, and "also," whichpresupposes that something is added to an al-ready established relation.
The semantic pro-cessing module uses the testing knowledge, thesublanguage semantic onstructs, and the do-main model to derive the appropriate canonicalform for a sentence.The database dictionary is written in an in-formal style and contains many incompletesentences.
The partially structured nature ofthe text assists in anaphora resolution and el-lipses expansion for these sentences.
For ex-ample, "Only relevant for software" in a san-ity check for the BACKWARD-COMPATIBLEattribute is equivalent to the sentence "TheBACKWARD-COMPATIBLE attribute is onlyrelevant for software."
The parsing systemkeeps track of the name of the attribute be-ing described and it uses it to fill in missingsentence components.EXPERIMENTAL RESULTSExperiments were done to investigate theutility of the document parser.
A portion of thedatabase dictionary was analyzed to determinethe ways the target concepts are expressed inthat portion of the document.
Then a gram-mar was constructed tocover these initial sen-tences.
The grammar was run on the entiredocument to evaluate its recall and precision inidentifying additional relevant sentences.
Theoutcome of the run on the entire document was296used to augment the grammar, which can thenbe run on successive versions of the documentover time to determine its value.Preliminary experiments u ing the grammarto extract information about the allowableXCS attribute and class combinations showedthat the system works with good recall (sixof twenty-six testcases were missed) and pre-cision (only two incorrect estcases were re-turned).
The grammar was augmented tocover the additional cases and not returnthe incorrect ones.
Subsequent versions ofthe database dictionary will provide additionaldata on its effectiveness.SUMMARYA document parser can be an effective soft-ware engineering tool for reverse ngineeringand populating test systems.
Questions re-main about the potential depth and robust-ness of the system for more complex types oftestable conditions, for additional documenttypes, and for additional domains.
Experi-ments in these areas will investigate deeperrepresentational structures for modal, condi-tional, and generic sentences, appropriate do-main modeling techniques, and representa-tions for general testing knowledge.ACKNOWLEDGMENTSI would like to thank James Pustejovsky forhis helpful comments on earlier drafts of thispaper.REFERENCESBarker, Virginia, & O'Connor, Dennis (1989).Expert systems for configuration at DIGITAL:XCON and beyond.
Communications of theACM, 32, 298-318.Lutsky, Patricia (1989).
Analysis of asublanguage grammar for parsing softwaredocumentation.
Unpublished master's thesis,Harvard University Extension.Schlimmer, Jeffrey (1991) Learning meta knowl-edge for database checking.
Proceedings ofAAAI 91, 335-340.
