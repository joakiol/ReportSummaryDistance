Coling 2010: Poster Volume, pages 725?729,Beijing, August 2010Collective Semantic Role Labeling on Open News Corpusby Leveraging Redundancy1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,2Long Jiang, 5Daniel Tse* and 3Zhongyang Xiong1School of Computer Science and TechnologyHarbin Institute of Technology2Microsoft Research Asia3College of Computer ScienceChongqing University4School of SoftwareDalian University of Technology5School of Information TechnologiesThe University of Sydney{xiaoliu, v-kuli, v-bohan, mingzhou, longj}@microsoft.comdtse6695@it.usyd.edu.auzyxiong@cqu.edu.cnAbstractWe propose a novel MLN-based methodthat collectively conducts SRL ongroups of news sentences.
Our method isbuilt upon a baseline SRL, which usesno parsers and leverages redundancy.We evaluate our method on a manuallylabeled news corpus and demonstratethat news redundancy significantly im-proves the performance of the baseline,e.g., it improves the F-score from64.13% to 67.66%.
*1 IntroductionSemantic Role Labeling (SRL, M?rquez, 2009)is generally understood as the task of identifyingthe arguments of a given predicate and assigningthem semantic labels describing the roles theyplay.
For example, given a sentence The luxuryauto maker sold 1,214 cars., the goal is to iden-tify the arguments of sold and produce the fol-lowing output: [A0 The luxury auto maker] [Vsold] [A1 1,214 cars].
Here A0 represents theseller, and A1 represents the things sold (CoNLL2008 shared task, Surdeanu et al, 2008).
* This work has been done while the author was visitingMicrosoft Research Asia.Gildea and Jurafsky (2002) first tackled SRLas an independent task, which is divided intoseveral sub-tasks such as argument identifica-tion, argument classification, global inference,etc.
Some researchers (Xue and Palmer, 2004;Koomen et al, 2005; Cohn and Blunsom, 2005;Punyakanok et al, 2008; Toutanova et al, 2005;Toutanova et al, 2008) used a pipelined ap-proach to attack the task.
Some others resolvedthe sub-tasks simultaneously.
For example, somework (Musillo and Merlo, 2006; Merlo and Mu-sillo, 2008) integrated syntactic parsing and SRLinto a single model, and another (Riedel andMeza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009)jointly handled all sub-tasks using Markov Log-ic Networks (MLN, Richardson and Domingos,2005).All the above methods conduct sentence levelSRL, and rely on parsers.
Parsers have showedgreat effects on SRL performance.
For example,Xue and Palmer (2004) reported that SRL per-formance dropped more than 10% when theyused syntactic features from an automatic parserinstead of the gold standard parsing trees.
Evenworse, parsers are not robust and cannot alwaysanalyze any input, due to the fact that some in-puts are not in the language described by theparser?s formal grammar, or adequately repre-sented within the parser?s training data.725We propose a novel MLN-based method thatcollectively conducts SRL on groups of newssentences to leverage the content redundancy innews.
To isolate the negative effect of noisefrom parsers and thus focus on the study of thecontribution of redundancy to SRL, we use noparsers in our approach.
We built a baseline SRL,which depends on no parsers, and use the MLNframework to exploit  redundancy.
Our intuitionis that SRL on one sentence can help that onother differently phrased sentences with similarmeaning.
For example, consider the followingsentence from a news article:A suicide bomber blew himself up Sunday inmarket in Pakistan's northwest crowded withshoppers ahead of a Muslim holiday, killing12 people, including a mayor who once sup-ported but had turned against the Taliban, of-ficials said.The state-of-art MLN-based system (Meza-Ruizand Riedel, 2009), hereafter referred to asMLNBS for brevity, incorrectly labels northwestinstead of bomber as A0 of killing.
Now consideranother sentence from another news article:Police in northwestern Pakistan say that a su-icide bomber has killed at least 13 people andwounded dozens of others.Here MLNBS correctly identify bomber as A0of killing.
When more sentences are observedwhere bomber as A0 of killing is correctly identi-fied, we will be more confident that bombershould be labeled as A0 of killing, and thatnorthwest should not be the A0 of killing accord-ing to the constraint that one predicate has atmost one A0.We manually construct a news corpus toevaluate our method.
In the corpus, semanticrole information is annotated and sentences withsimilar meanings are grouped together.
Experi-mental results show that news redundancy cansignificantly improve the performance of thebaseline system.Our contributions can be summarized as fol-lows:1.
We present a novel method that conductsSRL on a set of sentences collectively, in-stead of on a single sentence, by extend-ing MLNBS to leverage redundancy.2.
We show redundancy can significantlyimprove the performance of the baselinesystem, indicating a promising researchdirection towards open SRL.In the next section, we introduce news sen-tence extraction and clustering.
In Section 3, wedescribe our collective inference method.
In Sec-tion 4, we show our experimental results.
Finally,in Section 5 we conclude our paper with a dis-cussion of future work.2 Extraction and Clustering of NewsSentencesTo construct a corpus to evaluate our method,we extract sentences from clustered news arti-cles returned by news search engines such asBing and Google, and divide them into groupsso that sentences in a group have similar mean-ing.News articles in the same cluster are supposedto report the same event.
Thus we first groupsentences according to the news cluster theycome from.
Then we split sentences in the samecluster into several groups according to the simi-larity of meaning.
We assume that two sentencesare more similar in meaning if they share moresynonymous proper nouns and verbs.
The syno-nyms of verbs, like plod and trudge, are mainlyextracted from the Microsoft Encarta Diction-ary1, and the proper nouns thesaurus, containingsynonyms such as U.S. and the United States, ismanually compiled.As examples, below are two sentence groupswhich are extracted from a news cluster describ-ing Hurricane Ida.Group 1:?
Hurricane Ida, the first Atlantic hurri-cane to target the U.S. this year, plod-ded yesterday toward the Gulf Coast??
Hurricane Ida trudged toward the GulfCoast??
?Group 2:?
It could make landfall as early as Tues-day morning, although it was forecast toweaken further.1http://uk.encarta.msn.com/encnet/features/dictionary/dictionaryhome.aspx726?
Authorities said Ida could make landfallas early as Tuesday morning, althoughit was forecast to weaken by then.?
?3 Collective Inference Based on MLNOur method includes two core components: abaseline system that conducts SRL on every sen-tence; and a collective inference system that ac-cepts as input a group of sentences with prelimi-nary SRL information provided by the baseline.We build the baseline by removing formulasinvolving syntactic parsing information fromMLNBS (while keeping other rules) and retrain-ing the system using the tool and scripts provid-ed by Riedel and Meza-Ruiz (2008) on the man-ually annotated news corpus described in Sec-tion 4.A collective inference system is constructedto leverage redundancy in the SRL informationfrom the baseline.We first redefine the predicate role and treat itas observed:predicate role: Int x Int x Int x Role;role has four parameters: the first one stands forthe number of sentence in the input, which isnecessary to distinguish the sentences in a group;the other three are taken from the arguments ofthe role predicate defined by Riedel and Meza-Ruiz (2008), which denote the positions of thepredicate and the argument in the sentence andthe role of the argument, respectively.
If thepredication holds, it returns 1, otherwise 0.A hidden predicate final_role is defined topresent the final output, which has the same pa-rameters as the predicate role:predicate final_role: Int x Int x Int x Role;We introduce the following formula, whichdirectly passes the semantic role from the base-line to the final output:role(s, p, a, +r)=> final_role (s, p, a, +r)    (1)Here s is the sentence number in a group; p anda denote the positions of the predicate and ar-gument in s, respectively; r stands for the role ofthe argument; the ?+?
before the variable r indi-cates that different r has different weight.Then we define another formula for collectiveinference:s1?s2^lemma(s1,p1,p_lemma)^lemma(s2,p2,p_lemma)^lemma(s1,a1,a_lemma)^lemma(s2,a2,a_lemma)^role(s2,p2,a2,+r)=>final_role(s1,p1,a1,+r)                                                 (2)Here p_lemma(a_lemma) stands for the lemmaof the predicate(argument), which is obtainedfrom the lemma dictionary.
This dictionary isextracted from the dataset of CoNLL 2008shared task and is normalized using synonymdictionary described in Section 2; lemma is anobserved predicate that states whether or not theword has the lemma.Formula 2 encodes our basic ideas about col-lective SRL: given several sentences expressingsimilar meaning, if one sentence has a predicatep with an argument a of role r, the other sen-tences would be likely to have a predicate p?with an argument a?
of role r, where p?
and a?are the same or synonymous with p and a, re-spectively, as illustrated by the example in Sec-tion 1.Besides, we also apply structural constraints(Riedel and Meza-Ruiz, 2008) to final_role.To learn parameters of the collective infer-ence system, we use  thebeast (Riedel and Meza-Ruiz, 2008),  which is an open Markov LogicEngine, and train it on manually annotated newscorpus described in Section 4.4 ExperimentsTo train and test the collective inference system,we extract 1000 sentences from news clusters,and group them into 200 clusters using themethod described in Section 2.
For every sen-tence, POS tagging is conducted with theOpenNLP toolkit (Jason Baldridge et al, 2009),lemma of each word is obtained through thenormalized lemma dictionary described in Sec-tion 3, and SRL is manually labeled.
To reducehuman labeling efforts, we retrain our baselineon the WSJ corpus of CoNLL 2008 shared taskand run it on our news corpus, and then edit theSRL outputs by hand.We implement the collective inference systemwith the thebeast toolkit.
Precision, recall, andF-score are used as evaluation metrics.
In bothtraining and evaluation, we follow the CoNLL2008 shared task and regard only heads ofphrases as arguments.727Table 1 shows the averaged 10-fold cross val-idation results of our systems and the baseline,where the third and second line report the resultsof using and not using Formula 1 in our collec-tive inference system, respectively.Systems Pre.
(%) Rec.
(%) F-score (%)Baseline 69.87 59.26 64.13CI-1 62.99 72.96 67.61CI 67.01 68.33 67.66Table 1.
Averaged 10-fold cross validation re-sults (Pre.
: precision; Rec.
: recall).Experimental results show that the two collec-tive inference engines (CI-1 and CI) performsignificantly better than the baseline in terms ofthe recall and F-score, though a little worse inthe precision.
We observe that predicate-argument relationships in sentences with com-plex syntax are usually not recognized by thebaseline, but some of them are correctly identi-fied by the collective inference systems.
This,we guess, explains in large part the difference inperformance.
For instance, consider the follow-ing sentences in a group, where order and tellare synonyms:?
Colombia said on Sunday it will appealto the U.N. Security Council and theOAS after Hugo Chavez, the fiery leftistpresident of neighboring Venezuela, or-dered his army to prepare for war in or-der to assure peace.?
President Hugo Chavez ordered Vene-zuela's military to prepare for a possiblearmed conflict with Colombia, sayingyesterday that his country's soldiersshould be ready if the U.S. tries to pro-voke a war between the South Americanneighbors.?
Venezuelan President Hugo Chavez toldhis military and civil militias to preparefor a possible war with Colombia as ten-sions mount over an agreement givingU.S.
troops access to Colombian mili-tary bases.The baseline cannot label (ordered, Chavez, A0)for the first sentence, partially owing to the syn-tactic complexity of the sentence, but can identi-fy the relationship for the second and third sen-tence.
In contrast, the collective inference sys-tems can identify Chavez in the first sentence asA0 of order because of its occurrence in the oth-er sentences of the same group.As Table 1 shows, the CI system achieves thehighest F-score (67.66%), and a higher precisionthan the CI-1 system, indicating the effective-ness of Formula 1.
Consider the above three sen-tences.
CI-1 mislabels (ordered, Venezuela, A1)for the first sentence because the baseline labelsit for the second sentence.
In contrast, CI doesnot label it for the first sentence because thebaseline does not and (ordered, Venezuela, A1)rarely occurs in the outputs of the baseline forthis sentence group.We also find cases where the collective infer-ence systems do not but should help.
For exam-ple, consider the following group of sentences:?
A Brazilian university expelled a womanwho was heckled by hundreds of fellowstudents when she wore a short, pinkdress to class, taking out newspaper adsSunday to publicly accuse her of immo-rality.?
The university also published newspaperads accusing the student, Geisy Arruda,of immorality.The baseline has identified (published, univer-sity, A0) for the second sentence.
But neitherthe baseline nor our method labels (taking, uni-versity, A0) for the first one.
This happens be-cause publish is not considered as a synonymof take, and thus (published, university, A0) inthe second provides no evidence for (taking,university, A0) in the first.
We plan to developa context based synonym detection componentto address this issue in the future.5 Conclusions and Future WorkWe present a novel MLN-based method that col-lectively conducts SRL on groups of sentences.To help build training and test corpora, we de-sign a method to collect news sentences and todivide them into groups so that sentences of sim-ilar meaning fall into the same cluster.
Experi-mental results on a manually labeled news cor-pus show that collective inference, which lever-ages redundancy, can effectively improve theperformance of the baseline.728In the future, we plan to evaluate our methodon larger news corpora, and to extend our meth-od to other genres of corpora, such as tweets.ReferencesBaldridge, Jason, Tom Morton, and Gann.
2009.OpenNLP, http://opennlp.sourceforge.net/Cohn, Trevor and Philip Blunsom.
2005.
Semanticrole labelling with tree conditional random fields.Proceedings of the Ninth Conference on Computa-tional Natural Language Learning, pages: 169-172.Gildea, Daniel and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Journal of Computa-tional Linguistics, 28(3):245?288.Koomen, Peter, Vasin Punyakanok, Dan Roth, andWen-tau Yih.
2005.
Generalized inference withmultiple semantic role labeling systems.
Proceed-ings of the Ninth Conference on ComputationalNatural Language Learning, pages: 181-184.M?rquez, Llu?s.
2009.
Semantic Role Labeling Past,Present and Future, Tutorial of ACL-IJCNLP2009.Merlo, Paola and Gabriele Musillo.
2008.
Semanticparsing for high-precision semantic role labelling.Proceedings of the Twelfth Conference onComputational Natural Language Learning,pages: 1-8.Meza-Ruiz, Ivan and Sebastian Riedel.
2009.
JointlyIdentifying Predicates, Arguments and Sensesusing Markov Logic.
Human LanguageTechnologies: The 2009 Annual Conference of theNorth American Chapter of the ACL, pages: 155-163.Musillo, Gabriele and Paola Merlo.
2006.
AccurateParsing of the proposition bank.
Proceedings ofthe Human Language Technology Conference ofthe NAACL, pages: 101-104.Punyakanok, Vasin, Dan Roth and Wen-tau Yih.2008.
The importance of syntactic parsing andinference in semantic role labeling.
Journal ofComputational Linguistics, 34(2), 257-287.Richardson, Matthew and Pedro Domingos.
2005.Markov logic networks.
Technical Report, Univer-sity of Washington, 2005.Riedel, Sebastian and Ivan Meza-Ruiz.
2008.Collective semantic role labelling with MarkovLogic.
Proceedings of the Twelfth Conference onComputational Natural Language Learning,pages: 193-197.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
The conll2008 shared task on joint parsing of syntactic andsemantic dependencies.
Proceedings of the TwelfthConference on Computational Natural LanguageLearning, pages: 159-177.Toutanova, Kristina, Aria Haghighi and ChristopherD.
Manning.
2005.
Joint learning improves seman-tic role labeling.
Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics, pages: 589-596.Toutanova, Kristina, Aria Haghighi and ChristopherD.
Manning.
2008.
A global joint model for se-mantic role labeling.
Journal of ComputationalLinguistics, 34(2), 161-191.Xue, Nianwen and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages: 88-94.729
