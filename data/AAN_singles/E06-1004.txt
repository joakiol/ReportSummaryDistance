Computational Complexity of Statistical Machine TranslationRaghavendra Udupa U.IBM India Research LabNew DelhiIndiauraghave@in.ibm.comHemanta K. MajiDept.
of Computer ScienceUniversity of Illinois at Urbana-Champaignehemanta.maji@gmail.comAbstractIn this paper we study a set of prob-lems that are of considerable importanceto Statistical Machine Translation (SMT)but which have not been addressed satis-factorily by the SMT research community.Over the last decade, a variety of SMTalgorithms have been built and empiri-cally tested whereas little is known aboutthe computational complexity of some ofthe fundamental problems of SMT.
Ourwork aims at providing useful insights intothe the computational complexity of thoseproblems.
We prove that while IBM Mod-els 1-2 are conceptually and computation-ally simple, computations involving thehigher (and more useful) models are hard.Since it is unlikely that there exists a poly-nomial time solution for any of these hardproblems (unless P = NP and P#P =P), our results highlight and justify theneed for developing polynomial time ap-proximations for these computations.
Wealso discuss some practical ways of deal-ing with complexity.1 IntroductionStatistical Machine Translation is a data drivenmachine translation technique which uses proba-bilistic models of natural language for automatictranslation (Brown et al, 1993), (Al-Onaizan etal., 1999).
The parameters of the models areestimated by iterative maximum-likelihood train-ing on a large parallel corpus of natural languagetexts using the EM algorithm (Brown et al, 1993).The models are then used to decode, i.e.
trans-late texts from the source language to the targetlanguage 1 (Tillman, 2001), (Wang, 1997), (Ger-mann et al, 2003), (Udupa et al, 2004).
Themodels are independent of the language pair andtherefore, can be used to build a translation sys-tem for any language pair as long as a parallelcorpus of texts is available for training.
Increas-ingly, parallel corpora are becoming availablefor many language pairs and SMT systems havebeen built for French-English, German-English,Arabic-English, Chinese-English, Hindi-Englishand other language pairs (Brown et al, 1993), (Al-Onaizan et al, 1999), (Udupa, 2004).In SMT, every English sentence e is consideredas a translation of a given French sentence f withprobability Pr (f |e).
Therefore, the problem oftranslating f can be viewed as a problem of findingthe most probable translation of f :e?
= argmaxePr(e|f) = argmaxePr(f |e)P (e).
(1)The probability distributions Pr(f |e) andPr(e) are known as translation model and lan-guage model respectively.
In the classic work onSMT, Brown and his colleagues at IBM introducedthe notion of alignment between a sentence f andits translation e and used it in the development oftranslation models (Brown et al, 1993).
An align-ment between f = f1 .
.
.
fm and e = e1 .
.
.
elis a many-to-one mapping a : {1, .
.
.
,m} ?
{0, .
.
.
, l}.
Thus, an alignment a between f and eassociates the french word fj to the English wordeaj 2.
The number of words of f mapped to ei bya is called the fertility of ei and is denoted by ?i.Since Pr(f |e) = ?a Pr(f ,a|e), equation 1 can1In this paper, we use French and English as the prototyp-ical examples of source and target languages respectively.2e0 is a special word called the null word and is used toaccount for those words in f that are not connected by a toany of the words of e.25be rewritten as follows:e?
= argmaxe?aPr(f ,a|e)Pr(e).
(2)Brown and his colleagues developed a seriesof 5 translation models which have become to beknown in the field of machine translation as IBMmodels.
For a detailed introduction to IBM trans-lation models, please see (Brown et al, 1993).
Inpractice, models 3-5 are known to give good re-sults and models 1-2 are used to seed the EM it-erations of the higher models.
IBM model 3 isthe prototypical translation model and it modelsPr(f ,a|e) as follows:P (f ,a|e) ?
n(?0|?li=1 ?i)?li=1 n (?i|ei)?i!?
?mj=1 t(fj|eaj)?
?j: aj 6=0 d (j|i,m, l)Table 1: IBM Model 3Here, n(?|e) is the fertility model, t(f |e) isthe lexicon model and d(j|i,m, l) is the distortionmodel.The computational tasks involving IBM Modelsare the following:?
Viterbi AlignmentGiven the model parameters and a sentencepair (f , e), determine the most probablealignment between f and e.a?
= argmaxaP (f ,a|e)?
Expectation EvaluationThis forms the core of model training via theEM algorithm.
Please see Section 2.3 fora description of the computational task in-volved in the EM iterations.?
Conditional ProbabilityGiven the model parameters and a sentencepair (f , e), compute P (f |e).P (f |e) =?aP (f ,a|e)?
Exact DecodingGiven the model parameters and a sentence f ,determine the most probable translation of f .e?
= argmaxe?aP (f ,a|e) P (e)?
Relaxed DecodingGiven the model parameters and a sentence f ,determine the most probable translation andalignment pair for f .(e?,a?)
= argmax(e,a)P (f ,a|e) P (e)Viterbi Alignment computation finds applica-tions not only in SMT but also in other areasof Natural Language Processing (Wang, 1998),(Marcu, 2002).
Expectation Evaluation is thesoul of parameter estimation (Brown et al, 1993),(Al-Onaizan et al, 1999).
Conditional Proba-bility computation is important in experimentallystudying the concentration of the probability massaround the Viterbi alignment, i.e.
in determiningthe goodness of the Viterbi alignment in compar-ison to the rest of the alignments.
Decoding isan integral component of all SMT systems (Wang,1997), (Tillman, 2000), (Och et al, 2001), (Ger-mann et al, 2003), (Udupa et al, 2004).
ExactDecoding is the original decoding problem as de-fined in (Brown et al, 1993) and Relaxed Decod-ing is the relaxation of the decoding problem typ-ically used in practice.While several heuristics have been developedby practitioners of SMT for the computationaltasks involving IBM models, not much is knownabout the computational complexity of these tasks.In their seminal paper on SMT, Brown and his col-leagues highlighted the problems we face as we gofrom IBM Models 1-2 to 3-5(Brown et al, 1993)3:?As we progress from Model 1 to Model 5, eval-uating the expectations that gives us counts be-comes increasingly difficult.
In Models 3 and 4,we must be content with approximate EM itera-tions because it is not feasible to carry out sumsover all possible alignments for these models.
Inpractice, we are never sure that we have found theViterbi alignment?.However, neither their work nor the subsequentresearch in SMT studied the computational com-plexity of these fundamental problems with theexception of the Decoding problem.
In (Knight,1999) it was proved that the Exact Decoding prob-lem is NP-Hard when the language model is a bi-gram model.Our results may be summarized as follows:3The emphasis is ours.261.
Viterbi Alignment computation is NP-Hardfor IBM Models 3, 4, and 5.2.
Expectation Evaluation in EM Iterations is#P-Complete for IBM Models 3, 4, and 5.3.
Conditional Probability computation is#P-Complete for IBM Models 3, 4, and 5.4.
Exact Decoding is #P-Hard for IBM Mod-els 3, 4, and 5.5.
Relaxed Decoding is NP-Hard for IBMModels 3, 4, and 5.Note that our results for decoding are sharperthan that of (Knight, 1999).
Firstly, we show thatExact Decoding is #P-Hard for IBM Models 3-5and not just NP-Hard.
Secondly, we show thatRelaxed Decoding is NP-Hard for Models 3-5even when the language model is a uniform dis-tribution.The rest of the paper is organized as follows.We formally define all the problems discussed inthe paper (Section 2).
Next, we take up each of theproblems discussed in this section and derive thestated result for them (Section 3).
After this, wediscuss the implications of our results (Section 4)and suggest future directions (Section 5).2 Problem DefinitionConsider the functions f, g : ??
?
{0, 1}.
Wesay that g ?mp f (g is polynomial-time many-onereducible to f ), if there exists a polynomial timereduction r(.)
such that g(x) = f(r(x)) for allinput instances x ?
??.
This means that given amachine to evaluate f(.)
in polynomial time, thereexists a machine that can evaluate g(.)
in polyno-mial time.
We say a function f is NP-Hard, if allfunctions in NP are polynomial-time many-onereducible to f .
In addition, if f ?
NP, then wesay that f is NP-Complete.Also relevant to our work are counting func-tions that answer queries such as ?how many com-putation paths exist for accepting a particular in-stance of input??
Let w be a witness for the ac-ceptance of an input instance x and ?
(x,w) bea polynomial time witness checking function (i.e.?
(x,w) ?
P).
The function f : ??
?
N such thatf(x) =?w???|w|?p(|x|)?
(x,w)lies in the class #P, where p(.)
is a polynomial.Given functions f, g : ??
?
N, we say that g ispolynomial-time Turing reducible to f (i.e.
g ?Tf ) if there is a Turing machine with an oracle forf that computes g in time polynomial in the sizeof the input.
Similarly, we say that f is #P-Hard,if every function in #P can be polynomial timeTuring reduced to f .
If f is #P-Hard and is in#P, then we say that f is #P-Complete.2.1 Viterbi Alignment ComputationVITERBI-3 is defined as follows.
Given the para-meters of IBM Model 3 and a sentence pair (f , e),compute the most probable alignment a?
betwen fand e:a?
= argmaxaP (f ,a|e).2.2 Conditional Probability ComputationPROBABILITY-3 is defined as follows.
Giventhe parameters of IBM Model 3, and a sen-tence pair (f , e), compute the probabilityP (f |e) =?a P (f ,a|e).2.3 Expectation Evaluation in EM Iterations(f, e)-COUNT-3, (?, e)-COUNT-3, (j, i,m, l)-COUNT-3, 0-COUNT-3, and 1-COUNT-3 are de-fined respectively as follows.
Given the parame-ters of IBM Model 3, and a sentence pair (f , e),compute the following 4:c(f |e; f , e) =?aP (a|f , e)?j?
(f, fj)?
(e, eaj ),c(?|e; f , e) =?aP (a|f , e)?i?
(?, ?i)?
(e, ei),c(j|i,m, l; f , e) =?aP (a|f , e)?
(i, aj),c(0; f , e) =?aP (a|f , e)(m?
2?0), andc(1; f , e) =?aP (a|f , e)?0.2.4 DecodingE-DECODING-3 and R-DECODING-3 are definedas follows.
Given the parameters of IBM Model 3,4As the counts are normalized in the EM iteration, we canreplace P (a|f , e) by P (f ,a|e) in the Expectation Evaluationtasks.27and a sentence f , compute its most probable trans-lation according to the following equations respec-tively.e?
= argmaxe?aP (f ,a|e) P (e)(e?,a?)
= argmax(e,a)P (f ,a|e) P (e).2.5 SETCOVERGiven a collection of sets C = {S1, .
.
.
,Sl} anda set X ?
?li=1Si, find the minimum cardinalitysubset C?
of C such that every element in X be-longs to at least one member of C?.SETCOVER is a well-known NP-Completeproblem.
If SETCOVER ?mp f , then f is NP-Hard.2.6 PERMANENTGiven a matrixM = [Mj,i]n?n whose entries areeither 0 or 1, compute the following:perm(M) = ?pi?nj=1Mj,pij where pi is a per-mutation of 1, .
.
.
, n.This problem is the same as that of counting thenumber of perfect matchings in a bipartite graphand is known to be #P-Complete (?).
If PERMA-NENT ?T f , then f is #P-Hard.2.7 COMPAREPERMANENTSGiven two matrices A = [Aj,i]n?n and B =[Bj,i]n?n whose entries are either 0 or 1, determinewhich of them has a larger permanent.
PERMA-NENT is known to be Turing reducible to COM-PAREPERMANENTS (Jerrum, 2005) and therefore,if COMPAREPERMANENTS ?T f , then f is #P-Hard.3 Main ResultsIn this section, we present the main reductionsfor the problems with Model 3 as the translationmodel.
Our reductions can be easily carried overto Models 4?5 with minor modifications.
In orderto keep the presentation of the main ideas simple,we let the lexicon, distortion, and fertility modelsto be any non-negative functions and not just prob-ability distributions in our reductions.3.1 VITERBI-3We show that VITERBI-3 is NP-Hard.Lemma 1 SETCOVER ?mp VITERBI-3.Proof: We give a polynomial time many-onereduction from SETCOVER to VITERBI-3.
Givena collection of sets C = {S1, .
.
.
,Sl} and a setX ?
?li=1Si, we create an instance of VITERBI-3as follows:For each set Si ?
C, we create a word ei (1 ?
i ?l).
Similarly, for each element vj ?
X we createa word fj (1 ?
j ?
|X| = m).
We set the modelparameters as follows:t (fj|ei) ={1 if vj ?
Si0 otherwisen (?|e) ={12?!
if ?
6= 01 if ?
= 0d (j|i,m, l) = 1.Now consider the sentences e =e1 .
.
.
el and f = f1 .
.
.
fm.P (f ,a|e) = n(?0|l?i=1?i) l?i=1n (?i|ei)?i!
?m?j=1t(fj|eaj) ?j: aj 6=0d (j|i,m, l)=l?i=1121??
(?i,0)We can construct a cover for X from the outputof VITERBI-3 by defining C?
= {Si|?i > 0}.
Wenote that P (f ,a|e) = ?ni=1 121??
(?i,0) = 2?|C?|.Therefore, Viterbi alignment results in the mini-mum cover for X.3.2 PROBABILITY-3We show that PROBABILITY-3 is #P-Complete.We begin by proving the following:Lemma 2 PERMANENT ?T PROBABILITY-3.Proof: Given a 0, 1-matrix M =[Mj, i]n?n, we define f = f1 .
.
.
fn and e =e1 .
.
.
en where each ei and fj is distinct and setthe Model 3 parameters as follows:t (fj|ei) ={1 if Mj,i = 10 otherwisen (?|e) ={1 if ?
= 10 otherwised (j|i, n, n) = 1.28Clearly, with the above parameter setting,P (f ,a|e) = ?nj=1Mj, aj if a is a permutationand 0 otherwise.
Therefore,P (f |e) =?aP (f ,a|e)=?a is a permutationn?j=1Mj, aj = perm (M)Thus, by construction, PROBABILITY-3 com-putes perm (M).
Besides, the construction con-serves the number of witnesses.
Hence, PERMA-NENT ?T PROBABILITY-3.We now prove thatLemma 3 PROBABILITY-3 is in #P.Proof: Let (f , e) be the input toPROBABILITY-3.
Let m and l be the lengthsof f and e respectively.
With each alignmenta = (a1, a2, .
.
.
, am) we associate a unique num-ber na = a1a2 .
.
.
am in base l + 1.
Clearly,0 ?
na ?
(l + 1)m ?
1.
Let w be the binaryencoding of na.
Conversely, with every binarystring w we can associate an alignment a if thevalue of w is in the range 0, .
.
.
, (l + 1)m ?
1.
Itrequires O (m log (l + 1)) bits to encode an align-ment.
Thus, given an alignment we can computeits encoding and given the encoding we can com-pute the corresponding alignment in time polyno-mial in l and m. Similarly, given an encoding wecan compute P (f ,a|e) in time polynomial in l andm.
Now, if p(.)
is a polynomial, then functionf (f , e) =?w?
{0,1}?|w|?p(|?f , e?|)P (f ,a|e)is in #P. Choose p (x) = dx log2 (x + 1)e.Clearly, all alignments can be encoded using atmost p (| (f , e) |) bits.
Therefore, if (f , e) com-putes P (f |e) and hence, PROBABILITY-3 is in#P.It follows immediately from Lemma 2 andLemma 3 thatTheorem 1 PROBABILITY-3 is #P-Complete.3.3 (f, e)-COUNT-3Lemma 4 PERMANENT ?T (f, e)-COUNT-3.Proof: The proof is similar to that ofLemma 2.
Let f = f1 f2 .
.
.
fn f?
and e =e1 e2 .
.
.
en e?.
We set the translation model para-meters as follows:t (f |e) =????
?1 if f = fj, e = ei and Mj,i = 11 if f = f?
and e = e?0 otherwise.The rest of the parameters are set as in Lemma 2.Let A be the set of alignments a, such that an+1 =n+1 and an1 is a permutation of 1, 2, .
.
.
, n. Now,c(f?
|e?
; f , e)=?aP (f ,a|e)n+1?j=1?(f?
, fj)?
(e?, eaj )=?a?AP (f ,a|e)n+1?j=1?(f?
, fj)?
(e?, eaj )=?a?AP (f ,a|e)=?a?An?j=1Mj, aj = perm (M) .Therefore, PERMANENT ?T COUNT-3.Lemma 5 (f, e)-COUNT-3 is in #P.Proof: The proof is essentially the same asthat of Lemma 3.
Note that given an encoding w,P (f ,a|e)?mj=1 ?
(fj, f) ?
(eaj , e)can be evalu-ated in time polynomial in |(f , e)|.Hence, from Lemma 4 and Lemma 5, it followsthatTheorem 2 (f, e)-COUNT-3 is #P-Complete.3.4 (j, i,m, l)-COUNT-3Lemma 6 PERMANENT ?T (j, i,m, l)-COUNT-3.Proof: We proceed as in theproof of Lemma 4 with some modifica-tions.
Let e = e1 .
.
.
ei?1e?ei .
.
.
en andf = f1 .
.
.
fj?1f?
fj .
.
.
fn.
The parametersare set as in Lemma 4.
Let A be the set ofalignments, a, such that a is a permutation of1, 2, .
.
.
, (n + 1) and aj = i.
Observe thatP (f ,a|e) is non-zero only for the alignments inA.
It follows immediately that with these para-meter settings, c(j|i, n, n; f , e) = perm (M) .Lemma 7 (j, i,m, l)-COUNT-3 is in #P.Proof: Similar to the proof of Lemma 5.Theorem 3 (j, i,m, l)-COUNT-3 is #P-Complete.293.5 (?, e)-COUNT-3Lemma 8 PERMANENT ?T (?, e)-COUNT-3.Proof: Let e = e1 .
.
.
ene?
and f =f1 .
.
.
fnk?
??
?f?
.
.
.
f?
.
Let A be the set of alignmentsfor which an1 is a permutation of 1, 2, .
.
.
, n andan+kn+1 =k?
??
?
(n + 1) .
.
.
(n + 1).
We setn (?|e) =????
?1 if ?
= 1 and e 6= e?1 if ?
= k and e = e?0 otherwise.The rest of the parameters are set as in Lemma 4.Note that P (f ,a|e) is non-zero only for the align-ments in A.
It follows immediately that with theseparameter settings, c(k|e?
; f , e) = perm (M) .Lemma 9 (?, e)-COUNT-3 is in #P.Proof: Similar to the proof of Lemma 5.Theorem 4 (?, e)-COUNT-3 is #P-Complete.3.6 0-COUNT-3Lemma 10 PERMANENT ?T 0-COUNT-3.Proof: Let e = e1 .
.
.
en and f = f1 .
.
.
fnf?
.Let A be the set of alignments, a, such that an1 isa permutation of 1, .
.
.
, n and an+1 = 0.
We sett (f |e) =????
?1 if f = fj, e = ei and Mj, i = 11 if f = f?
and e = NULL0 otherwise.The rest of the parameters are set as in Lemma 4.It is easy to see that with these settings, c(0;f ,e)(n?2) =perm (M) .Lemma 11 0-COUNT-3 is in #P.Proof: Similar to the proof of Lemma 5.Theorem 5 0-COUNT-3 is #P-Complete.3.7 1-COUNT-3Lemma 12 PERMANENT ?T 1-COUNT-3.Proof: We set the parameters as inLemma 10.
It follows immediately thatc(1; f , e) = perm (M) .Lemma 13 1-COUNT-3 is in #P.Proof: Similar to the proof of Lemma 5.Theorem 6 1-COUNT-3 is #P-Complete.3.8 E-DECODING-3Lemma 14 COMPAREPERMANENTS ?T E-DECODING-3Proof: Let M and N be the two 0-1 matri-ces.
Let f = f1f2 .
.
.
fn, e(1) = e(1)1 e(1)2 .
.
.
e(1)nand e(2) = e(2)1 e(2)2 .
.
.
e(2)n .
Further, let e(1) ande(2) have no words in common and each wordappears exactly once.
By setting the bigram lan-guage model probabilities of the bigrams that oc-cur in e(1) and e(2) to 1 and all other bigram prob-abilities to 0, we can ensure that the only trans-lations considered by E-DECODING-3 are indeede(1) and e(2) and P(e(1))= P(e(2))= 1.
Wethen sett (f |e) =????
?1 if f = fj, e = e(1)i and Mj,i = 11 if f = fj, e = e(2)i and Nj,i = 10 otherwisen (?|e) ={1 ?
= 10 otherwised (j|i, n, n) = 1.Now, P(f |e(1))= perm (M), and P(f |e(2))=perm (N ).
Therefore, given the output of E-DECODING-3 we can find out which of M andN has a larger permanent.Hence E-DECODING-3 is #P?Hard.3.9 R-DECODING-3Lemma 15 SETCOVER ?mp R-DECODING-3Proof: Given an instance of SETCOVER, weset the parameters as in the proof of Lemma 1 withthe following modification:n (?|e) ={12?!
if ?
> 00 otherwise.Let e be the optimal translation obtained by solv-ing R-DECODING-3.
As the language model isuniform, the exact order of the words in e is notimportant.
Now, we observe that:?
e contains words only from the set{e1, e2, .
.
.
, el}.
This is because, there can-not be any zero fertility word as n (0|e) = 0and the only words that can have a non-zerofertility are from {e1, e2, .
.
.
, el} due to theway we have set the lexicon parameters.?
No word occurs more than once in e. Assumeon the contrary that the word ei occurs k > 130times in e. Replace these k occurrences byonly one occurrence of ei and connect all thewords connected to them to this word.
Thiswould increase the score of e by a factor of2k?1 > 1 contradicting the assumption onthe optimality of e.As a result, the only candidates for e are subsets of{e1, e2, .
.
.
, el} in any order.
It is now straight for-ward to verify that a minimum set cover can be re-covered from e as shown in the proof of Lemma 1.3.10 IBM Models 4 and 5The reductions are for Model 3 can be easily ex-tended to Models 4 and 5.
Thus, we have the fol-lowing:Theorem 7 Viterbi Alignment computation isNP-Hard for IBM Models 3?
5.Theorem 8 Expectation Evaluation in the EMSteps is #P-Complete for IBM Models 3?
5.Theorem 9 Conditional Probability computationis #P-Complete for IBM Models 3?
5.Theorem 10 Exact Decoding is #P-Hard forIBM Models 3?
5.Theorem 11 Relaxed Decoding is NP-Hard forIBM Models 3?
5 even when the language modelis a uniform distribution.4 DiscussionOur results answer several open questions on thecomputation of Viterbi Alignment and ExpectationEvaluation.
Unless P = NP and P#P = P,there can be no polynomial time algorithms foreither of these problems.
The evaluation of ex-pectations becomes increasingly difficult as we gofrom IBM Models 1-2 to Models 3-5 exactly be-cause the problem is #P-Complete for the lattermodels.
There cannot be any trick for IBM Mod-els 3-5 that would help us carry out the sums overall possible alignments exactly.
There cannot exista closed form expression (whose representation ispolynomial in the size of the input) for P (f |e) andthe counts in the EM iterations for Models 3-5.It should be noted that the computation ofViterbi Alignment and Expectation Evaluation iseasy for Models 1-2.
What makes these computa-tions hard for Models 3-5?
To answer this ques-tion, we observe that Models 1-2 lack explicit fer-tility model unlike Models 3-5.
In the former mod-els, fertility probabilities are determined by thelexicon and alignment models.
Whereas, in Mod-els 3-5, the fertility model is independent of thelexicon and alignment models.
It is precisely thisfreedom that makes computations on Models 3-5harder than the computations on Models 1-2.There are three different ways of dealing withthe computational barrier posed by our problems.The first of these is to develop a restricted fertil-ity model that permits polynomial time computa-tions.
It remains to be found what kind of parame-terized distributions are suitable for this purpose.The second approach is to develop provably goodapproximation algorithms for these problems as isdone with many NP-Hard and #P-Hard prob-lems.
Provably good approximation algorithmsexist for several covering problems including SetCover and Vertex Cover.
Viterbi Alignment is itselfa special type of covering problem and it remainsto be seen whether some of the techniques devel-oped for covering algorithms are useful for findinggood approximations to Viterbi Alignment.
Sim-ilarly, there exist several techniques for approxi-mating the permanent of a matrix.
It needs to beexplored if some of these ideas can be adapted forExpectation Evaluation.As the third approach to deal with complex-ity, we can approximate the space of all possi-ble (l + 1)m alignments by an exponentially largesubspace.
To be useful such large subspacesshould also admit optimal polynomial time al-gorithms for the problems we have discussed inthis paper.
This is exactly the approach takenby (Udupa, 2005) for solving the decoding andViterbi alignment problems.
They show that veryefficient polynomial time algorithms can be de-veloped for both Decoding and Viterbi Alignmentproblems.
Not only the algorithms are prov-ably superior in a computational complexity sense,(Udupa, 2005) are also able to get substantial im-provements in BLEU and NIST scores over theGreedy decoder.5 ConclusionsIBM models 3-5 are widely used in SMT.
Thecomputational tasks discussed in this work formthe backbone of all SMT systems that use IBMmodels.
We believe that our results on the compu-tational complexity of the tasks in SMT will resultin a better understanding of these tasks from a the-oretical perspective.
We also believe that our re-sults may help in the design of effective heuristics31for some of these tasks.
A theoretical analysis ofthe commonly employed heuristics will also be ofinterest.An open question in SMT is whether there ex-ists closed form expressions (whose representationis polynomial in the size of the input) for P (f |e)and the counts in the EM iterations for models 3-5(Brown et al, 1993).
For models 1-2, closed formexpressions exist for P (f |e) and the counts in theEM iterations for models 3-5.
Our results showthat there cannot exist a closed form expression(whose representation is polynomial in the size ofthe input) for P (f |e) and the counts in the EMiterations for Models 3-5 unless P = NP.ReferencesK.
Knight.
1999.
Decoding Complexity in Word-Replacement Translation Models.
ComputationalLinguistics.Brown, P. et al 1993.
The Mathematics of MachineTranslation: Parameter Estimation.
ComputationalLinguistics, 2(19):263?311.Al-Onaizan, Y. et al 1999.
Statistical Machine Trans-lation: Final Report.
JHU Workshop Final Report.R.
Udupa, and T. Faruquie.
2004.
An English-HindiStatistical Machine Translation System.
Proceed-ings of the 1st IJCNLP.Y.
Wang, and A. Waibel.
1998.
Modeling with Struc-tures in Statistical Machine Translation.
Proceed-ings of the 36th ACL.D.
Marcu and W. Wong.
2002.
A Phrase-Based, JointProbability Model for Statistical Machine Transla-tion.
Proceedings of the EMNLP.L.
Valiant.
1979.
The complexity of computing thepermanent.
Theoretical Computer Science, 8:189?201.M.
Jerrum.
2005.
Personal communication.C.
Tillman.
2001.
Word Re-ordering and DynamicProgramming based Search Algorithm for StatisticalMachine Translation.
Ph.D. Thesis, University ofTechnology Aachen 42?45.Y.
Wang and A. Waibel.
2001.
Decoding algorithm instatistical machine translation.
Proceedings of the35th ACL 366?372.C.
Tillman and H. Ney.
2000.
Word reordering andDP-based search in statistical machine translation.Proceedings of the 18th COLING 850?856.F.
Och, N. Ueffing, and H. Ney.
2000.
An efficient A*search algorithm for statistical machine translation.Proceedings of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation 55?62.U.
Germann et al 2003.
Fast Decoding and OptimalDecoding for Machine Translation.
Artificial Intel-ligence.R.
Udupa, H. Maji, and T. Faruquie.
2004.
An Al-gorithmic Framework for the Decoding Problem inStatistical Machine Translation.
Proceedings of the20th COLING.R.
Udupa and H. Maji.
2005.
Theory of AlignmentGenerators and Applications to Statistical MachineTranslation.
Proceedings of the 19th IJCAI.32
