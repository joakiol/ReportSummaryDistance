Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951?962,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSystem Combination for Grammatical Error CorrectionRaymond Hendy Susanto Peter Phandi Hwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing Drive, Singapore 117417{raymondhs,peter-p,nght}@comp.nus.edu.sgAbstractDifferent approaches to high-qualitygrammatical error correction have beenproposed recently, many of which havetheir own strengths and weaknesses.
Mostof these approaches are based on classi-fication or statistical machine translation(SMT).
In this paper, we propose to com-bine the output from a classification-basedsystem and an SMT-based system toimprove the correction quality.
We adoptthe system combination technique ofHeafield and Lavie (2010).
We achieve anF0.5score of 39.39% on the test set of theCoNLL-2014 shared task, outperformingthe best system in the shared task.1 IntroductionGrammatical error correction (GEC) refers to thetask of detecting and correcting grammatical er-rors present in a text written by a second languagelearner.
For example, a GEC system to correctEnglish promises to benefit millions of learnersaround the world, since it functions as a learningaid by providing instantaneous feedback on ESLwriting.Research in this area has attracted much interestrecently, with four shared tasks organized in thepast several years: Helping Our Own (HOO) 2011and 2012 (Dale and Kilgarriff, 2010; Dale et al.,2012), and the CoNLL 2013 and 2014 shared tasks(Ng et al., 2013; Ng et al., 2014).
Each shared taskcomes with an annotated corpus of learner textsand a benchmark test set, facilitating further re-search in GEC.Many approaches have been proposed to de-tect and correct grammatical errors.
The mostdominant approaches are based on classification(a set of classifier modules where each module ad-dresses a specific error type) and statistical ma-chine translation (SMT) (formulated as a transla-tion task from ?bad?
to ?good?
English).
Other ap-proaches combine the classification and SMT ap-proaches, and often have some rule-based compo-nents.Each approach has its own strengths and weak-nesses.
Since the classification approach is able tofocus on each individual error type using a sep-arate classifier, it may perform better on an er-ror type where it can build a custom-made classi-fier tailored to the error type, such as subject-verbagreement errors.
The drawback of the classifica-tion approach is that one classifier must be builtfor each error type, so a comprehensive GEC sys-tem will need to build many classifiers which com-plicates its design.
Furthermore, the classificationapproach does not address multiple error types thatmay interact.The SMT approach, on the other hand, natu-rally takes care of interaction among words in asentence as it attempts to find the best overall cor-rected sentence.
It usually has a better coverageof different error types.
The drawback of this ap-proach is its reliance on error-annotated learnerdata, which is expensive to produce.
It is not pos-sible to build a competitive SMT system without asufficiently large parallel training corpus, consist-ing of texts written by ESL learners and the corre-sponding corrected texts.In this work, we aim to take advantage of boththe classification and the SMT approaches.
Bycombining the outputs of both systems, we hopethat the strengths of one approach will offset theweaknesses of the other approach.
We adopt thesystem combination technique of (Heafield andLavie, 2010), which starts by creating word-levelalignments among multiple outputs.
By perform-ing beam search over these alignments, it triesto find the best corrected sentence that combinesparts of multiple system outputs.The main contributions of this paper are as fol-951lows:?
It is the first work that makes use of a systemcombination strategy to improve grammaticalerror correction;?
It gives a detailed description of methodsand experimental setup for building compo-nent systems using two state-of-the-art ap-proaches; and?
It provides a detailed analysis of how one ap-proach can benefit from the other approachthrough system combination.We evaluate our system combination approachon the CoNLL-2014 shared task.
The approachachieves an F0.5score of 39.39%, outperformingthe best participating team in the shared task.The remainder of this paper is organized as fol-lows.
Section 2 gives the related work.
Section 3describes the individual systems.
Section 4 ex-plains the system combination method.
Section 5presents experimental setup and results.
Section 6provides a discussion and analysis of the results.Section 7 describes further experiments on systemcombination.
Finally, Section 8 concludes the pa-per.2 Related Work2.1 Grammatical Error CorrectionEarly research in grammatical error correction fo-cused on a single error type in isolation.
For ex-ample, Knight and Chander (1994) built an articlecorrection system for post-editing machine trans-lation output.The classification approach has been used todeal with the most common grammatical mistakesmade by ESL learners, such as article and prepo-sition errors (Han et al., 2006; Chodorow et al.,2007; Tetreault and Chodorow, 2008; Gamon,2010; Dahlmeier and Ng, 2011; Rozovskaya andRoth, 2011; Wu and Ng, 2013), and more recently,verb errors (Rozovskaya et al., 2014b).
Statis-tical classifiers are trained either from learner ornon-learner texts.
Features are extracted from thesentence context.
Typically, these are shallow fea-tures, such as surrounding n-grams, part-of-speech(POS) tags, chunks, etc.
Different sets of fea-tures are employed depending on the error typeaddressed.The statistical machine translation (SMT) ap-proach has gained more interest recently.
Earlierwork was done by Brockett et al.
(2006), wherethey used SMT to correct mass noun errors.
Themajor impediment in using the SMT approach forGEC is the lack of error-annotated learner (?par-allel?)
corpora.
Mizumoto et al.
(2011) mined alearner corpus from the social learning platformLang-8 and built an SMT system for correctinggrammatical errors in Japanese.
They further triedtheir method for English (Mizumoto et al., 2012).Other approaches combine the advantages ofclassification and SMT (Dahlmeier and Ng,2012a) and sometimes also include rule-basedcomponents.
Note that in the hybrid approachesproposed previously, the output of each compo-nent system might be only partially corrected forsome subset of error types.
This is different fromour system combination approach, where the out-put of each component system is a complete cor-rection of the input sentence where all error typesare dealt with.State-of-the-art performance is achieved byboth the classification (Dahlmeier et al., 2012;Rozovskaya et al., 2013; Rozovskaya et al.,2014a) and the SMT approach (Felice et al., 2014;Junczys-Dowmunt and Grundkiewicz, 2014),which motivates us to attempt system output com-bination from both approaches.2.2 System CombinationSystem combination is the task of combining theoutputs of multiple systems to produce an out-put better than each of its individual componentsystems.
In machine translation (MT), combin-ing multiple MT outputs has been attempted inthe Workshop on Statistical Machine Translation(Callison-Burch et al., 2009; Bojar et al., 2011).One of the common approaches in system com-bination is the confusion network approach (Rostiet al., 2007b).
In this approach, a confusion net-work is created by aligning the outputs of multi-ple systems.
The combined output is generated bychoosing the output of one single system as the?backbone?, and aligning the outputs of all othersystems to this backbone.
The word order of thecombined output will then follow the word orderof the backbone.
The alignment step is critical insystem combination.
If there is an alignment er-ror, the resulting combined output sentence maybe ungrammatical.Rosti et al.
(2007a) evaluated three system com-bination methods in their work:952?
Sentence level This method looks at the com-bined N-best list of the systems and selectsthe best output.?
Phrase level This method creates new hy-potheses using a new phrase translation ta-ble, built according to the phrase alignmentsof the systems.?
Word level This method creates a graph byaligning the hypotheses of the systems.
Theconfidence score of each aligned word is thencalculated according to the votes from the hy-potheses.Combining different component sub-systemswas attempted by CUUI (Rozovskaya et al.,2014a) and CAMB (Felice et al., 2014) in theCoNLL-2014 shared task.
The CUUI system em-ploys different classifiers to correct various errortypes and then merges the results.
The CAMBsystem uses a pipeline of systems to combine theoutputs of their rule based system and their SMTsystem.
The combination methods used in thosesystems are different from our approach, becausethey combine individual sub-system components,by piping the output from one sub-system to an-other, whereas we combine the outputs of wholesystems.
Moreover, our approach is able to com-bine the advantages of both the classification andSMT approaches.
In the field of grammatical errorcorrection, our work is novel as it is the first thatuses system combination to improve grammaticalerror correction.3 The Component SystemsWe build four individual error correction systems.Two systems are pipeline systems based on theclassification approach, whereas the other two arephrase-based SMT systems.
In this section, wedescribe how we build each system.3.1 PipelineWe build two different pipeline systems.
Each sys-tem consists of a sequence of classifier-based cor-rection steps.
We use two different sequences ofcorrection steps as shown in Table 1.
As shownby the table, the only difference between the twopipeline systems is that we swap the noun numberand the article correction step.
We do this becausethere is an interaction between noun number andarticle correction.
Swapping them generates sys-tem outputs that are quite different.Step Pipeline 1 (P1) Pipeline 2 (P2)1 Spelling Spelling2 Noun number Article3 Preposition Preposition4 Punctuation Punctuation5 Article Noun number6 Verb form, SVA Verb form, SVATable 1: The two pipeline systems.We model each of the article, preposition, andnoun number correction task as a multi-class clas-sification problem.
A separate multi-class confi-dence weighted classifier (Crammer et al., 2009)is used for correcting each of these error types.
Acorrection is only made if the difference betweenthe scores of the original class and the proposedclass is larger than a threshold tuned on the devel-opment set.
The features of the article and prepo-sition classifiers follow the features used by theNUS system from HOO 2012 (Dahlmeier et al.,2012).
For the noun number error type, we uselexical n-grams, ngram counts, dependency rela-tions, noun lemma, and countability features.For article correction, the classes are the arti-cles a, the, and the null article.
The article anis considered to be the same class as a.
A sub-sequent post-processing step chooses between aand an based on the following word.
For prepo-sition correction, we choose 36 common Englishprepositions as used in (Dahlmeier et al., 2012).We only deal with preposition replacement but notpreposition insertion or deletion.
For noun numbercorrection, the classes are singular and plural.Punctuation, subject-verb agreement (SVA),and verb form errors are corrected using rule-based classifiers.
For SVA errors, we assume thatnoun number errors have already been correctedby classifiers earlier in the pipeline.
Hence, onlythe verb is corrected when an SVA error is de-tected.
For verb form errors, we change a verb intoits base form if it is preceded by a modal verb, andwe change it into the past participle form if it ispreceded by has, have, or had.The spelling corrector uses Jazzy, an opensource Java spell-checker1.
We filter the sugges-tions given by Jazzy using a language model.
Weaccept a suggestion from Jazzy only if the sugges-tion increases the language model score of the sen-tence.1http://jazzy.sourceforge.net/9533.2 Statistical Machine TranslationThe other two component systems are basedon phrase-based statistical machine translation(Koehn et al., 2003).
It follows the well-known log-linear model formulation (Och andNey, 2002):e?
= argmaxeP (e|f)= argmaxeexp(M?m=1?mhm(e, f))(1)where f is the input sentence, e is the correctedoutput sentence, hmis a feature function, and ?mis its weight.
The feature functions include a trans-lation model learned from a sentence-aligned par-allel corpus and a language model learned from alarge English corpus.
More feature functions canbe integrated into the log-linear model.
A decoderfinds the best correction e?
that maximizes Equa-tion 1 above.The parallel corpora that we use to trainthe translation model come from two differentsources.
The first corpus is NUCLE (Dahlmeier etal., 2013), containing essays written by students atthe National University of Singapore (NUS) whichhave been manually corrected by English instruc-tors at NUS.
The other corpus is collected fromthe language exchange social networking websiteLang-8.
We develop two versions of SMT sys-tems: one with two phrase tables trained on NU-CLE and Lang-8 separately (S1), and the otherwith a single phrase table trained on the concate-nation of NUCLE and Lang-8 data (S2).
Multiplephrase tables are used with alternative decodingpaths (Birch et al., 2007).
We add a word-levelLevenshtein distance feature in the phrase tableused by S2, similar to (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014).
This featureis not included in S1.4 System CombinationWe use MEMT (Heafield and Lavie, 2010) tocombine the outputs of our systems.
MEMT usesMETEOR (Banerjee and Lavie, 2005) to performalignment of each pair of outputs from the compo-nent systems.
The METEOR matcher can identifyexact matches, words with identical stems, syn-onyms, and unigram paraphrases.MEMT uses an approach similar to the confu-sion network approach in SMT system combina-tion.
The difference is that it performs alignmenton the outputs of every pair of component systems,so it does not need to choose a single backbone.As MEMT does not choose any single system out-put as its backbone, it can consider the output ofeach component system in a symmetrical manner.This increases word order flexibility, as choosinga single hypothesis as the backbone will limit thenumber of possible word order permutations.After creating pairwise alignments using ME-TEOR, the alignments form a confusion network.MEMT will then perform a beam search over thisgraph to find the one-best hypothesis.
The searchis carried out from left to right, one word at a time,creating a partial hypothesis.
During beam search,it can freely switch among the component sys-tems, combining the outputs together into a sen-tence.
When it adds a word to its hypothesis, allthe words aligned to it in the other systems are alsomarked as ?used?.
If it switches to another inputsentence, it has to use the first ?unused?
word inthat sentence.
This is done to make sure that ev-ery aligned word in the sentences is used.
In somecases, a heuristic could be used to allow skippingover some words (Heafield et al., 2009).During beam search, MEMT uses a few featuresto score the hypotheses (both partial hypothesesand full hypotheses):?
Length The number of tokens in a hypoth-esis.
It is useful to normalize the impact ofsentence length.?
Language model Log probability from a lan-guage model.
It is especially useful in main-taining sentence fluency.?
Backoff The average n-gram length found inthe language model.?
Match The number of n-gram matches be-tween the outputs of the component systemsand the hypothesis, counted for small ordern-grams.The weights of these features are tuned using Z-MERT (Zaidan, 2009) on a development set.This system combination approach has a fewadvantages in grammatical error correction.
ME-TEOR not only can match words with exactmatches, but also words with identical stems, syn-onyms, and unigram paraphrases.
This means thatit can deal with word form, noun number, and verbform corrections that share identical stems, as well954Data set # sentences # source tokensNUCLE 57,151 1,161,567Lang-8 1,114,139 12,945,666CoNLL-2013 1,381 29,207CoNLL-2014 1,312 30,144EnglishWikipedia86,992,889 1,778,849,655Table 2: Statistics of the data sets.as word choice corrections (with synonyms andunigram paraphrases).
Also, MEMT uses a lan-guage model feature to maintain sentence fluency,favoring grammatical output sentences.In this paper, we combine the pipeline systemP1 (Table 1) with the SMT system S1, and alsocombine P2 with S2.
The two component sys-tems in each pair have comparable performance.For our final system, we also combine all four sys-tems together.5 ExperimentsOur approach is evaluated in the context of theCoNLL-2014 shared task on grammatical errorcorrection.
Specific details of the shared task canbe found in the overview paper (Ng et al., 2014),but we summarize the most important details rele-vant to our study here.5.1 DataWe use NUCLE version 3.2 (Dahlmeier et al.,2013), the official training data of the CoNLL-2014 shared task, to train our component systems.The grammatical errors in this corpus are catego-rized into 28 different error types.
We also use the?Lang-8 Corpus of Learner English v1.0?2(Tajiriet al., 2012) to obtain additional learner data.
En-glish Wikipedia3is used for language modelingand collecting n-gram counts.
All systems aretuned on the CoNLL-2013 test data (which servesas the development data set) and tested on theCoNLL-2014 test data.
The statistics of the datasets can be found in Table 2.5.2 EvaluationSystem performance is evaluated based on pre-cision, recall, and F0.5(which weights precisiontwice as much as recall).
Given a set of n sen-tences, where giis the set of gold-standard edits2http://cl.naist.jp/nldata/lang-8/3http://dumps.wikimedia.org/enwiki/20140102/enwiki-20140102-pages-articles.xml.bz2for sentence i, and eiis the set of system edits forsentence i, precision, recall, and F0.5are definedas follows:P =?ni=1|gi?
ei|?ni=1|ei|(2)R =?ni=1|gi?
ei|?ni=1|gi|(3)F0.5=(1 + 0.52)?R?
PR+ 0.52?
P(4)where the intersection between giand eifor sen-tence i is defined asgi?
ei= {e ?
ei|?g ?
gi,match(g, e)} (5)The official scorer for the shared task wasthe MaxMatch (M2) scorer4(Dahlmeier and Ng,2012b).
The scorer computes the sequence of sys-tem edits between a source sentence and a systemhypothesis that achieves the maximal overlap withthe gold-standard edits.
Like CoNLL-2014, F0.5is used instead of F1to emphasize precision.
Forstatistical significance testing, we use the sign testwith bootstrap re-sampling on 100 samples.5.3 Pipeline SystemWe use ClearNLP5for POS tagging and depen-dency parsing, and OpenNLP for chunking6.
Weuse the WordNet (Fellbaum, 1998) morphologysoftware to generate singular and plural word sur-face forms.The article, preposition, and noun number cor-rectors use the classifier approach to correct errors.Each classifier is trained using multi-class confi-dence weighted learning on the NUCLE and Lang-8 corpora.
The classifier threshold is tuned using asimple grid search on the development data set foreach class of a classifier.5.4 SMT SystemThe system is trained using Moses (Koehn et al.,2007), with Giza++ (Och and Ney, 2003) for wordalignment.
The translation table is trained usingthe ?parallel?
corpora of NUCLE and Lang-8.
Thetable contains phrase pairs of maximum lengthseven.
We include five standard parameters in thetranslation table: forward and reverse phrase trans-lations, forward and reverse lexical translations,4http://www.comp.nus.edu.sg/?nlp/sw/m2scorer.tar.gz5https://code.google.com/p/clearnlp/6http://opennlp.apache.org/955and phrase penalty.
We further add a word-levelLevenshtein distance feature for S2.We do not use any reordering model in our sys-tem.
The intuition is that most error types do notinvolve long-range reordering and local reorder-ing can be easily captured in the phrase translationtable.
The distortion limit is set to 0 to prohibitreordering during hypothesis generation.We build two 5-gram language models using thecorrected side of NUCLE and English Wikipedia.The language models are estimated using theKenLM toolkit (Heafield et al., 2013) with mod-ified Kneser-Ney smoothing.
These two languagemodels are used as separate feature functions inthe log-linear model.
Finally, they are binarizedinto a probing data structure (Heafield, 2011).Tuning is done on the development data set withMERT (Och, 2003).
We use BLEU (Papineni etal., 2002) as the tuning metric, which turns out towork well in our experiment.5.5 Combined SystemWe use an open source MEMT implementationby Heafield and Lavie (2010) to combine the out-puts of our systems.
Parameters are set to the val-ues recommended by (Heafield and Lavie, 2010):a beam size of 500, word skipping using lengthheuristic with radius 5, and with the length nor-malization option turned off.
We use five match-ing features for each system: the number of exactunigram and bigram matches between hypothesesand the number of matches in terms of stems, syn-onyms, or paraphrases for unigrams, bigrams, andtrigrams.
We use the Wikipedia 5-gram languagemodel in this experiment.We tune the combined system on the develop-ment data set.
The test data is input into boththe pipeline and SMT system respectively and theoutput from each system is then matched usingMETEOR (Banerjee and Lavie, 2005).
Featureweights, based on BLEU, are then tuned using Z-MERT (Zaidan, 2009).
We repeat this process fivetimes and use the weights that achieve the bestscore on the development data set in our final com-bined system.5.6 ResultsOur experimental results using the CoNLL-2014test data as the test set are shown in Table 3.
Eachsystem is evaluated against the same gold standardhuman annotations.
As recommended in Ng et al.
(2014), we do not use the revised gold standard toSystem P R F0.5PipelineP1 40.24 23.99 35.44P2 39.93 22.77 34.70SMTS1 57.90 14.16 35.80S2 62.11 12.54 34.69CombinedP1+S1 53.85 17.65 38.19P2+S2 56.92 16.22 37.90P1+P2+S1+S2 53.55 19.14 39.39Top 4 Systems in CoNLL-2014CAMB 39.71 30.10 37.33CUUI 41.78 24.88 36.79AMU 41.62 21.40 35.01POST 34.51 21.73 30.88Table 3: Performance of the pipeline, SMT,and combined systems on the CoNLL-2014 testset.
All improvements of combined systems overtheir component systems are statistically signifi-cant (p < 0.01).
The differences between P1 andS1 and between P2 and S2 are not statistically sig-nificant.ensure a fairer evaluation (i.e., without using alter-native answers).First, we can see that both the pipeline andSMT systems individually achieve relatively goodresults that are comparable with the third high-est ranking participant in the CoNLL-2014 sharedtask.
It is worth noting that the pipeline systemsonly target the seven most common error types,yet still perform well in an all-error-type setting.In general, the pipeline systems have higher recallbut lower precision than the SMT systems.The pipeline system is also sensitive to the or-der in which corrections are applied; for exampleapplying noun number corrections before articlecorrections results in a better score.
This meansthat there is definitely some interaction betweengrammatical errors and, for instance, the phrase ahouses can be corrected to a house or houses de-pending on the order of correction.We noticed that the performance of the SMTsystem could be improved by using multiple trans-lation models.
This is most likely due to domaindifferences between the NUCLE and Lang-8 cor-pus, e.g., text genres, writing style, topics, etc.Note also that the Lang-8 corpus is more than10 times larger than the NUCLE corpus, so there956is some benefit from training and weighting twotranslation tables separately.The performance of the pipeline system P1 iscomparable to that of the SMT system S1, andlikewise the performance of P2 is comparable tothat of S2.
The differences between them are notstatistically significant, making it appropriate tocombine their respective outputs.Every combined system achieves a better resultthan its component systems.
In every combina-tion, there is some improvement in precision overthe pipeline systems, and some improvement in re-call over the SMT systems.
The combination ofthe better component systems (P1+S1) is also sta-tistically significantly better than the combinationof the other component systems (P2+S2).
Com-bining all four component systems yields an evenbetter result of 39.39% F0.5, which is even betterthan the CoNLL-2014 shared task winner.
This issignificant because the individual component sys-tems barely reached the score of the third highestranking participant before they were combined.6 DiscussionIn this section, we discuss the strengths and weak-nesses of the pipeline and SMT systems, and showhow system output combination improves perfor-mance.
Specifically, we compare P1, S1, andP1+S1, although the discussion also applies to P2,S2, and P2+S2.Type performance.
We start by computing therecall for each of the 28 error types achieved byeach system.
This computation is straightforwardas each gold standard edit is also annotated witherror type.
On the other hand, precision, as men-tioned in the overview paper (Ng et al., 2014), ismuch harder to compute because systems typicallydo not categorize their corrections by error type.Although it may be possible to compute the pre-cision for each error type in the pipeline system(since we know which correction was proposed bywhich classifier), this is more difficult to do in theSMT and combined system, where we would needto rely on heuristics which are more prone to er-rors.
As a result, we decided to analyze a sampleof 200 sentences by hand for a comparatively morerobust comparison.
The results can be seen in Ta-ble 4.We observe that the pipeline system has a higherrecall than the SMT system for the following er-ror types: ArtOrDet, Mec, Nn, Prep, SVA, Vform,and Vt. Conversely, the SMT system generally hasa higher precision than the pipeline system.
Thecombined system usually has slightly lower pre-cision than the SMT system, but higher than thepipeline system, and slightly higher recall than theSMT system but lower than the pipeline system.In some cases however, like for Vform correction,both precision and recall increase.The combined system can also make use of cor-rections which are only corrected in one of thesystems.
For example, it corrects both Wformand Pform errors, which are only corrected by theSMT system, and SVA errors, which are only cor-rected by the pipeline system.Error analysis.
For illustration on how sys-tem combination helps, we provide example out-put from the pipeline system P1, SMT systemS1, and the combined system P1+S1 in Table 5.We illustrate three common scenarios where sys-tem combination helps: the first is when P1 per-forms better than S1, and the combined systemchooses the corrections made by P1, the second isthe opposite where S1 performs better than P1 andthe combined system chooses S1, and the last iswhen the combined system combines the correc-tions made by P1 and S1 to produce output betterthan both P1 and S1.7 Additional System CombinationExperimentsWe further evaluate our system combination ap-proach by making use of the corrected system out-puts of 12 participating teams in the CoNLL-2014shared task, which are publicly available on theshared task website.7Specifically, we combinedthe system outputs of the top 2, 3, .
.
.
, 12 CoNLL-2014 shared task teams and computed the results.In our earlier experiments, the CoNLL-2013test data was used as the development set.
How-ever, the participants?
outputs for this 2013 dataare not available.
Therefore, we split the CoNLL-2014 test data into two parts: the first 500 sen-tences for the development set and the remaining812 sentences for the test set.
We then tried com-bining the n best performing systems, for n =2, 3, .
.
.
, 12.
Other than the data, the experimen-tal setup is the same as that described in Sec-tion 5.5.
Table 6 shows the ranking of the par-ticipants on the 812 test sentences (without alter-7http://www.comp.nus.edu.sg/?nlp/conll14st/official submissions.tar.gz957TypePipelineSMTCombinedTPFNFPPRF0.5TPFNFPPRF0.5TPFNFPPRF0.5ArtOrDet13385419.4025.4920.381135761.1123.9146.6116302143.2434.7841.24Cit0000.000.000.000000.000.000.000000.000.000.00Mec27354338.5743.5539.471847869.2327.6953.2520471066.6729.8553.48Nn27154139.7164.2942.99523362.5017.8641.671121761.1134.3852.88Npos01000.000.000.000900.000.000.000900.000.000.00Others0100.000.000.000300.000.000.000300.000.000.00Pform0700.000.000.00150100.0016.6750.00150100.0016.6750.00Pref110118.339.098.470900.000.000.000900.000.000.00Prep12253227.2732.4328.17426180.0013.3340.00427357.1412.9033.90Rloc?416180.0020.0050.0001600.000.000.0001600.000.000.00Sfrag0100.000.000.000000.000.000.000000.000.000.00Smod0000.000.000.000000.000.000.000000.000.000.00Spar0100.000.000.000300.000.000.000200.000.000.00Srun0200.000.000.000100.000.000.000100.000.000.00Ssub01200.000.000.00112150.007.6923.81112150.007.6923.81SVA411640.0026.6736.3601400.000.000.001140100.006.6726.32Trans1150100.006.2525.0001500.000.000.0001500.000.000.00Um140100.0020.0055.560500.000.000.000500.000.000.00V00300.000.000.000330.000.000.000330.000.000.00Vform412450.0025.0041.67313260.0018.7541.67512271.4329.4155.56Vm0200.000.000.000500.000.000.000600.000.000.00Vt216166.6711.1133.3301700.000.000.0001700.000.000.00Wa0000.000.000.000000.000.000.000000.000.000.00Wci1600100.001.647.69352175.005.4521.13355175.005.1720.27Wform01100.000.000.00210250.0016.6735.71210250.0016.6735.71WOadv0000.000.000.00110100.0050.0083.330100.000.000.00WOinc0500.000.000.000410.000.000.000400.000.000.00Wtone0600.000.000.000200.000.000.000200.000.000.00Table4:Truepositives(TP),falsenegatives(FN),falsepositives(FP),precision(P),recall(R),andF0.5(in%)foreacherrortypewithoutalternativeanswers,indicatinghowwelleachsystemperformsagainstaparticularerrortype.958System Example sentenceSource Nowadays , the use of the sociall media platforms is a commonplace in our lives .P1 Nowadays , the use of social media platforms is a commonplace in our lives .S1 Nowadays , the use of the sociall media platforms is a commonplace in our lives .P1+S1 Nowadays , the use of social media platforms is a commonplace in our lives .Gold Nowadays , the use of social media platforms is commonplace in our lives .Source Human has their own rights and privacy .P1 Human has their own rights and privacy .S1 Humans have their own rights and privacy .P1+S1 Humans have their own rights and privacy .Gold Humans have their own rights and privacy .Source People that living in the modern world really can not live without the social media sites .P1 People that living in the modern world really can not live without social media sites .S1 People living in the modern world really can not live without the social media sites .P1+S1 People living in the modern world really can not live without social media sites .Gold People living in the modern world really can not live without social media sites .Table 5: Example output from three systems.System P R F0.5CUUI 44.62 27.54 39.69CAMB 39.93 31.02 37.76AMU 40.77 21.31 34.47POST 38.88 23.06 34.19NTHU 36.30 20.50 31.45RAC 32.38 13.62 25.39PKU 30.14 13.12 23.93UMC 29.03 12.88 23.21SJTU 32.04 5.43 16.18UFC 76.92 2.49 11.04IPN 11.99 2.88 7.34IITB 28.12 1.53 6.28Table 6: Performance of each participant whenevaluated on 812 sentences from CoNLL-2014test data.native answers).
Note that since we use a subset ofthe original CoNLL-2014 test data for testing, theranking is different from the official CoNLL-2014ranking.Table 7 shows the results of system combina-tion in terms of increasing numbers of top sys-tems.
We observe consistent improvements in F0.5when we combine more system outputs, up to 5best performing systems.
When combining 6 ormore systems, the performance starts to fluctu-ate and degrade.
An important observation is thatwhen we perform system combination, it is moreeffective, in terms of F0.5, to combine a handfulof high-quality system outputs than many outputs# systems P R F0.52 44.72 29.78 40.643 56.24 25.04 45.024 59.16 23.63 45.485 63.41 24.09 47.806 65.02 19.54 44.377 64.95 18.13 42.838 66.09 14.70 38.909 70.22 14.81 40.1610 69.72 13.67 38.3111 70.23 14.23 39.3012 69.72 11.82 35.22Table 7: Performance with different numbers ofcombined top systems.of variable quality.
Precision tends to increase asmore systems are combined although recall tendsto decrease.
This indicates that combining multi-ple systems can produce a grammatical error cor-rection system with high precision, which is usefulin a practical application setting where high preci-sion is desirable.
Figure 1 shows how the perfor-mance varies as the number of combined systemsincreases.8 ConclusionWe have presented a system combination ap-proach for grammatical error correction usingMEMT.
Our approach combines the outputs fromtwo of the most common paradigms in GEC: thepipeline and statistical machine translation ap-9592 4 6 8 10 12204060Number of combined systemsPerformanceP R F0.5Figure 1: Performance in terms of precision (P ),recall (R), and F0.5versus the number of com-bined top systems.proach.
We created two variants of the pipelineand statistical machine translation approaches andshowed that system combination can be used tocombine their outputs together to yield a superiorsystem.Our best combined system achieves an F0.5score of 39.39% on the official CoNLL 2014 testset without alternative answers, higher than the topparticipating team in CoNLL 2014 on this dataset.
We achieved this by using component systemswhich were individually weaker than the top threesystems that participated in the shared task.AcknowledgmentsThis research is supported by Singapore Min-istry of Education Academic Research Fund Tier2 grant MOE2013-T2-1-150.
We would like tothank Christopher Bryant for his comments on thispaper.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72.Alexandra Birch, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 9?16.Ond?rej Bojar, Milo?s Ercegov?cevi?c, Martin Popel, andOmar Zaidan.
2011.
A grain of salt for the WMTmanual evaluation.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages1?11.Chris Brockett, William B Dolan, and Michael Ga-mon.
2006.
Correcting ESL errors using phrasalSMT techniques.
In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics, pages 249?256.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28.Martin Chodorow, Joel R Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involvingprepositions.
In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions, pages 25?30.Koby Crammer, Mark Dredze, and Alex Kulesza.2009.
Multi-class confidence weighted algorithms.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages496?504.Daniel Dahlmeier and Hwee Tou Ng.
2011.
Grammat-ical error correction with alternating structure opti-mization.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics, pages 915?923.Daniel Dahlmeier and Hwee Tou Ng.
2012a.
A beam-search decoder for grammatical error correction.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages568?578.Daniel Dahlmeier and Hwee Tou Ng.
2012b.
Betterevaluation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 568?572.Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun FengNg.
2012.
NUS at the HOO 2012 shared task.
InProceedings of the Seventh Workshop on the Inno-vative Use of NLP for Building Educational Appli-cations, pages 216?224.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerEnglish: The NUS Corpus of Learner English.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 22?31.Robert Dale and Adam Kilgarriff.
2010.
Helping OurOwn: Text massaging for computational linguisticsas a new shared task.
In Proceedings of the 6th Inter-national Natural Language Generation Conference,pages 263?267.960Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In Pro-ceedings of the Seventh Workshop on the InnovativeUse of NLP for Building Educational Applications,pages 54?62.Mariano Felice, Zheng Yuan, ?istein E. Andersen, He-len Yannakoudakis, and Ekaterina Kochmar.
2014.Grammatical error correction using hybrid systemsand type filtering.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 15?24.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Michael Gamon.
2010.
Using mostly native data tocorrect errors in learners?
writing: A meta-classifierapproach.
In Proceedings of the 2010 Annual Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics, pages 163?171.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineer-ing, 12(2):115?129.Kenneth Heafield and Alon Lavie.
2010.
Combiningmachine translation output with open source: TheCarnegie Mellon multi-engine machine translationscheme.
The Prague Bulletin of Mathematical Lin-guistics, 93:27?36.Kenneth Heafield, Greg Hanneman, and Alon Lavie.2009.
Machine translation system combination withflexible word ordering.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages56?60.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics, pages 690?696.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187?197.Marcin Junczys-Dowmunt and Roman Grundkiewicz.2014.
The AMU system in the CoNLL-2014shared task: Grammatical error correction by data-intensive and feature-rich statistical machine trans-lation.
In Proceedings of the Eighteenth Confer-ence on Computational Natural Language Learn-ing: Shared Task, pages 25?33.Kevin Knight and Ishwar Chander.
1994.
Auto-mated postediting of documents.
In Proceedings ofthe Twelfth National Conference on Artificial Intel-ligence, pages 779?784.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the ACL 2007 Demo and Poster Ses-sions, pages 177?180.Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-gata, and Yuji Matsumoto.
2011.
Mining revi-sion log of language learning SNS for automatedJapanese error correction of second language learn-ers.
In Proceedings of the Fifth International JointConference on Natural Language Processing, pages147?155.Tomoya Mizumoto, Yuta Hayashibe, Mamoru Ko-machi, Masaaki Nagata, and Yuji Matsumoto.
2012.The effect of learner corpus size in grammatical er-ror correction of ESL writings.
In Proceedings ofthe 24th International Conference on ComputationalLinguistics, pages 863?872.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 shared task on grammatical error correction.In Proceedings of the Seventeenth Conference onComputational Natural Language Learning: SharedTask, pages 1?12.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 shared taskon grammatical error correction.
In Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning: Shared Task, pages 1?14.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 295?302.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics, pages 311?318.961Antti-Veikko I. Rosti, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard Schwartz, and Bonnie J.Dorr.
2007a.
Combining outputs from multiple ma-chine translation systems.
In Proceedings of the2007 Conference of the North American Chapterof the Association for Computational Linguistics,pages 228?235.Antti-Veikko I. Rosti, Spyros Matsoukas, and RichardSchwartz.
2007b.
Improved word-level systemcombination for machine translation.
In Proceed-ings of the 45th Annual Meeting of the Associationfor Computational Linguistics, pages 312?319.Alla Rozovskaya and Dan Roth.
2011.
Algorithmselection and model adaptation for ESL correctiontasks.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics, pages 924?933.Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,and Dan Roth.
2013.
The University of Illinoissystem in the CoNLL-2013 shared task.
In Pro-ceedings of the Seventeenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 13?19.Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,Dan Roth, and Nizar Habash.
2014a.
The Illinois-Columbia system in the CoNLL-2014 shared task.In Proceedings of the Eighteenth Conference onComputational Natural Language Learning: SharedTask, pages 34?42.Alla Rozovskaya, Dan Roth, and Vivek Srikumar.2014b.
Correcting grammatical verb errors.
In Pro-ceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 358?367.Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-sumoto.
2012.
Tense and aspect error correctionfor ESL learners using global context.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Short Papers, pages198?202.Joel R Tetreault and Martin Chodorow.
2008.
Theups and downs of preposition error detection inESL writing.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics,pages 865?872.Yuanbin Wu and Hwee Tou Ng.
2013.
Grammat-ical error correction using integer linear program-ming.
In Proceedings of the 51tst Annual Meet-ing of the Association for Computational Linguis-tics, pages 1456?1465.Omar Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.962
