Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 186?191,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsMultimodal Annotation of Conversational DataP.
Blache1, R. Bertrand1, B. Bigi1, E. Bruno3, E. Cela6, R. Espesser1, G. Ferr?4, M. Guardiola1, D. Hirst1,E.-P. Magro6, J.-C. Martin2, C. Meunier1, M.-A.
Morel6, E. Murisasco3, I Nesterenko1, P. Nocera5,B.
Pallaud1, L. Pr?vot1, B. Priego-Valverde1, J. Seinturier3, N. Tan2, M. Tellier1, S. Rauzy1(1) LPL-CNRS-Universit?
de Provence (2) LIMSI-CNRS-Universit?
Paris Sud(3) LSIS-CNRS-Universit?
de Toulon (4) LLING-Universit?
de Nantes(5) LIA-Universit?
d?Avignon (6) RFC-Universit?
Paris 3blache@lpl-aix.frAbstractWe propose in this paper a broad-coverageapproach for multimodal annotation ofconversational data.
Large annotation pro-jects addressing the question of multimo-dal annotation bring together many dif-ferent kinds of information from differentdomains, with different levels of granula-rity.
We present in this paper the first re-sults of the OTIM project aiming at deve-loping conventions and tools for multimo-dal annotation.1 IntroductionWe present in this paper the first results of theOTIM1 project aiming at developing conventionsand tools for multimodal annotation.
We showhere how such an approach can be applied in theannotation of a large conversational speech cor-pus.Before entering into more details, let us men-tion that our data, tools and conventions are des-cribed and freely downlodable from our website(http ://www.lpl-aix.fr/ otim/).The annotation process relies on several toolsand conventions, most of them elaborated withinthe framework of the project.
In particular, we pro-pose a generic transcription convention, called En-riched Orthographic Trancription, making it pos-sible to annotate all specific pronunciation andspeech event, facilitating signal alignment.
Dif-ferent tools have been used in order to prepareor directly annotate the transcription : grapheme-phoneme converter, signal alignment, syllabifica-tion, prosodic analysis, morpho-syntactic analysis,chunking, etc.
Our ambition is to propose a largecorpus, providing rich annotations in all the dif-1OTIM stands for Outils pour le Traitement de l?Informa-tion Multimodale (Tools for Multimodal Annotation).
Thisproject in funded by the French ANR agency.ferent linguistic domains, from prosody to gesture.We describe in the following our first results.2 AnnotationsWe present in this section some of the annota-tions of a large conversational corpus, called CID(Corpus of Interactional Data, see (Bertrand08)),consisting in 8 dialogues, with audio and video si-gnal, each lasting 1 hour.Transcription : The transcription process isdone following specific conventions derived fromthat of the GARS (Blanche-Benveniste87).
Theresult is what we call an enriched orthographicconstruction, from which two derived transcrip-tions are generated automatically : the standard or-thographic transcription (the list of orthographictokens) and a specific transcription from whichthe phonetic tokens are obtained to be used by thegrapheme-phoneme converter.From the phoneme sequence and the audio si-gnal, the aligner outputs for each phoneme itstime localization.
This aligner (Brun04) is HMM-based, it uses a set of 10 macro-classes of vowel(7 oral and 3 nasal), 2 semi-vowels and 15 conso-nants.
Finally, from the time aligned phoneme se-quence plus the EOT, the orthographic tokens istime-aligned.Syllables : The corpus was automatically seg-mented in syllables.
Sub-syllabic constituents (on-set, nucleus and coda) are then identified as wellas the syllable structure (V, CV, CCV, etc.).
Sylla-bic position is specified in the case of polysyllabicwords.Prosodic phrasing : Prosodic phrasing refersto the structuring of speech material in terms ofboundaries and groupings.
Our annotation schemesupposes the distinction between two levels ofphrasing : the level of accentual phrases (AP, (Jun,2002)) and the higher level of intonational phrases186(IP).
Mean annotation time for IPs and APs was30 minutes per minute.Prominence : The prominence status of a syl-lable distinguishes between accentuability (thepossibility for syllable to be prominent) and pro-minence (at the perception level).
In French thefirst and last full syllables (not containing aschwa) of a polysyllabic word can be prominent,though this actual realization depends on spea-kers choices.
Accentuability annotation is auto-matic while prominence annotation is manual andperceptually based.Tonal layer : Given a lack of consensus on theinventory of tonal accents in French, we choose tointegrate in our annotation scheme three types oftonal events : a/ underlying tones (for an eventualFrenchToBI annotation) ; b/ surface tones (anno-tated in terms of MOMel-Intsint protocol Hirst etal 2000) ; c/ melodic contours (perceptually anno-tated pitch movements in terms of their form andfunction).
The interest to have both manual andautomatic INTSINT annotations is that it allowsthe study of their links.Hand gestures : The formal model we use forthe annotation of hand gestures is adapted fromthe specification files created by Kipp (2004) andfrom the MUMIN coding scheme (Allwood et al,2005).
Among the main gesture types, we anno-tate iconics, metaphoric, deictics, beats, emblems,butterworths or adaptors.We used the Anvil tool (Kipp, 2004) for the ma-nual annotations.
We created a specification filestaking into account the different information typesand the addition of new values adapted to theCID corpus description (e.g.
we added a separatetrack Symmetry).
For each hand, the scheme has 10tracks.
We allowed the possibility of a gesture per-taining to several semiotic types using a booleannotation.
A gesture phrase (i.e.
the whole gesture)can be decomposed into several gesture phases i.e.the different parts of a gesture such as the prepara-tion, the stroke (the climax of the gesture), the holdand the retraction (when the hands return to theirrest position) (McNeill, 1992).
The scheme alsoenables to annotate gesture lemmas (Kipp, 2004),the shape and orientation of the hand during thestroke, the gesture space, and contact.
We addedthe three tracks to code the hand trajectory, ges-ture velocity and gesture amplitude.Discourse and Interaction : Our discourse an-notation scheme relies on multidimensional fra-meworks such as DIT++ (Bunt, 2009) and is com-patible with the guidelines defined by the SemanticAnnotation Framework (Dialogue Act) workinggroup of ISO TC37/4.Discourse units include information about theirproducer, have a form (clause, fragment, dis-fluency, non-verbal), a content and a communi-cative function.
The same span of raw data maybe covered by several discourse units playing dif-ferent communicative functions.
Two discourseunits may even have exactly the same temporal ex-tension, due to the multifonctionality that cannotbe avoided (Bunt, 2009).Compared to standard dialogue act annotationframeworks, three main additions are proposed :rhetorical function, reported speech and humor.Our rhetorical layer is an adaptation of an exis-ting schema developed for monologic written datain the context of the ANNODIS project.Disfluencies : Disfluencies are organizedaround an interruption point, which can occur al-most anywhere in the production.
Disfluencies canbe prosodic (lenghtenings, silent and filled pauses,etc.
), or lexicalized.
In this case, they appear as aword or a phrase truncation, that can be comple-ted.
We distinguish three parts in a disfluency (see(Shriberg, 1994), (Blanche-Benveniste87)) :?
Reparandum : what precedes the interruptionpoint.
This part is mandatory in all disfluen-cies.
We indicate there the nature of the inter-rupted unit (word or phrase), and the type ofthe truncated word (lexical or grammatical) ;?
Break interval.
It is optional, some disfluen-cies do not bear any specific event there.?
Reparans : the part following the break, repai-ring the reparandum.
We indicate there typeof the repair (no restart, word restart, determi-ner restart, phrase restart, etc.
), and its func-tion (continuation, repair without change, re-pair with change, etc.
).3 Quantitative informationWe give in this section some indication aboutthe state of development of the CID annotation.Hand gestures : 75 minutes involving 6 spea-kers have been annotated, yielding a total numberof 1477 gestures.
The onset and offset of gesturescorrespond to the video frames, starting from and187going back to a rest position.Face and gaze : At the present time, head move-ments, gaze directions and facial expressions havebeen coded in 15 minutes of speech yielding a to-tal number of 1144 movements, directions and ex-pressions, to the exclusion of gesture phases.
Theonset and offset of each tag are determined in theway as for hand gestures.Body Posture : Our annotation scheme consi-ders, on top of chest movements at trunk level,attributes relevant to sitting positions (due to thespecificity of our corpus).
It is based on the Pos-ture Scoring System (Bull, 1987) and the Annota-tion Scheme for Conversational Gestures (Kipp etal., 2007).
Our scheme covers four body parts :arms, shoulders, trunk and legs.
Seven dimensionsat arm level and six dimensions at leg level, as wellas their related reference points we take in fixingthe spatial location, are encoded.Moreover, we added two dimensions to describerespectively the arm posture in the sagittal planeand the palm orientation of the forearm and thehand.
Finally, we added three dimensions for legposture : height, orientation and the way in whichthe legs are crossed in sitting position.We annotated postures on 15 minutes of the cor-pus involving one pair of speakers, leading to 855tags with respect to 15 different spatial locationdimensions of arms, shoulder, trunk and legs.Annotation Time (min.)
UnitsTranscript 480 -Hands 75 1477Face 15 634Gaze 15 510Posture 15 855R.
Speech 180Com.
Function 6 229Disfluencies At the moment, this annotation isfully manual (we just developed a tool helping theprocess in identifying disfluencies, but it has notyet been evaluated).
Annotating this phenomenonrequires 15mns for 1 minute of the corpus.
Thefollowing table illustrates the fact that disfluen-cies are speaker-dependent in terms of quantityand type.
These figures also shows that disfluen-cies affect lexicalized words as well as grammati-cal ones.Speaker_1 Speaker_1Total number of words 1,434 1,304Disfluent grammatical words 17 54Disfluent lexicalized words 18 92Truncated words 7 12Truncated phrases 26 134Transcription and phonemes The followingtable recaps the main figures about the differentspecific phenomena annotated in the EOT.
To thebest of our knowledge, these data are the first ofthis type obtained on a large corpus.
This informa-tion is still to be analyzed.Phenomenon NumberElision 11,058Word truncation 1,732Standard liaison missing 160Unusual liaison 49Non-standard phonetic realization 2,812Laugh seq.
2,111Laughing speech seq.
367Single laugh IPU 844Overlaps > 150 ms 4,150Syntax We used the stochastic parser developedat the LPL (Blache&Rauzy, 2008) to automaticalygenerate morppho-syntactic and syntactic annota-tions.
The parser has been adapted it in order to ac-count for the specificities of speech analysis.
First,the system implements a segmentation technique,identifying large syntactic units that can be consi-dered as the equivalent of sentences in writtentexts.
This technique distinguishes between strongand weak or soft punctuation marks.
A second mo-dification concerns the lexical frequencies used bythe parser model in order to capture phenomenaproper to conversational data.The categories and chunks counts for the wholecorpus are summarized in the following figure :Category Count Group Countadverb 15123 AP 3634adjective 4585 NP 13107auxiliary 3057 PP 7041determiner 9427 AdvP 15040conjunction 9390 VPn 22925interjection 5068 VP 1323preposition 8693 Total 63070pronoun 25199noun 13419 Soft Pct 9689verb 20436 Strong Pct 14459Total 114397 Total 241484 EvaluationsProsodic annotation : Prosodic annotation of1 dialogue has been done by 2 experts.
Theannotators worked separately using Praat.
Inter-transcriber agreement studies were done for theannotation of higher prosodic units.
First anno-tator marked 3,159 and second annotator 2,855188Intonational Phrases.
Mean percentage of inter-transcriber agreement was 91.4% and meankappa-statistics 0.79, which stands for a quite sub-stantial agreement.Gesture : We performed a measure of inter-reliability for three independent coders for GestureSpace.
The measure is based on Cohen?s correc-ted kappa coefficient for the validation of codingschemes (Carletta96).Three coders have annotated three minutes forGestureSpace including GestureRegion and Ges-tureCoordinates.
The kappa values indicated thatthe agreement is high for GestureRegion of righthand (kappa = 0.649) and left hand (kappa =0.674).
However it is low for GestureCoordinatesof right hand (k= 0.257) and left hand (k= 0.592).Such low agreement of GestureCoordinates mightbe due to several factors.
First, the number of ca-tegorical values is important.Second, three minutes might be limited in termsof data to run a kappa measure.
Third, GestureRe-gion affects GestureCoordinates : if the coders di-sagree about GestureRegion, they are likely to alsoannotate GestureCoordinates in a different way.For instance, it was decided that no coordinatewould be selected for a gesture in the center-centerregion, whereas there is a coordinate value for ges-tures occurring in other parts of the GestureRe-gion.
This means that whenever coders disagreebetween the center-center or center region, the an-notation of the coordinates cannot be congruent.5 Information representation5.1 XML encodingOur approach consists in first precisely definethe organization of annotations in terms of typed-feature structures.
We obtain an abstract descrip-tion from which we automatically generate a for-mal schema in XML.
All the annotations are thenencoded following this schema.Our XML schema, besides a basic encoding ofdata following AIF, encode all information concer-ning the organization as well as the constraints onthe structures.
In the same way as TFS are usedas a tree description language in theories such asHPSG, the XML schema generated from our TFSrepresentation also plays the same role with res-pect to the XML annotation data file.
On the onehand, basic data are encoded with AIF, on theother hand, the XML schema encode all higherlevel information.
Both components (basic data +structural constraints) guarantee against informa-tion loss that otherwise occurs when translatingfrom one coding format to another (for examplefrom Anvil to Praat).5.2 QueryingTo ease the multimodal exploitation of the data,our objective is to provide a set of operators dedi-cated to concurrent querying on hierarchical an-notation.
Concurrent querying consists in que-rying annotations belonging to two or more mo-dalities or even in querying the relationships bet-ween modalities.
For instance, we want to be ableto express queries over gestures and intonationcontours (what kind of intonational contour doesthe speaker use when he looks at the listener ?
).We also want to be able to query temporal relation-ships (in terms of anticipation, synchronization ordelay) between both gesture strokes and lexical af-filiates.Our proposal is to define these operators as anextension of XQuery.
From the XML encodingand the temporal alignment of annotated data, itwill possible to express queries to find patterns andto navigate in the structure.
We also want to en-able a user to check predicates on parts of the cor-pus using classical criteria on values, annotationsand existing relationships (temporal or structuralones corresponding to inclusions or overlaps bet-ween annotations).
First, we shall rely on one ofour previous proposal called MSXD (MultiStruc-tured XML Document).
It is a XML-compatiblemodel designed to describe and query concurrenthierarchical structures defined over the same tex-tual data which supports Allen?s relations.6 ConclusionMultimodal annotation is often reduced tothe encoding of gesture, eventually accompa-nied with another level of linguistic information(e.g.
morpho-syntax).
We reported in this paper abroad-coverage approach, aiming at encoding allthe linguistic domains into a unique framework.We developed for this a set of conventions andtools making it possible to bring together and alignall these different pieces of information.
The resultis the CID (Corpus of Interactional Data), the firstlarge corpus of conversational data bearing richannotations on all the linguistic domains.189ReferencesAllen J.
(1999) Time and time again : The many way to re-present time.
International Journal of Intelligent Systems,6(4)Allwood, J., Cerrato, L., Dybkjaer, L., Jokinen, K., Navar-retta, C., Paggio, P. (2005) The MUMIN Multimodal Co-ding Scheme, NorFA yearbook 2005.Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P.F.
Patel-Schneider (2003) The Description Logic Hand-book : Theory, Implementation, Applications.
CambridgeUniversity Press.Bertrand, R., Blache, P., Espesser, R., Ferr?, G., Meunier, C.,Priego-Valverde, B., Rauzy, S. (2008) ?Le CID - Corpusof Interactional Data - Annotation et Exploitation Multi-modale de Parole Conversationnelle?, in revue TraitementAutomatique des Langues, 49 :3.Bigi, C. Meunier, I. Nesterenko, R. Bertrand 2010.
?SyllableBoundaries Automatic Detection in Spontaneous Speech?,in proceedings of LREC 2010.Blache P. and Rauzy S. 2008.
?Influence de la qualit?
del?
?tiquetage sur le chunking : une corr?lation d?pendant dela taille des chunks?.
in proceedings of TALN 2008 (Avi-gnon, France), pp.
290-299.Blache P., R. Bertrand, and G. Ferr?
2009.
?Creating andExploiting Multimodal Annotated Corpora : The ToMAProject?.
In Multimodal Corpora : From Models of Natu-ral Interaction to Systems and Applications, Springer.Blanche-Benveniste C. & C. Jeanjean (1987) Le fran?aisparl?.
Transcription et ?dition, Didier Erudition.Blanche-Benveniste C. 1987.
?Syntaxe, choix du lexique etlieux de bafouillage?, in DRLAV 36-37Browman C. P. and L. Goldstein.
1989.
?Articulatory ges-tures as phonological units?.
In Phonology 6, 201-252Brun A., Cerisara C., Fohr D., Illina I., Langlois D., Mella O.& Smaili K. (2004- ?Ants : Le syst?l?me de transcriptionautomatique du Loria?, Actes des XXV Journ?es d?Etudessur la Parole, F?s.E.
Bruno, E. Murisasco (2006) Describing and Querying hie-rarchical structures defined over the same textual data, inProceedings of the ACM Symposium on Document Engi-neering (DocEng 2006).Bull, P. (1987) Posture and Gesture, Pergamon Press.Bunt H. 2009.
?Multifunctionality and multidimensionaldialogue semantics.?
In Proceedings of DiaHolmia?09,SEMDIAL.B?rki A., C. Gendrot, G. Gravier & al.
(2008) ?Alignementautomatique et analyse phon?tique : comparaison de dif-f?rents syst?mes pour l?analyse du schwa?, in revue TAL,49 :3Carletta, J.
(1996) ?Assessing agreement on classificationtasks : The kappa statistic?, in Computational Linguistics22.Corlett, E. N., Wilson,John R. Manenica.
I.
(1986) ?InfluenceParameters and Assessment Methods for Evaluating BodyPostures?, in Ergonomics of Working Postures : Models,Methods and Cases , Proceedings of the First InternationalOccupational Ergonomics Symposium.Di Cristo & Hirst D. (1996) ?Vers une typologie des unites in-tonatives du fran?ais?, XXI?me JEP, 219-222, 1996, Avi-gnon, FranceDi Cristo A.
& Di Cristo P. (2001) ?Syntaix, une approchem?trique-autosegmentale de la prosodie?, in revue Traite-ment Automatique des Langues, 42 :1.Dipper S., M. Goetze and S. Skopeteas (eds.)
2007.
Informa-tion Structure in Cross-Linguistic Corpora : AnnotationGuidelines, Working Papers of the SFB 632, 7 :07FGNet Second Foresight Report (2004) Faceand Gesture Recognition Working Group.http ://www.mmk.ei.tum.de/ waf/fgnet-intern/3rd-fgnet-foresight-workshop.pdfGendner V. et al 2003.
?PEAS, the first instantiation of acomparative framework for evaluating parsers of French?.in Research Notes of EACL 2003 (Budapest, Hungaria).Hawkins S. and N. Nguyen 2003.
?Effects on word re-cognition of syllable-onset cues to syllable-coda voicing?,in Papers in Laboratory Phonology VI.
Cambridge Univ.Press.Hirst, D., Di Cristo, A., Espesser, R. 2000.
?Levels of des-cription and levels of representation in the analysis of in-tonation?, in Prosody : Theory and Experiment, Kluwer.Hirst, D.J.
(2005) ?Form and function in the representationof speech prosody?, in K.Hirose, D.J.Hirst & Y.Sagisaka(eds) Quantitative prosody modeling for natural speechdescription and generation (Speech Communication 46 :3-4.Hirst, D.J.
(2007) ?A Praat plugin for Momel and INTSINTwith improved algorithms for modelling and coding into-nation?, in Proceedings of the XVIth International Confe-rence of Phonetic Sciences.Hirst, D. (2007), Plugin Momel-Intsint.
Inter-net : http ://uk.groups.yahoo.com/group/praat-users/files/Daniel_Hirst/plugin_momel-intsint.zip,Boersma, Weenink, 2007.Jun, S.-A., Fougeron, C. 2002.
?Realizations of accentualphrase in French intonation?, in Probus 14.Kendon, A.
(1980) ?Gesticulation and Speech : Two Aspectsof the Porcess of Utterance?, in M.R.
Key (ed.
), The Re-lationship of Verbal and Nonverbal Communication, TheHague : Mouton.Kita, S., Ozyurek, A.
(2003) ?What does cross-linguistic va-riation in semantic coordination of speech and gesture re-veal ?
Evidence for an interface representation of spatialthinking and speaking?, in Journal of Memory and Lan-guage, 48.Kipp, M. (2004).
Gesture Generation by Imitation - FromHuman Behavior to Computer Character Animation.
BocaRaton, Florida, Dissertation.com.Kipp, M., Neff, M., Albrecht, I.
(2007).
An annotationscheme for conversational gestures : how to economicallycapture timing and form.
Language Resources and Eva-luation, 41(3).Koiso H., Horiuchi Y., Ichikawa A.
& Den Y.
(1998) ?An ana-lysis of turn-taking and backchannels based on prosodicand syntactic features in Japanese map task dialogs?, inLanguage and Speech, 41.McNeill, D. (1992).
Hand and Mind.
What Gestures Re-veal about Thought, Chicago : The University of ChicagoPress.McNeill, D. (2005).
Gesture and Thought, Chicago, London :The University of Chicago Press.Milborrow S., F. Nicolls.
(2008).
Locating Facial Featureswith an Extended Active Shape Model.
ECCV (4).Nesterenko I.
(2006) ?Corpus du parler russe spontan?
: an-notations et observations sur la distribution des fronti?resprosodiques?, in revue TIPA, 25.190Paroubek P. et al 2006.
?Data Annotations and Measures inEASY the Evaluation Campaign for Parsers in French?.
inproceedings of the 5th international Conference on Lan-guage Resources and Evaluation 2006 (Genoa, Italy), pp.314-320.Pierrehumbert & Beckman (1988) Japanese Tone Structure.Coll.
Linguistic Inquiry Monographs, 15.
Cambridge,MA, USA : The MIT Press.Platzer, W., Kahle W. (2004) Color Atlas and Textbook ofHuman Anatomy, Thieme.
Project MuDis.
TechnischeUniversitat Munchen.
http ://www9.cs.tum.edu/researchScherer, K.R., Ekman, P. (1982) Handbook of methods innonverbal behavior research.
Cambridge University Press.Shriberg E. 1994.
Preliminaries to a theory of speech dis-fluencies.
PhD Thesis, University of California, BerkeleyWallhoff F., M. Ablassmeier, and G. Rigoll.
(2006) ?Mul-timodal Face Detection, Head Orientation and Eye GazeTracking?, in proceedings of International Conference onMultisensor Fusion and Integration (MFI).White, T. D., Folkens, P. A.
(1991) Human Osteology.
SanDiego : Academic Press, Inc.191
