Coling 2010: Demonstration Volume, pages 37?40,Beijing, August 2010PanLex and LEXTRACT: Translating all Words of all Languages of theWorldTimothy Baldwin,?
Jonathan Pool?
and Susan M.
Colowick??
CSSEUniversity of Melbournetb@ldwin.net?
Utilika Foundation{pool,smc}@utilika.orgAbstractPanLex is a lemmatic translation re-source which combines a large number oftranslation dictionaries and other translin-gual lexical resources.
It currently covers1353 language varieties and 12M expres-sions, but aims to cover all languages andup to 350M expressions.
This paper de-scribes the resource and current applica-tions of it, as well as lextract, a neweffort to expand the coverage of PanLexvia semi-automatic dictionary scraping.1 IntroductionTranslation dictionaries, multilingual thesauri,and other translingual lexical (more precisely,lemmatic) resources answer queries of the form?Given lemma X in language A, what possibletranslations of it into language B exist??
However,existing resources answer only a small fraction ofthe potential queries of this form.
For example,one may find attested translations of the Santiagodel Estero Quichua word unta into German, En-glish, Spanish, Italian, French, Danish, Aymara,and several other Quechua languages, but not intothe other (roughly 7 thousand) languages in theworld.Answers to the vast majority of possible lem-matic translation queries must be inferred.
If untacan be translated into Spanish as lleno, and llenocan be translated into Hungarian as tele, e.g., per-haps Quichua unta can be translated into Hungar-ian as tele.
But such inference is nontrivial, be-cause lexical ambiguity degenerates the quality ofindirect translations as the paths through interme-diate languages grow longer.Current GoalResources 766 10KLanguage varieties 1353 7000Expressions 12M 350MExpression?meaning pairs 27M 1000MExpression?expression pairs 91M 1000MTable 1: Current and goal PanLex coverageThus, it appears that the quality and range oflemmatic translation would be supported by aneasily accessible graph combining a large (or, ide-ally, complete) set of translations reported by theworld?s lexical resources.
PanLex (http://panlex.org) is a project developing a publiclyaccessible graph of attested lemmatic translationsamong all languages.
As of 2010, it provides about90 million undirected pairwise translations amongabout 12 million lemmata in over 1,300 languagevarieties, based on the consultation of over 750 re-sources, as detailed in Table 1.
By 2011 it is ex-pected that the resources consulted will approxi-mately quadruple.2 The PanLex ProjectPanLex is an attempt to generate as complete aspossible a translation graph, made up of expres-sion nodes, meaning nodes, and undirected edges,each of which links an expression node with ameaning node.
Each expression is uniquely de-fined by a character string and a language.
An ex-pression ei is a translation or synonym of an ex-pression ej iff there is at least one meaning mksuch that edges v(ei,mk) and v(ej,mk) exist.
Forexample, frame in English shares a meaning withbikar in Bahasa Malay, and bikar shares a mean-ing with beaker in English, but frame shares no37meaning with beaker.
Whether ei and ej are syn-onyms or translations depends on whether theirlanguages are identical.
In Table 1, ?expres-sion?meaning pairs?
refers to edges v(e,m) and?expression?expression pairs?
refers to expres-sions with at least one meaning in common.2.1 Current Applications of PanLexWhile lemmatic translation falls short of senten-tial and discourse translation, it is not withoutpractical applications.
It is particularly usefulin author?machine collaborative translation, whenauthors are in a position to lemmatize expres-sions.
The prototype PanImages application(http://www.panimages.org), based on Pan-Dictionary, elicits a lemmatic search queryfrom the user and expands the query into dozensof languages for submission to image-search ser-vices.
Hundreds of thousands of visitors have usedit to discover relevant images labeled in languagesthey do not know, sometimes selecting particulartarget languages for cultural specificity or to craftless ambiguous queries than their own languagewould permit (Christensen et al, 2009).In lemmatic messaging applications developedfor user studies, users lemmatized sentences to tellstories or send mail across language boundaries.Evenwith context-unaware translation of lemmataproducing mostly non-optimal translations, userswere generally able to reconstruct half or more ofthe originally intended sentences (Soderland et al,2009).
The PanLex database was also used in amultilingual extension of the image-labeling gameinitiated by Von Ahn and Dabbish (2004).User and programmatic interfaces to PanLexare under development.
A lemmatic user in-terface (http://panlex.org/u) communicateswith the user in a potentially unlimited set oflanguages, with PanLex dynamically using itsown data for the localization.
A primitive APImakes it possible for developers to provide, ormake infrastructural use of, lemmatic transla-tion via PanLex.
Prototype lemmatic transla-tion services like TeraDict (http://panlex.org/demo/treng.html), InterVorto (http://panlex.org/demo/trepo.html), and T?mS?z(http://panlex.org/demo/trtur.html) ex-ploit the API.2.2 Extraction and NormalizationThe approach taken by PanLex to populate thetranslation graph with nodes and edges is a combi-nation of: (a) extraction of translation pairs fromas many translingual lexical resources as can befound on the web and elsewhere; and (b) infer-ence of new edges between expressions that existin PanLex.To date, extraction has taken the form of handwriting a series of regular expression-based scriptsfor each individual dictionary, to generate normal-ized PanLex database records.
While this is ef-ficient for families of resources which adhere toa well-defined format (e.g.
Freedict or Star-dict dictionaries), it does not scale to the longtail of one-off dictionaries constructed by lexi-cographers using ad hoc formats, as detailed inSection 2.2. lextract is an attempt to semi-automate this process, as detailed in Section 3.Inference of new translation edges is nontrivial,because lexical ambiguity degenerates the qual-ity of indirect translations as the paths throughintermediate languages grow longer.
PanDic-tionary is an attempt to infer a denser translationgraph fromPanLex combining translations frommany resources based on path redundancy, evi-dence of ambiguity, and other information (Sam-mer and Soderland, 2007; Mausam et al, 2009;Mausam et al, 2010).PanLex is more than a collection, or docbase,of independent resources.
Its value in translationinference depends on its ability to combine factsattested by multiple resources into a single graph,in which lemmata frommultiple resources that aresubstantively identical are recognized as identi-cal.
The obstacles to such integration of heteroge-neous lexical data are substantial.
They include:(1) ad hoc formatting, including format changesbetween portions of a resource; (2) erratic spacing,punctuation, capitalization, and line wrapping; (3)undocumented and non-standard character encod-ings; (4) vagueness of the distinction between lem-matic (e.g.
Rana erythraea) and explanatory trans-lations (e.g.
a kind of tree frog); and (5) absence ofconsensus for some languages as to the representa-tion of lemmata, e.g.
hyphenation and prefixationin Bantu languages, and inclusion or exclusion oftones in tonal languages.38#NAME "English-Hindi Dictionary"#INDEX_LANGUAGE "English"a[p]det.[/p][m1][trn]?
?, ??????
???????
??
???
???
???
???
; (???
??)
????
?????
????
??
?????[/trn][/m]aback[p]adv.[/p][m1][trn]????
?, ?????
; ????[/trn][/m]...?2eng-00hin-00exawcdetrex?
?...Figure 1: A snippet of an English?Hindi dictionary, in its source form (left) and as normalizedPanLexrecords (right)3 lextractlextract is a sub-project of PanLex, aimedat automating the extraction and normalizationof data from arbitrary lexical resources, focusingin the first instance on text-based resources, butultimately including XML, (X)HTML, PDF andwiki markup-based resources.
The approach takenin lextract is to emulate the manual work-flow used by the PanLex developers to scrapedata from dictionary files, namely learning of se-ries of regular expressions to convert the sourcedictionary into structured database records.
Inthis, we assume that the source dictionary hasbeen transcoded into utf-8 encoding,1 and fur-ther that the first fivePanLex translation recordsfound in the source dictionary have been handgenerated as seed instances to bootstrap the ex-traction process off, as illustrated in Figure 1.Briefly, this provides vital data including: specifi-cation of the source and target languages; manualdisambiguation of expression?expression vs. ex-pression?meaning structuring; any optional fieldssuch as part of speech; and (implicitly) where therecords start from in the source file, and whatfields in the original dictionary should not be pre-served in the PanLex database.The procedure for learning regular expressionscan be broken down into 3 steps: (1) recordmatch-ing; (2) match lattice pruning; and (3) regular ex-pression generalization.Record matching involves determining the setof codepoint spans in the original dictionary wherethe component strings (minimally the source and1We have experimented with automatic character encod-ing detection methods, but the consensus to date has beenthat methods developed for web documents, such as thechardet library, are inaccurate when applied to dictionaryfiles.target language expressions, but possibly includ-ing domain information, word class information orother metadata) encoded in the five seed recordscan be found, to use as the basis for learningthe formatting idiom employed in the dictionary.For each record, we determine all positions in thesource dictionary file where all component stringscan be found within a fixed window width of oneanother.
This is returned as a match lattice, repre-senting the possible sub-extents (?spans?)
in thesource dictionary of each record, and the loca-tion(s) of each component string within each.Match lattice pruning takes the match latticefrom the record matching step, and prunes it basedon a combination of hard and soft constraints.
Thesingle hard constraint currently used at present isthat the records must occur in the lexicon in se-quence; any matches in the lattice which violatethis constraint can be pruned.
Soft constraints in-clude: each record should span the same num-ber of lines; the fields in each record should oc-cur in the same linear order; and the width of theinter-field string(s) should be consistent.
Theseare expectations on dictionary formatting, but canbe violated (e.g.
a given dictionary may have someentries on a single line and others spanning twolines).
To avoid over-pruning the lattice, we de-termine the coverage of each such soft constraintin the form of: (a) type-level coverage, i.e.
the pro-portion of records for which a given constraint set-ting (e.g.
record size in terms of the number oflines it spans) matches with at least one recordspan; and (b) token-level coverage, i.e.
the pro-portion of individual spans a given constraint set-ting matches.
We apply soft constraints conser-vatively, selecting the soft constraint setting withfull type-level coverage (i.e.
it matches all records)39and maximum token-level coverage (i.e.
it prunesthe least edges in the lattice).
Soft constraints areapplied iteratively, as indicated in Algorithm 1.Algorithm 1 Match lattice pruning algorithm1: Initialize l .
initialize record matching match lattice2: repeat3: change?
False4: for all hi ?
H do .
update hard constraint coverage5: (htypei,htokeni)?
coverage(hi, l)6: if htokeni < 1 then .
if pruneable edges7: l?
apply(hi, l) .
apply constraint8: change?
True9: end if10: end for11: for all si ?
S do .
update soft constraint coverage12: {(stypeij,stokenij)}?
coverage(ci, l)13: end for14: if s ?
argmaxsij(?stypeij = 1.0 ?
stoken < 1.0 ?(?i?
6= i : |stypei?| > 1, ?
j?
: stokenij < 1.0 : stokenij >stokeni?j?))
then15: l?
apply(s, l) .
apply constraint16: change?
True17: end if18: until change = FalseThe final step is regular expression generaliza-tion, whereby the disambiguated match lattice isused to identify the multiline span of all recordsin the source dictionary, and inter-field strings notcorresponding to any record field are generalizedacross records to form a regular expression, whichis then applied to the remainder of the dictionary toextract out normalized PanLex records.
As partof this, we build in dictionary-specific heuristics,such as the common practice of including optionalfields in parentheses.The lextract code is available from http://lextract.googlecode.com.lextract has been developed over 10 sampledictionaries, and record matching and match lat-tice pruning has been found to perform with 100%precision and recall over the seed records.
We arein the process of carrying out extensive evaluationof the regular expression generalization over fulldictionary files.Future plans for lextract to get closer totrue emulation of themanual extraction process in-clude: dynamic normalization of target languagestrings (e.g.
normalizing capitalization or correct-ing inconsistent pluralization) using a combina-tion of language-specific tools for high-densitytarget languages such as English, and analysis ofexisting PanLex expressions in that language;elicitation of user feedback for extents of the doc-ument where extraction has failed, fields wherethe correct normalization strategy is unclear (e.g.normalization of POS tags not seen in the seedrecords, as for det.
?detr in Figure 1); and extend-ing lextract to handle (X)HTML and other filetypes.ReferencesChristensen, Janara, Mausam, and Oren Etzioni.
2009.A rose is a roos is a ruusu: Querying translations forweb image search.
In Proc.
of the Joint conferenceof the 47th Annual Meeting of the Association forComputational Linguistics and the 4th InternationalJoint Conference on Natural Language Processingof the Asian Federation of Natural Language Pro-cessing (ACL-IJCNLP 2009), pages 193?196, Sun-tec, Singapore.Mausam, Stephen Soderland, Oren Etzioni, DanielWeld, Michael Skinner, and Jeff Bilmes.
2009.Compiling a massive, multilingual dictionary viaprobabilistic inference.
In Proc.
of the Joint con-ference of the 47th Annual Meeting of the Asso-ciation for Computational Linguistics and the 4thInternational Joint Conference on Natural Lan-guage Processing of the Asian Federation of NaturalLanguage Processing (ACL-IJCNLP 2009), pages262?270, Suntec, Singapore.Mausam, Stephen Soderland, Oren Etzioni, Daniel S.Weld, Kobi Reiter, Michael Skinner, Marcus Sam-mer, and Jeff Bilmes.
2010.
Panlingual lexicaltranslation via probabilistic inference.
Artificial In-telligence, 174(9?10):619?637.Sammer, Marcus and Stephen Soderland.
2007.
Build-ing a sense-distinguished multilingual lexicon frommonolingual corpora and bilingual lexicons.
InProc.
of the Eleventh Machine Translation Summit(MT Summit XI), pages 399?406, Copenhagen, Den-mark.Soderland, Stephen, Christopher Lim, Mausam,Bo Qin, Oren Etzioni, and Jonathan Pool.
2009.Lemmatic machine translation.
In Proc.
of Ma-chine Translation Summit XII, page 2009, Ottawa,Canada.Von Ahn, Luis and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proc.
of the SIGCHIconference on Human factors in computing systems,pages 319?326, Vienna, Austria.40
