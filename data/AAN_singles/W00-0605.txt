A Question Answering System Developed as a Project in aNatural Language Processing Course*W. Wang, J. Auer, R. Parasuraman, I. Zubarev, D. Brandyberry, and M. P. HarpmPurdue UniversityWest Lafayette, IN 47907{wang28,jauer,pram,dbrandyb,harper}@ecn.purdue.edu an  zubarevi@cs.purdue.eduAbstractThis paper describes the Question Answering Sys-tem constructed uring a one semester graduate-level course on Natural Language Processing (NLP).We hypothesized that by using a combination ofsyn-tactic and semantic features and machine learningtechniques, we could improve the accuracy of ques-tion answering on the test set of the Remedia corpusover the reported levels.
The approach, althoughnovel, was not entirely successful in the time frameof the course.1 IntroductionThis paper describes a preliminary reading com-prehension system constructed as a semester-longproject for a natural language processing course.This was the first exposure to this material forall but one student, and so much of the semesterwas spent learning about and constructing the toolsthat would be needed to attack this comprehen-sive problem.
The course was structured aroundthe project of building a question answering systemfollowing the HumSent evaluation as used by theDeep Read system (Hirschman eta\]., 1999).
TheDeep Read reading comprehension prototype system(Hirschman et al, 1999) achieves a level of 36% ofthe answers correct using a bag-of-words approachtogether with limited linguistic processing.
Since theaverage number of sentences per passage is 19.41,this performance is much better than chance (i.e.,5%).
We hypothesized that by using a combina-tion of syntactic and semantic features and machinelearning techniques, we could improve the accuracyof question answering on the test set of the Remediacorpus over these reported levels.2 System DescriptionThe overall architecture of our system is depictedin Figure 1.
The story sentences and its five ques-tions (who, what, where, when, and why) are firstpreprocessed and tagged by the Brill part-of-speech* We wou ld  l ike to  thank  the  Deep Read group for giving us" access  to  the i r  test bed.P la in  Text  ( Story and Questions ))i .
.
.
.
.
8__~'!P?_s__!,g_ ~L  .
.
.
.
.
.
.Tagged Text2 i Name IdentificationPropernoun Identified3 Tagged TextJ r _  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
T .
.
.
.
.
.
.
.
.
.
.Word Lex\]cat Lexica$ and Ro le  ?
In fonmat io r  L. .
.
.
.
.
.
.
Labe l ln fo rmal J~1 ~ .
.
.
.. .
.
.
~ Lex icon  .
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
L .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Wordnel .
.
.
.
.
.
.
.
.
.
.
Grammar Par t ia l  ~ : P ronounReso lu t ion  .
.
.
.
.
.
.
- .
.
.
.
.
.
.
.
.
.
.
.
Rules Parser 4- - -  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
-Gramnrmr  .
.
.
.
.
.
~ Pronouns  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ResolvedSentence-to-Question.
I, Compar i son  i.
- -~=: :  .
.
.
.
.
~__  .
.
~ .
.
.
.
.
.
.
~" Ru le -Based  " Neuro -  : Neura l  Genet icClassifier .'
Fuzzy  Net .
Network Algorithm .
'?
?
.ANS ANS ANS ~ - -  ~"  ANSVot ing.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Answer with iHighest Scores  iFigure 1: The architecture for our question answer-ing system.
(POS) tagger distributed with the Deep Read sys-tem.
This tagged text is then passed to the NameIdentification Module, which updates the tags ofnamed entities with semantic information and gen-der when appropriate."
The Partial Parser Mod-ule then takes this updated text and breaks it intophrases while attempting to \]exically disambiguatethe text.
The Pronoun Resolution Module is con-sulted by the parser in order to resolve pronouns be-fore passing partially parsed sentences and questionsto the Sentence-to-Question Comparison Module.The Comparison Module determines how stronglythe phrases of a sentence are related to those of aquestion, and this information is passed to several28modules which attempt to learn which features ofthe comparison are the most important for identify-ing whether a sentence is a strong answer candidate.We intended to set up a voting scheme among vari-ous modules; however, this part of the work has notbeen completed (as indicated by the dashed lines).Our system, like Deep Read, uses as the develop-ment set 28 stories from grade 2 and 27 from grade5, each with five short answer questions (who, what,when, where, and why), and 60 stories with ques-tions from grades 3 and 4 for testing 1.
We will referto the development and testing data as the Remediacorpus.
The following example shows the informa-tion added to a plain text sentence as it progressesthrough each module of the system we have created.Each module-is described in more detail in the fol-lowing sections.2.1 Name Ident i f i ca t ion  Modu leThe Name Identification Module expects as in-put a file that has been tagged by the Brilltagger distributed with the Deep Read system.The most important named entities in the Re-media corpus are the names of people and thenames of places.
To distinguish between thesetwo types, we created dictionaries for names ofpeople and names of places.
The first and lastname dictionaries were derived from the filesat http ://www.
census, gov/genealogy/names/.First names had an associated gender feature;names that were either male or female includedgender frequency.
Place names were extracted fromatlases and other references, and included namesof countries, major cities and capital cities, majorattractions and parks, continents, etc.
WordNetwas also consulted because of its coverage of placenames.
There are 5,165 first name entries, 88,798last name entries, and 1,086 place name entries inthe dictionaries used by this module.The module looks up possible names to decidewhether a word is a person's name or a location.
If itcannot find the word in the dictionaries, it then looksat the POS tags provided in the input file to deter-mine whether or not it is a propernoun.
Heuristics(e.g., looking at titles like Mr. or word endings likerifle) are then applied to decide the semantic typeof the propernoun, and if the type cannot be deter-mined, the module returns both person and locationas its type.
The accuracy of the Name IdentificationModule on the testing set was 79.6%.
The accuracyadjusted to take into account incorrect tagging was83.6%.There were differences between the Deep Read electronicversion of the passages and the Remedia  published passages.We used the electronic passages.29Rm3-5  (...
The club is fo r  boys who are under 12 years Bid.)
They are called Cub ScButs.
(Answer to Question 1 ): POS taggingThey are called i Cub ScoutsPO_S _-.~_P.R.
p_: P_O_ S_=_~V_B P" ' P_OS='__V_BN'_~ i_POS='N_N_ P21 POS_--.
:NN__S"Name IdentificationThey are ~ i called Cub Scouts'Pos.
't~s" :POS="PRP"  POS='VBP"  " POS='VBN"  PROP~PE.
'~tTheyTYPE=NPID=ILABEL=subjectBASE=theyAGR=3pGENDER=maleSEM_TYPE=per~ .z~,areTYPE=VPID=ILABEL=auKBASE=beAGR~,~pTENSE=presentVOtCE=ac~,'eSEM TYPE=tanThey are"P#PE=NP TYP~=VP11:>-i io=1IL'~'BE L=subjec~ LABEL~~',SElthey BAS.~MceAGR=3p , AGR.~3 pGENDER=nut  ~1 : TENBE=pQI=~I~BEM TYPE=per  ' VOI CE,=activePRONRE F=boys ' = _S E_M TYF~i=b~-pu~.EThey  areTYPE=NP TYPE.V'PID=I IB=~LABEL=subject LABEL=amBASE-beBASE=boy AGR=3pAGFt=3g TE NSIE.pms4mtGENDER=male voICE~ctive?
SEM_ ~PE=~.~ers~ __.
SE M "i'~f P _Ept?e-p~_Who: ~ 'V~?-  .
.
.
.
.
.
.
.
.ID=I, LABEL=subject= BABE=who: AGR=3p: GENDER=maleInitial Partial Parsingcalled \]TYPE.VP !ID..2Lt~,B EL .
rnvbBASE.,call !AGR.3pTENSE=pastp iVOICE=gas=ireSEM_TYPEiequat elPronoun ResolutionYcal ledTYPE=VP~O.2LABEL.nwbBASE=callAGFI=3pTENSE=pastpVOICE=pass)reL SEM TYPE=equateUpdating Featurescalled !TYPE=VP1\[:),=2LABEL=mvb iBASE~anAGRB3pTENSE~p~tpVOICE=passive_ SEU='r~'Pe=~q2~:Bracketed SentenceareTypE~.VPJD.ILABEL~vb !BABE.beAGR,,3p iTENSE=,preserdYOICE~ct i~ ,?
s E_M , .TY~,~Bracketed Question 1Cub ScoutsTYPE=NP1\[:)=2LABEL=objectBASE =CubScoutsAGR=3pGENDER=maleSEM ~__PE-~_ersonCub ScoutsTYPE=NPID=2LAeEL,=objoctBASE=CubScoutsAGR=3p !GENDER.mate ,_, SEM-_UPE=~__~Cub Scouts 'TYPE=NPID=2LABEL=objectBASE=CubSco~t= 'AGR=3pGENDER=mak~ ;Cub  ScoutsTYPE=NPID=2LABEL=object8ABE=CubScout= iAGR=3pGENDER=male.
.
.
.
SE_M= D'~,E:--p. ~.Figure 2: Processing an example sentence for match-ing with a question in our system.2.2 Par t ia l  Parser  Modu leThe Partial Parser Module follows sequentially af-ter the Name Identification Module.
The input isthe set of story sentences and questions, such thatthe words in each are tagged with POS tags andthe names are marked with type and gender infor-mation.
Initially pronouns have not been resolved;the partial parser provides segmented text with richlexical information and role labels directly to thePronoun Resolution Modffle.
After pronoun reso-lution, the segmented text with resolved pronounsis returned to the partial parser for the parser toupdate the feature values corresponding to the pro-nouns.
Finally, the partial parser provides bracketedtext to the Comparison Module, which extracts fea-tures that will be used to construct modules for an-swering questions.The Partial Parser Module utilizes informationin a lexicon and a grammar to provide the partialparses.
The lexicon and the parser will be detailedin the next two subsections.2.2.1 The  Lex iconThere were two methods we used to construct helexicon: open lexicon, which includes all wordsfrom the development set alng with all determiners,pronouns, prepositions, particles, and conjunctions(these words are essential to achieving ood sentencesegmentation), and closed lexicon,  which includesall of the development and testing words 2.
We con-structed the closed lexicon with the benefit of thedevelopment corpus only (i.e., we did not consult thetest materials to design the entries).
To improve cov-erage in the case of the open lexicon, we constructeda module for obtaining features fbr words that donot appear in the development set (unknown words)that interfaces with WordNet to determine a word'sbase/stem, semantic type, and synonyms.
When anunknown word has multiple senses, we have opted tochoose the first sense because WordNet orders sensesby frequency of use.
Ignoring numbers, there are1,999 unique words in the development set of theRemedia corpus, and 2,067 in the testing data, ofwhich 1,008 do not appear in the development set.Overall, there are 3,007 unique words across bothtraining and testing.One of our hypotheses was that by creating a lex-icon with a rich set of features, we would improvethe accuracy of question answering.
The entries inthe lexicon were constructed using the conventionsadopted for the Parsec parser (Harper and Helzer-man, 1995; Harper et al, 1995; Harper et al, 2000).Each word entry contains information about its rootword (if there is one), its lexical category (or cate-gories) along with a corresponding set of allowablefeatures and their corresponding values.
Lexical cat-egories include noun, verb, pronoun, propernoun,adjective, adverb, preposition, particle, conjunction,determiner, cardinal, ordinal, predeterminer, nounmodifier, and month.
Feature types used in thelexicon include subcat,  gender, agr, case, vtype(e.g., progressive), mood, gap, inver ted ,  voice,behav ior  (e.g., mass), type (e.g., interrogative, rel-ative), semtype, and con j type  (e.g., noun-type,verb-type, etc.).
We hypothesized that semtypeshould play a significant role in improving questionanswering performance, but the choice of semanticgranularity is a difficult problem.
We chose to keepthe number of semantic values relatively small.
Byusing the lexicographers' files in WordNet to groupthe semantic values, we selected 25 possible seman-tic values for the nouns and 15 for the verbs.
A2Initially, we created the closed lexicon because this listof words was in the Deep Read materials.
Once we spottedthat the list contained words not in the development material,we kept it as an alternative to see how important full lexicalknowledge would be for answering questions.30script was created to semi-automate he construc-tion of the lexicon from information extracted frompreviously existing dictionaries and from WordNet.2.2.2 The  Par t ia l  ParserThe parser segments each sentence into either a nounphrase (NP), a verb phrase (VP), or a prepositionalphrase (PP), each with various feature sets.
NPshave the feature types: Base (the root word of thehead word of the NP), AGR (number/person i for-mation), SemType (the semtype of the root form inthe lexicon, e.g., person, object, event, artifact, or-ganization), Label (the role type of the word in thesentence, e.g., subject), and Gender.
Verb phrases(VPs) have the feature types: Base, AGR, SemType(the semtype of the root form in the lexicon, e.g.,contact, act, possession), Tense (e.g., present, past),and Voice.
Prepositional phrases (PPs) have thefeature types: Prep (the root form of the prepositionword), SemType (the semtype of the root form in thelexicon, e.g., at-loc, at-time), Need (the object of thepreposition), and NeedSemType (the semtype of theobject of the preposition).
Feature values are as-signed using the lexicon, Pronoun Resolution Mod-ule, and grammar ules.We implemented a bottom-up partial parser tosegment each sentence into syntactic subparts.
Thegrammar used in the bottom-up arser is shown be-low:1.
NP -+ DET ADJ+ NOUN+2.
NP ~ DET NOUN3.
NP ~ ADJ PROPERNOUN+4.
VP ~ (AUX-VERB) MAIN-VERB5.
PP --~ ADV6.
PP ~ ADJ (PRED)7.
PP ~ PREP NPAt the outset, the parser checks whether there areany punctuation marks in the sentence, with corn-mass and periods being the most helpful.
A commais used in two ways in the Remedia corpus: it actsas a signal for the conjunction of a group of nounsor propernouns, or it acts as punctuation signallingan auxiliary phrase (usually a PP) or sentence.
Inthe NP conjunction case, the parser groups the con-joined nouns or propernouns together as a plural NP.In the second case, the sentence is partially parsed.The partial parser operates in a bottom-up fashiontaking as input a POS:tagged and name-identifiedsentence and matching it to the right-hand side ofthe grammar ules.
Starting from the beginning ofthe sentence or auxiliary phrase (or sentence), theparser looks for the POS tags of the words, trans-forming the POS tags into corresponding lexical cat-egories and tries to match the RHS of the rules.Phrases are maintained on an agenda until they arefinalized.NPs often require merging sincesome consecutiveNPs form a single multi-word token (i.e., multi-wordnames and conjunctions).
An NP that results frommerging two tokens into a single multi-word tokenhas its Base as the rootword of the combined token,and AGR and SemType features are updated accordingto the information retrieved from the lexicon basedon the multi-word token.
In the case of an NP con-junction, the Base is the union of the Base of eachNP, AGR is set to 3p, and SemType is assigned as thatof the head word of the merged NP.
The rule for find-ing the head word of an NP is: find the F IRST  con-secutive noun (propernoun) group in the NP, thenthe LAST  noun (propernoun) in this group is de-fined as the head word of the NP.The partial parser performs word-sense disam-biguation as it parses.
Words such as Washingtonhave multiple_semtype values in the lexicon for onelexical category.
The following are rules for word-sense disambiguation used by the parser:?
NP  plus VP  rules for word-sense disambiguation:If there are verbs such as name, call, or be,which have the semtype of equate, then the NPsthat precede and follow the VP  have the samesemtype.If a noun is the object of a verb, then the subcatfeature value of the verb can be used to disam-biguate its word sense (e.g., take generally hasthe subcat  of obj+time).?
PP  rules for word-sense disambiguation:For some nouns (propernouns) which are theobject of a preposition, the intersection of thesemtype value sets of the preposition word andits object determines their semtype.?
NPs  in the date line of each passage are all ei-ther dates or places with the typical order be-ing place then time.
For example, in (WASH-INGTON, June, 1989), Washington is assignedsemtype of location rather than person.To process unknown words (the 1,008 words in thetesting set that don't appear in the development set)in the case of the open lexicon, WordNet is used toassign the semtype feature for nouns and verbs, theAGR feature for verbs can be obtained in part fromthe POS tag, and AGR for unknown noun words canbe determined when they are used as the subject ofa sentence.
For the closed lexicon, the only unknownwords are numbers.
If a number is a four-digit num-ber starting with 16 to 19 or is followed by A.D orB.C.
then generally it is a year, so its semtype is de-fined as time.
Other numbers tend to be modifiersor predicates and have the semtype of num.2.3 P ronoun Reso lu t ion  Modu leA pronoun resolution module was developed usingthe rules given in Allen's text (Allen, 1995) alongwith other rules described in the work of Hobbs(Hobbs, 1979).
The module takes as input thefeature-augmented and segmented text provided bythe partial parser.
Hence, the words are marked31with lexical (including gender) and semantic featureinformation, and the phrase structure is also avail-able.
After the input file is provided by the Par-tial Parser Module, the Pronoun Resolution Modulesearches for the pronouns by looking through theNPs identified by the partial parser.
Candidate an-tecedents are identified and a comparison of the fea-tures is made between the pronoun and the possibleantecedent.
The phrase that passes the most rulefilters is chosen as the antecedent.
First and secondperson pronouns are handled by using default val-ues (i.e., writer and reader).
If the system fails toarrive at an antecedent, he pronoun is marked asnon-referential, which is often the case for pronounslike it or they.
Some of the most useful rules arelisted below:?
Reflexives must refer to an antecedent in the samesentence.
For simplicity, we chose the closestnoun preceding the pronoun in the sentence withmatching Gender, AGR, and SemType.?
Two NPs that co-refer must agree in AGR, Gender,and SemType (e.g., person, location).
Since, inmany cases the gender cannot be determined, thisinformation was used only when available.?
A subject was preferred over the object when thepronoun occurred as the subject in a sentence.?
When it occurs in the beginning of a paragraph,it is considered non-referential.?
We prefer a global entity (the first named entity ina paragraph) when there is a feature match.
In theabsence of such, we prefer the closest propernounpreceding the pronoun with a feature match.
Ifthat fails, we prefer the closest preceding noun orpronoun with a feature match.The accuracy of our pronoun resolution moduleon the training corpus was 79.5% for grade 2 and79.4% for grade 5.
On testing, it was 81.33% forgrade 3 and 80.39% for grade 4.
The overall accu-racy of this module on both the testing and train-ing corpus was 80.17%.
This was an improvementover the baseline Deep Read coreference modulewhich achieved a 51.61% accuracy on training anda 50.91% accuracy on testing, giving an overall ac-curacy of 51.26%.
This accuracy was determinedbased on Professor Harper's manual pronoun reso-lution of both the training and testing set (the per-fect coreference information was not included in thedistribution of the Deep Read system).2.4 Sentence- to -Quest ion  Compar i sonModu leThe Sentence-to-Question Comparison Moduletakes as input a set of tagged stories, for whichphrase types and features have been identified.The semantic and syntactic information is coded asshown in Figure 2 (using XML tags).
A mechanismto quantify a qualitative comparison of questionsand sentences has been developed.
The comparison:.'.
'provides data about how questions compare to theiranswers and how questions compare to non-answers.The classification of answers and non-answers i im-plemented by using feature comparison vectors ofphrase-to-phrase comparisons in questions and po-tential answer sentences.A comparison is made using phrase-to-phrasecomparisons between each sentence and each ques-tion in a passage.
In particular, NP-to-NP, VP-to-VP, PP-to-PP, and NP-to-PP comparisons are madebetween each sentence and each of the five questions.These comparisons are stored for each sentence inthe following arrays.
Note that in these arrays Qvaries from 1 to 5, signifying the question that thesentence matches.
F varies over the features for thephrase match.CN\[Q\]\[F\] Comparison of NP features (F = I{Base,h6R, and SemType}D between questionQ and the sentence.CV\[Q\]\[F\] Comparison of VP features (F = i{Base,AGR, SemType, Tense}\[) betweenquestion Q and the sentence.CP\[Q\]\[F\] Comparison of PP features (F = \[{NeedBase,Prep, PPSemType, NeedSemType}\[)between question Q and the sentence.CPN\[Q\]\[F\] Comparison of PP features in sentenceto NP features in question Q.
Here F=2,comparing lWeedBase and Base, andNeedSemType and SemType.Values for these comparison matrices were calcu-lated for each sentence by comparing the features ofeach phrase type in the sentence to features of theindicated phrase types in each of the five questions.The individual matrix values describe the compari-son of the best match between a sentence and a ques-tion for NP-to-NP (the three feature match scoresfor the best matching NP pair of the sentence andquestion Q are stored in CN\[Q\]), VP-to-VP (storedin CV\[Q\]), PP-to-PP (stored in CP\[Q\]), and PP-to-NP (store in CPN\[Q\]).
Selecting the phrase compar-ison vector for a phrase type that best matches asentence phrase to a question phrase was chosen asa heuristic to avoid placing more importance on asentence only because it contains more information.Comparisons between features were calculated us-ing the following equations.
The first is used whencomparing features such as Base, NeedBase, andPrep, where a partial match must be quantified.The second is used when comparing features uch asSemType, AGR, and Tense where only exact matchesmake sense.1 if Strl = Str2rain len~th(Strl,Str2) length(Sth)  # length(Str2)c = max length(Strl,Str2) A(Strl 6 Str2 V Str2 6 Strl )0 if Strl -~ Str21 if Strl = Str2c = 0 if Strl ~- Str232The matrices for the development set were pro-vided to the algorithms in the Answer Module fortraining the component answer classifiers.
The ma-trices for the testing set were also passed to the al-gorithms for testing.
Additionally, specific informa-tion about the feature values for each sentence waspassed to the Answer Module.2.5 Answer  Modu lesSeveral methods were developed in parallel in anattempt o learn the features that were central toidentifying the sentence from a story that correctlyanswer a question.
These methods are described inthe following subsections.
Due to time constraints,the evaluations of these Answer Modules were car-ried out with a closed lexicon and perfect pronounresolution.2.5.1 A Neuro--Fuzzy Network  Classif ierAn Adaptive Network-based Fuzzy Inference System(ANFIS) (Jang, 1993) from the Matlab Fuzzy LogicToolbox was used as one method to resolve the storyquestions.
A separate network was trained for eachquestion type in an attempt o make the networkslearn relationships between phrases that classify an-swer sentences and non-answer sentences differently.ANFIS has the ability to learn complex relationshipsbetween its input variables.
It was expected thatby learning the relationships in the training set, theresolution of questions could be performed on thetesting set.For ANFIS, the set of sentence-question pairs wasdivided into five groups according to question type.Currently the implementation of ANFIS on Matlabis restricted to 4 inputs.
Hence, we needed to devisea way to aggregate the feature comparison informa-tion for each comparison vector.
The comparisonvectors for each phrase-to-phrase comparison werereduced to a single number for each comparison pair(i.e., NP-NP, VP-VP, PP-PP, NP-PP).
This reduc-tion was performed by multiplying the vector valuesby a normalized weighting constant for the featurevalues (e.g., NP-comparison = (Base weight)*(Basecomparison value) + (AGR weight)*(AGR compari-son value) + (SemType weight)*(SemType compari-son value), with the weights umming to 1).
In mostcases that a match is found, the comparison valuesare 1 (exact match).
So weights were chosen that al-lowed the ANFIS to tell'something about the matchcharacteristics (e.g., if the AGR weight is 0.15 andthe SemType weight is 0.1, and the NP-comparisonvalue was 0.25, it can be concluded that the NPthat matched best between in the sentence-questionpair had the same AGR and SemType features).
Theaggregation weights were chosen so that all com-binations of exact matches on features would haveunique values and the magnitude of the weights werechosen based on the belief that the higher weightedfeatures contribute more useful information.
Theweights, ordered to correspond to the features in thetable on the previous page are: (.55, .15, .3) for CN,(.55, .1, .22, .13) for CV, (.55, .15, .2, .1) for CP, and(.55, .45) for CPN.ANFIS was trained using the update on the de-velopment set provided by the Sentence-to-QuestionComparison Module as described above.
Duringtesting, the data, provided by the Comparison Mod-ule and updated as described above, is used as inputto ANFIS.
The output is a confidence value that de-scribes the likelihood of a sentence being a answer.Every sentence is compared with every question inANFIS, and then within question, the sentences areranked by the likelihood that they are a question'sanswer.The accuracy of the best classifier produced withANFIS was quite poor.
In the grade 3 set, weachieved an accuracy of 13.33% on who questions,6.67% on what questions, 0% on where questions,6.67% on when questions, and 3.33% on why ques-tions.
In the grade 4 set, we achieved an accuracy of3.54% on who questions, 10.34% on what questions,10.34% on where questions, 0% on when questions,and 6.9% on why questions.
Although the best rank-ing sentence produced poor accuracy results on thetesting set, with some additional knowledge the top-ranking incorrect answers may be able to be elimi-nated.
The plots in Figure 3 display the number oftimes the answer sentence was assigned a particularrank by ANFIS.
The rank of the correct sentencetends to be in the top 10 fairly often for most ques-tion types.
This rank tendency is most noticeablefor who, what and when questions, but it is alsopresent for where questions.
The rank distributionfor why questions appears to be random, which isconsistent with our belief that they require a deeperanalysis than would be possible with simple featurecomparisons.2.5.2 A Neura l  Network  Classi f ierLike ANFIS, this module uses a neural network, butit has a different opology and uses an extended fea-ture set.
The nn (Neureka) neural network sim-ulation system (Mat, 1998) was used to create amulti-layer (one hidden layer) back-propagation net-work.
A single training/testing instance was gener-ated from each story sentence.
The network containsan input layer with two groups of features.
The sen-tence/question feature vectors that compare a sen-tence to each of the five story questions comprise thefirst group.
Sentence features that are independentof the questions, i.e., contains a location, contains atime/date, and contains a human, comprise the sec-ond group.
The hidden layer contains a number ofnodes that was experimentally varied to achieve bestperformance.
The output layer contains five nodes,each of which has a binary outpht value which indi-cates whether or not the sentence is the answer tothe corresponding question (i.e., question 1 through5).Several training trials were performed to deter-mine the opt imum parameters for the network.
Wetrained using various subsets of the full input fea-ture set since some features could be detrimental tocreating a good classifier.
However, in the end, thefull set of features performed better than or equiva-lently to the various subsets.
Increasing the numberof hidden nodes can often improve the accuracy ofthe network because it can learn more complex re-lationships; however, this did not help much in thecurrent domain, and so the number of hidden nodeswas set to 16.
For this domain, there are many moresentences that are not the answer to a question thanthat are.
An effort was made to artificially changethis distribution by replicating the answer sentencesin the training set; however, no additional accuracywas gained by this experimentation.
Finally, we cre-ated a neural network for each question type as inANFIS; however, these small networks had lower ac-curacy than the single network approach.The overall test set accuracy of the best neuralnetwork classifier was 14%.
In the grade 3 set, weachieved an accuracy of 30% on who questions, 0%on what questions, 23.3% on when questions, 13.3%on where questions, and 3.3% on why questions.
Inthe grade 4 set, we achieved an accuracy of 17.2%on who questions, 10.3% on what questions, 23.6%on when questions, 10.3% on where questions, and3.4% on why questions.2.5.3 A Ru le -based  Classi f ier  based  on C5.0We attempted to learn rules for filtering out sen-tences that are not good candidates as answers toquestions using C5.0 (Rul, 1999).
First we ex-tracted information from the sentence-to-questioncorrespondence data ignoring the comparison valuesto make the input C5.0-compatible, and producedfive different files (one for each question type).
Thesefiles were then fed to C5.0; however, the program didnot produce a useful tree.
The problem may havebeen that most sentences in the passages are nega-tive instances of answers to questions.2.5.4 GASGAS (Jelasity and Dombi, 1998) is a steady geneticalgorithm with subpopulation support.
It is capa-ble of optimizing functions with a high number oflocal optima.
The initial parameters were set theo-retically.
In the current matching problem, becausethe number of local opt ima can be high due to thecoarse level of sentence information (there can beseveral sentence candidates with very close scores),this algorithm is preferred over other common ge-netic algorithms.
This algorithm was trained on thetraining set, but due to the high noise level in the33Who Questions~2 '~t <-~ :~ ~.
~:~" ; ~ .~ ~ .~ ~~ ~.
,~!
-  :-::~ i~',~ .
'.
.
.
.
.
.
.
, t .
.
.
,~  ~ :~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ ,  " - :  .
.
.
.
.
.
.
.
.
.
.
.
:~ .......N-best order of sentencesWhat  Questions t2N-best order of sentencesWhen Questions18F~ 6.~- ~ ~g 4 ~, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ ~ ,~ .
.
.
.
.
.
.
.
.
.
:~  ~N-best order of sentencesWhere Questions 10 \ [ ;p~?
,~z~, : : : : : : :~ ,~, :~: :~.~:~- :~:~:~: ,~.~?
.
: :~f f : , :~N-best order of sentencesWhy Questions4.5 ~i~:~:~0N-best order of sentencesFigure 3: Correct answers ordered by ANFIS preference.training data, the algorithm fails to produce a win-ning population based on the mean square minimiza-tion function.3 A Closer Look at the FeaturesAfter observing the question answer accuracy resultsof the above classifiers, we concluded that the fea-tures we extracted for the classifiers are affected bynoise.
The fact that we take into consideration onlythe top matching phrase-to-phrase matches on a spe-cific set of features may have contributed to thisnoisiness.
To analyze the noise source of features,given that SemType was hypothesized to be essen-tial for answer candidate discrimination, we exam-ined those SemType values that occurred most fre-quently and calculated statistics on how often thevalues occurred in story sentences that are answersversus non-answers to the questions.
We observedthe following phenomena:1.
For who questions, the SemType value personplays an important role in ?identifying answersentences, since 83.64% answers have person asits NP SemType value, and 21.82% have it asits PP NeedSemType value.
However, 66.83% ofthe non-answer sentences also have person as itsNP SemType and 15.85% as its PP NeedSemType.Phrases with person SemType appear in most sen-tences, whether they are answers or not, and this34weakens its ability to act as an effective filter.2.
For what questions, the SemType value person ap-pears as the NP SemType of most answer and non-answer sentences.
The next most dominant fea-ture is the SemType value object, which appearsin the NP for 29.41% of the answer sentences andPP NeedSemType for 15.68% of the answer sen-tences.
Most of the other SemType values such astime contribute trivially to distinguishing answersfrom non-answers, as might be expected.3.
For when questions, person appears dominantamong NP SemType values; however, time fea-tures appear to be second most dominant since19.30% of the answer sentences have time as theirNP SemType, and 26.32% have at-time as their PPSemType.
N_o.te that the PP NeedSeraType and VPSemType appear to be less capable of guiding theselection of the correct answer.4.
For where questions, location features are impor-tant with 24.07% answer sentences having loca-tion as their NP SemType value, and 20.37% hav-ing at-loc as their PP SemType.
However, thedistribution of values for VP SemType and PPNeedSemType shows no interesting patterns.The current raining strategy weights the NP-NP,VP-VP, PP-PP, and NP-PP comparisons equiva-lently.
The above observations suggest hat trainingclassifiers based on these equally weighted compar-isons may have prevented the detection of a clearclass boundary, resulting in poor classification per-formance.
Since different phrase types do not appearto contribute in the same way across different ques-tion types, it may be better to generate a rule baseas a prefitter to assign more weight to certain phrasesor discard others before inputting the feature vectorinto the classifier for training.4 Future  D i rec t ionsAs a next step, we will try to tame our feature set.One possibility is to use a rule-based classifier thatis less impacted by the serious imbalance betweennegative and positive instances than C5.0 in orderto learn more effective feature sets for answer candi-date discrimination corresponding to different ques-tion types.
We could then use the classifier as a pre-processing filter to discard those less relevant com-parison vector elements before inputting them intothe classifiers, instead of inputting comparison re-sults based on the complete feature sets.
This shouldhelp to reduce noise generated by irrelevant features.Also, we will perform additional data analysis on theclassification results to gain further insight into thenoise sources.The classifiers we developed covered a wide rangeof approaches.
To optimize the classification perfor-mance, we would like to implement a voting mod-ule to process the answer candidates from differentclassifiers.
The confidence rankings of the classifierswould be determined f rom their corresponding an-swer selection accuracy in the training set, and willbe used horizontally over the classifiers to providea weighted confidence measure for each sentence,giving a final ordered list, where the head of thelist is the proposed answer sentence.
We proposeto use a voting neural network to train the confi-dence weights on different classifiers based on differ-ent question types, since we also want to explore therelationship of classifier performance with questiontypes.
We believe this voting scheme will optimizethe bagging of different classifiers and improve thehypothesis accuracy.ReferencesJ.
Allen.
1995.
Natural Language Understanding.The Benjamin/Cummings Publishing Company,Menlo Park, CA.M.
P. Harper and R. A. Helzerman.
1995.
Man-aging multiple knowledge sources in constraint-based parsing spoken language.
Fundamenta In-formaticae, 23(2,3,4):303-353.M.
P. Harper, R. A. Helzerman, C. B. Zoltowski,B.
L. Yeo, Y. Chan, T. Stewart, and B. L. Pellom.1995.
Implementation issues in the developmentof the parsec parser.
SOFTWARE - Practice andExperience, 25:831-862.M.
P. Harper, C. M. White, W. Wang, M. T. John-son, and R. A. Helzerman.
2000.
Effectivenessof corpus-induced dependency grammars for post-processing speech.
In Proceedings of the 1st An-nual Meeting of the North American Associationfor Computational Linguistics.L.
Hirschman, M. Light, E. Breck, and J.D.
Burger.1999.
Deep Read: A reading comprehension sys-tem.
In Proceedings of the 37th Annual Meetingof the Association for Computational Linguistics,pages 325-332.J.
R. Hobbs.
1979.
Coherence and coreference.
Cog-nitive Science, 1:67-90.J-SR Jang.
1993.
ANFIS: Adaptive-Network-basedFuzzy Inference System.
IEEE Transactions onSystem, Man, and Cybernetics, 23(3):665-685.M.
Jelasity and J. Dombi.
1998.
GAS, a concept onmodeling species in genetic algorithms.
ArtificialIntelligence, 99 (1) :1-19.The MathWorks, Inc., 1998.
Neural Network Tool-box, v3.
O.
1.Rulequest Research, 1999.
DataMining Tools See5 and C5.
O.http ://www.
rulequest, com/s eeS-in~o, html.35
