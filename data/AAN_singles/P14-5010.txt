Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55?60,Baltimore, Maryland USA, June 23-24, 2014.c?2014 Association for Computational LinguisticsThe Stanford CoreNLP Natural Language Processing ToolkitChristopher D. ManningLinguistics & Computer ScienceStanford Universitymanning@stanford.eduMihai SurdeanuSISTAUniversity of Arizonamsurdeanu@email.arizona.eduJohn BauerDept of Computer ScienceStanford Universityhoratio@stanford.eduJenny FinkelPrismatic Inc.jrfinkel@gmail.comSteven J. BethardComputer and Information SciencesU.
of Alabama at Birminghambethard@cis.uab.eduDavid McCloskyIBM Researchdmcclosky@us.ibm.comAbstractWe describe the design and use of theStanford CoreNLP toolkit, an extensiblepipeline that provides core natural lan-guage analysis.
This toolkit is quite widelyused, both in the research NLP communityand also among commercial and govern-ment users of open source NLP technol-ogy.
We suggest that this follows froma simple, approachable design, straight-forward interfaces, the inclusion of ro-bust and good quality analysis compo-nents, and not requiring use of a largeamount of associated baggage.1 IntroductionThis paper describe the design and development ofStanford CoreNLP, a Java (or at least JVM-based)annotation pipeline framework, which providesmost of the common core natural language pro-cessing (NLP) steps, from tokenization through tocoreference resolution.
We describe the originaldesign of the system and its strengths (section 2),simple usage patterns (section 3), the set of pro-vided annotators and how properties control them(section 4), and how to add additional annotators(section 5), before concluding with some higher-level remarks and additional appendices.
Whilethere are several good natural language analysistoolkits, Stanford CoreNLP is one of the mostused, and a central theme is trying to identify theattributes that contributed to its success.2 Original Design and DevelopmentOur pipeline system was initially designed for in-ternal use.
Previously, when combining multiplenatural language analysis components, each withtheir own ad hoc APIs, we had tied them togetherwith custom glue code.
The initial version of theTokeniza)on*Sentence*Spli0ng*Part4of4speech*Tagging*Morphological*Analysis*Named*En)ty*Recogni)on*Syntac)c*Parsing*Other*Annotators*Coreference*Resolu)on**Raw*text*Execu)on*Flow* Annota)on*Object*Annotated*text*(tokenize)*(ssplit)*(pos)*(lemma)*(ner)*(parse)*(dcoref)*(gender, sentiment)!Figure 1: Overall system architecture: Raw textis put into an Annotation object and then a se-quence of Annotators add information in an analy-sis pipeline.
The resulting Annotation, containingall the analysis information added by the Annota-tors, can be output in XML or plain text forms.annotation pipeline was developed in 2006 in or-der to replace this jumble with something better.A uniform interface was provided for an Annota-tor that adds some kind of analysis information tosome text.
An Annotator does this by taking in anAnnotation object to which it can add extra infor-mation.
An Annotation is stored as a typesafe het-erogeneous map, following the ideas for this datatype presented by Bloch (2008).
This basic archi-tecture has proven quite successful, and is still thebasis of the system described here.
It is illustratedin figure 1.
The motivations were:?
To be able to quickly and painlessly get linguis-tic annotations for a text.?
To hide variations across components behind acommon API.?
To have a minimal conceptual footprint, so thesystem is easy to learn.?
To provide a lightweight framework, using plainJava objects (rather than something of heav-ier weight, such as XML or UIMA?s CommonAnalysis System (CAS) objects).55In 2009, initially as part of a multi-site grantproject, the system was extended to be more easilyusable by a broader range of users.
We provideda command-line interface and the ability to writeout an Annotation in various formats, includingXML.
Further work led to the system being re-leased as free open source software in 2010.On the one hand, from an architectural perspec-tive, Stanford CoreNLP does not attempt to do ev-erything.
It is nothing more than a straightforwardpipeline architecture.
It provides only a Java API.1It does not attempt to provide multiple machinescale-out (though it does provide multi-threadedprocessing on a single machine).
It provides a sim-ple concrete API.
But these requirements satisfya large percentage of potential users, and the re-sulting simplicity makes it easier for users to getstarted with the framework.
That is, the primaryadvantage of Stanford CoreNLP over larger frame-works like UIMA (Ferrucci and Lally, 2004) orGATE (Cunningham et al., 2002) is that users donot have to learn UIMA or GATE before they canget started; they only need to know a little Java.In practice, this is a large and important differ-entiator.
If more complex scenarios are required,such as multiple machine scale-out, they can nor-mally be achieved by running the analysis pipelinewithin a system that focuses on distributed work-flows (such as Hadoop or Spark).
Other systemsattempt to provide more, such as the UIUC Cu-rator (Clarke et al., 2012), which includes inter-machine client-server communication for process-ing and the caching of natural language analyses.But this functionality comes at a cost.
The systemis complex to install and complex to understand.Moreover, in practice, an organization may wellbe committed to a scale-out solution which is dif-ferent from that provided by the natural languageanalysis toolkit.
For example, they may be usingKryo or Google?s protobuf for binary serializationrather than Apache Thrift which underlies Cura-tor.
In this case, the user is better served by a fairlysmall and self-contained natural language analysissystem, rather than something which comes witha lot of baggage for all sorts of purposes, most ofwhich they are not using.On the other hand, most users benefit greatlyfrom the provision of a set of stable, robust, high1Nevertheless, it can call an analysis component written inother languages via an appropriate wrapper Annotator, andin turn, it has been wrapped by many people to provide Stan-ford CoreNLP bindings for other languages.quality linguistic analysis components, which canbe easily invoked for common scenarios.
Whilethe builder of a larger system may have made over-all design choices, such as how to handle scale-out, they are unlikely to be an NLP expert, andare hence looking for NLP components that justwork.
This is a huge advantage that StanfordCoreNLP and GATE have over the empty tool-box of an Apache UIMA download, somethingaddressed in part by the development of well-integrated component packages for UIMA, suchas ClearTK (Bethard et al., 2014), DKPro Core(Gurevych et al., 2007), and JCoRe (Hahn et al.,2008).
However, the solution provided by thesepackages remains harder to learn, more complexand heavier weight for users than the pipeline de-scribed here.These attributes echo what Patricio (2009) ar-gued made Hibernate successful, including: (i) doone thing well, (ii) avoid over-design, and (iii)up and running in ten minutes or less!
Indeed,the design and success of Stanford CoreNLP alsoreflects several other of the factors that Patriciohighlights, including (iv) avoid standardism, (v)documentation, and (vi) developer responsiveness.While there are many factors that contribute to theuptake of a project, and it is hard to show causal-ity, we believe that some of these attributes ac-count for the fact that Stanford CoreNLP is one ofthe more used NLP toolkits.
While we certainlyhave not done a perfect job, compared to muchacademic software, Stanford CoreNLP has gainedfrom attributes such as clear open source licens-ing, a modicum of attention to documentation, andattempting to answer user questions.3 Elementary UsageA key design goal was to make it very simple toset up and run processing pipelines, from eitherthe API or the command-line.
Using the API, run-ning a pipeline can be as easy as figure 2.
Or,at the command-line, doing linguistic processingfor a file can be as easy as figure 3.
Real life israrely this simple, but the ability to get started us-ing the product with minimal configuration codegives new users a very good initial experience.Figure 4 gives a more realistic (and complete)example of use, showing several key properties ofthe system.
An annotation pipeline can be appliedto any text, such as a paragraph or whole storyrather than just a single sentence.
The behavior of56Annotator pipeline = new StanfordCoreNLP();Annotation annotation = new Annotation("Can you parse my sentence?
");pipeline.annotate(annotation);Figure 2: Minimal code for an analysis pipeline.export StanfordCoreNLP_HOME /where/installedjava -Xmx2g -cp $StanfordCoreNLP_HOME/*edu.stanford.nlp.StanfordCoreNLP-file input.txtFigure 3: Minimal command-line invocation.import java.io.
*;import java.util.
*;import edu.stanford.nlp.io.
*;import edu.stanford.nlp.ling.
*;import edu.stanford.nlp.pipeline.
*;import edu.stanford.nlp.trees.
*;import edu.stanford.nlp.trees.TreeCoreAnnotations.
*;import edu.stanford.nlp.util.
*;public class StanfordCoreNlpExample {public static void main(String[] args) throws IOException {PrintWriter xmlOut = new PrintWriter("xmlOutput.xml");Properties props = new Properties();props.setProperty("annotators","tokenize, ssplit, pos, lemma, ner, parse");StanfordCoreNLP pipeline = new StanfordCoreNLP(props);Annotation annotation = new Annotation("This is a short sentence.
And this is another.
");pipeline.annotate(annotation);pipeline.xmlPrint(annotation, xmlOut);// An Annotation is a Map and you can get and use the// various analyses individually.
For instance, this// gets the parse tree of the 1st sentence in the text.List<CoreMap> sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);if (sentences != null && sentences.size() > 0) {CoreMap sentence = sentences.get(0);Tree tree = sentence.get(TreeAnnotation.class);PrintWriter out = new PrintWriter(System.out);out.println("The first sentence parsed is:");tree.pennPrint(out);}}}Figure 4: A simple, complete example program.annotators in a pipeline is controlled by standardJava properties in a Properties object.
The mostbasic property to specify is what annotators to run,in what order, as shown here.
But as discussed be-low, most annotators have their own properties toallow further customization of their usage.
If noneare specified, reasonable defaults are used.
Run-ning the pipeline is as simple as in the first exam-ple, but then we show two possibilities for access-ing the results.
First, we convert the Annotationobject to XML and write it to a file.
Second, weshow code that gets a particular type of informa-tion out of an Annotation and then prints it.Our presentation shows only usage in Java, butthe Stanford CoreNLP pipeline has been wrappedby others so that it can be accessed easily frommany languages, including Python, Ruby, Perl,Scala, Clojure, Javascript (node.js), and .NET lan-guages, including C# and F#.4 Provided annotatorsThe annotators provided with StanfordCoreNLPcan work with any character encoding, making useof Java?s good Unicode support, but the systemdefaults to UTF-8 encoding.
The annotators alsosupport processing in various human languages,providing that suitable underlying models or re-sources are available for the different languages.The system comes packaged with models for En-glish.
Separate model packages provide supportfor Chinese and for case-insensitive processing ofEnglish.
Support for other languages is less com-plete, but many of the Annotators also supportmodels for French, German, and Arabic (see ap-pendix B), and building models for further lan-guages is possible using the underlying tools.
Inthis section, we outline the provided annotators,focusing on the English versions.
It should benoted that some of the models underlying annota-tors are trained from annotated corpora using su-pervised machine learning, while others are rule-based components, which nevertheless often re-quire some language resources of their own.tokenize Tokenizes the text into a sequence of to-kens.
The English component provides a PTB-style tokenizer, extended to reasonably handlenoisy and web text.
The corresponding com-ponents for Chinese and Arabic provide wordand clitic segmentation.
The tokenizer saves thecharacter offsets of each token in the input text.cleanxml Removes most or all XML tags fromthe document.ssplit Splits a sequence of tokens into sentences.truecase Determines the likely true case of tokensin text (that is, their likely case in well-editedtext), where this information was lost, e.g., forall upper case text.
This is implemented witha discriminative model using a CRF sequencetagger (Finkel et al., 2005).pos Labels tokens with their part-of-speech (POS)tag, using a maximum entropy POS tagger(Toutanova et al., 2003).lemma Generates the lemmas (base forms) for alltokens in the annotation.gender Adds likely gender information to names.ner Recognizes named (PERSON, LOCATION,ORGANIZATION, MISC) and numerical(MONEY, NUMBER, DATE, TIME, DU-RATION, SET) entities.
With the default57annotators, named entities are recognizedusing a combination of CRF sequence taggerstrained on various corpora (Finkel et al., 2005),while numerical entities are recognized usingtwo rule-based systems, one for money andnumbers, and a separate state-of-the-art systemfor processing temporal expressions (Changand Manning, 2012).regexner Implements a simple, rule-based NERover token sequences building on Java regularexpressions.
The goal of this Annotator is toprovide a simple framework to allow a user toincorporate NE labels that are not annotated intraditional NL corpora.
For example, a defaultlist of regular expressions that we distributein the models file recognizes ideologies (IDE-OLOGY), nationalities (NATIONALITY), reli-gions (RELIGION), and titles (TITLE).parse Provides full syntactic analysis, includingboth constituent and dependency representa-tion, based on a probabilistic parser (Klein andManning, 2003; de Marneffe et al., 2006).sentiment Sentiment analysis with a composi-tional model over trees using deep learning(Socher et al., 2013).
Nodes of a binarized treeof each sentence, including, in particular, theroot node of each sentence, are given a senti-ment score.dcoref Implements mention detection and bothpronominal and nominal coreference resolution(Lee et al., 2013).
The entire coreference graphof a text (with head words of mentions as nodes)is provided in the Annotation.Most of these annotators have various optionswhich can be controlled by properties.
These caneither be added to the Properties object when cre-ating an annotation pipeline via the API, or spec-ified either by command-line flags or through aproperties file when running the system from thecommand-line.
As a simple example, input to thesystem may already be tokenized and presentedone-sentence-per-line.
In this case, we wish thetokenization and sentence splitting to just work byusing the whitespace, rather than trying to do any-thing more creative (be it right or wrong).
This canbe accomplished by adding two properties, eitherto a properties file:tokenize.whitespace: truessplit.eolonly: truein code:/** Simple annotator for locations stored in a gazetteer.
*/package org.foo;public class GazetteerLocationAnnotator implements Annotator {// this is the only method an Annotator must implementpublic void annotate(Annotation annotation) {// traverse all sentences in this documentfor (CoreMap sentence:annotation.get(SentencesAnnotation.class)) {// loop over all tokens in sentence (the text already tokenized)List<CoreLabel> toks = sentence.get(TokensAnnotation.class);for (int start = 0; start < toks.size(); start++) {// assumes that the gazetteer returns the token index// after the match or -1 otherwiseint end = Gazetteer.isLocation(toks, start);if (end > start) {for (int i = start; i < end; i ++) {toks.get(i).set(NamedEntityTagAnnotation.class,"LOCATION");}}}}}}Figure 5: An example of a simple custom anno-tator.
The annotator marks the words of possiblymulti-word locations that are in a gazetteer.props.setProperty("tokenize.whitespace", "true");props.setProperty("ssplit.eolonly", "true");or via command-line flags:-tokenize.whitespace -ssplit.eolonlyWe do not attempt to describe all the propertiesunderstood by each annotator here; they are avail-able in the documentation for Stanford CoreNLP.However, we note that they follow the pattern ofbeing x.y, where x is the name of the annotatorthat they apply to.5 Adding annotatorsWhile most users work with the provided annota-tors, it is quite easy to add additional custom an-notators to the system.
We illustrate here both howto write an Annotator in code and how to load itinto the Stanford CoreNLP system.
An Annotatoris a class that implements three methods: a sin-gle method for analysis, and two that describe thedependencies between analysis steps:public void annotate(Annotation annotation);public Set<Requirement> requirementsSatisfied();public Set<Requirement> requires();The information in an Annotation is updated inplace (usually in a non-destructive manner, byadding new keys and values to the Annotation).The code for a simple Annotator that marks loca-tions contained in a gazetteer is shown in figure 5.2Similar code can be used to write a wrapper Anno-tator, which calls some pre-existing analysis com-ponent, and adds its results to the Annotation.2The functionality of this annotator is already provided bythe regexner annotator, but it serves as a simple example.58While building an analysis pipeline, StanfordCoreNLP can add additional annotators to thepipeline which are loaded using reflection.
To pro-vide a new Annotator, the user extends the classedu.stanford.nlp.pipeline.Annotatorand provides a constructor with the signature(String, Properties).
Then, the user addsthe propertycustomAnnotatorClass.FOO: BARto the properties used to create the pipeline.
IfFOO is then added to the list of annotators, theclass BAR will be loaded to instantiate it.
TheProperties object is also passed to the constructor,so that annotator-specific behavior can be initial-ized from the Properties object.
For instance, forthe example above, the properties file lines mightbe:customAnnotatorClass.locgaz: org.foo.GazetteerLocationAnnotatorannotators: tokenize,ssplit,locgazlocgaz.maxLength: 56 ConclusionIn this paper, we have presented the designand usage of the Stanford CoreNLP system, anannotation-based NLP processing pipeline.
Wehave in particular tried to emphasize the proper-ties that we feel have made it successful.
Ratherthan trying to provide the largest and most engi-neered kitchen sink, the goal has been to make itas easy as possible for users to get started usingthe framework, and to keep the framework small,so it is easily comprehensible, and can easily beused as a component within the much larger sys-tem that a user may be developing.
The broad us-age of this system, and of other systems such asNLTK (Bird et al., 2009), which emphasize acces-sibility to beginning users, suggests the merits ofthis approach.A PointersWebsite: http://nlp.stanford.edu/software/corenlp.shtmlGithub: https://github.com/stanfordnlp/CoreNLPMaven: http://mvnrepository.com/artifact/edu.stanford.nlp/stanford-corenlpLicense: GPL v2+Stanford CoreNLP keeps the models for ma-chine learning components and miscellaneousother data files in a separate models jar file.
If youare using Maven, you need to make sure that youlist the dependency on this models file as well asthe code jar file.
You can do that with code like thefollowing in your pom.xml.
Note the extra depen-dency with a classifier element at the bottom.<dependency><groupId>edu.stanford.nlp</groupId><artifactId>stanford-corenlp</artifactId><version>3.3.1</version></dependency><dependency><groupId>edu.stanford.nlp</groupId><artifactId>stanford-corenlp</artifactId><version>3.3.1</version><classifier>models</classifier></dependency>B Human language supportWe summarize the analysis components supportedfor different human languages in early 2014.Annotator Ara- Chi- Eng- Fre- Ger-bic nese lish nch manTokenize X X X X XSent.
split X X X X XTruecase XPOS X X X X XLemma XGender XNER X X XRegexNER X X X X XParse X X X X XDep.
Parse X XSentiment XCoref.
XC Getting the sentiment of sentencesWe show a command-line for sentiment analysis.$ cat sentiment.txtI liked it.It was a fantastic experience.The plot move rather slowly.$ java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotatorstokenize,ssplit,pos,lemma,parse,sentiment -file sentiment.txtAdding annotator tokenizeAdding annotator ssplitAdding annotator posReading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].Adding annotator lemmaAdding annotator parseLoading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [1.4 sec].Adding annotator sentimentReady to process: 1 files, skipped 0, total 1Processing file /Users/manning/Software/stanford-corenlp-full-2014-01-04/sentiment.txt ... writing to /Users/manning/Software/stanford-corenlp-full-2014-01-04/sentiment.txt.xml {Annotating file /Users/manning/Software/stanford-corenlp-full-2014-01-04/sentiment.txt [0.583 seconds]} [1.219 seconds]Processed 1 documentsSkipped 0 documents, error annotating 0 documentsAnnotation pipeline timing information:PTBTokenizerAnnotator: 0.0 sec.WordsToSentencesAnnotator: 0.0 sec.POSTaggerAnnotator: 0.0 sec.59MorphaAnnotator: 0.0 sec.ParserAnnotator: 0.4 sec.SentimentAnnotator: 0.1 sec.TOTAL: 0.6 sec.
for 16 tokens at 27.4 tokens/sec.Pipeline setup: 3.0 sec.Total time for StanfordCoreNLP pipeline: 4.2 sec.$ grep sentiment sentiment.txt.xml<sentence id="1" sentimentValue="3" sentiment="Positive"><sentence id="2" sentimentValue="4" sentiment="Verypositive"><sentence id="3" sentimentValue="1" sentiment="Negative">D Use within UIMAThe main part of using Stanford CoreNLP withinthe UIMA framework (Ferrucci and Lally, 2004)is mapping between CoreNLP annotations, whichare regular Java classes, and UIMA annotations,which are declared via XML type descriptors(from which UIMA-specific Java classes are gen-erated).
A wrapper for CoreNLP will typically de-fine a subclass of JCasAnnotator ImplBase whoseprocess method: (i) extracts UIMA annotationsfrom the CAS, (ii) converts UIMA annotations toCoreNLP annotations, (iii) runs CoreNLP on theinput annotations, (iv) converts the CoreNLP out-put annotations into UIMA annotations, and (v)saves the UIMA annotations to the CAS.To illustrate part of this process, the ClearTK(Bethard et al., 2014) wrapper converts CoreNLPtoken annotations to UIMA annotations and savesthem to the CAS with the following code:int begin = tokenAnn.get(CharacterOffsetBeginAnnotation.class);int end = tokenAnn.get(CharacterOffsetEndAnnotation.class);String pos = tokenAnn.get(PartOfSpeechAnnotation.class);String lemma = tokenAnn.get(LemmaAnnotation.class);Token token = new Token(jCas, begin, end);token.setPos(pos);token.setLemma(lemma);token.addToIndexes();where Token is a UIMA type, declared as:<typeSystemDescription><name>Token</name><types><typeDescription><name>org.cleartk.token.type.Token</name><supertypeName>uima.tcas.Annotation</supertypeName><features><featureDescription><name>pos</name><rangeTypeName>uima.cas.String</rangeTypeName></featureDescription><featureDescription><name>lemma</name><rangeTypeName>uima.cas.String</rangeTypeName></featureDescription></features></typeDescription></types></typeSystemDescription>ReferencesSteven Bethard, Philip Ogren, and Lee Becker.
2014.ClearTK 2.0: Design patterns for machine learningin UIMA.
In LREC 2014.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.Joshua Bloch.
2008.
Effective Java.
Addison Wesley,Upper Saddle River, NJ, 2nd edition.Angel X. Chang and Christopher D. Manning.
2012.SUTIME: A library for recognizing and normalizingtime expressions.
In LREC 2012.James Clarke, Vivek Srikumar, Mark Sammons, andDan Roth.
2012.
An NLP Curator (or: How Ilearned to stop worrying and love NLP pipelines).In LREC 2012.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:an architecture for development of robust HLTapplications.
In ACL 2002.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC 2006, pages 449?454.David Ferrucci and Adam Lally.
2004.
UIMA: anarchitectural approach to unstructured informationprocessing in the corporate research environment.Natural Language Engineering, 10:327?348.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In ACL 43, pages 363?370.I.
Gurevych, M. M?uhlh?auser, C. M?uller, J. Steimle,M.
Weimer, and T. Zesch.
2007.
Darmstadt knowl-edge processing repository based on UIMA.
InFirst Workshop on Unstructured Information Man-agement Architecture at GLDV 2007, T?ubingen.U.
Hahn, E. Buyko, R. Landefeld, M. M?uhlhausen,Poprat M, K. Tomanek, and J. Wermter.
2008.
Anoverview of JCoRe, the Julie lab UIMA componentregistry.
In LREC 2008.Dan Klein and Christopher D. Manning.
2003.
Fastexact inference with a factored model for naturallanguage parsing.
In Suzanna Becker, SebastianThrun, and Klaus Obermayer, editors, Advances inNeural Information Processing Systems, volume 15,pages 3?10.
MIT Press.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Computational Linguistics, 39(4).Anthony Patricio.
2009.
Why this project is success-ful?
https://community.jboss.org/wiki/WhyThisProjectIsSuccessful.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In EMNLP 2013, pages 1631?1642.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In NAACL 3, pages 252?259.60
