Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1979?1985,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning Robust Representations of TextYitong Li Trevor Cohn Timothy BaldwinDepartment of Computing and Information SystemsThe University of Melbourne, Australiayitongl4@student.unimelb.edu.au, {tcohn,tbaldwin}@unimelb.edu.auAbstractDeep neural networks have achieved remark-able results across many language processingtasks, however these methods are highly sen-sitive to noise and adversarial attacks.
Wepresent a regularization based method for lim-iting network sensitivity to its inputs, inspiredby ideas from computer vision, thus learningmodels that are more robust.
Empirical evalua-tion over a range of sentiment datasets with aconvolutional neural network shows that, com-pared to a baseline model and the dropoutmethod, our method achieves superior perfor-mance over noisy inputs and out-of-domaindata.11 IntroductionDeep learning has achieved state-of-the-art resultsacross a range of computer vision (Krizhevsky et al,2012), speech recognition (Graves et al, 2013) andnatural language processing tasks (Bahdanau et al,2015; Kalchbrenner et al, 2014; Yih et al, 2014;Bitvai and Cohn, 2015).
However, deep models areoften overconfident for noisy test instances, makingthem susceptible to adversarial attacks (Nguyen etal., 2015; Tabacof and Valle, 2016).
Goodfellow etal.
(2014) argued that the primary cause of neuralnetworks?
vulnerability to adversarial perturbationis their linear nature, due to neural models beingintentionally designed to behave in a mostly linearmanner to facilitate optimization.
Fawzi et al (2015)provided a theoretical framework for analyzing the1Implementation available at https://github.com/lrank/Robust-Representation.robustness of classifiers to adversarial perturbations,and also showed linear models are usually not robustto adversarial noise.In this work, we present a regularization methodwhich makes deep learning models more robust tonoise, inspired by Rifai et al (2011).
The intuitionbehind the approach is to stabilize predictions byminimizing the ability of features to perturb predic-tions, based on high-order derivatives.
Rifai et al(2011) introduced contractive auto-encoders basedon similar ideas, using the Frobenius norm of theJacobian matrix as a penalty term to extract robustfeatures.
Further, Gu and Rigazio (2014) intro-duced deep contractive networks, generalizing thisidea to a feed-forward neural network.
Also related,Martens (2010) investigated a second-order optimiza-tion method based on Hessian-free approach for train-ing deep auto-encoders.
Where our proposed ap-proach differs is that we train models using first-orderderivatives of the training loss as part of a regular-ization term, necessitating second-order derivativesfor computing the gradient.
We empirically demon-strate the effectiveness of the model over text corporawith increasing amounts of artificial masking noise,using a range of sentiment analysis datasets (Pangand Lee, 2008) with a convolutional neural networkmodel (Kim, 2014).
In this, we show that our methodis superior to dropout (Srivastava et al, 2014) and abaseline method using MAP training.2 Training for RobustnessOur method introduces a regularization term duringtraining to ensure model robustness.
We develop ourapproach based on a general class of parametric mod-1979els, with the following structure.
Let x be the input,which is a sequence of (discrete) words, representedby a fixed-size vector of continuous values, h. Atransfer function takes h as input and produces anoutput distribution, ypred.
Training proceeds usingstochastic gradient descent to minimize a loss func-tion L, measuring the difference between ypred andthe truth ytrue.The purpose of our work is to learn neural modelswhich are more robust to strange or invalid inputs.When small perturbations are applied on x, we wantthe prediction ypred to remain stable.
Text can behighly variable, allowing for the same informationto be conveyed with different word choice, differentsyntactic structures, typographical errors, stylisticchanges, etc.
This is a particular problem in transferlearning scenarios such as domain adaptation, wherethe inputs in distinct domains are drawn from related,but different, distributions.
A good model should berobust to these kinds of small changes to the input,and produce reliable and stable predictions.Next we discuss methods for learning modelswhich are robust to variations in the input, beforeproviding details of the neural network model usedin our experimental evaluation.2.1 Conventional Regularization and DropoutConventional methods for learning robust models in-clude l1 and l2 regularization (Ng, 2004), and dropout(Srivastava et al, 2014).
In fact, Wager et al (2013)showed that the dropout regularizer is first-orderequivalent to an l2 regularizer applied after scalingthe features.
Dropout is also equivalent to ?Followthe Perturbed Leader?
(FPL) which perturbs expo-nential numbers of experts by noise and then predictswith the expert of minimum perturbed loss for onlinelearning robustness (van Erven et al, 2014).
Givenits popularity in deep learning, we take dropout to bea strong baseline in our evaluation.The key idea behind dropout is to randomly zeroout units, along with their connections, from the net-work during training, thus limiting the extent of co-adaptation between units.
We apply dropout on therepresentation vector h, denoted h?
= dropout?
(h),where ?
is the dropout rate.
Similarly to our proposedmethod, training with dropout requires gradient basedsearch for the minimizer of the loss L.We also use dropout to generate noise in the testdata as part of our experimental simulations, as wewill discuss later.2.2 Robust RegularizationOur method is inspired by the work on adversarialtraining in computer vision (Goodfellow et al, 2014).In image recognition tasks, small distortions that areindiscernible to humans can significantly distort thepredictions of neural networks (Szegedy et al, 2014).An intuitive explanation of our regularization methodis, when noise is applied to the data, the variation ofthe output is kept lower than the noise.
We adapt thisidea from Rifai et al (2011) and develop the Jacobianregularization method.The proposed regularization method works as fol-lows.
Conventional training seeks to minimise thedifference between ytrue and ypred.
However, in or-der to make our model robust against noise, we alsowant to minimize the variation of the output whennoise is applied to the input.
This is to say, whenperturbations are applied to the input, there shouldbe as little perturbation in the output as possible.
For-mally, the perturbations of output can be written aspy = M(x+px)?M(x), where x is the input, px isthe vector of perturbations applied to x, M expressesthe trained model, py is the vector of perturbationsgenerated by the model, and the output distributiony = M(x).
Thereforelimpx?0py = limpx?0(M(x + px)?M(x))= ?y?x ?
px ,and distance(limpx?0py/px,0)=?????y?x???
?F.In other words, minimising local noise sensitivity isequivalent to minimising the Frobenius norm of theJacobean matrix of partial derivatives of the modeloutputs wrt its inputs.To minimize the effect of perturbation noise, ourmethod involves an additional term in the loss func-tion, in the form of the derivative of loss L withrespect to hidden layer h. Note that while in princi-ple we could consider robustness to perturbations inthe input x, the discrete nature of x adds additionalmathematical complications, and thus we defer thissetting for future work.
Combining the elements, thenew loss function can be expressed asL = L+ ?
??????L?h???
?2, (1)1980where ?
is a weight term, and distance takes the formof the l2 norm.
The training objective in Equation (1)supports gradient optimization, but note that it re-quires the calculation of second-order derivatives ofL during back propagation, arising from the ?L/?hterm.
Henceforth we refer to this method as robustregularization.2.3 Convolutional NetworkFor the purposes of this paper, we focus exclusivelyon convolutional neural networks (CNNs), but stressthat the method is compatible with other neural ar-chitectures and other types of parametric models (notjust deep neural networks).
The CNN used in thisresearch is based on the model proposed by Kim(2014), and is outlined below.Let S be the sentence, consisting of n words{w1, w2, ?
?
?
, wn}.
A look-up table is applied to S,made up of word vectors ei ?
Rm corresponding toeach word wi, where m is the word vector dimen-sionality.
Thus, sentence S can be represented asa matrix ES ?
Rm?n by concatenating the wordvectors ES = ?ni=1 ewi .A convolutional layer combined with a number ofwide convolutional filters is applied to ES.
Specif-ically, the k-th convolutional filter operator filterkinvolves a weight vector wk ?
Rm?t, which workson every tk-sized window of ES, and is accom-panied by a bias term b ?
R. The filter oper-ator is followed by the non-linear function F , arectified linear unit, ReLU, followed by a max-pooling operation, to generate a hidden activationhk = MaxPooling(F (filterk(ES;wk, b)).
Multi-ple filters with different window sizes are used tolearn different local properties of the sentence.
Weconcatenate all the hidden activations hk to form ahidden layer h, with size equal to the number of fil-ters.
Details of parameter settings can be found inSection 3.2.The feature vector h is fed into a final softmaxlayer with a linear transform to generate a probabilitydistribution over labelsypred = softmax(w ?
h + b) ,where w and b are parameters.
Finally, themodel minimizes the loss of the cross-entropy be-tween the ground-truth and the model prediction,L = CrossEntropy(ytrue,ypred), for which we usestochastic gradient descent.3 Datasets and Experimental SetupsWe experiment on the following datasets,2 followingKim (2014):?
MR: Sentence polarity dataset (Pang and Lee,2008)3?
Subj: Subjectivity dataset (Pang and Lee,2005)3?
CR: Customer review dataset (Hu and Liu,2004)4?
SST: Stanford Sentiment Treebank, using the3-class configuration (Socher et al, 2013)5In each case, we evaluate using classification accu-racy.3.1 Noisifying the DataDifferent to conventional evaluation, we corrupt thetest data with noise in order to evaluate the robust-ness of our model.
We assume that when dealingwith short text such as Twitter posts, it is commonto see unknown words due to typos, abbreviationsand sociolinguistic marking of different types (Hanand Baldwin, 2011; Eisenstein, 2013).
To simulatethis, we apply word-level dropout noise to each doc-ument, by randomly replacing words by a uniquesentinel symbol.6 This is applied to each word withprobability ?
?
{0, 0.1, 0.2, 0.3}.We also experimented with adding different levelsof Gaussian noise to the sentence embeddings ES,but found the results to be largely consistent withthose for word dropout noise, and therefore we haveomitted these results from the paper.To directly test the robustness under a more real-istic setting, we additionally perform cross-domainevaluation, where we train a model on one dataset2For datasets where there is no pre-defined training/test split,we evaluate using 10-fold cross validation.
Refer to Kim (2014)for more details on the datasets.3 https://www.cs.cornell.edu/people/pabo/movie-review-data/4http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html5http://nlp.stanford.edu/sentiment/6This was to avoid creating new n-grams which would oc-cur when symbols are deleted from the input.
Masking tokensinstead results in partially masked n-grams as input to the con-volutional filters.1981Dataset MR SubjWord dropout rate (?)
0 0.1 0.2 0.3 0 0.1 0.2 0.3Baseline 80.5 79.4 77.9 76.5 93.1 92.0 90.9 89.80.3 80.3 79.5 78.1 76.7 92.7 92.0 90.9 89.5Dropout (?)
0.5 80.3 79.0 78.0 76.5 93.0 92.0 91.1 89.90.7 80.3 79.3 78.3 76.8 92.8 91.9 90.9 89.810?3 80.5 79.4 78.3 76.7 93.0 92.2 91.1 89.8Robust 10?2 80.8 79.3 78.4 77.0 93.0 92.2 91.0 90.0Regularization (?)
10?1 80.4 78.8 77.8 77.0 92.7 91.9 91.0 89.81 79.3 77.1 76.1 75.5 91.7 91.1 90.1 89.3Dropout + Robust ?
= 0.5, ?
= 10?2 80.6 79.9 78.6 77.3 93.0 92.2 91.2 90.1Dataset CR SSTWord dropout rate (?)
0 0.1 0.2 0.3 0 0.1 0.2 0.3Baseline 83.2 82.3 80.4 77.9 84.1 82.3 80.3 77.80.3 83.3 82.1 80.3 78.9 84.2 82.3 80.2 78.0Dropout (?)
0.5 83.2 82.4 81.0 79.3 84.2 82.4 80.5 78.20.7 83.2 82.2 80.7 78.8 83.9 82.5 80.9 78.210?3 83.3 82.6 81.4 79.5 84.5 82.8 81.4 78.8Robust 10?2 83.4 82.5 81.6 79.3 84.2 82.4 80.7 78.6Regularization (?)
10?1 83.3 82.7 82.0 79.6 82.5 81.5 79.7 77.61 82.9 81.4 79.8 79.0 82.2 80.9 79.1 77.3Dropout + Robust ?
= 0.5, ?
= 10?2 83.3 82.5 81.5 79.7 84.3 82.6 80.8 79.1Table 1: Accuracy (%) with increasing word-level dropout across the four datasets.
For each dataset, we apply four levels of noise?
= {0, 0.1, 0.2, 0.3}; the best result for each combination of ?
and dataset is indicated in bold.
The Baseline model is a simpleCNN model without regularization.
The last model combines dropout and our method with fixed parameters ?
and ?
as indicated.and apply it to another.
For this, we use the pairingof MR and CR, where the first dataset is based onmovie reviews and the second on product reviews,but both use the same label set.
Note that there is asignificant domain shift between these corpora, dueto the very nature of the items reviewed.3.2 Word Vectors and Hyper-parametersTo set the hyper-parameters of the CNN, we followthe guidelines of Zhang and Wallace (2015), settingword embeddings to m = 300 dimensions and ini-tialising based on word2vec pre-training (Mikolov etal., 2013).
Words not in the pre-trained vector tablewere initialized randomly by the uniform distributionU([?0.25, 0.25)m).
The window sizes of filters (t)are set to 3, 4, 5, with 128 filters for each size, result-ing in a hidden layer dimensionality of 384 = 128?3.We use the Adam optimizer (Kingma and Ba, 2015)for training.4 Results and DiscussionsThe results for word-level dropout noise are pre-sented in Table 1.
In general, increasing the word-level dropout noise leads to a drop in accuracy for allfour datasets, however the relative dropoff in accu-racy for Robust Regularization is less than for WordDropout, and in 15 out of 16 cases (four noise levelsacross the four datasets), our method achieves thebest result.
Note that this includes the case of ?
= 0,where the test data is left in its original form, whichshows that Robust Regularization is also an effectivemeans of preventing overfitting in the model.For each dataset, we also evaluated based on thecombination of Word Dropout and Robust Regu-larization using the fixed parameters ?
= 0.5 and?
= 10?2, which are overall the best individual set-tings.
The combined approach performs better thaneither individual method for the highest noise levelstested across all datasets.
This indicates that Robust1982Train/Test MR/CR CR/MRBaseline 67.5 61.00.3 71.6 62.2Dropout (?)
0.5 71.0 62.10.7 70.9 62.010?3 70.8 61.6Robust 10?2 71.1 62.5Regularization (?)
10?1 72.0 62.21 71.8 62.3Dropout + Robust ?
= 0.5, ?
= 10?2 72.0 62.4Table 2: Accuracy under cross-domain evaluation; the bestresult for each dataset is indicated in bold.Regularization acts in a complementary way to WordDropout.Table 2 presents the results of the cross-domainexperiment, whereby we train a model on MR andtest on CR, and vice versa, to measure the robust-ness of the different regularization methods in a morereal-world setting.
Once again, we see that our regu-larization method is superior to word-level dropoutand the baseline CNN, and the techniques combineddo very well, consistent with our findings for syn-thetic noise.4.1 Running TimeOur method requires second-order derivatives, andthus is a little slower at training time.
Figure 1 is aplot of the training and test accuracy at varying pointsduring training over SST.We can see that the runtime till convergence is onlyslightly slower for Robust Regularization than stan-dard training, at roughly 30 minutes on a two-coreCPU (one fold) with standard training vs. 35?40 min-utes with Robust Regularization.
The convergencetime for Robust Regularization is comparable to thatfor Word Dropout.5 ConclusionsIn this paper, we present a robust regularizationmethod which explicitly minimises a neural model?ssensitivity to small changes in its hidden representa-tion.
Based on evaluation over four sentiment analy-sis datasets using convolutional neural networks, wefound our method to be both superior and comple-mentary to conventional word-level dropout undervarying levels of noise, and in a cross-domain evalu-0.750.80 1000 2000 3000 4000 5000Time [sec]TestAccuracy[%]baseline dropout dropout+robust reg robust regFigure 1: Time?accuracy evaluation over the different combi-nations of Word Dropout (dropout) and Robust Regularization(robust reg) over SST, without injecting noise.ation.For future work, we plan to apply our regular-ization method to other models and tasks to deter-mine how generally applicable our method is.
Also,we will explore methods for more realistic linguisticnoise, such as lexical, syntactic and semantic noise,to develop models that are robust to the kinds of dataoften encountered at test time.AcknowledgmentsWe are grateful to the anonymous reviewers for theirhelpful feedback and suggestions.
This work wassupported by the Australian Research Council (grantnumbers FT130101105 and FT120100658).
Also,we would like to thank the developers of Tensorflow(Abadi et al, 2015), which was used for the experi-ments in this paper.ReferencesMart?
?n Abadi, Ashish Agarwal, Paul Barham, EugeneBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,Andy Davis, Jeffrey Dean, Matthieu Devin, SanjayGhemawat, Ian Goodfellow, Andrew Harp, GeoffreyIrving, Michael Isard, Yangqing Jia, Rafal Jozefow-icz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,1983Dan Mane?, Rajat Monga, Sherry Moore, Derek Murray,Chris Olah, Mike Schuster, Jonathon Shlens, BenoitSteiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-cent Vanhoucke, Vijay Vasudevan, Fernanda Vie?gas,Oriol Vinyals, Pete Warden, Martin Wattenberg, MartinWicke, Yuan Yu, and Xiaoqiang Zheng.
2015.
Tensor-Flow: Large-scale machine learning on heterogeneoussystems.
Technical report, Google Research.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.2015.
Neural machine translation by jointly learning toalign and translate.
In Proceedings of the InternationalConference on Learning Representations.Zsolt Bitvai and Trevor Cohn.
2015.
Non-linear textregression with a deep convolutional neural network.In Proceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7thInternational Joint Conference on Natural LanguageProcessing (Short Papers), pages 180?185.Jacob Eisenstein.
2013.
What to do about bad languageon the internet.
In Proceedings of the 2013 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 359?369.Alhussein Fawzi, Omar Fawzi, and Pascal Frossard.
2015.Analysis of classifiers?
robustness to adversarial pertur-bations.
arXiv preprint arXiv:1502.02590.Ian J. Goodfellow, Jonathon Shlens, and ChristianSzegedy.
2014.
Explaining and harnessing adversarialexamples.
In Proceedings of the International Confer-ence on Learning Representations.Alan Graves, Abdel-rahman Mohamed, and Geoffrey Hin-ton.
2013.
Speech recognition with deep recurrentneural networks.
In Proceedings of the IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing, pages 6645?6649.Shixiang Gu and Luca Rigazio.
2014.
Towards deep neu-ral network architectures robust to adversarial examples.In Proceedings of the NIPS 2014 Deep Learning andRepresentation Learning Workshop.Bo Han and Timothy Baldwin.
2011.
Lexical normalisa-tion of short text messages: Makn sens a #twitter.
InProceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics, pages 368?378.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 168?177.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics, pages 655?665.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of the 2014Conference on Empirical Methods in Natural LanguageProcessing, pages 1746?1751.Diederik P. Kingma and Jimmy Ba.
2015.
Adam: Amethod for stochastic optimization.
In Proceedings ofthe International Conference on Learning Representa-tions.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.2012.
Imagenet classification with deep convolutionalneural networks.
In Advances in Neural InformationProcessing Systems 25, pages 1097?1105.James Martens.
2010.
Deep learning via Hessian-freeoptimization.
In Proceedings of the 27th InternationalConference on Machine Learning, pages 735?742.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems26, pages 3111?3119.Andrew Y. Ng.
2004.
Feature selection, L1 vs. L2 regu-larization, and rotational invariance.
In Proceedings ofthe Twenty-first International Conference on MachineLearning.Anh Nguyen, Jason Yosinski, and Jeff Clune.
2015.
Deepneural networks are easily fooled: High confidence pre-dictions for unrecognizable images.
In Proceedings ofthe IEEE Conference on Computer Vision and PatternRecognition.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the 43rdAnnual Meeting on Association for Computational Lin-guistics, pages 115?124.Bo Pang and Lillian Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot,and Yoshua Bengio.
2011.
Contractive auto-encoders:Explicit invariance during feature extraction.
In Pro-ceedings of the 28th International Conference on Ma-chine Learning, pages 833?840.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pages 1631?1642.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, IlyaSutskever, and Ruslan Salakhutdinov.
2014.
Dropout:A simple way to prevent neural networks from overfit-ting.
Journal of Machine Learning Research, 15:1929?1958.Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob1984Fergus.
2014.
Intriguing properties of neural net-works.
In Proceedings of the International Conferenceon Learning Representations.Pedro Tabacof and Eduardo Valle.
2016.
Exploring thespace of adversarial images.
In Proceedings of theIEEE International Joint Conference on Neural Net-works.Tim van Erven, Wojciech Kot?owski, and Manfred K. War-muth.
2014.
Follow the leader with dropout pertur-bations.
In Proceedings of the 27th Conference onLearning Theory, pages 949?974.Stefan Wager, Sida Wang, and Percy S. Liang.
2013.Dropout training as adaptive regularization.
In Ad-vances in Neural Information Processing Systems 26,pages 351?359.Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014.Semantic parsing for single-relation question answer-ing.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (ShortPapers), pages 643?648.Ye Zhang and Byron Wallace.
2015.
A sensitivity analysisof (and practitioners?
guide to) convolutional neuralnetworks for sentence classification.
arXiv preprintarXiv:1510.03820.1985
