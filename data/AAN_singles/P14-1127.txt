Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1349?1359,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsUnsupervised Morphology-Based Vocabulary ExpansionMohammad Sadegh Rasooli, Thomas Lippincott, Nizar Habash and Owen RambowCenter for Computational Learning SystemsColumbia University, New York, NY, USA{rasooli,tom,habash,rambow}@ccls.columbia.eduAbstractWe present a novel way of generating un-seen words, which is useful for certain ap-plications such as automatic speech recog-nition or optical character recognition inlow-resource languages.
We test our vo-cabulary generator on seven low-resourcelanguages by measuring the decrease inout-of-vocabulary word rate on a held-outtest set.
The languages we study havevery different morphological properties;we show how our results differ depend-ing on the morphological complexity ofthe language.
In our best result (on As-samese), our approach can predict 29% ofthe token-based out-of-vocabulary with asmall amount of unlabeled training data.1 IntroductionIn many applications in human language technolo-gies (HLT), the goal is to generate text in a targetlanguage, using its standard orthography.
Typicalexamples include automatic speech recognition(ASR, also known as STT or speech-to-text), opti-cal character recognition (OCR), or machine trans-lation (MT) into a target language.
We will callsuch HLT applications ?target-language genera-tion technologies?
(TLGT).
The best-performingsystems for these applications today rely on train-ing on large amounts of data: in the case of ASR,the data is aligned audio and transcription, pluslarge unannotated data for the language model-ing; in the case of OCR, it is transcribed opticaldata; in the case of MT, it is aligned bitexts.
Moredata provides for better results.
For languages withrich resources, such as English, more data is oftenthe best solution, since the required data is readilyavailable (including bitexts), and the cost of anno-tating (e.g., transcribing) data is outweighed by thepotential significance of the systems that the datawill enable.
Thus, in HLT, improvements in qual-ity are often brought about by using larger datasets (Banko and Brill, 2001).When we move to low-resource languages, thesolution of simply using more data becomes lessappealing.
Unannotated data is less readily avail-able: for example, at the time of publishing thispaper, 55% of all websites are in English, the top10 languages collectively account for 90% of webpresence, and the top 36 languages have a webpresence that covers at least 0.1% of web sites.1All other languages (and all languages consideredin this paper except Persian) have a web presenceof less than 0.1%.
Considering Wikipedia, anotherresource often used in HLT, English has 4.4 mil-lion articles, while only 48 other languages havemore than 100,000.2As attention turns to de-veloping HLT for more languages, including low-resource languages, alternatives to ?more-data?approaches become important.At the same time, it is often not possible to useknowledge-rich approaches.
For low-resource lan-guages, resources such as morphological analyz-ers are not usually available, and even good schol-arly descriptions of the morphology (from whicha tool could be built) are often not available.
Thechallenge is therefore to use data, but to make dowith a small amount of data, and thus to use databetter.
This paper is a contribution to this goal.Specifically, we address TLGTs, i.e., the typesof HLT mentioned above that generate target lan-guage text.
We propose a new approach to gener-ating unseen words of the target language whichhave not been seen in the training data.
Our ap-proach is entirely unsupervised.
It assumes thatword-units are specified, typically by whitespaceand punctuation.1http://en.wikipedia.org/wiki/Languages_used_on_the_Internet2http://meta.wikimedia.org/wiki/List_of_Wikipedias1349Expanding the vocabulary of the target lan-guage can be useful for TLGTs in different ways.For ASR and OCR, which can compose wordsfrom smaller units (phones or graphically recog-nized letters), an expanded target language vocab-ulary can be directly exploited without the needfor changing the technology at all: the new wordsneed to be inserted into the relevant resources (lex-icon, language model) etc, with appropriately es-timated probabilities.
In the case of MT into mor-phologically rich low-resource languages, mor-phological segmentation is typically used in devel-oping the translation models to reduce sparsity, butthis does not guarantee against generating wrongword combinations.
The expanded word combi-nations can be used to extend the language modelsused for MT to bias against incoherent hypothe-sized new sequences of segmented words.Our approach relies on unsupervised morpho-logical segmentation.
We do not in this paper con-tribute to research in unsupervised morphologicalsegmentation; we only use it.
The contributionof this paper lies in proposing how to use the re-sults of unsupervised morphological segmentationin order to generate unseen words of the language.We investigate several ways of doing so, and wetest them on seven low-resource languages.
Theselanguages have very different morphological prop-erties, and we show how our results differ depend-ing on the morphological complexity of the lan-guage.
In our best result (on Assamese), we showthat our approach can predict 29% of the token-based out-of-vocabulary with a small amount ofunlabeled training data.The paper is structured as follows.
We first dis-cuss related work in Section 2.
We then presentour method in Section 3, and present experimentalresults in Section 4.
We conclude with a discus-sion of future work in Section 5.2 Related WorkApproaches to Morphological ModelingComputational morphology is a very active areaof research with a multitude of approaches thatvary in the degree of manual annotation needed,and the amount of machine learning used.
At oneextreme, we find systems that are painstakinglyand carefully designed by hand (Koskenniemi,1983; Buckwalter, 2004; Habash and Rambow,2006; D?etrez and Ranta, 2012).
Next on thecontinuum, we find work that focuses on definingmorphological models with limited lexica thatare then extended using raw text (Cl?ement et al,2004; Forsberg et al, 2006).
In the middle ofthis continuum, we find efforts to learn completeparadigms using fully supervised methods relyingon completely annotated data points with richmorphological information (Durrett and DeNero,2013; Eskander et al, 2013).
Next, there iswork on minimally supervised methods that useavailable resources such as dictionaries, bitexts,and other additional morphological annotations(Yarowsky and Wicentowski, 2000; Cucerzan andYarowsky, 2002; Neuvel and Fulop, 2002; Snyderand Barzilay, 2008).
At the other extreme, wefind unsupervised methods that learn morphologymodels from unannotated data (Creutz and Lagus,2007; Monson et al, 2008; Dreyer and Eisner,2011; Sirts and Goldwater, 2013).The work we present in this paper makes nouse of any morphological annotations whatsoever,yet we are quite distinct from the approaches citedabove.
We compare our work to two efforts specif-ically.
First, consider work in automatic mor-phological segmentation learning from unanno-tated data (Creutz and Lagus, 2007; Monson etal., 2008).
Unlike these approaches which providesegmentations for training data and produce mod-els that can be used to segment unseen words, ourapproach can generate words that have not beenseen in the training data.
The focus of efforts israther complementary: we actually use an off-the-shelf unsupervised segmentation system (Creutzand Lagus, 2007) as part of our approach.
Second,consider paradigm completion methods such asthe work of Dreyer and Eisner (2011).
This effortis closely related to our work although unlike it,we make no assumptions about the data and do notintroduce any restrictions along the lines of deriva-tion/inflectional morphology: Dreyer and Eisner(2011) limited their work to verbal paradigms andused annotated training data in addition to basicassumptions about the problem such as the sizeof the paradigms.
In our approach, we have zeroannotated information and we do not distinguishbetween inflectional and derivational morphology,nor do we limit ourselves to a specific part-of-speech (POS).Vocabulary Expansion in HLT There havebeen diverse approaches towards dealing with out-of-vocabulary (OOV) words in ASR.
In somemodels, the approach is to expand the lexicon by1350adding new words or pronunciations.
Ohtsuki etal.
(2005) propose a two-run model where in thefirst run, the input speech is recognized by thereference vocabulary and relevant words are ex-tracted from the vocabulary database and addedthereafter to the reference vocabulary to build anexpanded lexicon.
Word recognition is done in thesecond run based on the lexicon.
Lei et al (2009)expanded the pronunciation lexicon via generat-ing all possible pronunciations for a word be-fore lattice generation and indexation.
There arealso other methods for generating abbreviations invoice search systems such as Yang et al (2012).While all of these approaches involve lexicon ex-pansion, they do not employ any morphologicalinformation.In the context of MT, several researchers haveaddressed the problem of OOV words by relatingthem to known in-vocabulary (INV) words.
Yangand Kirchhoff (2006) anticipated OOV wordsthat are potentially morphologically related usingphrase-based backoff models.
Habash (2008) con-sidered different techniques for vocabulary expan-sion online.
One of their techniques learned mod-els of morphological mapping between morpho-logically rich source words in Arabic that pro-duce the same English translation.
This was usedto relate an OOV word to a morphologically re-lated INV word.
Another technique expandedthe MT phrase tables with possible transliterationsand spelling alternatives.3 Morphology-based VocabularyExpansion3.1 ApproachOur approach to morphology-based vocabularyexpansion consists of three steps (Figure 1).
Westart with a ?training?
corpus of (unannotated)words and generate a list of new (unseen) wordsthat expands the vocabulary of the training corpus.1.
Unsupervised Morphology SegmentationThe first step is to segment each word in thetraining corpus into sequences of prefixes,stem and suffixes, where the prefixes and suf-fixes are optional.32.
FST-based Morphology Expansion Wethen construct new word models using the3In this paper, we use an off-the-shelf system for this stepbut plan to explore new methods in the future, such as jointsegmentation and expansion.segmented stems and affixes.
We explore twodifferent techniques for morphology-basedvocabulary expansion that we discuss below.The output of these models is represented asa weighted finite state machine (WFST).3.
Reranking Models Given that the size of theexpanded vocabulary can be quite large andit may include a lot of over-generation, wererank the expanded set of words before tak-ing the top n words to use in downstreamprocesses.
We consider four reranking con-ditions which we describe below.Training TranscriptsUnsupervisedMorphologySegmentationSegmented WordsFST-basedExpansion ModelExpanded ListRerankingReranked ExpansionFigure 1: The flowchart of the lexicon expansionsystem.3.2 Morphology Expansion TechniquesAs stated above, the input to the morphology ex-pansion step is a list of words segmented into mor-phemes: zero or more prefixes, one stem, and zeroor more suffixes.
Figure 2a presents an example ofsuch input using English words (for clarity).We use two different models of morphology ex-pansion in this paper: Fixed Affix model and Bi-gram Affix model.3.2.1 Fixed Affix Expansion ModelIn the Fixed Affix model, we construct a set offused prefixes from all the unique prefix sequencesin the training data; and we similarly construct a1351re+ pro+ duc +efunc +tion +alre+ duc +ere+ duc +tion +sinpro+ ductconcept +u +al + ly(a) Training data with morpheme boundaries.
Prefixes end with and suffixes start with ?+?
signs.30 1repro<epsilon>repro2ducfuncinconceptductetionaltionsutually<epsilon>(b) FST for the Fixed Affix expansion model30 4re<epsilon>1pro<epsilon>2ducfuncinconceptducte<epsilon>5tionu7tion6alsly<epsilon>(c) FST for the Bigram Affix expansion modelFigure 2: Two models of word generation from morphologically annotated data.
In our experiments, weused weighted finite state machine.
We use character-based WFST in the implementation to facilitateanalyzing inputs as well as word generation.set of fused suffixes from all the unique suffix se-quences in the training data.
In other words, wesimply pick characters from beginning of the wordup to the first stem as the prefix and charactersfrom the first suffix to the end of the word as thesuffix.
Everything in the middle is the stem.
Inthis model, each word has one single prefix andone single suffix (each of which can be empty in-dependently).
The Fixed Affix model is simplythe concatenation of the disjunction of all prefixeswith the disjunction of all stems and the disjunc-tion of all suffixes into one FST:prefix?
stem?
suffixThe morpheme paths in the FST are weighted toreflect their probability in the training corpus.4Figure 2b exemplifies a Fixed Affix model derivedfrom the example training data in Figure 2a.4We convert the probability into a cost by taking the neg-ative of the log of the probability.3.2.2 Bigram Affix Expansion ModelIn the Bigram Affix model, we do the same for thestem as in the Fixed Affix model, but for prefixesand suffixes, we create a bigram language modelin the finite state machine.
The advantage of thistechnique is that unseen compound affixes can begenerated by our model.
For example, the FixedAffix model in Figure 2b cannot generate the wordfunc+tion+al+ly since the suffix +tionally is notseen in the training data.
However, this word canbe generated in the Bigram Affix model as shownin Figure 2c: there is a path passing 0?
4?
1?2 ?
5 ?
6 ?
3 in the FST that can produce thisword.
We expect this model to have better recallfor generating new words in the language becauseof its affixation flexibility.3.3 Reranking TechniquesThe expanded models allow for a large number ofwords to be generated.
We limit the number of vo-cabulary expansion using different thresholds af-ter reranking or reweighing the WFSTs generated1352above.
We consider four reranking conditions.3.3.1 No Reranking (NRR)The baseline reranking option is no reranking(NRR).
In this approach we use the weights inthe WFST, which are based on the independentprefix/stem/suffix probabilities, to determine theranking of the expanded vocabulary.3.3.2 Trigraph-based Reweighting (W?Tr)We reweight the weights in the WFST model(Fixed or Bigram) by composing it with a lettertrigraph language model (W?Tr).
A letter tri-graph LM is itself a WFST where each trigraph (asequence of three consequent letters) has an asso-ciated weight equal to its negative log-likelihoodin the training data.
This reweighting allows usto model preferences of sequences of word lettersseen more in the training data.
For example, in aword like producttions, the trigraphs ctt and tti arevery rare and thus decrease its probability.3.3.3 Trigraph-based Reranking (TRR)When we compose our initial WFST with the tri-graph FST, the probability of each generated wordfrom the new FST is equal to the product of theprobability of its morphemes and the probabilitiesof each trigraph in that word.
This basically makesthe model prefer shorter words and may degradethe effect of morphology information.
Instead ofreweighting the WFST, we get the n-best list ofgenerated words and rerank them using their tri-graph probabilities.
We will refer to this techniqueas TRR.3.3.4 Reranking Morpheme Boundaries(BRR)The last reranking technique reranks the n-bestgenerated word list with trigraphs that are incidenton the morpheme boundaries (in case of BigramAffix model, the last prefix and first suffix).
Theintuition is that we already know that any mor-pheme that is generated from the morphology FSTis already seen in the training data but the bound-ary for different morphemes are not guaranteed tobe seen in the training data.
For example, for theword producttions, we only take into account thetrigraphs rod, odu, ctt and tti instead of all possibletrigraphs.
We will refer to this technique as BRR.4 Evaluation4.1 Evaluation Data and ToolsEvaluation Data The IARPA Babel program isa research program for developing rapid spokendetection systems for under-resourced languages(Harper, 2013).
We use the IARPA Babel pro-gram limited language pack data which consistsof 20 hours of telephone speech with transcrip-tion.
We use six languages which are knownto have rich morphology: Assamese (IARPA-babel102b-v0.5a), Bengali (IARPA-babel103b-v0.4b), Pashto (IARPA-babel104b-v0.4bY), Taga-log (IARPA-babel106-v0.2g), Turkish (IARPA-babel105b-v0.4) and Zulu (IARPA-babel206b-v0.1e).
Speech annotation such as silences andhesitations are removed from transcription and allwords are turned into lower-case (for languagesusing the Roman script ?
Tagalog, Turkish andZulu).
Moreover, in order to be able to perform amanual error analysis, we include a language thathas rich morphology and of which the first authoris a native speaker: Persian.
We sampled data fromthe training and development set of the Persian de-pendency treebank (Rasooli et al, 2013) to createa comparable seventh dataset in Persian.
Statis-tics about the datasets are shown in Table 1.
Wealso conduct further experiments on just verbs andnouns in the data set for Persian (Persian-N andPersian V).
As shown in Table 1, the training datais very small and the OOV rate is high especiallyin terms of types.
For some languages that havericher morphology such as Turkish and Zulu, theOOV rate is much higher than other languages.Word Generation Tools and Settings For un-supervised learning of morphology, we use Mor-fessor CAT-MAP (v. 0.9.2) which was shown to bea very accurate morphological analyzer for mor-phologically rich languages (Creutz and Lagus,2007).
In order to be able to analyze Unicode-based data, we convert each character in ourdataset to some conventional ASCII character andthen train Morfessor on the mapped dataset; afterfinishing the training, we map the data back to theoriginal character set.
We use the default settingin Morfessor for unsupervised learning.For preparing the WFST, we use OpenFST (Ri-ley et al, 2009).
We get the top one million short-est paths (i.e., least costly paths of words) and ap-ply our reranking models on them.
It is worthpointing out that our WFSTs are character-based1353LanguageTraining Data Development DataType Token Type Token Type OOV% Token OOV%Assamese 8694 73151 7253 66184 49.57 8.28Bengali 9460 81476 7794 70633 50.65 8.47Pashto 6968 115069 6135 108137 44.89 4.25Persian 14047 71527 10479 42939 44.16 12.78Tagalog 6213 69577 5480 64334 54.95 7.81Turkish 11985 77128 9852 67042 56.84 12.34Zulu 15868 65655 13756 57141 68.72 21.76Persian-N 9204 31369 7502 18816 46.36 22.11Persian-V 2653 11409 1332 7318 41.07 9.01Table 1: Statistics of training and development data for morphology-based unsupervised word generationexperiments.and thus we also have a morphological analyzerthat can give all possible segmentations for a givenword.
By running the morphological analyzer onthe OOVs, we can have the potential upper boundof OOV reduction by the system (labeled ???
inTables 2 and 3).4.2 Lexicon Expansion ResultsThe results for lexicon expansion are shown in Ta-ble 2 for types and Table 3 for tokens.We use the trigraph WFST as our baselinemodel.
This model does not use any morphologi-cal information.
In this case, words are generatedaccording to the likelihood of their trigraphs, with-out using any information from the morphologi-cal segmentation.
We call this model the trigraphWFST (Tr.
WFST).
We consistently have betternumbers than this baseline in all of our modelsexcept for Pashto when measured by tokens.
?is the upper-bound OOV reduction for our expan-sion model: for each word in the development set,we ask if our model, without any vocabulary sizerestriction at all, could generate it.The best results (again, except for Pashto) areachieved using one of the three reranking methods(reranking by trigraph probabilities or morphemeboundaries) as opposed to doing no reranking.
Toour surprise, the Fixed Affix model does a slightlybetter job in reducing out of vocabulary than theBigram Affix model.
We can also see from theresults that reranking in general is very effective.We also compare our models with the case thatthere is much more training data and we do not dovocabulary expansion at all.
In Table 2 and Ta-ble 3, ?FP?
indicates the full language pack forthe Babel project data which is approximately sixto eight times larger than the limited pack trainingdata, and the full training data for Persian whichis approximately five times larger.
We see thatthe larger training data outperforms our methodsin all languages.
However, from the results of?,which is the upper-bound OOV reduction by ourexpansion model, for some languages such as As-samese, our numbers are close to the FP resultsand for Zulu it is even better than FP.We also study how OOV reduction is affectedby the size of the generated vocabulary.
Thetrends for different sizes of the lexicon expansionby Fixed Affix model that is reranked by trigraphprobabilities is shown in Figure 3.
As seen in theresults, for languages that have richer morphol-ogy, it is harder to achieve results near to the up-per bound.
As an outlier, morphology does nothelp for Pashto.
One possible reason might be thatbased on the results in Table 4, Morfessor does notexplore morphology in Pashto as well as other lan-guages.Morphological Complexity As for further anal-ysis, we can study the correlation between mor-phological complexity and hardness of reducingOOVs.
Much work has been done in linguis-tics to classify languages (Sapir, 1921; Greenberg,1960).
The common wisdom is that languagesare not either agglutinative or fusional, but areon a spectrum; however, no work to our knowl-edge places all languages (or at least the ones weworked on) on such a spectrum.
We propose sev-eral metrics.
First, we can consider the numberof unique affixival morphemes in each language,as determined by Morfessor.
As shown in Table 4(|pr| + |sf |), Zulu has the most morphemes andPashto the fewest.
A second possible metric of the1354LanguageTr.
Fixed Affix Model Bigram Affix Model FPWFST NRR W?Tr TRR BRR ?
NRR W?Tr TRR BRR ?Assamese 15.94 24.03 28.46 28.15 27.15 48.07 23.50 28.15 27.84 26.59 51.02 50.96Bengali 15.68 20.09 24.75 24.49 22.54 40.98 21.78 24.65 24.67 23.51 42.55 48.83Pashto 18.70 19.03 19.28 19.24 18.63 25.13 19.43 18.81 18.92 18.77 25.24 64.96Persian 12.83 18.95 18.39 19.30 19.99 50.11 18.58 18.09 18.65 18.84 53.13 58.45Tagalog 11.39 14.61 16.51 16.21 16.81 35.64 14.45 16.01 15.81 16.74 38.72 53.64Turkish 07.75 09.11 14.79 14.79 14.71 55.48 09.04 13.63 14.34 13.52 66.54 53.54Zulu 07.63 11.87 12.96 13.87 13.68 66.73 12.04 12.35 13.69 13.75 82.38 35.62Average 12.85 16.81 19.31 19.31 19.07 46.02 17.02 18.81 19.13 18.81 51.37 52.29Persian-N 14.86 24.67 22.74 22.83 24.15 37.32 23.78 21.68 22.51 23.32 38.38 -Persian-V 54.84 68.19 72.39 73.49 71.12 80.44 67.28 71.48 72.58 70.02 80.62 -Table 2: Type-based expansion results for the 50k-best list for different models.
Tr.
WFST stands fortrigraph WFST, NRR for no reranking, W?Tr for trigraph reweighting, TRR for trigraph-based rereank-ing, BRR for reranking morpheme boundary, and?
for the upper bound of OOV reduction via lexiconexpansion if we produce all words.
FP (full-pack data) shows the effect of using bigger data with the sizeof about seven times larger than our data set, instead of using our unsupervised approach.LanguageTr.
Fixed Affix Model Bigram Affix Model FPWFST NRR W?Tr TRR BRR ?
NRR W?Tr TRR BRR ?Assamese 18.07 25.70 29.43 29.12 28.13 47.88 25.34 29.06 28.82 27.64 50.31 58.03Bengali 17.79 20.91 25.61 25.27 23.65 40.60 22.58 25.20 25.41 24.77 42.22 55.92Pashto 21.27 19.40 19.94 19.92 18.59 25.45 19.68 19.40 19.29 18.72 25.58 71.46Persian 14.78 20.77 20.32 21.30 22.03 51.00 20.63 19.72 20.61 20.95 54.01 63.10Tagalog 12.88 14.55 16.88 16.36 16.60 33.95 14.37 16.12 16.12 16.38 37.07 61.53Turkish 09.97 11.42 17.82 17.67 17.23 56.54 11.05 16.82 17.41 15.98 66.54 59.68Zulu 08.85 13.70 14.72 15.62 15.67 68.07 13.70 14.07 15.47 15.60 87.90 41.27Average 14.80 18.06 20.67 20.75 20.27 44.78 18.19 20.48 20.45 20.01 51.95 58.71Persian-N 16.82 26.46 24.42 24.56 25.71 38.40 25.69 23.50 24.20 25.04 39.41 ?Persian-V 60.09 71.47 75.57 76.48 73.60 82.55 70.56 74.81 75.72 72.53 82.70 ?Table 3: Token-based expansion results for the 50k-best list for different models.
Abbreviations are thesame as Table 2.complexity of the morphology is by calculatingthe average number of unique prefix-suffix pairsin the training data after morpheme segmentationwhich is shown as |If | in Table 4.
Finally, a thirdpossible metric is the number of all possible wordsthat can be generated (|L|).
These three metricscorrelate fairly well across the languages.The metrics we propose also correlate withcommonly accepted classifications: e.g., Zulu andTurkish (highly agglutinative) have higher scoresin terms of our |pr| + |sf |, |If | and |L| metrics inTable 4 than other languages.
The results from fulllanguage packs in Table 3 also show that there isa reverse interaction of morphological complexityand the effect of blindly adding more data.
Thusfor morphologically rich languages, adding moredata is less effective than for languages with poormorphology.The size of the languages (|L|) suggests that weare suffering from vast overgeneration; we over-generate because in our model any affix can at-tach to any stem, which is not in general true.Thus there is a lack of linguistic knowledge suchas paradigm information (Stump, 2001) for eachword category in our model.
In other words, allmorphemes are treated the same in our modelwhich is not true in natural languages.
One wayto tackle this problem is through an unsupervisedPOS tagger.
The challenge here is that fully unsu-pervised POS taggers (without any tag dictionary)are not very accurate (Christodoulopoulos et al,2010).
Another way is through using joint mor-1355Figure 3: Trends for token-based OOV reduction with different sizes for the Fixed Affix model withtrigraph reranking.Language |pr| |stm| |sf| |L| |If|Assamese 4 4791 564 10.8M 1.8Bengali 3 6496 378 7.4M 1.5Pashto 1 5395 271 1.5M 1.3Persian 49 6998 538 184M 2.0Tagalog 179 4259 299 228M 1.5Turkish 45 5266 1801 427M 2.3Zulu 2254 5680 427 5.5B 2.8Persian-N 3 6121 268 4.9M 1.5Persian-V 43 788 44 1.5M 3.4Table 4: Information about the number of uniquemorphemes in the Fixed Affix model for eachdataset including empty affixes.
|L| shows theupper bound of the number of possible uniquewords that can be generated from the word gener-ation model.
|If | is the average number of uniqueprefix-suffix pairs (including empty pairs) for eachstem.phology and tagging models such as Frank et al(2013).Error Analysis on Turkish Unfortunately formost languages we could not find an availablerule-based or supervised morphological analyzerto verify the words generated by our model.
Theonly available tool for us is a Turkish finite-statemorphological analyzer (Oflazer, 1996) imple-mented with the Xerox FST toolkit (Beesley andKarttunen, 2003).
As we can see in Table 5, thesystem with the largest proportion of correct gen-erated words reranks the expansion with trigraphprobabilities using a Fixed Affix model.
Resultsalso show that we are overgenerating many non-sense words that we ought to be pruning from ourresults.
Another observation is that the recognitionpercentage of the morphological analyzer on INVwords is much higher than on OOVs, which showsthat OOVs in Turkish dataset are much harder toanalyze.1356Model PrecisionTr.
WFST 17.19Fixed Affix ModelNRR 13.36W?Tr 25.66TRR 26.30BRR 25.14Bigram Affix ModelNRR 12.94W?Tr 24.21TRR 25.39BRR 23.45Developmentwords 89.30INVs 95.44OOVs 84.64Table 5: Results from running a hand-craftedTurkish morphological analyzer (Oflazer, 1996)on different expansions and on the developmentset.
Precision refers to the percentage of the wordsare recognized by the analyzer.
The results on de-velopment are also separated into INV and OOV.Error Analysis on Persian From the best 50kword result for Persian (Fixed Affix Model:BRR),we randomly picked 200 words and manually an-alyzed them.
89 words are correct (45.5%) where55.0% of these words are from noun affixation,23.6% from verb clitics, 9.0% from verb inflec-tions, 5.6% from incorrect affixations that acci-dentally resulted in possible words, 4.5% from un-inflected stems, and a few from adjective affixa-tion.
Among incorrectly generated words, 65.8%are from combining a stem of one POS with af-fixes from another POS (e.g., attaching a noun af-fix to a verb stem), 14.4% from combining a stemwith affixes which are compatible with POS butnot allowed for that particular stem (e.g., there isa noun suffix that can only attach to a subset ofnoun stems), 9.0% are from wrong affixes pro-duced by Morfessor and others are from incorrectvowel harmony or double affixation.In order to study the effect of vocabulary ex-pansion more deeply, we trained a subset of allnouns and verbs in the same dataset (also shownin Table 1).
Verbs in Persian have rich but moreor less regular morphology, while nouns, whichhave many irregular cases, have rich morphol-ogy but not as rich as verbs.
The results in Ta-ble 4 show that Morfessor captures these phenom-ena.
Furthermore, our results in Table 2 and Ta-ble 3 show that our performance on OOV reduc-tion with verbs is far superior to our performancewith nouns.
We also randomly picked 200 wordsfrom each of the experiments (noun and verbs)to study the degree of correctness of generatedforms.
For nouns, 94 words are correct and forverbs only 71 words are correct.
Most verb errorsare due to incorrect morpheme extraction by Mor-fessor.
In contrast, most noun errors result fromaffixes that are only compatible with a subset ofall possible noun stems.
This suggests that if weconduct experiments using more accurate unsu-pervised morphology and also have a more fine-grained paradigm completion model, we mightimprove our performance.5 Conclusion and Future WorkWe have presented an approach to generating newwords.
This approach is useful for low-resource,morphologically rich languages.
It provides wordsthat can be used in HLT applications that requiretarget-language generation in this language, suchas ASR, OCR, and MT.
An implementation of ourapproach, named BabelGUM (Babel General Un-supervised Morphology), will be publicly avail-able.
Please contact the authors for more infor-mation.In future work we will explore the possibil-ity of jointly performing unsupervised morpho-logical segmentation with clustering of wordsinto classes with similar morphological behavior.These classes will extend POS classes.
We willtune the system for our purposes, namely OOV re-duction.AcknowledgementsWe thank Anahita Bhiwandiwalla, Brian Kings-bury, Lidia Mangu, Michael Picheny, Beno?
?tSagot, Murat Saraclar, and G?eraldine Walther forhelpful discussions.
The project is supported bythe Intelligence Advanced Research Projects Ac-tivity (IARPA) via Department of Defense U.S.Army Research Laboratory (DoD/ARL) contractnumber W911NF-12-C-0012.
The U.S. Govern-ment is authorized to reproduce and distributereprints for Governmental purposes notwithstand-ing any copyright annotation thereon.
Disclaimer:The views and conclusions contained herein arethose of the authors and should not be interpretedas necessarily representing the official policiesor endorsements, either expressed or implied, ofIARPA, DoD/ARL, or the U.S. Government.1357ReferencesMichele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of the 39th Annual Meetingon Association for Computational Linguistics, ACL?01, pages 26?33, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Kenneth R Beesley and Lauri Karttunen.
2003.
Finite-state morphology: Xerox tools and techniques.CSLI, Stanford.Tim Buckwalter.
2004.
Buckwalter Arabic Morpho-logical Analyzer Version 2.0.
LDC catalog numberLDC2004L02, ISBN 1-58563-324-0.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsu-pervised pos induction: How far have we come?In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages575?584.
Association for Computational Linguis-tics.Lionel Cl?ement, Beno?
?t Sagot, and Bernard Lang.2004.
Morphology based automatic acquisition oflarge-coverage lexica.
In Proceedings of the FourthInternational Conference on Language Resourcesand Evaluation (LREC?04).
European Language Re-sources Association (ELRA).Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing (TSLP), 4(1):3.Silviu Cucerzan and David Yarowsky.
2002.
Boot-strapping a multilingual part-of-speech tagger in oneperson-day.
In The 6th Conference on Natural Lan-guage Learning (CoNLL-2002), pages 1?7.Gr?egoire D?etrez and Aarne Ranta.
2012.
Smartparadigms and the predictability and complexity ofinflectional morphology.
In Proceedings of the 13thConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 645?653.Association for Computational Linguistics.Markus Dreyer and Jason Eisner.
2011.
Discover-ing morphological paradigms from plain text using adirichlet process mixture model.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing, pages 616?627.
Associ-ation for Computational Linguistics.Greg Durrett and John DeNero.
2013.
Supervisedlearning of complete morphological paradigms.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics: Human Language Technolo-gies, pages 1185?1195.
Association for Computa-tional Linguistics.Ramy Eskander, Nizar Habash, and Owen Rambow.2013.
Automatic extraction of morphological lex-icons from morphologically annotated corpora.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pages1032?1043, Seattle, Washington, USA, October.Association for Computational Linguistics.Markus Forsberg, Harald Hammarstr?om, and AarneRanta.
2006.
Morphological lexicon extractionfrom raw text data.
Advances in Natural LanguageProcessing, pages 488?499.Stella Frank, Frank Keller, and Sharon Goldwater.2013.
Exploring the utility of joint morphologicaland syntactic learning from child-directed speech.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages30?41.
Association for Computational Linguistics.Joseph H Greenberg.
1960.
A quantitative approach tothe morphological typology of language.
Interna-tional journal of American linguistics, pages 178?194.Nizar Habash and Owen Rambow.
2006.
MAGEAD:A morphological analyzer and generator for the Ara-bic dialects.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 681?688, Sydney, Aus-tralia.Nizar Habash.
2008.
Four techniques for online han-dling of out-of-vocabulary words in Arabic-Englishstatistical machine translation.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics on Human Language Technolo-gies: Short Papers, pages 57?60.
Association forComputational Linguistics.Mary Harper.
2013.
The babel program and lowresource speech technology.
In Automatic SpeechRecognition and Understanding Workshop (ASRU)Invited talk.Kimmo Koskenniemi.
1983.
Two-Level Model forMorphological Analysis.
In Proceedings of the 8thInternational Joint Conference on Artificial Intelli-gence, pages 683?685.Xin Lei, Wen Wang, and Andreas Stolcke.
2009.Data-driven lexicon expansion for Mandarin broad-cast news and conversation speech recognition.
InInternational conference on Acoustics, Speech andSignal Processing (ICASSP), pages 4329?4332.Christian Monson, Jaime Carbonell, Alon Lavie, andLori Levin.
2008.
Paramor: Finding paradigmsacross morphology.
Advances in Multilingual andMultimodal Information Retrieval, pages 900?907.Sylvain Neuvel and Sean A Fulop.
2002.
Unsuper-vised learning of morphology without morphemes.In Proceedings of the ACL-02 workshop on Morpho-logical and phonological learning-Volume 6, pages31?40.
Association for Computational Linguistics.1358Kemal Oflazer.
1996.
Error-tolerant finite-state recog-nition with applications to morphological analysisand spelling correction.
Computational Linguistics,22(1):73?89.Katsutoshi Ohtsuki, Nobuaki Hiroshima, MasahiroOku, and Akihiro Imamura.
2005.
Unsupervisedvocabulary expansion for automatic transcription ofbroadcast news.
In International conference onAcoustics, Speech and Signal Processing (ICASSP),pages 1021?1024.Mohammad Sadegh Rasooli, Manouchehr Kouhestani,and Amirsaeid Moloodi.
2013.
Development ofa Persian syntactic dependency treebank.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages306?314.
Association for Computational Linguis-tics.Michael Riley, Cyril Allauzen, and Martin Jansche.2009.
Openfst: An open-source, weighted finite-state transducer library and its applications to speechand language.
In Human Language TechnologiesTutorials: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 9?10.Edward Sapir.
1921.
Language: An introduction tothe study of speech.
Harcourt, Brace and company(New York).Kairit Sirts and Sharon Goldwater.
2013.
Minimally-supervised morphological segmentation using adap-tor grammars.
Transactions for the ACL, 1:255?266.Benjamin Snyder and Regina Barzilay.
2008.
Un-supervised multilingual learning for morphologi-cal segmentation.
In Proceedings of the 46th an-nual meeting of the association for computationallinguistics: Human language Technologies (ACL-HLT), pages 737?745.
Association for Computa-tional Linguistics.Gregory T. Stump.
2001.
A theory of paradigm struc-ture.
Cambridge.Mei Yang and Katrin Kirchhoff.
2006.
Phrase-basedbackoff models for machine translation of highly in-flected languages.
In Proceedings of Conference ofthe European Chapter of the Association for Com-putational Linguistics (EACL), pages 41?48, Trento,Italy.Dong Yang, Yi-Cheng Pan, and Sadaoki Furui.
2012.Vocabulary expansion through automatic abbrevia-tion generation for Chinese voice search.
ComputerSpeech & Language, 26(5):321?335.David Yarowsky and Richard Wicentowski.
2000.Minimally supervised morphological analysis bymultimodal alignment.
In Proceedings of the 38thAnnual Meeting on Association for ComputationalLinguistics, pages 207?216.1359
