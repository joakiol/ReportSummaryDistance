Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1202?1211,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPJoint Optimization for Machine Translation System CombinationXiaodong HeMicrosoft ResearchOne Microsoft Way, Redmond, WAxiaohe@microsoft.comKristina ToutanovaMicrosoft ResearchOne Microsoft Way, Redmond, WAkristout@microsoft.comAbstractSystem combination has emerged as apowerful method for machine translation(MT).
This paper pursues a joint optimizationstrategy for combining outputs from multipleMT systems, where word alignment, ordering,and lexical selection decisions are madejointly according to a set of feature functionscombined in a single log-linear model.
Thedecoding algorithm is described in detail and aset of new features that support this jointdecoding approach is proposed.
The approachis evaluated in comparison to state-of-the-artconfusion-network-based system combinationmethods using equivalent features and shownto outperform them significantly.1 IntroductionSystem combination for machine translation(MT) has emerged as a powerful method ofcombining the strengths of multiple MT systemsand achieving results which surpass those ofeach individual system (e.g.
Bangalore, et.
al.,2001, Matusov, et.
al., 2006, Rosti, et.
al.,2007a).
Most state-of-the-art system combinationmethods are based on constructing a confusionnetwork (CN) from several input translationhypotheses, and choosing the best output fromthe CN based on several scoring functions (e.g.Rosti et.
al., 2007a, He et.
al., 2008, Matusov etal.
2008).
Confusion networks allow word-levelsystem combination, which was shown tooutperform sentence re-ranking methods andphrase-level combination (Rosti, et.
al.
2007a).We will review confusion-network-basedsystem combination with the help of theexamples in Figures 1 and 2.
Figure 1 showstranslation hypotheses from three Chinese-to-English MT systems.
The general idea is tocombine hypotheses in a representation such asthe ones in Figure 2, where for each wordposition there is a set of possible words, shownin columns.1The final output is determined bychoosing one word from each column, which canbe a real word or the empty word ?.
For example,the CN in Figure 2a) can generate eight distinctsequences of words, including e.g.
?she boughtthe Jeep?
and ?she bought the SUV Jeep?.
Thechoice is performed to maximize a scoringfunction using a set of features and a log-linearmodel (Matusov, et.
al 2006, Rosti, et al 2007a).We can view a confusion network as anordered sequence of columns (correspondencesets).
Each word from each input hypothesisbelongs to exactly one correspondence set.
Eachcorrespondence set contains at most one wordfrom each input hypothesis and contributesexactly one of its words (including the possible?)
to the final output.
Final words are output inthe order of correspondence sets.
In order toconstruct such a representation, we need to solvethe following two sub-problems:  arrange wordsfrom all input hypotheses into correspondencesets (alignment problem) and ordercorrespondence sets (ordering problem).
Afterconstructing the confusion network we need tosolve a third sub-problem: decide which words tooutput from each correspondence set (lexicalchoice problem).In current state-of-the-art approaches, theconstruction of the confusion network isperformed as follows: first, a backbonehypothesis is selected.
The backbone hypothesisdetermines the order of words in the final systemoutput, and guides word-level alignments forconstruction of columns of possible words ateach position.
Let us assume that for ourexample in Figure 1, the second hypothesis isselected as a backbone.
All other hypotheses arealigned to the backbone such that thesealignments are one-to-one; empty words areinserted where necessary to make one-to-one1 This representation is alternative to directed acyclicgraph representations of confusion networks.1202alignment possible.
Words in all hypotheses aresorted by the position of the backbone word theyalign to and the confusion network is determined.It is clear that the quality of selection of thebackbone and alignments has a large impact onthe performance, because the word order isdetermined by the backbone, and the set ofpossible words at each position is determined byalignment.
Since the space of possiblealignments is extremely large, approximate andheuristic techniques have been employed toderive them.
In pair-wise alignment, eachhypothesis is aligned to the backbone in turn,with separate processing to combine the multiplealignments.
Several models have been used forpair-wise alignment, starting with TER andproceeding with more sophisticated techniquessuch as HMM models, ITG, and IHMM (Rostiet.
al 2007a, Matusov et al2008, Krakos et al2008, He et al 2008).
A major problem withsuch methods is that each hypothesis is alignedto the backbone independently, leading to sub-optimal behavior.
For example, suppose that weuse a state-of-the-art word alignment model forpairs of hypotheses, such as the IHMM.
Figure 1shows likely alignment links between every pairof hypotheses.
If Hypothesis 1 is aligned toHypothesis 2 (the backbone), Jeep is likely toalign to SUV because they express similarChinese content.
Hypothesis 3 is separatelyaligned to the backbone and since the alignmentis constrained to be one-to-one, SUV is aligned toSUV and Jeep to an empty word which isinserted after SUV.
The network in Figure 2a) isthe result of this process.
An undesirableproperty of this CN is that the two instances ofJeep are placed in separate columns and cannotvote to reinforce each other.Incremental alignment methods have beenproposed to relax the independence assumptionof pair-wise alignment (Rosti et al 2008, Li et al2009).
Such methods align hypotheses to apartially constructed CN in some order.
Forexample, if in such method, Hypothesis 3 is firstaligned to the backbone, followed by Hypothesis1, we are likely to arrive at the CN in Figure 2b)in which the two instances of Jeep are aligned.However, if Hypothesis 1 is aligned to thebackbone first, we would still get the CN inFigure 2a).
Notice that the desirable output ?Shebought the Jeep SUV?
cannot be generated fromeither of the confusion networks because a re-reordering of columns would be required.A common characteristic of CN-basedapproaches is that the order of words (backbone)and the alignment of words (correspondencesets) are decided as greedy steps independentlyof the lexical choice for the final output.
Thebackbone and alignment are optimized accordingto auxiliary scoring functions and heuristicswhich may or may not be optimal with respect toproducing CNs leading to good translations.
Insome recent approaches, these assumptions arerelaxed to allow each input hypothesis as abackbone.
Each backbone produces a separateCN and the decision of which CN to choose istaken at a later decoding stage, but this stillrestricts the possible orders and alignmentsgreatly (Rosti et al 2008, Matusov et al 2008).In this paper, we present a joint optimizationmethod for system combination.
In this method,the alignment, ordering and lexical selection sub-problems are solved jointly in a single decodingframework based on a log-linear model.sheshsheboughtboughtbuysthethetheJeepSUVSUV Jeep??
?Figure 1.
Three MT system hypotheses with pair-wise alignments.she bought the Jeep ?she buys the SUV ?she bought the SUV Jeepa) Confusion network with pair-wise alignment.she bought the ?
Jeepshe buys the SUV ?she bought the SUV Jeepb) Confusion network with incremental alignment.Figure 2.
Correspondence sets of confusion networksunder pair-wise and incremental alignment, using thesecond hypothesis as a backbone.2 Related WorkThere has been a large body of work on MTsystem combination.
Among confusion-network-based algorithms, most relevant to our work arestate-of-the-art methods for constructing wordalignments (correspondence sets) and methodsfor improving the selection of a backbonehypothesis.
We have already reviewed such workin the introduction and will note relation to1203specific models throughout the paper as wediscuss specifics of our scoring functions.In confusion network algorithms which usepair-wise (or incremental) word-level alignmentalgorithms for correspondence set construction,problems of converting many-to-manyalignments and handling multiple insertions anddeletions need to be addressed.
Prior work hasused a number of heuristics to deal with theseproblems (Matusov, et.
al., 2006, He et al08).Some work has made such decisions in a moreprincipled fashion by computing model-basedscores (Matusov et al 2008), but still special-purpose algorithms and heuristics are needed anda single alignment is fixed.In our approach, no heuristics are used toconvert alignments and no concept of a backboneis used.
Instead, the globally highest scoringcombination of alignment, order, and lexicalchoice is selected (subject to search error).Other than confusion-network-basedalgorithms, work most closely related to ours isthe method of MT system combination proposedin (Jayaraman and Lavie 2005), which we willrefer to as J&L.
Like our method, this approachperforms word-level system combination and isnot limited to following the word order of asingle backbone hypothesis; it also allows moreflexibility in the selection of correspondence setsduring decoding, compared to a confusion-network-based approach.
Even though theiralgorithm and ours are broadly similar, there areseveral important differences.Firstly, the J&L approach is based on pair-wise alignments between words in differenthypotheses, which are hard and do not haveassociated probabilities.
Every word in everyhypothesis is aligned to at most one word fromeach of the remaining hypotheses.
Thus there isno uncertainty about which words should belongto the correspondence set of an aligned word w,once that word is selected to extend a partialhypothesis during search.
If words do not havecorresponding matching words in somehypotheses, heuristic matching to currentlyunused words is attempted.In contrast, our algorithm is based on thedefinition of a joint scoring model, which takesinto account alignment uncertainty and combinesinformation from word-level alignment models,ordering and lexical selection models, to addressthe three sub-problems of word-level systemcombination.
In addition to the language modeland word-voting features used by the J&Lmodel, we incorporate features which measurealignment confidence via word-level alignmentmodels and features which evaluate re-orderingvia distortion models with respect to originalhypotheses.
While the J&L search algorithmincorporates a number of special-purposeheuristics to address phenomena of unused wordslagging behind the last used words, the goal inour work is to minimize heuristics and performsearch to jointly optimize the assignment ofhidden variables (ordered correspondence sets)and observed output variables (words in finaltranslations).Finally, the J&L method has not beenevaluated in comparison to confusion-network-based methods to study the impact of performingjoint decoding for the three sub-problems.3 NotationBefore elaborating the models and decodingalgorithms, we first clarify the notation that willbe used in the paper.We denote by ?
=  ?1 ,?
,??
the set ofhypotheses from multiple MT systems, where ?
?is the hypothesis from the i-th system and ??
is aword sequence ?
?,1 ,?
,??,?(?)
with length ?(?)
.For simplicity, we assume that each systemcontributes only its 1-best hypothesis forcombination.
Accordingly, the i-th hypothesis ?
?will be associated with a weight ?(?)
which isthe weight of the i-th system.
In the scenario thatN-best lists are available from individual systemsfor combination, the weight of each hypothesiscan be computed based on its rank in the N-bestlist (Rosti et.
al.
2007a).Like in CN-based system combination, weconstruct a set of ordered correspondence sets(CS) from input hypotheses, and select one wordfrom each CS to form the final output.
A CS isdefined as a set of (possibly empty) words, onefrom each hypothesis, that implicitly align toeach other and that contributes exactly one of itswords to the final output.
A valid complete set ofCS includes each non-empty word from eachhypothesis in exactly one CS.
As opposed to CN-based algorithms, our ordered correspondencesets are constructed during a joint decodingprocess which performs lexical selection at thesame time.To facilitate the presentation of our features,we define notation for ordered CS.
A sequenceof correspondence sets is denoted byC=??1,?
,???
.
Each correspondence set isspecified by listing the positions of each of thewords in the CS in their respective input1204hypotheses.
Each input hypothesis is assumedto have one special empty word ?
at position 0.A CS is denoted by ??
?1 ,?
, ?
?= ?1,?1 ,?
,??,??
, where ??,??
is the li-th word inthe i-th hypothesis and the word position vector?
=  ?1 ,?
, ???
specifies the position of eachword in its original hypothesis.
Correspondingly,word ??
,??
has the same weight ?(?)
as itsoriginal hypothesis??
.
As an example, the lasttwo correspondence sets specified by the CN inFigure 2a) would be specified as ?
?4 =??
4,4,4 = {???
?, ??
?, ???}
and ?
?5 =??
0,0,5 = {?, ?, ????
}.As opposed to the CS defined in aconventional CN, words that have the samesurface form but come from different hypothesesare not collapsed to be one single candidate sincethey have different original word positions.
Weneed to trace each of them separately during thedecoding process.4 A Joint Optimization Framework ForSystem CombinationThe joint decoding framework chooses optimaloutput according to the following log-linearmodel:??
=  argmax???,???,??????
??
?
??(?,?,?,?)?
?=1where we denote by C the set of all possiblevalid arrangements of CS, O the set of allpossible orders of CS, W the set of all possibleword sequences, consisting of words from theinput hypotheses.
{??(?,?,?,?)}
are thefeatures and {??}
are the feature weights in thelog-linear model, respectively.4.1 FeaturesA set of features are used in this framework.Each of them models one or more of thealignment, ordering, and lexical selection sub-problems.
Features are defined as follows.Word posterior model:The word posterior feature is the same as theone proposed by Rosti et.
al.
(2007a).
i.e.,???
?,?,?,?
=  ???
?
??
????
?=1where the posterior of a single word in a CS iscomputed based on a weighted voting score:?
??
,??
??
= ?
??,??
??
?1 ,?
, ?
?=  ?(?)??=1?(??
,??
= ??
,??
)and M is the number of CS generated.
Notethat M may be larger than the length of theoutput word sequence w since some CS maygenerate empty words.Bi-gram voting model:The second feature we used is a bi-gramvoting feature proposed by Zhao and He (2009),i.e., for each bi-gram  ??
,?
?+1  , a weightedposition-independent voting score is computed:?
??
,?
?+1  ?
=  ?(?)??=1?
( ??
,?
?+1 ?
??
)And the global bi-gram voting feature isdefined as:????
?,?,?,?
=  ???
?
??
,?
?+1  ?|?
|?1?=1Distortion model:Unlike in the conventional CN-based systemcombination, flexible orders of CS are allowed inthis joint decoding framework.
In order to modelthe distortion of different orderings, a distortionmodel between two CS is defined as follows:First we define the distortion cost between twowords at a single hypothesis.
Similarly to thedistortion penalty in the conventional phrase-based decoder (Koehn 2004b), the distortion costof jumping from a word at position i to anotherword at position j, d(i,j), is proportional to thedistance between i and j, e.g., |i-j|.
Then, thedistortion cost of jumping from one CS, whichhas a position vector recording the originalposition of each word in that CS, to another CSis a weighted sum of single-hypothesis-baseddistortion costs:?(???
,??
?+1)  =  ?(?)??=1?
|??
,?
?
??+1,?
|where ??
,?
and ??+1,?
are the k-th element ofthe word position vector of CSm and CSm+1,respectively.
For the purpose of computing thedistortion feature, the position of an emptyword is taken to be the same as the position of1205the last visited non-empty word from the samehypothesis.The overall ordering feature can then becomputed based on ?(???
,???+1):????
?,?,?,?
= ?
?(???
,???+1)?
?1?=1It is worth noting that this is not the onlyfeature modeling the re-ordering behavior.Under the joint decoding framework, otherfeatures such as the language model and bi-gramvoting affect the ordering as well.Alignment model:Each CS consists of a set of words, one fromeach hypothesis, that are implicitly aligned toeach other.
Therefore, a valid complete set of CSdefines the word alignment among differenthypotheses.
In this paper, we derive an alignmentscore of a CS based on alignment scores of wordpairs in that CS.
To compute scores for wordpairs, we perform pair-wise hypothesis alignmentusing the indirect HMM (He et al 2008) forevery pair of input hypotheses.
Note that thisinvolves a total of N by (N-1)/2 bi-directionalhypothesis alignments.
The alignment score for apair of words  ??
,??
and  ??
,??
is defined as theaverage of posterior probabilities of alignmentlinks in both directions and is thus directionindependent:?
??
,??
,??
,??
=12?(???
= ??
|??
,??)
+  ?(???
= ??
|??
,??
)If one of the two words is ?, the posterior ofaligning word ?
to state j is computed assuggested by Liang et al (2006), i.e.,?
?0 = ??
??
,??
=  1?
?
??
= ??
??
,???(?
)?=1And ?(???
= 0|??
,??)
can be computed by theHMM directly.If both words are ?, then a pre-defined  ???
isassigned, i.e., ?
?0 = 0 ??
,??
= ???
, where ??
?can be optimized on a held-out validation set.For a CS of words, if we set the j-th word asan anchor word, the probability that all otherwords align to that word is:?(?|??)
=  ?
??
,??
,??
,????=1??
?The alignment score of the whole CS is aweighted sum of the logarithm of the abovealignment probabilities, i.e.,????
(??)
= ?(?)??=1???
?(?|??
)and the global alignment score is computed as:????
?,?,?,?
=  ????
(???)?
?=1Entropy model:In general, it is preferable to align the sameword from different hypotheses into a commonCS.
Therefore, we use entropy to model thepurity of a CS.
The entropy of a CS is defined as:???
??
= ???(??
?1 ,?
, ?? )
=?
??
,??
??
????
??,??
????
?=1where the sum is taken over all distinct words inthe CS.
Then the global entropy score iscomputed as:????
?,?,?,?
=  ???(??=1???
)Other features used in our log-linear modelinclude the count of real words |w|, a n-gramlanguage model, and the count M of CS sets.These features address one or more of thethree sub-problems of MT system combination.By performing joint decoding with all thesefeatures working together, we hope to derivebetter decisions on alignment, ordering andlexical selection.5 Joint Decoding5.1 Core algorithmDecoding is based on a beam search algorithmsimilar to that of the phrase-based MT decoder(Koehn 2004b).
The input is a set of translationhypotheses to be combined, and the final output1206sentence is generated left to right.
Figure 3illustrates the decoding process, using theexample input hypotheses from Figure 1.
Eachdecoding state represents a partial sequence ofcorrespondence sets covering some of the wordsin the input hypotheses and a sequence of wordsselected from the CS to form a partial outputhypothesis.
The initial decoding state has anempty sequence of CS and an empty outputsequence.
A state corresponds to a completeoutput candidate if its CS covers all input words.lm: ?
bought thea) a decoding statelm: ?
bought the lm: ?
bought theb)  seed stateslm: ?
bought the lm: ?
bought thec) correspondence set stateslm: ?
the Jeep lm: ?
the Jeepd) decoding statesFigure 3.
Illustration of the decoding process.In practice, because the features overhypotheses can be decomposed, we do not needto encode all of this information in a decodingstate.
It suffices to store a few attributes.
Theyinclude positions of words from each inputhypothesis that have been visited, the last twonon-empty words generated (if a tri-gram LM isused), and an "end position vector (EPV)"recording positions of words in the last CS,which were just visited.
In the figure, the visitedwords are shown with filled circles and the EPVis shown with a dotted pattern in the filledcircles.
Words specified by the EPV areimplicitly aligned.
In the state in Figure 3 a) thefirst three words of each hypothesis have beenvisited, the third word of each hypothesis is thelast word visited (in the EPV), and the last twowords produced are ?bought the?.
The states alsorecord the decoding score accumulated so far andan estimated future score to cover words thathave not been visited yet (not shown).The expansion from one decoding state to aset of new decoding states is illustrated in Figure3.
The expansion is done in three steps with thehelp of intermediate states.
Starting from adecoding state as shown in Figure 3a), first a setof ?seed states?
as shown in Figure 3b) aregenerated.
Each seed state represents a choice ofone of unvisited words, called a ?seed word?which is selected and marked as visited.
Forexample, the word Jeep from the first hypothesisand the word SUV from the second hypothesisare selected in the two seed states shown inFigure 3b), respectively.
These seed statesfurther expand into a set of "CS states" as shownin Figure 3c).
I.e., a CS is formed by picking oneword from each of the other hypotheses which isunvisited and has a valid alignment link to theseed word.
Figure 3c) shows two CS statesexpanded from the first seed state of Figure 3b),using Jeep from the first hypothesis as a seedword.
In one of them the empty word from thesecond hypothesis is chosen, and in the other, theword SUV is chosen.
Both are allowed by thealignments illustrated in Figure 1.
Finally, eachCS state generates one or more completdecoding states, in which a word is chosen fromthe current CS and the EPV vector is advanced toreflect the last newly visited words.
Figure 3d)shows two such states, descending from thecorresponding CS states in 3c).
After one moreexpansion the state in 3d) on the left can generatethe translation ?She bought the Jeep SUV?,which cannot be produced by either confusionnetwork in Figure 2.5.2 PruningThe full search space of joint decoding is aproduct of the alignment, ordering, and lexicalselection spaces.
Its size is exponential in thelength of the sentence and the number ofhypotheses involved in combination.
Therefore,pruning techniques are necessary to reduce thesearch space.First we will prune down the alignment space.Instead of allowing any alignment link between1207arbitrary words of two hypotheses, only linksthat have alignment score higher than a thresholdare allowed, plus links in the union of the Viterbialignments in both directions.
In order to preventthe garbage collection problem where manywords align to a rare word at the other side(Moore, 2004), we further impose the limit that ifone word is aligned to more than T words, theselinks are sorted by their alignment score and onlythe top T links are kept.
Meanwhile, alignmentsbetween a real word and ?
are always allowed.We then prune down the ordering space bylimiting the expansion of new states.
Only statesthat are adjacent to their preceding states arecreated.
Two states are called adjacent if theirEPVs are adjacent, i.e., given the EPV of thepreceding state m as  ??
,1 ,?
, ??
,?
?and theEPV of the next state m+1 as??+1,1,?
, ??+1,?
?, if at least at onedimension k, ??+1,?
= ??
,?+1, then these twostates are adjacent.
When checking theadjacency of two states, the position of anempty word is taken to be the same as theposition of the last visited non-empty wordfrom the same hypothesis.The number of possible CS states expandedfrom a decoding state is exponential in thenumber of hypotheses.
In decoding, these CSstates are sorted by their alignment scores andonly the top K CS states are kept.The search space can be further pruned downby the widely used technique of pathrecombination and by best-first pruning.Path recombination is a risk-free pruningmethod.
Two paths can be recombined if theyagree on a) words from each hypothesis that havebeen visited so far, b) the last two real wordsgenerated, and c) their EPVs.
In such case, weonly need to keep the path with the higher score.Best-first pruning can help to reduce thesearch space even further.
In the decodingprocess we compare paths that have generatedthe same number of words (both real and emptywords) and only keep a certain number of mostpromising paths.
Pruning is based on anestimated overall score of each path, which is thesum of the decoding score accumulated so farand an estimated future score to cover the wordsthat have not been visited.
Next we discuss thefuture score computation.5.3 Computing the future scoreIn order to estimate the future cost of anunfinished path, we treat the unvisited words ofone input hypothesis as a backbone, and apply agreedy search for alignment based on it; i.e., foreach word of this backbone, the most likelywords (based on the alignment link scores) fromother hypotheses, one word from eachhypothesis, are collected to form a CS.
These CSare ordered according to the word order of thebackbone and form a CN.
Then, a light decodingprocess with a search beam of size one is appliedto decode this CN and find the approximatefuture path, with future feature scores computedduring the decoding process.
If there are leftoverwords not included in this CN, they are treated inthe way described in section 5.4.
Additionally,caching techniques are applied to speed up thecomputation of future scores further.Given the method discussed above, we canestimate a future score based on each inputhypothesis, and the final future score is estimatedas the best of these hypothesis-dependent scores.5.4  Dealing with leftover input wordsAt a certain point a path will reach the end, i.e.,no more states can be generated from itaccording to the state expansion requirement.Then it is marked as a finished path.
However,sometimes the state may contain a few inputwords that have not been visited.
An example ofthis situation is the second state in Figure 3d).The word SUV in the third input hypothesis isleft unvisited and it cannot be selected nextbecause there is no adjacent state that can begenerated.
For such cases, we need to computean extra score of covering these leftover words.Our approach is to create a state that producesthe same output translation, but also covers allremaining words.
For each leftover word, wecreate a pseudo CS that contains just that wordplus ?
?s from all other hypotheses, and let itoutput ?.
Moreover, that CS is inserted at a placesuch that no extra distortion cost is incurred.Figure 4 shows an example using the secondstate in Figure 3d).
The last two words from thefirst two MT hypotheses ?the Jeep?
and ?theSUV?
align to the third and fifth words of thethird hypothesis ?the Jeep?
; the word w3,4 fromthe third hypothesis is left unvisited.
The originalpath has two CS and one left-over word w3,4.
It isexpanded to have three CS, with a pseudo CSinserted between the two CS.It is worth noting that the new inserted pseudoCS will not affect the word count feature andcontextually dependent feature scores such as theLM and bi-gram voting, since it only generatesan empty word.
Moreover, it will not affect the1208distortion score either.
For example, as shown inFigure 4, the distortion cost of jumping fromword w2,3  to ?2  and then to w2,4 is the same asthe cost of jumping from w2,3  to w2,4 given theway we assign position to empty word and thefact that the distortion cost is proportional to thedifference between word positions.Scores of other features for this pseudo CSsuch as word posterior (of ?
), alignment score,CS entropy, and CS count are all local scores andcan be computed easily.
Unlike future scoreswhich are approximate, the score computed inthis process is exact.
Adding this extra score tothe existing score accumulated in the final stategives the complete score of this finished path.When all paths are finished, the one with the bestcomplete score is returned as the final outputsentence.w1,3 w1,4   w1,3 ?1 w1,4w2,3 w2,4  =>  w2,3  ?2  w2,4w3,3 w3,4 w3,5  w3,3 w3,4 w3,5Figure 4.
Expanding a leftover word to a pseudocorrespondence set.6 Evaluation6.1 Experimental conditionsFor the joint decoding method, the threshold foralignment-score-based pruning is set to 0.25 andthe maximum number of words that can align tothe same word is limited to 3.
We call this thestandard setting.
The joint decoding approach isevaluated on the Chinese-to-English (C2E) testset of the 2008 NIST Open MT Evaluation(NIST 2008).
Results are reported in caseinsensitive BLEU score in percentages(Papineni et.
al., 2002).The NIST MT08 C2E test set contains 691and 666 sentences of data from two genres,newswire and web-data, respectively.
Each testsentence has four references provided by humantranslators.
Individual systems in ourexperiments belong to the official submissions ofthe MT08 C2E constraint-training track.
Eachsubmission provides 1-best translation of thewhole test set.
In order to train feature weights,the original test set is divided into two parts,called the dev and test set, respectively.
The devset consists of the first half of both newswire andweb-data, and the test set consists of the secondhalf of data of both genres.There are 20 individual systems available.
Weranked them by their BLEU score results on thedev set and picked the top five systems,excluding systems ranked 5th and 6th since theyare subsets of the first entry (NIST 2008).Performance of these systems on the dev and testsets is shown in Table 1.The baselines include a pair-wise hypothesisalignment approach using the indirect HMM(IHMM) proposed by He et al (2008), and anincremental hypothesis alignment approach usingthe incremental HMM (IncHMM) proposed byLi et al (2009).
The lexical translation modelused to compute the semantic similarity isestimated from two million parallel sentence-pairs selected from the training corpus of MT08.The backbone for the IHMM-based approach isselected based on Minimum Bayes Risk (MBR)using a BLEU-based loss function.
The variousparameters of the IHMM and the IncHMM aretuned on the dev set.
The same IHMM is used tocompute the alignment feature score for the jointdecoding approach.The final combination output can be obtainedby decoding the CN with a set of features.
Thefeatures used for the baseline systems are thesame as the features used by the joint decodingapproach.
Some of these features are constantacross decoding hypotheses and can be ignored.The non-constant features are word posterior, bi-gram voting, language model score, and wordcount.
They are computed in the same way as forthe joint decoding approach.System weights and feature weights aretrained together using Powell's search for theIHMM-based approach.
Then the same systemweights are applied to both IncHMM and JointDecoding -based approaches, and the featureweights of them are trained using the max-BLEUtraining method proposed by Och (2003) andrefined by Moore and Quirk (2008).Table 1: Performance of individual systems onthe dev and test setSystem ID dev testSystem A 32.88 31.81System B 32.82 32.03System C 32.16 31.87System D 31.40 31.32System E 27.44 27.676.2 Comparison against baselinesTable 2 lists the BLEU scores achieved by thetwo baselines and the joint decoding approach.Both baselines surpass the best individual system1209significantly.
However, the gain of incrementalHMM over IHMM is smaller than that reportedin Li et al (2009).
One possible reason of suchdiscrepancy could be that fewer hypotheses areused for combination in this experimentcompared to that of Li et al (2009), so theperformance difference between them isnarrowed accordingly.
Despite that, the proposedjoint decoding method outperforms both IHMMand IncHMM baselines significantly.Table 2: Comparison between the joint decodingapproach and the two baselinesmethod dev testIHMM 36.91 35.85IncHMM 37.32 36.38Joint Decoding 37.94 37.20** The gains of Joint Decoding over IHMM andIncHMM are both with a statistical significance level >99%, measured based on the paired bootstrap re-sampling method (Koehn 2004a)6.3 Comparison of alignment pruningThe effect of alignment pruning is also studied.We tested with limiting the allowable links tojust those that in the union of bi-directionalViterbi alignments.The results are presented in Table 3.Compared to the standard setting, allowing onlylinks in the union of the bi-directional Viterbialignments causes slight performancedegradation.
On the other hand, it stilloutperforms the IHMM baseline by a fair margin.This is because the joint decoding approach iseffectively resolving the ambiguous 1-to-manyalignments and deciding proper places to insertempty words during decoding.Table 3: Comparison between different settingsof alignment pruningSetting Teststandard settings 37.20union of Viterbi 36.886.4 Comparison of ordering constraintsIn order to investigate the effect of allowingflexible word ordering, we conductedexperiments using different constraints on theordering of CS in the decoding process.
In thefirst case, we restrict the order of CS to followthe word order of a backbone, which is one ofthe input hypotheses selected by MBR-BLEU.
Inthe second case, the order of CS is constrained tofollow the word order of at least one of the inputhypotheses.
As shown in Table 4, in comparisonto the standard setting that allows backbone-freeword ordering, the constrained settings did notlead to significant performance degradation.
Thisindicates that most of the gain due to the jointdecoding approach comes from the jointoptimization of alignment and word selection.
Itis possible, though, that if we lift the CSadjacency constraint during search, we mightderive more benefit from flexible word ordering.Table 4: Effect of ordering constraintsSetting teststandard settings 37.20monotone w.r.t.
backbone 37.22monotone w.r.t.
any hyp.
37.127 DiscussionThis paper proposed a joint optimizationapproach for word-level combination oftranslation hypotheses from multiple machinetranslation systems.
Unlike conventionalconfusion-network-based methods, alignmentsbetween words from different hypotheses are notpre-determined and flexible word orderings areallowed.
Decisions on word alignment betweenhypotheses, word ordering, and the lexical choiceof the final output are made jointly according toa set of features in the decoding process.
A newset of features to model alignment and re-ordering behavior is also proposed.
The methodis evaluated against state-of-the-art baselines onthe NIST MT08 C2E task.
The joint decodingapproach is shown to outperform baselinessignificantly.Because of the complexity of search, achallenge for our approach is combining a largenumber of input hypotheses.
When N-besthypotheses from the same system are added, it ispossible to pre-compute and fix the one-to-oneword alignment among the same-systemhypotheses; such pre-computation is reasonablegiven our observation that the disagreementamong hypotheses from different systems islarger than that among hypotheses from the samesystem.
This will reduce the alignment searchspace to be the same as that for 1-best case.
Weplan to study this setting in future work.To further improve the performance of ourapproach we see the biggest opportunity indeveloping better estimates of future scores andincorporating additional features.
Besidepotential performance improvement, they mayhelp on more effective pruning and speed up theoverall decoding process as well.1210ReferencesSrinivas Bangalore, German Bordel, and GiuseppeRiccardi.
2001.
Computing consensus translationfrom multiple machine translation systems.
InProceedings of IEEE ASRU.Xiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore 2008.
Indirect HMMbased Hypothesis Alignment for CombiningOutputs from Machine Translation Systems.
InProceedings of EMNLP.Shyamsundar Jayaraman and Alon Lavie.
2005.Multi-engine machine translation guided by explicitword matching.
In Proceedings of EAMT.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer 2008.
Machine TranslationSystem Combination using ITG-based Alignments.In Proceedings of ACL.Philipp Koehn, 2004a, Statistical Significance Testsfor Machine Translation Evaluation.
In Proceedingsof EMNLP.Philipp Koehn.
2004b.
Pharaoh: A Beam SearchDecoder For Phrase Based Statistical MachineTranslation Models.
In Proceedings of AMTA.Chi-Ho Li, Xiaodong He, Yupeng Liu and Ning Xi,2009.
Incremental HMM Alignment for MTSystem Combination.
In Proceedings of ACL.Percy Liang, Ben Taskar, and Dan Klein.
2006.Personal CommunicationEvgeny Matusov, Nicola Ueffing and Hermann Ney.2006.
Computing Consensus Translation fromMultiple Machine Translation Systems usingEnhanced Hypothesis Alignment.
In Proceedings ofEACL.Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,Nicola Bertoldi, Daniel D?chelotte, MarcelloFederico, Muntsin Kolss, Young-Suk Lee, Jos?
B.Mari?o, Matthias Paulik, Salim Roukos, HolgerSchwenk, and Hermann Ney.
2008.
Systemcombination for  machine translation of spoken andwritten language.
IEEE transactions on audiospeech and language processing 16(7).Robert C. Moore and Chris Quirk.
2008.
RandomRestarts in Minimum Error Rate Training forStatistical Machine Translation, In Proceedings ofCOLINGRobert C. Moore.
2004.
Improving IBM WordAlignment Model 1, In Proceedings of ACL.NIST 2008.
The NIST Open Machine TranslationEvaluation.www.nist.gov/speech/tests/mt/2008/doc/Franz J. Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofACL.Kishore Papineni, Salim Roukos, Todd Ward andWei- Jing Zhu 2002.
BLEU: a Method forAutomatic Evaluation of Machine Translation.
InProceedings of ACL.Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,Richard Schwartz, Necip Fazil Ayan, and Bonnie J.Dorr.
2007a.
Combining outputs from multiplemachine translation systems.
In Proceedings ofNAACL-HLT.Antti-Veikko I. Rosti, Spyros Matsoukas, and RichardSchwartz 2007b.
Improved Word-level SystemCombination for Machine Translation.
InProceedings of ACL.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz 2008.
IncrementalHypothesis Alignment for Building ConfusionNetworks with Application to Machine TranslationSystem Combination.
In Proceedings of the 3rdACL Workshop on SMT.Yong Zhao and Xiaodong He, 2009.
Using N-grambased Features for Machine Translation SystemCombination.
In Proceedings of NAACL-HLT1211
