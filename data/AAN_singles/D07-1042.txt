Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
400?409, Prague, June 2007. c?2007 Association for Computational LinguisticsFlexible, Corpus-Based Modelling of Human Plausibility JudgementsSebastian Pad?
and Ulrike Pad?Computational LinguisticsSaarland UniversitySaarbr?cken, Germany{pado,ulrike}@coli.uni-sb.deKatrin ErkDept.
of LinguisticsUniversity of Texas at AustinAustin, Texaskatrin.erk@mail.utexas.eduAbstractIn this paper, we consider the computationalmodelling of human plausibility judgementsfor verb-relation-argument triples, a taskequivalent to the computation of selectionalpreferences.
Such models have applicationsboth in psycholinguistics and in computa-tional linguistics.By extending a recent model, we obtaina completely corpus-driven model for thistask which achieves significant correlationswith human judgements.
It rivals or exceedsdeeper, resource-driven models while exhibit-ing higher coverage.
Moreover, we show thatour model can be combined with deeper mod-els to obtain better predictions than from ei-ther model alone.1 IntroductionOne fundamental and intuitive finding in experimen-tal psycholinguistics is that humans judge the plau-sibility of a verb-argument pair vastly differently de-pending on the semantic relation in the pair.
Table 1lists example human judgements which McRae etal.
(1998) elicited by asking about the plausibility of,e.g., a hunter shooting (relation agent) or being shot(relation patient).
McRae et al found that ?hunter?
isjudged to be a very plausible agent of ?shoot?
andan implausible patient, while the reverse is true for?deer?.
In linguistics, this phenomenon is explainedby selectional preferences on verbs?
argument po-sitions; we use plausibility and fit with selectionalpreferences interchangeably.Verb Relation Noun Plausibilityshoot agent hunter 6.9shoot patient hunter 2.8shoot agent deer 1.0shoot patient deer 6.4Table 1: Verb-relation-noun triples with plausibilityjudgements on a 7-point scale (McRae et al, 1998)In this paper, we consider computational mod-els that predict human plausibility ratings, or thefit of selectional preferences and argument, forsuch (verb, relation, argument), in short, (v, r, a),triples.
Being able to model this type of data is rel-evant in a number of ways.
From the point of viewof psycholinguistics, selectional preferences have animportant effect in human sentence processing (e.g.,McRae et al (1998), Trueswell et al (1994)), andmodels of selectional preferences are therefore nec-essary to inform models of this process (Pad?
et al,2006).
In computational linguistics, a multitude oftasks is sensitive to selectional preferences, such asthe resolution of ambiguous attachments (Hindle andRooth, 1993), word sense disambiguation (McCarthyand Carroll, 2003), semantic role labelling (Gildeaand Jurafsky, 2002), or testing the applicability ofinference rules (Pantel et al, 2007).A number of approaches has been proposed tomodel selectional preference data (Pad?
et al, 2006;Resnik, 1996; Clark and Weir, 2002; Abe and Li,1996).
These models generally operate by general-ising from seen (v, r, a) triples to unseen ones.
Byrelying on resources like corpora with semantic roleannotation or the WordNet ontology, these models400generally share two problems: (a), limited coverage;and (b), the resource (at least partially) predeterminesthe generalisations that they can make.In this paper, we investigate whether it is possi-ble to predict the plausibility of (v, r, a) triples ina completely corpus-driven way.
We build on a re-cent selectional preference model (Erk, 2007) thatbases its generalisations on word similarity in a vec-tor space.
While that model relies on corpora withsemantic role annotation, we show that it is possibleto predict plausibility ratings solely on the basis of aparsed corpus, by using shallow cues and a suitablevector space specification.For evaluation, we use two balanced data sets ofhuman plausibility judgements, i.e., datasets whereeach verb is paired both with a good agent and a goodpatient, and where both nouns are presented in eithersemantic relation (as in Table 1).
Using balanced testdata is a particularly difficult task, since it forcesthe models to account reliably both for the influenceof the semantic relation (agent/patient) and of theargument head (?hunter?/?deer?
).We obtain three main results: (a), our model is ableto match the superior performance of the model pro-posed by Pad?
et al (2006), while retaining the highcoverage of the model proposed by Resnik (1996);(b), using parsing as a preprocessing step improvesthe model?s performance significantly; and (c), a com-bination of our model with the Pad?
model exceedsboth individual models in accuracy.Plan of the paper.
In Section 2, we give anoverview of existing selectional preferences and vec-tor space models.
Section 3 introduces our model anddiscusses its parameters.
Sections 4 and 5 present ourexperimental setup and results.
Section 6 concludes.2 Related WorkModelling Selectional Preferences with Gram-matical Functions.
The idea of inducing selec-tional preferences from corpora was introduced byResnik (1996).
He approximated the semantic verb-argument relations in (v, r, a) triples by grammaticalfunctions, which are readily available for large train-ing corpora.
His basic two-step procedure was fol-lowed by all later approaches: (1), extract argumentheadwords for a given predicate and relation froma corpus; (2), generalise to other, similar words us-ing the WordNet noun hierarchy.
Other models alsorelying on the WordNet resource include Abe andLi (1996) and Clark and Weir (2002).We present Resnik?s model in some detail, sincewe will use it for comparison below.
Resnik firstcomputes the overall selectional preference strengthfor each verb-relation pair, i.e.
the degree of ?con-strainedness?
of each relation.
This quantity is esti-mated as the difference (in terms of the Kullback-Leibler divergence D) between the distribution overWordNet argument classes given the relation, p(c|r),and the distribution of argument classes given thecurrent verb-relation combination, p(c|v, r).
The in-tuition is that a verb-relation pair that only allowsfor a limited range of argument heads will have aprobability distribution over argument classes thatstrongly diverges from the prior distribution.Next, the selectional association of the triple,A(v, r, c), is computed as the ratio of the selectionalpreference strength for this particular class, dividedby the overall selectional preference strength of theverb-relation pair.
This is shown in Equation 1.A(v, r, c) =p(c|v, r)log p(c|v,r)p(c|r)D(p(c|r)||p(c|v, r))(1)Finally, the selectional preference between a verb,a relation, and an argument head is taken to be theselectional association of the verb and relation withthe most strongly associated WordNet ancestor classof the argument.WordNet-based approaches however face twoproblems.
One is a coverage problem due to the lim-ited size of the resource (see the task-based evalu-ation in Gildea and Jurafsky (2002)).
The other isthat the shape of the WordNet hierarchy determinesthe generalisations that the models make.
These arenot always intuitive.
For example, Resnik (1996) ob-serves that (answer, obj, tragedy) receives a highpreference because ?tragedy?
in WordNet is a typeof written communication, which is a preferred argu-ment class of ?answer?.Rooth et al (1999) present a fundamentally dif-ferent approach to selectional preference inductionwhich uses soft clustering to form classes for general-isation and does not take recourse to any hand-craftedresource.
We will argue in Section 6 that our modelallows more control over the generalisations made.401Modelling Selectional Preferences with ThematicRoles.
Pad?
et al (2006) present a deeper modelfor the plausibility of (v, r, a) triples that approxi-mates the relations with thematic roles.
It estimatesthe selectional preferences of a verb-role pair witha generative probability model that equates the plau-sibility of a (v, r, a) triple with the joint probabilityof seeing the thematic role with the verb-argumentpair.
In addition, the model also considers the verb?ssense s and the grammatical function gf of the ar-gument; however, since the model is generative, itcan make predictions even when not all variables areinstantiated.
The final model is shown in Equation 2.Plausibilityv,r,a = P (v, s, r, a, gf ) (2)The induction of this model from the FrameNet cor-pus of semantically annotated training data (Fillmoreet al, 2003) encounters a serious sparse data prob-lem, which is approached by the application of word-class-based and Good-Turing re-estimation smooth-ing.
The resulting model?s plausibility predictionsare significantly correlated to human judgements, butbecause of the use of verb-specific thematic roles,the model?s coverage is still restricted by the verbcoverage of the training corpus.Vector Space Models.
Another class of modelsthat has found wide application in lexical semanticsis the family of vector space models.
In a vector spacemodel, each target word is represented as a vector,typically constructed from co-occurrence counts withcontext words in a large corpus (the so-called basiselements).
The underlying assumption is that wordswith similar meanings occur in similar contexts, andwill be assigned similar vectors.
Thus, the distancebetween the vectors of two target words, as given bysome distance measure (e.g., Cosine or Jaccard), is ameasure of their semantic similarity.Vector space models are simple to construct, andthe semantic similarity they provide has found a widerange of applications.
Examples in NLP include in-formation retrieval (Salton et al, 1975), automaticthesaurus extraction (Grefenstette, 1994), and pre-dominant sense identification (McCarthy et al, 2004).In cognitive science, they have been used to accountfor the influence of context on human lexical pro-cessing (McDonald and Brew, 2004), and to modellexical priming (Lowe and McDonald, 2000).A drawback of vector space models is the diffi-culty of interpreting what some degree of ?genericsemantic similarity?
between two target words meansin linguistic terms.
In particular, this similarity isnot sensitive to selectional preferences over specificsemantic relations, and thus cannot model the plau-sibility data we are interested in.
The next sectiondemonstrates how the integration of ideas from se-lectional preference induction makes this distinctionpossible.3 The Vector Similarity Model:Corpus-Based Modelling of Plausibility3.1 Model ArchitectureOur model builds on the architecture of Erk (2007).
Itcombines the idea underlying the selectional prefer-ence models from Section 2, namely to predict plau-sibility by generalising over head words, with vectorspace similarity.
The fundamental idea of our modelis to model the plausibility of the triple (v, r, a) bycomparing the argument head a to other headwordsa?
which we have already seen in a corpus for thesame verb-relation pair (v, r), and which we there-fore assume to be plausible.
We write Seenr(v) forthe set of seen headwords.
Our intuition is that if ais similar to the words in Seenr(v), then the triple(v, r, a) is plausible; conversely, if it is very dissimi-lar, then the triple is implausible.Concretely, we judge the plausibility of the tripleby averaging over the similarity of the vector for a toall vectors for the seen headwords in Seenr(v):Pl(v, r, a) =?a??Seenr(v)w(a?)
?
sim(a, a?
)|Seenr(v)|(3)where w is a weight factor specific to each a?.
w canbe used to implement different weighting schemesthat encode prior knowledge, e.g., about the reliabil-ity of different words in Seenr(v).
In this paper, weonly consider a very simple weighting factor, namelythe frequency of the seen headwords.
This encodesthe assumption that similarity to frequent head wordsis more important than similarity to infrequent ones:Pl(v, r, a) =?a??Seenr(v)f(a?)
?
sim(a, a?
)|Seenr(v)|(4)402deerlionhunterpoacherdirectorseen patientsof "shoot"seen agentsof "shoot"Figure 1: A vector space for estimating theplausibilities of (shoot, agent, hunter) and(shoot, patient, hunter).This model can be seen as a straightforward imple-mentation of the selectional preference induction pro-cess of generalising from seen headwords to other,similar words.
By using vector space representationsto judge the similarity of words, we obtain a com-pletely corpus-driven model that does not require anyadditional resources and is very flexible.
A comple-mentary view on this model is as a generalisation oftraditional vector space models that computes simi-larity not between two vectors, but between a vectorand a set of other vectors.
By using the vectors forseen headwords of a given relation as this set, thesimilarity we compute is specific to this relation.Example.
Figure 1 shows an example vector space.Consider v = ?shoot?, r = agent, and a = ?hunter?.In order to judge whether a hunter is a plausible agentof ?shoot?, the vector space representation of ?hunter?is compared to all representations of known agentsof "shoot?, namely ?poacher?
and ?director?.
Dueto the nearness of the vector for ?hunter?
to thesetwo vectors, ?hunter?
will be judged a fairly goodagent of ?shoot?.
Compare this with the result for therole patient : ?hunter?
is further away from ?lion?
and?deer?, and will therefore be found to be a rather badpatient of ?shoot?.
However, ?hunter?
is still moreplausible as a patient of ?shoot?
than e.g., ?director?.3.2 Instantiating the Model: Unparsedvs.
Parsed CorporaThe two major tasks which need to be addressed toobtain an instance of this model are (a), determiningthe sets of seen head words Seenr(v), and (b), theconstruction of a vector space.
Erk (2007) extractedthe set of seen head words from corpora with se-mantic role annotation, and used only a single vectorspace representation.
In this paper, we eliminate thereliance on special annotation by considering shallowapproximations of the semantic relations in question.In addition, we discuss in detail which properties ofthe vector space are crucial for the prediction of plau-sibility ratings, a much more fine-grained task thanthe pseudo-word disambiguation task presented inErk (2007) that is more closely related to semanticrole labelling.
The goal of our exposition is thus todevelop a model that can use more training data, andrepresent the corpus information optimally in orderto obtain superior coverage.In fact, tasks (a) and (b) can be solved on the basisof unparsed corpora, but we would expect the resultsto be rather noisy.
Fortunately, the state of the art inbroad-coverage (Lin, 1993) and unsupervised (Kleinand Manning, 2004) dependency parsing allows us totreat dependency parsing merely as a preprocessingstep.
We therefore describe two instantiations of ourmodel: one based on an unprocessed corpus, and onebased on a dependency-based parsed corpus.
By com-paring the models, we can gauge whether syntacticpreprocessing improves model performance.
In thefollowing, we describe the strategies the two modelsadopt for (a) and (b).Identifying seen head words for relations.
Re-call that the set Seenr(v) is supposed to containknown head words a that are observed in the corpusas triples (v, r, a).
In a parsed corpus, we can approx-imate the relation agent by the dependency relationof subject provided by the parser, and the relationpatient by the dependency relation of object.
Inan unparsed corpus, these grammatical relations areunavailable, and the only straightforward evidencewe can use is word order.
In this case, we assumethat words directly adjacent to the left of a predicateare subjects, and therefore agents, whereas wordsdirectly to its right are objects, and thus patients.Vector space topology.
The success of our methoddepends directly on the topology of the vector space.More specifically, two words should only be assignedsimilar vectors if they are in fact of similar plausibil-ity.
If this is not the case, there is no guarantee that aword a that is similar to the words in Seenr(v) forms403``````````````Basis elementsTargetdeer huntershoot 10 10escape 12 12``````````````Basis elementsTargetdeer huntershoot-SUBJ 0 8shoot-OBJ 10 2escape-SUBJ 10 5escape-OBJ 2 7Figure 2: Two vector spaces, using as basis elementseither context words (above) or words paired withgrammatical functions (below)a plausible triple (v, r, a) itself (cf.
Figure 1).The topology, in turn, is related to the choice ofbasis elements.
Traditional vector space models usecontext words as basis elements of the space.
Thetop table in Figure 2 illustrates our intuition that suchspaces are problematic: ?deer?
and ?hunter?
receiveidentical vectors, even though they show complemen-tary plausibility ratings (cf.
Table 1).
The reason isthat ?deer?
and ?hunter?
often co-occur quite closelyto one another (e.g., in the vicinity of ?shoot?
), andthus show a very similar profile in terms of contextwords.
In preliminary experiments, we found that vec-tor spaces with context words as basis elements arein fact unable to distinguish such word pairs reliably.In contrast, the bottom table in Figure 2 indicatesthat this problem can be alleviated by using contextwords combined with the grammatical relation tothe target word as basis elements.
Target words nowreceive different representations, depending on thegrammatical function in which they occur with con-text words.
In consequence, resulting spaces can dis-tinguish, for example, between ?hunter?
and ?deer?.We adopt word-function pairs as basis elements forthe vector spaces in all our models.
In a dependency-parsed corpus, the basis elements can be directly readoff the syntactic structure.
In an unparsed corpus, weagain fall back on word order, appending to eachcontext word its relative position to the target word.4 Experimental SetupExperimental Materials.
In order to make ourevaluation comparable to the earlier modelling studyby Pad?
et al (2006), we present evaluations on thetwo plausibility judgement datasets used there.1The first dataset consists of 100 data points fromMcRae et al (1998).
Our example in Table 1, whichis taken from this dataset, demonstrates its balancedstructure: 25 verbs are paired with two argumentsand two relations each, such that each argument ishighly plausible in one relation, but implausible inthe other.
The resulting distribution of ratings is thushighly bimodal.
Models can only reliably predict thehuman ratings in this data set if they can capture thedifference between verb argument slots as well as asbetween individual fillers.The second, larger dataset is less strictly balanced,since its triples are constructed on the basis of corpusco-occurrences (Pad?
et al, 2006).
18 verbs are com-bined with the three most frequent subjects and ob-jects from both the Penn Treebank and the FrameNetcorpus.
Each verb-argument pair was rated both asan agent and as a patient, which leads to a total of24 rated triples per verb.
The dataset contains ratingsfor a total of 414 triples, due to overlap between cor-pora.
The resulting judgements show a more evendistribution of ratings than the McRae data.Vector Similarity Models.
Following our exposi-tion in the last section, we construct two instantia-tions of our vector similarity model, one using un-parsed and one parsed data.
Both are trained on thecomplete British National Corpus (Burnard, 1995,BNC) with more than six million sentences.The unparsed model (Unparsed) uses the BNCwithout any pre-processing.
We first construct theset of known headwords, Seenr(v), as follows: Allwords up to 2 words to the left of instances of vare assumed to be subjects, and thus agents; viceversa for patients to the right.
Then, we constructsemantic space representations for the experimentalarguments and known headwords, adopting optimalparameter settings from the literature (Pad?
and Lap-ata, 2007).
This means a context window of 5 wordsto either side and 2,000 basis elements (dimensions),which are formed by the most frequent 1,000 words1We are grateful to Ken McRae for his dataset.404in the BNC, combined with each of the relationsagent and patient.
All counts are log-likelihood trans-formed (Lowe, 2001).To construct the parsed model (Parsed), wedependency-parsed the BNC with Minipar (Lin,1993).
We first obtain the seen headwords Seenr(v)by using all subjects and objects of v as agents and pa-tients, respectively.
We then construct a vector spacefor the experimental arguments and known head-words.2 We use 2,000 dimensions again, but adopt themost frequent (head , grammatical function) pairsin the BNC as basis elements.
The context windowis formed by subject and object dependencies.All counts are log-likelihood transformed.We experiment with two distance measures to com-pute vector similarity, namely the Jaccard Coefficientand Cosine Distance, both of which have been shownto yield good performance in NLP tasks (Lee, 1999;McDonald and Lowe, 1998).Evaluation Procedure.
We evaluate our modelsby correlating the predicted plausibility values withthe human judgements, which range between 1 and7.
Since the human judgement data is not normallydistributed, we use Spearman?s ?, a non-parametricrank-order test.
We determine the statistical signif-icance of differences in correlation strength usingthe method described in Raghunathan (2003).
Thismethod can deal with missing values and thus allowsus to compare models with different coverage.It is difficult to specify a straightforward baselinefor our correlation-based evaluation.
In contrast toclassification tasks, where models choose one out ofa fixed number of classes, our model predicts contin-uous data.
This task is more difficult to approximate,e.g., using frequency information.With respect to upper bounds, we hold that au-tomatic models of plausibility cannot be expectedto surpass the typical agreement on the plausibilityjudgement task between human participants.
Thus,we assume an upper bound of ?
?
0.7.Comparison against Other Models.
We compareour performance to two models from the literature dis-cussed in Section 2.
The first model (Pado) is the the-2This space was computed using theDependencyVectors software described in Pad?
andLapata (2007).
This software can be downloaded from http://www.coli.uni-saarland.de/~pado/dv.html.Model Coverage Spearman?s ?Unparsed Cosine 90% 0.023, nsUnparsed Jaccard 90% 0.044, nsParsed Cosine 91% 0.218, *Parsed Jaccard 91% 0.129, nsResnik 94% 0.028, nsPado 56% 0.415, **Table 2: Model performance on McRae data.
*: p < 0.05, **: p < 0.01matic role-based model by Pad?
et al (2006) trainedon the FrameNet (Fillmore et al, 2003) release 1.2 ex-ample sentences, a subset of the BNC annotated withsemantic roles.
This corpus contains about 57,000sentences, which corresponds to roughly 1% of theBNC data.The second model (Resnik) is the WordNet-basedselectional preference model by Resnik (1996),trained on the dependency-parsed BNC (see above).5 Experimental EvaluationThe McRae Dataset.
Table 2 summarises our re-sults on the McRae dataset.
The upper part showsthe results for our two vector similarity models(Parsed/Unparsed), combined with the two distancemeasures (Cosine/Jaccard).
The lower part shows thetwo resource-based models we use for comparison.We find that all vector similarity models exhibithigh coverage (above 90%), and one model (ParsedCosine) can predict human judgements with a signifi-cant correlation.
The instantiation of the model hasa significant impact on the performance: The Parsedmodels clearly outperform the Unparsed models.
Theeffect of the distance measure is less clear-cut, sincethe Unparsed models perform better with Jaccard,while the Parsed models prefer Cosine.The deep semantic plausibility model (Pado)makes predictions only for slightly more than half ofthe data.
This low coverage is a direct result of thesmall overlap in verbs between the McRae datasetand the FrameNet corpus.
However, on the datapoints it covers, it achieves a significant correlationto human judgements.
The correlation coefficient isnumerically much higher than that of the Parsed Co-sine model, but due to the large coverage difference,the two models are not statistically distinguishable.405Model Coverage Spearman?s ?Unparsed Cosine 98% 0.117, *Unparsed Jaccard 98% 0.149, **Parsed Cosine 98% 0.479, ***Parsed Jaccard 98% 0.120, *Resnik 98% 0.237, ***Pado 97% 0.515, ***Table 3: Model performance on Pado data.
*: p < 0.05, **: p < 0.01, ***: p < 0.001Resnik?s WordNet-based model shows a coveragethat is comparable to the vector similarity models,but does not achieve a significant correlation to thehuman judgements.The Pado Dataset.
Table 3 summarises the resultsfor the Pado dataset.
Since all verbs in this dataset arecovered in FrameNet, the deep Pado model shows acoverage comparable to all other models, at >95%.The main difference to the McRae dataset lies inthe models?
performance.
We find that all models,including the Unparsed vector models and Resnik,manage to achieve significant correlations with thehuman judgements.
Within the vector similarity mod-els, the same trends hold as for the McRae dataset:Parsed outperforms Unparsed, and the best combina-tion is Parsed Cosine.
The models fall into two clearlyseparated groups: The Pado and Parsed Cosine mod-els achieve a highly significant correlation, and arestatistically indistinguishable.
They significantly out-perform the second group (p < 0.001), formed byall other models.
Within this second group, Resnik isnumerically the best model and shows a significantcorrelation with human data; nevertheless, the differ-ence to the first group is evident from its substantiallylower correlation coefficient.The construction of the Pado dataset alows a fur-ther analysis.
As mentioned in Section 4, the datasetconsists of verb-argument pairs drawn from two dif-ferent corpora.
Therefore, each verb is combinedboth with some arguments that are seen in FrameNet,and some that are not.
Our hypothesis is that theFrameNet-trained Pado model performs consider-ably better on the 216 ?FN-Seen?
data points (verb-argument pairs observed in FrameNet in at least onerelation) than on the 198 ?FN-Unseen?
data points(verb-argument pairs unseen in both relations).Table 4 shows the results of this analysis for thebest-performing models.
We observe a pattern corre-sponding to our expectations: The performance of thePado model is clearly worse for FN-Unseen than forFN-Seen, while the Resnik and Parsed Cosine mod-els perform more evenly across both datasets.
Whilethe Pado model is significantly better on the FN-Seendataset, it is numerically outperformed by the ParsedCosine model for the FN-Unseen data points.
Weconclude that the deep model is more accurate withinthe coverage of its resources, but loses its advantagewhen it has to resort to smoothing.Model combination.
Our last analysis indicatesthat the models have complementary strengths: thethematic role-based Pado model is the best plausi-bility predictor on the data points it has seen, whilethe Parsed cosine model overall predicts human dataonly numerically worse, and with better coverage.We therefore suggest to combine the predictions ofthe two models to combine their respective strengths.For the moment, we only consider a naive backoffscheme: For each data point, we use the predictionof the Pado model if the data point is ?FN-Seen?
(cf.the last paragraph), and the prediction of the ParsedCosine model otherwise.
Note that this criterion doesnot consider the predictions of the models themselves,only properties of the underlying training set.The actual combination requires a normalisationof the respective predictions, since one of the models(Pado) is probabilistic, while the other one (ParsedCosine) is similarity-based, and their predictions arenot directly comparable.
We perform a simple nor-malisation by z-transforming the complete predic-tions of each model.3 The combination of the scaledpredictions in fact results in an improved correlationwith the human data.
The correlation coefficient of?=0.552 numerically exceeds either base model, andthe coverage of 98% corresponds to the coverage ofthe more robust Parsed Cosine model.We take this result as evidence that even a simplecombination technique can lead to improved predic-tions.
Unfortunately, our naive backoff scheme doesnot directly carry over to the McRae dataset, whereonly 2 out of 100 data points are ?FN-Seen?, and thePado model would thus hardly contribute.3The z transformation scales a dataset to a mean of 0 and astandard deviation of 1.406Model FN-Seen Data FN-Unseen DataParsed Cosine 94% 0.426, *** 100% 0.461, ***Resnik 96% 0.217, ** 100% 0.263, ***Pado 97% 0.569, *** 96% 0.383, ***Table 4: Performance on data points seen and unseen in FrameNet (Pado dataset).
**: p < 0.01 ***: p < 0.001Discussion.
We have verified experimentally thatour vector similarity model is able to match the per-formance of a deep plausibility model, exceeding itin coverage, and to outperform a WordNet-based se-lectional preference model.
We conclude that a com-pletely corpus-driven approach constitutes a viablealternative to resource-based models.One insight from our experiments is that vec-tor similarity models constructed from dependency-parsed corpora perform significantly better than un-parsed models.
This indicates that dependency rela-tions like subject and object are reliable syntac-tic correlates of semantic relations like agent and pa-tient, but that their approximation in terms of word or-der introduces considerable noise.
The Parsed modelsare best combined with Cosine Distance.
We surmisethat Cosine, which tends to consider low-frequencywords more than Jaccard, is more susceptible to theadditional noise in unparsed corpora.Furthermore, the choice of basis elements for thevector space is vital: Plausibilities could only be pre-dicted successfully with word-relation pairs as basiselements.
This is in contrast to recent results on pre-dominant sense acquisition, the task of identifyingthe most frequent sense for a given word in an un-supervised manner (McCarthy et al, 2004).
On thattask, Pad?
and Lapata (2007) found vector spaceswith words as basis elements are in fact competitivewith models using word-relation pairs.
This diver-gence underlines an interesting difference betweenthe two tasks.
Evidently, predominant senses identi-fication, as a WSD-related task, can succeed on thebasis of topical information, which is representedwell in word-based spaces.
In contrast, plausibilityjudgments can only be predicted by a space basedon word-relation pairs which can represent the finer-grained distinctions arising from different relationsbetween verb and noun.A second important finding is that the relative per-formance of the different models is the same on theMcRae and Pado datasets.
The Pado model performsbest, followed by our Parsed Cosine vector similaritymodel, followed by the Unparsed and Resnik models.The McRae dataset, however, is much more diffi-cult to account for than the Pado data, independent ofthe model.
This effect was already noted by Pad?
etal.
(2006), who attributed it to the very limited over-lap between the McRae dataset and FrameNet.
Whilethis explanation can account for the difference for thePado model, we observe the same pattern across allmodels.
This suggests that a more general frequencyeffect is at work here: The median frequency of thehand-selected McRae nouns is 1,356 in the BNC, asopposed to 8,184 for the corpus-derived Pado nouns.The resulting sparseness affects all model families,since all ultimately rely on co-occurrences.The performance difference between the twodatasets is particularly large for the WordNet-basedselectional preference model (Resnik).
A furtheranalysis of the model?s predictions shows thatthe model has difficulty in distinguishing betweenverb-relation-argument triples that differ only inthe argument, such as (shoot, agent, hunter) and(shoot, agent, deer).
Recall that it is crucial for theprediction of the McRae data to make this distinc-tion, since the arguments for each relation are cho-sen to differ widely in plausibility.
The reason forthe Resnik model?s difficulty is that arguments aremapped onto WordNet synsets, and whenever twoarguments are mapped onto closely related synsets,their plausibility ratings are similar.
This problem isgraver for the McRae test set, where all arguments areanimates, and thus more similar in terms of WordNet,than for the Pado set, which also contains a portion ofinanimate arguments with animate counterparts.
Thisanalysis highlights again the fundamental problemof resource-based models, where design decisions ofthe underlying resource may limit, or even mislead,the models?
generalisations.Finally, we have shown in a first experiment that407the syntax-based vector similarity model can be com-bined with the role-base model to obtain a combinedmodel that performs superior to both.
In this com-bined model, the shallowmodel?s better coverage sup-plements the accurate predictions of the deep model.6 ConclusionsIn this paper, we have considered the computationalmodelling of human plausibility judgements for verb-relation-argument triples, a task equivalent to thecomputation of selectional preferences.
We have ex-tended a recent proposal (Erk, 2007) which com-bines ideas from selectional preference induction andvector space models.
Our model can be constructedfrom a large corpus with partial syntactic information(specifically, subject and object relations) from whichit builds an optimally informative vector space.We have demonstrated that the successful evalua-tion of the model in Erk (2007) on the coarse-grainedpseudo-word disambiguation task carries over to theprediction of human plausibility judgments which re-quires relatively fine-grained, relation-based distinc-tions.
Our model is competitive with existing ?deep?models while exhibiting a higher coverage.
We havealso shown that our vector similarity model can becombined with a ?deep?
model so that the combinedmodel outperforms both base models.
A thoroughinvestigation of strategies for prediction combinationand scaling remains future work.The strategy of our model to derive generalisationsdirectly from corpus data, without recourse to re-sources, is similar to another family of corpus-drivenselectional preference models, namely EM-basedclustering models (Rooth et al, 1999).
However, webelieve that our model has a number of advantages.
(1), It is conceptually simple and implements theintuition behind selectional preference models, ?gen-eralise from known headwords to unknown ones?,particularly directly through the comparison of newheadwords to known ones according to a given defini-tion of similarity.
(2), The separation of the similaritycomputation and the acquisition of seen headwordsgives the experimenter fine-grained control over thetypes and sources of information which inform theconstruction of the model.
(3), The instantiation ofthe similarity computation with a vector space makesit possible to integrate additional linguistic informa-tion beyond verb-argument co-occurrences into themodel, building on a large body of work in vectorspace construction.
In sum, our modular model pro-vides a higher degree of control than one-step modelslike the EM-based proposal.An important avenue of further research is theability of the vector plausibility model to model finer-grained distinctions between semantic relations be-yond the agent/patient dichotomy, as thematic role-based models are able to.
Excluding the direct use ofrole-annotated corpora like FrameNet for coveragereasons, the most promising strategy is to extend ourpresent scheme of approximating semantic relationsby grammatical realisations.
How much noise thisapproximation introduces when finer role sets areused is an open research question.Acknowledgments.
The work presented in this pa-per was supported by the financial support of DFG(grants Pi-154/9-2 and IRTG ?Language Technologyand Cognitive Systems?
).ReferencesNaoki Abe and Hang Li.
1996.
Learning word associa-tion norms using tree cut pair models.
In Proceedingsof ICML 1996, pages 3?11.Lou Burnard, 1995.
User?s guide for the British NationalCorpus.
British National Corpus Consortium, OxfordUniversity Computing Services.Stephen Clark and David Weir.
2002.
Class-based prob-ability estimation using a semantic hierarchy.
Compu-tational Linguistics, 28(2):187?206.Katrin Erk.
2007.
A simple, similarity-based model forselectional preferences.
In Proceedings of the 45thACL, Prague, Czech Republic.Charles J. Fillmore, Christopher R. Johnson, andMiriam R.L.
Petruck.
2003.
Background to FrameNet.International Journal of Lexicography, 16:235?250.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers.Donald Hindle and Mats Rooth.
1993.
Structural ambi-guity and lexical relations.
Computational Linguistics,19(1):103?120.408Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the 42thACL, pages 478?485, Barcelona, Spain.Lillian Lee.
1999.
Measures of distributional similarity.In Proceedings of the 37th ACL, pages 25?32, CollegePark, MA.Dekang Lin.
1993.
Principle-based parsing without over-generation.
In Proceedings of the 31st ACL, pages112?120, Columbus, OH.Will Lowe and Scott McDonald.
2000.
The direct route:Mediated priming in semantic space.
In Proceedingsof the 22nd CogSci, pages 675?680, Philadelphia, PA.Will Lowe.
2001.
Towards a theory of semantic space.In Proceedings of the 23rd CogSci, pages 576?581, Ed-inburgh, UK.Diana McCarthy and John Carroll.
2003.
Disambiguat-ing nouns, verbs and adjectives using automatically ac-quired selectional preferences.
Computatinal Linguis-tics, 29(4):639?654.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In Proceedings of the 42th ACL, pages279?286, Barcelona, Spain.Scott McDonald and Chris Brew.
2004.
A distributionalmodel of semantic context effects in lexical process-ing.
In Proceedings of the 42th ACL, pages 17?24,Barcelona, Spain.Scott McDonald and Will Lowe.
1998.
Modelling func-tional priming and the associative boost.
In Proceed-ings of the 20th CogSci, pages 675?680, Madison, WI.Ken McRae, Michael Spivey-Knowlton, and MichaelTanenhaus.
1998.
Modeling the influence of thematicfit (and other constraints) in on-line sentence compre-hension.
Journal of Memory and Language, 38:283?312.Sebastian Pad?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2).Ulrike Pad?, Frank Keller, and Matthew W. Crocker.2006.
Combining syntax and thematic fit in a proba-bilistic model of sentence processing.
In Proceedingsof the 28th CogSci, pages 657?662, Vancouver, BC.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Tim-othy Chklovski, and Eduard Hovy.
2007.
ISP: Learn-ing inferential selectional preferences.
In Proceedingsof NAACL 2007, Rochester, NY.Trivellore Raghunathan.
2003.
An approximate test forhomogeneity of correlated correlations.
Quality andQuantity, 37:99?110.Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computational re-alization.
Cognition, 61:127?159.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing an semanti-cally annotated lexicon via EM-based clustering.
InProceedings of the 37th ACL, pages 104?111, CollegePark, MA.Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975.A vector-space model for information retrieval.
Jour-nal of the American Society for Information Science,18:613?620.John Trueswell, Michael Tanenhaus, and Susan Garnsey.1994.
Semantic influences on parsing: Use of the-matic role information in syntactic ambiguity resolu-tion.
Journal of Memory and Language, 33:285?318.409
