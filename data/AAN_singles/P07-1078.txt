Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 616?623,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSelf-Training for Enhancement and Domain Adaptation ofStatistical Parsers Trained on Small DatasetsRoi ReichartICNCHebrew University of Jerusalemroiri@cs.huji.ac.ilAri RappoportInstitute of Computer ScienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractCreating large amounts of annotated data totrain statistical PCFG parsers is expensive,and the performance of such parsers declineswhen training and test data are taken fromdifferent domains.
In this paper we use self-training in order to improve the quality ofa parser and to adapt it to a different do-main, using only small amounts of manuallyannotated seed data.
We report significantimprovement both when the seed and testdata are in the same domain and in the out-of-domain adaptation scenario.
In particu-lar, we achieve 50% reduction in annotationcost for the in-domain case, yielding an im-provement of 66% over previous work, and a20-33% reduction for the domain adaptationcase.
This is the first time that self-trainingwith small labeled datasets is applied suc-cessfully to these tasks.
We were also ableto formulate a characterization of when self-training is valuable.1 IntroductionState of the art statistical parsers (Collins, 1999;Charniak, 2000; Koo and Collins, 2005; Charniakand Johnson, 2005) are trained on manually anno-tated treebanks that are highly expensive to create.Furthermore, the performance of these parsers de-creases as the distance between the genres of theirtraining and test data increases.
Therefore, enhanc-ing the performance of parsers when trained onsmall manually annotated datasets is of great impor-tance, both when the seed and test data are takenfrom the same domain (the in-domain scenario) andwhen they are taken from different domains (the out-of-domain or parser adaptation scenario).
Since theproblem is the expense in manual annotation, we de-fine ?small?
to be 100-2,000 sentences, which are thesizes of sentence sets that can be manually annotatedby constituent structure in a few hours1.Self-training is a method for using unannotateddata when training supervised models.
The model isfirst trained using manually annotated (?seed?)
data,then the model is used to automatically annotate apool of unannotated (?self-training?)
data, and thenthe manually and automatically annotated datasetsare combined to create the training data for the fi-nal model.
Self-training of parsers trained on smalldatasets is of enormous potential practical impor-tance, due to the huge amounts of unannotated datathat are becoming available today and to the highcost of manual annotation.In this paper we use self-training to enhance theperformance of a generative statistical PCFG parser(Collins, 1999) for both the in-domain and the parseradaptation scenarios, using only small amounts ofmanually annotated data.
We perform four experi-ments, examining all combinations of in-domain andout-of-domain seed and self-training data.Our results show that self-training is of substantialbenefit for the problem.
In particular, we present:?
50% reduction in annotation cost when the seedand test data are taken from the same domain,which is 66% higher than any previous resultwith small manually annotated datasets.1We note in passing that quantitative research on the cost ofannotation using various annotation schemes is clearly lacking.616?
The first time that self-training improves a gen-erative parser when the seed and test data arefrom the same domain.?
20-33% reduction in annotation cost when theseed and test data are from different domains.?
The first time that self-training succeeds inadapting a generative parser between domainsusing a small manually annotated dataset.?
The first formulation (related to the number ofunknown words in a sentence) of when self-training is valuable.Section 2 discusses previous work, and Section 3compares in-depth our protocol to a previous one.Sections 4 and 5 present the experimental setup andour results, and Section 6 analyzes the results in anattempt to shed light on the phenomenon of self-training.2 Related WorkSelf-training might seem a strange idea: why shoulda parser trained on its own output learn anythingnew?
Indeed, (Clark et al, 2003) applied self-training to POS-tagging with poor results, and(Charniak, 1997) applied it to a generative statisti-cal PCFG parser trained on a large seed set (40Ksentences), without any gain in performance.Recently, (McClosky et al, 2006a; McClosky etal., 2006b) have successfully applied self-training tovarious parser adaptation scenarios using the rerank-ing parser of (Charniak and Johnson, 2005).
Areranking parser (see also (Koo and Collins, 2005))is a layered model: the base layer is a generative sta-tistical PCFG parser that creates a ranked list of kparses (say, 50), and the second layer is a rerankerthat reorders these parses using more detailed fea-tures.
McClosky et al(2006a) use sections 2-21 ofthe WSJ PennTreebank as seed data and between50K to 2,500K unlabeled NANC corpus sentencesas self-training data.
They train the PCFG parser andthe reranker with the manually annotated WSJ data,and parse the NANC data with the 50-best PCFGparser.
Then they proceed in two directions.
Inthe first, they reorder the 50-best parse list with thereranker to create a new 1-best list.
In the second,they leave the 1-best list produced by the genera-tive PCFG parser untouched.
Then they combine the1-best list (each direction has its own list) with theWSJ training set, to retrain the PCFG parser.
Thefinal PCFG model and the reranker (trained only onannotated WSJ material) are then used to parse thetest section (23) of WSJ.There are two major differences between these pa-pers and the current one, stemming from their usageof a reranker and of large seed data.
First, whentheir 1-best list of the base PCFG parser was usedas self training data for the PCFG parser (the sec-ond direction), the performance of the base parserdid not improve.
It had improved only when the 1-best list of the reranker was used.
In this paper weshow how the 1-best list of a base (generative) PCFGparser can be used as a self-training material for thebase parser itself and enhance its performance, with-out using any reranker.
This reveals a noteworthycharacteristic of generative PCFG models and offersa potential direction for parser improvement, sincethe quality of a parser-reranker combination criti-cally depends on that of the base parser.Second, these papers did not explore self-trainingwhen the seed is small, a scenario whose importancehas been discussed above.
In general, PCFG mod-els trained on small datasets are less likely to parsethe self-training data correctly.
For example, the f-score of WSJ data parsed by the base PCFG parserof (Charniak and Johnson, 2005) when trained onthe training sections of WSJ is between 89% to90%, while the f-score of WSJ data parsed with theCollins?
model that we use, and a small seed, is be-tween 40% and 80%.
As a result, the good results of(McClosky et al 2006a; 2006b) with large seed setsdo not immediately imply success with small seedsets.
Demonstration of such success is a contribu-tion of the present paper.Bacchiani et al(2006) explored the scenario ofout-of-domain seed data (the Brown training setcontaining about 20K sentences) and in-domainself-training data (between 4K to 200K sentencesfrom the WSJ) and showed an improvement overthe baseline of training the parser with the seed dataonly.
However, they did not explore the case of smallseed datasets (the effort in manually annotating 20Kis substantial) and their work addresses only one ofour scenarios (OI, see below).617A work closely related to ours is (Steedman etal., 2003a), which applied co-training (Blum andMitchell, 1998) and self-training to Collins?
pars-ing model using a small seed dataset (500 sentencesfor both methods and 1,000 sentences for co-trainingonly).
The seed, self-training and test datasets theyused are similar to those we use in our II experi-ment (see below), but the self-training protocols aredifferent.
They first train the parser with the seedsentences sampled from WSJ sections 2-21.
Then,iteratively, 30 sentences are sampled from these sec-tions, parsed by the parser, and the 20 best sentences(in terms of parser confidence defined as probabilityof top parse) are selected and combined with the pre-viously annotated data to retrain the parser.
The co-training protocol is similar except that each parseris trained with the 20 best sentences of the otherparser.
Self-training did not improve parser perfor-mance on the WSJ test section (23).
Steedman etal (2003b) followed a similar co-training protocolexcept that the selection function (three functionswere explored) considered the differences betweenthe confidence scores of the two parsers.
In this pa-per we show a self-training protocol that achievesbetter results than all of these methods (Table 2).The next section discusses possible explanations forthe difference in results.
Steedman et al(2003b) andHwa et al (2003) also used several versions of cor-rected co-training which are not comparable to oursand other suggested methods because their evalua-tion requires different measures (e.g.
reviewed andcorrected constituents are separately counted).As far as we know, (Becker and Osborne, 2005)is the only additional work that tries to improve agenerative PCFG parsers using small seed data.
Thetechniques used are based on active learning (Cohnet al, 1994).
The authors test two novel methods,along with the tree entropy (TE) method of (Hwa,2004).
The seed, the unannotated and the test sets,as well as the parser used in that work, are similarto those we use in our II experiment.
Our results aresuperior, as shown in Table 3.3 Self-Training ProtocolsThere are many possible ways to do self-training.A main goal of this paper is to identify a self-training protocol most suitable for enhancement anddomain adaptation of statistical parsers trained onsmall datasets.
No previous work has succeeded inidentifying such a protocol for this task.
In this sec-tion we try to understand why.In the protocol we apply, the self-training set con-tains several thousand sentences A parser trainedwith a small seed set parses the self-training set, andthen the whole automatically annotated self-trainingset is combined with the manually annotated seedset to retrain the parser.
This protocol and that ofSteedman et al(2003a) were applied to the problem,with the same seed, self-training and test sets.
Aswe show below (see Section 4 and Section 5), whileSteedman?s protocol does not improve over the base-line of using only the seed data, our protocol does.There are four differences between the protocols.First, Steedman et als seed set consists of consecu-tive WSJ sentences, while we select them randomly.In the next section we show that this difference isimmaterial.
Second, Steedman et als protocol looksfor sentences of high quality parse, while our pro-tocol prefers to use many sentences without check-ing their parse quality.
Third, their protocol is itera-tive while ours uses a single step.
Fourth, our self-training set is orders of magnitude larger than theirs.To examine the parse quality issue, we performedtheir experiment using their setting but selecting thehigh quality parse sentences using their f-score rel-ative to the gold standard annotation from secs 2-21 rather than a quality estimate.
No improvementover the baseline was achieved even with this or-acle.
Thus the problem with their protocol doesnot lie with the parse quality assessment function;no other function would produce results better thanthe oracle.
To examine the iteration issue, we per-formed their experiment in a single step, selecting atonce the oracle-best 2,000 among 3,000 sentences2,which produced only a mediocre improvement.
Wethus conclude that the size of the self-training set is amajor factor responsible for the difference betweenthe protocols.4 Experimental SetupWe used a reimplementation of Collins?
parsingmodel 2 (Bikel, 2004).
We performed four experi-ments, II, IO, OI, and OO, two with in-domain seed2Corresponding to a 100 iterations of 30 sentences each.618(II, IO) and two with out-of-domain seed (OI, OO),examining in-domain self-training (II, OI) and out-of-domain self-training (IO, OO).
Note that being?in?
or ?out?
of domain is determined by the test data.Each experiment contained 19 runs.
In each run adifferent seed size was used, from 100 sentences on-wards, in steps of 100.
For statistical significance,we repeated each experiment five times, in each rep-etition randomly sampling different manually anno-tated sentences to form the seed dataset3.The seed data were taken from WSJ sections 2-21.
For II and IO, the test data is WSJ section 23(2416 sentences) and the self-training data are eitherWSJ sections 2-21 (in II, excluding the seed sen-tences) or the Brown training section (in IO).
ForOI and OO, the test data is the Brown test section(2424 sentences), and the self-training data is eitherthe Brown training section (in OI) or WSJ sections2-21 (in OO).
We removed the manual annotationsfrom the self-training sections before using them.For the Brown corpus, we based our divisionon (Bacchiani et al, 2006; McClosky et al, 2006b).The test and training sections consist of sentencesfrom all of the genres that form the corpus.
Thetraining division consists of 90% (9 of each 10 con-secutive sentences) of the data, and the test sectionare the remaining 10% (We did not use any held outdata).
Parsing performance is measured by f-score,f = 2?P?RP+R , where P, R are labeled precision andrecall.To further demonstrate our results for parser adap-tation, we also performed the OI experiment whereseed data is taken from WSJ sections 2-21 and bothself-training and test data are taken from the Switch-board corpus.
The distance between the domains ofthese corpora is much greater than the distance be-tween the domains of WSJ and Brown.
The Brownand Switchboard corpora were divided to sections inthe same way.We have also performed all four experiments withthe seed data taken from the Brown training section.3 (Steedman et al, 2003a) used the first 500 sentences ofWSJ training section as seed data.
For direct comparison, weperformed our protocol in the II scenario using the first 500 or1000 sentences of WSJ training section as seed data and gotsimilar results to those reported below for our protocol with ran-dom selection.
We also applied the protocol of Steedman et alto scenario II with 500 randomly selected sentences, getting noimprovement over the random baseline.The results were very similar and will not be detailedhere due to space constraints.5 Results5.1 In-domain seed dataIn these two experiments we show that when theseed and test data are taken from the same domain, avery significant enhancement of parser performancecan be achieved, whether the self-training materialis in-domain (II) or out-of-domain (IO).
Figure 1shows the improvement in parser f-score when self-training data is used, compared to when it is notused.
Table 1 shows the reduction in manually an-notated seed data needed to achieve certain f-scorelevels.
The enhancement in performance is very im-pressive in the in-domain self-training data scenario?
a reduction of 50% in the number of manually an-notated sentences needed for achieving 75 and 80 f-score values.
A significant improvement is achievedin the out-of-domain self-training scenario as well.Table 2 compares our results with self-trainingand co-training results reported by (Steedman et al20003a; 2003b).
As stated earlier, the experimentalsetup of these works is similar to ours, but the self-training protocols are different.
For self-training,our II improves an absolute 3.74% over their 74.3%result, which constitutes a 14.5% reduction in error(from 25.7%).The table shows that for both seed sizes ourself training protocol outperforms both the self-training and co-training protocols of (Steedman etal, 20003a; 2003b).
Results are not included in thetable only if they are not reported in the relevant pa-per.
The self-training protocol of (Steedman et al,2003a) does not actually improve over the baselineof using only the seed data.
Section 3 discussed apossible explanation to the difference in results.In Table 3 we compare our results to the results ofthe methods tested in (Becker and Osborne, 2005)(including TE)4.
To do that, we compare the reduc-tion in manually annotated data needed to achievean f-score value of 80 on WSJ section 23 achievedby each method.
We chose this measure since it is4The measure is constituents and not sentences because thisis how results are reported in (Becker and Osborne, 2005).However, the same reduction is obtained when sentences arecounted, because the number of constituents is averaged whentaking many sentences.619f-score 75 80Seed data only 600(0%) 1400(0%)II 300(50%) 700(50%)IO 500(17%) 1200(14.5%)Table 1: Number of in-domain seed sentencesneeded for achieving certain f-scores.
Reductionscompared to no self-training (line 1) are given inparentheses.SeedsizeourIIourIOSteedmanSTSteedmanCTSteedmanCT2003a 2003b500sent.78.04 75.81 74.3 76.9 ?-1,000sent.81.43 79.49 ?- 79 81.2Table 2: F-scores of our in-domain-seed self-training vs. self-training (ST) and co-training (CT)of (Steedman et al 20003a; 2003b).the only explicitly reported number in that work.
Asthe table shows, our method is superior: our reduc-tion of 50% constitutes an improvement of 66% overtheir best reduction of 30.6%.When applying self-training to a parser trainedwith a small dataset we expect the coverage of theparser to increase, since the combined training setshould contain items that the seed dataset does not.On the other hand, since the accuracy of annota-tion of such a parser is poor (see the no self-trainingcurve in Figure 1) the combined training set surelyincludes inaccurate labels that might harm parserperformance.
Figure 2 (left) shows the increase incoverage achieved for in-domain and out-of-domainself-training data.
The improvements induced byboth methods are similar.
This is quite surpris-ing given that the Brown sections we used as self-training data contain science, fiction, humor, ro-mance, mystery and adventure texts while the testsection in these experiments, WSJ section 23, con-tains only news articles.Figure 2 also compares recall (middle) and preci-sion (right) for the different methods.
For II thereis a significant improvement in both precision andrecall even though many more sentences are parsed.For IO, there is a large gain in recall and a muchsmaller loss in precision, yielding a substantial im-provement in f-score (Figure 1).F -scoreThiswork - IIBeckerunparsedBecker en-tropy/unparsedHwaTE80 50% 29.4% 30.6% -5.7%Table 3: Reduction of the number of manually anno-tated constituents needed for achieving f score valueof 80 on section 23 of the WSJ.
In all cases the seedand additional sentences selected to train the parserare taken from sections 02-21 of WSJ.5.2 Out-of-domain seed dataIn these two experiments we show that self-trainingis valuable for adapting parsers from one domain toanother.
Figure 3 compares out-of-domain seed dataused with in-domain (OI) or out-of-domain (OO)self-training data against the baseline of trainingonly with the out-of-domain seed data.The left graph shows a significant improvementin f-score.
In the middle and right graphs we exam-ine the quality of the parses produced by the modelby plotting recall and precision vs. seed size.
Re-garding precision, the difference between the threeconditions is small relative to the f-score differenceshown in the left graph.
The improvement in therecall measure is much greater than the precisiondifferences, and this is reflected in the f-score re-sult.
The gain in coverage achieved by both meth-ods, which is not shown in the figure, is similar tothat reported for the in-domain seed experiments.The left graph along with the increase in coverageshow the power of self-training in parser adaptationwhen small seed datasets are used: not only do OOand OI parse many more sentences than the baseline,but their f-score values are consistently better.To see how much manually annotated data canbe saved by using out-of-domain seed, we train theparsing model with manually annotated data fromthe Brown training section, as described in Sec-tion 4.
We assume that given a fixed number oftraining sentences the best performance of the parserwithout self-training will occur when these sen-tences are selected from the domain of the test sec-tion, the Brown corpus.
We compare the amounts ofmanually annotated data needed to achieve certain f-score levels in this condition with the correspondingamounts of data needed by OI and OO.
The resultsare summarized in Table 4.
We compare to two base-lines using in- and out-of-domain seed data without6200 200 400 600 800 1000405060708090number of manually annotated sentencesf scoreno self trainingwsj self?trainingbrown self?training1000 1200 1400 1600 1800 200078798081828384number of manually annotated sentencesf scoreno self?trainingwsj self?trainingbrown self?trainingFigure 1: Number of seed sentences vs. f-score, for the two in-domain seed experiments: II (triangles) andIO (squares), and for the no self-training baseline.
Self-training provides a substantial improvement.0 500 1000 1500 20001000150020002500number of manually annotated sentencesnumber of coveredsentencesno self?trainingwsj self?trainingbrown self?training0 500 1000 1500 200020406080100number of manually annotated sentencesrecallno self?trainingwsj self?trainingbrown self?training0 500 1000 1500 20006570758085number of manually annotated sentencesprecisionno self?trainingwsj self?trainingbrown self?trainingFigure 2: Number of seed sentences vs. coverage (left), recall (middle) and precision (right) for the twoin-domain seed experiments: II (triangles) and IO (squares), and for the no self-training baseline.any self-training.
The second line (ID) serves as areference to compute how much manual annotationof the test domain was saved, and the first line (OD)serves as a reference to show by how much self-training improves the out-of-domain baseline.
Thetable stops at an f-score of 74 because that is thebest that the baselines can do.A significant reduction in annotation cost over theID baseline is achieved where the seed size is be-tween 100 and 1200.
Improvement over the ODbaseline is for the whole range of seed sizes.
BothOO and OI achieve 20-33% reduction in manual an-notation compared to the ID baseline and enhancethe performance of the parser by as much as 42.9%.The only previous work that adapts a parsertrained on a small dataset between domains is thatof (Steedman et al, 2003a), which used co-training(no self-training results were reported there or else-where).
In order to compare with that work, we per-formed OI with seed taken from the Brown corpusand self-training and test taken from WSJ, whichis the setup they use, obtaining a similar improve-ment to that reported there.
However, co-training isa more complex method that requires an additionalparser (LTAG in their case).To further substantiate our results for the parseradaptation scenario, we used an additional corpus,Switchboard.
Figure 4 shows the results of an OIexperiment with WSJ seed and Switchboard self-training and test data.
Although the domains of thesetwo corpora are very different (more so than WSJand Brown), self-training provides a substantial im-provement.We have also performed all four experiments withBrown and WSJ trading places.
The results obtainedwere very similar to those reported here, and will notbe detailed due to lack of space.6 AnalysisIn this section we try to better understand the ben-efit in using self-training with small seed datasets.We formulate the following criterion: the number ofwords in a test sentence that do not appear in theseed data (?unknown words?)
is a strong indicator6210 500 1000 1500 2000304050607080number of manually annotated sentencesf scoreno self?trainingwsj self?trainingbrown self?training0 500 1000 1500 200020304050607080number of manually annotated sentencesrecallno self?trainingwsj self?trainingbrown self?training0 500 1000 1500 2000727476788082number of manually annotated sentencesprecisionno self?trainingwsj self?trainingbrown self?trainingFigure 3: Number of seed sentences vs. f-score (left), recall (middle) and precision (right), for the twoout-of-domain seed data experiments: OO (triangles) and OI (squares), and for the no self-training baseline.f-sc.
66 68 70 72 74OD 600 800 1, 000 1, 400 ?ID 600 700 800 1, 000 1, 200OO 400 500 600 800 110033, 33 28.6, 37.5 33, 40 20, 42.9 8, ?OI 400 500 600 800 1, 30033, 33 28.6, 37.5 33, 40 20, 42.9 ?8, ?Table 4: Number of manually annotated seed sen-tences needed for achieving certain f-score values.The first two lines show the out-of-domain and in-domain seed baselines.
The reductions compared tothe baselines is given as ID, OD.0 500 1000 1500 20001020304050number of manually annotated sentencesf scoreswitchboard self?trainingno self?trainingFigure 4: Number of seed sentences vs. f-score,for the OI experiment using WSJ seed data andSwitchBoard self-training and test data.
In spite ofthe strong dissimilarity between the domains, self-training provides a substantial improvement.to whether it is worthwhile to use small seed self-training.
Figure 5 shows the number of unknownwords in a sentence vs. the probability that the self-training model will parse a sentence no worse (up-per curve) or better (lower curve) than the baselinemodel.The upper curve shows that regardless of the0 10 20 30 40 5000.20.40.60.81number of unknown wordsprobabilityST > baselineST >= baselineFigure 5: For sentences having the same number ofunknown words, we show the probability that theself-training model parses a sentence from the setno worse (upper curve) or better (lower curve) thanthe baseline model.number of unknown words in the sentence, there ismore than 50% chance that the self-training modelwill not harm the result.
This probability decreasesfrom almost 1 for a very small number of unknownwords to about 0.55 for 50 unknown words.
Thelower curve shows that when the number of un-known words increases, the probability that theself-training model will do better than the baselinemodel increases from almost 0 (for a very smallnumber of unknown words) to about 0.55.
Hence,the number of unknown words is an indication forthe potential benefit (value on the lower curve)and risk (1 minus the value on the upper curve) inusing the self-training model compared to using thebaseline model.
Unknown words were not identifiedin (McClosky et al, 2006a) as a useful predictor forthe benefit of self-training.622We also identified a length effect similar to thatstudied by (McClosky et al, 2006a) for self-training(using a reranker and large seed, as detailed in Sec-tion 2).
Due to space limitations we do not discussit here.7 DiscussionSelf-training is usually not considered to be a valu-able technique in improving the performance of gen-erative statistical parsers, especially when the man-ually annotated seed sentence dataset is small.
In-deed, in the II scenario, (Steedman et al, 2003a;McClosky et al, 2006a; Charniak, 1997) reportedno improvement of the base parser for small (500sentences, in the first paper) and large (40K sen-tences, in the last two papers) seed datasets respec-tively.
In the II, OO, and OI scenarios, (McClosky etal, 2006a; 2006b) succeeded in improving the parserperformance only when a reranker was used to re-order the 50-best list of the generative parser, with aseed size of 40K sentences.
Bacchiani et al(2006)improved the parser performance in the OI scenariobut their seed size was large (about 20K sentences).In this paper we have shown that self-trainingcan enhance the performance of generative parsers,without a reranker, in four in- and out-of-domainscenarios using a small seed dataset.
For the II, IOand OO scenarios, we are the first to show improve-ment by self-training for generative parsers.
Weachieved a 50% (20-33%) reduction in annotationcost for the in-domain (out-of-domain) seed datascenarios.
Previous work with small seed datasetsconsidered only the II and OI scenarios.
Our resultsfor the former are better than any previous method,and our results for the latter (which are the firstreported self-training results) are similar to previ-ous results for co-training, a more complex method.We demonstrated our results using three corpora ofvarying degrees of domain difference.A direction for future research is combiningself-training data from various domains to enhanceparser adaptation.Acknowledgement.
We would like to thank DanRoth for his constructive comments on this paper.ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat, 2006.
MAP adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Markus Becker and Miles Osborne, 2005.
A two-stagemethod for active learning of statistical grammars.
IJ-CAI ?05.Daniel Bikel, 2004.
Code developed at University ofPennsylvania.
http://www.cis.upenn.edu.bikel.Avrim Blum and Tom M. Mitchell, 1998.
Combining la-beled and unlabeled data with co-training.
COLT ?98.Eugene Charniak, 1997.
Statistical parsing with acontext-free grammar and word statistics.
AAAI ?97.Eugene Charniak, 2000.
A maximum-entropy-inspiredparser.
ANLP ?00.Eugene Charniak and Mark Johnson, 2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
ACL ?05.Stephen Clark, James Curran, and Miles Osborne,2003.
Bootstrapping pos taggers using unlabelleddata.
CoNLL ?03.David A. Cohn, Les Atlas, and Richard E. Ladner, 1994.Improving generalization with active learning.
Ma-chine Learning, 15(2):201?221.Michael Collins, 1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Universityof Pennsylvania.Rebecca Hwa, Miles Osborne, Anoop Sarkar and MarkSteedman, 2003.
Corrected co-training for statisticalparsers.
In ICML ?03, Workshop on the Continuumfrom Labeled to Unlabeled Data in Machine Learningand Data Mining.Rebecca Hwa, 2004.
Sample selection for statisticalparsing.
Computational Linguistics, 30(3):253?276.Terry Koo and Michael Collins, 2005.
Hidden-variablemodels for discriminative reranking.
EMNLP ?05.David McClosky, Eugene Charniak, and Mark John-son, 2006a.
Effective self-training for parsing.
HLT-NAACL ?06.David McClosky, Eugene Charniak, and Mark Johnson,2006b.
Reranking and self-training for parser adapta-tion.
ACL-COLING ?06.Mark Steedman, Anoop Sarkar, Miles Osborne, RebeccaHwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim, 2003a.
Bootstrap-ping statistical parsers from small datasets.
EACL ?03.Mark Steedman, Rebecca Hwa, Stephen Clark, MilesOsborne, Anoop Sarkar, Julia Hockenmaier, PaulRuhlen,Steven Baker, and Jeremiah Crim, 2003b.
Ex-ample selection for bootstrapping statistical parsers.NAACL ?03.623
