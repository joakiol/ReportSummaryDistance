Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 25?28,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsThe Complexity of Phrase Alignment ProblemsJohn DeNero and Dan KleinComputer Science Division, EECS DepartmentUniversity of California at Berkeley{denero, klein}@cs.berkeley.eduAbstractMany phrase alignment models operate overthe combinatorial space of bijective phrasealignments.
We prove that finding an optimalalignment in this space is NP-hard, while com-puting alignment expectations is #P-hard.
Onthe other hand, we show that the problem offinding an optimal alignment can be cast asan integer linear program, which provides asimple, declarative approach to Viterbi infer-ence for phrase alignment models that is em-pirically quite efficient.1 IntroductionLearning in phrase alignment models generally re-quires computing either Viterbi phrase alignmentsor expectations of alignment links.
For some re-stricted combinatorial spaces of alignments?thosethat arise in ITG-based phrase models (Cherry andLin, 2007) or local distortion models (Zens et al,2004)?inference can be accomplished using poly-nomial time dynamic programs.
However, for morepermissive models such as Marcu and Wong (2002)and DeNero et al (2006), which operate over the fullspace of bijective phrase alignments (see below), nopolynomial time algorithms for exact inference havebeen exhibited.
Indeed, Marcu and Wong (2002)conjectures that none exist.
In this paper, we showthat Viterbi inference in this full space is NP-hard,while computing expectations is #P-hard.On the other hand, we give a compact formula-tion of Viterbi inference as an integer linear program(ILP).
Using this formulation, exact solutions to theViterbi search problem can be found by highly op-timized, general purpose ILP solvers.
While ILPis of course also NP-hard, we show that, empir-ically, exact solutions are found very quickly formost problem instances.
In an experiment intendedto illustrate the practicality of the ILP approach, weshow speed and search accuracy results for aligningphrases under a standard phrase translation model.2 Phrase Alignment ProblemsRather than focus on a particular model, we describefour problems that arise in training phrase alignmentmodels.2.1 Weighted Sentence PairsA sentence pair consists of two word sequences, eand f. A set of phrases {eij} contains all spans eijfrom between-word positions i to j of e. A link is analigned pair of phrases, denoted (eij , fkl).1Let a weighted sentence pair additionally includea real-valued function ?
: {eij}?
{fkl} ?
R, whichscores links.
?
(eij , fkl) can be sentence-specific, forexample encoding the product of a translation modeland a distortion model for (eij , fkl).
We impose noadditional restrictions on ?
for our analysis.2.2 Bijective Phrase AlignmentsAn alignment is a set of links.
Given a weightedsentence pair, we will consider the space of bijectivephrase alignments A: those a ?
{eij} ?
{fkl} thatuse each word token in exactly one link.
We firstdefine the notion of a partition: unionsqiSi = T means Siare pairwise disjoint and cover T .
Then, we can for-mally define the set of bijective phrase alignments:A =??
?a :?
(eij ,fkl)?aeij = e ;?
(eij ,fkl)?afkl = f??
?1As in parsing, the position between each word is assignedan index, where 0 is to the left of the first word.
In this paper,we assume all phrases have length at least one: j > i and l > k.25Both the conditional model of DeNero et al(2006) and the joint model of Marcu and Wong(2002) operate in A, as does the phrase-based de-coding framework of Koehn et al (2003).2.3 Problem DefinitionsFor a weighted sentence pair (e, f, ?
), let the scoreof an alignment be the product of its link scores:?
(a) =?
(eij ,fkl)?a?
(eij , fkl).Four related problems involving scored alignmentsarise when training phrase alignment models.OPTIMIZATION, O: Given (e, f, ?
), find the high-est scoring alignment a.DECISION, D: Given (e, f, ?
), decide if there is analignment a with ?
(a) ?
1.O arises in the popular Viterbi approximation toEM (Hard EM) that assumes probability mass isconcentrated at the mode of the posterior distribu-tion over alignments.
D is the corresponding deci-sion problem for O, useful in analysis.EXPECTATION, E: Given a weighted sentence pair(e, f, ?)
and indices i, j, k, l, compute?a ?
(a)over all a ?
A such that (eij , fkl) ?
a.SUM, S: Given (e, f, ?
), compute?a?A ?
(a).E arises in computing sufficient statistics forre-estimating phrase translation probabilities (E-step) when training models.
The existence of apolynomial time algorithm for E implies a poly-nomial time algorithm for S , because A =?|e|j=1?|f|?1k=0?|f|l=k+1 {a : (e0j , fkl) ?
a,a ?
A} .3 Complexity of Inference in AFor the space A of bijective alignments, problems Eand O have long been suspected of being NP-hard,first asserted but not proven in Marcu and Wong(2002).
We give a novel proof that O is NP-hard,showing that D is NP-complete by reduction fromSAT, the boolean satisfiability problem.
This re-sult holds despite the fact that the related problem offinding an optimal matching in a weighted bipartitegraph (the ASSIGNMENT problem) is polynomial-time solvable using the Hungarian algorithm.3.1 Reducing Satisfiability to DA reduction proof of NP-completeness gives a con-struction by which a known NP-complete problemcan be solved via a newly proposed problem.
From aSAT instance, we construct a weighted sentence pairfor which alignments with positive score correspondexactly to the SAT solutions.
Since SAT is NP-complete and our construction requires only poly-nomial time, we conclude that D is NP-complete.2SAT: Given vectors of boolean variables v = (v)and propositional clauses3 C = (C), decidewhether there exists an assignment to v that si-multaneously satisfies each clause in C.For a SAT instance (v,C), we construct f to con-tain one word for each clause, and e to contain sev-eral copies of the literals that appear in those clauses.?
scores only alignments from clauses to literals thatsatisfy the clauses.
The crux of the construction liesin ensuring that no variable is assigned both true andfalse.
The details of constructing such a weightedsentence pair wsp(v,C) = (e, f, ?
), described be-low, are also depicted in figure 1.1. f contains a word for each C, followed by anassignment word for each variable, assign(v).2. e contains c(`) consecutive words for each lit-eral `, where c(`) is the number of times that `appears in the clauses.Then, we set ?
(?, ?)
= 0 everywhere except:3.
For all clauses C and each satisfying literal `,and each one-word phrase e in e containing `,?
(e, fC) = 1. fC is the one-word phrase con-taining C in f.4.
The assign(v) words in f align to longer phrasesof literals and serve to consistently assign eachvariable by using up inconsistent literals.
Theyalso align to unused literals to yield a bijection.Let ek[`] be the phrase in e containing all literals` and k negations of `.
fassign(v) is the one-wordphrase for assign(v).
Then, ?
(ek[`], fassign(v)) =1 for ` ?
{v, v?}
and all applicable k.2Note that D is trivially in NP: given an alignment a, it iseasy to determine whether or not ?
(a) ?
1.3A clause is a disjunction of literals.
A literal is a bare vari-able vn or its negation v?n.
For instance, v2?
v?7?
v?9 is a clause.26v1?
v2?
v3v?1?
v2?
v?3v?1?
v?2?
v?3v?1?
v?2?
v3v1v?1v?2v?3v3v2v?1v?1v2v?2v3v?3v1v?1v?2v?3v3v2v?1v?1v2v?2v3v?3(a) (b) (c)assign(v1)assign(v2)assign(v3)(d)v1is truev2is falsev3is falseFigure 1: (a) The clauses of an example SAT instance with v = (v1, v2, v3).
(b) The weighted sentence pair wsp(v,C)constructed from the SAT instance.
All links that have ?
= 1 are marked with a blue horizontal stripe.
Stripes in thelast three rows demarcate the alignment options for each assign(vn), which consume all words for some literal.
(c) Abijective alignment with score 1.
(d) The corresponding satisfying assignment for the original SAT instance.Claim 1.
If wsp(v,C) has an alignment a with?
(a) ?
1, then (v,C) is satisfiable.Proof.
The score implies that f aligns using all one-word phrases and ?ai ?
a, ?
(ai) = 1.
By condition4, each fassign(v) aligns to all v?
or all v in e. Then,assign each v to true if fassign(v) aligns to all v?, andfalse otherwise.
By condition 3, each C must alignto a satisfying literal, while condition 4 assures thatall available literals are consistent with this assign-ment to v, which therefore satisfies C.Claim 2.
If (v,C) is satisfiable, then wsp(v,C) hasan alignment a with ?
(a) = 1.Proof.
We construct such an alignment a from thesatisfying assignment v. For each C, we choose asatisfying literal ` consistent with the assignment.Align fC to the first available ` token in e if the cor-responding v is true, or the last if v is false.
Aligneach fassign(v) to all remaining literals for v.Claims 1 and 2 together show that D is NP-complete, and therefore that O is NP-hard.3.2 Reducing Perfect Matching to SWith another construction, we can show that S is #P-hard, meaning that it is at least as hard as any #P-complete problem.
#P is a class of counting prob-lems related to NP, and #P-hard problems are NP-hard as well.COUNTING PERFECT MATCHINGS, CPMGiven a bipartite graph G with 2n vertices,count the number of matchings of size n.For a bipartite graphGwith edge setE = {(vj , vl)},we construct e and f with n words each, and set?
(ej?1 j , fl?1 l) = 1 and 0 otherwise.
The num-ber of perfect matchings in G is the sum S forthis weighted sentence pair.
CPM is #P-complete(Valiant, 1979), so S (and hence E) is #P-hard.4 Solving the Optimization ProblemAlthough O is NP-hard, we present an approach tosolving it using integer linear programming (ILP).4.1 Previous Inference ApproachesMarcu and Wong (2002) describes an approximationto O.
Given a weighted sentence pair, high scoringphrases are linked together greedily to reach an ini-tial alignment.
Then, local operators are applied tohill-climb A in search of the maximum a.
This pro-cedure also approximates E by collecting weightedcounts as the space is traversed.DeNero et al (2006) instead proposes anexponential-time dynamic program to systemati-cally explore A, which can in principle solve eitherO or E. In practice, however, the space of align-ments has to be pruned severely using word align-ments to control the running time of EM.Notably, neither of these inference approaches of-fers any test to know if the optimal alignment is everfound.
Furthermore, they both require small datasets due to computational expense.4.2 Alignment via an Integer ProgramWe cast O as an ILP problem, for which many opti-mization techniques are well known.
First, we in-27troduce binary indicator variables ai,j,k,l denotingwhether (eij , fkl) ?
a.
Furthermore, we introducebinary indicators ei,j and fk,l that denote whethersome (eij , ?)
or (?, fkl) appears in a, respectively.
Fi-nally, we represent the weight function ?
as a weightvector in the program: wi,j,k,l = log ?
(eij , fkl).Now, we can express an integer program that,when optimized, will yield the optimal alignment ofour weighted sentence pair.max?i,j,k,lwi,j,k,l ?
ai,j,k,ls.t.
?i,j:i<x?jei,j = 1 ?x : 1 ?
x ?
|e| (1)?k,l:k<y?lfk,l = 1 ?y : 1 ?
y ?
|f | (2)ei,j =?k,lai,j,k,l ?i, j (3)fk,l =?i,jai,j,k,l ?k, l (4)with the following constraints on index variables:0 ?
i < |e|, 0 < j ?
|e|, i < j0 ?
k < |f |, 0 < l ?
|f |, k < l .The objective function is log ?
(a) for a impliedby {ai,j,k,l = 1}.
Constraint equation 1 ensures thatthe English phrases form a partition of e ?
each wordin e appears in exactly one phrase ?
as does equa-tion 2 for f. Constraint equation 3 ensures that eachphrase in the chosen partition of e appears in exactlyone link, and that phrases not in the partition are notaligned (and likewise constraint 4 for f).5 ApplicationsThe need to find an optimal phrase alignment for aweighted sentence pair arises in at least two appli-cations.
First, a generative phrase alignment modelcan be trained with Viterbi EM by finding optimalphrase alignments of a training corpus (approximateE-step), then re-estimating phrase translation param-eters from those alignments (M-step).Second, this is an algorithm for forced decoding:finding the optimal phrase-based derivation of a par-ticular target sentence.
Forced decoding arises inonline discriminative training, where model updatesare made toward the most likely derivation of a goldtranslation (Liang et al, 2006).Sentences per hour on a four-core server 20,000Frequency of optimal solutions found 93.4%Frequency of -optimal solutions found 99.2%Table 1: The solver, tuned for speed, regularly reportssolutions that are within 10?5 of optimal.Using an off-the-shelf ILP solver,4 we were ableto quickly and reliably find the globally optimalphrase alignment under ?
(eij , fkl) derived from theMoses pipeline (Koehn et al, 2007).5 Table 1 showsthat finding the optimal phrase alignment is accurateand efficient.6 Hence, this simple search techniqueeffectively addresses the intractability challenges in-herent in evaluating new phrase alignment ideas.ReferencesColin Cherry and Dekang Lin.
2007.
Inversion transduc-tion grammar for joint phrasal translation modeling.In NAACL-HLT Workshop on Syntax and Structure inStatistical Translation.John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why generative phrase models underperformsurface heuristics.
In NAACL Workshop on StatisticalMachine Translation.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In ACL.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In EMNLP.Leslie G. Valiant.
1979.
The complexity of computingthe permanent.
In Theoretical Computer Science 8.Richard Zens, Hermann Ney, Taro Watanabeand, andE.
Sumita.
2004.
Reordering constraints for phrasebased statistical machine translation.
In Coling.4We used Mosek: www.mosek.com.5?
(eij , fkl) was estimated using the relative frequency ofphrases extracted by the default Moses training script.
We eval-uated on English-Spanish Europarl, sentences up to length 25.6ILP solvers include many parameters that trade off speedfor accuracy.
Substantial speed gains also follow from explicitlypruning the values of ILP variables based on prior information.28
