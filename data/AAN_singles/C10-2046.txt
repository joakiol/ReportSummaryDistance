Coling 2010: Poster Volume, pages 400?408,Beijing, August 2010Learning to Model Domain-Specific Utterance Sequences for ExtractiveSummarization of Contact Center DialoguesRyuichiro Higashinaka?, Yasuhiro Minami?, Hitoshi Nishikawa?,Kohji Dohsaka?, Toyomi Meguro?, Satoshi Takahashi?, Genichiro Kikui??
NTT Cyber Space Laboratories, NTT Corporation?
NTT Communication Science Laboratories, NTT Corporationhigashinaka.ryuichiro@lab.ntt.co.jp, minami@cslab.kecl.ntt.co.jpnishikawa.hitoshi@lab.ntt.co.jp, {dohsaka,meguro}@cslab.kecl.ntt.co.jp{takahashi.satoshi,kikui.genichiro}@lab.ntt.co.jpAbstractThis paper proposes a novel extractivesummarization method for contact cen-ter dialogues.
We use a particulartype of hidden Markov model (HMM)called Class Speaker HMM (CSHMM),which processes operator/caller utterancesequences of multiple domains simulta-neously to model domain-specific utter-ance sequences and common (domain-wide) sequences at the same time.
Weapplied the CSHMM to call summariza-tion of transcripts in six different con-tact center domains and found that ourmethod significantly outperforms compet-itive baselines based on the maximumcoverage of important words using integerlinear programming.1 IntroductionIn modern business, contact centers are becom-ing more and more important for improving cus-tomer satisfaction.
Such contact centers typicallyhave quality analysts who mine calls to gain in-sight into how to improve business productivity(Takeuchi et al, 2007; Subramaniam et al, 2009).To enable them to handle the massive number ofcalls, automatic summarization has been utilizedand shown to successfully reduce costs (Byrd etal., 2008).
However, one of the problems in cur-rent call summarization is that a domain ontologyis required for understanding operator/caller utter-ances, which makes it difficult to port one summa-rization system from domain to domain.This paper describes a novel automatic sum-marization method for contact center dialogueswithout the costly process of creating domain on-tologies.
More specifically, given contact centerdialogues categorized into multiple domains, wecreate a particular type of hidden Markov model(HMM) called Class Speaker HMM (CSHMM)to model operator/caller utterance sequences.
TheCSHMM learns to distinguish sequences of indi-vidual domains and common sequences in all do-mains at the same time.
This approach makes itpossible to accurately distinguish utterances spe-cific to a certain domain and thereby has the po-tential to generate accurate extractive summaries.In Section 2, we review recent work on auto-matic summarization, including its application tocontact center dialogues.
In Section 3, we de-scribe the CSHMM.
In Section 4, we describeour automatic summarization method in detail.
InSection 5, we describe the experiment we per-formed to verify our method and present the re-sults.
In Section 6, we summarize and mentionfuture work.2 Related WorkThere is an abundance of research in automaticsummarization.
It has been successfully applied tosingle documents (Mani, 2001) as well as to mul-tiple documents (Radev et al, 2004), and varioussummarization methods, such as the conventionalLEAD method, machine-learning based sentenceselection (Kupiec et al, 1995; Osborne, 2002),and integer linear programming (ILP) based sen-tence extraction (Gillick and Favre, 2009), havebeen proposed.
Recent years have seen work onsummarizing broadcast news speech (Hori andFurui, 2003), multi-party meetings (Murray et al,2005), and contact center dialogues (Byrd et al,2008).
However, despite the large amount of pre-vious work, little work has tackled the automaticsummarization of multi-domain data.400In the past few decades, contact center dia-logues have been an active research focus (Gorinet al, 1997; Chu-Carroll and Carpenter, 1999).Initially, the primary aim of such research wasto transfer calls from answering agents to oper-ators as quickly as possible in the case of prob-lematic situations.
However, real-time processingof calls requires a tremendous engineering effort,especially when customer satisfaction is at stake,which led to recent work on the offline process-ing of calls, such as call mining (Takeuchi et al,2007) and call summarization (Byrd et al, 2008).The work most related to ours is (Byrd et al,2008), which maps operator/caller utterances toan ontology in the automotive domain by usingsupport vector machines (SVMs) and creates astructured summary by heuristic rules that assignthe mapped utterances to appropriate summarysections.
Our work shares the same motivationas theirs in that we want to make it easier forquality analysts to analyze the massive number ofcalls.
However, we tackle the problem differentlyin that we propose a newmodeling of utterance se-quences for extractive summarization that makesit unnecessary to create heuristics rules by handand facilitates the porting of a summarization sys-tem.HMMs have been successfully applied to au-tomatic summarization (Barzilay and Lee, 2004).In their work, an HMM was used to model thetransition of content topics.
The Viterbi decod-ing (Rabiner, 1990) was performed to find con-tent topics that should be incorporated into a sum-mary.
Their approach is similar to ours in thatHMMs are utilized to model topic sequences, butthey did not use data of multiple domains in creat-ing their model.
In addition, their method requirestraining data (original articles with their referencesummaries) in order to find which content top-ics should be included in a summary, whereas ourmethod requires only the raw sequences with theirdomain labels.3 Class Speaker HMMA Class Speaker HMM (CSHMM) is an exten-sion of Speaker HMM (SHMM), which has beenutilized to model two-party conversations (Me-guro et al, 2009).
In an SHMM, there are twostates, and each state emits utterances of one ofthe two conversational participants.
The states are1:speaker1 2:speaker2Speaker HMM for Class 13:speaker1 4:speaker2Speaker HMM for Class 2Figure 1: Topology of an ergodic CSHMM.
Num-bers before ?speaker1?
and ?speaker2?
denote stateIDs.connected ergodically and the emission/transitionprobabilities are learned from training data byusing the EM-algorithm.
Although Meguro etal., (2009) used SHMMs to analyze the flow oflistening-oriented dialogue, we extend their ideato make it applicable to classification tasks, suchas dialogue segmentation.A CSHMM is simply a concatenation ofSHMMs, each of which is trained by using ut-terance sequences of a particular dialogue class.After such SHMMs are concatenated, the Viterbialgorithm is used to decode an input utterancesequence into class labels by estimating fromwhich class each utterance has most likely to havebeen generated.
Figure 1 illustrates the basictopology of a CSHMM where two SHMMs areconcatenated ergodically.
When the most likelystate sequence for an input utterance sequence is<1,3,4,2>, we can convert these state IDs intotheir corresponding classes; that is, <1,2,2,1>,which becomes the result of utterance classifica-tion.We have conceived three variations of CSHMMas we describe below.
They differ in how we treatutterance sequences that appear commonly in allclasses and how we train the transition probabili-ties between independently trained SHMMs.3.1 Ergodic CSHMMThe most basic CSHMM is the ergodic CSHMM,which is a simple concatenation of SHMMs inan ergodic manner as shown in Fig.
1.
For Kclasses, K SHMMs are combined with the initialand transition probabilities all set to equal.
In thisCSHMM, the assignment of class labels solely de-pends on the output distributions of each class.3.2 Ergodic CSHMM with Common StatesThis type of CSHMM is the same as the ergodicCSHMM except that it additionally has a SHMMtrained from all dialogues of all classes.
There-4013:speaker1 4:speaker21:speaker1 2:speaker25:speaker1 6:speaker2Speaker HMM for Class 1Speaker HMM for Class 2Speaker HMM for All Classes (Class 0)Figure 2: CSHMM with common states.CopyClass1M1M1M0RetrainTrainClasskM0MkMkRetrainTrainClassKM0MKMKRetrainTrainAllClassesM0Train+M0M1 Mk MKAVGConcatenateM1+0 Mk+0 MK+0M1M0 M0 MkM0 MKM1+0 Mk+0 MK+0Step 1Step 2Step 3Step 2?ENDMconcatIf the fitting hasconverged forall Mk+0Split Mconcat intopairs again andretrain Mk+0M1?MK becomeless likely tooutput commonsequencesTransition probabilitiesof M0 are redistributedbetween M0 and MkFigure 3: Three steps to create a CSHMM usingconcatenated training.fore, for K classes, this CSHMM has K + 1SHMMs.
Figure 2 shows the model topology.This newly added SHMM works in a manner sim-ilar to the background model (Reynolds et al,2000) representing sequences that are commonto all classes.
By having these common states,common utterance sequences can be classified as?common?, making it possible to avoid forcefullyclassifying common utterance sequences into oneof the given classes.Detecting common sequences is especiallyhelpfulwhen several classes overlap in nature.
Forexample, most dialogues commonly start and endwith greetings, and many calls at contact centerscommonly contain exchanges in which the opera-tor requests personal information about the callerfor confirmation.
Regarding the model topologyin Fig.
2, if the most likely state sequence bythe Viterbi decoding is <1,4,5,6,3,2>, we obtaina class label sequence <1,2,0,0,2,1> where thethird and fourth utterances are classified as ?zero?,meaning that they do not belong to any class.3.3 CSHMM using Concatenated TrainingThe CSHMMs presented so far have two prob-lems: one is that the order of utterances of differ-ent classes cannot be taken into account becauseof the equal transition probabilities.
As a result,the very merit of HMMs, their ability to modeltime series data, is lost.
The other is that the out-put distributions of common states may be overlybroad because they are the averaged distributionsover all classes; that is, the best path determinedby the Viterbi decoding may not go through thecommon states at all.Our solution to these problems is to apply con-catenated training (Lee, 1989), which has beensuccessfully used in speech recognition to modelphoneme sequences in an unsupervised manner.The procedure for concatenated training is illus-trated in Fig.
3 and has three steps.step 1 Let Mk (Mk ?
M, 1 ?
k ?
K) be theSHMM trained using dialogues Dk whereDk = {?dj|c(dj) = k}, and M0 be theSHMM trained using all dialogues; i.e., D.Here, K means the total number of classesand c(dj) the class assigned to a dialogue dj .step 2 Connect each Mk ?
M with a copy ofM0 using equal initial and transition proba-bilities (we call this connected model Mk+0)and retrain Mk+0 with ?dj ?
Dk wherec(dj) = k.step 3 Merge all models Mk+0 (1 ?
k ?
K) toproduce one concatenated HMM (Mconcat).Here, the output probabilities of the copiesof M0 are averaged over K when all modelsare merged to create a combined model.
Ifthe fitting of all Mk+0 models has convergedagainst the training data, exit this procedure;otherwise, go to step 2 by connecting a copyof M0 and Mk for all k. Here, the transi-tion probabilities from M0 to Ml(l 6= k) aresummed and equally distributed between thecopied M0?s self-loop and transitions to thestates in Mk.In concatenated training, the transition and outputprobabilities can be optimized between M0 and402ContactCenterDialoguesDomain 1Domain K?HMM for Domain 1 HMM for Domain KHMM for All DomainsModel topic labelsequencesINPUT: A dialogue in Domain kTopic ModelTopic label sequenceLSA/LDAAssigntopiclabelsDomain label sequence OUTPUT: summaryViterbi decodingAssigntopic labelsSelect utterances labeled with Domain kClass SpeakerHMM?..Utterance sequenceFeature sequenceExtract content wordsas utterance featuresFigure 4: Overview of our summarizationmethod.Mk, meaning that the output probabilities of utter-ance sequences that are common and also foundin Mk can be moved from Mk to M0.
This makesthe distribution of Mk sharp (not broad/uniform),making it likely to output only the utterances rep-resentative of a class k. As regards M0, its distri-bution of output probabilities can also be sharp-ened for utterances that occur commonly in allclasses.
This sharpening of distributions is likelyto be helpful for class discrimination.4 Summarization MethodWe apply CSHMMs to extractive summarizationof contact center dialogues because such dia-logues are two-party, can be categorized into mul-tiple classes by their call domains (e.g., inquirytypes), and are likely contain many overlappingexchanges between an operator and a caller acrossdomains, such as greetings, the confirmation ofpersonal information, and other cliches in busi-ness (e.g., name exchanges, thanking/apologizingphrases, etc.
), making them the ideal target forCSHMMs.In our method, summarization is performed bydecoding a sequence of utterances of a domainDMk into domain labels and selecting those ut-terances that have domain labels DMk.
Thismakes it possible to extract utterances that arecharacteristic of DMk in relation to other do-mains.
Our assumption is that extracting charac-teristic sequences of a given domain provides agood summary for that domain because such se-quences should contain important information ne-cessitated by the domain.Figure 4 outlines our extractive summarizationprocess.
The process consists of a training phaseand a decoding phase as described below.Training phase: Let D (d1 .
.
.
dN ) be the entireset of contact center dialogues, DMk (DMk ?DM, 1 ?
k ?
K) the domain assigned to do-main k, and Udi,1 .
.
.Udi,H the utterances in di.Here, H is the number of utterances in di.
FromD, we create two models: a topic model (TM )and a CSHMM.The topic model is used to assign a single topicto each utterance so as to facilitate the trainingof the CSHMM by reducing the dimensions ofthe feature space.
The same approach has beentaken in (Barzilay and Lee, 2004).
The topicmodel can be created by such techniques as prob-abilistic latent semantic analysis (PLSA) (S?ingliarand Hauskrecht, 2006) and latent Dirichlet alo-cation (LDA) (Tam and Schultz, 2005).
PLSAmodels the latent topics of the documents and itsBaysian extension is LDA, which also models theco-occurrence of topics using the Dirichlet prior.We first derive features Fd1 .
.
.
FdN for the dia-logues.
Here, we assume a bag-of-words repre-sentation for the features; therefore, Fdi is repre-sented as {< w1, c1 > .
.
.
< wV , cV >}, whereV means the total number of content words in thevocabulary and < wi, ci > denotes that a contentword wi appears ci times in a dialogue.
Note thatwe derive the features for dialogues, not for utter-ances, because utterances in dialogue can be veryshort, often consisting of only one or two wordsand thus making it hard to calculate the word co-occurrence required for creating a topic model.From the features, we build a topic model that in-cludes P(z|w), where w is a word and z is a topic.Using the topic model, we can assign a singletopic label to every utterance in D by finding itslikely topic; i.e., argmaxz?w?words(Udi) P(z|w).After labeling all utterances in D with topic la-bels, we train a CSHMM that learns characteristictopic label sequences in each domain as well ascommon topic label sequences across domains.Decoding phase: Let dj be the input dialogue,DM(dj) (?
DM ) the table for obtaining the do-main label of dj , and Udj ,1 .
.
.Udj ,Hdj the utter-ances in dj, where Hdj is the number of the utter-ances.
We use TM to map the utterances to topic403Domain # Tasks Sentences CharactersFIN 15 8.93 289.93ISP 15 7.20 259.53LGU 20 9.85 328.55MO 15 10.07 326.20PC 15 9.40 354.07TEL 18 8.44 322.22ALL 98 9.01 314.46Table 1: Scenario statistics: the number of tasksand averaged number of sentences/characters in atask scenario in the six domains.labels Tdj ,1 .
.
.Tdj ,Hdj and convert them into do-main label sequences DMdj ,1 .
.
.DMdj ,Hdj us-ing the trained CSHMM by the Viterbi decoding.Then, we select Udj ,h (1 ?
h ?
Hdj ) whose cor-responding domain labelDMdj ,h equalsDM(dj)and output the selected utterances in the order ofappearance in the original dialogue as a summary.5 ExperimentWe performed an experiment to verify our sum-marization method.
We first collected simulatedcontact center dialogues using human subjects.Then, we compared our method with baseline sys-tems.
Finally, we analyzed the created summariesto investigate what had been learned by our CSH-MMs.5.1 Dialogue DataSince we do not have access to actual contact cen-ter data, we recruited human subjects to collectsimulated contact center dialogues.
A total of 90participants (49 males and 41 females) took theroles of operator or a caller and talked over tele-phones in separate rooms.
The callers were givenrealistic scenarios that included their motivationfor a call as well as detailed instructions aboutwhat to ask.
The operators, who had experienceof working at contact centers, were given manualscontaining the knowledge of the domain and ex-plaining how to answer questions in specific sce-narios.The dialogues took place in six different do-mains: Finance (FIN), Internet Service Provider(ISP), Local Government Unit (LGU), Mail Or-der (MO), PC support (PC), and Telecommuni-cation (TEL).
In each domain, there were 15?20tasks.
Table 1 shows the statistics of the task sce-narios used by the callers.
We cannot describe thedetails of each domain for lack of space, but ex-MO task No.
3: It is becoming a good season for theJapanese Nabe (pan) cuisine.
You own a Nabe restau-rant and it is going well.
When you were searching onthe Internet, thinking of creating a new dish, you sawthat drop-shipped Shimonoseki puffer fish was on sale.Since you thought the puffer fish cuisine would becomehot in the coming season, you decided to order it as atrial.
.
.
.
You ordered a puffer fish set on the Internet,but you have not received the confirmation email thatyou were supposed to receive.
.
.
.
You decided to callthe contact center to make an inquiry, ask them whetherthe order has been successful, and request them to sendyou the confirmation email.Figure 5: Task scenario in the MO domain.
Thescenario was originally in Japanese and was trans-lated by the authors.amples of the tasks for FIN are inquiries about in-surance, notifications of the loss of credit cards,and applications for finance loans, and those forISP are inquiries about fees for Internet access, re-quests to forward emails, and reissuance of pass-words.
Figure 5 shows one of the task scenariosin the MO domain.We collected data on two separate occasions us-ing identical scenarios but different participants,which gave us two sets of dialogue data.
We usedthe former for training our summarization sys-tem and the latter for testing.
We only use thetranscriptions in this paper so as to avoid partic-ular problems of speech.
All dialogues were inJapanese.
Tables 2 and 3 show the statistics of thetraining data and the test data, respectively.
Ascan be seen from the tables, each dialogue is quitelong, which attests to the complexity of the tasks.5.2 Training our Summarization SystemFor training our system, we first created a topicmodel using LDA.We performed a morphologicalanalysis using ChaSen1 to extract content wordsfrom each dialogue and made its bag-of-wordsfeatures.
We defined content words as nouns,verbs, adjectives, unknown words, and interjec-tions (e.g., ?yes?, ?no?, ?thank you?, and ?sorry?
).We included interjections because they occur veryfrequently in dialogues and often possess impor-tant content, such as agreement and refusal, intransactional communication.
We use this defini-tion of content words throughout the paper.Then, using an LDA software package2, webuilt a topic model.
We tentatively set the number1http://chasen-legacy.sourceforge.jp/2http://chasen.org/?daiti-m/dist/lda/404Utterances/Dial.
Characters/Utt.Domain # dial.
OPE CAL Both OPE CAL BothFIN 59 75.73 72.69 148.42 17.44 7.54 12.59ISP 64 55.09 53.17 108.27 20.11 8.03 14.18LGU 76 58.28 50.55 108.83 12.83 8.55 10.84MO 70 66.39 58.74 125.13 15.09 7.43 11.49PC 56 89.34 77.80 167.14 15.48 6.53 11.31TEL 66 75.58 63.97 139.55 12.74 8.24 10.67ALL 391 69.21 61.96 131.17 15.40 7.69 11.76Table 2: Training data statistics: Averaged num-ber of utterances per dialogue and characters perutterance for each domain.
OPE and CAL denoteoperator and caller, respectively.
See Section 5.1for the full domain names.Utterances/Dial.
Characters/Utt.Domain # dial.
OPE CAL Both OPE CAL BothFIN 60 73.97 61.05 135.02 14.53 7.50 11.35ISP 59 76.08 61.24 137.32 15.43 6.94 11.65LGU 56 66.55 51.59 118.14 14.54 7.53 11.48MO 47 75.53 64.87 140.40 10.53 6.79 8.80PC 44 124.02 94.16 218.18 14.23 7.79 11.45TEL 41 93.71 68.54 162.24 13.94 7.85 11.37ALL 307 83.07 65.69 148.76 13.98 7.41 11.08Table 3: Test data statistics.of topics to 100.
Using this topic model, we la-beled all utterances in the training data using these100 topic labels.We trained seven different CSHMMs in all: oneergodic CSHMM (ergodic0), three variants of er-godic CSHMMs with common states (ergodic1,ergodic2, ergodic3), and three variants of CSH-MMs with concatenated training (concat1, con-cat2, concat3).
The difference within the variantsis in the number of common states.
The numbers0?3 after ?ergodic?
and ?concat?
indicate the num-ber of SHMMs containing common states.
Forexample, ergodic3 has nine SHMMs (six SHMMsfor the six domains plus three SHMMs contain-ing common states).
Since more states wouldenable more minute modeling of sequences, wemade such variants in the hope that common se-quences could be more accurately modeled.
Wealso wanted to examine the possibility of creat-ing sharp output distributions in common stateswithout the concatenated training by such minutemodeling.
These seven CSHMMs make seven dif-ferent summarization systems.5.3 BaselinesBaseline-1: BL-TF We prepared two baselinesystems for comparison.
One is a simple sum-marizer based on the maximum coverage of highterm frequency (TF) content words.
We callthis baseline BL-TF.
This baseline summarizes adialogue by maximizing the following objectivefunction:max?zi?Zweight(wi) ?
ziwhere ?weight?
returns the importance of a con-tent word wi and zi is a binary value indicatingwhether to include wi in the summary.
Here,?weight?
returns the count of wi in a given dia-logue.
The maximization is done using ILP (weused an off-the-shelf solver lp solve3) with thefollowing three constraints:xi, zi ?
{0, 1}?xi?Xlixi ?
K?imijxi ?
zj (?zj ?
Z)where xi is a binary value that indicates whetherto include the i-th utterance in the summary, li isthe length of the i-th utterance,K is the maximumnumber of characters to include in a summary, andmij is a binary value that indicates whether wi isincluded in the j-th utterance.
The last constraintmeans that if a certain utterance is included in thesummary, all words in that utterance have to beincluded in the summary.Baseline-2: BL-DD Although BL-TF should bea very competitive baseline because it uses thestate-of-the-art formulation as noted in (Gillickand Favre, 2009), having only this baseline israther unfair because it does not make use of thetraining data, whereas our proposed method usesthem.
Therefore, we made another baseline thatlearns domain-specific dictionaries (DDs) fromthe training data and incorporates them into theweights of content words of the objective functionof BL-TF.
We call this baseline BL-DD.
In thisbaseline, the weight of a content word wi in a do-main DMk isweight(wi,DMk) =log(P(wi|DMk))log(P(wi|DM\DMk))3http://lpsolve.sourceforge.net/5.5/405Metric ergodic0 ergodic1 ergodic2 ergodic3 concat1 concat2 concat3PROPOSEDF 0.177 0.177 0.177 0.177 0.187?e0e1e2e3 0.198?+e0e1e2e3c1 0.199?+e0e1e2e3c1precision 0.145 0.145 0.145 0.145 0.161?
0.191?+ 0.195?+recall 0.294 0.294 0.294 0.294 0.280?
0.259?+ 0.259?+(Same-length) BL-TFF 0.171 0.171 0.171 0.171 0.168 0.164 0.163precision 0.132 0.132 0.132 0.132 0.135 0.140 0.140recall 0.294 0.294 0.294 0.294 0.270 0.241 0.240(Same-length) BL-DDF 0.189 0.189 0.189 0.189 0.189 0.187 0.187precision 0.155 0.155 0.155 0.155 0.162 0.170 0.172recall 0.287 0.287 0.287 0.287 0.273 0.250 0.248Compression Rate 0.42 0.42 0.42 0.42 0.37 0.30 0.30Table 4: F-measure, precision, and recall averaged over all 307 dialogues (cf.
Table 3) in the testset for the proposed methods and baselines BL-TF and BL-DD configured to output the same-lengthsummaries as the proposed systems.
The averaged compression rate for each proposed system is shownat the bottom.
The columns (ergodic0?concat3) indicate our methods as well as the character lengthsused by the baselines.
Asterisks, ?+?, e0?e3, and c1?c3 indicate our systems?
statistical significance bythe Wilcoxon signed-rank test (p<0.01) over BL-TF, BL-DD, ergodic0?3, and concat1?3, respectively.Statistical tests for the precision and recall were only performed between the proposed systems andtheir same-length baseline counterparts.
Bold font indicates the best score in each row.where P(wi|DMk) denotes the occurrence prob-ability of wi in the dialogues of DMk , andP(wi|DM\DMk) the occurrence probability ofwi in all domains except for DMk.
This log like-lihood ratio estimates how much a word is char-acteristic of a given domain.
Incorporating suchweights would make a very competitive baseline.5.4 Evaluation ProcedureWe made our seven proposed systems and twobaselines (BL-TF and BL-DD) output extractivesummaries for the test data.
Since one of theshortcomings of our proposedmethod is its inabil-ity to set the compression rate, we made our sys-tems output summaries first and made the baselinesystems output their summaries within the charac-ter lengths of our systems?
summaries.We used scenario texts (See Fig.
5) as referencedata; that is, a dialogue dealing with a certain taskis evaluated using the scenario text for that task.As an evaluation criterion, we used the F-measure(F1) to evaluate the retrieval accuracy on the ba-sis of the recall and precision of retrieved contentwords.
We used the scenarios as references be-cause they contain the basic content exchangedbetween an operator and a caller, the retrieval ac-curacy of which should be important for qualityanalysts.We could have used ROUGE (Lin and Hovy,2003), but we did not because ROUGE does notcorrelate well with human judgments in conversa-tional data (Liu and Liu, 2008).
Another benefit ofusing the F-measure is that summaries of varyinglengths can be compared.5.5 ResultsTable 4 shows the evaluation results for the pro-posed systems and the baselines.
It can be seenthat concat3 shows the best performance in F-measure among all systems, having a statisticallybetter performance over all systems except forconcat2.
The CSHMMs with concatenated train-ing were all better than ergodic0?3.
Here, the per-formance (and output) of ergodic0?3 was exactlythe same.
This happened because of the broad dis-tributions in their common states; no paths wentthrough the common states and all paths wentthrough the SHMMs of the six domains instead.The evaluation results in Table 4 may be ratherin favor of our systems because the summarizationlengths were set by the proposed systems.
There-fore, we performed another experiment to inves-tigate the performance of the baselines with vary-ing compression rates and compared their perfor-mance with the proposed systems in F-measure.We found that the best performance was achievedby BL-DD when the compression rate was 0.4with the F-measure of 0.191, which concat3 sig-nificantly outperformed by the Wilcoxon signed-rank test (p<0.01).
Note that the performanceshown in Table 4 may seem low.
However, wefound that the maximum recall is 0.355 (cal-406CAL1 When I order a product from you, I get a confir-mation emailCAL2 Puffer fishCAL3 Sets I have ordered, but I haven?t receivedthe confirmation emailOPE1 OrderOPE2 I will make a confirmation whether you haveorderedOPE3 Ten sets of Shimonoseki puffer fish by drop-shipOPE4 ?Yoriai?
(name of the product)OPE5 Two kilos of bony parts of tiger puffer fishOPE6 Baked fins for fin sakeOPE7 600 milliliter of puffer fish soy sauceOPE8 And, grated radish and red pepperOPE9 Your desired delivery date is the 13th of Febru-aryCAL4 Yes, all in small casesCAL5 This is q in alphabet right?CAL6 Hyphen gCAL7 You mean that the order was successfulOPE10 Yes, it was Nomura at JDS call centerFigure 6: Example output of concat3 for MO taskNo.
3 (cf Fig.
5).
The utterances were translatedby the authors.
The compression rate for this dia-logue was 0.24.culated by using summaries with no compres-sion).
This means that the maximum F-measurewe could attain is lower than 0.524 (when the pre-cision is ideal with 1).
This is because of the dif-ferences between the scenarios and the actual di-alogues.
We want to pursue ways to improve ourevaluation methodology in the future.Despite such issues in evaluation, from the re-sults, we conclude that our extractive summa-rization method is effective and that having thecommon states and training CSHMMs with con-catenated training are useful in modeling domain-specific sequences of contact center dialogues.5.6 Example of System OutputFigure 6 shows an example output of concat3 forthe scenario MO task No.
3 (cf.
Fig.
5).
Bold fontindicates utterances that were NOT included in thesummary of concat3?s same-length-BF-DD coun-terpart.
It is clear that sequences related to theMO domain were successfully extracted.
Whenwe look at the summary of BF-DD, we see suchutterances as ?Can I have your address from thepostcode?
and ?Finally, can I have your email ad-dress?, which are obvious cliches in contact centerdialogues.
This indicates the usefulness of com-mon states for ignoring such common exchanges.6 Summary and Future WorkThis paper proposed a novel extractive sum-marization method for contact center dialogues.We devised a particular type of HMM calledCSHMM, which processes operator/caller utter-ance sequences of multiple domains simulta-neously to model domain-specific utterance se-quences and common sequences at the same time.We trained a CSHMM using the transcripts ofsimulated contact center dialogues and verified itseffectiveness for the summarization of calls.There still remain several limitations in our ap-proach.
One is its inability to change the com-pression rate, which we aim to solve in the nextstep using the forward-backward algorithm (Ra-biner and Juang, 1986).
This algorithm can cal-culate the posterior probability of each state ateach time frame given an input dialogue sequence,enabling us to extract top-N domain-specific se-quences.
We also need to find the appropriatetopic number for the topic model.
In our imple-mentation, we used a tentative value of 100, whichmay not be appropriate.
In addition, we believethe topic model and the CSHMM can be unifiedbecause these models are fundamentally similar,especially when LDA is employed.
Model topolo-gies may also have to be reconsidered.
In ourCSHMM with concatenated training, the states indomain-specific SHMMs are only connected tothe common states, which may be inappropriatebecause there could be a case where a domainchanges from one to another without having acommon sequence.
ApplyingCSHMMs to speechand other NLP tasks is another challenge.
As anear-term goal, we aim to apply our method to thesummarization of meetings, where we will need toextend our CSHMMs to deal with more than twoparticipants.
Finally, we also want to build a con-tact center dialogue agent by extending the CSH-MMs to partially observableMarkov decision pro-cesses (POMDPs) (Williams and Young, 2007) byfollowing the recent work on building POMDPsfrom dialogue data in the dynamic Bayesian net-work (DBN) framework (Minami et al, 2009).AcknowledgmentsWe thank the members of the Spoken DialogSystem Group, especially Noboru Miyazaki andSatoshi Kobashikawa, for their effort in dialoguedata collection.407ReferencesBarzilay, Regina and Lillian Lee.
2004.
Catching the drift:Probabilistic content models, with applications to gener-ation and summarization.
In Proceedings of the HumanLanguage Technology Conference of the North AmericanChapter of the Association for Computational Linguistics(HLT-NAACL), pages 113?120.Byrd, Roy J., Mary S. Neff, Wilfried Teiken, YoungjaPark, Keh-Shin F. Cheng, Stephen C. Gates, and KarthikVisweswariah.
2008.
Semi-automated logging of contactcenter telephone calls.
In Proceeding of the 17th ACMconference on Information and knowledge management(CIKM), pages 133?142.Chu-Carroll, Jennifer and Bob Carpenter.
1999.
Vector-based natural language call routing.
Computational Lin-guistics, 25(3):361?388.Gillick, Dan and Benoit Favre.
2009.
A scalable globalmodel for summarization.
In Proceedings of the Work-shop on Integer Linear Programming for Natural Lan-guage Processing, pages 10?18.Gorin, Allen L., Giuseppe Riccardi, and Jerry H. Wright.1997.
How may I help you?
Speech Communication,23(1-2):113?127.Hori, Chiori and Sadaoki Furui.
2003.
A new approach toautomatic speech summarization.
IEEE Transactions onMultimedia, 5(3):368?378.Kupiec, Julian, Jan Pedersen, and Francine Chen.
1995.
Atrainable document summarizer.
In Proceedings of the18th annual international ACM SIGIR conference on Re-search and development in information retrieval (SIGIR),pages 68?73.Lee, Kai-Fu.
1989.
Automatic speech recognition: the de-velopment of the SPHINX system.
Kluwer Academic Pub-lishers.Lin, Chin-Yew and Eduard Hovy.
2003.
Automatic evalua-tion of summaries using n-gram co-occurrence statistics.In Proceedings of the 2003Conference of the North Amer-ican Chapter of the Association for Computational Lin-guistics on Human Language Technology (NAACL-HLT),pages 71?78.Liu, Feifan and Yang Liu.
2008.
Correlation betweenROUGE and human evaluation of extractive meeting sum-maries.
In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics on HumanLanguage Technologies (HLT), pages 201?204.Mani, Inderjeet.
2001.
Automatic summarization.
JohnBenjamins Publishing Company.Meguro, Toyomi, Ryuichiro Higashinaka, Kohji Dohsaka,Yasuhiro Minami, and Hideki Isozaki.
2009.
Analysis oflistening-oriented dialogue for building listening agents.In Proceedings of the SIGDIAL 2009 conference, pages124?127.Minami, Yasuhiro, Akira Mori, Toyomi Meguro, RyuichiroHigashinaka, Kohji Dohsaka, and Eisaku Maeda.
2009.Dialogue control algorithm for ambient intelligence basedon partially observable Markov decision processes.
InProceedings of the 1st international workshop on spokendialogue systems technology (IWSDS), pages 254?263.Murray, Gabriel, Steve Renals, and Jean Carletta.
2005.
Ex-tractive summarization of meeting recordings.
In Pro-ceedings of the 9th European Conference on SpeechCommunication and Technology (EUROSPEECH), pages593?596.Osborne, Miles.
2002.
Using maximum entropy for sen-tence extraction.
In Proceedings of the ACL-02 Workshopon Automatic Summarization, pages 1?8.Rabiner, Lawrence R. and Biing-Hwang Juang.
1986.
Anintroduction to hiddenMarkov models.
IEEE ASSP Mag-azine, 3(1):4?16.Rabiner, Lawrence R. 1990.
A tutorial on hidden Markovmodels and selected applications in speech recognition.Readings in speech recognition, 53(3):267?296.Radev, Dragomir R., Hongyan Jing, Ma?gorzata Stys?, andDaniel Tam.
2004.
Centroid-based summarization ofmultiple documents.
Information Processing & Manage-ment, 40(6):919?938.Reynolds, Douglas A., Thomas F. Quatieri, and Robert B.Dunn.
2000.
Speaker verification using adaptedGaussianmixture models.
Digital Signal Processing, 10(1-3):19 ?41.Subramaniam, L. Venkata, Tanveer A. Faruquie, Shajith Ik-bal, Shantanu Godbole, and Mukesh K. Mohania.
2009.Business intelligence from voice of customer.
In Pro-ceedings of the 2009 IEEE International Conference onData Engineering (ICDE), pages 1391?1402.Takeuchi, Hironori, L Venkata Subramaniam, Tetsuya Na-sukawa, Shourya Roy, and Sreeram Balakrishnan.
2007.A conversation-mining system for gathering insights toimprove agent productivity.
In Proceedings of the IEEEInternational Conference on E-Commerce Technologyand the IEEE International Conference on EnterpriseComputing, E-Commerce, and E-Services, pages 465?468.Tam, Yik-Cheung and Tanja Schultz.
2005.
Dynamiclanguage model adaptation using variational Bayes in-ference.
In Proceedings of the 9th European Confer-ence on Speech Communication and Technology (EU-ROSPEECH), pages 5?8.S?ingliar, Tomas and Milos Hauskrecht.
2006.
Noisy-ORcomponent analysis and its application to link analy-sis.
The Journal of Machine Learning Research, 7:2189?2213.Williams, JasonD.
and Steve Young.
2007.
Partially observ-able Markov decision processes for spoken dialog sys-tems.
Computer Speech & Language, 21(2):393?422.408
