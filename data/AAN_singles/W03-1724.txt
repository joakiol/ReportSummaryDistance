Integrating Ngram Model and Case-based LearningFor Chinese Word SegmentationChunyu Kit Zhiming Xu Jonathan J. WebsterDepartment of Chinese, Translation and LinguisticsCity University of Hong KongTat Chee Ave., Kowloon, Hong Kong{ctckit, ctxuzm, ctjjw}@cityu.edu.hkAbstractThis paper presents our recent workfor participation in the First Interna-tional Chinese Word Segmentation Bake-off (ICWSB-1).
It is based on a general-purpose ngram model for word segmen-tation and a case-based learning approachto disambiguation.
This system excelsin identifying in-vocabulary (IV) words,achieving a recall of around 96-98%.Here we present our strategies for lan-guage model training and disambiguationrule learning, analyze the system?s perfor-mance, and discuss areas for further im-provement, e.g., out-of-vocabulary (OOV)word discovery.1 IntroductionAfter about two decades of studies of Chinese wordsegmentation, ICWSB-1 (henceforth, the bakeoff)is the first effort to put different approaches andsystems to the test and comparison on commondatasets.
We participated in the bakeoff with asegmentation system that is designed to integrate ageneral-purpose ngram model for probabilistic seg-mentation and a case- or example-based learningapproach (Kit et al, 2002) for disambiguation.The ngram model, with words extracted fromtraining corpora, is trained with the EM algorithm(Dempster et al, 1977) using unsegmented train-ing corpora.
Originally it was developed to en-hance word segmentation accuracy so as to facili-tate Chinese-English word alignment for our ongo-ing EBMT project, where only unsegmented textsare available for training.
It is expected to be ro-bust enough to handle novel texts, independent ofany segmented texts for training.
To simplify theEM training, we used the uni-gram model for thebakeoff and relied on the Viterbi algorithm (Viterbi,1967) for the most probable segmentation, instead ofattempting to exhaust all possible segmentations ofeach sentence for a complicated full version of EMtraining.The case-based learning works in a straightfor-ward way.
It first extracts case-based knowledge,as a set of context-dependent transformation rules,from the segmented training corpus, and then ap-plies them to ambiguous strings in a test corpus interms of the similarity of their contexts.
The simi-larity is empirically computed in terms of the lengthof relevant common affixes of context strings.The effectiveness of this integrated approach isverified by its outstanding performance on IV wordidentification.
Its IV recall rate, ranging from 96%to 98%, stands at the top or the next to the top in allclosed tests in which we have participated.
Unfortu-nately, its overall performance is not sustainable atthe same level, due to the lack of a module for OOVword detection.This paper is intended to present the implementa-tion of the system and analyze its performance andproblems, aiming at exploration of directions for fur-ther improvement.
The remaining sections are or-ganized as follows.
Section 2 presents the ngrammodel and its training with the EM algorithm, andSection 3 presents the case-based learning for dis-ambiguation.
The overall architecture of our systemis given in Section 4, and its performance and prob-lems are analyzed in Section 5.
Section 6 concludesthe paper and previews future work.2 Ngram model and trainingAn ngram model can be utilized to find the mostprobable segmentation of a sentence.
Given a Chi-nese sentence s = c1c2 ?
?
?
cm (also denoted as cn1 ),its probabilistic segmentation into a word sequencew1w2 ?
?
?wk (also denoted as wk1 ) with the aid of anngram model can be formulated asseg(s) = arg maxs= w1?w2????
?wkk?ip(wi|wi?1i?n+1) (1)where ?
denotes string concatenation, wi?1i?n+1 thecontext (or history) of wi, and n is the order of thengram model in use.
We have opted for uni-gram forthe sake of simplicity.
Accordingly, p(wi|wi?1i?n+1)in (1) becomes p(wi), which is commonly estimatedas follows, given a corpus C for training.p(wi) .= f(wi)/?w?Cf(w) (2)In order to estimate a reliable p(wi), the ngrammodel needs to be trained with the EM algorithmusing the available training corpus.
Each EM itera-tion aims at approaching to a more reliable f(w) forestimating p(w), as follows:fk+1(w) =?s?C?s??S(s)pk(s?)
fk(w ?
s?)
(3)where k denotes the current iteration, S(s) the set ofall possible segmentations for s, and f k(w ?
s?)
theoccurrences of w in a particular segmentation s?.However, assuming that every sentence alwayshas a segmentation, the following equation holds:?s??S(s)pk(s?)
= 1 (4)Accordingly, we can adjust (3) as (5) with a normal-ization factor ?
= ?s?
?S(s) pk(s?
), to avoid favor-ing words in shorter sentences too much.
In general,shorter sentences have higher probabilities.fk+1(w) =?s?C?s??S(s)pk(s?)?
fk(w ?
s?)
(5)Following the conventional idea to speed up theEM training, we turned to the Viterbi algorithm.
Theunderlying philosophy is to distribute more prob-ability to more probable events.
The Viterbi seg-mentation, by utilizing dynamic programming tech-niques to go through the word trellis of a sentenceefficiently, finds the most probable segmentation un-der the current parameter estimation of the languagemodel, fulfilling (1)).
Accordingly, (6) becomesfk+1(w) =?s?Cpk(seg(s)) fk(w ?
seg(s)) (6)and (5) becomesfk+1(w) =?s?Cfk(w ?
seg(s)) (7)where the normalization factor is skipped, foronly the Viterbi segmentation is used for EM re-estimation.
Equation (7) makes the EM trainingwith the Viterbi algorithm very simple for the uni-gram model: iterate word segmentation, as (1), andword count updating, via (7), sentence by sentencethrough the training corpus until there is a conver-gence.Since the EM algorithm converges to a local max-ima only, it is critical to start the training with aninitial f 0(w) for each word not too far away from its?true?
value.
Our strategy for initializing f 0(w) isto assume all possible words in the training corpusas equiprobable and count each of them as 1; andthen p0(w) is derived using (2).
This strategy is sup-posed to have a weaker bias to favor longer wordsthan maximal matching segmentation.For the bakeoff, the ngram model is trained withthe unsegmented training corpora together with thetest sets.
It is a kind of unsupervised training.Adding the test set to the training data is reasonable,to allow the model to have necessary adaptation to-wards the test sets.
Experiments show that the train-ing converges very fast, and the segmentation per-formance improves significantly from iteration to it-eration.
For the bakeoff experiments, we carried outthe training in 6 iterations, because more iterationsthan this have not been observed to bring any signif-icant improvement on segmentation accuracy to thetraining sets.3 Case-based learning for disambiguationNo matter how well the language model is trained,probabilistic segmentation cannot avoid mistakes onambiguous strings, although it resolves most ambi-guities by virtue of probability.
For the remainingunresolved ambiguities, however, we have to resortto other strategies and/or resources.
Our recent study(Kit et al, 2002) shows that case-based learning isan effective approach to disambiguation.The basic idea behind the case-based learning isto utilize existing resolutions for known ambiguousstrings to do disambiguation if similar ambiguitiesoccur again.
This learning strategy can be imple-mented in two straightforward steps:1.
Collection of correct answers from the train-ing corpus for ambiguous strings together withtheir contexts, resulting in a set of context-dependent transformation rules;2.
Application of appropriate rules to ambiguousstrings.A transformation rule of this type is actually an ex-ample of segmentation, indicating how an ambigu-ous string is segmented within a particular context.It has the following general form:C l?
Cr : ?
?
w1 w2 ?
?
?wkwhere ?
is the ambiguous string, C l and Cr its leftand right contexts, respectively, and w1 w2 ?
?
?wkthe correct segmentation of ?
given the contexts.In our implementation, we set the context length oneach side to two words.For a particular ambiguity, the example with themost similar context in the example (or, rule) baseis applied.
The similarity is measured by the sumof the length of the common suffix and prefix of,respectively, the left and right contexts.
The detailsof computing this similarity can be found in (Kit etal., 2002) .
If no rule is applicable, its probabilisticsegmentation is retained.For the bakeoff, we have based our approach toambiguity detection and disambiguation rule extrac-tion on the assumption that only ambiguous stringscause mistakes: we detect the discrepancies of ourprobabilistic segmentation and the standard segmen-tation of the training corpus, and turn them intotransformation rules.
An advantage of this approachis that the rules so derived carry out not only disam-biguation but also error correction.
This links ourdisambiguation strategy to the application of Brill?s(1993) transformation-based error-driven learning toChinese word segmentation (Palmer, 1997; Hocken-maier and Brew, 1998).4 System architectureThe overall architecture of our word segmentationsystem is presented in Figure 1.Figure 1: Overall architecture of the system5 Performance and analysisThe performance of our system in the bakeoff is pre-sented in Table 1 in terms of precision (P), recall(R) and F score in percentages, where ?c?
denotesclosed tests.
Its IV word identification performanceis remarkable.However, its overall performance is not in bal-ance with this, due to the lack of a module for OOVword discovery.
It only gets a small number of OOVwords correct by chance.
The higher OOV propor-tion in the test set, the worse is its F score.
The rel-atively high Roov for PKc track is, mostly, the resultof number recognition with regular expressions.Test P R F OOV Roov RivSAc 95.2 93.1 94.2 02.2 04.3 97.2CTBc 80.0 67.4 73.2 18.1 07.6 95.9PKc 92.3 86.7 89.4 06.9 15.9 98.0Table 1: System performance, in percentages (%)5.1 Error analysisMost errors on IV words are due to the side-effectof the context-dependent transformation rules.
Therules resolve most remaining ambiguities and cor-rect many errors, but at the same time they also cor-rupt some proper segmentations.
This side-effect ismost likely to occur when there is inadequate con-text information to decide which rules to apply.There are two strategies to remedy, or at least al-leviate, this side-effect: (1) retrain probabilistic seg-mentation ?
a conservative strategy; or, (2) incorpo-rate Brill?s error-driven learning with several roundsof transformation rule extraction and application, al-lowing mistakes caused by some rules in previousrounds to be corrected by other rules in later rounds.However, even worse than the above side-effect isa bug in our disambiguation module: it always ap-plies the first available rule, leading to many unex-pected errors, each of which may result in more thanone erroneous word.
For instance, among 430 er-rors made by the system in the SA closed test, some70 are due to this bug.
A number of representativeexamples of these errors are presented in Table 2,together with some false errors resulting from theinconsistency in the standard segmentation.Errors Standard False errors Standard2?
(8) 2 ?
??
?D ?
?
?D?
u (7) ?u tjs? tj s?.
?
(7) .?
????
??
?
?_ A (5) _A P_,?
P_ ,??
 (4) ? 1w??
1w?
?n?
(4) n ?
.???
.
?
?
?Table 2: Errors and false errors6 Conclusion and future workWe have presented our recent work for partici-pation in ICWSB-1 based on a general-purposengram model for probabilistic word segmentationand a case-based learning strategy for disambigua-tion.
The ngram model is trained using availableunsegmented texts with the EM algorithm with theaid of Viterbi segmentation.
The learning strategyacquires a set of context-dependent transformationrules to correct mistakes in the probabilistic segmen-tation of ambiguous substrings.
This integrated ap-proach demonstrates an impressive effectiveness byits outstanding performance on IV word identifica-tion.
With elimination of the bug and false errors, itsperformance could be significantly better.6.1 Future workThe above problem analysis points to two main di-rections for improvement in our future work: (1)OOV word detection; (2) a better strategy for learn-ing and applying transformation rules to reduce theside-effect.
In addition, we are also interested instudying the effectiveness of higher-order ngrammodels and variants of EM training for Chineseword segmentation.AcknowledgementsThe work is part of the CERG project ?EBMT forHK Legal Texts?
funded by HK UGC under thegrant #9040482, with Jonathan J. Webster as theprincipal investigator and Chunyu Kit, Caesar S.Lun, Haihua Pan, King Kuai Sin and Vincent Wongas investigators.
The authors wish to thank all teammembers for their contribution to this paper.ReferencesE.
Brill.
1993.
A Corpus-Based Approach to LanguageLearning.
Ph.D. thesis, University of Pennsylvania,Philadelphia, PA.A.
P. Dempster, N. M. Laird, and D. B.Rubin.
1977.Maximum likelihood from incomplete data via the emalgorithm.
Journal of the Royal Statistical Society, Se-ries B, 34:1?38.J.
Hockenmaier and C. Brew.
1998.
Error-driven learn-ing of Chinese word segmentation.
In PACLIC-12,pages 218?229, Singapore.
Chinese and Oriental Lan-guages Processing Society.C.
Kit, H. Pan, and H. Chen.
2002.
Learning case-basedknowledge for disambiguating Chinese word segmen-tation: A preliminary study.
In COLING2002 work-shop: SIGHAN-1, pages 33?39, Taipei.D.
Palmer.
1997.
A trainable rule-based algorithmfor word segmentation.
In ACL-97, pages 321?328,Madrid.A.
J. Viterbi.
1967.
Error bounds for convolutional codesand an asymptotically optimum decoding algorithm.IEEE Transactions on Information Theory, IT-13:260?267.
