Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 55?63,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsMultilingual Summarization: Dimensionality Reduction and a StepTowards Optimal Term CoverageJohn M. ConroyIDA / Center for Computing Sciencesconroy@super.orgSashka T. DavisIDA / Center for Computing Sciencesstdavi3@super.orgJeff KubinaDepartment of Defensejmkubin@tycho.ncsc.milYi-Kai LiuNational Institute of Standards and Technologyyi-kai.liu@nist.govDianne P. O?LearyUniversity of Maryland and NISToleary@cs.umd.eduJudith D. SchlesingerIDA/Center for Computing Sciencesdrj1945@gmail.comAbstractIn this paper we present three term weight-ing approaches for multi-lingual docu-ment summarization and give results onthe DUC 2002 data as well as on the2013 Multilingual Wikipedia feature arti-cles data set.
We introduce a new interval-bounded nonnegative matrix factorization.We use this new method, latent semanticanalysis (LSA), and latent Dirichlet al-cation (LDA) to give three term-weightingmethods for multi-document multi-lingualsummarization.
Results on DUC and TACdata, as well as on the MultiLing 2013data, demonstrate that these methods arevery promising, since they achieve ora-cle coverage scores in the range of hu-mans for 6 of the 10 test languages.
Fi-nally, we present three term weighting ap-proaches for the MultiLing13 single docu-ment summarization task on the Wikipediafeatured articles.
Our submissions signifi-cantly outperformed the baseline in 19 outof 41 languages.1 Our Approach to Single andMulti-Document SummarizationThe past 20 years of research have yielded abounty of successful methods for single docu-ment summarization (SDS) and multi-documentsummarization (MDS).
Techniques from statistics,machine learning, numerical optimization, graphtheory, and combinatorics are generally language-independent and have been applied both to singleand multi-document extractive summarization ofmulti-lingual data.In this paper we extend the work of our re-search group, most recently discussed in Davis etal.
(2012) for multi-document summarization, andapply it to both single and multi-document multi-lingual document summarization.
Our extractivemulti-document summarization performs the fol-lowing steps:1.
Sentence boundary detection;2.
Tokenization and term identification;3.
Term-sentence matrix generation;4.
Term weight determination;5.
Sentence selection;6.
Sentence ordering.Sentence boundary detection and tokenization arelanguage dependent, while steps (3)-(6) are lan-guage independent.
We briefly discuss each ofthese steps.We use a rule based sentence splitter FASST-E (very Fast, very Accurate Sentence Splitter forText ?
English) (Conroy et al 2009) and its multi-lingual extensions (Conroy et al 2011) for deter-mining the boundary of individual sentences.Proper tokenization improves the quality ofthe summary and may include stemming andalso morphological analysis to disambiguate com-pound words in languages such as Arabic.
To-kenization may also include stop word removal.The result of this step is that each sentence is rep-resented as a sequence of terms, where a term canbe a single word, a sequence of words, or char-acter n-grams.
The specifics of tokenization arediscussed in Section 2.Matrix generation (the vector space model) waspioneered by Salton (1991).
Later Dumais (1994)introduced dimensionality reduction in documentretrieval systems, and this approach has also been55used by many researchers for document summa-rization.
(In addition to our own work, see, forexample Steinberger and Jezek (2009).)
We con-struct a single term-sentence matrix A = (ai,j),where i = 1, .
.
.
,m ranges over all terms, andj = 1, .
.
.
, n ranges over all sentences, for ei-ther a single document, when we perform SDS, orfor a collection of documents for MDS.
The rowlabels of the term-sentence matrix are the termsT = (t1, .
.
.
, tm) determined after tokenization.The column labels are the sentences S1, .
.
.
, Sn ofthe document(s).
The entries of the matrix A aredefined byai,j = `i,jgi,Here, `i,j is the local weight, which is 1 when termi appears in sentence j and 0 otherwise.The global weight gi should be proportional tothe contribution of the term in describing the majorthemes of the document.
While the global weightcould be used as a term weight in a sentence se-lection scheme, it may be beneficial to perform di-mensionality reduction on the matrix A and com-pute term weights based on the lower dimensionalmatrix.
In this work we seek to find strong termweights for both single- and and multi-documentsummarization.
These cases are handled sepa-rately, as we found that multi-document summa-rization benefits a lot from dimensionality reduc-tion while single document summarization doesnot.Our previous multi-document summarizationalgorithm, OCCAMS (Davis et al 2012), usedthe linear algebraic technique of Latent Seman-tic Analysis (LSA) to determine term weights andused techniques from combinatorial optimizationfor sentence selection.
In our CLASSY algorithms(e.g., (Conroy et al 2011)), we used both a lan-guage model and machine learning as two alterna-tive approaches to assign term weights.
CLASSYthen used linear algebraic techniques or an inte-ger linear program for sentence selection.
Sec-tion 3 describes the term weights we use whenwe summarize single documents.
In Section 4we present three different dimensionality reduc-tion techniques for the term-sentence matrix A.Once term weight learning has assigned weightsfor each term of the document(s) and dimension-ality reduction has been applied (if desired), thenext step, sentence selection, chooses a set of sen-tences of maximal length L for the extract sum-mary.
These sentences should cover the majorthemes of the document(s), minimize redundancy,and satisfy the bound on the length of the sum-mary.
We discuss our OCCAMS V sentence se-lection algorithm in Section 5.Sentence ordering is performed using an ap-proximate traveling salesperson algorithm (Con-roy et al 2009).Three term weighting variants were used to gen-erate summaries for each of the 10 languages inthe MultiLing 2013 multi-document summariza-tion task.
The target summary length was set to be250 words for all languages except Chinese, where700 characters were generated.We now present the details of our improvementsto our algorithms and results of our experiments.2 From Text to Term-Sentence MatrixAfter sentence boundaries are determined, weused one of three simple tokenization methodsand then one of two term-creation methods, assummarized in Table 1.
Languages were dividedinto three categories: English, non-English lan-guages with space delimited words, and ideo-graphic languages (Chinese for MDS and Chi-nese, Japanese, and Thai for the SDS pilot task).For non-ideographic languages, tokens are formedbased on a regular expression.
For English, to-kens are defined as contiguous sequences of upperor lower case letters and numbers.
For other non-ideographic languages, tokens were defined sim-ilarly, and the regular expression describes whatcharacters are used to break the tokens.
Thesecharacters include white space and most punctu-ation except the apostrophe.
For English, Porterstemming was used for both SDS and MDS, with astop list of approximately 600 words for SDS.
ForEnglish and other word-based languages, lower-cased bi-tokens were used in MDS and lower-cased tokens for SDS.
For all languages, and bothSDS and MDS, Dunning?s mutual informationstatistic (Dunning, 1993) is used to select terms,using the other documents as background.
The p-value (rejection threshold), initially set at 5.0e-4,is repeatedly doubled until the number of termsis at least twice the length of the target summary(250 for MDS, 150 words or 500 characters forSDS).
Note that these terms are high confidencesignature terms (Lin and Hovy, 2000) i.e., the p-value is small.
We describe our terms as high mu-tual information (HMI), since Dunning?s statisticis equivalent to mutual information as defined by56Language Tokens Terms for MDS Terms for SDSEnglish [?A-Za-z0-9] HMI bi-tokens HMI non-stop-wordtokensNon-English [\s.?,";:?!
[](){}<>&*=+@#$] HMI bi-tokens HMI tokensIdeographic 4-byte grams HMI tokens HMI tokensTable 1: Term and token definition as a function of language and task.Cover and Thomas (1991).3 Determining Term Weights for SingleDocument SummarizationFor SDS we consider three term weighting meth-ods.The first is global entropy as proposed by Du-mais et alfor information retrieval (Dumais,1994) (Rehder et al 1997) and by Steinberger andJezek for document summarization (Steinbergerand Jezek, 2009).
Global entropy weighting isgiven byw(GE)i = 1?
?j pi,j ?
log pi,jlog n,where n is the number of sentences, pi,j = tij/fi,tij is the number of times term i appears in sen-tence j, and fi is the total number of times term iappears in all sentences.1The second term weighting is simply the loga-rithm of frequency of the term in all the sentences:w(LF)i = 1 + log(fi).Log frequency is motivated by the fact that thesum of the term scores for a given sentence is(up to an affine transformation) the log probabil-ity of generating that sentence by sampling termsindependently at random, where the probabilityof each term is estimated by maximum likelihoodfrom the observed frequencies fi.The third method is a personalized variant ofTextRank, which was first proposed by Mihal-cea (2005) and motivated by PageRank (Page etal., 1999).
The personalized version smooths theMarkov chain used in TextRank (PageRank) withterm (page) preferences.
Previously, a sentencebased version of personalization has been usedfor summarization; see, for example, Zhao et al(2009).
Our current work may be the first use of1We make the usual convenient definition that pi log pi =0 when pi = 0.a term based personalized TextRank (TermRank),which we call PTR.
The personalization vector wechoose is simply the normalized frequency, andthe Markov chain is defined by the transition ma-trixM =12LLTD +12peTwherepi = fi/?i(fi),L is the incidence term-sentence matrix.
The el-ements of L are previously defined local weights,`ij .
The vector e is all 1?s and D is a diagonalmatrix chosen to make the column sums equal toone.
The estimated weight vector used by OC-CAMS V, w(PTR), is computed using 5 iterationsof the power method to approximate the station-ary vector of this matrix.
Note, there is no need toform the matrix M since the applications of M toa vector may be achieved by vector operations andmatrix-vector multiplies by L and LT .We test the performance of these three termweighting methods on two data sets: DUC 2002English single-document data and the WikipediaPilot at MultiLing 2013.3.1 Results for DUC 2002 DataThe DUC 2002 English single-document data con-tains 567 newswire documents for which there areone or two human-generated summaries.In addition to computing ROUGE-2 scores, wealso compute an oracle coverage score (Conroy etal., 2006).
At TAC 2011 (Conroy et al 2011)(Rankel et al 2012) bigram coverage was shownto be a useful feature for predicting the perfor-mance of automated summarization systems rela-tive to a human summarizer.
Oracle unigram cov-erage score is defined byC1(X) =?i?Tf1(i),where T is the set of terms and f1(i) is the frac-tion of humans who included the ith term in the57Term Weight ROUGE-2 C1 GroupPTR 0.194 19.1 1LF 0.192 18.7 2GE 0.190 18.6 2Table 2: ROUGE-2 and Coverage bi-grams Scoressummary.
More generally, we define Cn in similarway for n-gram oracle coverage scores.
Coveragescores differ from ROUGE scores since the scoreis not affected by the number of times that a givenhuman or machine-generated summary uses theterm, but only whether or not the term is includedin the machine summary and the estimated frac-tion of humans that would use this term.
We notethat this score can be modified to compute scoresfor human summarizers using the analogous jack-knife procedure employed by ROUGE.Table 2 gives a summary of the results.
Weran a Wilcoxon test to check for statistical dis-tinguishability in the performance of the differentterm-weighting methods.
Methods were placed inthe same group if they produced results in cover-age (C1) that were indistinguishable.
More pre-cisely, we used the null hypothesis that the differ-ence between the vector of scores for two methodshas median 0.
If the p-value of two consecutiveentries in the table was less than 0.05, the grouplabel was increased and is shown in the last col-umn.Log frequency (LF) and global entropy (GE)are correlated.
For the DUC 2002 data they per-form comparably.
Personalized term rank (PTR)weighting is statistically stronger than the othertwo approaches, as measured by the oracle termcoverage score.
For these data the definition ofterm for the purposes of the computation of theoracle coverage score is non-stop word stemmed(unigram) tokens.3.2 Results for the Wikipedia Pilot atMultiLing 2013This task involves single-document summariza-tion for 1200 Wikipedia feature articles: 30 doc-uments in each of 40 languages.
For each doc-ument, the organizers generated a baseline leadsummary consisting of the first portion of the fea-ture article following the ?hidden summary.?
Sum-mary lengths were approximately 150 words forall non-ideograph languages and 500 charactersfor the ideograph languages.
Sentences were or-dered in the order selected by OCCAMS V. Thus,sentences covering the largest number of relevantterms, as measured by the term-weighting scheme,will appear first.Results of this pilot study will be presented indetail in the overview workshop paper, but we notehere that, as measured by ROUGE-1, in 19 of the40 languages, at least one of our three submit-ted methods significantly outperformed the lead-summary baseline.4 Dimensionality ReductionThe goal of dimensionality reduction is to iden-tify the major factors of the term-sentence ma-trix A and to throw away those factors which are?irrelevant?
for summarization.
Here we surveythree algorithms: the well-known LSA, the morerecent latent Dirichlet alcation (LDA), and thenew interval-bounded nonnegative matrix factor-ization.4.1 Latent Semantic AnalysisDavis et al(2012) successfully used an approx-imation to A, computed using the singular valuedecomposition (SVD) A = USV T .
They usedthe first 200 columns U200 of the singular vectormatrix U and the corresponding part of the singu-lar value matrix S. They eliminated negative en-tries in U200 by taking absolute values.
The termweights were computed as the L1 norm (sum ofthe entries) in the rows of W = |U200|S200.Our method is similar, except that we use 250columns and form them in a slightly different way.Observe that in the SVD, if ui is a column of Uand vTi is a row of V , then they can be replacedby ?ui and ?vTi .
This is true since if D is anydiagonal matrix with entries +1 and ?1, thenA = USV T = (UD)S(DV T ).Therefore, we propose choosingD so that the sumof the positive entries in each column of U is max-imized.
Then we form U?
by setting each negativeentry of UD to zero and form W = U?250S250.4.2 Latent Dirichlet AllocationWe use the term-sentence matrix to train a simplegenerative topic model based on LDA (Blei et al2003).
This model is described by the followingparameters: the number of terms m; the numberof topics k; a vector w representing a probabilitydistribution over topics; and an m?
k matrix A in58which each column represents a probability distri-bution over terms.In this model, sentences are generated inde-pendently.
We use the ?pure-topic?
LDA modeland assume, for simplicity, that the length of thesentence is fixed a priori.
First, a topic i ?
{1, .
.
.
, k} is chosen from the probability distribu-tion w. Then, terms are generated by sampling in-dependently from the distribution specified by theith column of the matrix A.We train this model using a recently-developedspectral algorithm based on third-order tensor de-compositions (Anandkumar et al 2012a; Anand-kumar et al 2012b).
This algorithm is guaranteedto recover the parameters of the LDA model, pro-vided that the columns of the matrixA are linearlyindependent.
For our experiments, we used a Mat-lab implementation from Hsu (2012).4.3 Interval Bounded Nonnegative MatrixFactorization (IBNMF)We also use a new method for dimensionalityreduction, a nonnegative matrix factorization al-gorithm that handles uncertainty in a new way(O?Leary and et al In preparation).Since the term-sentence matrix A is not knownwith certainty, let?s suppose that we are given up-per and lower bound matrices U and L so thatL ?
A ?
U .
We compute a sparse nonnega-tive low-rank approximation toA of the formXY ,where X is nonnegative (i.e., X ?
0) and hasr columns and Y is nonnegative and has r rows.This gives us an approximate nonnegative factor-ization of A of rank at most r.We choose to measure closeness of two ma-trices using the Frobenius norm-squared, where?Z?2F denotes the sum of the squares of the entriesof Z.
Since A is sparse, we also want X and Y tobe sparse.
We use the common trick of forcing thisby minimizing the sum of the entries of the matri-ces, denoted by sum(X) + sum(Y ).
This leadsus to determine X and Y by choosing a weightingconstant ?
and solvingminX,Y,Z?
?XY ?
Z?2F + sum(X) + sum(Y )subject to the constraintsL ?
Z ?
U,X ?
0,Y ?
0.We simplify this problem by noting that for anyW = XY , the entries of the optimal Z arezij =??
?`ij , wij ?
`ij ,wij , `ij ?
wij ?
uij ,uij , uij ?
wij .We solve our minimization problem by an alter-nating algorithm, iterating by fixing X and deter-mining the optimal Y and then fixing Y and de-termining the optimal X .
Either non-negativity isimposed during the solution to the subproblems,making each step more expensive, or negative en-tries of the updated matrices are set to zero, ruin-ing theoretical convergence properties but yieldinga more practical algorithm.
Each iteration reducesthe distance to the term matrix, but setting nega-tive values to zero increases it again.For our summarization system we chose r = 50and ?
= 1000.
We scaled the rows of the matrixusing global entropy weights and used L = 0.9Aand U = 1.1A.4.4 Term Weighting and Dimension Choicefor Multi-Document SummarizationA natural term weighting can be obtained by com-puting the row sums of the dimension-reducedapproximation to the term-sentence matrix.
ForLSA, the resulting term weights are the sum ofthe entries in the rows of W = U?250S250.
Forthe LDA method the initial matrix is the matrix ofcounts.
The model has three components similarto that of the SVD in LSA, and the term weightsare computed analogously.
For IBNMF, the termweights are the sum of the entries in the rows ofthe optimal XY .Each of the three dimensionality reductionmethods require us to specify the dimension of the?topic space.?
We explored this question using theDUC 2005-2007 and the TAC 2011 data.
Tables 3,4, 5, and 6 give the average ROUGE-2, ROUGE-4, and bi-gram coverage scores, with confidenceintervals, for the dimension that gave the best cov-erage.
The optimal ranks were 250 for LSA, 5 forLDA, and 50 for IBNMF.
We emphasize the theseresults are very strong despite the fact that no useof the topic descriptions or the guided summaryaspects for the TAC 2010 and 2011 are used.
Thus,we treat these data as if the task were to generatea generic summary, as is the case in the MultiLing2013 task.
22We note that some of the coverage (C2), and ROUGE-59System R2 R4 C2A 0.117 ( 0.106,0.129) 0.016 ( 0.011, 0.021) 26.333 (23.849,28.962)C 0.118 ( 0.105,0.131) 0.016 ( 0.012, 0.022) 25.882 (23.086,28.710)E 0.105 ( 0.092,0.120) 0.016 ( 0.010, 0.022) 23.625 (18.938,28.573)F 0.100 ( 0.089,0.111) 0.014 ( 0.010, 0.019) 23.500 (19.319,27.806)B 0.100 ( 0.086,0.115) 0.013 ( 0.008, 0.019) 23.118 (20.129,26.285)D 0.100 ( 0.089,0.113) 0.012 ( 0.007, 0.017) 22.957 (20.387,25.742)I 0.099 ( 0.085,0.116) 0.010 ( 0.007, 0.014) 21.806 (17.722,26.250)H 0.088 ( 0.077,0.101) 0.011 ( 0.007, 0.016) 20.972 (17.389,24.750)J 0.100 ( 0.090,0.111) 0.010 ( 0.007, 0.013) 20.472 (17.167,24.389)G 0.097 ( 0.085,0.108) 0.012 ( 0.008, 0.017) 20.111 (16.694,24.000)LSA250 0.085 ( 0.076,0.093) 0.008 ( 0.006, 0.009) 17.950 (17.072,18.838)IBNMF50 0.079 ( 0.068,0.089) 0.007 ( 0.005, 0.009) 17.730 (16.843,18.614)LDA5 0.077 ( 0.074,0.080) 0.008 ( 0.007, 0.009) 17.165 (16.320,18.024)Table 3: DUC 2005System R2 R4 C2C 0.133 ( 0.116,0.152) 0.025 ( 0.018, 0.033) 30.517 (26.750,34.908)D 0.124 ( 0.108,0.140) 0.017 ( 0.011, 0.023) 27.283 (23.567,31.050)B 0.118 ( 0.105,0.134) 0.015 ( 0.012, 0.020) 25.933 (23.333,29.033)G 0.113 ( 0.102,0.124) 0.016 ( 0.011, 0.022) 25.717 (23.342,28.017)H 0.108 ( 0.098,0.117) 0.013 ( 0.010, 0.016) 24.767 (22.433,27.067)F 0.109 ( 0.093,0.128) 0.016 ( 0.010, 0.023) 24.183 (20.650,28.292)I 0.106 ( 0.096,0.116) 0.012 ( 0.008, 0.015) 24.133 (22.133,26.283)J 0.107 ( 0.093,0.125) 0.015 ( 0.010, 0.022) 23.933 (20.908,27.233)A 0.104 ( 0.093,0.116) 0.015 ( 0.010, 0.022) 23.283 (20.483,26.283)E 0.104 ( 0.089,0.119) 0.014 ( 0.010, 0.020) 22.950 (19.833,26.450)LDA5 0.103 ( 0.099,0.107) 0.012 ( 0.011, 0.013) 22.620 (21.772,23.450)IBNMF50 0.095 ( 0.091,0.099) 0.010 ( 0.009, 0.011) 22.400 (21.615,23.177)LSA250 0.099 ( 0.096,0.103) 0.012 ( 0.011, 0.013) 22.335 (21.497,23.200)Table 4: DUC 2006System R2 R4 C2D 0.175 ( 0.157,0.196) 0.038 ( 0.029, 0.050) 39.481 (34.907,44.546)C 0.151 ( 0.134,0.169) 0.035 ( 0.024, 0.049) 34.148 (29.870,38.926)E 0.139 ( 0.125,0.154) 0.025 ( 0.020, 0.030) 30.907 (27.426,34.574)J 0.139 ( 0.120,0.160) 0.028 ( 0.019, 0.038) 30.759 (25.593,36.389)B 0.140 ( 0.116,0.163) 0.027 ( 0.019, 0.036) 30.537 (25.815,35.537)I 0.136 ( 0.113,0.159) 0.022 ( 0.014, 0.030) 30.537 (25.806,35.241)G 0.134 ( 0.118,0.150) 0.027 ( 0.018, 0.035) 30.259 (26.509,33.926)F 0.134 ( 0.120,0.149) 0.024 ( 0.017, 0.033) 29.944 (26.481,33.870)A 0.133 ( 0.117,0.149) 0.024 ( 0.016, 0.033) 29.315 (25.685,33.093)H 0.130 ( 0.117,0.143) 0.020 ( 0.015, 0.027) 28.815 (25.537,32.185)IBNMF50 0.140 ( 0.122,0.158) 0.023 ( 0.017, 0.031) 28.350 (27.092,29.567)LSA250 0.125 ( 0.120,0.130) 0.022 ( 0.020, 0.024) 28.144 (26.893,29.344)LDA5 0.124 ( 0.118,0.129) 0.021 ( 0.019, 0.023) 27.722 (26.556,28.893)Table 5: DUC 200760System R2 R4 C2IBNMF50 0.132 ( 0.124,0.140) 0.033 ( 0.029, 0.038) 12.585 (11.806,13.402)D 0.128 ( 0.110,0.146) 0.024 ( 0.017, 0.032) 12.212 (10.394,14.045)LSA250 0.128 ( 0.120,0.136) 0.030 ( 0.025, 0.034) 12.210 (11.441,12.975)A 0.119 ( 0.099,0.138) 0.024 ( 0.016, 0.033) 11.591 ( 9.758,13.455)LDA5 0.120 ( 0.112,0.128) 0.028 ( 0.024, 0.033) 11.409 (10.678,12.159)E 0.118 ( 0.099,0.138) 0.025 ( 0.016, 0.035) 11.288 ( 9.409,13.258)H 0.115 ( 0.097,0.132) 0.020 ( 0.014, 0.027) 11.212 ( 9.439,12.955)B 0.111 ( 0.099,0.125) 0.018 ( 0.013, 0.023) 10.591 ( 9.379,11.864)F 0.109 ( 0.090,0.128) 0.017 ( 0.010, 0.025) 10.530 ( 8.515,12.500)C 0.110 ( 0.095,0.126) 0.015 ( 0.010, 0.021) 10.379 ( 8.939,11.924)G 0.110 ( 0.092,0.127) 0.016 ( 0.010, 0.023) 10.258 ( 8.682,11.894)Table 6: TAC 20115 Sentence SelectionOur sentence selection algorithm, OCCAMS V,is an extension of the one used in (Davis et al2012), which uses the (1 ?
e?1/2)-approximationscheme for the Budgeted Maximal Coverage(BMC) problem and the Dynamic Programmingbased FPTAS for the knapsack problem.Algorithm OCCAMS V (T, D,W, c, L)1.
K1 = Greedy BMC(T,D,W, c, L)2.
K2 = Smax ?
Greedy BMC(T ?,D?,W, c?, L?
)),where Smax = argmax{Si?D}{?tj?Si w(tj)}and T ?,D?,W, c?, L?
represent quantities updatedby deleting sentence Smax from the collection.3.
K3 = KS(Greedy BMC(T,D,W, c, 5L), L);4.
K4 = KS(K ?4, L), whereK ?4 = Smax ?
Greedy BMC(T?,D?,W, C?, 5L?));5.
K = argmaxk=1,2,3,4{?T (Ki)w(ti)}where T (Ki) is the set of terms covered by Ki.This algorithm selects minimally overlappingsentences, thus reducing redundancy, while maxi-mizing term coverage.
The algorithm guaranteesa (1?
e?1/2) approximation ratio for BMC.We use the m terms T = {t1, .
.
.
, tm} andtheir corresponding weightsW = {w1, .
.
.
, wm}.We also use the n sentences D = {S1, .
.
.
, Sn},where each Si is the set of terms in the ith sen-tence, so that Si ?
T .
We define c to be a vec-tor whose components are the lengths of each sen-tence.
Our algorithm, OCCAMS V, determinesfour candidate sets of summary sentences and then2 scores reported in (Davis et al 2012), where a rank 200approximation and a large background corpus were used,are higher than the ones reported here, where a small self-background and a rank 250 approximation is used.chooses the one with maximal coverage weight.The first three candidate sets were used in the OC-CAMS algorithm (Davis et al 2012).
The setK1 is determined using the Greedy BMC heuris-tic of Khuller et al(1999) to maximize the sumof weights corresponding to terms in the sum-mary sentences.
The set K2 is determined thesame way, but the sentence that has the best sumof weights is forced to be included.
The thirdcandidate K3 is determined by applying a fullypolynomial-time approximation scheme (FPTAS)dynamic programming algorithm, denoted by KS,to the knapsack problem using sentences chosenby the Greedy BMC heuristic, asking for a lengthof 5L.
The fourth candidate K4 is similar, but thesentence with the best sum of weights is forced tobe included in the input to KS.OCCAMS V guarantees an approximation ratioof (1?
e?1/2) for the result because the quality ofthe solution chosen is no worse than the approxi-mation ratio achieved by the OCCAMS algorithm.6 Coverage Results for MultiLing 2013We defined a term oracle coverage score in Section3.1, an automatic summarization evaluation scorethat computes the expected number of n-gramsthat a summary will have in common with a hu-man summary selected at random, assuming thathumans select terms independently.
As reportedin (Davis et al 2012), the 2-gram oracle cover-age correlates as well with human evaluations ofEnglish summaries as ROUGE-2 does for Englishnewswire summaries.3 It is natural then to ask towhat extent oracle coverage scores can predict asummary?s quality for other languages.3Here a term is defined as a stemmed 2-gram token.61For each of the 10 MultiLing 2013 languageswe can tokenize and generate bigrams (or charac-ter n-grams for Chinese) for the human-generatedsummaries and the machine-generated summaries.Table 7 gives the average oracle term (bi-gram)coverage score (C2) for the lowest-scoring humanand for each of the dimensionality reduction algo-rithms described in Section 4.In all but four of the languages (Romanian,Hindi, Spanish, and Chinese), at least one of ourmethods scored higher than the lowest scoring hu-man.
As with the DUC and TAC testing, the LDAmethod of term-weighting was the weakest of thethree.
In fact, in eight of the languages one or bothof OCCAMS V(LSA) and OCCAMS V(IBNMF)(indicated in boldface in the table) scored signif-icantly higher than OCCAMS V(LDA) (p-value< 0.05 using a paired Wilcoxon test).The human coverage scores for three of the lan-guages (Romanian, Hindi, and Chinese) are sur-prisingly high.
Examining these data more closelyindicates that a large number of the summaries arenearly identical.
As an example, in one of the Ro-manian document sets, there were 266 bi-gramsin the union of the three summaries, and the sum-mary length was 250.
Document sets similar tothis are the major cause of the anomalously highscores for humans in these languages.Language Human LSA IBNMF LDAenglish 37 38 37 34arabic 22 29 28 23czech 22 34 35 33french 28 38 38 34greek 19 25 25 24hebrew 16 19 22 19hindi 64 20 20 18spanish 47 40 44 36romanian 118 31 28 29chinese 68 23 24 18Table 7: MultiLing 2013 Coverage ResultsHuman evaluation of the multi-lingual multi-document summaries is currently under way.These evaluations will be extremely informativeand will help measure to what extent ROUGE,coverage, and character n-gram based methodssuch as MeMoG (Giannakopoulos et al 2010),are effective in predicting performance.7 Conclusions and Future WorkIn this paper we presented three term weight-ing approaches for single document multi-lingualsummarization.
These approaches were tested onthe DUC 2002 data and on a submission to theMultiLing 2013 single document pilot task for all40 languages.
Automatic evaluation of these sum-maries with ROUGE-1 indicates that the strongestof the approaches significantly outperformed thelead baseline.
The Wikipedia feature articles posea challenge due to their variable summary size andgenre.
Further analysis of the results as well as hu-man evaluation of the submitted summaries woulddeepen our understanding.A new nonnegative matrix factorizationmethod, interval bounded nonnegative matrixfactorization (IBNMF), was used.
This methodallows specifying interval bounds, which givean intuitive way to express uncertainty in theterm-sentence matrix.For MDS we presented a variation of a LSAterm-weighting for OCCAMS V as well as noveluse of both of the IBNMF and an LDA model.Based on automatic evaluation using cover-age, it appears that the LSA method and theIBNMF term-weighting give rise to competitivesummaries with term coverage scores approach-ing that of humans for 6 of the 10 languages.
Theautomatic evaluation of these summaries, whichshould soon be finished, will be illuminating.Note: Contributions to this article by NIST, an agency of theUS government, are not subject to US copyright.
Any men-tion of commercial products is for information only, and doesnot imply recommendation or endorsement by NIST.ReferencesAnima Anandkumar, Dean P. Foster, Daniel Hsu, ShamKakade, and Yi-Kai Liu.
2012a.
A spectral al-gorithm for latent dirichlet alcation.
In Peter L.Bartlett, Fernando C. N. Pereira, Christopher J. C.Burges, Le?on Bottou, and Kilian Q. Weinberger, ed-itors, NIPS, pages 926?934.Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M.Kakade, and Matus Telgarsky.
2012b.
Tensor de-compositions for learning latent variable models.CoRR, abs/1210.7559.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
Journal of Ma-chine Learning Research, 3:993?1022.62John M. Conroy, Judith D. Schlesinger, and Dianne P.O?Leary.
2006.
Topic-focused multi-documentsummarization using an approximate oracle score.In Proceedings of the ACL?06/COLING?06 Confer-ences, pages 152?159, Sydney, Australia, July.John M Conroy, Judith D Schlesinger, and Dianne PO?leary.
2009.
Classy 2009: summarization andmetrics.
Proceedings of the text analysis conference(TAC).John M Conroy, Judith D Schlesinger, Jeff Kubina, Pe-ter A Rankel, and Dianne P O?Leary.
2011.
Classy2011 at tac: Guided and multi-lingual summariesand evaluation metrics.
Proceedings of the TextAnalysis Conference.Thomas M. Cover and Joy A. Thomas.
1991.
El-ements of information theory.
Wiley-Interscience,New York, NY, USA.Sashka T. Davis, John M. Conroy, and Judith D.Schlesinger.
2012.
Occams - an optimal combina-torial covering algorithm for multi-document sum-marization.
In Jilles Vreeken, Charles Ling, Mo-hammed Javeed Zaki, Arno Siebes, Jeffrey Xu Yu,Bart Goethals, Geoffrey I. Webb, and Xindong Wu,editors, ICDM Workshops, pages 454?463.
IEEEComputer Society.Susan T. Dumais.
1994.
Latent semantic indexing(lsi): Trec-3 report.
In TREC, pages 105?115.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.George Giannakopoulos, George A. Vouros, and Van-gelis Karkaletsis.
2010.
Mudos-ng: Multi-document summaries using n-gram graphs (tech re-port).
CoRR, abs/1012.2042.Daniel Hsu.
2012.
Estimating a simple topic model.http://cseweb.ucsd.edu/?djhsu/code/learn_topics.m.Samir Khuller, Anna Moss, and Joseph Naor.
1999.The Budgeted Maximum Coverage Problem.
Inf.Process.
Lett., 70(1):39?45.Chin-Yew Lin and Eduard Hovy.
2000.
The automatedacquisition of topic signatures for text summariza-tion.
In Proceedings of the 18th conference on Com-putational linguistics, pages 495?501, Morristown,NJ, USA.
Association for Computational Linguis-tics.Rada Mihalcea.
2005.
Language independent extrac-tive summarization.
In Proceedings of ACL 2005,Ann Arbor, MI, USA.Dianne P. O?Leary and et alIn preparation.
An inter-val bounded nonnegative matrix factorization.
Tech-nical report.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The pagerank citation rank-ing: Bringing order to the web.
Technical Report1999-66, Stanford InfoLab, November.
Previousnumber = SIDL-WP-1999-0120.Peter A. Rankel, John M. Conroy, and Judith D.Schlesinger.
2012.
Better metrics to automaticallypredict the quality of a text summary.
Algorithms,5(4):398?420.Bob Rehder, Michael L. Littman, Susan T. Dumais, andThomas K. Landauer.
1997.
Automatic 3-languagecross-language information retrieval with latent se-mantic indexing.
In TREC, pages 233?239.Gerard Salton.
1991.
The smart information retrievalsystem after 30 years - panel.
In Abraham Book-stein, Yves Chiaramella, Gerard Salton, and Vijay V.Raghavan, editors, SIGIR, pages 356?358.
ACM.Josef Steinberger and Karel Jezek.
2009.
Updatesummarization based on novel topic distribution.
InACM Symposium on Document Engineering, pages205?213.Lin Zhao, Lide Wu, and Xuanjing Huang.
2009.
Usingquery expansion in graph-based approach for query-focused multi-document summarization.
Informa-tion Processing & Management, 45(1):35 ?
41.63
