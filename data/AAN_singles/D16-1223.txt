Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2077?2083,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLeveraging Sentence-level Information with Encoder LSTMfor Semantic Slot FillingGakuto KurataIBM Researchgakuto@jp.ibm.comBing XiangIBM Watsonbingxia@us.ibm.comBowen ZhouIBM Watsonzhou@us.ibm.comMo YuIBM Watsonyum@us.ibm.comAbstractRecurrent Neural Network (RNN) and oneof its specific architectures, Long Short-TermMemory (LSTM), have been widely used forsequence labeling.
Explicitly modeling out-put label dependencies on top of RNN/LSTMis a widely-studied and effective extension.We propose another extension to incorpo-rate the global information spanning overthe whole input sequence.
The proposedmethod, encoder-labeler LSTM, first encodesthe whole input sequence into a fixed lengthvector with the encoder LSTM, and then usesthis encoded vector as the initial state of an-other LSTM for sequence labeling.
With thismethod, we can predict the label sequencewhile taking the whole input sequence in-formation into consideration.
In the experi-ments of a slot filling task, which is an es-sential component of natural language under-standing, with using the standard ATIS cor-pus, we achieved the state-of-the-art F1-scoreof 95.66%.1 IntroductionNatural language understanding (NLU) is an essen-tial component of natural human computer interac-tion and typically consists of identifying the intent ofthe users (intent classification) and extracting the as-sociated semantic slots (slot filling) (De Mori et al,2008).
We focus on the latter semantic slot fillingtask in this paper.Slot filling can be framed as a sequential label-ing problem in which the most probable semanticslot labels are estimated for each word of the givenword sequence.
Slot filling is a traditional task andtremendous efforts have been done, especially sincethe 1980s when the Defense Advanced ResearchProgram Agency (DARPA) Airline Travel Informa-tion System (ATIS) projects started (Price, 1990).Following the success of deep learning (Hinton etal., 2006; Bengio, 2009), Recurrent Neural Net-work (RNN) (Elman, 1990; Jordan, 1997) and oneof its specific architectures, Long Short-Term Mem-ory (LSTM) (Hochreiter and Schmidhuber, 1997),have been widely used since they can capture tem-poral dependencies (Yao et al, 2013; Yao et al,2014a; Mesnil et al, 2015).
The RNN/LSTM-basedslot filling has been extended to be combined withexplicit modeling of label dependencies (Yao et al,2014b; Liu and Lane, 2015).In this paper, we extend the LSTM-based slotfilling to consider sentence-level information.
Inthe field of machine translation, an encoder-decoderLSTM has been gaining attention (Sutskever et al,2014), where the encoder LSTM encodes the globalinformation spanning over the whole input sentencein its last hidden state.
Inspired by this idea, we pro-pose an encoder-labeler LSTM that leverages the en-coder LSTM for slot filling.
First, we encode the in-put sentence into a fixed length vector by the encoderLSTM.
Then, we predict the slot label sequence bythe labeler LSTM whose hidden state is initializedwith the encoded vector by the encoder LSTM.
Withthis encoder-labeler LSTM, we can predict the la-bel sequence while taking the sentence-level infor-mation into consideration.The main contributions of this paper are two-folds:20771.
Proposed an encoder-labeler LSTM to leveragesentence-level information for slot filling.2.
Achieved the state-of-the-art F1-score of95.66% in the slot filling task of the standardATIS corpus.2 Proposed MethodWe first revisit the LSTM for slot filling and enhancethis to explicitly model label dependencies.
Then weexplain the proposed encoder-labeler LSTM.2.1 LSTM for Slot FillingFigure 1(a) shows a typical LSTM for slot filling andwe call this as labeler LSTM(W)where words are fedto the LSTM (Yao et al, 2014a).Slot filling is a sequential labeling task to map asequence of T words xT1 to a sequence of T slotlabels yT1 .
Each word xt is represented with a Vdimensional one-hot-vector where V is the vocabu-lary size and is transferred to de dimensional con-tinuous space by the word embedding matrix E ?Rde?V as Ext.
Instead of simply feeding Ext intothe LSTM, Context Window is a widely used tech-nique to jointly consider k preceding and succeedingwords as Ext+kt?k ?
Rde(2k+1).
The LSTM has thearchitecture based on Jozefowicz et al (2015) thatdoes not have peephole connections and yields thehidden state sequence hT1 .
For each time step t, theposterior probabilities for each slot label are calcu-lated by the softmax layer over the hidden state ht.The word embedding matrix E, LSTM parameters,and softmax layer parameters are estimated to mini-mize the negative log likelihood over the correct la-bel sequences with Back-Propagation Through Time(BPTT) (Williams and Peng, 1990).2.2 Explicit Modeling of Label DependencyA shortcoming of the labeler LSTM(W) is that itdoes not consider label dependencies.
To explic-itly model label dependencies, we introduce a newarchitecture, labeler LSTM (W+L), as shown in Fig-ure 1(b), where the output label of previous time stepis fed to the hidden state of current time step, jointlywith words, as Mesnil et al (2015) and Liu and Lane(2015) tried with RNN.
For model training, one-hot-vector of ground truth label of previous time step isfed to the hidden state of current time step and forevaluation, left-to-right beam search is used.2.3 Encoder-labeler LSTM for Slot FillingWe propose two types of the encoder-labeler LSTMthat uses the labeler LSTM(W) and the labelerLSTM(W+L).
Figure 1(d) shows the encoder-labeler LSTM(W).
The encoder LSTM, to the leftof the dotted line, reads through the input sentencebackward.
Its last hidden state contains the en-coded information of the input sentence.
The la-beler LSTM(W), to the right of the dotted line, isthe same with the labeler LSTM(W) explained inSection 2.1, except that its hidden state is initializedwith the last hidden state of the encoder LSTM.
Thelabeler LSTM(W) predicts the slot label conditionedon the encoded information by the encoder LSTM,which means that slot filling is conducted with tak-ing sentence-level information into consideration.Figure 1(e) shows the encoder-labeler LSTM(W+L),which uses the labeler LSTM(W+L) and predictsthe slot label considering sentence-level informationand label dependencies jointly.Model training is basically the same as with thebaseline labeler LSTM(W), as shown in Section 2.1,except that the error in the labeler LSTM is propa-gated to the encoder LSTM with BPTT.This encoder-labeler LSTM is motivated by theencoder-decoder LSTM that has been applied to ma-chine translation (Sutskever et al, 2014), grapheme-to-phoneme conversion (Yao and Zweig, 2015), textsummarization (Nallapati et al, 2016) and so on.The difference is that the proposed encoder-labelerLSTM accepts the same input sequence twice whilethe usual encoder-decoder LSTM accepts the in-put sequence once in the encoder.
Note that theLSTMs for encoding and labeling are different in theencoder-labeler LSTM, but the same word embed-ding matrix is used both for the encoder and labelersince the same input sequence is fed twice.2.4 Related Work on ConsideringSentence-level InformationBi-directional RNN/LSTM have been proposed tocapture sentence-level information (Mesnil et al,2015; Zhou and Xu, 2015; Vu et al, 2016).
Whilethe bi-directional RNN/LSTM model the preced-ing and succeeding contexts at a specific word and2078O O O Otoneed a SeattleticketIO B-ToCity(a) Labeler LSTM(W).O O O OOO O OO<B>O B-ToCitytoneed a SeattleticketI(b) Labeler LSTM(W+L).Encoder (backward) LSTM Decoder LSTMto needa IO O O OSeattle ticket OO O OO<B>O B-ToCity(c) Encoder-decoder LSTM.to needa IO O O OSeattle ticket toneed a SeattleticketIO B-ToCityEncoder LSTM (backward) Labeler LSTM(W)(d) Encoder-labeler LSTM(W).to needa IO O O OSeattle ticket OO O OO<B>O B-ToCitytoneed a SeattleticketIEncoder LSTM (backward) Labeler LSTM(W+L)(e) Encoder-labeler LSTM(W+L).Figure 1: Neural network architectures for slot filling.
Input sentence is ?I need a ticket to Seattle?.
?B-ToCity?
is slot label forspecific meaning and ?O?is slot label without specific meaning.
?<B>?
is beginning symbol for slot sequence.SentenceSlots showO flightsO fromO BostonB-FromCity toO NewB-ToCity YorkI-ToCity todayB-DateFigure 2: Example of ATIS sentence and annotated slots.don?t explicitly encode the whole sentence, ourproposed encoder-labeler LSTM explicitly encodeswhole sentence and predicts slots conditioned on theencoded information.Another method to consider the sentence-level in-formation for slot filling is the attention-based ap-proach (Simonnet et al, 2015).
The attention-basedapproach is novel in aligning two sequences of dif-ferent length.
However, in the slot filling task wherethe input and output sequences have the same lengthand the input word and the output label has strongrelations, the effect of introducing ?soft?
attentionmight become smaller.
Instead, we directly fed theinput word into the labeler part with using contextwindow method as explained in Section 2.3.3 ExperimentsWe report two sets of experiments.
First we use thestandard ATIS corpus to confirm the improvementby the proposed encoder-labeler LSTM and com-pare our results with the published results while dis-cussing the related works.
Then we use a large-scaledata set to confirm the effect of the proposed methodin a realistic use-case.3.1 ATIS Experiment3.1.1 Experimental SetupWe used the ATIS corpus, which has been widelyused as the benchmark for NLU (Price, 1990; Dahlet al, 1994; Wang et al, 2006; Tur et al, 2010).Figure 2 shows an example sentence and its seman-tic slot labels in In-Out-Begin (IOB) representation.The slot filling task was to predict the slot label se-quences from input word sequences.The performance was measured by the F1-score:F1 = 2?Precision?RecallPrecision+Recall , where precision is the ra-tio of the correct labels in the system?s output andrecall is the ratio of the correct labels in the groundtruth of the evaluation data (van Rijsbergen, 1979).The ATIS corpus contains the training data of4,978 sentences and evaluation data of 893 sen-tences.
The unique number of slot labels is 127 andthe vocabulary size is 572.
In the following exper-iments, we randomly selected 80% of the originaltraining data to train the model and used the remain-ing 20% as the heldout data (Mesnil et al, 2015).We reported the F1-score on the evaluation data withhyper-parameters that achieved the best F1-score onthe heldout data.For training, we randomly initialized parame-ters in accordance with the normalized initializa-tion (Glorot and Bengio, 2010).
We used ADAMfor learning rate control (Kingma and Ba, 2014) anddropout for generalization with a dropout rate of0.5 (Srivastava et al, 2014; Zaremba et al, 2014).3.1.2 Improvement by Encoder-labeler LSTMWe conducted experiments to compare the labelerLSTM(W) (Section 2.1), the labeler LSTM(W+L)(Section 2.2), and the encoder-labeler LSTM (Sec-tion 2.3).
As for yet another baseline, we tried theencoder-decoder LSTM as shown in Figure 1(c)1.For all architectures, we set the initial learn-ing rate to 0.001 (Kingma and Ba, 2014) and1Length of the output label sequence is equal to that of theinput word sequence in a slot filling task.
Therefore, endingsymbol for slot sequence is not necessary.2079the dimension of word embeddings to de = 30.We changed the number of hidden units in theLSTM, dh ?
{100, 200, 300}2, and the size ofthe context window, k ?
{0, 1, 2}3.
We usedbackward encoding for the encoder-decoder LSTMand the encoder-labeler LSTM as suggested inSutskever et al (2014).
For the encoder-decoderLSTM, labeler LSTM(W+L), and encoder-labelerLSTM(W+L), we used the left-to-right beam searchdecoder (Sutskever et al, 2014) with beam sizes of1, 2, 4, and 8 for evaluation where the best F1-scorewas reported.
During 100 training epochs, we re-ported the F1-score on the evaluation data with theepoch when the F1-score for the heldout data wasmaximized.
Table 1 shows the results.The proposed encoder-labeler LSTM(W) andencoder-labeler LSTM(W+L) both outperformedthe labeler LSTM(W) and labeler LSTM(W+L),which confirms the novelty of considering sentence-level information with the encoder LSTM by ourproposed method.Contrary to expectations, F1-score by theencoder-labeler LSTM(W+L) was not improvedfrom that by the encoder-labeler LSTM(W).
A pos-sible reason for this is the propagation of label pre-diction errors.
We compared the label prediction ac-curacy for the words after the first label predictionerror in the evaluation sentences and confirmed thatthe accuracy deteriorated from 84.0% to 82.6% byusing pthe label dependencies.For the encoder-labeler LSTM(W) which was bet-ter than the encoder-labeler LSTM(W+L), we triedthe deep architecture of 2 LSTM layers (Encoder-labeler deep LSTM(W)).
We also trained the cor-responding labeler deep LSTM(W).
As in Table 1,we obtained improvement from 94.91% to 95.47%by the proposed encoder-labeler deep LSTM(W),which was statistically significant at the 90% level.Lastly, F1-score by the encoder-decoder LSTMwas worse than other methods as shown in the firstrow of Table 1.
Since the slot label is closely relatedwith the input word, the encoder-decoder LSTMwasnot an appropriate approach for the slot filling task.2When using deep architecture later in this section, dh wastuned for each layer.3In our preliminary experiments with using the labelerLSTM(W), F1-scores deteriorated with k ?
3.F1-score(c) Encoder-decoder LSTM 80.11(a) Labeler LSTM(W) 94.80(d) Encoder-labeler LSTM(W) 95.29(b) Labeler LSTM(W+L) 94.91(e) Encoder-labeler LSTM(W+L) 95.19Labeler Deep LSTM(W) 94.91Encoder-labeler Deep LSTM(W) 95.47Table 1: Experimental results on ATIS slot filling task.
Left-most column corresponds to Figure 1.
Lines with bold fontsuse proposed encoder-labeler LSTM.
[%]3.1.3 Comparison with Published ResultsTable 2 summarizes the recently published resultson the ATIS slot filling task and compares them withthe results from the proposed methods.Recent research has been focusing on RNN andits extensions.
Yao et al (2013) used RNN and out-performed methods that did not use neural networks,such as SVM (Raymond and Riccardi, 2007) andCRF (Deng et al, 2012).
Mesnil et al (2015) triedbi-directional RNN, but reported degradation com-paring with their single-directional RNN (94.98%).Yao et al (2014a) introduced LSTM and deepLSTM and obtained improvement over RNN.
Pengand Yao (2015) proposed RNN-EM that used an ex-ternal memory architecture to improve the memorycapability of RNN.Many studies have been also conducted to explic-itly model label dependencies.
Xu and Sarikaya(2013) proposed CNN-CRF that explicitly modelsthe dependencies of the output from CNN.
Mesnil etal.
(2015) used hybrid RNN that combined Elman-type and Jordan-type RNNs.
Liu and Lane (2015)used the output label for the previous word to modellabel dependencies (RNN-SOP).Vu et al (2016) recently proposed to use rankingloss function over bi-directional RNNs with achiev-ing 95.47% (R-biRNN) and reported 95.56% by en-semble (5?R-biRNN).By comparing with these methods, the main dif-ference of our proposed encoder-labeler LSTM isthe use of encoder LSTM to leverage sentence-levelinformation 4.4Since Simonnet et al (2015) did not report the experimen-tal results on ATIS, we could not experimentally compare ourresult with their attention-based approach.
Theoretical compar-ison is available in Section 2.4.2080F1-scoreRNN (Yao et al, 2013) 94.11CNN-CRF (Xu and Sarikaya, 2013) 94.35Bi-directional RNN (Mesnil et al, 2015) 94.73LSTM (Yao et al, 2014a) 94.85RNN-SOP (Liu and Lane, 2015) 94.89Hybrid RNN (Mesnil et al, 2015) 95.06Deep LSTM (Yao et al, 2014a) 95.08RNN-EM (Peng and Yao, 2015) 95.25R-biRNN (Vu et al, 2016) 95.475?R-biRNN (Vu et al, 2016) 95.56Encoder-labeler LSTM(W) 95.40Encoder-labeler Deep LSTM(W) 95.66Table 2: Comparison with published results on ATIS slot fillingtask.
F1-scores by proposed method are improved from Table 1due to sophisticated hyper-parameters.
[%]For our encoder-labeler LSTM(W) and encoder-labeler deep LSTM(W), we further conductedhyper-parameter search with a random search strat-egy (Bergstra and Bengio, 2012).
We tuned the di-mension of word embeddings, de ?
{30, 50, 75},number of hidden states in each layer, dh ?
{100, 150, 200, 250, 300}, size of context window,k ?
{0, 1, 2}, and initial learning rate sampled fromuniform distribution in range [0.0001, 0.01].
To thebest of our knowledge, the previously publishedbest F1-score was 95.56%5 (Vu et al, 2016).
Ourencoder-labeler deep LSTM(W) achieved 95.66%F1-score, outperforming the previously publishedF1-score as shown in Table 2.Note some of the previous results used wholetraining data for model training while others usedrandomly selected 80% of data for model trainingand the remaining 20% for hyper-parameter tuning.Our results are based on the latter setup.3.2 Large-scale ExperimentWe prepared a large-scale data set by mergingthe MIT Restaurant Corpus and MIT Movie Cor-5There are other published results that achieved better F1-scores by using other information on top of word features.Vukotic et al (2015) achieved 96.16% F1-score by using thenamed entity (NE) database when estimating word embeddings.Yao et al (2013) and Yao et al (2014a) used NE features in ad-dition to word features and obtained improvement with both theRNN and LSTM upto 96.60% F1-score.
Mesnil et al (2015)also used NE features and reported F1-score of 96.29% withRNN and 96.46% with Recurrent CRF.pus (Liu et al, 2013a; Liu et al, 2013b; SpokenLaungage Systems Group, 2013) with the ATIS cor-pus.
Since users of the NLU system may pro-vide queries without explicitly specifying their do-main, building one NLU model for multiple do-mains is necessary.
The merged data set contains30,229 training and 6,810 evaluation sentences.
Theunique number of slot labels is 191 and the vocab-ulary size is 16,049.
With this merged data set, wecompared the labeler LSTM(W) and the proposedencoder-labeler LSTM(W) according to the exper-imental procedure explained in Section 3.1.2.
Thelabeler LSTM(W) achieved the F1-score of 72.80%and the encoder-labeler LSTM(W) improved it to74.41%, which confirmed the effect of the proposedmethod in large and realistic data set 6.4 ConclusionWe proposed an encoder-labeler LSTM that canconduct slot filling conditioned on the encodedsentence-level information.
We applied this methodto the standard ATIS corpus and obtained the state-of-the-art F1-score in a slot filling task.
We alsotried to explicitly model label dependencies, but itwas not beneficial in our experiments, which shouldbe further investigated in our future work.In this paper, we focused on the slot labeling inthis paper.
Previous papers reported that jointlytraining the models for slot filling and intent classi-fication boosted the accuracy of both tasks (Xu andSarikaya, 2013; Shi et al, 2015; Liu et al, 2015).Leveraging our encoder-labeler LSTM approach injoint training should be worth trying.AcknowledgmentsWe are grateful to Dr. Yuta Tsuboi, Dr. RyukiTachibana, and Mr. Nobuyasu Itoh of IBM Re-search - Tokyo for the fruitful discussion and theircomments on this and earlier versions of the paper.We thank Dr. Ramesh M. Nallapati and Dr. CiceroNogueira dos Santos of IBM Watson for their valu-able suggestions.
We thank the anonymous review-ers for their valuable comments.6The purpose of this experiment is to confirm the effect ofthe proposed method.
The absolute F1-scores can not be com-pared with the numbers in Liu et al (2013b) since the capitaliza-tion policy and the data size of the training data were different.2081ReferencesYoshua Bengio.
2009.
Learning deep architectures forAI.
Foundations and trends R?
in Machine Learning,2(1):1?127.James Bergstra and Yoshua Bengio.
2012.
Randomsearch for hyper-parameter optimization.
The Journalof Machine Learning Research, 13(1):281?305.Deborah A Dahl, Madeleine Bates, Michael Brown,William Fisher, Kate Hunicke-Smith, David Pallett,Christine Pao, Alexander Rudnicky, and ElizabethShriberg.
1994.
Expanding the scope of the ATIStask: The ATIS-3 corpus.
In Proc.
HLT, pages 43?48.Renato De Mori, Fre?de?ric Bechet, Dilek Hakkani-Tur,Michael McTear, Giuseppe Riccardi, and Gokhan Tur.2008.
Spoken language understanding.
IEEE SignalProcessing Magazine, 3(25):50?58.Li Deng, Gokhan Tur, Xiaodong He, and Dilek Hakkani-Tur.
2012.
Use of kernel deep convex networks andend-to-end learning for spoken language understand-ing.
In Proc.
SLT, pages 210?215.Jeffrey L Elman.
1990.
Finding structure in time.
Cog-nitive science, 14(2):179?211.Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In Proc.
AISTATS, pages 249?256.Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh.2006.
A fast learning algorithm for deep belief nets.Neural computation, 18(7):1527?1554.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Michael I Jordan.
1997.
Serial order: A parallel dis-tributed processing approach.
Advances in psychol-ogy, 121:471?495.Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.2015.
An empirical exploration of recurrent networkarchitectures.
In Proc.
ICML, pages 2342?2350.Diederik Kingma and Jimmy Ba.
2014.
ADAM: Amethod for stochastic optimization.
arXiv preprintarXiv:1412.6980.Bing Liu and Ian Lane.
2015.
Recurrent neural net-work structured output prediction for spoken languageunderstanding.
In Proc.
NIPS Workshop on MachineLearning for Spoken Language Understanding and In-teractions.Jingjing Liu, Panupong Pasupat, Scott Cyphers, andJames Glass.
2013a.
Asgard: A portable architecturefor multilingual dialogue systems.
In Proc.
ICASSP,pages 8386?8390.Jingjing Liu, Panupong Pasupat, Yining Wang, ScottCyphers, and James Glass.
2013b.
Query understand-ing enhanced by hierarchical parsing structures.
InProc.
ASRU, pages 72?77.Chunxi Liu, Puyang Xu, and Ruhi Sarikaya.
2015.
Deepcontextual language understanding in spoken dialoguesystems.
In Proc.
INTERSPEECH, pages 120?124.Gre?goire Mesnil, Yann Dauphin, Kaisheng Yao, YoshuaBengio, Li Deng, Dilek Hakkani-Tur, Xiaodong He,Larry Heck, Gokhan Tur, Dong Yu, et al 2015.
Us-ing recurrent neural networks for slot filling in spokenlanguage understanding.
IEEE/ACM Transactions onAudio, Speech, and Language Processing, 23(3):530?539.Ramesh Nallapati, Bowen Zhou, C?a glar Gulc?ehre, andBing Xiang.
2016.
Abstractive text summarization us-ing sequence-to-sequence RNNs and beyond.
In Proc.CoNLL.Baolin Peng and Kaisheng Yao.
2015.
Recurrent neuralnetworks with external memory for language under-standing.
arXiv preprint arXiv:1506.00195.Patti Price.
1990.
Evaluation of spoken language sys-tems: The ATIS domain.
In Proc.
DARPA Speech andNatural Language Workshop, pages 91?95.Christian Raymond and Giuseppe Riccardi.
2007.
Gen-erative and discriminative algorithms for spoken lan-guage understanding.
In Proc.
INTERSPEECH, pages1605?1608.Yangyang Shi, Kaisheng Yao, Hu Chen, Yi-Cheng Pan,Mei-Yuh Hwang, and Baolin Peng.
2015.
Contextualspoken language understanding using recurrent neuralnetworks.
In Proc.
ICASSP, pages 5271?5275.Edwin Simonnet, Camelin Nathalie, Dele?glise Paul, andEste`ve Yannick.
2015.
Exploring the use of attention-based recurrent neural networks for spoken languageunderstanding.
In Proc.
NIPS Workshop on MachineLearning for Spoken Language Understanding and In-teractions.Spoken Laungage Systems Group.
2013.
TheMIT Restaurant Corpus and The MIT MovieCorpus.
https://groups.csail.mit.edu/sls/downloads/, MIT Computer Science and Ar-tificial Intelligence Laboratory.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural networks.In Proc.
NIPS, pages 3104?3112.Gokhan Tur, Dilek Hakkani-Tur, and Larry Heck.
2010.What is left to be understood in ATIS?
In Proc.
SLT,pages 19?24.Cornelis Joost van Rijsbergen.
1979.
Information Re-trieval.
Butterworth.2082Ngoc Thang Vu, Pankaj Gupta, Heike Adel, and Hin-rich Schu?tze.
2016.
Bi-directional recurrent neuralnetwork with ranking loss for spoken language under-standing.
In Proc.
ICASSP, pages 6060?6064.Vedran Vukotic, Christian Raymond, and GuillaumeGravier.
2015.
Is it time to switch to word embeddingand recurrent neural networks for spoken language un-derstanding?
In Proc.
INTERSPEECH, pages 130?134.Ye-Yi Wang, Alex Acero, Milind Mahajan, and JohnLee.
2006.
Combining statistical and knowledge-based spoken language understanding in conditionalmodels.
In Proc.
COLING-ACL, pages 882?889.Ronald J Williams and Jing Peng.
1990.
An effi-cient gradient-based algorithm for on-line training ofrecurrent network trajectories.
Neural Computation,2(4):490?501.Puyang Xu and Ruhi Sarikaya.
2013.
Convolutional neu-ral network based triangular CRF for joint intent detec-tion and slot filling.
In Proc.
ASRU, pages 78?83.Kaisheng Yao and Geoffrey Zweig.
2015.
Sequence-to-sequence neural net models for grapheme-to-phonemeconversion.
Proc.
INTERSPEECH, pages 3330?3334.Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,Yangyang Shi, and Dong Yu.
2013.
Recurrent neu-ral networks for language understanding.
In Proc.
IN-TERSPEECH, pages 2524?2528.Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Geof-frey Zweig, and Yangyang Shi.
2014a.
Spoken lan-guage understanding using long short-term memoryneural networks.
In Proc.
SLT, pages 189?194.Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Dong Yu,Xiaolong Li, and Feng Gao.
2014b.
Recurrent con-ditional random field for language understanding.
InProc.
ICASSP, pages 4077?4081.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2014.
Recurrent neural network regularization.
arXivpreprint arXiv:1409.2329.Jie Zhou and Wei Xu.
2015.
End-to-end learning of se-mantic role labeling using recurrent neural networks.In Proc.
ACL, pages 1127?1137.2083
