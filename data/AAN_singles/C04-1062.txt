Multi-Human Dialogue Understanding for AssistingArtifact-Producing MeetingsJohn Niekrasz and Alexander Gruenstein and Lawrence CavedonCenter for the Study of Language and Information (CSLI)Stanford UniversityCordura Hall, Stanford, CA, 94305-4115, USAhttp://www-csli.stanford.edu/semlab/{niekrasz, alexgru, lcavedon}@csli.stanford.eduAbstractIn this paper we present the dialogue-understanding components of an architec-ture for assisting multi-human conversa-tions in artifact-producing meetings: meet-ings in which tangible products such asproject planning charts are created.
Novelaspects of our system include multimodalambiguity resolution, modular ontology-driven artifact manipulation, and a meetingbrowser for use during and after meetings.We describe the software architecture anddemonstrate the system using an examplemultimodal dialogue.1 IntroductionRecently, much attention has been focused onthe domain of multi-person meeting under-standing.
Meeting dialogue presents a widerange of challenges including continuous multi-speaker automatic speech recognition (ASR),2D whiteboard gesture and handwriting recog-nition, 3D body and eye tracking, and multi-modal multi-human dialogue management andunderstanding.
A significant amount of re-search has gone toward understanding the prob-lems facing the collection, organization, andvisualization of meeting data (Moore, 2002;Waibel et al, 2001), and meeting corpora likethe ICSI Meeting Corpus (Janin et al, 2003) arebeing made available.
Continuing research inthe multimodal meeting domain has since blos-somed, including ongoing work from projectssuch as AMI1 and M42, and efforts from sev-eral institutions.Previous work on automatic meeting un-derstanding has mostly focused on surface-level recognition, such as speech segmentation,for obvious reasons: understanding free multi-human speech at any level is an extremely diffi-cult problem for which best performance is cur-rently poor.
In addition, the primary focus for1http://www.amiproject.org/2http://www.m4project.org/applications has been on off-line tools such aspost-meeting multimodal information browsing.In parallel to such efforts we are applyingdialogue-management techniques to attempt tounderstand and monitor meeting dialogues asthey occur, and to supplement multimodalmeeting records with information relating to thestructure and purpose of the meeting.Our efforts are focused on assisting artifact-producing meetings, i.e.
meetings for which theintended outcome is a tangible product such asa project management plan or a budget.
Thedialogue-understanding system helps to createand manipulate the artifact, delivering a finalproduct at the end of the meeting, while thestate of the artifact is used as part of the dia-logue context under which interpretation of fu-ture utterances is performed, serving a num-ber of useful roles in the dialogue-understandingprocess:?
The dialogue manager employs generic di-alogue moves with plugin points to be de-fined by specific artifact types, e.g.
projectplan, budget;?
The artifact state helps resolve ambiguityby providing evidence for multimodal fu-sion and constraining topic-recognition;?
The artifact type can be used to bias ASRlanguage-models;?
The constructed artifact provides a inter-face for a meeting browser that supportsdirected queries about discussion that tookplace in the meeting, e.g.
?Why did wedecide on that date?
?In addition, we focus our attention on thehandling of ambiguities produced on manylevels, including those produced during au-tomatic speech recognition, multimodal com-munication, and artifact manipulation.
Thepresent dialogue manager uses several tech-niques to do this, including the maintenance ofMultimodalIntegrator3-DGestureRecognizer2-DDrawingRecognizerASRCIANLParserInformation StateOntologyKB DMT,Active Node,Salience List,etc.DialogueManagerMeeting BrowserHypothesisRepository (CMU)(OGI)(OGI)(MIT)Figure 1: The meeting assistant architecture,highlighting the dialogue-management compo-nents.multiple dialogue-move hypotheses, fusion withmultimodal gestures, and the incorporation ofartifact-specific plug-ins.The software architecture we use for manag-ing multi-human dialogue is an enhancement ofa dialogue-management toolkit previously usedat CSLI in a range of applications, includingcommand-and-control of autonomous systems(Lemon et al, 2002) and intelligent tutoring(Clark et al, 2002).
In this paper, we detail thedialogue-management components (Section 3),which support a larger project involving mul-tiple collaborating institutions (Section 2) tobuild a multimodal meeting-understanding sys-tem capable of integrating speech, drawing andwriting on a whiteboard, and physical gesturerecognition.We also describe our toolkit for on-line andoff-line meeting browsing (Section 4), which al-lows a meeting participant, observer, or devel-oper to visually and interactively answer ques-tions about the history of a meeting, the pro-cesses performed to understand it, and thecausal relationships between dialogue and ar-tifact manipulation.2 Meeting Assistant ArchitectureThe complete meeting assistant architecture isa highly collaborative effort from several insti-tutions.
Its overall architecture, focusing on ourcontributions to the system is illustrated in Fig-ure 1.The components for drawing and writingrecognition and multimodal integration (Kaiseret al, 2003) were developed at The OregonGraduate Institute (OGI) Center for Human-Computer Communication3; the component forphysical gesture recognition (Ko et al, 2003)was developed at The Massachusetts Instituteof Technology (MIT) AI Lab4.
Integration be-tween all components was performed by projectmembers at those sites and at SRI Interna-tional5, and integration between our CSLI Con-versational Intelligence Architecture and OGI?sMultimodal Integrator (MI) was performed bymembers of both teams.
ASR is done usingCMU Sphinx6, from which the n-best list of re-sults are passed to SRI?s Gemini parser (Dowd-ing et al, 1993).
Gemini incorporates a suiteof techniques for handling noisy input, includ-ing fragment detection, and its dynamic gram-mar capabilities are used to register new lexicalitems, such as names of tasks that may be out-of-grammar.An example of a multimodal meeting conver-sation that the meeting assistant currently sup-ports can be found in Figure 2.7 There are twomeeting participants in a conference room withan electronic whiteboard which can record theirpen strokes and a video camera that tracks theirbody movements; A is standing at the white-board and drawing while B is sitting at thetable.
A gloss of how the system behaves inresponse to each utterance and gesture followseach utterance; these glosses will be explainedin greater detail throughout the rest of the pa-per.
The drawing made on the whiteboard isin Figure 3(a), and the chart artifact as it wasconstructed by the system is displayed in Figure3(b).3 Conversational IntelligenceArchitectureTo meet the challenges presented by multi-person meeting dialogue, we have extendedand enhanced our previously used Conversa-tional Intelligence Architecture (CIA).
The CIAis a modular and highly configurable multi-application system: a separation is made be-tween generic dialogue processes and those spe-cific to a particular domain.
Creating a newapplication may involve writing new dialoguemoves and configuring the CIA to use these.
We3http://www.cse.ogi.edu/CHCC/4http://www.ai.mit.edu/5http://www.sri.com/6http://www.speech.cs.cmu.edu/sphinx/7A video demonstration will be available soon athttp://www-csli.stanford.edu/semlab/calo/A: So, lets uh figure out what uh needs uh needs to be done.
Let?slook at the schedule.
[draws a chart axes] utterance and gestureinformation fused, a new milestone chart artifact is createdB: So, if all goes well, we?ve got funding for five years.
system setsunit on axis to ?years?A: Yeah.
Let?s see one, two ... [draws five tick marks on the x-axis]system assumes tick marks are yearsB: Well, the way I see it, uh we?ve got three tasks.
dialogue man-ager hypothesizes three tasks should be added, waits for multimodalconfirmationA: Yeah right [draws three task lines horizontally on the axis] multi-modal confirmation is given, information about task start and enddates is fused from the drawingA: Let?s call this task line demo [touches the top line with thepen], call this task line signoff [touches the middle line with thepen], and call this task line system [touches the bottom line withthe pen].
each utterance causes the dialogue manager to hypoth-esize three distinct hypotheses, in each task a different hypothesisis named, the gestures disambiguate these in the multimodal inte-gratorB: So we have two demos to get done.A: uh huhB: Darpatech is at the end of month fifteen [A draws a diamondat month fifteen on the demo task line] dialogue manager hy-pothesizes a milestone called ?darpatech?
at month fifteen; gestureconfirms this and pinpoints appropriate task lineB: And the final demonstrations are at the end of year five [A drawsa diamond at year five on the demo task line] same processing aspreviousA: Hmm, so when do the signoffs need to happen do you think?dialogue manager expects next utterance to be an answerB: Six months before the demos [A draws two diamonds on the sig-noff task line, each one about 6 months before the demo mile-stones drawn above] answer arrives; dialogue manager hypothe-sizes two new milestones which are confirmed by gestureA: And we?ll need the systems by then too [A draws two diamondson the system task line] dialogue manager hypothesizes two moremilestones, confirmed by gestureB: That?s a bit aggressive I think.
Let?s move the system milestoneback six months.
[B points finger at rightmost system milestone.A crosses it out and draws another one six months earlier] di-alogue manager hypothesizes a move of the milestone, 3D gestureand drawing confirm thisFigure 2: Example conversation understood bythe system.
(a) The whiteboard in-put captured by OGI?sCharter gesture recog-nizer(b) The artifact asmaintained in thedialogue systemFigure 3: Ink-captured vs ?idealized?
artifactoutput.have successfully used this ?toolkit?
approachin our previous applications at CSLI to inter-face novel devices without modifying the coredialogue manager.The present application is however very dif-ferent to our previous applications, and thosecommonly encountered in the literature, whichtypically involve a single human user interact-ing with a dialogue-enabled artificial agent.
Inthe meeting environment, the dialogue managershould at most very rarely interpose itself intothe discussion?to do so would be disruptiveto the interaction between the human partic-ipants.
This requirement prohibits ambiguityand uncertainty from being resolved with, say,a clarification question, which is the usual strat-egy in conversational interfaces.
Instead, uncer-tainty must be maintained in the system untilit can be resolved by leveraging context, usingevidence from another modality, or by a futureutterance.The meeting-understanding domain has thusprompted several extensions to our existingCIA, many of which we expect will be appliedin other conversational domains.
These include:?
Support for handling multiple competingspeech parses; (Section 3.2)?
A generic artifact ontology which enablesdesigning generically useful artifact-savvydialogue applications; (Section 3.3)?
Support for the generation and subsequentconfirmation of dialogue-move hypothesesin a multimodal integration framework;(Section 3.4)?
The acceptance of non-verbal unimodalgestures into the dialogue-move repertoire.
(Section 3.5)?
A preliminary mechanism for supportinguncertainty across multiple conversationalmoves; (Section 3.6)Before discussing these new features in detail,the following section introduces the CIA andits persisting core dialogue-management com-ponents.3.1 Core Components: InformationState and ContextThe core dialogue management components ofthe CIA maintain dialogue context using theinformation-state and dialogue-move approach(Larsson and Traum, 2000) where each con-tributed utterance modifies the current context,or information state, of the dialogue.
Each newutterance is then interpreted within the currentcontext (see (Lemon et al, 2002) for a detaileddescription).A number of data structures are employedin this process.
The central dialogue state-maintaining structure is the Dialogue Move Tree(DMT).
The DMT represents the historical con-text of a dialogue.
An incoming utterance, clas-sified by dialogue move, is interpreted in con-text by attaching itself to an appropriate activenode on the DMT; e.g., an answer attaches toan active corresponding question node.
Cur-rently, active nodes are kept on an Active NodeList , which is ordered so that those most likelyto be relevant to the current conversation areat the front of the list.
Incoming utterancesare displayed to each node in turn, and at-tach to the first appropriate node (determinedby information-state-update functions).
Otherstructures include the context-specific SalienceList , which maintains recently used terms forperforming anaphora resolution, and a Knowl-edge Base containing application specific infor-mation, which may be leveraged to interpret in-coming utterances.8We now present the various enhancementsmade to the CIA for use in the meeting domain.3.2 ASR and Robust ParsingThe first step in understanding any dialogue isrecognizing and interpreting spoken utterances.In the meeting domain, we are presented withthe particularly difficult task of doing this forspontaneous human-human speech.
We there-fore chose to perform ASR using a statisti-cal language model (LM) and employ CMU?sSphinx to generate an n-best list of recogni-tion results.
The recognition engine uses a tri-gram LM trained on the complete set of pos-sible utterances expected given a small hand-crafted scenario like that in the example dia-logue.
Despite the task?s limited domain, the re-alized speech is very disfluent, generating an ex-tremely broad range of possible utterances thatthe system must handle.
The resulting n-bestlist is therefore often extremely varied.To handle the ASR results of disfluent utter-ances, we employ SRI?s Gemini robust languageparser (Dowding et al, 1993).
In particular,we use Gemini to retrieve the longest stringsof valid S and NP fragments in each ASR re-sult.
Currently, we reject all but the parsed Sfragments?and NP fragments when expected8Command-and-control applications have also madeuse of an Activity Tree, which represents activities beingcarried out by the dialogue-enabled device (Gruenstein,2002); however, this application currently makes no useof this.by the system (e.g.
an answer to a questioncontaining an NP gap).
The parser uses genericsyntactic rules, but is constrained semanticallyby sorts specific to the domain.
In Section 3.4,we describe how the dialogue manager handlesthe multiple parses for a single utterance andhow it uses the uncertainty they represent.3.3 Artifact Knowledge Base andOntologyIn the present version of the CIA, all static do-main knowledge about meeting artifacts is de-fined in a modularized class-based ontology.
Inconjunction with the ontology, we also maintaina dynamic knowledge base (KB) which holdsthe current state of any artifacts.
This is storedas a collection of instances of the ontologicalclasses, and both components are maintainedtogether using the Prote?ge?-20009 ontology andknowledge-base toolkit (Grosso et al, 1999).The principal base classes in the artifact on-tology are designed to be both architecturallyelegant and intuitive.
To this end, we charac-terize the world of artifacts as being made upof three essential classes: entities which repre-sent the tangible objects themselves, relationswhich represent how the entities relate to oneanother, and events which change the state ofentities or relations.
Events are the most im-portant tool aiding the dialogue managementalgorithm.
They comprehensively characterizethe set of actions which can change the currentstate of an artifact.
They may be classified intothree categories: insert changes which insert anew entity or relation instance into the KB, re-move changes which remove an instance, andvalue changes which modify the value of a slotin an instance.
All changes to the KB can becharacterized as one of these three atomic eventsor a combination of them.3.4 Hypothesizers: A pluginarchitecture for artifact-drivenmultimodal integrationAbmiguities and uncertainties are both ram-pant in multimodal meeting dialogues, and inartifact-producing meetings, the majority per-tain to artifacts and the utterances performedto change them.
In this section we explain howthe CIA?s dialogue manager uses the artifact on-tology, and the repertoire of event classes in it,to formulate sets of artifact-changing dialogue-move hypotheses from single utterances.
We9http://protege.stanford.edu/also demonstrate how it uses the current stateof the artifact in the KB to constrain the in-terpretation of utterances in context, and howmultimodal gestures help to resolve ambiguousinterpretations.To begin, each dialogue-move hypothesis con-sists of the following elements: (1) the DMTnode associated with this hypothesis, (2) theparse that gave rise to the hypothesis, (3) theprobability of the hypothesis, (4) an isUnimodalflag indicating whether or not the dialogue moverequires confirmation from other modalities, (5)a list of artifact-change events to be made tothe KB, and (6) the information state updatefunction to be invoked if this hypothesis is con-firmed by the multimodal integrator.
Each ofthese elements participate in the generation andconfirmation process as detailed below.First, consider the utterance Darpatech is atthe end of month fifteen.
from the example dia-logue.
This utterance is much more likely toindicate the creation of a new milestone if atask line is pertinent to the current dialoguecontext, e.g.
the user has just created a newtask line.
In our system, the ambiguous or un-certain utterance, the current dialogue context,and the current state of the chart is delegated toartifact-type specific components called hypoth-esizers.
Hypothesizers take the above as input,and using the set of events available to its cor-responding artifact in the ontology, they pro-grammatically generate a list of dialogue-movehypotheses appropriate in the given context?or they can return the empty list to indicatethat there is no reasonable interpretation of theutterance given the current context.Hypothesizers work directly with the DMTarchitecture: as an incoming utterance is se-quentially presented to each active node in theDMT, the dialogue context and the proposedactive node are passed into a hypothesizer cor-responding to the particular artifact associatedwith that node.
If the hypothesizer can createone or more valid hypotheses, then the utter-ance is attached to the DMT as a child of thatactive node.10In a multimodal domain, some hypotheses re-quire confirmation in other modalities beforethe dialogue manager can confidently update10There are, in fact, other rules as well which allow forattachment.
For example, questions?which don?t im-mediately generate hypotheses?can also be attached tovarious nodes depending on the dialogue context.
Whilethe emphasis here is on hypothesizers, these are just onepart of the dialogue processing toolkitthe information state.
In this particular system,in fact, the dialogue manager does not directlyupdate the KB?s current artifact state; rather, ithypothesizes a set of dialogue-move hypothesesand assigns each a confidence derived from ASRconfidence, the fragmentedness of the parse, andconfidence in the proposed attachment to a con-versational thread.
Each conversational move isthen provided a Hypothesis Repository for stor-ing the hypotheses associated with it.
Whendialogue processing is completed for a partic-ular conversational move, i.e.
when all pos-sible attachments of all possible parses on then-best list have been made, the set of hypothe-ses is sent to the Multimodal Integrator (MI)for potential fusion with gesture.
Depending onthe information from other modalities, the MIconfirms or rejects the hypotheses?moreover, aconfirmed hypothesis might be augmented withinformation provided by other modalities.
Suchan augmentation occurs for the utterance Wehave three tasks from the example dialogue.
Inthis situation, the dialogue manager hypothe-sizes that the user may be creating three newtask lines on the chart.
When the user actuallydraws the three task lines, the MI infers thestart and stop date based on where the linesstart and stop on the axis.
In this case, it notonly confirms the dialogue manager?s hypoth-esis, but augments it to reflect the additionaldate information yielded from the whiteboardinput.3.5 Unimodal GesturesIn addition to the Information State updatesbased on both speech and gesture, multimodalmeeting dialogue can often include gestures inwhich a participant makes a change to an ar-tifact using a unimodal gesture not associatedwith an utterance.
For example, a user maydraw a diamond on a task line but say nothing.Even in the absence of speech, this can be unam-biguously understood as the creation of a mile-stone at a particular point on the line.
Theseunimodally produced changes to the chart mustbe noted by the dialogue manager, as they arepotential targets for later conversation.
To ac-commodate this, we introduce a new DMT nodeof type Unimodal Gesture, thus implicitly in-cluding gesture as a communicative act that canstand on its own in a conversation3.6 Uncertain DMT Node AttachmentSince hypotheses are not always immediatelyconfirmed, uncertainty must be maintainedFigure 4: A snapshot from the meeting browser.across multiple dialogue moves.
The system ac-complishes this by extending the CIA to main-tain multiple competing Information States.
Inparticular, the DMT has been extended to al-low for the same parse to attach in multiplelocations?these multiple attachments are even-tually pruned as more evidence is accumulatedin the form of further speech or gestures?thatis, as hypotheses are confirmed or rejected overtime.4 Meeting Viewer ToolkitThroughout an artifact-producing meeting, thedialogue system processes a complex chronolog-ical sequence of events and information statesthat form structures rich in information usefulto dialogue researchers and the dialogue partic-ipants themselves.
To harness the power of thisinformation, we have constructed a toolkit forvisualizing and investigating the meeting infor-mation state and its history.Central to the toolkit is our meeting historybrowser, which can be seen in Figure 4, dis-playing a portion of the example dialogue, withthe results of a search for ?demo?
highlighted.This record of the meeting is available both dur-ing the meeting and afterwards to assist usersin answering questions they might have aboutthe meeting.
Many kinds of questions can beanswered in the browser, like those a managermight ask the day after a meeting: ?Why did wemove the deadline on that task 6 months later?
?,?Did I approve setting that deadline so early?
?,and ?What were we thinking when we put thatmilestone at month fifteen??.
A meeting partic-ipant might have questions as the meeting oc-curs, like ?What did the chart look like 5 min-utes ago?
?, ?What did we say to make the sys-tem move that milestone?
?, and ?What did Mr.Smith say at the beginning of the meeting?
?.To help answer these questions, the browserperforms many of the functions found in currentmultimodal meeting browsers.
For example,it provides concise display of a meeting tran-scription, advanced searching capabilities, sav-ing and loading of meeting sessions, and person-alization of its own display characteristics.
Asa novel addition to these basic behaviors, thebrowser is also designed to display artifacts andthe causal relationships between artifacts andthe utterances that cause them to change.To effectively convey this information, therecord of components monitored by the historytoolkit is presented to the user through a win-dow which chronologically displays the visualembodiment of those components.
Recognizedutterances are shown as text, parses are shownas grouped string fragments, and artifacts andtheir sub-components are shown in their pro-totypical graphical form.
The window orga-nizes these visual representations of the meet-ing?s events and states into chronological tracks,each of which monitors a unified conceptual partof the meeting.
The user is then able to link theelements causally.Beyond the history browser, the toolkit alsodisplays the current state of all artifacts in anartifact-state window (e.g.
Figure 3(b)).
In thewindow, the user not only confirms the state ofthe artifact but can also gain insight into thecurrently interpreted dialogue context by mon-itoring how the artifact is highlighted.
In thefigure, the third task is highlighted because it isthe most recently talked-about task.
A meetingparticipant can therefore see that subsequentanaphoric references to an unknown task willbe resolved to the third one.Another GUI component of the toolkit isa small hypothesis window which shows thecurrent set of unresolved artifact-changing hy-potheses.
It does this by displaying an artifactfor each hypothesis, reflecting the artifact?s fu-ture state given confirmation of the hypothe-sis.
The hypothesis?
probability and associatedparse is displayed under the artifact.
The usermay even directly click a hypothesis to confirmit.
The hypothesized future states are howevernot displayed in the artifact-state window orartifact-history browser, which show only theresults of confirmed actions.In addition to being a GUI front-end, thetoolkit maintains a fully generic architecture forrecording the history of any object in the sys-tem software.
These objects can be anythingfrom the utterances of a participant, to the statehistory of an artifact component, or the recordof hypotheses formulated by the dialogue man-ager.
This generic functionality provides thetoolkit the ability to answer a wide variety ofquestions for the user about absolutely any as-pect of the dialogue context history.5 Future WorkWork is currently proceeding in a number ofdirections.
Firstly, we plan to incorporate fur-ther techniques for robust language understand-ing, including word-spotting and other topic-recognition techniques, within the context ofthe constructed artifact.
We also plan to in-vestigate using the current state of the artifactto further bias the ASR language model.
Wealso plan on generalizing the uncertainty man-agement within the dialogue manager, allowingmultiple competing hypotheses to be supportedover multiple dialogue moves.
Topic and otherambiguity management techniques will be usedto statistically filter and bias hypotheses, basedon artifact state.We are currently expanding the meetingbrowser to categorize utterances by dialogueact, and to recognize and categorize aggrega-tions as multi-move strategies, such as negoti-ations.
This will allow at-a-glance detection ofwhere disagreements took place, and where is-sues may have been left unresolved.
A longer-term aim of the project is to provide furthersupport to the participants in the meeting, e.g.by detecting opportunities to provide useful in-formation (e.g.
schedules, when discussing whoto allocate to a task; documents pertinent to atopic under discussion) to meeting participantsautomatically.
Evaluation criteria are currentlybeing designed that include both standard mea-sures, such as word error rate, and measures in-volving recognition of meeting-level phenomena,such as detecting agreement on action-items.Evaluation will be performed using both corpus-based approaches (e.g.
for evaluating recog-nition of meeting phenomena) and real (con-trolled) meetings with human subjects.6 AcknowledgementsWe would like to gratefully acknowledge PhilCohen?s group at OGI, especially Ed Kaiser,Xiaoguang Li, and Matt Wesson, and DavidDemirdjian at MIT.
This work was funded byDARPA grant NBCH-D-03-0010(1).ReferencesB.
Clark, E. Owen Bratt, O.
Lemon, S. Pe-ters, H. Pon-Barry, Z. Thomsen-Gray, andP.
Treeratpituk.
2002.
A general purpose ar-chitecture for intelligent tutoring systems.
InInternational CLASS Workshop on Natural,Intelligent and Effective Interaction in Multi-modal Dialogue Systems.J.
Dowding, J.M.
Gawron, D. Appelt, J. Bear,L.
Cherny, R. Moore, and D. Moran.
1993.Gemini: a natural language system forspoken-language understanding.
In Proc.ACL 93.W.
E. Grosso, H. Eriksson, R. W. Fergerson,J.
H. Gennari, S. W. Tu, and M. A. Musen.1999.
Knowledge modeling at the millen-nium: (the design and evolution of Prote?ge?-2000).
In Proc.
KAW 99.A.
Gruenstein.
2002.
Conversational interfaces:A domain-independent architecture for task-oriented dialogues.
Master?s thesis, StanfordUniversity.A.
Janin, D. Baron, J. Edwards, D. Ellis,D.
Gelbart, N. Morgan, B. Peskin, T. Pfau,E.
Shriberg, A. Stolcke, and C. Wooters.2003.
The ICSI meeting corpus.
In Proc.ICASSP 2003.E.
Kaiser, A. Olwal, D. McGee, H. Benko,A.
Corradini, X. Li, P. Cohen, and S. Feiner.2003.
Mutual disambiguation of 3D multi-modal interaction in augmented and virtualreality.
In Proc.
ICMI 2003.T.
Ko, D. Demirdjian, and T. Darrell.
2003.Untethered gesture acquisition and recogni-tion for a multimodal conversational system.In Proc.
ICMI 2003.S.
Larsson and D. Traum.
2000.
Informa-tion state and dialogue management in theTRINDI dialogue move engine toolkit.
Natu-ral Language Engineering, 6.O.
Lemon, A. Gruenstein, and S. Peters.
2002.Collaborative activities and multi-tasking indialogue systems.
Traitment Automatiquedes Langues, 43(2).D.
Moore.
2002.
The IDIAP smart meetingroom.
Technical Report IDIAP Communica-tion 02-07.A.
Waibel, M. Bett, F. Metze, K. Ries,T.
Schaaf, T. Schultz, H. Soltau, H. Yu, andK.
Zechner.
2001.
Advances in automaticmeeting record creation and access.
In Proc.ICASSP 2001.
