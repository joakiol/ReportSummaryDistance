Tagging Spoken Language Using Written Language StatisticsJoakim Nivre Leif Gr6nqvist  Mal in Gustafsson TorbjSrn Lager Sylvana SofkovaDept .
of  L ingu is t i csG5teborg  Un ivers i tyS-41298 GSteborgSweden{j oak im,  le i fg ,  ma l in ,  lager ,  sy lvana}@l ing ,  gu .
seAbstractThis paper reports on two experimentswith a probabilistic part-of-speech tag-ger, trained on a tagged corpus of writ-ten Swedish, being used to tag a corpusof (transcribed) spoken Swedish.
The re-sults indicate that with very little adap-tations an accuracy rate of 85% canbe achieved, with an accuracy rate forknown words of 90%.
In addition, twodifferent treatments of pauses were ex-plored but with no significant gain in ac-curacy under either condition.1 Introduct ionWhat happens when we take a probabilistic part-of-speech tagger trained on written language andtry to use it on spoken language transcriptions?The answer to this question is interesting fromseveral points of view, some more practical andsome more theoretically oriented.
From a practi-cal point of view, it, is interesting to know how wella written language tagger can perform on spokenlanguage, because it may save us a lot of work ifwe can reuse existing taggers instead of develop-ing new ones for spoken language.
Front a moretheoretical point of view, the results of such anexperiment may tell us something about the waysin which the strncture of spoken language is dif-ferent (or not so different;) from that of writtenlanguage.In this paper, we report on experimental workdealing with the part-of-speech tagging of a corpusof (transcribed) spoken Swedish.
The tagger usedimplements a standard probabilistic biclass model(see, e. g., (DeRose 1988)) trained on a taggedsubset of the Stockhohn-Ume?
Corpus of writtenSwedish (Ejerhed et al1992).
Given that the tran-scriptions contain many modifications of standardorthography (in order to capture spoken languagevariants, reductions, etc.)
a special lexicon hadto be developed to map spoken langnage variantsonto their canonical written language forms.
Inaddition, a special tokenizer had to be developedto handle "recta-symbols" in the transcriptions,such as markers for pauses, overlapping speech,inaudible speech, etc.
One of the interesting is-sues in this context is what use (if any) should bemade of information about panses, interruptions,etc.
In the experiment reported here, we com-pare two different reatments of pauses and evalu-ate the performance of the tagger under these twodifferent conditions.2 Background2.1 Probabilistie Part-of-speech TaggingThe problem of (automatically) assigning parts ofspeech to words in context has received a lot ofattention within computational corpus linguistics.A variety of diffexent methods have been investi-gated, most of which fall into two broad classes:?
Probabilistic methods, e. g. (DeRose 1988;Cutting et al1992; Merialdo 1994).?
Rule-based methods, e. g. (Brodda 1982;Karlsson 1990; Koskennienfi 1990; Brill1992).Probabilistic taggers have typically been imple-mented as hidden Markov models, using proha-bilistic models with two kinds of' basic probabili-ties:?
The lexical probability of seeing the word wgiven the part-of-speech t: P(w I t).?
The contextual pwbability of seeing thepart-of-speech ti given the context of n - 1parts-of-speech: P(ti I ti-(,~-,),...,ti 1).Models of this kind are usually referred to as n-class models, the most common instances of whichare the biclass (n = 2) and triclass (n = 3) models.The lexical and contextual probabilities of an n-class tagger are usually estimated using one of twomethods: ~1The terms 'RF training' and 'ML training' aretaken from Merialdo 1994.
It should be pointed out,though, that the use of relative frequencies to esti-mate occurrence probabilities is also a case of maxi-mmn likelihood estimation (MLE).1078?
Relative l,Yequency (RF) training: Given atagged training corpus, the i)rohabilities (:anbe estimated with relative frequencies.?
Maxinnun Likelihood (ML) training: Givenan untagged training corpus, the probabilitiescan be estimated using the Bauin-Welch algo-rithm (also known as tile Forward-Backwardalgorithin) (Baron 1972).Of these two methods, R.F training seelns to givebetter estilnations while t)eing more labor inten-sive (Merialdo 1994).
With proper training, r>class taggers typically readt all accuracy rate ofabout 95% \['or English texts (Charniak 1993), andsimilar results have been reported for other lan-guages such as lh'ench and Swedish (Chanod &Tapanainen 1995; Brants & Samuelsson 1995).2.2 Tagging Spoken LanguageSpoken language transcrit)tions are essentially aMud of text, and can therefore be tagged withthe methods used for otller kinds of text,.
IIow-ever, sin(:(; t, he transcription of spoken languageis a fairly labor-intensive tasks, the availabilityof suitable training corpora is much more limitexlthan for ordinary written texts.
One way to cir-cuinvent his problem is to use taggers trained onwritten texts to tag spoken language also.
Thishas apparently been done successflllly for the spo-ken language part of the British National Corpus,using the CLAWS tagger (Garsi(te).However, the application of writte, n languagetaggers to spol(en language is not entirely unprob-lematic.
First of all, spoken language transcrip-tions are typically produced ill a different formatand with different conventions than ordinary writ-ten texts.
For example, a transcription is likely tocontain markers tbr pauses, (aspects of) t)rosody,overlapping speech, etc.
Moreover, they do notusually contain the pun(:tuation marks found inordinary texts.
This means that the application ofa written language tagger to spoken language min-imally requires a special tokenizer, i. e., a prepro-cessor segmenting the text into appropriate codingunits (words).A second type of ditficulty arises from tile factthat spoken language is otten transcribed usingnon-standard orthograI)hy.
Even if no phonetict;ranscrit)tion is used, most transcription eonven-lions support the use of modified orthography tocapture typical features of st)oken language (suchas gem instead of going, kinda instead of kindof, etc.).
Thus, the application of a written lan-guage tagger to spoken language typically requiresa special lexicon, mapt)ing spoken language vari-ants onto their canonical written language forms,in addition to a special tokenizer.The problems considered so far may be seenas problems of a practical nature, but there isalso a more filndmnentat problem with tile useof written language statistics to analyze spokenlanguage, namely that the probability estimatesderived from written language may not be rcp-resentative for spoken language.
In the extremecase, some st)oken language phenomena (such ashesitation markers) Inay l)e (nearly) non-existent;in written language.
But even for words and collo-cations that occur both ill written and ill spokenlanguage, t;he occurrence probabilities may varygreatly between tile two media.
How riffs affectsthe performance of taggers and what methods canbe use(l to over(;olne or circunlvent ile I)rol)lemsm-e issues that, surprisingly, do not seem to havet)een discussed in the literature at all.
The I)resentpaper can be seen as a first attempt o ext)lore thisarea .2.3 Tagging SwedishAs far as we know, the methods for mltomaticpart-of-speech tagging have not before been ap-plied (;o (transcribed) spoken Swedish.
For writ-ten Swedish, there are a few tagge, d corpora avail-at)le, such as the Teleman tort)us (see, e. g.,(Brants &, Samuelsson 1995)) and the Stockholnl-Urneh Corpus (Ejerhed et al1992).
A subpart oftim latter has been used as training dal;a in theexperiments reported t)elow.3 Method3.1 The  TaggerThe tagger used fl)r tile experiments i  a standardItMM tagger using tile Viterbi algorithm to calcu-late the most probable sequence of parts-ofspee(:hfor each string of words actor(ling to the followingprol)al)ilistic t)iclass modeh(1) l '(,, ,1,... , , , , ,~,t,,.. .
,~,,,) =P(t,)P(w, Itt)II'?~ 2 P(t i l  t/--1)I)(WJ I*,~)The tagger is coupled with a tokenizer that seg-ments a transcription into utterances (strings ofwords), that are fed to the tagger one by one.
Be-sides ordinary words, the utterances may also con-tain markers for pauses and inaudibh: stretches ofspeech.
~3.2 Tra in ing  the  TaggerTile lexical and contextual probabilities were esti-mated with relative frequencies ill a tagged corpusof written Swedish, a subpart of the Stockholm-Ume'?
Cortms (SUC) containing 122,377 word to-kens (1.8,343 word types).
Tile tagset included 27parts-of-speech.
32Tile original transcriptions also contain inibrma-tion about overlapping speech, marking of certain as-pects of prosody, and various colninmlts.
This infor-mation is currently disregarded by the tokenizer.3For a lnore detailed description of the linguisticannotation system of the Stockhohn-Ume?
Cort)us,see (Ejerhed et al1992).10793.3 The Spoken Language LexiconAs noted earlier, the spoken language transcrip-tions contain many deviations fl'om standard or-thography.
Therefore, in order to inake optimaluse of tile written language statistics, a speciallexicon is required to map spoken language vari-ants onto their canonical written forms.
For thepresent experiments we have developed a lexiconcovering 2113 spoken language variants (which aremapped onto 1764 written language forms).
Weknow, however, that this lexicon has less than to-tal coverage and that many regular spoken lan-guage reductions are not currently covered.
43.4 Unknown Words  and  CollocationsThe occurrence of "unknown words", i. e., wordsnot occurring in the training corpus, is a notoriousproblem in (probabilistic) part-of-speech tagging.In our case, this problem is even more serious,since we know beforehand that some words willbe treated as unknown although they do in factoccur in the training corpus (because of deviationsDom standard orthography).
In the experimentsreported below, we have allowed unknown wordsto belong to any part-of-speech (which is possiblein the given context), but with different weight-ings for different parts-of-speech.
More precisely,when a word cannot be found in the lexicon, wereplace the product in (2) (cf.
equation 1 above)with the product in (3), where TTR(ti) is the type-token ratio of ti (in the training corpus).
(2) p(t  I I td(3) P(t{ I t{_l)P(ti)TTR(t{)In this way, we favor parts-of-speech with highprobability and high type-token ratio.
In practice,this favors open classes (such as nouns, verbs, ad-jectives) over closed classes (determiners, conjunc-tions, etc.
), and more frequent ones (e. g., nouns)over less frequent ones (e. g., adjectives).In addition to "unknown words", we have todeal with "unknown collocations", i. e., biclassesthat do not occur in the training data.
If these bi-classes are simply assigned zero probability, thenin tile extreme case a word which is in thelexicon may fail to get a tag because the contex-tual probabilities of all its known parts-of-speechare zero in the given context.
In order to preventthis, we use the following formula to assign con-textual probabilities to unknown collocations:(4) P(ti l t{_l) = P(t i )KThe constant K is chosen in such a way that tilecontextual probabilities defined by equation (4)are significantly lower than the "real" contextualprobabilities derived from the training corpus, so4A common example is the ending -igt, which ap-pears in many adjectives (neuter singular) and adverbsand which is usually reduced to -it in ordinary speech.that they only come into play when no known col-location is possible.3.5 Pauses and Inaudible SpeechAs indicated earlier, the utterances to be taggedincluded markers for pauses and inaudible speech,since these were thought to contain informationrelevant for tile tagging process.
The symbol forinaudible (and therethre untranscribed) speech( .
.
. )
-was  simply added to the lexicon andassigned the "t)art-of-speech" major de l imi ter(mad), which is the category assigned to full stops,etc.
in written texts.
The result is that the tag-ger will not treat the last, word before tile untran-scribed passage as immediate context for tile firstword after tile passage.For pintoes we have experimented with two dif-ferent treatments, which are compared below.
Werefer to these different reatments as tagging con-dition 1 and 2, respectively:?
Condition 1: Pauses are simply ignored in tiletagging process, which means that the lastword before a pause is treated as immediatecontext for the first word after the pause.?
Condition 2: Pause symbols are added to thelexicon, where short pauses are categorizedas minor de l imi ters  (mid) (commas, etc.
),while long pauses are categorized as mad (fllllstops, etc.
), which means that the contextualprobabilities of words occurring before andafter pauses in spoken language will be mod-elled on the probabilities of words occurringbefore and after certain punctuation marks inwritten language.It was hypothesized that, in certain cases, the tag-ger might perform better under condition 2, sincepauses in spoken language often though by nomeans always indicate major phrase boundariesor even breaks in the grammatical structure.3.6 Test CorpusThe test corpus was composed of a set of 47 ut-terances, chosen randomly from a corpus of tran-scribed spoken Swedish containing 267,206 words.The utterance length varied from 1 word to 688words (not counting pauses as words), with amean length of 29 words.
The test corpus con-tained 1360 word tokens and 498 word types.4 Resu l tsThe number of correctly tagged word tokens undercondition 1 was 1153 out of a total of 1360, i. e.,84.8?./o.
The results for condition 2 were slightlybetter: 1248/1457 = 85.7%.
However, the latterfigures also include the tagged imuses, for whichonly one category was possible.
If these tokensare subtracted, the results for condition 2 are:1151/1360 = 84.6%.10805 D iscuss ionThe overall ac(:uracy rate for the I,agger is al'Omld85%, which is not too imi)ressive wh(m (:oInl)m'e(|to the results reporte, d for writt;cn laitguage.
How-ever, if we take a closer look at the results, it; seemsthat an imt)ortant source of error is the lack of cov-erage of the, lexicon m,t  the training corpus.
Ofthe |;we lmndred or so errors made 1)y the tagger,more than eighty con(:ern tokens that could notbe matched with any word form occurring in thetraining corpus.
The most; common tyt)e of errorin this class is that a word is (~rroneously tagge, das a noun.
\[t is likely that this is an artifact ofthe way we assign lexical prol)abilities to unknownwords and that a more Sol)histi(:ated method maylint)rove the results for this class of words.
Moreimportantly, though, if we only (:oilsi(ler the re-suits for words that were known to the tagger, theaccuracy rate goes up to about 90%, mid most ofthe errors relnailfii~g concern classes that are noto-riously difficult even un(ter norlnal cir(:umstmLces,such as adverbs vs verb particles and prepositionsvs sut)ordinating conjunctions.
Taken togedmr,these results seen~ to indicate that with a moree.xtensive lexicon, a larger training corpus of writ-ten language, and l)erhat)s a more sot)histi(:atedtreatment of mtknown words, it should |)e possi-ble to el)Cain results al)proa<',hing those, ()I>taine<lfor written language.As regards the two treatments ()\[' \[)allses, theresults are virtually identi(:al in terms of overallaccuracy rate.
If we look at individual words,however, we find that the part-of-st)eech assign-illellt differs in 25 cases, hi 10 of these (:ases,the corrc(:t part-of-st)eech is assigned under con-dition 1; in 9 cases, the corre, ct ttLg is tbund under(:ondition 2; ittl(t in 6 cases, l)oth conditions yieldan incorrect assignlnent.
The conclusion to drawfrom the.se results is i)robably that the.
tre&tmcntof pauses as delimiters yields it t)etter analysis incases where the pause, marks an interruption ormajor phrase t)omldary, while it is better t() ig-nore pauses when they do iloi-, mark any break ingrmnlnatical structure.
Unfortunately, these twotyl)eS of t)auses eem to 1)e equally (:ommon, whi(:hmeans that neither treatment results in any gainin overall accuracy.
However, preliminary obser-vations seem to in(ticate thai, it may be possibleto get better results if a more line-grained analysiso\[" t)ause length is taken into account.
This pre--supposes, of course, that lifts kind of informal;ionis available in the transcriptions.6 Conc lus ionin this I)aper we, have ret)orted on an experiinentusing a probabilistic part-of-speech tagger trainedon written language to analyze (transcril)ed) spo-ken language.
The results indicate that, with littleor no adaptations, an overall accuracy rate of 85o/oc:ml 1)e a,chio, vcd, with ~1,i1 tC(;llFO, cy r;~te of 90% \['()rknown words.
()n the negative side, we, found thatthe treatment of pauses as delimiters (a,s ot)t)osedto siml)ly ignoring them) did not result in a 1)ctlx!rperformance, of the tagger.ReferencesMeriMdo, B.
(71995) Tagging English Text with aI'robabilistic Model.
Uomp'utatio'nal Linguistics20, t55 \[71.lbmm, L. E. (1972) An \[no, quality and AssociatedMaximizm;iou Technique, in Statistical \[';sLima--lion for Probabilistic lqm(:tions of it MarkovProcess.
l;ncqualil, es 3~ \] 8.Brants, T. & Samuelsson, C. (1995) Tagging theTeleman Corl)uS.
In I'rocccdin9 s of the lOI, hNordic Conference of Computational Lin quis-tics, NODALLDA-95, flelsinki, 7 20.lbill, E. (1.992) A Simple Rule-based Part ofSl)eech Tagger.
In 7'h, ird Conference of AppliedNatural Lan.quage l~roccssing, ACL.lbo(hla, 13.
(11982) Problems with T~l,gging and aSolution.
Nordic .lowrnal of Lin q'n, istics 5, 93I 16.Chanod, 3.-P. & Tapanahmn, P.
(19!
)5)Tag-zing l,'rench Coinparing a Statistical anda Constraint-t)ased Method.
In Seventh Con-flerc'nce of the Europeo, n Ch, aptcr" of the Asso-ciation for Computational Lin.quistics, l)ublin,149 \]56.Charniak, E.
(1!
)93) Statistical Language Lea'rn..rag.
Cambridge, MA: MIT I'ress.Cutting, l)., Kupiec, J., Pedersen, I.
82 Sibun,P.
(1992) A Practical Part-of-st)(!ech Tagger.
InTh, ird Confc.,vncc on Applied Natural Language.Processing, ACL, 133 140.DeRose, S. ,1.
(1988) Grammati(:al Cat(gory l)i-amt)iguation t)y Statisti(;al Optimization.
Co'm,-putational Anguistic.s 14, 31 39.i.\]jerhe(l, E., Kgllgren, G., Wennstedt, O.
)~strSm, M. (I992) The Linguistic AnnotationSyst(:m of (;he Stockholm-Ume/~, Corpus Project.Report 33.
University of Ume?
: Department ofLinguistics.Garside, R., Using (\]LAWS to Annotate thelh'itish National Corpus.
\[httt)://info.ox.ac.uk:80/bnc/garside_ alk:.html\].Kartsson, F. (1990) Constraint Grmnmar as a Sys--tern for Parsing l{,unning Text.
In Procecdings ofCOLING-90, Helsinki, 168 173.Koskenniemi, K. ( \[990)Finite-state Parsing an(lI)isambiguation.
In Proceedings of CO I, ING-00, lIelsinki, 229 232.1081
