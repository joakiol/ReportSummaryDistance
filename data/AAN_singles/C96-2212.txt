Hierarch ica l  C lus ter ing  o f  WordsAkira Ushioda*ATR h:lt(;rth'e.1;h:lg T(;l('(:omnnlni(;&l;ions Res('ax(:.h I,M)ort~tories2-2 ltika,ridM, S(;ika-(:ho, SorMcu-gun, Kyoto, ,\]a,l:)~m 6\] 9-02ema,i\]: ush ioda(a) i t l ,  a i r .
co .
jpAbst ractThis plq)er (lescril)es a (hit i~-(trivennlet, hod for hiera, rchicM chlstering ofwords ill whicii a, la, rge vo(:aJ)ul~ry of I,;ii.glis\]'l words is (:histered botl;oln--uf) >withresl)e(:t 1,o (:orpor;~ ranghig in size fi'otn5 to 50 nlillion wor(ts, using a greedy algorithm that I;ries I,o nliniluize i~veri~gelOS8 Of liCllltllal iriforuu:l,l, ion of a, djax:entclasses.
The resulting hierar('.hi('al (:illS-tiers of woMs are then tumirMly 1,rans-rorlned to a bit-string representld, ion of(i.e.
word bits for) all the words ill the vo-cabulary, Introducing wor(l bits hito i.heATI{ I)ecision-Tree DOS Tagger is shownto signific~mt,ly reduce l, he ti~gging errorrld;e. PortM)ility of word t)il.s h:om Olle(tonlMn to i~Hotilel: iS ~tlSO diss(:ussed.1 I n t roduct ion() l ie of bile fulida, rlrient~J issues concernhlg corpus--l)ased NI,P is t;he (tmLa 8I)a, rsetless prot)len'l.
Inview of the eft'e(',tiveliess of class-ha,seal ll-gl'a, lll \]<%ll--gllage nlodels i~gMnst Lhe (\]~ta s\]7)i~l'Seliess i)rol)lenl(Kneser iLli(l Ney 1993), it; is expected t;l-li~t classesof words are Mso usefiil for NI,P tasks ill such awi~y that statistics oil (:\]~sses ;tre used wheneverstal;istics oil individua, l words il, i'e una,vaihdlle orunreli&i)le.
Al l  ide, al type of clusi, ers for N I,P isthe ()lie which gu;tra, rltees in ut iia\[ substitu I;M)ilit, y,ill tern'is ()f t)oth synl;a,ctic a, ud selt i lult ic SOUll(l-lleSs, &lnOllg words in the sa, rtle class.Furthermore, chlstering is nnl(:h more iiseful ifthe clusl;ers i~i'e of vnriMJe grmnllarity, or hierar--chi('al.
We will consider i~ tree represent~tl, ion ofMI the words in t,he vocM)uh~ry in which the root;node l:ei)resenl;s the whole vo(:i~l)uli~l'y i~lltl ~ le~fllOde rel)rese\[lt;S a, word ill the voclJ)llli~ry.
Also,~.uiy set of nodes in I;ile tree constil, utes ~ i)m:t,i-tion (or cluslx)ring) of the vo(:M)ulary if t;here ex-ists one i%ll(I only Olle l lode iu i, lle seL ,-%lollg thep{~th from the root node ix) ei~(:}l eiff node, In thefollowing sectk)n<% we will describe i~ nletl iod Orcrea, th'lg bim~ry tree represeuti~l;ion ()f wor(|s a, udpresent restllts of ev;tlua,tiilg a, nd conll)aring thequalii;y of i;he hierarchi(:M clusters ot)tMne(I fronltexl, s ()r W.q:y dilTerent sizes.
*Calrrellt a.
(hlress: Me(lbt lutegr~ttion I,al}or;ttory,Fujitsu L;tboral, ories I,td., Ka.w~tsMd, .\]~tpa.n.
gtil~til:ushiod a(~gfl~d).fiiji tsu.
(:o.j p.2 Word Bi ts  Const ruct ionOur word bits coiJstruction <~lgorMlm is ;~ lno(ti-flotation mid mi extension of the mutual infornm.l, ion chistering Mgorithm proposed })y l}rown etill, (1992).
We will first il lustrate the dilTereltcebetween file original rormuh~e iul(t the oues weused, lind theft introduce the word bits co,.st.ruc-tion Mgorithni.
We will use the same no(.aA;ion ;ksill I lrown et M. to tm.Lke the conll);trison e;~sier.2.1 Mutua l  In f i ) rmat lon  C lus t ( ; rh igA lgor i thmMutuM information chlstering niethod enlploys at)ottuni-up merging t)roce,(hire with the ~wel'i%gelllUl.ll&\] illfOrlrllttioll (AMI) or ;sit,cent.
classes inthe text ms an o\[)jective hmction.
In the iltitialsta, ge, er~(:h word in the vocM)ul<'u'ly of size V isi~ssig.,e(l to il;s own (listii,(:t class.
We then inerge(,wo cla.sses if die merging of {;hem induces \ [ .
i , ,immn AMI reduction arllong all pMrs of classes,ttll(\] we rei)e~d; the nlergit lg 8(,e f) unti l  {,tie Numl)erof the (:lasses is reduced to the pre(leliiied nuni-t)er C,.
T ime colitplexity or this basic algorithnl isO(V 5) when iinph;rueui,ed sl, rMglitforwardly, l}ystoring the resu\]\[, of all the trim nierges ~(, (,lie pre-vious inerging step, however, the tinie coniplexitycall be reduced to O(V 3) ;~s shown t)elow.Suppos(; dia.t, stm'thig with V ch~sses, we haveMre;uiy made V - k nlerges, lelwing k (:lasses,(.:~.
(J), (:~(2), .. , c.'~(,:)_ The AMI i~t, ~his sti~geis given by the rollowillg e(llmtions.I~ : ~ q~(<',,,,) (J)q~(l, m) pk(l,',.
)log p~:(l,.,) (:~)pl~(l)pr~ (m)where p/~(l,m) is the probM)ility that a woM iN(,'~(1) is followed l)y i~ word ill C~(m), midpl~(l) : ~_f~l)~(l,,n), pr, : (m)-= ~_~l,~:(l,,n).tr~In equ;-~tion \[, qh's ;~re sunlrHed over l, he entire k Xk (:lass bigrlun table ill which (l,lu) cell rel)reseiltsqx+,(f,m), hi this irlerging step we invesi;igate atrial merge or c'~(i) mM (:~(j) tbr MI (:h~ss pairs(i, j),  lind con,pure the AMI reduction L~(i, j)I~: - l~:(i,j) efre(:i~ed by this .~erge, where l~:(i,j)is tile AMI  aft;or the lilertre,.Suppose that the I);dr (Cx:(i),C);(j)) was chosen to merge, thai.
is, l ,~(i,j) ~ L~,(l,m) for M11\ ] .59pairs (l,m).
In the next merging st.el) , we haveL~;'J)tl m) for all the pairs (l,m).
to cMculate -1~ ,ltere we use the superscript (i, j )  to indicate that(Ck (i), Ck (j)) w as merged in the previous mergingstep.Now note that the difference be-tween L (i'j)(l m) and L~(l,m) only comes fronl k-1  ~"the terms which are affected by mergiug the pair(C~(i),C~:(j)).Since L~.
(l, rn) = I~-1~(I, m) and L"'J)(l m) = k , - l~  ,- we have- -  , - -  ( l ( i ' J )  - -  l k  ) ,  m)) +Some part of the summation region of I~'j~)(l, ,n)and I~ cancels out with a part of l~i;~ ) or a partof a(t,.,).
Let "0, i (t .0, i andi~ denote the values of l~iLJ)(l, rn),lt:(l,m),l~i'_J 1)and I~, respectively, after all the common termsamong theln which Call be canceled are canceledout.
Then, we haveL(i'J)tl m)-- Lk(l,m) =k- lk  ,where\[(i'J)ll m)k-1  ~ ' = q~-l(l+ rn,i) + qk-l( i , l+ m)= q~( l+m, i )+q~( i , l+m)+q~(l + re, j) + q~(j,l + m)= q~_l(i,l) + q~_l(i,m) +qk-l(l, i) + qk-l(m, i)= qx:(i,l) + q~(i,m) + q~(j,l) +q~(j, rn) + qk(l,i) + qk(l,j) +q~(m, i) + qk(rn,j)Because quation 3 is expressed as the summationof a fixed number of q's, its value can be cMculatedin constant ime, whereas the cMculation of e(tua-tion 1 requires O(V 2) time.
Therefore, the totaltime complexity is reduced by O(V~).The summation regions of I's in equation 3 areillustrated in Figure 1.
Brown et al seem to haveignored the second term of the right hand side ofequation 3and used only the first term to calculateL~i,J~(l,m)_Lk(l,m) 1.
However, since thesecoudterm has as much weight as the first terln, we usedequation 3 to mgke the model complete.Even with the O(V a) algorithm, the calculationis not practical for a large vocabulary of order 10 4or higher.
Brown et al proposed the following1A(:tually, it is the first term of equation 3 times(-l) that appeared in their paper, but we believe thatit is simply due to a misprint.i j 1 mJ ::::::::::::::::::::::::::::::,iJl+ni j l+m::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::il il::::::::::::::::::::::::::::\[i !!
\[iik ik(Lm)========================== ....:::::::::::(~) l+m k-I1 ::::::::::::::::::::::::::::::k-it iii~,j!l ~'(i,j) : l  In  _. Ik_ D, ~ )Figure 1: Summation Regions for l'sMerging History: DendrogramMerge(A, B -> A) :=>Merge(C, D -> c) __~__Merge(A, C -> A)Merge(X,Y->Z) reads (A"merge X and Y and namethe new class as Z"Figure 2: I)endrogram Constructionmethod, which we also adopted.
We first make Vsingleton classes out of the V words in the vocab-ulary and arrange the (:lasses in descending orderof frequency, then define the merging region as thefirst C+ l positions in the sequence of classes.
Ateach merging step, merging of only the (:lasses inthe merging region is considered, thus reducingthe number of trial merges from O(V 2) to O(C'~).After each actual merge, the most frequent single-ton class outside of the merging region is shiftedinto the region.
With this algorithm, the timecomplexity is reduced to O(C ~ V).2.2 Word  B i t s  Const ruct ion  Algor i thmThe simplest way to construct a tree structuredrepresentation f words is to construct a dendro-gram from the record of the merging order.
A sim-ple example with a five-word vocabulary is shownin Figure 2.
If we apply this method to the aboveO(C'2V) algorithm, however, we obtain for eachclass an extremely unbalanced, Mmost left branch-ing subtree.
The reason is that after classes in themerging region are grown to a certain size, it ismuch less expensive, in terms of AM1, to merge asingleton class with lower frequency into a higherfrequency class than merging two higher frequencyclasses with substantiM sizes.A new approach we adopted is as follows.1.
Ml-clustering: Make C classes using the mutualinformation clustering algorithm with the merging1160l"ORJFigure 3: Sanlple Sub(.ree for One (:lassregion constraint mentioned in (2.1).2.
Outer-clustering: Replace all words in the textwith their class token 2 and execute binary merg-ing without l;lle merging region constraint until allthe classes are merged into a singe (:lass.
Make adendrogram out of this process.
This dendrograrn,1),.oo?, constitutes the upper part of the final tree.a.
l,),~.,.-,:~,,s~.,.i,,,j: Let {C(I), C(2),..., c(c)} ))ethe set of the classes obtained at, step l. l,'or eachi (1 < i < C) do the following.
(3) Replace all words in the text except those inC(i) with their <:lass token, l)efine a new vocabu--lary V' = V1 U V> where V1 = {all the words in( \ ] ( i )} ,  V 2 = {C' l , ( \ ]2 , .
.
.
,C .
, i _ l ,C ' i+ l ,C ,c}  , and 65is a token for (:(j) for I < j < C. Assign eachelement in V' to its own class and execute binm:ymerging with a merging constraint such (,ha(, onlythose classes which only contaiu elements of Vlcan be merged.
(b) Repeat merging until all the eletuents in VIare i)ut in a single (:lass.Make a dendrogrmn l).~,d~ out of the merging pro-tess for each class.
This (teudrogram coust, itutes asubtree for each (:lass with a leaf node rel)resent-ing each word in the class.4.
Combine tile dendrograms by substituting eachleaf node of l)root with coresponding l),,LbThis algorithm produces a b,~lanced binary treerepresent;ation of words in which (,hose wordswhich are close in meaning or syntactic featurecome close in posit, ion.
Figure 3 shows an exam-pie of l).,~b for orle class out of 500 (:lasses con-structed using this algorithm wit|) a vocabularyof the 70,000 most; frequently occm:ring words inthe Wall Street; Journal Corpus.
Finally, by trac-ing the path from the root node to a leaf node audassigning a bit to each bra, uch with zero or one rep-resenting a left or right branc\]b respectively, wecar, ass ign  a bit-string (word bits) to each word inthe vocabulary.~\]n the actuM implement~ttion, we only htwe towork on the bigr~ml t*Lble instead of tim whole text.Event- 128:{( wo,'cl(O),"like" } { wore\]{-1).
"flies" } ( WOl'd(-2),"ti,nt," }W()l'( l( l), l l \[ l |/" } ( .
.
.
.  '
( | (~),  u{t\['I'()VVu }tag(d),"Ve|.b-ard-Sg-lype3"} ( tag( 2),"Nou,,-Sg typeld" ).
.
.
.
.
.
.
.
(Basic Questions)Inclass?
(word(0), Clasu295), "yes" }Wo|'dBits(~cVord(-1), 29), "1" }.
.
.
.
.
.
.
.
( V~rOl'Cl Bi, s Quesl;ions)IsPrefix?
(Word(0), "anl;i"), "no" }.
.
.
.
.
.
.
.
(Linguist's Qt|estions )Tag, "Prep-type5" } }Figure 4: Examt)le of a.n event3 Exper imentsWe used phdu texts from six years of tile WSJC, ort)us to create word bits.
The sizes of tile textsare 5 million words (MW), t0MW, 20MW, and50M W. '|'he vocabulary is selected as the 70,000most; fl:eqneutly occurring words in the entire co>pus.
We set the number C of <:lasses to 500.The obtained hierarchical clusters are ewdua.tedvia the error rate of the ATI{ l)ecision-Tree Part--Of-Speech Tagger which is based on SPAT'\['I,;t{(Magerman 199,1).
The tagger employs a set of443 syntactic tags.
In the training phase, a set ofevents are extracted from the training texts.
Anevent is a set of feature-value pairs or question-answer pairs.
A feature can be any attribute ofthe context in which the current word word(O) ap-pears; it is conveniently expressed as a question.Figure 4 shows an example of an evetlt, with a cur-rent word "like".
The last \[)air in the event is aspecial item which shows the answer, i.e., the co lrect tag of the current word.
The first three linesshow questions about identity of words around tilecurrent word and tags for previous words.
Thesequestions are cMled basic que.slio~,s and alwaysused.
The second type of questions, word bitsquestions, are on clusters and word bits such aswhat is the 29th bit of the previous word's wordbits?.
The third type of questkms are cMled lin-gui.sl's questiona nd these are compiled by an ex-pert grmlmmrian.Out of the set of events, a decision tree isconstructed whose leaf nodes contain conditionMprobability distributi(ms of tags, conditioned bythe feature values.
In tile test phase the systemlooks up conditionM probability distributions oftags R)r eat:l, word in the test text and chooses themost probable tag sequences using beam search.We used WSJ texts and the ATI{ cor\[ms (lllacket al 1996) for the tagging experiment.
Both co lpora use the ATR syntactic tag set.
Since theATR corpus is still in the process of development,the size of the texts we have at hand for this ex-periment is rather ndnimal considering tim largesize of the tag set.
Table 1 shows the sizes of textsused for the experiment;.
Figure 5 shows the t;ag-ging error rat;es plotted against various clustering1161~/  Text: wsJ Text261 ?
WordBits0 Lh~llQ~t & WordBits24I\ Text: ATR Corp:20~ Reshuffled (WSJ Text)= \]\ '~ E1 WordBits16 ...... "~ ..........10  \[ d5 xS10 20 30 410 ~0Clustering Text Size (Million Words)l,'igure 5: Tagging Error Rate60Text Size(words) Training q'est Ileht-OutWSJ Text 75,139 5,831 6,534"ATR Text 76,132 23,163 6,68"0Table 1: Texts for Tagging Experimentstext sizes.
Out of the three types of questions, ba-sic questions and word bits questions are alwaysused in this ext)eriment. '
lb see the effect of in-troducing word bits information into the tagger,we performed a separate xperiment in which arandomly generated bit-string is assigned to eachword 3 and basic questions and word bits questionsare used.
The results are plotted at zero clusteringtext size.
For both WSJ texts and ATR corpus,the tagging error rate dropped by more than 30%when using word bits information extracted fromthe 5MW text, and increasing the clustering textsize further decreases the error rate.
At 50MW,the error rate drops by 43%.
This shows the ira:provement of the quality of the hierarchical clus-ters with increasing size of the clustering text.
InFigure 5, introduction of linguistic questions 4 isalso shown to significantly reduce the error ratesfor the WSJ corpus.
The dependency of the er-ror rates on the clustering text size is quite sin>liar to the ea.se in which no linguistic questionsare used, indicating the effectiveness of combin-3Since a distin<:tive bit-string is assigned to eachword, the tagger also uses a bit-string as an ID numberfor each word in the process, In this control experi-ment bit-strings are assigned in a random way, but notwo words are assigned the same word lilts.
Randomword bits are expected to give no class information tothe tagger except for the identity of words.4The linguistic questions we used her(.'
are still inthe initial stage of development and are by no meanscomprelmnsive.ing automatically created word bits and hand-crafted linguistic questions.
Figure 5 also showsthat reshuming the classes everal times just afterstep I (MLclustering) of the word bits construc-tion process filrther improves the word bits.
Oneround of reshuffling corresponds to moving eachword in the vocabulary from its original (:lass toanother class whenever the movement increasesthe AMI, starting from the most frequent wordthrough the least frequent one.
The figure showsthe error rates with zero, two, and five roundsof reshufi\]ing 5.
Overall high error rates are at-tributed to the very large tag set; and the smalltraining set.
Another notable point in the figure isthat introducing word bits constructed from WSJtexts is as effective for tagging Aq'R text.s as it isfor tagging WSJ texts even though these texts arefrom very different domains.
To \[;hat extent, theobtained hierarchical clusters are considered to beportable across domains.4 ConclusionWe presented an algorithm for hierarchical <:has:tering of words, and conducted a clustering exper-iment using large texts of:varying sizes.
High qtml-ity of the obtained clusters are confirmed by thePOS tagging experiments.
By introducing wordbits into the ATR l)ecision-Tree POS Tagger, thetagging error rate is reduced by up to 43%.
Thehierarchical clusters obtained fi'orn WSJ texts arealso shown to be usefld \['or tagging ATR textswhich are fi'om quite different domMns than WSJtexts.AcknowledgementsWe thank John Lafferty for his helpful suggestions.ReferencesBlack, E., Eubank, S., Kashioka, H., Magerman, D.,Garside, R., and Leech, G. (1996) "Beyond Skelc-ton Parsing: Producing a Comprehensive Large-ScaleGeneral-English Treebank With Full GrammaticalAnalysis".
Proceedings of the 16th International C'on-ference on Computational Linguistics.Brown, P., Della Pietra, V., deSouza, P., Lai, J., Mer-cer, R. (1992) "Class-Based n-gram Models of NaturalLanguage".
Computational Linguistics, Vol.
18, No 4,pp.
467 479.Kneser, R. and Ney, H. (71993) "hnproved ClusteringTechniques for C, lass-Bascd Statistical Language Mod-elling".
Proceedings <>f European C, onfl'n'cnce on Spee<:hCommunication and Technology.?
Magerman, D. (1994) Nabural Language Parsing asStatistical Pattern Recognition.
Doctoral dissertation.Stanford University, Stanford, California.~'The vocabulary used for tile reshuffling experi-ments is the one used for a preliminary e.xperimcntand its size is 63850.1162
