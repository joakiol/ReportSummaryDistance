Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203?1213,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNeural Text Generation from Structured Datawith Application to the Biography DomainRe?mi Lebret?EPFL, SwitzerlandDavid GrangierFacebook AI ResearchMichael AuliFacebook AI ResearchAbstractThis paper introduces a neural model forconcept-to-text generation that scales to large,rich domains.
It generates biographical sen-tences from fact tables on a new dataset ofbiographies from Wikipedia.
This set is anorder of magnitude larger than existing re-sources with over 700k samples and a 400kvocabulary.
Our model builds on conditionalneural language models for text generation.To deal with the large vocabulary, we ex-tend these models to mix a fixed vocabularywith copy actions that transfer sample-specificwords from the input database to the gener-ated output sentence.
To deal with structureddata, we allow the model to embed wordsdifferently depending on the data fields inwhich they occur.
Our neural model signif-icantly outperforms a Templated Kneser-Neylanguage model by nearly 15 BLEU.1 IntroductionConcept-to-text generation renders structuredrecords into natural language (Reiter et al, 2000).
Atypical application is to generate a weather forecastbased on a set of structured meteorological mea-surements.
In contrast to previous work, we scaleto the large and very diverse problem of generatingbiographies based on Wikipedia infoboxes.
Aninfobox is a fact table describing a person, similar toa person subgraph in a knowledge base (Bollackeret al, 2008; Ferrucci, 2012).
Similar generationapplications include the generation of productdescriptions based on a catalog of millions of itemswith dozens of attributes each.Previous work experimented with datasets thatcontain only a few tens of thousands of records suchas WEATHERGOV or the ROBOCUP dataset, whileour dataset contains over 700k biographies from?Re?mi performed this work while interning at Facebook.Wikipedia.
Furthermore, these datasets have a lim-ited vocabulary of only about 350 words each, com-pared to over 400k words in our dataset.To tackle this problem we introduce a statisticalgeneration model conditioned on a Wikipedia in-fobox.
We focus on the generation of the first sen-tence of a biography which requires the model toselect among a large number of possible fields togenerate an adequate output.
Such diversity makesit difficult for classical count-based models to esti-mate probabilities of rare events due to data sparsity.We address this issue by parameterizing words andfields as embeddings, along with a neural languagemodel operating on them (Bengio et al, 2003).
Thisfactorization allows us to scale to a larger number ofwords and fields than Liang et al (2009), or Kimand Mooney (2010) where the number of parame-ters grows as the product of the number of wordsand fields.Moreover, our approach does not restrict the re-lations between the field contents and the gener-ated text.
This contrasts with less flexible strategiesthat assume the generation to follow either a hybridalignment tree (Kim and Mooney, 2010), a proba-bilistic context-free grammar (Konstas and Lapata,2013), or a tree adjoining grammar (Gyawali andGardent, 2014).Our model exploits structured data both globallyand locally.
Global conditioning summarizes all in-formation about a personality to understand high-level themes such as that the biography is about ascientist or an artist, while as local conditioning de-scribes the previously generated tokens in terms ofthe their relationship to the infobox.
We analyze theeffectiveness of each and demonstrate their comple-mentarity.2 Related WorkTraditionally, generation systems relied on rules andhand-crafted specifications (Dale et al, 2003; Re-iter et al, 2005; Green, 2006; Galanis and Androut-11203sopoulos, 2007; Turner et al, 2010).
Generation isdivided into modular, yet highly interdependent, de-cisions: (1) content planning defines which parts ofthe input fields or meaning representations shouldbe selected; (2) sentence planning determines whichselected fields are to be dealt with in each outputsentence; and (3) surface realization generates thosesentences.Data-driven approaches have been proposed toautomatically learn the individual modules.
One ap-proach first aligns records and sentences and thenlearns a content selection model (Duboue and McK-eown, 2002; Barzilay and Lapata, 2005).
Hierar-chical hidden semi-Markov generative models havealso been used to first determine which facts to dis-cuss and then to generate words from the predi-cates and arguments of the chosen facts (Liang et al,2009).
Sentence planning has been formulated as asupervised set partitioning problem over facts whereeach partition corresponds to a sentence (Barzilayand Lapata, 2006).
End-to-end approaches havecombined sentence planning and surface realiza-tion by using explicitly aligned sentence/meaningpairs as training data (Ratnaparkhi, 2002; Wong andMooney, 2007; Belz, 2008; Lu and Ng, 2011).
Morerecently, content selection and surface realizationhave been combined (Angeli et al, 2010; Kim andMooney, 2010; Konstas and Lapata, 2013).At the intersection of rule-based and statisti-cal methods, hybrid systems aim at leveraging hu-man contributed rules and corpus statistics (Langk-ilde and Knight, 1998; Soricut and Marcu, 2006;Mairesse and Walker, 2011).Our approach is inspired by the recent success ofneural language models for image captioning (Kiroset al, 2014; Karpathy and Fei-Fei, 2015; Vinyals etal., 2015; Fang et al, 2015; Xu et al, 2015), ma-chine translation (Devlin et al, 2014; Bahdanau etal., 2015; Luong et al, 2015), and modeling conver-sations and dialogues (Shang et al, 2015; Wen et al,2015; Yao et al, 2015).Our model is most similar to Mei et al (2016)who use an encoder-decoder style neural networkmodel to tackle the WEATHERGOV and ROBOCUPtasks.
Their architecture relies on LSTM units andan attention mechanism which reduces scalabilitycompared to our simpler design.Figure 1: Wikipedia infobox of Frederick Parker-Rhodes.
Theintroduction of his article reads: ?Frederick Parker-Rhodes (21March 1914 ?
21 November 1987) was an English linguist,plant pathologist, computer scientist, mathematician, mystic,and mycologist.
?.3 Language Modeling for ConstrainedSentence generationConditional language models are a popular choiceto generate sentences.
We introduce a table-conditioned language model for constraining textgeneration to include elements from fact tables.3.1 Language modelGiven a sentence s = w1, .
.
.
, wT with T wordsfrom vocabularyW , a language model estimates:P (s) =T?t=1P (wt|w1, .
.
.
, wt?1) .
(1)Let ct = wt?
(n?1), .
.
.
, wt?1 be the sequence ofn ?
1 context words preceding wt.
An n-gram lan-guage model makes an order n Markov assumption,P (s) ?T?t=1P (wt|ct) .
(2)3.2 Language model conditioned on tablesA table is a set of field/value pairs, where values aresequences of words.
We therefore propose languagemodels that are conditioned on these pairs.Local conditioning refers to the informationfrom the table that is applied to the description of thewords which have already generated, i.e.
the previ-ous words that constitute the context of the language21204Table (gf , gw)name John Doebirthdate 18 April 1352birthplace Oxford UKoccupation placeholderspouse Jane Doechildren Johnnie Doeinput text (ct, zct)John Doe ( 18 April 1352 ) is act 13944 unk 17 37 92 25 18 12 4zct(name,1,2) (name,2,1) ?
(birthd.,1,3) (birthd.,2,2) (birthd.,3,1) ?
?
?
(spouse,2,1)(children,2,1)output candidates (w ?
W ?Q)the .
.
.
april .
.
.
placeholder .
.
.
john .
.
.
doew 1 .
.
.
92 .
.
.
5302 .
.
.
13944 .
.
.
unkzw?
(birthd.,2,2) (occupation,1,1) (name,1,2) (name,2,1)(spouse,2,1)(children,2,1)Figure 2: Table features (right) for an example table (left);W ?Q is the set of all output words as defined in Section 3.3.model.
The table allows us to describe each wordnot only by its string (or index in the vocabulary)but also by a descriptor of its occurrence in the ta-ble.
Let F define the set of all possible fields f .
Theoccurrence of a word w in the table is described bya set of (field, position) pairs.zw ={(fi, pi)}mi=1 , (3)where m is the number of occurrences of w. Eachpair (f, p) indicates that w occurs in field f at posi-tion p. In this scheme, most words are described bythe empty set as they do not occur in the table.
Forexample, the word linguistics in the table of Figure 1is described as follows:zlinguistics = {(fields, 8); (known for, 4)}, (4)assuming words are lower-cased and commas aretreated as separate tokens.Conditioning both on the field type and the po-sition within the field allows the model to encodefield-specific regularities, e.g., a number token in adate field is likely followed by a month token; know-ing that the number is the first token in the date fieldmakes this even more likely.The (field, position) description scheme of the ta-ble does not allow to express that a token terminatesa field which can be useful to capture field transi-tions.
For biographies, the last token of the namefield is often followed by an introduction of the birthdate like ?(?
or ?was born?.
We hence extend our de-scriptor to a triplet that includes the position of thetoken counted from the end of the field:zw ={(fi, p+i , p?i )}mi=1 , (5)where our example becomes:zlinguistics = {(fields, 8, 4); (known for, 4, 13)}.We extend Equation 2 to use the above informa-tion as additional conditioning context when gener-ating a sentence s:P (s|z) =T?t=1P (wt|ct, zct) , (6)where zct = zwt?
(n?1) , .
.
.
, zwt?1 are referred to asthe local conditioning variables since they describethe local context (previous word) relations with thetable.Global conditioning refers to information fromall tokens and fields of the table, regardless whetherthey appear in the previous generated words or not.The set of fields available in a table often impactsthe structure of the generation.
For biographies, thefields used to describe a politician are different fromthe ones for an actor or an athlete.
We introduceglobal conditioning on the available fields gf asP (s|z, gf ) =T?t=1P (wt|ct, zct , gf ).
(7)Similarly, global conditioning gw on the available31205words occurring in the table is introduced:P (s|z, gf , gw) =T?t=1P (wt|ct, zct , gf , gw).
(8)Tokens provide information complementary tofields.
For example, it may be hard to distinguish abasketball player from a hockey player by lookingonly at the field names, e.g.
teams, league, position,weight and height, etc.
However the actual fieldtokens such as team names, league name, player?sposition can help the model to give a better pre-diction.
Here, gf ?
{0, 1}F and gw ?
{0, 1}Ware binary indicators over fixed field and wordvocabularies.Figure 2 illustrates the model with a schematic ex-ample.
For predicting the next word wt after a givencontext ct, the language model is conditioned on setsof triplets for each word occurring in the table zct ,along with all fields and words from this table.3.3 Copy actionsSo far we extended the model conditioning with fea-tures derived from the fact table.
We now turn tousing table information when scoring output words.In particular, sentences which express facts from agiven table often copy words from the table.
Wetherefore extend our model to also score special fieldtokens such as name 1 or name 2 which are sub-sequently added to the score of the correspondingwords from the field value.Our model reads a table and defines an output do-mainW?Q.
Q defines all tokens in the table, whichmight include out of vocabulary words (/?
W).
Forinstance Park-Rhodes in Figure 1 is not inW .
How-ever, Park-Rhodes will be included in Q as name 2(since it is the second token of the name field) whichallows our model to generate it.
This mechanismis inspired by recent work on attention based wordcopying for neural machine translation (Luong et al,2015) as well as delexicalization for neural dialogsystems (Wen et al, 2015).
It also builds upon olderwork such as class-based language models for dialogsystems (Oh and Rudnicky, 2000).4 A Neural Language Model ApproachA feed-forward neural language model (NLM) es-timates P (wt|ct) with a parametric function ??
(Equation 1), where ?
refers to all learnable param-eters of the network.
This function is a compositionof simple differentiable functions or layers.4.1 Mathematical notations and layersWe denote matrices as bold upper case letters (X,Y, Z), and vectors as bold lower-case letters (a, b,c).
Ai represents the ith row of matrix A. WhenA is a 3-d matrix, then Ai,j represents the vectorof the ith first dimension and jth second dimension.Unless otherwise stated, vectors are assumed to becolumn vectors.
We use [v1;v2] to denote vectorconcatenation.
Next, we introduce the notation forthe different layers used in our approach.Embedding layer.
Given a parameter matrixX ?
RN?d, the embedding layer is a lookup tablethat performs an array indexing operation:?X(xi) = Xi ?
Rd , (9)where Xi corresponds to the embedding of the ele-ment xi at row i.
When X is a 3-d matrix, the lookuptable takes two arguments:?X(xi, xj) = Xi,j ?
Rd , (10)where Xi,j corresponds to the embedding of thepair (xi, xj) at index (i, j).
The lookup table op-eration can be applied for a sequence of elementss = x1, .
.
.
, xT .
A common approach is to concate-nate all resulting embeddings:?X(s) =[?X(x1); .
.
.
;?X(xT )]?
RT?d .
(11)Linear layer.
This layer applies a linear trans-formation to its inputs x ?
Rn:??
(x) = Wx+ b (12)where ?
= {W,b} are the trainable parameterswith W ?
Rm?n being the weight matrix, andb ?
Rm is the bias term.Softmax layer.
Given a context input ct, thefinal layer outputs a score for each word wt ?
W ,??
(ct) ?
R|W|.
The probability distribution is ob-tained by applying the softmax activation function:P (wt = w|ct) =exp(??
(ct, w))?|W|i=1 exp(??
(ct, wi))(13)412064.2 Embeddings as inputsA key aspect of neural language models is the useof word embeddings.
Similar words tend to havesimilar embeddings and thus share latent features.The probability estimates of those models aresmooth functions of these embeddings, and a smallchange in the features results in a small changein the probability estimates (Bengio et al, 2003).Therefore, neural language models can achievebetter generalization for unseen n-grams.
Next, weshow how we map fact tables to continuous space insimilar spirit.Word embeddings.
Formally, the embeddinglayer maps each context word index to a continuousd-dimensional vector.
It relies on a parameter ma-trix E ?
R|W|?d to convert the input ct into n ?
1vectors of dimension d:?E(ct) =[?E(wt?
(n?1)); .
.
.
;?E(wt?1)].
(14)E can be initialized randomly or with pre-trainedword embeddings.Table embeddings.
As described in Section 3.2,the language model is conditioned on elements fromthe table.
Embedding matrices are therefore definedto model both local and global conditioning infor-mation.
For local conditioning, we denote the maxi-mum length of a sequence of words as l. Each fieldfj ?
F is associated with 2 ?
l vectors of d di-mensions, the first l of those vectors embed all pos-sible starting positions 1, .
.
.
, l, and the remaining lvectors embed ending positions.
This results in twoparameter matrices Z = {Z+,Z?}
?
R|F|?l?d.For a given triplet (fj , p+i , p?i ), ?Z+(fj , p+i ) and?Z?
(fj , p?i ) refer to the embedding vectors of thestart and end position for field fj , respectively.Finally, global conditioning uses two parame-ter matrices Gf ?
R|F|?g and Gw ?
R|W|?g.
?Gf (fj) maps a table field fj into a vector ofdimension g, while ?Gw(wt) maps a word wt intoa vector of the same dimension.
In general, Gwshares its parameters with E, provided d = g.Aggregating embeddings.
We represent each oc-curence of a word w as a triplet (field, start, end)where we have embeddings for the start and end po-sition as described above.
Often times a particularword w occurs multiple times in a table, e.g., ?lin-guistics?
has two instances in Figure 1.
In this case,we perform a component-wise max over the startembeddings of all instances of w to obtain the bestfeatures across all occurrences ofw.
We do the samefor end position embeddings:?Z(zwt) =[max{?Z+(fj , p+i ), ?
(fj , p+i , p?i ) ?
zwt};max{?Z?
(fj , p?i ), ?
(fj , p+i , p?i ) ?
zwt}] (15)A special no-field embedding is assigned towt whenthe word is not associated to any fields.
An embed-ding ?Z(zct) for encoding the local conditioning ofthe input ct is obtained by concatenation.For global conditioning, we define Fq ?
F as theset of all the fields in a given table q, andQ as the setof all words in q.
We also perform max aggregation.This yields the vectors?Gf (gf ) = max{?Gf (fj), ?fj ?
Fq}, (16)and?Gw(gw) = max{?Gw(wt),?wt ?
Q}.
(17)The final embedding which encodes the context in-put with conditioning is then the concatenation ofthese vectors:?
?1(ct, zct , gf , gw) =[?E(ct); ?Z(zct);?Gf (gf ); ?Gw(gw)]?
Rd1 , (18)with ?1 = {E,Z+,Z?,Gf ,Gw} and d1 = (n ?1)?
(3?
d) + (2?
g).
For simplification purpose,we define the context input x = {ct, zct , gf , gw} inthe following equations.
This context embedding ismapped to a latent context representation using a lin-ear operation followed by a hyperbolic tangent:h(x) = tanh(??2(??1(x)))?
Rnhu , (19)where ?2 = {W2,b2}, with W2 ?
Rnhu?d1 andb2 ?
Rnhu.4.3 In-vocabulary outputsThe hidden representation of the context then goesto another linear layer to produce a real value scorefor each word in the vocabulary:?W?
(x) = ??3(h(x))?
R|W| , (20)51207where ?3 = {W3,b3}, with W3 ?
R|W|?nhu andb3 ?
R|W|, and ?
= {?1, ?2, ?3}.4.4 Mixing outputs for better copyingSection 3.3 explains that each word w from the tableis also associated with zw, the set of fields in whichit occurs, along with the position in that field.
Simi-lar to local conditioning, we represent each field andposition pair (fj , pi) with an embedding?F(fj , pi),where F ?
R|F|?l?d.
These embeddings are thenprojected into the same space as the latent represen-tation of context input h(x) ?
Rnhu.
Using the maxoperation over the embedding dimension, each wordis finally embedded into a unique vector:q(w) = max{tanh(??
(?F(fj , pi))), ?
(fj , pi) ?
zw}, (21)where ?
= {W4,b4} with W4 ?
Rnhu?d, andb4 ?
Rnhu.
A dot product with the context vectorproduces a score for each word w in the table,?Q?
(x,w) = h(x) ?
q(w) .
(22)Each word w ?
W ?
Q receives a final score bysumming the vocabulary score and the field score:??
(x,w) = ?W?
(x,w) + ?Q?
(x,w) , (23)with ?
= {?, ?
}, and where ?Q?
(x,w) = 0 whenw /?
Q.
The softmax function then maps the scoresto a distribution overW ?Q,logP (w|x) = ??(x,w)?log?w??W?Qexp??(x,w?)
.4.5 TrainingThe neural language model is trained to minimizethe negative log-likelihood of a training sentence swith stochastic gradient descent (SGD; LeCun et al2012) :L?
(s) = ?T?t=1logP (wt|ct, zct , gf , gw) .
(24)5 ExperimentsOur neural network model (Section 4) is designed togenerate sentences from tables for large-scale prob-lems, where a diverse set of sentence types needto be generated.
Biographies are therefore a goodframework to evaluate our model, with Wikipediaoffering a large and diverse dataset.5.1 Biography datasetWe introduce a new dataset for text generation,WIKIBIO, a corpus of 728,321 articles from En-glish Wikipedia (Sep 2015).
It comprises all biogra-phy articles listed by WikiProject Biography1 whichalso have a table (infobox).
We extract and tok-enize the first sentence of each article with StanfordCoreNLP (Manning et al, 2014).
All numbers aremapped to a special token, except for years whichare mapped to different special token.
Field valuesfrom tables are similarly tokenized.
All tokens arelower-cased.
Table 2 summarizes the dataset statis-tics: on average, the first sentence is twice as short asthe table (26.1 vs 53.1 tokens), about a third of thesentence tokens (9.5) also occur in the table.
Thefinal corpus has been divided into three sub-partsto provide training (80%), validation (10%) and testsets (10%).
The dataset is available for download2.5.2 BaselineOur baseline is an interpolated Kneser-Ney (KN)language model and we use the KenLM toolkitto train 5-gram models without pruning (Heafieldet al, 2013).
We also learn a KN languagemodel over templates.
For that purpose, we re-place the words occurring in both the table andthe training sentences with a special token reflect-ing its table descriptor zw (Equation 3).
The in-troduction section of the table in Figure 1 looksas follows under this scheme: ?name 1 name 2( birthdate 1 birthdate 2 birthdate 3 ?deathdate 1 deathdate 2 deathdate 3 ) wasan english linguist , fields 3 pathologist ,fields 10 scientist , mathematician , mystic andmycologist .?
During inference, the decoder is con-strained to emit words from the regular vocabularyor special tokens occurring in the input table.
Whenpicking a special token we copy the correspondingword from the table.5.3 Training setupFor our neural models, we train 11-gram languagemodels (n = 11) with a learning rate set to 0.0025.1https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Biography2https://github.com/DavidGrangier/wikipedia-biography-dataset61208Model Perplexity BLEU ROUGE NISTKN 10.51 2.21 0.38 0.93NLM 9.40 +?
0.01 2.41 +?
0.33 0.52 +?
0.08 1.27 +?
0.26+ Local (field, start, end) 8.61 +?
0.01 4.17 +?
0.54 1.48 +?
0.23 1.41 +?
0.11Template KN 7.46?
19.8 10.7 5.19Table NLM w/ Local (field, start) 4.60 +?
0.01?
26.0 +?
0.39 19.2 +?
0.23 6.08 +?
0.08+ Local (field, start, end) 4.60 +?
0.01?
26.6 +?
0.42 19.7 +?
0.25 6.20 +?
0.09+ Global (field) 4.30+?
0.01?
33.4 +?
0.18 23.9 +?
0.12 7.52 +?
0.03+ Global (field & word) 4.40 +?
0.02?
34.7+?
0.36 25.8+?
0.36 7.98+?
0.07Table 1: BLEU, ROUGE, NIST and perplexity without copy actions (first three rows) and with copy actions (last five rows).
Forneural models we report ?mean +?
standard deviation?
for five training runs with different initialization.
Decoding beam width is 5.Perplexities marked with ?
and ?
are not directly comparable as the output vocabularies differ slightly.Mean Percentile5% 95%# tokens per sentence 26.1 13 46# tokens per table 53.1 20 108# table tokens per sent.
9.5 3 19# fields per table 19.7 9 36Table 2: Dataset statisticsParameter Value# word types |W| = 20, 000# field types |F| = 1, 740Max.
# tokens in a field l = 10word/field embedding size d = 64global embedding size g = 128# hidden units nhu = 256Table 3: Model HyperparametersTable 3 describes the other hyper-parameters.
Weinclude all fields occurring at least 100 times in thetraining data in F , the set of fields.
We includethe 20, 000 most frequent words in the vocabulary.The other hyperparameters are set through valida-tion, maximizing BLEU over a validation subset of1, 000 sentences.
Similarly, early stopping is ap-plied: training ends when BLEU stops improvingon the same validation subset.
One should note thatthe maximum number of tokens in a field l = 10means that we encode only 10 positions: for longerfield values the final tokens are not dropped but theirposition is capped to 10.
We initialize the word em-beddingsW from Hellinger PCA computed over theset of training biographies.
This representation hasshown to be helpful for various applications (Lebretand Collobert, 2014).5.4 Evaluation metricsWe use different metrics to evaluate our models.Performance is first evaluated in terms of perplex-ity which is the standard metric for language mod-eling.
Generation quality is assessed automaticallywith BLEU-4, ROUGE-4 (F-measure) and NIST-43 (Belz and Reiter, 2006).6 ResultsThis section describes our results and discusses theimpact of the different conditioning variables.6.1 The more, the betterThe results (Table 1) show that more conditioninginformation helps to improve the performance of ourmodels.
The generation metrics BLEU, ROUGEand NIST all gives the same performance orderingover models.
We first discuss models without copyactions (the first three results) and then discuss mod-els with copy actions (the remaining results).
Notethat the factorization of our models results in threedifferent output domains which makes perplexitycomparisons less straightforward: models withoutcopy actions operate over a fixed vocabulary.
Tem-plate KN adds a fixed set of field/position pairs tothis vocabulary while Table NLM models a variableset Q depending on the input table, see Section 3.3.Without copy actions.
In terms of perplexity the(i) neural language model (NLM) is slightly better3We rely on standard software, NIST mteval-v13a.pl (forNIST, BLEU), and MSR rouge-1.5.5 (for ROUGE).71209than an interpolated KN language model, and (ii)adding local conditioning on the field start and endposition further improves accuracy.
Generation met-rics are generally very low but there is a clear im-provement when using local conditioning since it al-lows to learn transitions between fields by linkingprevious predictions to the table unlike KN or plainNLM.With copy actions.
For experiments with copyactions we use the full local conditioning (Equa-tion 4) in the neural language models.
BLEU,ROUGE and NIST all improves when moving fromTemplate KN to Table NLM and more features suc-cessively improve accuracy.
Global conditioning onthe fields improves the model by over 7 BLEU andadding words gives an additional 1.3 BLEU.
Thisis a total improvement of nearly 15 BLEU over theTemplate Kneser-Ney baseline.
Similar observa-tions are made for ROUGE +15 and NIST +2.8.ll llll l l l l l l l100 200 500 1000 200015202530354045time in msBLEUlll l l l l ll l l l l1 2 34 5 6 8 10 15 20251 345 67 810 15 20 25lTemplate KNTable NLMbeam sizeFigure 3: Comparison between our best model (Table NLM)and the baseline (Template KN) for different beam sizes.
Thex-axis is the average timing (in milliseconds) for generating onesentence.
The y-axis is the BLEU score.
All results are mea-sured on a subset of 1,000 samples of the validation set.6.2 Attention mechanismOur model implements attention over input tablefields.
For each word w in the table, Equation (23)takes the language model score ?Wct and adds a bias?Qct .
The bias is the dot-product between a represen-tation of the table field in which w occurs and a rep-resentation of the context, Equation (22) that sum-marizes the previously generated fields and words.namebirthdatebirthplacenationalityoccupation1 2 1 2 3 1 2 1 1 2< s >nelliewong(bornseptember12,1934)isanamericanpoetandactivist.Figure 4: Visualization of attention scores for Nellie Wong?sWikipedia infobox.
Each row represents the probability distri-bution over (field, position) pairs given the previous words (i.e.the words heading the preceding rows as well as the currentrow).
Darker colors depict higher probabilities.Figure 4 shows that this mechanism adds a largebias to continue a field if it has not generated alltokens from the table, e.g., it emits the word oc-curring in name 2 after generating name 1.
It alsonicely handles transitions between field types, e.g.,the model adds a large bias to the words occurringin the occupation field after emitting the birthdate.6.3 Sentence decodingWe use a standard beam search to explore a largerset of sentences compared to simple greedy search.This allows us to explore K times more paths whichcomes at a linear increase in the number of forwardcomputation steps for our language model.
We com-pare various beam settings for the baseline TemplateKN and our Table NLM (Figure 3).
The best vali-dation BLEU can be obtained with a beam size ofK = 5.
Our model is also several times faster thanthe baseline, requiring only about 200 ms per sen-tence with K = 5.
Beam search generates many n-gram lookups for Kneser-Ney which requires many81210Model Generated SentenceReference frederick parker-rhodes (21 march 1914 ?
21 november 1987) was an english linguist, plantpathologist, computer scientist, mathematician, mystic, and mycologist.Baseline(Template KN)frederick parker-rhodes ( born november 21 , 1914 ?
march 2 , 1987 ) was an english cricketer.Table NLM+Local (field, start)frederick parker-rhodes ( 21 november 1914 ?
2 march 1987 ) was an australian rules foot-baller who played with carlton in the victorian football league ( vfl ) during the XXXXs andXXXXs .+ Global (field) frederick parker-rhodes ( 21 november 1914 ?
2 march 1987 ) was an english mycology andplant pathology , mathematics at the university of uk .+ Global(field, word)frederick parker-rhodes ( 21 november 1914 ?
2 march 1987 ) was a british computer scientist, best known for his contributions to computational linguistics .Table 4: First sentence from the current Wikipedia article about Frederick Parker-Rhodes and the sentences generated from thethree versions of our table-conditioned neural language model (Table NLM) using the Wikipedia infobox seen in Figure 1.random memory accesses; while neural models per-form scoring through matrix-matrix products, an op-eration which is more local and can be performed ina block parallel manner where modern graphic pro-cessors shine (Kindratenko, 2014).6.4 Qualitative analysisTable 4 shows generations for different variants ofour model based on the Wikipedia table in Figure 1.First of all, comparing the reference to the fact tablereveals that our training data is not perfect.
The birthmonth mentioned in the fact table and the first sen-tence of the Wikipedia article are different; this mayhave been introduced by one contributor editing thearticle and not keeping the information consistent.All three versions of our model correctly generatethe beginning of the sentence by copying the name,the birth date and the death date from the table.
Themodel correctly uses the past tense since the deathdate in the table indicates that the person has passedaway.
Frederick Parker-Rhodes was a scientist, butthis occupation is not directly mentioned in the table.The model without global conditioning can there-fore not predict the right occupation, and it contin-ues the generation with the most common occupa-tion (in Wikipedia) for a person who has died.
Incontrast, the global conditioning over the fields helpsthe model to understand that this person was indeeda scientist.
However, it is only with the global con-ditioning on the words that the model can infer thecorrect occupation, i.e., computer scientist.7 ConclusionsWe have shown that our model can generate flu-ent descriptions of arbitrary people based on struc-tured data.
Local and global conditioning improvesour model by a large margin and we outperform aKneser-Ney language model by nearly 15 BLEU.Our task uses an order of magnitude more data thanprevious work and has a vocabulary that is three or-ders of magnitude larger.In this paper, we have only focused on generatingthe first sentence and we will tackle the generation oflonger biographies in future work.
Also, the encod-ing of field values can be improved.
Currently, weonly attach the field type and token position to eachword type and perform a max-pooling for local con-ditioning.
One could leverage a richer representationby learning an encoder conditioned on the field type,e.g.
a recurrent encoder or a convolutional encoderwith different pooling strategies.Furthermore, the current training loss functiondoes not explicitly penalize the model for generatingincorrect facts, e.g.
predicting an incorrect national-ity or occupation is currently not considered worsethan choosing an incorrect determiner.
A loss func-tion that could assess factual accuracy would cer-tainly improve sentence generation by avoiding suchmistakes.
Also it will be important to define a strat-egy for evaluating the factual accuracy of a genera-tion, beyond BLEU, ROUGE or NIST.91211ReferencesG.
Angeli, P. Liang, and D. Klein.
2010.
A simpledomain-independent probabilistic approach to genera-tion.
In Proceedings of the 2010 Conference on Empir-ical Methods in Natural Language Processing, pages502?512.
Association for Computational Linguistics.D.
Bahdanau, K. Cho, and Y. Bengio.
2015.
Neural ma-chine translation by jointly learning to align and trans-late.
In International Conference on Learning Repre-sentations.R.
Barzilay and M. Lapata.
2005.
Collective content se-lection for concept-to-text generation.
In Proceedingsof the conference on Human Language Technology andEmpirical Methods in Natural Language Processing,pages 331?338.R.
Barzilay and M. Lapata.
2006.
Aggregation via setpartitioning for natural language generation.
In Pro-ceedings of the main conference on Human LanguageTechnology Conference of the North American Chapterof the Association of Computational Linguistics, pages359?366.
Association for Computational Linguistics.A.
Belz and E. Reiter.
2006.
Comparing automatic andhuman evaluation of nlg systems.
In In Proc.
EACL06,pages 313?320.A.
Belz.
2008.
Automatic generation of weather forecasttexts using comprehensive probabilistic generation-space models.
Natural Language Engineering,14(04):431?455.Y.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003.A neural probabilistic language model.
Journal of Ma-chine Learning Research, 3:1137?1155.K.
Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Tay-lor.
2008.
Freebase: a collaboratively created graphdatabase for structuring human knowledge.
In Inter-national Conference on Management of Data, pages1247?1250.
ACM.R.
Dale, S. Geldof, and J.-P. Prost.
2003.
Coral: Us-ing natural language generation for navigational as-sistance.
In Proceedings of the 26th Australasiancomputer science conference-Volume 16, pages 35?44.Australian Computer Society, Inc.J.
Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, andJ.
Makhoul.
2014.
Fast and robust neural networkjoint models for statistical machine translation.
In Pro-ceedings of the 52nd Annual Meeting of the Associa-tion for Computational Linguistics, volume 1, pages1370?1380.P.
A. Duboue and K. R. McKeown.
2002.
Contentplanner construction via evolutionary algorithms and acorpus-based fitness function.
In Proceedings of INLG2002, pages 89?96.H.
Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng,P.
Dollar, J. Gao, X.
He, M. Mitchell, J. C. Platt, L. C.Zitnick, and G. Zweig.
2015.
From captions to visualconcepts and back.
In The IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), June.D.
Ferrucci.
2012.
Introduction to this is watson.
IBMJournal of Research and Development, 56(3.4):1?1.D.
Galanis and I. Androutsopoulos.
2007.
Generatingmultilingual descriptions from linguistically annotatedowl ontologies: the naturalowl system.
In Proceed-ings of the Eleventh European Workshop on NaturalLanguage Generation, pages 143?146.
Association forComputational Linguistics.N.
Green.
2006.
Generation of biomedical arguments forlay readers.
In Proceedings of the Fourth InternationalNatural Language Generation Conference, pages 114?121.
Association for Computational Linguistics.B.
Gyawali and C. Gardent.
2014.
Surface realisationfrom knowledge-bases.
In Proc.
of ACL.K.
Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.2013.
Scalable modified Kneser-Ney language modelestimation.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics,pages 690?696, Sofia, Bulgaria, August.A.
Karpathy and L. Fei-Fei.
2015.
Deep visual-semanticalignments for generating image descriptions.
In TheIEEE Conference on Computer Vision and PatternRecognition (CVPR), June.J.
Kim and R. J. Mooney.
2010.
Generative alignmentand semantic parsing for learning from ambiguous su-pervision.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,pages 543?551.
Association for Computational Lin-guistics.V.
Kindratenko.
2014.
Numerical Computations withGPUs.
Springer.R.
Kiros, R. Salakhutdinov, and R. S. Zemel.
2014.Unifying visual-semantic embeddings with multi-modal neural language models.
arXiv preprintarXiv:1411.2539.I.
Konstas and M. Lapata.
2013.
A global modelfor concept-to-text generation.
J. Artif.
Int.
Res.,48(1):305?346, October.I.
Langkilde and K. Knight.
1998.
Generation that ex-ploits corpus-based statistical knowledge.
In Proc.ACL, pages 704?710.R.
Lebret and R. Collobert.
2014.
Word embeddingsthrough hellinger pca.
In Proceedings of the 14th Con-ference of the European Chapter of the Association forComputational Linguistics, pages 482?490, Gothen-burg, Sweden, April.
Association for ComputationalLinguistics.Y.
A LeCun, L. Bottou, G. B. Orr, and K.-R. Mu?ller.2012.
Efficient backprop.
In Neural networks: Tricksof the trade, pages 9?48.
Springer.101212P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP: Volume 1-Volume 1, pages 91?99.
Associ-ation for Computational Linguistics.W.
Lu and H. T. Ng.
2011.
A probabilistic forest-to-string model for language generation from typedlambda calculus expressions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 1611?1622.
Association forComputational Linguistics.M.-T. Luong, I. Sutskever, Q. V Le, O. Vinyals, andW.
Zaremba.
2015.
Addressing the rare word prob-lem in neural machine translation.
In Proc.
ACL, pages11?19.F.
Mairesse and M. Walker.
2011.
Controlling user per-ceptions of linguistic style: Trainable generation ofpersonality traits.
Comput.
Linguist., 37(3):455?488.C.
D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.Bethard, and D. McClosky.
2014.
The StanfordCoreNLP natural language processing toolkit.
In As-sociation for Computational Linguistics (ACL) SystemDemonstrations, pages 55?60.H.
Mei, M. Bansal, and M. R. Walter.
2016.
What totalk about and how?
selective generation using lstmswith coarse-to-fine alignment.
In Proceedings of Hu-man Language Technologies: The 2016 Annual Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics.A.
Oh and A. Rudnicky.
2000.
Stochastic language gen-eration for spoken dialogue systems.
In ANLP/NAACLWorkshop on Conversational Systems, pages 27?32.A.
Ratnaparkhi.
2002.
Trainable approaches to sur-face natural language generation and their applicationto conversational dialog systems.
Computer Speech &Language, 16(3):435?455.E.
Reiter, R. Dale, and Z. Feng.
2000.
Building naturallanguage generation systems, volume 33.
MIT Press.E.
Reiter, S. Sripada, J.
Hunter, J. Yu, and I. Davy.
2005.Choosing words in computer-generated weather fore-casts.
Artificial Intelligence, 167(1):137?169.L.
Shang, Z. Lu, and H. Li.
2015.
Neural respondingmachine for short-text conversation.
arXiv preprintarXiv:1503.02364.Radu Soricut and Daniel Marcu.
2006.
Stochastic lan-guage generation using widl-expressions and its appli-cation in machine translation and summarization.
InProc.
ACL, pages 1105?1112.R.
Turner, S. Sripada, and E. Reiter.
2010.
Generatingapproximate geographic descriptions.
In Empiricalmethods in natural language generation, pages 121?140.
Springer.O.
Vinyals, A. Toshev, S. Bengio, and D. Erhan.
2015.Show and tell: A neural image caption generator.
InThe IEEE Conference on Computer Vision and PatternRecognition (CVPR), June.T.
Wen, M. Gasic, N.
Mrks?ic?, P. Su, D. Vandyke, andS.
Young.
2015.
Semantically conditioned lstm-based natural language generation for spoken dialoguesystems.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing,pages 1711?1721, Lisbon, Portugal, September.
Asso-ciation for Computational Linguistics.Y.
W. Wong and R. J. Mooney.
2007.
Generation byinverting a semantic parser that uses statistical machinetranslation.
In HLT-NAACL, pages 172?179.K.
Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov,R.
Zemel, and Y. Bengio.
2015.
Show, attend and tell:Neural image caption generation with visual attention.In Proceedings of The 32nd International Conferenceon Machine Learning, volume 37, July.K.
Yao, G. Zweig, and B. Peng.
2015.
Attention with in-tention for a neural network conversation model.
arXivpreprint arXiv:1510.08565.111213
