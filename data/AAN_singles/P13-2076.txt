Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 429?433,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsQuestion Classification TransferAnne-Laure LigozatLIMSI-CNRS / BP133, 91403 Orsay cedex, FranceENSIIE / 1, square de la re?sistance, Evry, Francefirstname.lastname@limsi.frAbstractQuestion answering systems have been de-veloped for many languages, but most re-sources were created for English, whichcan be a problem when developing a sys-tem in another language such as French.In particular, for question classification,no labeled question corpus is available forFrench, so this paper studies the possi-bility to use existing English corpora andtransfer a classification by translating thequestion and their labels.
By translatingthe training corpus, we obtain results closeto a monolingual setting.1 IntroductionIn question answering (QA), as in most NaturalLanguage Processing domains, English is the bestresourced language, in terms of corpora, lexicons,or systems.
Many methods are based on super-vised machine learning which is made possible bythe great amount of resources for this language.While developing a question answering systemfor French, we were thus limited by the lack ofresources for this language.
Some were created,for example for answer validation (Grappy et al,2011).
Yet, for question classification, althoughquestion corpora in French exist, only a small partof them is annotated with question classes, andsuch an annotation is costly.
We thus wonderedif it was possible to use existing English corpora,in this case the data used in (Li and Roth, 2002),to create a classification module for French.Transfering knowledge from one language toanother is usually done by exploiting parallel cor-pora; yet in this case, few such corpora exists(CLEF QA datasets could be used, but questionclasses are not very precise).
We thus investigatedthe possibility of using machine translation to cre-ate a parallel corpus, as has been done for spokenlanguage understanding (Jabaian et al, 2011) forexample.
The idea is that using machine transla-tion would enable us to have a large training cor-pus, either by using the English one and translat-ing the test corpus, or by translating the trainingcorpus.
One of the questions posed was whetherthe quality of present machine translation systemswould enable to learn the classification properly.This paper presents a question classificationtransfer method, which results are close to thoseof a monolingual system.
The contributions of thepaper are the following:?
comparison of train-on-target and test-on-source strategies for question classification;?
creation of an effective question classificationsystem for French, with minimal annotationeffort.This paper is organized as follows: The problemof Question Classification is defined in section 2.The proposed methods are presented in section 3,and the experiments in section 4.
Section 5 detailsthe related works in Question Answering.
Finally,Section 6 concludes with a summary and a fewdirections for future work.2 Problem definitionA Question Answering (QA) system aims at re-turning a precise answer to a natural languagequestion: if asked ?How large is the LincolnMemorial?
?, a QA system should return the an-swer ?164 acres?
as well as a justifying snippet.Most systems include a question classification stepwhich determines the expected answer type, forexample area in the previous case.
This type canthen be used to extract the correct answer in docu-ments.Detecting the answer type is usually consid-ered as a multiclass classification problem, witheach answer type representing a class.
(Zhang and429Englishtraining?corpus Englishtraining?corpusEnglishtest?corpus(translation)Frenchtest?corpus Frenchtest?corpusFrenchtraining?corpus(translation)Questionclassificationfor?EnglishQuestionclassificationfor?FrenchlearnpredicttranslationlearnpredicttranslationFigure 1: Methods for transfering question classi-ficationLee, 2003) showed that a training corpus of sev-eral thousands of questions was required to obtainaround 90% correct classification, which makes ita costly process to adapt a system to another lan-guage than English.
In this paper, we wish to learnsuch a system for French, without having to man-ually annotate thousands of questions.3 Transfering question classificationThe two methods tested for transfering the classi-fication, following (Jabaian et al, 2011), are pre-sented in Figure 1:?
The first one (on the left), called test-on-source, consists in learning a classificationmodel in English, and to translate the test cor-pus from French to English, in order to applythe English model on the translated test cor-pus.?
The second one (on the right), called train-on-target, consists in translating the trainingcorpus from English to French.
We obtain anlabeled French corpus, on which it is possibleto learn a classification model.In the first case, classification is learned on wellwritten questions; yet, as the test corpus is trans-lated, translation errors may disturb the classifier.In the second case, the classification model willbe learned on less well written questions, but thecorpus may be large enough to compensate for theloss in quality.Figure 2: Some of the question categories pro-posed by (Li and Roth, 2002)4 Experiments4.1 Question classesWe used the question taxonomy proposed by (Liand Roth, 2002), which enabled us to compareour results to those obtained by (Zhang and Lee,2003) on English.
This taxonomy contains twolevels: the first one contains 50 fine grained cat-egories, the second one contains 6 coarse grainedcategories.
Figure 2 presents a few of these cate-gories.4.2 CorporaFor English, we used the data from (Li and Roth,2002), which was assembled from USC, UIUCand TREC collections, and has been manually la-beled according to their taxonomy.
The trainingset contains 5,500 labeled questions, and the test-ing set contains 500 questions.For French, we gathered questions from severalevaluation campaigns: QA@CLEF 2005, 2006,2007, EQueR and Qu?ro 2008, 2009 and 2010.After elimination of duplicated questions, we ob-tained a corpus of 1,421 questions, which were di-vided into a training set of 728 questions, and a testset of 693 questions 1.
Some of these questionswere already labeled, and we manually annotatedthe rest of them.Translation was performed by Google Trans-late online interface, which had satisfactory per-formance on interrogative forms, which are notwell handled by all machine translation systems 2.1This distribution is due to further constraints on the sys-tem.2We tested other translation systems, but Google Trans-late gave the best results.430Train en en fr fr(trans.
)Test en en fr fr(trans.
)Method test-on-sourcetrain-on-target50 .798 .677 .794 .769classes6 .90 .735 .828 .84classesTable 1: Question classification precision for bothlevels of the hierarchy (features = word n-grams,classifier = libsvm)4.3 Classification parametersThe classifier used was LibSVM (Chang and Lin,2011) with default parameters, which offers one-vs-one multiclass classification, and which (Zhangand Lee, 2003) showed to be most effective for thistask.We only considered surface features, and ex-tracted bag-of-ngrams (with n = 1..2).4.4 Results and discussionTable 1 shows the results obtained with the basicconfiguration, for both transfer methods.Results are given in precision, i.e.
the propor-tion of correctly classified questions among thetest questions 3.Using word n-grams, monolingual English clas-sification obtains .798 correct classification for thefine grained classes, and .90 for the coarse grainedclasses, results which are very close to those ob-tained by (Zhang and Lee, 2003).On French, we obtain lower results: .769 forfine grained classes, and .84 for coarse grainedclasses, probably mostly due to the smallest sizeof the training corpus: (Zhang and Lee, 2003) hada precision of .65 for the fine grained classificationwith a 1,000 questions training corpus.When translating test questions from Frenchto English, classification precision decreases, aswas expected from (Cumbreras et al, 2006).
Yet,when translating the training corpus from Englishto French and learning the classification model3We measured the significance of precision differences(Student t test, p=.05), for each level of the hierarchy betweeneach test, and, unless indicated otherwise, comparable resultsare significantly different in each condition.Train en fr fr(trans.
)Test en fr frMethod train-on-target50 .822 .798 .807classes6classes .92 .841 .872Table 2: Question classification precision for bothlevels of the hierarchy (features = word n-gramswith abbreviations, classifier = libsvm)on this translated corpus, precision is close tothe French monolingual one for coarse grainedclasses and a little higher than monolingual forfine grained classification (and close to the Englishmonolingual one): this method gives precisions of.794 for fine grained classes and .828 for coarsegrained classes.One possible explanation is that the conditionwhen test questions are translated is very sensitiveto translation errors: if one of the test questionsis not correcly translated, the classifier will havea hard time categorizing it.
If the training cor-pus is translated, translation errors can be counter-balanced by correct translations.
In the followingresults, we do not consider the ?en to en (trans)?method since it systematically gives lower results.As results were lower than our existing rule-based method, we added parts-of-speech as fea-tures in order to try to improve them, as well assemantic classes: the classes are lists of words re-lated to a particular category; for example ?pres-ident?
usually means that a person is expected asan answer.
Table 2 shows the classification perfor-mance with this additional information.Classification is slightly improved, but only forcoarse grained classes (the difference is not signif-icant for fine grained classes).When analyzing the results, we noted that mostconfusion errors were due to the type of featuresgiven as inputs: for example, to correctly clas-sify the question ?What is BPH??
as a questionexpecting an expression corresponding to an ab-breviation (ABBR:exp class in the hierarchy), itis necessary to know that ?BPH?
is an abbrevia-tion.
We thus added a specific feature to detect ifa question word is an abbreviation, simply by test-431Train en fr fr(trans.
)Test en fr fr50 .804 .837 .828classes6classes .904 .869 .900Table 3: Question classification precision for bothlevels of the hierarchy (features = word n-gramswith abbreviations, classifier = libsvm)ing if it contains only upper case letters, and nor-malizing them.
Table 3 gives the results with thisadditional feature (we only kept the method withtranslation of the training corpus since results weremuch higher).Precision is improved for both levels of the hi-erarchy: for fine grained classes, results increasefrom .794 to .837, and for coarse grained classes,from .828 to .869.
Remaining classification errorsare much more disparate.5 Related workMost question answering systems include ques-tion classification, which is generally based on su-pervised learning.
(Li and Roth, 2002) trainedthe SNoW hierarchical classifier for question clas-sification, with a 50 classes fine grained hierar-chy, and a coarse grained one of 6 classes.
Thefeatures used are words, parts-of-speech, chunks,named entities, chunk heads and words related toa class.
They obtain 98.8% correct classificationof the coarse grained classes, and 95% on the finegrained one.
This hierarchy was widely used byother QA systems.
(Zhang and Lee, 2003) studied the classifica-tion performance according to the classifier andtraining dataser size, as well as the contribution ofquestion parse trees.
Their results are 87% correctclassification on coarse grained classes and 80%on fine grained classes with vectorial attributes,and 90% correct classification on coarse grainedclasses and 80% on fine grained classes with struc-tured input and tree kerneks.These question classifications were used forEnglish only.
Adapting the methods to otherlanguages requires to annotated large corpora ofquestions.In order to classify questions in different lan-guages, (Solorio et al, 2004) proposed an in-ternet based approach to determine the expectedtype.
By combining this information with ques-tion words, they obtain 84% correct classificationfor English, 84% for Spanish and 89% for Ital-ian, with a cross validation on a 450 question cor-pus for 7 question classes.
One of the limitationsraised by the authors is the lack of large labeledcorpora for all languages.A possibility to overcome this lack of resourcesis to use existing English resources.
(Cumbreraset al, 2006) developed a QA system for Spanish,based on an English QA system, by translating thequestions from Spanish to English.
They obtain a65% precision for Spanish question classification,while English classification are correctly classifiedwith an 80% precision.
This method thus leads toan important drop in performance.Crosslingual QA systems, in which the questionis in a different language than the documents, alsousually rely on English systems, and translate an-swers for example (Bos and Nissim, 2006; Bow-den et al, 2008).6 ConclusionThis paper presents a comparison between twotransfer modes to adapt question classificationfrom English to French.
Results show that trans-lating the training corpus gives better results thantranslating the test corpus.Part-of-speech information only was used, butsince (Zhang and Lee, 2003) showed that best re-sults are obtained with parse trees and tree kernels,it could be interesting to test this additional in-formation; yet, parsing translated questions mayprove unreliable.Finally, as interrogative forms occur rarely iscorpora, their translation is usually of a slightlylower quality.
A possible future direction for thiswork could be to use a specific model of transla-tion for questions in order to learn question classi-fication on higher quality translations.ReferencesJ.
Bos and M. Nissim.
2006.
Cross-lingual questionanswering by answer translation.
In Working Notesof the Cross Language Evaluation Forum.M.
Bowden, M. Olteanu, P. Suriyentrakorn, T. d?Silva,and D. Moldovan.
2008.
Multilingual questionanswering through intermediate translation: Lcc?spoweranswer at qa@clef 2007.
Advances in Mul-432tilingual and Multimodal Information Retrieval,5152:273?283.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.M.A?.G.
Cumbreras, L. Lo?pez, and F.M.
Santiago.2006.
Bruja: Question classification for spanish.
us-ing machine translation and an english classifier.
InProceedings of the Workshop on Multilingual Ques-tion Answering, pages 39?44.
Association for Com-putational Linguistics.Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco,Anne-Laure Ligozat, Isabelle Robba, and Anne Vil-nat.
2011.
Selecting answers to questions fromweb documents by a robust validation process.
InIEEE/WIC/ACM International Conference on WebIntelligence.Bassam Jabaian, Laurent Besacier, and FabriceLefe`vre.
2011.
Combination of stochastic under-standing and machine translation systems for lan-guage portability of dialogue systems.
In Acous-tics, Speech and Signal Processing (ICASSP), 2011IEEE International Conference on, pages 5612?5615.
IEEE.X.
Li and D. Roth.
2002.
Learning question classifiers.In Proceedings of the 19th international conferenceon Computational linguistics-Volume 1, pages 1?7.Association for Computational Linguistics.T.
Solorio, M. Pe?rez-Coutino, et al 2004.
A languageindependent method for question classification.
InProceedings of the 20th international conference onComputational Linguistics, pages 1374?1380.
Asso-ciation for Computational Linguistics.D.
Zhang and W.S.
Lee.
2003.
Question classifica-tion using support vector machines.
In Proceedingsof the 26th annual international ACM SIGIR con-ference on Research and development in informaionretrieval, pages 26?32.
ACM.433
