Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2084?2093,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsContinuous Profile Models in ASL Syntactic Facial Expression SynthesisHernisa KacorriCarnegie Mellon UniversityHuman-Computer Interaction Institute5000 Forbes AvenuePittsburgh, PA 15213, USAhkacorri@andrew.cmu.eduMatt HuenerfauthRochester Institute of TechnologyB.
Thomas Golisano College ofComputing and Information Sciences152 Lomb Memorial DriveRochester, NY 14623, USAmatt.huenerfauth@rit.eduAbstractTo create accessible content for deaf users,we investigate automatically synthesizinganimations of American Sign Language(ASL), including grammatically importantfacial expressions and head movements.Based on recordings of humans perform-ing various types of syntactic face andhead movements (which include idiosyn-cratic variation), we evaluate the efficacyof Continuous Profile Models (CPMs) atidentifying an essential ?latent trace?
ofthe performance, for use in producingASL animations.
A metric-based evalua-tion and a study with deaf users indicatedthat this approach was more effective thana prior method for producing animations.1 Introduction and MotivationWhile there is much written content online, manypeople who are deaf have difficulty reading textor may prefer sign language.
For example, in theU.S., standardized testing indicates that a major-ity of deaf high school graduates (age 18+) havea fourth-grade reading level or below (Traxler,2000) (U.S. fourth-grade students are typically age9).
While it is possible to create video-recordingsof a human performing American Sign Language(ASL) for use on websites, updating such materialis expensive (i.e., re-recording).
Thus, researchersinvestigate technology to automate the synthesisof animations of a signing virtual human, to makeit more cost-effective for organizations to providesign language content online that is easily updatedand maintained.
Animations can be automaticallysynthesized from a symbolic specification of themessage authored by a human or perhaps by ma-chine translation, e.g.
(Ebling and Glauert, 2013;Filhol et al, 2013; Stein et al, 2012).1.1 ASL Syntactic Facial ExpressionsFacial expressions are essential in ASL, conveyingemotion, semantic variations, and syntactic struc-ture.
Prior research has verified that ASL ani-mations with missing or poor facial expressionsare significantly less understandable for deaf users(Kacorri et al, 2014; Kacorri et al, 2013b; Ka-corri et al, 2013a).
While artists can produce indi-vidual animations with beautiful expressions, suchwork is time-consuming.
For efficiently maintain-able online content, we need automatic synthesisof ASL from a sparse script representing the lexi-cal items and basic elements of the sentence.Specifically, we are studying how to model andgenerate ASL animations that include syntacticfacial expressions, conveying grammatical infor-mation during entire phrases and therefore con-strained by the timing of the manual signs in aphrase (Baker-Shenk, 1983).
Generally speaking,in ASL, upper face movements (examined in thispaper) convey syntactic information across entirephrases, with the mouth movements conveyinglexical or adverbial information.The meaning of a sequence of signs performedwith the hands depends on the co-occuring fa-cial expression.
(While we use the term ?fa-cial expressions,?
these phenomena also includemovements of the head.)
For instance, the ASLsentence ?BOB LIKE CHOCOLATE?
(English:?Bob likes chocolate.?)
becomes a yes/no ques-tion (English: ?Does Bob like chocolate??
), withthe addition of a YesNo facial expression duringthe sentence.
The addition of a Negative facial ex-pression during the verb phrase ?LIKE CHOCO-LATE?
changes the meaning of the sentence to?Bob doesn?t like chocolate.?
(The lexical itemNOT may optionally be used.)
For interroga-tive questions, a WhQuestion facial expressionmust occur during the sentence, e.g., ?BOB LIKE2084WHAT.?
The five types of ASL facial expressionsinvestigated in this paper include:?
YesNo: The signer raises his eyebrows whiletilting the head forward to indicate that thesentence is a polar question.?
WhQuestion: The signer furrows his eye-brows and tilts his head forward during a sen-tence to indicate an interrogative question,typically with a ?WH?
word such as what,who, where, when, how, which, etc.?
Rhetorical: The signer raises his eyebrowsand tilts his head backward and to the sideto indicate a rhetorical question.?
Topic: The signer raises his eyebrows andtilts his head backward during a clause-initialphrase that should be interpreted as a topic.?
Negative: The signer shakes his head leftand right during the verb phrase to indicatenegated meaning, often with the sign NOT.1.2 Prior WorkA survey of recent work of several researchers onproducing animations of sign language with fa-cial expressions appears in (Kacorri, 2015).
Thereis recent interest in data-driven approaches usingfacial motion-capture of human performances togenerate sign language animations: For example,(Schmidt et al, 2013) used clustering techniquesto select facial expressions that co-occur with indi-vidual lexical items, and (Gibet et al, 2011) stud-ied how to map facial motion-capture data to ani-mation controls.In the most closely related prior work, we hadinvestigated how to generate a face animationbased on a set of video recordings of a humansigner performing facial expressions (Kacorri etal., 2016), with head and face movement data au-tomatically extracted from the video, and with in-dividual recordings labeled as each of the five syn-tactic types, as listed in section 1.1.
We wanted toidentify a single exemplar recording in our dataset,for each of the syntactic types, that could be usedas the basis for generating the movements of vir-tual human character.
(In a collection of record-ings of face and head movement, there will nat-urally be non-essential individual variation in themovements; thus, it may be desirable to select arecording that is maximally stereotypical of a setof recordings.)
To do so, we made use of a variantof Dynamic Time Warping (DTW) as a distancemetric to select the recording with minimal pair-wise normalized DTW distance from all of the ex-amples of each syntactic type.
We had used this?centroid?
recording as the basis for producing anovel animation of the face and head movementsfor a sign language sentence.2 MethodIn this paper, we present a new methodology forgenerating face and head movements for sign lan-guage animations, given a set of human recordingsof various syntactic types of facial expressions.Whereas we had previously selected a single ex-emplar recording of a human performance to serveas a basis for producing an animation (Kacorri etal., 2016), in this work, we investigate how to con-struct a model that generalizes across the entire setof recordings, to produce an ?average?
of the faceand head movements, which can serve as a basisfor generating an animation.
To enable compar-ison of our new methodology to our prior tech-nique, we make use of an identical training datasetas in (Kacorri et al, 2016) and an identical ani-mation rendering pipeline, described in (Huener-fauth and Kacorri, 2015a).
Briefly, the animationpipeline accepts a script of the hand location, handorientation, and hand-shape information to poseand move the arms of the character over time, andit also accepts a file containing a stream of facemovement information in MPEG4 Facial Anima-tion Parameters format (ISO/IEC, 1999) to pro-duce a virtual human animation.2.1 Dataset and Feature ExtractionASL is a low-resource language, and it does nothave a writing system in common use.
Therefore,ASL corpora are generally small in size and inlimited supply; they are usually produced throughmanual annotation of video recordings.
Thus,researchers generally work with relatively smalldatasets.
In this work, we make use of two datasetsthat consist of video recordings of humans per-forming ASL with annotation labeling the times inthe video when each of the five types of syntacticfacial expressions listed in section 1.1 occur.The training dataset used in this study was de-scribed in (Kacorri et al, 2016), and consists of199 examples of facial expressions performed bya female signer recorded at Boston University.While the Training dataset can naturally be par-titioned into five subsets, based on each of the fivesyntactic facial expression types, because adjacent2085Type Subgroup ?
A?(Num.
of Videos)Subgroup ?
B?(Num.
of Videos)YesNo Immediately pre-ceded by a facialexpression withraised eyebrows,e.g.
Topic.
(9)Not immediatelypreceded by aneyebrow-raisingexpression.
(10)WhQuestion Performed duringa single word,namely the wh-word (e.g., what,where, when).
(4)Performed duringa phrase consist-ing of multiplewords.
(8)Rhetorical Performed duringa single word,namely the wh-word (e.g., what,where, when).
(2)Performed duringa phrase consist-ing of multiplewords.
(8)Topic Performed during asingle word.
(29)Performed duringa phrase consist-ing of multiplewords.
(15)Negative Immediately pre-ceded by a facialexpression withraised eyebrows,e.g.
Topic.
(16)Not immediatelypreceded byeyebrow-raisingexpression.
(25)Table 1: Ten subgroups of the training dataset.facial expressions or phrase durations may affectthe performance of ASL facial expressions, in thiswork, we sub-divide the dataset further, into tensub-groups, as summarized in Table 1.The ?gold-standard?
dataset used in this studywas shared with the research community by(Huenerfauth and Kacorri, 2014); we use 10 ex-amples of ASL facial expressions (one for eachsub-group listed in Table 1) performed by a malesigner who was recorded at the Linguistic and As-sistive Technologies laboratory.To extract face and head movement informationfrom the video, a face-tracker (Visage, 2016) wasused to produce a set of MPEG4 facial animationparameters for each frame of video: These valuesrepresent face-landmark or head movements of thehuman appearing in the video, including 14 fea-tures used in this study: head x, head y, head z,head pitch, head yaw, head roll, raise l i brow,raise r i brow, raise l m brow, raise r m brow,raise l o brow, raise r o brow, squeeze l brow,squeeze r brow.
The first six values representhead location and orientation.
The next six valuesrepresent vertical movement of the outer (?o ?
),middle (?m ?
), or inner (?i ?)
portion of the right(?r ?)
or left (?l ?)
eyebrows.
The final values rep-resent horizontal movement of the eyebrows.2.2 Continuous Profile Models (CPM)Continuous Profile Model (CPM) aligns a setof related time series data while accounting forchanges in amplitude.
This model has beenpreviously evaluated on speech signals and onother biological time-series data (Listgarten et al,2004).
With the assumption that a noisy, stochas-tic process generates the observed time series data,the approach automatically infers the underlyingnoiseless representation of the data, the so-called?latent trace.?
Figure 6 (on the last page of thispaper) shows an example of multiple time seriesin unaligned and aligned space, with CPM identi-fying the the latent trace.Given a set K of observed time series ~xk=(xk1, xk2, ..., xkN), CPM assumes there is a latenttrace ~z = (z1, z2, ..., zM).
While not a require-ment of the model, the length of the time se-ries data is assumed to be the same (N ) and thelength of the latent trace used in practice is M =(2+?
)N , where an idealM would be large relativeto N to allow precise mapping between observeddata and an underlying point on the latent trace.Higher temporal resolution of the latent trace alsoaccommodates flexible alignments by allowing anobservational series to advance along the latenttrace in small or large jumps (Listgarten, 2007).Continuous Profile Models (CPMs) build onHidden Markov Models (HMMs) (Poritz, 1988)and share similarities with Profile HMMs whichaugment HMMs by two constrained-transitionstates: ?Insert?
and ?Delete?
(emitting no observa-tions).
Similar to the Profile HMM, the CPM hasstrict left-to-right transition rules, constrained toonly move forward along a sequence.
Figure 1 in-cludes a visualization we created, which illustratesthe graphical model of a CPM.2.3 Obtaining the CPM Latent TraceWe applied the CPM model to time align and co-herently integrate time series data from multipleASL facial expression performances of a partic-ular type, e.g., Topic A as listed in section 2.1,with the goal of using the inferred ?latent traces?to drive ASL animations with facial expressionsof that type.
This section describes our work totrain the CPM and to obtain the latent traces; im-plementation details appear in Appendix A.The input time-series data for each CPM modelis the face and head movement data extracted fromASL videos of one of the facial expression types,2086Figure 1: Depiction of a CPM for series xk, withhidden state variables pikiunderlying each obser-vation xki.
The table illustrates the state-space:time-state/scale-state pairs mapped to the hiddenvariables, where time states belong to the integerset (1...M) and scale states belong to an orderedset, here with 7 evenly spaced scales in logarith-mic space as in (Listgarten et al, 2004).as shown in Table 2.
For each dataset, all the train-ing examples are stretched (resampled using cubicinterpolation) to meet the length of the longest ex-ample in the set.
The length of time series, N ,corresponds to the duration in video frames of thelongest example in the data set.
The recordings inthe training set have 14 dimensions, correspondingto the 14 facial features listed in Section 2.1.
Asdiscussed above, the latent trace has a time axis oflengthM , which is approximately double the tem-poral resolution of the original training examples.CPM Models Training Data#Examples ?
N ?#FeaturesLatent TraceM ?
#Featureswhere M = (2 + ?
)NYesNo A 9 x 51 x 14 105 x 14YesNo B 10 x 78 x 14 160 x 14WhQuestion A 4 x 24 x 14 50 x 14WhQuestion B 8 x 41 x 14 84 x 14Rhetorical A 2 x 16 x 14 33 x 14Rhetorical B 8 x 55 x 14 113 x 14Topic A 29 x 29 x 14 60 x 14Topic B 15 x 45 x 14 93 x 14Negative A 16 x 67 x 14 138 x 14Negative B 25 x 76 x 14 156 x 14Table 2: Training data and the obtained latenttraces for each of the CPM models on ASL facialexpression subcategories.To demonstrate our experiments, Figure 6 il-lustrates one of the subcategories, Rhetorical B.
(This figure appears at the end of the paper, dueto its large size.)
We illustrate the training set,before and after the alignment and amplitude nor-malization with the CPM, and the obtained latenttrace for this subcategory.
Figure 6a and Figure6b illustrate each of the 8 training examples with asubplot extending from [0, N ] in the x-axis, whichis the observed time axis in video frames.
Eachof the 14 plots represents one of the head or facefeatures.
Figure 6c illustrates the learned latenttrace with a subplot extending from [0,M ] in thex-axis, which is the latent time axis.
While thetraining set for this subcategory is very small andhas high variability, upon visual inspection of Fig-ure 6, we can observe that the learned latent traceshares similarities with most of the time series inthe training set without being identical to any ofthem.We expect that during the Rhetorical facial ex-pression (Section 2.1), the signer?s eyebrows willrise and the head will be tilted back and to the side.In the latent trace, the inner, middle, and outer por-tions of the left eyebrow rise (Figure 6c, plots 7, 9,11), and so do the inner, middle, and outer portionsof the right eyebrow (Figure 6c, plots 8, 10, 12).Note how the height of the lines in those plots rise,which indicates increased eyebrow height.
For theRhetorical facial expression, we would also ex-pect symmetry in the horizontal displacement ofthe eyebrows, and we see such mirroring in thelatent-trace: In (Figure 6c, plots 13-14), note thetendency for the line in plot 13 (left eyebrow) toincrease in height as the line in plot 14 (right eye-brow) decreases in height, and vice versa.3 EvaluationThis section presents two forms of evaluation ofthe CPM latent trace model for ASL facial expres-sion synthesis.
In Section 3.1, the CPM model willbe compared to a ?gold-standard?
performance ofeach sub-category of ASL facial expression usinga distance-metric-based evaluation, and in Section3.2, the results of a user-study will be presented, inwhich ASL signers evaluated animations of ASLbased upon the CPM model.To provide a basis of comparison, in this sec-tion, we evaluate the CPM approach in compari-son to an alternative approach that we call ?Cen-troid?, which we described in prior work in (Ka-2087corri et al, 2016), where we used a multivariateDTW to select one of the time series in the train-ing set as a representative performance of the fa-cial expression.
The centroid examples are actualrecordings of human ASL signers that are usedto drive an animation.
Appendix A lists the co-denames of the videos from the training datasetselected as centroids and the codenames of thevideos used in the gold-standard dataset (Huener-fauth and Kacorri, 2014).3.1 Metric EvaluationThe gold-standard recordings of a male ASLsigner were described in Section 2.1.
In additionto the video recordings (which were processed toextract face and head movement data), we have an-notation of the timing of the facial expressions andthe sequence of signs performed on the hands.
Tocompare the quality of our CPM model and thatof the Centroid approach, we used each methodto produce a candidate sequence of face and headmovements for the sentence performed by the hu-man in the gold-standard recording.
Thus, the ex-tracted facial expressions from the human record-ing can serve as a gold standard for how the faceand head should move.
In this section, we com-pare: (a) the distance of the CPM latent tracefrom the gold standard to (b) the distance of thecentroid form the gold standard.
It is notablethat these gold-standard recordings were previ-ously ?unseen?
during the creation of the CPMor Centroid models, that is, they were not used inthe training data set during the creation of eithermodel.Since there was variability in the length of thelatent trace, centroid, and gold-standard videos,for a fairer comparison, we first resampled thesetime series, using cubic interpolation, to matchthe duration (in milliseconds) of the gold-standardASL sentence, and then we used multivariateDTW to estimate their distance, following themethodology of (Kacorri et al, 2016) and (Ka-corri and Huenerfauth, 2015).
In prior work (Ka-corri and Huenerfauth, 2015), we had shown thata scoring algorithm based on DTW had moderate(yet significant) correlation with scores that partic-ipants assigned to ASL animation with facial ex-pressions.Figure 2 shows an example of a DTW distancescoring between the gold standard and each of thelatent trace and the centroid, for one face featureFigure 2: DTW distances on the squeeze l browfeature (left eyebrow horizontal movement), dur-ing a Negative A facial expression: (left) betweenthe CPM latent trace and gold standard and (right)between the centroid and gold standard.
The time-line is given in milliseconds.Figure 3: Overall normalized DTW distances forlatent trace and centroid (left) and per each subcat-egory of ASL facial expression (right).
(horizontal movement of the left eyebrow) duringa Negative A facial expression.
Given that thecentroid and the training data for the latent traceare driven by recordings of a (female) signer andthe gold standard is a different (male) signer, thereare differences between these facial expressionsdue to idiosyncratic aspects of individual signers.Thus the metric evaluation in this section is chal-lenging because it is an inter-signer evaluation.Figure 3 illustrates the overall calculated DTWdistances, including a graph with the results bro-ken down per subcategory of ASL facial expres-sion.
The results indicate that the CPM latent traceis closer to the gold standard than the centroid is.Note that the distance values are not zero since thelatent trace and the centroid are being comparedto a recording from a different signer on novel,previously unseen, ASL sentences.
The resultsin these graphs suggest that the latent trace modelout-performed the centroid approach.2088Figure 4: Screenshots of YesNo A stimuli of threetypes: a) neutral, b) centroid, and c) latent trace.3.2 User EvaluationTo further assess our ASL synthesis approach,we conducted a user study where ASL signerswatched short animations of ASL sentences withidentical hand movements but differing in theirface, head, and torso movements.
There werethree conditions in this between-subjects study:a) animations with a static neutral face through-out the animation (as a lower baseline), b) ani-mations with facial expressions driven by the cen-troid human recording, and c) animations with fa-cial expressions driven by the CPM latent tracebased on multiple recordings of a human perform-ing that type of facial expression.
Figure 4 il-lustrates screenshots of each stimulus type for aYesNo A facial expression.
The specific sentencesused for this study were drawn from a standardtest set of stimuli released to the research commu-nity by (Huenerfauth and Kacorri, 2014) for eval-uating animations of sign language with facial ex-pressions.All three types of stimuli (neutral, centroid andlatent trace), shared identical animation-controlscripts specifying the hand and arm movements;these scripts were hand-crafted by ASL signers ina pose-by-pose manner.
For the neutral anima-tions, we did not specify any torso, head, nor facemovements; rather, we left them in their neutralpose throughout the sentences.
As for the cen-troid and latent trace animations, we applied thehead and face movements (as specified by the cen-troid model or by the latent trace model) only tothe portion of the animation where the facial ex-pression of interest occurs, leaving the head andface for the rest of the animation to a neutral pose.For instance, during a stimulus that contains a Wh-question, the face and head are animated only dur-ing the Wh-question, but they are left in a neutralpose for the rest of the stimulus (which may in-clude other sentences).
The period of time whenthe facial expression occurred was time-alignedwith the subset of words (the sequence of signsperformed on the hands) for the appropriate syn-tactic domain; the phrase-beginning and phrase-ending was aligned with the performance of thefacial expression.
Thus, the difference in appear-ance between our animation stimuli was subtle:The only portion of the animations that differedbetween the three conditions (neutral, centroid,and latent-trace) was the face and the head move-ments during the span of time when the syntac-tic facial expression should occur (e.g., during theWh-question).We resampled the centroid and CPM time se-ries, using cubic interpolation, to match the dura-tion (in milliseconds) of the animation they wouldbe applied to.
To convert the centroid and latenttrace time series into the input for the animation-generation system, we used the MPEG4-features-to-animation pipeline described in (Kacorri et al,2016).
That platform is based upon the open-source EMBR animation system for producing hu-man animation (Heloir and Kipp, 2009); specif-ically, the facial expressions were represented asan EMBR PoseSequence with a pose defined ev-ery 133 milliseconds.In prior work (Huenerfauth and Kacorri,2015b), we investigated key methodological con-siderations in conducting a study to evaluate signlanguage animations with deaf users, includingthe use of appropriate baselines for comparison,appropriate presentation of questions and instruc-tions, demographic and technology experiencefactors influencing acceptance of signing avatars,and other factors that we have considered in thedesign of this current study.
Our recent work(Kacorri et al, 2015) has established a set of de-mographic and technology experience questionswhich can be used to screen for the most criticalparticipants in a user study of ASL signers to eval-uate animation.
Specifically, we screened for par-ticipants that identified themselves as ?deaf/Deaf?or ?hard-of-hearing,?
who had grown up usingASL at home or had attended an ASL-basedschool as a young child, such as a residential ordaytime school.Deaf researchers (all fluent ASL signers) re-cruited and collected data from participants, dur-ing meetings conducted in ASL.
Initial advertise-2089ments were sent to local email distribution listsand Facebook groups.
A total of 17 participantsmet the above criteria, where 14 participants self-identified as deaf/Deaf and 3 as hard-of-hearing.Of our participants in the study, 10 had attended aresidential school for deaf students, and 7, a day-time school for deaf students.
14 participants hadlearned ASL prior to age 5, and the remaining 3had been using ASL for over 7 years.
There were8 men and 9 women of ages 19-29 (average age22.8).
In prior work, we (Kacorri et al, 2015)have advocated that participants in studies eval-uating sign language animation complete a twostandardized surveys about their technology ex-perience (MediaSharing and AnimationAttitude)and that researchers report these values for partici-pants, to enable comparison across studies.
In ourstudy, participant scores for MediaSharing variedbetween 3 and 6, with a mean score of 4.3, andscores for AnimationAttitude varied from 2 to 6,with a mean score of 3.8.At the beginning of the study, participantsviewed a sample animation, to familiarize themwith the experiment and the questions they wouldbe asked about each animation.
(This sampleused a different stimulus than the other ten anima-tions shown in the study.)
Next, they respondedto a set of questions that measured their subjec-tive impression of each animation, using a 1-to-10scalar response.
Each question was conveyed us-ing ASL through an onscreen video, and the fol-lowing English question text was shown on thequestionnaire: (a) Good ASL grammar?
(10=Per-fect, 1=Bad); (b) Easy to understand?
(10=Clear,1=Confusing); (c) Natural?
(10=Moves like per-son, 1=Like robot).
These questions have beenused in many prior experimental studies to evalu-ate animations of ASL, e.g.
(Kacorri and Huener-fauth, 2015), and were shared with research com-munity as a standard evaluation tool in (Huen-erfauth and Kacorri, 2014).
To calculate a sin-gle score for each animation, the scalar responsescores for the three questions were averaged.Figure 5 shows distributions of subjectivescores as boxplots with a 1.5 interquartile range(IQR).
For comparison, means are denoted witha star and their values are labeled above eachboxplot.
When comparing the subjective scoresthat participants assigned to the animations in Fig-ure 5, we found a significant difference (Kruskal-Wallis test used since the data was not normallyFigure 5: Subjective scores for centroid, latenttrace, and neutral animations.distributed) between the latent trace and centroid(p < 0.005) and between the latent trace and neu-tral (p < 0.05).In summary, our CPM modeling approach forgenerating an animation out-performed an anima-tion produced from an actual recording of a sin-gle human performance (the ?centroid?
approach).In prior methodological studies, we demonstratedthat it is valid to use either videos of humans oranimations (driven by a human performance) asthe baseline for comparison in a study of ASL an-imation (Kacorri et al, 2013a).
As suggested byFigure 4, the differences in face and head move-ments between the Centroid and CPM conditionswere subtle, yet fluent ASL signers rated the CPManimations higher in this study.4 Conclusion and Future WorkTo facilitate the creation of ASL content that caneasily be updated or maintained, we have investi-gated technologies for automating the synthesis ofASL animations from a sparse representation ofthe message.
Specifically, this paper has focusedon the synthesis of syntactic ASL facial expres-sions, which are essential to sentence meaning,using a data-driven methodology in which record-ings of human ASL signers are used as a basis forgenerating face and head movements for anima-tion.
To avoid idiosyncratic aspects of a singleperformance, we have modeled a facial expres-sion based on the underlying trace of the move-ment trained on multiple recordings of differentsentences where this type of facial expression oc-curs.
We obtain the latent trace with ContinuousProfile Model (CPM), a probabilistic generativemodel that relies on Hidden Markov Models.
We2090assessed our modeling approach through compar-ison to an alternative centroid approach, where asingle performance was selected as a representa-tive.
Through both a metric evaluation and anexperimental user study, we found that the facialexpressions driven by our CPM models producehigh-quality facial expressions that are more simi-lar to human performance of novel sentences.While this work used the latent trace as the basisfor animation, in future work, we also plan to ex-plore methods for sampling from the model to pro-duce variations in face and head movement.
In ad-dition, to aid CPM convergence to a good local op-timum, in future work we will investigate dimen-sionality reduction approaches that are reversiblesuch as Principal Component Analysis (Pearson,1901) and other pre-processing approaches similarto (Listgarten, 2007), where the training data set iscoarsely pre-aligned and pre-scaled based on thecenter of mass of the time series.
In addition weplan to further investigate how to fine-tune someof the hyper parameters of the CPM such as splinescaling, single global scaling factor, convergencetolerance, and initialization of the latent trace witha centroid.
In subsequent work, we would like toexplore alternatives for enhancing CPMs by incor-porating contextual features in the training data setsuch as timing of hand movements, and preceding,succeeding, and co-occurring facial expressions.AcknowledgmentsThis material is based upon work supported by theNational Science Foundation under award num-ber 1065009 and 1506786.
This material is alsobased upon work supported by the Science Fel-lowship and Dissertation Fellowship programs ofThe Graduate Center, CUNY.
We are grateful forsupport and resources provided by Ali Raza Syedat The Graduate Center, CUNY, and by Carol Nei-dle at Boston University.ReferencesCharlotte Baker-Shenk.
1983.
A microanalysis ofthe nonmanual components of questions in americansign language.Sarah Ebling and John Glauert.
2013.
Exploiting thefull potential of jasigning to build an avatar signingtrain announcements.
In Proceedings of the ThirdInternational Symposium on Sign Language Trans-lation and Avatar Technology (SLTAT), Chicago,USA, October, volume 18, page 19.Michael Filhol, Mohamed N Hadjadj, and Beno??tTestu.
2013.
A rule triggering system for automatictext-to-sign translation.
Universal Access in the In-formation Society, pages 1?12.Sylvie Gibet, Nicolas Courty, Kyle Duarte, andThibaut Le Naour.
2011.
The signcom system fordata-driven animation of interactive virtual signers:Methodology and evaluation.
ACM Transactions onInteractive Intelligent Systems (TiiS), 1(1):6.Alexis Heloir and Michael Kipp.
2009.
Embr?a re-altime animation engine for interactive embodiedagents.
In Intelligent Virtual Agents, pages 393?404.
Springer.Matt Huenerfauth and Hernisa Kacorri.
2014.
Releaseof experimental stimuli and questions for evaluat-ing facial expressions in animations of american signlanguage.
In Proceedings of the 6thWorkshop on theRepresentation and Processing of Sign Languages:Beyond the Manual Channel, The 9th InternationalConference on Language Resources and Evaluation(LREC 2014), Reykjavik, Iceland.Matt Huenerfauth and Hernisa Kacorri.
2015a.
Aug-menting embr virtual human animation system withmpeg-4 controls for producing asl facial expres-sions.
In International Symposium on Sign Lan-guage Translation and Avatar Technology, vol-ume 3.Matt Huenerfauth and Hernisa Kacorri.
2015b.
Bestpractices for conducting evaluations of sign lan-guage animation.
Journal on Technology and Per-sons with Disabilities, 3.ISO/IEC.
1999.
Information technology?Coding ofaudio-visual objects?Part 2: Visual.
ISO 14496-2:1999, International Organization for Standardiza-tion, Geneva, Switzerland.Hernisa Kacorri and Matt Huenerfauth.
2015.
Eval-uating a dynamic time warping based scoring algo-rithm for facial expressions in asl animations.
In 6thWorkshop on Speech and Language Processing forAssistive Technologies (SLPAT), page 29.Hernisa Kacorri, Pengfei Lu, and Matt Huenerfauth.2013a.
Effect of displaying human videos during anevaluation study of american sign language anima-tion.
ACM Transactions on Accessible Computing(TACCESS), 5(2):4.Hernisa Kacorri, Pengfei Lu, and Matt Huenerfauth.2013b.
Evaluating facial expressions in americansign language animations for accessible online infor-mation.
In Universal Access in Human-ComputerInteraction.
Design Methods, Tools, and InteractionTechniques for eInclusion, pages 510?519.
Springer.Hernisa Kacorri, Allen Harper, and Matt Huenerfauth.2014.
Measuring the perception of facial expres-sions in american sign language animations with eyetracking.
In Universal Access in Human-ComputerInteraction.
Design for All and Accessibility Prac-tice, pages 553?563.
Springer.2091Hernisa Kacorri, Matt Huenerfauth, Sarah Ebling, Kas-mira Patel, and Mackenzie Willard.
2015.
Demo-graphic and experiential factors influencing accep-tance of sign language animation by deaf users.
InProceedings of the 17th International ACM SIGAC-CESS Conference on Computers & Accessibility,pages 147?154.
ACM.Hernisa Kacorri, Ali Raza Syed, Matt Huenerfauth,and Carol Neidle.
2016.
Centroid-based exem-plar selection of asl non-manual expressions us-ing multidimensional dynamic time warping andmpeg4 features.
In Proceedings of the 7th Work-shop on the Representation and Processing ofSign Languages: Corpus Mining, The 10th In-ternational Conference on Language Resourcesand Evaluation (LREC 2016), Portoroz, Slovenia.http://huenerfauth.ist.rit.edu/pubs/lrec2016.pdf.Hernisa Kacorri.
2015.
Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation.
Tech-nical report, The Graduate Center, CUNY.http://academicworks.cuny.edu/gc cs tr/403.Jennifer Listgarten, Radford M Neal, Sam T Roweis,and Andrew Emili.
2004.
Multiple alignment ofcontinuous time series.
In Advances in neural infor-mation processing systems, pages 817?824.Jennifer Listgarten.
2007.
Analysis of sibling time se-ries data: alignment and difference detection.
Ph.D.thesis, University of Toronto.Carol Neidle, Jingjing Liu, Bo Liu, Xi Peng, ChristianVogler, and Dimitris Metaxas.
2014.
Computer-based tracking, analysis, and visualization of lin-guistically significant nonmanual events in americansign language (asl).
In LREC Workshop on the Rep-resentation and Processing of Sign Languages: Be-yond the Manual Channel.
Citeseer.Karl Pearson.
1901.
Principal components analysis.The London, Edinburgh, and Dublin PhilosophicalMagazine and Journal of Science, 6(2):559.Alan B Poritz.
1988.
Hidden markov models: Aguided tour.
In Acoustics, Speech, and Signal Pro-cessing, 1988.
ICASSP-88., 1988 International Con-ference on, pages 7?13.
IEEE.Christoph Schmidt, Oscar Koller, Hermann Ney,Thomas Hoyoux, and Justus Piater.
2013.
Enhanc-ing gloss-based corpora with facial features usingactive appearance models.
In International Sym-posium on Sign Language Translation and AvatarTechnology, volume 2.Daniel Stein, Christoph Schmidt, and Hermann Ney.2012.
Analysis, preparation, and optimization ofstatistical sign language machine translation.
Ma-chine Translation, 26(4):325?357.Carol Bloomquist Traxler.
2000.
The stanfordachievement test: National norming and perfor-mance standards for deaf and hard-of-hearing stu-dents.
Journal of deaf studies and deaf education,5(4):337?348.Technologies Visage.
2016.
Face tracking.https://visagetechnologies.com/products-and-services/visagesdk/facetrack.
Accessed: 2016-03-10.A Appendix: Supplemental MaterialIn Section 2.3, we made use of a freelyavailable CPM implementation available fromhttp://www.cs.toronto.edu/?jenn/CPM/ in MAT-LAB, Version 8.5.0.197613 (R2015a).One parameter for regularizing the latent trace(Listgarten, 2007) is a smoothing parameter (?
),with values being dataset-dependent.
To select agood ?, we experimented with held-out data andfound that ?
= 4 and NumberOfIterations =3 resulted in a latent trace curve that captures theshape of the ASL features well.
Other CPM pa-rameters were:?
USE SPLINE = 0: if set to 1, uses splinescaling rather than HMM scale states?
oneScaleOnly = 0: no HMM scale states(only a single global scaling factor is appliedto each time series.)?
extraPercent(?)
= 0.05: slack on the lengthof the latent trace M , where M = (2 + ?
)N .?
learnStateTransitions = 0: whether tolearn the HMM state-transition probabilities?
learnGlobalScaleFactor = 1: learn singleglobal scale factor for each time seriesSection 3.1 described how the centroids wereselected from among videos in the Boston Univer-sity dataset (Neidle et al, 2014), and the gold stan-dard videos were selected from among videos in adifferent dataset (Huenerfauth and Kacorri, 2014).Table 3 lists the code names of the selected videos,using the nomenclature of each dataset.Subcategory Centroid Codename Gold-Standard CodenameYesNo A 2011-12-01 0037-cam2-05 Y4YesNo B 2011-12-01 0037-cam2-09 Y3WhQuestion A 2011-12-01 0038-cam2-05 W1WhQuestion B 2011-12-01 0038-cam2-07 W2Rhetorical A 2011-12-01 0041-cam2-04 R3Rhetorical B 2011-12-01 0041-cam2-02 R9Topic A 2012-01-27 0050-cam2-05 T4Topic B 2012-01-27 0051-cam2-09 T3Negative A 2012-01-27 0051-cam2-03 N2Negative B 2012-01-27 0051-cam2-30 N5Table 3: Codenames of videos selected as centoidsand gold standards for comparison in section 3.1.2092Figure 6: Example of CPM modeling for Rhetorical B: (a) training examples before CPM (each plotshows one of the 14 face features over time, with 8 colored lines in each plot showing each of the 8training examples), (b) after CPM time-alignment and rescaling, and (c) the final latent trace based uponall 8 examples.2093
