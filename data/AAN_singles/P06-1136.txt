Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1081?1088,Sydney, July 2006. c?2006 Association for Computational LinguisticsReranking Answers for Definitional QA Using Language ModelingYi ChenSchool of Software Engi-neeringChongqing UniversityChongqing, China, 400044126cy@126.comMing ZhouMicrosoft Research Asia5F Sigma Center, No.49 ZhichunRoad, HaidianBejing, China, 100080mingzhou@microsoft.comShilong WangCollege of Mechanical En-gineeringChongqing UniversityChongqing, China, 400044slwang@cqu.edu.cnAbstract*Statistical ranking methods based on cen-troid vector (profile) extracted from ex-ternal knowledge have become widelyadopted in the top definitional QA sys-tems in TREC 2003 and 2004.
In theseapproaches, terms in the centroid vectorare treated as a bag of words based on theindependent assumption.
To relax this as-sumption, this paper proposes a novellanguage model-based answer rerankingmethod to improve the existing bag-of-words model approach by considering thedependence of the words in the centroidvector.
Experiments have been conductedto evaluate the different dependencemodels.
The results on the TREC 2003test set show that the reranking approachwith biterm language model, significantlyoutperforms the one with the bag-of-words model and unigram languagemodel by 14.9% and 12.5% respectivelyin F-Measure(5).1 IntroductionIn recent years, QA systems in TREC (Text RE-trieval Conference) have made remarkable pro-gress (Voorhees, 2002).
The task of TREC QAbefore 2003 has mainly focused on the factoidquestions, in which the answer to the question isa number, a person name, or an organizationname, or the like.Questions like ?Who is Colin Powell??
or?What is mold??
are definitional questions*This work was finished while the first author was visitingMicrosoft Research Asia during March 2005-March 2006 asa component of the project of AskBill Chatbot led by Dr.Ming Zhou.
(Voorhees, 2003).
Statistics from 2,516 Fre-quently Asked Questions (FAQ) extracted fromInternet FAQ Archives1 show that around 23.6%are definitional questions.
This indicates thatdefinitional questions occur frequently and areimportant question types.
TREC started theevaluation for definitional QA in 2003.
The defi-nitional QA systems in TREC are required toextract definitional nuggets/sentences that con-tain the highly descriptive information about thequestion target from a given large corpus.For definitional question, statistical rankingmethods based on centroid vector (profile) ex-tracted from external resources, such as theonline encyclopedia, are widely adopted in thetop systems in TREC 2003 and 2004 (Xu et al,2003; Blair-Goldensohn et al, 2003; Wu et al,2004).
In these systems, for a given question, avector is formed consisting of the most frequentco-occurring terms with the question target as thequestion profile.
Candidate answers extractedfrom a given large corpus are ranked based ontheir similarity to the question profile.
The simi-larity is normally the TFIDF score in which boththe candidate answer and the question profile aretreated as a bag of words in the framework ofVector Space Model (VSM).VSM is based on an independence assumption,which assumes that terms in a vector are statisti-cally independent from one another.
Althoughthis assumption makes the development of re-trieval models easier and the retrieval operationtractable, it does not hold in textual data.
For ex-ample, for question ?Who is Bill Gates??
words?born?
and ?1955?
in the candidate answer arenot independent.In this paper, we are interested in consideringthe term dependence to improve the answerreranking for definitional QA.
Specifically, the1http://www.faqs.org/faqs/1081language model is utilized to capture the termdependence.
A language model is a probabilitydistribution that captures the statistical regulari-ties of natural language use.
In a language model,key elements are the probabilities of word se-quences, denoted as P(w1, w2, ..., wn) or P (w1,n)for short.
Recently, language model has beensuccessfully used for information retrieval (IR)(Ponte and Croft, 1998; Song and Croft, 1998;Lafferty et al, 2001; Gao et al, 2004; Cao et al,2005).
Our natural thinking is to apply languagemodel to rank the candidate answers as it hasbeen applied to rank search results in IR task.The basic idea of our research is that, given adefinitional question q, an ordered centroid OCwhich is learned from the web and a languagemodel LM(OC) which is trained with it.
Candi-date answers can be ranked by probability esti-mated by LM(OC).
A series of experiments onstandard TREC 2003 collection have been con-ducted to evaluate bigram and biterm languagemodels.
Results show that both these two lan-guage models produce promising results by cap-turing the term dependence and biterm modelachieves the best performance.
Biterm languagemodel interpolating with unigram modelsignificantly improves the VSM and unigrammodel by 14.9% and 12.5% in F-Measure(5).In the rest of this paper, Section 2 reviews re-lated work.
Section 3 presents details of the pro-posed method.
Section 4 introduces the structureof our experimental system.
We show the ex-perimental results in Section 5, and conclude thepaper in Section 6.2 Related WorkWeb information has been widely used for an-swer reranking and validation.
For factoid QAtask, AskMSR (Brill et al, 2001) ranks the an-swers by counting the occurrences of candidateanswers returned from a search engine.
Similarly,DIOGENE (Magnini et al, 2002) applies searchengines to validate candidate answers.For definitional QA task, Lin (2002) presentedan approach in which web-based answer rerank-ing is combined with dictionary-based (e.g.,WordNet) reranking, which leads to a 25% in-crease in mean reciprocal rank (MRR).
Xu et al(2003) proposed a statistical ranking methodbased on centroid vector (i.e., vector of wordsand frequencies) learned from the online ency-clopedia (i.e., Wikipedia2) and the web.
Candi-2http://www.wikipedia.orgdate answers were reranked based on their simi-larity (TFIDF score) to the centroid vector.
Simi-lar techniques were explored in (Blair-Goldensohn et al, 2003).
In this paper, we ex-plore the dependence among terms in centroidvector for improving the answer reranking fordefinitional QA.In recent years, language modeling has beenwidely employed in IR (Ponte and Croft, 1998;Song and Croft, 1998; Miller and Zhai, 1999;Lafferty and Zhai, 2001).
The basic idea is tocompute the conditional probability P(Q|D), i.e.,the probability of generating a query Q given theobservation of a document D. The searcheddocuments are ranked in descending order of thisprobability.Song and Croft (1998) proposed a general lan-guage model to incorporate word dependence byusing bigrams.
Srikanth and Srihari (2002) intro-duced biterm language models similar to the bi-gram model except that the constraint of order interms is relaxed and improved performance wasobserved.
Gao et al (2004) presented a newmethod of capturing word dependencies, inwhich they extended state-of-the-art languagemodeling approaches to information retrieval byintroducing a dependence structure that learnedfrom training data.
Cao et al (2005) proposed anovel dependence model to incorporate both re-lationships of WordNet and co-occurrence withthe language modeling framework for IR.
In ourapproach, we propose bigram and biterm modelsto capture the term dependence in centroid vector.Applying language modeling for the QA taskhas not been widely researched.
Zhang D. andLee (2003) proposed a method using languagemodel for passage retrieval for the factoid QA.They trained two language models, in which onewas the question-topic language model and theother was passage language model.
They utilizedthe divergence between the two language modelsto rank passages.
In this paper, we focus onreranking answers for definitional questions.As other ranking approaches, Xu, et al (2005)formalized ranking definitions as classificationproblems, and Cui et al (2004) proposed softpatterns to rank answers for definitional QA.3 Reranking Answers Using LanguageModel3.1 Model backgroundIn practice, language model is often approxi-mated by N-gram models.Unigram:1082(1)                     211 ))...P(w)P(wP(w)P(w n,n =Bigram:(2)        11211 )|w)...P(w|w)P(wP(w)P(w n-n,n =The unigram model makes a strong assump-tion that each word occurs independently.
Thebigram model takes the local context into con-sideration.
It has been proved to work better thanthe unigram language model in IR (e.g., Songand Croft, 1998).Biterm language models are similar to bigramlanguage models except that the constraint oforder in terms is relaxed.
Therefore, a documentcontaining information retrieval and a documentcontaining retrieval (of) information will be as-signed the same generation probability.
Thebiterm probabilities can be approximated usingthe frequency of occurrence of terms.Three approximation methods were proposedin Srikanth and Srihari (2002).
The so-calledmin-Adhoc approximation truly relaxes the con-straint of word order and outperformed other twoapproximation methods in their experiments.
(3)            )}(),(min{),(),()|(1111iiiiiiiiBTwCwCwwCwwCwwP???
?+?Equation (3) is the min-Adhoc approximation.Where C(X) gives the occurrences of the string X.3.2 Reranking based on language modelIn our approach, we adopt bigram and bitermlanguage models.
As a smoothing approach, lin-ear interpolation of unigrams and bigrams is em-ployed.Given a candidate answer A=t1t2...ti...tn and abigram or biterm back-off language model OCtrained with the ordered centroid, the probabilityof generating A can be estimated by Equation (4).
[ ]?=?
?+==niiiinOttPOCtPOCtPOCttPOCAP2111C) ,|()1(  )|()|((4)                                                            )|,...,(   )|(?
?where OC stands for the language model of theordered centroid and ?
is the mixture weightcombining the unigram and bigram (or biterm)probabilities.
After taking logarithm and expo-nential for Equation (4), we get Equation (5).
[ ] (5)   ) ,|()1(  )|(log)|(logexp )(211???????????
?++==?niiii OCttPOCtPOCtPAScore ?
?We observe that this formula penalizes ver-bose candidate answers.
This can be alleviatedby adding a brevity penalty, BP, which is in-spired by machine translation evaluation (Pap-ineni et al, 2001).
(6)                                    1 ,1minexp  ????????????????
?=ArefLLBPwhere Lref is a constant standing for the length ofreference answer (i.e., centroid vector).
LA is thelength of the candidate answer.
By combiningEquation (5) and (6), we get the final scoringfunction.
[ ]???????????
?++?????????????????
?=?==?niiiiArefOCttPOCtPOCtPLLAScoreBPAFinalScore211) ,|()1(  )|(log)|(logexp  1 ,1minexp(7)                                                                )(    )(?
?3.3 Parameter estimationIn Equation (7), we need to estimate three pa-rameters: P(ti|OC), P(ti|ti-1, OC) and ?
.For P(ti|OC), P(ti|ti-1, OC), maximum likeli-hood estimation (MLE) is employed.
(8)                                          )(  )|(OCiOCi NtCountOCtP =(9)                                )(),(),|(111??
?=iOCiiOCii tCountttCountOCttPwhere CountOC(X) is the occurrences of the stringX in the ordered centroid and NOC stands for thetotal number of tokens in the ordered centroid.For biterm language model, we use the abovementioned min-Adhoc approximation (Srikanthand Srihari, 2002).
(10)   )}(),(min{),(),(),|(1111iOCiOCiiOCiiOCiiBT tCounttCountttCountttCountOCttP???
?+=For unigram, we do not need smoothing be-cause we only concern terms in the centroid vec-tor.
Recall that bigram and biterm probabilitieshave already been smoothed by interpolation.The ?
can be learned from a training corpususing an Expectation Maximization (EM) algo-rithm.
Specifically, we estimate ?
by maximiz-ing the likelihood of all training instances, giventhe bigram or biterm model:[ ]?
?
?= =?=???????
?+==||1 2)(1)()(||1)()()(1)|()1()(logmax arg(11)                                                   )|...(max argINSjlijijijiINSjjjljjttPtPOCttP????
?BP and P(t1) are ignored because they do notaffect ?
.
?
can be estimated using EM iterativeprocedure:1) Initialize ?
to a random estimate between 0and 1, i.e., 0.5;2) Update ?
using:??=?=+?+?
?=jlijijirjirjirINSj jrttPtPtPlINS 2 )( 1)()()()()()(||1)1( (12)  )|()1()()(11||1???
?where INS denotes all training instances and|INS| gives the number of training instanceswhich is used as a normalization factor.
lj gives1083the number of tokens in the jth instance in thetraining data;3) Repeat Step 2 until ?
converges.We use the TREC 2004 test set3 as our train-ing data and we set ?
as 0.4 for bigram modeland 0.6 for biterm model according to the ex-perimental results.4 System ArchitectureTarget(e.g., Aaron Copland)Ordered centroid list(e.g., born Nov 14 1900)Candidate answersRemovingredundant answersExtractingcandidate answersAnswers(e.g., American composer)Learning orderedcentroidAnswer rerankingTraining languagemodelAQUAINTWebStage 1 Training language modelStage 3 Removing redundancies Stage 2 Reranking using LMFigure 1.
System architecture.We propose a three-stage approach for answerextraction.
It involves: 1) learning a languagemodel from the web; 2) adopting the languagemodel to rerank candidate answers; 3) removingredundancies.
Figure 1 shows five main modules.Learning ordered centroid:1) Query expansion.
Definitional questions arenormally short (i.e., who is Bill Gates?).
Queryexpansion is used to refine the query intention.First, reformulate query via simply adding cluewords to the questions.
i.e., for ?Who is ...?
?question, we add the word ?biography?
; and for?What is ...??
question, we add the word ?is usu-ally?, ?refers to?, etc.
We learn these clue wordsusing the similar method proposed in (Ravi-chandran and Hovy, 2002).
Second, query a websearch engine (i.e., Google4) with reformulatedquery and learn top-R (we empirically set R=5)most frequent co-occurring terms with the targetfrom returned snippets as query expansion terms;2) Learning centroid vector (profile).
We queryGoogle again with the target and expanded termslearned in the previous step, download top-N (weempirically set N=500 based on the tradeoff be-tween the snippet number and the time complex-ity) snippets, and split snippets into sentences.Then, we retain the generated sentences that con-tain the target, denoted as W. Finally, learn top-M (We empirically set M=350) most frequent co-3The test data for TREC-13 includes 65 definition questions.NIST drops one in the official evaluation.4http://www.google.comoccurring terms (stemmed) from W using Equa-tion (15) (Cui et al, 2004) as the centroid vector.
(13)     )()1)(log()1)(log()1),(log()( tidfTCounttCountTtCotWeight ?++++=where Co(t, T) denotes the number of sentencesin which t co-occurs with the target T, andCount(t) gives the number of sentences contain-ing the word t. We also use the inverse documentfrequency of t, idf(t) 5, as a measurement of theglobal importance of the word;3) Extracting ordered centroid.
For each sentencein W, we retain the terms in the centroid vectoras the ordered centroid list.
Words not containedin the centroid vector will be treated as the ?stopwords?
and ignored.E.g., ?Who is Aaron Copland?
?, the orderedcentroid list is shown below(where italics areextracted and put in the ordered centroid list):1.
Today's Highlight in History: On No-vember 14, 1900, Aaron Copland, oneof America's leading 20th century com-posers, was born in New York City.
?November 14 1900 Aaron CoplandAmerica composer born New York City2.
...Extracting candidate answers: We extract can-didates from AQUAINT corpus.1) Querying AQUAINT corpus with the targetand retrieve relevant documents;2) Splitting documents into sentences and ex-tracting the sentences containing the target.
Herein order to improve recall, simple heuristics rulesare used to handle the problem of coreferenceresolution.
If a sentence is deemed to contain thetarget and its next sentence starts with ?he?,?she?, ?it?, or ?they?, then the next sentence isretained.Training language models: As mentionedabove, we train language models using the ob-tained ordered centroid for each question.Answer reranking: Once the language modelsand the candidate answers are ready for a givenquestion, candidate answers are reranked basedon the probabilities of the language models gen-erating candidate answers.Removing redundancies: Repetitive and similarcandidate sentences will be removed.
Given areranked candidate answer set CA, redundancyremoving is conducted as follows:5We use the statistics from British National Corpus (BNC)site to approximate words?
IDF,http://www.itri.brighton.ac.uk/~Adam.Kilgarriff/bnc-readme.html.1084Step 1: Initially set the result A={}, and gettop j=1 element from CA and thenadd it to A, j=2.Step 2: Get the jth element from CA, de-noted as CAj.
Compute cosine simi-larity between CAj and each ele-ment i of A, which is expressed assij.
Then let sik=max{s1j, s2j, ..., sij},if sik < threshold (we set it to 0.75),then add j to the set A.Step 3: If length of A exceeds a predefinedthreshold, exit; otherwise, j=j+1,go to Step 2.Figure 2.
Algorithm for removing redundancy.5 Experiment & EvaluationIn order to get comparable evaluation, we applyour approach to TREC 2003 definitional QA task.More details will be shown in the following sec-tions.5.1 Experiment setup5.1.1 DatasetWe employ the dataset from the TREC 2003 QAtask.
It includes the AQUAINT corpus of morethan 1 million news articles from the New YorkTimes (1998-2000), Associated Press (1998-2000), Xinhua News Agency (1996-2000) and 50definitional question/answer pairs.
In these 50definitional questions, 30 are for people (e.g.,Aaron Copland), 10 are for organizations (e.g.,Friends of the Earth) and 10 are for other entities(e.g., Quasars).
We employ Lemur6 to retrieverelevant documents from the AQUAINT corpus.For each query, we return the top 500 documents.5.1.2 Evaluation metricsWe adopt the evaluation metrics used in theTREC definitional QA task (Voorhees, 2003 and2004).
TREC provides a list of essential and ac-ceptable nuggets for answering each question.We use these nuggets to assess our approach.During this progress, two human assessors exam-ine how many essential and acceptable nuggetsare covered in the returned answers.
Every ques-tion is scored using nugget recall (NR) and anapproximation to nugget precision (NP) based onanswer length.
The final score for a definitionresponse is computed using F-Measure.
In TREC2003, the ?
parameter was set to 5 indicatingthat recall is 5 times as important as precision(Voorhees, 2003).6A free IR tool, http://www.lemurproject.org/(14)                                )15(5)5( 22NRNPNRNPF++?
?==?in which,(15)uggetsl answer n# essentiareturnedl nuggets # essentiaNR =(16)      )(otherwise  , 1)(                     ,1?????
<=lengthallowance)(length --allowancelengthNPwhere allowance = 100 * (# essential + # ac-ceptable nuggets returned) and length = # non-white space characters in strings returned.5.1.3 Baseline systemWe employ the TFIDF heuristics algorithm-based approach as our baseline system, in whichthe candidate answers and the centroid aretreated as a bag of words.
(17)               lniiiii DFNTFIDFTFweight ?=?=where TFi gives the occurrences of term i. DF i 7is the number of documents containing term i. Ngives the total number of documents.For comparison purpose, the unigram model isadopted and its scoring function is similar withEquation (7).
The main difference is that we onlyconcern unigram probability P(ti|OC) in uni-gram-based scoring function.For all systems, we empirically set the thresh-old of answer length to 12 sentences for peopletargets (i.e., Aaron Copland), and 10 sentencesfor other targets (i.e., Quasars).5.2 Performance evaluationAs the first evaluation, we assess the perform-ance obtained by our language model methodagainst the baseline system without query expan-sion (QE).
The evaluation results are shown inTable 1.Average NR Average NP F(5)Baseline(TFIDF)0.469 0.221 0.432Unigram 0.508(+8.3%)0.204(-7.7%)0.459(+6.3%)Bigram 0.554(+18.1%)0.234(+5.9%)0.505(+16.9%)Biterm 0.567(+20.9%)0.222(+0.5%)0.511(+18.3%)Table 1.
Comparisons without QE.From Table 1, it is easy to observe that theunigram, bigram and biterm-based approachesimprove the F(5) by 6.3%, 16.9% and 18.3%against the baseline system respectively.
At thesame time, the bigram and biterm improves the7We also use British National Corpus (BNC) to estimate it.1085F(5) by 10.0% and 11.3% against the unigramrespectively.
The unigram slightly outperformthe baseline.
We also notice that the bitermmodel improves slightly over the bigram modelsince it ignores the order of term-occurrence.This observation coincides with the experimentalresults of Srikanth and Srihari (2002).
These re-sults show that the bigram and biterm modelsoutperform the VSM model and the unigrammodel dramatically.
It is a clear indication thatthe language model which takes into account theterm dependence among centroid vector is aneffective way to rerank answers.As mentioned above, QE is involved in oursystem.
In the second evaluation, we assess theperformance obtained by the language modelmethod against the baseline system with QE.
Welist the evaluation results in Table 2.Average NR Average NP F(5)Baseline(QE)0.508 0.207 0.462Unigram(QE)0.518(+2.0%)0.223(+7.7%)0.472(+2.2%)Bigram(QE)0.573(+12.8%)0.228(+10.1%)0.518(+12.1%)Biterm(QE)0.582(+14.6%)0.240(+15.9%)0.531(+14.9%)Table 2.
Comparisons with QE.From Table 2, we observe that, with QE, thebigram and biterm still outperform the baselinesystem (VSM) significantly by 12.1% (p8=0.03)and 14.9% (p=0.004) in F(5).
Furthermore, thebigram and biterm perform significantly betterthan the unigram by 9.7% (p=0.07) and 12.5%(p=0.02) in F(5) respectively.
This indicates thatthe term dependence is effective in keeping im-proving the performance.
It is easy to observethat the baseline is close to the unigram modelsince both two systems are based on the inde-pendent assumption.
We also notice that thebiterm model improves slightly over the bigrammodel.
At the same time, all of the four systemsimprove the performance against the correspond-ing system without QE.
The main reason is thatthe qualities of the centroid vector can be en-hanced with QE.
We are interested in the per-formance comparison with or without QE foreach system.
Through comparison it is found thatthe baseline system relies on QE more heavilythan our approach does.
With QE, the baselinesystem improves the performance by 6.9% andthe language model approaches improve the per-formance by 2.8%, 2.6% and 3.9%, respectively.8T-Test has been performed.F(5) performance comparison between thebaseline model and the biterm model for each of50 TREC questions is shown in Figure 3.
QE isused in both the baseline system and the bitermsystem.F(5) performance comparision for each question (Both with QE)00.20.40.60.811.21 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49Question IDF-5ScoreBaseline Our Biterm LMFigure 3.
Biterm vs. Baseline.We are also interested in the comparison withthe systems in TREC 2003.
The best F(5) scorereturned by our proposed approach is 0.531,which is close to the top 1 run in TREC 2003(Voorhees, 2003).
The F(5) score of the best sys-tem is 0.555, reported by BBN?s system (Xu etal., 2003).
In BBN?s experiments, the centroidvector was learned from the human made exter-nal knowledge resources, such as encyclopediaand the web.
Table 3 gives the comparison be-tween our biterm model-based system with theBBN?s run with different ?
values.F( ? )
ScoreRun Tag ?
=1 ?
=2 ?
=3 ?
=4 ?
=5BBN 0.310 0.423 0.493 0.532 0.555Ours 0.288 0.382 0.470 0.509 0.531Table 3.
Comparison with BBN?s run.5.3 Case studyA positive example returned by our proposedapproach is given below.
For Qid: 2304: ?Who isNiels Bohr?
?, the reference answers are given inTable 4 (only vital nuggets are listed):vital Danishvital Nuclear physicistvital Helped create atom bombvital Nobel Prize winnerTable 4.
Reference answers for question?Who is Niels Bohr?
?.Answers returned by the baseline system andour proposed system are presented in Table 5.System Returned answers (Partly)Baselinesystem1.
..., Niels Bohr, the great Danish scien-tist2.
...the German physicist WernerHeisenberg and the Danish physicist1086Niels Bohr3.
...took place between the Danishphysicist Niels Bohr and his onetimeprotege, the German scientist ...4.
... two great physicists, the Dane NielsBohr and Werner Heisenberg ...5.
...Proposedsystem1.
...physicist Werner Heisenberg travelto ... his colleague and old mentor,Niels Bohr, the great Danish scientist2.
... two great physicists, the Dane NielsBohr and Werner Heisen-berg ...3.
Today's Birthdays: ... Danish nuclearphysicist and Nobel Prize winner NielsBohr (1885-1962)4. the Danish atomic physicist, and hisGerman pupil, Werner Heisenberg, theauthor of the uncertainty principle5.
...Table 5.
Baseline vs. our system for question?Who is Niels Bohr?
?.From Table 5, it can be seen that the baselinesystem returned only one vital nugget: Danish(here we don?t think that physicist is equal tonuclear physicist semantically).
Our proposedsystem returned three vital nuggets: Danish, Nu-clear physicist, and Nobel Prize winner.
The an-swer sentence ?Today's Birthdays: ... Danish nu-clear physicist and Nobel Prize winner NielsBohr (1885-1962)?
contains more descriptiveinformation for the question target ?Niels Bohr?and is ranked 3rd in the top 12 answers in ourproposed system.5.4 Error analysisAlthough we have shown that the languagemodel-based approach significantly improves thesystem performance, there is still plenty of roomfor improvement.1) Sparseness of search results derogated thelearning of the ordered centroid: E.g.
: Qid2348: ?What is the medical condition shin-gles?
?, in which we treat the words ?medicalcondition shingles?
as the question target.We found that few sentences contain the tar-get ?medical condition shingles?.
We foundutilizing multiple search engines, such asMSN9, AltaVista10 might alleviate this prob-lem.
Besides, more effective smoothingtechniques could be promising.2) Term ambiguity: for some queries, the irre-lated documents are returned.
E.g., for Qid2267: ?Who is Alexander Pope?
?, all docu-ments returned from the IR tool Lemur for9http://www.msn.com10http://www.altavista.comthis question are about ?Pope John Paul II?,not ?Alexander Pope?.
This may be causedby the ambiguity of the word ?Pope?.
In thiscase, term disambiguation or adding someconstraint terms which are learned from theweb to the query to the AQUAINT corpusmight be helpful.6 Conclusions and Future WorkIn this paper, we presented a novel answerreranking method for definitional question.
Weuse bigram and biterm language models tocapture the term dependence.
Our contributioncan be summarized as follows:1) Word dependence is explored from orderedcentroid learned from snippets of a searchengine;2) Bigram and biterm models are presented tocapture the term dependence and rerank can-didate answers for definitional QA;3) Evaluation results show that both bigram andbiterm models outperform the VSM and uni-gram model significantly on TREC 2003 testset.In our experiments, centroid words werelearned from the returned snippets of a websearch engine.
In the future, we are interested inenhancing the centroid learning using humanknowledge sources such as encyclopedia.
In ad-dition, we will explore new smoothing tech-niques to enhance the interpolation method inour current approach.7 AcknowledgementsThe authors are grateful to Dr. Cheng Niu,Yunbo Cao for their valuable suggestions on thedraft of this paper.
We are indebted to ShiqiZhao, Shenghua Bao, Wei Yuan for their valu-able discussions about this paper.
We also thankDwight for his assistance to polish the English.Thanks also go to anonymous reviewers whosecomments have helped improve the final versionof this paper.ReferencesE.
Brill, J. Lin, M. Banko, S. Dumais and A. Ng.
2001.Data-Intensive Question Answering.
In Proceed-ings of the Tenth Text Retrieval Conference (TREC2001), Gaithersburg, MD, pp.
183-189.S.
Blair-Goldensohn, K.R.
McKeown and A. HazenSchlaikjer.
2003.
A Hybrid Approach for QATrack Definitional Questions.
In Proceedings ofthe Tenth Text Retrieval Conference (TREC 2003),pp.
336-343.1087S.
F. Chen and J. T. Goodman.
1996.
An empiricalstudy of smoothing techniques for language model-ing.
In Proceedings of the 34th Annual Meeting ofthe ACL, pp.
310-318.Hang Cui, Min-Yen Kan and Tat-Seng Chua.
2004.Unsupervised Learning of Soft Patterns for Defini-tional Question Answering.
In Proceedings of theThirteenth World Wide Web conference (WWW2004), New York, pp.
90-99.Guihong Cao, Jian-Yun Nie, and Jing Bai.
2005.
Inte-grating Word Relationships into Language Models.In Proceedings of the 28th Annual InternationalACM SIGIR Conference on Research and Devel-opment of Information Retrieval (SIGIR 2005),Salvador, Brazil.Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu andGuihong Cao.
2004.
Dependence language modelfor information retrieval.
In Proceedings of the 27thAnnual International ACM SIGIR Conference onResearch and Development of Information Re-trieval (SIGIR 2004), Sheffield, UK.Chin-Yew Lin.
2002.
The Effectiveness of Dictionaryand Web-Based Answer Reranking.
In Proceed-ings of the 19th International Conference on Com-putational Linguistics (COLING 2002), Taipei,Taiwan.Lafferty, J. and Zhai, C. 2001.
Document languagemodels, query models, and risk minimization forinformation retrieval.
In W.B.
Croft, D.J.
Harper,D.H.
Kraft, & J. Zobel (Eds.
), In Proceedings ofthe 24th Annual International ACM-SIGIR Confer-ence on Research and Development in InformationRetrieval, New Orleans, Louisiana, New York,pp.111-119.Magnini, B., Negri, M., Prevete, R., and Tanev, H.2002.
Is It the Right Answer?
Exploiting Web Re-dundancy for Answer Validation.
In Proceedingsof  the  40th  Annual Meeting  of  the  Associationfor  Computational Linguistics (ACL-2002), Phila-delphia, PA.Miller, D., Leek, T., and Schwartz, R. 1999.
A hiddenMarkov model information retrieval system.
InProceedings of the 22nd Annual International ACMSIGIR Conference, pp.
214-221.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2001.Bleu: a Method for Automatic Evaluation of Ma-chine Translation.
IBM Research Report rc22176(w0109022), Thomas J. Watson Research Center.Ponte, J., and Croft, W.B.
1998.
A language modelingapproach to information retrieval.
In Proceedingsof the 21st Annual International ACM-SIGIR Con-ference on Research and Development in Informa-tion Retrieval, New York, pp.275-281.J.
Prager, D. Radev, and K. Czuba.
2001.
Answeringwhat-is questions by virtual annotation.
In Pro-ceedings of the Human Language Technology Con-ference (HLT 2001), San Diego, CA.Deepak Ravichandran and Eduard Hovy.
2002.Learning Surface Text Patterns for a Question An-swering System.
In Proceedings of the 40th AnnualMeeting of the ACL, pp.
41-47.Song, F., and Croft, W.B.
1999.
A general languagemodel for information retrieval.
In Proceedings ofthe 22nd Annual International ACM-SIGIR Confer-ence on Research and Development in InformationRetrieval, New York, pp.279-280.Srikanth, M. and Srihari, R. 2002.
Biterm languagemodels for document retrieval.
In Proceedings ofthe 2002 ACM SIGIR Conference on Research andDevelopment in Information Retrieval,  Tampere,Finland.Ellen M. Voorhees.
2002.
Overview of the TREC2002 question answering track.
In Proceedings ofthe Eleventh Text REtrieval Conference (TREC2002).Ellen M. Voorhees.
2003.
Overview of the TREC2003 question answering track.
In Proceedings ofthe Twelfth Text REtrieval Conference (TREC2003).Ellen M. Voorhees.
2004.
Overview of the TREC2004 question answering track.
In Proceedings ofthe Twelfth Text REtrieval Conference (TREC2004).Lide Wu, Xuanjing Huang, Lan You, Zhushuo Zhang,Xin Li, and Yaqian Zhou.
2004.
FDUQA onTREC2004 QA Track.
In Proceedings of the Thir-teenth Text REtrieval Conference (TREC 2004).Jinxi Xu, Ana Licuanan, and Ralph Weischedel.
2003.TREC2003 QA at BBN: Answering definitionalquestions.
In Proceedings of the Twelfth Text RE-trieval Conference (TREC 2003).Jun Xu, Yunbo Cao, Hang Li and Min Zhao.
2005.Ranking Definitions with Supervised LearningMethods.
In Proceedings of 14th InternationalWorld Wide Web Conference (WWW 2005), Indus-trial and Practical Experience Track, Chiba, Japan,pp.811-819.Zhang D. and Lee WS.
2003.
A Language ModelingApproach to Passage Question Answering.
In Pro-ceedings of The 12th Text Retrieval Conference(TREC2003), NIST, Gaithersburg.Zhai, C, and Lafferty, J.
2001.
A Study of SmoothingMethods for Language Models Applied to Informa-tion Retrieval.
In Proceedings of the 2001 ACMSIGIR Conference on Research and Developmentin Information Retrieval, pp.
334-342.1088
