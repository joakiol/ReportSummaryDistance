Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 140?144,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsAn Improved Statistical Transfer System for French?EnglishMachine TranslationGreg Hanneman, Vamshi Ambati, Jonathan H. Clark, Alok Parlikar, Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213 USA{ghannema,vamshi,jhclark,aup,alavie}@cs.cmu.eduAbstractThis paper presents the Carnegie MellonUniversity statistical transfer MT systemsubmitted to the 2009 WMT shared taskin French-to-English translation.
We de-scribe a syntax-based approach that incor-porates both syntactic and non-syntacticphrase pairs in addition to a syntacticgrammar.
After reporting developmenttest results, we conduct a preliminary anal-ysis of the coverage and effectiveness ofthe system?s components.1 IntroductionThe statistical transfer machine translation groupat Carnegie Mellon University has been devel-oping a hybrid approach combining a traditionalrule-based MT system and its linguistically ex-pressive formalism with more modern techniquesof statistical data processing and search-based de-coding.
The Stat-XFER framework (Lavie, 2008)provides a general environment for building newMT systems of this kind.
For a given languagepair or data condition, the framework depends ontwo main resources extracted from parallel data: aprobabilistic bilingual lexicon, and a grammar ofprobabilistic synchronous context-free grammarrules.
Additional monolingual data, in the form ofan n-gram language model in the target language,is also used.
The statistical transfer framework op-erates in two stages.
First, the lexicon and gram-mar are applied to synchronously parse and trans-late an input sentence; all reordering is appliedduring this stage, driven by the syntactic grammar.Second, a monotonic decoder runs over the lat-tice of scored translation pieces produced duringparsing and assembles the highest-scoring overalltranslation according to a log-linear feature model.Since our submission to last year?s Workshopon Machine Translation shared translation task(Hanneman et al, 2008), we have made numerousimprovements and extensions to our resource ex-traction and processing methods, resulting in sig-nificantly improved translation scores.
In Section2 of this paper, we trace our current methods fordata resource management for the Stat-XFER sub-mission to the 2009 WMT shared French?Englishtranslation task.
Section 3 explains our tuning pro-cedure, and Section 4 gives our experimental re-sults on various development sets and offers somepreliminary analysis.2 System ConstructionBecause of the additional data resources providedfor the 2009 French?English task, our system thisyear is trained on nearly eight times as muchdata as last year?s.
We used three officially pro-vided data sets to make up a parallel corpus forsystem training: version 4 of the Europarl cor-pus (1.43 million sentence pairs), the News Com-mentary corpus (0.06 million sentence pairs), andthe pre-release version of the new Giga-FrEn cor-pus (8.60 million sentence pairs)1.
The combinedcorpus of 10.09 million sentence pairs was pre-processed to remove blank lines, sentences of 80words or more, and sentence pairs where the ra-tio between the number of English and Frenchwords was larger than 5 to 1 in either direction.These steps removed approximately 3% of the cor-pus.
Given the filtered corpus, our data prepara-tion pipeline proceeded according to the descrip-tions below.1Because of data processing time, we were unable to usethe larger verions 1 or 2 of Giga-FrEn released later in theevaluation period.1402.1 Parsing and Word AlignmentWe parsed both sides of our parallel corpus withindependent automatic constituency parsers.
Weused the Berkeley parser (Petrov and Klein, 2007)for both English and French, although we obtainedbetter results for French by tokenizing the datawith our own script as a preprocessing step andnot allowing the parser to change it.
There wereapproximately 220,000 English sentences that didnot return a parse, which further reduced the sizeof our training corpus by 2%.After parsing, we re-extracted the leaf nodesof the parse trees and statistically word-alignedthe corpus using a multi-threaded implementa-tion (Gao and Vogel, 2008) of the GIZA++ pro-gram (Och and Ney, 2003).
Unidirectional align-ments were symmetrized with the ?grow-diag-final?
heuristic (Koehn et al, 2005).2.2 Phrase Extraction and CombinationPhrase extraction for last year?s statistical transfersystem used automatically generated parse treeson both sides of the corpus as absolute constraints:a syntactic phrase pair was extracted from a givensentence only when a contiguous sequence of En-glish words exactly made up a syntactic con-stituent in the English parse tree and could alsobe traced though symmetric word alignments to aconstituent in the French parse tree.
While this?tree-to-tree?
extraction method is precise, it suf-fers from low recall and results in a low-coveragesyntactic phrase table.
Our 2009 system uses anextended ?tree-to-tree-string?
extraction process(Ambati and Lavie, 2008) in which, if no suit-able equivalent is found in the French parse treefor an English node, a copy of the English node isprojected into the French tree, where it spans theFrench words aligned to the yield of the Englishnode.
This method can result in a 50% increasein the number of extracted syntactic phrase pairs.Each extracted phrase pair retains a syntactic cat-egory label; in our current system, the node labelin the English parse tree is used as the category forboth sides of the bilingual phrase pair, although wesubsequently map the full set of labels used by theBerkeley parser down to a more general set of 19syntactic categories.We also ran ?standard?
phrase extraction on thesame corpus using Steps 4 and 5 of the Moses sta-tistical machine translation training script (Koehnet al, 2007).
The two types of phrases were thenmerged in a syntax-prioritized combination thatremoves all Moses-extracted phrase pairs that havesource sides already covered by the tree-to-tree-string syntactic phrase extraction.
The syntax pri-oritization has the advantage of still including a se-lection of non-syntactic phrases while producing amuch smaller phrase table than a direct combina-tion of all phrase pairs of both types.
Previous ex-periments we conducted indicated that this comeswith only a minor drop in automatic metric scores.In our current submission, we modify the proce-dure slightly by removing singleton phrase pairsfrom the syntactic table before the combinationwith Moses phrases.
The coverage of the com-bined table is not affected ?
our syntactic phraseextraction algorithm produces a subset of the non-syntactic phrase pairs extracted from Moses, up tophrase length constraints ?
but the removal al-lows Moses-extracted versions of some phrases tosurvive syntax prioritization.
In effect, we are lim-iting the set of category-labeled syntactic transla-tions we trust to those that have been seen morethan once in our training data.
For a given syn-tactic phrase pair, we also remove all but the mostfrequent syntactic category label for the pair; thisremoves a small number of entries from our lexi-con in order to limit label ambiguity, but does notaffect coverage.From our training data, we extracted 27.6 mil-lion unique syntactic phrase pairs after single-ton removal, reducing this set to 27.0 million en-tries after filtering for category label ambiguity.Some 488.7 million unique phrase pairs extractedfrom Moses were reduced to 424.0 million aftersyntax prioritization.
(The remaining 64.7 mil-lion phrase pairs had source sides already coveredby the 27.0 million syntactically extracted phrasepairs, so they were thrown out.)
This means non-syntactic phrases outnumber syntactic phrases bynearly 16 to 1.
However, when filtering the phrasetable to a particular development or test set, wefind the syntactic phrases play a larger role, as thisratio drops to approximately 3 to 1.Sample phrase pairs from our system are shownin Figure 1.
Each pair includes two rule scores,which we calculate from the source-side syntac-tic category (cs), source-side text (ws), target-sidecategory (ct), and target-side text (wt).
In thecase of Moses-extracted phrase pairs, we use the?dummy?
syntactic category PHR.
Rule score rt|sis a maximum likelihood estimate of the distri-141cs ct ws wt rt|s rs|tADJ ADJ espagnols Spanish 0.8278 0.1141N N repre?sentants officials 0.0653 0.1919NP NP repre?sentants de la Commission Commission officials 0.0312 0.0345PHR PHR haute importance a` very important to 0.0357 0.0008PHR PHR est charge?
de has responsibility for 0.0094 0.0760Figure 1: Sample lexical entries, including non-syntactic phrases, with rule scores (Equations 1 and 2).bution of target-language translations and source-and target-language syntactic categories given thesource string (Equation 1).
The rs|t score is simi-lar, but calculated in the reverse direction to give asource-given-target probability (Equation 2).rt|s =#(wt, ct, ws, cs)#(ws) + 1(1)rs|t =#(wt, ct, ws, cs)#(wt) + 1(2)Add-one smoothing in the denominators counter-acts overestimation of the rule scores of lexical en-tries with very infrequent source or target sides.2.3 Syntactic GrammarSyntactic phrase extraction specifies a node-to-node alignment across parallel parse trees.
If thesealigned nodes are used as decomposition points,a set of synchronous context-free rules that pro-duced the trees can be collected.
This is our pro-cess of syntactic grammar extraction (Lavie et al,2008).
For our 2009 WMT submission, we ex-tracted 11.0 million unique grammar rules, 9.1million of which were singletons, from our paral-lel parsed corpus.
These rules operate on our syn-tactically extracted phrase pairs, which have cat-egory labels, but they may also be partially lexi-calized with explicit source or target word strings.Each extracted grammar rule is scored accordingto Equations 1 and 2, where now the right-handsides of the rule are used as ws and wt.As yet, we have made only minimal use of theStat-XFER framework?s grammar capabilities, es-pecially for large-scale MT systems.
For the cur-rent submission, the syntactic grammar consistedof 26 manually chosen high-frequency grammarrules that carry out some reordering between En-glish and French.
Since rules for high-level re-ordering (near the top of the parse tree) are un-likely to be useful unless a large amount of parsestructure can first be built, we concentrate ourrules on low-level reorderings taking place withinor around small constituents.
Our focus for thisselection is the well-known repositioning of adjec-tives and adjective phrases when translating fromFrench to English, such as from le Parlement eu-rope?en to the European Parliament or from l?
in-tervention forte et substantielle to the strong andsubstantial intervention.
Our grammar thus con-sists of 23 rules for building noun phrases, tworules for building adjective phrases, and one rulefor building verb phrases.2.4 English Language ModelWe built a suffix-array language model (Zhang andVogel, 2006) on approximately 700 million wordsof monolingual data: the unfiltered English side ofour parallel training corpus, plus the 438 millionwords of English monolingual news data providedfor the WMT 2009 shared task.
With the relativelylarge amount of data available, we made the some-what unusual decision of building our languagemodel (and all other data resources for our system)in mixed case, which adds approximately 12.3%to our vocabulary size.
This saves us the need tobuild and run a recaser as a postprocessing stepon our output.
Our mixed-case decision may alsobe validated by preliminary test set results, whichshow that our submission has the smallest drop inBLEU score (0.0074) between uncased and casedevaluation of any system in the French?Englishtranslation task.3 System TuningStat-XFER uses a log-linear combination of sevenfeatures in its scoring of translation fragments:language model probability, source-given-targetand target-given-source rule probabilities, source-given-target and target-given-source lexical prob-abilities, a length score, and a fragmentation scorebased on the number of parsed translation frag-ments that make up the output sentence.
We tunethe weights for these features with several roundsof minimum error rate training, optimizing to-142Primary ContrastiveData Set METEOR BLEU TER METEOR BLEU TERnews-dev2009a-425 0.5437 0.2299 60.45 ?
?
?news-dev2009a-600 ?
?
?
0.5134 0.2055 63.46news-dev2009b 0.5263 0.2073 61.96 0.5303 0.2104 61.74nc-test2007 0.6194 0.3282 51.17 0.6195 0.3226 51.49Figure 2: Primary and contrastive system results on tuning and development test sets.wards the BLEU metric.
For each tuning itera-tion, we save the n-best lists output by the sys-tem from previous iterations and concatenate themonto the current n-best list in order to present theoptimizer with a larger variety of translation out-puts and score values.From the provided ?news-dev2009a?
develop-ment set we create two tuning sets: one using thefirst 600 sentences of the data, and a second usingthe remaining 425 sentences.
We tuned our sys-tem separately on each set, saving the additional?news-dev2009b?
set as a final development test tochoose our primary and contrastive submissions2.At run time, our full system takes on average be-tween four and seven seconds to translate each in-put sentence, depending on the size of the finalbilingual lexicon.4 Evaluation and AnalysisFigure 2 shows the results of our primary and con-trastive systems on four data sets.
First, we reportfinal (tuned) performance on our two tuning sets?
the last 425 sentences of news-dev2009a for theprimary system, and the first 600 sentences of thesame set for the contrastive.
We also include ourdevelopment test (news-dev2009b) and, for addi-tional comparison, the ?nc-test2007?
news com-mentary test set from the 2007 WMT shared task.For each, we give case-insensitive scores on ver-sion 0.6 of METEOR (Lavie and Agarwal, 2007)with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5of TER (Snover et al, 2006).From these results, we highlight two interest-ing areas of analysis.
First, the low tuning anddevelopment test set scores bring up questionsabout system coverage, given that the news do-main was not strongly represented in our system?s2Due to a data processing error, the choice of the primarysubmission was based on incorrectly computed scores.
Infact, the contrastive system has better performance on our de-velopment test set.training data.
We indeed find a significantly largerproportion of out-of-vocabulary (OOV) words innews-domain sets: the news-dev2009b set is trans-lated by our primary submission with 402 of 6263word types (6.42%) or 601 of 27,821 word tokens(2.16%) unknown.
The same system running onthe 2007 WMT ?test2007?
set of Europarl-deriveddata records an OOV rate of only 87 of 7514word types (1.16%) or 105 of 63,741 word tokens(0.16%).Second, we turn our attention to the usefulnessof the syntactic grammar.
Though small, we findit to be both beneficial and precise.
In the 1026-sentence news-dev2009b set, for example, we find351 rule applications ?
the vast majority of them(337) building noun phrases.
The three most fre-quently occurring rules are those for reordering thesequence [DET N ADJ] to [DET ADJ N] (52 oc-currences), the sequence [N ADJ] to [ADJ N] (51occurrences), and the sequence [N1 de N2] to [N2N1] (45 occurrences).
We checked precision bymanually reviewing the 52 rule applications in thefirst 150 sentences of news-dev2009b.
There, 41of the occurrences (79%) were judged to be cor-rect and beneficial to translation output.
Of theremainder, seven were judged incorrect or detri-mental and four were judged either neutral or ofunclear benefit.We expect to continue to analyze the output andeffectiveness of our system in the coming months.In particular, we would like to learn more aboutthe usefulness of our 26-rule grammar with theview of using significantly larger grammars in fu-ture versions of our system.AcknowledgmentsThis research was supported in part by NSF grantsIIS-0121631 (AVENUE) and IIS-0534217 (LE-TRAS), and by the DARPA GALE program.
Wethank Yahoo!
for the use of theM45 research com-puting cluster, where we ran the parsing stage ofour data processing.143ReferencesVamshi Ambati and Alon Lavie.
2008.
Improvingsyntax driven translation models by re-structuringdivergent and non-isomorphic parse tree structures.In Proceedings of the Eighth Conference of the As-sociation for Machine Translation in the Americas,pages 235?244, Waikiki, HI, October.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, pages 49?57, Columbus, OH,June.Greg Hanneman, Edmund Huber, Abhaya Agarwal,Vamshi Ambati, Alok Parlikar, Erik Peterson, andAlon Lavie.
2008.
Statistical transfer systemsfor French?English and German?English machinetranslation.
In Proceedings of the Third Workshopon Statistical Machine Translation, pages 163?166,Columbus, OH, June.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.In Proceedings of IWSLT 2005, Pittsburgh, PA, Oc-tober.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the ACL 2007 Demo andPoster Sessions, pages 177?180, Prague, Czech Re-public, June.Alon Lavie and Abhaya Agarwal.
2007.
METEOR:An automatic metric for MT evaluation with highlevels of correlation with human judgments.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 228?231, Prague, CzechRepublic, June.Alon Lavie, Alok Parlikar, and Vamshi Ambati.
2008.Syntax-driven learning of sub-sentential translationequivalents and translation rules from parsed parallelcorpora.
In Proceedings of the Second ACL Work-shop on Syntax and Structure in Statistical Transla-tion, pages 87?95, Columbus, OH, June.Alon Lavie.
2008.
Stat-XFER: A general search-basedsyntax-driven framework for machine translation.
InComputational Linguistics and Intelligent Text Pro-cessing, Lecture Notes in Computer Science, pages362?375.
Springer.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevalution of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,PA, July.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACLHLT 2007, pages 404?411, Rochester, NY, April.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the Seventh Conference of the As-sociation for Machine Translation in the Americas,pages 223?231, Cambridge, MA, August.Ying Zhang and Stephan Vogel.
2006.
Suffix arrayand its applications in empirical natural languageprocessing.
Technical Report CMU-LTI-06-010,Carnegie Mellon University, Pittsburgh, PA, Decem-ber.144
