Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1278?1287,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsProfiting from Mark-Up: Hyper-Text Annotations for Guided ParsingValentin I. SpitkovskyComputer Science DepartmentStanford University and Google Inc.valentin@google.comDaniel JurafskyDepartments of Linguistics andComputer Science, Stanford Universityjurafsky@stanford.eduHiyan AlshawiGoogle Inc.hiyan@google.comAbstractWe show how web mark-up can be usedto improve unsupervised dependency pars-ing.
Starting from raw bracketings of fourcommon HTML tags (anchors, bold, ital-ics and underlines), we refine approximatepartial phrase boundaries to yield accurateparsing constraints.
Conversion proce-dures fall out of our linguistic analysis ofa newly available million-word hyper-textcorpus.
We demonstrate that derived con-straints aid grammar induction by trainingKlein and Manning?s Dependency Modelwith Valence (DMV) on this data set: pars-ing accuracy on Section 23 (all sentences)of the Wall Street Journal corpus jumpsto 50.4%, beating previous state-of-the-art by more than 5%.
Web-scale exper-iments show that the DMV, perhaps be-cause it is unlexicalized, does not benefitfrom orders of magnitude more annotatedbut noisier data.
Our model, trained on asingle blog, generalizes to 53.3% accuracyout-of-domain, against the Brown corpus?
nearly 10% higher than the previouspublished best.
The fact that web mark-upstrongly correlates with syntactic structuremay have broad applicability in NLP.1 IntroductionUnsupervised learning of hierarchical syntacticstructure from free-form natural language text isa hard problem whose eventual solution promisesto benefit applications ranging from question an-swering to speech recognition and machine trans-lation.
A restricted version of this problem that tar-gets dependencies and assumes partial annotation?
sentence boundaries and part-of-speech (POS)tagging ?
has received much attention.
Kleinand Manning (2004) were the first to beat a sim-ple parsing heuristic, the right-branching baseline;today?s state-of-the-art systems (Headden et al,2009; Cohen and Smith, 2009; Spitkovsky et al,2010a) are rooted in their Dependency Model withValence (DMV), still trained using variants of EM.Pereira and Schabes (1992) outlined three ma-jor problems with classic EM, applied to a relatedproblem, constituent parsing.
They extended clas-sic inside-outside re-estimation (Baker, 1979) torespect any bracketing constraints included witha training corpus.
This conditioning on partialparses addressed all three problems, leading to:(i) linguistically reasonable constituent boundariesand induced grammars more likely to agree withqualitative judgments of sentence structure, whichis underdetermined by unannotated text; (ii) feweriterations needed to reach a good grammar, coun-tering convergence properties that sharply deterio-rate with the number of non-terminal symbols, dueto a proliferation of local maxima; and (iii) better(in the best case, linear) time complexity per it-eration, versus running time that is ordinarily cu-bic in both sentence length and the total num-ber of non-terminals, rendering sufficiently largegrammars computationally impractical.
Their al-gorithm sometimes found good solutions frombracketed corpora but not from raw text, sup-porting the view that purely unsupervised, self-organizing inference methods can miss the treesfor the forest of distributional regularities.
Thiswas a promising break-through, but the problemof whence to get partial bracketings was left open.We suggest mining partial bracketings from acheap and abundant natural language resource: thehyper-text mark-up that annotates web-pages.
Forexample, consider that anchor text can match lin-guistic constituents, such as verb phrases, exactly:..., whereas McCain is secure on the topic, Obama<a>[VP worries about winning the pro-Israel vote]</a>.To validate this idea, we created a new data set,novel in combining a real blog?s raw HTML withtree-bank-like constituent structure parses, gener-1278ated automatically.
Our linguistic analysis of themost prevalent tags (anchors, bold, italics and un-derlines) over its 1M+ words reveals a strong con-nection between syntax and mark-up (all of ourexamples draw from this corpus), inspiring severalsimple techniques for automatically deriving pars-ing constraints.
Experiments with both hard andmore flexible constraints, as well as with differentstyles and quantities of annotated training data ?the blog, web news and the web itself, confirm thatmark-up-induced constraints consistently improve(otherwise unsupervised) dependency parsing.2 Intuition and Motivating ExamplesIt is natural to expect hidden structure to seepthrough when a person annotates a sentence.
As ithappens, a non-trivial fraction of the world?s pop-ulation routinely annotates text diligently, if onlypartially and informally.1 They inject hyper-links,vary font sizes, and toggle colors and styles, usingmark-up technologies such as HTML and XML.As noted, web annotations can be indicative ofphrase boundaries, e.g., in a complicated sentence:In 1998, however, as I <a>[VP established in<i>[NP The New Republic]</i>]</a> and BillClinton just <a>[VP confirmed in his memoirs]</a>,Netanyahu changed his mind and ...In doing so, mark-up sometimes offers useful cueseven for low-level tokenization decisions:[NP [NP Libyan ruler]<a>[NP Mu?ammar al-Qaddafi]</a>] referred to ...(NP (ADJP (NP (JJ Libyan) (NN ruler))(JJ Mu))(??
?)
(NN ammar) (NNS al-Qaddafi))Above, a backward quote in an Arabic name con-fuses the Stanford parser.2 Yet mark-up lines upwith the broken noun phrase, signals cohesion, andmoreover sheds light on the internal structure ofa compound.
As Vadas and Curran (2007) pointout, such details are frequently omitted even frommanually compiled tree-banks that err on the sideof flat annotations of base-NPs.Admittedly, not all boundaries between HTMLtags and syntactic constituents match up nicely:..., but [S [NP the <a><i>TorontoStar</i>][VP reports [NP this][PP in thesoftest possible way]</a>,[S stating only that ...]]]Combining parsing with mark-up may not bestraight-forward, but there is hope: even above,1Even when (American) grammar schools lived up to theirname, they only taught dependencies.
This was back in thedays before constituent grammars were invented.2http://nlp.stanford.edu:8080/parser/one of each nested tag?s boundaries aligns; andToronto Star?s neglected determiner could be for-given, certainly within a dependency formulation.3 A High-Level Outline of Our ApproachOur idea is to implement the DMV (Klein andManning, 2004) ?
a standard unsupervised gram-mar inducer.
But instead of learning the unan-notated test set, we train with text that containsweb mark-up, using various ways of convertingHTML into parsing constraints.
We still test onWSJ (Marcus et al, 1993), in the standard way,and also check generalization against a hiddendata set ?
the Brown corpus (Francis and Kucera,1979).
Our parsing constraints come from a blog?
a new corpus we created, the web and news (seeTable 1 for corpora?s sentence and token counts).To facilitate future work, we make the finalmodels and our manually-constructed blog datapublicly available.3 Although we are unableto share larger-scale resources, our main resultsshould be reproducible, as both linguistic analysisand our best model rely exclusively on the blog.Corpus Sentences POS TokensWSJ?
49,208 1,028,347Section 23 2,353 48,201WSJ45 48,418 986,830WSJ15 15,922 163,715Brown100 24,208 391,796BLOGp 57,809 1,136,659BLOGt45 56,191 1,048,404BLOGt15 23,214 212,872NEWS45 2,263,563,078 32,119,123,561NEWS15 1,433,779,438 11,786,164,503WEB45 8,903,458,234 87,269,385,640WEB15 7,488,669,239 55,014,582,024Table 1: Sizes of corpora derived from WSJ andBrown, as well as those we collected from the web.4 Data Sets for Evaluation and TrainingThe appeal of unsupervised parsing lies in its abil-ity to learn from surface text alone; but (intrinsic)evaluation still requires parsed sentences.
Follow-ing Klein and Manning (2004), we begin with ref-erence constituent parses and compare against de-terministically derived dependencies: after prun-ing out all empty subtrees, punctuation and ter-minals (tagged # and $) not pronounced wherethey appear, we drop all sentences with morethan a prescribed number of tokens remaining anduse automatic ?head-percolation?
rules (Collins,1999) to convert the rest, as is standard practice.3http://cs.stanford.edu/?valentin/1279Length Marked POS Bracketings Length Marked POS BracketingsCutoff Sentences Tokens All Multi-Token Cutoff Sentences Tokens All Multi-Token0 6,047 1,136,659 7,731 6,015 8 485 14,528 710 6841 of 57,809 149,483 7,731 6,015 9 333 10,484 499 4792 4,934 124,527 6,482 6,015 10 245 7,887 365 3523 3,295 85,423 4,476 4,212 15 42 1,519 65 634 2,103 56,390 2,952 2,789 20 13 466 20 205 1,402 38,265 1,988 1,874 25 6 235 10 106 960 27,285 1,365 1,302 30 3 136 6 67 692 19,894 992 952 40 0 0 0 0Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only thosesentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).Our primary reference sets are derived from thePenn English Treebank?s Wall Street Journal por-tion (Marcus et al, 1993): WSJ45 (sentences withfewer than 46 tokens) and Section 23 of WSJ?
(allsentence lengths).
We also evaluate on Brown100,similarly derived from the parsed portion of theBrown corpus (Francis and Kucera, 1979).
Whilewe use WSJ45 and WSJ15 to train baseline mod-els, the bulk of our experiments is with web data.4.1 A News-Style Blog: Daniel PipesSince there was no corpus overlaying syntacticstructure with mark-up, we began constructing anew one by downloading articles4 from a news-style blog.
Although limited to a single genre ?political opinion, danielpipes.org is clean, consis-tently formatted, carefully edited and larger thanWSJ (see Table 1).
Spanning decades, Pipes?editorials are mostly in-domain for POS taggersand tree-bank-trained parsers; his recent (internet-era) entries are thoroughly cross-referenced, con-veniently providing just the mark-up we hoped tostudy via uncluttered (printer-friendly) HTML.5After extracting moderately clean text andmark-up locations, we used MxTerminator (Rey-nar and Ratnaparkhi, 1997) to detect sentenceboundaries.
This initial automated pass begot mul-tiple rounds of various semi-automated clean-upsthat involved fixing sentence breaking, modifyingparser-unfriendly tokens, converting HTML enti-ties and non-ASCII text, correcting typos, and soon.
After throwing away annotations of fractionalwords (e.g., <i>basmachi</i>s) and tokens (e.g.,<i>Sesame Street</i>-like), we broke up all mark-up that crossed sentence boundaries (i.e., looselyspeaking, replaced constructs like <u>...][S...</u>with <u>...</u> ][S <u>...</u>) and discarded any4http://danielpipes.org/art/year/all5http://danielpipes.org/article print.php?id=.
.
.tags left covering entire sentences.We finalized two versions of the data: BLOGt,tagged with the Stanford tagger (Toutanova andManning, 2000; Toutanova et al, 2003),6 andBLOGp, parsed with Charniak?s parser (Charniak,2001; Charniak and Johnson, 2005).7 The rea-son for this dichotomy was to use state-of-the-artparses to analyze the relationship between syntaxand mark-up, yet to prevent jointly tagged (andnon-standard AUX[G]) POS sequences from interfer-ing with our (otherwise unsupervised) training.84.2 Scaled up Quantity: The (English) WebWe built a large (see Table 1) but messy data set,WEB ?
English-looking web-pages, pre-crawledby a search engine.
To avoid machine-generatedspam, we excluded low quality sites flagged by theindexing system.
We kept only sentence-like runsof words (satisfying punctuation and capitalizationconstraints), POS-tagged with TnT (Brants, 2000).4.3 Scaled up Quality: (English) Web NewsIn an effort to trade quantity for quality, we con-structed a smaller, potentially cleaner data set,NEWS.
We reckoned editorialized content wouldlead to fewer extracted non-sentences.
Perhapssurprisingly, NEWS is less than an order of magni-tude smaller than WEB (see Table 1); in part, thisis due to less aggressive filtering ?
we trust sitesapproved by the human editors at Google News.9In all other respects, our pre-processing of NEWSpages was identical to our handling of WEB data.6http://nlp.stanford.edu/software/stanford-postagger-2008-09-28.tar.gz7ftp://ftp.cs.brown.edu/pub/nlparser/parser05Aug16.tar.gz8However, since many taggers are themselves trained onmanually parsed corpora, such as WSJ, no parser that relieson external POS tags could be considered truly unsupervised;for a fully unsupervised example, see Seginer?s (2007) CCLparser, available at http://www.seggu.net/ccl/9http://news.google.com/12805 Linguistic Analysis of Mark-UpIs there a connection between mark-up and syn-tactic structure?
Previous work (Barr et al, 2008)has only examined search engine queries, show-ing that they consist predominantly of short nounphrases.
If web mark-up shared a similar char-acteristic, it might not provide sufficiently dis-ambiguating cues to syntactic structure: HTMLtags could be too short (e.g., singletons like?click <a>here</a>?)
or otherwise unhelpful in re-solving truly difficult ambiguities (such as PP-attachment).
We began simply by counting vari-ous basic events in BLOGp.Count POS Sequence Frac Sum1 1,242 NNP NNP 16.1%2 643 NNP 8.3 24.43 419 NNP NNP NNP 5.4 29.84 414 NN 5.4 35.25 201 JJ NN 2.6 37.86 138 DT NNP NNP 1.8 39.57 138 NNS 1.8 41.38 112 JJ 1.5 42.89 102 VBD 1.3 44.110 92 DT NNP NNP NNP 1.2 45.311 85 JJ NNS 1.1 46.412 79 NNP NN 1.0 47.413 76 NN NN 1.0 48.414 61 VBN 0.8 49.215 60 NNP NNP NNP NNP 0.8 50.0BLOGp +3,869 more with Count ?
49 50.0%Table 3: Top 50% of marked POS tag sequences.Count Non-Terminal Frac Sum1 5,759 NP 74.5%2 997 VP 12.9 87.43 524 S 6.8 94.24 120 PP 1.6 95.75 72 ADJP 0.9 96.76 61 FRAG 0.8 97.47 41 ADVP 0.5 98.08 39 SBAR 0.5 98.59 19 PRN 0.2 98.710 18 NX 0.2 99.0BLOGp +81 more with Count ?
16 1.0%Table 4: Top 99% of dominating non-terminals.5.1 Surface Text StatisticsOut of 57,809 sentences, 6,047 (10.5%) are anno-tated (see Table 2); and 4,934 (8.5%) have multi-token bracketings.
We do not distinguish HTMLtags and track only unique bracketing end-pointswithin a sentence.
Of these, 6,015 are multi-token?
an average per-sentence yield of 10.4%.1010A non-trivial fraction of our corpus is older (pre-internet)unannotated articles, so this estimate may be conservative.As expected, many of the annotated words arenouns, but there are adjectives, verbs and otherparts of speech too (see Table 3).
Mark-up is short,typically under five words, yet (by far) the mostfrequently marked sequence of POS tags is a pair.5.2 Common Syntactic SubtreesFor three-quarters of all mark-up, the lowest domi-nating non-terminal is a noun phrase (see Table 4);there are also non-trace quantities of verb phrases(12.9%) and other phrases, clauses and fragments.Of the top fifteen ?
35.2% of all ?
annotatedproductions, only one is not a noun phrase (see Ta-ble 5, left).
Four of the fifteen lowest dominatingnon-terminals do not match the entire bracketing?
all four miss the leading determiner, as we sawearlier.
In such cases, we recursively split internalnodes until the bracketing aligned, as follows:[S [NP the <a>Toronto Star][VP reports [NP this][PP in the softest possible way]</a>,[S stating ...]]]S?
NP VP?
DT NNP NNP VBZ NP PP SWe can summarize productions more compactlyby using a dependency framework and clippingoff any dependents whose subtrees do not cross abracketing boundary, relative to the parent.
Thus,DT NNP NNP VBZ DT IN DT JJS JJ NNbecomes DT NNP VBZ, ?the <a>Star reports</a>.
?Viewed this way, the top fifteen (now collapsed)productions cover 59.4% of all cases and includefour verb heads, in addition to a preposition andan adjective (see Table 5, right).
This exposes fivecases of inexact matches, three of which involveneglected determiners or adjectives to the left ofthe head.
In fact, the only case that cannot be ex-plained by dropped dependents is #8, where thedaughters are marked but the parent is left out.Most instances contributing to this pattern are flatNPs that end with a noun, incorrectly assumed tobe the head of all other words in the phrase, e.g.,...
[NP a 1994 <i>New Yorker</i> article] ...As this example shows, disagreements (as wellas agreements) between mark-up and machine-generated parse trees with automatically perco-lated heads should be taken with a grain of salt.1111In a relatively recent study, Ravi et al (2008) reportthat Charniak?s re-ranking parser (Charniak and Johnson,2005) ?
reranking-parserAug06.tar.gz, also availablefrom ftp://ftp.cs.brown.edu/pub/nlparser/ ?
at-tains 86.3% accuracy when trained on WSJ and tested againstBrown; its nearly 5% performance loss out-of-domain is con-sistent with the numbers originally reported by Gildea (2001).1281Count Constituent Production Frac Sum1 746 NP?
NNP NNP 9.6%2 357 NP?
NNP 4.6 14.33 266 NP?
NP PP 3.4 17.74 183 NP?
NNP NNP NNP 2.4 20.15 165 NP?
DT NNP NNP 2.1 22.26 140 NP?
NN 1.8 24.07 131 NP?
DT NNP NNP NNP 1.7 25.78 130 NP?
DT NN 1.7 27.49 127 NP?
DT NNP NNP 1.6 29.010 109 S ?
NP VP 1.4 30.411 91 NP?
DT NNP NNP NNP 1.2 31.612 82 NP?
DT JJ NN 1.1 32.713 79 NP?
NNS 1.0 33.714 65 NP?
JJ NN 0.8 34.515 60 NP?
NP NP 0.8 35.3BLOGp +5,000 more with Count ?
60 64.7%Count Head-Outward Spawn Frac Sum1 1,889 NNP 24.4%2 623 NN 8.1 32.53 470 DT NNP 6.1 38.64 458 DT NN 5.9 44.55 345 NNS 4.5 49.06 109 NNPS 1.4 50.47 98 VBG 1.3 51.68 96 NNP NNP NN 1.2 52.99 80 VBD 1.0 53.910 77 IN 1.0 54.911 74 VBN 1.0 55.912 73 DT JJ NN 0.9 56.813 71 VBZ 0.9 57.714 69 POS NNP 0.9 58.615 63 JJ 0.8 59.4BLOGp +3,136 more with Count ?
62 40.6%Table 5: Top 15 marked productions, viewed as constituents (left) and as dependencies (right), afterrecursively expanding any internal nodes that did not align with the bracketing (underlined).
Tabulateddependencies were collapsed, dropping any dependents that fell entirely in the same region as their parent(i.e., both inside the bracketing, both to its left or both to its right), keeping only crossing attachments.5.3 Proposed Parsing ConstraintsThe straight-forward approach ?
forcing mark-upto correspond to constituents ?
agrees with Char-niak?s parse trees only 48.0% of the time, e.g.,...
in [NP<a>[NP an analysis]</a>[PP of perhaps themost astonishing PC item I have yet stumbled upon]].This number should be higher, as the vast major-ity of disagreements are due to tree-bank idiosyn-crasies (e.g., bare NPs).
Earlier examples of in-complete constituents (e.g., legitimately missingdeterminers) would also be fine in many linguistictheories (e.g., as N-bars).
A dependency formula-tion is less sensitive to such stylistic differences.We begin with the hardest possible constraint ondependencies, then slowly relax it.
Every exampleused to demonstrate a softer constraint doublesas a counter-example against all previous versions.?
strict ?
seals mark-up into attachments, i.e.,inside a bracketing, enforces exactly one externalarc ?
into the overall head.
This agrees withhead-percolated trees just 35.6% of the time, e.g.,As author of <i>The Satanic Verses</i>, I ...?
loose ?
same as strict, but allows the bracket-ing?s head word to have external dependents.
Thisrelaxation already agrees with head-percolated de-pendencies 87.5% of the time, catching many(though far from all) dropped dependents, e.g.,.
.
.
the <i>Toronto Star</i> reports .
.
.?
sprawl ?
same as loose, but now allows allwords inside a bracketing to attach external de-pendents.12 This boosts agreement with head-percolated trees to 95.1%, handling new cases,e.g., where ?Toronto Star?
is embedded in longermark-up that includes its own parent ?
a verb:.
.
.
the <a>Toronto Star reports .
.
.</a> .
.
.?
tear ?
allows mark-up to fracture after all,requiring only that the external heads attaching thepieces lie to the same side of the bracketing.
Thispropels agreement with percolated dependenciesto 98.9%, fixing previously broken PP-attachmentambiguities, e.g., a fused phrase like ?Fox News inCanada?
that detached a preposition from its verb:... concession ... has raised eyebrows among thosewaiting [PP for <a>Fox News][PP in Canada]</a>.Most of the remaining 1.1% of disagreements aredue to parser errors.
Nevertheless, it is possible formark-up to be torn apart by external heads fromboth sides.
We leave this section with a (very rare)true negative example.
Below, ?CSA?
modifies?authority?
(to its left), appositively, while ?Al-Manar?
modifies ?television?
(to its right):13The French broadcasting authority, <a>CSA, banned... Al-Manar</a> satellite television from ...12This view evokes the trapezoids of the O(n3) recognizerfor split head automaton grammars (Eisner and Satta, 1999).13But this is a stretch, since the comma after ?CSA?
ren-ders the marked phrase ungrammatical even out of context.12826 Experimental Methods and MetricsWe implemented the DMV (Klein and Manning,2004), consulting the details of (Spitkovsky et al,2010a).
Crucially, we swapped out inside-outsidere-estimation in favor of Viterbi training.
Not onlyis it better-suited to the general problem (see ?7.1),but it also admits a trivial implementation of (mostof) the dependency constraints we proposed.145 10 15 20 25 30 35 40 454.55.05.5WSJkbptlowest cross-entropy (4.32bpt) attained at WSJ8x-Entropy h (in bits per token) on WSJ15Figure 1: Sentence-level cross-entropy on WSJ15for Ad-Hoc?
initializers of WSJ{1, .
.
.
, 45}.Six settings parameterized each run:?
INIT: 0?
default, uniform initialization; or1 ?
a high quality initializer, pre-trained usingAd-Hoc?
(Spitkovsky et al, 2010a): we chose theLaplace-smoothed model trained at WSJ15 (the?sweet spot?
data gradation) but initialized offWSJ8, since that ad-hoc harmonic initializer hasthe best cross-entropy on WSJ15 (see Figure 1).?
GENRE: 0?
default, baseline training on WSJ;else, uses 1?
BLOGt; 2?
NEWS; or 3?
WEB.?
SCOPE: 0 ?
default, uses all sentences up tolength 45; if 1, trains using sentences up to length15; if 2, re-trains on sentences up to length 45,starting from the solution to sentences up to length15, as recommended by Spitkovsky et al (2010a).?
CONSTR: if 4, strict; if 3, loose; and if 2,sprawl.
We did not implement level 1, tear.
Over-constrained sentences are re-attempted at succes-sively lower levels until they become possible toparse, if necessary at the lowest (default) level 0.15?
TRIM: if 1, discards any sentence without a sin-gle multi-token mark-up (shorter than its length).?
ADAPT: if 1, upon convergence, initializes re-training on WSJ45 using the solution to <GENRE>,attempting domain adaptation (Lee et al, 1991).These make for 294 meaningful combinations.
Wejudged each one by its accuracy on WSJ45, usingstandard directed scoring ?
the fraction of correctdependencies over randomized ?best?
parse trees.14We analyze the benefits of Viterbi training in a compan-ion paper (Spitkovsky et al, 2010b), which dedicates morespace to implementation and to the WSJ baselines used here.15At level 4, <b> X<u> Y</b> Z</u> is over-constrained.7 Discussion of Experimental ResultsEvaluation on Section 23 of WSJ and Brown re-veals that blog-training beats all published state-of-the-art numbers in every traditionally-reportedlength cutoff category, with news-training not farbehind.
Here is a mini-preview of these results, forSection 23 of WSJ10 and WSJ?
(from Table 8):WSJ10 WSJ?
(Cohen and Smith, 2009) 62.0 42.2(Spitkovsky et al, 2010a) 57.1 45.0NEWS-best 67.3 50.1BLOGt-best 69.3 50.4(Headden et al, 2009) 68.8Table 6: Directed accuracies on Section 23 ofWSJ{10,? }
for three recent state-of-the-art sys-tems and our best runs (as judged against WSJ45)for NEWS and BLOGt (more details in Table 8).Since our experimental setup involved testingnearly three hundred models simultaneously, wemust take extreme care in analyzing and interpret-ing these results, to avoid falling prey to any loom-ing ?data-snooping?
biases.16 In a sufficientlylarge pool of models, where each is trained usinga randomized and/or chaotic procedure (such asours), the best may look good due to pure chance.We appealed to three separate diagnostics to con-vince ourselves that our best results are not noise.The most radical approach would be to write offWSJ as a development set and to focus only on theresults from the held-out Brown corpus.
It was ini-tially intended as a test of out-of-domain general-ization, but since Brown was in no way involvedin selecting the best models, it also qualifies asa blind evaluation set.
We observe that our bestmodels perform even better (and gain more ?
seeTable 8) on Brown than on WSJ ?
a strong indi-cation that our selection process has not overfitted.Our second diagnostic is a closer look at WSJ.Since we cannot graph the full (six-dimensional)set of results, we begin with a simple linear re-gression, using accuracy on WSJ45 as the depen-dent variable.
We prefer this full factorial designto the more traditional ablation studies because itallows us to account for and to incorporate everysingle experimental data point incurred along the16In the standard statistical hypothesis testing setting, itis reasonable to expect that p% of randomly chosen hy-potheses will appear significant at the p% level simply bychance.
Consequently, multiple hypothesis testing requiresre-evaluating significance levels ?
adjusting raw p-values,e.g., using the Holm-Bonferroni method (Holm, 1979).1283Corpus Marked Sentences All Sentences POS Tokens All Bracketings Multi-Token BracketingsBLOGt45 5,641 56,191 1,048,404 7,021 5,346BLOG?t45 4,516 4,516 104,267 5,771 5,346BLOGt15 1,562 23,214 212,872 1,714 1,240BLOG?t15 1,171 1,171 11,954 1,288 1,240NEWS45 304,129,910 2,263,563,078 32,119,123,561 611,644,606 477,362,150NEWS?45 205,671,761 205,671,761 2,740,258,972 453,781,081 392,600,070NEWS15 211,659,549 1,433,779,438 11,786,164,503 365,145,549 274,791,675NEWS?15 147,848,358 147,848,358 1,397,562,474 272,223,918 231,029,921WEB45 1,577,208,680 8,903,458,234 87,269,385,640 3,309,897,461 2,459,337,571WEB?45 933,115,032 933,115,032 11,552,983,379 2,084,359,555 1,793,238,913WEB15 1,181,696,194 7,488,669,239 55,014,582,024 2,071,743,595 1,494,675,520WEB?15 681,087,020 681,087,020 5,813,555,341 1,200,980,738 1,072,910,682Table 7: Counts of sentences, tokens and (unique) bracketings for web-based data sets; trimmed versions,restricted to only those sentences having at least one multi-token bracketing, are indicated by a prime (?).way.
Its output is a coarse, high-level summary ofour runs, showing which factors significantly con-tribute to changes in error rate on WSJ45:Parameter (Indicator) Setting ??
p-valueINIT 1 ad-hoc @WSJ8,15 11.8 ***GENRE 1 BLOGt -3.7 0.062 NEWS -5.3 **3 WEB -7.7 ***SCOPE 1 @15 -0.5 0.402 @15?45 -0.4 0.53CONSTR 2 sprawl 0.9 0.233 loose 1.0 0.154 strict 1.8 *TRIM 1 drop unmarked -7.4 ***ADAPT 1 WSJ re-training 1.5 **Intercept (R2Adjusted = 73.6%) 39.9 ***We use a standard convention: *** for p < 0.001;** for p < 0.01 (very signif.
); and * for p < 0.05 (signif.
).The default training mode (all parameters zero) isestimated to score 39.9%.
A good initializer givesthe biggest (double-digit) gain; both domain adap-tation and constraints also make a positive impact.Throwing away unannotated data hurts, as doestraining out-of-domain (the blog is least bad; theweb is worst).
Of course, this overview should notbe taken too seriously.
Overly simplistic, a firstorder model ignores interactions between parame-ters.
Furthermore, a least squares fit aims to cap-ture central tendencies, whereas we are more in-terested in outliers ?
the best-performing runs.A major imperfection of the simple regressionmodel is that helpful factors that require an in-teraction to ?kick in?
may not, on their own, ap-pear statistically significant.
Our third diagnosticis to examine parameter settings that give rise tothe best-performing models, looking out for com-binations that consistently deliver superior results.7.1 WSJ BaselinesJust two parameters apply to learning from WSJ.Five of their six combinations are state-of-the-art,demonstrating the power of Viterbi training; onlythe default run scores worse than 45.0%, attainedby Leapfrog (Spitkovsky et al, 2010a), on WSJ45:Settings SCOPE=0 SCOPE=1 SCOPE=2INIT=0 41.3 45.0 45.21 46.6 47.5 47.6@45 @15 @15?457.2 BlogSimply training on BLOGt instead of WSJ hurts:GENRE=1 SCOPE=0 SCOPE=1 SCOPE=2INIT=0 39.6 36.9 36.91 46.5 46.3 46.4@45 @15 @15?45The best runs use a good initializer, discard unan-notated sentences, enforce the loose constraint onthe rest, follow up with domain adaptation andbenefit from re-training ?
GENRE=TRIM=ADAPT=1:INIT=1 SCOPE=0 SCOPE=1 SCOPE=2CONSTR=0 45.8 48.3 49.6(sprawl) 2 46.3 49.2 49.2(loose) 3 41.3 50.2 50.4(strict) 4 40.7 49.9 48.7@45 @15 @15?45The contrast between unconstrained learning andannotation-guided parsing is higher for the defaultinitializer, still using trimmed data sets (just over athousand sentences for BLOG?t15 ?
see Table 7):INIT=0 SCOPE=0 SCOPE=1 SCOPE=2CONSTR=0 25.6 19.4 19.3(sprawl) 2 25.2 22.7 22.5(loose) 3 32.4 26.3 27.3(strict) 4 36.2 38.7 40.1@45 @15 @15?45Above, we see a clearer benefit to our constraints.12847.3 NewsTraining on WSJ is also better than using NEWS:GENRE=2 SCOPE=0 SCOPE=1 SCOPE=2INIT=0 40.2 38.8 38.71 43.4 44.0 43.8@45 @15 @15?45As with the blog, the best runs use the good initial-izer, discard unannotated sentences, enforce theloose constraint and follow up with domain adap-tation ?
GENRE=2; INIT=TRIM=ADAPT=1:Settings SCOPE=0 SCOPE=1 SCOPE=2CONSTR=0 46.6 45.4 45.2(sprawl) 2 46.1 44.9 44.9(loose) 3 49.5 48.1 48.3(strict) 4 37.7 36.8 37.6@45 @15 @15?45With all the extra training data, the best new scoreis just 49.5%.
On the one hand, we are disap-pointed by the lack of dividends to orders of mag-nitude more data.
On the other, we are comfortedthat the system arrives within 1% of its best result?
50.4%, obtained with a manually cleaned upcorpus ?
now using an auto-generated data set.7.4 WebThe WEB-side story is more discouraging:GENRE=3 SCOPE=0 SCOPE=1 SCOPE=2INIT=0 38.3 35.1 35.21 42.8 43.6 43.4@45 @15 @15?45Our best run again uses a good initializer, keepsall sentences, still enforces the loose constraintand follows up with domain adaptation, but per-forms worse than all well-initialized WSJ base-lines, scoring only 45.9% (trained at WEB15).We suspect that the web is just too messy forus.
On top of the challenges of language iden-tification and sentence-breaking, there is a lot ofboiler-plate; furthermore, web text can be difficultfor news-trained POS taggers.
For example, notethat the verb ?sign?
is twice mistagged as a nounand that ?YouTube?
is classified as a verb, in thetop four POS sequences of web sentences:17POS Sequence WEB CountSample web sentence, chosen uniformly at random.1 DT NNS VBN 82,858,487All rights reserved.2 NNP NNP NNP 65,889,181Yuasa et al3 NN IN TO VB RB 31,007,783Sign in to YouTube now!4 NN IN IN PRP$ JJ NN 31,007,471Sign in with your Google Account!17Further evidence: TnT tags the ubiquitous but ambigu-ous fragments ?click here?
and ?print post?
as noun phrases.7.5 The State of the ArtOur best model gains more than 5% over previ-ous state-of-the-art accuracy across all sentencesof WSJ?s Section 23, more than 8% on WSJ20 andrivals the oracle skyline (Spitkovsky et al, 2010a)on WSJ10; these gains generalize to Brown100,where it improves by nearly 10% (see Table 8).We take solace in the fact that our best mod-els agree in using loose constraints.
Of these,the models trained with less data perform better,with the best two using trimmed data sets, echo-ing that ?less is more?
(Spitkovsky et al, 2010a),pace Halevy et al (2009).
We note that orders ofmagnitude more data did not improve parsing per-formance further and suspect a different outcomefrom lexicalized models: The primary benefit ofadditional lower-quality data is in improved cover-age.
But with only 35 unique POS tags, data spar-sity is hardly an issue.
Extra examples of lexicalitems help little and hurt when they are mistagged.8 Related WorkThe wealth of new annotations produced in manylanguages every day already fuels a number ofNLP applications.
Following their early andwide-spread use by search engines, in service ofspam-fighting and retrieval, anchor text and linkdata enhanced a variety of traditional NLP tech-niques: cross-lingual information retrieval (Nieand Chen, 2002), translation (Lu et al, 2004), bothnamed-entity recognition (Mihalcea and Csomai,2007) and categorization (Watanabe et al, 2007),query segmentation (Tan and Peng, 2008), plussemantic relatedness and word-sense disambigua-tion (Gabrilovich and Markovitch, 2007; Yeh etal., 2009).
Yet several, seemingly natural, can-didate core NLP tasks ?
tokenization, CJK seg-mentation, noun-phrase chunking, and (until now)parsing ?
remained conspicuously uninvolved.Approaches related to ours arise in applicationsthat combine parsing with named-entity recogni-tion (NER).
For example, constraining a parser torespect the boundaries of known entities is stan-dard practice not only in joint modeling of (con-stituent) parsing and NER (Finkel and Manning,2009), but also in higher-level NLP tasks, such asrelation extraction (Mintz et al, 2009), that couplechunking with (dependency) parsing.
Althoughrestricted to proper noun phrases, dates, times andquantities, we suspect that constituents identifiedby trained (supervised) NER systems would also1285Model Incarnation WSJ10 WSJ20 WSJ?DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100Leapfrog (Spitkovsky et al, 2010a) 57.1 48.7 45.0 43.6default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIM=0,ADAPT=0 55.9 45.8 41.6 40.5WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIM=0,ADAPT=0 65.3 53.8 47.9 50.8BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIM=1,ADAPT=1 69.3 56.8 50.4 53.3NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIM=1,ADAPT=1 67.3 56.2 50.1 51.6WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIM=0,ADAPT=1 64.1 52.7 46.3 46.9EVG Smoothed (skip-head), Lexicalized (Headden et al, 2009) 68.8Table 8: Accuracies on Section 23 of WSJ{10, 20,? }
and Brown100 for three recent state-of-the-artsystems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets.be helpful in constraining grammar induction.Following Pereira and Schabes?
(1992) successwith partial annotations in training a model of(English) constituents generatively, their idea hasbeen extended to discriminative estimation (Rie-zler et al, 2002) and also proved useful in mod-eling (Japanese) dependencies (Sassano, 2005).There was demand for partially bracketed corpora.Chen and Lee (1995) constructed one such corpusby learning to partition (English) POS sequencesinto chunks (Abney, 1991); Inui and Kotani (2001)used n-gram statistics to split (Japanese) clauses.We combine the two intuitions, using the webto build a partially parsed corpus.
Our approachcould be called lightly-supervised, since it doesnot require manual annotation of a single completeparse tree.
In contrast, traditional semi-supervisedmethods rely on fully-annotated seed corpora.189 ConclusionWe explored novel ways of training dependencyparsing models, the best of which attains 50.4%accuracy on Section 23 (all sentences) of WSJ,beating all previous unsupervised state-of-the-artby more than 5%.
Extra gains stem from guid-ing Viterbi training with web mark-up, the looseconstraint consistently delivering best results.
Ourlinguistic analysis of a blog reveals that web an-notations can be converted into accurate parsingconstraints (loose: 88%; sprawl: 95%; tear: 99%)that could be helpful to supervised methods, e.g.,by boosting an initial parser via self-training (Mc-Closky et al, 2006) on sentences with mark-up.Similar techniques may apply to standard word-processing annotations, such as font changes, andto certain (balanced) punctuation (Briscoe, 1994).We make our blog data set, overlaying mark-upand syntax, publicly available.
Its annotations are18A significant effort expended in building a tree-bankcomes with the first batch of sentences (Druck et al, 2009).75% noun phrases, 13% verb phrases, 7% simpledeclarative clauses and 2% prepositional phrases,with traces of other phrases, clauses and frag-ments.
The type of mark-up, combined with POStags, could make for valuable features in discrimi-native models of parsing (Ratnaparkhi, 1999).A logical next step would be to explore the con-nection between syntax and mark-up for genresother than a news-style blog and for languagesother than English.
We are excited by the possi-bilities, as unsupervised parsers are on the cuspof becoming useful in their own right ?
re-cently, Davidov et al (2009) successfully appliedSeginer?s (2007) fully unsupervised grammar in-ducer to the problems of pattern-acquisition andextraction of semantic data.
If the strength of theconnection between web mark-up and syntacticstructure is universal across languages and genres,this fact could have broad implications for NLP,with applications extending well beyond parsing.AcknowledgmentsPartially funded by NSF award IIS-0811974 and by the AirForce Research Laboratory (AFRL), under prime contractno.
FA8750-09-C-0181; first author supported by the Fannie& John Hertz Foundation Fellowship.
We thank Angel X.Chang, Spence Green, Christopher D. Manning, RichardSocher, Mihai Surdeanu and the anonymous reviewers formany helpful suggestions, and we are especially grateful toAndy Golding, for pointing us to his sample Map-Reduceover the Google News crawl, and to Daniel Pipes, for allow-ing us to distribute the data set derived from his blog entries.ReferencesS.
Abney.
1991.
Parsing by chunks.
Principle-Based Pars-ing: Computation and Psycholinguistics.J.
K. Baker.
1979.
Trainable grammars for speech recogni-tion.
In Speech Communication Papers for the 97th Meet-ing of the Acoustical Society of America.C.
Barr, R. Jones, and M. Regelson.
2008.
The linguisticstructure of English web-search queries.
In EMNLP.T.
Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In ANLP.1286T.
Briscoe.
1994.
Parsing (with) punctuation, etc.
Technicalreport, Xerox European Research Laboratory.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-bestparsing and MaxEnt discriminative reranking.
In ACL.E.
Charniak.
2001.
Immediate-head parsing for languagemodels.
In ACL.H.-H. Chen and Y.-S. Lee.
1995.
Development of a partiallybracketed corpus with part-of-speech information only.
InWVLC.S.
B. Cohen and N. A. Smith.
2009.
Shared logistic nor-mal distributions for soft parameter tying in unsupervisedgrammar induction.
In NAACL-HLT.M.
Collins.
1999.
Head-Driven Statistical Models for Nat-ural Language Parsing.
Ph.D. thesis, University of Penn-sylvania.D.
Davidov, R. Reichart, and A. Rappoport.
2009.
Supe-rior and efficient fully unsupervised pattern-based conceptacquisition using an unsupervised parser.
In CoNLL.G.
Druck, G. Mann, and A. McCallum.
2009.
Semi-supervised learning of dependency parsers using general-ized expectation criteria.
In ACL-IJCNLP.J.
Eisner and G. Satta.
1999.
Efficient parsing for bilexicalcontext-free grammars and head-automaton grammars.
InACL.J.
R. Finkel and C. D. Manning.
2009.
Joint parsing andnamed entity recognition.
In NAACL-HLT.W.
N. Francis and H. Kucera, 1979.
Manual of Informationto Accompany a Standard Corpus of Present-Day EditedAmerican English, for use with Digital Computers.
De-partment of Linguistic, Brown University.E.
Gabrilovich and S. Markovitch.
2007.
Computing seman-tic relatedness using Wikipedia-based Explicit SemanticAnalysis.
In IJCAI.D.
Gildea.
2001.
Corpus variation and parser performance.In EMNLP.A.
Halevy, P. Norvig, and F. Pereira.
2009.
The unreasonableeffectiveness of data.
IEEE Intelligent Systems, 24.W.
P. Headden, III, M. Johnson, and D. McClosky.
2009.Improving unsupervised dependency parsing with richercontexts and smoothing.
In NAACL-HLT.S.
Holm.
1979.
A simple sequentially rejective multiple testprocedure.
Scandinavian Journal of Statistics, 6.N.
Inui and Y. Kotani.
2001.
Robust N -gram based syntacticanalysis using segmentation words.
In PACLIC.D.
Klein and C. D. Manning.
2004.
Corpus-based inductionof syntactic structure: Models of dependency and con-stituency.
In ACL.C.-H. Lee, C.-H. Lin, and B.-H. Juang.
1991.
A study onspeaker adaptation of the parameters of continuous den-sity Hidden Markov Models.
IEEE Trans.
on Signal Pro-cessing, 39.W.-H. Lu, L.-F. Chien, and H.-J.
Lee.
2004.
Anchor textmining for translation of Web queries: A transitive trans-lation approach.
ACM Trans.
on Information Systems, 22.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Effectiveself-training for parsing.
In NAACL-HLT.R.
Mihalcea and A. Csomai.
2007.
Wikify!
: Linking docu-ments to encyclopedic knowledge.
In CIKM.M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.
Distantsupervision for relation extraction without labeled data.
InACL-IJCNLP.J.-Y.
Nie and J. Chen.
2002.
Exploiting the Web as paral-lel corpora for cross-language information retrieval.
WebIntelligence.F.
Pereira and Y. Schabes.
1992.
Inside-outside reestimationfrom partially bracketed corpora.
In ACL.A.
Ratnaparkhi.
1999.
Learning to parse natural languagewith maximum entropy models.
Machine Learning, 34.S.
Ravi, K. Knight, and R. Soricut.
2008.
Automatic predic-tion of parser accuracy.
In EMNLP.J.
C. Reynar and A. Ratnaparkhi.
1997.
A maximum entropyapproach to identifying sentence boundaries.
In ANLP.S.
Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T.Maxwell, III, and M. Johnson.
2002.
Parsing the WallStreet Journal using a lexical-functional grammar and dis-criminative estimation techniques.
In ACL.M.
Sassano.
2005.
Using a partially annotated corpus tobuild a dependency parser for Japanese.
In IJCNLP.Y.
Seginer.
2007.
Fast unsupervised incremental parsing.
InACL.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2010a.
FromBaby Steps to Leapfrog: How ?Less is More?
in unsuper-vised dependency parsing.
In NAACL-HLT.V.
I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-ning.
2010b.
Viterbi training improves unsupervised de-pendency parsing.
In CoNLL.B.
Tan and F. Peng.
2008.
Unsupervised query segmenta-tion using generative language models and Wikipedia.
InWWW.K.
Toutanova and C. D. Manning.
2000.
Enriching theknowledge sources used in a maximum entropy part-of-speech tagger.
In EMNLP-VLC.K.
Toutanova, D. Klein, C. D. Manning, and Y.
Singer.
2003.Feature-rich part-of-speech tagging with a cyclic depen-dency network.
In HLT-NAACL.D.
Vadas and J. R. Curran.
2007.
Adding noun phrase struc-ture to the Penn Treebank.
In ACL.Y.
Watanabe, M. Asahara, and Y. Matsumoto.
2007.
Agraph-based approach to named entity categorization inWikipedia using conditional random fields.
In EMNLP-CoNLL.E.
Yeh, D. Ramage, C. D. Manning, E. Agirre, and A. Soroa.2009.
WikiWalk: Random walks on Wikipedia for se-mantic relatedness.
In TextGraphs.1287
