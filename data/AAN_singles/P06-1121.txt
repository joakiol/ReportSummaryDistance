Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 961?968,Sydney, July 2006. c?2006 Association for Computational LinguisticsScalable Inference and Training ofContext-Rich Syntactic Translation ModelsMichel Galley*, Jonathan Graehl?, Kevin Knight?
?, Daniel Marcu?
?,Steve DeNeefe?, Wei Wang?
and Ignacio Thayer?
*Columbia UniversityDept.
of Computer ScienceNew York, NY 10027galley@cs.columbia.edu, {graehl,knight,marcu,sdeneefe}@isi.edu,wwang@languageweaver.com, thayer@google.com?University of Southern CaliforniaInformation Sciences InstituteMarina del Rey, CA 90292?Language Weaver, Inc.4640 Admiralty WayMarina del Rey, CA 90292AbstractStatistical MT has made great progress in the lastfew years, but current translation models are weakon re-ordering and target language fluency.
Syn-tactic approaches seek to remedy these problems.In this paper, we take the framework for acquir-ing multi-level syntactic translation rules of (Gal-ley et al, 2004) from aligned tree-string pairs, andpresent two main extensions of their approach: first,instead of merely computing a single derivation thatminimally explains a sentence pair, we constructa large number of derivations that include contex-tually richer rules, and account for multiple inter-pretations of unaligned words.
Second, we pro-pose probability estimates and a training procedurefor weighting these rules.
We contrast differentapproaches on real examples, show that our esti-mates based on multiple derivations favor phrasalre-orderings that are linguistically better motivated,and establish that our larger rules provide a 3.63BLEU point increase over minimal rules.1 IntroductionWhile syntactic approaches seek to remedy word-ordering problems common to statistical machinetranslation (SMT) systems, many of the earliermodels?particularly child re-ordering models?fail to account for human translation behavior.Galley et al (2004) alleviate this modeling prob-lem and present a method for acquiring millionsof syntactic transfer rules from bilingual corpora,which we review below.
Here, we make the fol-lowing new contributions: (1) we show how toacquire larger rules that crucially condition onmore syntactic context, and show how to com-pute multiple derivations for each training exam-ple, capturing both large and small rules, as wellas multiple interpretations for unaligned words;(2) we develop probability models for these multi-level transfer rules, and give estimation methodsfor assigning probabilities to very large rule sets.We contrast our work with (Galley et al, 2004),highlight some severe limitations of probabilityestimates computed from single derivations, anddemonstrate that it is critical to account for manyderivations for each sentence pair.
We also usereal examples to show that our probability mod-els estimated from a large number of derivationsfavor phrasal re-orderings that are linguisticallywell motivated.
An empirical evaluation againsta state-of-the-art SMT system similar to (Och andNey, 2004) indicates positive prospects.
Finally,we show that our contextually richer rules providea 3.63 BLEU point increase over those of (Galleyet al, 2004).2 Inferring syntactic transformationsWe assume we are given a source-language (e.g.,French) sentence f , a target-language (e.g., En-glish) parse tree pi, whose yield e is a translationof f , and a word alignment a between f and e.Our aim is to gain insight into the process of trans-forming pi into f and to discover grammatically-grounded translation rules.
For this, we needa formalism that is expressive enough to dealwith cases of syntactic divergence between sourceand target languages (Fox, 2002): for any given(pi, f ,a) triple, it is useful to produce a derivationthat minimally explains the transformation be-tween pi and f , while remaining consistent with a.Galley et al (2004) present one such formalism(henceforth ?GHKM?
).2.1 Tree-to-string alignmentsIt is appealing to model the transformation of piinto f using tree-to-string (xRs) transducers, sincetheir theory has been worked out in an exten-sive literature and is well understood (see, e.g.,(Graehl and Knight, 2004)).
Formally, transfor-mational rules ri presented in (Galley et al, 2004)are equivalent to 1-state xRs transducers mappinga given pattern (subtree to match in pi) to a righthand side string.
We will refer to them as lhs(ri)and rhs(ri), respectively.
For example, some xRs961rules may describe the transformation of does notinto ne ... pas in French.
A particular instance maylook like this:VP(AUX(does), RB(not), x0:VB) ?
ne, x0, paslhs(ri) can be any arbitrary syntax tree fragment.Its leaves are either lexicalized (e.g.
does) or vari-ables (x0, x1, etc).
rhs(ri) is represented as a se-quence of target-language words and variables.Now we give a brief overview of how suchtransformational rules are acquired automaticallyin GHKM.1 In Figure 1, the (pi, f ,a) triple is rep-resented as a directed graph G (edges going down-ward), with no distinction between edges of pi andalignments.
Each node of the graph is labeled withits span and complement span (the latter in italicin the figure).
The span of a node n is defined bythe indices of the first and last word in f that arereachable from n. The complement span of n isthe union of the spans of all nodes n?
in G thatare neither descendants nor ancestors of n. Nodesof G whose spans and complement spans are non-overlapping form the frontier set F ?
G.What is particularly interesting about the fron-tier set?
For any frontier of graph G containinga given node n ?
F , spans on that frontier de-fine an ordering between n and each other frontiernode n?.
For example, the span of VP[4-5] eitherprecedes or follows, but never overlaps the span ofany node n?
on any graph frontier.
This propertydoes not hold for nodes outside of F .
For instance,PP[4-5] and VBG[4] are two nodes of the samegraph frontier, but they cannot be ordered becauseof their overlapping spans.The purpose of xRs rules in this framework isto order constituents along sensible frontiers in G,and all frontiers containing undefined orderings,as between PP[4-5] and VBG[4], must be disre-garded during rule extraction.
To ensure that xRsrules are prevented from attempting to re-orderany such pair of constituents, these rules are de-signed in such a way that variables in their lhs canonly match nodes of the frontier set.
Rules thatsatisfy this property are said to be induced by G.2For example, rule (d) in Table 1 is valid accord-ing to GHKM, since the spans corresponding to1Note that we use a slightly different terminology.2Specifically, an xRs rule ri is extracted fromG by takinga subtree ?
?
pi as lhs(ri), appending a variable to eachleaf node of ?
that is internal to pi, adding those variables torhs(ri), ordering them in accordance to a, and if necessaryinserting any word of f to ensure that rhs(ri) is a sequence ofcontiguous spans (e.g., [4-5][6][7-8] for rule (f) in Table 1).DTCDVBPNNSINNNPNPNNSVBG32217-844591234567893 1-2,4-92 1-92 1-91 2-97-8 1-5,94 1-94 1-95 1-4,7-99 1-81-2 3-9NP 7-8 1-5,9NP 5 1-4, 7-9PP 4-5 1-4,7-9VP 4-5 1-3,7-9NP 4-8 1-3,9VP 3-8 1-2,9S 1-9 ?7!
"#$%&'()*+,.ThesepeopleincludeastronautscomingfromFrance..7-Figure 1: Spans and complement-spans determine whatrules are extracted.
Constituents in gray are members of thefrontier set; a minimal rule is extracted from each of them.
(a) S(x0:NP, x1:VP, x2:.)
?
x0, x1, x2(b) NP(x0:DT, CD(7), NNS(people)) ?
x0, 7?
(c) DT(these) ??
(d) VP(x0:VBP, x1:NP) ?
x0, x1(e) VBP(include) ?-?
(f) NP(x0:NP, x1:VP) ?
x1,?, x0(g) NP(x0:NNS) ?
x0(h) NNS(astronauts) ??
*,X(i) VP(VBG(coming), PP(IN(from), x0:NP)) ?e?, x0(j) NP(x0:NNP) ?
x0(k) NNP(France) ???
(l) .(.)
?
.Table 1: A minimal derivation corresponding to Figure 1.its rhs constituents (VBP[3] and NP[4-8]) do notoverlap.
Conversely, NP(x0:DT, x1:CD:, x2:NNS)is not the lhs of any rule extractible from G, sinceits frontier constituents CD[2] and NNS[2] haveoverlapping spans.3 Finally, the GHKM proce-dure produces a single derivation from G, whichis shown in Table 1.The concern in GHKM was to extract minimalrules, whereas ours is to extract rules of any arbi-trary size.
Minimal rules defined over G are thosethat cannot be decomposed into simpler rules in-duced by the same graph G, e.g., all rules in Ta-ble 1.
We call minimal a derivation that only con-tains minimal rules.
Conversely, a composed ruleresults from the composition of two or more min-imal rules, e.g., rule (b) and (c) compose into:NP(DT(these), CD(7), NNS(people)) ?
?, 7?3It is generally reasonable to also require that the root nof lhs(ri) be part of F , because no rule induced by G cancompose with ri at n, due to the restrictions imposed on theextraction procedure, and ri wouldn?t be part of any validderivation.962ORNP(x0:NP, x1:VP)!x1,!, x0VP(x0:VBP,x1:NP)!x0, x1S(x0:NP,x1:VP, x2:.
)!x0, x1, x2NP(x0:DTCD(7), NNS(people))!x0, 7".(.
)!.DT(these)!#VBP(include)!$%&NP(x0:NP, x1:VP)!x1, x0NP(x0:NP, x1:VP)!x1, x0VP(VBG(coming),PP(IN(from),x0:NP))!
'(, x0, !VP(VBG(coming),PP(IN(from),x0:NP))!
'(, x0NP(x0:NNS)!x0NP(x0:NNS)!
!,x0NP(x0:NNP)!x0, !NNP(France)!
)*NNS(astronauts)!+,, -OROR NNS(astronauts)!!,+,,-ORNP(x0:NNP)!x0NP(x0:NNP)!x0NNP(France)!
)*, !NP(x0:NNS)!x0VP(VBG(coming),PP(IN(from),x0:NP))!
'(, x0comingfromNNSINNNPNPVPNPVBGPPNP7-857-857-844545678444-54-54-8NNP(France)!
)*,!NP(x0:NNP)!x0, !VP(VBG(coming),PP(IN(from),x0:NP))!
'(, x0, !NNS(astronauts)!!
,+,,-NP(x0:NNS)!!
,x0NP(x0:NP, x1:VP)!x1, !, x0(a)(b)-'()*!+,astronautsFranceFigure 2: (a) Multiple ways of aligning?
to constituents in the tree.
(b) Derivation corresponding to the parse tree in Figure 1,which takes into account all alignments of?
pictured in (a).Note that these properties are dependent on G, andthe above rule would be considered a minimal rulein a graph G?
similar to G, but additionally con-taining a word alignment between 7 and ?.
Wewill see in Sections 3 and 5 why extracting onlyminimal rules can be highly problematic.2.2 Unaligned wordsWhile the general theory presented in GHKM ac-counts for any kind of derivation consistent withG, it does not particularly discuss the case wheresome words of the source-language string f arenot aligned to any word of e, thus disconnectedfrom the rest of the graph.
This case is highly fre-quent: 24.1% of Chinese words in our 179 mil-lion word English-Chinese bilingual corpus areunaligned, and 84.8% of Chinese sentences con-tain at least one unaligned word.
The question iswhat to do with such lexical items, e.g., ?
inFigure 2(a).
The approach of building one mini-mal derivation for G as in the algorithm describedin GHKM assumes that we commit ourselves toa particular heuristic to attach the unaligned itemto a certain constituent of pi, e.g., highest attach-ment (in the example, ?
is attached to NP[4-8]and the heuristic generates rule (f)).
A more rea-sonable approach is to invoke the principle of in-sufficient reason and make no a priori assump-tion about what is a ?correct?
way of assigningthe item to a constituent, and return all derivationsthat are consistent with G. In Section 4, we willsee how to use corpus evidence to give preferenceto unaligned-word attachments that are the mostconsistent across the data.
Figure 2(a) shows thesix possible ways of attaching ?
to constituentsof pi: besides the highest attachment (rule (f)),?can move along the ancestors of France, since it isto the right of the translation of that word, and beconsidered to be part of an NNP, NP, or VP rule.We make the same reasoning to the left: ?
caneither start the NNS of astronauts, or start an NP.Our account of all possible ways of consistentlyattaching ?
to constituents means we must ex-tract more than one derivation to explain transfor-mations in G, even if we still restrict ourselves tominimal derivations (a minimal derivation for Gis unique if and only if no source-language wordin G is unaligned).
While we could enumerateall derivations separately, it is much more effi-cient both in time and space to represent them as aderivation forest, as in Figure 2(b).
Here, the for-est covers all minimal derivations that correspondto G. It is necessary to ensure that for each deriva-tion, each unaligned item (here ?)
appears onlyonce in the rules of that derivation, as shown inFigure 2 (which satisfies the property).
That re-quirement will prove to be critical when we ad-dress the problem of estimating probabilities forour rules: if we allowed in our example to spuri-ously generate?s in multiple successive steps ofthe same derivation, we would not only representthe transformation incorrectly, but also ?-ruleswould be disproportionately represented, leadingto strongly biased estimates.
We will now see howto ensure this constraint is satisfied in our rule ex-traction and derivation building algorithm.9632.3 AlgorithmThe linear-time algorithm presented in GHKM isonly a particular case of the more general one wedescribe here, which is used to extract all rules,minimal and composed, induced by G. Similarlyto the GHKM algorithm, ours performs a top-down traversal of G, but differs in the operationsit performs at each node n ?
F : we must exploreall subtrees rooted at n, find all consistent waysof attaching unaligned words of f, and build validderivations in accordance to these attachments.We use a table or-dforest[x, y, c] to store OR-nodes, in which each OR-node can be uniquelydefined by a syntactic category c and a span [x, y](which may cover unaligned words of f).
This ta-ble is used to prevent the same partial derivationto be followed multiple times (the in-degrees ofOR-nodes generally become large with composedrules).
Furthermore, to avoid over-generating un-aligned words, the root and variables in each ruleare represented with their spans.
For example, inFigure 2(b), the second and third child of the top-most OR-node respectively span across [4-5][6-8]and [4-6][7-8] (after constituent reordering).
Inthe former case, ?
will eventually be realized inan NP, and in the latter case, in a VP.The preprocessing step consists of assigningspans and complement spans to nodes of G, inthe first case by a bottom-up exploration of thegraph, and in the latter by a top-down traversal.To assign complement spans, we assign the com-plement span of any node n to each of its children,and for each of them, add the span of the childto the complement span of all other children.
Inanother traversal of G, we determine the minimalrule extractible from each node in F .We explore all tree fragments rooted at n bymaintaining an open and a closed queue of rulesextracted from n (qo and qc).
At each step, wepick the smallest rule in qo, and for each of itsvariable nodes, try to discover new rules (?succes-sor rules?)
by means of composition with minimalrules, until a given threshold on rule size or maxi-mum number of rules in qc is reached.
There maybe more that one successor per rule, since we mustaccount for all possible spans than can be assignedto non-lexical leaves of a rule.
Once a threshold isreached, or if the open queue is empty, we connecta new OR-node to all rules that have just been ex-tracted from n, and add it to or-dforest.
Finally,we proceed recursively, and extract new rules fromeach node at the frontier of the minimal rule rootedat n. Once all nodes of F have been processed, theor-dforest table contains a representation encod-ing only valid derivations.3 Probability modelsThe overall goal of our translation system is totransform a given source-language sentence finto an appropriate translation e in the set Eof all possible target-language sentences.
In anoisy-channel approach to SMT, we uses Bayes?theorem and choose the English sentence e?
?
Ethat maximizes:4e?
= argmaxe?E{Pr(e) ?
Pr(f |e)}(1)Pr(e) is our language model, and Pr(f |e) ourtranslation model.
In a grammatical approach toMT, we hypothesize that syntactic informationcan help produce good translation, and thusintroduce dependencies on target-language syntaxtrees.
The function to optimize becomes:e?
= argmaxe?E{Pr(e) ??pi??
(e)Pr(f |pi) ?Pr(pi|e)}(2)?
(e) is the set of all English trees that yield thegiven sentence e. Estimating Pr(pi|e) is a prob-lem equivalent to syntactic parsing and thus is notdiscussed here.
Estimating Pr(f |pi) is the task ofsyntax-based translation models (SBTM).Given a rule set R, our SBTM makes thecommon assumption that left-most compositionsof xRs rules ?i = r1 ?
... ?
rn are independentfrom one another in a given derivation ?i ?
?,where ?
is the set of all derivations constructiblefrom G = (pi, f ,a) using rules of R. Assumingthat ?
is the set of all subtree decompositions of picorresponding to derivations in ?, we define theestimate:Pr(f |pi) =1|?|??i???rj?
?ip(rhs(rj)|lhs(rj)) (3)under the assumption:?rj?R:lhs(rj)=lhs(ri)p(rhs(rj)|lhs(rj)) = 1 (4)It is important to notice that the probabilitydistribution defined in Equation 3 requires anormalization factor (|?|) in order to be tight, i.e.,sum to 1 over all strings fi ?
F that can be derived4We denote general probability distributions with Pr(?
)and use p(?)
for probabilities assigned by our models.964XaY ba?b?c?c(!,f 1,a 1):XaY bb?a?c?c(!,f 2,a 2):Figure 3: Example corpus.from pi.
A simple example suffices to demonstrateit is not tight without normalization.
Figure 3contains a sample corpus from which four rulescan be extracted:r1: X(a, Y(b, c)) ?
a?, b?, c?r2: X(a, Y(b, c)) ?
b?, a?, c?r3: X(a, x0:Y) ?
a?, x0r4: Y(b, c) ?
b?, c?From Equation 4, the probabilities of r3 and r4must be 1, and those of r1 and r2 must sum to1.
Thus, the total probability mass, which is dis-tributed across two possible output strings a?b?c?and b?a?c?, is: p(a?b?c?|pi) + p(b?a?c?|pi) = p1 +p3 ?
p4 + p2 = 2, where pi = p(rhs(ri)|lhs(ri)).It is relatively easy to prove that the probabil-ities of all derivations that correspond to a givendecomposition ?i ?
?
sum to 1 (the proof is omit-ted due to constraints on space).
From this prop-erty we can immediately conclude that the modeldescribed by Equation 3 is tight.5We examine two estimates p(rhs(r)|lhs(r)).The first one is the relative frequency estimatorconditioning on left hand sides:p(rhs(r)|lhs(r)) =f(r)?r?:lhs(r?
)=lhs(r) f(r?
)(5)f(r) represents the number of times rule r oc-curred in the derivations of the training corpus.One of the major negative consequences ofextracting only minimal rules from a corpus isthat an estimator such as Equation 5 can becomeextremely biased.
This again can be observedfrom Figure 3.
In the minimal-rule extraction ofGHKM, only three rules are extracted from the ex-ample corpus, i.e.
rules r2, r3, and r4.
Let?s as-sume now that the triple (pi, f1,a1) is represented99 times, and (pi, f2,a2) only once.
Given a treepi, the model trained on that corpus can generatethe two strings a?b?c?
and b?a?c?
only through twoderivations, r3 ?
r4 and r2, respectively.
Sinceall rules in that example have probability 1, and5If each tree fragment in pi is the lhs of some rule in R,then we have |?| = 2n, where n is the number of nodes ofthe frontier set F ?
G (each node is a binary choice point).given that the normalization factor |?| is 2, bothprobabilities p(a?b?c?|pi) and p(b?a?c?|pi) are 0.5.On the other hand, if all rules are extracted andincorporated into our relative-frequency probabil-ity model, r1 seriously counterbalances r2 and theprobability of a?b?c?
becomes: 12 ?
(99100+1) = .995(since it differs from .99, the estimator remains bi-ased, but to a much lesser extent).An alternative to the conditional model ofEquation 3 is to use a joint model conditioning onthe root node instead of the entire left hand side:p(r|root(r)) =f(r)?r?:root(r?
)=root(r) f(r?
)(6)This can be particularly useful if no parser orsyntax-based language model is available, and weneed to rely on the translation model to penalizeill-formed parse trees.
Section 6 will describe anempirical evaluation based on this estimate.4 EM trainingIn our previous discussion of parameter estima-tion, we did not explore the possibility that onederivation in a forest may be much more plau-sible than the others.
If we knew which deriva-tion in each forest was the ?true?
derivation, thenwe could straightforwardly collect rule counts offthose derivations.
On the other hand, if we hadgood rule probabilities, we could compute themost likely (Viterbi) derivations for each trainingexample.
This is a situation in which we can em-ploy EM training, starting with uniform rule prob-abilities.
For each training example, we would liketo: (1) score each derivation ?i as a product of theprobabilities of the rules it contains, (2) computea conditional probability pi for each derivation ?i(conditioned on the observed training pair) by nor-malizing those scores to add to 1, and (3) collectweighted counts for each rule in each ?i, wherethe weight is pi.
We can then normalize the countsto get refined probabilities, and iterate; the corpuslikelihood is guaranteed to improve with each it-eration.
While it is infeasible to enumerate themillions of derivations in each forest, Graehl andKnight (2004) demonstrate an efficient algorithm.They also analyze how to train arbitrary tree trans-ducers into two steps.
The first step is to build aderivation forest for each training example, wherethe forest contains those derivations licensed bythe (already supplied) transducer?s rules.
The sec-ond step employs EM on those derivation forests,running in time proportional to the size of the965Best minimal-rule derivation (Cm) p(r)(a) S(x0:NP-C x1:VP x2:.)
?
x0 x1 x2 .845(b) NP-C(x0:NPB) ?
x0 .82(c) NPB(DT(the) x0:NNS) ?
x0 .507(d) NNS(gunmen) ?
?K .559(e) VP(VBD(were) x0:VP-C) ?
x0 .434(f) VP-C(x0:VBN x1:PP) ?
x1 x0 .374(g) PP(x0:IN x1:NP-C) ?
x0 x1 .64(h) IN(by) ??
.0067(i) NP-C(x0:NPB) ?
x0 .82(j) NPB(DT(the) x0:NN) ?
x0 .586(k) NN(police) ?f?
.0429(l) VBN(killed) ???
.0072(m) .(.)
?
.
.981.Thegunmenwerekilledbythepolice.DTVBDVBNDTNNNPPPVP-CVPSNNSINNP.!
"#$%&'Best composed-rule derivation (C4) p(r)(o) S(NP-C(NPB(DT(the) NNS(gunmen))) x0:VP .(.))
?
?K x0 .
1(p) VP(VBD(were) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ??
x1 x0 0.00724(q) NP-C(NPB(DT(the) NN(police))) ?f?
0.173(r) VBN(killed) ???
0.00719Figure 4: Two most probable derivations for the graph on the right: the top table restricted to minimal rules; the bottom one,much more probable, using a large set of composed rules.
Note: the derivations are constrained on the (pi, f ,a) triple, and thusinclude some non-literal translations with relatively low probabilities (e.g.
killed, which is more commonly translated as{?
).rule nb.
of nb.
of deriv- EM-set rules nodes time timeCm 4M 192M 2 h. 4 h.C3 142M 1255M 52 h. 34 h.C4 254M 2274M 134 h. 60 h.Table 2: Rules and derivation nodes for a 54M-word, 1.95Msentence pair English-Chinese corpus, and time to buildderivations (on 10 cluster nodes) and run 50 EM iterations.forests.
We only need to borrow the second stepfor our present purposes, as we construct our ownderivation forests when we acquire our rule set.A major challenge is to scale up this EM train-ing to large data sets.
We have been able to runEM for 50 iterations on our Chinese-English 54-million word corpus.
The derivation forests forthis corpus contain 2.2 billion nodes; the largestforest contains 1.1 million nodes.
The outcomeis to assign probabilities to over 254 million rules.Our EM runs with either lhs normalization or lhs-root normalization.
In the former case, each lhshas an average of three corresponding rhs?s thatcompete with each other for probability mass.5 Model coverageWe now present some examples illustrating thebenefit of composed rules.
We trained threep(rhs(ri)|lhs(ri)) models on a 54 million-wordEnglish-Chinese parallel corpus (Table 2): the firstone (Cm) with only minimal rules, and the twoothers (C3 and C4) additionally considering com-posed rules with no more than three, respectivelyfour, internal nodes in lhs(ri).
We evaluated thesemodels on a section of the NIST 2002 evaluationcorpus, for which we built derivation forests andlhs: S(x0:NP-C VP(x1:VBD x2:NP-C) x3:.
)corpus rhsi p(rhsi|lhs)Chinese x1 x0 x2 x3 .3681(minimal) x0 x1 , x3 x2 .0357x2 , x0 x1 x3 .0287x0 x1 , x3 x2 .
.0267Chinese x0 x1 x2 x3 .9047(composed) x0 x1 , x2 x3 .016x0 , x1 x2 x3 .0083x0 x1 ?
x2 x3 .0072Arabic x1 x0 x2 x3 .5874(composed) x0 x1 x2 x3 .4027x1 x2 x0 x3 .0077x1 x0 x2 " x3 .0001Table 3: Our model transforms English subject-verb-object(SVO) structures into Chinese SVO and into Arabic VSO.With only minimal rules, Chinese VSO is wrongly preferred.extracted the most probable one (Viterbi) for eachsentence pair (based on an automatic alignmentproduced by GIZA).
We noticed in general thatViterbi derivations according to C4 make exten-sive usage of composed rules, as it is the case inthe example in Figure 4.
It shows the best deriva-tion according to Cm and C4 on the unseen (pi,f,a)triple displayed on the right.
The second deriva-tion (log p = ?11.6) is much more probable thanthe minimal one (log p = ?17.7).
In the caseof Cm, we can see that many small rules must beapplied to explain the transformation, and at eachstep, the decision regarding the re-ordering of con-stituents is made with little syntactic context.
Forexample, from the perspective of a decoder, theword by is immediately transformed into a prepo-sition (IN), but it is in general useful to knowwhich particular function word is present in thesentence to motivate good re-orderings in the up-966lhs1: NP-C(x0:NPB PP(IN(of) x1:NP-C)) (NP-of-NP)lhs2: PP(IN(of) NP-C(x0:NPB PP(IN(of) NP-C(x1:NPB x2:VP)))) (of-NP-of-NP-VP)lhs3: VP(VBD(said) SBAR-C(IN(that) x0:S-C)) (said-that-S)lhs4: SBAR(WHADVP(WRB(when)) S-C(x0:NP-C VP(VBP(are) x1:VP-C))) (when-NP-are-VP)rhs1i p(rhs1i|lhs1) rhs2i p(rhs2i|lhs2) rhs3i p(rhs3i|lhs3) rhs4i p(rhs4i|lhs4)x1 x0 .54 x2 ?
x1 ?
x0 .6754 ?
, x0 .6062 ( x1 x0 ?
.6618x0 x1 .2351 ( x2 ?
x1 ?
x0 .035 ?
x0 .1073 S x1 x0 ?
.0724x1 ?
x0 .0334 x2 ?
x1 ?
x0 , .0263 h: , x0 .0591 ( x1 x0 ?
, .0579x1 x0 ?
.026 x2 ?
x1 ?
x0 	 .0116 ?
?
, x0 .0234 , ( x1 x0 ?
.0289Table 4: Translation probabilities promote linguistically motivated constituent re-orderings (for lhs1 and lhs2), and enablenon-constituent (lhs3) and non-contiguous (lhs4) phrasal translations.per levels of the tree.
A rule like (e) is particu-larly unfortunate, since it allows the word were tobe added without any other evidence that the VPshould be in passive voice.
On the other hand, thecomposed-rule derivation of C4 incorporates morelinguistic evidence in its rules, and re-orderingsare motivated by more syntactic context.
Rule(p) is particularly appropriate to create a passiveVP construct, since it expects a Chinese passivemarker (?
), an NP-C, and a verb in its rhs, andcreates the were ... by construction at once in theleft hand side.5.1 Syntactic translation tablesWe evaluate the promise of our SBTM by analyz-ing instances of translation tables (t-table).
Table 3shows how a particular form of SVO construc-tion is transformed into Chinese, which is also anSVO language.
While the t-table for Chinese com-posed rules clearly gives good estimates for the?correct?
x0 x1 ordering (p = .9), i.e.
subject be-fore verb, the t-table for minimal rules unreason-ably gives preference to verb-subject ordering (x1x0, p = .37), because the most probable transfor-mation (x0 x1) does not correspond to a minimalrule.
We obtain different results with Arabic, anVSO language, and our model effectively learnsto move the subject after the verb (p = .59).lhs1 in Table 4 shows that our model is ableto learn large-scale constituent re-orderings, suchas re-ordering NPs in a NP-of-NP construction,and put the modifier first as it is more commonlythe case in Chinese (p = .54).
If more syntac-tic context is available as in lhs2, our modelprovides much sharper estimates, and appropri-ately reverses the order of three constituents withhigh probability (p = .68), inserting modifiers first(possessive markers?
are needed here for bettersyntactic disambiguation).A limitation of earlier syntax-based systems istheir poor handling of non-constituent phrases.Table 4 shows that our model can learn rules forsuch phrases, e.g., said that (lhs3).
While the thathas no direct translation, our model effectivelylearns to separate?
(said) from the relative clausewith a comma, which is common in Chinese.Another promising prospect of our model seemsto lie in its ability to handle non-contiguousphrases, a feature that state of the art systemssuch as (Och and Ney, 2004) do not incorpo-rate.
The when-NP-are-VP construction of lhs4presents such a case.
Our model identifies that areneeds to be deleted, that when translates into thephrase( ...?, and that the NP needs to be movedafter the VP in Chinese (p = .66).6 Empirical evaluationThe task of our decoder is to find the most likelyEnglish tree pi that maximizes all models involvedin Equation 2.
Since xRs rules can be converted tocontext-free productions by increasing the numberof non-terminals, we implemented our decoder asa standard CKY parser with beam search.
Its rulebinarization is described in (Zhang et al, 2006).We compare our syntax-based system againstan implementation of the alignment template(AlTemp) approach to MT (Och and Ney, 2004),which is widely considered to represent the stateof the art in the field.
We registered both systemsin the NIST 2005 evaluation; results are presentedin Table 5.
With a difference of 6.4 BLEU pointsfor both language pairs, we consider the resultsof our syntax-based system particularly promis-ing, since these are the highest scores to date thatwe know of using linguistic syntactic transforma-tions.
Also, on the one hand, our AlTemp sys-tem represents quite mature technology, and in-corporates highly tuned model parameters.
Onthe other hand, our syntax decoder is still work inprogress: only one model was used during search,i.e., the EM-trained root-normalized SBTM, andas yet no language model is incorporated in thesearch (whereas the search in the AlTemp sys-tem uses two phrase-based translation models and967Syntactic AlTempArabic-to-English 40.2 46.6Chinese-to-English 24.3 30.7Table 5: BLEU-4 scores for the 2005 NIST test set.Cm C3 C4Chinese-to-English 24.47 27.42 28.1Table 6: BLEU-4 scores for the 2002 NIST test set, with rulesof increasing sizes.12 other feature functions).
Furthermore, our de-coder doesn?t incorporate any syntax-based lan-guage model, and admittedly our ability to penal-ize ill-formed parse trees is still limited.Finally, we evaluated our system on the NIST-02 test set with the three different rule sets (seeTable 6).
The performance with our largest ruleset represents a 3.63 BLEU point increase (14.8%relative) compared to using only minimal rules,which indicates positive prospects for using evenlarger rules.
While our rule inference algorithmscales to higher thresholds, one important area offuture work will be the improvement of our de-coder, conjointly with analyses of the impact interms of BLEU of contextually richer rules.7 Related workSimilarly to (Poutsma, 2000; Wu, 1997; Yamadaand Knight, 2001; Chiang, 2005), the rules dis-cussed in this paper are equivalent to productionsof synchronous tree substitution grammars.
Webelieve that our tree-to-string model has severaladvantages over tree-to-tree transformations suchas the ones acquired by Poutsma (2000).
Whiletree-to-tree grammars are richer formalisms thatprovide the potential benefit of rules that are lin-guistically better motivated, modeling the syntaxof both languages comes as an extra cost, and itis admittedly more helpful to focus our syntac-tic modeling effort on the target language (e.g.,English) in cases where it has syntactic resources(parsers and treebanks) that are considerably moreavailable than for the source language.
Further-more, we think there is, overall, less benefit inmodeling the syntax of the source language, sincethe input sentence is fixed during decoding and isgenerally already grammatical.With the notable exception of Poutsma, mostrelated works rely on models that are restrictedto synchronous context-free grammars (SCFG).While the state-of-the-art hierarchical SMT sys-tem (Chiang, 2005) performs well despite strin-gent constraints imposed on its context-free gram-mar, we believe its main advantage lies in itsability to extract hierarchical rules across phrasalboundaries.
Context-free grammars (such as PennTreebank and Chiang?s grammars) make indepen-dence assumptions that are arguably often unrea-sonable, but as our work suggests, relaxationsof these assumptions by using contextually richerrules results in translations of increasing quality.We believe it will be beneficial to account for thisfinding in future work in syntax-based SMT and inefforts to improve upon (Chiang, 2005).8 ConclusionsIn this paper, we developed probability models forthe multi-level transfer rules presented in (Galleyet al, 2004), showed how to acquire larger rulesthat crucially condition on more syntactic context,and how to pack multiple derivations, includinginterpretations of unaligned words, into derivationforests.
We presented some theoretical argumentsfor not limiting extraction to minimal rules, val-idated them on concrete examples, and presentedexperiments showing that contextually richer rulesprovide a 3.63 BLEU point increase over the min-imal rules of (Galley et al, 2004).AcknowledgmentsWe would like to thank anonymous review-ers for their helpful comments and suggestions.This work was partially supported under theGALE program of the Defense Advanced Re-search Projects Agency, Contract No.
HR0011-06-C-0022.ReferencesD.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of ACL.H.
Fox.
2002.
Phrasal cohesion and statistical machine trans-lation.
In Proc.
of EMNLP, pages 304?311.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a translation rule?
In Proc.
of HLT/NAACL-04.J.
Graehl and K. Knight.
2004.
Training tree transducers.
InProc.
of HLT/NAACL-04, pages 105?112.F.
Och and H. Ney.
2004.
The alignment template approachto statistical machine translation.
Computational Linguis-tics, 30(4):417?449.A.
Poutsma.
2000.
Data-oriented translation.
In Proc.
ofCOLING, pages 635?641.D.
Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
ComputationalLinguistics, 23(3):377?404.K.
Yamada and K. Knight.
2001.
A syntax-based statisticaltranslation model.
In Proc.
of ACL, pages 523?530.H.
Zhang, L. Huang, D. Gildea, and K. Knight.
2006.
Syn-chronous binarization for machine translation.
In Proc.
ofHLT/NAACL.968
