Semantic Role Labelling of Prepositional PhrasesPatrick Ye1 and Timothy Baldwin1,21 Department of Computer Science and Software Engineering,University of Melbourne, VIC 3010, Australia2 NICTA Victoria Laboratories,University of Melbourne, VIC 3010, Australia{jingy, tim}@cs.mu.oz.auAbstract.
We propose a method for labelling prepositional phrases ac-cording to two different semantic role classifications, as contained in thePenn treebank and the CoNLL 2004 Semantic Role Labelling data set.Our results illustrate the difficulties in determining preposition seman-tics, but also demonstrate the potential for PP semantic role labelling toimprove the performance of a holistic semantic role labelling system.1 IntroductionPrepositional phrases (PPs) are both common and semantically varied in openEnglish text.
Learning the semantics of prepositions is not a trivial task in gen-eral.
It may seem that the semantics of a given PP can be predicted with rea-sonable reliability independent of its context.
However, it is actually common forprepositions or even identical PPs to exhibit a wide range of semantic fuctionsin different open English contexts.
For example, consider the PP to the car : thisPP will generally occur as a directional adjunct (e.g.
walk to the car), but it canalso occur as an object to the verb (e.g.
refer to the car) or contrastive argu-ment (e.g.
the default mode of transport has shifted from the train to the car); tofurther complicate the situation, in key to the car it functions as a complementto the N-bar key.
Based on this observation, we may consider the possibility ofconstructing a semantic tagger specifically for PPs, which uses the surroundingcontext of the PP to arrive at a semantic analysis.
It is this task of PP semanticrole labelling that we target in this paper.A PP semantic role labeller would allow us to take a document and identifyall adjunct PPs with their semantics.
We would expect this to include a largeportion of locative and temporal expressions, e.g., in the document, providingvaluable data for tasks such as information extraction and question answering.Indeed our initial foray into PP semantic role labelling relates to an interest ingeospatial and temporal analysis, and the realisation of the importance of PPsin identifying and classifying spatial and temporal references.The contributions of this paper are to propose a method for PP semantic rolelabelling, and evaluate its performance over both the Penn treebank (includingcomparative evaluation with previous work) and also the data from the CoNLLSemantic Role Labelling shared task.
As part of this process, we identify theR.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
779?791, 2005.c?
Springer-Verlag Berlin Heidelberg 2005780 P. Ye and T. BaldwinFig.
1.
An example of the preposition semantic roles in Penn Teebanklevel of complementarity of a dedicated PP semantic role labeller with a conven-tional holistic semantic role labeller, suggesting PP semantic role labelling as apotential avenue for boosting the performance of existing systems.2 Preposition Semantic Role Disambiguation in PennTreebankSignificant numbers of prepositional phrases (PPs) in the Penn treebank [1] aretagged with their semantic role relative to the governing verb.
For example,Figure 1, shows a fragment of the parse tree for the sentence [Japan?s reservesof gold, convertible foreign currencies, and special drawing rights] fell by a hefty$1.82 billion in October to $84.29 billion [the Finance Ministry said], in whichthe three PPs governed by the verb fell are tagged as, respectively: PP-EXT(?extend?
), meaning how much of the reserve fell; PP-TMP (?temporal?
), meaningwhen the reserve fell; and PP-DIR (?direction?
), meaning the direction of the fall.According to our analysis, there are 143 preposition semantic roles in the tree-bank.
However, many of these semantic roles are very similar to one another;for example, the following semantic roles were found in the treebank: PP-LOC,PP-LOC-1, PP-LOC-2, PP-LOC-3, PP-LOC-4, PP-LOC-5, PP-LOC-CLR, PP-LOC-CLR-2, PP-LOC-CLR-TPC-1.
Inspection of the data revealed no systematicsemantic differences between these PP types.
Indeed, for most PPs, it was im-possible to distinguish the subtypes of a given superclass (e.g.
PP-LOC in ourexample).
We therefore decided to collapse the PP semantic roles based on theirfirst semantic feature.
For example, all semantic roles that start with PP-LOCare collapsed to the single class PP-LOC.
Table 1 shows the distribution of thecollapsed preposition semantic roles.
[2] describe a system1 for disambiguating the semantic roles of prepositions inthe Penn treebank according to 7 basic semantic classes.
In their system, O?Haraand Weibe used a decision tree classifier, and the following types of features:?
POS tags of surrounding tokens: The POS tags of the tokens before andafter the target preposition within a predefined window size.
In O?Hara andWiebe?s work, this window size is 2.1 This system was trained with WEKA?s J48 decision tree implementation.Semantic Role Labelling of Prepositional Phrases 781Table 1.
Penn treebank semantic role distribution (top-9 roles)Semantic Role Count Frequency MeaningPP-LOC 21106 38.2 LocativePP-TMP 12561 22.7 Temporal?Closely related?
(somewhere betweenPP-CLR 11729 21.2an argument and an adjunct)PP-DIR 3546 6.4 Direction (from/to X)PP-MNR 1839 3.3 Manner (incl.
instrumentals)PP-PRD 1819 3.3 Predicate (non-VP)PP-PRP 1182 2.1 Purpose or reasonPP-CD 654 1.2 Cardinal (numeric adjunct)PP-PUT 296 0.5 Locative complement of put?
POS tag of the target preposition?
The target preposition?
Word collocation: All the words in the same sentence as the target prepo-sition; each word is treated as a binary feature.?
Hypernym collocation: The WordNet hypernyms [3] of the open classwords before and after the target preposition within a predefined windowsize (set to 5 words); each hypernym is treated as a binary feature.O?Hara and Wiebe?s system also performs the following pre-classificationfiltering on the collocation features:?
Frequency constraint: f(coll) > 1, where coll is either a word from theword collocation or a hypernym from the hypernym collocation?
Conditional independence threshold: p(c|coll)?p(c)p(c) >= 0.2, where c is aparticular semantic role and coll is from the word collocation or a hypernymfrom the hypernym collocationWe began our research by replicating O?Hara and Wiebe?s method and seek-ing ways to improve it.
Our initial investigation revealed that there were around44000 word and hypernym collocation features even after the frequency con-straint filter and the conditional independence filter have been applied.
We didnot believe all these collocation features were necessary, and we deployed an ad-ditional ranking-based filtering mechanism over the collocation features to onlyselect collocation features which occur in the top N frequency bins.
Algorithm 1shows the details of this filtering mechanism.This ranking-based filtering mechanism allows us to select collocation featuresets of differing size, and in doing so not only improve the training and taggingAlgorithm 1.
Ranking based filtering algorithm1.
Let s be the list that contains the frequency of all the collocation features2.
Sort s in descending order3.
minFrequency = s[N ]4.
Discard all features whose frequency is less than minFrequency782 P. Ye and T. BaldwinTable 2.
Penn treebank preposition semantic role disambiguation resultsAccuracy (%)Ranking Classifier 1 Classifier 210 74.75 81.2820 76.53 83.5250 79.21 86.34100 80.13 87.02300 81.32 87.621000 82.34 87.71all 82.76 87.45O?Hara & Wiebe N/A 85.8speed of the preposition semantic role labelling, but also observe how the numberof collocation features affects the performance of the PP semantic role labellerand which collocation features are more important.2.1 ResultsSince some of the preposition semantic roles in the treebank have extremely lowfrequencies, we decided to build our first classifier using only the top 9 seman-tic roles, as detailed in Table 1.
We also noticed that the semantic roles PP-CLR,PP-CD and PP-PUT were excluded from O?Hara?s system which only used PP-BNF,PP-EXT, PP-MNR, PP-TMP, PP-DIR, PP-LOC and PP-PRP, therefore we built a sec-ond classifier using only the semantic roles used by O?Hara?s system2.
The twoclassifiers were trained with a maximum entropy [4] learner3.Table 2 shows the results of our classifier under stratified 10-fold cross val-idation4 using different parameters for the rank-based filter.
We also list theaccuracy reported by O?Hara and Wiebe for comparison.The results show that the performance of the classifier increases as we addmore collocation features.
However, this increase is not linear, and the improve-ment of performance is only marginal when the number collocation features isgreater than 100.
It also can be observed that there is a consistent performancedifference between classifiers 1 and 2, which may suggest that PP-CLR may beharder to distinguish from other semantic roles.
This is not totally surprisinggiven the relatively vague definition of the semantics of PP-CLR.
We return toanalyse these results in greater depth in Section 4.3 Preposition Semantic Role Labelling over the CoNLL2004 DatasetHaving built a classifier which has reasonable performance on the task of tree-bank preposition semantic role disambiguation, we decided to investigate2 PP-BNF with only 47 counts was not used by the second classifier.3 http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html4 O?Hara?s system was also evaluated using stratified 10-fold cross validation.Semantic Role Labelling of Prepositional Phrases 783whether we could use the same feature set to perform PP semantic role labellingover alternate systems of PP classification.
We chose the 2004 CoNLL SemanticRole Labelling (SRL) dataset [5] because it contained a wide range of semanticclasses of PPs, in part analogous to the Penn treebank data, and also becausewe wished to couple our method with a holistic SRL system to demonstrate theability of PP semantic role labelling to enhance overall system performance.Since the focus of the CoNLL data is on SRL relative to a set of pre-determined verbs for each sentence input,5 our primary objective is to inves-tigate whether the performance of SRL systems in general can be improved inany way by an independent preposition SRL system.
We achieve this by embed-ding our PP classification method within an existing holistic SRL system?thatis a system which attempts to tag all semantic role types in the CoNLL 2004data?through the following three steps:1.
Perform SRL on each preposition in the CoNLL dataset;2.
Merge the output of the preposition SRL with the output of a given verbSRL system over the same dataset;3.
Perform standard CoNLL SRL evaluation over the merged output.The details of preposition SRL and combination with the output of a holisticSRL system are discussed below.3.1 Breakdown of the Preposition Semantic Role Labelling ProblemPreposition semantic role labelling over the CoNLL dataset is considerably morecomplicated than the task of disambiguating preposition semantic roles in thePenn treebank.
There are three separate subtasks which are required to performpreposition SRL:1.
PP Attachment: determining which verb to attach each preposition to.2.
Preposition Semantic Role Disambiguation3.
Argument Segmentation: determining the boundaries of the semanticroles.The three subtasks are not totally independent of each other, as we demon-strate in the results section, and improved performance over one of the subtasksdoes not necessarily correlate with an improvement in the final results.3.2 PP Attachment ClassificationPP attachment (PPA) classification is the first step of preposition semantic rolelabelling and involves determining the verb attachment site for a given prepo-sition, i.e.
which of the pre-identified verbs in the sentence the preposition is5 Note that the CoNLL 2004 data identifies certain verbs as having argument struc-ture, and that the semantic role annotation is relative to these verbs only.
This isoften not the sum total of all verbs in a given sentence: the verbs in relative clauses,e.g., tend not to be identified as having argument structure.784 P. Ye and T. Baldwingoverned by.
Normally, this task would be performed by a parser.
However, sincethe CoNLL dataset contains no parsing information6 and we did not want to useany resources not explicitly provided in the CoNLL data, we had to construct aPPA classifier to specifically perform this task.This classifier uses the following features, all of which are derived from infor-mation provided in the CoNLL data:?
POS tags of surrounding tokens: The POS tags of the tokens before andafter the target preposition within a window size of 2 tokens ([?2, 2]).?
POS tag of the target preposition?
The target preposition?
Verbs and their relative position (VerbRelPos): All the (pre-identified) verbs in the same sentence as the target preposition and theirrelative positions to the preposition are extracted as features.
Each (verb,relative position) tuple is treated as a binary feature.
The relative positionsare determined in a way such that the 1st verb before the preposition willbe given the position ?1, the 2nd verb before the preposition will be giventhe position ?2, and so on.?
The type of the clause containing the target preposition?
Neighbouring chunk type: The types (NP, PP, VP, etc.)
of chunks beforeand after the target preposition within a window of 3 chunks.?
Word collocation (WordColl): All the open class words in the phrasesbefore and after the target preposition within a predefined window of 3chunks.?
Hypernym collocation (HyperColl): All the hypernyms from the openclass words in the phrases before and after the target preposition within apredefined window of 3 chunks.?
Named Entity collocation NEColl: All the named entity informationfrom the phrases before and after the target preposition within a predefinedwindow of 3 chunks.The PPA classifier outputs the relative position of the governing verb to thetarget preposition, or None if the preposition does not have a semantic role.We trained the PPA classifier over the CoNLL 2004 training set, and tested iton the testing set.
Table 3 shows the distribution of the classes in the testing set.The same maximum entropy learner used in the treebank SRL task was usedto train the PPA classifier.
The accuracy of this classifier on the CoNLL 2004testing set is 78.99%.3.3 Preposition Semantic Role DisambiguationFor the task of preposition semantic role disambiguation (SRD), we constructeda classifier using the same features as the PPA classifier, with the followingdifferences:6 The CoNLL 2005 SRL data does contain parse trees for the sentences, possiblyobviating the need for independent verb attachment classification.Semantic Role Labelling of Prepositional Phrases 785Table 3.
PPA class distributionPPA Count FrequencyNone 3005 60.71-1 1454 29.371 411 8.30-2 40 0.812 29 0.593 8 0.16-3 2 0.04-6 1 0.02Table 4.
CoNLL 2004 semantic role distribution in the CoNLL 2004 test dataset(top-14 roles)Semantic Role Count Frequency MeaningA1 424 21.79 Argument 1A2 355 18.24 Argument 2AM-TMP 299 15.36 Temporal adjunctAM-LOC 188 9.66 Locative adjunctA0 183 9.40 Argument 0AM-MNR 125 6.42 Manner adjunctA3 106 5.45 Argument 3AM-ADV 71 3.65 General-purpose adjunctA4 44 2.26 Argument 4AM-CAU 40 2.06 Causal adjunctAM-PNC 32 1.64 Purpose adjunctAM-DIS 32 1.64 Discourse markerAM-DIR 19 0.97 Directional adjunctAM-EXT 7 0.36 Extent adjunct1.
The window size for the POS tags of surrounding tokens is 5 tokens.2.
The window sizes for the WordColl, the HyperColl and the NeColl fea-tures are set to include the entire sentence.We trained the SRD classifier once again on the CoNLL 2004 training set,and tested it on the testing set.
Table 4 shows the distribution of the classes inthe testing set.We used the same maximum entropy leaner as for the PPA classifier to trainthe SRD classifier.
The accuracy of the SRD classifier on the CoNLL 2004 testingset is 58.68%.3.4 Argument SegmentationIn order to determine the extent of each NP selected for by a given preposition(i.e.
the span of words contained in the NP), we use a simple regular expressionover the chunk parser analysis of the sentence provided in the CoNLL 2004 data,786 P. Ye and T. Baldwinnamely: PP NP+.
We additionally experimented with a robust statistical parser[6] to determine PP extent, but found that the regular expression-based methodperformed equally well or marginally better, without requiring any resourcesexternal to the original task data.We make no attempt to perform separate evaluation of this particular subtaskbecause without the semantic role information, no direct comparison can bemade with the CoNLL data.3.5 Combining the Output of the SubtasksOnce we have identified the association between verbs and prepositions, and dis-ambiguated the semantic roles of the prepositions, we can begin the process of cre-ating the final output of the preposition semantic role labelling system.
This takesplace by identifying the data column corresponding to the verb governing eachclassified PP in the CoNLL data format (as determined by the PPA classifier),and recording the semantic role of that PP (as determined by the SRD classifier)over the full extent of the PP (as determined by the segmentation classifier).3.6 Merging the Output of Preposition SRL and Verb SRLOnce we have generated the output of the preposition SRL system, we canproceed to the final stage where the semantic roles of the prepositions are mergedwith the semantic roles of an existing holistic SRL system.It is possible, and indeed likely, that the semantic roles produced by the twosystems will conflict in terms of overlap in the extent of labelled constituentsand/or the semantic role labelling of constituents.
To address any such conflicts,we designed three merging strategies to identify the right balance between theoutputs of the two component systems:S1 When a conflict is encountered, only use the semantic role information fromthe holistic SRL system.S2 When a conflict is encountered, if the start positions of the semantic roleare the same for both SRL systems, then replace the semantic role of theholistic SRL system with that of the preposition SRL system, but keep theholistic SRL system?s boundary end.S3 When a conflict is encountered, only use the semantic role information fromthe preposition SRL system.3.7 ResultsTo evaluate the performance of our preposition SRL system, we combined itsoutputs with the 3 top-performing holistic SRL systems from the CoNLL 2004SRL shared task.7 The three systems are [7], [8] and [9].
Furthermore, in orderto establish the upper bound of the improvement of preposition SRL on verb7 Using the test data outputs of the three systems made available athttp://www.lsi.upc.edu/?srlconll/st04/st04.html.Semantic Role Labelling of Prepositional Phrases 787Table 5.
Preposition SRL results before merging with the holistic SRL systems, (P =precision, R = recall, F = F-score; above-baseline results in boldface)SRDAUTO SRDORACLESEGNP SEGORACLE SEGNP SEGORACLEP R F P R F P R F P R FVAAUTO 38.77 4.58 8.2 55.12 6.96 12.36 62.68 7.42 13.27 91.41 11.53 20.48VAORACLE 42.2 6.96 11.95 56.64 10.36 17.51 71.64 11.81 20.28 99.37 18.15 30.69Table 6.
Preposition SRL combined with [7] (P = precision, R = recall, F = F-score;above-baseline results in boldface)SRDAUTO SRDORACLESEGNP SEGORACLE SEGNP SEGORACLEP R F P R F P R F P R FORIG 72.43 66.77 69.49 72.43 66.77 69.49 72.43 66.77 69.49 72.43 66.77 69.49VAAUTO 72.00 66.84 69.32 72.08 66.91 69.40 72.13 66.95 69.44 72.31 67.11 69.61S1VAORACLE 71.92 67.02 69.38 71.97 67.30 69.55 72.29 67.39 69.75 72.81 68.12 70.39VAAUTO 71.34 66.22 68.68 70.66 65.60 68.04 73.12 67.89 70.41 73.42 68.16 70.69S2VAORACLE 71.01 66.16 68.50 69.78 65.21 67.42 73.68 68.67 71.08 74.35 69.55 71.87VAAUTO 70.10 65.00 67.46 72.25 66.83 69.43 73.12 67.84 70.38 77.16 71.39 74.16S3VAORACLE 70.38 65.91 68.07 73.10 68.67 70.81 75.58 70.82 73.12 81.42 76.55 78.91Table 7.
Preposition SRL combined with [8] (P = precision, R = recall, F = F-score;above-baseline results in boldface)SRDAUTO SRDORACLESEGNP SEGORACLE SEGNP SEGORACLEP R F P R F P R F P R FORIG 70.07 63.07 66.39 70.07 63.07 66.39 70.07 63.07 66.39 70.07 63.07 66.39VAAUTO 68.50 63.79 66.06 69.17 64.44 66.72 69.37 64.60 66.90 70.58 65.73 68.07S1VAORACLE 68.18 64.59 66.33 68.93 65.57 67.21 69.75 66.09 67.87 71.65 68.18 69.87VAAUTO 68.21 63.52 65.79 68.31 63.64 65.89 70.53 65.68 68.02 71.87 66.94 69.32S2VAORACLE 67.77 64.19 65.93 67.50 64.19 65.81 71.43 67.68 69.51 73.51 69.95 71.69VAAUTO 67.14 62.30 64.63 69.39 64.23 66.71 70.19 65.14 67.57 74.34 68.81 71.47S3VAORACLE 66.79 63.22 64.96 69.58 66.05 67.76 71.98 68.14 70.01 77.87 73.93 75.85SRL, and investigate how the three subtasks interact with each other and whattheir respective limits are, we also used oracled outputs from each subtask incombining the final outputs of the preposition SRL system.
The oracled outputsare what would be produced by perfect classifiers, and are emulated by inspectionof the gold-standard annotations for the testing data.Table 5 shows the results of the preposition SRL systems before they aremerged with the verb SRL systems.
These results show that the coverage of ourpreposition SRL system is quite low relative to the total number of arguments788 P. Ye and T. BaldwinTable 8.
Preposition SRL combined with [9] (P = precision, R = recall, F = F-score;above-baseline results in boldface)SRDAUTO SRDORACLESEGNP SEGORACLE SEGNP SEGORACLEP R F P R F P R F P R FORIG 71.81 61.11 66.03 71.81 61.11 66.03 71.81 61.11 66.03 71.81 61.11 66.03VAAUTO 70.23 61.87 65.78 70.74 62.43 66.32 71.13 62.65 66.62 72.34 63.83 67.82S1VAORACLE 69.61 62.63 65.94 70.20 63.60 66.74 71.57 64.38 67.79 73.49 66.60 69.87VAAUTO 69.92 61.60 65.50 69.91 61.69 65.54 72.10 63.50 67.53 73.39 64.75 68.80S2VAORACLE 69.14 62.19 65.48 68.84 62.35 65.43 72.79 65.47 68.94 74.83 67.82 71.15VAAUTO 69.01 60.66 64.57 71.31 62.57 66.65 72.24 63.49 67.58 76.54 67.15 71.54S3VAORACLE 68.77 61.86 65.13 71.59 64.81 68.03 74.19 66.74 70.27 80.25 72.67 76.27in the testing data, even when oracled outputs from all three subsystems areused (recall = 18.15%).
However, this is not surprising because we expected themajority of semantic roles to be noun phrases.In Tables 6, 7 and 8, we show how our preposition SRL system performswhen merged with the top 3 systems under the 3 merging strategies introducedin Section 3.6.
In each table, ORIG refers to the base system without prepositionSRL merging.We can make a few observations from the results of the merged systems.First, out of verb attachment, SRD and segmentation, the SRD module is both:(a) the component with the greatest impact on overall performance, and (b)the component with the greatest differential between the oracle performanceand classifier (AUTO) performance.
This would thus appear to be the area inwhich future efforts should be concentrated in order to boost the performanceof holistic SRLs through preposition SRL.Second, the results show that in most cases, the recall of the merged system ishigher than that of the original SRL system.
This is not surprising given that weare generally relabelling or adding information to the argument structure of eachverb, although with the more aggressive merging strategies (namely S2 and S3)it sometimes happens that recall drops, by virtue of the extent of an argumentbeing aversely affected by relabelling.
It does seem to point to a complementaritybetween verb-driven SRL and preposition-specific SRL, however.Finally, it was somewhat disappointing to see that in no instance did a fully-automated method surpass the base system in precision or F-score.
Having saidthis, we were encouraged by the size of the margin between the base systems andthe fully oracle-based systems, as it supports our base hypothesis that preposi-tion SRL has the potential to boost the performance of holistic SRL systems,up to a margin of 10% in F-score for S3.4 Analysis and DiscussionIn the previous 2 sections, we presented the methodologies and results of twosystems that perform statistical analysis on the semantics of prepositions, eachSemantic Role Labelling of Prepositional Phrases 789using a different data set.
The performance of the 2 systems was very differ-ent.
The SRD system trained on the treebank produced highly credible results,whereas the SRL system trained on CoNLL 2004 SRL data set produced some-what negative results.
In the remainder of this section, we will analyze theseresults and discuss their significance.There is a significant difference between the results obtained by the tree-bank classifier and that obtained by the CoNLL SRL classifier.
In fact, evenwith a very small number of collocation features, the treebank classifier stilloutperformed the CoNLL SRL classifier.
This suggests that the semantic tag-ging of prepositions is somewhat artificial.
This is evident in three ways.
First,the proportion of prepositional phrases tagged with semantic roles is small ?around 57,000 PPs out of the million-word Treebank corpus.
This small pro-portion suggests that the preposition semantic roles were tagged only in cer-tain prototypical situations.
Second, we were able to achieve reasonably highresults even when we used a collocation feature set with fewer than 200 fea-tures.
This further suggests that the semantic roles were tagged for only a smallnumber of verbs in relatively fixed situations.
Third, the preposition SRD sys-tem for the CoNLL data set used a very similar feature set to the treebanksystem, but was not able to produce anywhere near comparable results.
Sincethe CoNLL dataset is aimed at holistic SRL across all argument types, it in-corporates a much larger set of verbs and tagging scenarios; as a result, thesemantic role labelling of PPs is far more heterogeneous and realistic than isthe case in the treebank.
Therefore, we conclude that the results of our tree-bank preposition SRD system are not very meaningful in terms of predict-ing the success of the method at identifying and semantically labelling PPsin open text.A few interesting facts came out of the results over the CoNLL dataset.
Themost important one is that by using an independent preposition SRL system,the results of a general verb SRL system can be significantly boosted.
Thisis evident because when the oracled results of all three subtasks were used, themerged results were around 10% higher than those for the original systems, in allthree cases.
Unfortunately, it was also evident from the results that we were notsuccessful in automating preposition SRL.
Due to the strictness of the CoNLLevaluation, it was not always possible to achieve a better overall performanceby improving just one of the three subsystems.
For example, in some cases,worse results were achieved by using the oracled results for PPA, and the resultsproduced by SRD classifier than using the PPA classifier and the SRD classifiersin conjunction.
The reason for the worse results is that in our experiments, theoracled PPA always identifies more prepositions attached to verbs than the PPAclassifier, therefore more prepositions will be given semantic roles by the SRDclassifier.
However, since the performance of the SRD classifier is not high, andthe segmentation subsystem does not always produce the same semantic roleboundaries as the CoNLL data set, most of these additional prepositions wouldeither be given a wrong semantic role or wrong phrasal extent (or both), therebycausing the overall performance to fall.790 P. Ye and T. BaldwinFinally, it is evident that the merging strategy also plays an important rolein determining the performance of the merged preposition SRL and verb SRLsystems: when the performance of the preposition SRL system is high, a morepreposition-oriented merging scheme would produce better overall results, andvice versa.5 Conclusion and Future WorkIn this paper, we have proposed a method for labelling preposition semantics anddeployed the method over two different data sets involving preposition semantics.We have shown that preposition semantics is not a trivial problem in general,and also that has the potential to complement other semantic analysis tasks,such as semantic role labelling.Our analysis of the results of the preposition SRL system shows that sig-nificant improvement in all three stages of preposition semantic role labelling?namely verb attachment, preposition semantic role disambiguation and argu-ment segmentation?must be achieved before preposition SRL can make a sig-nificant contribution to holistic SRL.
The unsatisfactory results of our CoNLLpreposition SRL system show that the relatively simplistic feature sets used inour research are far from sufficient.
Therefore, we will direct our future worktowards using additional NLP tools, information repositories and feature engi-neering to improve all three stages of preposition semantic role labelling.AcknowledgementsWe would like to thank Phil Blunsom and Steven Bird for their suggestions andencouragement, Tom O?Hara for providing insight into the inner workings ofhis semantic role disambiguation system, and the anonymous reviewers for theircomments.References1.
Marcus, M.P., Marcinkiewicz, M.A., Santorini, B.: Building a large annotated corpusof English: the Penn treebank.
Computational Linguistics 19 (1993) 313?3302.
O?Hara, T., Wiebe, J.: Preposition semantic classification via treebank andFrameNet.
In: Proc.
of the 7th Conference on Natural Language Learning (CoNLL-2003), Edmonton, Canada (2003)3.
Miller, G.A.
: WordNet: a lexical database for English.
Communications of the ACM38 (1995) 39?414.
Berger, A.L., Pietra, V.J.D., Pietra, S.A.D.
: A maximum entropy approach tonatural language processing.
Computational Linguistics 22 (1996) 39?715.
Carreras, X., Ma`rquez, L.: Introduction to the CoNLL-2004 shared task: Seman-tic role labeling.
In: Proc.
of the 8th Conference on Natural Language Learning(CoNLL-2004), Boston, USA (2004) 89?97Semantic Role Labelling of Prepositional Phrases 7916.
Briscoe, T., Carroll, J.: Robust accurate statistical annotation of general text.
In:Proc.
of the 3rd International Conference on Language Resources and Evaluation(LREC 2002), Las Palmas, Canary Islands (2002) 1499?15047.
Hacioglu, K., Pradhan, S., Ward, W., Martin, J.H., Jurafsky, D.: Semantic rolelabeling by tagging syntactic chunks.
In: Proc.
of the 8th Conference on NaturalLanguage Learning (CoNLL-2004), Boston, USA (2004)8.
Punyakanok, V., Roth, D., Yih, W.T., Zimak, D., Tu, Y.: Semantic role labelingvia generalized inference over classifiers.
In: Proc.
of the 8th Conference on NaturalLanguage Learning (CoNLL-2004), Boston, USA (2004)9.
Carreras, X., Ma`rquez, L., Chrupa, G.: Hierarchical recognition of propositionalarguments with perceptrons.
In: Proc.
of the 8th Conference on Natural LanguageLearning (CoNLL-2004), Boston, USA (2004)
