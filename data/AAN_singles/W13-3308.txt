Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60?69,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsFeature Weight Optimization for Discourse-Level SMTSara Stymne, Christian Hardmeier, Jo?rg Tiedemann and Joakim NivreUppsala UniversityDepartment of Linguistics and PhilologyBox 635, 751 26 Uppsala, Swedenfirstname.lastname@lingfil.uu.seAbstractWe present an approach to feature weightoptimization for document-level decoding.This is an essential task for enabling futuredevelopment of discourse-level statisticalmachine translation, as it allows easy inte-gration of discourse features in the decod-ing process.
We extend the framework ofsentence-level feature weight optimizationto the document-level.
We show experi-mentally that we can get competitive andrelatively stable results when using a stan-dard set of features, and that this frame-work also allows us to optimize document-level features, which can be used to modeldiscourse phenomena.1 IntroductionDiscourse has largely been ignored in traditionalmachine translation (MT).
Typically each sentencehas been translated in isolation, essentially yield-ing translations that are bags of sentences.
It iswell known from translation studies, however, thatdiscourse is important in order to achieve goodtranslations of documents (Hatim and Mason,1990).
Most attempts to address discourse-levelissues for statistical machine translation (SMT)have had to resort to solutions such as post-processing to address lexical cohesion (Carpuat,2009) or two-step translation to address pronounanaphora (Le Nagard and Koehn, 2010).
Recently,however, we presented Docent (Hardmeier et al2012; Hardmeier et al 2013), a decoder basedon local search that translates full documents.
Sofar this decoder has not included a feature weightoptimization framework.
However, feature weightoptimization, or tuning, is important for any mod-ern SMT decoder to achieve a good translationperformance.In previous research with Docent, we used gridsearch to find weights for document-level featureswhile base features were optimized using stan-dard sentence-level techniques.
This approach isimpractical since many values for the extra fea-tures have to be tried, and, more importantly, itmight not give the same level of performance asjointly optimizing all parameters.
Principled fea-ture weight optimization is thus essential for re-searchers that want to use document-level featuresto model discourse phenomena such as anaphora,discourse connectives, and lexical consistency.
Inthis paper, we therefore propose an approach thatsupports discourse-wide features in document-level decoding by adapting existing frameworksfor sentence-level optimization.
Furthermore, weinclude a thorough empirical investigation of thisapproach.2 Discourse-Level SMTTraditional SMT systems translate texts sentenceby sentence, assuming independence between sen-tences.
This assumption allows efficient algo-rithms based on dynamic programming for explor-ing a large search space (Och et al 2001).
Be-cause of the dynamic programming assumptions itis hard to directly include discourse-level featuresinto a traditional SMT decoder.
Nevertheless,there have been several attempts to integrate inter-sentential and long distance models for discourse-level phenomena into standard decoders, usuallyas ad-hoc additions to standard models, address-ing a single phenomenon.Several studies have tried to improve pro-noun anaphora by adding information about theantecedent, either by using two-step decoding(Le Nagard and Koehn, 2010; Guillou, 2012) orby extracting information from previously trans-lated sentences (Hardmeier and Federico, 2010),unfortunately without any convincing results.
Toaddress the translation of discourse connectives,source-side pre-processing has been used to anno-tate surface forms either in the corpus or in the60phrase-table (Meyer and Popescu-Belis, 2012) orby using factored decoding (Meyer et al 2012)to disambiguate connectives, with small improve-ments.
Lexical consistency has been addressedby the use of post-processing (Carpuat, 2009),multi-pass decoding (Xiao et al 2011; Ture et al2012), and cache models (Tiedemann, 2010; Gonget al 2011).
Gong et al(2012) addressed theissue of tense selection for translation from Chi-nese, by the use of inter-sentential tense n-grams,exploiting information from previously translatedsentences.
Another way to use a larger contextis by integrating word sense disambiguation andSMT.
This has been done by re-initializing phraseprobabilities for each sentence (Carpuat and Wu,2007), by introducing extra features in the phrase-table (Chan et al 2007), or as a k-best re-rankingtask (Specia et al 2008).
Another type of ap-proach is to integrate topic modeling into phrasetables (Zhao and Xing, 2010; Su et al 2012).
Fora more thorough overview of discourse in SMT,see Hardmeier (2012).Here we instead choose to work with the re-cent document-level SMT decoder Docent (Hard-meier et al 2012).
Unlike in traditional decod-ing were documents are generated sentence bysentence, feature models in Docent always haveaccess to the complete discourse context, evenbefore decoding is finished.
It implements thephrase-based SMT approach (Koehn et al 2003)and is based on local search, where a state con-sists of a full translation of a document, which isimproved by applying a series of operations to im-prove the translation.
A hill-climbing strategy isused to find a (local) maximum.
The operationsallow changing the translation of a phrase, chang-ing the word order by swapping the positions oftwo phrases, and resegmenting phrases.
The initialstate can either be initialized randomly in mono-tonic order, or be based on an initial run from astandard sentence-based decoder.
The number ofiterations in the decoder is controlled by two pa-rameters, the maximum number of iterations anda rejection limit, which stops the decoder if nochange was made in a certain number of iterations.This setup is not limited by dynamic programmingconstraints, and enables the use of the translatedtarget document to extract features.
It is thus easyto directly integrate discourse-level features intoDocent.
While we use this specific decoder in ourexperiments, the method proposed for document-level feature weight optimization is not limited toit.
It can be used with any decoder that outputsfeature values at the document level.3 Sentence-Level TuningTraditionally, feature weight optimization, or tun-ing, for SMT is performed by an iterative processwhere a development set is translated to produce ak-best list.
The parameters are then optimized us-ing some procedure, generally to favor translationsin the k-best list that have a high score on someMT metric.
The translation step is then repeatedusing the new weights for decoding, and optimiza-tion is continued on a new k-best list, or on a com-bination of all k-best lists.
This is repeated untilsome end condition is satisfied, for instance for aset number of iterations, until there is only verysmall changes in parameter weights, or until thereare no new translations in the k-best lists.SMT tuning is a hard problem in general, partlybecause the correct output is unreachable andalso because the translation process includes la-tent variables, which means that many efficientstandard optimization procedures cannot be used(Gimpel and Smith, 2012).
Nevertheless, thereare a number of techniques including MERT (Och,2003), MIRA (Chiang et al 2008; Cherry andFoster, 2012), PRO (Hopkins and May, 2011),and Rampion (Gimpel and Smith, 2012).
All ofthese optimization methods can be plugged intothe standard optimization loop.
All of the meth-ods work relatively well in practice, even thoughthere are limitations, for instance that many meth-ods are non-deterministic meaning that their re-sults are somewhat unstable.
However, there aresome important differences.
MERT is based onscores for the full test set, whereas the other meth-ods are based on sentence-level scores.
MERTalso has the drawback that it only works well forsmall sets of features.
In this paper we are notconcerned with the actual optimization algorithmand its properties, though, but instead we focuson the integration of document-level decoding intothe existing optimization frameworks.In order to adapt sentence-level frameworks toour needs we need to address the granularity ofscoring and the process of extracting k-best lists.For document-level features we do not have mean-ingful scores on the sentence level which are re-quired in standard optimization frameworks.
Fur-thermore, the extraction of k-best lists is not as61Input: inputDocs, refDocs, init weights ?0, max decoder iters max, sample start ss, sample interval si,Output: learned weights ?1: ?
?
?02: Initialize empty klist3: run?
14: repeat5: Initialize empty klistrun6: for doc?
1, inputDocs.size do Initialize decoder state randomly for inputDocs[doc]7: for iter?
1,max do8: Perform one hill-climbing step for inputDocs[doc]9: if iter >= ss & iter mod si == 0 then10: Add translation for inputDocs[doc] to klistrun11: end if12: end for13: end for14: Merge klistrun with klist15: modelScoresdoc ?
ComputeModelScores(klist)16: metricStatsdoc ?
ComputeMetricStats(klist, refDocs)17: ?run ?
?18: ?
?
Optimize(?run,modelScoresdoc,metricStatsdoc)19: run?
run + 120: until Done(run, ?, ?run)Figure 1: Document-level feature weight optimization algorithmstraightforward in our hill-climbing decoder as instandard sentence-level decoders such as Moses(Koehn et al 2007) where such a list can be ap-proximated easily from the internal beam searchstrategy.
Working on output lattices is another op-tion in standard approaches (Cherry and Foster,2012) which is also not applicable in our case.In the following section we describe how wecan address these issues in order to adapt sentence-level frameworks for our purposes.4 Document-Level TuningTo allow document-level feature weight optimiza-tion, we make some small changes to the sentence-level framework.
Figure 1 shows the algorithm weuse.
It assumes access to an optimization algo-rithm, Optimize, and an end criterion, Done.The changes from standard sentence-level opti-mization is that we compute scores on the docu-ment level, and that we sample translations insteadof using standard k-best lists.The main challenge is that we need meaning-ful scores which we do not have at the sentencelevel in document decoding.
We handle this bysimply computing all scores (model scores andmetric scores) exclusively at the document level.Remember that all standard MT metrics based onsentence-level comparisons with reference trans-lations can be aggregated for a complete test set.Here we do the same for all sentences in a givendocument.
This can actually be an advantage com-pared to optimization methods that use sentence-level scores, which are known to be unreliable(Callison-Burch et al 2012).
Document-levelscores should thus be more stable, since they arebased on more data.
A potential drawback is thatwe get fewer data points with a test set of the samesize, which might mean that we need more data toachieve as good results as with sentence-level op-timization.
We will see the ability of our approachto optimize weights with reasonable data sets inour experiments further down.The second problem, the extraction of k-bestlists can be addressed in several ways.
It is pos-sible to get a k-best list from Docent by extract-ing the results from the last k iterations.
However,since Docent operates on the document-level anddoes not accept updates in each iteration, there willbe many identical and/or very similar hypotheseswith such an approach.
Another option would beto extract the translations from the k last differ-ent iterations, which would require some smallchanges to the decoder.
Instead, we opt to use k-lists, lists of translations sampled with some inter-val, which contains k translations, but not neces-sarily all the k best translations that could be foundby the decoder.
A k-best list is of course a k-list,which we get with a sample interval of 1.We also choose to restart Docent randomly ineach optimization iteration, since it allows us toexplore a larger part of the search space.
Weempirically found that this strategy worked betterthan restarting the decoder from the previous beststate.62German?English English?SwedishType Sentences Documents Type Sentences DocumentsTrainingEuroparl 1.9M ?
Europarl 1.5M ?News Commentary 178K ?
?
?
?TuningNews2009 2525 111 Europarl (Moses) 2000 ?News2008-2010 7567 345 Europarl (Docent) 1338 100Test News2012 3003 99 Europarl 690 20Table 1: Domain and number of sentences and documents for the corporaAs seen in Figure 1, there are some additionalparameters in our procedure: the sample start iter-ation and the sample interval.
We also need to setthe number of decoder iterations to run.
In Sec-tion 5 we empirically investigate the effect of theseparameters.Compared to sentence-level optimization, wealso have a smaller number of units to get scoresfrom, since we use documents as units, and notsentences.
The importance of this depends on theoptimization algorithm.
MERT calculates metricscores over the full tuning set, not for individualsentences, and should not be affected too muchby the change in granularity.
Many other opti-mization algorithms, like PRO, work on the sen-tence level, and will likely be more affected bythe reduction of units.
In this work we focus onMERT, which is the most commonly used opti-mization procedure in the SMT community, andwhich tends to work quite well with relatively fewfeatures.
However, we also show contrastive re-sults for PRO (Hopkins and May, 2011).
A fur-ther issue is that Docent is non-deterministic, i.e.,it can give different results with the same param-eter weights.
Since the optimization process is al-ready somewhat unstable this is a potential issuethat needs to be explored further, which we do inSection 5.Implementation-wise we adapted Docent to out-put k-lists and adapted the infrastructure availablefor tuning in the Moses decoder (Koehn et al2007) to work with document-level scores.
Thissetup allows us to use the variety of optimizationprocedures implemented there.5 ExperimentsIn this section we report experimental resultswhere we investigate several issues in connec-tion with document-level feature weight optimiza-tion for SMT.
We first describe the experimentalsetup, followed by baseline results using sentence-level optimization.
We then present validation ex-periments with standard sentence-level features,which can be compared to standard optimization.Finally, we report results with a set of document-level features that have been proposed for jointtranslation and text simplification (Stymne et al2013).5.1 Experimental SetupMost of our experiments are for German-to-English news translation using data from theWMT13 workshop.1 We also show results withdocument-level features for English-to-SwedishEuroparl (Koehn, 2005).
The size of the training,tuning, and test sets are shown in Table 1.
First ofall, we need to extract documents for tuning andtesting with Docent.
Fortunately, the news data al-ready contain document markup, corresponding toindividual news articles.
For Europarl we define adocument as a consecutive sequence of utterancesfrom a single speaker.
To investigate the effect ofthe size of the tuning set, we used different subsetsof the available tuning data.All our document-level experiments are car-ried out with Docent but we also contrast withthe Moses decoder (Koehn et al 2007).
For thepurpose of comparison, we use a standard set ofsentence-level features used in Moses in most ofour experiments: five translation model features,one language model feature, a distance-based re-ordering penalty, and a word count feature.
Forfeature weight optimization we also apply thestandard settings in the Moses toolkit.
We opti-mize towards the Bleu metric, and optimizationends either when no weights are changed by morethan 0.00001, or after 25 iterations.
MERT is usedunless otherwise noted.Except for one of our baselines, we always runDocent with random initialization.
For test we runthe document decoder for a maximum of 227 iter-ations with a rejection limit of 100,000.
In ourexperiments, the decoder always stopped whenreaching the rejection limit, usually between 1?51http://www.statmt.org/wmt13/translation-task.html63million iterations.We show results on the Bleu (Papineni et al2002) and NIST (Doddington, 2002) metrics.
ForGerman?English we show the average result andstandard deviation of three optimization runs, tocontrol for optimizer instability as proposed byClark et al(2011).
For English?Swedish we re-port results on single optimization runs, due totime constraints.5.2 BaselinesMost importantly, we would like to show the effec-tiveness of the document-level tuning proceduredescribed above.
In order to do this, we createda baseline using sentence-level optimization witha tuning set of 2525 sentences and the News2009corpus for evaluation.
Increasing the tuning set isknown to give only modest improvements (Turchiet al 2012; Koehn and Haddow, 2012).The feature weights optimized with the stan-dard Moses decoder can then directly be used inour document-level decoder as we only includesentence-level features in our baseline model.
Asexpected, these optimized weights also lead toa better performance in document-level decodingcompared to an untuned model as shown in Ta-ble 2.
Note, that Docent can be initialized intwo ways, by Moses and randomly.
Not surpris-ingly, the result for the runs initialized with Mosesare identical with the pure sentence-level decoder.Initializing randomly gives a slightly lower Bleuscore but with a larger variation than with Mosesinitialization, which is also expected.
Docent isnon-deterministic, and can give somewhat varyingresults with the same weights.
However, this vari-ation has been shown experimentally to be verysmall (Hardmeier et al 2012).Our goal now is to show that document-leveltuning can perform equally well in order to verifyour approach.
For this, we set up a series of ex-periments looking at varying tuning sets and dif-ferent parameters of the decoding and optimiza-tion procedure.
With this we like to demonstratethe stability of the document-level feature weightoptimization approach presented above.
Note thatthe most important baselines for comparison withthe results in the next sections are the ones withDocent and random initialization.5.3 Sentence-Level FeaturesIn this section we present validation results wherewe investigate different aspects of document-System Tuning Bleu NISTMoses None 17.7 6.25Docent-M None 17.7 6.25Docent-R None 15.2 (0.05) 5.88 (0.00)Moses Moses 18.3 (0.04) 6.22 (0.01)Docent-M Moses 18.3 (0.04) 6.22 (0.01)Docent-R Moses 18.1 (0.13) 6.23 (0.01)Table 2: Baseline results, where Docent-M is ini-tialized with Moses and Docent-R randomlyDocs Sent.
Min Max Bleu NIST111 2525 3 127 18.0 (0.11) 6.19 (0.04)345 7567 3 127 18.1 (0.14) 6.25 (0.02)100 1921 8 40 18.0 (0.05) 6.25 (0.10)200 3990 8 40 17.9 (0.25) 6.20 (0.09)100 2394 8 100 18.0 (0.12) 6.27 (0.07)200 4600 8 100 18.1 (0.29) 6.26 (0.10)300 6852 8 100 18.2 (0.13) 6.27 (0.03)Table 3: Results for German?English with varyingsizes of tuning set, where the number of sentencesand documents are varied, as well as the minimumand maximum number of sentences per documentlevel feature weight optimization with standardsentence-level features.
In this way we can com-pare the results directly to standard sentence-leveloptimization, and to the results of Moses.Corpus size We investigate how tuning is af-fected by corpus size.
The corpus size was var-ied in two ways, by changing the number of docu-ments in the tuning set, and by changing the lengthof documents in the tuning sets.
In this exper-iment we run 20000 decoder iterations per opti-mization iteration, and use a k-list of size 101,with sample interval 100.
Table 3 shows the re-sults with varying tuning set sizes for German?English.
There is very little variation between thescores, and no clear tendencies.
All results are ofsimilar quality to the baseline with random initial-ization and sentence-level tuning, and better thannot using any tuning.
The top line in Table 3 isNews2009, the same tuning set as for the base-lines.
The scores are somewhat more unstable thanthe baseline scores, but stability is not related tocorpus size.
In the following sections we will usethe tuning set with 200 documents, size 8-40.Number of decoder iterations and k-list sam-pling Two issues that are relevant for featureweight optimization with the document-level de-coder is the number of decoder hill-climbing iter-ations in each optimization iteration, and the set-tings for k-list sampling.
These choices affect the64Iterations K-list UTK Bleu NIST20000 101 55.6 17.9 (0.25) 6.20 (0.09)30000 201 67.2 17.9 (0.06) 6.21 (0.01)40000 301 79.9 18.2 (0.11) 6.28 (0.09)50000 401 86.9 18.1 (0.20) 6.22 (0.05)75000 651 99.2 17.8 (0.15) 6.13 (0.03)100000 901 106.8 17.9 (0.17) 6.16 (0.03)30000 101 21.6 18.0 (0.15) 6.21 (0.02)40000 101 12.6 17.7 (0.53) 6.12 (0.15)50000 101 8.2 17.9 (0.24) 6.18 (0.06)Table 4: Results for German?English with a vary-ing number of iterations and k-list size (UTK isthe average number of unique translations per doc-ument in the k-lists)quality of the translations in each optimization it-eration, and the spread in the k-list.
We will reportthe average number of unique translations per doc-ument in the k-lists, UTK, during feature weightoptimization, in this section.The top half of Table 4 shows results with adifferent number of iterations, when we samplek-lists from iteration 10000 with interval 100 forGerman?English, which means that the size of thek-lists also changes.
The differences on MT met-rics are very small.
The number of new uniquetranslations in the k-lists decrease with the numberof decoder iterations.
With 20K iterations, 55%of the k-lists entries are unique, which could becompared to only 12% with 100K iterations.
Themajority of the unique translations are thus foundin the beginning of the decoding, which is not sur-prising.The bottom half of Table 4 shows results witha different number of decoder iterations, but a setk-list size.
In this setting the number of uniquehypotheses in the k-lists obviously decreases withthe number of decoder iterations.
Despite this,there are mostly small result differences, exceptfor 40K iterations, which has more unstable resultsthan the other settings.
It does not seem useful toincrease the number of decoder iterations withoutalso increasing the size of the k-list.
An even bet-ter strategy might be to only include unique entriesin the k-lists.
We will explore this in future work.We also ran experiments where we did notrestart the decoder with a random state in each iter-ation, but instead saved the previous state and con-tinued decoding with the new weights from there.This, however, was largely unsuccessful, and gavevery low scores.
We believe that the reason for thisis mainly that a much smaller part of the searchspace is explored when the decoder is not restartedInterval Start UTK Bleu NIST1 19900 1.4 18.2 (0.07) 6.25 (0.04)10 19000 5.2 18.1 (0.08) 6.22 (0.03)100 10000 55.6 17.9 (0.25) 6.20 (0.09)200 0 82.2 17.9 (0.19) 6.15 (0.05)Table 5: Results with different k-list-sample inter-vals for k-lists size 101 (UTK is the average num-ber of unique translations per document in the k-lists)with a new seed repeatedly.
The fact that a higheroverall quality can be achieved with a higher num-ber of iterations (see Figure 2) can apparently notcompensate for this drawback.Finally, we investigate the effect of the sam-ple interval for the k-lists.
To get k-lists of equalsize, 101, we start the sampling at different itera-tions.
Table 5 shows the results, and we can seethat with a small sample interval, the number ofunique translations decreases drastically.
Despitethis, there are no large result differences.
Thereis actually a slight trend that a smaller sample in-terval is better.
This does not confirm our intuitionthat it is important with many different translationsin the k-list.
Especially for interval 1 it is surpris-ing, since there is often only 1 unique translationfor a single document.
We believe that the fact thatk-lists from different iterations are joined, can bepart of the explanation for these results.
We thinkmore work is needed in the future, to further ex-plore these settings, and the interaction with thetotal number of decoder iteration, and the k-listsampling.To further shed some light to these results, weshow learning curves from the optimization.
Fig-ure 2 shows Bleu scores for the system optimizedwith 100K decoder iterations after different num-bers of iterations, for the last three iterations ineach of the three optimization runs.
As shown inHardmeier et al(2012), the translation quality in-creases fast at first, but start to level out at around40K iterations.
Despite this, the optimization re-sults are good even with 20K iterations, which issomewhat surprising.
Figures 3 and 4 show theBleu scores after each tuning iteration for the sys-tems in Tables 4 and 5.
As is normal for SMT tun-ing, the convergence is slow, and there are someoscillations even late in the optimization.
Over-all systems with many iterations seem somewhatmore stable.Overall, the results are better than the untuned651414.5 1515.5 1616.5 1717.5102030405060708090100BleuDecoder iterations (*1000)1-23 1-24 1-25 2-23 2-24 2-25 3-23 3-24 3-25Figure 2: Bleu scores during 100000 Docent iter-ations during feature weight optimization46810121416180510152025BleuTuningiterations20K 30K 40K 50K 75K 100KFigure 3: Bleu scores during feature weight opti-mization for systems with different number of de-coder iterations and k-list sizes.baseline and on par with the sentence-level tuningbaselines in all settings, with a relatively modestvariation, even across settings.
In fact, if we cal-culate the total scores of all 36 systems in Tables 4and 5, we get a Bleu score of 18.0 (0.23) and aNIST score of 6.19 (0.07), with a variation that isnot higher than for many of the different settings.Optimization method In this section we com-pare the performance of the MERT optimiza-tion algorithm with that of PRO, and a combi-nation that starts MERT with weights initializedwith PRO (MERT+PRO), suggested by Koehn andHaddow (2012).
Here we run 30000 decoder it-erations.
Table 6 shows the results.
InitializingMERT with PRO did not affect the scores much.The scores with only PRO, however, are slightlylower than for MERT, and have a much largerscore variation.
This could be because PRO is46810121416180510152025BleuTuningiterations1 (20K)10 (20K)100(20K)100(30K)100(40K)100(50K)200(20K)Figure 4: Bleu scores during feature weight opti-mization for systems with different k-list sampleinterval and number of decoder iterations.Bleu NISTMERT 17.9 (0.06) 6.21 (0.01)PRO 17.5 (0.41) 6.15 (0.20)MERT+PRO 18.0 (0.12) 6.18 (0.06)Table 6: Results with different optimization algo-rithms for German?Englishlikely to need more data, since it calculates met-ric scores on individual units, sentences or docu-ments, not across the full tuning set, like MERT.This likely means that 200 documents are too fewfor stable results with optimization methods thatdepend on unit-level metric scores.5.4 Document-Level FeaturesIn this section we investigate the effect of opti-mization with a number of document-level fea-tures.
We use a set of features proposed in Stymneet al(2013), in order to promote the readabilityof texts.
In this scenario, however, we use thesefeatures in a standard SMT setting, where theycan potentially improve the lexical consistency oftranslations.
The features are:?
Type token ratio (TTR) ?
the ratio of types,unique words, to tokens, total number ofwords?
OVIX ?
a reformulation of TTR that has tra-ditionally been used for Swedish and that isless sensible to text length than TTR, seeEq.
1?
Q-value, phrase level (QP) - The Q-value wasdeveloped as a measure for bilingual termquality (Dele?ger et al 2006), to promotecommon and consistently translated terms.See Eq.
2, where f(st) is the frequency of66German?English English?SwedishSystem Optimization Bleu NIST Bleu NISTMoses Sentence 18.3 (0.04) 6.22 (0.01) 24.3 6.12Docent Sentence 18.1 (0.13) 6.23 (0.01) 24.1 6.06Docent Document 17.9 (0.25) 6.20 (0.09) 23.4 6.01TTR Document 18.3 (0.16) 6.33 (0.04) 23.6 6.15OVIX Document 18.3 (0.13) 6.30 (0.03) 23.4 5.99QW Document 18.1 (0.14) 6.22 (0.03) 24.2 6.11QP Document 18.0 (0.10) 6.23 (0.05) 21.2 5.70Table 7: Results when using document-level featuresthe phrase pair, n(s) is the number of uniquetarget phrases which the source phrase isaligned to in the document, and n(t) is thesame for the target phrase.
Here the Q-valueis applied on the phrase level.?
Q-value, word level (QW) - Same as above,but here we apply the Q-value for sourcewords and their alignments on the target side.OVIX =log(count(tokens))log(2?log(count(types))log(count(tokens))) (1)Q-value =f(st)n(s) + n(t)(2)We added these features one at a time to thestandard feature set.
Optimization was performedwith 20000 decoder iterations, and a k-list of size101.
As shown in the previous sections, thereare slightly better settings, which could have beenused to boost the results somewhat.The results are shown in Table 7.
For German?English, the results are generally on par with thebaselines for Bleu and slightly higher on NIST forOVIX and TTR.
For English?Swedish, we used asmaller tuning set on the document level than onthe sentence level, see Table 1, due to time con-straints.
This is reflected in the scores, which aregenerally lower than for sentence-level decoding.Using the QW feature, however, we receive com-petitive scores to the sentence-based baselines,which indicates that it can be meaningful to usedocument-level features with the suggested tuningapproach.While the results do not improve much overthe baselines, these experiments still show thatwe can optimize discourse-level features withour approach.
We need to identify more usefuldocument-level features in future work, however.6 ConclusionWe have shown how the standard feature weightoptimization workflow for SMT can be adapted todocument-level decoding, which allows easy inte-gration of discourse-level features into SMT.
Wemodified the standard framework by calculatingscores on the document-level instead of the sen-tence level, and by using k-lists rather than k-bestlists.Experimental results show that we can achieverelatively stable results, on par with the results forsentence-level optimization and better than with-out tuning, with standard features.
This is de-spite the fact that we use the hill-climbing de-coder without initialization by a standard decoder,which means that it is somewhat unstable, andis not guaranteed to find any global maximum,even according to the model.
We also show thatwe can optimize document-level features success-fully.
We investigated the effect of a number ofparameters relating to tuning set size, the numberof decoder iterations, and k-list sampling.
Therewere generally small differences relating to theseparameters, however, indicating that the suggestedapproach is robust.
The interaction between pa-rameters does need to be better explored in futurework, and we also want to explore better sampling,without duplicate translations.This is the first attempt of describing and exper-imentally investigating feature weight optimiza-tion for direct document-level decoding.
While weshow the feasibility of extending sentence-leveloptimization to the document level, there is stillmuch more work to be done.
We would, for in-stance, like to investigate other optimization pro-cedures, especially for systems with a high num-ber of features.
Most importantly, there is a largeneed for the development of useful discourse-levelfeatures for SMT, which can now be optimized.AcknowledgmentsThis work was supported by the Swedish strategicresearch programme eSSENCE.67ReferencesChris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montre?al, Canada.Marine Carpuat and Dekai Wu.
2007.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 61?72, Prague, Czech Republic.Marine Carpuat.
2009.
One translation per discourse.In Proceedings of the Workshop on Semantic Evalu-ations: Recent Achievements and Future Directions(SEW-2009), pages 19?27, Boulder, Colorado.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word sense disambiguation improves statisti-cal machine translation.
In Proceedings of the 45thAnnual Meeting of the ACL, pages 33?40, Prague,Czech Republic.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages427?436, Montre?al, Canada.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of HumanLanguage Technologies: The 2008 Annual Con-ference of the NAACL, pages 224?233, Honolulu,Hawaii.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for op-timizer instability.
In Proceedings of the 49th An-nual Meeting of the ACL: Human Language Tech-nologies, pages 176?181, Portland, Oregon, USA.Louise Dele?ger, Magnus Merkel, and Pierre Zweigen-baum.
2006.
Enriching medical terminologies:an approach based on aligned corpora.
In Inter-national Congress of the European Federation forMedical Informatics, pages 747?752, Maastricht,The Netherlands.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology, pages 228?231, San Diego, California,USA.Kevin Gimpel and Noah A. Smith.
2012.
Struc-tured ramp loss minimization for machine transla-tion.
In Proceedings of the 2012 Conference ofthe NAACL: Human Language Technologies, pages221?231, Montre?al, Canada.Zhengxian Gong, Min Zhang, and Guodong Zhou.2011.
Cache-based document-level statistical ma-chine translation.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 909?919, Edinburgh, Scotland,UK.Zhengxian Gong, Min Zhang, Chew Lim Tan, andGuodong Zhou.
2012.
N-gram-based tense modelsfor statistical machine translation.
In Proceedingsof the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, pages 276?285,Jeju Island, Korea.Liane Guillou.
2012.
Improving pronoun translationfor statistical machine translation.
In Proceedings ofthe EACL 2012 Student Research Workshop, pages1?10, Avignon, France.Christian Hardmeier and Marcello Federico.
2010.Modelling pronominal anaphora in statistical ma-chine translation.
In Proceedings of the Interna-tional Workshop on Spoken Language Translation,pages 283?289, Paris, France.Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-mann.
2012.
Document-wide decoding for phrase-based statistical machine translation.
In Proceed-ings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1179?1190, Jeju Island, Korea.Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,and Joakim Nivre.
2013.
Docent: A document-leveldecoder for phrase-based statistical machine transla-tion.
In Proceedings of the 51st Annual Meeting ofthe ACL, Demonstration session, Sofia, Bulgaria.Christian Hardmeier.
2012.
Discourse in statisticalmachine translation: A survey and a case study.
Dis-cours, 11.Basil Hatim and Ian Mason.
1990.
Discourse and theTranslator.
Longman, London, UK.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?1362, Edinburgh, Scotland.Philipp Koehn and Barry Haddow.
2012.
Towardseffective use of training data in statistical machinetranslation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 317?321,Montre?al, Canada.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Human Language TechnologyConference of the NAACL, pages 48?54, Edmonton,Alberta, Canada.68Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACL,Demo and Poster Sessions, pages 177?180, Prague,Czech Republic.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof MT Summit X, pages 79?86, Phuket, Thailand.Ronan Le Nagard and Philipp Koehn.
2010.
Aidingpronoun translation with co-reference resolution.
InProceedings of the Joint Fifth Workshop on Statis-tical Machine Translation and MetricsMATR, pages252?261, Uppsala, Sweden.Thomas Meyer and Andrei Popescu-Belis.
2012.
Us-ing sense-labeled discourse connectives for statisti-cal machine translation.
In Proceedings of the JointWorkshop on Exploiting Synergies between Informa-tion Retrieval and Machine Translation (ESIRMT)and Hybrid Approaches to Machine Translation(HyTra), pages 129?138, Avignon, France.Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,and Andrea Gesmundo.
2012.
Machine translationof labeled discourse connectives.
In Proceedings ofthe 10th Biennial Conference of the Association forMachine Translation in the Americas, San Diego,California, USA.Franz Josef Och, Nicola Ueffing, and Hermann Ney.2001.
An efficient A* search algorithm for Statisti-cal Machine Translation.
In Proceedings of the ACL2001 Workshop on Data-Driven Machine Transla-tion, pages 55?62, Toulouse, France.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 42nd Annual Meeting of the ACL, pages 160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting of the ACL, pages 311?318, Philadelphia, Pennsylvania, USA.Lucia Specia, Baskaran Sankaran, and Maria dasGrac?as Volpe Nunes.
2008.
N-best reranking forthe efficient integration of word sense disambigua-tion and statistical machine translation.
In Proceed-ings of the 9th International Conference on Intelli-gent Text Processing and Computational Linguistics(CICLING), pages 399?410, Haifa, Israel.Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,and Joakim Nivre.
2013.
Statistical machine trans-lation with readability constraints.
In Proceedingsof the 19th Nordic Conference on ComputationalLinguistics (NODALIDA?13), pages 375?386, Oslo,Norway.Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,Xiaodong Shi, Huailin Dong, and Qun Liu.
2012.Translation model adaptation for statistical machinetranslation with monolingual topic information.
InProceedings of the 50th Annual Meeting of the ACL,pages 459?468, Jeju Island, Korea.Jo?rg Tiedemann.
2010.
Context adaptation in statisti-cal machine translation using models with exponen-tially decaying cache.
In Proceedings of the ACL2010 Workshop on Domain Adaptation for NaturalLanguage Processing (DANLP), pages 8?15, Upp-sala, Sweden.Marco Turchi, Tijl De Bie, Cyril Goutte, and NelloCristianini.
2012.
Learning to translate: A statis-tical and computational analysis.
Advances in Arti-ficial Intelligence, 2012.
Article ID 484580.Ferhan Ture, Douglas W. Oard, and Philip Resnik.2012.
Encouraging consistent translation choices.In Proceedings of the 2012 Conference of theNAACL: Human Language Technologies, pages417?426, Montre?al, Canada.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.2011.
Document-level consistency verification inmachine translation.
In Proceedings of MT SummitXIII, pages 131?138, Xiamen, China.Bing Zhao and Eric P. Xing.
2010.
HM-BiTAM: Bilin-gual topic exploration, word alignment,and transla-tion.
In Advances in Neural Information ProcessingSystems 20 (NIPS), pages 1689?1696, Cambridge,Massachusetts, USA.69
