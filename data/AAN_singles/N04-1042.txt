Accurate Information Extraction from Research Papersusing Conditional Random FieldsFuchun PengDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003fuchun@cs.umass.eduAndrew McCallumDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003mccallum@cs.umass.eduAbstractWith the increasing use of research papersearch engines, such as CiteSeer, for both lit-erature search and hiring decisions, the accu-racy of such systems is of paramount impor-tance.
This paper employs Conditional Ran-dom Fields (CRFs) for the task of extractingvarious common fields from the headers andcitation of research papers.
The basic the-ory of CRFs is becoming well-understood, butbest-practices for applying them to real-worlddata requires additional exploration.
This papermakes an empirical exploration of several fac-tors, including variations on Gaussian, expo-nential and hyperbolic-L1 priors for improvedregularization, and several classes of featuresand Markov order.
On a standard benchmarkdata set, we achieve new state-of-the-art perfor-mance, reducing error in average F1 by 36%,and word error rate by 78% in comparison withthe previous best SVM results.
Accuracy com-pares even more favorably against HMMs.1 IntroductionResearch paper search engines, such as CiteSeer(Lawrence et al, 1999) and Cora (McCallum et al,2000), give researchers tremendous power and conve-nience in their research.
They are also becoming increas-ingly used for recruiting and hiring decisions.
Thus theinformation quality of such systems is of significant im-portance.
This quality critically depends on an informa-tion extraction component that extracts meta-data, suchas title, author, institution, etc, from paper headers andreferences, because these meta-data are further used inmany component applications such as field-based search,author analysis, and citation analysis.Previous work in information extraction from researchpapers has been based on two major machine learn-ing techniques.
The first is hidden Markov models(HMM) (Seymore et al, 1999; Takasu, 2003).
AnHMM learns a generative model over input sequenceand labeled sequence pairs.
While enjoying wide his-torical success, standard HMM models have difficultymodeling multiple non-independent features of the ob-servation sequence.
The second technique is basedon discriminatively-trained SVM classifiers (Han et al,2003).
These SVM classifiers can handle many non-independent features.
However, for this sequence label-ing problem, Han et al (2003) work in a two stages pro-cess: first classifying each line independently to assign itlabel, then adjusting these labels based on an additionalclassifier that examines larger windows of labels.
Solvingthe information extraction problem in two steps loosesthe tight interaction between state transitions and obser-vations.In this paper, we present results on this research papermeta-data extraction task using a Conditional RandomField (Lafferty et al, 2001), and explore several practi-cal issues in applying CRFs to information extraction ingeneral.
The CRF approach draws together the advan-tages of both finite state HMM and discriminative SVMtechniques by allowing use of arbitrary, dependent fea-tures and joint inference over entire sequences.CRFs have been previously applied to other tasks suchas name entity extraction (McCallum and Li, 2003), tableextraction (Pinto et al, 2003) and shallow parsing (Shaand Pereira, 2003).
The basic theory of CRFs is nowwell-understood, but the best-practices for applying themto new, real-world data is still in an early-explorationphase.
Here we explore two key practical issues: (1) reg-ularization, with an empirical study of Gaussian (Chenand Rosenfeld, 2000), exponential (Goodman, 2003), andhyperbolic-L1 (Pinto et al, 2003) priors; (2) explorationof various families of features, including text, lexicons,and layout, as well as proposing a method for the bene-ficial use of zero-count features without incurring largememory penalties.We describe a large collection of experimental resultson two traditional benchmark data sets.
Dramatic im-provements are obtained in comparison with previousSVM and HMM based results.2 Conditional Random FieldsConditional random fields (CRFs) are undirected graph-ical models trained to maximize a conditional probabil-ity (Lafferty et al, 2001).
A common special-case graphstructure is a linear chain, which corresponds to a finitestate machine, and is suitable for sequence labeling.
Alinear-chain CRF with parameters ?
= {?, ...} definesa conditional probability for a state (or label1) sequencey = y1...yT given an input sequence x = x1...xT to beP?
(y|x) =1Zxexp( T?t=1?k?kfk(yt?1, yt,x, t)),(1)where Zx is the normalization constant that makesthe probability of all state sequences sum to one,fk(yt?1, yt,x, t) is a feature function which is oftenbinary-valued, but can be real-valued, and ?k is a learnedweight associated with feature fk.
The feature functionscan measure any aspect of a state transition, yt?1 ?
yt,and the observation sequence, x, centered at the currenttime step, t. For example, one feature function mighthave value 1 when yt?1 is the state TITLE, yt is the stateAUTHOR, and xt is a word appearing in a lexicon of peo-ple?s first names.
Large positive values for ?k indicate apreference for such an event, while large negative valuesmake the event unlikely.Given such a model as defined in Equ.
(1), the mostprobable labeling sequence for an input x,y?
= argmaxyP?
(y|x),can be efficiently calculated by dynamic programmingusing the Viterbi algorithm.
Calculating the marginalprobability of states or transitions at each position inthe sequence by a dynamic-programming-based infer-ence procedure very similar to forward-backward for hid-den Markov models.The parameters may be estimated by maximumlikelihood?maximizing the conditional probability ofa set of label sequences, each given their correspond-ing input sequences.
The log-likelihood of training set1We consider here only finite state models in which there isa one-to-one correspondence between states and labels; this isnot, however, strictly necessary.
?6 ?5 ?4 ?3 ?2 ?1 0 1 2 3024681012lambdacountsof lamda(inlog scale)Figure 1: Empirical distribution of ?
{(xi, yi) : i = 1, ...M} is writtenL?
=?ilogP?
(yi|xi)=?i( T?t=1?k?kfk(yt?1, yt,x, t) ?
logZxi).
(2)Maximizing (2) corresponds to satisfying the follow-ing equality, wherein the the empirical count of each fea-ture matches its expected count according to the modelP?(y|x).
?ifk(yt?1, yt, xi, t) =?iP?
(y|x)fk(yt?1, yt, xi, t)CRFs share many of the advantageous properties ofstandard maximum entropy models, including their con-vex likelihood function, which guarantees that the learn-ing procedure converges to the global maximum.
Tra-ditional maximum entropy learning algorithms, such asGIS and IIS (Pietra et al, 1995), can be used to trainCRFs, however, it has been found that a quasi-Newtongradient-climber, BFGS, converges much faster (Malouf,2002; Sha and Pereira, 2003).
We use BFGS for opti-mization.
In our experiments, we shall focus instead ontwo other aspects of CRF deployment, namely regulariza-tion and selection of different model structure and featuretypes.2.1 Regularization in CRFsTo avoid over-fitting, log-likelihood is often penalized bysome prior distribution over the parameters.
Figure 1shows an empirical distribution of parameters, ?, learnedfrom an unpenalized likelihood, including only featureswith non-zero count in the training set.
Three prior dis-tributions that have shape similar to this empirical dis-tribution are the Gaussian prior, exponential prior, andhyperbolic-L1 prior, each shown in Figure 2.
In this pa-per we provide an empirical study of these three priors.
?10 ?8 ?6 ?4 ?2 0 2 4 6 8 1000.050.10.150.20.250.30.350.4Gaussian varianec=2Exponential a=0.5HyperbolicFigure 2: Shapes of prior distributions2.1.1 Gaussian priorWith a Gaussian prior, log-likelihood (2) is penalizedas follows:L?
=?ilogP?
(yi|xi) ?
?k?2k2?2k, (3)where ?2k is a variance.Maximizing (3) corresponds to satisfying?ifk(yt?1, yt, xi, t)??k?2k=?iP?
(y|x)fk(yt?1, yt, xi, t)This adjusted constraint (as well as the adjustments im-posed by the other two priors) is intuitively understand-able: rather than matching exact empirical feature fre-quencies, the model is tuned to match discounted featurefrequencies.
Chen and Rosenfeld (2000) discuss this inthe context of other discounting procedures common inlanguage modeling.
We call the term subtracted from theempirical counts (in this case ?k/?2) a discounted value.The variance can be feature dependent.
However forsimplicity, constant variance is often used for all features.In this paper, however, we experiment with several alter-nate versions of Gaussian prior in which the variance isfeature dependent.Although Gaussian (and other) priors are graduallyovercome by increasing amounts of training data, per-haps not at the right rate.
The three methods below allprovide ways to alter this rate by changing the varianceof the Gaussian prior dependent on feature counts.1.
Threshold Cut: In language modeling, e.g, Good-Turing smoothing, only low frequency words aresmoothed.
Here we apply the same idea and onlysmooth those features whose frequencies are lowerthan a threshold (7 in our experiments, followingstandard practice in language modeling).2.
Divide Count: Here we let the discounted valuefor a feature depend on its frequency in the trainingset, ck =?i?t fk(yt?1, yt,x, t).
The discountedvalue used here is ?kck?
?2 where ?
is a constant overall features.
In this way, we increase the smoothingon the low frequency features more so than the highfrequency features.3.
Bin-Based: We divide features into classes basedon frequency.
We bin features by frequency in thetraining set, and let the features in the same bin sharethe same variance.
The discounted value is set to be?kdck/Ne?
?2 where ck is the count of features, N isthe bin size, and dae is the ceiling function.
Alterna-tively, the variance in each bin may be set indepen-dently by cross-validation.2.1.2 Exponential priorWhereas the Gaussian prior penalizes according to thesquare of the weights (an L2 penalizer), the intention hereis to create a smoothly differentiable analogue to penal-izing the absolute-value of the weights (an L1 penalizer).L1 penalizers often result in more ?sparse solutions,?
inwhich many features have weight nearly at zero, and thusprovide a kind of soft feature selection that improves gen-eralization.Goodman (2003) proposes an exponential prior,specifically a Laplacian prior, as an alternative to Gaus-sian prior.
Under this prior,L?
=?ilogP?(yi|xi)?
?k?k|?k| (4)where ?k is a parameter in exponential distribution.Maximizing (4) would satisfy?ifk(yt?1, yt, xi, t)?
?k =?iP?
(y|x)fk(yt?1, yt, xi, t)This corresponds to the absolute smoothing method inlanguage modeling.
We set the ?k = ?
; i.e.
all featuresshare the same constant whose value can be determinedusing absolute discounting?
= n1n1+2n2 , where n1 and n2are the number of features occurring once and twice (Neyet al, 1995).2.1.3 Hyperbolic-L1 priorAnother L1 penalizer is the hyperbolic-L1 prior, de-scribed in (Pinto et al, 2003).
The hyperbolic distributionhas log-linear tails.
Consequently the class of hyperbolicdistribution is an important alternative to the class of nor-mal distributions and has been used for analyzing datafrom various scientific areas such as finance, though lessfrequently used in natural language processing.Under a hyperbolic prior,L?
=XilogP?
(yi|xi) ?Xklog(e?k + e?
?k2 ) (5)which corresponds to satisfyingXifk(yt?1, yt, xi, t) ?e|?k| ?
e?|?k|e|?k| + e?|?k| =XiP?
(y|x)fi(yt?1, yt, xi, t)The hyperbolic prior was also tested with CRFs in Mc-Callum and Li (2003).2.2 Exploration of Feature SpaceWise choice of features is always vital the performanceof any machine learning solution.
Feature induction (Mc-Callum, 2003) has been shown to provide significant im-provements in CRFs performance.
In some experimentsdescribed below we use feature induction.
The focus inthis section is on three other aspects of the feature space.2.2.1 State transition featuresIn CRFs, state transitions are also represented as fea-tures.
The feature function fk(yt?1, yt,x, t) in Equ.
(1)is a general function over states and observations.
Differ-ent state transition features can be defined to form dif-ferent Markov-order structures.
We define four differ-ent state transitions features corresponding to differentMarkov order for different classes of features.
Higherorder features model dependencies better, but also createmore data sparse problem and require more memory intraining.1.
First-order: Here the inputs are examined in the con-text of the current state only.
The feature functionsare represented as f(yt,x).
There are no separateparameters or preferences for state transitions at all.2.
First-order+transitions: Here we add parameterscorresponding to state transitions.
The feature func-tions used are f(yt,x), f(yt?1, yt).3.
Second-order: Here inputs are examined in the con-text of the current and previous states.
Feature func-tion are represented as f(yt?1, yt,x).4.
Third-order: Here inputs are examined in the con-text of the current, two previous states.
Feature func-tion are represented as f(yt?2, yt?1, yt,x).2.2.2 Supported features and unsupported featuresBefore the use of prior distributions over parameterswas common in maximum entropy classifiers, standardpractice was to eliminate all features with zero countin the training data (the so-called unsupported features).However, unsupported, zero-count features can be ex-tremely useful for pushing Viterbi inference away fromcertain paths by assigning such features negative weight.The use of a prior allows the incorporation of unsup-ported features, however, doing so often greatly increasesthe number parameters and thus the memory require-ments.Below we experiment with models containing and notcontaining unsupported features?both with and withoutregularization by priors, and we argue that non-supportedfeatures are useful.We present here incremental support, a method of in-troducing some useful unsupported features without ex-ploding the number of parameters with all unsupportedfeatures.
The model is trained for several iterations withsupported features only.
Then inference determines thelabel sequences assigned high probability by the model.Incorrect transitions assigned high probability by themodel are used to selectively add to the model those un-supported features that occur on those transitions, whichmay help improve performance by being assigned nega-tive weight in future training.
If desired, several iterationsof this procedure may be performed.2.2.3 Local features, layout features and lexiconfeaturesOne of the advantages of CRFs and maximum entropymodels in general is that they easily afford the use of arbi-trary features of the input.
One can encode local spellingfeatures, layout features such as positions of line breaks,as well as external lexicon features, all in one framework.We study all these features in our research paper extrac-tion problem, evaluate their individual contributions, andgive some guidelines for selecting good features.3 Empirical Study3.1 Hidden Markov ModelsHere we also briefly describe a HMM model we usedin our experiments.
We relax the independence assump-tion made in standard HMM and allow Markov depen-dencies among observations, e.g., P (ot|st, ot?1).
Wecan vary Markov orders in state transition and observa-tion transitions.
In our experiments, a model with secondorder state transitions and first order observation transi-tions performs the best.
The state transition probabilitiesand emission probabilities are estimated using maximumlikelihood estimation with absolute smoothing, whichwas found to be effective in previous experiments, includ-ing Seymore et al (1999).3.2 DatasetsWe experiment with two datasets of research paper con-tent.
One consists of the headers of research papers.
Theother consists of pre-segmented citations from the refer-ence sections of research papers.
These data sets havebeen used as standard benchmarks in several previousstudies (Seymore et al, 1999; McCallum et al, 2000;Han et al, 2003).3.2.1 Paper header datasetThe header of a research paper is defined to be all ofthe words from the beginning of the paper up to eitherthe first section of the paper, usually the introduction,or to the end of the first page, whichever occurs first.It contains 15 fields to be extracted: title, author, affil-iation, address, note, email, date, abstract, introduction,phone, keywords, web, degree, publication number, andpage (Seymore et al, 1999).
The header dataset contains935 headers.
Following previous research (Seymore etal., 1999; McCallum et al, 2000; Han et al, 2003), foreach trial we randomly select 500 for training and the re-maining 435 for testing.
We refer this dataset as H.3.2.2 Paper reference datasetThe reference dataset was created by the Coraproject (McCallum et al, 2000).
It contains 500 refer-ences, we use 350 for training and the rest 150 for test-ing.
References contain 13 fields: author, title, editor,booktitle, date, journal, volume, tech, institution, pages,location, publisher, note.
We refer this dataset as R.3.3 Performance MeasuresTo give a comprehensive evaluation, we measure per-formance using several different metrics.
In addition tothe previously-used word accuracy measure (which over-emphasizes accuracy of the abstract field), we use per-field F1 measure (both for individual fields and averagedover all fields?called a ?macro average?
in the informa-tion retrieval literature), and whole instance accuracy formeasuring overall performance in a way that is sensitiveto even a single error in any part of header or citation.3.3.1 Measuring field-specific performance1.
Word Accuracy: We define A as the number of truepositive words, B as the number of false negativewords, C as the number of false positive words, Das the number of true negative words, and A+ B +C +D is the total number of words.
Word accuracyis calculated to be A+DA+B+C+D2.
F1-measure: Precision, recall and F1 measure aredefined as follows.
Precision = AA+C Recall =AA+BF1 = 2?Precision?RecallPrecision+Recall3.3.2 Measuring overall performance1.
Overall word accuracy: Overall word accuracyis the percentage of words whose predicted labelsequal their true labels.
Word accuracy favors fieldswith large number of words, such as the abstract.2.
Averaged F-measure: Averaged F-measure is com-puted by averaging the F1-measures over all fields.Average F-measure favors labels with small num-ber of words, which complements word accuracy.Thus, we consider both word accuracy and averageF-measure in evaluation.3.
Whole instance accuracy: An instance here is de-fined to be a single header or reference.
Wholeinstance accuracy is the percentage of instances inwhich every word is correctly labeled.3.4 Experimental ResultsWe first report the overall results by comparing CRFswith HMMs, and with the previously best benchmark re-sults obtained by SVMs (Han et al, 2003).
We then breakdown the results to analyze various factors individually.Table 1 shows the results on dataset H with the best re-sults in bold; (intro and page fields are not shown, fol-lowing past practice (Seymore et al, 1999; Han et al,2003)).
The results we obtained with CRFs use second-order state transition features, layout features, as well assupported and unsupported features.
Feature inductionis used in experiments on dataset R; (it didn?t improveaccuracy on H).
The results we obtained with the HMMmodel use a second order model for transitions, and a firstorder for observations.
The results on SVM is obtainedfrom (Han et al, 2003) by computing F1 measures fromthe precision and recall numbers they report.HMM CRF SVMOverall acc.
93.1% 98.3% 92.9%Instance acc.
4.13% 73.3% -acc.
F1 acc.
F1 acc.
F1Title 98.2 82.2 99.7 97.1 98.9 96.5Author 98.7 81.0 99.8 97.5 99.3 97.2Affiliation 98.3 85.1 99.7 97.0 98.1 93.8Address 99.1 84.8 99.7 95.8 99.1 94.7Note 97.8 81.4 98.8 91.2 95.5 81.6Email 99.9 92.5 99.9 95.3 99.6 91.7Date 99.8 80.6 99.9 95.0 99.7 90.2Abstract 97.1 98.0 99.6 99.7 97.5 93.8Phone 99.8 53.8 99.9 97.9 99.9 92.4Keyword 98.7 40.6 99.7 88.8 99.2 88.5Web 99.9 68.6 99.9 94.1 99.9 92.4Degree 99.5 68.8 99.8 84.9 99.5 70.1Pubnum 99.8 64.2 99.9 86.6 99.9 89.2Average F1 75.6 93.9 89.7Table 1: Extraction results for paper headers on HTable 2 shows the results on dataset R. SVM resultsare not available for these datasets.3.5 Analysis3.5.1 Overall performance comparisonFrom Table (1, 2), one can see that CRF performssignificantly better than HMMs, which again supportsthe previous findings (Lafferty et al, 2001; Pinto et al,HMM CRFOverall acc.
85.1% 95.37%instance acc.
10% 77.33%acc.
F1 acc.
F1Author 96.8 92.7 99.9 99.4Booktitle 94.4 0.85 97.7 93.7Date 99.7 96.9 99.8 98.9Editor 98.8 70.8 99.5 87.7Institution 98.5 72.3 99.7 94.0Journal 96.6 67.7 99.1 91.3Location 99.1 81.8 99.3 87.2Note 99.2 50.9 99.7 80.8Pages 98.1 72.9 99.9 98.6Publisher 99.4 79.2 99.4 76.1Tech 98.8 74.9 99.4 86.7Title 92.2 87.2 98.9 98.3Volume 98.6 75.8 99.9 97.8Average F1 77.6% 91.5%Table 2: Extraction results for paper references on R2003).
CRFs also perform significantly better than SVM-based approach, yielding new state of the art performanceon this task.
CRFs increase the performance on nearly allthe fields.
The overall word accuracy is improved from92.9% to 98.3%, which corresponds to a 78% error ratereduction.
However, as we can see word accuracy can bemisleading since HMM model even has a higher word ac-curacy than SVM, although it performs much worse thanSVM in most individual fields except abstract.
Interest-ingly, HMM performs much better on abstract field (98%versus 93.8% F-measure) which pushes the overall accu-racy up.
A better comparison can be made by compar-ing the field-based F-measures.
Here, in comparison tothe SVM, CRFs improve the F1 measure from 89.7% to93.9%, an error reduction of 36%.3.5.2 Effects of regularizationThe results of different regularization methods aresummarized in Table (3).
Setting Gaussian variance offeatures depending on feature count performs better, from90.5% to 91.2%, an error reduction of 7%, when onlyusing supported features, and an error reduction of 9%when using supported and unsupported features.
Re-sults are averaged over 5 random runs, with an aver-age variance of 0.2%.
In our experiments we found theGaussian prior to consistently perform better than theothers.
Surprisingly, exponential prior hurts the perfor-mance significantly.
It over penalizes the likelihood (sig-nificantly increasing cost?defined as negative penalizedlog-likelihood).
We hypothesized that the problem couldbe that the choice of constant ?
is inappropriate.
So wetried varying ?
instead of computing it using absolutediscounting, but found the alternatives to perform worse.These results suggest that Gaussian prior is a safer priorsupport feat.
all featuresMethod F1 F1Gaussian infinity 90.5 93.3Gaussian variance = 0.1 81.7 91.8Gaussian variance = 0.5 87.2 93.0Gaussian variance = 5 90.1 93.7Gaussian variance = 10 89.9 93.5Gaussian cut 7 90.1 93.4Gaussian divide count 90.9 92.8Gaussian bin 5 90.9 93.6Gaussian bin 10 90.2 92.9Gaussian bin 15 91.2 93.9Gaussian bin 20 90.4 93.2Hyperbolic 89.4 92.8Exponential 80.5 85.6Table 3: Regularization comparisons: Gaussian infinity isnon-regularized, Gaussian variance = X sets variance tobe X. Gaussian cut 7 refers to the Threshold Cut method,Gaussian divide count refers to the Divide Count method,Gaussian bin N refers to the Bin-Based method with binsize equals N, as described in 2.1.1to use in practice.3.5.3 Effects of exploring feature spaceState transition features and unsupported features.We summarize the comparison of different state tran-sition models using or not using unsupported features inTable 4.
The first column describes the four different statetransition models, the second column contains the overallword accuracy of these models using only support fea-tures, and the third column contains the result of usingall features, including unsupported features.
Comparingthe rows, one can see that the second-order model per-forms the best, but not dramatically better than the first-order+transitions and the third order model.
However, thefirst-order model performs significantly worse.
The dif-ference does not come from sharing the weights, but fromignoring the f(yt?1, yt).
The first order transition featureis vital here.
We would expect the third order model toperform better if enough training data were available.Comparing the second and the third columns, we cansee that using all features including unsupported features,consistently performs better than ignoring them.
Ourpreliminary experiments with incremental support haveshown performance in between that of supported-onlyand all features, and are still ongoing.Effects of layout featuresTo analyze the contribution of different kinds of fea-tures, we divide the features into three categories: localfeatures, layout features, and external lexicon resources.The features we used are summarized in Table 5.support allfirst-order 89.0 90.4first-order+trans 95.6 -second-order 96.0 96.5third-order 95.3 96.1Table 4: Effects of using unsupported features and statetransitions on HFeature name DescriptionLocal featuresINITCAP Starts with a capitalized letterALLCAPS All characters are capitalizedCONTAINSDIGITS Contains at least one digitALLDIGITS All characters are digitsPHONEORZIP Phone number or zip codeCONTAINSDOTS Contains at least one dotCONTAINSDASH Contains at least one -ACRO AcronymLONELYINITIAL Initials such as A.SINGLECHAR One character onlyCAPLETTER One capitalized characterPUNC PunctuationURL Regular expression for URLEMAIL Regular expression for e-addressWORD Word itselfLayout featuresLINE START At the beginning of a lineLINE IN In middle of a lineLINE END At the end of a lineExternal lexicon featuresBIBTEX AUTHOR Match word in author lexiconBIBTEX DATE Words like Jan. Feb.NOTES Words like appeared, submittedAFFILIATION Words like institution, Labs, etcTable 5: List of features usedThe results of using different features are shown in Ta-ble 6.
The layout feature dramatically increases the per-formance, raising the F1 measure from 88.8% to 93.9%,whole sentence accuracy from 40.1% to 72.4%.
Addinglexicon features alone improves the performance.
How-ever, when combing lexicon features and layout fea-tures, the performance is worse than using layout featuresalone.The lexicons were gathered from a large collection ofBibTeX files, and upon examination had difficult to re-move noise, for example words in the author lexicon thatwere also affiliations.
In previous work, we have gainedsignificant benefits by dividing each lexicon into sectionsbased on point-wise information gain with respect to thelexicon?s class.3.5.4 Error analysisTable 7 is the classification confusion matrix of headerextraction (field page is not shown to save space).
MostWord Acc.
F1 Inst.
Acc.local feature 96.5% 88.8% 40.1%+ lexicon 96.9% 89.9% 53.1%+ layout feature 98.2% 93.4% 72.4%+ layout + lexicon 98.0% 93.0% 71.7%Table 6: Results of using different features on Herrors happen at the boundaries between two fields.
Es-pecially the transition from author to affiliation, from ab-stract to keyword.
The note field is the one most con-fused with others, and upon inspection is actually labeledinconsistently in the training data.
Other errors couldbe fixed with additional feature engineering?for exam-ple, including additional specialized regular expressionsshould make email accuracy nearly perfect.
Increasingthe amount of training data would also be expected tohelp significantly, as indicated by consistent nearly per-fect accuracy on the training set.4 Conclusions and Future WorkThis paper investigates the issues of regularization, fea-ture spaces, and efficient use of unsupported features inCRFs, with an application to information extraction fromresearch papers.For regularization we find that the Gaussian prior withvariance depending on feature frequencies performs bet-ter than several other alternatives in the literature.
Featureengineering is a key component of any machine learn-ing solution?especially in conditionally-trained mod-els with such freedom to choose arbitrary features?andplays an even more important role than regularization.We obtain new state-of-the-art performance in extract-ing standard fields from research papers, with a signifi-cant error reduction by several metrics.
We also suggestbetter evaluation metrics to facilitate future research inthis task?especially field-F1, rather than word accuracy.We have provided an empirical exploration of a fewpreviously-published priors for conditionally-trained log-linear models.
Fundamental advances in regularizationfor CRFs remains a significant open research area.5 AcknowledgmentsThis work was supported in part by the Cen-ter for Intelligent Information Retrieval, in part bySPAWARSYSCEN-SD grant number N66001-02-1-8903, in part by the National Science Foundation Co-operative Agreement number ATM-9732665 through asubcontract from the University Corporation for Atmo-spheric Research (UCAR) and in part by The Cen-tral Intelligence Agency, the National Security Agencyand National Science Foundation under NSF grant #IIS-0326249.
Any opinions, findings and conclusions or rec-title auth.
pubnum date abs.
aff.
addr.
email deg.
note ph.
intro k.w.
webtitle 3446 0 6 0 22 0 0 0 9 25 0 0 12 0author 0 2653 0 0 7 13 5 0 14 41 0 0 12 0pubnum 0 14 278 2 0 2 7 0 0 39 0 0 0 0date 0 0 3 336 0 1 3 0 0 18 0 0 0 0abstract 0 0 0 0 53262 0 0 1 0 0 0 0 0 0affil.
19 13 0 0 10 3852 27 0 28 34 0 0 0 1address 0 11 3 0 0 35 2170 1 0 21 0 0 0 0email 0 0 1 0 12 2 3 461 0 2 2 0 15 0degree 2 2 0 2 0 2 0 5 465 95 0 0 2 0note 52 2 9 6 219 52 59 0 5 4520 4 3 21 3phone 0 0 0 0 0 0 0 1 0 2 215 0 0 0intro 0 0 0 0 0 0 0 0 0 32 0 625 0 0keyword 57 0 0 0 18 3 15 0 0 91 0 0 975 0web 0 0 0 0 2 0 0 0 0 31 0 0 0 294Table 7: Confusion matrix on Hommendations expressed in this material are the author(s)and do not necessarily reflect those of the sponsor.ReferencesS.
Chen and R. Rosenfeld.
2000.
A Survey of SmoothingTechniques for ME Models.
IEEE Trans.
Speech andAudio Processing, 8(1), pp.
37?50.
January 2000.J.
Goodman.
2003.
Exponential Priors for MaximumEntropy Models.
MSR Technical report, 2003.H.
Han, C. Giles, E. Manavoglu, H. Zha, Z. Zhang, and E.Fox.
2003.
Automatic Document Meta-data Extrac-tion using Support Vector Machines.
In Proceedingsof Joint Conference on Digital Libraries 2003.J.
Lafferty, A. McCallum and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proceed-ings of International Conference on Machine Learning2001.S.
Lawrence, C. L. Giles, and K. Bollacker.
1999.
DigitalLibraries and Autonomous Citation Indexing.
IEEEComputer, 32(6): 67-71.R.
Malouf.
2002.
A Comparison of Algorithms for Max-imum Entropy Parameter Estimation.
In Proceedingsof the Sixth Conference on Natural Language Learning(CoNLL)A. McCallum.
2003.
Efficiently Inducing Featuresof Conditional Random Fields.
In Proceedings ofConference on Uncertainty in Articifical Intelligence(UAI).A.
McCallum, K. Nigam, J. Rennie, K. Seymore.
2000.Automating the Construction of Internet Portals withMachine Learning.
Information Retrieval Journal,volume 3, pages 127-163.
Kluwer.
2000.A.
McCallum and W. Li.
2003.
Early Results for NamedEntity Recognition with Conditional Random Fields,Feature Induction and Web-Enhanced Lexicons.
InProceedings of Seventh Conference on Natural Lan-guage Learning (CoNLL).H.
Ney, U. Essen, and R. Kneser 1995.
On the Estima-tion of Small Probabilities by Leaving-One-Out.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 17(12):1202-1212, 1995.S.
Pietra, V. Pietra, J. Lafferty 1995.
Inducing Fea-tures Of Random Fields.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, Vol.
19, No.4.D.
Pinto, A. McCallum, X. Wei and W. Croft.
2003.
Ta-ble Extraction Using Conditional Random Fields.
InProceedins of the 26th Annual International ACM SI-GIR Conference on Research and Development in In-formation Retrieval (SIGIR?03)K. Seymore, A. McCallum, R. Rosenfeld.
1999.
Learn-ing Hidden Markov Model Structure for InformationExtraction.
In Proceedings of AAAI?99 Workshop onMachine Learning for Information Extraction.F.
Sha and F. Pereira.
2003.
Shallow Parsing with Con-ditional Random Fields.
In Proceedings of HumanLanguage Technology Conference and North Ameri-can Chapter of the Association for Computational Lin-guistics (HLT-NAACL?03)A. Takasu.
2003.
Bibliographic Attribute Extrac-tion from Erroneous References Based on a StatisticalModel.
In Proceedings of Joint Conference on DigitalLibraries 2003.
