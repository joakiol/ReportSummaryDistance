R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
827 ?
837, 2005.?
Springer-Verlag Berlin Heidelberg 2005Principles of Non-stationary Hidden Markov Modeland Its Applications to Sequence Labeling TaskXiao JingHui, Liu BingQuan, and Wang XiaoLongSchool of Computer Science and Techniques,Harbin Institute of Technology, Harbin, 150001, China{xiaojinghui, liubq, wangxl}@insun.hit.edu.cnAbstract.
Hidden Markov Model (Hmm) is one of the most popular languagemodels.
To improve its predictive power, one of Hmm hypotheses, namedlimited history hypothesis, is usually relaxed.
Then Higher-order Hmm is builtup.
But there are several severe problems hampering the applications of high-order Hmm, such as the problem of parameter space explosion, data sparsenessproblem and system resource exhaustion problem.
From another point of view,this paper relaxes the other Hmm hypothesis, named stationary (time invariant)hypothesis, makes use of time information and proposes a non-stationary Hmm(NSHmm).
This paper describes NSHmm in detail, including its definition, therepresentation of time information, the algorithms and the parameter space andso on.
Moreover, to further reduce the parameter space for mobile applications,this paper proposes a variant form of NSHmm (VNSHmm).
Then NSHmm andVNSHmm are applied to two sequence labeling tasks: pos tagging and pinyin-to-character conversion.
Experiment results show that compared with Hmm,NSHmm and VNSHmm can greatly reduce the error rate in both of the twotasks, which proves that they have much more predictive power than Hmm does.1   IntroductionStatistical language model plays an important role in natural language processing andgreat efforts are devoted to the research of language modeling.
Hidden Markov Model(Hmm) is one of the most popular language models.
It was first proposed by IBM inspeech recognition [1] and achieved great success.
Then Hmm has a wide range ofapplications in many domains, such as OCR [2], handwriting recognition [3], machinetranslation [4], Chinese pinyin-to-character conversion [5] and so on.To improve Hmm?s predictive power, one of Hmm hypotheses [6] named limitedhistory hypothesis, is usually relaxed and higher-order Hmm is proposed.
But as theorder of Hmm increases, its parameter space explodes at an exponential rate, whichmay result in several severe problems, such as data sparseness problem [7], systemresource exhaustion problem and so on.
From another point of view, this paperrelaxes the other Hmm hypothesis, named stationary hypothesis, makes use of timeinformation and proposes non-stationary Hmm (NSHmm).
This paper first definesNSHmm in a formalized form, and then discusses how to represent time informationin NSHmm.
After that, the algorithms of NSHmm are provided and the parameterspace complexity is calculated.
Moreover, to further reduce the parameter space, a828 J. Xiao, B. Liu, and X. Wangvariant form of NSHmm (VNSHmm) is proposed later.
At last, NSHmm andVNSHmm are applied to two sequence labeling tasks: pos tagging and pinyin-to-character conversion.
As the experiment results show, compared with Hmm, NSHmmand VNSHmm can greatly reduce the error rate in the both two tasks.The rest of this paper is structured as follows: in section 2 we briefly review thedefinition of standard Hmm.
In section 3, NSHmm is proposed and the relativequestions are discussed in detail.
Experiments and results are discussed in section 4.Finally, we give our conclusions in section 5 and plan the further work in section 6.2   Hidden Markov ModelHmm is a function of Markov process and can be mathematically defined as a five-tuple M = <?, ?, ?, ?, ?> which consists of:1.
A finite set of (hidden) states ?.2.
A finite set of (observed) symbols ?.3.
A state transition function ?
: ??
?-> [0, 1].4.
A symbol emission function ?
: ??
?-> [0, 1].5.
And an initial state probability function ?
: Omega -> [0, 1].The functions of ?, ?
and ?
are usually estimated by MLE principle on large scalecorpus.
Based on the above definition, Hmm makes two hypotheses at the same time:1.
Limited history hypothesis: the current state is completely decided by the laststate before, but irrelative to the entire state history.2.
Stationary hypothesis: the state transition function ?
is completely determinedby states, but irrelative to the time when state transition occurs.
So it is with thesymbol emission function.There are three fundamental questions and a series of corresponding algorithms forHmm:1.
Given Hmm, how to calculate the probability of a sequence observation?Forward algorithm and backward algorithm can handle that question.2.
Given Hmm and an observation sequence, how to find the best state sequence toexplain the observation?
Viterbi algorithm can fulfill that task.3.
Given an observation sequence, how to estimate the parameters of Hmm to bestexplain the observed data?
Baum-Welch algorithm can solve that problem.Hmm is a popular language model and has been applied to many tasks in naturallanguage processing.
For example, in pos tagging, the word sequence is taken as theobservation of Hmm, and the pos sequence as the hidden state chain.
Viterbialgorithm can find the best pos sequence corresponding to the word sequence.3   Non-stationary Hidden Markov Model3.1   MotivationThere are many approaches to improve the predictive power of Hmm in practice.
Forexample, factorial Hmm [8] is proposed by decomposing the hidden statePrinciples of Non-stationary Hidden Markov Model and Its Applications 829representation into multiple independent Markov chains.
In speech recognition, afactorial Hmm can represent the combination of multiple signals which are producedindependently and the characteristics of each signal are described by a distinctMarkov chain.
And some Hmms use neural networks to estimate phonetic posteriorprobability in speech recognition [9].
The input layer of the network typically coversboth the past states and the further states.
However, from the essential definition ofHmm, there are two ways to improve the predictive power of Hmm.
One approach isto relax the limited history hypothesis and involve more history information intolanguage model.
The other is to relax the stationary hypothesis and make use of timeinformation.
In recent years, much research focuses on the first approach [10] andhigher-order Hmm is built up.
But as the order increases, the parameter spaceexplodes at such an exponential rate that training corpus becomes too sparse andsystem resource exhausts soon.
This paper adopts the second approach and tries tomake good use of time information.
Then NSHmm is proposed.
Since there is notheoretical conflict between NSHmm and high-order Hmm, the two models can becombined together in proper conditions.3.2   Definition for NSHmmSimilarly with Hmm, NSHmm is also mathematically defined as a five-tuple M =<?, ?, ?
?, ?
?, ?
?> which consists of:1.
A finite set of (hidden) states ?.2.
A finite set of (observed) symbols ?.3.
A state transition function ??
: ??
??
t -> [0, 1].4.
A symbol emission function ??
: ??
??
t -> [0, 1].5.
And an initial state probability function ??
: ??
t -> [0, 1].In the above definition, t is the time variable indicating when state transition orsymbol emission occurs.
Different from Hmm?s definition, ?
?, ??
and ??
are all thefunctions of t. And they can still be estimated by MLE principle on large scale corpus.This key question of NSHmm is how to represent time information.
We?ll discuss thatquestion in the next section.3.3   Representation of Time InformationSince time information is to describe when the events of Hmm (e.g.
state transition orsymbol emission) occur, a natural way is to use the event index in Markov chain torepresent the time information.
But there are two serious problems with that method.Firstly, index has different meanings in the Markov chains of different length.Secondly, since a Markov chain may have arbitrary length, the event index can be anynatural number.
However, computer system can only deal with finite value.
A refinedmethod is to use the ratio of the event index and the length of Markov chain which isa real number of the range [0, 1].
But there are infinite real numbers in the range [0,1].
In this paper, we divide the range [0, 1] into several equivalence classes (bins) andeach class share the same time information.
When training NSHmm, the functions of?
?, ??
and ??
should be estimated in each bin respectively according to their timeinformation.
And when they are accessed, they should also get the value in the830 J. Xiao, B. Liu, and X. Wangaccording bin.
For example, the state transition function ??
can be estimated by theformula below:( , , )( , )ijtC i j tpC i t=  (1)where ( , , )C i j t is the co-occurrence frequency of state i and state j at time t and it canbe estimated by counting the co-occurrence times of state i and state j in the tth bin ineach sentence of corpus.
( , )C i t is the frequency of state i at time t and can beestimated by counting the occurrence times of state i in the tth bin in the sentence ofcorpus.
And the result ijtP is the transition probability between state i and j at time t.It?s similar to estimate the functions of ??
and ?
?.3.4   Algorithms on Non-stationary Hidden Markov ModelThe three fundamental questions of Hmm also exist in NSHmm.
The correspondingalgorithms, such as forward algorithm, viterbi algorithm and Baum-Welch algorithm,can work well in NSHmm, except that they have to first calculate the timeinformation and then compute the function values of ?
?, ??
and ??
according to thestatistical information in the corresponding bins.3.5   Space Complexity AnalysisIn this section, we will analyze the space complexity of NSHmm.
Compared withHmm, some conclusions can be drawn at the end of this section.
For simplicity andconvenience, we define some notations below:?
The hidden state number n?
The observed symbol number m?
The bin number for NSHmm kIn Hmm and NSHmm, all system parameters are devoted to simulate the threefunctions of ?, ?
and ?.
For Hmm, a vector of size n is usually used to store the initialprobability of each state.
An n ?
n matrix is adopted to store the transitionprobabilities between every two states, and n ?
m matrix to record the emissionprobabilities between states and observed symbols.
The space complexity for Hmm isthe sum of these three parts which is ( )n n n n m?
+ ?
+ ?
.
For NSHmm, since ?
?, ?
?and ??
are all the functions of time t, time information should be counted in.
An n ?
kmatrix is used to store the initial probability of each state at different time.
An n ?
n?k matrix is used to store the transition probability between each state at different timeand n ?
m ?
k matrix to keep the emission probability.
Thus, the space complexity ofNSHmm is (( ) )n n n n m k?
+ ?
+ ?
?
which is k times than that of Hmm.
As theanalysis shows, the space complexity of NSHmm increases at a linear speed with k,rather than at an exponential speed as high-order Hmm dose.
Moreover, as k isusually far below than n, NSHmm is much easier to avoid the problem of parameterspace explosion.Principles of Non-stationary Hidden Markov Model and Its Applications 8313.6   Variant Form of NSHmmIn this section, this paper proposes a variant form of NSHmm (VNSHmm).
It?s basedon these facts: for some applications, such as on mobile platform, there is not enoughsystem resource to build up a whole NSHmm.
Then NSHmm has to be compressed.This paper constructs some statistical variables for time information and uses thesestatistical variables to substitute concrete time information in NSHmm.
Whencomputing the probability in VNSHmm, these statistical variables are combinedtogether to calculate a coefficient for normal probability of Hmm.Two statistical variables, expectation and variance of time information, are adoptedin VNSHmm.
And such assumptions are made that more weight should be awarded ifthe time of event occurring fits better with the training corpus, and less weight viceversa.
The probability function in VNSHmm is defined as below:2(( ) )1 V t Etp e pZ?
??
?
+= ?
(2)where Z is a normalizing factor, and is defined as:2(( ) )1t kV t EtZ e p?
?=?
?
+== ??
(3)The notations in the formulation (2) and (3) are described in the following:?
Current time information t?
Expectation of time information E?
Variance of time information V?
State transition probability ( or symbol emission probability ) p?
Adjusted coefficients ?
and  ?pt is descendent with the term t-E which defines the difference between currenttime and time expectation in training corpus.
As the value of t-E decreases, t fits fortraining corpus better and more weight is added to pt.
For example, we take aChinese sentence as a state chain of Markov process.
The word ????
(first of all)usually leads a sentence in training corpus.
For test corpus, more weight should begiven to pt if ??
(first of all) appears at the beginning of the sentence, whereas lessweight if at the sentence end.
pt is ascendant with the variance V. The item V ismainly used to balance the value of term t-E for some active states.
For example, inChinese, some adjectives, such as ????
(beautiful), can appear at any position ofthe sentence.
Then it?s unreasonable to decrease pt just because the term t-Eincreases.
In such a situation, the value of item V for ????
(beautiful) is usuallybigger than that of those inactive states (e.g.??
(first of all)).
Then the item V canprovide a balance for the value of t-E.Since VNSHmm just makes use of expectation and variance, rather than the wholetime information, its space complexity is equal to that of the NSHmm with only twobins, which is (( ) 2)n n n n m?
+ ?
+ ?
?
.832 J. Xiao, B. Liu, and X. Wang4   ExperimentsIn the experiments, NSHmm and VNSHmm have been applied to two sequencelabeling tasks: pos tagging and pinyin-to-character conversion.
This paper willdescribe them in detail in the following two sections.4.1   Pos TaggingFor pos tagging, this paper chooses the People?s Daily corpus in 1998 which has beenlabeled by Peking University [11].
The first 5 month corpus is taken as trainingcorpus and the 6th month as test corpus.
Since most of pos-taggers are based on 2-order Hmm (trigram), 2-order NSHmm and 2-order VNSHmm are constructedrespectively in the experiments.We first calculate KL distances between the emission probability distribution ofHmm and the distributions of NSHmm at different time.
Only when the distances aregreat, could NSHmm be expected to outperform Hmm; otherwise NSHmm wouldhave similar performance as Hmm has.
Since there are totally k different distancevalues for NSHmm with k bins, we just calculate the average distance for eachNSHmm.
The results are presented in table 1 as below:Table 1.
Average KL Distances between Emission Probability Distributions of NSHmm andHmmBin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8Aver KL Dis 0 0.08 0.12 0.15 0.17 0.19 0.21 0.22From table 1 we can see that as the bin number increases, the average KL distancebecome bigger and bigger, which indicates there is more and more difference betweenthe emission probability distributions of Hmm and that of NSHmm.
Similar results canbe gotten by comparing state-transition-probability distributions of the two models.And as time information increases, we expect more predictive power for NSHmm.To prove the effectiveness of NSHmm and VNSHmm, in the rest of this section,two sets of experiments, close test and open test, are performed.
The results of closetest are showed in table 2, figure 1 and the results of open test are presented in table 3,figure 2 as below.Table 2.
Pos Tagging Close TestBin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8Hmm (baseline) 6.04% --- --- --- --- --- --- ---ErrorRate6.04% 5.63% 5.55% 5.52% 5.47% 5.44% 5.42% 5.47% NSHmmReduction --- 6.79% 8.11% 8.61% 9.44% 9.93% 10.26% 9.43%ErrorRate6.04% 5.85% 5.85% 5.85% 5.85% 5.85% 5.85% 5.85% VNSHmmReduction --- 3.15% 3.15% 3.15% 3.15% 3.15% 3.15% 3.15%Principles of Non-stationary Hidden Markov Model and Its Applications 8330 1 2 3 4 5 6 7 8 95.25.45.65.86.06.2ErrorRate(%)Bins Number: KHmmNSHmmVNSHmmFig.
1.
Pos Tagging Close TestTable 3.
Pos Tagging Open TestBin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8Hmm (baseline) 6.99% --- --- --- --- --- --- ---ErrorRate6.99% 6.44% 6.39% 6.42% 6.40% 6.43% 6.47% 6.58% NSHmmReduction --- 7.87% 8.58% 8.15% 8.44% 8.01% 7.44% 5.87%ErrorRate6.99% 6.59% 6.59% 6.59% 6.59% 6.59% 6.59% 6.59% VNSHmmReduction --- 5.72% 5.72% 5.72% 5.72% 5.72% 5.72% 5.72%0 1 2 3 4 5 6 7 8 96.46.66.87.07.2Error Rate(%)Bins Number: KHmmNSHmmVNSHmmFig.
2.
Pos Tagging Open TestAs table 2 and table 3 have showed, no matter in close test or in open test, NSHmmand VNSHmm achieve much lower error rates than Hmm.
NSHmm gets at most10.26% error rate reduction and VNSHmm obtains 3.15% reduction in close test; and834 J. Xiao, B. Liu, and X. Wangthey achieve 8.58% and 5.72% reductions respectively in open test.
These facts provethat NSHmm and VNSHmm have much more predictive power than Hmm has.
Fromfigure 1 we can see that in close test, as the bin number increases, the error rate ofNSHmm is decreased constantly, which proves that the improvement of NSHmm isdue to the increasing time information.
But in the open test as figure 2 shows, theerror rate stops decreasing after k=3.
That is because of the overfitting problem.
As aconsequence, this paper suggests k=3 in NSHmm for pos tagging task.
From figure 1and figure 2, VNSHmm performs stably after k=2, which indicates a small number ofparameters are enough to stat reliable statistical variables for VNSHmm and getimproved performance.4.2   Pinyin-to-Character ConversionFor the experiments of pinyin-to-character conversion, this paper adopts the sametraining corpus and test corpus as in pos tagging experiments.
And 6763 Chinesefrequent characters are chosen as the lexicon.
This paper firstly converts all rawChinese corpuses to the pinyin corpuses.
Then based on the both kinds of corpuses,Hmm, NSHmm and VNSHmm are built up.In the experiments, we first calculate KL distances between the state-transition-probability distributions of Hmm and the distributions of NSHmm at different time.As we have done in the pos tagging experiments, we just calculate the average KLdistance for each NSHmm.
The results are presented in table 4.Table 4.
Average KL Distances between State-Transition-Probability Distributions of NSHmmand HmmBin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8Aver KL Dis 0 0.08 0.12 0.15 0.17 0.18 0.19 0.21From table 4 we can see that as the bin number increases, the average KL distancebecome bigger and bigger and more predictive power is expected for NSHmm.
Andsimilar results can be gotten by comparing emission probability distributions of thetwo models.
Then in the rest of this section, we perform the pinyin-to-characterconversion experiments.
Close test and open test are performed respectively.
Theresults of close test are showed in table 5, figure 3 and the results of open test arepresented in table 6, figure 4 respectively.Table 5.
Pinyin-to-Character Conversion Close TestBin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8Hmm (baseline) 8.30% --- --- --- --- --- --- ---ErrorRate8.30% 7.17% 6.55% 6.08% 5.74% 5.43% 5.19% 4.98% NSHmmReduction --- 13.61% 21.08% 26.75% 30.84% 34.58% 37.47% 40.00%ErrorRate8.30% 8.28% 8.27% 8.28% 8.28% 8.28% 8.28% 8.28% VNSHmmReduction --- 0.24% 0.24% 0.24% 0.24% 0.24% 0.24% 0.24%Principles of Non-stationary Hidden Markov Model and Its Applications 8350 1 2 3 4 5 6 7 8 94.55.05.56.06.57.07.58.08.59.0ErrorRate(%)Bins Number: KHmmNSHmmVNSHmmFig.
3.
Pinyin-to-Character Conversion Close TestTable 6.
Pinyin-to-Character Conversion Open TestBin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8Hmm (baseline) 14.97% --- --- --- --- --- --- ---ErrorRate14.97%12.62% 13.16% 13.61% 13.93% 14.23% 14.52% 14.81% NSHmmReduction --- 15.70% 12.09% 9.08% 6.95% 4.94% 3.01% 1.07%ErrorRate14.97% 11.98% 11.96% 11.96% 11.96% 11.97% 11.97% 11.97% VNSHmmReduction --- 19.97% 20.11% 20.11% 20.11% 20.04% 20.04% 20.04%0 1 2 3 4 5 6 7 8 91213141516Error Rate(%)Bins Number: KHmmNSHmmVNSHmmFig.
4.
Pinyin-to-Character Conversion Open Test836 J. Xiao, B. Liu, and X. WangIn the experiments of pinyin-to-character conversion, the results are very similar tothose in the pos tagging experiments.
NSHmm and VNSHmm show much morepredictive power than Hmm does.
NSHmm gets at most 40% error rate reduction andVNSHmm obtains 0.24% reduction in close test; and they achieve 15.7% and 20.11%reductions respectively in open test.
As time information increases, the error rate ofNSHmm decreases drastically in close test as it dose in pos tagging task.
And theoverfitting problem arises after k=2 in open test.However, different from the results of pos tagging experiments, VNSHmmoutperforms NSHmm in open test.
Since 6763 characters are adopted as states set inpinyin-to-character conversion system, which is much larger than the states set in postagging system, data sparseness problem is more likely to occur.
VNSHmm can beview as a natural smoothing technique for NSHmm.
Thus it works better.
We alsonotice that the improvements in pinyin-to-character conversion experiments are moresignificant than those in pos-tagging experiments.
In pinyin-to-character conversiontask, the state chain is the Chinese sentence.
Intuitively, some Chinese characters andwords are much more likely to occur at some certain positions in the sentence, forinstance, the beginning or the end of a sentence.
As we discuss in section 3.3, inpractice the time information of events in NSHmm is defined as the positioninformation where the events occur.
Then NSHmm and VNSHmm can capture thosecharacteristics straightforwardly.
But in pos-tagging, the state chain is the pos tagstream.
Pos is a more abstract concept than word, and their positional characteristicsare not as apparent as words?.
Henceforth, the improvements in pos-taggingexperiments are less significant than those in pinyin-to-character conversionexperiments.
But NSHmm and VNSHmm can still model and make good use of thosepositional characteristics, and notable improvements have been achieved.In a word, NSHmm and VNSHmm achieve much lower error rates in both of thetwo sequence labeling tasks and show much more predictive power than Hmm.5   ConclusionsTo improve Hmm?s predictive power and meanwhile avoid the problems of high-order Hmm, this paper relaxes the stationary hypothesis of Hmm, makes use of timeinformation and proposes NSHmm.
Moreover, to further reduce NSHmm?s parameterspace for mobile applications, VNSHmm is proposed by constructing statisticalvariables on the time information of NSHmm.
Then NSHmm and VNSHmm areapplied to two sequence labeling tasks: pos tagging and pinyin-to-characterconversion.
From the experiment results, we can draw three conclusions in this paper:?
Firstly, NSHmm and VNSHmm achieve much lower error rates than Hmm inboth of the two tasks and thus have more predictive power.?
Secondly, the improvement of NSHmm is due to the increasing timeinformation.?
Lastly, a small number of parameters are enough to stat the statistical variablesfor VNSHmm.Principles of Non-stationary Hidden Markov Model and Its Applications 8376   Further ResearchSince NSHmm is an enhanced Hmm, some problems of Hmm also exist in NSHmm.For example, data sparseness problem is arising as time information increases inNSHmm.
Some smoothing algorithms should be designed to solve it in our furtherwork.
Also it?s difficult to describe long distance constraint for NSHmm and furtherresearch should be devoted to this problem.
To construct more compact NSHmm,proper prone techniques should be further studied and be compared with VNSHmm.AcknowledgementsThis investigation was supported emphatically by the National Natural ScienceFoundation of China (No.60435020) and the High Technology Research andDevelopment Programme of China (2002AA117010-09).We especially thank the three anonymous reviewers for their valuable suggestionsand comments.References1.
F. Jelinek.
Self-Organized Language Modeling for Speech Recognition.
IEEE ICASSP,1989.2.
George Nagy.
At the Frontier of OCR.
Processing of IEEE.
1992, 80(7).3.
ZhiMing Xu, XiaoLong Wang, Kai Zhang, Yi Guan.
A Post Processing Method for OnlineHandwritten Chinese Character recognition.
Journal of Computer Research andDevelopment.
Vol.36, No.
5, May 1999.4.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer.The Mathematics of Statistical Machine Translation: Parameter Estimation.
ComputationalLinguistics.
1992, 19(2).5.
Liu Bingquan, Wang Xiaolong and Wang Yuying, Incorporating Linguistic Rules inStatistical Chinese Language Model for Pinyin-to-Character Conversion.
High TechnologyLetters.
Vol.7 No.2, June 2001, P:8-136.
Christopher D. Manning and Hinrich Schutze.
Foundation of Statistic Natural LanguageProcessing.
The MIT Press.
1999.7.
Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L.Mercer.
Class-based n-gram models of natural language.
Computational Linguistics,18(4):467-479.
1992.8.
Z. Ghahramani and M. Jordan.
Factorial hidden Markov models.
Machine Learning, 29,1997.9.
J. Fritsch.
ACID/HNN: A framework for hierarchical connectionist acoustic modeling.
InProc.
IEEE ASRU, Santa Barbara, December 1997.10.
Goodman, J.
A bit of progress in language modeling.
Computer Speech and Language,403-434.
2001.11. http://www.icl.pku.edu.cn
