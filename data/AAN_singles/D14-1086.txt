Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787?798,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsReferItGame: Referring to Objects in Photographs of Natural ScenesSahar Kazemzadeh1?Vicente Ordonez1?Mark Matten2Tamara L. Berg11University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, USA2The Bishop?s School, San Diego, CA 92037, USAvicente@cs.unc.edu,tlberg@cs.unc.eduAbstractIn this paper we introduce a new gameto crowd-source natural language referringexpressions.
By designing a two playergame, we can both collect and verify refer-ring expressions directly within the game.To date, the game has produced a datasetcontaining 130,525 expressions, referringto 96,654 distinct objects, in 19,894 pho-tographs of natural scenes.
This dataset islarger and more varied than previous REGdatasets and allows us to study referringexpressions in real-world scenes.
We pro-vide an in depth analysis of the resultingdataset.
Based on our findings, we designa new optimization based model for gen-erating referring expressions and performexperimental evaluations on 3 test sets.1 IntroductionMuch of everyday language and discourse con-cerns the visual world around us, making under-standing the relationship between objects in thephysical world and language describing those ob-jects an important challenge problem for AI.
Fromrobotics, to image search, to situated languagelearning, and natural language grounding, thereare a number of research areas that would bene-fit from a better understanding of how people referto physical entities in the world.Recent advances in automatic computer visionmethods have started to make technologies for rec-ognizing thousands of object categories a near re-ality (Perronnin et al., 2012; Deng et al., 2012;Deng et al., 2010; Krizhevsky et al., 2012).
As aresult, there has been a spurt of recent work tryingto estimate higher level semantics, including ex-citing efforts to automatically produce natural lan-guage descriptions of images and video (Farhadi et?Indicates equal author contribution.al., 2010; Kulkarni et al., 2011; Yang et al., 2011;Ordonez et al., 2011; Kuznetsova et al., 2012;Feng and Lapata, 2013).
Common challenges en-countered in these pursuits include the fact thatdescriptions can be highly task dependent, open-ended, and difficult to evaluate automatically.Therefore, we look at the related, but more fo-cused problem of referring expression generation(REG).
Previous work on REG has made signif-icant progress toward understanding how peoplegenerate expressions to refer to objects (a recentsurvey of techniques is provided in Krahmer andvan Deemter (2012)).
In this paper, we study therelatively unexplored setting of how people referto objects in complex photographs of real-worldcluttered scenes.
One initial stumbling block toexamining this scenario is lack of existing rele-vant datasets, as previous collections for studyingREG have used relatively focused domains suchas graphics generated objects (van Deemter et al.,2006; Viethen and Dale, 2008), crafts (Mitchell etal., 2010), or small everyday (home and office) ob-jects arrayed on a simple background (Mitchell etal., 2013a; FitzGerald et al., 2013).In this paper, we collect a new large-scale cor-pus, currently containing 130,525 expressions, re-ferring to 96,654 distinct objects, in 19,894 pho-tographs of real world scenes.
Some examplesfrom our dataset are shown in Figure 5.
To con-struct this corpus efficiently, we design a new twoplayer referring expression game (ReferItGame)to crowd-source the data collection.
Popular-ized by efforts like the ESP game (von Ahn andDabbish, 2004) and Peekaboom (von Ahn et al.,2006b), Human Computation based games can bean effective way to engage users and collect largeamounts of data inexpensively.
Two player gamescan also automate verification of human providedannotations.Our resulting corpus is both more real-worldand much bigger than previous datasets, allowing787us to examine referring expression generation ina new setting at large scale.
To understand andquantify this new dataset, we perform an exten-sive set of analyses.
One significant differencefrom previous work is that we study how refer-ring expressions vary for different categories.
Wefind that an object?s category greatly influences thetypes of attributes used in their referring expres-sion (e.g.
people use color words to describe carsmore often than mountains).
Additionally, we findthat references to an object are sometimes madewith respect to other nearby objects, e.g.
?the ballto left of the man?.
Interestingly, the types of ref-erence objects (i.e.
?the man?)
used in referringexpressions is also biased toward some categories.Finally, we find that the word used to refer to theobject category itself displays consistencies acrosspeople.
This notion is related to ideas of entry-level categories from Psychology (Rosch, 1978).Given these findings, we propose an optimiza-tion model for generating referring expressionsthat jointly selects which attributes to include inthe expression, and what attribute values to gener-ate.
This model incorporates both visual modelsfor selecting attribute-values and object categoryspecific priors.
Experimental evaluations indicatethat our proposed model produces reasonable re-sults for REG.In summary, contributions of our paper include:?
A two player online game to collect and ver-ify natural language referring expressions.?
A new large-scale dataset containing naturallanguage expressions referring to objects inphotographs of real world scenes.?
Analyses of the collected dataset, includingstudying category-specific variations in refer-ring expressions.?
An optimization based model to generatereferring expressions for objects in real-world scenes with experimental evaluationson three labeled test sets.The rest of the paper is organized as follows.First we outline related work from the vision andlanguage communities (?2).
Then we describe ouronline game for collecting referring expressions(?3) and provide an analysis of our new Refer-ItGame Dataset (?4).
Finally, we present and eval-uate our model for generating referring expres-sions (?5) and discuss conclusions and future work(?6).2 Related WorkReferring Expression Generation: There hasbeen a long history of research on understandinghow people generate referring expressions, datingback to the 1970s (Winograd, 1972).
One com-mon approach is the Incremental Algorithm (Daleand Reiter, 1995; Dale and Reiter, 2000) whichuses logical expressions for generation.
Muchwork in REG follows the Gricean maxims (Grice,1975) which provide principles for how peoplewill behave in conversation.Recently, there has been progress examiningother aspects of the referring expression prob-lem such as understanding what types of attributesare used (Mitchell et al., 2013a), modeling varia-tions between speakers (Viethen and Dale, 2010;Viethen et al., 2013; Van Deemter et al., 2012;Mitchell et al., 2013b), incorporating visual classi-fiers (Mitchell et al., 2011), producing algorithmsto refer to object sets (Ren et al., 2010; FitzGeraldet al., 2013), or examining impoverished percep-tion REG (Fang et al., 2013).
A good survey ofwork in this area is provided in Krahmer and vanDeemter (2012).
We build on past work, extendingmodels to generate attributes jointly in a categoryspecific framework.Referring Expression Datasets: Some initialdatasets in REG used graphics engines to pro-duce images of objects (van Deemter et al., 2006;Viethen and Dale, 2008).
Recently more realis-tic datasets have been introduced, consisting ofcraft objects like pipecleaners, ribbons, and feath-ers (Mitchell et al., 2010), or everyday homeand office objects such as staplers, combs, orrulers (Mitchell et al., 2013a), arrayed on a sim-ple background.
These datasets helped moved re-ferring expression generation research into the do-main of real world objects.
We seek to furtherthese pursuits by constructing a dataset of naturalobjects in photographs of the real world.Image & Video Description Generation: Re-cent research on automatic image description hasfollowed two main directions.
Retrieval basedmethods (Aker and Gaizauskas, 2010; Farhadi etal., 2010; Ordonez et al., 2011; Feng and Lap-ata, 2010; Feng and Lapata, 2013) retrieve exist-ing captions or phrases to describe a query image.Bottom up methods (Kulkarni et al., 2011; Yanget al., 2011; Yao et al., 2010) rely on visual classi-fiers to first recognize image content and then con-struct captions from scratch, perhaps with some788Figure 1: An example game.
Player 1 (left) sees an image with an object outlined in red (the man)and provides a referring expression for the object (?man in red shirt on horse?).
Player 2 (right) seesthe image and the expression from Player 1 and must localize the correct object by clicking on it (clickindicated by the red square).
Elapsed time and current scores are also provided.input from natural language statistics.
Very re-cently, these ideas have been extended to producedescriptions for videos (Guadarrama et al., 2013;Barbu et al., 2012).
Like these methods, we gen-erate descriptions for natural scenes, but focus onreferring to particular objects rather than provid-ing an overall description of an image or video.Human Computation Games: Games can bea useful tool for collecting large amounts of la-beled data quickly.
Human Computation Gameswere first introduced by Luis von Ahn in the ESPgame (von Ahn and Dabbish, 2004) for image la-beling, and later extended to segment objects (vonAhn et al., 2006b), collect common-sense knowl-edge (von Ahn et al., 2006a), or disambiguatewords (Seemakurty et al., 2010).
Recently, crowdgames have also been introduced into the com-puter vision community for tasks like fine grainedcategory recognition (Deng et al., 2013).
Thesegames can be released publicly on the web orused on Mechanical Turk to enhance and encour-age turker participation (Deng et al., 2013).
In-spired by the success of previous games, we cre-ate a game to collect and verify natural languageexpressions referring to objects in natural scenes.3 Referring Expression Game(ReferItGame)In this section we describe our referring expres-sion game (ReferItGame?
), a simple two playergame where players alternate between generatingexpressions referring to objects in images of nat-ural scenes, and clicking on the locations of de-scribed objects.
An example game is shown inFigure 1.?Available online at http://referitgame.com3.1 Game PlayPlayer 1: is shown an image with an object out-lined in red and provided with a text box in whichto write a referring expression.
Player 2: is shownthe same image and the referring expression writ-ten by Player 1 and must click on the location ofthe described object (note, Player 2 does not seethe object segmentation).
If Player 2 clicks onthe correct object, then both players receive gamepoints and the Player 1 and Player 2 roles swap forthe next image.
If Player 2 does not click on thecorrect object then no points are received and theplayers remain in their current roles.This provides us with referring expressions forour dataset and verification that the expressionsare valid since they led to correct object localiza-tions.
Expressions written for games where theobject was not correctly localized are kept and re-leased with the dataset for future study, but are notincluded in our final dataset analyses or statistics.A game timer encourages players to write expres-sions quickly, resulting in more natural expres-sions.
Also, IP addresses are filtered to preventpeople from simultaneously playing both roles.3.2 Playing Against the ComputerTo promote engagement, we implement a singleplayer version of the game.
When a player con-nects, if there is another player online then the twopeople are paired.
If there are currently no otheravailable players, then the person plays a ?canned?game against the computer.
If at any point anotherperson connects, the canned game ends and theplayer is paired with the new person.To implement canned games we seed thegame with 5000 pre-recorded referring expressiongames (5 referring expressions and resulting clicks789for each of 1000 objects) collected using Ama-zon?s Mechanical Turk service.
Implementing anautomated version of Player 1 is simple; we justshow the person one of the pre-collected referringexpressions and they click as usual.Automating the role of Player 2 is a bit morecomplicated.
In this case, we compare the per-son?s written expression against the pre-recordedexpressions for the same object.
For this compar-ison we use a parser to lemmatize the words in anexpression and then compute cosine similarity be-tween expressions with a bag of words representa-tion.
Based on this measure the closest matchingexpression is determined.
If there is no similaritybetween the newly generated expression and thecanned expressions, the expression is deemed in-correct and a random click location (outside of theobject) is generated.
If there is a successful matchwith a previously generated expression, then thecanned click from the most similar pre-recordedgame is used.
More complex similarities could beused, but since we require real-time performancein our game setting we use this simple implemen-tation which works well for our expressions.4 ReferItGame DatasetIn this section we describe the ReferItGamedataset?, including images and labels, processingthe dataset, and analysis of the collection.4.1 Images and LabelsWe build our dataset of referring expressionson top of the ImageCLEF IAPR image retrievaldataset (Grubinger et al., 2006).
This dataset isa collection of 20,000 images available free ofcharge without copyright restrictions, depicting avariety of aspects of everyday life, from sports,to animals, to cities, and landscapes.
Crucial forour purposes, the SAIAPR TC-12 expansion (Es-calante et al., 2010) includes segmentations ofeach image into regions indicating the locations ofconstituent objects.
238 different object categoriesare labeled, including animals, people, buildings,objects, and background elements like grass orsky.
This provides us with information regardingobject category, object location, and object size, aswell as the location and categories of other objectspresent in the same image.
?Available at http://tamaraberg.com/referitgame4.2 Collecting the DatasetFrom the ImageCLEF dataset, we created a totalof over 100k distinct games (one per object labeledin the dataset).
For the games we imposed an or-dering to allow for collecting the most interestingexpressions first.
Initially we prioritized gamesfor objects in images with multiple objects of thesame category.
Once these games were completed,we prioritized ordering based on object category toinclude a comprehensive range of objects.
Finally,after successfully collecting referring expressionsfrom the prioritized games, we posted games forthe remaining objects.
In order to evaluate consis-tency of expression generation across people, wealso include a probability of repeating previouslyplayed games during collection.To date, we have collected 130,525 successfullycompleted games.
This includes 10,431 cannedgames (a person playing against the computer, notincluding the initial seed set) and 120,094 realgames (two people playing).
96,654 distinct ob-jects from 19,984 photographs are represented inthe dataset.
This covers almost all of the objectspresent in the IAPR corpus.
The remaining ob-jects from the collection were either too small ortoo ambiguous to result in successful games.For data collection, we posted the game onlinefor anyone on the web to play and encouraged par-ticipation through social media and the survey sec-tion of reddit.
In this manner we collected over4 thousand referring expressions over a period of3 weeks.
To speed up data collection, we alsoposted the game on Mechanical Turk.
Turkerswere paid upon completion of 10 correct games(games where Player 2 clicks on the correct objectof interest).
Turkers were pre-screened to have ap-proval ratings above 80% and to be located in theUS for language consistency.4.3 Processing the DatasetBecause of the size of the dataset, hand annotationof all referring expressions is prohibitive.
There-fore, similar to past work (FitzGerald et al., 2013),we design an automatic method to pre-process theexpressions and extract object and attribute men-tions.
These automatically processed expressionsare used only for analysis and model training.
Wealso fully hand label portions of the dataset forevaluation (?5.2).By examining the expressions in the collecteddataset, we define a set of attributes with broad790S ::= subject wordcolor word?
::= rel(S, color word)color word?=color word|prep in(S, color word)color word?=color wordsize word?
::= rel(S, size word)size word?=size wordabs loc word?
::= rel(S, abs loc word)abs loc word?=abs loc word|prep on(S, orientation word) ?
?prep of(S, )abs loc word?=on+orientation wordrel loc word?
::= RLRL ::= prep rel loc word(S, object word)RL=rel loc word|prep on(S, orientation word) ?
prep of(S, object word)RL=on orientation word|prep to(S, orientation word) ?
prep of(S, object word)RL=to orientation word|prep at(S, orientation word) ?
prep of(S, object word)RL=at orientation wordgeneric word?
::= amod(S, generic word)Figure 2: Templates for parsing attributes from referring expressions (?4.3).coverage of the attribute types used in the re-ferring expressions.
We define the set of at-tributes for a referring expression as a 7-tupleR ={r1, r2, r3, r4, r5, r6, r7}:?
r1is an entry-level category attribute,?
r2is a color attribute,?
r3is a size attribute,?
r4is an absolute location attribute,?
r5is a relative location relation attribute,?
r6is a relative location object attribute,?
r7is a generic attribute,Color and size attributes refer to the object color(e.g.
?blue?)
and object size (e.g.
?tiny?)
respec-tively.
Absolute location refers to the location ofthe object in the image (e.g.
?top of the image?
).Relative location relation and relative location ob-ject attributes allow for referring expressions thatlocalize the object with respect to another objectin the picture (e.g.
?the car to the left of the tree?
).Generic attributes cover all less frequently ob-served attribute types (e.g.
?wooden?
or ?round?
).The entry-level category attribute is related tothe concept of entry-level categories first proposedby Psychologists in the 1970s (Rosch, 1978) andrecently explored in visual recognition (Ordonezet al., 2013).
The idea of entry-level categories isthat an object can belong to many different cate-gories; an indigo bunting is an oscine, a bird, avertebrate, a chordate, and so on.
But, a personlooking at a picture of one would probably call ita bird (unless they are very familiar with ornithol-ogy).
Therefore, we include this attribute to cap-ture how people name object categories in refer-ring expressions.Parsing the referring expressions: We parsethe expressions using the most recent versionof the StanfordCoreNLP parser (Socher et al.,2013).
We begin by traversing the parse tree in abreadth-first manner and selecting the head nounof the sentence to determine the object of thereferring expression, denoted as subject word.We pre-define a dictionary of attribute-values(color word, size word, abs location word,rel location word) for each of the attributesbased on the observed data using a combinationof POS-tagging and manual labeling.We then apply a template-based approach on thecollapsed dependency relations to recover the setof attributes (the main template rules are shownin Figure 2).
The relationship rel indicates anylinguistic binary relationship between the subjectword S and another word, including the amod re-lationship.
Orientation word captures the wordslike left, right, top and bottom.
For generic wordwe consider any modifier words other than thosecaptured by our other attributes (color, size, loca-tion).Using this template-based parser we canfor instance parse the following expression:?Red flower on top of pedestal?.
The firstrule would match the prep(S, color word)relation, effectively recovering the attributecolor word?as ?red?.
The second rule wouldmatch the prep on(S, orientation word) ?prep of(S, object word) relations, recoveringrel loc word?as ?on top of ?
and object wordas ?pedestal?.The accuracy of our parser based processing is91%.
This was evaluated on 4,500 expressions79100.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1Plot D: Object Locations for Abs-LocAttributesRight Left Bottom Middle Top Front0 2000 4000 6000sky-bluemangroup-of-personsgroundrockcloudgrasswomantreesmountainvegetationwallskywindowbuildingoceansky-lighttreecarpersonhousefloorcouple-of-personsface-of-personplanthatstreethillfabricbedbottlelampsand-beachchairdoorchild-boypaintingpalmriverbicyclePlot A: Attribute Breakdown by CategoryNone Color Size Absolute Location Relative Location  Other050%141%29%Plot C: Number of Attributes PerExpression0 0.1 0.2 0.3 0.4 0.5 0.6NoneSizeRelativeLocationPlot F: Attribute Use FrequenciesOverall Single Multiple00.050.10.150.20.250.30.35None Small Big Tall Little Tiny Large Short Huge LongPlot E: Object Area vs Indicated Size0 200 400 600peopleshirtsideguyheadtreemancornerbackgroundbuildingwallwomantablebedwaterhatgirlmountainpersonsigncarpicladywindowskyfootboatforegroundjacketgroundhandbikegroupcloudhorsegrassrockhousePlot B: Most Frequently Used RelativeObjectsFigure 3: Analyses of the ReferItGame Dataset.
Plot A shows frequency and attribute occurrence forcommon object categories.
Plot B shows objects frequently used as reference points, ie ?to the left of theman?.
Plot C shows frequencies of using 0, 1 or 2 attributes within the same expression.
Plot D showsobject locations vs location words used.
Plot E shows normalized object size vs size words used (barsshow 1stthrough 3rdquartiles).
Plot F shows the frequency of usage of each attribute type for imagescontaining either a single instance of the object category or multiple instances of the category.792People Man WomanCar Bottle StreetColor ObjectsBeigeRedYellowBlueBlackFigure 4: Left: Tag clouds showing entry-Level category words used in referring expressions to namevarious object categories, with word size indicating frequency.
For example, this indicates that ?streets?are often called ?road?, sometimes ?ground?, sometimes ?roadway?, etc.
Right: example objects pre-dicted to portray some of our color attribute values.
Note sometimes our color predictor is quite accurate,and sometimes it makes mistakes (see the man in a red shirt predicted as ?yellow?
).that were manually parsed by a human annotator.4.4 Dataset AnalysisIn the resulting dataset, we have a range of cov-erage over objects.
For 10,304 of the objects wehave 2 or more referring expressions while for therest of the objects we have collected only one ex-pression.
This creates a dataset that emphasizesbreadth while also containing enough data to studyspeaker variation.Multiple attribute analyses are provided in Fig-ure 3.
We find that most expressions use 0, 1, or2 attributes (in addition to the entry-level attributeobject word), with very few expressions contain-ing more than 2 attributes (frequencies are shownin Fig 3c).
We also examine what types of at-tributes are used most frequently, according to ob-ject category in Fig 3a, and when associated withsingle or multiple occurrences of the same objectcategory in an image in Fig 3f.
The frequencyof attribute usage in images containing multipleobjects of the same type increases for all types,compared to single object occurrences.
Perhapsmore interestingly, the use of different attributes ishighly category dependent.
People use more at-tribute words overall to describe some categories,like ?man?, ?woman?, or ?plant?, and the distribu-tion of attribute types also varies by category.
Forexample, color attributes are used more frequentlyfor categories like ?car?
or ?woman?
than for cat-egories like ?sky?
or ?rock?.We also examine which objects are most fre-quently used as points of reference, e.g.,?the chairnext to the man?
in Fig 3b.
We observe that peo-ple and some background categories like ?tree?
or?wall?
are often used to help localize objects inreferring expressions.
Additionally, we provideplots showing the relationship between object lo-cation in the image and use of absolute locationwords, Fig 3d, as well as size words vs object area,Fig 3e.Finally, we study entry-level category attribute-values to understand how people name objects inreferring expressions.
Tag clouds indicating thefrequencies of words used to name various ob-ject categories are provided in Fig 4 (left).
Ob-jects like ?street?
are usually referred to as ?road?,but sometimes they are called ?ground?, ?road-way?, etc.
?Bottles?
are usually called ?bottle?,but sometimes referred to as ?coke?
or ?beer?.
In-terestingly, ?man?
is usually called ?man?
while?woman?
is most often called ?person?
in the re-ferring expressions.5 Generating Referring ExpressionsIn this section we describe our proposed genera-tion model and provide experimental evaluationson three test sets.5.1 Generation ModelGiven an input tuple I = {P, S}, where P is atarget object and S is a scene (image containingmultiple objects), our goal is to generate an outputreferring expression, R. For instance, the repre-sentation R for the referring expression: The bigold white cabin beside the tree would be R ={cabin, white, big,?, beside, tree, old}.To generate referring expressions we constructvocabularies Vriwith candidate values for each at-tribute ri?
R, where attribute vocabulary Vricon-tains the set of words observed in our parsed refer-ring expressions for attribute riplus an additional793Image Human Expressions Generated Expressionspicture on the wall picture pictureBaseline:[picture, white, , right, , , ]  Full: [picture, , , , prep_on, wall, ]Door white door middle white doorBaseline:[door, white, , right, , , ]  Full:[door, white, , right, , , ]big gated window on right of white section black big window right brown railings on rightBaseline:[window, white, , right, , , ]  Full:[window, brown, , right, , , ]white shirt man white shirt on right man on rightBaseline:[man, white, , right, , , ]  Full:[man, white, , right, , , ]building on right behind guys blue right building building on rightBaseline:[building, white, , right, , , ]  Full:[building, white, , right, , , ]Image Human Expressions Generated Expressionspicture santa the santa pictureBaseline:[picture, white, , right, , , ]  Full:[picture, , , , prep_on, plant, ]right doorway right brown door right doorBaseline:[door, , , right, prep_on, person, ]   Full:[door, , , right, prep_above, person, ]with flag window top 2nd left 2nd window top leftBaseline:[window, , , right, prep_on, person, ]  Full:[window, , , left, prep_above, door, ]red guy left sitting left bottom guy red shirt lefBaseline:[man, , , right, prep_on, wall, ]  Full:[man, , , left, prep_in, woman, ]buildings buildings buildingsBaseline:[building, white, , right, , , ]   Full:[building, brown, , middle, , ,Figure 5: Example results, including human generated expressions, baseline and full model generatedexpressions.
For some images the model does well at mimicking human expressions (left).
For others itdoes not generate the correct attributes (right).?
value indicating that the attribute should be om-mited from the referring expression entirely.In this way, our framework can jointly deter-mine which attributes to include in the expression(e.g.,?size?
and ?color?)
and what attribute valuesto generate (e.g.,?small?
and ?blue?)
from the listof all possible values.
We enforce a constraint toalways include an ?entry-level category?
attribute(e.g.
?boy?)
so that we always generate a wordreferring to the object.We pose our problem as an optimization wherewe map a tuple {P, S} to a referring expressionR?as:R?= argmaxRE(R,P, S)s. t. fi(R) ?
bi(1)Where the objective function E is decomposed as:E(R,P, S) = ?6?i=2?i(ri, P, S)+ ?7?i=1?i(ri, type(P ))+?i>j?i,j(ri, rj)(2)Where ?iis the compatibility function between anattribute-value for riand the properties of the ob-served scene S and object P (described in ?5.1.1).The terms ?iand ?i,jare unary and pairwise pri-ors computed based on observed co-occurrencestatistics of attribute-values for riwith categories(where type(P ) denotes the type or category of anobject) and between pairs of attribute-values (de-scribed in ?5.1.2).
Attributes r1and r7are mod-eled only in the priors since we do not have visualmodels for these attributes.The constraints fi(R) ?
biare restricted to belinear constraints and are used to impose hard con-straints on the solution.
The first such constraint isused to control the verbosity (length) of the gener-ated referring expression using a constraint func-tion that imposes a minimum attribute length re-quirement by restricting the number of entries rithat can take value ?
in the solution.
?i1[ri= ?]
?
7?
?
(P, S) (3)Where 1[.]
is the indicator function and ?
(P, S) isa term that allows us to change the length require-ment based on the object and scene (so that imageswith a larger number of objects of the same typehave a larger length requirement).Finally we add hard constraints such that r5= ???
r6= ?, so that relative location and relativeobject attributes are produced together.5.1.1 Content-based potentialsPotentials ?iare defined for attributes r2to r6.Attribute r7represents a variety of different at-tributes, e.g.
material or shape attributes, butwe lack sufficient data to train visual models forthese infrequent attribute terms.
Therefore, wemodel these attributes using only prior statistics-based potentials (?5.1.2).
Visual recognition mod-els for recognizing entry-level object categories794could also be incorporated for modeling r1, but weleave this as future work.Color attribute:?2(r2= ck, P, S) = sim(histck, hist(P ))Where hist(P ) is the HSV color histogram of theobject P .
We compute similarity sim using cosinesimilarity, and histckis the mean histogram of allobjects in our training data that were referred towith color attribute-value ck?
Vr2.Size attribute:?3(r3= sk, P, S) =1?sk?2pie?
(size(P )?
?sk)2/2?2skWhere size(P ) is the size of object P normalizedby image size.
We model the probabilities of eachsize word sk?
Vr3as a Gaussian learned on ourtraining set.Absolute-location attribute:?4(r4= ak, P, S) =1?
(2pi)n|?ak|e?12(loc(P )?
?ak)T?ak?1(loc(P )?
?ak)Where loc(P ) are the 2-dimensional coordi-nates of the object P normalized to be ?
[0 ?
1].Parameters ?akand ?akare estimated fromtraining data for each absolute location wordak?
Vr4.Relative-location and Relative object:?5(r5= lk, P, S) =1[lk= ?]
?
g(count(type(P ), S))If there are a larger number of objects of the sametype in the image we find that the probability of us-ing a relative-location-object increases (e.g., ?thecar to the right of the man?).
For images where Pwas the only object of that category type, the prob-ability of using a relative-location-object is 0.12.This increases to 0.22 when there were two ob-jects of the same type and further increases to 0.26for additional objects of the same type.
There-fore, we model the probability of selecting rela-tive location value lk?
Vr5as a function g, wherecount(type(P ), S) counts the number of objectsin the scene S of the same category type as theobject P .
?6(r6= ok, P, S) =1[ok?
objectsnear(location(P ), S)]The above expression filters out potential relativeobjects ok?
Vr6that are not located in sufficientproximity to object P or are not present in the im-age at all.5.1.2 Prior statistics-based potentialsPrior statistics-based potentials are modeled for allof the attributes r1- r7.
Note that these potentialsdo not depend on specific attribute-values but onlyon the given object category type(P ).Unary prior potentials ?iare defined as:?i(ri, type(P )) =|D|?j=11[(r(j)i6= ) ?
(type(P(j)) = type(P ))]|D|?j=11[type(P(j)) = type(P )]+|D|?j=11[r(j)i6= ]|D|+ ?Where D = {P(j), S(j), R(j)} is our trainingdataset and ?
is a small additive smoothing term.The two terms in the above expression representcategory-specific counts and global counts of thenumber of times a given attribute riwas output ina referring expression in training data.
Pairwiseprior potentials ?i,jare defined as:?i<j?i,j(ri, rj) =?i<j?
(1)i,j(ri, rj) + ?
(2)5,6(r5, r6)?
(1)i,j(ri, rj) ={1 if ri= rj= ?C + ?
o.w.?
(2)5,6(r5= a, r6= b) =|D|?t=11[(r(t)5= a) ?
(r(t)6= b)]|D|where C =|D|?t=11[(r(t)i6=) ?
(r(t)j6=)]|D|.
The pairwisepotential ?
(1)i,jcaptures the pairwise statistics ofhow frequently people use pairs of attribute types.795SOURCE PREC(%) RECALL(%)Baseline - A 27.92 43.27Full Model - A 36.28 53.44Baseline - B 29.87 50.57Full Model - B 36.68 59.80Baseline - C 28.85 37.41Full Model - C 37.73 48.54Table 1: Baseline Model & Full Model perfor-mance on the three test sets (A,B,C).For instance how frequently people use both colorand size attributes to refer to an object.
The pair-wise potential ?
(2)i,jproduces a cohesion score be-tween relative-location words and relative-objectwords based on global dataset statistics.5.2 ExperimentsWe implement the proposed model using commer-cial binary integer linear programming software(IBM ILOG CPLEX).
This requires introducinga set of indicator variables for each of our multi-valued attributes and another set of indicator vari-ables to model pairwise interactions between ourvariables, as well as incorporating additional con-sistency constraints between variables.
Model pa-rameters (?
and ?)
are tuned on data randomlysampled from the training set.Test Sets: We evaluate our model on three testsets, each containing 500 objects.
For each ob-ject in the test sets we collect 3 referring expres-sions using the ReferItGame and manually labelthe attributes mentioned in each expression.
Wefind human agreement to be 72.31% on our dataset(where we measure agreement as mean match-ing accuracy of attribute values for pairs of usersacross images in our test sets).
The three testsets are created to evaluate different aspects of ourdata.Test Set A contains objects sampled randomlyfrom the entire dataset.
This test set is meant toclosely resemble the full dataset distribution.
Thegoal of the other two test sets is to sample expres-sions for ?interesting?
objects.
We first identifycategories that are mainly related to backgroundcontent elements, e.g.
?sky, ground, floor, sand,sidewalk, etc?.
We consider these categories tobe potentially less interesting for study than cat-egories like people, animals, cars, etc.
Test Set Bcontains objects sampled from the most frequentlyoccurring object categories in the dataset, selectedto contain a balanced number of objects from eachcategory, excluding the less interesting categories.Test Set C contains objects sampled from imagesthat contain at least 2 objects of the same category,excluding the less interesting categories.Results: Qualitative examples are shown in Fig 5comparing our results to the human produced ex-pressions.
For some images (left) we do quite wellat predicting the correct attributes and values.
Forothers we do less well (right).
We also show exam-ple objects predicted for some color words in Fig 4(right).
We see that our model can fail in severalways, such as generating the wrong attribute-valuedue to inaccurate predictions by visual models orselecting incorrect attributes to include in the gen-erated expression.Quantitative results: precision and recall mea-sures for the 3 test sets are reported in Table 1,including evaluation of a baseline version of ourmodel which incorporates only the prior potentials(?5.1.2) without any content based estimates.
Wesee that our model performs reasonably on bothmeasures, and outperforms the baseline by a largemargin on all test sets, with highest performanceon the broadly sampled interesting category testset.
Note that our problem is somewhat differ-ent than traditional REG where the input is oftenattribute-value pairs and the task is to select whichpairs to include in the expression.
Our goal is tojointly select which attributes to include and whatvalues to predict from a list of all possible valuesfor the attribute.6 Conclusions & Future WorkIn this paper we have introduced a new game tocrowd-source referring expressions for objects innatural scenes.
We have used this game to pro-duce a new large-scale dataset with analysis.
Wehave also proposed an optimization based modelfor REG and performed experimental evaluations.Future work includes developing fully automaticvisual recognition methods for REG in real worldscenes, and incorporating linguistically inspiredmodels for entry-level category prediction.AcknowledgmentsThis work was funded by NSF Awards #1417991and #1444234.
M.M.
was supported by the StonyBrook Simons Summer Research Program forHigh School students.
We also thank Alex Bergfor many helpful discussions.796ReferencesAhmet Aker and Robert Gaizauskas.
2010.
Generatingimage descriptions using dependency relational pat-terns.
In Association for Computational Linguistics(ACL).Andrei Barbu, Alexander Bridge, Zachary Burchill,Dan Coroian, Sven J. Dickinson, Sanja Fi-dler, Aaron Michaux, Sam Mussman, SiddharthNarayanaswamy, Dhaval Salvi, Lara Schmidt,Jiangnan Shangguan, Jeffrey Mark Siskind, Jar-rell W. Waggoner, Song Wang, Jinlian Wei, YifanYin, and Zhiqi Zhang.
2012.
Video in sentencesout.
In Uncertainty in Artificial Intelligence (UAI).Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the gricean maxims in the gener-ation of referring expressions.
Cognitive Science(CogSci), 19:233264.Robert Dale and Ehud Reiter.
2000.
Building naturallanguage generation systems.
In Cambridge Univer-sity Press.Jia Deng, Alexander C. Berg, Kai Li, and Fei-Fei Li.2010.
What does classifying more than 10,000 im-age categories tell us?
In European Conference onComputer Vision (ECCV).Jia Deng, Alex Berg, Sanjeev Satheesh, Hao Su, AdityaKhosla, and Fei-Fei Li.
2012.
Large scale vi-sual recognition challenge.
In http://www.image-net.org/challenges/LSVRC/2012/index.Jia Deng, Jonathan Krause, and Li Fei-Fei.
2013.
Fine-grained crowdsourcing for fine-grained recognition.In IEEE Conference on Computer Vision and PatternRecognition (CVPR).Hugo Jair Escalante, Carlos A. Hernandez, Jesus A.Gonzalez, A. Lopez-Lopez, Manuel Montes, Ed-uardo F. Morales, L. Enrique Sucar, Luis Villasenor,and Michael Grubinger.
2010.
The segmented andannotated iapr tc-12 benchmark.
Computer Visionand Image Understanding (CVIU).Rui Fang, Changsong Liu, Lanbo She, and Joyce Chai.2013.
Towards situated dialogue: Revisiting refer-ring expression generation.
In Empirical Methodson Natural Language Processing (EMNLP).Ali Farhadi, Mohsen Hejrati, Mohammad AminSadeghi, Peter Young, Cyrus Rashtchian, JuliaHockenmaier, and David Forsyth.
2010.
Everypicture tells a story: generating sentences for im-ages.
In European Conference on Computer Vision(ECCV).Yansong Feng and Mirella Lapata.
2010.
How manywords is a picture worth?
automatic caption genera-tion for news images.
In Association for Computa-tional Linguistics (ACL).Yansong Feng and Mirella Lapata.
2013.
Automaticcaption generation for news images.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,35(4):797?812.Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-moyer.
2013.
Learning distributions over logicalforms for referring expression generation.
In Em-pirical Methods on Natural Language Processing(EMNLP).H.
Paul Grice.
1975.
Logic and conversation.
page4158.Michael Grubinger, Paul D. Clough, Henning Muller,and Thomas Deselaers.
2006.
The iapr benchmark:A new evaluation resource for visual informationsystems.
In Proceedings of the International Work-shop OntoImage (LREC).Sergio Guadarrama, Niveda Krishnamoorthy, GirishMalkarnenkar, Subhashini Venugopalan, RaymondMooney, Trevor Darrell, and Kate Saenko.
2013.Youtube2text: Recognizing and describing arbitraryactivities using semantic hierarchies and zero-shotrecognition.
In International Conference on Com-puter Vision (ICCV).Emiel Krahmer and Kees van Deemter.
2012.
Compu-tational generation of referring expressions: A sur-vey.
In Computational Linguistics, volume 38, page173218.Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.2012.
Imagenet classification with deep convolu-tional neural networks.
In Neural Information Pro-cessing Systems (NIPS).Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-ing Li, Yejin Choi, Alexander C Berg, and Tamara LBerg.
2011.
Babytalk: Understanding and generat-ing simple image descriptions.
In IEEE ComputerVision and Pattern Recognition (CVPR).Polina Kuznetsova, Vicente Ordonez, Alex Berg,Tamara L Berg, and Yejin Choi.
2012.
Collectivegeneration of natural image descriptions.
In Associ-ation for Computational Linguistics (ACL).Margaret Mitchell, Kees van Deemter, and Ehud Re-iter.
2010.
Natural reference to objects in a visualdomain.
In International Natural Language Gener-ation Conference (INLG).Margaret Mitchell, Kees van Deemter, and Ehud Reiter.2011.
Two approaches for generating size modifiers.In European Workshop on Natural Language Gener-ation.Margaret Mitchell, Ehud Reiter, and Kees van Deemter.2013a.
Typicality and object reference.
In CognitiveScience (CogSci).Margaret Mitchell, Kees van Deemter, and Ehud Reiter.2013b.
Generating expressions that refer to visibleobjects.
In North American Chapter of the Associa-tion for Computational Linguistics (NAACL).797Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2text: Describing images using 1 millioncaptioned photographs.
In Neural Information Pro-cessing Systems (NIPS).Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C.Berg, and Tamara L. Berg.
2013.
From large scaleimage categorization to entry-level categories.
In In-ternational Conference on Computer Vision (ICCV).Florent Perronnin, Zeynep Akata, Zaid Harchaoui, andCordelia Schmid.
2012.
Towards good practicein large-scale learning for image classification.
InComputer Vision and Pattern Recognition (CVPR).Yuan Ren, Kees Van Deemter, and Jeff Z Pan.
2010.Charting the potential of description logic for thegeneration of referring expressions.
In InternationalNatural Language Generation Conference (INLG).Eleanor Rosch.
1978.
Principles of categorization.Cognition and Categorization, page 2748.Nitin Seemakurty, Jonathan Chu, Luis von Ahn, andAnthony Tomasic.
2010.
Word sense disambigua-tion via human computation.
In Human Computa-tion Workshop.Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013.
Parsing With Composi-tional Vector Grammars.
In Association for Compu-tational Linguistics (ACL).Kees van Deemter, Ielka van der Sluis, and Albert Gatt.2006.
Building a semantically transparent corpusfor the generation of referring expressions.
In In-ternational Conference on Natural Language Gen-eration (INLG).Kees Van Deemter, Albert Gatt, Roger PG van Gompel,and Emiel Krahmer.
2012.
Toward a computationalpsycholinguistics of reference production.
In Topicsin Cognitive Science, volume 4(2), page 166183.Jette Viethen and Robert Dale.
2008.
The use of spa-tial relations in referring expression generation.
InInternational Natural Language Generation Confer-ence (INLG).Jette Viethen and Robert Dale.
2010.
Speaker-dependent variation in content selection for referringexpression generation.
In Australasian LanguageTechnology Workshop.Jette Viethen, Margaret Mitchell, and Emiel Krahmer.2013.
Graphs and spatial relations in the generationof referring expressions.
In European Workshop onNatural Language Generation.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In ACM Conf.
on Hu-man Factors in Computing Systems (CHI).Luis von Ahn, Mihir Kedia, and Manuel Blum.
2006a.Verbosity: A game for collecting common-senseknowledge.
In ACM Conference on Human Factorsin Computing Systems (CHI).Luis von Ahn, Ruoran Liu, and Manuel Blum.
2006b.Peekaboom: A game for locating objects in images.In ACM Conference on Human Factors in Comput-ing Systems (CHI).Terry Winograd.
1972.
Understanding natural lan-guage.
Cognitive Psychology, 3(1):1191.Yezhou Yang, Ching Lik Teo, Hal Daume III, and Yian-nis Aloimonos.
2011.
Corpus-guided sentence gen-eration of natural images.
In Empirical Methods onNatural Language Processing (EMNLP).Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun WaiLee, and Song-Chun Zhu.
2010.
I2t: Image parsingto text description.
Proc.
IEEE, 98(8).798
