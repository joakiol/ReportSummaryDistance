Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 377?384Manchester, August 2008Generating Chinese Couplets using a Statistical MT ApproachLong JiangMicrosoft Research AsiaSigma Center, No.
49, Zhichun RoadHaidian District, Beijing 100190, PRClongj@microsoft.comMing ZhouMicrosoft Research AsiaSigma Center, No.
49, Zhichun RoadHaidian District, Beijing 100190, PRCmingzhou@microsoft.comAbstractPart of the unique cultural heritage ofChina is the game of Chinese couplets(du?li?n).
One person challenges the oth-er person with a sentence (first sentence).The other person then replies with a sen-tence (second sentence) equal in lengthand word segmentation, in a way thatcorresponding words in the two sentencesmatch each other by obeying certain con-straints on semantic, syntactic, and lexi-cal relatedness.
This task is viewed as adifficult problem in AI and has not beenexplored in the research community.In this paper, we regard this task as akind of machine translation process.
Wepresent a phrase-based SMT approach togenerate the second sentence.
First, thesystem takes as input the first sentence,and generates as output an N-best list ofproposed second sentences, using aphrase-based SMT decoder.
Then, a setof filters is used to remove candidates vi-olating linguistic constraints.
Finally, aRanking SVM is applied to rerank thecandidates.
A comprehensive evaluation,using both human judgments and BLEUscores, has been conducted, and the re-sults demonstrate that this approach isvery successful.1 IntroductionChinese antithetical couplets, called ?du?li?n?,form a special type of poetry composed of two?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.sentences.
They use condensed language, buthave deep and sometimes ambivalent meanings.The two sentences making up the couplet arecalled the ?first sentence?
(FS) and the ?secondsentence?
(SS) respectively.Chinese couplets are considered an importantcultural heritage.
A couplet is often written incalligraphy on vertical red banners, and typicallyplaced on either side of a door or in a large hallduring special occasions such as wedding cere-monies and the Chinese New Year.
People alsouse couplets to celebrate birthdays, mark theopenings of a business, and commemorate histor-ical events.
Chinese couplets have also been usedeffectively in teaching Chinese in China.An example of a Chinese couplet is ??
??
?
?
; ?
?
?
?
?
?, where the FS is??
?
?
?
??
and the SS is ??
?
?
???.
It says that the sea is wide enough so thatfish can jump at their pleasure, and the sky ishigh enough so that bird can fly unrestrictedly.The correspondence between individual words ofthe FS and SS is shown here:?
?
?
?
?sea wide allow fish  jump| | | | |?
?
?
?
?sky  high  permit  bird flyFigure 1.
An Example of a Chinese Couplet.Generating the SS of a Chinese couplet giventhe FS can be viewed as a big challenge in AI.As far as we know, there is no previous work totackle this problem.The general process of generating a SS given aFS is like this: for each word in the FS, findsome words that can be used as the counterpartsin the SS; then from the word lattice, select oneword at each position in the SS so that the se-377lected words form a fluent sentence satisfying theconstraints of Chinese couplets.
This process issimilar to translating a source language sentenceinto a target language sentence without word in-sertion, deletion and reordering, but the targetsentence should satisfy some linguistic con-straints.
Based on this observation, we propose amulti-phase statistical machine translation ap-proach to generate the SS.
First, a phrase-basedSMT model is applied to generate an N-best listof SS candidates.
Then, a set of filters based onlinguistic constraints for Chinese couplets is usedto remove low quality candidates.
Finally, aRanking SVM is applied to rerank the candidates.We implemented a web service based on ourapproach (anonymous URL).
A user can input aFS, and our software outputs its top 10 best-scoring SS candidates.
Tens of thousands ofpeople use our service every day.The rest of the paper is organized as follows.In Section 2, we explain the motivation of ourwork.
Then Sections 3 and 4 detail our multi-phase SMT approach for the SS generation.
Theexperimental results and evaluation are reportedin Section 5 and related work on computer poetryis summarized in Section 6.
In Section 7, weconclude our study and point out the future work.2 MotivationChinese couplets vary widely in length.
A shortcouplet could consist of two sentences each con-taining only one or two characters while a longercouplet may reach several hundred characters.However, the length of sentences in most Chi-nese couplets is between 5 and 10 characters.There are also diverse forms of writing couplets.For instance, in one form, the FS and SS are sim-ilar in meaning, while in another, they have tooppose in meaning.However, no matter which form a couplet fol-lows, it generally must conform to the followingconstraints:Constraint 1: The two sentences of a coupletagree in length and word segmentation.
For ex-ample, if a FS contains 7 characters and the firsttwo characters form a word, then the qualified SSshould also contain 7 characters with the firsttwo forming a word.Constraint 2: Tones are generally ?coincidingand harmonious?
: In Chinese, every character ispronounced either ?Ping?
(?)
or ?Ze?
(?).
In aChinese couplet, the character at the end of theFS should be ?Ze?
(pronounced in a sharpdownward tone); the character at the end of theSS should be ?Ping?
(pronounced in a level tone).Constraint 3: Corresponding words in the twosentences should agree in their part of speech andcharacteristics.
For instance, a noun in the SSshould correspond to a noun at the same positionin the FS.
A named entity should correspond to anamed entity.Constraint 4: The contents of the two sen-tences should be related, but not duplicated.Constraint 5: The two sentences should beidentical in their writing styles.
For instance, ifthere is a repetition of words, characters, or pro-nunciations in the FS, the SS should contain anidentical repetition.
And if there is a characterdecomposition in the FS, i.e., the FS contains acharacter and its ?component characters?, the SSshould contain a character decomposition at thecorresponding positions.Character decomposition is an interesting lan-guage phenomenon in Chinese: some Chinesecharacters can be decomposed into other charac-ters.
For example, ???
(good) can be decom-posed into ???
(daughter) and ???
(son).
Asillustrated in Figure 2, the left part of ???
is???
and the right part is ???.
???
and ???
arecalled the ?component characters?
of ???.??
?Figure 2.
Character Decomposition.Compared to western couplets, which alsoconsist of two sentences that usually rhyme andhave the same number of syllables, Chinesecouplets have much stronger constraints.
Be-cause in Chinese each character has one and onlyone syllable, the same number of syllables meansthe same number of characters.
Moreover, theconstraints of the FS and SS on consistency ofpart of speech sequence and writing style makeChinese couplets have more regular form.Given the FS, writing a good SS to match it isa difficult task because the SS must conform toconstraints on syntax, rhyme and semantics, asdescribed above.
It also requires the writer toinnovatively use extensive knowledge in differ-ent disciplines.
Some of the difficulties can beseen from the following example:378?
?
?
?
?
?
?have daughter have son so call good| | | | | | |?
?
?
?
?
?
?lack fish lack mutton dare call deliciousFigure 3.
An Example of a Complicated Couplet.Figure 3 shows a complicated couplet of ???
?
?
?
?
?
; ?
?
?
?
?
?
??
(Onceone has a daughter and son, one?s life is com-plete; who would dare call a meal without fishand mutton delicious?
In China, there is an oldsaying that courses made of fish and mutton aremost delicious).
The FS contains a repeated cha-racter ???
(have), and a character decomposi-tion: ???
(good) and its ?component characters????
(daughter) and ???
(son).
So it requiresthat the qualified FS should contain identicalcharacter repletion and character decomposition.A perfect SS worked out after multiple attemptsby many people for this FS is ??
?
?
?
?
??
?, which equally contains a repeated character???
(lack), and a character decomposition: ???
(fresh) and its ?component characters?
???
(fish)and ???
(mutton) at the corresponding positions.And the meanings of the two sentences are alsoparallel: they tell us what is important in life andwhat is important in cuisine, respectively.3 Couplet Generation ModelIn this paper, a multi-phase SMT approach isdesigned, where an SMT system generates an N-best list of candidates and then a ranking modelis used to determine the new ranking of the N-best results using additional features.
This ap-proach is similar to recent reranking approachesof SMT (Och and Ney, 2004).
In the SMT sys-tem, a phrase-based log-linear model is appliedwhere two phrase translation models, two lexicalweights and a language model are used to scorethe output sentences, and a monotone phrase-based decoder is employed to get the N-best re-sults.
Then a set of filters based on linguisticconstraints of Chinese couplets are used to re-move candidates of low quality.
Finally a Rank-ing SVM model is used to rerank the candidatesusing additional features like word associations,etc.3.1 Phrase-based SMT ModelGiven a FS denoted as },...,,{ 21 nfffF ?
, ourobjective is to seek a SS denoted as},...,,{ 21 nsssS ?
, where fi and si are Chinesecharacters, so that p(S|F) is maximized.Following Och and Ney (2002), we depart fromthe traditional noisy-channel approach and use amore general log-linear model.
Then the S* thatmaximizes p(S|F) can be expressed as follows:????MiiiSSFShFSpS1),(logmaxarg)|(maxarg*?
(1)where the hi(S,F) are feature functions and Mis the number of feature functions.
In our design,characters are used instead of words as transla-tion units to form phrases.
This is because Chi-nese couplets use dense language like traditionalChinese and most of words contain only one cha-racter.
If we try to incorporate Chinese wordsegmentation, it may bring in unexpected errors.However, we will still report the comparison tothe word-based method in Subsection 5.3.Among features commonly used in phrase-based SMT, five features, listed in Table 1, wereselected for our model.
To apply phrase-basedfeatures, S and F are segmented into phrasesIss ...1  and Iff ...1 , respectively.
We assume auniform distribution over all possible segmenta-tions.???
Iiii sfpFSh11 )|(),(Phrase translationmodel???
Iiii fspFSh12 )|(),(Inverted phrasetranslation model???
Iiiiw sfpFSh13 )|(),(Lexical weight???
Iiiiw fspFSh14 )|(),(Inverted lexicalweight)(),(5 SpFSh ?
Language modelTable 1.
Features in our SMT Model.Phrase translation model (PTM)In a phrase-based SMT model, phrases can beany substring that may not necessarily be linguis-tically motivated.
In our implementation, we ex-tract phrases of up to 4-character-grams.In a Chinese couplet, there is generally a directone-to-one mapping between correspondingwords in the FS and SS, respectively.
As a result,the ith character/phrase in F is exactly ?trans-lated?
into the ith character/phrase in S. Based onthis rule, the phrase translation probability)|( ii sfp  can be estimated by relative frequencyon a training corpus:379???
mririiiisfcountsfcountsfp1),(),()|((2)where m is the number of distinct phrases thatcan be mapped to the phrase is  and),( ii sfcount  is the number of occurrences thatif  and is  appear at the corresponding positionsin a couplet.The inverted phrase translation model)|( ii fsp  has been proven useful in previousSMT research work (Och and Ney, 2002); so wealso include it in our phrase-based SMT model.Lexical weight (LW)Previous research work on phrase-based SMThas found that it is important to validate the qual-ity of a phrase translation pair (Koehn et al,2003).
A good way to do this is to check its lexi-cal weight )|( iiw sfp , which indicates how wellits words translate to each other:???
Nijjjiiw sfpsfp1)|()|((3)where Ni is the number of characters inif  oris , jf  and js  are characters in if  and is  respec-tively, and )|( jj sfp  is the character translationprobability of js  into jf .
Like in phrase transla-tion probability estimation, )|( jj sfp  can becomputed by relative frequency:???
mrjrjjjjsfcountsfcountsfp1),(),()|((4)where m is the number of distinct charactersthat can be mapped to the character js  and),( jj sfcount  is the number of occurrences thatjs  and jf  appear at the corresponding positionsin a couplet.Like for the phrase translation model, we alsouse an inverted lexical weight )|( iiw fsp  in addi-tion to the conventional lexical weight )|( iiw sfpin our phrase-based SMT model.Language model (LM)A character-based trigram language model withKatz back-off is constructed from the trainingdata to estimate the language model p(S) usingMaximum Likelihood Estimation.3.2 Model TrainingA Chinese couplet corpus is necessary for esti-mating the phrase and character translation prob-abilities.
Currently, there is, however, no large-sized Chinese couplet collection available.
Basedon our observation, there are many pages on theweb containing classic Chinese couplets collec-tively.
So we used the method proposed by (Fanet al, 2007) to recursively mine those coupletswith the help of some seed couplets.
The methodcan automatically learn patterns in a page whichcontains collectively Chinese couplets and thenapply the learned pattern to extract more Chinesecouplets.
There are also some online forumswhere Chinese couplet fans meet.
When somepeople post FSs on the forums, many otherpeople submit their SSs in response.
Such dataseems useful for our model training.
So wecrawled all posted FSs with all their replied SSs.Then from the crawled data, FSs having over 20unique SSs are selected as development or test-ing set (see Subsection 5.1), and others are usedfor model training.
Finally, with web mining ap-proach, we collected 670,000 couplets.To enhance the couplet database crawled fromthe web, we also mined pairs of sentences ofpoetry which satisfied the constraints of coupletsalthough they were not originally intended ascouplets.
For instance, in eight-sentence Tangpoetry, the third and fourth sentences and thefifth and sixth sentences form pairs basically sa-tisfying the constraints of Chinese couplets.Therefore, these sentence pairs can be used ascouplets in our training algorithm.
In that way weget additional 300,000 sentence pairs yielding atotal of 970,000 sentence pairs of training data.Because the relationships between words andphrases in the FS and SS are usually reversible,to alleviate the data sparseness, we reverse theFS and SS in the training couplets and mergethem with original training data for estimatingtranslation probabilities.For the language people use in Chinese coup-lets is same as that in Chinese poetry, for thepurpose of smoothing the language model weadd about 1,600,000 sentences from ancient Chi-nese poetry to train language model, which arenot necessarily couplets.To estimate the weights ?i in formula (1), weuse Minimum Error Rate Training (MERT) algo-rithm, which is widely used for phrase-basedSMT model training (Och, 2003).
The trainingdata and criteria (BLEU) for MERT will be ex-plained in Subsection 5.1.3804 Couplet GenerationIn this section, we will detail each step of thegeneration of the second sentence.4.1 Decoding for N-best CandidatesFirst, we use a phrase-based decoder similar tothe one by (Koehn et al, 2003) to generate an N-best list of SS candidates.
Because there is noword reordering operation in the SS generation,our decoder is a monotonic decoder.
In addition,the input FS is often shorter than ordinary MTinput sentence, so our decoder is more efficient.4.2 Linguistic FiltersA set of filters is used to remove candidates thatviolate linguistic constraints that well-formedChinese couplets should obey.Repetition filterThis filter removes candidates based on variousrules related to word or character repetition.
Onesuch rule requires that if there are characters thatare identical in the FS, then the correspondingcharacters in the SS should be identical too.
Forexample, in a FS ??
?
?
?
?
?
??
(havedaughter have son so call good), the word ???
isrepeating.
The legal SS should also contain cor-responding repeating words.
For instance, a qual-ified second sentence ??
?
?
?
?
?
??
(lack fish lack mutton dare call delicious) wouldbe legal because ???
corresponds to ???
and isrepeating in the same way.
Conversely, if thereare no identical words in the FS, then the SSshould have no identical words.Pronunciation repetition filterThis filter works similarly to the repetition filterabove except it checks the pronunciation of cha-racters not the character surfaces.
The pronuncia-tion of a character can be looked up from a Chi-nese character pronunciation dictionary.
Forsimplicity, we only use the first pronunciation inthe dictionary for polyphones.Character decomposition filterWe compiled a Chinese character decompositiontable from which one can look up what charac-ters a Chinese character can be decomposed into.The decomposition information can be derivedfrom the strokes of each character in a dictionaryand then verified by human.
Based on this table,we can easily filter out those SS candidateswhich contain different character decompositionsat the corresponding positions from the FS.Phonetic harmony filterWe filter out the SSs with improper tones at theend character position according to the Chinesecharacter pronunciation dictionary.4.3 Reranking Based on Multiple FeaturesIn many cases, long-distance constraints are veryhelpful in selecting good SSs, however, it is dif-ficult to incorporate them in the framework ofdynamic programming decoding algorithm.
Tosolve this issue, we designed an SVM-based re-ranking model incorporating long-distance fea-tures to select better candidates.As shown in formula (5), x?
is the feature vec-tor of a SS candidate, and w?
is the vector ofweights.
???
?,  stands for an inner product.
f is thedecision function with which we rank the candi-dates.???
xwxfw ????
,)(  (5)Besides the five features used in the phrase-based SMT model, additional features for rerank-ing are as follows:1.
Mutual information (MI) score:This feature is designed to measure the semanticconsistency of words in a SS candidate.
For ex-ample, the two candidates ??
?
?
?
??
(skyhigh permit bird fly) and ??
?
?
?
??
(skyhigh permit dog bark) have similar PTM, LWand LM scores.
However, human beings recog-nize the former as a better phrase, because ????
(sky high) and ??
??
(dog bark) in the lat-ter sentence do not make any sense together.
MIcan capture the associations between words,whether they are adjacent or not.Specifically, given a SS candidate},...,,{ 21 nsssS ?
, we use the following formula tocompute the MI score:?
???????
??????
1111 11 )()(),(log),()( nininij jijinijji spspsspssISMI(6)The parameters p(si,sj), p(si) and p(sj) are esti-mated using Maximum Likelihood Estimation onthe same training data as for training PTM.2.
MI-based structural similarity (MISS) score:In a Chinese couplet, if two words in the FS arestrongly associated, their corresponding words inthe SS should also be strongly associated, andvice versa.
For example, in the couplet ??
?
??
?
; ?
?
?
?
??
(sea wide allow fish jump;sky high permit bird fly), the word pairs ??
?381(sea) and ???
(wide), ???
(sea) and ???
(fish),???
(fish) and ???
(jump) in the FS are allstrongly associated.
Similarly, the correspondingword pairs ???
(sky) and ???
(high), ???
(sky)and ???
(bird), ???
(bird) and ???
(fly) in theSS are all strongly associated.
To measure thiskind of structural similarity, we develop a meas-ure function called MI-based structural similarityscore.
Specifically, given the FS},...,,{ 21 nfffF ?
, we first build its vector}.,,,..,,{ 12311312 nnnf vvvvvV ??
, where vij is the mu-tual information of fi and fj (i.e., ),( ji ffI  in for-mula (6)).
Then we build a vector Vs for each SScandidate in the same way.
We use a cosinefunction to compute the similarity between thetwo vectors as the MISS score:||||),cos(),( sfsfsf VVVVVVSFMISS ????
(7)To estimate the parameter vector in the Rank-ing SVM model, we used an existing trainingtool, SVM Light2, and a labeled training corpus.We selected 200 FSs with a length of 7 or 8 cha-racters.
For each of them, 50 SS candidates aregenerated using the N-best SMT decoder.
Twooperators are asked to label each SS candidate aspositive if the candidate is acceptable and asnegative if not.
After removing 10 FSs and theirSS candidates as they had no corresponding posi-tive SS, we got 190 FSs with 9,500 labeled SScandidates (negative: 6,728; positive: 2,772) totrain the Ranking SVM model.5 Experimental Results5.1 Evaluation MethodAutomatic evaluation is very important forparameter estimation and system tuning.
Anautomatic evaluation needs a standard answerdata set and a metric to show for a given inputsentence the closeness of the system output to thestandard answers.
Since generating the SS giventhe FS can be viewed as a kind of machinetranslation process, the widely acceptedautomatic SMT evaluation methods may beapplied to evaluate the generated SSs.BLEU (Papineni, et al 2002) is widely usedfor automatic evaluation of machine translationsystems.
It measures the similarity between theMT system output and human-made referencetranslations.
The BLEU metric ranges from 0 to2 http://svmlight.joachims.org/1 and a higher BLEU score stands for bettertranslation quality.)logexp(1????
Nnnn pwBPBLEU(8)Some adaptation is necessary to use BLEU forevaluation of our couplet generator.
First, pn, then-gram precision, should be position-sensitive inthe evaluation of SSs.
Second, BP, the brevitypenalty, should be removed, because all systemoutputs have the same length and it has no effectin evaluating SSs.
Moreover, because the coupletsentences usually have less than 10 characters,we set n to 3 for the evaluation of SSs, while inMT evaluation n is often set to 4.It is important to note that the more referencetranslations we have for a testing sentence, themore reasonable the evaluation score is.
Fromcouplet forums mentioned in Subsection 3.2, wecollected 1,051 FSs with diverse styles and eachof them has over 20 unique SS references.
Afterremoving some noisy references by human, eachof them has 24.3 references on average.
The min-imum and maximum number of references is 20and 40.
Out of these data, 600 were selected forMERT training and the remaining 451 for testing.5.2 BLEU vs. Human EvaluationTo justify whether BLEU is suitable for evaluat-ing generated SSs, we compare BLEU with hu-man evaluation.
Figure 4 shows a linear regres-sion of the human evaluation scores as a functionof the BLEU score for our 6 systems which gen-erate SSs given FSs.
Among the 6 systems, threeare implemented using a word-based SMT modelwith 100K, 400K, and 970K couplets for training,respectively, while the other three are imple-mented using a phrase-based SMT model with100K, 400K, and 970K couplets for training, re-spectively.
The word-based SMT model containsonly two features: word translation model andlanguage model.
The word translation model istrained on the corpus segmented by a Chineseword breaker implemented by (Gao et al, 2003).We selected 100 FSs from the testing data set;for each of them, the best SS candidate was gen-erated using each system.
Then we computed theBLEU score and the human score of each system.The human score is the average score of all SScandidates.
Each candidate is scored 1 if it is ac-ceptable, and 0 if not.
The correlation of 0.92indicates that BLEU tracks human judgment well.382Figure 4: BLEU Predicts Human Judgments.5.3 Translation Unit SettingWe conducted some experiments to compare thesystem performances with different translationunit settings: character-based, word-based andphrase-based.
In each setting, we only use trans-lation probability and language model as features.And after SMT decoder, we use the same filter-ing but no reranking.
We use all 451 testing dataand the results are listed below:Translation Unit setting BLEUcharacter-based 0.236word-based 0.261phrase-based 0.276Table 2.
Different Translation Unit Setting.As shown in Table 2, the word-based transla-tion model achieves 0.025 higher of BLEU scorethan the character-based model.
And the phrase-based model gets the highest score.
The im-provement shows that phrase-based model worksbetter than word-based and character-based mod-el in our task of SS generation.5.4 Feature EvaluationWe also conducted some experiments incremen-tally to evaluate the features used in our phrase-based SMT model and reranking model.
All test-ing data are used.
The results are listed below.Features BLEUPhrase-basedSMTModelPhrase TM(PTM) + LM 0.276+ Inverted PTM 0.282+ Lexical Weight (LW) 0.315+ Inverted LW 0.348RankingSVM+ Mutual information (MI) 0.356+ MI-based structuralsimilarity0.361Table 3.
Feature Evaluation.As shown in Table 3, with two features: thephrase translation model and the language model,the phrase-based SMT model can achieve a0.276 of BLEU score.
When we add more fea-tures incrementally, the BLEU score is improvedconsistently.
Furthermore, with the RankingSVM model, the score is improved by 0.13 per-cent, from 0.348 to 0.361.
This means our re-ranking model is helpful.5.5 Overall Performance EvaluationIn addition to the BLEU evaluation, we also car-ried out human evaluation.
We select 100 FSsfrom the log data of our couplet web servicementioned in Section 1.
For each FS, 10 best SScandidates are generated using our best system.Then each SS candidate is labeled by human asacceptable or not.
The evaluation is carried outusing top-1 and top-10 results based on top-ninclusion rate.
Top-n inclusion rate is defined asthe percentage of the test sentences whose top-noutputs contain at least one acceptable SS.
Theresults are listed below:Top-1 Top-10Top-n inclusion rate 0.21 0.73Table 4.
Overall Performance Evaluation.As shown in Table 4, our system can get a0.21 of top-1 inclusion rate and 0.73 of top-10inclusion rate.
The numbers seem a little low, butremember that generating a SS given a FS is aquite difficult job, and even humans cannot do itwell in limit time, for example, 5 minute per FS.However, what is more important is that our sys-tem can provide users diversified SSs and manyunacceptable SSs generated by our system can beeasily refined by users to become acceptable.We also made careful analysis on the 27 FSswhose top-10 outputs contain no acceptable SS.As shown in Table 5, the errors mainly comefrom three aspects: unidentified named entity,complicated character decomposition and repeti-tion.
An example of complicated repetition is ???
??
??
???
(modern /scholar/all/myopic,modern scholars are all myopic).
In this sentence,the pronunciations of the four words are identical(j?nsh?
), a qualified SS must be meaningful andposses same repetitions, which poses a big chal-lenge to the system.Mistake types # of FSMistakes with named entities 6Complicated character decomposition 5Complicated repetition 4Mistakes of miscellaneous types 12Table 5.
Error Analysis.3836 Related WorkTo the best of our knowledge, no research hasbeen published on generating the SS given theFS of a Chinese couplet.
However, because ourtask can be viewed as generating the second lineof a special type of poetry given the first line, weconsider automatic poetry generation to be themost closely related existing research area.As to computer-assisted Chinese poetry gener-ation, Luo has developed a tool3 which providesthe rhyme templates of forms of classical Chi-nese poetry and a dictionary in which one canlook up the tone of a Chinese character.
Both therhyme templates and the dictionary were com-piled by human efforts.For other languages, approaches to creatingpoetry with computers began in 1959 when TheoLutz created the first examples of ?ComputerPoetry?
in Germany (Hartman, 1996).
Master-man finished a haiku producer (Manurung et al,2000).
Other systems include RACTER andPROSE (Hartman, 1996).
Approaches to poetrygeneration can roughly be classified into tem-plate-based, evolutionary, and case-based reason-ing.
Typically, for the template-based approach,the generation process randomly chooses wordsfrom a hand-crafted lexicon and then fills in thegaps provided by a template-based grammar.
Incomputer poetry systems, the starting point is agiven message, or communicative goal, and theaim is to produce a string of text that conveysthat message according to the linguistic resourcesavailable.There is a big difference between our task andpoetry generation.
When generating the SS of aChinese couplet, the FS is given.
The task of ge-nerating the SS to match the FS is more well-defined than generating all sentences of a poem.Furthermore, the constraints on Chinese coupletsmentioned above will enable us to do a more ob-jective evaluation of the generated SSs.7 Conclusions and Future WorkThis paper presents a novel approach to solve theproblem of generating Chinese couplets.
AnSMT approach is proposed to generate the SSsfor a FS of a Chinese couplet.
The system iscomprised of a phrase-based SMT model for thegeneration of an N-best list of SS candidates, aset of linguistic filters to remove unqualifiedcandidates to meet the special constraints of Chi-nese couplets, and a discriminative reranking3 http://cls.hs.yzu.edu.twmodel incorporating multi-dimensional featuresto get better results.
The experimental resultsshow that this approach is very promising.As a future work, it would be interesting to in-vestigate how this approach can be used in poe-try generation.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Compu-tational Linguistics, 19:2, 263-311.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.of the 43rd Meeting of the Association for Compu-tational Linguistics, pages 263-270.B.
D?az-Agudo, P. Gerv?s and P. Gonz?lez-Calero.2002.
Poetry generation in COLIBRI.
In Proc.
ofthe 6th European Conference on Case Based Rea-soning, Aberdeen, Scotland.C.
Fan, L. Jiang, M. Zhou, S.-L. Wang.
2007.
MiningCollective Pair Data from the Web.
In Proc.
of theInternational Conference on Machine Learning andCybernetics 2007, pages 3997-4002.Jianfeng Gao, Mu Li and Changning Huang.
2003.Improved source-channel models for Chinese wordsegmentation.
In Proc.
of the 41st Meeting of theAssociation for Computational Linguistics.Charles O. Hartman.
1996.
Virtual Muse: Experi-ments in Computer Poetry.
Wesleyan UniversityPress.P.
Koehn, F. J. Och and D. Marcu.
2003.
Statisticalphrase-based translation, In HLT-NAACL 2003,pages 48-54.H.
Manurung, G. Ritchie and H. Thompson.
2001.Towards a computational model of poetry genera-tion.
In Proc.
of the AISB-00 Symposium on Crea-tive and Cultural Aspects of AI, 2001.F.
J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
of the 41st Meet-ing of the Association for Computational Linguis-tics.F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical ma-chine translation.
In Proc.
of the 40th Meeting ofthe Association for Computational Linguistics.F.
J. Och and H. Ney.
2004.
The Alignment TemplateApproach to Statistical Machine Translation.
Com-putational Linguistics, 30:417-449.K.
Papineni, S. Roukos, T. Ward and W.-J.
Zhu.
2002.BLEU: a Method for automatic evaluation of ma-chine translation.
In Proc.
of the 40th Meeting ofthe Association for Computational Linguistics.384
