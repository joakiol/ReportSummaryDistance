Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 37?43,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsGenerating Semantic Orientation Lexicon using Large Data and ThesaurusAmit Goyal and Hal Daume?
IIIDept.
of Computer ScienceUniversity of MarylandCollege Park, MD 20742{amit,hal}@umiacs.umd.eduAbstractWe propose a novel method to construct se-mantic orientation lexicons using large dataand a thesaurus.
To deal with large data, weuse Count-Min sketch to store the approxi-mate counts of all word pairs in a boundedspace of 8GB.
We use a thesaurus (like Roget)to constrain near-synonymous words to havethe same polarity.
This framework can easilyscale to any language with a thesaurus and aunzipped corpus size ?
50 GB (12 billion to-kens).
We evaluate these lexicons intrinsicallyand extrinsically, and they perform compara-ble when compared to other existing lexicons.1 IntroductionIn recent years, the field of natural language process-ing (NLP) has seen tremendous growth and inter-est in the computational analysis of emotions, sen-timents, and opinions.
This work has focused onmany application areas, such as sentiment analy-sis of consumer reviews e.g., (Pang et al, 2002;Nasukawa and Yi, 2003), product reputation anal-ysis e.g., (Morinaga et al, 2002; Nasukawa and Yi,2003), tracking sentiments toward events e.g., (Dasand Chen, 2001; Tong, 2001), and automaticallyproducing plot unit representations e.g., (Goyal etal., 2010b).
An important resource in accomplishingthe above tasks is a list of words with semantic ori-entation (SO): positive or negative.
The goal of thiswork is to automatically create such a list of wordsusing large data and a thesaurus structure.For this purpose, we store exact counts of allthe words in a hash table and use Count-Min (CM)sketch (Cormode and Muthukrishnan, 2004; Goyalet al, 2010) to store the approximate counts of allword pairs for a large corpus in a bounded space of8GB.
(Storing the counts of all word pairs is compu-tationally expensive and memory intensive on largedata (Agirre et al, 2009; Pantel et al, 2009)).
Stor-age space saving in CM sketch is achieved by ap-proximating the frequency of word pairs in the cor-pus without explicitly storing the word pairs them-selves.
Both updating (adding a new word pair orincreasing the frequency of existing word pair) andquerying (finding the frequency of a given wordpair) are constant time operations making it an ef-ficient online storage data structure for large data.Once we have these counts, we find semanticorientation (SO) (Turney and Littman, 2003) of aword using its association strength with positive(e.g.
good, and nice) and negative (e.g., bad andnasty) seeds.
Next, we make use of a thesaurus (likeRoget) structure in which near-synonymous wordsappear in a single group.
We compute the SO ofthe whole group by computing SO of each individ-ual word in the group and assign that SO to all thewords in the group.
The hypothesis is that nearsynonym words should have similar polarity.
How-ever, similar words in a group can still have differ-ent connotations.
For example, one group has ?slen-der?, ?slim?, ?wiry?
and ?lanky?.
One can arguethat, first two words have positive connotation andlast two have negative.
To remove these ambigu-ous words errors from the lexicon, we discard thosewords which have conflicting SO compared to theirgroup SO.
The idea behind using thesaurus struc-ture is motivated from the idea of using number ofpositive and negative seed words (Mohammad et al,2009) in thesaurus group to determine the polarityof words in the group.In our experiments, we show the effectiveness ofthe lexicons created using large data and freely avail-37able thesaurus both intrinsically and extrinsically.2 Background2.1 Related WorkThe literature on sentiment lexicon induction can bebroadly classified into three categories: (1) Corporabased, (2) using thesaurus structure, and (3) com-bination of (1) and (2).
Pang and Lee (2008) pro-vide an excellent survey on the literature of sen-timent analysis.
We briefly discuss some of theworks which have motivated our research for thiswork.
A web-derived lexicon (Velikovich et al,2010) was constructed for all words and phrases us-ing graph propagation algorithm which propagatespolarity from seed words to all other words.
Thegraph was constructed using distributional similar-ity between the words.
The goal of their work wasto create a high coverage lexicon.
In a similar direc-tion (Rao and Ravichandran, 2009), word-net wasused to construct the graph for label propagation.Our work is most closely related to Mohammad etal.
(2009) which exploits thesaurus structure to de-termine the polarity of words in the thesaurus group.2.2 Semantic OrientationWe use (Turney and Littman, 2003) framework toinfer the Semantic Orientation (SO) of a word.
Wetake the seven positive words (good, nice, excellent,positive, fortunate, correct, and superior) and theseven negative words (bad, nasty, poor, negative, un-fortunate, wrong, and inferior) used in (Turney andLittman, 2003) work.
The SO of a given word iscalculated based on the strength of its associationwith the seven positive words, and the strength ofits association with the seven negative words usingpointwise mutual information (PMI).
We computethe SO of a word ?w?
as follows:SO(w) =?p?PwordsPMI(p, w)?
?n?NwordsPMI(n,w)where, Pwords and Nwords denote the seven pos-itive and seven negative prototype words respec-tively.
If this score is negative, the word is predictedas negative.
Otherwise, it is predicted as positive.2.3 CM sketchThe Count-Min sketch (Cormode and Muthukrish-nan, 2004) with user chosen parameters (,?)
isrepresented by a two-dimensional array with widthw and depth d. Parameters  and ?
control theamount of tolerable error in the returned count ()and the probability with which the returned countis not within this acceptable error (?)
respectively.These parameters determine the width and depthof the two-dimensional array.
We set w=2 , andd=log(1?
).
The depth d denotes the number ofpairwise-independent hash functions and there ex-ists an one-to-one mapping between the rows andthe set of hash functions.
Each of these hash func-tions hk:{x1 .
.
.
xN} ?
{1 .
.
.
w}, 1 ?
k ?
d, takesan item from the input stream and maps it into acounter indexed by the corresponding hash function.For example, h3(x) = 8 indicates that the item ?x?is mapped to the 8th position in the third row of thesketch.Update Procedure: When a new item ?x?
withcount c, the sketch is updated by:sketch[k,hk(x)]?
sketch[k,hk(x)] + c, ?1 ?
k ?
dQuery Procedure: Since multiple items can behashed to the same position, the stored frequency inany one row is guaranteed to overestimate the truecount.
Thus, to answer the point query, we returnthe minimum over all the positions indexed by thek hash functions.
The answer to Query(x) is: c?
=mink sketch[k, hk(x)].2.4 CU sketchThe Count-Min sketch with conservative update(CU sketch) (Goyal et al, 2010) is similar to CMsketch except the update operation.
It is based onthe idea of conservative update (Estan and Vargh-ese, 2002) introduced in the context of networking.It is used with CM sketch to further improve the es-timate of a point query.
To update an item, x withfrequency c, we first compute the frequency c?
of thisitem from the existing data structure and the countsare updated according to:c?
= mink sketch[k,hk(x)], ?1 ?
k ?
dsketch[k,hk(x)]?
max{sketch[k,hk(x)], c?+ c}The intuition is that, since the point query returnsthe minimum of all the d values, we will update acounter only if it is necessary as indicated by theabove equation.383 Generating Polarity LexiconOur framework to generate lexicon has three mainsteps: First, we compute Semantic Orientation (SO)of words using a formula defined in Section 2.2 us-ing a large corpus.
Second, we use a thesaurus (likeRoget) to constrain all synonym words in a groupto have the same polarity.
Third, we discard wordswhich do not follow the above constraints.
The threesteps are discussed in the following subsections.3.1 Computing SO of a wordWe use CM sketch to store counts of word pairs (ex-cept word pairs involving stop words and numbers)within a sliding window of size1 7 using a large cor-pus: GWB66 of size 64GB (see Section 4.3).
Wefix the number of counters of the sketch to 2 bil-lion (2B) (8GB of memory) with conservative up-date (CU) as it performs the best for (Goyal et al,2010) with d = 5 (see Section 2.3) hash functions.We store exact counts of words in hash table.Once, we have stored the counts for all words andword pairs, we can compute the SO of a word usinga formula defined in Section 2.2.
Moreover, a wordcan have multiple senses, hence it can belong to mul-tiple paragraphs.
To assign a single label to a word,we combine all its SO scores.
We use positive SOscores to label words as positive and negative SO tolabel words as negative.
We discard words with SOequal to zero.
We apply this strategy to all the wordsin a thesaurus (like Roget) (refer to Section 3.2), wecall the lexicon constructed using SO scores usingthesaurus words as ?SO?
lexicon.3.2 Using Thesaurus structureThesaurus like Roget2, Macquarie are available inseveral languages.
We use freely available versionof Roget thesaurus which has 1046 categories, eachcontaining on average 64 words and phrases.
Termswithin a category are closely related to one another,and they are further grouped into near-synonymouswords and phrases called paragraphs.
There areabout 3117 paragraphs in Roget thesaurus.
Oneof the examples of paragraphs from the Roget the-saurus is shown in Table 1.
All the words appears tobe near-synonymous with positive polarity.1Window size 7 is chosen from intuition and not tuned.2http://www.nzdl.org/ELKB/pure undefiled modest delicate decent decorous cherry chastecontinent virtuous honest platonic virgin unsullied simonpureTable 1: A paragraph from the Roget thesaurusWe assign semantic orientation (SO) score to athesaurus paragraph3 (SO(TP )) by averaging overSO scores over all the words in it.
The SO(TP )score constrains all the words in a paragraph to havesame polarity.
If SO(TP ) > 0, all the words in aparagraph are marked as positive.
If SO(TP ) < 0,all the words in a group are marked as negative.
ForSO(TP ) = 0, we discard all the words of a para-graph.
For the paragraph in Table 1, the SO(TP )for the paragraph is 8.72.
Therefore, all the words inthis paragraph are labeled as positive.
However, theSO scores for ?virgin?
and ?decorous?
are negative,therefore they are marked as negative by previouslexicon ?SO?, however they seem to be more pos-itive than negative.
Therefore, using the structureof the lexicon helps us in correcting the polarity ofthese words to negative.
We apply this strategy to allthe 3117 Roget thesaurus paragraphs and construct?SO-TP?
lexicon using SO(TP ) scores.3.3 Words and Thesaurus ConsensusSince near-synonymous words could have differentconnotation or polarity.
Hence, here we use bothSO of word and SO(TP ) of its paragraph to assignpolarity to a word.
If SO(w) > 0 and SO(TP ) >0, then we mark that word as positive.
If SO(w) <0 and SO(TP ) < 0, then we mark that word asnegative.
In other cases, we discard the word.We refer to the lexicon constructed using thisstrategy on Roget thesaurus paragraphs as ?SO-WTP?
lexicon.
The motivation behind this is to gen-erate precision orientated lexicon by having consen-sus over both individual and paragraph scores.
Forthe paragraph in Table 1, we discard words ?virgin?and ?decorous?
from the lexicon, as they have con-flicting SO(w) and SO(TP ) scores.
In experimentsin Section 5.2.1, we also examine existing lexiconsto constrain the polarity of thesaurus paragraphs.4 Evaluating SO computed using sketchWe compare the accuracy of computed SO using dif-ferent sized corpora.
We also compare exact countswith approximate counts using sketch.3We do not assign polarity to phrases and stop words.394.1 DataWe use Gigaword corpus (Graff, 2003) and a 66%portion of a copy of web crawled by (Ravichan-dran et al, 2005).
For both the corpora, we splitthe text into sentences, tokenize and convert intolower-case.
We generate words and word pairs overa sliding window of size 7.
We use four differentsized corpora: Gigaword (GW), GigaWord + 16%of web data (GWB16), GigaWord + 50% of webdata (GWB50), and GigaWord + 66% of web data(GWB66).
Corpus Statistics are shown in Table 2.We store exact counts of words in a hash table andstore approximate counts of word pairs in the sketch.4.2 Test SetWe use General Inquirer lexicon4 (Stone et al, 1966)as a benchmark to evaluate the semantic orientationscores similar to (Turney and Littman, 2003) work.Our test set consists of 1597 positive and 1980 nega-tive words.
Accuracy is used as an evaluation metric.Corpus GW GWB16 GWB50 GWB66Unzipped9.8 22.8 49 64Size (GB)# of sentences56.78 191.28 462.60 608.74(Million)# of Tokens1.8 4.2 9.1 11.8(Billion)Stream Size2.67 6.05 13.20 17.31(Billion)Table 2: Corpus Description4.3 Effect of Increasing Corpus SizeWe evaluate SO of words on four different sizedcorpora (see Section 4.1): GW (9.8GB), GWB20(22.8GB), GWB50 (49GB) and GWB66 (64GB).First, we will fix number of counters to 2 billion(2B) (CU-2B) as it performs the best for (Goyalet al, 2010).
Second, we will compare the CU-2Bmodel with the Exact over increasing corpus size.We can make several observations from the Fig-ure 1: ?
It shows that increasing the amount of dataimproves the accuracy of identifying the SO of aword.
We get an absolute increase of 5.5 pointsin accuracy when we add 16% Web data to Giga-Word (GW).
Adding 34% more Web data (GWB50),gives a small increase of 1.3 points.
Adding 16%4The General Inquirer lexicon which is freely available athttp://www.wjh.harvard.edu/?inquirer/more Web data (GWB66), give an increase of 0.5points.
?
Second, CU-2B performs as good as Ex-act.
?
These results are also comparable to Turney?s(2003) state-of-the-art work where they report an ac-curacy of 82.84%.
Note, they use a 100 billion to-kens corpus which is larger than GWB66 (12 billiontokens).This experiments shows that using unzipped cor-pus size ?
50 GB (12 billion tokens), we get per-formance comparable to the state-of-the-art.
Hence,this approach is applicable for any language whichhas large collection of monolingual data availablein it.
Note that these results compared to best re-sults of (Goyal et al, 2010) that is 77.11 are 4.5points better; however in their work their goal wasto show their approach scales to large data.
We sus-pect the difference in results is due to difference inpre-processing and choosing the window size.
Weused counts from GWB66 (64GB) to generate lexi-cons in Section 3.0 10GB 20GB 30GB 40GB 50GB 60GB 70GB727476788082Corpus SizeAccuracyCU?2BExactFigure 1: Evaluating Semantic Orientation of words with Ex-act and CU counts with increase in corpus size5 Lexicon evaluationWe evaluate the lexicons proposed in Section 3both intrinsically (by comparing their lexicon en-tries against General Inquirer (GI) lexicon) and ex-trinsically (by using them in a phrase polarity anno-tation task).
We remove stop words and phrases forcomparison from existing lexicons as our frameworkdoes not assign polarity to them.5.1 Intrinsic evaluationWe compare the lexicon entries of ?SO?, ?SO-TP?
,and ?SO-WTP?
against entries of GI Lexicon.
Thisevaluation is similarly used by other authors (Tur-ney and Littman, 2003; Mohammad et al, 2009) toevaluate sentiment lexicons.Table 3 shows the percentage of GI positive (Pos),negative (Neg) and all (All) lexicon entries that40Lexicon (size) Pos (1597) Neg (1980) All (3577)SO (32.2K) 0.79 0.73 0.76S0-TP (33.1K) 0.88 0.64 0.75SO-WTP (22.6K) 0.78 0.65 0.71Roget-ASL (27.8K) 0.79 0.40 0.57Table 3: The percentage of GI entries (positive, negative, andall) that match those of the automatically generated lexiconsmatch the proposed lexicons.
The recall of our pre-cision orientated lexicon SO-WTP is only 5 and4 % less compared to SO and SO-TP respectivelywhich are more recall oriented.
We evaluate theselexicons against Roget-ASL (discussed in Section5.2.1).
Even, Our SO-WTP precision oriented lexi-con has more recall than Roget-ASL.5.2 Extrinsic evaluationIn this section, we compare the effectiveness of ourlexicons on a task of phrase polarity identification.We use the MPQA corpus which contains news ar-ticles from a wide variety of news sources manuallyannotated for opinions and other private states (likebeliefs, emotions, sentiments, speculations, etc.
).Moreover, it has polarity annotations (positive/neg-ative) at the phrase level.
We use MPQA5 version2.0 collection of 2789 positive and 6079 negativephrases.
We perform an extrinsic evaluation of ourautomatic generated lexicons (using large data andthesaurus) against existing automated and manuallygenerated lexicons by using them to automaticallydetermine the phrase polarity.
This experimentalsetup is similar to Mohammad et al (2009).
How-ever, in their work, they used MPQA version 1.0.We use a similar algorithm as used by Mohammadet al (2009) to determine the polarity of the phrase.If any of the words in the target phrase is labeled inthe lexicon as having negative SO, then the phrase ismarked as negative.
If there are no negative words inthe target phrase and it contains one or more positivewords, then the phrase is marked as positive.
In allother cases, do not assign any tag.The only difference with respect to Mohammad etal.
(2009) is that we use a list of 58 negation wordsused in OpinionFinder6 (Wilson et al, 2005b) (Ver-sion 1.4) to flip the polarity of a phrase if it containsodd number of negation words.
We can get better5http://www.cs.pitt.edu/mpqa/databaserelease/6www.cs.pitt.edu/mpqa/opinionfinderreleaseLexicon # of positives # of negatives # of allGI 1597 1980 3577MPQA 2666 4888 7554ASL 2320 2616 4936Roget (ASL) 21637 6161 27798Roget (GI) 10804 16319 27123Roget (ASL+GI) 16168 12530 28698MSOL 22088 32712 54800SO 16620 15582 32202SO-TP 22959 10117 33076SO-WTP 14357 8257 22614SO+GI 8629 9936 18565SO-TP+GI 12049 9317 21366Table 4: Summarizes all lexicons sizeaccuracies on phrase polarity identification using su-pervised classifiers (Wilson et al, 2005a).
However,the goal of this work is only to show the effective-ness of large data and thesaurus learned lexicons.5.2.1 BaselinesWe compare our method against the followingbaselines: First, MPQA Lexicon7 ((Wilson et al,2005a)).
Second, we use Affix seed lexicon (ASL)seeds used by Mohammad et al (2009) to assignlabels to Roget thesaurus paragraphs.
ASL wasconstructed using 11 affix patterns, e.g.
honest-dishonest (X-disX pattern).
If ASL matches morepositive words than negative words in a paragraphthen all the words in the paragraph are labeled aspositive.
However, if ASL matches more negativewords than positive words in a paragraph, then allwords in the paragraph are labeled as negative.
Forother cases, we do not assign any labels.
The gen-erated lexicon is referred as Roget (ASL).
Third, weuse GI Lexicon instead of ASL and generate Roget(GI) Lexicon.
Fourth, we use ASL + GI, and gen-erate Roget (ASL+GI) Lexicon.
Fifth, MSOL8 gen-erated by Mohammad et al (2009) using ASL+GIlexicon on Macquarie Thesaurus.
Note that Mac-quarie Thesaurus is not freely available and its sizeis larger than the freely available Roget?s thesaurus.5.2.2 GI seeds information with SO LexiconWe combine the GI seed lexicon with seman-tic orientation of word computed using large cor-pus to mark the words positive or negative in the-saurus paragraphs.
We combine the information7www.cs.pitt.edu/mpqa/lexiconrelease/collectinfo1.html8http://www.umiacs.umd.edu/?saif/Release/MSOL-June15-09.txt41Polarity + (2789) - (6079) All (8868)SO Lexicon R P F R P F R P FMPQA .48 .73 .58 .48 .95 .64 .48 .87 .62Roget (ASL) .64 .45 .53 .32 .90 .47 .42 .60 .49Roget (GI) .50 .60 .55 .55 .86 .67 .53 .76 .62Roget (ASL+GI) .62 .57 .59 .49 .91 .64 .53 .75 .62MSOL .51 .58 .54 .60 .84 .70 .57 .74 .64SO .63 .54 .58 .50 .90 .64 .54 .73 .62SO-TP .68 .51 .58 .44 .93 .60 .52 .69 .59SO-WTP .65 .54 .59 .44 .93 60 .51 .72 .60SO+GI .60 .57 .58 .46 .93 .62 .50 .75 .60SO-TP+GI .62 .58 .60 .45 .93 .61 .51 .76 .61Table 5: Results on marking polarity of phrases using variouslexicons.
The # in parentheses is the # of gold +/-/all phrases.from large corpus with GI in two forms: ?
SO+GI:If GI matches more number of positive words thannegative words in a paragraph and SO of a word> 0, then that word is labeled as positive.
However,if GI matches more number of negative words thanpositive words in a paragraph and SO of a word< 0,that word is labeled as negative.
For other cases,we do not assign any labels to words.
?
SO-TP+GI:Here, we use SO(TP ) scores instead of SO scoresand use the same strategy as in previous bullet togenerate the lexicon.Table 4 summarizes the size of all lexicons.MPQA has the largest size among manually createdlexicons.
It is build on top of GI Lexicon.
Ro-get (ASL) has 78% positive entries.
MSOL is thebiggest lexicon and it is about 2.5 times bigger thanour precision oriented SO-WTP lexicon.5.2.3 ResultsTable 5 demonstrates the performance of the algo-rithm (discussed in Section 5.2) when using differentlexicons.
The performance of existing lexicons isshown in the top part of the table.
The performanceof large data and thesaurus lexicons is shown in themiddle of the table.
The bottom of the table com-bines GI information with large data and thesaurus.In the first part of the Table 5, our results demon-strate that MPQA in the first row of the table has thebest precision on this task for both positive and neg-ative phrases.
Roget (ASL) in the second row hasthe best recall for positives which is double the re-call for negatives.
Hence, this indicates that ASL isbiased towards positive words.
Using GI with Ro-get gives more balanced recall for both positives andnegatives in third row.
Roget (ASL+GI) are morebiased towards positive words.
MSOL has the bestrecall for negatives; however it comes at an expenseof equal drop in precision with respect to MPQA.In the second part of the Table using large data,?SO?
lexicon has same F-score as MPQA with pre-cision and recall trade-offs.
Using thesaurus alongwith large data has comparable F-score; however itagain gives some precision and recall trade-offs withnoticeable 6 points drop in recall for negatives.
Thesmall decrease in F-score for SO-WTP precision-oriented lexicon (22, 614 entries) is due to its smallsize in comparison to SO lexicon (32, 202 entries).We are currently working with a small sized freelyavailable thesaurus which is smaller than Macquarie,hence MSOL performs the best.Using GI lexicon in bottom part of the Table, weincorporate another form of information, which pro-vides overall better precision than SO, SO-TP, andSO-WTP approaches.
Even for languages, wherewe have only large amounts of data available, ?SO?can be beneficial.
If we have thesaurus available fora language, it can be combined with large data toproduce precision oriented lexicons.6 Discussion and ConclusionWe constructed lexicons automatically using largedata and a thesaurus and evaluated its quality bothintrinsically and extrinsically.
This framework caneasily scale to any language with a thesaurus anda unzipped corpus size of ?
50 GB (12 billion to-kens).
However, if a language does not have the-saurus, word similarity between words can be usedto generate word clusters.
Currently we are explor-ing using word clusters instead of using thesaurusin our framework.
Moreover, if a language doesnot have large collection of data, we like to explorebilingual lexicons to compute semantic orientationof a word in another language.
Another promisingdirection would be to explore the idea of word simi-larity combined with CM sketch (stores the approx-imate counts of all word pairs in a bounded space of8GB) in graph propagation setting without explicitlyrepresenting the graph structure between words.AcknowledgmentsWe thank the anonymous reviewers for helpful com-ments.
This work is partially funded by NSF grantIIS-0712764 and Google Research Grant Grant forLarge-Data NLP.42ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In NAACL ?09: Pro-ceedings of HLT-NAACL.Graham Cormode and S. Muthukrishnan.
2004.
An im-proved data stream summary: The count-min sketchand its applications.
J. Algorithms.S.
R. Das and M. Y. Chen.
2001.
Yahoo!
for Ama-zon: Opinion extraction from small talk on the Web.In Proceedings of the 8th Asia Pacific Finance Associ-ation Annual Conference (APFA), Bangkok, Thailand.Cristian Estan and George Varghese.
2002.
New di-rections in traffic measurement and accounting.
SIG-COMM Comput.
Commun.
Rev., 32(4).Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume?
III, andSuresh Venkatasubramanian.
2010.
Sketching tech-niques for Large Scale NLP.
In 6th WAC Workshop atNAACL-HLT.Amit Goyal, Ellen Riloff, and Hal Daume III.
2010b.Automatically producing plot unit representations fornarrative text.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Process-ing, pages 77?86.
Association for Computational Lin-guistics, October.D.
Graff.
2003.
English Gigaword.
Linguistic Data Con-sortium, Philadelphia, PA, January.Saif Mohammad, Cody Dunne, and Bonnie Dorr.
2009.Generating high-coverage semantic orientation lexi-cons from overtly marked words and a thesaurus.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages 599?608.
Association for Computational Linguistics.Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, andToshikazu Fukushima.
2002.
Mining product reputa-tions on the Web.
In Proceedings of the 8th Associa-tion for Computing Machinery SIGKDD InternationalConference on Knowledge Discovery and Data Mining(KDD-2002), pages 341?349, Edmonton, Canada.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Senti-ment analysis: Capturing favorability using naturallanguage processing.
In Proceedings of the 2nd Inter-national Conference on Knowledge Capture (K-CAP2003), pages 70?77, Sanibel Island, Florida.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, Vol.
2(1-2):pp.
1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
pages 79?86, Philadelphia,Pennsylvania.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of EMNLP.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceedingsof the 12th Conference of the European Chapter of theACL (EACL 2009), pages 675?682, Athens, Greece,March.
Association for Computational Linguistics.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and nlp: using localitysensitive hash function for high speed noun clustering.In Proceedings of ACL.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General Inquirer:A Computer Approach to Content Analysis.
MITPress.Richard Tong.
2001.
An operational system for detectingand tracking opinions in on-line discussions.
In Work-ing Notes of the Special Interest Group on InformationRetrieval (SIGIR) Workshop on Operational Text Clas-sification, pages 1?6, New Orleans, Louisianna.Peter D. Turney and Michael L. Littman.
2003.
Measur-ing praise and criticism: Inference of semantic orienta-tion from association.
ACM Trans.
Inf.
Syst., 21:315?346, October.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-nan, and Ryan McDonald.
2010.
The viability of web-derived polarity lexicons.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 777?785, Los Angeles, Cal-ifornia, June.
Association for Computational Linguis-tics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005a.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing,pages 347?354.
Association for Computational Lin-guistics.T.
Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,J.
Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-wardhan.
2005b.
OpinionFinder: A system for sub-jectivity analysis.
In Proceedings of Human LanguageTechnology Conference and Conference on EmpiricalMethods in Natural Language Processing InteractiveDemonstrations, pages 34?35.43
