Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 13?18,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 04:Classification of Semantic Relations between NominalsRoxana GirjuUniv.
of Illinoisat Urbana-ChampaignUrbana, IL 61801girju@uiuc.eduPreslav NakovUniv.
of California at BerkeleyBerkeley, CA 94720nakov@cs.berkeley.eduVivi NastaseEML Research gGmbHHeidelberg, Germany 69118nastase@eml-research.deStan SzpakowiczUniversity of OttawaOttawa, ON K1N 6N5szpak@site.uottawa.caPeter TurneyNational Research Council of CanadaOttawa, ON K1A 0R6peter.turney@nrc-cnrc.gc.caDeniz YuretKoc?
UniversityIstanbul, Turkey 34450dyuret@ku.edu.trAbstractThe NLP community has shown a renewedinterest in deeper semantic analyses, amongthem automatic recognition of relations be-tween pairs of words in a text.
We present anevaluation task designed to provide a frame-work for comparing different approaches toclassifying semantic relations between nom-inals in a sentence.
This is part of SemEval,the 4th edition of the semantic evaluationevent previously known as SensEval.
We de-fine the task, describe the training/test dataand their creation, list the participating sys-tems and discuss their results.
There were14 teams who submitted 15 systems.1 Task Description and Related WorkThe theme of Task 4 is the classification of semanticrelations between simple nominals (nouns or basenoun phrases) other than named entities ?
honeybee, for example, shows an instance of the Product-Producer relation.
The classification occurs in thecontext of a sentence in a written English text.
Al-gorithms for classifying semantic relations can beapplied in information retrieval, information extrac-tion, text summarization, question answering and soon.
The recognition of textual entailment (Tatu andMoldovan, 2005) is an example of successful use ofthis type of deeper analysis in high-end NLP appli-cations.The literature shows a wide variety of methodsof nominal relation classification.
They depend asmuch on the training data as on the domain of ap-plication and the available resources.
Rosario andHearst (2001) classify noun compounds from thedomain of medicine, using 13 classes that describethe semantic relation between the head noun andthe modifier in a given noun compound.
Rosarioet al (2002) classify noun compounds using theMeSH hierarchy and a multi-level hierarchy of se-mantic relations, with 15 classes at the top level.Nastase and Szpakowicz (2003) present a two-levelhierarchy for classifying noun-modifier relations inbase noun phrases from general text, with 5 classesat the top and 30 classes at the bottom; other re-searchers (Turney and Littman, 2005; Turney, 2005;Nastase et al, 2006) have used their class schemeand data set.
Moldovan et al (2004) propose a 35-class scheme to classify relations in various phrases;the same scheme has been applied to noun com-pounds and other noun phrases (Girju et al, 2005).Chklovski and Pantel (2004) introduce a 5-class set,designed specifically for characterizing verb-verbsemantic relations.
Stephens et al (2001) propose17 classes targeted to relations between genes.
La-pata (2002) presents a binary classification of rela-tions in nominalizations.There is little consensus on the relation sets andalgorithms for analyzing semantic relations, and itseems unlikely that any single scheme could workfor all applications.
For example, the gene-gene re-lation scheme of Stephens et al (2001), with rela-tions like X phosphorylates Y, is unlikely to be trans-ferred easily to general text.We have created a benchmark data set to allow theevaluation of different semantic relation classifica-tion algorithms.
We do not presume to propose a sin-gle classification scheme, however alluring it would13Relation Training data Test data Agreement Examplepositive set size positive set size (independent tagging)Cause-Effect 52.1% 140 51.3% 80 86.1% laugh (cause) wrinkles (effect)Instrument-Agency 50.7% 140 48.7% 78 69.6% laser (instrument) printer (agency)Product-Producer 60.7% 140 66.7% 93 68.5% honey (product) bee (producer)Origin-Entity 38.6% 140 44.4% 81 77.8% message (entity) from outer-space (origin)Theme-Tool 41.4% 140 40.8% 71 47.8% news (theme) conference(tool)Part-Whole 46.4% 140 36.1% 72 73.2% the door (part) of the car (whole)Content-Container 46.4% 140 51.4% 74 69.1% the apples (content) in the basket (container)Table 1: Data set statisticsbe to try to design a unified standard ?
it would belikely to have shortcomings just as any of the otherswe have just reviewed.
Instead, we have decided tofocus on separate semantic relations that many re-searchers list in their relation sets.
We have built an-notated data sets for seven such relations.
Every dataset supports a separate binary classification task.2 Building the Annotated Data SetsOurs is a new evaluation task, so we began with dataset creation and annotation guidelines.
The data setthat Nastase and Szpakowicz (2003) created had re-lation labels and part-of-speech and WordNet senseannotations, to facilitate classification.
(Moldovanet al, 2004; Girju et al, 2005) gave the annotatorsan example of each phrase in a sentence along withWordNet senses and position of arguments.
Ourannotations include all these, to support a varietyof methods (since we work with relations betweennominals, the part of speech is always noun).
Wehave used WordNet 3.0 on the Web and sense indextags.We chose the following semantic relations:Cause-Effect, Content-Container, Instrument-Agency, Origin-Entity, Part-Whole, Product-Producer and Theme-Tool.
We wrote seven detaileddefinitions, including restrictions and conventions,plus prototypical positive and near-miss negativeexamples.
For each relation separately, we baseddata collection on wild-card search patterns thatGoogle allows.
We built the patterns manually,following Hearst (1992) and Nakov and Hearst(2006).
Instances of the relation Content-Container,for example, come up in response to queries such as?
* contains *?, ?
* holds *?, ?the * in the *?.
Fol-lowing the model of the Senseval-3 English LexicalSample Task, we set out to collect 140 training andat least 70 test examples per relation, so we had anumber of different patterns to ensure variety.
Wealso aimed to collect a balanced number of positiveand negative examples.
The use of heuristic patternsto search for both positive and negative examplesshould naturally result in negative examples thatare near misses.
We believe that near misses aremore useful for supervised learning than negativeexamples that are generated randomly.
?Among the contents of the <e1>vessel</e1>were a set of carpenter?s <e2>tools</e2>, sev-eral large storage jars, ceramic utensils, ropes andremnants of food, as well as a heavy load of ballaststones.
?WordNet(e1) = ?vessel%1:06:00::?,WordNet(e2) = ?tool%1:06:00::?,Content-Container(e2, e1) = ?true?,Query = ?contents of the * were a?Figure 1: Annotations illustratedFigure 1 illustrates the annotations.
We tag thenominals, so parsing or chunking is not necessary.For Task 4, we define a nominal as a noun or basenoun phrase, excluding names entities.
A base nounphrase, e.g., lawn or lawn mower, is a noun with pre-modifiers.
We also exclude complex noun phrases(e.g., with attached prepositional phrases ?
the en-gine of the lawn mower).The procedure was the same for each relation.One person gathered the sample sentences (aim-ing approximately for a similar number of positiveand negative examples) and tagged the entities; twoother people annotated the sentences with WordNetsenses and classified the relations.
The detailed re-lation definitions and the preliminary discussions ofpositive and negative examples served to maximizethe agreement between the annotators.
They firstclassified the data independently, then discussed ev-ery disagreement and looked for consensus.
Onlythe agreed-upon examples went into the data sets.Next, we split each data set into 140 training andno fewer than 70 test examples.
(We published thetraining set for the Content-Container relation as de-velopment data two months before the test set.)
Ta-ble 1 shows the number of positive and negative ex-14amples for each relation.1The average inter-annotator agreement on rela-tions (true/false) after the independent annotationstep was 70.3%, and the average agreement onWordNet sense labels was 71.9%.
In the process ofarriving at a consensus between annotators, the def-inition of each relation was revised to cover explic-itly cases where there had been disagreement.
Weexpect that these revised definitions would lead tomuch higher levels of agreement than the originaldefinitions did.3 The ParticipantsThe task of classifying semantic relations betweennominals has attracted the participation of 14 teamswho submitted 15 systems.
Table 4 lists the sys-tems, the authors and their affiliations, and brief de-scriptions.
The systems?
performance informationin terms of precision, recall, F -measure and accu-racy, macroaveraged over all relations, appears inTable 3.
We computed these measures as describedin Lewis (1991).We distinguish four categories of systems basedon the type of information used ?
WordNet sensesand/or Google queries:A ?
WordNet = NO & Query = NO;B ?
WordNet = YES & Query = NO;C ?
WordNet = NO & Query = YES;D ?
WordNet = YES & Query = YES.WordNet = ?YES?
or WordNet = ?NO?
tells usonly whether a system uses the WordNet sense la-bels in the data sets.
A system may use WordNetinternally for varied purposes, but ignore our senselabels; such a system would be in category A or C .Based on the input variation, each submitted systemmay have up to 4 variations ?
A,B,C,D.Table 2 presents three baselines for a relation.Majority always guesses either ?true?
or ?false?,whichever is the majority in the test set (maximizesaccuracy).
Alltrue always guesses ?true?
(maxi-mizes recall).
Probmatch randomly guesses ?true?(?false?)
with the probability matching the distribu-tion of ?true?
(?false?)
in the test dataset (balancesprecision and recall).We present the results in Table 3 grouped by cat-egory, to facilitate system comparison.1As this paper serves also as a documentation of the data set,the order of relations in the table is the same as in the data set.Type P R F Accmajority 81.3 42.9 30.8 57.0alltrue 48.5 100.0 64.8 48.5probmatch 48.5 48.5 48.5 51.7Table 2: Baselines: precision, recall, F -measure andaccuracy averaged over the 7 binary classifications.Team P R F AccA ?
WordNet = NO & Query = NOUCD-FC 66.1 66.7 64.8 66.0ILK 60.5 69.5 63.8 63.5UCB?
62.7 63.0 62.7 65.4UMELB-B 61.5 55.7 57.8 62.7UTH 56.1 57.1 55.9 58.8UC3M 48.2 40.3 43.1 49.9avg?stdev 59.2?6.3 58.7?10.5 58.0?8.1 61.1?6.0B ?
WordNet = YES & Query = NOUIUC?
79.7 69.8 72.4 76.3FBK-IRST 70.9 73.4 71.8 72.9ILK 72.8 70.6 71.5 73.2UCD-S1 69.9 64.6 66.8 71.4UCD-PN 62.0 71.7 65.4 67.0UC3M 66.7 62.8 64.3 67.2CMU-AT 55.7 66.7 60.4 59.1UCD-FC 66.4 58.1 60.3 63.6UMELB-A 61.7 56.8 58.7 62.5UVAVU 56.8 56.3 56.1 57.7LCC-SRN 55.9 57.8 51.4 53.7avg ?
stdev 65.3?7.7 64.4?6.5 63.6?6.9 65.9?7.2C ?
WordNet = NO & Query = YESUCB?
64.2 66.5 65.1 67.0UCD-FC 66.1 66.7 64.8 66.0UC3M 49.4 43.9 45.3 50.1avg?stdev 59.9?9.1 59.0?13.1 58.4?11.3 61.0?9.5D ?
WordNet = YES & Query = YESUTD-HLT-CG 67.3 65.3 62.6 67.2UCD-FC 66.4 58.1 60.3 63.6UC3M 60.9 57.8 58.8 62.3avg?stdev 64.9?3.5 60.4?4.2 60.6?1.9 64.4?2.5Systems tagged with ?
have a Task 4 organizer as part of the team.Table 3: System performance grouped by category.Precision, recall, F -measure and accuracy macro-averaged over each system?s performance on all 7relations.4 DiscussionThe highest average accuracy on Task 4 was 76.3%.Therefore, the average initial agreement between an-notators (70.3%), before revising the definitions, isnot an upper bound on the accuracy that can beachieved.
That the initial agreement between anno-tators is not a good indicator of the accuracy that canbe achieved is also supported by the low correlation15System Institution Team Description System TypeUVAVU Univ.
of AmsterdamTNO Science & IndustryFree Univ.
AmsterdamSophia KatrenkoWillem Robert vanHagesimilarity measures in WordNet; syn-tactic dependencies; lexical patterns;logical combination of attributesBCMU -AT Carnegie Mellon Univ.
Alicia TribbleScott E. FahlmanWordNet; manually-built ontologies;Scone Knowledge Representation Lan-guage; semantic distanceBILK Tilburg University Caroline SporlederRoser MoranteAntal van den Boschsemantic clusters based on noun simi-larity; WordNet supersenses; grammat-ical relation between entities; head ofsentence; WEKAA, BFBK-IRST Fondazione BrunoKessler - IRSTClaudio GiulianoAlberto LavelliDaniele PighinLorenza Romanoshallow and deep syntactic information;WordNet synsets and hypernyms; ker-nel methods; SVMBLCC-SRN Language ComputerCorp.Adriana Badulescu named entity recognition; lexical, se-mantic, syntactic features; decision treeand semantic scatteringBUMELB-A Univ.
of Melbourne Su KimTimothy Baldwinsense collocations; similarity of con-stituents; extending training and testingdata using similar wordsBUMELB-B Univ.
of Melbourne Su KimTimothy Baldwinsimilarity of nearest-neighbor matchingover the union of senses for the twonominals; cascaded tagging with de-creasing thresholdsAUCB?
Univ.
of California atBerkeleyPreslav NakovMarti HearstVSM; joining terms; KNN-1 A, CUC3M Univ.
Carlos III of Madrid Isabel Segura BedmarDoaa SammyJose?
Luis Mart?
?nezFerna?ndezWordNet path; syntactic features; SVM A, B, C, DUCD-S1 Univ.
College Dublin Cristina ButnariuTony Vealelexical-semantic categories from Word-Net; syntactic patterns from corpora,SVMBUCD-FC Univ.
College Dublin Fintan Costello WordNet; additional noun compoundstagged corpus; Naive BayesA, B, C, DUCD-PN Univ.
College Dublin Paul Nulty WordNet supersenses; web-based fre-quency counts for specific joiningterms; WEKA (SMO)BUIUC?
Univ.
of Illinois at UrbanaChampaignRoxana GirjuBrandon BeamerSuma BhatBrant CheeAndrew FisterAlla Rozovskayafeatures based on WordNet, NomLex-PLUS, grammatical roles, lexico-syntactic patterns, semantic parsesBUTD-HLT-CG Univ.
of Texas at Dallas Cristina NicolaeGarbiel NicolaeSanda Harabagiulexico-semantic features from Word-Net, VerbNet; semantic features from aPropBank parser; dependency featuresDUTH Univ.
of Tokio Eiji AramakiTakeshi ImaiKengo MiyoKazuhiko Ohejoining phrases; physical size for enti-ties; web-mining; SVMASystems tagged with ?
have a Task 4 organizer as part of the team.Table 4: Short description of the teams and the participating systems.16Relation Team Type P R F Acc Test size Base-F Base-Acc Avg.
rankCause-Effect UIUC B4 69.5 100.0 82.0 77.5 80 67.8 51.2 3.4Instrument-Agency FBK-IRST B4 76.9 78.9 77.9 78.2 78 65.5 51.3 3.4Product-Producer UCD-S1 B4 80.6 87.1 83.7 77.4 93 80.0 66.7 1.7Origin-Entity ILK B3 70.6 66.7 68.6 72.8 81 61.5 55.6 6.0Theme-Tool ILK B4 69.0 69.0 69.0 74.6 71 58.0 59.2 6.0Part-Whole UC3M B4 72.4 80.8 76.4 81.9 72 53.1 63.9 4.5Content-Container UIUC B4 93.1 71.1 80.6 82.4 74 67.9 51.4 3.1Table 5: The best results per relation.
Precision, recall, F -measure and accuracy macro-averaged over eachsystem?s performance on all 7 relations.
Base-F shows the baseline F -measure (alltrue), Base-Acc ?
thebaseline accuracy score (majority).
The last column shows the average rank for each relation.of 0.15 between the Acc column in Table 5 and theAgreement column in Table 1.We performed various analyses of the results,which we summarize here in four questions.
Wewrite Xi to refer to four possible system categories(Ai, Bi, Ci, and Di) with four possible amounts oftraining data (X1 for training examples 1 to 35, X2for 1 to 70, X3 for 1 to 105, and X4 for 1 to 140).Does more training data help?Overall, the results suggest that more training dataimproves the performance.
There were 17 cases inwhich we had results for all four possible amountsof training data.
All average F -measure differences,F (X4)?F (Xi) where X = A to D, i = 1 to 3, forthese 17 sets of results are statistically significant:F (X4)?F (X1): N = 17, avg = 8.3, std = 5.8, min =1.1, max = 19.6, t-value = ?5.9, p-value = 0.00001.F (X4)?F (X2): N = 17, avg = 4.0, std = 3.7, min =?3.5, max = 10.5, t-value = 4.5, p-value = 0.0002.F (X4)?F (X3): N = 17, avg = 0.9, std = 1.7, min =?2.6, max = 4.7, t-value = 2.1, p-value = 0.03.Does WordNet help?The statistics show that WordNet is important, al-though the contribution varies across systems.
Threeteams submitted altogether 12 results both for A1?A4 and B1?B4.
The average F -measure difference,F (Bi)?F (Ai), i = 1 to 4, is significant:F (Bi)?F (Ai): N = 12, avg = 6.1, std = 8.4, min =?4.5, max = 21.2, t-value = ?2.5, p-value = 0.01.The results of the UCD-FC system actually wentdown when WordNet was used.
The statistics for theremaining two teams, however, are a bit better:F (Bi)?F (Ai): N = 8, avg = 10.4, std = 6.7, min =?1.0, max = 21.2, t-value = ?4.4, p-value = 0.002.Does knowing the query help?Overall, knowing the query did not seem to improvethe results.
Three teams submitted 12 results bothfor A1?A4 and C1?C4.
The average F -measure dif-ference, F (Ci)?F (Ai) , i = 1 to 4, is not significant:F (Ci)?F (Ai): N = 12, avg = 0.9, std = 1.8, min =?2.0, max = 5.0, t-value = ?1.6, p-value = 0.06.Again, the UCD-FC system differed from theother systems in that the A and C scores were iden-tical, but even averaging over the remaining two sys-tems and 8 cases does not show a statistically signif-icant advantage:F (Ci)?F (Ai): N = 8, avg = 1.3, std = 2.2, min =?2.0, max = 5.0, t-value = ?1.7, p-value = 0.07.Are some relations harder to classify?Table 5 shows the best results for each relation interms of precision, recall, and F -measure, per teamand system category.
Column Base-F presents thebaseline F -measure (alltrue), while Base-Acc thebaseline accuracy score (majority).
For all seven re-lations, the best team significantly outperforms thebaseline.
The category of the best-scoring systemin almost every case is B4 (only the ILK B4 systemscored second on the Origin-Entity relation).Table 5 suggests that some relations are more dif-ficult to classify than others.
The best F -measureranges from 83.7 for Product?Producer to 68.6 forOrigin?Entity.
The difference between the best F -measure and the baseline F -measure ranges from23.3 for Part-Whole to 3.7 for Product-Producer.The difference between the best accuracy and thebaseline accuracy ranges from 31.0 for Content-Container to 10.7 for Product-Producer.The F column shows the best result for each rela-tion, but similar differences among the relations maybe observed when all results are pooled.
The Avg.rank column computes the average rank of each re-lation in the ordered list of relations generated byeach system.
For example, Product?Producer is of-ten listed as the first or the second easiest relation(with an average rank of 1.7), while Origin?Entityand Theme?Tool are identified as the most difficult17relations to classify (with average ranks of 6.0).5 ConclusionThis paper describes a new semantic evaluation task,Classification of Semantic Relations between Nom-inals.
We have accomplished our goal of providinga framework and a benchmark data set to allow forcomparisons of methods for this task.
The data in-cluded different types of information ?
lexical se-mantic information, context, query used ?
meant tofacilitate the analysis of useful sources of informa-tion for determining the semantic relation betweennominals.
The results that the participating systemshave reported show successful approaches to thisdifficult task, and the advantages of using lexical se-mantic information.The success of the task ?
measured in the inter-est of the community and the results of the partici-pating systems ?
shows that the framework and thedata are useful resources.
By making this collectionfreely accessible, we encourage further research intothis domain and integration of semantic relation al-gorithms in high-end applications.AcknowledgmentsWe thank Eneko Agirre, Llu?
?s Ma`rquez and RichardWicentowski, the organizers of SemEval 2007, fortheir guidance and prompt support in all organiza-tional matters.
We thank Marti Hearst for valu-able advice throughout the task description and de-bates on semantic relation definitions.
We thank theanonymous reviewers for their helpful comments.ReferencesT.
Chklovski and P. Pantel.
2004.
Verbocean: Mining theweb for fine-grained semantic verb relations.
In Proc.Conf.
on Empirical Methods in Natural Language Pro-cessing, EMNLP-04, pages 33?40, Barcelona, Spain.R.
Girju, D. Moldovan, M. Tatu, and D. Antohe.
2005.On the semantics of noun compounds.
ComputerSpeech and Language, 19:479?496.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proc.
14th InternationalConf.
on Computational Linguistics (COLING-92),pages 539?545.M.
Lapata.
2002.
The disambiguation of nominaliza-tions.
Computational Linguistics, 28(3):357?388.D.D.
Lewis.
1991.
Evaluating text categorization.In Proceedings of the Speech and Natural LanguageWorkshop, pages 312?318, Asilomar.D.
Moldovan, A. Badulescu, M. Tatu, D. Antohe, andR.
Girju.
2004.
Models for the semantic classificationof noun phrases.
In Proc.
Computational Lexical Se-mantics Workshop at HLT-NAACL 2004, pages 60?67,Boston, MA.P.
Nakov and M. Hearst.
2006.
Using verbs to char-acterize noun-noun relations.
In Proc.
Twelfth Inter-national Conf.
in Artificial Intelligence (AIMSA-06),pages 233?244, Varna,Bulgaria.V.
Nastase and S. Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Fifth Interna-tional Workshop on Computational Semantics (IWCS-5), pages 285?301, Tilburg, The Netherlands.V.
Nastase, J. Sayyad-Shirabad, M. Sokolova, and S. Sz-pakowicz.
2006.
Learning noun-modifier semanticrelations with corpus-based and WordNet-based fea-tures.
In Proc.
21st National Conf.
on Artificial Intel-ligence (AAAI 2006), pages 781?787, Boston, MA.B.
Rosario and M. Hearst.
2001.
Classifying the seman-tic relations in noun-compounds via domain-specificlexical hierarchy.
In Proc.
2001 Conf.
on EmpiricalMethods in Natural Language Processing (EMNLP-01), pages 82?90.B.
Rosario, M. Hearst, and C. Fillmore.
2002.
The de-scent of hierarchy, and selection in relational seman-tics.
In Proc.
40th Annual Meeting of the Associationfor Computational Linguistics (ACL-02), pages 417?424, Philadelphia, PA.M.
Stephens, M. Palakal, S. Mukhopadhyay, and R. Raje.2001.
Detecting gene relations from MEDLINE ab-stracts.
In Proc.
Sixth Annual Pacific Symposium onBiocomputing, pages 483?496.M.
Tatu and D. Moldovan.
2005.
A semantic approach torecognizing textual entailment.
In Proc.
Human Lan-guage Technology Conf.
and Conf.
on Empirical Meth-ods in Natural Language Processing (HLT/EMNLP2005), pages 371?378, Vancouver, Canada.P.D.
Turney and M.L.
Littman.
2005.
Corpus-basedlearning of analogies and semantic relations.
MachineLearning, 60(1-3):251?278.P.D.
Turney.
2005.
Measuring semantic similarity bylatent relational analysis.
In Proc.
Nineteenth Interna-tional Joint Conf.
on Artificial Intelligence (IJCAI-05),pages 1136?1141, Edinburgh, Scotland.18
