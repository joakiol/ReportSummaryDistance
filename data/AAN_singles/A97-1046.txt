Fast Statistical Parsing of Noun Phrasesfor Document IndexingChengx iang  Zha iLaboratory  for Computat iona l  LinguisticsCarnegie Mellon UniversityP i t tsburgh,  PA 15213{cz25@andrew.cmu.edu}Abst rac tInformation Retrieval (IR) is an importantapplication area of Natural Language Pro-cessing (NLP) where one encounters thegenuine challenge of processing large quan-tities of unrestricted natural language text.While much effort has been made to applyNLP techniques to IR, very few NLP tech-niques have been evaluated on a documentcollection larger than several megabytes.Many NLP techniques are simply not ef-ficient enough, and not robust enough, tohandle a large amount of text.
This pa-per proposes a new probabilistic model fornoun phrase parsing, and reports on theapplication of such a parsing technique toenhance document indexing.
The effective-ness of using syntactic phrases provided bythe parser to supplement single words forindexing is evaluated with a 250 megabytesdocument collection.
The experiment's re-sults show that supplementing single wordswith syntactic phrases for indexing consis-tently and significantly improves retrievalperformance.1 In t roduct ionInformation Retrieval (IR) is an increasingly impor-tant application area of Natural Language Process-ing (NLP).
An IR task can be described as to find,from a given document collection, a subset of docu-ments whose content is relevant o the informationneed of a user as expressed by a query.
As the doc-uments and query are often natural language texts,an IR task can usually be regarded as a special NLPtask, where the document text and the query textneed to be processed in order to judge the relevancy.A general strategy followed by most IR systems isto transform documents and the query into certainlevel of representation.
A query representation canthen be compared with a document representationto decide if the document is relevant to the query.
Inpractice, the level of representation in an IR systemis quite "shallow" - -  often merely a set of word-likestrings, or indexing terms.
The process to extract in-dexing terms from each document in the collectionis called indexing.
A query is often subject o simi-lax processing, and the relevancy is judged based onthe matching of query terms and document terms.In most systems, weights are assigned to terms toindicate how well they can be used to discriminaterelevant documents from irrelevant ones.The challenge in applying NLP to IR is to dealwith a large amount of unrestricted natural lan-guage text.
The NLP techniques used must be veryefficient and robust, since the amount of text inthe databases accessed is typically measured in gi-gabytes.
In the past, NLP techniques of differentlevels, including morphological, syntactic/semantic,and discourse processing, were exploited to enhanceretrieval (Smeaton 92; Lewis and Spaxck Jones 96),but were rarely evaluated using collections of docu-ments larger than several megabytes.
Many NLPtechniques are simply not efficient enough or aretoo labor-intensive to successfully handle a large sizedocument set.
However, there are some exceptions.Evans et al used selective NLP techniques, that areespecially robust and efficient, for indexing (Evanset al 91).
Strzalkowski reported a fast and robustparser called TTP in (Strzalkowski 92; Strzalkowskiand Vauthey 92).
These NLP techniques have beensuccessfully used to process quite large collections,as shown in a series of TREC conference reports bythe CLARIT TM1 system group and the New YorkUniversity (later GE/NYU) group (cf., for example,(Evans and Lefferts 95; Evans et al 96), and (Strza-lkowski 95; Strzalkowski et al 96)) These researchefforts demonstrated the feasibility of using selec-tive NLP to handle large collections.
A special NLPtrack emphasizing the evaluation of NLP techniquesfor IR is currently held in the context of TREC (Hat-man 96).In this paper, a fast probabilistic noun phraseparser is described.
The parser can be exploited to1CLARIT is a registered trademark ofCLARITECHCorporation.312automatically extract syntactic phrases from a largeamount of documents for indexing.
A 250-megabytedocument set 2 is used to evaluate the effectiveness ofindexing using the phrases extracted by the parser.The experiment's results show that using syntacticphrases to supplement single words for indexing im-proves the retrieval performance significantly.
Thisis quite encouraging compared to earlier experimentson phrase indexing.
The noun phrase parser pro-vides the possibility of combining different kinds ofphrases with single words.The rest of the paper is organized as follows.
Sec-tion 2 discusses document indexing, and argues forthe rationality of using syntactic phrases for index-ing; Section 3 describes the fast noun phrase parserthat we use to extract candidate phrases; Section 4describes how we use a commercial IR system to per-form the desired experiments; Section 5 reports anddiscusses the experiment results; Section 6 summa-rizes the conclusions.2 Phrases  fo r  Document  Index ingIn most current IR systems, documents are primarilyindexed by single words, sometimes supplemented byphrases obtained with statistical approaches, uch asfrequency counting of adjacent word pairs.
However,single words are often ambiguous and not specificenough for accurate discrimination of documents.For example, only using the word "baalS' and "ter-minology" for indexing is not enough to distinguish"bank terminology" from "terminology baalS'.
Morespecific indexing units are needed.
Syntactic phrases(i.e., phrases with certain syntactic relations) are al-most always more specific than single words and thusare intuitively attractive for indexing.
For example,if "bank terminology" occurs in the document, then,we can use the phrase "bank terminology" as an ad-ditional unit to supplement the single words "banld'and "terminology" for indexing.
In this way, a querywith "terminology banlZ' will match better with thedocument han one with "bank terminology", sincethe indexing phrase "bank terminology" provides ex-tra discrimination.Despite the intuitive rationality of using phrasesfor indexing, syntactic phrases have been reportedto show no significant improvement of retrieval per-formance (Lewis 91; Belkin and Croft 87; Fagan87).
Moreover Fagan (Fagan 87) found that syn-tactic phrases are not superior to simple statisticalphrases.
Lewis discussed why the syntactic phraseindexing has not worked and concluded that theproblems with syntactic phrases are for the mostpart statistical (Lewis 91).
Indeed, many (perhapsmost) syntactic phrases have very low frequency andtend to be over-weighted by the normal weightingmethod.
However, the size of the collection used in2the Wall Street Journal database in Tipster Disk2(Harman 96)these early experiments i  relatively small.
We wantto see if a much larger size of collection will make adifference.
It is possible that a larger document col-lection might increase the frequency of most phrases,and thus alleviate the problem of low frequency.We only consider noun phrases and the sub-phrases derived from them.
Specifically, we want toobtain the full modification structure of each nounphrase in the documents and query.
From the view-point of NLP, the task is noun phrase parsing (i.e.,the analysis of noun phrase structure).
When thephrases are used only to supplement, not replace,the single words for indexing, some parsing errorsmay be tolerable.
This means that the penalty fora parsing error may not be significant.
The chal-lenge, however, is to be able to parse gigabytes oftext in practically feasible time and as accuratelyas possible.
The previous work taking on this chal-lenge includes (Evans et al 91; Evans et al 96;Evans and Zhal 96; Strzalkowski and Carballo 94;Strzalkowski et al 95) among others.
Evans etal.
exploited the "attestedness" of subphrases topartially reveal the structure of long noun phrases(Evans et al 91; Evans et al 96).
Strzalkowski etal.
adopted a fast Tagged Text Parser (TTP)  to ex-tract head modifier pairs including those in a nounphrase (Strzalkowski 92; Strzalkowski and Vauthey92; Strzalkowski and Carballo 94; Strzalkowski etal.
95).
In (Strzalkowski et al 95), the structureof a noun phrase is disambiguated based on certainstatistical heuristics, but there seems to be no ef-fort to assign a full structure to every noun phrase.Furthermore, manual effort is needed in constructinggrammar rules.
Thus, the approach in (Strzalkowskiet M. 95) does not address the special need of scal-ability and robustness along with speed.
Evans andZhai explored a hybrid noun phrase analysis methodand used a quite rich set of phrases for document in-dexing (Evans and Zhai 96).
The indexing methodwas evaluated using the Associated Press newswire89 (AP89) database in Tipster Diskl, and a generalimprovement of retrieval performance over the in-dexing with single words and full noun phrases wasreported.
However, the phrase extraction systemas reported in (Evans and Zhal 96) is still not fastenough to deal with document collections measuredby gigabytes.
3We propose here a probabilistic model of nounphrase parsing.
A fast statistical noun phrase parserhas been developed based on the probabilistic model.The parser works fast and can be scaled up to parsegigabytes text within acceptable time.
4 Our goalis to generate different kinds of candidate syntactic3It was reported to take about 3.5 hours to process20 MB documents4With a 133MH DEC alpha workstation, it is esti-mated to parse at a speed of 4 hours/gigabyte-text or8 hours/gigabyte-nps, after 20 hours of training with 1gigabyte text313phrases from the structure of a noun phrase so thatthe effectiveness of different combinations ofphrasesand single words can be tested.3 Fas t  Noun Phrase  Pars ingA fast and robust noun phrase parser is a key tothe exploration of syntactic phrase indexing.
Nounphrase parsing, or noun phrase structure analy-sis ( also known as compound noun analysisS),is itself an important research issue in computa-tional linguistics and natural language processing.Long noun phrases, especially long compound nounssuch as "information retrieval technique", generallyhave ambiguous tructures.
For instance, "informa-tion retrieval technique" has two possible structures:"\[\[information retrieval\] technique\]' and "\[informa-tion \[retrieval technique\]\]'.
A principal difficultyin noun phrase structure analysis is to resolve suchstructural ambiguity.
When a large corpus is avail-able, which is true for an IR task, statistical prefer-ence of word combination or word modification canbe a good clue for such disambiguation.
As summa-rized in (Lauer 95), there are two different modelsfor corpus-based parsing of noun phrases: the adja-cency model and the dependency model.
The differ-ence between the two models can be illustrated bythe example compound noun "informationsretrievaltechnique".
In the adjacency model, the structurewould be decided by looking at the adjacency as-sociation of "information retrievaF and "retrievaltechnique".
"information retrievat' will be groupedfirst, if "information retrievaF has a stronger as-sociation than "retrieval technique", otherwise, "re-trieval technique" will be grouped first.
In the de-pendency model, however, the structure would bedecided by looking at the dependency between "in-formation" and "retrievaP (i.e., the tendency for"information" to modify "retrievat') and the depen-dency between "information" and "technique".
If"information" has a stronger dependency associa-tion with "retrievaP than with "technique", "infor-mation retrievat' will be grouped first, otherwise,"retrieval technique" will be grouped first.
The ad-jacency model dates at least from (Marcus 80) andhas been explored recently in (Liberman and Sproat92; Pustejovsky et al 93; Resnik and Hearst 93;Lauer 95; Strzalkowski et al 95; Evans and Zhai96); The dependency model has mainly been stud-ied in (Lauer 94).
Evans and Zhai (Evans and Zhai96) use primarily the adjacency model, but the as-sociation score also takes into account some degreeof dependency.
Lauer (Lauer 95) compared the ad-jacency model and the dependency model for com-pound noun disambiguation, and concluded that theSStrictly speaking, however, compound noun analysisis a special case of noun phrase analysis, but the sametechnique can oRen be used for both.314dependency model provides a substantial advantageover the adjacency model.We now propose aprobabilistic model in which thedependency structure, or the modification structure,of a noun phrase is treated as "hidden", similar tothe tree structure in the probabilistic ontext-freegrammar (Jelinek et al 90).
The basic idea is asfollows.A noun phrase can be assumed to be generatedfrom a word modification structure (i.e., a depen-dency structure).
Since noun phrases with morethan two words are structurally ambiguous, if weonly observe the noun phrase, then the actual struc-ture that generates the noun phrase is "hidden".
Wetreat the noun phrases with their possible structuresas the complete data and the noun phrases occur-ring in the corpus (without the structures) as theobserved incomplete data.
In the training phase, anExpectation Maximization (EM) algorithm (Demp-ster et al 77) can be used to estimate the parame-ters of word modification probabilities by iterativelymaximizing the conditional expectation of the likeli-hood of the complete data given the observed incom-plete data and a previous estimate of the parameters.In the parsing phase, a noun phrase is assigned thestructure that has the maximum conditional proba-bility given the noun phrase.Formally, assume that each noun phrase is gener-ated using a word modification structure.
For exam-ple, "information retrieval technique" may be gener-ated using either the structure "\[XI\[X2Xz\]\]" or thestructure "\[\[X1X2\]X3\]".
The log likelihood of gen-erating a noun phrase, given the set of noun phrasesobserved in a corpus NP = {npi} can be written as:L(?)
= ~\ ]  c(npi)log ~ P?
(npi, sj)npiENP sjESwhere, S is the set of all the possible modificationstructures; c(npi) is the count of the noun phrase npiin the corpus; and P?
(npi, sj) gives the probability ofderiving the noun phrase npi using the modificationstructure sj.With the simplification that generating a nounphrase from a modification structure is the same asgenerating all the corresponding word modificationpairs in the noun phrase and with the assumptionthat each word modification pair in the noun phraseis generated independently, P?
(npi, sj) can furtherbe written asP?
(npi, sj) = P?
(sj) H PC(u, v) c(u'v;'~p''sD(u,v)eM(np~,sj)where, M(npi, sj) is the set of all word pairs (u, v)in npi such that u modifies (i.e., depends on) v ac-cording to sj.
6 c(u, v; npi, sj) is the count of the~For example, if npl is "information retrieval tech-nique", and sj is "\[\[X1X~IX3\]", then, M(npi, sj) ={(information, retrieval), (retrieval, technique)}.modification pairs (u, v) being generated when npiis derived from sj.
P?
(sj) is the probability of struc-ture sj; while Pc(u, v) is the probability of generat-ing the word pair (u, v) given any word modifica-tion relation.
P?
(sj) and Pc(u, v) are subject to theconstraint of summing up to 1 over all modificationstructures and over all possible word combinationsrespectively.
7The model is clearly a special case of the class ofthe algebraic language models, in which the proba-bilities are expressed as polynomials in the param-eters(Lafferty 95).
For such models, the M-step inthe EM algorithm can be carried out exactly, andthe parameter update formulas are:P,+I(U, v)= A'{ 1 ~ c(npi) ~ P~(sjlnpi)c(u,v;np,,sj)npi6NP s16S= )~1 ~ c(npi)P,(sklnpi)np iENPwhere, A1 and A2 are the Lagrange multipliers cor-responding to the two constraints mentioned above,and are given by the following formulas:(u,v)EWP rtpi 6NP  sj ES8kESnpi6NPwhere, WP is the set of all possible word pairs.Pn(sj Inpi) can be computed as:st)P.(np , st), , )''j);'One problem with such simplification is that themodel may generate a set of word modification pairs thatdo not form a noun phrase, although such "illegal nounphrases" are never observed.
A better model would beto write the probability of each word modification pairas the conditional probability of the modifier (i.e., themodifying word) given the head (i.e., the word beingmodified).
That is,P,(npi, st) =P?
(si)P~(h(npi)JsJ) 1-\[ P*(ulv)?
(~'~;'"~J)(u,v)EM(npi,sj)where h(np,) is the head (i.e., the last word) of the nounphrase npi(Lafferty 96).The EM algorithm ensures that L(n+ 1) is greaterthan L(n).
In other words, every step of parameterupdate increases the likelihood.
Thus, at the time oftraining, the parser can first randomly initialize theparameters, and then, iteratively update the param-eters according to the update formulas until the in-crease of the likelihood is smaller than some pre-setthreshold, s In the implementation described here,the maximum length of any noun phrase is limitedto six.
In practice, this is not a very tight limit, sincesimple noun phrases with more than six words arequite rare.
Summing over all the possible structuresfor any noun phrase is computed by enumerating allthe possible structures with an equal length as thenoun phrase.
For example, in the case of a three-word noun phrase, only two structures need to beenumerated.At the time of parsing noun phrases, the structureof any noun phrase np (S(np)) is determined byS(np) = argmaxsP(slnp)= argmax,P(np\[s)P(s)= argmaxs H P(u, v)P(s)(u,v)eM(np,s)We found that the parameters may easily be bi-ased owing to data sparseness.
For example, themodification structure parameters naturally preferleft association to right association in the case ofthree-word noun phrases, when the data is sparse.Such bias in the parameters of the modificationstructure probability will be propagated to the wordmodification parameters when the parameters areiteratively updated using EM algorithm.
In the ex-periments reported in this paper, an over-simplifiedsolution is adopted.
We simply fixed the modifica-tion structure parameter and assumed every depen-dency structure is equally likely.Fast training is achieved by reading all the nounphrase instances into memory.
9 This forces us tosplit the whole noun phrase corpus into small chunksfor training.
In the experiments reported in thispaper, we split the corpus into chunks of a size ofaround 4 megabytes.
Each chunk has about 170,000(or about 100,000 unique) raw multiple word nounphrases.
The parameters estimated on each sub-corpus are then merged (averaged).
We do not knowhow much the merging of parameters affects the pa-rameter estimation, but it seems that a majority ofphrases are correctly parsed with the merged param-eter estimation, based on a rough check of the pars-ing results.
With this approach, it takes a 133-MHzDEC Alpha workstation about 5 hours to train theparser over the noun phrases from a 250-megabyteSFor the experiments reported in this paper, thethreshold is 2.9An alternative way would be to keep the corpus inthe disk.
In this way, it is not necessary to split thecorpus, unless it is extremely large.315text corpus.
Parsing is much faster, taking less than1 hour to parse all noun phrases in the corpus ofa 250-megabyte xt.
The parsing speed can bescaled up to gigabytes of text, even when the parserneeds to be re-trained over the noun phrases in thewhole corpus.
However, the speed has not taken intoaccount he time required for extracting the nounphrases for training.
In the experiments describedin the following section, the CLARIT noun phraseextractor is used to extract all the noun phrases fromthe 250-megabyte text corpus.After the training on each chunk, the estimationof the parameter of word modifications i smoothedto account for the unseen word modification pairs.Smoothing is made by "dropping" a certain numberof parameters that have the least probabilities, tak-ing out the probabilities of the dropped parameters,and evenly distributing these probabilities amongall the unseen word pairs as well as those pairs ofthe dropped parameters.
It is unnecessary to keepthe dropped parameters after smoothing, thus thismethod of smoothing helps reduce the memory over-load when merging parameters.
In the experimentsreported in the paper, nearly half of the total num-ber of word pairs seen in the training chunk weredropped.
Since, word pairs with the least probabil-ities generally occur quite rarely in the corpus andusually represent semantically illegal word combina-tions, dropping such word pairs does not affect theparsing output so significantly as it seems.
In fact, itmay not affect the parsing decisions for the majorityof noun phrases in the corpus at all.The potential parameter space for the probabilis-tic model can be extremely large, when the size ofthe training corpus is getting larger.
One solutionto this problem is to use a class-based model similarto the one proposed in (Brown et al 92) or use pa-rameters of conceptual association rather than wordassociation, as discussed in (Lauer 94)(Lauer 95).4 Experiment DesignWe used the CLARIT commercial retrieval systemas a retrieval engine to test the effectiveness of differ-ent indexing sets.
The CLARIT system uses the vec-tor space retrieval model(Salton and McGill 83), inwhich documents and the query are all representedby a vector of weighted terms (either single words orphrases), and the relevancy judgment is based on thesimilarity (measured by the cosine measure) betweenthe query vector and any document vector(Evans etal.
93; Evans and Lefferts 95; Evans et al 96).
Theexperiment procedure is described by Figure 1.First, the original database is parsed to form dif-ferent sets of indexing terms (say, using differentcombination of phrases).
Then, each indexing set ispassed to the CLARIT retrieval engine as a sourcedocument set.
The CLARIT system is configured toaccept he indexing set we passed as is to ensure that316Original Document Set )tCLARIT NP Extractor IRaw Noun Phrases )Statistical NP ParserPhrase ExtractorIndexing Term Set )CLARIT Retrieval Engine IFigure h Phrase indexing experiment procedurethe actual indexing terms used inside the CLARITsystem are exactly those generated.It is possible to generate three differentkinds/levels of indexing units from a noun phrase:(1) single words; (2) head modifier pairs (i.e., anyword pair in the noun phrase that has a linguis-tic modification relation); and (3) the full nounphrase.
For example, from the phrase structure"\[\[~neavy=construction\]-industry\]\]-group\]" (a realexample from WS390), it is possible to generate thefollowing candidate terms:SINGLE WORDs :heavy, construction, industry, groupHEAD MODIF IERS :construction industry, industry group,heavy constructionFULL NP :heavy construction industry groupDifferent combinations ofthe three kinds of termscan be selected for indexing.
In particular, the in-dexing set formed solely of single words is used as abaseline to test the effect of using phrases.
In the ex-periments reported here, we generated four differentcombinations of phrases:-- WD-SET :single word only (no phrases, baseline)-- WD-HM-SET:s ing le  word  + head  mod i f ie r  pa i r- -  ND-NP-SET  :s ing le  word  + fu l l  NP- -  WD-HM-NP-SET  :s ing le  word  + head  mod i f ie r  + fu l l  NPThe results from these different phrase sets arediscussed in the next section.5 Results analysisWe used, as our document set, the Wall Street Jour-nal database in Tipster Disk2 (Harman 96) the sizeof which is about 250 megabytes.
We performedthe experiments by using the TREC-5 ad hoc topics(i.e., TREC topics 251-300).
Each run involves anautomatic feedback with the top 10 documents re-turned from the initial retrieval.
The CLARIT au-tomatic feedback is performed by adding terms froma query-specific thesaurus extracted from the top Ndocuments returned from the initial retrieval(Evansand Lefferts 95).
The results are evaluated usingthe standard measures of recall and precision.
Re-call measures how many of the relevant documentshave actually been retrieved.
Precision measureshow many of the retrieved ocuments are indeed rel-evant.
They are calculated by the following simpleformulas:number o f  relevant items retrieved Recall =total number of  relevant items in collectionnumber of  relevant items retrieved Precision =total number of  items retrievedWe used the standard TREC evaluation packageprovided by Cornell University and used the judged-relevant documents from the TREC evaluations asthe gold standard(Harman 94).In Table 1, we give a summary of the results andcompare the three phrase combination runs with thecorresponding baseline run.
In the table, "Ret-rel"means "retrieved-relevant" and refers to the totalnumber of relevant documents retrieved.
"Init Prec"means "initial precision" and refers to the highestlevel of precision over all the points of recall.
"AvgPrec" means "average precision" and is the averageof all the precision values computed after each newrelevant document is retrieved.It is clear that phrases help both recall and pre-cision when supplementing single words, as can beseen from the improvement of all phrase runs (WD-HM-SET, WD-NP-SET, WD-I-IM-NP-SET) overthe single word run WD-SET.It can also be seen that when only one kind ofphrase (either the full NPs or the head modifiers) isused to supplement the single words, each can leadto a great improvement in precision.
However, whenwe combine the two kinds of phrases, the effect is agreater improvement in recall rather than precision.The fact that each kind of phrase can improve pre-cision significantly when used separately shows thatExper imentsWD-SETWD-HM-SETi nc  over  WD-SETWD-NP-SETinc  over  WD-SETWD-HM-NP-SET 0.63(666)i nc  over  WD-SET 13%Recall (l:tet-Rel) Init Prec Av~ Prec0.56(597) 0.4546 0.22080.60( 638 ) 0.5162 0.24027% 14% 9%0.56(613) 0.5373 0.25644% 18% 16%0.4747 0.22854% 3%Total re levant  documents :  1064Table 1: Effects of Phrases with feedback andTREC-5 topicsthese phrases are indeed very useful for indexing.The combination of phrases results in only a smallerprecision improvement but causes a much greaterincrease in recall.
This may indicate that more ex-periments are needed to understand how to combineand weight different phrases effectively.The same parsing method has also been usedto generate phrases from the same data for theCLARIT NLP track experiments in TREC-5(Zhaiet al 97), and similar results were obtained, al-though the WD-NP-SET was not tested.
The resultsin (Zhai et al 97) are not identical to the resultshere, because they are based on two separate train-ing processes.
It is possible that different rainingprocesses may result in slightly different parameterestimations, because the corpus is arbitrarily seg-mented into chunks of only roughly 4 megabytes fortraining, and the chunks actually used in differenttraining processes may vary slightly.6 ConclusionsInformation retrieval provides a good way to quanti-tatively (although indirectly) evaluate various NLPtechniques.
We explored the application of a faststatistical noun phrase parser to enhance documentindexing in information retrieval.
We proposed anew probabilistic model for noun phrase parsing anddeveloped a fast noun phrase parser that can han-dle relatively large amounts of text efficiently.
Theeffectiveness of enhancing document indexing withthe syntactic phrases provided by the noun phraseparser was evaluated on the Wall Street Journaldatabase in Tipster Disk2 using 50 TREC-5 ad hoctopics.
Experiment results on this 250-megabytedocument collection have shown that using differ-ent kinds of syntactic phrases provided by the nounphrase parser to supplement single words for index-ing can significantly improve the retrieval perfor-mance, which is more encouraging than many earlyexperiments on syntactic phrase indexing.
Thus, us-ing selective NLP, such as the noun phrase parsingtechnique we proposed, is not only feasible for use ininformation retrieval, but also effective in enhancingthe retrieval performance./?1?Whether such syntactic phrases are more effectivethan simple statistical phrases (e.g., high frequency word317There are two lines of future work:First, the results from information retrieval ex-periments often show variances on different kindsof document collections and different sizes of collec-tions.
It is thus desirable to test the noun phraseparsing technique in other and larger collections.More experiments and analyses are also needed tobetter understand how to more effectively combinedifferent phrases with single words.
In addition, itis very important o study how such phrase effectsinteract with other useful IR techniques such as rel-evancy feedback, query expansion, and term weight-ing.Second, it is desirable to study how the parsingquality (e.g., in terms of the ratio of phrases parsedcorrectly) would affect the retrieval performance.
Itis very interesting to try the conditional probabil-ity model as mentioned in a footnote in section 3The improvement of the probabilistic model of nounphrase parsing may result in phrases of higher qual-ity than the phrases produced by the current nounphrase parser.
Intuitively, the use of higher qual-ity phrases might enhance document indexing moreeffectively, but this again needs to be tested.7 AcknowledgmentsThe author is especially grateful to David A. Evansfor his advising and supporting of this work.
Thanksare also due to John Lafferty, Nata~a Milid-Frayling,Xiang Tong, and two anonymous reviewers for theiruseful comments.
Naturally, the author alone is re-sponsible for all the errors.ReferencesBelkin, N., and Croft, B.
1987.
Retrieval techniques.In: Williams, Martha E.(Ed.
), Annual Review ofInformation Science Technology, Vol.
22.
Amster-dam, NL: Elsevier Science Publishers.
1987.
110-145.Brown, P. et at.
1992.
Class-based n-gram modelsof natural language.
Computational Linguistics,18(4), December, 1992.
467-479.Dempster, A. P. et al 1977.
Maximum likelihoodfrom incomplete data via the EM algorithm.
Jour-nal of the Royal Statistical Society, 39 B, 1977.1-38.Evans, D. A., Ginther-Webster, K. , Hart, M., Lef-ferts, R., Monarch, I., 1991.
Automatic indexingusing selective NLP and first-order thesauri.
In:A. Lichnerowicz (ed.
), Intelligent Text and Im-age Handling.
Proceedings of a Conference, RIAO'91.
Amsterdam, NL: Elsevier.
1991. pp.
624-644.Evans, D. A., Lefferts, R. G., Grefenstette, G., Han-derson, S. H., Hersh, W. R., and Archbold, A.bigrams) remains to be tested.318A.
1993.
CLARIT TREC design, experiments,and results.
In: Donna K. Harman (ed.
), TheFirst Text REtrieval Conference (TREC-1).
NISTSpecial Publication 500-207.
Washington, DC:U.S. Government Printing Office, 1993. pp.
251-286; 494-501.Evans, David A. and Lefferts, Robert G. 1995.CLARIT-TREC experiments, Information Pro-cessing and Management, Vol.
31, No.
3, 1995.385-395.Evans, D., Milid-Frayling, N., and Lefferts, R. 1996.CLARIT TREC-4 Experiments, in Donna K. Hat-man (Ed.
), The Fourth Text REtrieval Confer-ence (TREC-~).
NIST Special Publication 500-236.
Washington, DC: U.S. Government PrintingOffice, 1996. pp.
305-321.Evans, D. and Zhai, C. 1996.
Noun-phrase analy-sis in unrestricted text for information retrieval.Proceedings of the 34th Annual meeting of Associ-ation for Computational Linguistics, Santa Cruz,University of California, June 24-28, 1996.17-24.Fagan, Joel L. 1987.
Experiments in AutomaticPhrase Indexing for Document Retrieval: A Com-parison of Syntactic and Non-syntactic methods,PhD thesis, Dept.
of Computer Science, CornellUniversity, Sept. 1987.Harman, D. 1994.
The Second Text REtrieval Con-ference (TREC-2), NIST Special publication 500-215.
National Institute of Standards and Technol-ogy, 1994.Harman, D. 1996.
TREC 5 Conference Notes, Nov.20-22, 1996.Jelinek, F., Lafferty, J.D., and Mercer, R. L. 1990.Basic methods of probabilistic ontext free gram-mars.
Yorktown Heights,N.Y.
: IBM T.J. Wat-son Research Center, 1990.
Research report RC.16374.Lafferty, J.
1995.
Notes on the EM Algorithm, In-formation Theory course notes, Carnegie MellonUniversity.Lafferty, J.
1996.
Personal Communications.Lauer, Mark.
1994.
Conceptual association for com-pound noun analysis.
Proceedings of the 32nd An-nual Meeting of the Association for Computa-tional Linguistics, Student Session, Las Cruces,NM, 1994.
337-339.Lauer, Mark.
1995.
Corpus statistics meet with thenoun compound: Some empirical results.
Proceed-ings of the 33th Annual Meeting of the Associationfor Computational Linguistics, 1995.Lewis, D. 1991.
Representation a d Learning in In-formation Retrieval.
Ph.D thesis, COINS Techni-cal Report 91-93, Univ.
of Massachusetts, 1991.Lewis, D. and Sparck Jones, K. 1996.
Applicationsof natural language processing in information re-trieval.
Communications ofACM, Vol.
39, No.
1,1996, 92-101.Liberman, M. and Sproat, R. 1992.
The stress andstructure of modified noun phrases in English.
In:Sag, I. and Szabolcsi, A.
(Eds.
), Lexical Matters,CSLI Lecture Notes No.
24.
University of ChicagoPress, 1992.
131-181.Marcus, Mitchell.
1980.
A Theory of Syntactic Recognition for Natural Language.
MIT Press, Cam-bridge, MA, 1980.Pustejovsky, J., Bergler, S., and Anick, P. 1993.
Lex-ical semantic techniques for corpus analysis.
In:Computational Linguistics, Vol.
19 (2), Special Is-sue on Using Large Corpora II, 1993.
331-358.Resnik, P. and Hearst, M. 1993.
Structural ambi-guity and conceptual relations.
In: Proceedings ofthe Workshop on Very Large Corpora: Academicand Industrial Perspectives, June 22, 1993.
OhioState University.
58-64.Salton, G. and McGill, M. 1983.
Introduction toModern Information Retrieval, New York, NY:McGraw-Hill, 1983.Smeaton, Alan F. 1992.
Progress in application ofnatural language processing to information re-trieval.
The Computer Journal, Vol.
35, No.
3,1992.
268-278.Strzalkowski, T. 1992.
TTP: A fast and robustparser for natural language processing.
Proceed-ings of the l~th International Conference on Com-putational Linguistics (COLING),Nantes, France,July, 1992.
198-204.Strzalkowski, T. and Vauthey, B.
1992.
Informationretrieval using robust natural anguage processing.Proceedings of the 30th ACL Meeting, Neward,DE, June-July, 1992.
104-111.Strzalkowski, T. and Carballo, J.
1994.
Recent de-velopments in natural anguage text retrieval.
In:Harman, D.
(Ed.
), The Second Text REtrievalConference (TREC-2), NIST Special Publication500-215.
1994.
123-136.Strzalkowski, T. 1995.
Natural language informa-tion retrieval.
Information Processing and Man-agement.
Vol.
31, No.
3, 1995.
397-417.Strzalkowski, T. et al 1995.
Natural language in-formation retrieval: TREC-3 report.
In: Har-man, D.
(Ed.
), The Third Text REtrieval Con-ference (TREC-3), NIST Special Publication 500-225.
1995.39-53.Strzalkowski, T. et al 1996.
Natural language in-formation retrieval: TREC-4 report.
In: Har-man, D.
(Ed.
), The Fourth Text REtrieval Con-ference (TREC-4).
NIST Special Publication 500-319236.
Washington, DC: U.S. Government PrintingOffice, 1996. pp.
245-258.Zhai, C., Tong, X., Milid-Frayling, N., and Evans D.1997.
Evaluation of syntactic phrase indexing -CLARIT TREC5 NLP track report, to appear inThe Fifth Text REtrieval Conference (TREC-5),NIST special publication, 1997, forthcoming.
