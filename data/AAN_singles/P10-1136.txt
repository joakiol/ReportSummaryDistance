Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1337?1345,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsUnderstanding the Semantic Structure of Noun Phrase QueriesXiao LiMicrosoft ResearchOne Microsoft WayRedmond, WA 98052 USAxiaol@microsoft.comAbstractDetermining the semantic intent of webqueries not only involves identifying theirsemantic class, which is a primary focusof previous works, but also understandingtheir semantic structure.
In this work, weformally define the semantic structure ofnoun phrase queries as comprised of intentheads and intent modifiers.
We presentmethods that automatically identify theseconstituents as well as their semantic rolesbased on Markov and semi-Markov con-ditional random fields.
We show that theuse of semantic features and syntactic fea-tures significantly contribute to improvingthe understanding performance.1 IntroductionWeb queries can be considered as implicit ques-tions or commands, in that they are performed ei-ther to find information on the web or to initiateinteraction with web services.
Web users, how-ever, rarely express their intent in full language.For example, to find out ?what are the movies of2010 in which johnny depp stars?, a user may sim-ply query ?johnny depp movies 2010?.
Today?ssearch engines, generally speaking, are based onmatching such keywords against web documentsand ranking relevant results using sophisticatedfeatures and algorithms.As search engine technologies evolve, it is in-creasingly believed that search will be shiftingaway from ?ten blue links?
toward understandingintent and serving objects.
This trend has beenlargely driven by an increasing amount of struc-tured and semi-structured data made available tosearch engines, such as relational databases andsemantically annotated web documents.
Search-ing over such data sources, in many cases, canoffer more relevant and essential results com-pared with merely returning web pages that con-tain query keywords.
Table 1 shows a simplifiedview of a structured data source, where each rowrepresents a movie object.
Consider the query?johnny depp movies 2010?.
It is possible to re-trieve a set of movie objects from Table 1 thatsatisfy the constraints Year = 2010 and Cast 3Johnny Depp.
This would deliver direct answers tothe query rather than having the user sort throughlist of keyword results.In no small part, the success of such an ap-proach relies on robust understanding of query in-tent.
Most previous works in this area focus onquery intent classification (Shen et al, 2006; Liet al, 2008b; Arguello et al, 2009).
Indeed, theintent class information is crucial in determiningif a query can be answered by any structured datasources and, if so, by which one.
In this work, wego one step further and study the semantic struc-ture of a query, i.e., individual constituents of aquery and their semantic roles.
In particular, wefocus on noun phrase queries.
A key contributionof this work is that we formally define query se-mantic structure as comprised of intent heads (IH)and intent modifiers (IM), e.g.,[IM:Title alice in wonderland] [IM:Year 2010] [IH cast]It is determined that ?cast?
is an IH of the abovequery, representing the essential information theuser intends to obtain.
Furthermore, there are twoIMs, ?alice in wonderland?
and ?2010?, serving asfilters of the information the user receives.Identifying the semantic structure of queries canbe beneficial to information retrieval.
Knowingthe semantic role of each query constituent, we1337Title Year Genre Director Cast ReviewPrecious 2009 Drama Lee Daniels Gabby Sidibe, Mo?Nique,.
.
.2012 2009 Action, Sci Fi Roland Emmerich John Cusack, Chiwetel Ejiofor,.
.
.Avatar 2009 Action, Sci Fi James Cameron Sam Worthington, Zoe Saldana,.
.
.The Rum Diary 2010 Adventure, Drama Bruce Robinson Johnny Depp,Giovanni Ribisi,.
.
.Alice in Wonderland 2010 Adventure, Family Tim Burton Mia Wasikowska, Johnny Depp,.
.
.Table 1: A simplified view of a structured data source for the Movie domain.can reformulate the query into a structured formor reweight different query constituents for struc-tured data retrieval (Robertson et al, 2004; Kimet al, 2009; Paparizos et al, 2009).
Alternatively,the knowledge of IHs, IMs and semantic labels ofIMs may be used as additional evidence in a learn-ing to rank framework (Burges et al, 2005).A second contribution of this work is to presentmethods that automatically extract the semanticstructure of noun phrase queries, i.e., IHs, IMsand the semantic labels of IMs.
In particular, weinvestigate the use of transition, lexical, semanticand syntactic features.
The semantic features canbe constructed from structured data sources or bymining query logs, while the syntactic features canbe obtained by readily-available syntactic analy-sis tools.
We compare the roles of these featuresin two discriminative models, Markov and semi-Markov conditional random fields.
The secondmodel is especially interesting to us since in ourtask it is beneficial to use features that measuresegment-level characteristics.
Finally, we evaluateour proposed models and features on manually-annotated query sets from three domains, whileour techniques are general enough to be appliedto many other domains.2 Related Works2.1 Query intent understandingAs mentioned in the introduction, previous workson query intent understanding have largely fo-cused on classification, i.e., automatically map-ping queries into semantic classes (Shen et al,2006; Li et al, 2008b; Arguello et al, 2009).There are relatively few published works on un-derstanding the semantic structure of web queries.The most relevant ones are on the problem ofquery tagging, i.e., assigning semantic labels toquery terms (Li et al, 2009; Manshadi and Li,2009).
For example, in ?canon powershot sd850camera silver?, the word ?canon?
should be taggedas Brand.
In particular, Li et al leveraged click-through data and a database to automatically de-rive training data for learning a CRF-based tagger.Manshadi and Li developed a hybrid, generativegrammar model for a similar task.
Both works areclosely related to one aspect of our work, whichis to assign semantic labels to IMs.
A key differ-ence is that they do not conceptually distinguishbetween IHs and IMs.On the other hand, there have been a series ofresearch studies related to IH identification (Pascaand Durme, 2007; Pasca and Durme, 2008).
Theirmethods aim at extracting attribute names, suchas cost and side effect for the concept Drug, fromdocuments and query logs in a weakly-supervisedlearning framework.
When used in the contextof web queries, attribute names usually serve asIHs.
In fact, one immediate application of theirresearch is to understand web queries that requestfactual information of some concepts, e.g.
?asiprincost?
and ?aspirin side effect?.
Their framework,however, does not consider the identification andcategorization of IMs (attribute values).2.2 Question answeringQuery intent understanding is analogous to ques-tion understanding for question answering (QA)systems.
Many web queries can be viewed as thekeyword-based counterparts of natural languagequestions.
For example, the query ?california na-tional?
and ?national parks califorina?
both implythe question ?What are the national parks in Cali-fornia??.
In particular, a number of works investi-gated the importance of head noun extraction inunderstanding what-type questions (Metzler andCroft, 2005; Li et al, 2008a).
To extract headnouns, they applied syntax-based rules using theinformation obtained from part-of-speech (POS)tagging and deep parsing.
As questions posedin natural language tend to have strong syntacticstructures, such an approach was demonstrated tobe accurate in identifying head nouns.In identifying IHs in noun phrase queries, how-ever, direct syntactic analysis is unlikely to be aseffective.
This is because syntactic structures arein general less pronounced in web queries.
In this1338work, we propose to use POS tagging and parsingoutputs as features, in addition to other features, inextracting the semantic structure of web queries.2.3 Information extractionFinally, there exist large bodies of work on infor-mation extraction using models based on Markovand semi-Markov CRFs (Lafferty et al, 2001;Sarawagi and Cohen, 2004), and in particular forthe task of named entity recognition (McCallumand Li, 2003).The problem studied in this work is concernedwith identifying more generic ?semantic roles?
ofthe constituents in noun phrase queries.
Whilesome IM categories belong to named entities suchas IM:Director for the intent class Movie, therecan be semantic labels that are not named entitiessuch as IH and IM:Genre (again for Movie).3 Query Semantic StructureUnlike database query languages such as SQL,web queries are usually formulated as sequencesof words without explicit structures.
This makesweb queries difficult to interpret by computers.For example, should the query ?aspirin side effect?be interpreted as ?the side effect of aspirin?
or ?theaspirin of side effect??
Before trying to build mod-els that can automatically makes such decisions,we first need to understand what constitute the se-mantic structure of a noun phrase query.3.1 DefinitionWe let C denote a set of query intent classes thatrepresent semantic concepts such as Movie, Prod-uct and Drug.
The query constituents introducedbelow are all defined w.r.t.
the intent class of aquery, c ?
C, which is assumed to be known.Intent headAn intent head (IH) is a query segment that cor-responds to an attribute name of an intent class.For example, the IH of the query ?alice in won-derland 2010 cast?
is ?cast?, which is an attributename of Movie.
By issuing the query, the user in-tends to find out the values of the IH (i.e., cast).
Aquery can have multiple IHs, e.g., ?movie avatardirector and cast?.
More importantly, there canbe queries without an explicit IH.
For example,?movie avatar?
does not contain any segment thatcorresponds to an attribute name of Movie.
Such aquery, however, does have an implicit intent whichis to obtain general information about the movie.Intent modifierIn contrast, an intent modifier (IM) is a query seg-ment that corresponds to an attribute value (ofsome attribute name).
The role of IMs is to impos-ing constraints on the attributes of an intent class.For example, there are two constraints implied inthe query ?alice in wonderland 2010 cast?
: (1) theTitle of the movie is ?alice in wonderland?
; and(2) the Year of the movie is ?2010?.
Interestingly,the user does not explicitly specify the attributenames, i.e., Title and Year, in this query.
Suchinformation, however, can be inferred given do-main knowledge.
In fact, one important goal ofthis work is to identify the semantic labels of IMs,i.e., the attribute names they implicitly refer to.
Weuse Ac to denote the set of IM semantic labels forthe intent class c.OtherAdditionally, there can be query segments that donot play any semantic roles, which we refer to asOther.3.2 Syntactic analysisThe notion of IHs and IMs in this work is closelyrelated to that of linguistic head nouns and modi-fiers for noun phrases.
In many cases, the IHs ofnoun phrase queries are exactly the head nouns inthe linguistic sense.
Exceptions mostly occur inqueries without explicit IHs, e.g., ?movie avatar?in which the head noun ?avatar?
serves as an IMinstead.
Due to the strong resemblance, it is inter-esting to see if IHs can be identified by extractinglinguistic head nouns from queries based on syn-tactic analysis.
To this end, we apply the follow-ing heuristics for head noun extraction.
We firstrun a POS-tagger and a chunker jointly on eachquery, where the POS-tagger/chunker is based onan HMM system trained on English Penn Tree-bank (Gao et al, 2001).
We then mark the rightmost NP chunk before any prepositional phraseor adjective clause, and apply the NP head rules(Collins, 1999) to the marked NP chunk.The main problem with this approach, however,is that a readily-available POS tagger or chunker isusually trained on natural language sentences andthus is unlikely to produce accurate results on webqueries.
As shown in (Barr et al, 2008), the lexi-cal category distribution of web queries is dramat-ically different from that of natural languages.
Forexample, prepositions and subordinating conjunc-tions, which are strong indicators of the syntactic1339structure in natural languages, are often missing inweb queries.
Moreover, unlike most natural lan-guages that follow the linear-order principle, webqueries can have relatively free word orders (al-though some orders may occur more often thanothers statistically).
These factors make it diffi-cult to produce reliable syntactic analysis outputs.Consequently, the head nouns and hence the IHsextracted therefrom are likely to be error-prone, aswill be shown by our experiments in Section 6.3.Although a POS tagger and a chunker may notwork well on queries, their output can be used asfeatures for learning statistical models for seman-tic structure extraction, which we introduce next.4 ModelsThis section presents two statistical models for se-mantic understanding of noun phrase queries.
As-suming that the intent class c ?
C of a query isknown, we cast the problem of extracting the se-mantic structure of the query into a joint segmen-tation/classification problem.
At a high level, wewould like to identify query segments that corre-spond to IHs, IMs and Others.
Furthermore, foreach IM segment, we would like to assign a se-mantic label, denoted by IM:a, a ?
Ac, indicatingwhich attribute name it refers to.
In other words,our label set consists of Y = {IH, {IM:a}a?Ac ,Other}.Formally, we let x = (x1, x2, .
.
.
, xM ) denotean input query of length M .
To avoid confusion,we use i to represent the index of a word tokenand j to represent the index of a segment in thefollowing text.
Our goal is to obtains?
= argmaxsp(s|c,x) (1)where s = (s1, s2, .
.
.
, sN ) denotes a query seg-mentation as well as a classification of all seg-ments.
Each segment sj is represented by a tu-ple (uj, vj , yj).
Here uj and vj are the indices ofthe starting and ending word tokens respectively;yj ?
Y is a label indicating the semantic role ofs.
We further augment the segment sequence withtwo special segments: Start and End, representedby s0 and sN+1 respectively.
For notional simplic-ity, we assume that the intent class is given anduse p(s|x) as a shorthand for p(s|c,x), but keep inmind that the label space and hence the parameterspace is class-dependent.
Now we introduce twomethods of modeling p(s|x).4.1 CRFsOne natural approach to extracting the semanticstructure of queries is to use linear-chain CRFs(Lafferty et al, 2001).
They model the con-ditional probability of a label sequence giventhe input, where the labels, denoted as y =(y1, y2, .
.
.
, yM ), yi ?
Y , have a one-to-one cor-respondence with the word tokens in the input.Using linear-chain CRFs, we aim to find the la-bel sequence that maximizesp?
(y|x) =1Z?(x)exp{M+1?i=1?
?
f(yi?1, yi,x, i)}.
(2)The partition function Z?
(x) is a normalizationfactor.
?
is a weight vector and f(yi?1, yi,x) isa vector of feature functions referred to as a fea-ture vector.
The features used in CRFs will be de-scribed in Section 5.Given manually-labeled queries, we estimate ?that maximizes the conditional likelihood of train-ing data while regularizing model parameters.
Thelearned model is then used to predict the label se-quence y for future input sequences x.
To obtain sin Equation (1), we simply concatenate the maxi-mum number of consecutive word tokens that havethe same label and treat the resulting sequence as asegment.
By doing this, we implicitly assume thatthere are no two adjacent segments with the samelabel in the true segment sequence.
Although thisassumption is not always correct in practice, weconsider it a reasonable approximation given whatwe empirically observed in our training data.4.2 Semi-Markov CRFsIn contrast to standard CRFs, semi-Markov CRFsdirectly model the segmentation of an input se-quence as well as a classification of the segments(Sarawagi and Cohen, 2004), i.e.,p(s|x) = 1Z?(x)expN+1?j=1?
?
f(sj?1, sj,x) (3)In this case, the features f(sj?1, sj ,x) are de-fined on segments instead of on word tokens.More precisely, they are of the function formf(yj?1, yj,x, uj , vj).
It is easy to see that byimposing a constraint ui = vi, the model isreduced to standard linear-chain CRFs.
Semi-Markov CRFs make Markov assumptions at thesegment level, thereby naturally offering means to1340CRF featuresA1: Transition ?
(yi?1 = a)?
(yi = b) transiting from state a to bA2: Lexical ?
(xi = w)?
(yi = b) current word is wA3: Semantic ?
(xi ?
WL)?
(yi = b) current word occurs in lexicon LA4: Semantic ?
(xi?1:i ?
WL)?
(yi = b) current bigram occurs in lexicon LA5: Syntactic ?
(POS(xi) = z)?
(yi = b) POS tag of the current word is zSemi-Markov CRF featuresB1: Transition ?
(yj?1 = a)?
(yj = b) Transiting from state a to bB2: Lexical ?
(xuj :vj = w)?
(yj = b) Current segment is wB3: Lexical ?
(xuj :vj 3 w)?
(yj = b) Current segment contains word wB4: Semantic ?
(xuj :vj ?
L)?
(yj = b) Current segment is an element in lexicon LB5: Semantic maxl?Ls(xuj :vj , l)?
(yj = b) The max similarity between the segment and elements in LB6: Syntactic ?
(POS(xuj :vj ) = z)?
(yj = b) Current segment?s POS sequence is zB7: Syntactic ?
(Chunk(xuj :vj ) = c)?
(yj = b) Current segment is a chunk with phrase type cTable 2: A summary of feature types in CRFs and segmental CRFs for query understanding.
We assumethat the state label is b in all features and omit this in the feature descriptions.incorporate segment-level features, as will be pre-sented in Section 5.5 FeaturesIn this work, we explore the use of transition, lexi-cal, semantic and syntactic features in Markov andsemi-Markov CRFs.
The mathematical expressionof these features are summarized in Table 2 withdetails described as follows.5.1 Transition featuresTransition features, i.e., A1 and B1 in Table 2,capture state transition patterns between adjacentword tokens in CRFs, and between adjacent seg-ments in semi-Markov CRFs.
We only use first-order transition features in this work.5.2 Lexical featuresIn CRFs, a lexical feature (A2) is implemented asa binary function that indicates whether a specificword co-occurs with a state label.
The set of wordsto be considered in this work are those observedin the training data.
We can also generalize thistype of features from words to n-grams.
In otherwords, instead of inspecting the word identity atthe current position, we inspect the n-gram iden-tity by applying a window of length n centered atthe current position.Since feature functions are defined on segmentsin semi-Markov CRFs, we create B2 that indicateswhether the phrase in a hypothesized query seg-ment co-occurs with a state label.
Here the set ofphrase identities are extracted from the query seg-ments in the training data.
Furthermore, we createanother type of lexical feature, B3, which is acti-vated when a specific word occurs in a hypothe-sized query segment.
The use of B3 would favorunseen words being included in adjacent segmentsrather than to be isolated as separate segments.5.3 Semantic featuresModels relying on lexical features may requirevery large amounts of training data to produceaccurate prediction performance, as the featurespace is in general large and sparse.
To make ourmodel generalize better, we create semantic fea-tures based on what we call lexicons.
A lexicon,denoted as L, is a cluster of semantically-relatedwords/phrases.
For example, a cluster of movietitles or director names can be such a lexicon.
Be-fore describing how such lexicons are generatedfor our task, we first introduce the forms of thesemantic features assuming the availability of thelexicons.We let L denote a lexicon, and WL denote theset of n-grams extracted from L. For CRFs, wecreate a binary function that indicates whether anyn-gram in WL co-occurs with a state label, withn = 1, 2 for A3, A4 respectively.
For both A3and A4, the number of such semantic features isequal to the number of lexicons multiplied by thenumber of state labels.The same source of semantic knowledge can beconveniently incorporated in semi-Markov CRFs.One set of semantic features (B4) inspect whetherthe phrase of a hypothesized query segmentmatches any element in a given lexicon.
A sec-ond set of semantic features (B5) relax the exactmatch constraints made by B4, and take as the fea-ture value the maximum ?similarity?
between thequery segment and all lexicon elements.
The fol-1341lowing similarity function is used in this work ,s(xuj :vj , l) = 1?
Lev(xuj :vj , l)/|l| (4)where Lev represents the Levenshtein distance.Notice that we normalize the Levenshtein distanceby the length of the lexicon element, as we em-pirically found it performing better compared withnormalizing by the length of the segment.
In com-puting the maximum similarity, we first retrieve aset of lexicon elements with a positive tf-idf co-sine distance with the segment; we then evaluateEquation (4) for each retrieved element and findthe one with the maximum similarity score.Lexicon generationTo create the semantic features described above,we generate two types of lexicons leveragingdatabases and query logs for each intent class.The first type of lexicon is an IH lexicon com-prised of a list of attribute names for the intentclass, e.g., ?box office?
and ?review?
for the intentclass Movie.
One easy way of composing such alist is by aggregating the column names in the cor-responding database such as Table 1.
However,this approach may result in low coverage on IHsfor some domains.
Moreover, many database col-umn names, such as Title, are unlikely to appear asIHs in queries.
Inspired by Pasca and Van Durme(2007), we apply a bootstrapping algorithm thatautomatically learns attribute names for an intentclass from query logs.
The key difference fromtheir work is that we create templates that consistof semantic labels at the segment level from train-ing data.
For example, ?alice in wonderland 2010cast?
is labeled as ?IM:Title IM:Year IH?, and thus?IM:Title + IM:Year + #?
is used as a template.
Weselect the most frequent templates (top 2 in thiswork) from training data and use them to discovernew IH phrases from the query log.Secondly, we have a set IM lexicons, each com-prised of a list of attribute values of an attributename in Ac.
We exploit internal resources to gen-erate such lexicons.
For example, the lexicon forIM:Title (in Movie) is a list of movie titles gener-ated by aggregating the values in the Title columnof a movie database.
Similarly, the lexicon forIM:Employee (in Job) is a list of employee namesextracted from a job listing database.
Note thata substantial amount of research effort has beendedicated to automatic lexicon acquisition fromthe Web (Pantel and Pennacchiotti, 2006; Pennac-chiotti and Pantel, 2009).
These techniques can beused in expanding the semantic lexicons for IMswhen database resources are not available.
But wedo not use such techniques in our work since thelexicons extracted from databases in general havegood precision and coverage.5.4 Syntactic featuresAs mentioned in Section 3.2, web queries oftenlack syntactic cues and do not necessarily followthe linear order principle.
Consequently, applyingsyntactic analysis such as POS tagging or chunk-ing using models trained on natural language cor-pora is unlikely to give accurate results on webqueries, as supported by our experimental evi-dence in Section 6.3.
It may be beneficial, how-ever, to use syntactic analysis results as additionalevidence in learning.To this end, we generate a sequence of POS tagsfor a given query, and use the co-occurrence ofPOS tag identities and state labels as syntactic fea-tures (A5) for CRFs.For semi-Markov CRFs, we instead examinethe POS tag sequence of the corresponding phrasein a query segment.
Again their identities are com-bined with state labels to create syntactic featuresB6.
Furthermore, since it is natural to incorporatesegment-level features in semi-Markov CRFs, wecan directly use the output of a syntactic chunker.To be precise, if a query segment is determined bythe chunker to be a chunk, we use the indicator ofthe phrase type of the chunk (e.g., NP, PP) com-bined with a state label as the feature, denoted byB7 in the Table.
Such features are not activated ifa query segment is determined not to be a chunk.6 Evaluation6.1 DataTo evaluate our proposed models and features, wecollected queries from three domains, Movie, Joband National Park, and had them manually anno-tated.
The annotation was given on both segmen-tation of the queries and classification of the seg-ments according to the label sets defined in Ta-ble 3.
There are 1000/496 samples in the train-ing/test set for the Movie domain, 600/366 for theJob domain and 491/185 for the National Park do-main.
In evaluation, we report the test-set perfor-mance in each domain as well as the average per-formance (weighted by their respectively test-setsize) over all domains.1342Movie Job National ParkIH trailer, box office IH listing, salary IH lodging, calendarIM:Award oscar best picture IM:Category engineering IM:Category national forestIM:Cast johnny depp IM:City las vegas IM:City pageIM:Character michael corleone IM:County orange IM:Country usIM:Category tv series IM:Employer walmart IM:Name yosemiteIM:Country american IM:Level entry level IM:POI volcanoIM:Director steven spielberg IM:Salary high-paying IM:Rating bestIM:Genre action IM:State florida IM:State flordiaIM:Rating best IM:Type full timeIM:Title the godfatherOther the, in, that Other the, in, that Other the, in, thatTable 3: Label sets and their respective query segment examples for the intent class Movie, Job andNational Park.6.2 MetricsThere are two evaluation metrics used in our work:segment F1 and sentence accuracy (Acc).
Thefirst metric is computed based on precision and re-call at the segment level.
Specifically, let us as-sume that the true segment sequence of a queryis s = (s1, s2, .
.
.
, sN ), and the decoded segmentsequence is s?
= (s?1, s?2, .
.
.
, s?K).
We say thats?k is a true positive if s?k ?
s. The precisionand recall, then, are measured as the total num-ber of true positives divided by the total num-ber of decoded and true segments respectively.We report the F1-measure which is computed as2 ?
prec ?
recall/(prec + recall).Secondly, a sentence is correct if all decodedsegments are true positives.
Sentence accuracy ismeasured by the total number of correct sentencesdivided by the total number of sentences.6.3 ResultsWe start with models that incorporate first-ordertransition features which are standard for bothMarkov and semi-Markov CRFs.
We then exper-iment with lexical features, semantic features andsyntactic features for both models.
Table 4 andTable 5 give a summarization of all experimentalresults.Lexical featuresThe first experiment we did is to evaluate the per-formance of lexical features (combined with tran-sition features).
This involves the use of A2 in Ta-ble 2 for CRFs, and B2 and B3 for semi-MarkovCRFs.
Note that adding B3, i.e., indicators ofwhether a query segment contains a word iden-tity, gave an absolute 7.0%/3.2% gain in sentenceaccuracy and segment F1 on average, as shownin the row B1-B3 in Table 5.
For both A2 andB3, we also tried extending the features based onword IDs to those based on n-gram IDs, wheren = 1, 2, 3.
This greatly increased the number oflexical features but did not improve learning per-formance, most likely due to the limited amountsof training data coupled with the sparsity of suchfeatures.
In general, lexical features do not gener-alize well to the test data, which accounts for therelatively poor performance of both models.Semantic featuresWe created IM lexicons from three in-housedatabases on Movie, Job and National Parks.Some lexicons, e.g., IM:State, are shared acrossdomains.
Regarding IH lexicons, we applied thebootstrapping algorithm described in Section 5.3to a 1-month query log of Bing.
We selected themost frequent 57 and 131 phrases to form the IHlexicons for Movie and National Park respectively.We do not have an IH lexicon for Job as the at-tribute names in that domain are much fewer andare well covered by training set examples.We implemented A3 and A4 for CRFs, whichare based on the n-gram sets created from lex-icons; and B4 and B5 for semi-Markov CRFs,which are based on exact and fuzzy match withlexicon items.
As shown in Table 4 and 5, drasticincreases in sentence accuracies and F1-measureswere observed for both models.Syntactic featuresAs shown in the row A1-A5 in Table 4, combinedwith all other features, the syntactic features (A5)built upon POS tags boosted the CRF model per-formance.
Table 6 listed the most dominant pos-itive and negative features based on POS tags forMovie (features for the other two domains are notreported due to space limit).
We can see thatmany of these features make intuitive sense.
For1343Movie Job National Park AverageFeatures Acc F1 Acc F1 Acc F1 Acc F1A1,A2: Tran + Lex 59.9 75.8 65.6 84.7 61.6 75.6 62.1 78.9A1-A3: Tran + Lex + Sem 67.9 80.2 70.8 87.4 70.5 80.8 69.4 82.8A1-A4: Tran + Lex + Sem 72.4 83.5 72.4 89.7 71.1 82.3 72.2 85.0A1-A5: Tran + Lex + Sem + Syn 74.4 84.8 75.1 89.4 75.1 85.4 74.8 86.5A2-A5: Lex + Sem + Syn 64.9 78.8 68.1 81.1 64.8 83.7 65.4 81.0Table 4: Sentence accuracy (Acc) and segment F1 (F1) using CRFs with different features.Movie Job National Park AverageFeatures Acc F1 Acc F1 Acc F1 Acc F1B1,B2: Tran + Lex 53.4 71.6 59.6 83.8 60.0 77.3 56.7 76.9B1-B3: Tran + Lex 61.3 77.7 65.9 85.9 66.0 80.7 63.7 80.1B1-B4: Tran + Lex + Sem 73.8 83.6 76.0 89.7 74.6 85.3 74.7 86.1B1-B5: Tran + Lex + Sem 75.0 84.3 76.5 89.7 76.8 86.8 75.8 86.6B1-B6: Tran + Lex + Sem + Syn 75.8 84.3 76.2 89.7 76.8 87.2 76.1 86.7B1-B5,B7: Tran + Lex + Sem + Syn 75.6 84.1 76.0 89.3 76.8 86.8 75.9 86.4B2-B6:Lex + Sem + Syn 72.0 82.0 73.2 87.9 76.5 89.3 73.8 85.6Table 5: Sentence accuracy (Acc) and segment F1 (F1) using semi-Markov CRFs with different features.example, IN (preposition or subordinating con-junction) is a strong indicator of Other, while TOand IM:Date usually do not co-occur.
Some fea-tures, however, may appear less ?correct?.
Thisis largely due to the inaccurate output of the POStagger.
For example, a large number of actornames were mis-tagged as RB, resulting in a highpositive weight of the feature (RB, IM:Cast).Positive Negative(IN, Other), (TO, IM:Date)(VBD, Other) (IN, IM:Cast)(CD, IM:Date) (CD, IH)(RB, IM:Cast) (IN, IM:Character)Table 6: Syntactic features with the largest posi-tive/negative weights in the CRF model for MovieSimilarly, we added segment-level POS tag fea-tures (B6) to semi-Markov CRFs, which lead tothe best overall results as shown by the highlightednumbers in Table 5.
Again many of the dominantfeatures are consistent with our intuition.
For ex-ample, the most positive feature for Movie is (CDJJS, IM:Rating) (e.g.
100 best).
When syntacticfeatures based on chunking results (B7) are usedinstead of B6, the performance is not as good.Transition featuresIn addition, it is interesting to see the importanceof transition features in both models.
Since webqueries do not generally follow the linear orderprinciple, is it helpful to incorporate transition fea-tures in learning?
To answer this question, wedropped the transition features from the best sys-tems, corresponding to the last rows in Table 4and 5.
This resulted in substantial degradationsin performance.
One intuitive explanation is thatalthough web queries are relatively ?order-free?,statistically speaking, some orders are much morelikely to occur than others.
This makes it benefi-cial to use transition features.Comparison to syntactic analysisFinally, we conduct a simple experiment by usingthe heuristics described in Section 3.2 in extract-ing IHs from queries.
The precision and recall ofIHs averaged over all 3 domains are 50.4% and32.8% respectively.
The precision and recall num-bers from our best model-based system, i.e., B1-B6 in Table 5, are 89.9% and 84.6% respectively,which are significantly better than those based onpure syntactic analysis.7 ConclusionsIn this work, we make the first attempt to definethe semantic structure of noun phrase queries.
Wepropose statistical methods to automatically ex-tract IHs, IMs and the semantic labels of IMs us-ing a variety of features.
Experiments show the ef-fectiveness of semantic features and syntactic fea-tures in both Markov and semi-Markov CRF mod-els.
In the future, it would be useful to exploreother approaches to automatic lexicon discoveryto improve the quality or to increase the coverageof both IH and IM lexicons, and to systematicallyevaluate their impact on query understanding per-formance.The author would like to thank Hisami Suzukiand Jianfeng Gao for useful discussions.1344ReferencesJaime Arguello, Fernando Diaz, Jamie Callan, andJean-Francois Crespo.
2009.
Sources of evidencefor vertical selection.
In SIGIR?09: Proceedings ofthe 32st Annual International ACM SIGIR confer-ence on Research and Development in InformationRetrieval.Cory Barr, Rosie Jones, and Moira Regelson.
2008.The linguistic structure of English web-searchqueries.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 1021?1030.Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton, and Greg Hullender.2005.
Learning to rank using gradient descent.
InICML?05: Proceedings of the 22nd internationalconference on Machine learning, pages 89?96.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun,Ming Zhou, and Chang-Ning Huang.
2001.
Im-proving query translation for CLIR using statisticalmodels.
In SIGIR?01: Proceedings of the 24th An-nual International ACM SIGIR conference on Re-search and Development in Information Retrieval.Jinyoung Kim, Xiaobing Xue, and Bruce Croft.
2009.A probabilistic retrieval model for semistructureddata.
In ECIR?09: Proceedings of the 31st Euro-pean Conference on Information Retrieval, pages228?239.John Lafferty, Andrew McCallum, and FerdandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the InternationalConference on Machine Learning, pages 282?289.Fangtao Li, Xian Zhang, Jinhui Yuan, and XiaoyanZhu.
2008a.
Classifying what-type questions byhead noun tagging.
In COLING?08: Proceedingsof the 22nd International Conference on Computa-tional Linguistics, pages 481?488.Xiao Li, Ye-Yi Wang, and Alex Acero.
2008b.
Learn-ing query intent from regularized click graph.
InSIGIR?08: Proceedings of the 31st Annual Interna-tional ACM SIGIR conference on Research and De-velopment in Information Retrieval, July.Xiao Li, Ye-Yi Wang, and Alex Acero.
2009.
Extract-ing structured information from user queries withsemi-supervised conditional random fields.
In SI-GIR?09: Proceedings of the 32st Annual Interna-tional ACM SIGIR conference on Research and De-velopment in Information Retrieva.Mehdi Manshadi and Xiao Li.
2009.
Semantic taggingof web search queries.
In Proceedings of the 47thAnnual Meeting of the ACL and the 4th IJCNLP ofthe AFNLP.Andrew McCallum and Wei Li.
2003.
Early results fornamed entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings of the seventh conference on Naturallanguage learning at HLT-NAACL 2003, pages 188?191.Donald Metzler and Bruce Croft.
2005.
Analysis ofstatistical question classification for fact-based ques-tions.
Jounral of Information Retrieval, 8(3).Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: Leveraging generic patterns for automati-cally har-vesting semantic relations.
In Proceedingsof the 21st International Conference on Computa-tional Linguis-tics and the 44th annual meeting ofthe ACL, pages 113?120.Stelios Paparizos, Alexandros Ntoulas, John Shafer,and Rakesh Agrawal.
2009.
Answering web queriesusing structured data sources.
In Proceedings of the35th SIGMOD international conference on Manage-ment of data.Marius Pasca and Benjamin Van Durme.
2007.
Whatyou seek is what you get: Extraction of class at-tributes from query logs.
In IJCAI?07: Proceedingsof the 20th International Joint Conference on Artifi-cial Intelligence.Marius Pasca and Benjamin Van Durme.
2008.Weakly-supervised acquisition of open-domainclasses and class attributes from web documents andquery logs.
In Proceedings of ACL-08: HLT.Marco Pennacchiotti and Patrick Pantel.
2009.
Entityextraction via ensemble semantics.
In EMNLP?09:Proceedings of Conference on Empirical Methods inNatural Language Processing, pages 238?247.Stephen Robertson, Hugo Zaragoza, and Michael Tay-lor.
2004.
Simple BM25 extension to multipleweighted fields.
In CIKM?04: Proceedings of thethirteenth ACM international conference on Infor-mation and knowledge management, pages 42?49.Sunita Sarawagi and William W. Cohen.
2004.
Semi-Markov conditional random fields for informationextraction.
In Advances in Neural Information Pro-cessing Systems (NIPS?04).Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen.2006.
Building bridges for web query classification.In SIGIR?06: Proceedings of the 29th Annual Inter-national ACM SIGIR conference on research and de-velopment in information retrieval, pages 131?138.1345
