c?
2004 Association for Computational LinguisticsLearning Domain Ontologies fromDocument Warehouses and DedicatedWeb SitesRoberto Navigli?
Paola VelardiUniversita` di Roma ?La Sapienza?
Universita` di Roma ?La Sapienza?We present a method and a tool, OntoLearn, aimed at the extraction of domain ontologies fromWeb sites, and more generally from documents shared among the members of virtual organiza-tions.
OntoLearn first extracts a domain terminology from available documents.
Then, complexdomain terms are semantically interpreted and arranged in a hierarchical fashion.
Finally, ageneral-purpose ontology, WordNet, is trimmed and enriched with the detected domain concepts.The major novel aspect of this approach is semantic interpretation, that is, the association of acomplex concept with a complex term.
This involves finding the appropriate WordNet conceptfor each word of a terminological string and the appropriate conceptual relations that hold amongthe concept components.
Semantic interpretation is based on a new word sense disambiguationalgorithm, called structural semantic interconnections.1.
IntroductionThe importance of domain ontologies is widely recognized, particularly in relation tothe expected advent of the Semantic Web (Berners-Lee 1999).
The goal of a domain on-tology is to reduce (or eliminate) the conceptual and terminological confusion amongthe members of a virtual community of users (for example, tourist operators, commer-cial enterprises, medical practitioners) who need to share electronic documents andinformation of various kinds.
This is achieved by identifying and properly defining aset of relevant concepts that characterize a given application domain.
An ontology istherefore a shared understanding of some domain of interest (Uschold and Gruninger1996).
The construction of a shared understanding, that is, a unifying conceptual frame-work, fosters?
communication and cooperation among people?
better enterprise organization?
interoperability among systems?
system engineering benefits (reusability, reliability, and specification)Creating ontologies is, however, a difficult and time-consuming process that involvesspecialists from several fields.
Philosophical ontologists and artificial intelligence lo-gicians are usually involved in the task of defining the basic kinds and structuresof concepts (objects, properties, relations, and axioms) that are applicable in every?
Dipartimento di Informatica, Universita` di Roma ?La Sapienza,?
Via Salaria, 113 - 00198 Roma, Italia.E-mail: {navigli, velardi}@di.uniroma1.it.152Computational Linguistics Volume 30, Number 2Core OntologyFoundational  OntologySpecific Domain OntologyFigure 1The three levels of generality of a domain ontology.possible domain.
The issue of identifying these very few ?basic?
principles, now oftenreferred to as foundational ontologies (FOs) (or top, or upper ontologies; see Figure 1)(Gangemi et al 2002), meets the practical need of a model that has as much generalityas possible, to ensure reusability across different domains (Smith and Welty 2001).Domain modelers and knowledge engineers are involved in the task of identify-ing the key domain conceptualizations and describing them according to the organi-zational backbones established by the foundational ontology.
The result of this effortis referred to as the core ontology (CO), which usually includes a few hundred ap-plication domain concepts.
While many ontology projects eventually succeed in thetask of defining a core ontology,1 populating the third level, which we call the specificdomain ontology (SDO), is the actual barrier that very few projects have been able toovercome (e.g., WordNet [Fellbaum 1995], Cyc [Lenat 1993], and EDR [Yokoi 1993]),but they pay a price for this inability in terms of inconsistencies and limitations.2It turns out that, although domain ontologies are recognized as crucial resourcesfor the Semantic Web, in practice they are not available and when available, they arerarely used outside specific research environments.So which features are most needed to build usable ontologies??
Coverage: The domain concepts must be there; the SDO must besufficiently (for the application purposes) populated.
Tools are needed toextensively support the task of identifying the relevant concepts and therelations among them.?
Consensus: Decision making is a difficult activity for one person, and itgets even harder when a group of people must reach consensus on agiven issue and, in addition, the group is geographically dispersed.When a group of enterprises decide to cooperate in a given domain, theyhave first to agree on many basic issues; that is, they must reach aconsensus of the business domain.
Such a common view must bereflected by the domain ontology.?
Accessibility: The ontology must be easily accessible: tools are needed toeasily integrate the ontology within an application that may clearly show1 Several ontologies are already available on the Internet, including a few hundred more or lessextensively defined concepts.2 For example, it has been claimed by several researchers (e.g., Oltramari et al, 2002) that in WordNetthere is no clear separation between concept-synsets, instance-synsets, relation-synsets, andmeta-property-synsets.153Navigli and Velardi Learning Domain Ontologiesits decisive contribution, e.g., improving the ability to share andexchange information through the web.In cooperation with another research institution,3 we defined a general architectureand a battery of systems to foster the creation of such ?usable?
ontologies.
Consensusis achieved in both an implicit and an explicit way: implicit, since candidate conceptsare selected from among the terms that are frequently and consistently employed inthe documents produced by the virtual community of users; explicit, through the useof Web-based groupware aimed at consensual construction and maintenance of anontology.
Within this framework, the proposed tools are OntoLearn, for the automaticextraction of domain concepts from thematic Web sites; ConSys, for the validationof the extracted concepts; and SymOntoX, the ontology management system.
Thisontology-learning architecture has been implemented and is being tested in the con-text of several European projects,4 aimed at improving interoperability for networkedenterprises.In Section 2, we provide an overview of the complete ontology-engineering archi-tecture.
In the remaining sections, we describe in more detail OntoLearn, a system thatuses text mining techniques and existing linguistic resources, such as WordNet andSemCor, to learn, from available document warehouses and dedicated Web sites, do-main concepts and taxonomic relations among them.
OntoLearn automatically buildsa specific domain ontology that can be used to create a specialized view of an exist-ing general-purpose ontology, like WordNet, or to populate the lower levels of a coreontology, if available.2.
The Ontology Engineering ArchitectureFigure 2 reports the proposed ontology-engineering method, that is, the sequence ofsteps and the intermediate outputs that are produced in building a domain ontol-ogy.
As shown in the figure, ontology engineering is an iterative process involvingconcept learning (OntoLearn), machine-supported concept validation (ConSys), andmanagement (SymOntoX).The engineering process starts with OntoLearn exploring available documentsand related Web sites to learn domain concepts and detect taxonomic relations amongthem, producing as output a domain concept forest.
Initially, we base concept learningon external, generic knowledge sources (we use WordNet and SemCor).
In subsequentcycles, the domain ontology receives progressively more use as it becomes adequatelypopulated.Ontology validation is undertaken with ConSys (Missikoff and Wang 2001), aWeb-based groupware package that performs consensus building by means of thor-ough validation by the representatives of the communities active in the applicationdomain.
Throughout the cycle, OntoLearn operates in connection with the ontologymanagement system, SymOntoX (Formica and Missikoff 2003).
Ontology engineers usethis management system to define and update concepts and their mutual connections,thus allowing the construction of a semantic net.
Further, SymOntoX?s environmentcan attach the automatically learned domain concept trees to the appropriate nodes ofthe core ontology, thereby enriching concepts with additional information.
SymOntoX3 The LEKS-CNR laboratory in Rome.4 The Fetish EC project, ITS-13015 (http://fetish.singladura.com/index.php) and the Harmonise ECproject, IST-2000-29329 (http://dbs.cordis.lu), both in the tourism domain, and the INTEROP Networkof Excellence on interoperability IST-2003-508011.154Computational Linguistics Volume 30, Number 2DomainConceptForestOntoLearn ConSys  DomainOntologyApplication domainCommunitiesDomainWeb sitesGenericKnowledgeSourcesSelf-learningSymOntoXValidationSelf-learniS ntoXFigure 2The ontology-engineering chain.also performs consistency checks.
The self-learning cycle in Figure 2 consists, then, oftwo steps: first, domain users and experts use ConSys to validate the automaticallylearned ontology and forward their suggestions to the knowledge engineers, who im-plement them as updates to SymOntoX.
Then, the updated domain ontology is usedby OntoLearn to learn new concepts from new documents.The focus of this article is the description of the OntoLearn system.
Details onother modules of the ontology-engineering architecture can be found in the referencedpapers.3.
Architecture of the OntoLearn SystemFigure 3 shows the architecture of the OntoLearn system.
There are three main phases:First, a domain terminology is extracted from available texts in the application domain(specialized Web sites and warehouses, or documents exchanged among members ofa virtual community), and filtered using natural language processing and statisticaltechniques.
Second, terms are semantically interpreted (in a sense that we clarify inSection 3.2) and ordered according to taxonomic relations, generating a domain con-cept forest (DCF).
Third, the DCF is used to update the existing ontology (WordNetor any available domain ontology).In a ?stand-alone?
mode, OntoLearn automatically creates a specialized view ofWordNet, pruning certain generic concepts and adding new domain concepts.
Whenused within the engineering chain shown in Figure 2, ontology integration and up-dating is performed by the ontology engineers, who update an existing core ontologyusing SymOntoX.In this article we describe the stand-alone procedure.3.1 Phase 1: Terminology ExtractionTerminology is the set of words or word strings that convey a single (possibly complex)meaning within a given community.
In a sense, terminology is the surface appearance,in texts, of the domain knowledge of a community.
Because of their low ambiguityand high specificity, these words are also particularly useful for conceptualizing a knowledgedomain or for supporting the creation of a domain ontology.
Candidate terminologicalexpressions are usually captured with more or less shallow techniques, ranging fromstochastic methods (Church and Hanks 1989; Yamamoto and Church 2001) to moresophisticated syntactic approaches (Jacquemin 1997).155Navigli and Velardi Learning Domain OntologiesWordNetdomaincorpuscontrastivecorporaterminology extractioncandidateextractionterminologyfilteringsemantic interpretationsemanticdisambiguationidentification oftaxonomic relationsidentification ofconceptual relationsInductivelearnerNaturalLanguageProcessorontology integrationand updating321LexicalResourcesDomain Concept ForestFigure 3The architecture of OntoLearn.Obviously, richer syntactic information positively influences the quality of theresult to be input to the statistical filtering.
In our experiments we used the linguis-tic processor ARIOSTO (Basili, Pazienza, and Velardi 1996) and the syntactic parserCHAOS (Basili, Pazienza, and Zanzotto 1998).
We parsed the available documents inthe application domain in order to extract a list Tc of syntactically plausible termino-logical noun phrases (NPs), for example, compounds (credit card), adjective-NPs (localtourist information office), and prepositional-NPs (board of directors).
In English, the firsttwo constructs are the most frequent.OntoLearn uses a novel method for filtering ?true?
terminology, described in detailin (Velardi, Missikoff, and Basili 2001).
The method is based on two measures, calledDomain Relevance (DR) and Domain Consensus (DC), that we introduce hereafter.156Computational Linguistics Volume 30, Number 2High frequency in a corpus is a property observable for terminological as well asnonterminological expressions (e.g., last week or real time).
We measure the specificity ofa terminological candidate with respect to the target domain via comparative analysisacross different domains.
To this end a specific DR score has been defined.
A quantita-tive definition of the DR can be given according to the amount of information capturedwithin the target corpus with respect to a larger collection of corpora.
More precisely,given a set of n domains {D1, .
.
.
, Dn} and related corpora, the domain relevance of aterm t in class Dk is computed asDRt,k =P(t|Dk)max1?j?nP(t|Dj)(1)where the conditional probabilities (P(t|Dk)) are estimated asE(P(t|Dk)) =ft,k?t?
?Dkft?,kwhere ft,k is the frequency of term t in the domain Dk (i.e., in its related corpus).Terms are concepts whose meaning is agreed upon by large user communities in agiven domain.
A more selective analysis should take into account not only the overalloccurrence of a term in the target corpus but also its appearance in single documents.Domain terms (e.g., travel agent) are referred to frequently throughout the documentsof a domain, while there are certain specific terms with a high frequency within singledocuments but completely absent in others (e.g., petrol station, foreign income).
Dis-tributed usage expresses a form of consensus tied to the consolidated semantics of aterm (within the target domain) as well as to its centrality in communicating domainknowledge.A second relevance indicator, DC, is then assigned to candidate terms.
DC mea-sures the distributed use of a term in a domain Dk.
The distribution of a term t indocuments d ?
Dk can be taken as a stochastic variable estimated throughout alld ?
Dk.
The entropy of this distribution expresses the degree of consensus of t in Dk.More precisely, the domain consensus is expressed as follows:DCt,k =?d?Dk(Pt(d) log1Pt(d))(2)whereE(Pt(dj)) =ft,j?dj?Dkft,jNonterminological (or nondomain) candidate terms are filtered using a combinationof measures (1) and (2).For each candidate term the following term weight is computed:TWt,k = ?DRt,k + ?DCnormt,kwhere DCnormt,k is a normalized entropy and ?,?
?
(0, 1).
We experimented with severalthresholds for ?
and ?, with consistent results in two domains (Velardi, Missikoff, andBasili 2001).
Usually, a value close to 0.9 is to be chosen for ?.
The threshold for157Navigli and Velardi Learning Domain OntologiesTable 1The first 10 terms from a tourism (left) andfinance (right) domain.tourism financetravel information vice presidentshopping street net incomeairline ticket executive officerbooking form composite tradingbus service stock marketcar rental interest rateairport transfer million sharecontact detail holding companycontinental breakfast third-quarter nettourist information office chief executiveserviceferry service boat servicecar ferry servicebus service transportservicepublic transportservicecoachservicetaxi serviceexpress servicetrain servicecar service customerserviceFigure 4A lexicalized tree in a tourism domain.?
depends upon the number N of documents in the training set of Dk.
When N issufficiently large, ?good?
values are between 0.35 and 0.25.
Table 1 shows some of theaccepted terms in two domains, ordered by TW.3.2 Phase 2: Semantic InterpretationThe set of terms accepted by the filtering method described in the previous section arefirst arranged in subtrees, according to simple string inclusion.5 Figure 4 is an exampleof what we call a lexicalized tree T .
In absence of semantic interpretation, it is notpossible to fully capture conceptual relationships between concepts (for example, thetaxonomic relation between bus service and public transport service in Figure 4).Semantic interpretation is the process of determining the right concept (sense) foreach component of a complex term (this is known as sense disambiguation) and thenidentifying the semantic relations holding among the concept components, in order tobuild a complex concept.
For example, given the complex term bus service, we wouldlike to associate a complex concept with this term as in Figure 5, where bus#1 andservice#1 are unique concept names taken from a preexisting concept inventory (e.g.,WordNet, though other general-purpose ontologies could be used), and INSTR is asemantic relation indicating that there is a service, which is a type of work (service#1),operated through (instrument) a bus, which is a type of public transport (bus#1).5 Inclusion is on the right side in the case of compound terms (the most common syntactic construct forterminology in English).158Computational Linguistics Volume 30, Number 2bus#1 service#1INSTRFigure 5A complex term represented as a complex concept.This kind of semantic interpretation is indeed possible if the meaning of a newcomplex concept can be interpreted compositionally from its components.
Clearly, thisis not always possible.
Furthermore, some of the component concepts may be absentin the initial ontology.
In this case, other strategies can be adopted, as sketched inSection 6.To perform semantic disambiguation, we use available lexical resources, like Word-Net and annotated corpora, and a novel word sense disambiguation (WSD) algorithmcalled structural semantic interconnection.
A state-of-art inductive learner is used tolearn rules for tagging concept pairs with the appropriate semantic relation.In the following, we first describe the semantic disambiguation algorithm (Sec-tions 3.2.1 to 3.2.4).
We then describe the semantic relation extractor (Section 3.2.5).3.2.1 The Structural Semantic Interconnection Algorithm.
OntoLearn is a tool forextending and trimming a general-purpose ontology.
In its current implementation,it uses a concept inventory taken from WordNet.
WordNet associates one or moresynsets (e.g., unique concept names) to over 120,000 words but includes very fewdomain terms: for example, bus and service are individually included, but not busservice as a unique term.The primary strategy used by OntoLearn to attach a new concept under the ap-propriate hyperonym of an existing ontology is compositional interpretation.
Lett = wn ?
.
.
.
?
w2 ?
w1 be a valid multiword term belonging to a lexicalized tree T .Let w1 be the syntactic head of t (e.g., the rightmost word in a compound, or theleftmost in a prepositional NP).
The process of compositional interpretation associatesthe appropriate WordNet synset Sk with each word wk in t. The sense of t is hencecompositionally defined asS(t) = [Sk|Sk ?
Synsets(wk), wk ?
t]where Synsets(wk) is the set of senses provided by WordNet for word wk, for instance:S (?transport company??)
= [{transportation#4, shipping#1, transport#3},{company#1}]corresponding to sense 1 of company (an institution created to conduct business) andsense 3 of transport (the commercial enterprise of transporting goods and materials).Compositional interpretation is a form of word sense disambiguation.
In this sec-tion, we define a new approach to sense disambiguation called structural semanticinterconnections (SSI).The SSI algorithm is a kind of structural pattern recognition.
Structural patternrecognition (Bunke and Sanfeliu 1990) has proven to be effective when the objects tobe classified contain an inherent, identifiable organization, such as image data andtime-series data.
For these objects, a representation based on a ?flat?
vector of fea-tures causes a loss of information that has a negative impact on classification per-159Navigli and Velardi Learning Domain Ontologiescoachvehicle transport passengerPATIENTPURPOSEKIND-OF(vehicle, passenger, transport) (a)(b)Figure 6Two representations of the same concept: (a) as a feature vector and (b) as a semantic graph.formances.
The classification task in a structural pattern recognition system is imple-mented through the use of grammars that embody precise criteria to discriminateamong different classes.
The drawback of this approach is that grammars are by theirvery nature application and domain specific.
However, automatic learning techniquesmay be adopted to learn from available examples.Word senses clearly fall under the category of objects that are better describedthrough a set of structured features.
Compare for example the following two feature-vector (a) and graph-based (b) representations of the WordNet definition of coach#5 (avehicle carrying many passengers, used for public transport) in Figure 6.
The graphrepresentation shows the semantic interrelationships among the words in the defini-tion, in contrast with the flat feature vector representation.Provided that a graph representation for alternative word senses in a context isavailable, disambiguation can be seen as the task of detecting certain ?meaningful?
intercon-necting patterns among such graphs.
We use a context-free grammar to specify the typeof patterns that are the best indicators of a semantic interrelationship and to select theappropriate sense configurations accordingly.In what follows, we first describe the method to obtain a graph representationof word senses from WordNet and other available resources.
Then, we illustrate thedisambiguation algorithm.Creating a graph representation for word senses.
A graph representation of word senses isautomatically built using a variety of knowledge source:1.
WordNet.
In WordNet, in addition to synsets, the following informationis provided:(a) a textual sense definition (gloss);(b) hyperonymy links (i.e., kind-of relations: for example, bus#1is a kind of public transport#1);(c) meronymy relations (i.e., part-of relations: for example, bus#1has part roof#2 and window#2);(d) other syntactic-semantic relations, as detailed later, notsystematically provided throughout the lexical knowledgebase.2.
Domain labels6 extracted by a semiautomatic methodology described inMagnini and Cavaglia (2000) for assigning domain information (e.g.,tourism, zoology, sport) to WordNet synsets.3.
Annotated corpora providing examples of word sense usages in contexts:6 Domain labels have been kindly made available by the IRST to our institution for research purposes.160Computational Linguistics Volume 30, Number 2(a) SemCor7 is a corpus in which each word in a sentence isassigned a sense selected from the WordNet sense inventory forthat word.
Examples of a SemCor document are the following:Color#1 was delayed#1 until 1935, the widescreen#1 until the early#1fifties#1.Movement#7 itself was#7 the chief#1 and often#1 the only#1attraction#4 of the primitive#1 movies#1 of the nineties#1.
(b) LDC/DSO8 is a corpus in which each document is a collection ofsentences having a certain word in common.
The corpusprovides a sense tag for each occurrence of the word within thedocument.
Examples from the document focused on the nounhouse are the following:Ten years ago, he had come to the house#2 to be interviewed.Halfway across the house#1, he could have smelled her morningperfume.
(c) In WordNet, besides glosses, examples are sometimes providedfor certain synsets.
From these examples, as for the LDC andSemCor corpora, co-occurrence information can be extracted.Some examples are the following:Overnight accommodations#4 are available.Is there intelligent#1 life in the universe?An intelligent#1 question.The use of other semantic knowledge repositories (e.g., FrameNet9 and Verbnet10)is currently being explored, the main problem being the need of harmonizing theseresources with the WordNet sense and relations inventory.The information available in WordNet and in the other resources described in theprevious section is used to automatically generate a labeled directed graph (digraph)representation of word senses.
We call this a semantic graph.Figure 7 shows an example of the semantic graphs generated for senses 1 (coach)and 2 (conductor) of bus; in the figure, nodes represent concepts (WordNet synsets)and edges are semantic relations.
In each graph in the figure, we include only nodeswith a maximum distance of three from the central node, as suggested by the dashedoval.
This distance has been experimentally established.The following semantic relations are used: hyperonymy (car is a kind of vehicle,denoted with kind?of??
), hyponymy (its inverse, has?kind??
), meronymy (room has-part wall,has?part??
), holonymy (its inverse, part?of??
), pertainymy (dental pertains-to tooth, pert??
), at-tribute (dry value-of wetness, att?
), similarity (beautiful similar-to pretty, sim?
), gloss (gloss??
),7 http://www.cs.unt.edu/?rada/downloads.html#semcor8 http://www.ldc.upenn.edu/9 http://www.icsi.berkeley.edu/?framenet/10 http://www.cis.upenn.edu/verbnet/161Navigli and Velardi Learning Domain Ontologiesbus#1publictransport#1transport#1school bus#1 window#2instrumentation#1roof#2 vehicle#1passenger#1traveler#1express#2glosswindow frame#1protection#2framework#3pane#1covering#2kind-of kind-ofhas-parthas-partkind-ofhas-kindkind-of kind-ofkind-ofglosskind-ofhas-part has-partkind-ofplate glass#1person#1kind-ofhas-kindglossbus#2conductor#4device#1electrical#2instrumentality#3computer#1connection#2wiring#1machine#1calculator#2has-kindunion#4kind-ofkind-ofkind-ofglosselectrical device#1part-ofglosselectricity#1glosscircuit#1kind-ofinterconnection#1has-kindkind-ofglosshas-kindglosskind-ofstate#4kind-ofkind-ofpertconnected#6(a)(b)Figure 7Graph representations for (a) sense 1 and (b) sense 2 of bus.topic (topic??
), and domain ( dl?).
All these relations are explicitly encoded in WordNet, ex-cept for the last three.
Topic, gloss, and domain are extracted from annotated corpora,sense definitions, and domain labels, respectively.
Topic expresses a co-occurrence rela-tion between concepts in texts, extracted from annotated corpora and usage examples.Gloss relates a concept to another concept occurring in its natural language defini-tion.
Finally, domain relates two concepts sharing the same domain label.
In parsingglosses, we use a stop list to eliminate the most frequent words.The SSI algorithm.
The SSI algorithm is a knowledge-based iterative approach toword sense disambiguation.
The classification problem can be stated as follows:?
t is a term?
T (the context of t) is a list of co-occurring terms, including t.?
I is a structural representation of T (the semantic context).162Computational Linguistics Volume 30, Number 2?
St1, St2, .
.
.
, Stn are structural specifications of the possible senses for t(semantic graphs).?
G is a grammar describing structural relations (semanticinterconnections) among the objects to be analyzed.?
Determine how well the structure of I matches that of each ofSt1, St2, .
.
.
, Stn, using G.?
Select the best matching.Structural representations are graphs, as previously detailed.
The SSI algorithm con-sists of an initialization step and an iterative step.In a generic iteration of the algorithm, the input is a list of co-occurring termsT = [t1, .
.
.
, tn] and a list of associated senses I = [St1 , .
.
.
, Stn ], that is, the semanticinterpretation of T, where Sti 11 is either the chosen sense for ti (i.e., the result of aprevious disambiguation step) or the empty set (i.e., the term is not yet disambiguated).A set of pending terms is also maintained, P = {ti|Sti = ?}.
I is referred to as the semanticcontext of T and is used, at each step, to disambiguate new terms in P.The algorithm works in an iterative way, so that at each stage either at leastone term is removed from P (i.e., at least one pending term is disambiguated) orthe procedure stops because no more terms can be disambiguated.
The output is theupdated list I of senses associated with the input terms T.Initially, the list I includes the senses of monosemous terms in T. If no monosemousterms are found, the algorithm uses an initialization policy described later.During a generic iteration, the algorithm selects those terms t in P showing aninterconnection between at least one sense S of t and one or more senses in I. Thelikelihood that a sense S will be the correct interpretation of t, given the semanticcontext I, is estimated by the function fI : Synsets ?
T ?
, where Synsets is the set ofall the concepts in WordNet, and defined as follows:fI(S, t) ={?({?
(S, S?)|S??
I}) if S ?
Senses(t) ?
Synsets0 otherwisewhere Senses(t) is the subset of synsets in WordNet associated with the term t, and?
(S, S?)
= ??
({w(e1, e2, .
.
.
, en)|Se1?
S1e2?
.
.
.
en?1?
Sn?1en?
S?
}), that is, a function (??)
ofthe weights (w) of each path connecting S with S?, where S and S?
are representedby semantic graphs.
A semantic path between two senses S and S?, S e1?
S1e2?
.
.
.
en?1?Sn?1en?
S?, is represented by a sequence of edge labels e1, e2, .
.
.
, en.
A proper choicefor both ?
and ??
may be the sum function (or the average sum function).A context-free grammar G = (E, N, SG, PG) encodes all the meaningful semanticpatterns.
The terminal symbols (E) are edge labels, while the nonterminal symbols (N)encode (sub)paths between concepts; SG is the start symbol of G, and PG the set of itsproductions.We associate a weight with each production A ?
?
?
PG, where A ?
N and?
?
(N ?
E)?, that is, ?
is a sequence of terminal and nonterminal symbols.
If thesequence of edge labels e1, e2, .
.
.
, en belongs to L(G), the language generated by thegrammar, and G is not ambiguous, then w(e1, e2, .
.
.
, en) is given by the sum of the11 Note that with Sti we refer interchangeably to the semantic graph associated with a sense or to thesense label (i.e., the synset).163Navigli and Velardi Learning Domain Ontologiesweights of the productions applied in the derivation SG ??
e1, e2, .
.
.
, en.
(The grammarG is described in the next subsection.
)Finally, the algorithm selects St =argmaxS?SynsetsfI(S, t) as the most likely interpretation oft and updates the list I with the chosen concept.
A threshold can be applied to fI(S, t)to improve the robustness of the system?s choices.At the end of a generic iteration, a number of terms are disambiguated, and eachof them is removed from the set of pending terms P. The algorithm stops with outputI when no sense S?
can be found for the remaining terms in P such that fI(S?, t?)
> 0,that is, P cannot be further reduced.
In each iteration, interconnections can be foundonly between the sense of a pending term t and the senses disambiguated during theprevious iteration.If no monosemous words are found, we explore two alternatives: either we providemanually the synset of the root term h (e.g., service#1 in Figure 4: work done by oneperson or group that benefits another), or we fork the execution of the algorithm intoas many processes as the number of senses of the root term h. Let n be such a number.For each process i (i = 1, .
.
.
, n), the input is given by Ii = [?, ?, .
.
.
, Shi , .
.
.
, ?
], whereShi is the ith sense of h in Senses(h).
Each execution outputs a (partial or complete)semantic context Ii.
Finally, the most likely context Im is obtained by choosingm = arg max1?i?n?St?IifIi(St, t)Figure 8 provides pseudocode for the SSI algorithm.3.2.2 The Grammar.
The grammar G has the purpose of describing meaningful inter-connecting patterns among semantic graphs representing concepts in the ontology.
Wedefine a pattern as a sequence of consecutive semantic relations e1?e2?.
.
.
?en where ei ?
E,the set of terminal symbols, that is, the vocabulary of conceptual relations.
Two rela-tions ei ?
ei+1 are consecutive if the edges labeled with ei and ei+1 are incoming and/oroutgoing from the same concept node, for example,ei?
S ei+1?
, ei?
S ei+1?
, ei?
S ei+1?
, ei?
S ei+1?
.A meaningful pattern between two senses S and S?
is a sequence e1 ?
e2 ?
.
.
.
?
en thatbelongs to L(G).In its current version, the grammar G has been defined manually, inspecting theintersecting patterns automatically extracted from pairs of manually disambiguatedword senses co-occurring in different domains.
Some of the rules in G are inspiredby previous work in the eXtended WordNet12 project.
The terminal symbols ei arethe conceptual relations extracted from WordNet and other on-line lexical-semanticresources, as described in Section 3.2.1.G is defined as a quadruple (E, N, SG, PG), where E = { ekind-of, ehas-kind, epart-of,ehas-part, egloss, eis-in-gloss, etopic, .
.
.
}, N = { SG, Ss, Sg, S1, S2, S3, S4, S5, S6, E1, E2, .
.
.
}, andPG includes about 50 productions.
An excerpt from the grammar is shown in Table 2.As stated in the previous section, the weight w(e1, e2, .
.
.
, en) of a semantic pathe1, e2, .
.
.
, en is given by the sum of the weights of the productions applied in thederivation SG ??
e1, e2, .
.
.
, en.
These weights have been experimentally established onstandard word sense disambiguation data, such as the SemCor corpus, and have beennormalized so that the weight of a semantic path always ranges between 0 and 1.The main rules in G are as follows (S1 and S2 are two synsets in I):12 http://xwn.hlt.utdallas.edu/papers.html.164Computational Linguistics Volume 30, Number 2SSI(T : list of terms, I : initial list of interpretation synsets){for each t ?
Tif (t is monosemous) I[t] = the only sense of tP := {t ?
T : I[t] = ?
}{ while there are more terms to disambiguate }do{P?
:= Pfor each t ?
P?
{ for each pending term }{bestSense := ?maxValue := 0{ for each possible interpretation of t }for each sense S of t in WordNet{f [S] := 0for each synset S?
?
I{?
:= 0for each semantic path e1e2 .
.
.
en between S and S??
:= ?
+ w(e1e2 .
.
.
en)f [S] := f [S] + ?
}if (f [S] > maxValue){maxValue := f [S]bestSense := S}}if (maxValue > 0){I[t] := bestSenseP := P \ {t}}}} while(P = P?
)return I}Figure 8The SSI algorithm in pseudocode.165Navigli and Velardi Learning Domain OntologiesTable 2Excerpt from the context-free grammar for the recognition of semanticinterconnections.SG ?
Ss|Sg (all the rules)Ss ?
S1|S2|S3 (simple rules)S1 ?
E1S1|E1 (hyperonymy/meronymy)E1 ?
ekind?of|epart?ofS2 ?
E2S2|E2 (hyponymy/holonymy)E2 ?
ehas?kind|ehas?partS3 ?
ekind?ofS3ehas?kind|ekind?ofehas?kind (parallelism)Sg ?
eglossSs|S4|S5|S6 (gloss rules)S4 ?
egloss (gloss rule)S5 ?
etopic (topic rule)S6 ?
eglosseis?in?gloss (gloss + gloss?1 rule)1. color, if S1 is in the same adjectival cluster as chromatic#3 and S2 is ahyponym of a concept that can assume a color like physical object#1 andfood#1 (e.g., S1 ?
yellow#1 and S2 ?
wall#1)2. domain, if the gloss of S1 contains one or more domain labels and S2 is ahyponym of those labels (for example, white#3 is defined as ?
(of wine)almost colorless,?
therefore it is the best candidate for wine#1 in order todisambiguate the term white wine)3. synonymy, if(a) S1 ?
S2 or (b) ?N ?
Synsets : S1pert??
N ?
S2(for example, in the term open air, both the words belong to synset{ open#8, air#2, .
.
.
, outdoors#1 })4. hyperonymy/meronymy path, if there is a sequence ofhyperonymy/meronymy relations (for example, mountain#1has-part?
?mountain peak#1 kind-of??
top#3 provides the right sense for each word ofmountain top)5. hyponymy/holonymy path, if there is a sequence ofhyponymy/holonymy relations (for example, in sand beach, sand#1part-of??beach#1);6.
parallelism, if S1 and S2 have a common ancestor (for example, inenterprise company, organization#1 is a common ancestor of bothenterprise#2 and company#1)7. gloss, if S1gloss??
S2 (for example, in web site, the gloss of web#5 containsthe word site; in waiter service, the gloss of restaurant attendant#1,hyperonym of waiter#1, contains the word service)8. topic, if S1topic??
S2 (for example, in the term archeological site, in whichboth words are tagged with sense 1 in a SemCor file; notice thatWordNet provides no mutual information about them; also considerpicturesque village: WordNet provides the example ?a picturesque village?for sense 1 of picturesque)166Computational Linguistics Volume 30, Number 29. gloss+hyperonymy/meronymy path, if ?G ?
Synsets : S1gloss??
G and thereis a hyperonymy/meronymy path between G and S2 (for example, inrailway company, the gloss of railway#1 contains the word organization andcompany#1 kind-of??
institution#1 kind-of??
organization#1)10. gloss+parallelism, if ?G ?
Synsets : S1gloss??
G and there is a parallelismpath between G and S2 (for example, in transport company, the gloss oftransport#3 contains the word enterprise and organization#1 is a commonancestor of both enterprise#2 and company#1)11. gloss+gloss, if ?G ?
Synsets : S1gloss??
G gloss??
S2 (for example, in mountainrange, mountain#1 and range#5 both contain the word hill so that theright senses can be chosen)12. hyperonymy/meronymy+gloss path, if ?G ?
Synsets : G gloss??
S2 and thereis a hyperonymy/meronymy path between S1 and G13.
parallelism+gloss, if ?G ?
Synsets : G gloss??
S2 and there is a parallelismpath between S1 and G.3.2.3 A Complete Example.
We now provide a complete example of the SSI algorithmapplied to the task of disambiguating a lexicalized tree T .
With reference to Figure 4,the list T is initialized with all the component words in T , that is, [service, train, ferry,car, boat, car-ferry, bus, coach, transport, public transport, taxi, express, customer].Step 1.
In T there are four monosemous words, taxi, car-ferry, public transport, andcustomer; therefore, we haveI = [taxi#1, car ferry#1, public transport#1, customer#1 ]P = {service, train, ferry, car, boat, bus, coach, transport, express}.Step 2.
During the second iteration, the following rules are matched:13{taxi} kind-of??
{car, auto}(hyper){taxi} kind-of??
{car, auto} kind-of??
{motor vehicle,automotive vehicle}kind-of??
{vehicle} gloss??
{bus, autobus, coach}(hyper + gloss){taxi} kind-of??
{car, auto} kind-of??
{motor vehicle,automotive vehicle} kind-of??
{vehicle}gloss??
{ferry, ferryboat}(hyper + gloss){bus, autobus, coach} kind-of??
{public transport}(hyper){car ferry} kind-of??
{ferry, ferryboat}(hyper)13 More than one rule may contribute to the disambiguation of a term.
We list here only some of thedetected patterns.167Navigli and Velardi Learning Domain Ontologies{customer, client} topic??
{service}(topic){service} gloss??
{person, someone} has-kind??
{consumer}has-kind??
{customer, client}(gloss + hypo){train, railroad train} kind-of??
{public transport}(hyper){express, expressbus} kind-of??
{bus, autobus, coach} kind-of??
{public transport}(hyper){conveyance, transport} has-kind??
{public transport}(hypo)obtaining:I = [taxi#1, car ferry#1, public transport#1, customer#1, car#1, ferry#1, bus#1,coach#5, train#1, express#2, transport#1, service#1] 14P = {boat}.Step 3.
{boat} has-kind??
{ferry, ferryboat}(hypo)I = [taxi#1, car ferry#1, public transport#1, customer#1, car#1, ferry#1, bus#1,coach#5, train#1, express#2, boat#1, transport#1, service#1 ]P = ?.Then the algorithm stops since the list P is empty.3.2.4 Creating Domain Trees.
During the execution of the SSI algorithm, (possibly)all the terms in a lexicalized tree T are disambiguated.
Subsequently, we proceed asfollows:a.
Concept clustering: Certain concepts can be clustered in a uniqueconcept on the basis of pertainymy, similarity, and synonymy (e.g.,manor house and manorial house, expert guide and skilled guide, bus serviceand coach service, respectively); notice again that we detect semanticrelations between concepts, not words.
For example, bus#1 and coach#5are synonyms, but this relation does not hold for other senses of thesetwo words.b.
Hierarchical structuring: Taxonomic information in WordNet is used toreplace syntactic relations with kind-of relations (e.g., ferry service kind-of?
?boat service), on the basis of hyperonymy, rather than string inclusion asin T .14 Notice that bus#1 and coach#5 belong to the same synset, therefore they are disambiguated by the samerule.168Computational Linguistics Volume 30, Number 2servicetransport servicecar service public transport service car service#2 boat servicecoach service, bus service train servicebus service#2 taxi servicecoach service#2express service#2express servicecoach service#3 ferry servicecar-ferry servicecustomer serviceFigure 9Domain concept tree.Each lexicalized tree T is finally transformed into a domain concept tree ?.
Fig-ure 9 shows the concept tree obtained from the lexicalized tree of Figure 4.
For thesake of legibility, in Figure 9 concepts are labeled with the associated terms (ratherthan with synsets), and numbers are shown only when more than one semantic in-terpretation holds for a term.
In fact, it is possible to find more than one matchinghyperonymy relation.
For example, an express can be a bus or a train, and both inter-pretations are valid, because they are obtained from relations between terms withinthe domain.3.2.5 Adding Conceptual Relations.
The second phase of semantic interpretation in-volves finding the appropriate semantic relations holding among concept components.In order to extract semantic relations, we need to do the following:?
Select an inventory of domain-appropriate semantic relations.?
Learn a formal model to select the relations that hold between pairs ofconcepts, given ontological information on these concepts.?
Apply the model to semantically relate the components of a complexconcept.First, we selected an inventory of semantic relations types.
To this end, we con-sulted John Sowa?s (1984) formalization on conceptual relations, as well as otherstudies conducted within the CoreLex,15 FrameNet, and EuroWordNet (Vossen 1998)projects.
In the literature, no systematic definitions are provided for semantic relations;therefore we selected only the more intuitive and widely used ones.To begin, we selected a kernel inventory including the following 10 relations,which we found pertinent (at least) to the tourism and finance16 domains: place (e.g.,room PLACE??
service, which reads ?the service has place in a room?
or ?the room isthe place of service?
), time (afternoon TIME??
tea), matter (ceramics MATTER??
tile), topic (artTOPIC??
gallery), manner (bus MANNER??
service), beneficiary (customer BENEF??
service), purpose(booking PURPOSE??
service), object (wine OBJ??
production), attribute (historical ATTR??
town),15 http://www.cs.brandeis.edu/?paulb/CoreLex/corelex.html16 Financial terms are extracted from the Wall Street Journal.169Navigli and Velardi Learning Domain Ontologiescharacteristics (first-class CHRC??
hotel).
This set can be easily adapted or extended toother domains.In order to associate the appropriate relation(s) that hold among the componentsof a domain concept, we decided to use inductive machine learning.
In inductivelearning, one has first to manually tag with the appropriate semantic relations a subsetof domain concepts (this is called the learning set) and then let an inductive learnerbuild a tagging model.
Among the many available inductive learning programs, weexperimented both with Quinlan?s C4.5 and with TiMBL (Daelemans et al 1999).An inductive learning system requires selecting a set of features to represent in-stances in the learning domain.
Instances in our case are concept-relation-concepttriples (e.g., wineOBJ??
production), where the type of relation is given only in the learningset.We explored several alternatives for feature selection.
We obtained the best resultwhen representing each concept component by the complete list of its hyperonyms(up to the topmost), as follows:feature ?
vector[[list of hyperonyms]?modifier[list of hyperonyms]head]For example, the feature vector for tourism operator, where tourism is the modifier andoperator is the head, is built as the sequence of hyperonyms of tourism#1: [tourism#1,commercial enterprise#2, commerce#1, transaction#1, group-action#1, act#1, human-action#1], followed by the sequence of hyperonyms for operator#2 [operator#2, capi-talist#2, causal agent#1, entity#1, life form#1, person#1, individual#1].Features are converted into a binary representation to obtain vectors of equallength.
We ran several experiments, using a tagged set of 405 complex concepts, avarying fragment of which were used for learning, the remainder for testing (we usedtwo-fold cross-validation).
Overall, the best experiment provided a 6% error rate over405 examples and produced around 20 classification rules.The following are examples of extracted rules (from C4.5), along with their confi-dence factor (in parentheses) and examples:If in modifier [knowledge domain#1, knowledge base#1 ]= 1 then relation THEME(63%)Examples : arts festival, science centerIf in modifier [building material#1 ] = 1 then relation MATTER(50%)Examples : stave church, cobblestone streetIf in modifier [conveyance#3, transport#1 ] = 1 and in head[act#1,human act#1 ]= 1 then relation MANNER(92.2%)Examples : bus service, coach tourSelection and extraction of conceptual relations is one of the active research areas inthe OntoLearn project.
Current research is directed toward the exploitation of on-lineresources (e.g., the tagged set of conceptual relations in FrameNet) and the automatic170Computational Linguistics Volume 30, Number 2generation of glosses for complex concepts (e.g., for travel service we have travel#1PURPOSE??
service#1: ?a kind of service, work done by one person or group that benefitsanother, for travel, the act of going from one place to another?).
Automatic generationof glosses (see Navigli et al [2004] for preliminary results) relies on the compositionalinterpretation criterion, as well as the semantic information provided by conceptualrelations.3.3 Phase 3: Ontology IntegrationThe domain concept forest generated by OntoLearn is used to trim and update Word-Net, creating a domain ontology.
WordNet is pruned and trimmed as follows:?
After the domain concept trees are attached to the appropriate nodes inWordNet in either a manual or an automatic manner, all branches notcontaining a domain node can be removed from the WordNet hierarchy.?
An intermediate node in WordNet is pruned whenever the followingconditions all hold:1.
It has no ?brother?
nodes.2.
It has only one direct hyponym.3.
It is not the root of a domain concept tree.4.
It is not at a distance greater than two from a WordNet uniquebeginner (this is to preserve a ?minimal?
top ontology).Figure 10 shows an example of pruning the nodes located over the domain concepttree rooted at wine#1.
The appendix shows an example of a domain-adapted branchof WordNet in the tourism domain.4.
EvaluationThe evaluation of ontologies is recognized to be an open problem.17 Though the num-ber of contributions in the area of ontology learning and construction has considerablyincreased in the past few years, especially in relation to the forthcoming Semantic Web,experimental data on the utility of ontologies are not available, other than those in Far-quhar et al (1998), in which an analysis of user distribution and requests is presentedfor the Ontology Server system.
A better performance indicator would have been thenumber of users that access the Ontology Server on a regular basis, but the authorsmention that regular users account for only a small percentage of the total.
Effortshave recently being made on the side of ontology evaluation tools and methods, butavailable results are on the methodological rather than on the experimental side.
Theontology community is still in the process of assessing an evaluation framework.We believe that, in absence of a commonly agreed-upon schema for analyzing theproperties of an ontology, the best way to proceed is evaluating an ontology withinsome existing application.
Our current work is precisely in this direction: The results ofa terminology translation experiment appear in Navigli, Velardi, and Gangemi (2003),while preliminary results on a query expansion task are presented in Navigli andVelardi (2003).17 OntoWeb D.1.3 Tools (2001), ?Whitepaper: Ontology Evaluation Tools,?
available athttp://www.aifb.unikarlsruhe.de/WBS/ysu/publications/eon2002 whitepaper.pdf171Navigli and Velardi Learning Domain Ontologiesdrug of abuse#1object#1substance#1fluid#1liquid#1  artifact#1drug#1wine#1food#1beverage#1alchool#1entity#1(a)object#1substance#1fluid#1liquid#1  artifact#1drug#1drug of abuse#1wine#1food#1beverage#1alchool#1entity#1(b)object#1substance#1fluid#1liquid#1    artifact#1drug#1wine#1food#1entity#1(d)object#1substance#1fluid#1liquid#1  artifact#1drug#1drug of abuse#1wine#1food#1beverage#1entity#1(c)Figure 10Pruning steps over the domain concept tree for wine1.In this evaluation section we proceed as follows: First, we provide an account ofthe feedback that we obtained from tourism experts participating in the HarmoniseEC project on interoperability in the tourism domain.
Then, we evaluate in detail theSSI algorithm, which is the ?heart?
of the OntoLearn methodology.4.1 OntoLearn as a Support for Ontology EngineersDuring the first year of the Harmonise project, a core ontology of about three hundredconcepts was developed using ConSys and SymOntoX.
In parallel, we collected acorpus of about one million words from tourism documents, mainly descriptions oftravels and tourism sites.
From this corpus, OntoLearn extracted an initial list of 14,383172Computational Linguistics Volume 30, Number 2candidate terms (the first phase of terminology extraction in Section 3.1), from whichthe system derived a domain concept forest of 3,840 concepts, which were submittedto the domain experts for ontology updating and integration.The Harmonise ontology partners lacked the requisite expertise to evaluate theWordNet synset associations generated by OntoLearn for each complex term, thereforewe asked them to evaluate only the domain appropriateness of the terms, arranged ina hierarchical fashion (as in Figure 9).
We obtained a precision ranging from 72.9% toabout 80% and a recall of 52.74%.18 The precision shift is due to the well-known factthat experts may have different intuitions about the relevance of a concept for a givendomain.
The recall estimate was produced by manually inspecting 6,000 of the initial14,383 candidate terms, asking the experts to mark all the terms judged as ?good?domain terms, and comparing the obtained list with the list of terms automaticallyfiltered by OntoLearn (the phase of terminology filtering described in Section 3.1).As a result of the feedback obtained from the tourism experts, we decided thatexperts?
interpretation difficulties could indeed be alleviated by associating a textualdefinition with each new concept proposed by OntoLearn.
This new research (auto-matic generation of glosses) was mentioned in Section 3.2.5.
We still need to producean in-field evaluation of the improved readability of the ontology enriched with textualdefinitions.In any case, OntoLearn favored a considerable speed up in ontology development,since shortly after we provided the results of our OntoLearn tool, the Harmoniseontology reached about three thousand concepts.
Clearly, the definition of an initialset of basic domain concepts is sufficiently crucial, to justify long-lasting and evenheated discussions.
But once an agreement is reached, filling the lower levels of theontology can still take a long time, simply because it is a tedious and time-consumingtask.
Therefore we think that OntoLearn revealed itself indeed to be a useful toolwithin Harmonise.4.2 Evaluation of the SSI Word Sense Disambiguation AlgorithmAs we will argue in Section 5, one of the novel aspects of OntoLearn with respectto current ontology-learning literature is semantic interpretation of extracted terms.The SSI algorithm described in section 3.2 was subjected to several evaluation exper-iments by the authors of this article.
The output of these experiments was used totune certain heuristics adopted by the algorithm, for example, the dimension of thesemantic graph (i.e., the maximum distance of a concept S?
from the central conceptS) and the weights associated with grammar rules.
To obtain a domain-independenttuning, tuning experiments were performed applying the SSI algorithm on standardword sense disambiguation data,19 such as SemCor and Senseval all-words.20However, OntoLearn?s main task is terminology disambiguation, rather than plainword sense disambiguation.
In complex terms, words are likely to be more tightly se-mantically related than in a sentence; therefore the SSI algorithm seems moreappropriate.21 To test the SSI algorithm, we selected 650 complex terms from the set of3,840 concepts mentioned in Section 4.1, and we manually assigned the appropriate18 In a paper specifically dedicated to terminology extraction and evaluation (Velardi, Missikoff, andBasili 2001) we performed an evaluation also on an economics domain, with similar results.19 In standard WSD tasks, the list T in input to the SSI algorithm is the set of all words in a sentencefragment to be disambiguated.20 http://www.itri.brighton.ac.uk/events/senseval/ARCHIVE/resources.html#test21 For better performance on a standard WSD task, it would be essential to improve lexical knowledge ofverbs (e.g.
by integrating VerbNet and FrameNet, as previously mentioned), as well as to enhance thegrammar.173Navigli and Velardi Learning Domain Ontologies81.15%79.78%83.12%84.56%79.05%75.18%73.22%77.83%79.76%72.24%0%10%20%30%40%50%60%70%80%90%100%allrulesexcl.7,9,10, 12,13excl.9excl.11,12, 13excl.7,8,9,10,12,13including monosemous termsexcluding monosemous termsFigure 11Different runs of the semantic disambiguation algorithm when certain rules in the grammar Gare removed.WordNet synset to each word composing the term.
We used two annotators to ensuresome degree of objectivity in the test set.
In this task we experienced difficulties al-ready pointed out by other annotators, namely, that certain synsets are very similar, tothe point that choosing one or the other?even with reference to our specific tourismdomain?seemed a mere guess.
Though we can?t say that our 650 tagged terms area ?gold standard,?
evaluating OntoLearn against this test set still produced interest-ing outcomes and a good intuition of system performance.
Furthermore, as shown bythe example of Section 3.2.3, OntoLearn produces a motivation for its choices, thatis, the detected semantic patterns.
Though it was not feasible to analyze in detail allthe output of the system, we found more than one example in which the choicesof OntoLearn were more consistent22 and more convincing than those produced bythe annotators, to the point that OntoLearn could also be used to support humanannotators in disambiguation tasks.First, we evaluated the effectiveness of the rules in G (Section 3.2.2) in regard to thedisambiguation algorithm.
Since certain rules are clearly related (for example, rules 4and 5, rules 9 and 11), we computed the precision of the disambiguation when addingor removing groups of rules.
The results are shown in Figure 11.
The shaded bars inthe figure show the results obtained when those terms containing unambiguous wordsare removed from the set of complex terms.We found that the grammar rules involving the gloss and hyperonym relationscontribute more than others to the precision of the algorithm.
Certain rules (not listedin 3.2.2 since they were eventually removed) were found to produce a negative effect.All the rules described in 3.2.2 were found to give more or less a comparable positivecontribution to the final performance.22 Consistent at least with respect to the lexical knowledge encoded in WordNet.174Computational Linguistics Volume 30, Number 284.56%82.25%75.74%73.80%0%10%20%30%40%50%60%70%80%90%100%manually dis.
head fully automaticprecision recallFigure 12Precision and recall for the terminology disambiguation task: manual disambiguation of thehead and fully automatic disambiguation.The precision computed in Figure 11 refers to the case in which the head node ofeach term tree is sense-tagged manually.
In Figure 12 the light and dark bars representprecision and recall, respectively, of the algorithm when the head (i.e., the root) ofa term tree is manually assigned and when the disambiguation is fully automatic.The limited drop in performance (2%) of the fully automated task with respect tomanual head disambiguation shows that, indeed, the assumption of a strong semanticinterrelationship between the head and the other terms of the term tree is indeedjustified.Finally, we computed a baseline, comparing the performance of the algorithm withthat obtained by a method that always chooses the first synset for each word in a com-plex term.
(We remind readers that in WordNet, the first sense is the most probable.
)The results are shown in Figure 13, where it is seen, as expected, that the incrementin performance with respect to the baseline is higher (around 5%) when only polyse-mous terms are considered.
A 5% difference (3% with respect to the fully automaticdisambiguation) is not striking, however, the tourism domain is not very technical,and often the first sense is the correct one.
We plan in the future to run experimentswith more technical domains, for example, economics or software products.5.
Related WorkComprehensive ontology construction and learning has been an active research fieldin the past few years.
Several workshops23 have been dedicated to ontology learningand related issues.
The majority of papers in this area propose methods for extendingan existing ontology with unknown words (e.g., Agirre et al 2000 and Alfonseca andManandhar 2002).
Alfonseca and Manandhar present an algorithm to enrich WordNetwith unknown concepts on the basis of hyponymy patterns.
For example, the patternhypernism(N2, N1) :?appositive(N2, N1) captures a hyponymy relation between Shake-speare and poet in the appositive NP ?Shakespeare, the poet.?
This approach heavily23 ECAI-2000 First Workshop on Ontology Learning (http://ol2000.aifb.uni-karlsruhe.de/) andIJCAI-2001 Second Workshop on Ontology Learning (http://ol2001.aifb.uni-karlsruhe.de/).175Navigli and Velardi Learning Domain Ontologies84.56%80.61%79.76%73.96%0%10%20%30%40%50%60%70%80%90%100%best result baselineincluding monosemous termsexcluding monosemous termsFigure 13Comparison with a baseline.depends upon the ability of discovering such patterns, however, it appears a usefulcomplementary strategy with respect to OntoLearn.
OntoLearn, in fact, is unable toanalyze totally unknown terms (though ongoing research is in progress to remedy thislimitation).
Berland and Charniak (1999) propose a method for extracting whole-partrelations from corpora and enrich an ontology with this information.
Few papers pro-pose methods of extensively enriching an ontology with domain terms.
For example,Vossen (2001) uses statistical methods and string inclusion to create lexicalized trees,as we do (see Figure 4).
However, no semantic disambiguation of terms is performed.Very often, in fact, ontology-learning papers regard domain terms as concepts.
A statis-tical classifier for automatic identification of semantic roles between co-occuring termsis presented in Gildea and Jurafsky (2002).
In order to tag texts with the appropriatesemantic role, Gildea and Jurafsky use a training set of fifty thousand sentences man-ually annotated within the FrameNet semantic labeling project.
Finally, in Maedcheand Staab (2000, 2001), an architecture is presented to help ontology engineers in thedifficult task of creating an ontology.
The main contribution of this work is in thearea of ontology engineering, although machine-learning methods are also proposedto automatically enrich the ontology with semantic relations.6.
Conclusions and Ongoing DevelopmentsWe believe that the OntoLearn system is innovative in several respects:1. in presenting an overall ontology development system.2.
in stressing the importance of appropriate terminology extraction to theontology-building enterprise.3.
in avoiding a common confusion between domain terms and domainconcepts, since it performs a semantic interpretation of terms.
This is indeedthe strongest aspect of our method.176Computational Linguistics Volume 30, Number 24. in presenting a new structural approach to sense classification (SSI).
Thismethod is general and has been applied to other sense disambiguationtasks, such as sense-based query expansion (Navigli and Velardi 2003)and gloss disambiguation (Gangemi, Navigli, and Velardi 2003).Ontology learning is a complex enterprise, and much is left to be done.
We listhere some of the drawbacks and gaps of our method, along with hints for ongoingand future developments.
OntoLearn is in fact a fully active area of research withinour group.1.
The SSI method presupposes that each term component has at least onesynset in WordNet.
In our ongoing research, we try to cope with thislimitation, parsing textual definitions in glossaries (e.g., in a computernetwork application) whenever a term cannot be interpretedcompositionally in WordNet.
Terms in glossaries are first arranged intrees according to detected taxonomic relations, then the head terms ofeach tree are attached to the appropriate node of WordNet, if anappropriate node indeed exists.
Rule-based and algebraic methods arejointly used to construct term trees and to compute measures of thesimilarity between the textual definitions in glossaries and those inWordNet.2.
OntoLearn detects taxonomic relations between complex concepts andother types of semantic relations among the components of a complexconcept.
However, an ontology is more than a taxonomy.
The result ofconcept disambiguation in OntoLearn is more than an ordered list ofsynsets, since we obtain semantic nets and intersecting patterns amongthem (Section 3.2.2).
This information is not currently exploited togenerate richer concept definitions.
A preliminary attempt to generateformal concept definitions from informal ones is described in Gangemi,Navigli, and Velardi (2003).
Furthermore, an automatic gloss generationalgorithm has been defined (Navigli et al 2004).3.
A large-scale evaluation is still to be done.
As we have already pointedout, evaluation of ontologies is recognized as an open problem, and fewresults are available, mostly on the procedural (?how to?)
side.
We partlyevaluated OntoLearn in an automatic translation task (Navigli, Velardi,and Gangemi 2003), and the SSI algorithm in generic WSD tasks asmentioned in item 4 of the previous list.
In addition, it would beinteresting to run OntoLearn on different domains, in order to study theeffect of higher or lower levels of ambiguity and technicality on theoutput domain ontology.Appendix: A Fragment of Trimmed WordNet for the Tourism Domain{ activity%1 }{ work%1 }{ project:00508925%n }{ tourism project:00193473%n }{ ambitious project:00711113%a }{ service:00379388%n }177Navigli and Velardi Learning Domain Ontologies{ travel service:00191846%n }{ air service#2:00202658%n }{ air service#4:00194802%n }{ transport service:00716041%n }{ ferry service#2:00717167%n }{ express service#3:00716943%n }{ exchange service:02413424%n }{ guide service:04840928%n }{ restaurant service:03233732%n }{ rail service:03207559%n }{ customer service:07197309%n }{ guest service:07304921%n }{ regular service#2:07525988%n }{ outstanding customer service:02232741%a }{ tourism service:00193473%n }{ waiter service:07671545%n }{ regular service:02255650%a,scheduled service:02255439%a }{ personalized service:01703424%a,personal service:01702632%a }{ secretarial service:02601509%a }{ religious service:02721678%a }{ church service:00666912%n }{ various service:00462055%a }{ helpful service:02376874%a }{ quality service:03714294%n }{ air service#3:03716758%n }{ room service:03250788%n }{ maid service:07387889%n }{ laundry service:02911395%n }{ car service#5:02364995%n }{ hour room service:10938063%n }{ transport service#2:02495376%n }{ car service:02383458%n }{ bus service#2:02356871%n }{ taxi service:02361877%n }{ coach service#2:02459686%n }{ public transport service:03184373%n }{ bus service:02356526%n,coach service:02356526%n }{ express service#2:02653414%n }{ local bus service:01056664%a }{ train service:03528724%n }{ express service:02653278%n }{ car service#2:02384604%n }{ coach service#3:03092927%n }{ boat service:02304226%n }{ ferry service:02671945%n }{ car-ferry service:02388365%n }{ air service:05270417%n }{ support service:05272723%n }178Computational Linguistics Volume 30, Number 2ReferencesAgirre, Eneko, Olatz Ansa, Eduard, Hovy,and David Mart??nez.
2000.
Enriching verylarge ontologies using the WWW.
In ECAIOntology Learning Workshop 2000, availableat http://ol2000.aifb.uni-karlsruhe.de/Alfonseca, Enrique and Suresh Manandhar.2002.
Improving an ontology refinementMethod with hyponymy patterns.
InLanguage Resources and Evaluation(LREC-2002), LasPalmas, Spain, May.Basili, Roberto, Maria Teresa Pazienza, andPaola Velardi.
1996.
An empiricalsymbolic approach to natural languageprocessing, Artificial Intelligence, 85(1?2):59?99.Basili, Roberto, Maria Teresa Pazienza, andFabio Massimo Zanzotto.
1998.
A robustparser for information extraction.
InProceedings of the European Conference onArtificial Intelligence (ECAI ?98), Brighton,U.K., August.Berland, Matthew and Eugene Charniak.1999.
Finding parts in very large corpora.In Proceedings of the the 37th Annual Meetingof the Association for ComputationalLinguistics (ACL-99), College Park, MD.Berners-Lee, Tim.
1999.
Weaving the Web.Harper, San Francisco.Bunke, Horst and Alberto Sanfeliu, editors.1990.
Syntactic and Structural PatternRecognition: Theory and Applications.
WorldScientific.Church, Kenneth Ward and Patrick Hanks.1989.
Word association norms, mutualinformation and lexicography.
In ACL-89,Vancouver, British Columbia, Canada.Daelemans, Walter, Jakub Zavrel, Ko vander Sloot, and Antal van den Bosch.
1999.TiMBL: Tilburg Memory Based Learner,version 2.0, reference manual.
TechnicalReport ILK-9901, ILK, Tilburg University,Tilburg, the Netherlands.Farquhar, Adam, Richard Fikes, WandaPratt, and James Rice.
1998.?Collaborative Ontology Construction forInformation Integration.?
http://www-ksl-svc.stanford.edu:5915/doc/project-papers.html.Fellbaum, Christiane, editor.
1995.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Formica, Anna, and Michele Missikoff.
2003.Ontology Validation in OPAL.
In 2003International Conference on Web Services(ICWS?
03), Las Vegas, NV.
Springer.Gangemi, Aldo, Nicola Guarino, ClaudioMasolo, Alessandro Oltramari, and LucSchneider.
2001.
Sweetening ontologieswith DOLCE.
In Proceedings of EKAW02,Siguenza, Spain.
Springer, pages 166?181Gangemi, Aldo, Roberto Navigli, and PaolaVelardi.
2003.
Axiomatising WordNet: Ahybrid methodology.
In Workshop onHuman Language Technology for the SemanticWeb and Web Services, Held in Conjunctionwith Second International Semantic WebConference, Sanibel Island, FL.Gildea, Daniel and Daniel Jurafsky.
2002.Automatic labeling of semantic roles.Computational Linguistics, 28(3): 245?288.Jacquemin, Christian.
1997.
Guessingmorphology from terms and corpora.
InProceedings of the 20th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR?97), Philadelphia, PA, pages 156?167.Lenat, Douglas.
1993.
CYC: A large scaleinvestment in knowledge infrastructure.Communications of the ACM, 3(11).Maedche, Alexander and Steffen Staab.2000.
Semi-automatic engineering ofontologies from text.
In Proceedings of the12th International Conference on SoftwareEngineering and Knowledge Engineering(SEKE?2000), Chicago, IL.Maedche, Alexander and Steffen Staab 2001.Ontology learning for the semantic web.IEEE Intelligent Systems, 16(2): 72?79.Magnini, Bernardo and Gabriella Cavaglia.2000.
Integrating subject field codes intoWordNet.
In Proceedings of the secondInternational Conference on LanguageResources and Evaluation (LREC2000),Atenes.Missikoff, Michele and X.F.
Wang.
2001.Consys?A group decision-makingsupport system for collaborative ontologybuilding.
In Proceedings of Group Decision &Negotiation 2001 Conference, La Rochelle,France.Navigli, Roberto, and Paola Velardi.
2003.An analysis of ontology-based queryexpansion strategies.
Workshop on AdaptiveText Extraction and Mining, held inconjunction with ECML 2003, CavtatDubrovnik, Croatia, September 22.Navigli, Roberto, Paola Velardi, AlessandroCucchiarelli, and Francesca Neri.
2004.Extending and enriching WordNet withOntoLearn.
In Second Global WordNetConference, Brno, Czech Republic, January20?23.
Springer-Verlag.Navigli, Roberto, Paola Velardi, and AldoGangemi.
2003.
Corpus driven ontologylearning: A method and its application toautomated terminology translation, IEEEIntelligent Systems, 18(1): 11?27.179Navigli and Velardi Learning Domain OntologiesOltramari, Alessandro, Aldo Gangemi,Nicola Guarino, and Claudio Masolo.2002.
Restructuring WordNet?s top-level:The OntoClean approach.
In Proceedings ofthe International Conference on LanguageResources and Evaluation (LREC2002), LasPalmas, Spain.Smith, Barry and Christopher A. Welty.2001.
Ontology: Towards a new synthesis.Formal Ontology in Information Systems,ACM Press.Sowa, John F. 1984.
Conceptual Structures:Information Processing in Mind and Machine.Addison-Wesley, Reading, MA.Uschold, Mike and Michael Gruninger.1996.
Ontologies: Principles, methods andapplications.
Knowledge EngineeringReview, 11(2).Velardi, Paola, Michele Missikoff, andRoberto Basili.
2001.
Identification ofrelevant terms to support the constructionof domain ontologies.
In ACL-EACLWorkshop on Human Language Technologies,Toulouse, France, July.Vossen, Piek, editor.
1998.
EuroWordNet: AMultilingual Database with Lexical SemanticNetworks.
Kluwer Academic, Dordrecht,Netherlands.Vossen, Piek.
2001.
Extending, trimming andfusing WordNet for technical documents.In NAACL 2001 Workshop on WordNet andOther Lexical Resources, Pittsburgh, July.Yamamoto, Mikio and Kenneth W. Church.2001.
Using suffix arrays to compute termfrequency and document frequency for allsubstrings in a corpus.
ComputationalLinguistics, 27(1): 1?30.Yokoi, Toshio.
1993.
The EDR electronicdictionary.
Communications of the ACM,38(11): 42?44.
