Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1199?1208,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics?Ask not what Textual Entailment can do for You...?Mark Sammons V.G.Vinod Vydiswaran Dan RothUniversity of Illinois at Urbana-Champaign{mssammon|vgvinodv|danr}@illinois.eduAbstractWe challenge the NLP community to par-ticipate in a large-scale, distributed effortto design and build resources for devel-oping and evaluating solutions to new andexisting NLP tasks in the context of Rec-ognizing Textual Entailment.
We arguethat the single global label with whichRTE examples are annotated is insufficientto effectively evaluate RTE system perfor-mance; to promote research on smaller, re-lated NLP tasks, we believe more detailedannotation and evaluation are needed, andthat this effort will benefit not just RTEresearchers, but the NLP community asa whole.
We use insights from success-ful RTE systems to propose a model foridentifying and annotating textual infer-ence phenomena in textual entailment ex-amples, and we present the results of a pi-lot annotation study that show this modelis feasible and the results immediately use-ful.1 IntroductionMuch of the work in the field of Natural Lan-guage Processing is founded on an assumptionof semantic compositionality: that there are iden-tifiable, separable components of an unspecifiedinference process that will develop as researchin NLP progresses.
Tasks such as Named En-tity and coreference resolution, syntactic and shal-low semantic parsing, and information and rela-tion extraction have been identified as worthwhiletasks and pursued by numerous researchers.
Whilemany have (nearly) immediate application to realworld tasks like search, many are also motivatedby their potential contribution to more ambitiousNatural Language tasks.
It is clear that the compo-nents/tasks identified so far do not suffice in them-selves to solve tasks requiring more complex rea-soning and synthesis of information; many othertasks must be solved to achieve human-like perfor-mance on tasks such as Question Answering.
Butthere is no clear process for identifying potentialtasks (other than consensus by a sufficient num-ber of researchers), nor for quantifying their po-tential contribution to existing NLP tasks, let alneto Natural Language Understanding.Recent ?grand challenges?
such as Learning byReading, Learning To Read, andMachine Readingare prompting more careful thought about the waythese tasks relate, and what tasks must be solvedin order to understand text sufficiently well to re-liably reason with it.
This is an appropriate timeto consider a systematic process for identifyingsemantic analysis tasks relevant to natural lan-guage understanding, and for assessing theirpotential impact on NLU system performance.Research on Recognizing Textual Entailment(RTE), largely motivated by a ?grand challenge?now in its sixth year, has already begun to addresssome of the problems identified above.
Tech-niques developed for RTE have now been suc-cessfully applied in the domains of Question An-swering (Harabagiu and Hickl, 2006) and Ma-chine Translation (Pado et al, 2009), (Mirkinet al, 2009).
The RTE challenge examples aredrawn from multiple domains, providing a rel-atively task-neutral setting in which to evaluatecontributions of different component solutions,and RTE researchers have already made incremen-tal progress by identifying sub-problems of entail-ment, and developing ad-hoc solutions for them.In this paper we challenge the NLP communityto contribute to a joint, long-term effort to iden-tify, formalize, and solve textual inference prob-lems motivated by the Recognizing Textual Entail-ment setting, in the following ways:(a) Making the Recognizing Textual Entailmentsetting a central component of evaluation for1199relevant NLP tasks such as NER, Coreference,parsing, data acquisition and application, and oth-ers.
While many ?component?
tasks are consid-ered (almost) solved in terms of expected improve-ments in performance on task-specific corpora, itis not clear that this translates to strong perfor-mance in the RTE domain, due either to prob-lems arising from unrelated, unsolved entailmentphenomena that co-occur in the same examples,or to domain change effects.
The RTE task of-fers an application-driven setting for evaluating abroad range of NLP solutions, and will reinforcegood practices by NLP researchers.
The RTEtask has been designed specifically to exercise tex-tual inference capabilities, in a format that wouldmake RTE systems potentially useful componentsin other ?deep?
NLP tasks such as Question An-swering and Machine Translation.
1(b) Identifying relevant linguistic phenomena,interactions between phenomena, and theirlikely impact on RTE/textual inference.
Deter-mining the correct label for a single textual en-tailment example requires human analysts to makemany smaller, localized decisions which may de-pend on each other.
A broad, carefully conductedeffort to identify and annotate such local phenom-ena in RTE corpora would allow their distributionsin RTE examples to be quantified, and allow eval-uation of NLP solutions in the context of RTE.
Itwould also allow assessment of the potential im-pact of a solution to a specific sub-problem on theRTE task, and of interactions between phenomena.Such phenomena will almost certainly correspondto elements of linguistic theory; but this approachbrings a data-driven approach to focus attention onthose phenomena that are well-represented in theRTE corpora, and which can be identified with suf-ficiently close agreement.
(c) Developing resources and approaches thatallow more detailed assessment of RTE sys-tems.
At present, it is hard to know what spe-cific capabilities different RTE systems have, andhence, which aspects of successful systems areworth emulating or reusing.
An evaluation frame-work that could offer insights into the kinds ofsub-problems a given system can reliably solvewould make it easier to identify significant ad-vances, and thereby promote more rapid advances1The Parser Training and Evaluation using Textual En-tailment track of SemEval 2 takes this idea one step further,by evaluating performance of an isolated NLP task using theRTE methodology.through reuse of successful solutions and focus onunresolved problems.In this paper we demonstrate that Textual En-tailment systems are already ?interesting?, in thatthey have made significant progress beyond a?smart?
lexical baseline that is surprisingly hardto beat (section 2).
We argue that Textual Entail-ment, as an application that clearly requires so-phisticated textual inference to perform well, re-quires the solution of a range of sub-problems,some familiar and some not yet known.
We there-fore propose RTE as a promising and worthwhiletask for large-scale community involvement, as itmotivates the study of many other NLP problemsin the context of general textual inference.We outline the limitations of the present modelof evaluation of RTE performance, and identifykinds of evaluation that would promote under-standing of the way individual components canimpact Textual Entailment system performance,and allow better objective evaluation of RTE sys-tem behavior without imposing additional burdenson RTE participants.
We use this to motivate alarge-scale annotation effort to provide data withthe mark-up sufficient to support these goals.To stimulate discussion of suitable annotationand evaluation models, we propose a candidatemodel, and provide results from a pilot annota-tion effort (section 3).
This pilot study establishesthe feasibility of an inference-motivated annota-tion effort, and its results offer a quantitative in-sight into the difficulty of the TE task, and the dis-tribution of a number of entailment-relevant lin-guistic phenomena over a representative samplefrom the NIST TAC RTE 5 challenge corpus.
Weargue that such an evaluation and annotation ef-fort can identify relevant subproblems whose so-lution will benefit not only Textual Entailment buta range of other long-standing NLP tasks, and canstimulate development of new ones.
We also showhow this data can be used to investigate the behav-ior of some of the highest-scoring RTE systemsfrom the most recent challenge (section 4).2 NLP Insights from Textual EntailmentThe task of Recognizing Textual Entailment(RTE), as formulated by (Dagan et al, 2006), re-quires automated systems to identify when a hu-man reader would judge that given one span of text(the Text) and some unspecified (but restricted)world knowledge, a second span of text (the Hy-1200Text: The purchase of LexCorp by BMI for $2Bnprompted widespread sell-offs by traders as theysought to minimize exposure.Hyp 1: BMI acquired another company.Hyp 2: BMI bought LexCorp for $3.4Bn.Figure 1: Some representative RTE examples.pothesis) is true.
The task was extended in (Gi-ampiccolo et al, 2007) to include the additionalrequirement that systems identify when the Hy-pothesis contradicts the Text.
In the exampleshown in figure 1, this means recognizing that theText entails Hypothesis 1, while Hypothesis 2 con-tradicts the Text.
This operational definition ofTextual Entailment avoids commitment to any spe-cific knowledge representation, inference method,or learning approach, thus encouraging applica-tion of a wide range of techniques to the problem.2.1 An Illustrative ExampleThe simple RTE examples in figure 1 (most RTEexamples have much longer Texts) illustrate sometypical inference capabilities demonstrated by hu-man readers in determining whether one span oftext contains the meaning of another.To recognize that Hypothesis 1 is entailed by thetext, a human reader must recognize that ?anothercompany?
in the Hypothesis can match ?Lex-Corp?.
She must also identify the nominalizedrelation ?purchase?, and determine that ?A pur-chased by B?
implies ?B acquires A?.To recognize that Hypothesis 2 contradicts theText, similar steps are required, together with theinference that because the stated purchase price isdifferent in the Text and Hypothesis, but with highprobability refers to the same transaction, Hypoth-esis 2 contradicts the Text.It could be argued that this particular examplemight be resolved by simple lexical matching; butit should be evident that the Text can be madelexically very dissimilar to Hypothesis 1 whilemaintaining the Entailment relation, and that con-versely, the lexical overlap between the Text andHypothesis 2 can be made very high, while main-taining the Contradiction relation.
This intuitionis borne out by the results of the RTE challenges,which show that lexical similarity-based systemsare outperformed by systems that use other, morestructured analysis, as shown in the next section.Rank System id Accuracy1 I 0.7352 E 0.6853 H 0.6704 J 0.6675 G 0.6626 B 0.6387 D 0.6338 F 0.6329 A 0.6159 C 0.6159 K 0.615- Lex 0.612Table 1: Top performing systems in the RTE 5 2-way task.Lex E G H I JLex 1.000 0.667 0.693 0.678 0.660 0.778(184,183) (157,132) (168,122) (152,136) (165,137) (165,135)E 1.000 0.667 0.675 0.673 0.702(224,187) (192,112) (178,131) (201,127) (186,131)G 1.000 0.688 0.713 0.745(247,150) (186,120) (218,115) (198,125)H 1.000 0.705 0.707(219,183) (194,139) (178,136)I 1.000 0.705(260,181) (198,135)J 1.000(224,178)Table 2: In each cell, top row shows observedagreement and bottom row shows the number ofcorrect (positive, negative) examples on which thepair of systems agree.2.2 The State of the Art in RTE 5The outputs for all systems that participated in theRTE 5 challenge were made available to partici-pants.
We compared these to each other and toa smart lexical baseline (Do et al, 2010) (lexicalmatch augmented with a WordNet similarity mea-sure, stemming, and a large set of low-semantic-content stopwords) to assess the diversity of theapproaches of different research groups.
To getthe fullest range of participants, we used resultsfrom the two-way RTE task.
We have anonymizedthe system names.Table 1 shows that many participating systemssignificantly outperform our smart lexical base-line.
Table 2 reports the observed agreement be-tween systems and the lexical baseline in terms ofthe percentage of examples on which a pair of sys-tems gave the same label.
The agreement betweenmost systems and the baseline is about 67%, whichsuggests that systems are not simply augmentedversions of the lexical baseline, and are also dis-tinct from each other in their behaviors.2Common characteristics of RTE systems re-2Note that the expected agreement between two randomRTE decision-makers is 0.5, so the agreement scores accord-ing to Cohen?s Kappa measure (Cohen, 1960) are between0.3 and 0.4.1201ported by their designers were the use of struc-tured representations of shallow semantic content(such as augmented dependency parse trees andsemantic role labels); the application of NLP re-sources such as Named Entity recognizers, syn-tactic and dependency parsers, and coreferenceresolvers; and the use of special-purpose ad-hocmodules designed to address specific entailmentphenomena the researchers had identified, such asthe need for numeric reasoning.
However, it isnot possible to objectively assess the role these ca-pabilities play in each system?s performance fromthe system outputs alone.2.3 The Need for Detailed EvaluationAn ablation study that formed part of the of-ficial RTE 5 evaluation attempted to evaluatethe contribution of publicly available knowledgeresources such as WordNet (Fellbaum, 1998),VerbOcean (Chklovski and Pantel, 2004), andDIRT (Lin and Pantel, 2001) used by many ofthe systems.
The observed contribution was inmost cases limited or non-existent.
It is premature,however, to conclude that these resources have lit-tle potential impact on RTE system performance:most RTE researchers agree that the real contribu-tion of individual resources is difficult to assess.As the example in figure 1 illustrates, most RTEexamples require a number of phenomena to becorrectly resolved in order to reliably determinethe correct label (the Interaction problem); a per-fect coreference resolver might as a result yield lit-tle improvement on the standard RTE evaluation,even though coreference resolution is clearly re-quired by human readers in a significant percent-age of RTE examples.Various efforts have been made by individ-ual research teams to address specific capabili-ties that are intuitively required for good RTEperformance, such as (de Marneffe et al, 2008),and the formal treatment of entailment phenomenain (MacCartney and Manning, 2009) depends onand formalizes a divide-and-conquer approach toentailment resolution.
But the phenomena-specificcapabilities described in these approaches are farfrom complete, and many are not yet invented.
Todevote real effort to identify and develop such ca-pabilities, researchers must be confident that theresources (and the will!)
exist to create and eval-uate their solutions, and that the resource can beshown to be relevant to a sufficiently large subsetof the NLP community.
While there is widespreadbelief that there are many relevant entailment phe-nomena, though each individually may be rele-vant to relatively few RTE examples (the Sparse-ness problem), we know of no systematic analysisto determine what those phenomena are, and howsparsely represented they are in existing RTE data.If it were even known what phenomena wererelevant to specific entailment examples, it mightbe possible to more accurately distinguish systemcapabilities, and promote adoption of successfulsolutions to sub-problems.
An annotation-sidesolution also maintains the desirable agnosticismof the RTE problem formulation, by not imposingthe requirement on system developers of generat-ing an explanation for each answer.
Of course, ifexamples were also annotated with explanationsin a consistent format, this could form the basis ofa new evaluation of the kind essayed in the pilotstudy in (Giampiccolo et al, 2007).3 Annotation Proposal and Pilot StudyAs part of our challenge to the NLP commu-nity, we propose a distributed OntoNotes-style ap-proach (Hovy et al, 2006) to this annotation ef-fort: distributed, because it should be undertakenby a diverse range of researchers with interestsin different semantic phenomena; and similar tothe OntoNotes annotation effort because it shouldnot presuppose a fixed, closed ontology of entail-ment phenomena, but rather, iteratively hypoth-esize and refine such an ontology using inter-annotator agreement as a guiding principle.
Suchan effort would require a steady output of RTE ex-amples to form the underpinning of these annota-tions; and in order to get sufficient data to repre-sent less common, but nonetheless important, phe-nomena, a large body of data is ultimately needed.A research team interested in annotating a newphenomenon should use examples drawn from thecommon corpus.
Aside from any task-specificgold standard annotation they add to the entail-ment pairs, they should augment existing explana-tions by indicating in which examples their phe-nomenon occurs, and at which point in the exist-ing explanation for each example.
In fact, thislatter effort ?
identifying phenomena relevant totextual inference, marking relevant RTE examples,and generating explanations ?
itself enables otherresearchers to select from known problems, assesstheir likely impact, and automatically generate rel-1202evant corpora.To assess the feasibility of annotating RTE-oriented local entailment phenomena, we devel-oped an inference model that could be followed byannotators, and conducted a pilot annotation study.We based our initial effort on observations aboutRTE data we made while participating in RTEchallenges, together with intuitive conceptions ofthe kinds of knowledge that might be available insemi-structured or structured form.
In this sec-tion, we present our annotation inference model,and the results of our pilot annotation effort.3.1 Inference ProcessTo identify and annotate RTE sub-phenomena inRTE examples, we need a defensible model for theentailment process that will lead to consistent an-notation by different researchers, and to an exten-sible framework that can accommodate new phe-nomena as they are identified.We modeled the entailment process as one ofmanipulating the text and hypothesis to be as sim-ilar as possible, by first identifying parts of thetext that matched parts of the hypothesis, and thenidentifying connecting structure.
Our inherent as-sumption was that the meanings of the Text andHypothesis could be represented as sets of n-aryrelations, where relations could be connected toother relations (i.e., could take other relations asarguments).
As we followed this procedure for agiven example, we marked which entailment phe-nomena were required for the inference.
We illus-trate the process using the example in figure 1.First, we would identify the arguments ?BMI?and ?another company?
in the Hypothesis asmatching ?BMI?
and ?LexCorp?
respectively, re-quiring 1) Parent-Sibling to recognize that ?Lex-Corp?
can match ?company?.
We would tag theexample as requiring 2) Nominalization Resolu-tion to make ?purchase?
the active relation and3) Passivization to move ?BMI?
to the subject po-sition.
We would then tag it with 4) Simple VerbRule to map ?A purchase B?
to ?A acquire B?.These operations make the relevant portion of theText identical to the Hypothesis, so we are done.For the same Text, but with Hypothesis 2 (a neg-ative example), we follow the same steps 1-3.
Wewould then use 4) Lexical Relation to map ?pur-chase?
to ?buy?.
We would then observe that theonly possible match for the hypothesis argument?for $3.4Bn?
is the text argument ?for $2Bn?.
Wewould label this as a 5) Numerical Quantity Mis-match and 6) Excluding Argument (it can?t be thecase that in the same transaction, the same com-pany was sold for two different prices).Note that neither explanation mentionsthe anaphora resolution connecting ?they?
to?traders?, because it is not strictly required todetermine the entailment label.As our example illustrates, this process makessense for both positive and negative examples.
Italso reflects common approaches in RTE systems,many of which have explicit alignment compo-nents that map parts of the Hypothesis to parts ofthe Text prior to a final decision stage.3.2 Annotation LabelsWe sought to identify roles for background knowl-edge in terms of domains and general inferencesteps, and the types of linguistic phenomena thatare involved in representing the same informationin different ways, or in detecting key differencesin two similar spans of text that indicate a differ-ence in meaning.
We annotated examples with do-mains (such as ?Work?)
for two reasons: to estab-lish whether some phenomena are correlated withparticular domains; and to identify domains thatare sufficiently well-represented that a knowledgeengineering study might be possible.While we did not generate an explicit repre-sentation of our entailment process, i.e.
explana-tions, we tracked which phenomena were strictlyrequired for inference.
The annotated corpora andsimple CGI scripts for annotation are available athttp://cogcomp.cs.illinois.edu/Data/ACL2010 RTE.php.The phenomena that we considered during an-notation are presented in Tables 3, 4, 5, and 6.
Wetried to define each phenomenon so that it wouldapply to both positive and negative examples, butran into a problem: often, negative examples canbe identified principally by structural differences:the components of the Hypothesis all match com-ponents in the Text, but they are not connectedby the appropriate structure in the Text.
In thecase of contradictions, it is often the case that akey relation in the Hypothesis must be matched toan incompatible relation in the Text.
We selectednames for these structural behaviors, and taggedthem when we observed them, but the counterpartfor positive examples must always hold: it mustnecessarily be the case that the structure in theText linking the arguments that match those in the1203Hypothesis must be comparable to the Hypothesisstructure.
We therefore did not tag this for positiveexamples.We selected a subset of 210 examples from theNIST TAC RTE 5 (Bentivogli et al, 2009) Testset drawn equally from the three sub-tasks (IE, IRand QA).
Each example was tagged by both an-notators.
Two passes were made over the data: thefirst covered 50 examples from each RTE sub-task,while the second covered an additional 20 exam-ples from each sub-task.
Between the two passes,concepts the annotators identified as difficult toannotate were discussed and more carefully spec-ified, and several new concepts were introducedbased on annotator observations.Tables 3, 4, 5, and 6 present informationabout the distribution of the phenomena wetagged, and the inter-annotator agreement (Co-hen?s Kappa (Cohen, 1960)) for each.
?Occur-rence?
lists the average percentage of examples la-beled with a phenomenon by the two annotators.Domain Occurrence Agreementwork 16.90% 0.918name 12.38% 0.833die kill injure 12.14% 0.979group 9.52% 0.794be in 8.57% 0.888kinship 7.14% 1.000create 6.19% 1.000cause 6.19% 0.854come from 5.48% 0.879win compete 3.10% 0.813Others 29.52% 0.864Table 3: Occurrence statistics for domains in theannotated data.Phenomenon Occurrence AgreementNamed Entity 91.67% 0.856locative 17.62% 0.623Numerical Quantity 14.05% 0.905temporal 5.48% 0.960nominalization 4.05% 0.245implicit relation 1.90% 0.651Table 4: Occurrence statistics for hypothesis struc-ture features.From the tables it is apparent that good perfor-mance on a range of phenomena in our inferencemodel are likely to have a significant effect onRTE results, with coreference being deemed es-sential to the inference process for 35% of exam-ples, and a number of other phenomena are suffi-ciently well represented to merit near-future atten-tion (assuming that RTE systems do not alreadyhandle these phenomena, a question we address insection 4).
It is also clear from the predominanceof Simple Rewrite Rule instances, together withPhenomenon Occurrence Agreementcoreference 35.00% 0.698simple rewrite rule 32.62% 0.580lexical relation 25.00% 0.738implicit relation 23.33% 0.633factoid 15.00% 0.412parent-sibling 11.67% 0.500genetive relation 9.29% 0.608nominalization 8.33% 0.514event chain 6.67% 0.589coerced relation 6.43% 0.540passive-active 5.24% 0.583numeric reasoning 4.05% 0.847spatial reasoning 3.57% 0.720Table 5: Occurrence statistics for entailment phe-nomena and knowledge resourcesPhenomenon Occurrence Agreementmissing argument 16.19% 0.763missing relation 14.76% 0.708excluding argument 10.48% 0.952Named Entity mismatch 9.29% 0.921excluding relation 5.00% 0.870disconnected relation 4.52% 0.580missing modifier 3.81% 0.465disconnected argument 3.33% 0.764Numeric Quant.
mismatch 3.33% 0.882Table 6: Occurrences of negative-only phenomenathe frequency of most of the domains we selected,that knowledge engineering efforts also have a keyrole in improving RTE performance.3.3 DiscussionPerhaps surprisingly, given the difficulty of thetask, inter-annotator agreement was consistentlygood to excellent (above 0.6 and 0.8, respec-tively), with few exceptions, indicating that formost targeted phenomena, the concepts were well-specified.
The results confirmed our initial intu-ition about some phenomena: for example, thatcoreference resolution is central to RTE, and thatdetecting the connecting structure is crucial in dis-cerning negative from positive examples.
We alsofound strong evidence that the difference betweencontradiction and unknown entailment examplesis often due to the behavior of certain relations thateither preclude certain other relations holding be-tween the same arguments (for example, winninga contest vs. losing a contest), or which can onlyhold for a single referent in one argument position(for example, ?work?
relations such as job title aretypically constrained so that a single person holdsone position).We found that for some examples, there wasmore than one way to infer the hypothesis from thetext.
Typically, for positive examples this involvedoverlap between phenomena; for example, Coref-erence might be expected to resolve implicit rela-1204tions induced from appositive structures.
In suchcases we annotated every way we could find.In future efforts, annotators should record theentailment steps they used to reach their decision.This will make disagreement resolution simpler,and could also form a possible basis for generatinggold standard explanations.
At a minimum, eachinference step must identify the spans of the Textand Hypothesis that are involved and the name ofthe entailment phenomenon represented; in addi-tion, a partial order over steps must be specifiedwhen one inference step requires that another hasbeen completed.Future annotation efforts should also add acategory ?Other?, to indicate for each examplewhether the annotator considers the listed entail-ment phenomena sufficient to identify the label.
Itmight also be useful to assess the difficulty of eachexample based on the time required by the anno-tator to determine an explanation, for comparisonwith RTE system errors.These, together with specifications that mini-mize the likely disagreements between differentgroups of annotators, are processes that must berefined as part of the broad community effort weseek to stimulate.4 Pilot RTE System AnalysisIn this section, we sketch out ways in whichthe proposed analysis can be applied to learnsomething about RTE system behavior, evenwhen those systems do not provide anythingbeyond the output label.
We present the analysisin terms of sample questions we hope to answerwith such an analysis.1.
If a system needs to improve its performance,which features should it concentrate on?
To an-swer this question, we looked at the top-5 systemsand tried to find which phenomena are active inthe mistakes they make.
(a) Most systems seem to fail on examples thatneed numeric reasoning to get the entailment de-cision right.
For example, system H got all 10 ex-amples with numeric reasoning wrong.
(b) All top-5 systems make consistent errors incases where identifying a mismatch in named en-tities (NE) or numerical quantities (NQ) is impor-tant to make the right decision.
System G got 69%of cases with NE/NQ mismatches wrong.
(c) Most systems make errors in examples thathave a disconnected or exclusion component (ar-gument/relation).
System J got 81% of cases witha disconnected component wrong.
(d) Some phenomena are handled well by certainsystems, but not by others.
For example, failingto recognize a parent-sibling relation betweenentities/concepts seems to be one of the top-5phenomena active in systems E and H. SystemH also fails to correctly label over 53% of theexamples having kinship relation.2.
Which phenomena have strong correlationsto the entailment labels among hard examples?We called an example hard if at least 4 of the top 5systems got the example wrong.
In our annotationdataset, there were 41 hard examples.
Some ofthe phenomena that strongly correlate with theTE labels on hard examples are: deeper lexicalrelation between words (?
= 0.542), and needfor external knowledge (?
= 0.345).
Further, wefind that the top-5 systems tend to make mistakesin cases where the lexical approach also makesmistakes (?
= 0.355).3.
What more can be said about individualsystems?
In order to better understand the systembehavior, we wanted to check if we could predictthe system behavior based on the phenomenawe identified as important in the examples.We learned SVM classifiers over the identifiedphenomena and the lexical similarity score topredict both the labels and errors systems makefor each of the top-5 systems.
We could predict all10 system behaviors with over 70% accuracy, andcould predict labels and mistakes made by two ofthe top-5 systems with over 77% accuracy.
Thisindicates that although the identified phenomenaare indicative of the system performance, it isprobably too simplistic to assume that systembehavior can be easily reproduced solely as adisjunction of phenomena present in the examples.4.
Does identifying the phenomena correctlyhelp learn a better TE system?
We tried tolearn an entailment classifier over the phenomenonidentified and the top 5 system outputs.
The resultsare summarized in Table 7.
All reported num-bers are 20-fold cross-validation accuracy froman SVM classifier learned over the features men-tioned.
The results show that correctly identify-ing the named-entity and numeric quantity mis-1205No.
Feature description No.
of Accuracy over which featuresfeats phenomena pheno.
+ sys.
labels(0) Only system labels 5 ?
0.714(1) Domain and hypothesis features (Tables 3, 4) 16 0.510 0.705(2) (1) + NE + NQ 18 0.619 0.762(3) (1) + Knowledge resources (subset of Table 5) 22 0.662 0.762(4) (3) + NE + NQ 24 0.738 0.805(5) (1) + Entailment and Knowledge resources (Table 5) 29 0.748 0.791(6) (5) + negative-only phenomena (Table 6) 38 0.971 0.943Table 7: Accuracy in predicting the label based on the phenomena and top-5 system labels.matches improves the overall accuracy signifi-cantly.
If we further recognize the need for knowl-edge resources correctly, we can correctly explainthe label for 80% of the examples.
Adding theentailment and negation features helps us explainthe label for 97% of the examples in the annotatedcorpus.It must be clarified that the results do not showthe textual entailment problem itself is solved with97% accuracy.
However, we believe that if asystem could recognize key negation phenomenasuch as Named Entity mismatch, presence of Ex-cluding arguments, etc.
correctly and consistently,it could model them as a Contradiction featuresin the final inference process to significantly im-prove its overall accuracy.
Similarly, identifyingand resolving the key entailment phenomena inthe examples, would boost the inference processin positive examples.
However, significant effortis still required to obtain near-accurate knowledgeand linguistic resources.5 DiscussionNLP researchers in the broader community contin-ually seek new problems to solve, and pose moreambitious tasks to develop NLP and NLU capabil-ities, yet recognize that even solutions to problemswhich are considered ?solved?
may not perform aswell on domains different from the resources usedto train and develop them.
Solutions to such NLPtasks could benefit from evaluation and further de-velopment on corpora drawn from a range of do-mains, like those used in RTE evaluations.It is also worthwhile to consider each task aspart of a larger inference process, and thereforemotivated not just by performance statistics onspecial-purpose corpora, but as part of an inter-connected web of resources; and the task of Rec-ognizing Textual Entailment has been designed toexercise a wide range of linguistic and reasoningcapabilities.The entailment setting introduces a potentiallybroader context to resource development and as-sessment, as the hypothesis and text provide con-text for each other in a way different than localcontext from, say, the same paragraph in a docu-ment: in RTE?s positive examples, the Hypothe-sis either restates some part of the Text, or makesstatements inferable from the statements in theText.
This is not generally true of neighboring sen-tences in a document.
This distinction opens thedoor to ?purposeful?, or goal-directed, inferencein a way that may not be relevant to a task studiedin isolation.The RTE community seems mainly convincedthat incremental advances in local entailment phe-nomena (including application of world knowl-edge) are needed to make significant progress.They need ways to identify sub-problems of tex-tual inference, and to evaluate those solutions bothin isolation and in the context of RTE.
RTE systemdevelopers are likely to reward well-engineeredsolutions by adopting them and citing their au-thors, because such solutions are easier to incor-porate into RTE systems.
They are also morelikely to adopt solutions with established perfor-mance levels.
These characteristics promote pub-lication of software developed to solve NLP tasks,attention to its usability, and publication of mate-rials supporting reproduction of results presentedin technical papers.For these reasons, we assert that RTE is a nat-ural motivator of new NLP tasks, as researcherslook for components capable of improving perfor-mance; and that RTE is a natural setting for evalu-ating solutions to a broad range of NLP problems,though not in its present formulation: we mustsolve the problem of credit assignment, to recog-nize component contributions.
We have thereforeproposed a suitable annotation effort, to providethe resources necessary for more detailed evalua-tion of RTE systems.We have presented a linguistically-motivated1206analysis of entailment data based on a step-wiseprocedure to resolve entailment decisions, in-tended to allow independent annotators to reachconsistent decisions, and conducted a pilot anno-tation effort to assess the feasibility of such a task.We do not claim that our set of domains or phe-nomena are complete: for example, our illustra-tive example could be tagged with a domain Merg-ers and Acquisitions, and a different team of re-searchers might consider Nominalization Resolu-tion to be a subset of Simple Verb Rules.
This kindof disagreement in coverage is inevitable, but webelieve that in many cases it suffices to introducea new domain or phenomenon, and indicate its re-lation (if any) to existing domains or phenomena.In the case of introducing a non-overlapping cate-gory, no additional information is needed.
In othercases, the annotators can simply indicate the phe-nomena being merged or split (or even replaced).This information will allow other researchers tointegrate different annotation sources and main-tain a consistent set of annotations.6 ConclusionsIn this paper, we have presented a case for a broad,long-term effort by the NLP community to coordi-nate annotation efforts around RTE corpora, and toevaluate solutions to NLP tasks relating to textualinference in the context of RTE.
We have iden-tified limitations in the existing RTE evaluationscheme, proposed a more detailed evaluation toaddress these limitations, and sketched a processfor generating this annotation.
We have proposedan initial annotation scheme to prompt discussion,and through a pilot study, demonstrated that suchannotation is both feasible and useful.We ask that researchers not only contributetask specific annotation to the general pool, andindicate how their task relates to those alreadyadded to the annotated RTE corpora, but also in-vest the additional effort required to augment thecross-domain annotation: marking the examplesin which their phenomenon occurs, and augment-ing the annotator-generated explanations with therelevant inference steps.These efforts will allow a more meaningfulevaluation of RTE systems, and of the compo-nent NLP technologies they depend on.
We seethe potential for great synergy between differentNLP subfields, and believe that all parties stand togain from this collaborative effort.
We thereforerespectfully suggest that you ?ask not what RTEcan do for you, but what you can do for RTE...?AcknowledgmentsWe thank the anonymous reviewers for their help-ful comments and suggestions.
This research waspartly sponsored by Air Force Research Labora-tory (AFRL) under prime contract no.
FA8750-09-C-0181, by a grant from Boeing and by MIAS,the Multimodal Information Access and Synthesiscenter at UIUC, part of CCICADA, a DHS Centerof Excellence.
Any opinions, findings, and con-clusion or recommendations expressed in this ma-terial are those of the author(s) and do not neces-sarily reflect the view of the sponsors.ReferencesLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernando Magnini.
2009.
Thefifth pascal recognizing textual entailment chal-lenge.
In Notebook papers and Results, Text Analy-sis Conference (TAC), pages 14?24.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained SemanticVerb Relations.
In Proceedings of Conference onEmpirical Methods in Natural Language Processing(EMNLP-04), pages 33?40.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.I.
Dagan, O. Glickman, and B. Magnini, editors.
2006.The PASCAL Recognising Textual Entailment Chal-lenge., volume 3944.
Springer-Verlag, Berlin.Marie-Catherine de Marneffe, Anna N. Rafferty, andChristopher D. Manning.
2008.
Finding contradic-tions in text.
In Proceedings of ACL-08: HLT, pages1039?1047, Columbus, Ohio, June.
Association forComputational Linguistics.Quang Do, Dan Roth, Mark Sammons, YuanchengTu, and V.G.Vinod Vydiswaran.
2010.
Robust,Light-weight Approaches to compute Lexi-cal Similarity.
Computer Science Researchand Technical Reports, University of Illinois.http://L2R.cs.uiuc.edu/?danr/Papers/DRSTV10.pdf.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third pascal recognizingtextual entailment challenge.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing, pages 1?9, Prague, June.
Associationfor Computational Linguistics.1207Sanda Harabagiu and Andrew Hickl.
2006.
Meth-ods for Using Textual Entailment in Open-DomainQuestion Answering.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 905?912, Sydney,Australia, July.
Association for Computational Lin-guistics.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In Proceedings of HLT/NAACL,New York.D.
Lin and P. Pantel.
2001.
DIRT: discovery of in-ference rules from text.
In Proc.
of ACM SIGKDDConference on Knowledge Discovery and Data Min-ing 2001, pages 323?328.Bill MacCartney and Christopher D. Manning.
2009.An extended model of natural logic.
In The EighthInternational Conference on Computational Seman-tics (IWCS-8), Tilburg, Netherlands.Shachar Mirkin, Lucia Specia, Nicola Cancedda, IdoDagan, Marc Dymetman, and Idan Szpektor.
2009.Source-language entailment modeling for translat-ing unknown terms.
In ACL/AFNLP, pages 791?799, Suntec, Singapore, August.
Association forComputational Linguistics.Sebastian Pado, Michel Galley, Dan Jurafsky, andChristopher D. Manning.
2009.
Robust machinetranslation evaluation with entailment features.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 297?305, Suntec, Singapore,August.
Association for Computational Linguistics.1208
