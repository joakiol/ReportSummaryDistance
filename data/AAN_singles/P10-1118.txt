Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1158?1167,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA Cognitive Cost Model of Annotations Based on Eye-Tracking DataKatrin TomanekLanguage & InformationEngineering (JULIE) LabUniversita?t JenaJena, GermanyUdo HahnLanguage & InformationEngineering (JULIE) LabUniversita?t JenaJena, GermanySteffen LohmannDept.
of Computer Science &Applied Cognitive ScienceUniversita?t Duisburg-EssenDuisburg, GermanyJu?rgen ZieglerDept.
of Computer Science &Applied Cognitive ScienceUniversita?t Duisburg-EssenDuisburg, GermanyAbstractWe report on an experiment to track com-plex decision points in linguistic meta-data annotation where the decision behav-ior of annotators is observed with an eye-tracking device.
As experimental con-ditions we investigate different forms oftextual context and linguistic complexityclasses relative to syntax and semantics.Our data renders evidence that annotationperformance depends on the semantic andsyntactic complexity of the decision pointsand, more interestingly, indicates that full-scale context is mostly negligible ?
withthe exception of semantic high-complexitycases.
We then induce from this obser-vational data a cognitively grounded costmodel of linguistic meta-data annotationsand compare it with existing non-cognitivemodels.
Our data reveals that the cogni-tively founded model explains annotationcosts (expressed in annotation time) moreadequately than non-cognitive ones.1 IntroductionToday?s NLP systems, in particular those rely-ing on supervised ML approaches, are meta-datagreedy.
Accordingly, in the past years, we havewitnessed a massive quantitative growth of anno-tated corpora.
They differ in terms of the nat-ural languages and domains being covered, thetypes of linguistic meta-data being solicited, andthe text genres being served.
We have seen large-scale efforts in syntactic and semantic annotationsin the past related to POS tagging and parsing,on the one hand, and named entities and rela-tions (propositions), on the other hand.
More re-cently, we are dealing with even more challeng-ing issues such as subjective language, a largevariety of co-reference and (e.g., RST-style) textstructure phenomena, Since the NLP communityis further extending their work into these more andmore sophisticated semantic and pragmatic analyt-ics, there seems to be no end in sight for increas-ingly complex and diverse annotation tasks.Yet, producing annotations is pretty expensive.So the question comes up, how we can rationallymanage these investments so that annotation cam-paigns are economically doable without loss in an-notation quality.
The economics of annotations areat the core of Active Learning (AL) where thoselinguistic samples are focused on in the entire doc-ument collection, which are estimated as beingmost informative to learn an effective classifica-tion model (Cohn et al, 1996).
This intentionalselection bias stands in stark contrast to prevailingsampling approaches where annotation examplesare randomly chosen.When different approaches to AL are comparedwith each other, or with standard random sam-pling, in terms of annotation efficiency, up untilnow, the AL community assumed uniform annota-tion costs for each linguistic unit, e.g.
words.
Thisclaim, however, has been shown to be invalid inseveral studies (Hachey et al, 2005; Settles et al,2008; Tomanek and Hahn, 2010).
If uniformitydoes not hold and, hence, the number of annotatedunits does not indicate the true annotation effortsrequired for a specific sample, empirically moreadequate cost models are needed.Building predictive models for annotation costshas only been addressed in few studies for now(Ringger et al, 2008; Settles et al, 2008; Aroraet al, 2009).
The proposed models are basedon easy-to-determine, yet not so explanatory vari-ables (such as the number of words to be anno-tated), indicating that accurate models of anno-tation costs remain a desideratum.
We here, al-ternatively, consider different classes of syntac-tic and semantic complexity that might affect thecognitive load during the annotation process, with1158the overall goal to find additional and empiricallymore adequate variables for cost modeling.The complexity of linguistic utterances can bejudged either by structural or by behavioral crite-ria.
Structural complexity emerges, e.g., from thestatic topology of phrase structure trees and pro-cedural graph traversals exploiting the topologyof parse trees (see Szmrecsa?nyi (2004) or Cheungand Kemper (1992) for a survey of metrics of thistype).
However, structural complexity criteria donot translate directly into empirically justified costmeasures and thus have to be taken with care.The behavioral approach accounts for this prob-lem as it renders observational data of the an-notators?
eye movements.
The technical vehicleto gather such data are eye-trackers which havealready been used in psycholinguistics (Rayner,1998).
Eye-trackers were able to reveal, e.g.,how subjects deal with ambiguities (Frazier andRayner, 1987; Rayner et al, 2006; Traxler andFrazier, 2008) or with sentences which requirere-analysis, so-called garden path sentences (Alt-mann et al, 2007; Sturt, 2007).The rationale behind the use of eye-tracking de-vices for the observation of annotation behavior isthat the length of gaze durations and behavioralpatterns underlying gaze movements are consid-ered to be indicative of the hardness of the lin-guistic analysis and the expenditures for the searchof clarifying linguistic evidence (anchor words) toresolve hard decision tasks such as phrase attach-ments or word sense disambiguation.
Gaze dura-tion and search time are then taken as empiricalcorrelates of linguistic complexity and, hence, un-cover the real costs.
We therefore consider eye-tracking as a promising means to get a better un-derstanding of the nature of the linguistic annota-tion processes with the ultimate goal of identifyingpredictive factors for annotation cost models.In this paper, we first describe an empiricalstudy where we observed the annotators?
readingbehavior while annotating a corpus.
Section 2deals with the design of the study, Section 3 dis-cusses its results.
In Section 4 we then focus onthe implications this study has on building costmodels and compare a simple cost model mainlyrelying on word and character counts and addi-tional simple descriptive characteristics with onethat can be derived from experimental data as pro-vided from eye-tracking.
We conclude with ex-periments which reveal that cognitively groundedmodels outperform simpler ones relative to costprediction using annotation time as a cost mea-sure.
Based on this finding, we suggest that cog-nitive criteria are helpful for uncovering the realcosts of corpus annotation.2 Experimental DesignIn our study, we applied, for the first time ever tothe best of our knowledge, eye-tracking to studythe cognitive processes underlying the annotationof linguistic meta-data, named entities in particu-lar.
In this task, a human annotator has to decidefor each word whether or not it belongs to one ofthe entity types of interest.We used the English part of the MUC7 corpus(Linguistic Data Consortium, 2001) for our study.It contains New York Times articles from 1996 re-porting on plane crashes.
These articles come al-ready annotated with three types of named entitiesconsidered important in the newspaper domain,viz.
?persons?, ?locations?, and ?organizations?.Annotation of these entity types in newspaperarticles is admittedly fairly easy.
We chose thisrather simple setting because the participants inthe experiment had no previous experience withdocument annotation and no serious linguisticbackground.
Moreover, the limited number ofentity types reduced the amount of participants?training prior to the actual experiment, and posi-tively affected the design and handling of the ex-perimental apparatus (see below).We triggered the annotation processes by givingour participants specific annotation examples.
Anexample consists of a text document having onesingle annotation phrase highlighted which thenhad to be semantically annotated with respect tonamed entity mentions.
The annotation task wasdefined such that the correct entity type had to beassigned to each word in the annotation phrase.
Ifa word belongs to none of the three entity types afourth class called ?no entity?
had to be assigned.The phrases highlighted for annotation werecomplex noun phrases (CNPs), each a sequence ofwords where a noun (or an equivalent nominal ex-pression) constitutes the syntactic head and thusdominates dependent words such as determin-ers, adjectives, or other nouns or nominal expres-sions (including noun phrases and prepositionalphrases).
CNPs with even more elaborate inter-nal syntactic structures, such as coordinations, ap-positions, or relative clauses, were isolated from1159their syntactic host structure and the interveninglinguistic material containing these structures wasdeleted to simplify overly long sentences.
We alsodiscarded all CNPs that did not contain at leastone entity-critical word, i.e., one which might be anamed entity according to its orthographic appear-ance (e.g., starting with an upper-case letter).
Itshould be noted that such orthographic signals areby no means a sufficient condition for the presenceof a named entity mention within a CNP.The choice of CNPs as stimulus phrases is mo-tivated by the fact that named entities are usuallyfully encoded by this kind of linguistic structure.The chosen stimulus ?
an annotation example withone phrase highlighted for annotation ?
allows foran exact localization of the cognitive processesand annotation actions performed relative to thatspecific phrase.2.1 Independent VariablesWe defined two measures for the complexity ofthe annotation examples: The syntactic complex-ity was given by the number of nodes in the con-stituent parse tree which are dominated by the an-notation phrase (Szmrecsa?nyi, 2004).1 Accordingto a threshold on the number of nodes in such aparse tree, we classified CNPs as having eitherhigh or low syntactic complexity.The semantic complexity of an annotation ex-ample is based on the inverse document frequencydf of the words in the annotation phrase accordingto a reference corpus.2 We calculated the seman-tic complexity score of an annotation phrase asmax i 1df (wi) , where wi is the i-th word of the anno-tation phrase.
Again, we empirically determined athreshold classifying annotation phrases as havingeither high or low semantic complexity.
Addition-ally, this automatically generated classificationwas manually checked and, if necessary, revisedby two annotation experts.
For instance, if an an-notation phrase contained a strong trigger (e.g., asocial role or job title, as with ?spokeswoman?
inthe annotation phrase ?spokeswoman Arlene?
), itwas classified as a low-semantic-complexity itemeven though it might have been assigned a highinverse document frequency (due to the infrequentword ?Arlene?
).1Constituency parse structure was obtained from theOPENNLP parser (http://opennlp.sourceforge.net/) trained on PennTreeBank data.2We chose the English part of the Reuters RCV2 corpusas the reference corpus for our experiments.Two experimental groups were formed to studydifferent contexts.
In the document context con-dition the whole newspaper article was shown asannotation example, while in the sentence contextcondition only the sentence containing the annota-tion phrase was presented.
The participants3 wererandomly assigned to one of these groups.
We de-cided for this between-subjects design to avoid anyirritation of the participants caused by constantlychanging contexts.
Accordingly, the participantswere assigned to one of the experimental groupsand corresponding context condition already in thesecond training phase that took place shortly be-fore the experiment started (see below).2.2 Hypotheses and Dependent VariablesWe tested the following two hypotheses:Hypothesis H1: Annotators perform differentlyin the two context conditions.H1 is based on the linguistically plausibleassumption that annotators are expected tomake heavy use of the surrounding contextbecause such context could be helpful for thecorrect disambiguation of entity classes.
Ac-cordingly, lacking context, an annotator is ex-pected to annotate worse than under the con-dition of full context.
However, the availabil-ity of (too much) context might overload anddistract annotators, with a presumably nega-tive effect on annotation performance.Hypothesis H2: The complexity of the annota-tion phrases determines the annotation per-formance.The assumption is that high syntactic or se-mantic complexity significantly lowers theannotation performance.In order to test these hypotheses we collected datafor the following dependent variables: (a) the an-notation accuracy ?
we identified erroneous enti-ties by comparison with the original gold annota-tions in the MUC7 corpus, (b) the time needed perannotation example, and (c) the distribution andduration of the participants?
eye gazes.320 subjects (12 female) with an average age of 24 years(mean = 24, standard deviation (SD) = 2.8) and normal orcorrected-to-normal vision capabilities took part in the study.All participants were students with a computing-related studybackground, with good to very good English language skills(mean = 7.9, SD = 1.2, on a ten-point scale with 1 = ?poor?and 10 = ?excellent?, self-assessed), but without any priorexperience in annotation and without previous exposure tolinguistic training.11602.3 Stimulus MaterialAccording to the above definition of complex-ity, we automatically preselected annotation ex-amples characterized by either a low or a high de-gree of semantic and syntactic complexity.
Aftermanual fine-tuning of the example set assuring aneven distribution of entity types and syntactic cor-rectness of the automatically derived annotationphrases, we finally selected 80 annotation exam-ples for the experiment.
These were divided intofour subsets of 20 examples each falling into oneof the following complexity classes:sem-syn: low semantic/low syntactic complexitySEM-syn: high semantic/low syntactic complexitysem-SYN: low semantic/high syntactic complexitySEM-SYN: high semantic/high syntactic complexity2.4 Experimental Apparatus and ProcedureThe annotation examples were presented in acustom-built tool and its user interface was keptas simple as possible not to distract the eye move-ments of the participants.
It merely contained oneframe showing the text of the annotation example,with the annotation phrase being highlighted.
Ablank screen was shown after each annotation ex-ample to reset the eyes and to allow a break, ifneeded.
The time the blank screen was shown wasnot counted as annotation time.
The 80 annotationexamples were presented to all participants in thesame randomized order, with a balanced distribu-tion of the complexity classes.
A variation of theorder was hardly possible for technical and ana-lytical reasons but is not considered critical due toextensive, pre-experimental training (see below).The limitation on 80 annotation examples reducesthe chances of errors due to fatigue or lack of at-tention that can be observed in long-lasting anno-tation activities.Five introductory examples (not considered inthe final evaluation) were given to get the subjectsused to the experimental environment.
All anno-tation examples were chosen in a way that theycompletely fitted on the screen (i.e., text lengthwas limited) to avoid the need for scrolling (andeye distraction).
The position of the CNP withinthe respective context was randomly distributed,excluding the first and last sentence.The participants used a standard keyboard to as-sign the entity types for each word of the annota-tion example.
All but 5 keys were removed fromthe keyboard to avoid extra eye movements for fin-ger coordination (three keys for the positive en-tity classes, one for the negative ?no entity?
class,and one to confirm the annotation).
Pre-tests hadshown that the participants could easily issue theannotations without looking down at the keyboard.We recorded the participant?s eye movementson a Tobii T60 eye-tracking device which is in-visibly embedded in a 17?
TFT monitor and com-paratively tolerant to head movements.
The partic-ipants were seated in a comfortable position withtheir head in a distance of 60-70 cm from the mon-itor.
Screen resolution was set to 1280 x 1024 pxand the annotation examples were presented in themiddle of the screen in a font size of 16 px and aline spacing of 5 px.
The presentation area had nofixed height and varied depending on the contextcondition and length of the newspaper article.
Thetext was always vertically centered on the screen.All participants were familiarized with theannotation task and the guidelines in a pre-experimental workshop where they practiced an-notations on various exercise examples (about 60minutes).
During the next two days, one after theother participated in the actual experiment whichtook between 15 and 30 minutes, including cali-bration of the eye-tracking device.
Another 20-30minutes of training time directly preceded the ex-periment.
After the experiment, participants wereinterviewed and asked to fill out a questionnaire.Overall, the experiment took about two hours foreach participant for which they were financiallycompensated.
Participants were instructed to fo-cus more on annotation accuracy than on annota-tion time as we wanted to avoid random guess-ing.
Accordingly, as an extra incentive, we re-warded the three participants with the highest an-notation accuracy with cinema vouchers.
None ofthe participants reported serious difficulties withthe newspaper articles or annotation tool and allunderstood the annotation task very well.3 ResultsWe used a mixed-design analysis of variance(ANOVA) model to test the hypotheses, with thecontext condition as between-subjects factor andthe two complexity classes as within-subject fac-tors.3.1 Testing Context ConditionsTo test hypothesis H1 we compared the numberof annotation errors on entity-critical words made1161above before anno phrase after belowpercentage of participants looking at a sub-area 35% 32% 100% 34% 16%average number of fixations per sub-area 2.2 14.1 1.3Table 1: Distribution of annotators?
attention among sub-areas per annotation example.by the annotators in the two contextual conditions(complete document vs. sentence).
Surprisingly,on the total of 174 entity-critical words withinthe 80 annotation examples, we found exactly thesame mean value of 30.8 errors per participant inboth conditions.
There were also no significantdifferences in the average time needed to annotatean example in both conditions (means of 9.2 and8.6 seconds, respectively, with F (1, 18) = 0.116,p = 0.74).4 These results seem to suggest that itmakes no difference (neither for annotation accu-racy nor for time) whether or not annotators areshown textual context beyond the sentence thatcontains the annotation phrase.To further investigate this finding we analyzedeye-tracking data of the participants gathered forthe document context condition.
We divided thewhole text area into five sub-areas as schemat-ically shown in Figure 1.
We then determinedthe average proportion of participants that directedtheir gaze at least once at these sub-areas.
We con-sidered all fixations with a minimum duration of100 ms, using a fixation radius (i.e., the smallestdistance that separates fixations) of 30 px and ex-cluded the first second (mainly used for orientationand identification of the annotation phrase).Figure 1: Schematic visualization of the sub-areasof an annotation example.Table 1 reveals that on average only 35% of the4In general, we observed a high variance in the number oferrors and time values between the subjects.
While, e.g., thefastest participant handled an example in 3.6 seconds on theaverage, the slowest one needed 18.9 seconds; concerningthe annotation errors on the 174 entity-critical words, theseranged between 21 and 46 errors.participants looked in the textual context above theannotation phrase embedding sentence, and evenless perceived the context below (16%).
The sen-tence parts before and after the annotation phrasewere, on the average, visited by one third (32%and 34%, respectively) of the participants.
Theuneven distribution of the annotators?
attention be-comes even more apparent in a comparison of thetotal number of fixations on the different text parts:14 out of an average of 18 fixations per examplewere directed at the annotation phrase and the sur-rounding sentence, the text context above the an-notation chunk received only 2.2 fixations on theaverage and the text context below only 1.3.Thus, the eye-tracking data indicates that thetextual context is not as important as might havebeen expected for quick and accurate annotation.This result can be explained by the fact that par-ticipants of the document-context condition usedthe context whenever they thought it might help,whereas participants of the sentence-context con-dition spent more time thinking about a correct an-swer, overall with the same result.3.2 Testing Complexity ClassesTo test hypothesis H2 we also compared the av-erage annotation time and the number of errorson entity-critical words for the complexity subsets(see Table 2).
The ANOVA results show highlysignificant differences for both annotation timeand errors.5 A pairwise comparison of all sub-sets in both conditions with a t-test showed non-significant results only between the SEM-syn andsyn-SEM subsets.6Thus, the empirical data generally supports hy-pothesis H2 in that the annotation performanceseems to correlate with the complexity of the an-notation phrase, on the average.5Annotation time results: F (1, 18) = 25, p < 0.01 forthe semantic complexity and F (1, 18) = 76.5, p < 0.01for the syntactic complexity; Annotation complexity results:F (1, 18) = 48.7, p < 0.01 for the semantic complexity andF (1, 18) = 184, p < 0.01 for the syntactic complexity.6t(9) = 0.27, p = 0.79 for the annotation time in thedocument context condition, and t(9) = 1.97, p = 0.08 forthe annotation errors in the sentence context condition.1162experimental complexity e.-c. time errorscondition class words mean SD mean SD ratesem-syn 36 4.0s 2.0 2.7 2.1 .075document SEM-syn 25 9.2s 6.7 5.1 1.4 .204condition sem-SYN 51 9.6s 4.0 9.1 2.9 .178SEM-SYN 62 14.2s 9.5 13.9 4.5 .224sem-syn 36 3.9s 1.3 1.1 1.4 .031sentence SEM-syn 25 7.5s 2.8 6.2 1.9 .248condition sem-SYN 51 9.6s 2.8 9.0 3.9 .176SEM-SYN 62 13.5s 5.0 14.5 3.4 .234Table 2: Average performance values for the 10 subjects of each experimental condition and 20 anno-tation examples of each complexity class: number of entity-critical words, mean annotation time andstandard deviations (SD), mean annotation errors, standard deviations, and error rates (number of errorsdivided by number of entity-critical words).3.3 Context and ComplexityWe also examined whether the need for inspect-ing the context increases with the complexity ofthe annotation phrase.
Therefore, we analyzed theeye-tracking data in terms of the average num-ber of fixations on the annotation phrase and onits embedding contexts for each complexity class(see Table 3).
The values illustrate that while thenumber of fixations on the annotation phrase risesgenerally with both the semantic and the syntacticcomplexity, the number of fixations on the contextrises only with semantic complexity.
The num-ber of fixations on the context is nearly the samefor the two subsets with low semantic complexity(sem-syn and sem-SYN, with 1.0 and 1.5), whileit is significantly higher for the two subsets withhigh semantic complexity (5.6 and 5.0), indepen-dent of the syntactic complexity.7complexity fix.
on phrase fix.
on contextclass mean SD mean SDsem-syn 4.9 4.0 1.0 2.9SEM-syn 8.1 5.4 5.6 5.6sem-SYN 18.1 7.7 1.5 2.0SEM-SYN 25.4 9.3 5.0 4.1Table 3: Average number of fixations on the anno-tation phrase and context for the document condi-tion and 20 annotation examples of each complex-ity class.These results suggest that the need for contextmainly depends on the semantic complexity of theannotation phrase, while it is less influenced by itssyntactic complexity.7ANOVA result of F (1, 19) = 19.7, p < 0.01 and sig-nificant differences also in all pairwise comparisons.phrase antecedentFigure 2: Annotation example with annotationphrase and the antecedent for ?Roselawn?
in thetext (left), and gaze plot of one participant show-ing a scanning-for-coreference behavior (right).This finding is also qualitatively supported bythe gaze plots we generated from the eye-trackingdata.
Figure 2 shows a gaze plot for one partici-pant that illustrates a scanning-for-coreference be-havior we observed for several annotation phraseswith high semantic complexity.
In this case, wordswere searched in the upper context, which accord-ing to their orthographic signals might refer to anamed entity but which could not completely beresolved only relying on the information given bythe annotation phrase itself and its embedding sen-tence.
This is the case for ?Roselawn?
in the an-notation phrase ?Roselawn accident?.
The con-text reveals that Roselawn, which also occurs inthe first sentence, is a location.
A similar proce-dure is performed for acronyms and abbreviationswhich cannot be resolved from the immediate lo-cal context ?
searches mainly visit the upper con-text.
As indicated by the gaze movements, it alsobecame apparent that texts were rather scanned forhints instead of being deeply read.11634 Cognitively Grounded Cost ModelingWe now discuss whether the findings on dependentvariables from our eye-tracking study are fruitfulfor actually modeling annotation costs.
There-fore, we learn a linear regression model with time(an operationalization of annotation costs) as thedependent variable.
We compare our ?cognitive?model against a baseline model which relies onsome simple formal text features only, and testwhether the newly introduced features help predictannotation costs more accurately.4.1 FeaturesThe features for the baseline model, character- andword-based, are similar to the ones used by Ring-ger et al (2008) and Settles et al (2008).8 Ourcognitive model, however, makes additional useof features based on linguistic complexity, and in-cludes syntactic and semantic criteria related to theannotation phrases.
These features were inspiredby the insights provided by our eye-tracking ex-periments.
All features are designed such that theycan automatically be derived from unlabeled data,a necessary condition for such features to be prac-tically applicable.To account for our findings that syntactic andsemantic complexity correlates with annotationperformance, we added three features based onsyntactic, and two based on semantic complex-ity measures.
We decided for the use of multiplemeasures because there is no single agreed-uponmetric for either syntactic or semantic complex-ity.
This decision is further motivated by find-ings which reveal that different measures are oftencomplementary to each other so that their combi-nation better approximates the inherent degrees ofcomplexity (Roark et al, 2007).As for syntactic complexity, we use two mea-sures based on structural complexity including (a)the number of nodes of a constituency parse treewhich are dominated by the annotation phrase(cf.
Section 2.1), and (b) given the dependencygraph of the sentence embedding the annotationphrase, we consider the distance between wordsfor each dependency link within the annotationphrase and consider the maximum over such dis-8In preliminary experiments our set of basic features com-prised additional features providing information on the usageof stop words in the annotation phrase and on the numberof paragraphs, sentences, and words in the respective annota-tion example.
However, since we found these features did nothave any significant impact on the model, we removed them.tance values as another metric for syntactic com-plexity.
Lin (1996) has already shown that humanperformance on sentence processing tasks can bepredicted using such a measure.
Our third syn-tactic complexity measure is based on the prob-ability of part-of-speech (POS) 2-grams.
Givena POS 2-gram model, which we learned fromthe automatically POS-tagged MUC7 corpus, thecomplexity of an annotation phrase is defined by?ni=2 P (POSi|POSi?1) where POSi refers to thePOS-tag of the i-th word of the annotation phrase.A similar measure has been used by Roark et al(2007) who claim that complex syntactic struc-tures correlate with infrequent or surprising com-binations of POS tags.As far as the quantification of semantic com-plexity is concerned, we use (a) the inverse docu-ment frequency df (wi) of each word wi (cf.
Sec-tion 2.1), and a measure based on the semanticambiguity of each word, i.e., the number of mean-ings contained in WORDNET,9 within an annota-tion phrase.
We consider the maximum ambigu-ity of the words within the annotation phrase asthe overall ambiguity of the respective annotationphrase.
This measure is based on the assumptionthat annotation phrases with higher semantic am-biguity are harder to annotate than low-ambiguityones.
Finally, we add the Flesch-Kincaid Read-ability Score (Klare, 1963), a well-known metricfor estimating the comprehensibility and readingcomplexity of texts.As already indicated, some of the hardness ofannotations is due to tracking co-references andabbreviations.
Both often cannot be resolved lo-cally so that annotators need to consult the con-text of an annotation chunk (cf.
Section 3.3).Thus, we also added features providing informa-tion whether the annotation phrases contain entity-critical words which may denote the referent of anantecedent of an anaphoric relation.
In the samevein, we checked whether an annotation phrasecontains expressions which can function as an ab-breviation by virtue of their orthographical appear-ance, e.g., consist of at least two upper-case letters.Since our participants were sometimes scanningfor entity-critical words, we also added featuresproviding information on the number of entity-critical words within the annotation phrase.
Ta-ble 4 enumerates all feature classes and single fea-tures used for determining our cost model.9http://wordnet.princeton.edu/1164Feature Group # Features Feature Descriptioncharacters (basic) 6 number of characters and words per annotation phrase; test whetherwords in a phrase start with capital letters, consist of capital letters only,have alphanumeric characters, or are punctuation symbolswords 2 number of entity-critical words and percentage of entity-critical wordsin the annotation phrasecomplexity 6 syntactic complexity: number of dominated nodes, POS n-gram proba-bility, maximum dependency distance;semantic complexity: inverse document frequency, max.
ambiguity;general linguistic complexity: Flesch-Kincaid Readability Scoresemantics 3 test whether entity-critical word in annotation phrase is used in docu-ment (preceding or following current phrase); test whether phrase con-tains an abbreviationTable 4: Features for cost modeling.4.2 EvaluationTo test how well annotation costs can be mod-eled by the features described above, we used theMUC7T corpus, a re-annotation of the MUC7 cor-pus (Tomanek and Hahn, 2010).
MUC7T has timetags attached to the sentences and CNPs.
Thesetime tags indicate the time it took to annotate therespective phrase for named entity mentions of thetypes person, location, and organization.
We heremade use of the time tags of the 15,203 CNPs inMUC7T .
MUC7T has been annotated by two an-notators (henceforth called A and B) and so weevaluated the cost models for both annotators.
Welearned a simple linear regression model with theannotation time as dependent variable and the fea-tures described above as independent variables.The baseline model only includes the basic featureset, whereas the ?cognitive?
model incorporates allfeatures described above.Table 5 depicts the performance of both mod-els induced from the data of annotator A and B.The coefficient of determination (R2) describesthe proportion of the variance of the dependentvariable that can be described by the given model.We report adjusted R2 to account for the differentnumbers of features used in both models.model R2 on A?s data R2 on B?s databaseline 0.4695 0.4640cognitive 0.6263 0.6185Table 5: Adjusted R2 values on both models andfor annotators A and B.For both annotators, the baseline model is sig-nificantly outperformed in terms of R2 by our?cognitive?
model (p < 0.05).
Considering thefeatures that were inspired from the eye-trackingstudy, R2 is increased from 0.4695 to 0.6263 onthe timing data of annotator A, and from 0.464 to0.6185 on the data of annotator B.
These numbersclearly demonstrate that annotation costs are moreadequately modelled by the additional features weidentified through our eye-tracking study.Our ?cognitive?
model now consists of 21 co-efficients.
We tested for the significance of thismodel?s regression terms.
For annotator A wefound all coefficients to be significant with respectto the model (p < 0.05), for annotator B all coeffi-cients except one were significant.
Figure 6 showsthe coefficients of annotator A?s ?cognitive?
modelalong with the standard errors and t-values.5 Summary and ConclusionsIn this paper, we explored the use of eye-trackingtechnology to investigate the behavior of humanannotators during the assignment of three types ofnamed entities ?
persons, organizations and loca-tions ?
based on the eye-mind assumption.
Wetested two main hypotheses ?
one relating to theamount of contextual information being used forannotation decisions, the other relating to differ-ent degrees of syntactic and semantic complex-ity of expressions that had to be annotated.
Wefound experimental evidence that the textual con-text is searched for decision making on assigningsemantic meta-data at a surprisingly low rate (with1165Feature Group Feature Name/Coefficient Estimate Std.
Error t value Pr(>|t|)(Intercept) 855.0817 33.3614 25.63 0.0000characters (basic) token number -304.3241 29.6378 -10.27 0.0000char number 7.1365 2.2622 3.15 0.0016has token initcaps 244.4335 36.1489 6.76 0.0000has token allcaps -342.0463 62.3226 -5.49 0.0000has token alphanumeric -197.7383 39.0354 -5.07 0.0000has token punctuation -303.7960 50.3570 -6.03 0.0000words number tokens entity like 934.3953 13.3058 70.22 0.0000percentage tokens entity like -729.3439 43.7252 -16.68 0.0000complexity sem compl inverse document freq 392.8855 35.7576 10.99 0.0000sem compl maximum ambiguity -13.1344 1.8352 -7.16 0.0000synt compl number dominated nodes 87.8573 7.9094 11.11 0.0000synt compl pos ngram probability 287.8137 28.2793 10.18 0.0000syn complexity max dependency distance 28.7994 9.2174 3.12 0.0018flesch kincaid readability -0.4117 0.1577 -2.61 0.0090semantics has entity critical token used above 73.5095 24.1225 3.05 0.0023has entity critical token used below -178.0314 24.3139 -7.32 0.0000has abbreviation 763.8605 73.5328 10.39 0.0000Table 6: ?Cognitive?
model of annotator A.the exception of tackling high-complexity seman-tic cases and resolving co-references) and that an-notation performance correlates with semantic andsyntactic complexity.The results of these experiments were taken asa heuristic clue to focus on cognitively plausi-ble features of learning empirically rooted costmodels for annotation.
We compared a simplecost model (basically taking the number of wordsand characters into account) with a cognitivelygrounded model and got a much higher fit for thecognitive model when we compared cost predic-tions of both model classes on the recently re-leased time-stamped version of the MUC7 corpus.We here want to stress the role of cognitive evi-dence from eye-tracking to determine empiricallyrelevant features for the cost model.
The alterna-tive, more or less mechanical feature engineering,suffers from the shortcoming that is has to dealwith large amounts of (mostly irrelevant) features?
a procedure which not only requires increasedamounts of training data but also is often compu-tationally very expensive.Instead, our approach introduces empirical,theory-driven relevance criteria into the featureselection process.
Trying to relate observablesof complex cognitive tasks (such as gaze dura-tion and gaze movements for named entity anno-tation) to explanatory models (in our case, a time-based cost model for annotation) follows a muchwarranted avenue in research in NLP where fea-ture farming becomes a theory-driven, explanatoryprocess rather than a much deplored theory-blindengineering activity (cf.
ACL-WS-2005 (2005)).In this spirit, our focus has not been on fine-tuning this cognitive cost model to achieve evenhigher fits with the time data.
Instead, we aimed attesting whether the findings from our eye-trackingstudy can be exploited to model annotation costsmore accurately.Still, future work will be required to optimizea cost model for eventual application where evenmore accurate cost models may be required.
Thisoptimization may include both exploration of ad-ditional features (such as domain-specific ones)as well as experimentation with other, presum-ably non-linear, regression models.
Moreover,the impact of improved cost models on the effi-ciency of (cost-sensitive) selective sampling ap-proaches, such as Active Learning (Tomanek andHahn, 2009), should be studied.1166ReferencesACL-WS-2005.
2005.
Proceedings of the ACL Work-shop on Feature Engineering for Machine Learn-ing in Natural Language Processing.
accessiblevia http://www.aclweb.org/anthology/W/W05/W05-0400.pdf.Gerry Altmann, Alan Garnham, and Yvette Dennis.2007.
Avoiding the garden path: Eye movementsin context.
Journal of Memory and Language,31(2):685?712.Shilpa Arora, Eric Nyberg, and Carolyn Rose?.
2009.Estimating annotation cost for active learning in amulti-annotator environment.
In Proceedings of theNAACL HLT 2009 Workshop on Active Learning forNatural Language Processing, pages 18?26.Hintat Cheung and Susan Kemper.
1992.
Competingcomplexity metrics and adults?
production of com-plex sentences.
Applied Psycholinguistics, 13:53?76.David Cohn, Zoubin Ghahramani, and Michael Jordan.1996.
Active learning with statistical models.
Jour-nal of Artificial Intelligence Research, 4:129?145.Lyn Frazier and Keith Rayner.
1987.
Resolution ofsyntactic category ambiguities: Eye movements inparsing lexically ambiguous sentences.
Journal ofMemory and Language, 26:505?526.Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the effects of selective sampling on theannotation task.
In CoNLL 2005 ?
Proceedings ofthe 9th Conference on Computational Natural Lan-guage Learning, pages 144?151.George Klare.
1963.
The Measurement of Readability.Ames: Iowa State University Press.Dekang Lin.
1996.
On the structural complexity ofnatural language sentences.
In COLING 1996 ?
Pro-ceedings of the 16th International Conference onComputational Linguistics, pages 729?733.Linguistic Data Consortium.
2001.
Message Under-standing Conference (MUC) 7.
Philadelphia: Lin-guistic Data Consortium.Keith Rayner, Anne Cook, Barbara Juhasz, and LynFrazier.
2006.
Immediate disambiguation of lex-ically ambiguous words during reading: Evidencefrom eye movements.
British Journal of Psychol-ogy, 97:467?482.Keith Rayner.
1998.
Eye movements in reading andinformation processing: 20 years of research.
Psy-chological Bulletin, 126:372?422.Eric Ringger, Marc Carmen, Robbie Haertel, KevinSeppi, Deryle Lonsdale, Peter McClanahan, JamesCarroll, and Noel Ellison.
2008.
Assessing thecosts of machine-assisted corpus annotation througha user study.
In LREC 2008 ?
Proceedings of the 6thInternational Conference on Language Resourcesand Evaluation, pages 3318?3324.Brian Roark, Margaret Mitchell, and Kristy Holling-shead.
2007.
Syntactic complexity measures fordetecting mild cognitive impairment.
In Proceed-ings of the Workshop on BioNLP 2007: Biological,Translational, and Clinical Language Processing,pages 1?8.Burr Settles, Mark Craven, and Lewis Friedland.
2008.Active learning with real annotation costs.
InProceedings of the NIPS 2008 Workshop on Cost-Sensitive Machine Learning, pages 1?10.Patrick Sturt.
2007.
Semantic re-interpretation andgarden path recovery.
Cognition, 105:477?488.Benedikt M. Szmrecsa?nyi.
2004.
On operationalizingsyntactic complexity.
In Proceedings of the 7th In-ternational Conference on Textual Data StatisticalAnalysis.
Vol.
II, pages 1032?1039.Katrin Tomanek and Udo Hahn.
2009.
Semi-supervised active learning for sequence labeling.
InACL 2009 ?
Proceedings of the 47th Annual Meet-ing of the ACL and the 4th IJCNLP of the AFNLP,pages 1039?1047.Katrin Tomanek and Udo Hahn.
2010.
Annotationtime stamps: Temporal metadata from the linguisticannotation process.
In LREC 2010 ?
Proceedings ofthe 7th International Conference on Language Re-sources and Evaluation.Matthew Traxler and Lyn Frazier.
2008.
The role ofpragmatic principles in resolving attachment ambi-guities: Evidence from eye movements.
Memory &Cognition, 36:314?328.1167
