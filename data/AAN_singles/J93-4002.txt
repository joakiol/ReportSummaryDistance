Parsing Some Constrained GrammarFormalismsK.
Vijay-Shanker*University of DelawareDavid J. Weir*University of SussexIn this paper we present a scheme to extend a recognition algorithm for Context-Free Gram-mars (CFG) that can be used to derive polynomial-time r cognition algorithms for a set of for-malisms that generate a superset of languages generated by CFG.
We describe the scheme bydeveloping a Cocke-Kasami-Younger (CKY)-like pure bottom-up recognition algorithm for Lin-ear Indexed Grammars and show how it can be adapted to give algorithms for Tree AdjoiningGrammars and Combinatory Categorial Grammars.
This is the only polynomial-time recognitionalgorithm for Combinatory Categorial Grammars that we are aware of.The main contribution of this paper is the general scheme we propose for parsing a variety offormalisms whose derivation process is controlled by an explicit or implicit stack.
The ideas pre-sented here can be suitably modified for other parsing styles or used in the generalized frameworkset out by Lang (1990).1.
Introduct ionThis paper presents a scheme to extend known recognition algorithms for Context-FreeGrammars (CFG) in order to obtain recognition algorithms for a class of grammaticalformalisms that generate a strict superset of the set of languages generated by CFG.In particular, we use this scheme to give recognition algorithms for Linear IndexedGrammars (LIG), Tree Adjoining Grammars (TAG), and a version of CombinatoryCategorial Grammars (CCG).
These formalisms belong to the class of mildly context-sensitive grammar formalisms identified by Joshi (1985) on the basis of some propertiesof their generative capacity.
The parsing strategy that we propose can be applied tothe formalisms listed as well as others that have similar characteristics (as outlinedbelow) in their derivational process.
Some of the main ideas underlying our schemehave been influenced by the observations that can be made about the constructionsused in the proofs of the equivalence of these formalisms and Head Grammars (HG)(Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993).There are similarities between the TAG and HG derivation processes and that ofContext-Free Grammars (CFG).
This is reflected in common features of the parsingalgorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and theCKY algorithm for CFG (Kasami 1965; Younger 1967).
In particular, what can happenat each step in a derivation can depend only on which of a finite set of "states" thederivation is in (for CFG these states can be considered to be the nonterminal symbols).This property, which we refer to as the context-freeness property, is important becauseit allows one to keep only a limited amount of context during the recognition process,* Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716.E-mail: vijay@udel.edu.School of Cognitive and Computing Sciences, University ofSussex, Brighton BN1 9QH, U.K.
E-mail:davidw@cogs.susx,ac.uk.?
1994 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 4which results in polynomial time algorithms.
In the recognition algorithms mentionedabove for CFG, HG, and TAG this is reflected in the fact that the recognizer can encodeintermediate stages of the derivation with a bounded number  of states.
An array isused whose entries are associated with a given component of the input.
In the case ofthe CKY algorithm, the presence of a particular nonterminal in an array entry is usedto encode the fact that the nonterminal derives the associated substring of the input.The context-freeness of CFG has the consequence that there is no need to encode theway, or ways, in which a nonterminal came to be placed in an array entry.In this respect, the derivation processes of CCG and LIG would appear to differfrom that of CFG.
In these systems unbounded stacklike structures replace the roleplayed by nonterminals in controlling derivation choices.
This would seem to suggestthat the context-freeness property of CFG, HG, and TAG derivations no longer holds.Unbounded stacks can encode an unbounded number  of earlier derivation choices.
Infact, while the path sets 1 of CFG, HG, and TAG derivation trees are regular languages,the path sets of CCG and LIG are context-free languages.
With respect o recognitionalgorithms, this suggests that the array (whose entries contain nonterminals in thecase of CFG) would need to contain complete ncodings of unbounded stacks givingan exponential time algorithm.However, in LIG and CCG, the use of stacks to control derivations is limited inthat different branches of a derivation cannot share stacks.
Thus, despite the aboveobservations, the context-freeness property does in fact hold.
A detailed explanationof why this is so will be presented below.
We propose a method to extend the CKYalgorithm to handle the limited use of stacks found in CCG and LIG.
We have chosen toadapt the CKY algorithm since it is the simplest form of bottom-up arsing.
A similarapproach using Earley algorithm is also possible, although not considered here.
Sincethe use of the stacks is most explicit in the LIG formalism we describe our approach indetail by developing a recognition algorithm for LIG (Sections 2 and 3).
We then showhow the general approach suggested in the parser for LIG can be tailored to CCG (inSection 4).
In the above discussion TAG has been grouped with HG.
However,  TAGcan also be viewed as making use of stacks in the same way as LIG and CCG.
InSection 5 we show how the LIG algorithm presented in Section 3 can be adapted forTAG.2.
Linear Indexed GrammarsAn Indexed Grammar  (Aho 1968) can be viewed as a CFG in which objects are nonter-minals with an associated stack of symbols.
In addition to rewriting nonterminals, therules of the grammar  can have the effect of pushing or popping symbols on top of thestacks that are associated with each nonterminal.
Gazdar (1988) discussed a restrictedform of Indexed Grammars  in which the stack associated with the nonterminal on theleft of each production can only be associated with one of the occurrences of non-terminals on the right of the production.
Stacks of bounded size are associated withother occurrences of nonterminals on the right of the production.
We call this LinearIndexed Grammars  (LIG).
21 The path set of a tree is the set of strings labeling paths from the root to the frontier of the tree.
Thepath set of a tree set is the union of path sets of trees in the set.2 The name Linear Indexed Grammars i used by Duske and Parchmann (1984) to refer to a differentrestriction on Indexed Grammars in which production was restricted to have only a single nonterminalon their right-hand side.592K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsDefinition 2.1A LIG, G, is denoted by (VN, VT, VI, S, P) whereVN is a finite set of nonterminals,VT is a finite set of terminals,VI is a finite set of indices (stack symbols),S c VN is the start symbol, andP is a finite set of productions.We adopt the convention that (~, fl (with or without subscripts and primes) de-note members of V~, and ~ denotes a stack symbol.
As usual, A, B, C will denotenonterminals, a b, c will denote terminals, and u, v, w will denote members of V~.Definition 2.2A pair consisting of a nonterminal, say A, and a string of stack symbols, say (~, willbe called an object of the grammar and will be written as A (c~).
Given a grammar, G,we define the set of objects Vc(G) = { A ((~) I A C VN, (~ E V~ }.We use T to denote strings in (Vc(G) U VT)*.
We write A(-.~) to denote the non-terminal A associated with an arbitrary stack (~ with the string on top.
Also, we useA () to denote that an empty stack is associated with A.
The general form of a pro-duction in a LIG is:a (.. (~) --+ Wla l  (oL1)w2... ai-1 (oq-1) wiai (.. oq) Wi+lai+ 1 (oq+ 1)... A n (o@) Wn+ 1 for n >0 and wl .
.
.
,  W,+l are members of V~-.Definition 2.3The derivation relation, ~ ,  is defined below.
If the above production is used then forany fl ~ V{, T1, T2 E (Vc(G) U Wv) *:T1A (rico T2 ~ TlWlA1 (o~1)W2... Ai-1 (oq-1) wiAi (tic, i)Wi+lAi+l (Oq+l)?
.. An (oln) wn+IT2.We use ~ as the reflexive, transitive closure of ~ .
As a result of the linearity inthe general form of the rules, we can observe that the stack flc~ associated with theobject in the left-hand side of the derivation and flc~i associated with one object inthe right-hand side have the initial part fl in common.
In the derivation above, wewill say that this object a i (flOq) is the distinguished child of A (flo0.
Given a deriva-tion, the distinguished descendant relation is the reflexive, transitive closure of thedistinguished child relation.The language generated by a LIG, G, L(G) = { w I S() ~ w }.Example 2.1The LIG, G = ({ S, T }, { a, b, c }, { ")/a~ "Yb )~ S~/9) generates ( wcw \] w C {a, b} + } where Pcontains the following productions.S( .
. )
- *aS( .
.%)  S(..)--~bS(..q/b) S(..)---~ T(..)T(.
.%)--,  T(.
.
)a T(..',/b)-+ T(.
.
)b T() - -*cA derivation tree for the string abbcabb is given in Figure 1.593Computational Linguistics Volume 19, Number 4sf )b s%v"~aV bT(~_ ) bT()  aI?Figure 1Derivation tree for LIG.In this paper rather than adopting the general form of rules as given above, werestrict our attention to grammars whose rules have the following form.
In fact, thiscan be easily seen to constitute a normal form for LIG.1.
A (c0 ~ c where ~ C VT U {c} and length of c~, len (,9<) >>_ 1.2.
A (.. "/1.. .
Q/m) ----> Ap (.. Vp) As (O<s) where m > 0.3. a ('" '71".
"Ym) --" As (OLs) ap (.. ~p) where m > 0.4.
A ("71.. .
7m) "--+ Ap (.. 7p) where m > 0.We allow at most two symbols in the right-hand side of productions because weintend to develop CKY-style algorithms.
In the above rules we say that AF (.. "yp) isthe primary constituent and As (c~s) is the secondary constituent.
Notice also thatin a derivation using such a rule, the primary constituent yields the distinguishedchild.
(In grammatical theories that use a stack of subcategorized arguments, the topof the stack in the primary constituent determines which secondary constituent i cancombine with.
)2.1 TerminatorsLet us consider how we may extend the CKY algorithm for the recognition of LIG.Given a fixed grammar G and an input al ?
.. an, the recognition algorithm will completean n x n array P such that an encoding of A (cO is stored in P \[i, d\] if and only if A (oQai... ai+d-1.
The algorithm will operate bottom-up.
For example, if G contains the rulea ('" ")11... "Ym) ---+ ap (.. "~p) A s (O~s) and we find an encoding of Ap (O<p'yp) in P Ii, dp\] andan encoding of As (C~s) in P Ii + dp~ ds\] then an encoding of A (C~p'yl... "Ym) will be stored594K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalismsin P Ii, dp + dsl.
What encoding scheme should be used?
The most straightforwardpossibility would be to store a complete encoding of A (c~p3,~... 3,m) in P \[i, dp + ds\].However, in general, if an object A (~) derives a string of length d then the length ofo~ is (,.9(d).
3 Hence there can be O(/d) objects that derive a substring of the input (oflength d), for some constant k. Hence, the space and time complexity of this algorithmis exponential in the worst case.
4The inefficiency of this approach can be seen by drawing an analogy with thefollowing algorithm for CFG.
Suppose rather than storing sets of nonterminals in eacharray entry, we store a set of trees containing all derivation subtrees that yield thecorresponding substring.
The problem with this is that the number of derivation trees isexponential with respect o the length of the string spanned.
However, there is no needto store derivation trees since in considering the combination of subderivation treesin the CFG, only the nonterminals at the root of the tree are relevant in determiningwhether there is a production that licenses the combination.Likewise because of the last-in first-out behavior in the manipulation of stacksin LIG, we will argue that it is not necessary to store the entire stack.
For instance,consider the derivation (depicted by the tree shown in Figure 2) from the point ofview of recording the derivation in a bottom-up arser (such as CKY).
Let a node ~?1labeled B (fl3,1 .. .
3,k... 3,m) be a distinguished descendant of a node ~1 labeled A (fl3,1 .. .
3,k)as shown in the figure.
Viewing the tree bottom-up, let the node ~\], labeled A (fl3,1 ?
?.
3,k),be the first node above the node ~71, labeled B (fl3,1 ?.
?
3,k.
?
?
3,m), where 3,k gets exposedas the top of the stack.
Because of the last-in first-out behavior, every distinguisheddescendant of ~\] above 711 will have a label of the form A I (fl3,1 .. .
3,k~) where len (~) > 1.In order to record the derivation from A (fl3,1 .. .
3,k) it would be sufficient o store Aand 3'1 .. ?
3,k if we could also access the entry that records the derivation from At (fl3,t).In the entry for ~?, using a pointer to the entry for At (fl3,t) would enable the recoveryof the stack below the top k symbols, 3,1 ?
.. "Yk.
However, this scheme works well onlywhen k _> 2.
For instance, when k = 1, suppose we recorded only A, 3,1, and a pointerto entry for At (fl3,t).
Suppose that we are looking for the symbol below 3,1, i.e., thetop of ft. Then it is possible that in a similar way the latter entry could also recordjust At~ 3,t, and a pointer to some other entry to retrieve ft.
This situation can occurarbitrarily many times.Consider the derivation depicted in Figure 3.
In this derivation we have indi-cated the branch containing only the distinguished escendants.
We will assume thatthe node labeled D (f13,, ..-3,k-13,~ .-.
3/~n ,) is the closest distinguished escendant ofC (fl3,1..-3,k-13,~) such that every node between them will have a label of the formC' (fl"Yl-., 3,k-13,~ O/) where len (~') > 1.
Therefore, any node between that labeledC (fl3,1..-3,k-13,~) and B(fl3,1...3,rn) will have a label of the form C" (fl3,1..-"~k-10/')where fen (c~") > 1.
Now the entries representing derivations from both A(fl3,1...3,k-13,k) and C (fl3,1... 3,k-13,~) could point back to the entry for the derivation fromAt (fl3,t), whereas the entry for C' (fl3,1 ...3,k-13,~c~') will point back to the entry forAWe shall now formalize these notions by defining a terminator.3 For instance, consider the grammar in Example 2.1 and the derivation in Figure 1.
In general we canhave derivations of the form T (q'a3"~) ~ cab n. However, if there exists productions of the formA (c~) --~ ~ then the length of the stack in objects is not even bounded by the length of strings theyderive.4 The CCG parsing algorithms that have been proposed so far follow this strategy (Pareschi andSteedman 1987; Tomita 1988).595Computational Linguistics Volume 19, Number 4H 1Figure 2Recovering the rest of stack-1.
)m ZFigure 3Recovering the rest of stack-2.v vFigure 4Definition of a Terminator.596K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsDefinition 2.4Suppose that we have the derivation tree in Figure 4 that depicts the following deriva-tion:A (fl3q ... %-17) ~ uB (fl'Ya .. .
q/k--lq/k.''
q/m) WuAt (flq/t) As (O~s) wuvwor similarly:A (flq/1"'- q/k-lq/) uB (flq/1... q/k-lq/i.
.
,  q/m) WuAs (Ols) At (flq/t) wl~ V Wwhere the following conditions hold2<k<mThe nodes labeled B (flq/1 .
.
.
q /k - lq /k  .
.
.
q/m) and At (flq/t) are distinguisheddescendants of the node labeled A (flq/1 ... q/k-lq/) in the respective trees.For any distinguished escendent labeled C (c~') between the nodeslabeled A (flq/1... ")~k-lq/) and B (flq/1.-.
q/k-lq/k' ' '  q/m), O/ is of the formflq/1 ?
?
?
q/kC~ where len (c~) > 1.
Note that the nodes labeleda (flq/1... q/k-lq/) and B (flq/1... q/k-lq/k''" q/m) need not be different.The node labeled At (flq/t) is the k-terminator of the node labeled A (flq/1 .. .
q/k-lq/).When it is clear from context, rather than saying that a node is a terminator ofanother we will assume that terminators have been defined on objects that participatein a derivation as well.
For instance, in the above derivations, we will say that At (flq/t)is the k-terminator of A (fl71 ... 7k-l"Y).
Also when the derivation is clear from context,we will omit the mention of the derivation (or derivation tree).
Additionally, we willsay that a node (object) has a terminator, if it has a k-terminator for some k.We will now state some properties of terminators that influence the design of ourrecognition algorithm.Definition 2.5Given a grammar, G, define MCL(G) (Maximum Change in Length) as:MCL(G) = max { m \] A (.. q/1.
.
.
q/m) --* T1Ap ('" ~p) T2 is a production of G }Henceforth, we will write MCL since the grammar in question will always be knownfrom context.Observation 2.1In a derivation tree, if a node (say ~) has a k-terminator (say ~t) then ~t is a dis-tinguished descendant of ~/.
If the node ~/is labeled A (flc~) (where len (c~) = k) thenthe node 7/t must be labeled A t (flq/t) for some At C VN and q/t ff VI.
Furthermore,2 < k < MCL.Observation 2.2In a derivation tree, if a node has a k-terminator then it has a unique terminator.597Computational Linguistics Volume 19, Number 4If ~/is the node in question then we are claiming here that not only does it have aunique k-terminator but also that there does not exist k ~ with k' ~ k such that ~ has akMerminator.
To see why this is the case, let some node ~?
have a k-terminator (for somek), say ~t.
Using Observation 2.1 we can assume that they are labeled A (fl~l .
.
.
~k-l"Y)and At (flq/t), respectively, where we have (k-1)  > 1.
From the definition of terminatorswe can assume that the parent of the terminator, ~/t, is a node (say ~') that has a label ofthe form B (fl3'1 .. .
"/k-l"~k... "Ym).
Since (from the definition of terminators) every nodebetween ~ and 7/~ (inclusive) must have a label of the form C (fl'Yl -.. ")'k-la ~) wherelen (a ~) >_ 1, it immediately follows that Tit is the closest distinguished escendant ofsuch that the length of the stack in the object labeling ~\]t is strictly less than the lengthof the stack in the object labeling ~/.
From this, the uniqueness of terminators follows.Observation 2.3Consider the derivation A (fl"Yl .
.
.
"Yk-l"~) ~ uAt (fl"Yt) w ~ uvw where At (fl'~t) is thek-terminator of A (fl~/1---'Tk-l"Y).
Then for any fl' and v', if At (fl'~'t) ~ v' then wehave the derivation A (fl'~l .
.
.
"/k-~"/) ~ uAt (fl"Yt) w ~ uv'w where At (fl"~t) is thek-terminator of A (fl"~l ... 3~k-~'Y).This follows from the fact that the derivation of uAt (fl"yt) w from A (fl'Yl .
.
.
"Yk-l"7)is independent of ft.
Therefore we can replace At (fl')'t) ~ v by At (fl'"/t) =~ v'.
Thisis a very important property that is crucial for obtaining polynomial-time algorithm.Note that not all nodes have terminators.
For example, if a node labeled A (a) is theparent of a node labeled a (i.e., corresponding to the use of the production A (a) --* awhere a is a terminal symbol) then obviously this node does not have a terminator.Definit ion 2.6Given a grammar, G, we define MTL(G) (Maximum Length in, Terminal production)as :MTL(G) = max { len (a) \] A (a) --* c is a production of G where ~ c VT (_J{?}
}.As in the case of MCL, we will use MTL rather than MTL(G).Observation 2.4In the derivation A (a) ~ w if len (a) > MTL then A (a) has a terminator.There must be at least two steps in the above derivation since len (a) > MTL.However, we can assume that the node (say 7) in question labeled by the object1A (a) has a distinguished escendant, say ~/~, with label B (fl) such that B (fl) ~ ?.Therefore, len (fl) <_ MTL and we may rewrite w as u?v.
Since fen (a) > len (fl) wecan find the closest distinguished escendant of ~/labeled C (a ~) for some C, a ~ suchthat len (a ~) < fen (a).
That node is the terminator of ~\] from the arguments made inObservation 2.2.The above observations will be used in the following sections to explain the wayin which we represent derivations in the parsing table.
We conclude this section withan observation that has a bearing on the steps of the recognition algorithm.598K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsObservation 2.5Consider the following derivation.a (fl'~l""" /k-l) T IA .
(fl~/l... ')'k-l"Yk) T2l~llap (fl71... 7k-17k) U2Ul Vl At (fl'/t) v2u2Ul Vl WV2U2where Ap (fiVe... "Yk-l'Yk) is the distinguished child of A (f17~... ')'k-l) and At (flVt) is thek-terminator of Ap (fl71..- 7k-l"Yk).
At (fl~t) is the (k - 1)-terminator f A (fl71 .-- ")'k-l)if and only if k > 2.
If k = 2 then A (fl71) has a terminator if and only if At (fl'Tt)does.
In fact, in this case, if At (fl'Tt) has a kMerminator then that terminator is also thek~-terminator f A (flVt).This can be seen by considering the derivation shown in Figure 3 and noting thesharing of the terminator of C (fl3'1.-.
7k-17~) and A (fl"/1-.. 7k-l")/k) ?3.
Recognition AlgorithmsAs in the CKY algorithm we will use a two-dimensional rray, P, such that if A (c~)ai.. ?
ai+d-1 then a representation f this derivation will be recorded with an encoding ofA ((~) in P \[i, d\].
Here we assume that the given input is al .. .
an.
We start our discussionby considering the data structures we use to record such objects and derivations fromthem.3.1 Anatomy of an EntryWe mentioned earlier that the stack in an object can be unboundedly large.
We mustfirst find a compact way to store encodings of such objects whose size is not boundedby the grammar.
In this section we provide some motivation for the encoding schemeused in the recognition algorithm by considering the bottom-up application of the ruleand the encoding of the primary constituent:A (..'y1.
.
.
'~m) --* Ap ("~/p) As (,~s)The Head.
An object with nonterminal Ap and top of stack "Tp will match the primarycategory of this rule.
Thus, the first requirement is that at least this much of the objectmust be included in every entry since it is needed to determine if the rule can apply.This component is denoted lap, vp/and called the head of the entry.
Thus, in general,an entry in P Ii, d I with the head {A,'~/ encodes derivations of ai...ai+cl-1 from anobject of the form A (fl'y) for some f l?
V 7.Terminator-pointer.
An encoding of the object Ap (fl'Tp) (the primary constituent) hatderives the substring ai ... ai+dp_ 1 (o f  the input string al ?
?
.
an) will be stored in the arrayelement P {i, dp\] in our CKY-style recognition algorithms.
Now consider the encodingof Ap (fl'yp) for some sufficiently long fl-yp.
While the head, lAp, ~p), of the entry issufficient o determine whether the object in question can match the primary categoryof the rule, we will need to store more information in order that we can determine thecontent of the rest of the stack.
In the above production, if m = 0 then the combinationof Ap (fl~/p) and As (~s) results in A (fl).
In order to record the derivation from A (fl),we need to know the top symbol in the stack fl, i.e., the symbol below the top ofthe stack associated with the primary constituent.
We need to recover the identity of599Computational Linguistics Volume 19, Number 4this symbol from the encoding of the primary category.
This is why we introducedthe notion of terminators.
As mentioned in Section 2.1, terminators can be used toaccess information about the rest of the stack.
In the encoding of Ap (fl'yp), we willstore information that allows us to access the encoding of its terminator.
The part ofthe entry encoding the terminator will be called terminator pointer.The Middle.
Note that the object Ap (fl,yp) (in the derivation Ap (fl3'p) =~ a i .
.
.
ai+dp-1)can have a k-terminator where k is between 2 and MCL.
Therefore, from Observa-tion 2.1 it follows that the terminator-pointer can only be used to determine the (k+l )  stsymbol from the top.
Therefore, assuming that fl = fl"yl ?
.. "Yk-1, the terminator-pointerwill allow us to access fl~.
(Recall from the definition, ak-terminator of A (fl"yl .
.
.
"Yk-13'p)will have the form At (fl"Yt).
Thus the (k + 1) st symbol from the top in A (fl-yp) is thesame as the symbol below the top of the stack of the terminator.)
Thus, we will needto record the string "yl - ' '  "Yk-1 in the encoding of Ap (fl'q/1 .
.
.
3'k-1~'p) as well.
This partof the entry will be called the middle.To summarize, the entry stored in P \[i, dp\] (where f l"yl.
.
.
"Yk-l"Yp is assumed to besufficiently long that we know A m (fl'71-.. 7k-l"Yp) is guaranteed to have a termina-tor) will have a head, (Ap,-yp); and a tail comprised of a middle, "Yl..-'Yk-1; and aterminator-pointer.
Note that the length of the middle must be at least one, but atmost MCL - 1, since from Observation 2.1, we know 2 < k < MCL.
We will call anentry of this kind a terminator-type entry.We will now discuss what we need to store in order to point to the termina-tor.
Suppose we would like to record in P\[i,d\] the derivation of ai.
.
.ai+d-1 fromA (fl'Yl... 7k-l"Y) as shown below.
We assume that At (fl'yt) is the terminator in thisderivation.a (fl"/1...'Yk-l"Y) ai.
.. at_lAt (fl')'t) at+dt .
.
.
ai+d-1ai ?
?
?
at- l  at ?
?
?
a t+dt - l  at q-dt ?
?
?
ai+d-1~- ai ?
?.
ai+d-1From Observation 2.3, it follows that it would be sufficient o use  ((at~ "Ytl~ \[t~ dt\]) asthe terminator-pointer.
This is because any entry with the head (At~ ")'tl in P It, dt\] willrepresent in general a derivation At (fl"Yt) ~ at .
.
.
at+dr-1.
This not only matches theabove case, but even if fl' ~ fl, from the Observation 2.1, we haveA (fl'"/1... "Yk-lq/) ~ a i .
.
.
at - lAt  (fl'"/t) at+dr.., aiq-d--1 ~ ai .
.
.
ai+d-1.Thus, the use of the head information (plus the two indices) in the terminator-pointercaptures the essence of Observation 2.3.
It is this structure-sharing that allows us toachieve polynomial bounds for space and time.
Note that the string derived fromthe terminator, at.
.
.at+dr- i ,  is a substring of ai.
.
.ai+d-1.
In such a case, i.e., wheni G t and i+  t >>_ t + dr, we will say that /t, dt/ <_ {i~dl.
We define {t, dt/ < {i, dl if{t, dtl <_ {i, dl and {t, dtl # {i, dl.
Since any terminator-type entry in P\[i,d\] can onlyhave terminator-pointers of the form ({At ,  "Ytl ~ {t, dtl ) where It, dtl <_ {i, dl, the numberof terminator-type entries in P \[i, d\] is O(d2).Definition 3.1Given a grammar, G, define MSL(G) (Maximum Secondary constituent's stack Length)as MSL(G) = max { len (as) I As (e~s) is the secondary constituent of a production }Henceforth we will use MSL rather than MSL(G).600K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsWe now consider the question of when a terminator-type entry is appropriate.
Ofcourse, if A (~) ~ a i .
.
.
ai+d-1 we could store such an entry in P Ii, dpl only when A (c~)has a terminator in this derivation.
From Observation 2.4 we know that if len (c~) >MTL then there exists a terminator of A (~) in this derivation.
However, it is possiblethat for some grammar  MSL > MTL.
Therefore even when len (c~) > MTL (i.e., theobject has a terminator) A (~) can still match the secondary category of a rule if len (c~) GMSL.
In order to verify that an object matches the secondary category of a rule weneed to consider the entire stack in the object.
When A (~) ~ ai .
.
.
ai+d-1 and lengthof ~ does not exceed MSL, it would be convenient o store A as well as the entirestack c~ because such an object can potentially match a secondary category of a rule.To be certain that such an object is stored in its entirety when len (~) < MSL, theterminator-type entry can only be used when len (c~) > max(MSL~ MTL).
However,we prefer to use the terminator-type entry for representing a derivation from A (~)only when its terminator, say At (fl), is such that len (fl) >_ max(MSL~ MTL) rather thanwhen len (c~) > max(MSL~ MTL).
Again, we point out that this choice is made only forconvenience and because we feel it leads to a simpler algorithm.
The alternate choicecould also be made, which would lead to a slightly different algorithm.Definition 3.2Define the constant TTC (Terminal-Type Case) as TTC = max(MSL MTL).
In a deriva-tion A (fl71 ... 7k) ~ W we will say that A (flY1 -.. Vk) has the TC-property iff it has ak-terminator, say At (flTt), such that len (flVt) _> TTC.If A (fl31 .
.
.
3k) ~ ai.
.
.
ai+d_l, where A (fl31 .-.
3k) does not have the TC-property thenwe record the object in its entirety in P Ii~ d\].
In order for such an entry to have thesame format as the terminator-type entry, we say that the entry has a head /A~ 3k); atail with a middle 31.-.
7k-1 and a nil terminator-pointer.
Note that in this case themiddle can be an empty string; for instance, when we encode A (V) ~ ai.. ?
ai+d-1.
Ingeneral, if c~ = f13 then we say top (~) = 3 and rest (c~) = ft.
If o~ = ?
then we say thattop (c~) = rest (c~) ~- ~.To summarize, the structure of an entry in P Ii, d I is described by the followingrules.?
An entry consists of a head and a tail.?
A head consists of a nonterminal and a stack symbol.?
A tail consists of a middle and a terminator-pointer.
The exact nature ofthe middle and the terminator-pointer are as given below.- -  The terminator-pointer may be of the form (IAt~ 7tl~ \[t~dtl)where At E VN~ 3t E W I and It~ dtl <_ li~ d).
In this case, the middleis a string of stack symbols of length at least one.
This form of aterminator pointer is used in the encoding of a derivation froman object if its terminator has a stack length greater than orequal to TTC.
Recall that we had called this type of an entry aterminator-type ntry.A terminator-pointer can be a nil.
Then the middle is a (possiblyempty) string of stack symbols.
However, the length of themiddle is less than TTC + MCL - 1.
This form of a terminatorpointer is used in the encoding of a derivation from an object ifit does not satisfy the TC-property; i.e., either it has no601Computational Linguistics Volume 19, Number 4terminator or if the terminator exists then its stack length is lessthan TTC.3.2 Recognition Algorithms for LIGSince the full algorithm involves a number of cases, we develop it in stages by restrict-ing the forms of productions.
The first algorithm that considers the most restrictedform of productions introduces much of what lies at the core of our approach.
Nextwe relax these restrictions to some degree.
After giving the algorithm at this stage,we switch to discuss how this algorithm can be adapted to yield one for CCG.
Later,in Section 5, we consider further relaxation of the restrictions on the form of LIGproductions, which can help us produce an algorithm for TAG.Regardless of which set of restrictions we consider, in every algorithm we shallestablish that the following proposition holds.Proposition 3.1?
((at ~k) (')'1.-.
"Yk-1, ((at,,,/t), \[t, dt\]))) E P\[i,d\] if and only if for somefl c v~,a (fl~'l... "Yk-l"/k) ~ ai... at- la (fl"/t) at+dt-1...ai+d-1ai.
.
.ai+a-1where At (fl"/t) is the k-terminator of A (fl"/1 ' ' '  "Yk) and len (fl'Yt) >_ TTC.?
((A,'yk) (3'1...Tk-1,nil)) E P\[i,d\] i fand  only ifa ( ' ) '1 .
.
.
q/k-lq/k) ~ ai...ai+d-1where in this derivation A ("/1 .
.
.
"Yk-l'Yk) does not have the TC-property.3.2.1 Algorithm 1.
Recall that the general form of rules that are to be considered areas follows.1.
A (c~) --* c where e ?
{e} U VT, and len (c~) > 1.2.
A(..'y~.
. '
rm)~ Ap(..'~p)As(~s)3. a(..~l..."ym)----~ as(ozs)ap (.."fp).4. a ( "~l .
.
.3 'm)~Ap("3 'p ) .At this stage we assume that the following restrictions hold of the above rules.In the first type of production we assume that e c VT and len (c~) > 1.Thus MTL > 1.len (C~s) _> 1 in productions of type 2 and type 3, i.e., MSL > 1.There are no productions of type 4.We will now give the following rules that specify how entries get added in theparsing array.
The control structure of the algorithm (a CKY-style dynamic program-ming structure) will be added later.
We assume that the input given is al ... an, wheren>l .602K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsInit ial izat ion PhaseIn the initialization phase of the algorithm we store lexical objects (objects deriving aterminal symbol in one step) entirely in a single entry.
In other words,Rule 1.LA (c~) ---~ a a = ai l< i<n( {A, top (c~)} (rest (ee), nil)) ?
P\[ i, 1\]Inductive phaseHere productions of type 2 and type 3 will be considered.
Let us assume the presenceof the following production in the grammar: A (.. "Yl .
.
.  "
/m) --+ Ap (.. %) As (o~s).
5Suppose that while considering which entries are to be included in P \[i, d\] we findthe following for some dp, ds such that dp + ds = d.?
The entry ((Ap,,ypl ( f lp,tpp))E P\[i, dp\].
This is consistent with the rule'sprimary constituent.
Regardless of whether tpp = nil or not, for somefl E V~: Ap (flflp'yp) ~ ai...ai+dp-1.
That is, when tpp = nil we have?
The entry ((As, top (o~s)) (rest (c~s), nil)) E P \[i + dp, ds\].
This is consistentwith the rule's secondary object.
Thus if d = dp + ds we may assumeAs (Ols) ~ ai+dp .
.
.
ai+d_l.From the presence of the two entries pecified above (and the derivations they rep-resent) we have A (flflp'Yl ... 7m) ~ Ap (flflp'yp) As (c~s) ~ ai.. .
ai+d-1.
This derivationmust be recorded with an entry in P \[i, d\].
The content of the entry depends on sev-eral factors: the value of m; whether or not the terminator-pointer in the entry for theprimary constituent (i.e., tpp) is nil; and the length of the middle in this entry (i.e., tip).These determine whether or not the new entry will be a terminator-type entry.
Wehave cases for m = 0, m = 1 and m _> 2.CASE WHEN m = 0The new object to be stored is A (flflp).
The top of the stack in this object can beobtained from the stack associated with the primary constituent.
How this is donedepends on whether the entry encoding the primary constituent is of terminator typeor not.When m = 0 and tpp = nilThis means that the primary constituent has been represented in its entirety; i.e., theprimary constituent is Ap (flpTp).
Since tpp = nil the primary constituent does not satisfythe TC-property (i.e., it does not have a terminator with a stack of length greater thanor equal to TTC), the new constituent too cannot be encoded using a terminator-typeentry.
Therefore,Rule 2.ps.L(lAp, 3'pl (tip, nil)) ?
P\[i, dp\] ((As, top(c~s)) (rest(o~s),nil) ) C P\[i +dp, d -dp\]( IA, t?P (flp) l (rest (flp),nil) ) E P\[i,d\]5 Similar arguments can be used when we consider the production: A (.. 3'1 ... 7m) --* As (C~s) Ap (" 3'p).603Computational Linguistics Volume 19, Number 4The following rule is the counterpart of Rule2.ps.L 6 that corresponds to the use of theproduction A (..) ~ As (C~s) Ap (.. 7p).Rule 2.sp.L( (As, top(c~s)) (rest(c~s),nil) ) ?
P\[i, ds\] ((Ap, Vp ) (tip,nil)) ?
P\[i + ds,d-  ds\]((A, top (tip)) (rest (tip),nil) ) ?
P\[i,d\]When m = 0 and  tpp ~ nilLet the entry for the primary constituent be ((Ap, 7p) (tip, ((At, 7t), It, dr\]))).
Since theprimary constituent is Ap (flflpTp) we will assume that its terminator is At (fl'Yt) wherelen (flVt) > TTC.
Note also that len (flp'yp) > 2.
The entry for the new object (A (flflp))is determined based on whether len (tip) = 1 or len (tip) > 1.
In the latter case thelen (flpVp)-terminator f the primary constituent is the len (&)-terminator of the newobject.
This is not so in the former case, as noted in Observation 2.5.Considering the latter case first, i.e., len (tip) > 1, we may write tip as 71...'Yk-lVkwhere k > 2.
Since in this case the new object and the primary constituent have thesame terminator and since the primary constituent has the TC-property (tpp ~ nil),the new object must also be encoded with a terminator-type entry.
Thus we have thefollowing rule:Rule 3.ps.L((Ap,~/p)('y1...3%tpp)) ?P\[i ,  dp\]tpp = ((At,vt), \[t, dt\]) ,k >_ 2 ((As, top (c~s)) (rest (c~s), nil)) ?
P \[i + dp, d - dp\]( (A,"Ykl  ( "Y1 .
.
.Tk - l , tpp) )  cP \ [ i ,d  1Henceforth we shall give the ps versions of the rules only and omit sp versions.Now let us consider the case when len (tip) = 1.
Rewriting tip as 71, the entriesrepresent derivation for fl E V~ (len (fl'yl) = len (fl"/t) > TTC).a (ti"/1) ~ ap (ti"/l"/p) As (0@)ai .
.
.
at- l At (tiTt ) at+a,.., ai+ap- l As (C~s)ai .
.
.
at_  lat .
.
.
at+d t_ lat+dt ?
.
.
ai+dp_ laiq-d p .
.
.
a i+d_lwhere At (ti'Yt) is the 2-terminator of Ap (ti"/l"Yp)- From Observation 2.5 it follows thatif At(tiVt) has a terminator then the terminator of A(ti'yl) in this derivation is thesame as the terminator of At (fl'Yt); and if At (fl'Yt) has no terminator then neither doesA (ti'Yl).
Additionally, in this derivation A (ti'yl) satisfies the TC-property if and onlyif At (ti'Yt) has the TC-property.
That is, we should use a terminator-type entry torecord this derivation from A (ti'Yl) if and only if a terminator-type entry has beenused for At (tiTt).
Since these two objects share the same terminator (if it exists) theterminator-pointer must be the same when we record derivations from them.
There-fore, suppose we use the terminator-pointer of ((Ap, ?)
(tip, ((At, "Yt), \[t, dt\]))) to lo-cate an entry ((At, "Yt) (tit, tPt)) ?
P It, dr\].
This would suggest he addition of the entry6 Here L indicates a rule we use in LIG parsing; ps indicates that the primary constituent appears beforethe secondary constituent.
Similarly, sp will be used to indicate that the secondary constituent appearsbefore the primary constituent.604K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms(IA, "/1)(tit, tpt) ) to P\[i,d\], regardless of whether or not tPt = nil.
However, we givethe two cases (tpt = nil or tPt = (IAr, %1, \[r, dr\]) for some At, %, r, dr) in the form oftwo different rules.
This is because (as we shall see later) these two rules will have toappear in different points of the control-structure of the parsing algorithm.Rule 4.ps.L((Ap, 7p) (71, ((At, 7t), \[t, dt\]) )cP\[i, dp\]((As, top (c~s) ) (rest (c~s), nil))?
P\[i+dp,d-dp\]( (At, Tt) (fit,nil))E P\[t, dt\]( (A, 71) (fit,nil)) E P\[i,d\]Rule 5.ps.L( (Ap, vp) (71, ( (At, 7t) , \[t, dt\] ) ) )?
P\[i,d~,\]((As, top (c~s) ) (rest (C~s), nil))?
P\[i+dp,d-dp\]((At, "~/t) (fit, tPt) )P\[t, at\]tpt = ((Ar,'Yr), Jr, dr\])((A, 7"/1) (flt,tpt) E P\[i,d\]CASE WHEN m -- 1The length of the stack in the new object is equal to that of the primary object.
Infact, the terminator of the primary object (if it exists) is the same as the terminator ofthe new object, and when the primary object has no terminator neither does the newobject.
Therefore the encoding of the new object can easily be derived from that of theprimary object by simply modifying the head (to change the top of the stack symbol).Thus we have:Rule 6.ps.L((Ap,'Tp) (tip, nil)) ?
P\[i, dp\] ((As, top(c~s)) (rest(c~s),nil)) c P\ [ i+dp,d-dp\ ]((A,")'I) (tip, nil)) ?
P\[i,d\]Rule 7.ps.L( (Ap, 7p) (tip, ((At, 7t) , \[t, dt\] ) ) ) ?
P \[i, dp\] ( ( A~ ,top ( c~s ) ) (rest(as), nil)) E P \[ i + dp , d - dp \]CASE WHEN m > 2If the primary constituent is Ap (titip,,/p) then the new constituent is A (tiflp'Yl... "/m).
Infact, in this case, we have the primary constituent being the m-terminator ofA (fltip3'l... "Ym).
Of course, this does not mean that the derivation from the new objectshould be recorded with the use of a terminator-type entry.
We use the terminator-typeentry only when len (tip3'p) ~ TTC.
In order to determine the length of this stack wehave to use the entry for the primary constituent (i.e., (IAp,.Tp)(tip, tpp) lE  PIi, dp\])and consider whether this is a terminator-type entry or not (i.e., whether tpp = nil ornot).605Computational Linguistics Volume 19, Number 4When m _> 2 and tpp ~ nilTherefore the length of the stack of the terminator of the primary constituent is greaterthan or equal to TTC.
This means that stack length of the primary constituent ( heterminator of the new object) exceeds TTC.
Thus we have the following rule:Rule 8.ps.L({Ap,'Tp) (tip, tpp) ) ?
P\[i, dp\]tpp = (Iat,'~t), It, dr\]) ({As, top (C~s)) (rest (as), nil)) E P\[i +dp, d - dpl(/A, ~m) (")/1.-.
"/m-l, ( {Ap, "yp) , \[i, dp\]) ) c P\[i,d\]When m _> 2 and tpp = nilThe primary constituent (which is the terminator of the new object) should be repre-sented in its entirety.
Therefore, in order to determine whether we have to encode thenew object with a terminator-type entry or not, we have to look at the entry for theprimary constituent.
Thus we obtain the following rules:Rule 9.ps.Llen (tip'yp) < TTC( IAp, 3,pl (tip, nil)) E P\[i, dp\] ((As, top (as)) (rest (as), nil)) E P \[i + dp, d - dp\]( {A, ~m) (tipVl .
.
.
Tm-l,nil) ) E P\[i,d\]Rule 10.ps.Llen (tip-yp) > TTC( (Ap, 7p) (tip, nil)) c P\[i, dp\] ( IAs, top (as)) (rest (~s), nil)) E P \[i + dp, d - dp\]((A, "Ym) ('Yl... "/m-l, (lAp, "/p) , \[i, dp\] ) ) ) C P\[i,d\]In the discussions that follow, we find it convenient torefer to the entries mentioned inthe above rules as either antecedent entries (or entries that appear in the antecedent) ofa rule or consequent entry (or entry that appears in the consequent) of a rule.
For ex-ample, (lAp, q/pl (tip, nil)) in PIi, dpl and ( IAs,top(e~s)l (rest (c~s),nil) in PIi + dp,d - dplare the antecedent entries of Rule 10.ps.L and ( IA, "Yml ('Y'"" "Ym--l~ ( I ap~ "~p I ' \[i, dp\] ) ) )that is added to P \[i, d I is the entry in the consequent of Rule 10.ps.L.3.3 The Control  StructureWe will start by giving a simple control structure for the recognition algorithm thatfollows the dynamic programming style used in the CKY algorithm.606K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsIn this section we modify the notation for entries lightly.
In the above discussion,the terminator-pointer of a terminator-type entry contains a pair of indices repre-senting input positions.
Thus, in effect, P is a four-dimensional rray.
As an alter-native to saying that (( A, 3,) (fl, (CA', 3"), \[t,d;\]))) is in P\[i,d\] we will sometimes say(C A, -y)(fl, (A', 3/))) is in P\[i, d\] \[t, dt\].
Also  as an alternative to saying (C A, oe)(fl, nil))is in P\[i,d\] we will sometimes ay ((A, c,) (fl, nil)) is in P\[i,d\]\[O,O\].
Thus P can beconsidered to be an array of size n x n x (n + 1) x (n + 1).In the specification of the algorithm (Figure 5) we will not restate all the rules wediscussed in the previous section.
Instead we will only indicate where in the controlstructure ach rule fits.
As an example, when we state "Use Rule 2.ps.L with dp = d"within the i, d, and d' loops we mean the following: for current values of i, d, and d' (andhence dp, ds) consider every production of the form A (.. "Yl .. .
7m) --+ Ap (.. 7p) As (as)with m = 0.
For each such production, look for entries of the form ((Ap, 7p) (tip, nil)) EP \[i, dp\] \[0,0\] for some tip and ( (As, top (as) ) (rest (as), nil) ) E P \[i + dp, d - dp\] \[0,0\].
In theevent we find such entries, we add ((A, top (tip)) (rest (flp),nil)) to P\[i,d\] \[0,0\] if it isnot already there.Since the entries in P\[i, d\] have the form ( (A, -,/) (fl, (CAt, 3q) , It, dt\] ) ) ) (where (t, dt) G(i,d)) or the form (CA,')')(fl, nil)), there are O(d 2) many entries in P\[i,d\] (where1 G i < n and 1 G d G n - d).
Thus space complexity of this algorithm is O(nd).Note that within the body within the r loop will be attempted for all possible valuesof i, d, d', t, dt, r, dr.
Since the range of each loop is O(n), the time complexity is O(n7).The asymptotic complexity of the above algorithm can be improved to O(n 6) witha simple rearrangement of the control structure.
The key point here is that the stepsinvolving the use of rules 5.ps.L and 5.sp.L can be split into two parts each.
Consider,for example, the use of the Rule 5.ps.L, which is repeated below.Rule 5.ps.L((Ap,',/p) (~I, ((At, q:t) , \[t, dt\]) ) )EP\[i, dp\]((A,, top ( o~, )) (rest (c~), nil))GP\[i+dp,d-dp\]((At, ")'t) (fit, tpt) )tpt = ((Ar,")'r), \[r, dr\])((A, q'l) (fit, ((Ar,'Yr), \[r, dr\]))) E P\[i,d\]This rule corresponds to the use of the production A (..) ~ Ap (.. q/p) As (as).
The valuesof i, d, d', t, d t are necessary to determine the span of the substrings derived from theprimary constituent and the secondary constituent, and the values of i, d, t, dt, r, dr areneeded to locate the entry for the terminator, i.e., (CAt,,),t)(fit, (CAr,"/r), \[r, dr\]))) andto place the new entry in the appropriate parsing table element.
That is, the values ofr and dr are not required for the first part and the value of d' need not be known forthe second part.
This indicates that the second part need not be done within the loopfor dq Therefore, we can modify the control structure in the following way.
Withinthe t loop (which appears within the loops for d, i, d',dt) we find the entries for theprimary and secondary constituents.
Having found the two relevant entries, we mustrecord the head of the new entry (A, tip) and the terminator-pointer of the primaryconstituent, i.e., (CAt, ~t), \[t, dt\]).
We can do this by using a two-dimensional rraycalled TEMP where we store CA, q'l, At, 7t).
Outside the d' loop (and hence outside theloops for t and dt as well), but within the loops for i and d, we can have the loopsthat vary t, dt, r, dr (note (r, dr) < (t, dr)) in order to locate the entry for the terminatorby using the information recorded in TEMP.
Finally, having found the entry for the607Computational Linguistics Volume 19, Number 4Algorithm 1beginfor i:= 2 to n doInitialization phaseUse Rule 1for d := 2 to n do % d loopfo r i := l ton-d+ldo%i loopbeginfo rd ' := l tod- ldo  %d' loopbeginUse Rule 2.ps.L, 6.ps.L, 9.ps.L, 10.ps.L with dp = d'.for dt := (d' - 1) to 1 do % dt loopfor t := i to (i + d' - dt) do % t' loopbeginUse Rule 3.ps.L, 4.ps.L, 7.ps.L, 8.ps.L with dp = d'for dr :-- dt to 1 dofor r := t to t + dt - dr dobeginUse Rule 5.ps.L with dp = d'end% end of dr loop% end of r loopend% end of t loop% end of dt loopfo r  dt :=  (d  - d '  - 1) to 1 do % dt loopfor t := (i + d') to (i + d - dr) do % t' loopbeginUse Rule 3.ps.L, 4.ps.L, 7.ps.L, 8.ps.L with ds = d'for dr := dt - 1 to 1 dofor r := t to (t + dt - dr) dobeginUse Rule 5.sp.L with ds = d'end% end of r loop% end of dr loopend% end of t loop% end of dt loopend% end of d' loopend% end of i loop% end of d loopFigure 5Algorithm 1.608K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalismsterminator we then store the resulting entry in P \[i, d\].
These steps are captured by thefollowing rules.
For a specific value of (i, d) we haveRule 5.i.ps.L( (Ap, q/p) (71, ( (At, q/t) , It, dr\])))C P\[i, dp\]((As, top (C~s)) (rest (C~s), nil))e\[ i  + clp, cl - G\](A, q/1,At, 7t) E TEMP\[t, dt\]Rule 5.ii.ps.L(a~"/l~.,z~t,q/t) E TEMP\[t, dt\] ((At, q/t) (fit, ((Ar, q/r), \[r, dr\]))) C P\[t, dt\]((A,q/1) (fit, ((Ar, q/r), \[r, dr\]))) C P\[i,d\]Similarly, we assume we have the pair Rule 5.i.sp.L and Rule 5.ii.sp.L correspondingto Rule 5.sp.L.
This leads to the algorithm given in Figure 6.
In this algorithm we dropthe sp rules and specify the ps rules only for the sake of simplicity.The correctness of Algorithm 2 can be established from the correctness of Algo-rithm 1 (which is established in Appendix A) and the following Lemma.Lemma 3.1Given a grammar G and an input al.
.
.an an entry ((A, q/} (fl, tp)) is added to P\[i,d\]by Algorithm 1 if and only if ((A,q/) (fl, tp)) is added to P\[i,d\] by Algorithm 2.Outline of Proof: Using induction on d. The base case corresponding to d = 1 in-volves only the initialization step, which is the same in the two algorithms.
Theonly difference between the two algorithms (apart from the control structure) is theuse of Rule 5.ps.L (and Rule 5.sp.L) by Algorithm 1 versus the use of Rule 5.i.ps.Land Rule 5.ii.ps.L (Rule 5.i.sp.L and Rule 5.i.sp.L) in Algorithm 2.
Rule 5.ps.L isused to add entries of the form ((A, ~Yl)(fit, ((ar~ q/r)~ Jr, dr\]))).
We can establish that((A, ,`/1) (fit, ((Ar, q/r), \[r, dr\]))) is added to P \[i, d\] due to the application of Rule 5.ps.Lif and only if there exist entries of the form ((Ap, q/p) ('71, ((At, q/t), \[t, dt\]))) in P\[i, dp\];((As, top(c~s))(rest(c~s),nil)) in P\[i+dp,d-dp\]; ((at, q/t)(flt~((ar, q/r)~\[Y~dr\])) ) inP It, dt\]; and the production A (..) --* Ap (.. q/p) As (~s).
Using induction, we can estab-lish that these entries exist if and only if (A, q/1,At~ q/t) is added to TEMP\[t, dt\] usingRule 5.ps.i.L (or Rule 5.sp.i.L) and ((A, q/1) (fit, ((Ar, q/r), Jr, dr\]))) is added to P\[i,d\]using Rule 5.ii.ps.L.4.
Combinatory Categorial GrammarsCombinatory Categorial Grammars (CCG) (Steedman 1985, 1986) are extensions ofClassical Categorial Grammars in which both function composition and function ap-plication are allowed.
In addition, forward and backward slashes are used to placeconditions concerning the relative ordering of adjacent categories that are to be com-bined.Definition 4.1The set of categories generated from a set, VN, of atomic categories i defined as thesmallest set such that all members of VN are categories, and if cl, c2 are categories thenso are (Cl/C2) and (el\e2).609Computational Linguistics Volume 19, Number 4Algorithm 2beginfor i:= 1 to n doInitialization phaseUse Rule 1fo rd :=2tondo%dloopfor i := 1 to n -  d + 1 do % i loopbeginInitialize TEMP It, dt\] to ~ for all (t~ dt) ~ (i~ d)fo rdp := l tod- ldo  %dploopUse Rule 2.ps.L, 6.ps.L, 9.ps.L, 10.ps.Lfor dt := dp - 1 to 1 do % dt loopfor t := i to i + dp - dt do % t loopUse Rule 3.ps.L, 4.ps.L, 5.i.ps.L, 7.ps.L, 8.ps.L% end of t loop% end of dt loop% end of dp loopfor dt := d - 1 to 1 do % dt loopfor t := i to i + d - dt do % t loopfor dr := dt - 1 to 1 dofor r := t to t + dt - dr dobeginUse Rule 5.ii.ps.Lend% end of r loop% end of dr loop% end of dt loop% end of t loopend% end of i loop% end of d loopFigure 6Algorithm 2.Definition 4.2A CCG, G, is denoted by (VT, VN, S~f~ R) whereVT is a finite set of terminals (lexical items),VN is a finite set of nonterminals  (atomic categories),S is a dist inguished member  of VN,f is a function that maps  each element of VT to a finite set of categories,R is a finite set of combinatory  rules, where combinatory  rules have the fol lowingform.1.
A forward rule has the fol lowing form where m > 0.
(x/y) (yllZl\[2... ImZm) ---4.
(XllZll2... \[mZm)2.
A backward rule has the fol lowing form where m > 0.(y11Zl12...
ImZm) (x\y)  ---+ (XIIZl12... \]mZm)610K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsHere x,y, z l , .
.
.
,Zm are meta-variables and h , .
.
- ,  \[m E {\, /}.
For m -- 0 these rulescorrespond to function application and for m > 0 to function composition.
Note thatthe set R contains a finite subset of these possible forward and backward rules; i.e.,for a given CCG only some of the combinatory rules will be available.Definition 4.3In the forward and backward rules given above, we say that (x/y) (resp.
(x\y)) is theprimary constituent of the forward (resp.
backward) rules and (y\]1zl\[2... \]mZm) is thesecondary constituent of the rule.
The notion of a distinguished child is defined as inthe case of LIG, i.e., a category is the distinguished child of its parent if it correspondsto the primary constituent of the rule used.
As before, the distinguished escendantis the reflexive, transitive closure of the distinguished child relation.In discussing CCG we use the notational conventions that the variables \] andc (when used with or without primes and subscripts) range over the forward andbackward slashes and categories, respectively.
We use x,y, z for meta-variables; a, flfor strings of directional categories (i.e., a string of the form \]1Cl\]2.,.
\]nOn from somen ~ 0); and A, B, C for atomic categories (i.e., members of VN).Derivations in a CCG, G = (VT, VN~ S,f, R), involve the use of the combinatoryrules in R. Let ~ be defined as follows, where T1 and T2 are strings of categoriesGand terminal symbols.If ClC 2 ---+ C is an instance of a rule in R, then TlCT 2 ~ "~1ClC2T2 .GIf c C f(a) for some a c VT and c is a category, then TlCT2 ~ TlaT2.The string languages generated by a CCG, G, L(G) = { w \] S ~ w \] w E V~ }.GExample 4.1The following CCG generates { wcw \] w E {a, b} + }.
Let G = ({at b, c}, {S, T, A, B}, S,f, R)wheref(a) = (A, T \A/T ,  T\A} f(b) = {B, T\B/T,  T\B} f(c) = (S/T}The set of rules R includes the following three rules.y (x\y) ~ x (x/y) (y\zl/z2) ---+ (y\zl/z2) (x/y) (y\zl) ~ (y\zl)In each of these rules, the target of the category matched with x must be S. 7 Figure 7shows a derivation of the string abbcabb.We find it convenient to represent categories in a minimally parenthesized form(i.e., without parentheses unless they are needed to override the left associativity ofthe slashes), where minimally parenthesized form is defined as follows.7 Following Steedman (1985), we allow certain very limited restrictions on the substitutions of variablesin the combinatory ules.
A discussion on the use of such restrictions is given in Vijay-Shanker andWeir (in press).
However, we have not included this in the formal definition since it does not have asignificant impact on the algorithm presented.611Computational Linguistics Volume 19, Number 4A S~AS~A~BBbS~AkBkBS~B/TS'~JFSIT T'OdrI IacT~B/FIbT~BIbFigure 7CCG example derivation tree.Definit ion 4.4?
A is the minimally parenthesized form of A where A C VN.?
If c l , .
.
.
,c ,  are the minimally parenthesized forms of categories c~,..., c"respectively, then (al lcl l2.. .
Incn) is the minimally parenthesized form of(('-' (allc~)12'' ")l~c').A category c is in minimally parenthesized form if c is the minimally parenthesizedform of itself.Definit ion 4.5Let a category c = A l lC112.
.
.
\[nCn be in minimally parenthesized such that n > 0,A E VN, and Cl,..., Cn are minimally parenthesized categories.?
The target category of c = allClI2... InCn denoted by tar(c) is A.?
The arity of c = AllC112... InCn, denoted as arity (c), is n.?
The argument categories of c = AllC 112...
Inch denoted byargs (c) = { ci \] 1 < i < n }.4.1 CCG and LIGBefore showing how the general parsing scheme illustrated by the LIG recognitionalgorithm can be instantiated as a recognition algorithm for CCG, we show that CCGand LIG are very closely related.
The details of the examination of the relationshipbetween CCG and LIG may be found in Weir and Joshi (1988) and Weir (1988).A minimally parenthesized category (AIlc 112... InCn) can be viewed as the atomiccategory, A, associated with a stack of directional argument categories, ILC112... Inc,.The rule(x /y)  (y l lZ l l2 .
.
.
ImZm) ~ (X I1Z l l2 .
.
.
ImZm)!
!
!
!
has as  an  instance (ApieCe... InCh~As) (AsliCl}2...  ImCm) --~ (apieCe...  InCnllCll2... ImCm)I I I I I I as we l l  as (A~L~c~... InCn/(Asl c )) (AI'c'L~c~I2... I,~cm) ~ (&l~c'~... I~Cnh~lL2... Imam)612K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms!
!
as an instance.
Thus x matches the category (Apl~c ~ .
.
.
\[nCn), y matches an atomic cat-egory As in the first example and a nonatomic ategory (As\]~C ') in the second, andeach zi matches ci for 1 ( i ( m. A derivation involving the second instance (viewedbottom-up) can be seen as popping the top directional argument /(Asl'c') from theprimary category and pushing the m directional arguments IlCl\]2.-.
ImCm ?
Thus, eachinstance of the combinatory rule appears to closely resemble a LIG production.
Forexample, in case of the second instance we haveap (.. ILC112... ImCm) ---+ ap ( .
.
/ (As\] 'c '))  As (\['c'11c112... ImCm) .We now show that, like the set of stack symbols of a LIG, the set of directional argu-ment categories that we need to be concerned with is finite.Definition 4.6Let c be a useful category with respect o a grammar G if and only if c ~ w forsome w E V~.
The set of argument categories, args (G) of a CCG, G = (VT~ VN~ S,f, R),is defined as args (G) = Uc~f(a) args (c).Observation 4.1If c is a useful category then args (c) c args (G), a finite set determined by the gram-mar, G.This observation can be shown by an induction on the length of the derivationof some string from c. The base case corresponds to a lexical assignment and hencetrivially args (c) C args (G).
The inductive step corresponds to the use of a combinationusing a rule of the form(x /y)  (yllZll2... ImZm) ---+ (XIlZll2... ImZm )or(yllZll2...\]mZm) (x \y)  ~ (XIlZll2...ImZm)By inductive hypothesis, any useful category matching either (x/y)} (x\y) or(y\]lZll2... ImZm) must take its arguments from args (G) (a finite set) and therefore theresulting useful category also shares this property.The above property makes it possible to adapt the LIG algorithm for CCG.
Notethat in the CKY-style CCG recognition we only need to record the derivations fromuseful categories.
From Observation 4.1 it follows that the lexical category assignment,f, determines the number of "stack" symbols we need to be concerned with.
Therefore,only one of the variables (x) in a combinatory rule is essential in the sense that thenumber of categories that it can usefully match is not bound by the grammar.
There-fore, it would be possible to map each combinatory rule to an equivalent finite set ofinstances in which ground categories (from args (G)) were substituted for all variablesother than x; i.e., y, zl}...Zm in the combinatory rule above.
This would result in agrammar that was a slight notational variant of a LIG where the CCG variable x andthe LIG notation .- perform similar roles.
However, for the purpose of constructing arecognition algorithm it is both unnecessary and undesirable to expand the number ofrules in this way.
We adapt the LIG algorithm so that it, in effect, constructs appropriateinstances of the combinatory rules as needed during the recognition process.613Computational Linguistics Volume 19, Number 44.2 Recognition of CCGThe first step in modifying the LIG algorithm is to define the constants MSL and MTLfor the case of CCG.
Let G -- (VT, VN, S,f,  R) be a CCG.
These definitions follow im-mediately from the similarities between CCG combinatory rules and LIG productions.Observation 4.2If we were to express a combinatory rule(x /y )  (yIlZl...ImZm) ~ (X\]IZ1...\]mZm)in terms of LIG productionA ("")'1..."/m) "-'+ Ap (""yp) As(o@)then we have the following correspondences:?
% w i th /y .?
"~i with = \]izi for 1 < i < m, i.e., "Y1 ' ' '  "Ym with IlZl .
.
.
\]mZm.?
A = Ap.?
As (as) with yhz~.
.
.
ImZm .Given such a direct correspondence b tween combinatory rules and LIG productions,we will define the following constants to be used in the the CCG algorithm withminimal explanation.?
MTL is the maximum arity of a lexical category.
Thus,MTL = max { arity (c) \] c c f(a), a C VT }.?
MSL should be the maximum arity of a useful category that can matchthe secondary category of a rule.
Note that a category matching(y\]lZ1\]2-.. ImZm) will have an arity that is the sum of m and the arity ofthe category matching y.
Furthermore, note that since y is an argumentof the primary category it must be bound to a member of args (G).
Thus,MSL = max { m \] (y\[lz112... \]mZm) } is the secondary category of a rule inR + max { arity (c) \] c E args (G) }.?
Note that in the case of CCG, MCL need not be defined independentlyof MSL.?
As before, we define TTC as TTC = max { MSL, MTL }.Since directional categories play the same role that stack symbols have in LIG, werevise the notions of length top ( ) and rest ( ) as follows.
We say that the string of direc-tional arguments categories \]lCl I2"'" InCh has a length n, i.e., len (IlCl 12.-.
\]nCn) = n. Notethat arity ((A\]1c112... InCn)) = len (\[1Cl\]2... \]nCn) = n. We define top ((IlCl\]2... InCn)) =\[nCn and rest ((\]1c112... \]nCn)) = \]1Cl \]2.-.
\]n--lCn--1 ?
Additionally, top (?)
= rest (~) = ?.4.2.1 Terminators in CCG.
We can define a k-terminator in essentially the same wayas in the case for LIG.
Note that a category shares its target category with all of itsdistinguished escendants.614K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsDefinit ion 4.7Suppose that we have the following derivation:af l l lC l  .
.
.
Ik_lCk_l lC ~ u a f l l lC l  .
.
.
\ ]k_lCk_l lkCk.
.
.
ImCm==~ U a f l /Cp  CpllCl .
.
.
IkCk.
.
.
ImCmldVWWWor similarlyaf l l lC l .
.
.
I k_ lCk - l lC  ~ U a f l l lC l .
.
.
I k_ lCk_ l l kCk .
.
.
ImC m Wu CpllCl...\[kCk...ImCm Aft\c, wUVWwhere the following conditions hold?
fl is a string of direction categories, i.e., fl E ({/~ \}args (G))*.?
k - l> l ,?
A f l l lC l .
.
.
Ik-lCk-llkCk... ImCm and Afl/c are distinguished escendents ofaf l l lC l  .
.
.
Ik_ lCk_l  \[C?
any distinguished escendent between Afl/Cp and AflllCl ... \]k-lCk-llC canbe expressed in the form afll~cl ... Ik-lCk-1 le~ where len (c~) >_ 1We say that Aft/@ is the len (11Cl .
.
.
Ik_lCk_llc)-terminator f Afl l lc l .
.
.
Ik_lCk_l\[C.Note that cp need not be atomic.
Hence if we write the secondary category ascpllcl ... ImCm we are not necessarily expressing it in minimal parenthesis form.4.2.2 Anatomy of a CCG Entry.
In the CCG algorithm we will use entries that have aform similar to that of the entries in the LIG algorithm.
The choices we make are basedon Observation 4.2.
For a derivation AllCl ... Ijcj ~ al ... ai+d-1 (where the input isal .
.
.
an), we will have an entry in P Ii~ d\] with a head IA~ IjCjl, where  A E VN, Ij C {\~/},and cj E args (G).First consider the case when a terminator-type entry is used.
The terminator-type entry is applicable when Aflllcl ...  Ik-lCk-11c has a k-terminator, say Af l l t c t  wherelen (flltCt) ~_ TTC.
As before we say that in such a case  Af l l lC l .
.
.
\]k_lCk_ 11c satisfies theTC-property.
Assuming the terminator derives the substring at.. .
at+d~-l, we can usethe terminator-pointer (llctl~ \[t~ dtl) and a middle I1c1... Ik_lCk_l .
Notice that since thetarget of the category Alfl\] lcl.
.
.
Ik-lCk-11C as well as the target of its terminator is Aand since A is already noted in the head, it is not recorded in the terminator-pointer.For entries that are not terminator-pointer, the entire category is noted in theentry.
Such an entry has the form (IA~ Ijcj~ (11cl... Ij-lCj-l, nil)) assuming that j > 1.However, it is possible that j = 0.
In this case the category being represented is A,and the entry will be written as (IA~ c I (~ nil)).
In general, we use the non-terminator-type entry for recording a derivation from As when it has no terminator or when theterminator, say af l l tCt  (rewriting c~ as flllCl... Ik-lCk-1 \[C) is such that len (flltCt) ~__ TTC;i.e., when the category As does not satisfy the TC-property.4.2.3 CCG Algorithm.
It is straightforward to derive the rules for the CCG recognitionalgorithm from those used in LIG algorithm.
Using Observation 4.2, we can now givethe rules for the CCG algorithm with no explanation.615Computational Linguistics Volume 19, Number 4Rule 1.CAc~cf(a) a=ai  l< i<nAssume the combinatory rule (x/y)When m = 0 and tpp = nilRule 2.ps.C((Ap,/Cp) (tip, nil)) C P\[i, dp\]((Ap, top (~)) (rest (c~), nil)) E P\[i, 1\](YllZl --.
ImZm) ---+ (XIlZl ... ImZm).Asc~s = Cp((As, top (C~s)) (rest (C~s), nil)) E P \[i + de, d - dp\]((Ap, top (tip)) (rest (tip),nil) ) E P\[i,d\]When m = 0 and  tpp ?
nilRule 3.ps.Ctp e = ((I,c,), \[t,d~\])k>2 Asc~s = Cp((As, top(as)) (rest(o~s),nil) ) C P\[i + de,d-de\]( (Ap, IkCk) (llCl... Ik_lCk_i, tpp) ) ~ P\[i,d\]Rule 4.ps .C((ap,/ep) (lie1, ((\]tct), \[t, dt\]) )E P\[i, dp\]Asc~s = Cp((As, top (cts)) (rest (C~s), nil) )~ P\[i+de, a -de\ ]( ( )) (A e, \]tct) tit, nil* P\[t, dt\]((Ap, I1c,) (tit,nil)) C P\[i,d\]Rule 5.ps.C((Ae,/ce) O~c,, ((Ltc,), \[t,d,\])))C P\[i, de\]Asc~s = Cp((At, top (C~s)) (rest (C~s), nil))cP\[ i+dp,d-de\]tpt-~ ((IrCr), \[r, dr\])( (Ae, ltct) (ti,, tp,) )((A e, Ilcl) (tir, tPt) ) C P\[i,d\]When m -- 1Rule 6.ps.C( (Ap, /Cp) (tip, nil)) C P\[i, dp\]Asc~s = CpllCl((As, top(c~s)) (rest(c~s),nil) ) C P\[i + dp,d-d,\]((A e, \]1c,) (tie,nil)) E P\[i,d\]616K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsRule 7.ps.C( (Ap, /Cp) (flp, ( (Itct) , \[t, dt\] )) ) c Pfi, dp\]As~s = Cp\[lCl((As,top(c~s)l (rest(c~s),nil)) C P\[i +dp~d-dp\]((Ap, llCl ) (tip, ((ItCt), \[t, dt\]) ) C P\[i,d\]When m > 2 and tpp ~ nilRule 8.ps.Ctpp = (l l tct l ,  \[t~dt\])(IAp, /Cp> (tip, tpp) ) E PIi, dp\]As~s = CpllCl .
.
.
ImCm( IA~, top (c~s) l (rest (c~s), nil)) E P \[i + dp, d - dp\]( (Ap, ImCm} (11Cl... Im--lCm--l~ ( (/Cp) , \[i, dp\] )) ) C P\[i,d\]When m > 2 and tpp = nilRule 9.ps.Clen (tip/@) < TTC((Ap,/Cp) (tip,nil)) E PIi, dp\]asoz s = CpllCl .
.
.
\]mCm( IAs, top (c~s) ) (rest (c~s), nil) ) E P \[i + dp, d - dp\](lap, Imem) (tip\]lCl .
.
.
Im-lCm--l,nil) E P\[i,d\]Rule 10.ps.Clen (tip/ep) > TTC((Ap,/Cp) (tip, nil)) E P\[i, dp\]aso @ = CpllCl .
.
.
\[mCm((As, top(c~s)) (rest(c~s),nil) ) c P\[i + dp,d -dp\]( (ap,  IrnCrn} (11Cl... Irn--lCm--l~ ((/Cp} , Ii, dp\]) ) ) C P\[ i ,d\]Proposition 4.1The CCG recognition algorithm can be seen to establish the following.?
(lAp, Ic~ (t3, (lltctl, \[t~ dt\]))) C P \[i~ d\] if and only if there is some c~ suchthat Ac~tilc ~ ai... ai+d-1 and the (len (/3) + 1)-terminator (Aozltct) ofAc~tilc derives the string at... at+dr-1 and len (O~ltCt) ~ TTC.?
((Ap, top(cO~, (rest(cO, nil,)) E P\[i,d\] if and only if Ac~ ~ ai...ai+d-1and either Ac~ has no terminator or its terminator, say Ac~ t is such thatlen (c~') < TTC.5.
TAG RecognitionWe begin this section by first considering how to extend our algorithm for LIG tohandle unary productions.
This will be needed to show we can instantiate our schemeto give a recognition algorithm for TAG.5.1 Handling Unary Productions and Epsilon ProductionsWe will now show how the LIG algorithm given earlier can be extended to considerunary productions of the form A (.. ~1.
.
.
"Ym) -'-9 Ap (.. 3'p) as well as e productions of617Computational Linguistics Volume 19, Number 4the form: A (c~) --+ e. However, we will now assume that m G 2 in productions of theform A (.. 2/1 ... q/m) --+ T1ap ('" q/p) T2.
Thus, henceforth MCL < 2.
Note that this refersto both unary and binary productions.
This additional restriction does not changethe generative power.
We have introduced these restrictions in order to reduce thenumber of cases we have to consider and also because we can restrict our attentionto the productions that are used in the TAG to LIG construction.Consider the processing of a binary production A (.. 2/1 .
.
.
2 /m) ---+ Ap (.. "yp) As (as).Since CKY-style parsers work bottom-up, we check to see if the primary and secondarycategories derive adjacent strings (say ai...ai+dp-1 and ai+dp...ai+dp+&, respectively)and then we store an encoding for the new object that results from the combination.The processing of unary productions i similar except hat we do not have to considera secondary constituent.
The rules that express the processing of such productions willbe very similar to those for the binary productions.
For example, consider Rule 2.ps.Lfor the binary production A (..) ---+ Ap (.. ",/p) As (C~s).Rule 2.ps.L((Ap,,yp) (tip, nil)) E P\[i, dp\] ((As,top(e~s)) (rest(e~s),nil)) E P \ [ i+dp,d -dp \ ]( ( A, top (tip)) (rest (tip), nil) ) E P \[ i, d\]Given a unary production A (..) --+ Ap (.. 2/p) we have the Rule 2.u.L (where u standsfor unary).Rule 2.u.L((Ap,',/p) (tip,nil)) c P\[i, dp\]((A, top (G) ) (rest (&),ni l))  E P\[i,d\]In addition, with the introduction of e productions, we have to consider derivationsof strings of length d -- 0.
We shall assume that if A (c~) ~ e then an encoding ofA (~) will be stored in P\[i, 0\] (for all i).
We must also consider the possibility that theprimary constituent or the secondary constituent derive the empty string, i.e., dp = 0or ds = 0.
Processing of such cases becomes imilar to that of unary productions.To indicate the additional processing required due to the introduction of unaryproductions and the possibility of the derivation of the empty string, let us considerRule 8.ps.L.
"Use Rule 8.ps.U' can be paraphrased as follows.If there exists a production A (.. 71 ... 2/m) -+ A m ('" 7p) As (C~s) wherem >__ 2, el = ((Ap,q'p)(flp,(at,'Tt))) belongs to P\[i,d\]\[t, dt\] and e2 =((As, top (C~s))(rest (c~s), nil)) belongs to P\[i +dp, d - dp\] \[0, 0\] then adde3 = ((A~'ym)(qrl...~/m-l,(Ap~2/p))) to P\[i,d\]\[i~dp\] if e3 is not alreadypresent in this array element.If we allow ~ productions it is possible that ds --- d -  dp -- 0.
Consider the case where wehave As (as) ~ c. That is, we expect he entry e2 to be present in P \[i + d, 0\] \[0, 0\].
Thismeans that the resulting entry e3 must be added to P \[i, d\] \[i, d\] since we now havedp = d. Note that the addition of e3 = ((A,"/m)("yl..."Yrn--l~ZZ~p~p~)) (that encodesthe derivation from A (fl'yl ... %) for some fl) can result in more entries being addedto the same array element P \[i, d\] \[i, d\] (for instance, when we have the productionB (--2/~ ... 2/~) --+ A (-.
"Ym)).
This is similar to the prediction phase in Earley's algorithmand the state construction in LR parsing.
Based on this analogy, we will define our618K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalismsnotion of closure.
Closure (e,i,d,t, dt) will add entries to PIi, d\] It, dr\] or PIi, d\] Ii, d\] thatresult from the inclusion of the entry P Ii, d\] It, dt\] by considering unary productions(or binary productions when the primary or secondary constituent derives the emptystring).
Before we define Closure () we note that for each occurrence in the algorithmof "use Rule X" is replaced by "use closure of Rule X."
For example, "Use closure ofRule 8.ps.U' stands forIf we have the production A ("~1.
.
.
?m) ~ Ap (.. %) As (C~s) where m ~ 2,el = (IAp, ~pl (flp,At~',/t) ) belongs to PIi, d\] It, dt\] ande2 = ( IAs~ top (C~s) l (rest (C~s)~ nil) ) belongs to P Ii + dp~ d - dp\] IO~ 0\] ande3 ~- (/a~ '~m/(~/1-.. "Ym-l~ap~ "Yp)) does not belong to P\[i,d\] Ii, dp\] thenadd e3 to PIi, d\] Ii~dp\] and then invoke Closure (e3,i~d~i~dp).Closure is defined as follows:Closure (e, il, dl , t, dr)beginuse closure of Rule 2.ps.L, 6.ps.L, 7.ps.L, 8.ps.L, 9.ps.L, 10.ps.Lwith d = dp and the entry e as the primary constituent in the antecedent.use closure of Rule 2.sp.L, 6.sp.L, 7.sp.L, 8.sp.L, 9.sp.L, 10.sp.Lwith ds -- d and the entry e as the secondary constituent in the antecedent.use closure of Rule 2.u.L, 6.u.L, 7.u.L, 8.u.L, 9.u.L, 10.u.Lwith d = dp and the entry e as the primary constituent in the antecedent.end.Note Rule 3 does not apply since we have to assume MCL _< 2 (hence any ter-minator is a 2-terminator and the length of the middle in a terminator-type entry isalways one).
We have not included Rule 4 and Rule 5 while computing the closure.These correspond irectly to the completor step in Earley's algorithm and to the pop-ping of stack elements and hence are not considered a part of the closure.
They haveto be applied later in the control structure.We will now consider the effect of including unary rules on the control structure ofthe algorithm.
Let/ / i l ,  dl/~/i2, d2//G// i3,  d3/~ lid, dd// i f  and only if (1) (il, dl/ < (i3, d3/or (2) /il~ dl/ --- /i3~ d3/and/i2~ d2/ < (i4~ dd/.
The simplicity of the loop structure in thealgorithms een thus far stems from the fact that for any parsing rule if the entry in theconsequent is to be added to P Ii3, d3\] Iid~ dd\] based on the existence of an antecedententry in P Iil, dl\] I/'2, d2\], then Ilil, dll, lid, d211 ~ Ili3, d31~ lid, dd}l. This no longer holdswhen we consider Rule 5.u.L or Rule 5.ps.L when the secondary constituent derives theempty string.
Consider the following derivation (and the presence of the productionsassumed) for a sufficiently long fl:a (flo~) ~ a 1 (fl"~l) ~ A2 (fl~l~Y2) ~ A3 (fl'y) ~ ai...  ai+d-1Consider the addition of an entry e3 to P\[i,d\] It, dr\] (for some It, dtl) to record thederivation from A3 (fl'y).
Closure (e3~ i,d, t, dr) is invoked, resulting in the addition ofe2 (corresponding to A2 (fl'Y13'2)) to P\[i, d\] Ii, d\].
From Rule 5.u.L and the presence ofentry e2 and e3 we would add el (corresponding to al  (fl3/1) to P Ii, d\] It, dt\]).
This couldresult in the need to add more entries to P Ii, d\] Ii, d\], which in turn could cause newentries being added back to PIi, d\] It, dt\], and so on.
Thus we have a situation where619Computational Linguistics Volume 19, Number 4initialization phasefor loops for d, i, d' as beforebeginconsider closure of Rules in Rule set Ifor dt :-- d' - 1 to 1 dofor t := i to i + d' - dt dorepeatconsider closure of Rules in Rule set IIfor dr := dt - 1 to 1 dofor r := t to t + dt - dr doconsider closure of Rules 5.ps.L and Rule 5.u.Luntil no new entries are added to P\[i,d\] \[t~dt\]Figure 8Control structure with unary productions.an antecedent entry in P\[i,d\] It, dr\] ((t, dt) < (i,d)) causes an entry to be added toP \[i, d\] \[i, d\], which, acting as an antecedent entry, causes a new entry to be added toP\[i,d\] It, dr\].A simple strategy to take care of this situation would be to add another loopwithin the t loop (as shown in the partial control structure given in Figure 8) that isrepeated until no new entries are added to P \[i, d\] It, dt\].
It is straightforward to provethe correctness of the algorithm with this additional loop and also that the asymptoticcomplexity remains the same.
The latter is the case because only a bounded numberof entries can belong to P\[i,d\] It, dr\] for any fixed value of i, d, t, dr, and hence therepeat loop can be iterated only a bounded number of times (as determined by thegrammar).
In the partially specified control structure given in Figure 8, we have notconsidered the sp rules.
Also we only consider the changes that need to be made toAlgorithm 1; the changes to Algorithm 2 can be made in a similar fashion.
Finally, forpurposes of abbreviation, we have grouped Rules 2.ps.L, 6.ps.L, 9.ps.L, and 10.ps.Ltogether and called it the Rule set I, and Rules 3.ps.L, 4.ps.L, 7.ps.L, and 8.ps.L theRule set II.The repeat loop shown in Figure 8 is not needed in some situations.
Considerthe derivation and the sequence of addition of entries, e3~ e2~ el, as discussed above.Viewing this derivation as a bottom-up recognizer would, we have a "prediction" fromentry e3 followed by a "completion" that results in the entry el.
In this case the twoentries both encode objects with the same stack length.
We generalize this situationand call such derivations auxiliary derivations (named after auxiliary trees in TAG).a (fl"Yl) ~ TIA1 (fl~/l"/2)T2 ~ TlUAt (fl'Tt)wT2 ~ UlUAt ( f l "Y t )WWlwhere At (fl'Tt) is the 2-terminator of A1 (fl"/1"/2).
We will say that this auxiliary deriva-tion spans at least one terminal if len (UlUWWl) _> 1.
Notice that if for a particular gram-mar every auxiliary derivation spans at least one terminal, then the extra repeat loopadded becomes unnecessary.
This is because now, with this assumption, for every pars-ing rule if the entry in the consequent is to be added to P \[/3~ d3\] \[i4~ dd\] based on the exis-tence of an antecedent entry in P\[/1, dl\] \[12~ d2\] then ((il, dl) ,  (12~ d2)) -< ((i3, d3), (i4~ dd)).620K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsWe end this section by noting that in the case of a lexicalized TAG, we can verifythat every auxiliary derivation spans at least one terminal, and hence in the TAGalgorithm we do not have to include this additional repeat loop.5.2 Tree Adjoining GrammarsTree Adjoining Grammars (TAG) is a tree generating formalism introduced by Joshi,Levy, and Takahashi (1975).
A TAG is defined by a finite set of trees composed bymeans of the operation of tree adjunction.Definition 5.1A TAG, G, is denoted by (VN, VT~ 57 Iv A) whereVN is a finite set of nonterminals symbols,VT is a finite set of terminal symbols,S E VN is the start symbol,I is a finite set of initial trees,A is a finite set of auxiliary trees.An initial tree is a tree with root labeled by S and internal nodes and leaf nodeslabeled by nonterminal and terminal symbols, respectively.
An auxiliary tree is a treethat has a leaf node (the foot node) that is labeled by the same nonterminal that labelsthe root node.
The remaining leaf nodes are labeled by terminals and all internal nodeslabeled by nonterminals.
The path from the root node to the foot node of an auxiliarytree is called the spine of the auxiliary tree.
An elementary tree is either an initialtree or an auxiliary tree.
We will use c~ to refer to an initial tree, and fl to refer toan auxiliary tree.
"y may be used to refer to either an elementary tree or a tree that isderived from an elementary tree.We will call a node in an elementary tree an elementary node.
We can give aunique name to each elementary node by using an elementary node address.
Anelementary node address is a pair composed of the name of the elementary tree towhich the node belongs and the address of the node within that tree.
We will assumethe standard addressing scheme where the root node has an address c. If a nodeaddressed # has k children then the k children (in left to right order) have addresses# ?
1 , .
.
.
,  # ?
k. Thus, if dV" is the set of natural numbers then # E W'*.
In this sectionwe will use # to refer to addresses and ~/to refer to elementary node addresses.
Ingeneral, we can write ~ = IV, #/ where 3  `is an elementary tree and # E Domain (3').We will use Domain (3') for the set of addresses of the nodes in %Definition 5.2Let 3  `be a tree with internal node labeled by a nonterminal A.
Let fl be an auxiliarytree with root and foot node labeled by the same nonterminal A.
The tree, 3`~, thatresults from the adjunction of fl at the node in 3  `labeled A (as shown in Figure 9) isformed by removing the subtree of 3  `rooted at this node, inserting fl in its place, andsubstituting it at the foot node of ft.Each elementary node is associated with a selective adjoining (SA) constraint thatdetermines the set of auxiliary trees that can be adjoined at that node.
In addition,when adjunction is mandatory at a node it is said to have an obligatory adjoining(OA) constraint.
Figure 9 shows how constraints are associated with nodes in treesderived from adjunctions.
Whether fl can be adjoined at the node (labeled by A) in 3  `is determined by c, the SA constraint of the node.
In 3 r` the nodes contributed by fl621Computational Linguistics Volume 19, Number 4AclA c2A c2aNFigure 9The operation of adjoining.have the same constraints as those associated with the corresponding nodes in ft. Theremaining nodes in 7' have the constraints of the corresponding nodes in 3 .`Given/* E Domain (3`), by LABEL(% #/we refer to the label of the node addressed/* in 7.
If the tree in question is clear from context, we will simply use LABELI#/.Similarly, we will use SA(%/*) (or SA(#)) and OA(% #) (or OA(/*)) to refer to the SAand OA constraints of a node addressed /* in a tree 3 .` Finally, we will use ft (/3) torefer to the address of the foot node of an auxiliary tree/3.To be precise, we define the adjunction of/3 at a node in 7 with address /* asfollows.
This operation is defined when/3 is included in the SA constraints of nodeaddressed #in 3 .` If the operation is defined, we will use ADJ (3`, #,/3) to refer to the treethat results.
Let 3'' = AD3 (%/*,/3).
Then the nodes in 3'' and their labels and adjoiningconstraints are defined as follows.?
Domain (-y') = {/.1 I /.1 E Domain(7) , / .1  ~/* ' / *2 ,  for some/*2 E Af*} U{/* ",1 \[/.1 E Domain (fl)} U {#.
ft (fl)'/~l I/* "/.1 C Domain (3'), and/.1  e}?
When/.1 C Domain (3`) such that/.1 #/* ?/.1 for some/.1 E d~ f*}, i.e., thenode in ~ with address/.1 is not equal to or dominated by the nodeaddressed/* in 3,:LABEL(-/',/*~) = LABEL('),,/.1),- -  SA(3" , / .1 )  = SA(3` , / .1 ) ,- -  OA(7',/*1) = OA(-y,/.1),?
when/* '>1 E Domain (7') such that #1 C Domain(fl):LABEL(7',/*.
>1) = LABEL(fl,/*I),- -  SA(" / t , / * " / .1 )  = SA(fl,/.1),- -  OA(3`',/*'/*1) ~-  OA(fl,/.1),?
when/*,  ft (fl)./.1 E Domain (3") such that/*./.1 C Domain (7) and/.1 # e:LABEL ('T t,/*- ft (/3).
/.1) = LABEL (-y, #./xl ),622K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms{z S {IBt,IB2}sb s {131,132}b s?
aFigure 10Example of a TAG G.- -  SA(3/, # ?
ft (fl) - it1 ) = SA('y,# -#1},OA(~/', # - ft (fl).
#~) = OA(%#.
#1),In general, if # is the address of a node in "~ then by (% #) we refer to the elementarynode address of the node that contributes to its presence, and hence its label andconstraints.The tree language, T(G), generated by a TAG, G, is the set of trees derived startingfrom an initial tree such that no node in the resulting tree has an OA constraint.
The(string) language, L(G), generated by a TAG, G, is the set of strings that appear on thefrontier of trees in T(G).Example 5.1Figure 10 gives a TAG, G, which generates the language {wcw I w E {a, b}+}.
Theconstraints associated with the root and foot of fl specify that no auxiliary trees canbe adjoined at these nodes.
This is indicated in Figure 10 by associating the empty set,G with these nodes.
An example derivation of the strings aca and abcab is shown inFigure 11.5.3 TAG and LIGIn this section, we examine bottom-up recognition of a TAG.
In doing so, we constructa LIG that simulates the derivations of the TAG.
Based on this construction, we derivea recognizer for TAG from the algorithms given earlier.Consider bottom-up TAG recognition.
Having recognized the substring dominatedby an elementary node there are two possible actions: (1) move up the tree by combin-ing this node with its siblings; or (2) consider adjunction at that node.
In bottom-uprecognition, the second action (i.e., adjunction) must be considered before the first.Therefore, there are two phases involved in the consideration of each node.
On enter-ing the bottom phase of a node, having just combined the derivations of its children,we predict an adjunction.
On entering the top phase, having just finished adjunctionat that node, we must now combine with any siblings in order to move up the tree.Note that in the bottom phase we may also predict hat there is no adjunction at thenode (if there is no OA constraint on that node) and hence move to its top phasedirectly.Figure 12 shows why, because of the nature of the adjoining operation, TAG can beseen to involve stacking.
Suppose, during recognition, the bottom phase of a node, 7\],623Computational Linguistics Volume 19, Number 43,1c~ S {\[31,\[52}IcFigure 11Sample derivations in G.s~a S {61,~2}Soo aIcs~a S~b S {\[51,\[52}Sq~ bS~ aIcFigure 12Stacking in a TAG.has been reached.
When adjunction by the auxiliary tree fl is predicted, control shiftsto the bottom phase of fl's foot node.
As we move up the spine of fl it is necessary toremember that fl was adjoined at 7.
On reaching the top phase of fl's root we mustreturn to (the top phase of) 7.
Therefore, the adjunction point, ~7, must be propagatedup the spine of ft.
In general, we may need to propagate a stack of adjunction pointsas we move up the spine as shown in Figure 12 where 3'2 is obtained by adjoining 131at a node ~11 on the spine of ft. From this figure, it can be seen that the informationabout the adjunction points (that must be propagated along the spine of an auxiliarytree) follows the stack (last-in first-out) discipline.
Notice also that only the nodes onthe spine participate in the propagation of adjunction points.Consider how a LIG that simulates this process can be constructed.
The details ofthe equivalence between LIG and TAG can be found in Vijay-Shanker (1987).
In the624K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsLIG, we use two nonterminals, t and b to capture the differences between the top andbottom phases associated with a node.
The stack holds an appropriate sequence ofadjunction points in the form of elementary node addresses.
The top of the stack isthe elementary node address of the node that is currently being visited (thus all objectshave at least one element on the stack).
Nodes that are not on the spine, or belong toan initial tree, do not participate in the propagation of adjunction points.
Therefore, theobjects for such nodes will have stacks that contain only their elementary node address.The set of LIG productions is determined as follows.
We assume that internalelementary nodes have either a single child labeled by a terminal symbol (or ~), orexactly two children labeled by nonterminals.
In this discussion below, we will use ~\]for a node and its elementary node address interchangeably.1.
If ?7 is a node that is labeled e where ~ c V T U{~} then we will includet 07) ---+ c.2.
If ?Tp and qs are the children of a node ?7 such that the left sibling ~/p (andhence ?7) is on the spine then the following holds: (1) the objectcorresponding to ~/p can have an unboundedly large stack, whereas theobject for ?7s will have a stack of size one; (2) the top of the stack in theseobjects will be ?Tp and ~Ts; (3) combination of these two sibling nodes ispossible only after the top parts of these nodes are reached; (4) the stackin the object for ~\]p must be propagated to object for 77, except hat thetop symbol ?7p is replaced by ?7; (5) when the two sibling nodes arecombined we reach the bottom part of ?7.
Hence, we include theproduction b(.. ?7) ---+ t (.. ?Tp) t 07s).3.
If ?7p, ~\]s are children of ~ as in the previous case except hat ?7p is theright sibling and is on the spine, then we include the productionb ('" n) -~ t(ns) t (..'qp).4.
If ?Tp, qs, and 77 are as before except hat neither sibling is on the spine ofan auxiliary tree then we include the production b(.. ?/) ~ t (.- ?7p) t 07s).5.
If ?7p is the only child of 77 we have b (.. ?7) --+ t (-- 77p ).6.
If ?7 is a node where fl can be adjoined and we are at the bottom of ~7,then, by predicting adjunction by fl, control moves to the bottom part of771 (the foot node of fl).
This is illustrated in Figure 13.
In this case weadd the production b(.. 77771) --+ b (.. 77).
When there is no OA constraint at77 then we can predict that no adjunction takes place.
This is capturedwith the production t (.. 7/) --+ b (-- ?7).7.
Suppose we have reached the top part of the root node, 772, of theauxiliary tree ft.
The corresponding object has the nonterminal t with 772on top of the stack and the node at which fl was adjoined is immediatelybelow 772.
Having reached the top of the root node of fl we must returnto the top of the node where fl was adjoined.
This is accomplished withthe production t (..) ---+ t (.. 72) (see Figure 13).Figure 13 captures the essence of the connection between TAG and LIG-- in par-ticular the way the adjoining operation in TAG can be simulated in LIG.
This figureis also useful in order to understand the notion of terminators.
As in the case of CCG,the construction of the LIG equivalent of the given grammar is unnecessary.
However,as in the case of CCG, this discussion of the connection between TAG and LIG can be625Computational Linguistics Volume 19, Number 4A7 Jn 1t (I~l)Ib(r'n) / \Figure 13TAG/LIG relationship.used to motivate the choices we make in the form of entries in TAG parser as well asthe rules in the algorithm.5.4 Recognit ion of TAGWe now give a CKY-style recognition algorithm for TAG.
But first we shall considerthe LIG constructed from a given TAG as described in Section 5.3.
Given this LIGgrammar, consider the objects derived and the form of entries that will be used by theLIG algorithm.?
If ~ is an elementary node address of a node on the spine of an auxiliarytree, say fl, then any object that has ~ as the top symbol of its stack mustbe of the form A (~h ... 7\]k~\]t~) where k > 0, A C {t, b}, and/It is theelementary node address of a node where fl can be adjoined.Furthermore, in any derivation, the terminator of A (~t77) will be b (~rlt).?
For this LIG, MSL = MTL = TTC = 1 and MCL = 2.
Hence it follows thatany terminator is a 2-terminator.
From the discussion above, an objectA (9~) (where A C {t, b} and len (~) > 0) has a terminator if and only ifis an elementary node address of a node on the spine of an auxiliary tree.?
Consider the forms of entries for a LIG in this form.
First, the length ofthe middle in a terminator-type entry will be one always, since anyterminator is a 2-terminator.
Note that the terminator of A (9~t~) will beb (~t ) .
Thus, a terminator type entry in a parsing array entry, say P \[i, d\]will have the form ({A,~)O\]t, ({b,~lt}, \[t, dt\]))) where A c {t,b} andIt, dr} < li, d}.
Note that {b, ~Tt) in the terminator-pointer is edundant.?
From the discussion above, a non-terminator-type entry wilt be used torecord derivations from A (7) where A E {t, b} and r/is the elementarynode address of a node that belongs to an initial tree or of a node that is626K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalismsnot on the spine of an auxiliary tree.
To record this object the entry((A, 71, nil) would have been used.From the above discussion it makes sense that terminator-type entries in the TAGparser have the form ((A, T\]/ (7t, t, dt)) where A E {t,b}, 7 is an elementary nodeaddress of a node on the spine of an auxiliary tree, say fl, and /'It is the elementarynode address of a node where fl can be adjoined in.
A non-terminator-type entry hasthe form ((A, 7), nil) where A E {t, b}, and 7 is an elementary node address of a nodethat is not on the spine of an auxiliary tree.Finally, consider an auxiliary derivation in the LIG obtained from a TAG as de-scribed in Section 5.3.
Recall that an auxiliary derivation has the formA (~71) ~ TIA1 (~')'172) T2T~uAt (9~"yt) wT 2In this case we would have:?
3/1 = "Yt,?
A = A1 = t,?
At = b, and?
"72 is the root of an auxiliary tree that can be adjoined at the node whoseelementary node address is given by ~?t.Since every auxiliary tree in a lexicalized TAG has at least one terminal node in itsfrontier, every auxiliary derivation spans at least one terminal in the LIG we haveconstructed.5.5 Recognition AlgorithmWe begin with a description of the cases involved in TAG recognition algorithm.Predicting adjunction: During the recognition phase, on reaching thebottom part of a node 7, we predict adjunction by each auxiliary tree, flthat can be adjoined at 7 as determined by its SA constraints.
As givenin Case 6 of the construction i  Section 5.3, this prediction is capturedwith the LIG production b(.. ~/~\]1) -~ b (.. 7) where 71 is the foot node ofthe auxiliary tree, ft.
Depending on whether 7 is on the spine of anauxiliary tree or not, we have the following counterparts of Rule 8.u.Land Rule 10.u.L:Rule 8.u.T71 = (fl, ft(fl)) fl c SA(7) ((b,7) (~h,t, dt)) E P\[i,d\]((b, 111) (7, i,d) ) C P\[i,d\]Rule 10.u.T7, = (fl~ft(fl)) fl C SA(7) ((b~7)~nil) E P\[i~d\]((b, 71) (7, i,d)) E P\[i,d\]627Computational Linguistics Volume 19, Number 4As in the second part of Case 6 of the LIG construction (i.e., when thereis no OA constraint at the node ~7) we have the following counterparts ofRule 6.u.L and Rule 7.u.L:Rule 6.i.u.TOA(rl) =false ((b,r/)(r\]t,t, dt)) E P\[i,d\]((t,) (,t,t, dt)) E P\[i,d\]Rule 7.i.u.TOA(~)) =false ((b,~7),nil) E P\[i,d\]((t, rl),nil) E P\[i,d\]Left sibling on the spine: This corresponds to Case 2 of the LIGconstruction.
The following rule that captures this situation correspondsto Rule 7.ps.L.Rule 7.ps.T~p is left child of~?p is on the spine of an auxiliary tree((t,~/p/(,,,t, dt)) E P\[i, dp\]~/p is right child of 7/((t, ~/s), nil) C P \[i + dp, d - dp\]((b,,) 01t, t, dt) ) C P\[i,d\]The following covers Case 4 of LIG construction where the two siblingsare not on the spine or belong to an initial tree and corresponds toRule 6.ps.L (or Rule 6.sp.L).Rule 6.ps.Tr/p is left child of z/r/is not on the spine of any auxiliary tree((t,,p} ,nil) C P\[i, dp\] ~p is right child of (t, rls}, nil) E P \[i + dp, d -- dp\]((b, rl} ,nil) E P\[i,d\]Right sibling on the spine: Corresponding to Case 3 of LIG constructionand Rule 7.sp.L we haveRule 7.sp.Trls is left child of ~1((t,,s} ,nil) E P\[i, ds\]~/p is right child ofWp is on the spine of an auxiliary tree((t,,p} (Zlt, t, dt)) E P\[i+ds,d-ds\]((b, rl) (rlt, t, dt)) C P\[i,d\]Single child case: Corresponding to Case 5 of LIG construction,Rule 7.u.L and Rule 6.u.L.Rule 7.ii.u.T~p is only child ofrlp ison the spine of some auxiliary tree ((t,~/p} (rlt, t, dt)) E e\[i,d\]((b,,) (rlt, t, dt)) C P\[i,d\]628K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsRule 6.ii.u.T~/p is only child of ~/is not on the spine of any auxiliary tree ((t ,%),ni l)  E P\[i,d\]((b,r/) ,nil) E P\[i,d\]Completing an adjunction: Corresponding to Case 7 of the constructionand depending on whether the node of adjunction is on the spine of anauxiliary tree, we have the following counterparts of Rule 4.u.L,Rule 5.u.L.Rule 4.u.T((t,~lp) (zlt, t, dt)) E P\[i,d\]~?t is not on the spine of any auxiliary tree((b,z\]t),nil) E Pit, dr\]((t,,t) ,nil) E P\[i,d\]Rule 5.u.T((t,e) (,t,t, dt)) E P\[i,d\]7/t is on the spine of an auxiliary tree((b,r\]t) (l'\]r, Gdr) ) E P\[t, dt\]((t,~\]t) (~lr, r, dr)) E P\[i,d\]From the nature of entries being created it will follow that if ~\]p = (fl, e),for some auxiliary tree fl, then fl is adjoinable at z\]t. Similarly, if~t ~- (fl', #) for some auxiliary tree fl', then fl' is adjoinable at/'Jr.Scanning a terminal symbol: If z/is a node labeled by a terminalmatching the i th input symbol, ai, then we have (corresponding toRule 1.L):Rule 1.TLABEL(7\])=ai l < i < n((t, r/), nil) E P\[i, 1\]Scanning empty string: If ~ is a node labeled by e, then we have(corresponding to Rule lx.L):Rule 1.e.TLABEL(z\]) = e(It, n) ,nil) E P\[i,0\]This concludes our discussion of the parsing rules for TAG.
With the correspon-dences with the LIG parsing rules given (via the numbering of rules), these rules maybe placed in the control structure as suggested in Section 5.1.
As noted earlier, in thecase of a lexicalized TAG, since every auxiliary derivation spans at least one terminalwe do not require the repeat loop discussed in Section 5.1.629Computational Linguistics Volume 19, Number 46.
ConclusionIn this paper we have presented a general scheme for parsing a set of grammar for-malisms whose derivation process is controlled by (explicit or implicit) stacking ma-chinery.
We have shown how this scheme can be instantiated to give polynomialtime algorithms for LIG, CCG, and TAG.
In the case of CCG, this provides the onlypolynomial parsing algorithm (apart from a slight variant of this scheme given inVijay-Shanker and Weir (1990)) we are aware of.The main contribution of this paper is the general recognition scheme and defi-nitions of some notions (e.g., terminators, data structures haring of stacks) crucial tothis scheme.
We believe that these ideas can be suitably adapted in order to produceparsing schemes based on other CFG parsing algorithms (such as Earley's algorithm).For instance, the definition of terminator given here was tailored for pure bottom-upparsing.
In the case of Earley's algorithm, a bottom-up arser with top-down predic-tion, an additional notion of terminator for the top-down prediction component canbe obtained in a straightforward manner.We have also introduced a new method of representing derivations in a TAG, onethat we believe is appropriate in capturing the stacking that occurs during a TAGderivation.
The derivations themselves represented can be in another TAG that wecall the derivation grammar (see Vijay-Shanker and Weir (1993)).We have not discussed the extraction of parses after the recognition is completebecause of space considerations.
However, an algorithm to extract he parses and builda shared forest representation of all parses for CCG was proposed in Vijay-Shankerand Weir (1990).
This scheme was based on the approach we have taken in our generalscheme.
The method of extracting parses and representing them using a shared forestgiven in Vijay-Shanker and Weir (1990) can be generalized in a straightforward mannerto be compatible with the generalized recognition scheme given here.AcknowledgmentsThis work has been partially supported byNSF Grants IRI-8909810 and IRI-9016591.We would like to thank A. K. Joshi, B. Lang,Y.
Schabes, S. M. Shieber, andM.
J. Steedman for many discussions.
Weare grateful to the anonymous reviewers fortheir numerous uggestions.ReferencesAho, A. V. (1968).
"Indexed grammars--Anextension to context free grammars."J.
ACM, 15, 647-671.Duske, J., and Parchmann, R. (1984).
"Linearindexed languages."
Theoretical Comput.Sci., 32, 47-60.Gazdar, G. (1988).
"Applicability of indexedgrammars to natural anguages."
InNatural Language Parsing and LinguisticTheories, edited by U. Reyle andC.
Rohrer.
D. Reidel, 69-94.Joshi, A. K. (1985).
"How muchcontext-sensitivity is necessary forcharacterizing structuraldescriptions--tree adjoining grammars.
"In Natural Language Processing--Theoretical,Computational nd Psychological Perspective,edited by D. Dowty, L. Karttunen, andA.
Zwicky.
Cambridge University Press,206-250.Joshi, A. K.; Levy, L. S.; and Takahashi, M.(1975).
"Tree adjunct grammars."
\].Comput.
Syst.
Sci., 10(1), 136-163.Kasami, T. (1965).
"An efficient recognitionand syntax algorithm for context-freelanguages."
Technical ReportAF-CRL-65-758, Air Force CambridgeResearch Laboratory, Bedford, MA.Lang, B.
(1990).
"Towards a uniform formalframework for parsing."
In Current Issuesin Parsing Technology, edited by M. Tomita.Kluwer Academic Publishers, 153-171.Pareschi, R., and Steedman, M. J.
(1987).
"Alazy way to chart-parse with categorialgrammars."
In Proceedings, 25th Meeting ofthe Association for Computational Linguistics,81-88.Pollard, C. (1984).
Generalized PhraseStructure Grammars, Head Grammars andNatural Language.
Doctoral dissertation,Stanford University.Steedman, M. (1986).
"Combinators andgrammars."
In Categorial Grammars and630K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar FormalismsNatural Language Structures, edited byR.
Oehrle, E. Bach, and D. Wheeler.
Foris,417-442.Steedman, M. J.
(1985).
"Dependency andcoordination i the grammar of Dutch andEnglish."
Language, 61:523-568.Tomita, M. (1988).
"Graph-structured stackand natural language parsing."
InProceedings, 26th Meeting of the Associationfor Computational Linguistics, 248-257.Vijay-Shanker, K. (1987).
A study of treeadjoining rammars.
Doctoral dissertation,University of Pennsylvania, Philadelphia,PA.Vijay-Shanker, K., and Joshi, A. K.
(1985).
"Some computational properties of treeadjoining rammars."
In Proceedings, 23rdMeeting of the Association for ComputationalLinguistics, 82-93.Vijay-Shanker, K., and Weir, D. J.
(In press).
"The equivalence of four extensions ofcontext-free grammars."
MathematicalSystems Theory.Vijay-Shanker, K., and Weir, D. J.
(1990).
"Polynomial parsing of combinatorycategorial grammars."
In Proceedings, 28thMeeting of the Association for ComputationalLinguistics, Pittsburgh, PA, 1-8.Vijay-Shanker, K., and Weir, D. J.
(1993).
"The use of shared forests in TAGparsing."
In Proceedings, 6th Meeting of theEuropean Association for ComputationalLinguistics, Utrecht, The Netherlands,384-393.Weir, D. J.
(1988).
Characterizing mildlycontext-sensitive grammar formalisms.Doctoral dissertation, University ofPennsylvania, Philadelphia, PA.Weir, D. J., and Joshi, A. K.
(1988).
"Combinatory categorial grammars:Generative power and relationship tolinear context-free r writing systems."
InProceedings, 26th Meeting of the Associationfor Computational Linguistics, 278-285.Younger, D. H. (1967).
"Recognition andparsing of context-free languages in timen3.  "
Inf.
Control, 10(2), 189-208.Appendix A: Correctness of Algorithm 1We will now prove the correctness of Algorithm 1.
In doing so, we will start byobserving some properties of the rules and the control structure used.Firstly, given an input is a l .
.
.
an, we can note that every entry added by a rule(i.e., consequents of rules) satisfies the requirements for the terminator-type and non-terminator-type entries; viz., if ((A,'y)(/3, ((At, "Yt), It, dr\]))) is added to an array ele-ment P \[i, d\] then?
A, At E VN,?
%'YtEVI,?
fl E V + where 1 < len(fl) <_ MCL-  1 and?
(t, dt) < li, d) < (1, n) where d _> 2.We can also note that if ((A,'71 (fl, nil)) is added to P\[i,d\] then?
AEVN,?
"yEVI,?
f l EV  7 where0~len( f l )  KTTC+MCL-1  and?
d>_l.These can be verified from noting the form of the rules and by simple induction on(i, d/.
We can also observe from the control structure given that entries to P \[il, dl \]\[/2, d21are added before entries are added to PIi3,d3\] \[id,dd\] if and only if (il,dll < (i3,d31 or(/1, dl) = (/3, d3) and (/2, d2) > (/4, dd).
This observation can be used to show that when631Computational Linguistics Volume 19, Number 4a rule is considered for the purposes of adding an entry to P\[il,dl\] \[/2,d2\] then thearray elements pecified in the antecedent of that rule would have already been filled.Verifying these properties of the algorithm enables us to establish the correctness ofthe algorithm more easily.Theorem A.1if and only if((A, 7) (c~, ((At, 7t), \[t, dt\]))) C P\[i,d\]A (flo~"y) ~ a i .
.
.
a t - lA t  (fl3/t) a t .
.
.
a i+d_ lai .
?
?
a i+d-1for some fl such that At (fl3/t) is the len (aT)-terminator f A (fla'~) in thisderivation and len (flVt) k TTC.
((A,7) (o~,nil)) E P\[i,d\]if and only ifA (oc'y) ~ ai...ai+d-1where A (a7) does not have the TC-property, i.e., A (aT) has noterminator in this derivation or the terminator, say At (fl3q), is such thatlen (flTt) < TTC.Proof of Soundness:We prove the soundness by inducting on d. The base case corresponds to d = 1.
Wehave to consider only entries of the form ((A,'y)(c~,nil)) in P\[i, 1\].
Such entries areadded only by the application of Rule 1.
Therefore, we have A (~/) --+ a and a = ai.Hence A (c~,y) ~ ai as required.Now, for the inductive step, let d > 2.
Any entry ((A,3'/(a, tp)) added to P\[i,d\]where d > 2 must be due to a rule other than Rule 1.L.
This means that we have eithera production A (.. "/1... 3'm) --* Ap (.. 3'p) As (C~s) or A (.. 71... "Ym) --+ As (O~s) A m ('" q/p).
Letus assume that the first production was used.
We will discuss the cases for m = 0,m = 1, and m _> 2 separately.Let m = 0.
In this case the production is A (..) --* Ap (.. ",/p) As (as).
Thenthe entry ((A, "~) (a, tp)) should have been added by using one of rules1.ps.L through 5.ps.L.
We take Rule 4.ps.L as a representative.
If((A, "Yl) (fit, nil)) were to be added as a result of this rule, then we haveto show that A (fit"Y1) :~  ai... ai+d-1 where A (fit"Y1) does not meet theTC-property.
Since (i, dp) < (i, d I, (i +dp, d - ds) < (i, d), and (t, tit) ( (i, d)the inductive hypothesis applies to the three entries in the antecedent.Thus, we have for some a the following derivations:At  (fit'}q)A~ (o~s)at .
.
.
a t+dt -1ai+dp ?
.
.
a i+d_ la i .
?
?
a t - lA t  (oz"/t) at+dr.
?
?
a i+dp-1ai ?.
?
a i+dp-1632K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalismssuch that At (fltTt) does not meet the TC-property.
However, Ap (ol'/l'/p)satisfies the TC-property and furthermore At (o~'/t) is the 2-terminator ofAp (c~'/l'/p).
From Observation 2.1, we can also infer the existence of thefollowing derivation.Ap (flt'/l'/p) ~ ai.
.
.at-lAt (flt'/t)at+dt .
.
.ai+dp-1ai... ai+dp-1Combining this derivation with the derivation from As (c~s) we havea( f l t ' /1 )~ai ?
?
.
at_l At (flt'Tt ) at+dr.
?
?
a i+dp- l  ai+dp ?
?
?
a i+d-1a i .
.
.
a i+d_lFrom Observation 2.5, we know that the terminator of At (fit'~t) in thisderivation is also the terminator of A (fit'~l) (and if At (fit'~t) has noterminator then neither does A (fit'/I)).
Since At (fit'~t) does not satisfy theTC-property (i.e., it does not have a terminator with stack length greaterthan or equal to TTC), A (fit'~l) does not satisfy the TC-property either.Thus we have shown the existence of the required derivation.Let m = 1.
Therefore the production may be written asA (.. "/1) --+ Ap (.. %) As (oes).
This time we will take Rule 6.ps.L as arepresentative.
Hence, we can assume that the entry added to P \[i, d\] hasthe form ((A,'/1)(tip, nil)).
Since (i, dp) < (i,d), and(i + dp, d -ds)  < (i, d), the inductive hypothesis applies to the two entriesin the antecedent.
Thus, we have the following derivations:Ap (gp'/,) a ,  ai+  -iAs (Oes) ~ ai+G.., ai+d-1Therefore we have the derivation:A (tip'/l) ~ ap (tip'~p) As (c~s)ai .
.
.
.
.
.
a i+dp-l  ai+dp ?
?
?
a i+d-1= ai ?
?.
a i+d-1Note that any terminator of Ap (tip'~p) is also the terminator of A (tip'~l)(and if Ap (tip'~p) has no terminator then neither has A (flpVl)).
SinceAp (tip'~p) does not meet the TC-property in this derivation (frominductive hypothesis), neither does A (tip'~p).
Thus we have shown theexistence of the required derivation.Let m > 2.
We will consider the application of Rule 10.ps.L as arepresentative.
Again, applying the inductive hypothesis we have thefollowing derivations:Ap (tip')p) ~ a i .
.
.a i+dp-1As(o@) ~ a i+dp.
.
.a i+d_  1633Computational Linguistics Volume 19, Number 4where len (flpTp) > TTC.
Combining the two derivations, we have:A ( t ip"Y1 .
.
.
"Ym) ap (~p-yp) As (as)ai .
.
.
.
.
.
?l i+dp_ l ai-bdp ?
.
.
a i+d_  lai ?
?
?
a iq -d - ISince m > 2, Ap (tip'/p) is the m-terminator of A (flp"Yl... "Ym) in the abovederivation.
Since fen (flp'yp) >_ TTC, we have shown the existence of therequired derivation and that A (tip71...'Ym) satisfies the TC-property.In a similar manner we can consider other rules (including those that assume a pro-duction of the form A (.. "Yl ' .
.
"Ym) ---+ As (c~s) Ap (.. 7p)) as well.Proof of Completeness:We will now show the completeness of Algorithm 1.
This time we use induction onthe number of steps in a derivation.
Suppose A (fl) ~ ai...ai+d-1; we have to showthat there is a corresponding entry (as specified in Theorem A.1) in P\[i, d\].The base case corresponds to l = 1.
From the form of the productions beingconsidered we can assume that d = 1 and that there exists a production A (~) ~ ai.Rule 1 would apply and thus we have the required entry.Let A (fl) ~ ai .
.
.
ai+d-i where l >__ 1.
The first production used in this derivationmust have the form A (.. "rl .
.
.
"Ym)--+ Ap  ( "  ")'p) As (C~s) or A (.. 3Zl... "ym)---+ As (ees) Ap (.. "yp).We will only assume that the production is A (..q,~ .
.
.
q,,,) ~ Ap (.. 7p)As (c~s).
Argu-ments similar to the one given below can be used when the production of the formA (.. "~1 .. .
7m) --+ As (%) Ap (.. %) is involved as the first step of the derivation.Case m = 0: We begin by considering the case when m = 0.
Since the first productionused in A (fl) =~ ai.
.
.
ai+d-1 is A (..) ---+ Ap (.. "yp) As (C~s), we can write the derivationasA (fl) ~ Ap (9"Yp) As (c~s)ai .
.
.ai+dp-lAs (o~s)ai .
.
.
a i+dp- la i+4 .
.
.
a i+d-1for some I < dp< d and lp + ls = I.
Applying the inductive hypothesis to the derivationAs  (o@) ~ ai+clp .
.
.
a i+d-1 ,  we can assume the existence of the entry(IAs, tOp(~s)) (rest(C~s),nil) )in P \[i + dp, d -dp\ ] .In order to show the existence of the appropriate type of entry corresponding tothe derivation of ai .
.
.
ai+cl-1 from A (fl), we need to consider whether A (fl) satisfies theTC-property in this derivation.
This could depend on whether the primary constituentA, (flpVp) does.
Since the inductive hypothesis applies for the derivation Ap (flVp) Gai .
.
.
ai+dp-1.
Let us start by assuming that A (fl) satisfies the TC-property.
This meansthat it has a (say) (k + 1)-terminator whose stack length is greater than or equal to TTC.Expressing fl as flt71 .
.
.
7k, we can then rewrite the derivation from A (fl) as follows.a ( f l t "Y l .
.
.
~k)  ~ ap  ( f i t l Y1 .
.
.
~k"Yp) As  (o@)ai  .
.
.
a t -  l A t  (flt"Yt ) at+at .
.
.
a i+dr -  l ai +d p .
.
.
a i+d-1ai .
?
?
a t - la t  ?
?
.
a t+dt - la t+d t ?
.
?
a i+d-1634K.
Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalismswhere At ( f l tq/t)  is the terminator of Ap (fltq/~... q/kq/p).
Thus, len ( f l tq/t)  ~ TTC and k _> 1.Now, At  (fltq/t) is the terminator of A (fltq/1 . '
'  q/k) if and only if k > 1 (from Observa-tion 2.5).Let k > 1.
At (fltq/t) is the terminator of A (fltq/1 ... q/k) and len (fit'~t) >_ TTC.Thus, A (fltq/1 .
."
q/k) satisfies the TC-property.
Therefore we must showthat the entry ((A, q/k) (q/1 ...  q/k-l, ((At, q/t)~ \[t, dt\]))) belongs to P\[i,d\].
Byinductive hypothesis we may assume( (Ap, q/p) ("/1.
.
.
q/k, ((At, q/t) , \[t, dt\]))) belongs to P\[i, dp\].
Now all theconditions in the antecedent of Rule 3.ps.L have been met and thus wehave shown the existence of the appropriate ntry to record thederivation of a i .
.
.
ai+d_l from A (fl).Let k -- 1.
From Observation 2.5 it follows that the k'-terminator ofAt  (fltq/t) (if it exists) is also the k~-terminator f A (fltq/1), and if At (fltq/t)has no terminator then neither does A (fltq/1).
Therefore A (fl) = A (flt'yl)satisfies the TC-property if and only if At (fit'~t) does.
Suppose At (fltq/t)satisfies the TC-property; then all conditions tated in the antecedent ofRule 5.ps.L are met and the appropriate ntry is added to record thederivation from A (fl).
On the other hand, if At (fit'/t) does not satisfy theTC-property then all conditions tated in the antecedent of Rule 4.ps.Lare met and the appropriate ntry is added to record the derivationfrom A (fl).Case m = 1" Here we are concerned with the situation where A (.. ")/1) ---+ Ap (.- q/p) As (as)is the first production used in the derivation of a i .
.
.
ai+d-1 from A (fl).
Rewriting fl asflpq/1 we haveA (flpq/1) ~ Ap (flpq/p) As(~s)a i .
.
.
.
.
.
a i+dp- la i+dp ?
.
.
a i+d_ lApplying the inductive hypothesis we have((As, top (C~s) ) (rest (o~s), nil)) c P \[i +dp, d - dp\] .Now any k-terminator f Ap (tipq/p) is also the k-terminator f A (tipq/1) (and if Ap (tipq/,)has no terminator then neither does A (flpq/1)).
That is, A (flpq/1) satisfies the WC-property in this derivation if and only if Ap (fl/~p) does.
If Ap (flpq/p) does not satisfythe TC-property, then, by inductive hypothesis, we have ((Am, q/p)(tip, nil)) c P\[i, dp\].Thus the entries corresponding to the antecedents of Rule 6.ps.L exist and the algo-rithm would have added the entry ((A,q/1)(tip,nil)) c P\[i,d\] as desired.
If A m (flpq/p)does satisfy the TC-property then Rule 7.ps.L would add the required entry to recordthe derivation from A (fl).Case m > 2" Finally, consider that case when m > 2.
The given derivation may beexpressed asA (flpq/,...q/m) =~ Ap (flpq/p) As (as)ai .
.
.
.
.
.
a i+dp- l  ai+dp ?
?
?
a i+d-1= ai ?
?
.
a i+d-1Applying the inductive hypothesis we have((As, top (C~s) ) (rest (C~s), nil)) E P \[i +dp, d -dp\].635Computational Linguistics Volume 19, Number 4Since Ap (flp3`p) is the m-terminator f A (tip3`1... 3`m), we have to consider its lengthin order to know whether A (flp'Yl ... 3`m) satisfies the TC-property, i.e., how it must berepresented.
Suppose len (flp3`p) < TTC, then by inductive hypothesis we have the en-try ((Ap, 3`p) (tip,nil)) E P\[i, dp\].
Thus all antecedents of Rule 9.ps.C have been found.Since the terminator of A (tip3`1 ... 3`m) has a stack of length less than TTC, the requiredentry, ((A,3`m)(flfYl...'~m-l,nil)), is added by the algorithm by the application ofRule 9.ps.L.
Suppose len (flp3`p) >__ TTC, then Ap (flp3`p) may or may not be representedas a terminator-type entry.
Let us take the case where Ap (flp3`p) does not satisfy theWC-property.
Again by inductive hypothesis, we have the entry ((Ap, 3'p)(tip,nil)) EP\[i, dp\].
Since len (flp3`p) > TTC and the antecedents entries of Rule 10.ps.L exist, the al-gorithm would add ((A, 3`m)(3'1-.. 3`m--1~ ( ap~ 3`pl ~ \[i~dp\]))) to P\[i~d\] as desired.
If wehad assumed Ap (flp3`p) satisfies the TC-property, then by applying the inductive hy-pothesis we can guarantee the existence of the entries corresponding tothe antecedentof Rule 8.ps.L, and therefore the algorithm would have added((A~ 3`m)(3`1 ..-3`m-1, ((Ap, "YPI , \[i, dp\] ) ) )to P \[i, d\] as desired.636
