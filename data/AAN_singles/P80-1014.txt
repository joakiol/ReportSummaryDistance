Computational Analogues of Constraints on Grammars:A Model of Syntactic AcquisitionRobert Cregar BerwickMIT  Art i f ic ial  Intelligence Laboratory, Cambridge, MA1.
Introduction: Constraints And Language AcquisitionA principal goal of modern linguistics is to account for theapparently rapid and uniform acquisition of syntactic knowledge,given the relatively impoverished input that evidently serves asthe basis for the induction of that knowledge - the so-calledprojection problem.
At least since Chomsky, the usual responseto the projection problem has been to characterize knowledge oflanguage as a grammar, and then proceed by restricting soseverely the class of grammars available for acquisition that theinduction task is greatly simplified - perhaps trivialized.consistent with our lcnowledge of what language is and of whichstages the child passes through in learning it."
\[2, page 218\] Inparticular, ahhough the final psycholinguistic evidence is not yetin, children do not appear to receive negative vidence as a basisfor the induction of syntactic rules.
That is, they do not receivedirect reinforcement for what is no_..~t a syntactically well-formedsentence (see Brown and Hanlon \[3\] and Newport, Gleitman,and Gleitman \[4\] for discussion).
?
If syntactic acquisition canproceed using just positive examples, then it would seemcompletely unnecessary to move to any enrichment of the inputdata that is as yet unsupported by psycholinguistic evidence.
2The work reported here describes an implemented LISP programthat explicitly reproduces this methodological approach toacquisitio,~ - but in a computational setting.
It asks: whatconstraints on a computational system are required to ensure theacquisition of syntactic knowledge, given relatively plausiblerestrictions on input examples (only positive data of limitedcomplexity).
The linguistic approach requires as the output ofacquisition a representation f adult knowledge in the form of agrammar.
In this research, an existing parser for English,Marcus' PARSIFAL \[1\], acts as the grammar.
PARSIFALdivides neatly into two parts: an interpreter and the grammarrules that the interpreter executes.
The grammar ules unwindthe mapping between a surface string and an annotated surfacestructure representation f that string.
In part this unraveling iscarried out under the control of a base phrase structurecomponent; the base rules direct some grammar ules to buildcanonically-ordered structure, while other grammar rules areused to detect deviations from canonical order.We mimic the acquisition process by fixing a stripped-downversion of the PARSIFAL interpreter, thereby assuming aninitial set of abilities (the basic PARSIFAL data structures, alexicon, and a pair of context-flee rule schemas).
The simplepattern-action grammar ules and the details of the base phrasestructure rules are acquired in a rule-by-rule fashion byattempting to parse grammatical sentences with a degree ofembedding of two or less.
The acquisition process itself is quitestraightforward.
Presented with a grammatical sentence, theprogram attempts to parse it.
If all goes well, the rules exist tohandle the sentence, and nothing happens besides a successfulparse.
However, suppose that the program reaches a point in itsattempt where no currently known grammar rules apply.
At thispoint, an acquisition procedure is invoked that tries to constructa single new rule that does apply.
If the procedure is successful,the new rule is saved; otherwise" the parse is stopped and thenext input sentence read in.Finally, since the program is designed to glean most of its newrules from simple example sentences (of limited embedding), itsdevelopmental course is at least broadly comparable to whatPinker \[2\] calls a "developmental" criterion: simple abilities comefirst, :rod sophistication with syntax emerges only later.
Thefirst rules acquired handle simple" few-word sentences andexpand the basic phrase structure for English.
Later on, rules todeal with more sophisticated phrase structure, alterations ofcanonical word order, and embedded sentences can be acquired.If an input datum is too complex for the acquisition program tohandle at its current stage of syntactic knowledge, it simplyparses what it can, and ignores the rest.2.
Const ra in ts  Establish the Program's Success2.
I Current Status of the Acquisition ProgramTo date, the accomplishments of the research are two-fold.First, from an engineering standpoint, the program succeedsadmirably; starting with no grammar ules and just two baseschema rules, the currently implemented version (dubbedLPARSIFAL) acquires from positive example sentences many ofthe grammar rules in a "core grammar" of English originallyhand-written by .Marcus.
The currently acquired rules aresufficient to parse simple declaratives, much of the Englishauxiliary system including auxiliary verb inversion, simplepassives, simple wh.questions (e.g., Who did John kiss.
'),imperatives, and negative adverbial preposing.
Carryingacquisition one step further, by starting with a relativelyrestricted set of context-free base rule schemas - the X-barsystem of Jackendoff \[7\] - the program can also easily inducethe proper phrase structure rules for the language at hand.Acquired base rules include those for noun phrases, verb phrases,prepositional phrases, and a substantial part of the Englishauxiliary verb system.The decision to l imit the program to restricted sorts of evidencefor its acquisition of new rules - that is, positive data of onlylimited complexity - arises out of a commitment to develop theweakest possible acquisition procedure that can still successfullyacquire syntactic rules.
This co,nmitment in turn follows fromthe position (cogently stated by Pinker) that "any plausibletheory of language learning will have to meet an unusually richset of empirical conditions.
The theory ... will have to be\[.
But clfildren might (and seem to) receive negative evidence for what i~ a,~emantically well-formed ,~entence.
See Brown and Hanlon \[3\]-2.
There is a another rea.,on for rejecting negative examples as inductiveevidence: from farina| results first established by Gold \[5\], it is known that bypairing positive and negative example string.~ with the appropriate labels"grammaticaC and "ungrammatical" one can learn "almost any" language.Thus.
enriching the input to admit negative evidence broadens the class of"l~'~ssibly learnable languages" enormously.
(Explicit instruction and negativeexamples are often closely yoked.
Compare the necessity for a benignteacher in Wlnston',~ blocks world learning program \[6'j.
)49Of course, many rules lie beyond the current program's reach.PARSIFAL employed dual mechanisms to distinguish NounPhrase ;rod wh-moveznents: at present, LPARSIFAL has only asingle device to handle all constituent movements.
Lacking adistinguished facility to keep track of wh-movements,LPARSIFAL cannot acqt, ire the rules where these movementsmight interact with Noun Phrase movements.
Currentexperiments with the system include adding the wh facility backinto the domain of acquisition.
Also, the present model cannotcapture all "knowledge of language" in the sense ;ntended bygenerative grammarians.
For example, since the weakest formof the acquisition procedure does not employ backup, theprogram cannot re-analyze "garden path" sentences and sodeduce that they are grammatically well-formed) In part, thisdeficit arises because it is not perfectly clear to what extentknowledge of parsing encompasses al_!
our knowledge aboutlanguage.
42.2 Constraints and the Acquisition ProgramHowever, beyond the simple demonstration of what can andcannot be acquired, there is a second, more importantaccomplishment of the research.
This is the demonstration thatconstraint is an essential element of the acquisition program'ssuccess.
To ease the computational burden of acquiringgrammar rules it was necessary to place certain constraints onthe operation of the model, tightly restricting both the class ofh.vpothesizable phrase structure rules and the class of possiblegramlnar rules.The constraints on grammar rules fall into two rough groups:consteainrs o,x rule application and constraints on rule form.The constraints on rule application can be formulated as specific/oca/i O, principles that govern the operation of the parser andthe acquisition procedure.
Recall that in Marcus' PARSIFALgrammar rules consist of simple production rules of the form If<pattern> then <action>, where a pattern is a set of featurepredicates that must be true of the current environment of theparse i,~ order for an action to be taken.
Actions are the basictree-building ol~raTions that construct the desired output, a(modified) annotated surface structure tree (in the sense ofFiengo \[S\] or Chomsky \[9\]).Adopting the operating principles of the original PARSIFAL,grammar rules can trigger only by successfully matching featuresof  the (finite) local em@onment of the parse, an environmentthat includes a small, three-cell look-ahead buffer holding?
"already-built constituents whose grammatical function is as yet3.
A related issue is that the current procedure do~ not acquire thePARSIFAL "diagnostic" grammar rules that exploit look.ahead.
Typically,diagnostic rules us.- the specific features of lexical items far ahead in theIo~k-ahead buffer to decide between alternative courts of action.However.
I~y extendih, the acqui~;tion procedure -- allowing it tore-analyze apparently "bad" ~ntences in a careful mode and adding thestipui;Jti,~n that more "specific" rules should take priority over more "general"rules (an c, ften-made assumption for production systems) -- one can begin toaecomodate he acquisition of diagnostic rules, and in fact provide a kind ofdevelopmental heory for such rules.
Work testing this idea is underway.4.
In mo.,t oo<lets, the string-to-structural description mapping implied bythe directionality of parsing is not "neutral" with respect speakers andlisteners.undecided (e.g., a noun phrase that is not yet known to be thesubject of a sentence) or single words.
It is Marcus' claim thatthe addition of the look-ahead buffer enables PARSIFAL toalways correctly decide what to do next - at least for English.The parser uses the buffer to make discriminations that wouldotherwise appear to require backtracking.
Marcus dubbed this"no bocktracking" stipulation the Determinism Hygothesis.
TheDetermiqism Hypothesis crucially entails that all structure theparser builds is correct - that already-executed grammar uleshave performed correctly.
This fact provides the key to easyacquisition: if parsing runs into trouble, the difficulty can bepinpointed as the current locus of parsing, and no_._tt with anyalready-built structure (previously executed grammar ules).
Inbrief, any errors are assumed to be locally and immediatelydetectable.
This constraint on error detectability appears to bea computational analogue of the restrictions on atransformational system advanced by Wexler and his colleagues.
(see Culicover ;rod Wexler \[I0\]) In their independent but relatedformal mathematical modelling, they have proved that a finiteerror detectability restrict/on suffices to ensure the learnabilityof a tr;msformational grammar, a fact that might be taken asindependent support for the basic design of LPARSIFAL.Turning now to constraints on rule form, it is easy to see thatany such constraints wilt aid acquisition directly, by cuttingdown the space of rules that can be hypothesized.
To introducethe constraints, we simply restrict the set of possible rule<patterns> and <actions>.
The trigger patterns for PARSIFALrules consist of just the items in the look-ahead buffer and alocal (two node) portion of the parse tree under construction-five "cells" in all.
Thus, patterns for acquired rules can beassumed to incorporate just five cells as well.
As for actions, amajor effort of this research was to demonstrate that just threeor so basic operations are sufficient to construct he annotatedsurface structure parse tree, thus eliminating many of thegrammar rule actions in the original PARSIFAL.
Together, therestrictions on rule patterns and actions ensure that the set ofrules available for hypothesis by the acquisition program isfinite.The restrictions just described constrain the space of availablegr:,mmnr rules.
However, in the case of phrase structure rules:ldditional strictures are necessary to reduce the acquisitiona\[burden.
LPARSIFAL depends heavily on the X.bar theory ofphrase structure rules \[7\] to furnish the necessary constraints.
Inthe X-bar theory, ,all phrase structure rules for human grammarsare assu,ned to be expansions of just a few schemas of a ratherspecific form: for example, XP->...X .....
Here, the "X" standsfor an oblig;,tory phrase structure category (such as a Noun,Verb, or Preposition): the ellipses represent slots for possible, butoptional "XP" elements or specified grammatical formatives.Actual phrase structure rules ;sre fleshed out by setting the "X"to some known category and settling upon some way to fill outthe ellipses.
For example, by setting X=N(oun) and allowingsome other "XP" to the left of the Noun (call it the category"Determiner") we would get one verson 3f a Noun Phrase rule,NP-->Determiner N .
In this case, the problem for the learnermust include figuring out what items are permitted to go in theslots on either side of the "N".
Note that the XP schematightly constrains the set of possible phrase structure rules; forinstance, no rule of the form, XP-->X X would be admissible,immediately excluding such forms as, Noun Phrase->NounNoun.
It is this rich source of constraint that makes the50induction of the proper phrase structure from positive examplesfeasible; section 4 below illustrates how this induction methodworks in practice.Finally, it should be pointed out that the category names like"N" and "V" are just arbitrary labels for the "X" categories; thestandard approach of X-bar theorists is to assume that thenames st:md for bundles of distinctive features that do theactual work of classifying tokens into one category bin oranother.
All important area for future research will be toformulate precise models of how the feature system evolves ininteraction with lexical and syntactic acquisition.This research completed so far assumes that the acquisitionprocedure is initially provided with just the X-bar schemadescribed above along with an ability to categorize lexical items;is noun.c, ~'erbs, or other.
In .addition, the program has an initialschema for a well-formed predicate argument structure, namely,a predicate (verb) along with its "object" arguments.
Otherphrase structure categories such as Prepositional P/ware areinferred by noticing lexical items of unknown categorization andthen insisting upon the constraint that only "XP" items orspecified formatives appear before and after the main "X" entry.To take im over-simplified example, given the Noun Phrase thebook behind the ~'indow, the presence of the non-Noun, non-Verbbehind and the Noun Phrase lhe window immediately after thenoun book would force creation of a new "X" category, sincepossible alternatives uch as, NP->NP \[the book\] NP \[behind...\]are prohibited by the X-bar ban on directly adjacent, duplicate"X" items.The X-bar acquisition component of the acquisition procedure isstill experimental, and so open to change.
However, even crudeuse of the X-bar restrictions has been fruitful.
For one thing, itenables the acquisition procedure to start without anypre-conceptions about canonical word order for the language athand.
This would seem essential if one is interested in theacquisition of phrase structure rules for languages whosecanonical Subject-Verb-Object ordering is different from that ofEnglish.
Ill addition, since so much of the acquisition of thecategory names is tied up with the elaboration of a distinctivefeature system for lexical items, adoption of the X-bar theoryappears to provide a driving wedge into the difficult problems oflexica\[ acquisition and lexical ambiguity.
To take but oneexample, the X-bar theory provides a framework for studyinghow items of one phrase structure category, e.g., verbs, can beconverted into items of another category, e.g., nouns.
This lineof research is also currently ander investigation.3.
The Acquisit ion Algorithm is SimpleAs mentioned, LPARSIFAL proceeds by trying its hand atparsing a series of positive example sentences.
Parsing normallyoperates by executing a series of tree-boilding and token-shiftinggrammar rule actions.
These actions are triggered by matches ofrule patterns against features of tokens in a small thtee-ceUconstituent look-ahead buffer and the local part of theannotated surface structure tree currently under construction-the lowest, right-most edge of the parse tree.Grammar nile execution is also controlled by reference to basephrase structure rules.
To implement his control, each of theparser's grammar rules are linked to one or more of thecomponeqts of the phrase structure rules.
Then, grammar ulesare defined to be eligible for triggering, or active, only if theyare associ:tted with that p:lrt of the phrase structure which isthe current locus of the parser's attentions; otherwise, agramm;ir rule does not even have the opportunity to triggeragainst the buffer, and is inactive.
This is best illustrated by anex;tmple.
Suppose there were but a single phrase structure rulefor English, Sentence->NounPhrase VerbPhrase.
Flow of controlduring a parse would travel left-to-right in accordance with theS--NP--VP order of this rule, and could activate and deactivatebuqdles of grammar rules along the way.
For example, if theparser had evidence to enter the S->NP VP phrase structurerule, pointers would first be set to its "S" and the "NP"portions.
Then, all the grammar ules associated with "S" and"NP" would have a chance to run and possibly build a NounPhrase constituent.
The parser would eventually advance inorder to construct a Verb Phrase, deactivating the Noun Phrasebuilding grammar rules and activating any grammar rules:lssociated with the Verb Phrase.
5 Together with (1) the itemsin the buffer and (2) the leading edge of the parse tree underconstruction, the currently pointed-at portion of the phrasestructure forms a triple that is called the current machine slateof the parser.If in the midst of a parse no currently known grammar ulescan trigger, acquisition is initiated: LPARSIFAL attempts toconstruct a single new executable grammar rule.
New ruleassembly is straightforward.
LPARSIFAL simply selects a newpattern and action, utilizing the current machine stale triple ofthe parser at the point of failure as the new pattern and one offour primitive (atomic) operations as the new action.
Theprimitive operations are: attach the item in the left-most buffercell to the node currently under construction; switch (exchange)the items in the first and second buffer cells; insert one of afinite number of lexical items into the first buffer cell; andinsert a trace (an anaphoric-like NP) into the first buffer cell.The actions have turned out to be sufficient and mutuallyexclusive, so that there is little if any combinatorial problem ofchoosing among many alternative new grammar ule candidates.As a further constraint on the program's abilities, the acquisitionprocedure itself cannot be recursively invoked; that is, if in itsattempt to build a single new executable grammar rule theprogram f inds that it must acquire still other new rules, thecurrent attempt at acquisition is immediately abandoned.
Thisrestriction has the apparently desirable effect of ensuring thatthe program use just local context o debug its new rules as wellas ignore overly complicated example sentences that are beyondits reach.5.
This mherne w&.L first ,',uggested by Marcus \[I.
~ge 60\].
The actu~procedure uses the X-bar ~hernas instead of explicitly labellad nodes like"Vl" or "S'.51In a pseudo-algorithmic form, the entire model looks like this:Step L Read in new (grammatical) example sentence.Step 2.
Attempt to parse the sentence, using modifiedPARSIFAL  parser.2.1 Any phrase structure schema rules apply?2.1.1 YES: Apply the rule; Go to Step 2.22.1.2 NO: Go to Step 2.22.2 Any grammar ules apply?
(<pattern> of rule matches current parser state)2.2.1 YES: apply rule <action>; (continue parse)Go to Step 2.1.2.2.2 NO: no known rules apply;Parse f inished?YES: (Get another sentence) Go to Step i.NO: parse is stuckAcquisit ion Procedure already Invoked?YES: (failure of parse oracquisition) Go m Step 3.4. or 3.2.3-4NO: (Attempt acqumuon~Go to Step 3.Step 3.
Acquisit ion Procedure3.1 Mark  Acquisit ion Procedure as Invoked.3.2 Attempt to construct new grammar ule3.2.2 Try attachSuccess: (Save new rule) Go to Step 3.3Fai lure: (Try next action) On to Step 3.2.33.2.3 Try to switch first and second buffer cell items.Success: (Save new rule) Go to Step 3.3.Failure:.
(Restore buffer and try next action)Re-switch buffer cells; Go to Step 3.2.43.2.4 Try insert traceSuccess: (Save new rule) Go to Step 3.3.Fai lure: (End of acquisition) On to Step 3A.3.3 (Successful a.cquisition)Store new rule; Go to Step 2.1.3.,I (Fa i lure of acquisit ion)3A.1 (Optional phrase structure rule)Continue parse; Advance past currentphrase structure component: Go to Step 2.1.3.4.2 (Fai lure of parse) Stop parse; (30 to Step 1..4.
Two Simple Scenarios4.1 Phrase Structure for Verb PhrasesTo see exactly how the X-bar constraints can simplify the phrasestru~ure induction task, suppose that the learner has alreadyacquired the phrase structure rule for sentences, i.e., somethinglike, Sentence->Noun Phrase Verb Phrase, and now requiresinformation to determir,, the proper expansion of a Verb phrase,Verb Phrase->..777.The X-bar theory cuts through the maze of possible expansionsfor the right-hand side of this rule.
Assuming that NounPhrases are the only other known category type, the X-bartheory then tells us is that these are the only possibleconfigurations for a Verb Phrase rule:Verb Phrase->Noun Phrase VerbVerb Phrase->Verb Noun PhraseVerb Phrase->Noun Phrase Verb Noun PhraseIf the learner can classify basic word tokens as either nouns orverbs, then by simply matching an example sentence such asJohn kissed Mary against the possible phrase structureexpansions, the correct Verb Phrase rule can be qu;:kly deduced:$ 8 $NP VP NP VP NP VP\[ NP V \] V NP i NP V NP1 ?
?
I I I t ?
?
?d.
kissed M. d, kissed M. d. kissed M.(N) (V) (N)Only one possible Verb Phrase rule expansion can successfully bematched against the sample string, VerbPhrase->Noun Phrase(NP)Verb(V) - exactly the right resultfor English.
Although this is but a simple example, it illustrateshow the phrase structure rules can be acquired on the basis of aprocess akin to "parameter setting"; given a highly constrainedinitial state, the desired final state can be obtained uponexposure to very simple triggering data.4.2,4 Subject-Auxiliary Verb Inversion RuleSuppose that at a certain point LPARSIFAL has all thegrammar rules and phrase structure rules sufficient o build aparse tree for John did kiss Mary.
The program now must parse,Did John kiss Mary?.
No currently known rule can fire, for allthe rules in the phrase structure component activated at thebeginning of a sentence will have a triggering pattern roughlylike f=Aroun Phrase?\]\[=i/erb?\], ut the input buffer will hold thepattern \[Did: auxrerb, verbffJohn: Noun Phrase\], and so thwartall attempts at triggering a grammar ule.
A new rule must bewritten.
Acting according to its acquisition procedure, theprogram first tries to attach the first item in the buffer, did, tothe current active node, S(entence) as the Subject Noun Phrase.The attach fails because of category restrictions from the X-bartheory; as a kztown verb, did can't be attached as a NounPhrase.
But switch works, because when the first and secondbuffer positions are interchanged, the buffer now looks like\[Johnffdid\] Since the ability to parse declaratives such as Johndid kiss.., was assumed, an NP-attaching rule will now match.Recording its success, the program saves the switch rule alongwith the current buffer pattern as a trigger for remembering thecontext of auxiliary inversion.
The rest of the sentence can nowbe parsed as if it were a declarative (the fact that a switch wasperformed is also permanently recorded at the appropriate placein the parse tree, so that a distinction between declarative andinverted sentence forms can be maintained for later "semantic"Ugh.)5.
SummaryA simple procedure for the acquisition of syntactic knowledgehas been presented, making crucial use of linguistically- andcomputationally-motivated constraints.
Computationally, thesystem exploits the local and incremental approach of theMarcus parser to ensure that the search space for hypothesizabienew rules is finite and small.
In addition, rule orderinginformation need not be explicitly acquired.
That is, the systemneed not learn that, say, Rule A must obligatorily precede RuleB.
Extrinsic ordering of this sort appears difficult (if notimpossible) to attain under conditions of positive-only evidence.Third, the system acquires its complement of rules via thestep-wise hypothesis of new rules.
This ability to incrementallyrefine a set of grammar rules rests upon the incrementalproperties of the Marcus parser, which in turn might reflect thecharacteristics of the English language itself.52The constraints on the parser and acquisition procedure alsoparallel many recent proposals in the linguistic literature, lendingconsiderable support to LPARSIFAL's design.
Both the powerand range of rule actions match those of constrainedtransformational systems; in this regard, one should compare the(independently) formalized transformational system of Lasnikand Kupin \[I1\] that ahnost point-for-point agrees with therestrictions on LPARSIFAL.
Turning to other proposals, two ofLPARSIFAL's rule actions, attach and switch, correspond toEmonds'  \[12\] categories of structure-preserving and local(minor-movement) rules.
A third, insert trace, is analagous to themore alpha rule of Chomsky \[13\].
Rule application iscorrespondingly restricted.
The Culicover and Wexler BinaryPrinciple (an independently discovered constraint akin toChomsky's Subiacency Condition; see \[10\]) can be identifiedwith the restriction of rule pattern-matching to a local radiusabout the current point of parse tree construction (eliminatingrules that directly require unbounded complexity forrefinement).
The remaining Culicover and Wexler sufficiencyconditions for learnability, including their Freezing and Ralsin~Principles, are subsumed by LPARSIFAL's assumption of strictlocal operation and no backtracking (eliminating rules thatpermit the unbounded cascading of errors, and hence unboundedcomplexity for refinement).These striking parallels should not be taken - at least notimmediately -- as a functional, "processing" explanation for theconstraints on grammars uncovered by modern linguistics.
Anexpl:mation of this sort would take computational issues as thebasis for an "evaluation metric" of grammars, and then proceedto tells us why constraints are the way they are and not someother way.
But this explanatory result does not necessarilyfollow from the identity of description between traditionaltransformational and LPARSIFAL accounts.
Rather,LPARSIFAL  ,night simply be translating the transformationalconstraints into a different medium - a computational one.Even more intriguing would be the finding that the constraintsdesirable from the standpoint of efficient parsing turn out to beexactly the constraints that ensure efficient acquisition.
Thecurrent  work with LPARSIFAL at least hints that this might bethe case.
However, at present he trade-off between the variouskinds of "computational issues" as they enter into the evaluationmetr ic is unknown ground; we simply do not yet know exactlywhat  "counts" in the computational evaluation of grammars.ACKNOWLEDGE}4ENTSThis article de,~rihes r~earch done at the Artificial IntelligenceLaboratory of the M&,~sachusetts Institute of Technology.
Support for theLaboratory's artificial intelligence research is provided in part by theAdvanced Research Projects Agency of the Department of Defenseunder Office of Naval Research contract N00014-75-C-0643.The author is also deeply indebted to Milch Marcus.
Only by startingwith a higi~ly restricted parser could one even begin to consider theproblem of acquiring the knowledge that such a par.
',er embodies.
Theeffort aimed at restricting the operation of PARSIFAL flows ?s muchfrom his thoughts in this direction as from the research into acquisitionalone.REFERENCESi l l  Marcus, ,,H. A Theory of Syntactic Recognition for NaturalLanguage.
Cambridge, ,,VIA: HIT Press,, 1980.\[2\] Pinker.
S. "Formal Models of Language Acquisition: Cognition, 7.1979. pp.
217-283,\[3\] Brnwn.
R.. and Hanlon, C., "Derivational Complexity and Order ofAcquisition in Child Speech," in J.R. Hayes.
ed, Cognition and theDevelopment of Language, New York: John Wiley and Sons, 1970.\[4\] Newport, E. Gleitman, H, and Gleitman.
I,.. "Hother.
l'd Rather doit My,~elf: Some Effects and Non-effects of Maternal Speech Style: in C.Snow and C. Ferguson.
Talking to Children.
Input and Acquisition,New York: Cambridge University l're~s, i977.\[5\] Gold.
E..M, "Language Identification in the Limit," Information andControl.
1O.
1967. pp.
447-474.\[6\] Winston.
P.. "Learning Structural Descriptions from Examples," in P.Winston.
editor, The Psychology of Computer Vision.
New York:McGraw-Hill, 1975.\[7\] Jackendoff.
R.. X-bar Syntax: A Study of Phrase StructureCambridge.
MA: MIT Press.
1977.\[8\] Fiengn.
R, "On Trace Theory: Linguistic Inquiry.
8. no.
1.
1977.pp.
35-61.\[9\] Chomsky, N., "Conditions on Transformations," in S.R.
Anderson andP.
Kiparsky.
(eds.).
A Festschrift for Morris Halle, New York: HoR.Rinehart.
and Winston, t973+\[10\] Culicover.
P. and Wexler.
K, Formal Models of LanguageAcquisition.
Cambridge.
,'VIA: MIT Press, 1980.\[l 1\] La.,nik, H. and Kupin.
J.
"A Restrictive Theory of TransformationalGrammar."
Theoretical Linguistics, 4. no.
3.
1977. pp.
173-196.\[12\] Emonds, J.
A Transformational Approach to English Syntax.New York: Academic Press.
1q76.\[13\] Chomsky, N+ "On Wh-movement: in P. Culicover, T. Wasow, and A.Akmajian.
Formal Syntax.
New York: Academic Press.
1977. pp.
71-t32.53
