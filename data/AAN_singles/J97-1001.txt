Empirical Studies in DiscourseMar i lyn  A. Walker*AT&T Labs ResearchJohanna D. Moore  tUniversity of Pittsburgh1.
IntroductionComputational theories of discourse are concerned with the context-based interpreta-tion or generation of discourse phenomena in text and dialogue.
In the past, research inthis area focused on specifying the mechanisms underlying particular discourse phe-nomena; the models proposed were often motivated by a few constructed examples.While this approach led to many theoretical dvances, models developed in this man-ner are difficult to evaluate because it is hard to tell whether they generalize beyondthe particular examples used to motivate them.Recently however the field has turned to issues of robustness and the coverageof theories of particular phenomena with respect o specific types of data.
This newempirical focus is supported by several recent advances: an increasing theoretical con-sensus on discourse models; a large amount of on-line dialogue and textual corporaavailable; and improvements in component technologies and tools for building andtesting discourse and dialogue testbeds.
This means that it is now possible to deter-mine how representative particular discourse phenomena re, how frequently theyoccur, whether they are related to other phenomena, what percentage of the cases aparticular model covers, the inherent difficulty of the problem, and how well an algo-rithm for processing or generating the phenomena should perform to be considered agood model.This issue brings together a collection of papers illustrating recent approaches toempirical research in discourse generation and interpretation.
Section 2 gives a generaloverview of empirical studies in discourse and describes an empirical research strategythat leads from empirical findings to general theories.
Section 3 discusses how eacharticle exemplifies the empirical research strategy and how empirical methods havebeen employed in each research project.2.
Why Empirical Studies?What is the role of empirical studies in research on computational models of discourse?We believe that developing eneral theories depends on the ability to characterize com-putational models of discourse, and their behaviors, tasks, and tasks contexts, in termsof sets of features that can be used to make and evaluate predictions about what affectsthe behavior under investigation (Cohen 1995; Sparck-Jones and Galliers 1996; Walker* AT&T Labs Research, 600 Mountain Ave. 2D-441, Murray Hill, NJ 07974.
E-mail:walker@research.att.comt Computer Science Dept., University of Pittsburgh, Pittsburgh, PA. E-mail: jmoore@cs.pitt.edu(~) 1997 Association for Computational LinguisticsComputational Linguistics Volume 23, Number 11996).
1The role of empirical methods is to help researchers discover general features byanalyzing specific discourse phenomena or programs that interpret or generate them.Once relevant features are identified, hypotheses about the relationship between themcan be formed, and controlled studies that test the hypothesized relationships can bedevised.
This approach leads to general theories via the following steps, which manyreaders will recognize as a variation of Cohen's empirical generalization strategy(Cohen 1995, 6):1.
Feature identification: identify features of the discourse, tasks, andcontext hat may influence the target behavior;2.
Modeling: develop a causal model of how these features influence thetarget behavior;3.
Evaluation: assess the performance of the model (often implemented in aprogram) for producing the target behavior on the tasks and in thecontexts for which it was devised;4.
Generalization: once the model makes accurate predictions, generalizethe features o that other discourses, tasks, and contexts are encompassedby the causal model, and test whether the general model accuratelypredicts behavior in the larger set of discourses, tasks, and contexts.This strategy provides a general research methodology in which the study ofdiscourse phenomena proceeds in several stages, each of which employs empiricalmethods.
The first stage is to identify features that may affect the behavior of interest.Hypotheses about what features are relevant may come from an analysis of a corpus ofnaturally occurring discourses, a study of the literature on the phenomena of interest,an analysis of tasks in a particular domain or the analysis of a program that exhibitsa behavior of interest.
These hypotheses are then used to develop or refine a causalmodel of how the features influence the target behavior.
At this point, the model canbe evaluated in many ways, for example, a program that embodies the model can beimplemented and evaluated.
If evaluation indicates that performance of the model isnot satisfactory, further work must be done at stages 1 and 2 to identify features thatinfluence the target behavior and to model their interactions appropriately.Stages 3 and 4 are key to the ability to generalize; once researchers are able toderive quantitative results by testing a model against a data set, general theories canarise from qualitatively analyzing which aspects of the model most directly affect thedesired behavior, and from evaluating the model in qualitatively different situations.For this reason, much recent work has been concerned with methodological issues ofhow to quantitatively measure performance.But how do empirical methods help researchers discover general features andgenerate hypotheses?
Recent work uses some combination of the following empiricalmethods: (1) Tagging of discourse phenomena in corpora; (2) Induction of algorithmsor discourse models from tagged data; (3) Comparison of algorithm output to humanperformance; (4) Human scoring of an algorithm's output; (5) Task efficacy evaluationbased on the the domain; (6) Ablation studies where algorithm features are systemat-ically turned off; (7) Wizard-of-Oz studies; (8) Testbeds of (parameterizable) dialogue1 Sparck-Jones and Galliers (1996, 23) call features performance factors and distinguish betweenenvironmental factors, which are features ofthe task that are fixed from the system designer'sviewpoint, system factors, which reflect design choices, algorithm features, orother input factors, andsystem effects, which are features that characterize the behavior of the system.Walker and Moore Empirical Studies in Discoursemodels using computer-computer dialogue simulation; and (9) Testbeds of (parame-terizable) dialogue models implemented in human-computer dialogue interfaces.
Howare these methods used and how do they contribute to the development of generaltheories?Discourse tagging classifies discourse units in naturally occurring texts or dia-logues into one of a set of categories.
Discourse units range from referring expressionsand syntactic onstructions (Fox 1987; Kroch and Hindle 1982; Prince 1985), to wordsor phrases (Heeman and Allen 1994; Hirschberg and Litman 1993; Novick and Sutton1994), to utterances and relationships among them (Dahlback 1991; Reithinger andMaier 1995; Moser and Moore 1995; Nagata 1992; Rose et al 1995), to multiutteranceunits identified by a range of criteria such as speaker intention or initiative (Flam-mia and Zue 1995a; Hirschberg and Nakatani 1996; Whittaker and Stenton 1988).
Thearticle by Carletta et al (this volume) presents a tagging scheme for three levels ofdiscourse structure.Discourse tags categorize ither features of the input (independent variables) orfeatures of the output (dependent variables).
Often hypotheses about input featuresthat affect the target behavior are found in previous work (stage 1 of the methodol-ogy).
In this case, the tagging contributes to developing a causal model.
The taggedcorpus is used to test whether the features predict the target behavior.
For example,researchers have devised algorithms for generating the surface form of explanationsand instructions from underlying intention-based representations by tagging naturallyoccurring discourses for surface form features, informational relations, and intentionalrelations (Vander Linden and Di Eugenio 1996; Moser and Moore 1995; Moore andPollack 1992; Paris and Scott 1994).
Another promising area is speech act (dialoguemove) tagging, where, for example, researchers have tested whether an automatic tag-ger trained on the tagged corpus can improve the performance of a speech recognizerwith tag-specific language models (Taylor et al 1996), and whether an induced dis-course model based on the tagging can predict the next dialogue act in the dialogue,and thus affect how the system translates the next utterance (Reithinger and Maier1995; Nagata 1992).Tagging is also critical for ablation studies, where algorithm features are selectivelyturned off, and performance xamined.
Tagging can characterize the input in termsof features the algorithm uses for producing the target behavior or characterize thetarget behavior.
For example, Lappin and Leass (1994) report an ablation study ofan anaphora resolution algorithm, operating on computer manuals, in which variousfactors that were hypothesized todetermine the most likely antecedent were selectivelyturned off.
(See also Dagan et al \[1995\]).
Sample results include the finding that thereis no effect on performance, for this type of text, when an antecedent's likelihood isincreased for parallelism.Another use of discourse tagging is for algorithm induction using automatic las-sifiers, such as C4.5 or CART, that produce decision trees from data sets described bya set of features (Brieman et al 1984; Quinlan 1993).
This approach uses automaticmethods for stages 2 and 3 of our empirical research strategy.
Since discourse taggingassociates ets of features with discourse phenomena, tagged data is used as inputto these automatic lassifiers.
The decision tree produced by the classifiers functionsas a causal model, which can then be examined and further tested.
For example, thismethod has been used to identify features for predicting accent assignment in text-to-speech (Hirschberg 1993), for repairing disfluencies (Nakatani and Hirschberg 1993),cue vs. noncue uses of discourse cue words (Litman 1996; Siegel and McKeown 1994),discourse segment boundaries (Grosz and Hirschberg 1992; Passonneau and Litman,this volume), intonational phrase boundaries (Wang and Hirschberg 1992), and theComputational Linguistics Volume 23, Number 1most likely antecedents for anaphors (Aone and Bennet 1995; Connolly, Burger, andDay 1994).Discourse tagging is also instrumental in stage 3 of our empirical method, byproducing a test set that can be used for comparison to a program's output.
Thismethod is used by most of the articles in this volume.
A common application istesting coreference r solution algorithms; the tags indicate the preferred interpretationof a potentially ambiguous utterance containing anaphoric noun phrases (Walker 1989;Suri and McCoy 1994; Chinchor and Sundheim 1995).
Coreference algorithms are thentested on their ability to select he right equivalence class for an anaphoric noun phrase.The same method has been applied to empirically testing an algorithm for resolvingverb phrase ellipsis (Hardt 1992).In each case, we can generalize on the basis of specific features from studies ofspecific algorithms operating on specific corpora, whenever the corpora represent ageneral task for the algorithm.
The more varied the test data is, the more generalizablewe expect he results to be.
For example, the claim that a model is general can besupported by test corpora representing different genres (Fox 1987; Kroch and Hindle1982), or different language families (Strube and Hahn 1996; Iida 1997; Di Eugenio1997; Hoffman 1997).Human performance can also be compared to algorithm output through the useof reaction time or comprehension r production experiments (Brennan 1995; Gordon,Grosz, and Gilliom 1993; Hudson-D'Zmura 1988).
These methods allow researchersfine-grained control of the phenomena studied, and avoid problems with sparse datathat can arise with corpus analyses.
Reaction time studies also provide researcherswith an indirect measure of how humans process aparticular phenomenon; processingtimes can then be compared with the predictions of a model.The method of having humans score the result of an algorithm, or compare it tohuman performance, is useful when the algorithm output is hard to classify as anelement of a finite set, for example, when the output is a summary or an explanationproduced by a natural anguage generation system (McKeown and Radev 1995; Robinand McKeown 1996; Sparck-Jones 1993).
This is the method used by Lester and Porter(this volume).The method of task efficacy evaluation tests the effectiveness of the target behav-ior in an environment in which it is embedded (Sparck-Jones and Galliers 1996).
Forexample, in evaluating the generation mechanism of a tutorial system, a task efficacymeasure valuates how well students comprehend or learn an explanation or an in-struction.
If the execution of an instruction can be monitored, performance metrics canbe collected (Biermann and Long 1996; Young 1997).
Another approach is based onfield studies with actual end-users (Jarke et al 1985).
Like human scoring, this methodappears to be well-suited for research with complex outputs such as instructional di-alogue systems.Because testing dialogue systems requires a fully implemented natural anguagesystem, there are two empirical methods for testing hypotheses about discourse modelsthat are independent of the current state of the art in speech or language processing.The first method is Wizard-of-Oz simulation, and the second is computational testbedsfor dialogue simulation.In the Wizard-of-Oz (WOZ) approach, a human wizard simulates the behaviorof a program interacting with a human to carry out a particular task in a particulardomain (Dahlb~ick and Jonsson 1989; Hirschman et al 1993; Oviatt and Cohen 1989;Whittaker and Stenton 1989).
The WOZ method can, in principle, be used to test, re-fine, or generalize any behavior implementable in a program and thus is appropriateat several stages of our methodology.
For example, the wizard may follow a proto-4Walker and Moore Empirical Studies in Discoursecol that includes particular system limitations or error-handling strategies, to explorepotential problems before implementation, e.g.
determining how the program's levelof interactivity affects the complexity of the instructions it is given (Oviatt and Cohen1989).
In addition, WOZ is often used to collect sample dialogues that are not affectedby system limitations; the human wizard can simulate behavior that would result ina system error, so that the resulting corpus of dialogues is not affected by humanadaptation to the system's limitations.
In some cases, the resulting corpus providestraining data for spoken language systems (Hirschman et al 1993), is used as a targetfor improved systems (Moore, Lemaire, and Rosenblum 1996), or forms a test set forevaluating the performance of an existing natural anguage system (Whittaker andStenton 1989; Hirschman et al 1993).Dialogue simulation testbeds evaluate specific models of agent communicative ac-tion.
This approach spans stages 1-3 of the methodology: human-human dialoguesare used to formulate hypotheses about features that determine a target behavior; aprogram is designed in which the target behavior is parameterizable and in whichmetrics for evaluating performance can be collected; and then simulations are run todetermine which behaviors, determined by the parameter settings, affect performanceon the task.
Hans, Pollack, and Cohen (1993) discuss the role of this method in ex-ploration, confirmation, and generalization of particular models of agent behavior.In discourse studies, these testbeds have been used to investigate the utility of plan-ning as an underlying model of dialogue (Power 1979; Houghton and Isard 1985), theinteraction of risk-taking dialogue strategies and corresponding repair strategies (Car-letta 1992), the relationship between resource bounds, task complexity, and dialoguestrategies (Walker 1996; Jordan and Walker 1996), the role of belief revision in tasks inwhich agents negotiate a problem solution (Logan, Reece, and Sparck-Jones 1994), andthe relationship between mixed initiative and knowledge distribution (Guinn 1994).Empirical research in testbeds for human-computer dialogue interfaces i very re-cent (Hirschman et al 1990; Allen et al 1996).
This method supports tudies of theinteraction of various components; for example Danieli and Gerbino (1995) proposean implicit recovery metric for evaluating how the dialogue manager overcomes lim-itations in the speech recognizer.
Other research parameterizes the dialogue managerto select different behaviors in different contexts, such as expert vs. novice discoursestrategies (Kamm 1995), different repair strategies (Hirschman and Pao 1993), or dif-ferent degrees of initiative (Potjer et al 1996; Smith and Gordon, this volume).
Thesedialogue interfaces also provide an opportunity for task efficacy evaluation, as dis-cussed above, and there are already many examples of dialogue systems being testedin field trials with representative populations of users (Danieli and Gerbino \[1995\],Kamm \[1995\], Meng et al \[1996\], Sadek et al \[1996\] inter alia.)3.
Overview of the IssueThe articles in this issue represent many of the empirical methods discussed aboveand span research on dialogue tagging, generation of referring expressions, generationof explanations, topic identification, identification of changes in speaker intention, andthe effect of initiative on the structure of dialogues.
Below we discuss each article inturn, both in terms of the methods it uses and in terms of how it contributes to thedevelopment of general theories.3.1 Carletta, Isard, Isard, Kowtko, Doherty-Sneddon, and AndersonDiscourse tagging is a key component of much empirical work in discourse, howeverthe development of discourse tag sets is still a relatively new area of endeavor.
CarlettaComputational Linguistics Volume 23, Number 1et al discuss a scheme for tagging human-human dialogues with three tag sets: trans-actions, conversational games, and dialogue moves.
A transaction is a subdialoguethat accomplishes one major step in the participants' plan for achieving the task (Isardand Carletta 1995).
The size and shape of transactions i therefore largely dependenton the task.
Each transaction consists of a sequence of conversational games, wherea conversational game is a set of utterances starting with an initiation (e.g., a requestfor information) and encompassing all successive utterances until the purpose of thegame has been fullfilled or the game has been abandoned.
Each game consists of asequence of moves, each of which is classified as either an initiative (e.g., instruction,explanation) or a response (e.g., reply, acknowledgment).The paper discusses issues with determining the reliability and generality of tag-ging sets, and with refining tag sets on the basis of reliability data (Carletta 1996;Condon and Cech 1995).
The Discourse Working Group is also applying this taggingscheme to other dialogue types to evaluate its generality (Hirschman et al 1996; Lu-perfoy 1996).3.2 HearstThe target behavior that Hearst is concerned with is subtopic identification in exposi-tory texts.
She tests the hypothesis that term repetition is a primary feature of textualcohesion (Morris and Hirst 1991) by using it as the basis for two different algorithms foridentifying multiparagraph subtopical units.
The algorithms are evaluated by compar-ing the units they propose against a baseline of randomly generated topic boundaries,and against a corpus tagged by human judges.
Precision, recall, and K (Carletta 1996;Krippendorf 1980) are used as evaluation metrics to assess the performance of the twoalgorithms.In order to generalize, Hearst tests the algorithm's performance on a new task:that of distinguishing boundaries between sets of concatenated news articles.
Hearst'salgorithm performs comparably to other algorithms on the new task, showing thatterm repetition may be a more general indicator of subtopic boundaries.
Future workcould test further generalizations; term repetition may also indicate subtopics in otherdiscourse or dialogue nvironments, and may interact with other features that correlatewith topic boundaries, uch as pauses, intonation, or cue words (Cahn 1992; Hirschbergand Nakatani 1996).3.3 Lester and PorterThe target behavior that concerns Lester and Porter is generating paragraph-lengthexplanations in the biology domain.
Given a goal to explain a biology concept orprocess, their KNIGHT system selects relevant information from a large knowledgebase, organizes it, and then generates it.
It is clearly not possible to evaluate how wellKNIGHT produces coherent paragraph-length explanations by comparing KNIGHT'Sexplanations word for word, or even proposition for proposition, with a corpus ofhuman explanations.
There are simply too many choices on the path from knowledgebase to surface form.
So Lester and Porter compare KNIGHT to human performanceby having domain experts score a corpus consisting of both KNIGHT and humanexplanations.
The domain experts are unaware of the fact that some explanations aregenerated by a computer.
KNIGHT'S performance is evaluated on the basis of gradingexplanations on a scale of A to F for the features of coherence, content, organization,writing style, and correctness.To generalize their results, Lester and Porter propose further fine-grained analysesof KNIGHT'S output by sentential form or referring expressions.
Other generalizationscould arise by showing that KNIGHT'S content organization operators (EDPs) couldWalker and Moore Empirical Studies in Discoursebe used in other domains for generating explanations, as in Robin and McKeown(1996).3.4 Passonneau and LitmanThe target behavior that Passonneau and Litman's article models is the identification ofmulti-utterance units in spoken story narratives that correspond to speaker's intention.In order to define a test set for the target behavior, seven human subjects taggedeach utterance in a narrative as a boundary where the speaker starts communicatinga new intention, or as a nonboundary, where the speaker continues discussing thecurrent intention.
The boundaries marked by either three or four subjects (out of seven)are used to define the target behavior for boundary identification.
Passonneau andLitman then examine three classes of utterance features to determine correlations withboundary and nonboundary utterances: (1) coreferential nd inferential relationshipsbetween oun phrases across two adjacent utterances; (2) the occurrence of discoursecue phrases at the beginning of an utterance; and (3) prosodic/acoustic utterancefeatures uch as phrase-final intonation and utterance-initial pauses.
They developand evaluate three algorithms for producing the target behavior from these features,two hand-developed and one automatically induced.Generalizations of this work arise from other research on different speech genresIn different environments hat also found that coreferential relationships, pausing, andintonation are correlated with discourse structure (Cahn 1992; Fox 1987; Hirschbergand Nakatani 1996).
Future work can further test the generalizability of the resultsreported here: the features used could be examined in other types of spoken mono-logues, in texts, and in dialogue.3.5 Smith and GordonSmith and Gordon examine the effect of initiative on dialogue structure in dialoguesin which human subjects interact with the computer to diagnose and repair problemswith simple circuits.
Initiative is varied by setting the system's initiative to directiveor declarative mode.
In directive mode, the system instructs the student at each step.In declarative mode, the system lets the student ake the initiative, but volunteersrelevant facts.
Dialogue structure is tagged via a model that segments circuit repairdialogues into five phases: introduction, assessment, diagnosis, repair and test.
ThenSmith and Gordon examine how the subdialogue l ngth varies depending on initiativemode.Their results exemplify the empirical generalization strategy by showing that asubdialogue model based on WOZ simulations can be generalized to human-computerdialogues.
They also show that claims about the effect of initiative on dialogue struc-ture in human-human dialogues in other domains (Whittaker and Stenton 1988; Walkerand Whittaker 1990) generalize to human-computer dialogues in the circuit repair do-main.
Further generalizations could result from determining whether the subdialoguemodel can be used in other types of human-human or human-computer p oblem-solving dialogues.3.6 Yeh and MellishThe target behavior that Yeh and Mellish model is the generation of anaphoric nounphrases in Chinese texts.
The algorithm must select from among zero pronouns, overtpronouns, and full noun phrases; in addition, for full noun phrases, appropriate con-tent must be determined.
Their training set is a corpus of Chinese texts tagged foranaphoric noun phrases and for features claimed to affect noun phrase form.
TheyComputational Linguistics Volume 23, Number 1construct a decision tree by sequentially allowing additional features to affect deci-sions on anaphoric form, wherever there is room for improvement.In order to test the derived decision tree, they construct a test set of texts generatedby a Chinese generator.
At each location where a noun phrase occurs, a set of choices ofnominal referring expressions are given.
Then they conduct wo comparisons.
First, hu-man judges select from among the forms and the selections of human judges are com-pared among themselves.
Second, a program implementing the derived decision treeselects among the forms and the program's behavior is compared to the human judges.This work could be generalized by comparing their predictive features with thoseused in algorithms for generating referring expressions in English (Passonneau 1995).Other generalizations could arise from comparing the decision trees with factors thataffect anaphoric forms in Japanese (Iida 1997), Italian (Di Eugenio 1997), or Turkish(Hoffman 1997).
In addition, it would be useful to test their suggestion that the lack ofagreement among native speakers as to the preferred form of anaphoric noun phraseswas partially determined by the use of texts generated by a natural anguage generator.It is possible that the same experiment carried out on naturally occurring texts wouldgenerate a similar amount of disagreement.4.
Future Direct ionsIn recent years, there has clearly been a groundswell of interest in empirical methodsfor analyzing discourse.
A survey of recent ACL papers shows that the percentageof empirical papers in semantics, pragmatics, and discourse hovered between 8% and20% until 1993 when it increased to 40%.
In 1995 and 1996, 75% of the ACL papers insemantics, pragmatics, and discourse used empirical methods.
While a great deal ofprogress has been made, several obstacles impede empirical research.
The discoursecommunity must develop more shared methods, tools, and resources.First, researchers in discourse must agree on methods for quantitatively char-acterizing performance and on ways to determine whether the metrics are servingtheir intended iagnostic function (Moore and Walker 1995; Cohen 1995; Sparck-Jonesand Galliers 1996).
Recent work includes discussion of appropriate statistical methodsand metrics for spoken dialogue systems (Bates and Ayuso 1993; Danieli et al 1992;Hirschman et al 1990; Hirschman and Pao 1993; Simpson and Fraser 1993), informa-tion extraction systems (Lewis 1991; Chinchor, Hirschman, and Lewis 1993; Chinchorand Sundheim 1995), and tagging reliability (Carletta 1996).Second, we must develop more shared tools.
The lack of tools greatly increasesthe cost of accurate coding, which could be reduced with coding tools that structurethe coder's input and checks that it is within the coding scheme's constraints.
To datemost coders enter data by hand in a word processor or using home-grown, hastilyconstructed tools.
To our knowledge, there is only one publicly available tool fordialogue structure coding (Flammia and Zue 1995b).Third, we must increase the number of and representativeness of dialogue and textcorpora.
To our knowledge, the only publicly available human-computer dialogue cor-pora is that for the ATIS task (Hirschman 1993), and there are no publicly availablecorpora of human-human dialogues representing a broad range of spoken-dialogueapplications.
Similarly, there are no publicly available corpora of text-based explana-tions in particular domains that could be a resource for the generation community.However, even if more corpora become available, most discourse studies require datato be tagged, and there are currently no publicly available tagged corpora.
In order todevelop a large shared resource of tagged materials, the discourse community mustshare efforts across sites.
We need to develop shared coding schemes and make codedWalker and Moore Empirical Studies in Discoursedata publicly available to support comparisons of different models.
The communityis currently addressing these issues through a series of working groups (Hirschmanet al 1996; Luperfoy 1996).
Given the current state of the art, we expect hese issuesto concern the community for some time.AcknowledgmentsWe are greatly indebted to the many peoplewho contributed to this special issue byserving as reviewers for the 29 papers thatwere submitted.
We would also like tothank Lynette Hirschman, Aravind Joshi,and Marti Hearst for helping us organizethe AAAI Workshop on Empirical Methodsin Discourse that provided the impetus forthis issue.ReferencesAllen, James E, Bradford W. Miller, Eric K.Ringger, and Teresa Sikorski.
1996.
Arobust system for natural spokendialogue.
In Proceedings ofthe AnnualMeeting, pages 62-70.
Association forComputational Linguistics.Aone, Chinatsu and Scott Bennet.
1995.Evaluating automated and manualacquisition of anaphora resolutionstrategies.
In Proceedings ofthe 33rd AnnualMeeting, pages 122-129.
Association forComputational Linguistics.Bates, Madeleine and Damaris Ayuso.
1993.A proposal for incremental dialogueevaluation.
In Proceedings ofthe DARPASpeech and Natural Language Workshop,pages 319-322.Biermann, A. W. and Philip M. Long.
1996.The composition of messages inspeech-graphics nteractive systems.
InProceedings ofthe 1996 InternationalSymposium on Spoken Dialogue, pages97-100.Brennan, Susan E. 1995.
Centering attentionin discourse.
Language and CognitiveProcesses, 10(2):137-167.Brieman, Leo, Jerome H. Friedman,Richard A. Olshen, and Charles J. Stone.1984.
Classification and Regression Trees.Wadsworth and Brooks, Monterey, CA.Cahn, Janet.
1992.
An investigation i to thecorrelation of cue phrases, unfilled pausesand the structuring of spoken discourse.In Workshop on Prosody in Natural Speech,pages 19-31.
Institute for Research inCognitive Science, University ofPennsylvania, TR IRCS-92-37.Carletta, Jean C. 1992.
Risk Taking andRecovery in Task-Oriented Dialogue.
Ph.D.thesis, Edinburgh University.Carletta, Jean C. 1996.
Assessing thereliability of subjective codings.Computational Linguistics, 20(4).Chinchor, Nancy, Lynette Hirschman, andDavid D. Lewis.
1993.
Evaluatingmessage understanding systems: Ananalysis of the third messageunderstanding conference (muc-3).Computational Linguistics, 19(3):409-451.Chinchor, Nancy and Beth Sundheim.
1995.Message understanding conference MUCtests of discourse processing.
In AAAISpring Symposium on Empirical Methods inDiscourse Interpretation and Generation,pages 21-27.Cohen, Paul.
R. 1995.
Empirical Methods forArtificial Intelligence.
MIT Press, Boston.Condon, Sherri L. and Claude G. Cech.1995.
Functional comparison offace-to-face and computer-mediateddecision-making interactions.
In SusanHerring, editor, Computer-MediatedConversation.
John Beniamins.Connolly, Dennis, John D. Burger, andDavid S. Day.
1994.
A machine learningapproach to anaphoric reference.
InProceedings ofthe International Conference onNew Methods in Language Processing(NEMLAP).Dagan, Ido, John Justeson, Shalom Lappin,Herbert Leass, and Amnon Ribak.
1995.Syntax and lexical statistics in anaphoraresolution.
Applied Artificial Intelligence -An International Journal, 9(4):633-644.Dahlba'ck, Nils.
1991.
Representations ofDiscourse: Cognitive and ComputationalAspects.
Ph.D. thesis, LinkopingUniversity.Dahlba'ck, Nils and Arne Jonsson.
1989.Empirical studies of discourserepresentations fornatural anguageinterfaces.
In Proceedings ofthe 4thConference ofthe European Chapter of theAssociation for Computational Linguistics,pages 291-298.Danieli, M., W. Eckert, N. Fraser, N. Gilbert,M.
Guyomard, P. Heisterkamp,M.
Kharoune, J. Magadur, S. McGlashan,D.
Sadek, J. Siroux, and N. Youd.
1992.Dialogue manager design evaluation.Technical Report Project Esprit 2218SUNDIAL, WP6000-D3.Danieli, Morena and Elisabetta Gerbino.1995.
Metrics for evaluating dialoguestrategies in a spoken language system.
InProceedings ofthe 1995 AAAI SpringComputational Linguistics Volume 23, Number 1Symposium on Empirical Methods inDiscourse Interpretation a d Generation,pages 34-39.Di Eugenio, Barbara.
1997.
Centering theoryand the Italian pronominal system.
InCentering in Discourse.
Oxford UniversityPress.
To appear.Flammia, Giovanni and Victor Zue.
1995a.Empirical results of dialogue coding.
InEUROSPEECH 95.Flammia, Giovanni and Victor Zue.
1995b.N.b.
: A graphical user interface forannotating spoken dialogue.
In MarilynWalker and Johanna Moore, editors, AAAISpring Symposium: Empirical Methods inDiscourse Interpretation a d Generation.Fox, Barbara A.
1987.
Discourse Structure andAnaphora: Written and ConversationalEnglish.
Cambridge University Press.Gordon, Peter C., Barbara J. Grosz, andLaura A. Gilliom.
1993.
Pronouns, namesand the centering of attention indiscourse.
Cognitive Science, 17(3):311-348.Grosz, Barbara J. and Julia B. Hirschberg.1992.
Some intonational characteristics ofdiscourse structure.
In ICSLP.Guinn, Curry I.
1994.
Meta-DialogueBehaviors: Improving the Efficiency ofHuman-Machine Dialogue.
Ph.D. thesis,Duke University.Hanks, Steve, Martha Pollack, and PaulCohen.
1993.
Benchmarks, testbeds,controlled experimentation a d thedesign of agent architectures.
AI Magazine,December.Hardt, Daniel.
1992.
An algorithm for VPellipsis.
In Proceedings ofthe 30th AnnualMeeting, pages 9-14.
Association forComputational Linguistics.Heeman, Peter A. and James Allen.
1994.Detecting and correcting speech repairs.In Proceedings ofthe 32nd Annual Meeting.Association for ComputationalLinguistics.Hirschberg, Julia B.
1993.
Pitch accent incontext: predicting intonationalprominence from text.
Artificial IntelligenceJournal, 63:305-340.Hirschberg, Julia and Diane Litman.
1993.Empirical studies on the disambiguationof cue phrases.
Computational Linguistics,19(3):501-530.Hirschberg, Julia and Christine Nakatani.1996.
A prosodic analysis of discoursesegments in direction-giving monologues.In Proceedings ofthe 34th Annual Meeting,pages 286-293, Santa Cruz, CA.Association for ComputationalLinguistics.Hirschman, L., M. Bates, D. Dahl, W. Fisher,J.
Garofolo, D. Pallett, K. Hunicke-Smith,P.
Price, A. Rudnicky, andE.
Tzoukermann.
1993.
Multi-site datacollection and evaluation in spokenlanguage understanding.
In Proceedings ofthe Human Language Technology Workshop,pages 19-24.Hirschman, Lynette, Deborah A. Dahl,Donald P. McKay, Lewis M. Norton, andMarcia C. Linebarger.
1990.
Beyond classa: A proposal for automatic evaluation ofdiscourse.
In Proceedings ofthe Speech andNatural Language Workshop, ages 109-113.Hirschman, Lynette, Aravind Joshi, JohannaMoore, and Marilyn Walker.
1996.
IRCSWorkshop on Discourse Tagging.Technical Report, University ofPennsylvania, http://www, cis.
upenn.edu: 80/-ircs/discourse-t agging/toplevel, html.Hirschman, Lynette and Christine Pao.
1993.The cost of errors in a spoken languagesystem.
In Proceedings ofthe Third EuropeanConference on Speech Communication a dTechnology, pages 1419-1422.Hoffman, Beryl.
1997.
Word order,information structure and centering inTurkish.
In Marilyn A. Walker,Aravind K. Joshi, and Ellen E Prince,editors, Centering in Discourse.
OxfordUniversity Press.
To appear.Houghton, G. and S. Isard.
1985.
Why tospeak, what to say and how to say it:Modelling language production indiscourse.
In Proceedings ofthe InternationalWorkshop on Modelling Cognition,University of Lancaster.Hudson-D'Zmura, Susan B.
1988.
TheStructure of Discourse and AnaphorResolution: The Discourse Center and theRoles of Nouns and Pronouns.
Ph.D. thesis,University of Rochester.Iida, Masayo.
1997.
Discourse coherenceand shifting centers in Japanese texts.
InMarilyn A. Walker, Aravind K. Joshi, andEllen F. Prince, editors, Centering inDiscourse.
Oxford University Press.
Toappear.Isard, Amy and Jean C. Carletta.
1995.Replicability of transaction and actioncoding in the map task corpus.
In MarilynWalker and Johanna Moore, editors, AAAISpring Symposium: Empirical Methods inDiscourse Interpretation a d Generation.Jarke, Matthias, Jon A. Turner, Edward A.Stohr, Yannis Vassiliou, Norman H.White, and Ken Michielsen.
1985.
A fieldevaluation of natural anguage for dataretrieval.
IEEE Transactions on SoftwareEngineering, SE-11, No.1:97-113.Jordan, Pamela and Marilyn A. Walker.1996.
Deciding to remind during10Walker and Moore Empirical Studies in Discoursecollaborative problem solving: Empiricalevidence for agent strategies.
InConference ofthe American Association ofArtificial Intelligence, AAAI.Kamm, Candace.
1995.
User interfaces forvoice applications.
In David Roe and JayWilpon, editors, Voice Communicationbetween Humans and Machines.
NationalAcademy Press, pages 422-442.Krippendorf, Klaus.
1980.
Content Analysis:An Introduction to its Methodology.
SagePublications, Beverly Hills, CA.Kroch, Anthony S. and Donald M. Hindle.1982.
A quantitative study of the syntaxof speech and writing.
Technical Report,University of Pennsylvania.Lappin, Shalom and Herbert Leass.
1994.An algorithm for pronominal anaphoraresolution.
Computational Linguistics,20(4):535-562.Lewis, David D. 1991.
Evaluating textcategorization.
I  Proceedings ofthe DARPASpeech and Natural Language Workshop,pages 312-318.Litman, Diane.
1996.
Cue phraseclassification using machine learning.Journal of Artifi'cial Intelligence Research,5:53-94.Logan, Brian, Steven Reece, and KarenSparck-Jones.
1994.
Modellinginformation retrieval agents with beliefrevision.
In Seventh Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages91-100, London.
Springer-Verlag.Luperfoy, Susan.
1996.
Discourse ResourceInitiative Home Page.
Technical Report,http://www.georgetown.edu/luperfoy/Discourse-Treebank/dri-home.html.McKeown, Kathleen R. and Dragomir R.Radev.
1995.
Generating summaries ofmultiple news articles.
In Edward A. Fox,Peter Ingwersen, and Raya Fidel, editors,Proceedings ofthe 18th Annual InternationalACM SIGIR Conference on Research andDevelopment i  Information Retrieval, pages74-82, Seattle, Washington, July.Meng, Helen, Senis Busayapongchai, JamesGlass, Dave Goddeau, Lee Hetherington,Ed Hurley, Christine Pao, Joe Polifroni,Stephanie Seneff, and Victor Zue.
1996.Wheels: A conversational system in theautomobile classifieds domain.
InProceedings ofthe 1996 InternationalSymposium on Spoken Dialogue, pages165-168.Moore, Johanna and Marilyn Walker.
1995.Proceedings of the 1995 aaai workshop onempirical methods in discourseinterpretation a d generation.
TechnicalReport, AAAI:http://www.aaai.org/Publications/TechReports/reportcatalog.html#spring.Moore, Johanna D., Benoff Lemaire, andJames A. Rosenblum.
1996.
Discoursegeneration for instructional pplications:Identifying and exploiting relevant priorexplanations.
Journal of the LearningSciences, 5(1):49-94.Moore, Johanna D. and Martha E. Pollack.1992.
A problem for RST: The need formulti-level discourse analysis.Computational Linguistics, 18(4).Morris and G. Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as anindicator of the structure of text.Computational Linguistics, 17(1):21--48.Moser, Margaret G. and Johanna Moore.1995.
Investigating cue selection andplacement in tutorial discourse.
InProceedings ofthe 33rd Annual Meeting,pages 130-135.
Association forComputational Linguistics.Nagata, Masaaki.
1992.
Using pragmatics torule out recognition errors in cooperativetask-oriented dialogues.
In ICSLP.Nakatani, Christine H. and Julia Hirschberg.1993.
A speech-first model for repairdetection and correction.
In Proceedings ofthe 31st Annual Meeting, pages 46--53.Association for ComputationalLinguistics.Novick, David and Stephen Sutton.
1994.An empirical model of acknowledgmentfor spoken language systems.
InProceedings ofthe 32nd Annual Meeting.Association for ComputationalLinguistics.Oviatt, Sharon L. and Philip R. Cohen.
1989.The effects of interaction on spokendiscourse.
In Proceedings ofthe 27th AnnualMeeting, pages 126-134.
Association forComputational Linguistics.Paris, Cecile and Donia Scott.
1994.
Stylisticvariation in multilingual instructions.
InThe 7th International Conference on NaturalLanguage Generation.Passonneau, Rebecca J.
1995.
Integratinggricean and attentional constraints.
InIJCA195.Potjer, J., A. Russel, L. Boves, and E. denOs.
1996.
Subjective and objectiveevaluation of two types of dialogues in acall assistance service.
In 1996 IEEE ThirdWorkshop: Interactive Voice Technology forTelecommunications Applications, IVTTA.IEEE, pages 89-92.Power, R. 1979.
The organisation ofpurposeful dialogues.
Linguistics,17:107-152.Prince, Ellen E 1985.
Fancy syntax andshared knowledge.
Journal of Pragmatics,11Computational Linguistics Volume 23, Number 1pages 65-81.Quinlan, J. Ross.
1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann.Reithinger, Norbert and Elisabeth Maier.1995.
Utilizing statistical speech actprocessing in verbmobil.
In ACL 95.Robin, J. and K. McKeown.
1996.Empirically designing and evaluating anew revision-based model for summarygeneration.
Artificial Intelligence, 85.Special Issue on Empirical Methods.Rose, Carolyn Penstein, Barbara Di Eugenio,Lori Levin, and Carolyn Van Ess-Dykema.1995.
Discourse processing of dialogueswith multiple threads.
In Proceedings ofthe34th Annual Meeting.
Association forComputational Linguistics.Sadek, M. D., A. Ferrieux, A. Cosannet,P.
Bretier, F. Panaget, and J. Simonin.1996.
Effective human-computercooperative spoken dialogue: The agsdemonstrator.
In Proceedings ofthe 1996International Symposium on Spoken Dialogue,pages 169-173.Siegel, Eric V. and Kathleen R. McKeown.1994.
Emergent linguistic rules frominducing decisions trees: Disambiguatingdiscourse clue words.
In AAAI94, pages820-826.Simpson, A. and N. A. Fraser.
1993.
Blackbox and glass box evaluation of thesundial system.
In Proceedings ofthe ThirdEuropean Conference on SpeechCommunication a d Technology, pages1423-1426.Sparck-Jones, Karen.
1993.
What might be ina summary?
In Proceedings ofInformationRetrieval 93: Von der Modellierung zurAnwendung, pages 9-26,Universitatsverlag Knstanz.Sparck-Jones, Karen and Julia R. Galliers.1996.
Evaluating Natural LanguageProcessing Systems.
Springer.Strube, Michael and Udo Hahn.
1996.Functional centering.
In Proceedings ofthe34th Annual Meeting.
Association forComputational Linguistics.Suri, Linda Z. and Kathleen F. McCoy.
1994.RAFT/RAPR and Centering: Acomparison and discussion of problemsrelated to processing complex sentences.Computational Linguistics, 20(2).Taylor, Paul, Hiroshi Shimodaira, StephenIsard, Simon King, and Jaqueline Kowtko.1996.
Using prosodic information toconstrain language models for spokendialogue.
In ICSLP96.Vander Linden, Keith and Barbara DiEugenio.
1996.
A corpus study ofnegative imperatives in natural languageinstructions.
In COLING96.Walker, Marilyn A.
1989.
Evaluatingdiscourse processing algorithms.
InProceedings ofthe 27th Annual Meeting,pages 251-261.
Association forComputational Linguistics.Walker, Marilyn A.
1996.
The Effect ofResource Limits and Task Complexity onCollaborative Planning in Dialogue.Artificial Intelligence Journal,85(1-2):181-243.Walker, Marilyn A. and Steve Whittaker.1990.
Mixed initiative in dialogue: Aninvestigation i to discourse segmentation.In Proceedings ofthe 28th Annual Meeting,pages 70-79.
Association forComputational Linguistics.Wang, Michelle and Julia B. Hirschberg.1992.
Automatic lassification ofintonational phrase boundaries.
ComputerSpeech and Language, 6:175--196.Whittaker, Steve and Phil Stenton.
1988.Cues and control in expert clientdialogues.
In Proceedings ofthe 26th AnnualMeeting, pages 123-130.
Association forComputational Linguistics.Whittaker, Steve and Phil Stenton.
1989.User studies and the design of naturallanguage systems.
In Proceedings ofthe 4thConference ofthe European Chapter of theAssociation of Computational Linguistics,pages 116-123.Young, Michael R. 1997.
Generating concisedescriptions ofcomplex activities.
Ph.D.thesis, University of Pittsburgh.12
