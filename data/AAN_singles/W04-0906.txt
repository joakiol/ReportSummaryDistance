Question Answering Using Ontological SemanticsStephen BEALE, Benoit LAVOIE, Marjorie MCSHANE, Sergei NIRENBURG,Tanya KORELSKYInstitute for Language and InformationTechnologies (ILIT-UMBC)1000 Hilltop CircleBaltimore, MD, USA 21250{sbeale,marge,sergei}@umbc.eduCoGenTex, Inc.840 Hanshaw Rd, Suite 1Ithaca, NY, USA, 14850{benoit,tanya}@cogentext.comAbstractThis paper describes the initial results of anexperiment in integrating knowledge-basedtext processing with real-world reasoning in aquestion answering system.
Our MOQA?meaning-oriented question answering?system seeks answers to questions not in opentext but rather in a structured fact repositorywhose elements are instances of ontologicalconcepts extracted from the text meaningrepresentations (TMRs) produced by theOntoSem text analyzer.
The queryinterpretation and answer content formulationmodules of MOQA use the same knowledgerepresentation substrate and the same staticknowledge resources as the ontologicalsemantic (OntoSem) semantic text analyzer.The same analyzer is used for deriving themeaning of questions and of texts from whichthe fact repository content is extracted.Inference processes in question answering relyon ontological scripts (complex events) thatalso support reasoning for purely NLP-relatedpurposes, such as ambiguity resolution in itsmany guises.1 The TaskPeople would have no problem answeringquestions like Has Tony Hall met with UmidMedhat Mubarak?
?
provided they know whothese two people are and have witnessed such ameeting or read about it.
Even in the absence ofovert evidence about such a meeting, people mightconclude ?
based on additional knowledge theymight have about the protagonists ?
that such ameeting could or might have taken place.
Somecurrent automatic question-answering (QA)systems might be able to answer such a question ifthey found a sentence like Tony Hall met with(saw, talked with) Umid Medhat Mubarak on July3, 2003 in Baghdad  in some text.
But what if thetext data was more typical, like, for instance, thefollowing two excerpts:April 18, 2000  Associated Press.
RepresentativeTony Hall, a Democrat from Ohio, arrived inBaghdad on a four-day visit to investigate theplight of Iraqi people under sanctions aimed atforcing the government of Iraq to give up itsweapons of mass destruction?Umid Medhat Mubarak returned to Baghdad onApril 17, 2000 after a visit to Jordan and plans tomeet with a visiting US politician.To the best of our knowledge, no current systemcan input the above texts and return a reasonedresponse about the likelihood of a meeting betweenTony Hall and Mubarak.
But in a realisticenvironment there are even further complications.What if the first text was in English and the secondin Arabic?
Will the system be able to make even atentative connection between Tony Hall (in thefirst text) and US politician (in the second)?
Whatif the reference to US politician was omitted; i.e.
ifthe second text contained only the information thatUmid Medhat Mubarak was in Baghdad on April17, 2000?
The system would have to infer thepossibility of a meeting on the basis of knowledgeabout (at least) the social and professionalbackground of the protagonists and the timesinvolved.This paper describes a system that is able tomake connections and inferences such as theabove.
Its most important properties are questionanswering against structured data stored in a factrepository (FR) and the fact that it uses the sameprocessing machinery and knowledge resources a)to process texts for conversion into facts, b) tounderstand questions and c) to find answers toquestions.
We describe the underlying technologythat supports such a capability, including theproduction of text meaning representations(TMRs), reference and date resolution, factextraction and retrieval, and event scripts thatallow us to infer (with some degree of probability)certain events or states not directly stated in anytext.2 The Environment for QAOur question answering system consists of fourmain and one auxiliary processing modules (seeFigure 1).
The question analysis module takes asinput the text of a user?s question and produces itstext meaning representation (TMR, see belowfor an illustration) that contains representations ofinstances of ontological concepts to which theinput refers plus speaker-attitude andcommunicative information.
The TMR is input tothe question interpretation module that interpretsthe question in terms of its type and transforms itinto a formal query against the fact repository orthe ontology (see below).
(Note that the format ofthe database query is the same as that of thequestion TMR.
In general, all internalrepresentations of knowledge in our system, bothelements of knowledge support and results ofactual processing, are compatible with the contentand format of the ontology and fact repository.
)apctmtfaktcwiCsWmgapqiimplementation simply returns fragments of facts(and fact reasoning chains) that answer the initialquestion.
In the future, natural language generationwill be employed to produce textual responses.In order to answer complex questions in context,a system must extract, manipulate and generate themeaning of natural language texts.
Questionanswering against a structured knowledge base,especially when the latter contains interpretableknowledge elements (e.g., instances of events andobjects defined in an ontology, not uninterpretedtext strings), can attain better results than QA thatworks by manipulating templates filled withsnippets of actual texts ?
at the least because of theadded benefit of disambiguation and referenceresolution.
The prerequisite for such a system isthe existence of a structured knowledge base usedas a source of answers to questions.
In a realapplication, the knowledge must be ample anddynamic, so that the knowledge resources must beconstantly and promptly augmented.
This is notpractical if knowledge is acquired entirely bypeople.
Automating structured knowledgeacquisition from open text is, therefore, anecessary condition for the success of an advancedQA application.
The CSK module of our system is Figure 1.
The top-level architecture of thesystem.Thus, text meaning representation in ourpproach ?doubles?
as the basis for reasoningrocesses.
The query serves as input to answerontent determination.
This latter module useshe knowledge resources of the system to infer theost preferred answer, once again, formulated inhe TMR metalanguage.
If an answer cannot beound, the system has the option to call theuxiliary module for creating structurednowledge, CSK.
The CSK module works also inhe background mode, using text sources toontinuously update the fact repository (in theork reported here there has been some humannvolvement in the process of TMR production forSK; we plan to study the degradation of theystem when used in a fully automatic mode).hen called by the answer content determinationodule, the CSK module analyzes texts andenerates entries in the fact repository that help tonswer the original question.
The text analysisrocess in this module is the same as that used inuestion analysis.
The final module in the systems answer formulation.
The currenta step toward this functionality, albeit not yet in afully automatic way.
At this point, we rely onTMRs that are obtained automatically butimproved through human interaction (seeNirenburg et al 2004 for details).
Note that fullyautomatic methods for creating structuredknowledge of a quality even remotely approachingthat needed to support realistic QA do not at thispoint exist.
Few of the numerous current andrecent machine learning and statistical processingexperiments in NLP deal with the analysis ofmeaning at all; and those that do address partialtasks (e.g., determining case role fillers in terms ofundisambiguated text elements in Gildea andJurafsky 2002) in a rather ?knowledge-lean?manner.
The results are very far away indeed fromeither good quality or good coverage, either interms of phenomena and text.
We believe that ourapproach, using as it does statistical as well asrecorded-knowledge evidence for extracting,representing and manipulating meaning is the mostpractical and holds the most promise for the future.Indeed, it is not even as expensive as many peoplebelieve.3 The Knowledge Support InfrastructureThe process of deriving TMRs from text isimplemented in our Ontosem text analyzer.Semantic analysis in OntoSem is described insome detail in Nirenburg and Raskin 2004;Nirenburg et al, 2004; Beale et al 1995, 1996,2003; Mahesh et al 1997.
Our description herewill be necessarily brief.
Also note that theanalysis process is described here as if it were astrict pipeline architecture; in reality, semanticanalysis is used to inform and disambiguatesyntactic analysis, for example, in cases ofprepositional phrase attachment.Text analysis in OntoSem relies on the results ofa battery of pre-semantic text processing modules.The preprocessor module deals with mark-up inthe input text, finds boundaries of sentences andwords, recognizes dates, numbers, named entitiesa lagtelsocaco(tcddiamaproduced in OntoSem using a variety of?microtheories,?
to produce extended TMRs.
Atboth steps, the analyzer has to deal with ambiguity,incongruity between the input and the expectationsrecorded in the static knowledge sources (SKSs),unknown words, and non-literal language.
In arecent evaluation, the basic analyzer was shown tocarry out word sense disambiguation at over 90%and semantic dependency determination at 87% onthe basis of correct syntactic analysis and onsentences of an average length of over 25 wordswith 1.33 unknown words on average per inputsentence (see Nirenburg et al, 2004).
While not nd acronyms and performs morphologicanalysis.
Once the morphological analyzer hasenerated the citation forms for word forms in aext, the system can activate the relevant lexicalntries in its lexicons, including the onomasticon (aexicon of proper names).
Figure 2 presents aample of preprocessor output.Figure 2: Sample preprocessor outputFigure 3: Sample parser output, in graphicalThe task of syntactic analysis (see Figure 3) inntological semantics is, essentially, to determinelause-level dependency structures for an input textnd assign grammatical categories to clauseonstituents (that is, establish subjects, directbjects, oblique objects and adjuncts).Semantic analysis proper uses the informationmutual constraints) in the active lexicon entries,he ontology and the results of earlier processing toarry out, at the first step, word senseisambiguation and establish basic semanticependencies in the text.
The results are recordedn basic TMRs (see below).
At the next step, thenalyzer determines the values of the variousodalities, aspect, time, speech acts, speakerttitudes and other knowledge elements that areperfect, these results show promise as training datafor machine learning work.The OntoSem ontology provides ametalanguage for describing the meaning of thelexical units in a language as well as for thespecification of meaning encoded in TMRs.
Theontology contains specifications ofconcepts corresponding to classes ofthings and events in the world.
It is acollection of frames, or named sets ofproperty-value pairs, organized into ahierarchy with multiple inheritance.
Theexpressive power of the ontology and the TMR isenhanced by multivalued fillers for properties,implemented using the ?facets?
DEFAULT, SEM,VALUE, and RELAXABLE-TO, among others.
At thetime of this writing, the ontology contains about6,000 concepts (events, objects and properties),with, on average, 16 properties each.
Temporallyand causally related events are encoded as valuesof a complex event?s HAS-EVENT-AS-PARTproperty.
These are essentially scripts that provideinformation that is very useful in general reasoningas well as reasoning for NLP (e.g., Schank andAbelson 1977, Lin and Hovy 2000, Clark andPorter 2000).
We use scripts in the answer contentdetermination module of the question answeringsystem.
Figure 4 illustrates a rather simple scriptthat supports reasoning for our example questionanswering session.The OntoSem lexicon contains not onlysemantic information, it also supportsmorphological and syntactic analysis.Semantically, it specifies what concept, concepts,property or properties of concepts defined in theontology must be instantiated in the TMR toaccount for the meaning of a given lexical unit ofinput.
At the time of writing, the latest version ofthe English semantic lexicon includes over 12,000handcrafted entries.
These entries cover some ofthe most complex lexical material in the language?
?closed-class?
grammatical lexemes such asconjunctions, prepositions, pronouns, auxiliary andmodal verbs, etc.
as well as about 3,000 of theFor lack of space, we will not be able to discussall the representational and descriptive devicesused in the lexicon or the variety of ways in whichsemantic information in the lexicon and theontology can interact.
See Nirenburg and Raskin(2004, Chapters 7 and 8) for a discussion.MEET-WITH(AGENT (VALUE $VAR1))(THEME (VALUE $VAR2))(LOCATION (VALUE $VAR3))(TIME(VALUE $VAR4))PRECONDITIONS(AND(LOCATION(DOMAIN (VALUE $VAR1))(RANGE (VALUE $VAR3))(TIME (VALUE $VAR4)))(LOCATION(DOMAIN (VALUE $VAR2))(RANGE (VALUE $VAR3))(TIME (VALUE $VAR4))))EFFECTS(SPEECH-ACT(AGENT (VALUE $VAR1))(BENEFICIARY (VALUE $VAR2)))(SPEECH-ACT(AGENT (VALUE $VAR2))(BENEFICIARY (VALUE $VAR1)))COME(AGENT (VALUE $VAR1))(DESTINATION (VALUE $VAR2))EFFECTS(LOCATION(DOMAIN (VALUE $VAR1))(RANGE (VALUE $VAR2)))LOCATION(DOMAIN (VALUE $VAR1))(RANGE (VALUE $VAR2))EFFECT-OF(COME(AGENT (VALUE $VAR1))(DESTINATION (VALUE $VAR2)))Figure 4: A sample script,presented in a simplifiedpresentation format.The English onomasticon (lexicon of propernames) currently contains over 350,000 entriessemantically linked to ontological concepts; it isincreasing in size daily by means of semi-automated knowledge-extraction methods.The TMR (automatically generated but shownhere in a simplified presentation format) for a shortsentence (He asked the UN to authorize thewar) from a recently processed text about ColinPowell is presented below.
The numbers associatedwith the ontological concepts indicate instances ofthose concepts: e.g., REQUEST-ACTION-69 meansthe 69th time that the concept REQUEST-ACTION hasbeen instantiated in the world model used for, andextended during, the processing of this text orcorpus.REQUEST-ACTION-69AGENT   HUMAN-72THEME   ACCEPT-70BENEFICIARY   ORGANIZATION-71SOURCE-ROOT-WORD  askTIME     (< (FIND-ANCHOR-TIME))ACCEPT-70THEME   WAR-73THEME-OF   REQUEST-ACTION-69SOURCE-ROOT-WORD   authorizeORGANIZATION-71HAS-NAME   UNITED-NATIONSBENEFICIARY-OF     REQUEST-ACTION-69SOURCE-ROOT-WORD  UNHUMAN-72HAS-NAME  COLIN-POWELLAGENT-OF   REQUEST-ACTION-69SOURCE-ROOT-WORD   he ; ref.
resolution doneWAR-73THEME-OF     ACCEPT-70SOURCE-ROOT-WORD  warmost frequent main verbs.
We illustrate thestructure of the lexicon entry on the example of thefirst verbal sense of alert:alert-v1cat    vmorph  regularex     "He alerted us to the danger" The above says that there is a REQUEST-ACTIONevent whose agent is HUMAN-72 (Colin Powell),whose beneficiary is ORGANIZATION-71 (UnitedNations) and whose THEME is an ACCEPT event.That ACCEPT event, in turn, has the THEME WAR-73.
Note that the concept ACCEPT is not the sameas the English word accept: its human-orienteddefinition in the ontology as ?To agree to carry outan action, fulfill a request, etc?, which fits wellhere.syn-strucsubject   $var1root    "alert"indirectobject  $var2pp   (opt +)root  "to"object $var3sem-strucWARNagent   ^$var1beneficiary  ^$var2theme   ^$var3The Fact Repository contains a list ofremembered instances of ontological concepts.
Forex theco iesforonrepdifinseatypareunen4demoIntFigexquananThe TMR of the question, the result of theQuestion Analysis module, is displayed in the ample, whereas the ontology containsncept CITY, the fact repository contains entrLondon, Paris and Rome; and whereas thetology contains the concept WAR, the factository contains the entry WWII.
The mainference between an ontological concept and itstance is the nature of the fillers of properties forch.
In the former, the fillers of properties are,ically, overridable constraints; in the latter, theyactual values (when known), or they are leftfilled when not known.
A simple fact repositorytry is illustrated below:HUMAN-33599NAME George W. BushALIASGeorge Bush,Figure 5: Querying about a known person.President Bush,George W,the president of the United States,the US presidentSOCIAL-ROLE  PRESIDENTGENDER        maleNATIONALITY        NATION-213 ;(USA)DATE-OF-BIRTH July 6, 1946spouse  human-33966 ;Laura BushThe Question Answering ModulesReferring back to Figure 1, we now will brieflyscribe the three central question answeringdules of Question Analysis, Questionerpretation and Answer Content Determination.ure 5 shows a question answering session thatemplifies these three stages.
The user enters aestion in the ?Natural Language Query?
text boxd clicks on the ?Submit?
button.
The OntoSemalyzer is then invoked to analyze the question.Query Analysis Details box.
Obviously, not manyof the details can be seen in these figures, but inthe interface, the user can scroll through the TMRoutput.
We will, in fact, be integrating our existingTMR and Fact graphical browsers into thisinterface in the near future.
The results of the nextmodule, Question Interpretation, are then displayedin the Query Paraphrase box.
From there, the factrepository is queried, an answer is returned(perhaps utilizing the inference techniques to bedescribed), and the supporting fact (or factreasoning chain) is displayed in the Answer Detailsbox.
Below we present three example sessions, thethird of which will be the basis for the moredetailed description of the three modules.For this discussion, we concentrate on thefollowing three sentences that have been processedby the CSK module and the facts that were derivedfrom them and stored in the fact repository (thefacts are represented using instances of ontologicalconcepts, of course):1.
Tony Hall Met with Umid Medhat Mubarak.2.
Tony Hall arrived in Baghdad (on Thursday).3.
Ali Atwa was in Baghdad (on April 18 andlater).In Figure 5, the question is ?Who is Tony Hall?
?In its current state, the system simply finds andresponds with the stored fact about Tony Hall,which includes the information about his arrival(the COME-1003 event derived from sentence 2)and the MEET-WITH-1001 event (derived fromsentence 1).
Figure 6 shows the results of thequery, ?Did Tony Hall meet with Umid MedhatMubarak??
A matchingfact was found in the FR(derived from sentence1) and is displayed in theAnswer Details.
Figure 7presents a more complexcase.
The query is ?DidTony Hall meet with AliAtwa??
The FR containsno fact that can directlyanswer this question.The system uses factsfrom sentences 2 and 3to return the possibleinference MEET-WITH-???
(the tentative natureof the inference ismarked by the -??
?appended as the instancenumber; and in theFigure 6: Querying about a known event.future, we will also generate a numerical valuereflecting the confidence level of the inferencesand sub-inferences).
The supporting reasoningchain is also displayed.
We will now use thisexample to discuss the three main questionanswering modules.The Question Analysis module takes the inputtext question and produces the TMR using theresources described in section 3 above.
Figure 8illustrates the resulting TMR in a graphicalbrowser.
(One can inspect the content of thevarious concept instances ?
e.g., OBJECT-56 ?
byclicking on graphical objects representing them).The main thing to point out is that the TMRinstantiates a MEET-WITH event which is the basisfor querying the FR, itself comprised of factsrepresented by ontological concept instances.The Question Interpretation module thenderives the canonical form of aninput question TMR (determiningquestion type and reformulatingthe content of an actual questionin a standard format), and appliesreference resolution.
The answerdisplayed in Figure 7 involvesreformulating the query Did $Xmeet with $Y?
to Find meetingsinvolving $X and $Y or moreparticularly, Find meetings wherea person named $X is AGENT anda person named $Y isBENEFICIARY, and meetingswhere a person named $Y isAGENT and a person named $X isBENEFICIARY.
Such a query canbe specified as a standard DBMSquery for the actual search.
The knowledge that weare dealing with people and knowledge of theirnames is used to help resolve the referencesbetween the instances Tony Halland Ali Atwa appearing in thequery and the references to TonyHall and Ali Atwa that may bestored in the fact repository.
Wewill report on the actual methodsof question interpretation andreference resolution we useseparately.The Answer ContentDetermination module isinvoked next.
The possiblequeries constructed in theprevious module are processed.First, direct queries are attempted.If an answer is found, it is returned directly.
In thisexample, no fact directly states that Tony Hall metAli Atwa.
Scripts are then activated which allow usto reason about the question.
In the script of Figure4, the preconditions of a MEET-WITH event includeboth participants having to be in the same place atthe same time.
This will invoke a series of queriesthat will determine if Tony Hall and Ali Atwa wereindeed in the same location at the same time.
Ingeneral, if the preconditions of an event aresatisfied, we infer that the event itself possiblytook place.
In this case, the fact that Ali Atwa wasin Baghdad is present in the FR by virtue ofsentence 3 above.
Using this knowledge, thesystem seeks to prove that Tony Hall was also inBaghdad at the same time.
Once again, there is nodirect fact that states this.
However, the facts aboutTony Hall include information that he arrived inBaghdad at a certain time (at the present time, wedo not attempt to match the times of the facts,although this will be a focus of our ongoing work).Figure 7: Querying about an unknownevent.Matching times ofThis information is representedby the COME-1003 fact.
We can look up COME inthe script of Figure 4 and see that one effect of aCOME event is that the agent?s location becomesthe destination of the COME event.
In general, wecan use known facts to infer additional facts abouttheir effects.
In this case, we can infer that TonyHall was, in fact, in Baghdad, which, in turn,allows us to make the top level inference that hemight have met with Ali Atwa, who we previouslydetermined was also in Baghdad.
We are awarethat the conjecture about the possible meetingshould involve additional knowledge of thebackground and histories of the participants (e.g.,if a cobbler and a head of state are in the samep ap go eoriTi??
?epedirect the inference process ?
even we are fullyaware of the abductive, defeasible nature of thisknowledge.The inference steps described above weredirected by the information in the MEET-WITH andCOME scripts.
Also, known facts about one of theparticipants, Tony Hall, were used to direct queriesto support a possible inference.
Obviously, muchwork remains to be done.
We must populate a largefact repository that should include a largecollection of facts about individuals as well asplaces, organizations and event instances.
At thistime, we are starting to use our TMR productionenvironment for extracting facts.
We hope to beable to report on the progress of this work at theworkshop.5 ConclusionWe have presented the first experiment with aknowledge-based QA system in which textprocessing is integrated with reasoning on the basisof shared knowledge and processinginfrastructures.
Indeed, the same processing andknowledge resources in the system carry outreasoning for the purposes of QA and reasoningthat is necessary to create a high-qualityunambiguous text meaning representation itself.While this is just a small experiment, we havespecific and, we believe, realistic plans for scalingthis system up ?
through automatic population ofthe fact repository, semi-automatic enlargement ofthe lexicons and the ontology and expansion of theinventory of scripts.Figure 8.
Sample output viewed through theTMR browserWe believe that integrating a comprehensivethroughput system for an advanced application,even one in which some of the modules are still ona relatively small scale, is a very important kind ofwork in our field.
It tackles real problems head on, lace at the same time, that does not implyotential meeting between them).
We are workinn enhancing the knowledge (centered on thntological MEET-WITH script) to improve sucheckoning.In a separate article in preparation, we will gonto much more detail about the reasoning process.here are obviously many additional issues,ncluding:events.
Our time resolution meaningprocedures enable this;Assigning probabilities to inferences.
Forexample, if two people were in the same room,the possibility of their meeting is much higherthan if they were in the same country;Controlling the inference process.With regard to this last issue, the OntoSemnvironment provides a useful mechanism.
Inarticular, the scripts that we are developingncode expectations and are meant to constrain andwithout resorting to a rather defeatist ?
thoughquite common in today?s NLP ?
claim that certaingoals are infeasible.ReferencesS.
Beale, S. Nirenburg and K. Mahesh.
1995.Semantic analysis in the Mikrokosmos machinetranslation project.
In Proceedings of the 2ndSymposium on Natural Language Processing,Kaset Sart University, Bangkok, Thailand.S.
Beale, S. Nirenburg and K. Mahesh.
1996.Hunter-Gatherer: Three search techniquesintegrated for natural language semantics.
InProceedings of the 13th National Conference onArtificial Intelligence.
Portland, OR.S.
Beale, S. Nirenburg and M. McShane.2003.
Just-in-time grammar.
In ProceedingsHLT-NAACL-2003, Edmonton, Canada.P.
Clark and B. Porter.
2000.
$RESTAURANT Ren-visited: A KM implementation of a compositionalapproach.
Technical Report, AI Lab, Universityof Texas at Austin.D.
Gildea and D. Jurafsky.
2002.
Automaticlabeling of semantic roles.
ComputationalLinguistics 28(3).
245-288.C.
Lin and E. H. Hovy.
2000.
The automatedacquisition of topic signatures for textsummarization.
In Proceedings of the COLINGWorkshop on Text Summarization.
Strasbourg,France.K.
Mahesh, S. Nirenburg and S. Beale.
1997.
Ifyou have it, flaunt it: Using full ontologicalknowledge for word sense disambiguation.
InProceedings of Theoretical and MethodologicalIssues in Machine Translation (TMI-97).
SantaFe, NM.S.
Nirenburg and V. Raskin.
2004.
OntologicalSemantics.
MIT Press.S.
Nirenburg, M. McShane and S. Beale.
2004.Evaluating the performance of OntoSem.
InProceedings ACL Workshop on Text Meaningand Interpretation, Barcelona.R.
Schank and R. Abelson.
1977.
Scripts, plans,goals, and understanding.
Hillsdale, NJ:Erlbaum.
