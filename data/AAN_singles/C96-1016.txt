Measuring Semantic CoverageSergei Nirenburg, Kavi Mahesh and Stephen BealeComput ing  Research  L~bor~toryNew Mex ico  St~tte Un ivers i tybt~s Cruces ,  NM 88003-0001USAserge i ;m~hesh;sb{~cr l .nmsu.eduAbstractThe developlnent of natural languageprocessing systems is currently driven toa large extent by measures of knowledge-base size and coverage of individual phe-nomena relative to a corpus.
While thesemeasures have led to significant advancesfor knowledge-lean applications, theydo not adequately motivate progress incomputational semantics leading to thedevelopment of large-scale, general pur-pose NLP systems.
In this article, weargue that depth of semantic represen-tation is essential for covering a broadrange of phenomena in the computa-tional treatment of language and propose(lepth as an important additional dimen-sion for measuring the semantic cover-age of NLP systems.
We propose anoperationalization of this measure andshow how to characterize an NLP systemalong the dimensions of size, corpus cov-erage, and depth.
The proposed frame-work is illustrated using sever~fl promi-nent NLP systems.
We hope the prelim-inary proposals made in this article willlead to prolonged ebates in the field andwill continue to be refined.1 Measures of Size versusMeasures of DepthEvaluation of current and potential performanceof' an NLP system or method is of crucial impor-tance to researchers, developers and users.
Cur-rent performance of systems is directly measuredusing a variety of tests and techniques.
Often, asin the case of machine translation or informationextraction, an entire "industry" of evaluation getsdeveloped (see, for example, ARPA MT Evalua-tion; MUC-4 ).
Measuring the performance of anNLP method, approach or technique (and throughit the promise of a system based on it) is more dif-ficult, as judgments must be made about "blameassigmnent" and the impact of improving a varietyof system components on the overall future per-formance.
One of the widely accepted measuresof potential performance improvement is the fea-sibility of scaling up the static knowledge sourcesof an NLP system its grammars, lexicons, worhtknowledge bases and other sets of language de-scriptions (the reasoning being that the larger thesystem's grammars and lexicons, the greater per-centage of input they would be able to matchand, therefore, the better the performance of thesysteml).
As a result, a system would be con-sidered very promising if its knowledge sourcescould be significantly scaled up at a reasonableexpense.
Natm'ally, the expense is lowest if acqui-sition is performed automatically.
This consider-ation and the recent resurgence of corpus-basedmethods heighten the interest in the automationof knowledge acquisition, llowever, we believethat such acquisition should not 1)e judged solelyby the utility of acquired knowledge for ~ partic-ular application.A preliminary to the sealability estimates is ajudgment of the current coverage of a system'sstatic knowledge sources.
Unfortunately, judg-ments based purely on size ace often mislead-ing.
While they may be sufficiently straightfor-ward for less km)wledgeAntensive methods usedin such applications as information extraction andretrieval, part of speech tagging, bilingual corpusalignment, and so on, the saute is not true aboutmore rule- and knowledge-based methods (suchas syntactic parsers, semantic analyzers, seman-tic lexicons, ontological world models, etc.).
It ixwidely accepted, for instance, that judgments ofthe coverage of a syntactic grammar in terms ofthe number of rules are tlawed.
It is somewhatless self-evident, however, that the number of lex-icon entries or ontology concepts is not an ade-quate measure of the quality or coverage of NLPa Incidentally, this consideration eontributes toevMuation of current perforntance as well.
In the ab-sence of actual evaluation results, it is customary toc|aim the utility of the system by simply mention-ing tit(: size of its knowledge sources (e.g., "over 550grammar rules, over 50,000 concepts in the ontologyand over 100,00(I word senses in the dictionary").33systems.
A.n adequate measure of these must ex-amine not only size and its scalability, but alsodepth of knowledge along with its scalability.
Inaddition, these size and depth measures cannotbe generalized over the whole system, but mustbe directly associated with individual areas thatcover the breadth of NLP problems (i.e.
morphol-ogy, word-sense ambiguity, semantic dependency,coreference, discourse, semantic inference, etc.
).And finally, the most helpfld measurements willnot judge the system solely as it stands, but mustin some way reflect the ultimate potential of thesystem, along with a quantification of how far ad-ditional work aimed at size and depth will bringabout advancement toward that potential.In this article, we attempt to formulate mea-sures of coverage important o the developmentand evaluation of semantic systems.
We proceedh'om the assumption that coverage is a function ofnot only the number of elements in (i.e., size of)a static knowledge source but also of the amountof information (i.e., depth)  and the types of in-formation (i.e., b readth)  contained in each suchelement.
Static size is often emphasized in evalu-ations with no attention paid to the often very in-significant amount of information associated witheach of the many "labels" or primitive symbols.We snggest a starting framework for measuringsize together with other significant dimensionsof semantic coverage.
In particular, the evalu-ation measures we propose reflect the necessarycontribution of the depth and breadth of seman-tic descriptions.
Depth and breadth of seman-tic description are essential for progress in com-putational semantics and, ultimately, for build-ing large-scale, general purpose NLP systems.
Ofcourse, for a number of applications a very lim-ited semantic analysis (e.g., in terms of, say, adozen separate features) may be adequate for suf-ficiently high performance.
However, in the longrun, progress towards the ultimate goal of NLP isnot possible without depth and breadth in seman-tic description and analysis.There is a well-known belief that it is not ap-propriate to measure success of NLP using field-internal criteria.
Its adherents maintain thatNLP should be evaluated exclusively throughevaluating its applications: information retrieval,machine translation, robotic planning, human-computer interaction, etc.
(see, for: example, theProc.
of the Active NLP Workshop; ARPA MTEvaluation).
This may be true for NLP users, butdevelopers must have internal measures of success.This is because it is very difficult to assign blamefor the success or failure of an application on spe-cific components of an NLP system.
For exam-ple, in reporting on the MUC-3 evaluation efforts,Lehnert and Sundheim (1991) write:A wide range of language processingstrategies was employed by the top-scoring systems, indicating that manynatnral language-processing techniquesprovide a viable foundation for sophis-ticated text analysis.
Further evaluationis needed to produce a more detailed as-sessment of the relative merits of specifictechnologies and establish true perfor-mance limits tbr automated informationextraction.
\[emphasis added.\]Thus, evaluating the information extraction ap-plication did not provide constructive criticism onparticular NLP techniques to enable advances inthe state of the art.
Also, evaluating an appli-cation does not directly contribute to progress inNLP as such.
This is in part because a majorityof current and exploratory NLP systems are notcomplete nough to fit an application but ratherare devoted to one or more of a variety of com-ponents of a comprehensive NLP system (statice.g., lexicons, grammars, etc.
; or dynamic e.g.,an algorithm for" treating metonymy in English).1.1 Cur rent  Measures  of  CoverageSuccess in NLP (including semantic analysis andrelated areas) is currently measured by the follow-ing criteria:?
Size of static knowledge sources: A merenmnber indicating the size of a knowledgesource does not tell us much about the cover-age of the system, let alne its semantic apa-bilities.
For example, most machine readabledictionaries (MRI)) are larger than compu-tational exicons but they are not usable for:computational semantics.?
Coverage of corpus, either blanket cover:-age ("56% of sentences were translated cor-rectly") or resolution of a certain phe-nomenon (" 78% of anaphors were determinedcorrectly").
These measures are ofl;en mis-leading by themselves since what may be cov-ered are just one or two highly specific phe-nomena such as recognizing place or prod-uct names (i.e., limited breadth).
NLP is notyet at a stage where "covering a corpus" canmean "analyzing all elenmnts of meanings oftexts in the corpus."
It may be noted that"correctly" is a problematic term since peo-ple often have difficulty judging what is "cor-rect" (Will, 1993).
Moreover, correctness isorthogonal to the entire discussion here sincewe would like to increase semantic coveragealong various dimensions while maintainingan acceptable degree of correctness.
On thesame lines, processing efficiency (often spec-ified in terms such as "A sentence of length9 takes 750 milliseconds to process") is alsomore or less orthogonal to the dimensions wepropose for measuring semantic overage.
In-creasing semantic (:overage would be Ntile if34l 'henome l)t~.
'ired S late" C .
r rent  S ta teiiiii , , -~ ~ : : .
: i  " Knowled e BaseFigure 1: Dimensions of Semantic Coverage: (hlr-rent and Desired l)irectionsprocessing became xponentially expensive asa result.Figure 1 shows the dimensions of size andbreadth (or phenomenon coverage) along tit(', hor-izontal plane.
Depth (or richness) of a semanticsystem is shown on the vertical axis.
We believethat recent progress in NLP with its emphasis oncorpus linguistics and statistic~d methods has re-suited in a significant spread akmg the horizontalplane but little been done to grow the Iield in thevertical dimension.
Figure 1 also shows the de-sired state of computational semantics advmlcedalor|g each of the three dimensions hown.
IfWe proceed from the assumption that high-quality NLI ) systems require opt imum coverageon all three scales, the|| apparently different roads(-an be taken to that target.
The speetrmn ofchoices ranges from developing all three dimen-sions more or less simultaneously to taking careof them in turn.
As is often the case in king-term high-risk enterprises, inany researchers optto start out with acquisition work which promisesshort-term gains on one of the coverage dimen-sions, with little thought about further steps.
Of_ten the reason they cite can be summarized bythe phrase "Science is the art of the possible.
"This position is quite defensihle .... if no claimsare made about broad semantic (:overage.
Indeed,it is quite legitimate to study a particular lan-guage phenomenon exclusively or to cover largechunks of the lexis of a language in a shallow man-ner.
IIowever, for practical gains in large-scalecomputational-selnantie applications one needs toachieve results on each of the three dimensions ofcoverage.1.2  Des iderata  for Large-Sca leCo lnputat iona l  Semant icsOnce the initial knowledge acquisition canq)aignfor a I)articular apt)lication has been concluded,the following crucial scalability issues 2 ira|st beaddressed, if any t|nderstanding of the longer-termsignificance of the research is sought:?
domain independence: scalability to new (lo-mains; general-purpose Nl,l )?
language independence: sealability acrosslanguages?
phenolnenon coverage: sealability to newphenomena; going beyond core semanticanalysis; ease of integrating component pro-eesses and resources.?
application-independence: sealability to newapplications; toolkit o\[' NLP techniques ap-plicable to any t~sk.We believe that coverage in terms of the det)thand breadth of the knowledge given to an NLI )system is mandatory for attaining the above goalsin the long run.
Such coverage is best esti(natednot in terms of raw sizes of lexicons or world mod-els but rather through the availability in them ofinformation ecessary for the treatment of a w>riety of l)henomena in natural language issuesrelated to semantic dependency bull(ling, lexicaldisambiguation, semantic onstraint racking andrelaxation (for the cases of unexpected input, in-cluding non-li~eral language as well as treatmentof unknown lexis), reference, pragmatic impactand discourse structure.
The resolution of theseissues is at the core of t)ost-syntactic text process-ing.
We believe that one can treat the al)ove phe-nomena only by acquiring a broad range of rele-vant knowledge lements for the system.
One nse-flfl measure for sufficiency of infbrmation would bean analysis of kinds of knowledge necessary to gen-erate a text (or (liMog) meaning representation.For applications in which more procedural com-putational semantics is l)refl~'rable, a corresponding measure of sutliciency should be developed.There exist other, broader desiderata which areapplicable to any All systetn.
They include con-cerns about system robustness, correctness, andefficiency which are orthogonal to the above is-sues.
EquMly important but more broadly appli-cable are considerations of economy and ease ofacquisition of knowledge sources for example,reducing the size of knowledge bases and sharingknowledge across applications.2At present, se~dability is considered in the fieldahnost exclusively ~ts propagation o\[ the nulnber ofentries in the NLP knowledge bases, not the quantityand quality of information inside each such entry.852 How to  Reason  about  Depth ,Breadth  and  S izeA useful measure of semantic coverage must in-volve measurement along each of the three dimen-sions with respect o correctness (or success rate)and efficiency (or speed).
In this first attemptat a qualitative metric, we list questions relevantfor assigning qualitative ("tendency") scores toan NLP system to measure its semantic overage.Our experience over the years has led us to thefollowing sets of criteria for measuring semanticcoverage.
Itowever, we understand that the fol-lowing are not complete or unique; they are rep-resentative of the types of issues that are relevantto measuring semantic overage.2.1 Lex ica l  Coverage? '
lb what extent do entries share semanticprimitives (or concepts) to represent wordmeanings?
What is the relation between thenumber of semantic primitives defined andthe number of word senses covered??
What is the size of the semantic zones of theentry?
tlow many semantic features are cov-ered??
How many word senses from standardhuman-oriented ictionaries are covered inthe NLP-oriented lexicon entry??
What types of information are included?- seleetional restrictions- constraint relaxation informationsyntax-semantics linking- collocations- procedural attachments for contextualprocessing-- stylistic parameters- aspectual, temporal, modal and attitu-dinal meanings-o ther  idiosyncratic information aboutthe word?
and, finally, the total number of entries in thelexicon.2.2 Onto log ica l  CoverageThe total number of primitive labels in a worldmodel is not a useful measure of the semantic ov-erage of a system.
At least the following consid-erations must be factored in:?
The number of properties and links definedfor an individual concept?
Number of types of non-taxonomic relation-ships among concepts?
Average number of links per concept: "con-nectivity"?
Types of knowledge included: defaults, selec-tional constraints, complex events, etc.?
Ratio of number of entries in a lexicon tonumber of concepts in the ontology?
and, finally, total number of concepts in theontology.2.3 Measur ing  Breadth  of  Mean ingRepresentat ionsApart from lexical and ontological coverage, thedepth and breadth of the meaning representationsconstructed by a system are good indicators ofthe overall semantic overage of the system.
Tilenumber of different ypes of meaning elements in-cluded fl'om the following set provides a reason-able measure of coverage:?
Argument structure only?
Template filling only?
Events and participants?
Thematic role assignments?
Time and temporal relations?
Aspect?
Properties: attributes of events and objects;relations between events and objects.?
R,eference and coreference?
Attitude, modality, stylistics?
Quantitative, comparative, and other mathe-matical relations* Textual relations and other discourse rela-tions?
Multiple ambiguous interpretations* Propositional and story/dialog structure3 Measur ing  Semant ic  Coverage:ExamplesFigure 2 shows the approximate position ofseveral well-known approaches and systems (in-cluding a possible Cyc-based system) in the 3-dimensional space of semantic overage.
We havechosen representative systems fl'om the differentapproaches for lack of precise terms to name tlleapproaches.How do the approaches illustrated in Figure 2rate with respect o the metrics suggested above'?When estimating their profiles, we thought eitherabout some representative systems belonging toan approach or thought of the properties of a pro-totypical system in a particular paradigm if noexamples presented themselves readily.
In the in-terests of space, we consider the above criteriafor measuring semantic overage but only providebrief summaries of how each system or approachis located along the dimensions of depth, breadthand size.The schema-based reasoner, Boris (behnert etal, 1983) was used as a prototype system for the8g+ +++++++++{ !!
}: i + :-+:::+ ::+.++++++5++ i ~i+ ++++++ +++5+i+i+?+++++++++++5+++++i+++ 5 i  +i  ++ +i~   +ii+i{5++++++51+5~+++++++ ~!
: : :  :, +77:+}+ ++ + .................................................... ~'ii ............. , ................................................ Ix ~,tA .................i{ {!
ii ii', :: : ?
{iii{i{5~!{iii!
!5 ~:i!
5{ 5!
7!
!5 ~ ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: ~ i i { {i{i:!Si{ i:~:s~;ii~:::::!
:!~:!i ~ ~/:i !
!
!i!::?!
: !
::::   = ::~:  ::: :::::::::::::::: ::::::::::::::::::::::::::::: ~ 5~!!!
: ~ :~ :!
~s :~ ~::: ?~:!i.
'.,~-: :::: ......... :":~ ::: : ,:{!~i{~i: ::::: :~::::::::: ::::::::::::~::~:.
:::::::::::::::::::::::::::::: ~ :::: : : : : : : : : : : : : : : : : : : : : : : :2: :::::!:!::!!!
:i: ::::::::::::::::::::::::::::::::::::::::::: ::::::::::::::::::::::::::::::::::::::::::::::::: :, .
:::i:2: ::::!
:: ::f?
::::i: :...............................................................
I "2; ": .. ....... ~*~ ............. : ~ ................. ~ .........?!:5!ii!i!iii!
:ili ~iiiiliiii:i:i :i:i:i:i:i:5:!
:i:?i:!~ ' !
!
!
!
!
i i  .~  ::::: :::: ::?,~.
::::::::::::::::::::::::::{::{!{:::i:i::{!
;:i:i ~:~:i:i::::i{i:: i i :~!~:i~\[: : i  .
::::::::: t,,:xx::::::::::: : : : : : : : : : : : : : : : : : : : : : : : : :  ::::::::::~:: :,"~::::::hl :::::::::::::::% :::::::::++::~:::i:i:::::!!
: z .
.
: : : : : : : : : : : : : : : : : : : : : : : : : : :i~ili~}iii::iil)+::i: ~t{i::~:ii::;:i: ~}:!}i}:.~}!
!~i!i~i} ~ .ilili~ii~iii~i #.
~:i:~:}ii~ v+} } if< ::::::::::::5::::: ;{:::::::::::::::::: ::::::::::::::::::::::: :::::::::: :~ : : : : : : : : : : :  qX : : : : : : : : : : : : : : : : : : : : : : :a {{+++++++++++++++++++{++t++++++++++++++++~+-:+ ~ i~} + i~2~+ ;++g~i ~ i)+ ;~ i ii +++)Y    :+~++ :: ::: :::::: ;;.
;~ ;;~a::::: ~,<E'I:~ :::: N~i~++:i/~i~: :: :~  'a::+" *~::i:i:i :;+JlKI ~ +p-'~' :~:~9.
:~  :: ~i<~.
+ ~.~t : :::::Figure 2: l)imensions of Semantic Coverage: Cur-+rent att(t l)esired l)ireetionsdomain- and task-del)endent, AI-style, schema-based NLP systc'm.
It may I>e considered an ex-treme example of a system with deep, rich knowledge of its, rather narrow, worhl in which cowwinglanguage phenomena is nee(ted only inasmuch as itsupports general reasoning.
Boris was able to pro-cess a very smMl number of texts sutticiently forits goMs.
The coverage of phenomena was strictlyutilitarian (which, we believe, is quite appropri-ate).
lilt was not demoimtratext that Boris can bescaled up to (:over a signiticant part of the Englishlexicon.As an example of an early knowletlge-basetl MTsystem (thai, is, unlike the above, a system whosegoals were mainly computational-lit|guistic) wechose the KBMT-89 system (Goodman and Niren-burg, 1991).
It covered its small corpus relativelycompletely and described the necessary phenom-ena relatively fldly, lilt was a primary goal of thisl ine of research to begin meet ing the above criteriafor semantic overage.A pntative NIA ) system based on the (~ycproject has been selected as a prototyl)e for sys-tems not devised h)r a particular application.
TheCyc large-scMe knowledge base \]|as significantamounts of deep knowledge, llowever, it is notclear whether the knowledge is apl>licM)le in astraightff)rward manner to deal with a range oflinguistic phenomena.
The big question for thiskind of system is whether it is, in fact, possible,to acquire knowledge without a reference to anintended application.A purely corpus--based, statisticM approach toNLP, on the other hand, has an extremely nar-row range of knowledge, but, may haw; a largesize.
For example, snch a system may have alarge lexicon with only word frequency and col-location in format ion in each entry.
A l though sta-t istical methods  have been shown to work on someproblenm and applications, they are typically ap-plied to one or two phenomena t a time.
It isnot rlear that statistical information acquired tbrone probleln (such as sense disambiguation) is ofuse in hmtdling other problems (such as processingnon-literal expressions).Mixed-strategy NI,I ~ systems are epitomized byI'angloss (199d), a multi-engine translation sys-tem in which semantic processing is only one ofthe possible translation engines.
The semanticsengine of this system is equipped with a large-sizeontology of over 50,000 entries (Knight and link,t99d) which is nsed essentially as an am:hot formapl)ing lexicM traits front the 8otlrce to the tar.-gel, language.
As shown in I,'igure 2, Pangloss hasa large size and covers a good range of l)het,)m.em~ as well.
llowew',r, there is little information(only taxonomic and partonolnie relationships) ineach concept in its Sensus ontology.
The limiteddepth constrains the ultimate potentia.1 of the sys-tetn as a sentatd,ic and pragmatic processor.
I"orexatnple, there is no hfl'ortnal;ion i  its knowledgesources to make judgements about constr~fint re-laxal,ion to process non-literal expressions snch asmetonymies and metal~hors.The Mikrokosntos ystem (e.g., Onyshkevychan(t Nirenburg, 1994), has attempted to cover eachdimension equally well.
Its knowledge bases andtext meaning representations are rather deep andof nontrivial sizes.
It has been designed froln thestart to deal with a comprehensiw; range ot'seman-.tic phenomena including the linldng of syntax attdsemantics, (-ore semantic analysis, sense disam-l)iguation, I)rocessing non-literM expressions, SLIt({so on, althongh not all of them have yet been implemented.Front the abow'~ examples, it is clea.r that hav-ing good coverage along one or two of the threedimensions is not good enough for meeting thelong-term goMs of NI,P.
Poor coverage of languagephenomena (i.e., poor brea, dth) indicates that theacquired knowh;dge, even when it is deep and largein size, may not be applicable to other phenom-ena and may not transfer to other applications.Poor depth suggests that knowledge and process-ing techniques are either application- or language-specific and limits the ultimate potential of thesystem in solving semantic problems.
Depth andbreadth are of course of little use if the systemcmmot bc scaled up to a signilicant size.
More-over, as already noted, cow;rage in depth, breadth,and size must all be achievetl in conjnnction withmaintaining good me, asnres of correctness, et\[i-ciency, and robustness.4 D iscuss ion  and Conc lus ionsAll oft-quoted objection to having deep semantic(:overage is the dilliculty in scMing up such a sys-tem along the dimension of size.
This is a validconcern, llowever, the situation (:an be amelio-87rated to a large extent by developing a method-ology (see, e.g., Mahesh and Nirenburg, 1995) forconstraining knowledge acquisition to minimallymeet semantic processing needs.
Such concen-tration of effort will allow knowledge acquirersto have spend a fraction of the effort that mustgo into building a general machine-tractable en-cyclopedia of knowledge and yet to attain signifi-cant coverage of language phenomena.
Significantscale-up can be accomplished under such a con-straint without jeopardizing the high values on thedepth and breadth scales.Size is important in NLP.
But size alone is nota sufficient metric for evaluating semantic over-age.
Focusing on size to the exclusion of othercriteria has biased the field away from semanticsolutions to NLP problems.
We have made a firststep in formulating a more appropriate and com-plete set of measures of semantic overage.
Depthand breadth of knowledge necessary to cover awide range language phenomena are at least asimportant to NLP as size.
The discussion of pe-culiarities of the various approaches should be ex-panded in at least two directions - greater detailof description and analysis of the relative ditficultyof reaching the set goal of attaining an optimumvalue on each of the three measurement scales.
Wehope that this paper will elicit interest in contin-ned discussion of the issues of coverage measure-ment, which, in turn, will lead to better -- quanti-tative as well as qualitative- measures, includinga methodology for comparing lexicons and ontolo-gies.AcknowledgmentsMany thanks to Yorick Wilks for his constructivecriticism.ReferencesActive NLP Workshop: Working Notes from theAAAI Spring Symposium "Active NLP: NaturalLanguage Understanding in Integrated Systems"March 21-23, 1994, Stanford University, Califor-nia (Also available as a Technical Report from theAmerican Association for Artificial Intelligence).AI{PA MT Evaluation: Report of the AdvancedResearch Projects Agency, Machine TranslationProgram System Evaluation, May-August 1993.Goodman, K. and S. Nirenburg (eds.)
(1991).The KBMT Project: A Case Study in Knowledge-Based Machine Translation.
San Marco, CA:Morgan Kaufmann.Knight, K. and Luk, S. K. (1994).
Building aLarge-Scale Knowledge Base for Machine Trans-lation.
In Proc.
Twelfth National Conf.
on Arti-ficial Intelligence, (AAAI-94).Leant, D. B. and Guha, R. V. (1990).
BuildingLarge Knowledge-Based Sysiems.
Reading, MA:Addison-Wesley.Lehnert, W. G., Dyer, M. G., Johnson, P. N.,Yang, C. J., and Harley, S. (1983).
BORIS - AnExperiment in In-I)epth Understanding of Narra-tives.
Artificial Intelligence, 20(1):15-62.Lehnert, W. G. and Sundheim, B.
(1991).
Aperformance evaluation of text-analysis technolo-gies.
AI Magazine, 12(3):81-94.Mahesh, K. and Nirenburg, S. (1995).
A situ-ated ontology for practical NLP.
In Proceedingsof the Workshop on Basic Ontological Issues inKnowledge Sharing, International Joint Confer-ence on Artificial Intelligence (IJCAI-95), Mon-treal, Canada, August 1995.MUC-4: Proc.
Fourth Message UnderstandingConference (MUC-4), June 1992.
Defense Ad-vanced Research Projects Agency.Morgan Kanf-mann Publishers.Onyshkevych, B. and Nirenburg, S. (1994).
Thelexicon in the scheme of KBMT things.
TechnicalReport MCCS-94-277, Computing Research Lab-oratory, New Mexico State University.
Also toappear in Machine rlh'anslation.Pangloss.
(1994).
The PANGLOSS Mark IllMachine Translation System.
A Joint TechnicalReport by NMSU CRL, USC ISI and CMU CMT,Jan.
1994.Will, C. A.
(1993).
Comparing human and ma-chine performance for natural anguage informa-tion extraction: Results from the Tipster evalua-tion.
Proc.
Tipster Text Program, ARPA, Mor-gan Kaufmann Publishers.38
