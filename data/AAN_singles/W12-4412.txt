Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 76?80,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCost-benefit Analysis of Two-Stage Conditional Random Fields basedEnglish-to-Chinese Machine TransliterationChan-Hung Kuoa  Shih-Hung Liuab  MikeTian-Jian JiangacCheng-Wei Leea  Wen-Lian HsuaaInstitute of Information Science, Academia SinicabDepartment of Electrical Engineering, National Taiwan UniversitycDepartment of Computer Science, National Tsing Hua University{laybow, journey, tmjiang, aska, hsu}@iis.sinica.edu.twAbstractThis work presents an English-to-Chinese(E2C) machine transliteration system basedon two-stage conditional random fields(CRF) models with accessor variety (AV)as an additional feature to approximatelocal context of the source language.Experiment results show that two-stageCRF method outperforms the one-stageopponent since the former costs less toencode more features and finer grainedlabels than the latter.1 IntroductionMachine transliteration is the phonetictranscription of names across languages and isessential in numerous natural language processingapplications, such as machine translation, cross-language information retrieval/extraction, andautomatic lexicon acquisition (Li et al, 2009).
Itcan be either phoneme-based, grapheme-based, ora hybrid of the above.
The phoneme-basedapproach transforms source and target names intocomparable phonemes for an intuitive phoneticsimilarity measurement between two names(Knight and Graehl, 1998; Virga and Khudanpur,2003).
The grapheme-based approach, which treatstransliteration as statistical machine translationproblem under monotonic constraint, aims toobtain a direct orthographical mapping (DOM) toreduce possible errors introduced in multipleconversions (Li et al, 2004).
The hybrid approachattempts to utilize both phoneme and graphemeinformation (Oh and Choi, 2006).
Phoneme-basedapproaches are usually not good enough, becausename entities have various etymological originsand transliterations are not always decided bypronunciations (Li et al, 2004).
The state-of-the-art of transliteration approach is bilingual DOMswithout intermediate phonetic projections (Yang etal., 2010).Due to the success of CRF on sequentiallabeling problem (Lafferty et al, 2001), numerousmachine transliteration systems applied it.
Some ofthem treat transliteration as a two-stage sequentiallabeling problem: the first stage predicts syllableboundaries of source names, and the second stageuses those boundaries to get correspondingcharacters of target names (Yang et al, 2010; Qinand Chen, 2011).
Dramatically de-creasing the costof training with complex features is the majoradvantage of two-stage methods, but theirdownside is, compared to one-stage methods,features of target language are not directly appliedin the first stage.Richer context generally gains better results ofsequential labeling, but squeezed performancealways comes with a price of computationalcomplexity.
To balance cost and benefit forEnglish-to-Chinese (E2C) transliteration, this workcompares the one-stage method with the two-stageone, using additional features of AV (Feng et al,2004) and M2M-aligner as an initial alignment(Jiampojamarn et al, 2007), to explore where thebest investment reward is.The remainder of this paper is organized asfollows.
Section 2 briefly introduces related works,including two-stage methods and AV.
Themachine transliteration system using M2M-aligner,CRF models, and AV features in this work isexplained in Section 3.
Section 4 describes76experiment results along with a discussion inSection 5.
Finally, Section 6 draws a conclusion.2 Related WorksReddy and Waxmonsky (2009) presented a phrase-based transliteration system that groups charactersinto substrings mapping onto target names, todemonstrate how a substring representation can beincorporated into CRF models with local contextand phonemic information.
Shishtla et al (2009)adopted a statistical transliteration technique thatconsists of alignment models of GIZA++ (Och andNey, 2003) and CRF models.
Jiang et al (2011)used M2M-aligner instead of GIZA++ and appliedsource grapheme?s AV in a CRF-basedtransliteration.A two-stage CRF-based transliteration was firstdesigned to pipeline two independent processes(Yang et al, 2009).
To recover from errorpropagations of the pipeline, a joint optimization oftwo-stage CRF method is then proposed to utilizen-best candidates of source name segmentations(Yang et al 2010).
Another approach to resisterrors from the first stage is split training data intopools to lessen computation cost of sophisticatedCRF models for the second stage (Qin and Chen,2011).3 System Description3.1 EM for Initial AlignmentsM2M-aligner first maximizes the probability ofobserved source-target pairs using EM algorithmand subsequently sets alignments via maximum aposteriori estimation.
To obtain initial alignmentsas good as possible, this work empirically sets theparameter ?maxX?
of M2M-aligner for themaximum size of sub-alignments in the source sideto 8, and sets the parameter ?maxY?
for themaximum size of sub-alignments in the target sideto 1 (denoted as X8Y1 in short), since one of thewell-known a priori of Chinese is that almost allChinese characters are monosyllabic.3.2 Format of Electronic ManuscriptThe two-stage CRF method consists of syllablesegmentation and Chinese character conversionCRF models, namely Stage-1 and Stage-2,respectively.
Stage-1 CRF model is trained withsource name segmentations initially aligned byM2M-aligner to predict syllable boundaries asaccurate as possible.
According to thediscriminative power of CRF, some syllableboundary errors from preliminary alignments couldbe counterbalanced.
Stage-2 CRF model then seespredicted syllable boundaries as input to produceoptimal target names.
For CRF modeling, thiswork uses Wapiti (Lavergne et al, 2010).Using ?BULLOUGH?
as an example, labelingschemes below are for Stage-1 training.?
B/B U/B L/I L/I O/I U/I G/I H/E?
B/S U/B L/1 L/2 O/3 U/4 G/5 H/EThe first one is the common three-tag set ?BIE?.The last one is the eight-tag set ?B8?, including B,1-5, E and S: tag B indicates the beginningcharacter of a syllable segment, tag E means theending character, tag I or 1-5 stand for charactersin-between, and tag S represents a single charactersegment.
The expectation of the eight-tag set is thefiner grained tags we used, the better segmentationaccuracy we would gain.For Stage-2, two labeling schemes are listed inthe following.?
B/?
ULLOUGH/??
B/?
U/?
L/I L/I O/I U/I G/I H/IThe former as substring-based labeling scheme arecommonly used in two-stage CRF-basedtransliteration.
Syllable segments in a source wordare composed from Stage-1 results and then areassociated with corresponding Chinese characters(Yang et al 2009; Yang et al 2010; Qin and Chen,2011).
The latter is a character-based labelingscheme where tags B or S from Stage-1 will belabeled with a Chinese character and others will belabeled as I.
The merit of character-based methodis to retrench the duration of the training, whilesubstring-based method takes too much time to beincluded in this work for NEWS shared task.Section 5 will discuss more about pros and consbetween substring and character based labelingschemes.This work tests numerous CRF featurecombinations, for example:?
C-3, C-2, C-1, C0, C1 , C2, C3 and?
C-3C-2, C-2C-1, C-1C0, C0C1, C1C2, C2C3, where local context is ranging from -3 to 3, and Cidenotes the characters bound individually to theprediction label at its current position i.773.3 CRF with AVAV was for unsupervised Chinese wordsegmentation (Feng et al, 2004).
Jiang et al,(2011) showed that using AV of source graphemeas CRF features could improve transliteration.
Inour two-stage system, Source AV is used in Stage-1 in hope for better syllable segmentations, but notin Stage-2 since it may be redundant and surelyincrease training cost of Stage-2.4 Experiment Results4.1 Results of Standard RunsFour standard runs are submitted to NEWS12 E2Cshared task.
Their configurations are listed in Table1, where ?U?
and ?B?
denote observationcombinations of unigram and bigram, respectively.A digit in front of a ?UB?, for example, ?2?,indicates local context ranging from -2 to 2.
PBIEstands for ?BIE?
tag set and PB8 is for ?B8?
tag set.To summarize, the 4th (i.e.
the primary) standardrun exceeds 0.3 in terms of top-1 accuracy (ACC),and other ACCs of standard runs are approximateto 0.3.
The 3rd standard run uses the one-stage CRFmethod to compare with the two-stage CRFmethod.
Experiment results show that the two-stage CRF method can excel the one-stageopponent, while AV and richer context alsoimprove performance.4.2 Results of Inside TestsNumerous pilot tests have been conducted bytraining with both the training and developmentsets, and then testing on the development set, as?inside?
tests.
Three of them are shown in Table 2,where configurations I and II use the two-stagemethod, and configuration III is in one-stage.Table 2 suggests a trend that the one-stage CRFmethod performs better than the two-stage one oninside tests, but Table 1 votes the opposite.
Sincethe development set includes semi-semantictransliterations that are unseen in both the trainingand the test sets (Jiang et al, 2011), models ofinside tests are probably over-fitted to these noises.Table 3 further indicates that the number offeatures in the one-stage CRF method is doubledthan that in the two-stage one.
By putting theseobservations together, the two-stage CRF methodis believed to be more effective and efficient thanthe one-stage CRF method.5 DiscussionsThere are at least two major differences of two-stage CRF-based transliteration between ourapproach and others.
One is that we enrich thelocal context as much as possible, such as usingeight-tag set in Stage-1.
The other is using acharacter-based labeling method instead of asubstring-based one in Stage-2.Reasonable alignments can cause CRF modelstroubles when a single source grapheme is mappedonto multiple phones.
For instance, the alignmentbetween ?HAX?
and ?????
generating byM2M-aligner.HA ?
?X ?
?
?In this case, a single grapheme <X> pronounced as/ks/ in English therefore is associated with twoChinese characters ???
?, and won?t be an easycase to common character-based linear-chain CRF.Although for the sake of efficiency, this workadopts character-based CRF models, only a few ofsuch single grapheme for consonant blends ordiphthongs appeared in training and test data, andthen the decline of accuracy would be moderate.One may want to know how high the price is forusing a substring-based method to solve thisproblem.
We explore the number of featuresbetween substring-based and character-basedID Configuration ACC MeanF-score1 Two-stage, 2UB, PBIE 0.295 0.652 2 Two-stage, 2UB, PBIE, AV 0.299 0.659 3 One-stage, 3UB, PBIE, AV 0.291 0.654 4 Two-stage, 3UB, PB8, AV 0.311  0.662Table 1.
Selected E2C standard runsID Configuration ACC Mean F-scoreI Two-stage, 2UB, PBIE, AV 0.363 0.707 II Two-stage, 3UB, PB8, AV 0.397 0.727III One-stage, 3UB, PBIE, AV 0.558 0.834Table 2.
Selected E2C inside testsID Number of Features  Numbers of LabelII Stage-1: 60,496 Stage-1: 8 Stage-2: 2,567,618 Stage-2: 547III 4,439,896 548Table 3.
Cost of selected E2C inside tests78methods in Stage-2 with the same configuration II,as shown in Table 4.
Features of substring-basedmethod are tremendously more than character-based one.
Qin (2011) also reported similarobservations.However, there is another issue in our character-based method: only the starting position of asource syllable segment will be labeled as Chinesecharacter, others are labeled as I.
Base on thislabeling strategy, the local context of the targetgraphemes is missing.6 Conclusions and Future WorksThis work analyzes cost-benefit trade-offs betweentwo-stage and one-stage CRF-based methods forE2C transliteration.
Experiment results indicatethat the two-stage method can outperform its one-stage opponent since the former costs less toencode more features and finer grained labels thanthe latter.
Recommended future investigationswould be encoding more features of targetgraphemes and utilizing n-best lattices from theoutcome of Stage-1.AcknowledgmentsThis research was supported in part by the NationalScience Council under grant NSC 100-2631-S-001-001, and the research center for Humanitiesand Social Sciences under grant IIS-50-23.
Theauthors would like to thank anonymous reviewersfor their constructive criticisms.ReferencesHaodi Feng, Kang Chen, Xiaotie Deng, and WieminZheng.
2004.
Accessor Variety Criteria for ChineseWord Extraction.
Computational Linguistics,30(1):75-93.Zellig Sabbetai Harris.
1970.
Morpheme boundarieswithin words.
Papers in Structural andTransformational Linguistics, 68-77.Sittichai Jiampojamarn, Grzegorz Kondrak and TarekSherif.
2007.
Applying Many-to-Many Alignmentsand Hidden Markov Models to Letter-to-PhonemeConversion.
Proceedings of NAACL 2007, 372-379.Mike Tian-Jian Jiang, Chan-Hung Kuo and Wen-LianHsu.
2011.
English-to-Chinese MachineTransliteration using Accessor Variety Features ofSource Graphemes.
Proceedings of the 2011 NamedEntities Workshop.
86-90.K.
Knight and J. Graehl.
1998.
Machine Transliteration.Computational Linguistics, 24(4):599-612.John Lafferty, Andrew McCallum, Fernando Pereira.2001.
Conditional Random Fields ProbabilisticModels for Segmenting and Labeling Sequence Data.Proceedings of ICML, 591-598.Thomas Lavergne, Oliver Capp?
and Fran?ois Yvon.2010.
Practical Very Large Scale CRF.
Proceedingsthe 48th ACL, 504-513.Haizhou Li, Min Zhang and Jian Su.
2004.
A JointSource Channel Model for Machine Transliteration.Proceedings of the 42nd ACL, 159-166.Haizhou Li, A Kumaran, Min Zhang and VladimirPervouchine.
2009.
Report of NEWS 2009Transliteration Generation Shared Task.
Proceedingsof the 2009 Named Entities Workshop.
1-18.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19-51.J.
H. Oh and K. S. Choi.
2006.
An Ensemble ofTransliteration Models for Information Retrieval.Information Processing and Management, 42:980-1002.Ying Qin.
2011.
Phoneme strings based machinetransliteration.
Proceedings of the 7th IEEEInternational Conference on Natural LanguageProcessing and Knowledge Engineering.
304-309.Ying Qin and Guohua Chen.
2011.
Forward-backwardMachine Transliteration between English andChinese Base on Combined CRF.
Proceedings of the2011 Named Entities Workshop.
82-85.Eric Sven Ristad and Peter N. Yianilos.
1998.
LearningString Edit Distance.
IEEE Transactions on PatternRecognition and Machine Intelligence, 20(5):522-532.Sravana Reddy and Sonjia Waxmonsky.
2009.Substring-based transliteration with conditionalrandom fields.
Proceedings of the 2009 NamedEntities Workshop, 92-95.Praneeth Shishtla, V. Surya Ganesh, SethuramalingamSubramaniam and Vasudeva Varma.
2009.
Alanguage-independent transliteration schema usingcharacter aligned models at NEWS 2009.Proceedings of the 2009 Named Entities Workshop,40-43.ID Substring-based Character-BasedII 106,070,874 2,567,618Table 4.
Number of features between substringand character based method in Stage-279P.
Virga and S. Khudanpur.
2003.
Transliteration ofProper Names in Cross-lingual Information Retrieval.In the Proceedings of the ACL Workshop on Multi-lingual Named Entity Recognition.Dong Yang, Paul Dixon, Yi-Cheng Pan, TasukuOonishi, Masanobu Nakamura, Sadaoki Furui.
2009.Combining a two-step conditional random fieldmodel and a joint source channel model for machinetransliteration.
Proceedings of the 2009 NamedEntities Workshop, 72-75.Dong Yang, Paul Dixon and Sadaoki Furui.
2010.Jointly optimizing a two-step conditional randomfield model for machine transliteration and its fastdecoding algorithm.
Proceedings of the ACL 2010.Conference Short Papers, 275-280Hai Zhao and Chunyu Kit.
2008.
UnsupervisedSegmentation Helps Supervised Learning ofCharacter Tagging for Word Segmentation andNamed Entity Recognition.
Proceedings of the SixthSIGHAN Workshop on Chinese LanguageProcessing.80
