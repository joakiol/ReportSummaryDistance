Automatic Labeling of Semantic RolesDaniel GildeaUniversity of California, Berkeley, andInternational Computer Science Institutegildea@cs.berkeley.eduDaniel JurafskyDepartment of LinguisticsUniversity of Colorado, Boulderjurafsky@colorado.eduAbstractWe present a system for identify-ing the semantic relationships, or se-mantic roles, lled by constituents ofa sentence within a semantic frame.Various lexical and syntactic fea-tures are derived from parse treesand used to derive statistical clas-siers from hand-annotated trainingdata.1 IntroductionIdentifying the semantic roles lled by con-stituents of a sentence can provide a level ofshallow semantic analysis useful in solving anumber of natural language processing tasks.Semantic roles represent the participants inan action or relationship captured by a se-mantic frame.
For example, the frame for onesense of the verb \crash" includes the rolesAgent, Vehicle and To-Location.This shallow semantic level of interpreta-tion can be used for many purposes.
Cur-rent information extraction systems often usedomain-specic frame-and-slot templates toextract facts about, for example, nancialnews or interesting political events.
A shal-low semantic level of representation is a moredomain-independent, robust level of represen-tation.
Identifying these roles, for example,could allow a system to determine that inthe sentence \The rst one crashed" the sub-ject is the vehicle, but in the sentence \Therst one crashed it" the subject is the agent,which would help in information extraction inthis domain.
Another application is in word-sense disambiguation, where the roles associ-ated with a word can be cues to its sense.
Forexample, Lapata and Brew (1999) and othershave shown that the dierent syntactic sub-catgorization frames of a verb like \serve" canbe used to help disambiguate a particular in-stance of the word \serve".
Adding seman-tic role subcategorization information to thissyntactic information could extend this ideato use richer semantic knowledge.
Semanticroles could also act as an important inter-mediate representation in statistical machinetranslation or automatic text summarizationand in the emerging eld of Text Data Mining(TDM) (Hearst, 1999).
Finally, incorporat-ing semantic roles into probabilistic models oflanguage should yield more accurate parsersand better language models for speech recog-nition.This paper proposes an algorithm for au-tomatic semantic analysis, assigning a se-mantic role to constituents in a sentence.Our approach to semantic analysis is totreat the problem of semantic role labelinglike the similar problems of parsing, part ofspeech tagging, and word sense disambigua-tion.
We apply statistical techniques thathave been successful for these tasks, includingprobabilistic parsing and statistical classica-tion.
Our statistical algorithms are trainedon a hand-labeled dataset: the FrameNetdatabase (Baker et al, 1998).
The FrameNetdatabase denes a tagset of semantic rolescalled frame elements, and includes roughly50,000 sentences from the British NationalCorpus which have been hand-labeled withthese frame elements.
The next section de-scribes the set of frame elements/semanticroles used by our system.
In the rest of thispaper we report on our current system, as wellas a number of preliminary experiments onextensions to the system.2 Semantic RolesHistorically, two types of semantic roles havebeen studied: abstract roles such as Agentand Patient, and roles specic to individualverbs such as Eater and Eaten for \eat".The FrameNet project proposes roles at an in-termediate level, that of the semantic frame.Frames are dened as schematic representa-tions of situations involving various partici-pants, props, and other conceptual roles (Fill-more, 1976).
For example, the frame \conver-sation", shown in Figure 1, is invoked by thesemantically related verbs \argue", \banter",\debate", \converse", and \gossip" as wellas the nouns \argument", \dispute", \discus-sion" and \ti".
The roles dened for thisframe, and shared by all its lexical entries,include Protagonist1 and Protagonist2or simply Protagonists for the participantsin the conversation, as well as Medium, andTopic.
Example sentences are shown in Ta-ble 1.
Dening semantic roles at the framelevel avoids some of the diculties of at-tempting to nd a small set of universal, ab-stract thematic roles, or case roles such asAgent, Patient, etc (as in, among manyothers, (Fillmore, 1968) (Jackendo, 1972)).Abstract thematic roles can be thought ofas being frame elements dened in abstractframes such as \action" and \motion" whichare at the top of in inheritance hierarchy ofsemantic frames (Fillmore and Baker, 2000).The preliminary version of the FrameNetcorpus used for our experiments contained 67frames from 12 general semantic domains cho-sen for annotation.
Examples of domains (seeFigure 1) include \motion", \cognition" and\communication".
Within these frames, ex-amples of a total of 1462 distinct lexical pred-icates, or target words, were annotated: 927verbs, 339 nouns, and 175 adjectives.
Thereare a total of 49,013 annotated sentences, and99,232 annotated frame elements (which donot include the target words themselves).3 Related WorkAssignment of semantic roles is an impor-tant part of language understanding, and hasbeen attacked by many computational sys-tems.
Traditional parsing and understand-ing systems, including implementations ofunication-based grammars such as HPSG(Pollard and Sag, 1994), rely on hand-developed grammars which must anticipateeach way in which semantic roles may be real-ized syntactically.
Writing such grammars istime-consuming, and typically such systemshave limited coverage.Data-driven techniques have recently beenapplied to template-based semantic interpre-tation in limited domains by \shallow" sys-tems that avoid complex feature structures,and often perform only shallow syntacticanalysis.
For example, in the context ofthe Air Traveler Information System (ATIS)for spoken dialogue, Miller et al (1996) com-puted the probability that a constituent suchas \Atlanta" lled a semantic slot such asDestination in a semantic frame for airtravel.
In a data-driven approach to infor-mation extraction, Rilo (1993) builds a dic-tionary of patterns for lling slots in a spe-cic domain such as terrorist attacks, andRilo and Schmelzenbach (1998) extend thistechnique to automatically derive entire caseframes for words in the domain.
These lastsystems make use of a limited amount of handlabor to accept or reject automatically gen-erated hypotheses.
They show promise fora more sophisticated approach to generalizebeyond the relatively small number of framesconsidered in the tasks.
More recently, a do-main independent system has been trained ongeneral function tags such as Manner andTemporal by Blaheta and Charniak (2000).4 MethodologyWe divide the task of labeling frame elementsinto two subtasks: that of identifying theboundaries of the frame elements in the sen-tences, and that of labeling each frame ele-ment, given its boundaries, with the correctrole.
We rst give results for a system whichconfer?vdebate?vconverse?vgossip?vdispute?ndiscussion?ntiff?nConversationFrame:Protagonist?1Protagonist?2ProtagonistsTopicMediumFrame Elements:talk?vDomain: Communication Domain: CognitionFrame: QuestioningTopicMediumFrame Elements: SpeakerAddresseeMessageFrame:TopicMediumFrame Elements: SpeakerAddresseeMessageStatementFrame:Frame Elements:JudgmentJudgeEvalueeReasonRoledispute?nblame?v fault?nadmire?vadmiration?n disapprove?vblame?nappreciate?vFrame:Frame Elements:CategorizationCognizerItemCategoryCriterionFigure 1: Sample domains and frames from the FrameNet lexicon.Frame Element Example (in italics) with target verb Example (in italics) with target nounProtagonist 1 Kim argued with Pat Kim had an argument with PatProtagonist 2 Kim argued with Pat Kim had an argument with PatProtagonists Kim and Pat argued Kim and Pat had an argumentTopic Kim and Pat argued about politics Kim and Pat had an argument about politicsMedium Kim and Pat argued in French Kim and pat had an argument in FrenchTable 1: Examples of semantic roles, or frame elements, for target words \argue" and \argu-ment" from the \conversation" framelabels roles using human-annotated bound-aries, returning to the question of automat-ically identifying the boundaries in Section5.3.4.1 Features Used in AssigningSemantic RolesThe system is a statistical one, based on train-ing a classier on a labeled training set, andtesting on an unlabeled test set.
The sys-tem is trained by rst using the Collins parser(Collins, 1997) to parse the 36,995 train-ing sentences, matching annotated frame el-ements to parse constituents, and extractingvarious features from the string of words andthe parse tree.
During testing, the parser isrun on the test sentences and the same fea-tures extracted.
Probabilities for each possi-ble semantic role r are then computed fromthe features.
The probability computationwill be described in the next section; the fea-tures include:Phrase Type: This feature indicates thesyntactic type of the phrase expressingthe semantic roles: examples includenoun phrase (NP), verb phrase (VP), andclause (S).
Phrase types were derived au-tomatically from parse trees generated bythe parser, as shown in Figure 2.
Theparse constituent spanning each set ofwords annotated as a frame element wasfound, and the constituent's nonterminallabel was taken as the phrase type.
Asan example of how this feature is useful,in communication frames, the Speakeris likely appear a a noun phrase, Topicas a prepositional phrase or noun phrase,and Medium as a prepostional phrase, asin: \We talked about the proposal overthe phone."
When no parse constituentwas found with boundaries matchingthose of a frame element during testing,the largest constituent beginning at theframe element's left boundary and lyingentirely within the element was used tocalculate the features.Grammatical Function: This feature at-tempts to indicate a constituent's syntac-tic relation to the rest of the sentence,SNPPRPVPVBDNPSBARINSNNPVPVBDNP PPPRP INNPNNGoal SourceTheme TargetNPHe heard the sound of liquid slurping in a metal container as approached him from behindFarrellFigure 2: A sample sentence with parser output (above) and FrameNet annotation (below).Parse constituents corresponding to frame elements are highlighted.for example as a subject or object of averb.
As with phrase type, this featurewas read from parse trees returned bythe parser.
After experimentation withvarious versions of this feature, we re-stricted it to apply only to NPs, as it wasfound to have little eect on other phrasetypes.
Each NP's nearest S or VP ances-tor was found in the parse tree; NPs withan S ancestor were given the grammati-cal function subject and those with a VPancestor were labeled object.
In general,agenthood is closely correlated with sub-jecthood.
For example, in the sentence\He drove the car over the cli", the rstNP is more likely to ll the Agent rolethan the second or third.Position: This feature simply indicateswhether the constituent to be labeled oc-curs before or after the predicate den-ing the semantic frame.
We expectedthis feature to be highly correlated withgrammatical function, since subjects willgenerally appear before a verb, andobjects after.
Moreover, this featuremay overcome the shortcomings of read-ing grammatical function from a con-stituent's ancestors in the parse tree, aswell as errors in the parser output.Voice: The distinction between active andpassive verbs plays an important rolein the connection between semantic roleand grammatical function, since directobjects of active verbs correspond to sub-jects of passive verbs.
From the parseroutput, verbs were classied as active orpassive by building a set of 10 passive-identifying patterns.
Each of the pat-terns requires both a passive auxiliary(some form of \to be" or \to get") and apast participle.Head Word: As previously noted, we ex-pected lexical dependencies to be ex-tremely important in labeling semanticroles, as indicated by their importancein related tasks such as parsing.
Sincethe parser used assigns each constituenta head word as an integral part of theparsing model, we were able to read thehead words of the constituents from theparser output.
For example, in a commu-nication frame, noun phrases headed by\Bill", \brother", or \he" are more likelyto be the Speaker, while those headedby \proposal", \story", or \question" aremore likely to be the Topic.For our experiments, we divided theFrameNet corpus as follows: one-tenth of theannotated sentences for each target word werereserved as a test set, and another one-tenthwere set aside as a tuning set for developingour system.
A few target words with fewerthan ten examples were removed from the cor-pus.
In our corpus, the average number ofsentences per target word is only 34, and thenumber of sentences per frame is 732 | bothrelatively small amounts of data on which totrain frame element classiers.Although we expect our features to inter-act in various ways, the data are too sparseto calculate probabilities directly on the fullset of features.
For this reason, we built ourclassier by combining probabilities from dis-tributions conditioned on a variety of combi-nations of features.An important caveat in using the FrameNetdatabase is that sentences are not chosen forannotation at random, and therefore are notnecessarily statistically representative of thecorpus as a whole.
Rather, examples are cho-sen to illustrate typical usage patterns foreach word.
We intend to remedy this in fu-ture versions of this work by bootstrappingour statistics using unannotated text.Table 2 shows the probability distributionsused in the nal version of the system.
Cov-erage indicates the percentage of the test datafor which the conditioning event had beenseen in training data.
Accuracy is the propor-tion of covered test data for which the correctrole is predicted, and Performance, simplythe product of coverage and accuracy, is theoverall percentage of test data for which thecorrect role is predicted.
Accuracy is some-what similar to the familiar metric of pre-cision in that it is calculated over cases forwhich a decision is made, and performance issimilar to recall in that it is calculated over alltrue frame elements.
However, unlike a tradi-tional precision/recall trade-o, these resultshave no threshold to adjust, and the task is amulti-way classication rather than a binarydecision.
The distributions calculated weresimply the empirical distributions from thetraining data.
That is, occurrences of eachrole and each set of conditioning events werecounted in a table, and probabilities calcu-lated by dividing the counts for each role bythe total number of observations for each con-ditioning event.
For example, the distributionP (rjpt; t) was calculated sas follows:P (rjpt; t) =#(r; pt; t)#(pt; t)Some sample probabilities calculated fromthe training are shown in Table 3.5 ResultsResults for dierent methods of combiningthe probability distributions described in theprevious section are shown in Table 4.
Thelinear interpolation method simply averagesthe probabilities given by each of the distri-butions in Table 2:P (rjconstituent) = 1P (rjt) +2P (rjpt; t) + 3P (rjpt; gf; t) +4P (rjpt; position; voice) +5P (rjpt; position; voice; t) + 6P (rjh) +7P (rjh; t) + 8P (rjh; pt; t)wherePii= 1.
The geometric mean, ex-pressed in the log domain, is similar:P (rjconstituent) =1Zexpf1logP (rjt) +2logP (rjpt; t) + 3logP (rjpt; gf; t) +4logP (rjpt; position; voice) +5logP (rjpt; position; voice; t) +6logP (rjh) + 7logP (rjh; t) +8logP (rjh; pt; t)gwhere Z is a normalizing constant ensuringthatPrP (rjconstituent) = 1.The results shown in Table 4 reect equalvalues of  for each distribution dened forthe relevant conditioning event (but exclud-ing distributions for which the conditioningevent was not seen in the training data).Distribution Coverage Accuracy PerformanceP (rjt) 100% 40.9% 40.9%P (rjpt; t) 92.5 60.1 55.6P (rjpt; gf; t) 92.0 66.6 61.3P (rjpt; position; voice) 98.8 57.1 56.4P (rjpt; position; voice; t) 90.8 70.1 63.7P (rjh) 80.3 73.6 59.1P (rjh; t) 56.0 86.6 48.5P (rjh; pt; t) 50.1 87.4 43.8Table 2: Distributions Calculated for Semantic Role Identication: r indicates semantic role,pt phrase type, gf grammatical function, h head word, and t target word, or predicate.P (rjpt; gf; t) Count in training dataP (r =Agtjpt =NP; gf =Subj; t =abduct) = :46 6P (r =Thmjpt =NP; gf =Subj; t =abduct) = :54 7P (r =Thmjpt =NP; gf =Obj; t =abduct) = 1 9P (r =Agtjpt =PP; t =abduct) = :33 1P (r =Thmjpt =PP; t =abduct) = :33 1P (r =CoThmjpt =PP; t =abduct) = :33 1P (r =Manrjpt =ADVP; t =abduct) = 1 1Table 3: Sample probabilities for P (rjpt; gf; t) calculated from training data for the verb abduct.The variable gf is only dened for noun phrases.
The roles dened for the removing frame inthe motion domain are: Agent, Theme, CoTheme (\... had been abducted with him") andManner.Other schemes for choosing values of , in-cluding giving more weight to distributionsfor which more training data was available,were found to have relatively little eect.
Weattribute this to the fact that the evaluationdepends only the the ranking of the probabil-ities rather than their exact values.P(r | h, t) P(r | pt, t) P(r | pt, position, voice)P(r | pt, position, voice, t)P(r | pt, gf, t)P(r | t)P(r | h)P(r | h, pt, t)Figure 3: Lattice organization of the distri-butions from Table 2, with more specic dis-tributions towards the top.In the \backo" combination method, alattice was constructed over the distributionsin Table 2 from more specic conditioningevents to less specic, as shown in Figure3.
The less specic distributions were usedonly when no data was present for any morespecic distribution.
As before, probabilitieswere combined with both linear interpolationand a geometric mean.Combining Method CorrectLinear Interpolation 79.5%Geometric Mean 79.6Backo, linear interpolation 80.4Backo, geometric mean 79.6Baseline: Most common role 40.9Table 4: Results on Development Set, 8148observationsThe nal system performed at 80.4% ac-curacy, which can be compared to the 40.9%achieved by always choosing the most prob-able role for each target word, essentiallychance performance on this task.
Results forthis system on test data, held out during de-velopment of the system, are shown in TableLinearBacko BaselineDevelopment Set 80.4% 40.9%Test Set 76.9 40.6%Table 5: Results on Test Set, using backolinear interpolation system.
The test set con-sists of 7900 observations.5.5.1 DiscussionIt is interesting to note that looking at a con-stituent's position relative to the target wordalong with active/passive information per-formed as well as reading grammatical func-tion o the parse tree.
A system using gram-matical function, along with the head word,phrase type, and target word, but no passiveinformation, scored 79.2%.
A similar systemusing position rather than grammatical func-tion scored 78.8% | nearly identical perfor-mance.
However, using head word, phrasetype, and target word without either positionor grammatical function yielded only 76.3%,indicating that while the two features accom-plish a similar goal, it is important to includesome measure of the constituent's syntacticrelationship to the target word.
Our nal sys-tem incorporated both features, giving a fur-ther, though not signicant, improvement.
Asa guideline for interpreting these results, with8176 observations, the threshold for statisti-cal signifance with p < :05 is a 1.0% absolutedierence in performance.Use of the active/passive feature made afurther improvement: our system using po-sition but no grammatical function or pas-sive information scored 78.8%; adding passiveinformation brought performance to 80.5%.Roughly 5% of the examples were identiedas passive uses.Head words proved to be very accurate in-dicators of a constituent's semantic role whendata was available for a given head word,conrming the importance of lexicalizationshown in various other tasks.
While the dis-tribution P (rjh; t) can only be evaluated for56.0% of the data, of those cases it gets 86.7%correct, without use of any of the syntacticfeatures.5.2 Lexical ClusteringIn order to address the sparse coverage of lex-ical head word statistics, an experiment wascarried out using an automatic clustering ofhead words of the type described in (Lin,1998).
A soft clustering of nouns was per-formed by applying the co-occurrence modelof (Hofmann and Puzicha, 1998) to a largecorpus of observed direct object relationshipsbetween verbs and nouns.
The clustering wascomputed from an automatically parsed ver-sion of the British National Corpus, using theparser of (Carroll and Rooth, 1998).
The ex-periment was performed using only frame el-ements with a noun as head word.
This al-lowed a smoothed estimate of P (rjh; nt; t) tobe computed asPcP (rjc; nt; t)P (cjh), sum-ming over the automatically derived clusters cto which a nominal head word h might belong.This allows the use of head word statisticseven when the headword h has not been seenin conjunction was the target word t in thetraining data.
While the unclustered nominalhead word feature is correct for 87.6% of caseswhere data for P (rjh; nt; t) is available, suchdata was available for only 43.7% of nominalhead words.
The clustered head word alonecorrectly classied 79.7% of the cases wherethe head word was in the vocabulary usedfor clustering; 97.9% of instances of nominalhead words were in the vocabulary.
Addingclustering statistics for NP constituents intothe full system increased overall performancefrom 80.4% to 81.2%.5.3 Automatic Identication ofFrame Element BoundariesThe experiments described above have usedhuman annotated frame element boundaries| here we address how well the frame ele-ments can be found automatically.
Exper-iments were conducted using features simi-lar to those described above to identify con-stituents in a sentence's parse tree that werelikely to be frame elements.
The systemwas given the human-annotated target wordand the frame as inputs, whereas a full lan-guage understanding system would also iden-tify which frames come into play in a sen-tence | essentially the task of word sensedisambiguation.
The main feature used wasthe path from the target word through theparse tree to the constituent in question, rep-resented as a string of parse tree nonterminalslinked by symbols indicating upward or down-ward movement through the tree, as shown inFigure 4.SNP VPV NPDet NProHeatesometargetwordframeelement pancakesFigure 4: In this example, the path from theframe element \He" to the target word \ate"can be represented as NP " S # VP # V, with" indicating upward movement in the parsetree and # downward movement.The other features used were the iden-tity of the target word and the identity ofthe constituent's head word.
The probabil-ity distributions calculated from the train-ing data were P (fejpath), P (fejpath; t), andP (fejh; t), where fe indicates an event wherethe parse constituent in question is a frame el-ement, path the path through the parse treefrom the target word to the parse constituent,t the identity of the target word, and h thehead word of the parse constituent.
By vary-ing the probability threshold at which a deci-sion is made, one can plot a precision/recallcurve as shown in Figure 5.
P (fejpath; t)performs relatively poorly due to fragmenta-tion of the training data (recall only about 30sentences are available for each target word).While the lexical statistic P (fejh; t) alone isnot useful as a classier, using it in linear in-terpolation with the path statistics improvesresults.
Note that this method can only iden-tify frame elements that have a correspond-ing constituent in the automatically gener-ated parse tree.
For this reason, it is inter-esting to calculate how many true frame el-ements overlap with the results of the sys-tem, relaxing the criterion that the bound-aries must match exactly.
Results for partialmatching are shown in Table 6.When the automatically identied con-stituents were fed through the role labelingsystem described above, 79.6% of the con-stituents which had been correctly identiedin the rst stage were assigned the correct rolein the second, roughly equivalent to the per-formance when assigning roles to constituentsidentied by hand.00.10.20.30.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1recallprecisionP(fe|path)P(fe|path, t).75*P(fe | path)+.25*P(fe | h, t)Figure 5: Precison/Recall plot for variousmethods of identifying frame elements.
Recallis calculated over only frame elements withmatching parse constituents.6 ConclusionOur preliminary system is able to automati-cally label semantic roles with fairly high ac-curacy, indicating promise for applications invarious natural language tasks.
Lexical statis-tics computed on constituent head words werefound to be the most important of the fea-tures used.
While lexical statistics are quiteaccurate on the data covered by observationsin the training set, the sparsity of the datawhen conditioned on lexical items meant thatcombining features was the key to high over-all performance.
While the combined sys-tem was far more accurate than any featureType of Overlap Identied Constituents NumberExactly Matching Boundaries 66% 5421Identied constituent entirely within true frame element 8 663True frame element entirely within identied constituent 7 599Partial overlap 0 26No match to true frame element 13 972Table 6: Results on Identifying Frame Elements (FEs), including partial matches.
Resultsobtained using P (fejpath) with threshold at .5.
A total of 7681 constituents were identied asFEs, 8167 FEs were present in hand annotations, of which matching parse constituents werepresent for 7053 (86%).taken alone, the specic method of combina-tion used was less important.We plan to continue this work by integrat-ing semantic role identication with parsing,by bootstrapping the system on larger, andmore representative, amounts of data, and byattempting to generalize from the set of pred-icates chosen by FrameNet for annotation togeneral text.ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The berkeley framenet project.In Proceedings of the COLING-ACL, Montreal,Canada.Dan Blaheta and Eugene Charniak.
2000.
As-signing function tags to parsed text.
In Pro-ceedings of the 1st Annual Meeting of the NorthAmerican Chapter of the ACL (NAACL), Seat-tle, Washington.Glenn Carroll and Mats Rooth.
1998.
Va-lence induction with a head-lexicalized pcfg.
InProceedings of the 3rd Conference on Empir-ical Methods in Natural Language Processing(EMNLP 3), Granada, Spain.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Pro-ceedings of the 35th Annual Meeting of theACL.Charles J. Fillmore and Collin F. Baker.
2000.Framenet: Frame semantics meets the corpus.In Linguistic Society of America, January.Charles Fillmore.
1968.
The case for case.
InBach and Harms, editors, Universals in Lin-guistic Theory, pages 1{88.
Holt, Rinehart, andWinston, New York.Charles J. Fillmore.
1976.
Frame semanticsand the nature of language.
In Annals of theNew York Academy of Sciences: Conference onthe Origin and Development of Language andSpeech, volume 280, pages 20{32.Marti Hearst.
1999.
Untangling text data mining.In Proceedings of the 37rd Annual Meeting ofthe ACL.Thomas Hofmann and Jan Puzicha.
1998.
Sta-tistical models for co-occurrence data.
Memo,Massachussetts Institute of Technology Arti-cial Intelligence Laboratory, February.Ray Jackendo.
1972.
Semantic Interpretation inGenerative Grammar.
MIT Press, Cambridge,Massachusetts.Maria Lapata and Chris Brew.
1999.
Usingsubcategorization to resolve verb class ambigu-ity.
In Joint SIGDAT Conference on Empiri-cal Methods in NLP and Very Large Corpora,Maryland.Dekang Lin.
1998.
Automatic retrieval and clus-tering of similar words.
In Proceedings of theCOLING-ACL, Montreal, Canada.Scott Miller, David Stallard, Robert Bobrow, andRichard Schwartz.
1996.
A fully statisticalapproach to natural language interfaces.
InProceedings of the 34th Annual Meeting of theACL.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven Phrase Structure Grammar.
Universityof Chicago Press, Chicago.Ellen Rilo and Mark Schmelzenbach.
1998.
Anempirical approach to conceptual case frame ac-quisition.
In Proceedings of the Sixth Workshopon Very Large Corpora.Ellen Rilo.
1993.
Automatically constructinga dictionary for information extraction tasks.In Proceedings of the Eleventh National Con-ference on Articial Intelligence (AAAI).
