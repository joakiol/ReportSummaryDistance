Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 48?53,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsMeasuring the compositionality of collocations viaword co-occurrence vectors: Shared task system descriptionAlfredo Maldonado-Guerra and Martin EmmsSchool of Computer Science and StatisticsTrinity College DublinIreland{maldonaa, mtemms}@scss.tcd.ieAbstractA description of a system for measuring thecompositionality of collocations within theframework of the shared task of the Distribu-tional Semantics and Compositionality work-shop (DISCo 2011) is presented.
The systemexploits the intuition that a highly composi-tional collocation would tend to have a consid-erable semantic overlap with its constituents(headword and modifier) whereas a colloca-tion with low compositionality would sharelittle semantic content with its constituents.This intuition is operationalised via three con-figurations that exploit cosine similarity mea-sures to detect the semantic overlap betweenthe collocation and its constituents.
The sys-tem performs competitively in the task.1 IntroductionCollocations or multiword expressions vary in thedegree to which a native speaker is able to under-stand them based on the interaction of their con-stituents?
individual meanings.
The concept of com-positionality of a collocation captures this notion.The shared task of the DISCo 2011 workshop (Bie-mann and Giesbrecht, 2011) consists in comparingsystems?
compositionality scores against composi-tionality scores based on human judgements.
Sys-tems were evaluated on the match of the compo-sitional scores generated by the system and thosebased on human judgements ?
specifically taking themean of the absolute difference of these scores.
Ad-ditionally the organisers also classified the human-derived scores into three coarse categories of com-positionality: non-compositional (low), somewhatcompositional (medium) and compositional (high).Systems were required to produce an additionalcompositionality labelling into these three coarsecategories and were evaluated on the precision ofthis labelling.The methods used by our system for measuringcompositionality take inspiration from the work ofMcCarthy et al (2003), who measured the simi-larity between a phrasal verb (a main verb and apreposition like blow up) and its main verb (blow)by comparing the words that are closely semanti-cally related to each, and use this similarity as anindicator of compositionality.
Our method for mea-suring compositionality is considerably different asit instead directly compares the semantic similar-ity between the headword and the collocation andbetween the modifier and the collocation by com-puting a cosine similarity score between word co-occurrence vectors that represent the headword, themodifier and the collocation (see 3.2).
Our systemcan be regarded as fully unsupervised as it does notemploy any parsers in its processing or any externaldata other than the corpus and the collocation listsprovided by the organisers.The rest of the paper is organised as follows: Sec-tion 2 describes the corpora and the collocation listprovided by the task organisers.
Section 3 intro-duces some definitions and describes the three con-figurations in detail.
Section 4 presents the resultsand concludes.2 DataShared task participants were provided with a list ofcollocations of three grammatical forms: adjective-48noun collocations (A-N), subject-verb collocations(S-V) and verb-object collocations (V-O).
Our sys-tem assumes that each collocation consists of aheadword and a modifier and it interprets these con-stituents in each grammatical form as follows: A-N:adjective - modifier, noun - headword; S-V: subject- modifier, verb - headword; V-O: verb - headword,object - modifier.As a corpus, our system uses a random sample of500,000 documents from the plain-text, non-parsedversion of the English ukWaC corpus (Baroni et al,2009).3 System descriptionOur system can be employed in three different con-figurations.
All three rely in representing wordsand collocations as word co-occurrence vectors andmeasure semantic similarity using the cosine mea-sure.3.1 Preliminary definitionsThese definitions are largely based on the con-struction of first-order context vectors, word co-occurrence vectors and second-order context vectorsvia global selection as described in Sch?tze (1998)and in Purandare and Pedersen (2004) by consider-ing context windows of 20 words centred at a targetword.The first-order context vector is a vector repre-senting a token of a word, or equivalently a positionp in a document.
Dimensions of the vector are wordtypes w, and the value on dimension w is a countof the frequency with which w occurs in a specifiedwindow around p in a given document doc.C1(p)(w) = ?p?
6=pp?10?p?p?
?p+10(1 if w = doc(p?
), else 0) (1)In this work the dimensions are the 2,000 non-function words that are most frequent in the corpus1.The word co-occurrence vector (or simply wordvector) is a vector recording the co-occurrence be-haviour of a particular word type w in a corpus.
As1We employ a modified version of the stop wordlist supplied with Ted Pedersen?s Text-NSP package(http://www.d.umn.edu/~tpederse/nsp.html)such it can be defined by summation over first-ordercontext vectors:W(w) =?p(1 if w = doc(p), else 0) ?C1(p) (2)And the second-order context vector is a furthervector representing an instance of a word.
For a par-ticular location p, it is defined to be sum of the wordvectors of words in a given window around pC2(p) = ?p?
6=pp?10?p?p?
?p+10W(doc(p)) (3)Although the above are defined for types and to-kens of words, they can be generalised to multiwordexpressions in various ways.
In this work, for anymultiword expression type x y, its tokens are takento be occurrences of the sequence x?y, where ?
canbe any sequence of intervening words of length l,0?
l ?
3.
By taking the position of x as the positionof the multiword token, and taking the first positionafter the token as position p + 1, the definitions ofC1, W and C2 can be carried over to multiword ex-pressions.All the configurations described below use the co-sine measure between vectors, defined in the stan-dard waycos(v,w) =?Ni=1 viwi?
?Ni=1 v2i ?Ni=1 w2i(4)3.2 System configurationsFor each collocation in the test set, the first configu-ration of our system starts off by building word vec-tors for the collocation, its headword and its modi-fier.The first configuration of the system outputs theaverage of two cosine similarity measures as thecompositionality score for the collocation:c1 =12[cos(W(x y) ,W(x))+cos(W(x y) ,W(y))](5)where W(x y) is the word vector representing thecollocation whose constituents are x and y, andW(x) and W(y) are the word vectors representingeach constituent x and y, respectively.49The second configuration of our system consid-ers the occurrences of the headword when accompa-nied by the modifier forming the collocation sepa-rately from occurrences of the headword appearingon its own and compares them.
If y is the headwordof a collocation and coll(p) is a Boolean functionthat determines whether the word at position p formsa collocation with x, letWx(y) =?p(1 ifdoc(p) = ycoll(p,x), else 0) ?C1(p) (6)be the word vector computed from all the occur-rences of the headword y that form a collocationwith x and conversely, letWx?
(y) =?p(1 ifdoc(p) = yqcoll(p,x), else 0) ?C1(p) (7)be the word vector representing the occurrences of ynot engaging in a collocation with x.
In this configu-ration, the compositionality score is then computedbyc2 = cos(Wx (y) ,Wx?
(y))(8)The intuition behind this configuration is that ifthe headword tends to co-occur with more or less thesame words in both cases (producing a high cosinescore), then the meaning of the headword is simi-lar regardless of whether the collocation?s modifieris present or not, implying a high degree of com-positionality.
If on the other hand, the headwordco-occurs with somewhat differing words in the twocases (a low cosine score), then we assume that thepresence of the collocation?s modifier is markedlychanging the meaning of the headword, implying alow degree of compositionality.In its third configuration, our system employsclustering techniques in order to exploit semanticdifferences that may naturally emerge from eachcontext in which the collocation and its constituentsare used.
Different senses of a collocation mighthave different compositionality measures as can beseen in these two example sentences employing thecollocation great deal:1.
Two cans of soup for the price of one is such agreat deal!C2(y) C2(y) C2(x)C2(x y)C2(x)C2(x y)C2(y)C2(y)C2(x)C2(x y)C2(x)C2(x)C2(y) C2(x y)Figure 1: Example of a clustered second-order contextvector space.2.
The tsunami caused a great deal of damage tothe country?s infrastructure.In Word Sense Induction, clustering is used to groupoccurrences of a target word according to its sense orusage in context (see e.g.
Pedersen (2010)) as it isexpected that each cluster will represent a differentsense or usage of the target word.
However, sincethe contexts that human annotators referred to whenjudging the compositionality of the collocations wasnot provided, our system employs a workaround thatuses a weighted average when measuring composi-tionality.
This workaround is explained in what fol-lows.In this configuration, the system first builds wordvectors for the 20,000 most frequent words in thecorpus (equation 2), and then uses these to computethe second-order context vectors for each occurrenceof the collocation and its constituents in the corpus(equation 3).
After context vectors for all occur-rences have been computed, they are clustered usingCLUTO?s repeated bisections algorithm2.
The vec-tors are clustered across a small number K of clus-ters (we employed K = 4).
We expect that each clus-ter will represent a different contextual usage of thecollocation, its headword and its modifier.
Figure 1depicts how a context vector space could be parti-tioned with K = 4.The system then for each cluster k builds the wordvectors (equation 2) Wk(x y), Wk(x), and Wk(y) forthe collocation, its headword and its modifier, fromthe contexts grouped within the cluster k. The com-positionality measure for the third configuration isthen basically a weighted average over the clusters2http://glaros.dtc.umn.edu/gkhome/views/cluto/50of the c1 score using each cluster, that is:c3 =K?k=1?k?N12[cos(Wk(x y),Wk(x))+cos(Wk(x y),Wk(y))](9)where ?k?
is the number of contexts in cluster kand N is the total number of contexts across all clus-ters.For all three configurations, the value reported asthe numeric compositionality score was the corre-sponding value obtained from equations (5), (8) or(9), multiplied by 100.
Each configuration?s nu-meric scores ci were binned into the three coarsecompositionality classes by comparing them withthe configuration?s maximum value through equa-tion (10).coarse(ci) =????
?high if 23 max?
cimedium if 13 max < ci <23 maxlow if ci ?
13 max(10)4 Results and conclusionTable 1 shows the evaluation results for the threesystem configurations and two baselines.
The left-hand side of the table shows the average differencebetween the gold-standard numeric score and eachconfiguration?s numeric score.
The right-hand sidereports the precision on binning the numeric scoresinto the coarse classes.
Evaluation scores are re-ported on all collocations and on the collocation sub-types separately.
Row R is the baseline suggestedby the workshop organisers, assigning random nu-meric scores, in turn binned into the coarse cate-gories.
Row A shows the performance of a con-stant output baseline, assigning all collocations themean gold-standard numeric score from the trainingset: 66.45, and then applying the binning strategyof equation (10) to this ?
which always assigns thecoarse category high.The first thing to note from this table is that con-figurations 1 and 2 generally outperform configu-ration 3, both on the mean difference and coarsescores.
Configuration 1 slightly outperforms con-figuration 2 on the mean numeric difference scores,whilst configuration 2 is very close to and slightlyC Average differences (numeric) Precision (coarse)ALL A-N S-V V-O ALL A-N S-V V-O1 17.95 18.56 20.80 15.58 53.4 63.5 19.2 62.52 18.35 19.62 20.20 15.73 54.2 63.5 19.2 65.03 25.59 24.16 32.04 23.73 44.9 40.4 42.3 52.5R 32.82 34.57 29.83 32.34 29.7 28.8 30.0 30.8A 16.86 17.73 15.54 16.52 58.5 65.4 34.6 65.0Table 1: Evaluation results of the three system configura-tions and two baselines on the test dataset.
Best systemscores on each grammatical subtype highlighted in bold.better than configuration 1 on the coarse precisionscores.
The exception is that configuration 3 was thebest performer on the coarse precision scoring forthe S-V subtype.The R baseline is outperformed by configurations1, 2 and 3; roughly speaking where 1 and 2 out-perform R by d, configuration 3 outperforms R byaround d/2.
The A baseline generally outperformsall our system configurations.
It seems to be also aquite competitive baseline for other systems partici-pating in the shared task.The other trend apparent from the table is that per-formance on the V-O and A-N subtypes tends to ex-ceed that on the the S-V subtype.An examination of the gold standard testfiles shows that the distribution over thelow/medium/high categories is similar for bothV-O and A-N, in both cases close to 0.08/0.27/0.65,with high covering nearly two-thirds of cases,whilst for S-V the distribution is quite different:0.0/0.654/0.346, with medium covering nearlytwo-thirds of cases.
This is reflected in the Abaseline precision scores, as for each subtype thesewill necessarily be the proportion of gold-standardhigh cases.
This explains for example why the Abaseline is much poorer on the S-V cases (34.6)than on the other cases (65.0, 65.4).Looking further into the differences between thethree subtypes, Figure 2 shows the gold standard nu-meric score distribution across the three collocationsubtypes (Test GS), and the corresponding distribu-tions for scores from the system?s first configuration(Conf 1).
This shows in more detail the nature ofthe poorer performance on S-V, with the gold stan-dard having a peak around 50-60, and the systemhaving a peak around 70-80.
For the other subtypes51Percentof Total01020300 20 40 60 80 100A?NConf 1 S?VConf 10 20 40 60 80 100V?OConf 1A?NTest GS0 20 40 60 80 100S?VTest GS0102030V?OTest GSFigure 2: The distribution of the gold standard numericscore vs. the distribution of the system?s first configura-tion numeric scores.A-N S-V V-OInstances 177254 11092 121317Avg intervening 0.0684 0.3867 0.4612Table 2: Some corpus statistics: the number of matchedcollocations per subtype (Instances) and the averagenumber of intervening words per subtype (Avg interven-ing).the contrast in the distributions seems broadly con-sistent with the mean numeric difference scores ofTable 1.One can speculate on the reasons for the system?spoorer performance on the S-V subtype.
The sys-tem treats intervening words in a collocation in aparticular way, namely by ignoring them.
This isone option, and another would be to include them asfeatures counted in the vectors.
Table 2 shows theaverage intervening words in the occurrences of thecollocations.
S-V and V-O are alike in this respect,both being much more likely to present interveningwords than collocations of the A-N subtype.
So theexplanation of the poorer performance on S-V can-not lie there.
Also because the average number ofintervening words is low, we believe it is unlikelythat including them as features will impact perfor-mance significantly.Table 2 also gives the number of matched collo-cations per subtype.
The number for the S-V collo-cations is an order of magnitude smaller than for theother subtypes.
Although the collocations suppliedby the organisers are in their base form, the systemattempts to match them ?as is?
in the unlemmatisedversion of the corpus.
Whilst for A-N and V-O thebase-form sequences relatively frequently do doubleservice as inflected forms, this is far less frequentlythe case for the S-V sequences (e.g.
user see (S-V) is far less common than make money (V-O) ).This much smaller number of occurrences for S-Vcases, or the fact that they are drawn from syntac-tically special contexts, may be a factor in the rel-atively poorer performance.
This perhaps is also afactor in the earlier noted fact that although config-uration 3 was generally outperformed, on the S-Vsubtype the reverse occurs.The unlemmatised version of the corpus was usedbecause initial experimentation with the validationset produced slightly better results when employingraw words as features rather than lemmas.
A possi-bility for future work would be to to refer to lemmasfor matching collocations in the corpus, but to con-tinue to use unlemmatised words as features.Other areas for future investigation involve the ef-fects of weighting schemes (such as IDF) and theuse of similarity measures other than cosine, aswell as alternatives in configurations 2 and 3.
Forexample, configuration 2 could involve the modifierin the computation of the compositionality score,and configuration 3 could create separate clusteringspaces for collocation, headword and modifier andcompute similarity scores based on vectors represen-ting these clusters.In sum, the simplest configuration of a totally un-supervised system yielded surprisingly good resultsat measuring compositionality of collocations in rawcorpora, and whereas there is scope for further de-velopment and refinement, the system as it is consti-tutes a robust baseline to compare against more ela-borate systems.5 AcknowledgementsWe would like to thank our anonymous reviewers fortheir insightful comments and ideas.
This research issupported by the Science Foundation Ireland (Grant07/CE/I1142) as part of the Centre for Next Gene-ration Localisation (www.cngl.ie) at Trinity CollegeDublin.52ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226, February.Chris Biemann and Eugenie Giesbrecht.
2011.
Distribu-tional Semantics and Compositionality 2011: SharedTask Description and Results.
In Proceedings of theDistributional Semantics and Compositionality work-shop (DISCo 2011) in conjunction with ACL 2011,Portland, Oregon.Diana McCarthy, Bill Keller, and John Carroll.
2003.Detecting a continuum of compositionality in phra-sal verbs.
In Proceedings of the ACL 2003 workshopon Multiword expressions: analysis, acquisition andtreatment-Volume 18, pages 73?80, Sapporo.
Associa-tion for Computational Linguistics.Ted Pedersen.
2010.
Duluth-WSI: SenseClusters appliedto the sense induction task of SemEval-2.
In Procee-dings of the 5th International Workshop on Seman-tic Evaluation, number July, pages 363?366, Uppsala,Sweden.
Association for Computational Linguistics.Amruta Purandare and Ted Pedersen.
2004.
Wordsense discrimination by clustering contexts in vectorand similarity spaces.
Proceedings of the Conferenceon Computational Natural Language Learning, pages41?48.Hinrich Sch?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.53
