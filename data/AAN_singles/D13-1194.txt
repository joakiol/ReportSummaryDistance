Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1892?1897,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDetection of Product Comparisons ?
How Far Does an Out-of-the-boxSemantic Role Labeling System Take You?Wiltrud Kessler and Jonas KuhnInstitute for Natural Language ProcessingUniversity of Stuttgartwiltrud.kessler@ims.uni-stuttgart.deAbstractThis short paper presents a pilot study in-vestigating the training of a standard Seman-tic Role Labeling (SRL) system on productreviews for the new task of detecting com-parisons.
An (opinionated) comparison con-sists of a comparative ?predicate?
and up tothree ?arguments?
: the entity evaluated posi-tively, the entity evaluated negatively, and theaspect under which the comparison is made.In user-generated product reviews, the ?predi-cate?
and ?arguments?
are expressed in highlyheterogeneous ways; but since the elementsare textually annotated in existing datasets,SRL is technically applicable.
We address theinteresting question how well training an out-of-the-box SRL model works for English data.We observe that even without any feature en-gineering or other major adaptions to our task,the system outperforms a reasonable heuristicbaseline in all steps (predicate identification,argument identification and argument classifi-cation) and in three different datasets.1 IntroductionSentiment analysis deals with the task of determin-ing the polarity of an opinionated document or asentence, in product reviews typically with regardto some target product.
A common way to expresssentiment about some product is by comparing it to adifferent product.
In the corpus data we use, around10% of sentences contain at least one comparison.Here are some examples of comparison sentencesfrom our corpus:(1) a.
?
[This camera]E+ .
.
.
its [screen]A is much big-ger than the [400D].?b.
?
[D70]E+ beats [EOS 300D]E?
in almost [ev-ery category]A, EXCEPT ONE.?c.
?
[Noise suppression]A1A2 was generallybetter1 than the [D80]E?1 ?s and much better2than the [Rebel]E?2 ?s.?d.
?A striking difference between the [EOS350D]E?
and the new [EOS 400D]E+ concernsthe [image sensor]A.?Note that our definition of comparisons is broaderthan the linguistic category of comparative sen-tences, which only includes sentences that containa comparative adjective or adverb.
For our work,we consider comparisons expressed by any Part ofSpeech (POS).A comparison contains several parts that must beidentified in order to get meaningful information.We call the word or phrase that is used to express thecomparison (?better?, ?beats?, .
.
. )
a comparativepredicate.
A comparison involves two entities, oneor both of them may be implicit.
In our data, mostof the entities are products, e.g., the two cameras?D70?
and ?EOS 300D?
in sentence 1b.
In gradedcomparisons, entity+ (E+) is the entity that is beingevaluated positively, entity- (E-) the entity evaluatednegatively.
In many sentences one attribute or partof a product is being compared, like ?image sensor?in sentence 1d.
We call this the aspect (A).The task we want to solve for a given compari-son sentence is to detect the comparative predicate,the entities that are involved and the aspect that isbeing compared.
We borrow our methodology fromSemantic Role Labeling (SRL).
In SRL, events areexpressed by predicates and participants of theseevents are expressed by arguments that fill differ-ent semantic roles.
Adapted to the problem of de-tecting comparisons, the events we are interested inare comparative predicates and the arguments are thetwo entities and the aspect that is being compared.Due to the diversity of possible ways of express-ing comparisons, the ?predicates?
and ?arguments?1892in this task are more heterogeneous categories thanin standard SRL based on PropBank and Nom-Bank annotations.
Moreoever, the existing labeleddatasets are based on an annotation methodologywhich gave the annotators a lot of freedom in de-ciding on the linguistic anchoring of the ?predicate?and ?arguments?.
This adds to the heterogeneity ofthe observed constructions and makes it even moreinteresting to ask the question how far an out-of-the-box SRL model can take you.In this work, we re-train an existing SRL system(Bjo?rkelund et al 2009) on product review data la-beled with comparative predicates and arguments.We show that we can get reasonable results with-out any feature engineering or other major adap-tions.
This is an encouraging result for a linguis-tically grounded modeling approach to comparisondetection.2 Related WorkThe syntax and semantics of comparative sentenceshave been the topic of research in linguistics for along time (Moltmann, 1992; Kennedy, 1999).
How-ever, our focus is on computational methods and wealso treat comparisons that are not comparative sen-tences in a linguistic sense.In sentiment analysis, some studies have been pre-sented to identify comparison sentences.
Jindal andLiu (2006a) report good results on English usingclass sequential rules based on keywords as featuresfor a Naive Bayes classifier.
A similar approach forKorean is presented by Yang and Ko (2009; 2011b;2011a).
In our work, we do not address the task ofidentifying comparison sentences, we assume thatwe are given a set of such sentences.The step we are concerned with is the detection ofrelevant parts of a comparison.
To identify entitiesand aspect, Jindal and Liu (2006b) use an involvedpattern mining process to mine label sequential rulesfrom annotated English sentences.
A similar ap-proach is again presented by Yang and Ko (2011a)for Korean.
In contrast to their complicated process-ing, we simply use an existing SRL system out ofthe box.
Both approaches consider only nouns andpronouns for entities and aspects, we use all POSand allow for multi-word arguments.
Jindal and Liu(2006b) base the recognition of comparative predi-cates on a list of manually compiled keywords.
Weuse this as our baseline.
Our approach is not de-pendent on a set of keywords and is therefore moreeasily adaptable to a new domain.All works label the entities according to their po-sition with respect to the predicate.
This requires theidentification of the preferred entity in a non-equalcomparison as an additional step.
Ganapathibhotlaand Liu (2008) use hand-crafted rules based on thepolarity of the predicate for this task.
As we labelthe entities with their roles from the start, we solveboth problems at the same time.Xu et al(2011) cast the task as a relation extrac-tion problem.
They present an approach that usesconditional random fields to extract relations (bet-ter, worse, same and no comparison) between twoentitites, an attribute and a predicate phrase.The approach of Hou and Li (2008) is most re-lated to our approach.
They use SRL with standardSRL features to extract comparative relations fromChinese sentences.
We confirm that SRL is a vi-able method also for English.
In their experimentsthey report good results on gold parses, but observea drop in performance when they use their methodon automatic parses.
All our experiments are con-ducted on automatically obtained parses.3 ApproachThe input to our system is a sentence that we assumeto contain at least one comparison.
The result of ourprocessing are one or more comparative predicatesand for each predicate three arguments: The two en-tities that are being compared, and the aspect theyare compared in.
More formally speaking, for ev-ery sentence we expect to get one or more 4-tupels(predicate, entity+, entity-, aspect).
Entity+ is theentity that is being evaluated as better than entity-.Any of the arguments may be empty.
Currently, wetreat only single words as comparative predicates.Annotated multi-word predicates are mapped to oneword.
We allow for multi-word arguments, but an-notate only the head word of the phrase and treat itas a one word argument for evaluation.
We do notplace any restrictions on possible POS.We use a standard pipeline approach from SRL.As a first step, the comparative predicate is iden-tified.
The next step in SRL would be predicate1893disambiguation to identify the different frames thispredicate can express.
As we do not have suchframe information, predicate disambiguation is notperformed in our pipeline.After we have identified the predicates, the nextstep is to identify their arguments.
The identifica-tion step is a binary classification whether a word inthe sentence is some argument of the identified pred-icate.
As a final classification step, it is determinedfor each found argument whether this argument isentity+, entity- or the aspect.We use an existing SRL system (Bjo?rkelund et al2009)1 and the features developed for SRL, based onthe output of the MATE dependency parser (Bohnet,2010).
Features use attributes of the predicate itself,its head or its dependents.
Additionally, for argu-ment identification and classification there are fea-tures that describe the relation of predicate and argu-ment, the argument itself, its leftmost and rightmostdependent and left and right sibling.For the classification tasks of the pipeline, theSRL system uses regularized linear logistic regres-sion from the LIBLINEAR package (Fan et al2008).
We set the SRL system to train separate clas-sifiers for predicates of different POS.
In preliminaryexperiments, we have found this to perform slightlybetter than training one classifier for all kinds ofpredicates, although the difference is not significant.We do not use the reranker.4 ExperimentsData.
We use the JDPA corpus2 by J. Kessler et al(2010) for our experiments.
It contains blog postsabout cameras and cars.
We use the annotation class?Comparison?
that has four annotation slots.
Weconvert the ?more?
slot to entity+, the ?less?
slot toentity- and the ?dimension?
slot to the aspect.
Fornow, we ignore the ?same?
slot which indicates ifthe two mentions are ranked as equal.We have also tested our approach on the datasetused in (Jindal and Liu, 2006b)3.
We use all com-1http://code.google.com/p/mate-tools/2Available from http://verbs.colorado.edu/jdpacorpus/ ?
we ignore cars batch 009 where noarguments of comparative predicates are annotated.3Available from http://www.cs.uic.edu/?liub/FBS/data.tar.gz ?
although the original paper works onsome unknown subset of this data, so our results are not directlyJDPA J&Lcameras carsall sentences 5230 14003 7986comparison sentences 505 1094 649predicates 642 1327 695distinct predicates 147 252 122preds.
occurring once 87 147 61Entity+ / 1 517 1091 657Entity- / 2 511 1068 331Aspect 623 1107 526Table 1: Statistics about the datasetsparisons annotated as types 1 to 3 (ignoring type 4,non-gradable comparisons).
In this dataset (J&L),entities are annotated as entity 1 or entity 2 depend-ing on their position before or after the predicate.We keep this annotation and train our system to as-sign these labels.We do sentence segmentation and tokenizationwith the Stanford Core NLP4.
Annotations aremapped to the extracted tokens.
We ignore anno-tations that do not correspond to complete tokens.In the JDPA corpus, if an annotated argument is out-side the current sentence, we follow the coreferencechain to find a coreferent annotation in the same sen-tence.
If this is not successful, the argument is ig-nored.
We extract all sentences where we found atleast one comparative predicate as our dataset.Table 1 shows some statistics of the data.Evaluation Setup.
We evaluate on each datasetseparately using 5-fold cross-validation.
We reportprecision (P), recall (R), F1-measure (F1), and forargument classification macro averaged F1-measure(F1m) over the three arguments.
Bold numbers de-note the best result in each column and dataset.
Wemark a F1-measure result with * if it is significantlyhigher than all previous lines.5Results on Predicates.
We have implementedtwo baselines based on previous work.
The sim-plest baseline, BL POS classifies all tokens witha comparative POS (?JJR?, ?JJS?, ?RBR?, ?RBS?
)as predicates.
A more sophisticated baseline, BLKeyphrases, uses a list of about 80 manually com-comparable to the results reported there.4http://nlp.stanford.edu/software/corenlp.shtml5Statistically significant at p < .05 using the approximaterandomization test (Noreen, 1989) with 10000 iterations.1894P R F1cams BL POS 66.6 38.2 48.5BL Keyphrases 53.1 62.8 57.5?SRL 73.8 58.7 65.4?carsBL POS 62.5 34.7 44.6BL Keyphrases 51.9 56.5 54.1?SRL 73.2 55.5 63.2?J&LBL POS 74.3 52.9 61.8BL Keyphrases 61.5 80.0 69.5?SRL 77.0 68.1 72.3?Table 2: Results predicate identificationP R F1cams BL 49.4 47.1 48.2SRL 66.5 38.0 48.4cars BL 50.2 50.1 50.1SRL 68.7 42.2 52.3?J&L BL 38.7 44.6 41.5SRL 68.5 45.2 54.5?Table 3: Results argument identification (gold predicates)Entity+ / 1 Entity- / 2 Aspect F1mP R F1 P R F1 P R F1cams BL 30.1 31.7 30.9 21.2 21.3 21.3 61.8 51.2 56.0 36.1SRL 38.6 17.4 24.0 43.7 24.5 31.4 69.9 47.7 56.7 37.3cars BL 31.1 32.7 31.9 23.0 24.0 23.5 49.3 44.5 46.8 34.0SRL 39.5 22.9 29.0 48.1 31.0 37.7 58.4 36.2 44.7 37.1?J&L BL 43.2 39.4 41.2 19.0 31.1 23.6 15.0 17.1 16.0 26.9SRL 58.3 47.2 52.1 60.8 35.6 45.0 58.8 30.6 40.3 45.8?Table 4: Results argument classification (gold predicates)piled comparative keyphrases from (Jindal and Liu,2006a) in addition to the POS tags.Table 2 shows the result of our experiments.
Ourmethod significantly outperforms both baselines inall datasets.
The generally low recall values aremainly a result of the wide variety of predicates thatare used to express comparisons (see Discussion).Results on Arguments.
To get results indepent ofthe errors introduced by the relatively low perfor-mance on predicate identification, we use annotatedpredicates (gold predicates) as a starting point forthe argument experiments.
All results drop about10% when system predicates are used.As a baseline (BL) for argument identificationand classification, we use some heuristics based onthe characteristics of our data.
Most entities are(pro)nouns and most predicates are positive, so weclassify the first noun or pronoun before the predi-cate as entity+ (entity 1 for J&L) and the first nounor pronoun after the predicate as a entity- (entity 2).If the predicate is a comparative adjective, we clas-sify the predicate itself as aspect, because this typeof annotation is very frequent in the JDPA data.
Forother predicates except nouns and verbs, we classifythe direct head of the predicate as aspect.Table 3 shows the results for argument identifica-tion, the results for argument classification can beseen in Table 4.
Our system outperforms the base-line for all datasets.
The differences are significantexcept for the cameras dataset.
In general, the num-bers are low.
We will discuss some reasons for thisin the next section.5 DiscussionSparseness.
There are many ways to express acomparison and the size of the available trainingdata is relatively small.
This strongly influences therecall of our system as many predicates and argu-ments occur only once.
As we can see in Table 1,60% of the predicates in the cameras dataset occuronly once.
In contrast, only 12 predicates occur tentimes or more.
The trends are similar in the otherdatasets.
This particularily affects verbs and nouns,where many colloquial expressions are used (?ham-mers?, ?pwns?, ?go head to head with?, ?put X tothe sword?, .
.
.
).Argument identification and classification wouldbenefit from generalizing over the many differentproduct identifiers like ?EOS 5D?
or ?D200?.
Wewant to try to use a Named Entity Recognition sys-tem trained on this type of entities for this purpose.1895Sentiment Relevance.
The following examplesshow a problem that is typical for sentiment analysisand responsible for many false positive predicates:(2) a.
?Relatively [lower]A noise at higher ISO .
.
.
?b.
?.
.
.
but [higher]A then [Sony]E+?Although ?higher?
often expresses a comparisonlike in sentence 2b, in sentence 2a it only describesa camera setting and should not be extracted as acomparative predicate.
There has been considerablework in the areas of subjectivity classification (Wil-son and Wiebe, 2003) and the related sentiment rel-evance (Scheible and Schu?tze, 2013) which we willtry to use to detect such irrelevant, ?descriptive?
usesof comparative words.Linguistic anchoring.
In contrast to SRL, the taskof comparison detection in reviews is a relativelynew task without universally recognized definitionsand annotation schemes.
The annotators of the cor-pora had a lot of freedom in their choice of linguis-tic anchoring of the predicates and arguments.
Con-sider these examples from the cameras dataset:(3) a.
?
[Lighter]A in weight compared to the[others]E?.?b.
?.
.
.
[its]E+ [better]A and faster compared vsthe [SB800 flash]E?
as well.?c.
?.
.
.
this camera?s [screen]E+ is [smaller]A thanthe [ones]E?
on some competing models .
.
.
?Sentences 3a and 3b show a situation where twowords are used to express the same comparison andit is unclear which one to chose as a predicate.
Thedecision is left to the individual annotators.There is some variety of annotations on argumentsas well.
In the JDPA data, a comparative adjectiveis often annotated as aspect, sometimes even whenthere is an alternative, e.g., ?weight?
in sentence 3a.Also, for a phrase like ?its screen?, we find ?screen?annotated as the aspect (sentence 1a) or an entity(sentence 3c) ?
and both have their merit.
We wantto further study how different linguistic anchoringsof comparisons effect classification performance.Equative comparisons.
As we can see from theconfusion matrix of our system, the distinction be-tween entity+ and entity- is very difficult to learn.In graded comparisons, the distinction is informa-tive, but sentiment information would be needed forthe correct assignment.
There are also some prob-lematic cases where the ranking cannot be inferredwithout the broader context, e.g., sentence 1d.A more annotation-related problem concernsequative comparisons, i.e., both entities are rated asequal.
The difference between entity+ and entity- ismeaningless in this case.
In the JDPA corpus, en-tities still have to be annotated as either entity+ orentity- and the annotation guidelines allow the anno-tator to choose freely.
As a result, the data is noisy,for the same predicate sometimes entity- is beforethe predicate, sometimes entity+.
If we eliminatethis noise by always assigning the entities in orderof surface position, we see a gain in macro averagedF1-measure for all systems of about 2% (cameras)to 4% (cars).6 ConclusionsWe presented a pilot experiment on using an SRL-inspired approach to detect comparisons (compara-tive predicate, entity+, entity-, aspect) in user gener-ated content.
We re-trained an existing SRL systemon data that is labeled with comparative predicatesand arguments.
Even without feature engineering ormajor adaptions, our approach outperforms the base-lines in three datasets in every task.
This is an en-couraging result for a linguistically grounded mod-eling approach to comparison detection.For future work, we plan to include features thathave been tailored specifically to the task of detect-ing product comparisons.
To address the inherent di-versity of expressions typical for user generated con-tent, we want to employ generalization techniques,e.g., to detect product names.
We also want to fur-ther study the different possible linguistic anchor-ings of comparisons and their effect on classificationperformance.
Studies of this kind may also informfuture data annotation efforts in that certain waysof anchoring the elements of a comparison linguis-tically may be more helpful than others.
We alsobelieve that the explicit modeling of different types(equative, superlative, non-equal gradable) of com-parisons will have a positive effect on performance.AcknowledgmentsThe work reported in this paper was supported by aNuance Foundation Grant.1896ReferencesAnders Bjo?rkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual Semantic Role Labeling.
In Pro-ceedings of CoNLL ?09 Shared Task, pages 43?48.Bernd Bohnet.
2010.
Very high accuracy and fast depen-dency parsing is not a contradiction.
In Proceedings ofCOLING ?10, pages 89?97.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
Liblinear: A libraryfor large linear classification.
J. Mach.
Learn.
Res.,9:1871?1874, June.Murthy Ganapathibhotla and Bing Liu.
2008.
Miningopinions in comparative sentences.
In Proceedings ofCOLING ?08, pages 241?248.Feng Hou and Guo-hui Li.
2008.
Mining Chinese com-parative sentences by semantic role labeling.
In Pro-ceedings of ICMLC ?08, pages 2563?2568.Nitin Jindal and Bing Liu.
2006a.
Identifying compar-ative sentences in text documents.
In Proceedings ofSIGIR ?06, pages 244?251.Nitin Jindal and Bing Liu.
2006b.
Mining comparativesentences and relations.
In Proceedings of AAAI ?06,pages 1331?1336.Christopher Kennedy.
1999.
Projecting the Adjective:The Syntax and Semantics of Gradability and Compar-ison.
Outstanding Dissertations in Linguistics.
Gar-land Pub.Jason S. Kessler, Miriam Eckert, Lyndsay Clark, andNicolas Nicolov.
2010.
The 2010 ICWSM JDPA Sen-timent Corpus for the Automotive Domain.
In Pro-ceedings of ICWSM-DWC ?10.Friederike Moltmann.
1992.
Coordination and Compar-atives.
Ph.D. thesis, Massachusetts Institute of Tech-nology.Eric W. Noreen.
1989.
Computer-intensive methods fortesting hypotheses ?
an introduction.
Wiley & Sons.Christian Scheible and Hinrich Schu?tze.
2013.
Senti-ment relevance.
In Proceedings of ACL ?13, pages954?963.Theresa Wilson and Janyce Wiebe.
2003.
Annotatingopinions in the world press.
In Proceedings of SIGdial?03, pages 13?22.Kaiquan Xu, Stephen Shaoyi Liao, Jiexun Li, and YuxiaSong.
2011.
Mining comparative opinions from cus-tomer reviews for competitive intelligence.
Decis.Support Syst., 50(4):743?754, March.Seon Yang and Youngjoong Ko.
2009.
Extracting com-parative sentences from Korean text documents us-ing comparative lexical patterns and machine learningtechniques.
In Proceedings of the ACL-IJCNLP ?09,pages 153?156.Seon Yang and Youngjoong Ko.
2011a.
Extracting com-parative entities and predicates from texts using com-parative type classification.
In Proceedings of HLT?11, pages 1636?1644.Seon Yang and Youngjoong Ko.
2011b.
Finding relevantfeatures for Korean comparative sentence extraction.Pattern Recogn.
Lett., 32(2):293?296, January.1897
