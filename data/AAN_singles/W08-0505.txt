Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 21?22,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAdapting naturally occurring test suitesfor evaluation of clinical question answeringDina Demner-FushmanLister Hill National Center for Biomedical Communications,National Library of Medicine, NIH, Bethesda, MD 20894, USAddemner@mail.nih.govAbstractThis paper describes the structure of a testsuite for evaluation of clinical question an-swering systems; presents several manuallycompiled resources found useful for test suitegeneration; and describes the adaptation ofthese resources for evaluation of a clinicalquestion answering system.1 IntroductionThe community-wide interest in rapid developmentin many areas of natural language processing and in-formation retrieval resulted in creation of reusabletest collections in large-scale evaluations such as theText REtrieval Conference (TREC)1.
Researchers inmore specific areas, for which no TREC or other col-lections are available, have to create or find suitabletest collections to evaluate their systems.For example, Cramer et al (2006) recruited vol-unteers and quickly gathered a sizeable corpus ofquestion-answer pairs for evaluation of Germanopen-domain question answering systems.
This wasachieved through a Web-based tool that allowedmarking up ?interesting?
passages in Wikipedia ar-ticles and then asking questions about the contentof those passages.
This appealing approach can noteasily be applied in the domain of clinical ques-tion answering because the quality of the questionsand answers as well as the answer completenessare paramount.
A test suite for evaluation of clini-cal question answering systems should contain a set1http://trec.nist.gov/of real-life questions asked by clinicians and high-quality answers compiled by experts.
The answersshould be presented in the form deemed useful byclinicians.One of the benefits of focusing on a specific do-main, such as clinical question answering, is that theuser-needs and desirable results are well-studied andtheir descriptions are readily-available.
In the caseof clinical question answering, clinicians?
desider-ata are: to see a ?bottom-line advice?
first, haveon-demand access to the context that was used ingeneration of the advice, and finally have accessto the original sources of information (Ely et al,2005).
A fair number of high-quality manually cre-ated collections present answers to clinical questionsin this form and could be obtained online.
Three par-tially freely-available sources: Family PractitionerInquiry Network (FPIN)2, Parkhurst Exchange Fo-rum (PE)3, and BMJ Clinical Evidence (BMJ-CE)4were used to design and develop the presented testsuites and evaluation methods.Although there seems to be a distinction betweentest collections and test suites (Co-hen et al, 2004) (the former defined as ?piecesof text?
and associated with corpora, the latter, aslists of specially constructed sentences, or sentencesequences, or sentence fragments (Balkan et al,1994)), evaluation of answers to clinical questionscrosses this boundary and requires the availabilityof carefully generated sentence fragments as well assuitable document collections.2http://www.primeanswers.org/primeanswers/3http://www.parkhurstexchange.com/qa/index.php4http://www.clinicalevidence.com/ceweb/conditions/index.jsp212 Test suite structureThe multi-tiered answer model of the FPIN andBMJ-CE resources is adapted in this work.
The toptier contains the ?bottom-line advice?.
FPIN pro-vides the key-points of the advice in the form of ashort sentence sequence, whereas BMJ-CE providesa list of sentence fragments (see Figure 1).
Bothsources employ experts in question areas to care-fully construct the answers.
The second tier elab-orates each of the key-points in 2-3 paragraph-longsummaries generated by the same experts.
The thirdtier provides references to the original sources usedin answer compilation.Likely to be beneficial:?
Angiotensin converting enzyme inhibitors?
Aspirin?
?
Blockers .
.
.Trade-off between benefits and harms:?
Nitrates (in the absence of thrombolysis)Likely to be ineffective or harmful: .
.
.Figure 1: The top tier of a multi-tiered answer to the clin-ical question How to improve outcomes in acute myocar-dial infarction?
contains key-points generated by a panelof cardiologists.3 Using the test suite in an evaluationThe answer presented in Figure 1 can be used toevaluate a system?s answer to this question by ex-tracting the reference list from the FPIN or BMJ-CEanswer.
Similarly, the second-tier summaries can beused to evaluate the context for the key-points gener-ated by a system.
The references can be used to eval-uate the quality of the original sources retrieved by asystem if the documents in both lists are representedusing their unique identifiers: DOI or a PubMed5identifier.
Availability of these test suites providesfor the following evaluation forms:?
diagnostic, in which developers could evaluatehow a tier is affected by changes in its ownmodule(s) or in the underlying tiers;5http://www.ncbi.nlm.nih.gov/sites/entrez?
task-oriented, in which the system is evaluatedas a whole on its ability to answer clinical ques-tions.It is conceivable to evaluate a system as a wholeby evaluating its performance in each tier and thencombining the results.
In a task-oriented evalua-tion, it seems reasonable to evaluate the quality ofthe first-tier answer and verify the adequacy of thesecond-tier context.3.1 CaveatsEven the simplest case of the top-tier evaluation,checking the list of fragments generated by a sys-tem against the reference list, ideally should be con-ducted manually by a person with biomedical back-ground.
For example, Acetylsalicylic acid in a sys-tem?s answer needs to be matched to Aspirin in thereference list.
Automation of this step is possiblethrough mapping of both lists to an ontology, e.g.,UMLS6, but such evaluation will be significantlyless accurate and potentially biased (if a system usesthe same mapping algorithm to find the answer).A manual evaluation based on 30 of 54 BMJ-CEquestion-answer pairs in the presented test suite isdescribed in (Demner-Fushman and Lin, 2006).
An-other 50 question-answer pairs originated in FPINand PE.ReferencesCramer I., Leidner J.L.
and Klakow D. 2006.
Buildingan Evaluation Corpus for GermanQuestion Answeringby Harvesting Wikipedia.
LREC-2006, Genoa, Italy.Cohen K.B., Tanabe L., Kinoshita S., and Hunter L.2004.
A resource for constructing customized testsuites for molecular biology entity identification sys-tems.
HLT-NAACL 2004 Workshop: Biolink 2004,Boston, MassachusettsBalkan L., Netter K., Arnold D. and Meijer S. 1994.TSNLP.
Test Suites for Natural Language Processing.Language Engineering Convention, Paris, France.Ely J.W., Osheroff J.A., Chambliss M.L., Ebell M.H.and Rosenbaum M.E.
2005.
Answering Physicians?Clinical Questions: Obstacles and Potential Solutions.JAMIA, 12(2):217?224.Demner-Fushman D. and Lin J.
2006.
Answer Extrac-tion, Semantic Clustering, and Extractive Summariza-tion for Clinical Question Answering.
ACL 2006, Syd-ney, Australia6http://www.nlm.nih.gov/research/umls/22
