Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 857?866,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDetecting Deceptive Groups Using Conversations and Network AnalysisDian Yu1, Yulia Tyshchuk2, Heng Ji1, William Wallace21Computer Science Department, Rensselaer Polytechnic Institute2Department of Industrial and Systems Engineering, Rensselaer Polytechnic Institute1,2{yud2,tyshcy,jih,wallaw}@rpi.eduAbstractDeception detection has been formulatedas a supervised binary classification prob-lem on single documents.
However, indaily life, millions of fraud cases involvedetailed conversations between deceiversand victims.
Deceivers may dynamicallyadjust their deceptive statements accord-ing to the reactions of victims.
In addition,people may form groups and collaborateto deceive others.
In this paper, we seek toidentify deceptive groups from their con-versations.
We propose a novel subgroupdetection method that combines linguis-tic signals and signed network analysis fordynamic clustering.
A social-eliminationgame called Killer Game is introduced as acase study1.
Experimental results demon-strate that our approach significantly out-performs human voting and state-of-the-art subgroup detection methods at dynam-ically differentiating the deceptive groupsfrom truth-tellers.1 IntroductionDeception generally entails messages and infor-mation intentionally transmitted to create a falseconclusion (Buller et al, 1994).
Deception detec-tion is an important task for a wide range of ap-plications including law enforcement, intelligencegathering, and financial fraud.
Most of the previ-ous work (e.g., (Ott et al, 2011; Feng et al, 2012))focused on content analysis of a single documentin isolation (e.g., a product review).
The promot-ers of a product may post fake complimentary re-views, while their competitors may hire people towrite fake negative reviews (Ott et al, 2011).1The data set is publicly available for research purposesat: http://nlp.cs.rpi.edu/data/killer.zipHowever, when we want to detect deceptionfrom text or voice conversations, the deception be-havior may be affected by the following factors be-yond textual statements.1.
Dynamic.
Recent research in social sciencesuggests that deception communication is dy-namic and involves interactions among peo-ple (e.g., (Buller and Burgoon, 1996)).
Addi-tionally, the research postulates that human?scapacity to learn by observation enables himto acquire large, integrated units of behav-ior by example (Bandura, 1971).
Therefore,a person?s behavior concerning deception ortruth-telling can change constantly, while helearns from others?
statements during conver-sations.2.
Global.
People may form groups for purposeof deception.
Research in social psychologyhas shown that an individual?s object-relatedbehavior may be affected by the attitudes ofother people due to group dynamics (Fried-kin, 2010).Recent studies typically have been conductedover ?static?
written or oral deceptive statements.There is no obligatory requirement for communi-cation between the author and the readers of thesestatements (Yancheva and Rudzicz, 2013).
As aresult, a victim of deception tends to trust the sto-ry mainly based on the statement he reads (Ott etal., 2011).
However, in daily life, millions of fraudcases involve detailed conversations between de-ceivers and victims.
A deceiver may make a state-ment, which is partially true in order to deceiveor mislead victims and adjust his deceptive strate-gies based on the reactions of victims (Zhou et al,2004).
Therefore, it is more challenging to identitya deceiver in an interactive process of deception.Most deception detection research addressed in-dividual deceivers, but deceivers often act in pairsor larger groups (Vrij et al, 2010).
The interac-857Identify a player?sattitude toward otherplayers based onhis statement duringeach roundClustering ???Subgroups??
?Signed Network(each round)1 1 -1 -1 -1-1??
?Player Attitude Profile(each round)????
?
?1  ?1 1?1 1 ?11 ?1 1PartitionSubgroups???ClusterEnsemblesSubgroups??
?Figure 1: Deceptive group detection for a single round.tions within a deceptive group have been ignored.For example, a product review from a deceivermay be supported by his teammates so that hisdeceptive comments can be read by more poten-tial buyers.
In this case, we can identify a decep-tive group based on their collaborations and com-mon characteristics, which is more promising thanthe typical methods of classifying individual state-ments as deceptive or trustworthy.In order to identify deceptive groups by analyz-ing the evolution of a person?s deception strategyduring his interactions with victims and the inter-actions within the deceptive group from conver-sations, we use a social-elimination game calledKiller Game which contains the ground-truth ofsubgroups.The killer game has many variants that involvedifferent roles and skills.
We choose a classicalversion played by three roles/teams: detectives, c-itizens, and killers.
The role of each player (gameparticipant) is randomly assigned by a third-partygame judge.
Every killer/detective is given the i-dentities of his teammates.
There are two alter-nating phases of the game: ?night?, when killer-s may covertly ?murder?
a player and detectivesmay learn one player?s role; and ?day?, when sur-viving players are informed of who was killed last?night?
and then asked to speculate about the rolesof other surviving players.
Before a ?day?
ends,every surviving player should vote for a suspect.The candidate with the most votes is eliminated.
Aplayer?s identity is not exposed after his ?death?.The game continues until all killers have been e-liminated or all detectives have been killed.
Thekillers are treated as deceivers, and citizens anddetectives as truth-tellers.In this paper, we present an unsupervised ap-proach for differentiating the deceptive groupsfrom truth-tellers in a game.
During each round,we use Natural Language Processing (NLP) tech-niques to identify a player?s attitude toward otherplayers (Section 2), which are used to construc-t a vector of attitudes for each surviving player(Section 3.1) and a signed social network repre-sentation (Section 3.2) for the discussions.
Thenwe use a clustering algorithm to cluster the atti-tude vector space and obtain results for each round(Section 3.1).
We also implement a greedy op-timization algorithm to partition the singed net-work based on the attitude clustering result (Sec-tion 3.2).
Finally, we apply a pairwise-similarityapproach that makes use of the predicted co-occurrence relations between players to combineall results from each round (Section 3.3).
Figure 1provides an overview of our system pipeline.The major novel contributions of this paper areas follows.?
This is the first study to investigate conversa-tions and deceptive groups for computerizeddeception detection.?
The proposed clustering technique is shownto be successful in separating deceptivegroups from truth-tellers.?
The method can be applied to dynamicallydetect subgroups in a network with discus-sants who tend to change their opinions.2 Attitude IdentificationIn this section, we describe how we take a player?sstatement in a single round as input to extract hisattitudes toward other players and represent themby an attitude 3-tuple (speaker, target, polarity)list.
For this work, the polarity of attitudes (Bal-ahur et al, 2009) can be positive (1), negative (-1)or neutral (0).
A game log from a single round858will be used as our illustrative example, as shownin Figure 2.as shown in Figure 2.
(1), negative (-1) or neutral (0).
A game logfrom a single round will be used as our illustrativeexample, as shown in Figure 2.tuple list is: [(16, 16, +1), (16, 11, -1), (16, 2, -1),(16, 1, 0), (16, 3, 0), .
.
.
, (16, 15, 0)].target, polarity)) list.
For this work, the polarityof attitudes can be positive (1), negative (-1) andneutral (0).
A game log from a single round,as shown in Figure 2, will be used as our walk-through example.
For instance, given Player 16?sstatement, its attitude tuple list is: [(16, 16, +1),(16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3, 0), .
.
.
,(16, 15, 0)].target, polarity)) list.
For this work, the polarityof attitudes can be positive (1), negative (-1) andneutral (0).
A game log from a single round,as shown in Figure 2, will be used as our walk-through example.
For instance, given Player 16?sstatement, its attitude tuple list is: [(16, 16, +1),(16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3, 0), .
.
.
,(16, 15, 0)].represents them by an attitude 3-tuple ((speaker,target, polarity)) list.
For this work, the polarityof attitudes can be positive, negative and neutral.A game log from a single round, as shown inFigure 2, will be used as our walk-through exam-ple.
For instance, given Player 16?s statement, itsattitude tuple list is: [(16, 16, +1), (16, 11, -1), (16,2, -1), (16, 1, 0), (16, 3, 0), .
.
.
, (16, 15, 0)].Combining Group Conversations and Signed Network Analysis forDeception DetectionAbstractIn previous studies, deception detection wasformulated as a traditional binary classificationproblem.
For example, given a product review, su-pervised learning methods could predict whetherit was deceptive or genuine with high accuracy.However, in a daily life, millions of fraud casesinvolve detailed conversations between criminalsand victims.
Deceivers may adjust their decep-tive statements according to the reactions of theirvictims rather than simply making a single state-ment.
In addition, people may form a group andcollaborate to deceive others.
In this paper, weaim to identify a deceptive group from dynamicconversations.
We propose a novel subgroup de-tection method that combines content and signednetwork analyses for dynamic clustering.
A socialelimination game called Killer Game is introducedas a case study.
Experimental results demonstratethat our approach significantly outperforms hu-man voting and state-of-the-art subgroup detectionmethods at dynamically differentiating the decep-tive group from the people telling truths.1 IntroductionDeception generally entails messages and infor-mation intentionally transmitted to create a falseconclusion (Buller et al, 1994).
Deception detec-tion is an important task for a wide range of ap-plications including law enforcement, intelligencegathering, and financial fraud.
Most of the previ-ous work (e.g., (Ott et al, 2011; Feng et al, 2012))focused on a content analysis of a single documentin isolation (e.g., hotel reviews).
However, whenwe aim to detect deceptions from conversations,the deception behavior may be affected by otherfactors beyond textual statements.
These factorscan be both1.
Dynamic.
The interpersonal deceptiontheory (Buller and Burgoon, 1996) suggeststhat deception communication is not staticbut rather dynamic and involves interactionbetween people.
The social learningtheory (Bandura, 1971) states that human?scapacity to learn by observation enableshim/her to acquire large, integrated units ofbehavior by example.
Therefore, a person?sbehavior on deception or truth-telling canchange constantly, while s/he learns fromothers?
statements during conversation and2.
Global.
In some scenarios, people may formgroups for deception.
The social influencenetwork theory (Friedkin, 2010) states thatindividuals?
object-related behaviors may beaffected by the attitudes of other persons dueto the group dynamics.There are numerous ways to categorize decep-tion.
According to the distance between partici-pants, we can classify deception into two types.The first type is a face-to-face deception, wherecues include the deceiver?s body movement, pulse,facial expressions, etc..
The other type is basedon text or voice messages.
For example, inproduct reviews, promoters of a product may postfake complimentary reviews, while their com-petitors may hire people to write fake negativereviews (Ott et al, 2011).Recent studies have been typically conductedover ?static?
written or oral deceptive statements.There is no obligatory requirement for communi-cation between the author and readers for thesestatements (Yancheva and Rudzicz, 2013).
As aresult, a person being deceived tends to trust thestory mainly based on the statement she reads (Ottet al, 2011).
However, in daily life, millionsof fraud cases involve detailed conversations be-tween criminals and victims.
Deceivers may makea statement, which is partially true in order to de-ceive or mislead victims and adjust their deceptivestrategies based on the reactions of victims (Zhouet al, 2004).
Therefore, it is more challenging toFigure 2: Killer game sample log (1st round).C: citizen; D: detective; K: killerSystem: First Round.System: 15 was killed last night.15(C): I?m a citizen.
Over.16(K): I?m a good person.
11 and 2 are suspicious.1(K): I?m a good person.
It has been a long time since I played as akiller.
I?m a citizen.
I don?t want to comment on 16?s statement.2(C): I?m a detective.
6 was proved as a killer last night.
Over.3(C): I don?t know 2?s identity.
It?s hard to judge 16?s statement.
1seems to be a good person.
I?m a citizen.4(C): Citizen.
I cannot find a killer.
I trust 2 since 2 sounds a goodperson.
16 is suspicious.
I regard 16 as a killer.
I?m 2?s teammate.5(D): I?m a detective.
I verify 2?s identity and 2 is a killer.
13 is good.6(C): Why do you want to attack 2?
I don?t understand.
14 is suspicious.7(K): It?s hard to define 6?s identity.
4 may be a citizen.
I will vote for2.
6 sounds very strange and I found 6 very suspicious.
I will follow thedetective 5 to vote for 2.8(C): We should calm down.
7 seems to be a bad person.9(C): 1 and 7 seem to be killers.
There is no evidence to support 2 as adetective.
3 is a citizen.
4 is possibly a detective.
6 is also good.10(D): I agree with you.
7 must be a killer.
2 and 7 should debate.11(C): I don?t know 2 but I think 2 is good.
3 is good.
There should beone or two killers among 1, 4 and 7.12(K): 11 sounds like a killer.
2 is a killer.
I?m a citizen.
Vote for 2.13(D): 15 is a citizen.
16 is logically good.
I think 1, 8, 9, 10 are OK. Idon?
think 2 is killer.
I doubt 7?s intention.
Please vote for 7.14(D): 10, 13, 16 are good.
I don?t think 7 must be a killer.
2 isobviously bad.
I?m a citizen.System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2.
10, 13, 5, 2 vote for 7.
9,6 vote for 11.
2 is out.2.1 Target and Attitude Word IdentificationWe start by identifying targets and attitudesfrom the conversations.
In the killer game, atarget is represented by his/her player ID.
Wecreate a domain-specific vocabulary by applyingword segmentation and part-of-speech (POS)tagging (Zhang et al, 2003) to identify gameterms from the game?s website1and relateddiscussion forums.
We collected 41 terms intotal.
There are two kinds of game terms:positive and negative.
Positive attitude wordsinclude ?citizen?, ?good person?, ?good personcertified by the detectives?, ?detective?, etc..Negative attitude words include ?killer?, ?killerverified by the detectives?, ?a killer who claimedhimself/herself to be a detective?, etc.. We assignthe polarity score +1, -1 to positive and negativeterms respectively.2.2 Attitude-Target PairingThen we associate each attitude term with itscorresponding target.
We remove interrogativeand exclamatory sentences and only keep the sen-tences that include at least one attitude term froma player?s statement during each round.We propose a rule-based approach for attitude-target pairing: if there is at least one ID in thesentence, we associate all attitude terms in thatsentence with it.
Otherwise, if ?I?
is the onlysubject or there are no subjects at all, we associateattitude terms with the speaker.
We reverse thepolarity of an attitude word if it appears in anegation context.
For each attitude-target pair, wecheck the POS-tag sequence between them.
Ifthere exists an attitude term, a belief-oriented verbsuch as ?think?, ?believe?, ?feel?, or more thantwo verbs, we will discard this pair because thestatement is too subjective.For those targets, the speaker didn?t mentionor there is no positive/negative attitude word usedwhen they are mentioned, the attitude polarityscore is set to 0.3 ClusteringIn this section, we introduce a method to constructan attitude profile for each player and a signednetwork based on the attitude tuple list in Section 2and combine them to handle a dynamic networkwith discussants telling lies and truths, which hasnot been explored previously.1e.g., http://www.3j3f.com/how/Figure 2: Killer game sample log st round).C: citizen; D: detective; K: killerSystem: First Round.System: 15 was killed last night.15(C): I?m a citizen.
Over.16(K): I?m a good person.
11 and 2 are suspicious.1(K): I?m a good person.
It has been a long time since I played as akiller.
I?m a citizen.
I don?t want to comment on 16?s statement.2(C): I?m a detective.
6 was proved as a killer last night.
Over.3(C): I don?t know 2?s identity.
It?s hard to judge 16?s statement.
1seems to be a good person.
I?m a citizen.4(C): Citizen.
I cannot find a killer.
I trust 2 since 2 sounds a goodperson.
16 is suspicious.
I regard 16 as a killer.
I?m 2?s teammate.5(D): I?m a detective.
I verify 2?s identity and 2 is a killer.
13 is good.6(C): Why do you want to attack 2?
I don?t understand.
14 is suspicious.7(K): It?s hard to define 6?s identity.
4 may be a citizen.
I will vote for2.
6 sounds very weird and I found 6 very suspicious.
I will follow thedetective 5 to vote for 2.8(C): We should calm down.
7 seems to be a bad person.9(C): 1 and 7 seem to be killers.
There is no evidence to support 2 as adetective.
3 is a citizen.
4 is possibly a detective.
6 is also good.10(D): I agree with you.
7 must be a killer.
2 and 7 should debate.11(C): I don?t know 2 but I think 2 is good.
3 is good.
There should beone or two killers among 1, 4 and 7.12(K): 11 sounds like a killer.
2 is a killer.
I?m a citizen.
Vote for 2.13(D): 15 is a citizen.
16 is logically good.
I think 1, 8, 9, 10 are OK. Idon?t think 2 is a killer.
I doubt 7?s intention.
Please vote for 7.14(D): 10, 13, 16 are good.
I don?t think 7 must be a killer.
2 isobviously bad.
I?m a citizen.System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2.
10, 13, 5, 2 vote for 7.
9,6 vote for 11.
2 is out.2.1 Target and Attitude Word IdentificationWe start by identifying targets and attitudesfrom the conversations.
In the killer game, atarget is represented by his/her player ID.
Wecreate a domain-specific vocabulary by applyingword segmentation and part-of-speech (POS)tagging (Zhang et al, 2003) to identify gameterms from the game?s website1and relateddiscussion forums.
We collected 41 terms intotal.
There are two kinds of game terms:positive and negative.
Positive attitude wordsinclude ?citizen?, ?good person?, ?good personcertified by the detectives?, ?detective?, etc..Negative attitude words include ?killer?, ?killerverified by the detectives?, ?a killer who claimedhimself/herself to be a detective?, etc.. We assignthe polarity score +1, -1 to positive and negativeterms respectively.2.2 Attitude-Target PairingThen we associate each attitude term with itscorresponding target.
We remove interrogativeand exclamatory sentences and only keep the sen-tences that include at least one attitude term froma player?s statement during each round.We propose a rule-based approach for attitude-target pairing: if there is at least one ID in thesentence, we associate all attitude terms in thatsentence with it.
Otherwise, if ?I?
is the onlysubject or there are no subjects at all, we associateattitude terms with the speaker.
We reverse thepolarity of an attitude word if it appears in anegation context.
For each attitude-target pair, wecheck the POS-tag sequence between them.
Ifthere exists an attitude term, a belief-oriented verbsuch as ?think?, ?believe?, ?feel?, or more thantwo verbs, we will discard this pair because thestatement is too subjective.For those targets, the speaker didn?t mentionor there is no positive/negative attitude word usedwhen they are mentioned, the attitude polarityscore is set to 0.3 ClusteringIn this section, we introduce a method to constructan attitude profile for each player and a signednetwork based on the attitude tuple list in Section 2and combine them to handle a dynamic networkwith discussants telling lies and truths, which hasnot been explored previously.1e.g., http://www.3j3f.com/how/Figure 2: Killer game sample log (1st round).C: citizen; D: detective; K: killerSystem: First Round.System: 15 was killed last night.15(C): I?m a citizen.
Over.16(K): I?m a good person.
11 and 2 are suspicious.1(K): I?m a good person.
It has been a long time since I played as akiller.
I?m a citizen.
I don?t want to comment on 16?s statement.2(C): I?m a detective.
6 was proved as a killer last night.
Over.3(C): I don?t know 2?s identity.
It?s hard to judge 16?s statement.
1seems to be a good person.
I?m a citizen.4(C): Citizen.
I cannot find a killer.
I trust 2 since 2 sounds a goodperson.
16 is suspicious.
I regard 16 as a killer.
I?m 2?s teammate.5(D): I?m a detective.
I verify 2?s identity and 2 is a killer.
13 is good.6(C): Why do you want to attack 2?
I don?t understand.
14 is suspicious.7(K): It?s hard to define 6?s identity.
4 may be a citizen.
I will vote for2.
6 sounds very weird and I found 6 very suspicious.
I will follow thedetective 5 to vote for 2.8(C): We should calm down.
7 seems to be a bad person.9(C): 1 and 7 seem to be killers.
There is no evidence to support 2 as adetective.
3 is a citizen.
4 is possibly a detective.
6 is also good.10(D): I agree with you.
7 must be a killer.
2 and 7 should debate.11(C): I don?t know 2 but I think 2 is good.
3 is good.
There should beone or two killers among 1, 4 and 7.12(K): 11 sounds like a killer.
2 is a killer.
I?m a citizen.
Vote for 2.13(D): 15 is a citizen.
16 is logically good.
I think 1, 8, 9, 10 are OK. Idon?t think 2 is a killer.
I doubt 7?s intention.
Please vote for 7.14(D): 10, 13, 16 are good.
I don?t think 7 must be a killer.
2 isobviously bad.
I?m a citizen.System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 ?
?
?
10, 13, 5, 2 vote for7 ?
?
?
9, 6 vote for 11 ?
?
?
2 is out.2.1 Target and Attitud Word I entificationWe start by identifying targets and attitudesfrom the conversations.
In the killer game, atarget is represented by his/her player ID.
Wecreate a domain-specific vocabulary by applyingword segmentation and part-of-speech (POS)agging (Zhang e al., 2003) to id ntify gameterms from the game?s website1and relateddiscussion forums.
We collected 41 terms intotal.
There are two kinds of game terms:positive and negative.
Positive attitude wordsinclude ?citizen?, ?good person?, ?good personcertified by the detectives?, ?detective?, etc..Negative attitude words include ?killer?, ?killerverified by the detectives?, ?a killer who claimedhimself/herself to be a detective?, etc.. We assignthe polarity score +1, -1 to positive and negativeterms respectively.2.2 Attitude-Target PairingThen we associate each attitude term with itscorresponding target.
We remove interrogativeand exclamatory sentences and only keep the sen-tences that include at least one attitude term froma player?s statement during each round.We propose a rule-based approach for attitude-target pairing: if there is at least one ID in thesentence, we associate all attitude terms in thatsentence with it.
Otherwise, if ?I?
is the onlysubject or there are no subjects at all, we associateattitude terms with the speaker.
We reverse thepolarity of an attitude word if it appears in anegation context.
For each attitude-target pair, wecheck the POS-tag sequence between them.
Ifthere exists an attitude term, a belief-oriented verbsuch as ?think?, ?believe?, ?feel?, or more thantwo verbs, we will discard this pair because thestatement is too subjective.For those targets, the speaker didn?t mentionor there is no positive/negative attitude word usedwhen they are mentioned, the attitude polarityscore is set to 0.3 ClusteringIn this section, we introduce a method to constructan attitude profile for each player and a signednetwork based on the attitude tuple list in Section 2and combine them to handle a dynamic networkwith discussants telling lies and truths, which hasnot been explored previously.1e.g., http://www.3j3f.com/how/Figure 2: Killer game sample log (1st round).C: citizen; D: detective; K: killerSystem: First Round.System: 15 was killed last night.15(C): I?m a citizen.
Over.16(K): I?m a good person.
11 and 2 are suspicious.1(K): I?m a good person.
It has been a long time since I played as akiller.
I?m a citizen.
11 is suspicious and I don?t want to comment on16?s statement.2(C): I?m a detective.
6 was proved as a killer last night.
Over.3(C): I don?t know 2?s identity.
It?s hard to judge 16?s statement.
1seems to be a good person.
I?m a citizen.4(C): Citizen.
I cannot find a killer.
I trust 2 since 2 sounds a goodperson.
16 is suspicious.
I regard 16 as a killer.
I?m 2?s teammate.5(D): I?m a detective.
I verify 2?s identity and 2 is a killer.
13 is good.6(C): Why do you want to attack 2?
I don?t understand.
14 is suspicious.7(K): It?s hard to define 6?s identity.
4 may be a citizen.
I will vote for2.
6 sounds very weird and I found 6 very suspicious.
I will follow thedetective 5 to vote for 2.8(C): We should calm down.
7 seems to be a bad person.9(C): 1 and 7 seem to be killers.
There is no evidence to support 2 as adetective.
3 is a citizen.
4 is possibly a detective.
6 is also good.10(D): I agree with you.
7 must be a killer.
2 and 7 should debate.11(C): I don?t know 2 but I think 2 is good.
3 is good.
There should beone or two killers among 1, 4 and 7.12(K): 11 sounds like a killer.
2 is a killer.
I?m a citizen.
Vote for 2.13(D): 15 is a citizen.
16 is logically good.
I think 1, 8, 9, 10 are OK. Idon?t think 2 is a killer.
I doubt 7?s intention.
Please vote for 7.14(D): 10, 13, 16 are good.
I don?t think 7 must be a killer.
2 isobviously bad.
I?m a citizen.System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 ?
?
?
10, 13, 5, 2 vote for7 ?
?
?
9, 6 vote for 11 ?
?
?
2 is out.2.1 Target and Attitude Word IdentificationWe start by identifying targets and attitudes fromthe conversations.
In the killer game, a target isrepresented by his/her player ID and game termsare regarded as attitude words.
We collected41 terms in total from the game?s website1andrelated discussion forum.
ICTCLAS (Zhang etal., 2003) is used for word segmentation and part-of-speech (POS) tagging (Zhang et al, 2003).There are two kinds of game terms: positive andnegative.
Positive terms include ?citizen?, ?goodperson?, ?good person certified by the detectives?,?detective?, etc..
Negative terms include ?killer?,?killer verified by the detectives?, ?a killer whoclaimed himself/herself to be a detective?, etc..We assign the polarity score +1, -1 to positive andnegative terms respectively.2.2 Attitude-Target PairingThen we associate each attitude term with itscorresponding target.
We remove interrogativeand exclamatory sentences and only keep the sen-tences that include at least one attitude term froma player?s state ent during each round.We develop a rule-based approach for attitude-targ t pairing: if there is at least one ID in thesentence, we associate all attitude terms in thatsentence with it.
Otherwise, if ?I?
is the onlysubject or there are no subjects at all, we associateattitude terms with the speaker.
We reverse thepolarity of an attitude word if it appears in anegation context.For each attitude-target pair, we check the POStag sequence between them.
If there exists an at-titude term, a belief-oriented verb such as ?think?,?believe?, ?feel?, or more than two verbs in thesequence, we will discard this pair.
In our task, weassume that a target and an attitude term can forma pair if the target is dominated by the term.
(Yuet al, 2015) showed that we can judge if a wordis dominated by any other word by analyzing thePOS tag sequence between them.For those targets, the speaker didn?t mentionor there is no positive/negative attitude term usedwhen they are mentioned, the attitude polarityscore is set to 0.3 ClusteringIn this section, we introduce a method to constructan attitude profile for each player and a signed net-work based on the attitude tuple list in Section 2,and combine them to handle a dynamic networkwith discussants telling lies and truths.1e.g., http://www.3j3f.com/how/Figure 2: Killer game sample log (1st round).C: citizen; D: detective; K: killerSystem: Fi st Round.System: 15 was killed last night.15(C): I?m a citizen.
Over.16(K): I?m a good person.
11 and 2 are suspicious.1(K): I?m a good person.
It has been a long time sinceI played as a killer.
I?m a citizen.
11 is suspicious andI don?t want to comment on 16?s statement.2(C): I?m a detective.
6 was proved as a killer last night.
Over.3(C): I don?t know 2?s identity.
It?s hard to judge 16?s statement.
1seems to be a good person.
I?m a citizen.4(C): Citizen.
I cannot find a killer.
I trust 2 since 2 sounds a goodperson.
16 is suspicious.
I regard 16 as a killer.
I?m 2?s teammate.5(D): I?m a detective.
I verify 2?s identity and 2 is a killer.
13 is good.6(C): Why do you want to attack 2?
I don?t understand.
14 is suspicious.7(K): It?s hard to define 6?s identity.
4 may be acitizen.
I will vote for 2.
6 sounds very weird andI found 6 very suspicious.
I will follow the detective5 to vote for 2.8(C): We should calm down.
7 seems to be a bad person.9(C): 1 and 7 seem to be killers.
There is no evidence to support 2 as adetective.
3 is a citizen.
4 is possibly a detective.
6 is also good.10(D): I agree with you.
7 must be a killer.
2 and 7 should debate.11(C): I don?t know 2 but I think 2 is good.
3 is good.
There should beone or two killers among 1, 4 and 7.12(K): 11 sounds like a killer.
2 is a killer.
I?m acitizen.
Vote for 2.13(D): 15 is a citizen.
16 is logically good.
I think 1, 8, 9, 10 are OK. Idon?t think 2 is a killer.
I doubt 7?s intention.
Please vote for 7.14(D): 10, 13, 16 are good.
I don?t think 7 must be a killer.
2 isobviously bad.
I?m a citizen.System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 ?
?
?
10, 13, 5, 2 vote for7 ?
?
?
9, 6 vote for 11 ?
?
?
2 is out.2.1 Target and Attitude Word IdentificationWe start by identifying targets and attitude wordsfrom conversations.
In the killer game, a targetis represented by his/her unique ID1and gameterms are regarded as attitude words.
We collected41 terms in total from the game?s website2andrelated discussion forum.
ICTCLAS (Zhang etal., 2003) is used for word segmentation and part-of-speech (POS) tagging.
There are two kindsof game terms: positive and negative.
Positiveter s include ?citizen?, ?good person?, ?goodperson c rtified by the detectives?, ?detective?,etc..
Negative terms include ?killer?, ?killer ver-ified by the detectives?, ?a killer who claimedhimself/herself to be a de ective?, etc.. We assignthe polarity score +1, -1 to positive and negativeterms respectively.2.2 Attitude-Target PairingThen we associate each attitude word with itscorresponding target.
We remove interrogativeand exclamatory sentences and only keep the sen-tences that include at least one attitude word froma player?s statement during each round.We develop a rule-based approach for attitude-target pairing: if there is at least one ID in thesentence, we associate all attitude words in thatsentence with it.
Otherwise, if ?I?
is the onlysubject or there are no subjects at all, we associateattitude words with the speaker.
We reverse thepolarity of an attitude word if it appears in anegation context.Previous methods pair a target and an atti-tude word if they satisfy at least one dependencyrules (e.g.,(Somasundaran and Wiebe, 2009)).
Wecheck the POS tag sequence between them.
Foreach attitude-target pair, if there exists an attitudeword, a belief-oriented verb such as ?think?, ?be-lieve?, ?feel?, or more than two verbs in the se-quence, we will discard this pair.
The assumptionis that POS tag sequences can be used to roughlysummarize dependency rules when statements arerelatively short.For those targets, the speaker didn?t mentionor there is no positive/negative attitude word usedwhen they are mentioned, the attitude polarityscore is set to 0.
For instance, given Player 16?sstatement in Figure 2, its attitude tuple list is: [(16,16, +1), (16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3,0), .
.
.
, (16, 15, 0)].1Each player has a game ID, assigned by the online gamesystem based on when s/he entered the game room.2e.g., http://www.3j3f.com/how/Figure 2: Killer game sample log (1st round).C: citizen; D: detective; K: killerSystem: First Round.System: 15 was killed last night.
15, please leave your last words.15(C): I?m a citizen.
Over.16(K): I?m a good person.
11 and 2 are suspicious.1(K): I?m a good person.
It has been a long time sinceI played as a killer.
I?m a citizen.
11 is suspicious andI don?t want to comment on 16?s statement.2(C): I?m a detective.
6 was proved as a killer last night.
Over.3(C): I don?t know 2?s identity.
It?s hard to judge 16?s statement.
1seems to be a good person.
I?m a citizen.4(C): Citizen.
I cannot find a killer.
I trust 2 since 2 sounds a goodperson.
16 is suspicious.
I regard 16 as a killer.
I?m 2?s teammate.5(D): I?m a detective.
I verify 2?s identity and 2 is a killer.
13 is good.6(C): Why do you want to attack 2?
I don?t understand.
14 is suspicious.7(K): It?s hard to define 6?s identity.
4 may be acitizen.
I will vote for 2.
6 sounds very weird andI found 6 very suspicious.
I will follow the detective5 to vote for 2.8(C): We should calm down.
7 seems to be a bad person.9(C): 1 and 7 seem to be killers.
There is no evidence to support 2 as adetective.
3 is a citizen.
4 is possibly a detective.
6 is also good.10(D): I agree with you.
7 must be a killer.
2 and 7 should debate.11(C): I don?t know 2 but I think 2 is good.
3 is good.
There should beone or two killers among 1, 4 and 7.12(K): 11 sounds like a killer.
2 is a killer.
I?m acitizen.
Vote for 2.13(D): 15 is a citizen.
16 is logically good.
I think 1, 8, 9, 10 are OK. Idon?t think 2 is a killer.
I doubt 7?s intention.
Please vote for 7.14(D): 10, 13, 16 are good.
I don?t think 7 must be a killer.
2 isobviously bad.
I?m a citizen.System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 ?
?
?
10, 13, 5, 2 vote for7 ?
?
?
9, 6 vote for 11 ?
?
?
2 is out.2.1 T rget and Attitude W rd IdentificationWe start by identifying targets and attitude wordsfrom conversations.
In the killer game, a targetis represented by his/her unique ID1and gameterms are regarded as attitude words.
We collected41 terms in total from the game?s website2andrelated discussion forum.
ICTCLAS (Zhang etal., 2003) is used for word segmentation and part-of-speech (POS) tagging.
There are two kindsof game terms: positive and negative.
Positiveterms include ?citizen?, ?good person?, ?goodperson certified by the detectives?, ?detective?,etc..
Negative terms include ?killer?, ?killerverified by the detectives?, ?a killer who claimedhimself/herself to be a detective?, etc.. We assignthe polarity score +1, -1 to positive and negativet rms respectively.2.2 Attitude-Target PairingThen we associate each attitude word with itscorresponding target.
We remove interrogativeand exclamatory sentences and only keep thesentences that include at least one attitude wordfrom a player?s statement during each round.We develop a rule-based approach for attitude-target pairing: if there is at least one ID in thesentence, we associate all attitude words in thatsentence with it.
Otherwise, if ?I?
is the onlysubject or there are no subjects at all, we associateattitude w rds wit th ID of the speaker.
Wereverse the polarity of an attitude word if it appearsin a negation context.Previous methods pair a target and an attitudeword if they satisfy at least one dependency rules(e.g.,(Somasundaran and Wiebe, 2009)).
Wecheck the POS tag sequence between them.
Foreach attitude-target pair, if there exists an attitudeword, a belief-oriented verb such as ?think?,?believe?, ?feel?, or more than two verbs inthe sequence, we will discard this pair.
Theassumption is that POS tag sequences can be usedto roughly summarize dependency rules whenstatements are relatively short.For those targets, the speaker didn?t mentionor there is no positive/negative attitude word usedwhen they are mentioned, the attitude polarityscore is set to 0.
For instance, given Player 16?sstatement in Figure 2, its attitude tuple list is: [(16,1Each player has a game ID, assigned by the online gamesystem based on when s/he entered the game room.2e.g., http://www.3j3f.com/how/Figure 2: Killer game sample log (1st round).C: CITIZEN; D: DETECTIVE; K: KILLERSystem: First Round.System: 15 was killed last night.
15, please leave your last words.15(C): I?m a citizen.
Over.16(K): I?m a good person.
11 and 2 are suspicious.1(K): I?m a good person.
It has been a long time since I played as akiller.
I?m a citizen.
11 is suspicious and I don?t want to commenton 16?s statement.2(C): I?m a detective.
6 was proved as a killer last night.
Over.3(C): I don?t know 2?s identity.
It?s hard to judge 16?s statement.
1seems to be a good person.
I?m a citizen.4(C): Citizen.
I cannot find a killer.
I trust 2 since 2 sounds a goodperson.
16 is suspicious.
I regard 16 as a killer.
I?m 2?s teammate.5(D): I?m a detective.
I verify 2?s identity and 2 is a killer.
13 is good.6(C): Why do you want to attack 2?
I don?t understand.
14 issuspicious.7(K): It?s hard to define 6?s identity.
4 may be a citizen.
I will votefor 2.
6 sounds very weird and I found 6 very suspicious.
I willfollow the detective 5 to vote for 2.8(C): We should calm down.
7 seems to be a bad person.9(C): 1 and 7 seem to be killers.
There is no evidence to support 2 as adetective.
3 is a citizen.
4 is possibly a detective.
6 is also good.10(D): I agree with you.
7 must be a killer.
2 and 7 should debate.11(C): I don?t know 2 but I think 2 is good.
3 is good.
There should beone or two killers among 1, 4 and 7.12(K): 11 sounds like a killer.
2 is a killer.
I?m a citizen.
Vote for 2.13(D): 15 is a citizen.
16 is logically good.
I think 1, 8, 9, 10 are OK. Idon?t think 2 is a killer.
I doubt 7?s intention.
Please vote for 7.14(D): 10, 13, 16 are good.
I don?t think 7 must be a killer.
2 isobviously bad.
I?m a citizen.System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 ?
?
?
10, 13, 5, 2 vote for7 ?
?
?
9, 6 vote for 11 ?
?
?
2 is out.2.1 Target and Attitude Word IdentificationWe start by identifying targets and attitude wordsfrom conversations.
In the killer game, a target isrepresented by his/her unique ID1and game termsare r garde as attitud w rds.
We collected 41terms in total from the game?s website2and re-lated discussion forum.
ICTCLAS (Zhang et al,2003) is used for word segmentation and part-of-speech (POS) tagging.
There are two kinds ofgame terms: positive and negative.
Positive termsinclude ?citizen?, ?good person?, ?good personcertified by the detectives?, ?detective?, etc.. Neg-ative terms include ?killer?, ?killer verified by thedetectives?, ?a killer who claimed himself/herselfto be a detective?, etc.. We assign the polarity s-core +1, -1 to positive and negative terms respec-tively.2.2 Attitude-Target PairingThen we associate each attitude word with it-s corresponding target.
We remove interrogativeand exclamatory sentences and only keep the sen-tences that include at least one attitude word froma player?s statement during each round.We develop a rule-based approach for attitude-target pairing: if there is at least one ID in the sen-tence, we associate all attitude words in that sen-tence with it.
Otherwise, if ?I?
is the only subjector there are no subjects at all, we associate atti-tude words with the ID of the speaker.
We reversethe polarity of an attitude word if it appears in anegation context.Previous methods pair a target and an atti-tude word if they satisfy at least one dependencyrules (e.g.,(Somasundaran and Wiebe, 2009)).
Wecheck the POS tag sequence between them.
Foreach attitude-target pair, if there exists an attitudeword, a belief-oriented verb such as ?think?, ?be-lieve?, ?feel?, or more than two verbs in the se-quence, we will discard this pair.
The assumptionis that POS tag sequences can be used to summa-rize dependency rules when statements are rela-tively short.For those targets, the speaker didn?t mentionor there is no positive/negative attitude word usedwhen they are mentioned, the attitude polarity s-core is set to 0.
For instance, given Player 16?s s-tatement in Figure 2, its attitude tuple list is: [(16,16, +1), (16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3,0), .
.
.
, (16, 15, 0)].1Each player has a game ID, assigned by the online gamesystem based on when s/he entered the game room.2e.g., http://www.3j3f.com/how/Figure 2: Killer game sample log (the 1st round).2.1 Target and Attitude Word IdentificationWe start by identifying targets and attitude word-s from conversations.
In the killer game, a targetis represented by his unique ID2and game termsare regarded as attitude words.
We collected 41terms in total from the game?s website3and re-lated discussion forum posts.
ICTCLAS (Zhanget al, 2003) is used for word segmentation andpart-of-speech (POS) tagging.
There are two kind-s of game terms: positive and negative.
Posi-tive terms include ?citizen?, ?good person?, ?goodperson certified by the detectives?
and ?detective?.Negative terms include ?killer?, ?killer verified bythe detectives?
and ?a killer who claimed him-self/herself to be a detective?.
We assign the po-larity score +1, -1 to positive and negative termsrespectively.2Each player has a game ID, assigned by the online gamesystem based on when he entered the game room.3e.g., http://www.3j3f.com/how/2.2 Attitude-Target PairingThen we associate each attitude word with it-s corresponding target.
We remove interrogativean exclamatory sentences and only keep th s n-tences that include t least one attitude word froma player?s statement during each round.We develop a rule-based approach for attitude-target pairing: if there is at least one ID in the sen-tence, we associate all attitude words in that sen-tence with it.
Otherwise, if ?I?
is the only subjector there are no subjects at all, we associate atti-tude words with the ID of the speaker.
We reversethe polarity of an attitude word if it appears in anegation context.Previous methods pair a target and an attitudeword if they satisfy at least one dependency rule(e.g., (Somasundaran and Wiebe, 2009)).
Wecheck the POS tag sequence between them.
Foreach attitude-target pair, if there exists an attitudeword, a belief-oriented verb such as ?think?, ?be-lieve?, ?feel?, or more than two verbs in the se-quence, we will discard this pair.
The assumptionis that POS tag sequences can be used to summa-rize dependency rules when statements are rela-tively short.For those targets, the speaker didn?t mentionor there is no positive/negative attitude word usedwhen they are mentioned, the attitude polarity s-core is set to 0.
For instance, given Player 16?s s-tatement in Figure 2, its attitude tuple list is: [(16,16, +1), (16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3,0), .
.
.
, (16, 15, 0)].3 ClusteringSince the statements in conversations are relativelyshort and concise, it is difficult to identify whichone is deceptive, even using deep linguistic fea-tures such as the language style.In this section, we introduce a method to con-struct an attitude profile for each player and asigned network based on the attitude tuple list inSection 2, and combine them to analyze a dynam-ic network with discussants telling lies and truths.3.1 Clustering based on Attitude ProfileWe use a vector containing numerical values torepresent each player?s attitude toward identifiedtargets in each round.
The values correspond tothe polarity scores in a player?s attitude tuple list.For example, the polarity score of player 16?s atti-tude toward target 11 is ?1 as shown in Figure 2.859We call this vector as the discussant attitude pro-file (DAP) following (Abu-Jbara et al, 2012a).Suppose there are n players who participate ina single game.
Since a player?s identity is not ex-posed to the public after his death4, people can stillanalyze the identity of a ?dead?
player.
Therefore,the number of possibly mentioned targets in eachround equals to n. Given all the statements fromm surviving players in a single round, each play-er?s DAP has n+ 1 dimensions including his voteand thus we can have a m ?
(n + 1) attitude ma-trixA whereAijrepresents the attitude polarity ofi toward j we got from Section 2.
Ai(n+1)repre-sents i?s vote.In a certain round, given a set of m survivingplayers X = {x1, x2, ?
?
?
, xm} to be clusteredand their respective DAPs, we can modify the Eu-clidean metric to compute the differences in atti-tudes and get an m?m distance matrix M :Mij=???
?n?k=1(Aik?Ajk)2+ (2?
2?Ai(n+1),Aj(n+1))2(1)The Kronecker delta function ?
is:?ij={1 i = j0 i 6= j(2)We use this function to compare the votes of t-wo players separately because a player?s vote canbe inconsistent with his previous statements.
Weassume that there is a larger distance between twoplayers when they vote for different suspects.A common assumption in previous research wasthat a member is more likely to show a positiveattitude toward other members in the same group,and a negative attitude toward the opposing group-s (Abu-Jbara et al, 2012a).
However, a deceivermay pretend to be innocent by supporting thosetruth-tellers and attacking his teammates, whose i-dentities have already been exposed.
Therefore,it is not enough to judge the relationship betweentwo players by simply measuring the distance be-tween their DAPs.In addition to comparing DAPs between player-s i and j, we also consider the attitudes of otherplayers toward i and j, as well as their attitudes4Each round, the player killed by killers and the playerwith the most votes are out.toward each other.
We modify Mijas follows andshow it in Figure 3:M?ij=Mij+???
?m?k=1(Aki?Akj)2+ (h(Aij) + h(Aji))2(3)where the function h detects the negative atti-tudes.
h(x) = 0 if x ?
0 and h(x) = ?1 other-wise.We perform hierarchical clustering on the con-densed distance matrix ofM and use the completelinkage method to compute the distance betweentwo clusters (Voorhees, 1986).
We set the num-ber of clusters as 3 since there are three naturalgroups in the game.
We focus on separating de-ceivers (killers) from truth-tellers (citizens and de-tectives).?
??
?compare  ?
and ?
?s DAPsFigure 3: Computation of the distance betweenplayer i and j based on the attitude matrix.3.2 Signed Network PartitionWhen we computed the distance between twoplayers in Section 3.1, we did not consider the net-work structure among all the players.
For exam-ple, if A supports C, B supports D and C and Ddislike each other, A and B may belong to differ-ent groups.
Thus, we propose to capture the in-teractions in the social network to further improvethe attitude-profile-based clustering result.We can easily convert the attitude matrix A intoa signed network by adding a directed edge i?
jbetween i and j if Aij6= 0.
We denote a directedgraph corresponding to a signed network as G =(V, S,N,W ), where V is the set of nodes, S is theset of positive edges,N is the set of negative edgesand W : (V ?
V ) ?
{?1, 1} is a function thatmaps every directed edge to a value, W (i, j) =Aij.We use a greedy optimization algorithm (Dor-eian and Mrvar, 1996) to find partitions.
A criteri-on function for an optimal partitioning procedure860is constructed such that positive links are densewithin groups and negative links are dense be-tween groups.
For any potential partition C, weseek to minimize the following error function:E(C) =?C?C[(1?
?
)?i?Cj/?CW (i, j)Si,j?
?
?i,j?CW (i, j)Ni,j](4)where ?
?
[0, 1] controls the balance of thepenalty difference between putting a positive edgeacross and a negative edge within a group.
We re-gard these two types of errors as equally importantand set ?
= 0.5 for our experiments.Initially, we use the clustering result in Sec-tion 3.1 to partition nodes into three differen-t groups and an error function, E, is evaluated forthat cluster.
Every cluster has a set of neighborclusters in the cluster space.
A neighbor clusteris obtained by moving a node from one group toanother, or exchanging two nodes in two differentgroups.
E is evaluated for all the neighbor clustersof the current cluster and the one with the lowestvalue is set as the new cluster.
The algorithm isrepeated until it finds a minimal solution5.
We setthe upper limit for the number of subgroups to 3.3.3 Cluster EnsemblesThe relationships between players are dynamicthroughout the game.
For example, a killer tendsto hide his identity and pretends to be friendly toothers at later stages in order to survive.
Thus, itis insufficient to rely on a single round?s discus-sion to cluster players.
In addition, for each singleround, we also need to combine the clustering re-sults from the attitude profiles of the players andthe signed network.In a game with information gathered from upto r rounds, let P = {P1, P2, ?
?
?
, Pr} be the setof r clusterings (partitionings) based on attitudeprofiles and P?= {P?1, P?2, ?
?
?
, P?r} be the set ofr clusterings based on the signed network.Using the co-occurrence relations betweenplayers, we can generate a n ?
n pairwise simi-larity matrix T based on the information of all rrounds:Trij=?
?
voteij+ (1?
?)
?
vote?ijrij(5)5Since our graphs are small, we search through all parti-tions.
We repeated 1000 times in our experiment.where voteij, vote?ijare the number of timesthat player i and j are assigned to the same clusterin P and P?respectively.
rijdenotes the numberof rounds when both of them survived (rij?
r).Trij?
[0, 1].
We assign a higher weight to the re-sult of P1and set ?
= 2/3 in our experiments.Given the input in Figure 2, x3and x4are as-signed to the same cluster in P1(vote34= 1) andin P?1(vote?34= 1) respectively as shown in Fig-ure 4. x3and x4co-occurred in the first round(r34= 1).
T134= (2/3?
1 + 1/3?
1)/1 = 1.?16????
?Round 1?2   ?11?1   ?7   ?12    ?14?3?15   ?4   ?5?13   ?10?8   ?9?6?2   ?11?14?3?15   ?4   ?5?13?10   ?8   ?9?6?16?1?7   ?12KILLER CITIZEN OR DETECTIVEFigure 4: Example of cluster ensemble for a singleround.We apply hierarchical clustering (Voorhees,1986) to the similarity matrix above to obtain thefinal global clustering results.4 Experiments4.1 Dataset ConstructionWe recorded 10 games from 3J3F6, one of themost popular Chinese online killer game web-sites7.
A screenshot of the game system inter-face is shown in Figure 5.
There are 16 partic-ipating players per game: 4 detectives, 4 killer-s and 8 citizens.
Each player occupies a posi-tion in 1 .
All the surviving players can expresstheir attitudes via a voice channel using 2 , whiledetectives and killers can also communicate withteammates in their respective private team chan-nels 3 via texts.
The system provides real-timeupdates on the game progress, voting results, andso on using the public channel 4 .
We manuallytranscribed speech and stored the text informationin the public channel, which contains the votingand death information.
The average game length6http://www.3j3f.com7All data sets and resources will be made available forresearch purposes upon the acceptance of the paper.861Game#Purity (%) EntropyD N H eD eD +N D N H eD eD +N1 68.8 75.0 75.0 68.8 75.0 0.48 0.50 0.78 0.63 0.502 75.0 68.8 68.8 43.8 81.3 0.71 0.69 0.81 0.73 0.433 43.8 81.3 56.3 75.0 75.0 0.77 0.67 0.81 0.72 0.724 75.0 62.5 75.0 93.8 93.8 0.78 0.68 0.74 0.28 0.285 62.5 75.0 81.3 75.0 75.0 0.61 0.50 0.61 0.72 0.726 81.3 81.3 75.0 81.3 81.3 0.64 0.38 0.74 0.60 0.607 81.3 75.0 81.3 81.3 87.5 0.65 0.70 0.68 0.51 0.518 87.5 75.0 75.0 93.8 93.8 0.41 0.73 0.78 0.23 0.239 75.0 43.8 75.0 81.3 87.5 0.76 0.80 0.78 0.67 0.4910 62.5 75.0 87.5 81.3 81.3 0.78 0.60 0.51 0.61 0.67Average 71.3 71.3 75.0 77.5 83.2 0.66 0.62 0.72 0.57 0.51Table 1: Results on subgroup detection.
D refers to DAPC, N refers to Network, H refers to Human Voting, and eDrefers to extended DAPC.is about 76.3 minutes and there are on average 5rounds and 411 sentences per game.
Note that ourmethod is language-independent and could easilybe adapted to other languages.Current Speaker: 14TEAM CHANNELPUBLIC CHANNELSTART ENDOUT123 4Figure 5: Screenshot of the online killer game in-terface.4.2 Evaluation MetricsWe use two metrics to evaluate the clustering ac-curacy: Purity and Entropy.
Purity (Manning etal., 2008) is a metric in which each cluster is as-signed to the class with the majority vote in thecluster, and then the accuracy of this assignmen-t is measured by dividing the number of correctlyassigned instances by the total number of instancesN .
More formally:purity(?, C) =1N?kmaxj|wk?
cj| (6)where ?
= {w1, w2, ?
?
?
, wk} is the set of clustersand C = {c1, c2, ?
?
?
, cj} is the set of classes.
wkis interpreted as the set of instances in wkand cjis the set of instances in cj.
The purity increasesas the quality of clustering improves.Entropy (Steinbach et al, 2000) measures theuniformity of a cluster.
The entropy for all clustersis defined by the weighted sum of the entropy ofeach cluster:Entropy = ?j?njni?P (i, j)?
log2P (i, j)(7)where P (i, j) is the probability of finding an el-ement from the category i in the cluster j, njisthe number of items in cluster j and n is the totalnumber of items in the distribution.
The entropydecreases as the quality of clustering improves.4.3 Overall PerformanceWe compare our approach with two state-of-the-art subgroup detection methods and human perfor-mance as follows:1.
DAPC: In Section 3.1, we introduced our im-plementation of the discussant attitude profileclustering (DAPC) method proposed in (Abu-Jbara et al, 2012a).
In the original DAPCmethod, for each opinion target, there are 3dimensions in the feature vector, correspond-ing to (1) the number of positive expression-s, (2) negative expressions toward the tar-get from the online posts and (3) the num-ber of times the discussant mentioned the tar-get.
For our experiment, we only keep onedimension representing the discussant?s atti-tude (positive, negative, neutral) toward thetarget since a discussant attitude remains thesame in his statement within a single round.2.
Network: We also implemented the signednetwork partition method for subgroup detec-tion proposed by (Hassan et al, 2012).
Todetermine the number of subgroups t, we setan upper limit of t = 3 in order to minimizethe optimization function.8623.
Human Voting: We also compare our meth-ods with human voting results.
There are twosubgroups based on the voting results.
Theplayers with the highest votes each round be-long to one subgroup and the rest of the play-ers are in the other subgroup.Table 1 shows the overall performance of vari-ous methods on subgroup detection and Figure 6depicts the average performance.
We can see thatour method significantly outperforms two baselinemethods and human voting.
The human perfor-mance is not satisfying, which indicates it?s verychallenging even for a human to identify a deceiv-er whose deceptive statement is mixed with plentyof truthful opinions (Xu and Zhao, 2012).1 Human_Voting BL_DAPC BL_Network EDAPCEDAPC+Network5055606570758085%MethodPurity EntropyFigure 6: An overview of the average performanceof all the methods.By extending the DAPC method (EDPAC), wecan estimate the distance between two playersmore accurately by considering the attitudes ofother players toward them and their attitudes to-ward each other.
Given the log in Figure 2 as in-put, players 5 (detective) and 7 (killer) are clus-tered into one group when DAPC is applied s-ince they don?t have conflicting views on the i-dentities of other players.
However, 5 voted for7 and is supported by more players compared with7, which indicates that they are less likely to beteammates.
We can successfully separate them af-ter re-computing the distance between them.Adding network information provided 5.7%further gain in Purity.
In some cases, the perfor-mance remains the same when EDAPC clusteringresult is already optimal with the minimum valueof the criterion function.4.4 Dynamic Subgroup DetectionAs shown in Figure 7, the performance of ourapproach improves as the game proceeds.
Play-ers seldom maintain their opinions throughout agame.
Figure 2 shows that most killers (16,1,12)insisted that citizen 11 should be a killer except 7.As a response to the group pressure (Asch, 1951),7 changed his opinion and stated that 11 could bea killer in the following round.In reality, a discussant who participates in anonline discussion tends to change his opinion-s about a target as he learns more information,which shows both the necessity and importance ofthe dynamic detection of subgroups.
Our methodcan be applied to detect subgroups dynamically bygrouping posts into multiple discussion ?rounds?based on their timestamps.1Purity Entropy50607080%1st round 1st + 2nd rounds all roundsFigure 7: Average performance based on differentrounds.5 Related Work5.1 Opinion AnalysisOur work on mining a player?s attitude toward oth-er players is related to opinion mining.
Attitudesand opinions are related and can be regarded asthe same in our task.
Compared with the previ-ous work (e.g.,(Qiu et al, 2011; Kim and Hovy,2006)), the opinion words and targets in our taskare relatively easier to recognize due to the sim-plicity of statements.
Some recent work (e.g., (So-masundaran and Wiebe, 2009; Abu-Jbara et al,2012a)) developed syntactic rules to pair an opin-ion word and a target if they satisfy at least onespecific dependency rule.
We use POS tag se-quences to efficiently help us filter out irrelevantpairs.8635.2 Deception DetectionMost of the previous computational work fordeception detection used supervised/semi-supervised classification methods (Li et al,2013b).
Besides lexical and syntactical fea-tures (Ott et al, 2011; Feng et al, 2012; Yanchevaand Rudzicz, 2013), Feng and Hirst (2013) pro-posed using profile compatibility to distinguishfake and genuine reviews.
Xu and Zhao (2012)used deep linguistic features such as text genreto detect deceptive opinion spams.
Banerjee etal.
(2014) used extended linguistic signals suchas keystroke patterns.
Li et al (2013a) used topicmodels to detect the difference between deceptiveand truthful topic-word distribution.
Researchershave began to realize the importance of analyzingcomputer-mediated communication in deceptiondetection.
Zhou and Sung (2008) conductedan empirical study on deception cues using thekiller game as a task scenario and obtained manyinteresting findings (e.g., deceivers send fewermessages than truth-tellers).Our work is most related to the work of Chit-taranjan and Hung (2010) on detecting deceptiveroles in the Werewolf Game which is another vari-ant of the killer game.
They created a Werewolfdata set by audio-visual recording 8 games playedby 2 groups of people face-to-face and extract-ed audio features and interaction features for theirexperiments.
However, we should note that nonface-to-face deception detection emphasizes ver-bal and linguistic cues over less controllable non-verbal communication cues (Walther, 1996).5.3 Subgroup DetectionIn online discussions, people usually split intosubgroups based on various topics.
The memberof a subgroup is more likely to show positive at-titude to the members of the same subgroup, andnegative attitude to the members of opposing sub-groups (Abu-Jbara et al, 2012a).
Previous workalso studied subgroup detection in social mediasites.
Abu-Jbara et al (2012a) constructed a dis-cussant attitude profile (DAP) for each discussantand then used clustering techniques to cluster theirattitudes.
Hassan et al (2012; 2012b; 2013) pro-posed various methods to automatically constructa signed social network representation of discus-sions and then identify subgroups by partitioningtheir signed networks.
Qiu et al (2013) appliedcollaborative filtering through Probabilistic MatrixFactorization (PMF) to generalize and improve ex-tracted opinion matrices.An underlying assumption of the previous workwas that a participant will not tell lies nor hide hisown stance.
Moreover, their work did not take in-to account that a person?s attitude or stance willchange as he learns more by reading the com-ments from others and acquiring more backgroundknowledge (Bandura, 1971).
Our contribution isthat we extend the DAP method and combine itwith the signed network partition in order to clus-ter the hidden group members.
We also develop anovel cluster ensemble approach in order to ana-lyze the dynamic network.6 Conclusions and Future WorkUsing the killer game as a case study, we presentan effective clustering method to detect subgroupsfrom dynamic conversations with lies and truth-s.
This is the first work to utilize the dynam-ics of group conversations for deception detec-tion.
Experiments demonstrated that truth-tellersand deceptive groups are separable and the pro-posed method significantly outperforms baselineapproaches and human voting.Our work builds a pathway to future work indeception detection in content-rich dynamic envi-ronments such as electronic commerce and repeat-ed interrogation which will require sophisticatedcontent and network analysis.
In real-life suspectsmay be interrogated about particular events on nu-merous occasions.
Our method can potentially bemodified to find criminals who act in groups basedon their statements.
Other applications of this re-search include law enforcement, financial fraud,fraudulent ad campaigns and social engineering.This study focuses on analyzing the verbal con-tent in conversations.
It will be interesting to studynon-verbal features such as blink rate, gaze aver-sion and pauses (Granhag and Str?omwall, 2002)when people play this game face-to-face and com-bine the non-verbal and verbal features for decep-tion detection.
In addition, it is worth exploringthe impact of cross-cultural analysis in detectingdeception.
When attempting to detect deceit inpeople of other ethnic origin than themselves, peo-ple perform even worse in terms of lie detectionaccuracy than when judging people of their ownethnic origin (Vrij, 2000).
For the future work,we aim to use automatic prediction of deceivers tohelp truth-tellers win games more easily.864AcknowledgementThis work was supported by the U.S. DARPADEFT Program No.
FA8750-13-2-0041, ARLNS-CTA No.
W911NF-09-2-0053, NSF Award-s IIS-0953149 and IIS-1523198, AFRL DREAMproject, gift awards from IBM, Google, Disneyand Bosch.
The views and conclusions containedin this document are those of the authors andshould not be interpreted as representing the of-ficial policies, either expressed or implied, of theU.S.
Government.
The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernment purposes notwithstanding any copyrightnotation here on.ReferencesA.
Abu-Jbara, M. Diab, P. Dasigi, and D. Radev.2012a.
Subgroup detection in ideological discus-sions.
In Proc.
Annual Meeting of the Associationfor Computational Linguistics (ACL 2012).A.
Abu-Jbara, A. Hassan, and D. Radev.
2012b.
Atti-tudeminer: mining attitude from online discussions.In Proc.
North American Chapter of the Associationfor Computational Linguistics - Human LanguageTechnologies (NAACL HLT 2012).A.
Abu-Jbara, B.
King, M. Diab, and D. Radev.
2013.Identifying opinion subgroups in arabic online dis-cussions.
In Proc.
Association for ComputationalLinguistics (ACL 2013).S.
Asch.
1951.
Effects of group pressure upon themodification and distortion of judgments.
Groups,leadership, and men.
S.A. Balahur, R. Steinberger, E. Goot, B. Pouliquen, andM.
Kabadjov.
2009.
Opinion mining on newspaperquotations.
In IEEE/WIC/ACM International JointConferences on Web Intelligence and Intelligent A-gent Technologies (WI-IAT 2009).A.
Bandura.
1971.
Social Learning Theory.
GeneralLearning Corporation.R.
Banerjee, S. Feng, J. Kang, and Y. Choi.
2014.Keystroke patterns as prosody in digital writings: Acase study with deceptive reviews and essays.
InProc.
Empirical Methods on Natural Language Pro-cessing (EMNLP 2014).D.
Buller and J. Burgoon.
1996.
Interpersonal decep-tion theory.
Communication theory.David B Buller, Judee K Burgoon, JA Daly, andJM Wiemann.
1994.
Deception: Strategic andnonstrategic communication.
Strategic interperson-al communication.G.
Chittaranjan and H. Hung.
2010.
Are you awere-wolf?
detecting deceptive roles and outcomes in aconversational role-playing game.
In IEEE Interna-tional Conference on Acoustics Speech and SignalProcessing (ICASSP 2010).P.
Doreian and A. Mrvar.
1996.
A partitioning ap-proach to structural balance.
Social networks.V.
Feng and G. Hirst.
2013.
Detecting deceptive opin-ions with profile compatibility.
In Proc.
Internation-al Joint Conference on Natural Language Process-ing (IJCNLP 2013).S.
Feng, R. Banerjee, and Y. Choi.
2012.
Syntacticstylometry for deception detection.
In Proc.
Associ-ation for Computational Linguistics (ACL 2012).N.
E. Friedkin.
2010.
The attitude-behavior linkage inbehavioral cascades.
Social Psychology Quarterly.P.
Granhag and L. Str?omwall.
2002.
Repeated inter-rogations: verbal and non-verbal cues to deception.Applied Cognitive Psychology.A.
Hassan, A. Abu-Jbara, and D. Radev.
2012.
De-tecting subgroups in online discussions by model-ing positive and negative relations among partici-pants.
In Proc.
Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2012).S.
Kim and E. Hovy.
2006.
Extracting opinions, opin-ion holders, and topics expressed in online news me-dia text.
In Proc.
ACL-COLING 2006 Workshop onSentiment and Subjectivity in Text.J.
Li, C. Cardie, and S. Li.
2013a.
Topicspam: atopic-model based approach for spam detection.
InProc.
Association for Computational Linguistics (A-CL 2013).J.
Li, M. Ott, and C. Cardie.
2013b.
Identifying ma-nipulated offerings on review portals.
In Proc.
Em-pirical Methods on Natural Language Processing(EMNLP 2013).C.
Manning, P. Raghavan, and H. Sch?utze.
2008.
In-troduction to information retrieval.
Cambridge uni-versity press Cambridge.M.
Ott, Y. Choi, C. Cardie, and J. Hancock.
2011.Finding deceptive opinion spam by any stretch of theimagination.
In Proc.
Association for Computation-al Linguistics (ACL 2011).G.
Qiu, B. Liu, J. Bu, and C. Chen.
2011.
Opinionword expansion and target extraction through doublepropagation.
Computational linguistics.M.
Qiu, L. Yang, and J. Jiang.
2013.
Mining us-er relations from online discussions using sentimen-t analysis and probabilistic matrix factorization.
InProc.
North American Chapter of the Association forComputational Linguistics - Human Language Tech-nologies (NAACL HLT 2013).865S.
Somasundaran and J. Wiebe.
2009.
Recognizing s-tances in online debates.
In Proc.
Joint Conferenceof the Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP.M.
Steinbach, G. Karypis, V. Kumar, et al 2000.
Acomparison of document clustering techniques.
InProc.
KDD 2000 workshop on text mining.E.
Voorhees.
1986.
Implementing agglomerative hi-erarchic clustering algorithms for use in documentretrieval.
Information Processing & Management.A.
Vrij, P. Granhag, and S. Porter.
2010.
Pitfalls andopportunities in nonverbal and verbal lie detection.Psychological Science in the Public Interest.A.
Vrij.
2000.
Detecting lies and deceit: The psychol-ogy of lying and implications for professional prac-tice.
Wiley.J.
Walther.
1996.
Computer-mediated communicationimpersonal, interpersonal, and hyperpersonal inter-action.
Communication research.Q.
Xu and H. Zhao.
2012.
Using deep linguisticfeatures for finding deceptive opinion spam.
InProc.
International Conference on ComputationalLinguistics (COLING 2012).M.
Yancheva and F. Rudzicz.
2013.
Automatic de-tection of deception in child-produced speech usingsyntactic complexity features.
In Proc.
Associationfor Computational Linguistics (ACL 2013).H.
Zhang, H. Yu, D. Xiong, and Q. Liu.
2003.
Hhmm-based chinese lexical analyzer ictclas.
In Proc.SIGHAN 2003 workshop on Chinese language pro-cessing.L.
Zhou and Y.
Sung.
2008.
Cues to deception in on-line chinese groups.
In Proc.
Hawaii InternationalConference on System Sciences (HICSS 2008).L.
Zhou, J Burgoon, J. Nunamaker, and D. Twitchell.2004.
Automating linguistics-based cues for detect-ing deception in text-based asynchronous computer-mediated communications.
Group decision and ne-gotiation.866
