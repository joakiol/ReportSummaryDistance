Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 57?62,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 13: TempEval-2Marc Verhagen?, Roser Saur??
?, Tommaso Caselli?and James Pustejovsky??
Computer Science Department, Brandeis University, Massachusetts, USA?Barcelona Media, Barcelona, Spain ?
ILC-CNR, Pisa, Italymarc@cs.brandeis.edu roser.sauri@barcelonamedia.orgtommaso.caselli@ilc.cnr.it jamesp@cs.brandeis.eduAbstractTempeval-2 comprises evaluation tasks fortime expressions, events and temporal re-lations, the latter of which was split up infour sub tasks, motivated by the notion thatsmaller subtasks would make both datapreparation and temporal relation extrac-tion easier.
Manually annotated data wereprovided for six languages: Chinese, En-glish, French, Italian, Korean and Spanish.1 IntroductionThe ultimate aim of temporal processing is the au-tomatic identification of all temporal referring ex-pressions, events and temporal relations within atext.
However, addressing this aim is beyond thescope of an evaluation challenge and a more mod-est approach is appropriate.The 2007 SemEval task, TempEval-1 (Verhagenet al, 2007; Verhagen et al, 2009), was an initialevaluation exercise based on three limited tempo-ral ordering and anchoring tasks that were consid-ered realistic both from the perspective of assem-bling resources for development and testing andfrom the perspective of developing systems capa-ble of addressing the tasks.1TempEval-2 is based on TempEval-1, but ismore elaborate in two respects: (i) it is a multilin-gual task, and (ii) it consists of six subtasks ratherthan three.In the rest of this paper, we first introduce thedata that we are dealing with.
Which gets us ina position to present the list of task introduced byTempEval-2, including some motivation as to whywe feel that it is a good idea to split up temporalrelation classification into sub tasks.
We proceedby shortly describing the data resources and theircreation, followed by the performance of the sys-tems that participated in the tasks.1The Semeval-2007 task was actually known simply asTempEval, but here we use Tempeval-1 to avoid confusion.2 TempEval AnnotationThe TempEval annotation language is a simplifiedversion of TimeML.2using three TimeML tags:TIMEX3, EVENT and TLINK.TIMEX3 tags the time expressions in the text andis identical to the TIMEX3 tag in TimeML.
Timescan be expressed syntactically by adverbial orprepositional phrases, as shown in the followingexample.
(1) a. on Thursdayb.
November 15, 2004c.
Thursday eveningd.
in the late 80?se.
later this afternoonThe two main attributes of the TIMEX3 tag areTYPE and VAL, both shown in the example (2).
(2) November 22, 2004type="DATE" val="2004-11-22"For TempEval-2, we distinguish four temporaltypes: TIME (at 2:45 p.m.), DATE (January 27,1920, yesterday), DURATION (two weeks) and SET(every Monday morning).
The VAL attribute as-sumes values according to an extension of the ISO8601 standard, as enhanced by TIMEX2.Each document has one special TIMEX3 tag,the Document Creation Time (DCT), which is in-terpreted as an interval that spans a whole day.The EVENT tag is used to annotate those ele-ments in a text that describe what is conventionallyreferred to as an eventuality.
Syntactically, eventsare typically expressed as inflected verbs, althoughevent nominals, such as ?crash?
in killed by thecrash, should also be annotated as EVENTS.
Themost salient event attributes encode tense, aspect,modality and polarity information.
Examples ofsome of these features are shown below:2See http://www.timeml.org for language speci-fications and annotation guidelines57(3) should have boughttense="PAST" aspect="PERFECTIVE"modality="SHOULD" polarity="POS"(4) did not teachtense="PAST" aspect="NONE"modality="NONE" polarity="NEG"The relation types for the TimeML TLINK tagform a fine-grained set based on James Allen?sinterval logic (Allen, 1983).
For TempEval, theset of labels was simplified to aid data preparationand to reduce the complexity of the task.
We useonly six relation types including the three core re-lations BEFORE, AFTER, and OVERLAP, the twoless specific relations BEFORE-OR-OVERLAP andOVERLAP-OR-AFTER for ambiguous cases, and fi-nally the relation VAGUE for those cases where noparticular relation can be established.Temporal relations come in two broad flavours:anchorings of events to time expressions and or-derings of events.
Events can be anchored to anadjacent time expression as in examples 5 and 6 orto the document creation time as in 7.
(5) Mary taughte1on Tuesday morningt1OVERLAP(e1,t1)(6) They cancelled the eveningt2classe2OVERLAP(e2,t2)(7) Most troops will leavee1Iraq by August of2010.
AFTER(e1,dct)The country defaultede2on debts for that en-tire year.
BEFORE(e2,dct)In addition, events can be ordered relative toother events, as in the examples below.
(8) The President spokee1to the nation onTuesday on the financial crisis.
He hadconferrede2with his cabinet regarding pol-icy the day before.
AFTER(e1,e2)(9) The students hearde1a fire alarme2.OVERLAP(e1,e2)(10) He saide1they had postponede2the meeting.AFTER(e1,e2)3 TempEval-2 TasksWe can now define the six TempEval tasks:A.
Determine the extent of the time expressionsin a text as defined by the TimeML TIMEX3tag.
In addition, determine value of the fea-tures TYPE and VAL.B.
Determine the extent of the events in a textas defined by the TimeML EVENT tag.
Inaddition, determine the value of the featuresCLASS, TENSE, ASPECT, POLARITY, andMODALITY.C.
Determine the temporal relation between anevent and a time expression in the samesentence.
This task is further restricted byrequiring that either the event syntacticallydominates the time expression or the eventand time expression occur in the same nounphrase.D.
Determine the temporal relation between anevent and the document creation time.E.
Determine the temporal relation between twomain events in consecutive sentences.F.
Determine the temporal relation between twoevents where one event syntactically domi-nates the other event.Of these tasks, C, D and E were also defined forTempEval-1.
However, the syntactic locality re-striction in task C was not present in TempEval-1.Task participants could choose to either do alltasks, focus on the time expression task, focus onthe event task, or focus on the four temporal rela-tion tasks.
In addition, participants could chooseone or more of the six languages for which we pro-vided data: Chinese, English, French, Italian, Ko-rean, and Spanish.We feel that well-defined tasks allow us to struc-ture the workflow, allowing us to create task-specific guidelines and using task-specific anno-tation tools to speed up annotation.
More im-portantly, each task can be evaluated in a fairlystraightforward way, contrary to for example theproblems that pop up when evaluating two com-plex temporal graphs for the same document.
Inaddition, tasks can be ranked, allowing systems tofeed the results of one (more precise) task as a fea-ture into another task.Splitting the task into substask reduces the errorrate in the manual annotation, and that mergingthe different sub-task into a unique layer as a post-processing operation (see figure 1) provides better58Figure 1: Merging Relationsand more reliable results (annotated data) than do-ing a complex task all at once.4 Data PreparationThe data for the five languages were prepared in-dependently of each other and do not comprise aparallel corpus.
However, annotation specifica-tions and guidelines for the five languages weredeveloped in conjunction with one other, in manycases based on version 1.2.1 of the TimeML an-notation guidelines for English3.
Not all corporacontained data for all six tasks.
Table 1 gives thesize of the training set and the relation tasks thatwere included.language tokens C D E F XChinese 23,000 X X X XEnglish 63,000 X X X XItalian 27,000 X X XFrench 19,000 XKorean 14,000Spanish 68,000 X XTable 1: Corpus size and relation tasksAll corpora include event and timex annota-tion.
The French corpus contained a subcorpuswith temporal relations but these relations werenot split into the four tasks C through F.Annotation proceeded in two phases: a dualannotation phase where two annotators annotateeach document and an adjudication phase wherea judge resolves disagreements between the an-notators.
Most languages used BAT, the BrandeisAnnotation Tool (Verhagen, 2010), a generic web-based annotation tool that is centered around thenotion of annotation tasks.
With the task decom-position allowed by BAT, it is possible to structurethe complex task of temporal annotation by split-ting it up in as many sub tasks as seems useful.
As3See http://www.timeml.org.such, BAT was well-suited for TempEval-2 anno-tation.We now give a few more details on the Englishand Spanish data, skipping the other languages forreasons that will become obvious at the beginningof section 6.The English data sets were based on TimeBank(Pustejovsky et al, 2003; Boguraev et al, 2007),a hand-built gold standard of annotated texts us-ing the TimeML markup scheme.4However, allevent annotation was reviewed to make sure thatthe annotation complied with the latest guidelinesand all temporal relations were added according tothe Tempeval-2 relation tasks, using the specifiedrelation types.The data released for the TempEval-2 Spanishedition is a fragment of the Spanish TimeBank,currently under development.
Its documents areoriginally from the Spanish part of the AnCoracorpus (Taul?e et al, 2008).
Data preparation fol-lowed the annotation guidelines created to dealwith the specificities of event and timex expres-sions in Spanish (Saur??
et al, 2009a; Saur??
et al,2009b).5 Evaluation MetricsFor the extents of events and time expres-sions (tasks A and B), precision, recall and thef1-measure are used as evaluation metrics, usingthe following formulas:precision = tp/(tp + fp)recall = tp/(tp + fn)f -measure = 2 ?
(P ?
R)/(P + R)Where tp is the number of tokens that are partof an extent in both key and response, fp is thenumber of tokens that are part of an extent in theresponse but not in the key, and fn is the numberof tokens that are part of an extent in the key butnot in the response.For attributes of events and time expressions(the second part of tasks A and B) and for relationtypes (tasks C through F) we use an even simplermetric: the number of correct answers divided bythe number of answers.4See www.timeml.org for details on TimeML, Time-Bank is distributed free of charge by the LinguisticData Consortium (www.ldc.upenn.edu), catalog num-ber LDC2006T08.596 System ResultsEight teams participated in TempEval-2, submit-ting a grand total of eighteen systems.
Some ofthese systems only participated in one or two taskswhile others participated in all tasks.
The distribu-tion over the six languages was very uneven: six-teen systems for English, two for Spanish and onefor English and Spanish.The results for task A, recognition and normal-ization of time expressions, are given in tables 2and 3.team p r f type valUC3M 0.90 0.87 0.88 0.91 0.83TIPSem 0.95 0.87 0.91 0.91 0.78TIPSem-B 0.97 0.81 0.88 0.99 0.75Table 2: Task A results for Spanishteam p r f type valEdinburgh 0.85 0.82 0.84 0.84 0.63HeidelTime1 0.90 0.82 0.86 0.96 0.85HeidelTime2 0.82 0.91 0.86 0.92 0.77JU CSE 0.55 0.17 0.26 0.00 0.00KUL 0.78 0.82 0.80 0.91 0.55KUL Run 2 0.73 0.88 0.80 0.91 0.55KUL Run 3 0.85 0.84 0.84 0.91 0.55KUL Run 4 0.76 0.83 0.80 0.91 0.51KUL Run 5 0.75 0.85 0.80 0.91 0.51TERSEO 0.76 0.66 0.71 0.98 0.65TIPSem 0.92 0.80 0.85 0.92 0.65TIPSem-B 0.88 0.60 0.71 0.88 0.59TRIOS 0.85 0.85 0.85 0.94 0.76TRIPS 0.85 0.85 0.85 0.94 0.76USFD2 0.84 0.79 0.82 0.90 0.17Table 3: Task A results for EnglishThe results for Spanish are more uniform andgenerally higher than the results for English.For Spanish, the f-measure for TIMEX3 extentsranges from 0.88 through 0.91 with an average of0.89; for English the f-measure ranges from 0.26through 0.86, for an average of 0.78.
However,due to the small sample size it is hard to makeany generalizations.
In both languages, type de-tection clearly was a simpler task than determiningthe value.The results for task B, event recognition, are givenin tables 4 and 5.
Both tables contain results forboth Spanish and English, the first part of each ta-ble contains the results for Spanish and the nextpart the results for English.team p r fTIPSem 0.90 0.86 0.88TIPSem-B 0.92 0.85 0.88team p r fEdinburgh 0.75 0.85 0.80JU CSE 0.48 0.56 0.52TIPSem 0.81 0.86 0.83TIPSem-B 0.83 0.81 0.82TRIOS 0.80 0.74 0.77TRIPS 0.55 0.88 0.68Table 4: Event extent resultsThe column headers in table 5 are abbrevia-tions for polarity (pol), mood (moo), modality(mod), tense (tns), aspect (asp) and class (cl).
Notethat the English team chose to include modalitywhereas the Spanish team used mood.team pol moo tns asp clTIPSem 0.92 0.80 0.96 0.89 0.66TIPSem-B 0.92 0.79 0.96 0.89 0.66team pol mod tns asp clEdinburgh 0.99 0.99 0.92 0.98 0.76JU CSE 0.98 0.98 0.30 0.95 0.53TIPSem 0.98 0.97 0.86 0.97 0.79TIPSem-B 0.98 0.98 0.85 0.97 0.79TRIOS 0.99 0.95 0.91 0.98 0.77TRIPS 0.99 0.96 0.67 0.97 0.67Table 5: Event attribute resultsAs with the time expressions results, the samplesize for Spanish is small, but note again the higherf-measure for event extents in Spanish.Table 6 shows the results for all relation tasks, withthe Spanish systems in the first two rows and theEnglish systems in the last six rows.
Recall that forSpanish the training and test sets only containeddata for tasks C and D.Interestingly, the version of the TIPSem sys-tems that were applied to the Spanish data didmuch better on task C compared to its Englishcousins, but much worse on task D, which is ratherpuzzling.Such a difference in performance of the systemscould be due to differences in annotation accurate-ness, or it could be due to some particularities ofhow the two languages express certain temporal60team C D E FTIPSem 0.81 0.59 - -TIPSem-B 0.81 0.59 - -JU CSE 0.63 0.80 0.56 0.56NCSU-indi 0.63 0.68 0.48 0.66NCSU-joint 0.62 0.21 0.51 0.25TIPSem 0.55 0.82 0.55 0.59TIPSem-B 0.54 0.81 0.55 0.60TRIOS 0.65 0.79 0.56 0.60TRIPS 0.63 0.76 0.58 0.59USFD2 0.63 - 0.45 -Table 6: Results for relation tasksaspects, or perhaps the one corpus is more ho-mogeneous than the other.
Again, there are notenough data points, but the issue deserves furtherattention.For each task, the test data provided the eventpairs or event-timex pairs with the relation typeset to NONE and participating systems would re-place that value with one of the six allowed rela-tion types.
However, participating systems wereallowed to not replace NONE and not be penalizedfor it.
Those cases would not be counted whencompiling the scores in table 6.
Table 7 lists thosesystems that did not classify all relation and thepercentage of relations for each task that those sys-tems did not classify.team C D E FTRIOS 25% 19% 36% 31%TRIPS 20% 10% 17% 10%Table 7: Percentage not classifiedA comparison with the Tempeval-1 results fromSemeval-2007 may be of interest.
Six systemsparticipated in the TempEval-1 tasks, comparedto seven or eight systems for TempEval-2.
Table8 lists the average scores and the standard devi-ations for all the tasks (on the English data) thatTempeval-1 and Tempeval-2 have in common.C D Etempeval-1 average 0.59 0.76 0.51stddev 0.03 0.03 0.05tempeval-2 average 0.61 0.70 0.53stddev 0.04 0.22 0.05Table 8: Comparing TempevalsThe results are very similar except for task D,but if we take a away the one outlier (the NCSU-joint score of 0.21) then the average becomes 0.78with a standard deviation of 0.05.
However, wehad expected that for TempEval-2 the systemswould score better on task C since we added therestriction that the event and time expression hadto be syntactically adjacent.
It is not clear why theresults on task C have not improved.7 ConclusionIn this paper, we described the TempEval-2 taskwithin the SemEval 2010 competition.
This taskinvolves identifying the temporal relations be-tween events and temporal expressions in text.
Us-ing a subset of TimeML temporal relations, weshow how temporal relations and anchorings canbe annotated and identified in six different lan-guages.
The markup language adopted presentsa descriptive framework with which to examinethe temporal aspects of natural language informa-tion, demonstrating in particular, how tense andtemporal information is encoded in specific sen-tences, and how temporal relations are encodedbetween events and temporal expressions.
Thiswork paves the way towards establishing a broadand open standard metadata markup language fornatural language texts, examining events, tempo-ral expressions, and their orderings.One thing that would need to be addressed ina follow-up task is what the optimal number oftasks is.
Tempeval-2 had six tasks, spread out oversix languages.
This brought about some logisti-cal challenges that delayed data delivery and mayhave given rise to a situation where there was sim-ply not enough time for many systems to properlyprepare.
And clearly, the shared task was not suc-cessful in attracting systems to four of the six lan-guages.8 AcknowledgementsMany people were involved in TempEval-2.
Wewant to express our gratitude to the following keycontributors: Nianwen Xue, Estela Saquete, Lo-tus Goldberg, Seohyun Im, Andr?e Bittar, NicolettaCalzolari, Jessica Moszkowicz and Hyopil Shin.Additional thanks to Joan Banach, JudithDomingo, Pau Gim?enez, Jimena del Solar, TeresaSu?nol, Allyson Ettinger, Sharon Spivak, NahedAbul-Hassan, Ari Abelman, John Polson, Alexan-dra Nunez, Virginia Partridge, , Amber Stubbs,Alex Plotnick, Yuping Zhou, Philippe Muller and61Irina Prodanof.The work on the Spanish corpus was supportedby a EU Marie Curie International ReintegrationGrant (PIRG04-GA-2008-239414).
Work on theEnglish corpus was supported under the NSF-CRIgrant 0551615, ?Towards a Comprehensive Lin-guistic Annotation of Language?
and the NSF-INT-0753069 project ?Sustainable Interoperabil-ity for Language Technology (SILT)?, funded bythe National Science Foundation.Finally, thanks to all the participants, for stick-ing with a task that was not always as flawless andtimely as it could have been in a perfect world.ReferencesJames Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Communications of the ACM,26(11):832?843.Bran Boguraev, James Pustejovsky, Rie Ando, andMarc Verhagen.
2007.
Timebank evolution as acommunity resource for timeml parsing.
LanguageResource and Evaluation, 41(1):91?115.James Pustejovsky, David Day, Lisa Ferro, RobertGaizauskas, Patrick Hanks, Marcia Lazo, RoserSaur?
?, Andrew See, Andrea Setzer, and Beth Sund-heim.
2003.
The TimeBank Corpus.
Corpus Lin-guistics, March.Roser Saur?
?, Olga Batiukova, and James Pustejovsky.2009a.
Annotating events in spanish.
timeml an-notation guidelines.
Technical Report VersionTempEval-2010., Barcelona Media - InnovationCenter.Roser Saur?
?, Estela Saquete, and James Pustejovsky.2009b.
Annotating time expressions in spanish.timeml annotation guidelines.
Technical ReportVersion TempEval-2010, Barcelona Media - Inno-vation Center.Mariona Taul?e, Toni Mart?
?, and Marta Recasens.
2008.Ancora: Multilevel annotated corpora for catalanand spanish.
In Proceedings of the LREC 2008,Marrakesh, Morocco.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval tempo-ral relation identification.
In Proc.
of the FourthInt.
Workshop on Semantic Evaluations (SemEval-2007), pages 75?80, Prague, Czech Republic, June.Association for Computational Linguistics.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Jessica Moszkowicz, and JamesPustejovsky.
2009.
The tempeval challenge: iden-tifying temporal relations in text.
Language Re-sources and Evaluation.Marc Verhagen.
2010.
The Brandeis Annotation Tool.In Language Resources and Evaluation Conference,LREC 2010, Malta.62
