Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 636?641,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLearning Condensed Feature Representations from Large UnsupervisedData Sets for Supervised LearningJun Suzuki, Hideki Isozaki, and Masaaki NagataNTT Communication Science Laboratories, NTT Corp.2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan{suzuki.jun, isozaki.hideki, nagata.masaaki}@lab.ntt.co.jpAbstractThis paper proposes a novel approach for ef-fectively utilizing unsupervised data in addi-tion to supervised data for supervised learn-ing.
We use unsupervised data to gener-ate informative ?condensed feature represen-tations?
from the original feature set used insupervised NLP systems.
The main con-tribution of our method is that it can of-fer dense and low-dimensional feature spacesfor NLP tasks while maintaining the state-of-the-art performance provided by the recentlydeveloped high-performance semi-supervisedlearning technique.
Our method matches theresults of current state-of-the-art systems withvery few features, i.e., F-score 90.72 with344 features for CoNLL-2003 NER data, andUAS 93.55 with 12.5K features for depen-dency parsing data derived from PTB-III.1 IntroductionIn the last decade, supervised learning has becomea standard way to train the models of many naturallanguage processing (NLP) systems.
One simple butpowerful approach for further enhancing the perfor-mance is to utilize a large amount of unsuperviseddata to supplement supervised data.
Specifically,an approach that involves incorporating ?clustering-based word representations (CWR)?
induced fromunsupervised data as additional features of super-vised learning has demonstrated substantial perfor-mance gains over state-of-the-art supervised learn-ing systems in typical NLP tasks, such as named en-tity recognition (Lin and Wu, 2009; Turian et al,2010) and dependency parsing (Koo et al, 2008).We refer to this approach as the iCWR approach,The iCWR approach has become popular for en-hancement because of its simplicity and generality.The goal of this paper is to provide yet anothersimple and general framework, like the iCWR ap-proach, to enhance existing state-of-the-art super-vised NLP systems.
The differences between theiCWR approach and our method are as follows; sup-pose F is the original feature set used in supervisedlearning, C is the CWR feature set, andH is the newfeature set generated by our method.
Then, with theiCWR approach, C is induced independently fromF , and used in addition to F in supervised learning,i.e., F ?
C. In contrast, in our method H is directlyinduced from F with the help of an existing modelalready trained by supervised learning with F , andused in place of F in supervised learning.The largest contribution of our method is thatit offers an architecture that can drastically reducethe number of features, i.e., from 10M featuresin F to less than 1K features in H by construct-ing ?condensed feature representations (COFER)?,which is a new and very unique property that can-not be matched by previous semi-supervised learn-ing methods including the iCWR approach.
Onenoteworthy feature of our method is that there is noneed to handle sparse and high-dimensional featurespaces often used in many supervised NLP systems,which is one of the main causes of the data sparse-ness problem often encountered when we learn themodel with a supervised leaning algorithm.
As aresult, NLP systems that are both compact and high-performance can be built by retraining the modelwith the obtained condensed feature set H.2 Condensed Feature RepresentationsLet us first define the condensed feature set H. Inthis paper, we call the feature set generally used insupervised learning, F , the original feature set.
LetN andM represent the numbers of features inF andH, respectively.
We assume M?N , and generallyM N .
A condensed feature hm ?H is charac-636Potencies are multiplied by a positive constant ?0 1 2-1-2 3 4Section 3.3: Feature potency quantizationFeature potency Section 3.4: Condensed feature constructionF N  (e.g., N=100M)Original feature set Section 3.1: Feature potency estimationFeatures mapped into this area will be zeroed by the effect of CC0-C Section 3.2: Feature potency discountingFeature potency?  0Feature potency ? ( Integer Space N )Condensed feature setH Each condensed feature is represented as a set of features in the original feature set F.-1/?1/?3/??
-2/?M  (e.g., M=1K) The potencies are also utilized as an (M+1)-th condensed featureH(Quantized feature potency)Features mapped into zero are discarded and never mapped into any condensed featuresFigure 1: Outline of our method to construct a condensedfeature set.r?
(x) =Xy?Y(x)r(x,y)/|Y(x)|.V +D (fn) =Xx?Dfn(x, y?
)(r(x, y?)
?
r?
(x))V ?D (fn) = ?Xx?DXy?Y(x)\y?fn(x,y)(r(x,y) ?
r?
(x))Rn=Xx?DXy?Y(x)r(x,y)fn(x,y), An=Xx?Dr?
(x)Xy?Y(x)fn(x,y)Figure 2: Notations used in this paper.terized as a set of features in F , that is, hm =Smwhere Sm ?
F .
We assume that each original fea-ture fn?F maps, at most, to one condensed featurehm.
This assumption prevents two condensed fea-tures from containing the same original feature, andsome original features from not being mapped to anycondensed feature.
Namely, Sm ?
Sm?
=?
for all mand m?, where m 6=m?, and?Mm=1 Sm?F hold.The value of each condensed feature is calcu-lated by summing the values of the original fea-tures assigned to it.
Formally, let X and Y repre-sent the sets of all possible inputs and outputs ofa target task, respectively.
Let x ?
X be an in-put, and y ?
Y(x) be an output, where Y(x) ?
Yrepresents the set of possible outputs given x. Wewrite the n-th feature function of the original fea-tures, whose value is determined by x and y, asfn(x,y), where n ?
{1, .
.
.
, N}.
Similarly, wewrite them-th feature function of the condensed fea-tures as hm(x,y), where m?
{1, .
.
.
,M}.
We statethat the value of hm(x,y) is calculated as follows:hm(x,y)=?fn?Sm fn(x,y).3 Learning COFERsThe remaining part of our method consists of theway to map the original features into the condensedfeatures.
For this purpose, we define the feature po-tency, which is evaluated by employing an existingsupervised model with unsupervised data sets.
Fig-ure 1 shows a brief sketch of the process to constructthe condensed features described in this section.3.1 Self-taught-style feature potency estimationWe assume that we have a model trained by super-vised learning, which we call the ?base supervisedmodel?, and the original feature set F that is usedin the base supervised model.
We consider a casewhere the base supervised model is a (log-)linearmodel, and use the following equation to select thebest output y?
given x:y?=argmaxy?Y(x)?Nn=1 wnfn(x,y), (1)where wn is a model parameter (or weight) of fn.Linear models are currently the most widely-usedmodels and are employed in many NLP systems.To simplify the explanation, we define functionr(x,y), where r(x,y) returns 1 if y = y?
is obtainedfrom the base supervised model given x, and 0 oth-erwise.
Let r?
(x) represent the average of r(x,y) inx (see Figure 2 for details).
We also define V +D (fn)and V ?D (fn) as shown in Figure 2 where D repre-sents the unsupervised data set.
V +D (fn) measuresthe positive correlation with the best output y?
givenby the base supervised model since this is the sum-mation of all the (weighted) feature values used inthe estimation of the one best output y?
over all x inthe unsupervised data D. Similarly, V ?D (fn) mea-sures the negative correlation with y?.
Next, we de-fine VD(fn) as the feature potency of fn: VD(fn) =V +D (fn)?
V?D (fn).An intuitive explanation of VD(fn) is as follows;if |VD(fn)| is large, the distribution of fn has eithera large positive or negative correlation with the bestoutput y?
given by the base supervised model.
Thisimplies that fn is an informative and potent featurein the model.
Then, the distribution of fn has verysmall (or no) correlation to determine y?
if |VD(fn)|is zero or near zero.
In this case, fn can be evaluatedas an uninformative feature in the model.
From thisperspective, we treat VD(fn) as a measure of featurepotency in terms of the base supervised model.The essence of this idea, evaluating featuresagainst each other on a certain model, is widelyused in the context of semi-supervised learning,i.e., (Ando and Zhang, 2005; Suzuki and Isozaki,6372008; Druck and McCallum, 2010).
Our methodis rough and a much simpler framework for imple-menting this fundamental idea of semi-supervisedlearning developed for NLP tasks.
We create asimple framework to achieve improved flexibility,extendability, and applicability.
In fact, we applythe framework by incorporating a feature mergingand elimination architecture to obtain effective con-densed feature sets for supervised learning.3.2 Feature potency discountingTo discount low potency values, we redefine featurepotency as V ?D(fn) instead of VD(fn) as follows:V ?D(fn) =??
?log [Rn+C]?log[An] if Rn?An<?C0 if ?
C?Rn?An?Clog [Rn?C]?log[An] if C<Rn?Anwhere Rn and An are defined in Figure 2.
Notethat VD(fn) = V +D (fn) ?
V?D (fn) = Rn ?
An.The difference from VD(fn) is that we cast it in thelog-domain and introduce a non-negative constantC.
The introduction of C is inspired by the L1-regularization technique used in supervised learningalgorithms such as (Duchi and Singer, 2009; Tsu-ruoka et al, 2009).
C controls how much we dis-count VD(fn) toward zero, and is given by the user.3.3 Feature potency quantizationWe define V ?D(fn) as V ?D(fn) = d?V ?D(fn)e ifV ?D(fn) > 0 and V ?D(fn) = b?V ?D(fn)c otherwise,where ?
is a positive user-specified constant.
Notethat V ?D(fn) always becomes an integer, that is,V ?D(fn) ?N where N = {.
.
.
,?2,?1, 0, 1, 2, .
.
.
}.This calculation can be seen as mapping each fea-ture into a discrete (integer) space with respect toV ?D(fn).
?
controls the range of V ?D(fn) mappinginto the same integer.3.4 Condensed feature constructionSuppose we have M different quantized feature po-tency values in V ?D(fn) for all n, which we rewriteas {um}Mm=1.
Then, we define Sm as a set of fnwhose quantized feature potency value is um.
Asdescribed in Section 2, we define the m-th con-densed feature hm(x,y) as the summation of allthe original features fn assigned to Sm.
That is,hm(x,y) =?fn?Sm fn(x,y).
This feature fusionprocess is intuitive since it is acceptable if featureswith the same (similar) feature potency are given thesame weight by supervised learning since they havethe same potency with regard to determining y?.
?determines the number of condensed features to bemade; the number of condensed features becomeslarge if ?
is large.
Obviously, the upper bound ofthe number of condensed features is the number oforiginal features.To exclude possibly unnecessary original featuresfrom the condensed features, we discard feature fnfor all n if un = 0.
This is reasonable since, as de-scribed in Section 3.1, a feature has small (or no)effect in achieving the best output decision in thebase supervised model if its potency is near 0.
C in-troduced in Section 3.2 mainly influences how manyoriginal features are discarded.Additionally, we also utilize the ?quantized?
fea-ture potency values themselves as a new feature.The reason behind is that they are also very infor-mative for supervised learning.
Their use is impor-tant to further boost the performance gain offeredby our method.
For this purpose, we define ?
(x,y)as ?
(x,y) =?Mm=1(um/?)hm(x,y).
We thenuse ?
(x,y) as the (M + 1)-th feature of our con-densed feature set.
As a result, the condensed fea-ture set obtained with our method is represented asH = {h1(x,y), .
.
.
, hM (x,y), ?
(x,y)}.Note that the calculation cost of ?
(x,y) is negli-gible.
We can calculate the linear discriminant func-tion g(x,y) as: g(x,y) =?Mm=1 wmhm(x,y) +wM+1?
(x,y) =?Mm=1 w?mhm(x,y), where w?m =(wm + wM+1um/?).
We emphasize that once{wm}M+1m=1 are determined by supervised learning,we can calculate w?m in a preliminary step beforethe test phase.
Thus, our method also takes the formof a linear model.
The number of features for ourmethod is essentially M even if we add ?.3.5 Application to Structured Prediction TasksWe modify our method to better suit structured pre-diction problems in terms of calculation cost.
For astructured prediction problem, it is usual to decom-pose or factorize output structure y into a set of lo-cal sub-structures z to reduce the calculation costand to cope with the sparsity of the output spaceY .
This factorization can be accomplished by re-stricting features that are extracted only from the in-formation within decomposed local sub-structure z638and given input x.
We write z ?
y when the lo-cal sub-structure z is a part of output y, assumingthat output y is constructed by a set of local sub-structures.
Then formally, the n-th feature is writtenas fn(x, z), and fn(x,y) =?z?y fn(x, z) holds.Similarly, we introduce r(x, z), where r(x, z) = 1if z ?
y?, and r(x, z) = 0 otherwise, namely z /?
y?.We define Z(x) as the set of all local sub-structures possibly generated for all y in Y(x).Z(x) can be enumerated easily, unless we use typi-cal first- or second-order factorization models by therestriction of efficient decoding algorithms, which isthe typical case for many NLP tasks such as namedentity recognition and dependency parsing.Finally, we replace all Y(x) with Z(x), and usefn(x, z) and r(x, z) instead of fn(x,y) and r(x,y),respectively, in Rn and An.
When we use these sub-stitutions, there is no need to incorporate an efficientalgorithm such as dynamic programming into ourmethod.
This means that our feature potency esti-mation can be applied to the structured predictionproblem at low cost.3.6 Efficient feature potency computationOur feature potency estimation described in Section3.1 to 3.3 is highly suitable for implementation inthe MapReduce framework (Dean and Ghemawat,2008), which is a modern distributed parallel com-puting framework.
This is because Rn and An canbe calculated by the summation of a data-wise cal-culation (map phase), and V ?D(fn) can be calculatedindependently by each feature (reduce phase).
Weemphasize that our feature potency estimation canbe performed in a ?single?
map-reduce process.4 ExperimentsWe conducted experiments on two different NLPtasks, namely NER and dependency parsing.
To fa-cilitate comparisons with the performance of previ-ous methods, we adopted the experimental settingsused to examine high-performance semi-supervisedNLP systems; i.e., NER (Ando and Zhang, 2005;Suzuki and Isozaki, 2008) and dependency pars-ing (Koo et al, 2008; Chen et al, 2009; Suzukiet al, 2009).
For the supervised datasets, we usedCoNLL?03 (Tjong Kim Sang and DeMeulder, 2003)shared task data for NER, and the Penn Treebank III(PTB) corpus (Marcus et al, 1994) for dependencyparsing.
We prepared a total of 3.72 billion tokentext data as unsupervised data following the instruc-tions given in (Suzuki et al, 2009).4.1 Comparative MethodsWe mainly compare the effectiveness of COFERwith that of CWR derived by the Brown algorithm.The iCWR approach yields the state-of-the-art re-sults with both dependency parsing data derivedfrom PTB-III (Koo et al, 2008), and the CoNLL?03shared task data (Turian et al, 2010).
By compar-ing COFER with iCWR we can clarify its effective-ness in terms of providing better features for super-vised learning.
We use the term active features torefer to features whose corresponding model param-eter is non-zero after supervised learning.
It is well-known that we can discard non-active features fromthe trained model without any loss after finishing su-pervised learning.
Finally, we compared the perfor-mance in terms of the number of active features inthe model given by supervised learning.
We notehere that the number of active features for COFERis the number of features hm if w?m = 0, which isnot wm = 0 for a fair comparison.Unlike COFER, iCWR does not have any archi-tecture to winnow the original feature set used insupervised learning.
For a fair comparison, weprepared L1-regularized supervised learning algo-rithms, which try to reduce the non-zero parametersin a model.
Specifically, we utilized L1-regularizedCRF (L1CRF) optimized by OWL-QN (Andrewand Gao, 2007) for NER, and the online struc-tured output learning version of FOBOS (Duchiand Singer, 2009; Tsuruoka et al, 2009) with L1-regularization (ostL1FOBOS) for dependency pars-ing.
In addition, we also examined L2 regular-ized CRF (Lafferty et al, 2001) optimized by L-BFGS (Liu and Nocedal, 1989) (L2CRF) for NER,and the online structured output learning version ofthe Passive-Aggressive algorithm (ostPA) (Cram-mer et al, 2006) for dependency parsing to illus-trate the baseline performance regardless of the ac-tive feature number.4.2 Settings for COFERWe utilized baseline supervised learning mod-els as the base supervised models of COFER.63986.088.090.092.094.096.01.0E+01 1.0E+03 1.0E+05 1.0E+07 1.0E+09iCWR+COFER: L2CRF iCWR+COFER: L1CRFCOFER: L2CRF COFER: L1CRFiCWR: L2CRF iCWR: L1CRFSup.L2CRF Sup.L1CRFF-score# of active features [log-scale]?=1e+01?=1e+02 ?=1e+04?=1e+00proposed90.091.092.093.094.095.01.E+02 1.E+04 1.E+06 1.E+08iCWR+COFER: ostPA iCWR+COFER: ostL1FOBOSCOFER: ostPA COFER: ostL1FOBOSiCWR: ostPA iCWR: ostL1FOBOSSup.ostPA Sup.ostL1FOBOS# of active features [log-scale]Unlabeled Attachment Score ?=1e+00?=1e+05?=1e+01 ?=1e+03proposed(a) NER (F-score) (b) dep.
parsing (UAS)Figure 3: Performance vs. size of active features in thetrained model on the development setsIn addition, we also report the results when wetreat iCWR as COFER?s base supervised mod-els (iCWR+COFER).
This is a very natural andstraightforward approach to combining these two.We generally handle several different types of fea-tures such as words, part-of-speech tags, word sur-face forms, and their combinations.
Suppose wehave K different feature types, which are often de-fined by feature templates, i.e., (Suzuki and Isozaki,2008; Lin andWu, 2009).
In our experiments, we re-strict the merging of features during the condensedfeature construction process if and only if the fea-tures are the same feature type.
As a result, COFERessentially consists ofK different condensed featuresets.
The numbers of feature typesK were 79 and 30for our NER and dependency parsing experiments,respectively.
We note that this kind of feature par-tition by their types is widely used in the context ofsemi-supervised learning (Ando and Zhang, 2005;Suzuki and Isozaki, 2008).4.3 Results and DiscussionFigure 3 displays the performance on the develop-ment set with respect to the number of active fea-tures in the trained models given by each supervisedlearning algorithm.
In both NER and dependencyparsing experiments, COFER significantly outper-formed iCWR.
Moreover, COFER was surprisinglyrobust in relation to the number of active featuresin the model.
These results reveal that COFER pro-vides effective feature sets for certain NLP tasks.We summarize the noteworthy results in Figure 3,and also the performance of recent top-line systemsfor NER and dependency parsing in Table 1.
Over-all, COFER matches the results of top-line semi-NER system dev.
test #.USD #.AFSup.L1CRF 90.40 85.08 0 0.57MiCWR: L1CRF 93.33 89.99 3,720M 0.62MCOFER: L1CRF (?
= 1e+ 00) 93.42 88.81 3,720M 359(?
= 1e+ 04) 93.60 89.22 3,720M 2.46MiCWR+COFER: (?
= 1e+ 00) 94.39 90.72 3,720M 344L1CRF (?
= 1e+ 04) 94.91 91.02 3,720M 5.94M(Ando and Zhang, 2005) 93.15 89.31 27M N/A(Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A(Ratinov and Roth, 2009) 93.50 90.57 N/A N/A(Turian et al, 2010) 93.95 90.36 37M N/A(Lin and Wu, 2009) N/A 90.90 700,000M N/ADependency parser dev.
test #.USD #.AFostL1FOBOS 93.15 92.82 0 6.80MiCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67MCOFER:ostL1FOBOS (?
= 1e+ 03) 93.53 93.23 3,720M 20.7K(?
= 1e+ 05) 93.91 93.71 3,720M 3.23MiCWR+COFER: (?
= 1e+ 03) 93.93 93.55 3,720M 12.5KostL1FOBOS (?
= 1e+ 05) 94.33 94.22 3,720M 5.77M(Koo and Collins, 2010) 93.49 93.04 0 N/A(Martins et al, 2010) N/A 93.26 0 55.25M(Koo et al, 2008) 93.30 93.16 43M N/A(Chen et al, 2009) N/A 93.16 43M N/A(Suzuki et al, 2009) 94.13 93.79 3,720M N/ATable 1: Comparison with previous top-line systems ontest data.
(#.USD: unsupervised data size.
#.AF: the sizeof active features in the trained model.
)supervised learning systems even though it uses farfewer active features.In addition, the combination of iCWR+COFERsignificantly outperformed the current best resultsby achieving a 0.12 point gain from 90.90 to 91.02for NER, and a 0.43 point gain from 93.79 to 94.22for dependency parsing, with only 5.94M and 5.77Mfeatures, respectively.5 ConclusionThis paper introduced the idea of condensed featurerepresentations (COFER) as a simple and generalframework that can enhance the performance of ex-isting supervised NLP systems.
We also proposeda method that efficiently constructs condensed fea-ture sets through discrete feature potency estima-tion over unsupervised data.
We demonstrated thatCOFER based on our feature potency estimation canoffer informative dense and low-dimensional featurespaces for supervised learning, which is theoreti-cally preferable to the sparse and high-dimensionalfeature spaces often used in many NLP tasks.
Exist-ing NLP systems can be made more compact withhigher performance by retraining their models withour condensed features.640ReferencesRie Kubota Ando and Tong Zhang.
2005.
A High-Performance Semi-Supervised Learning Method forText Chunking.
In Proceedings of 43rd Annual Meet-ing of the Association for Computational Linguistics,pages 1?9.Galen Andrew and Jianfeng Gao.
2007.
ScalableTraining of L1-regularized Log-linear Models.
InZoubin Ghahramani, editor, Proceedings of the 24thAnnual International Conference on Machine Learn-ing (ICML 2007), pages 33?40.
Omnipress.Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,and Kentaro Torisawa.
2009.
Improving DependencyParsing with Subtrees from Auto-Parsed Data.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 570?579.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online Passive-Aggressive Algorithms.
Journal of Machine LearningResearch, 7:551?585.Jeffrey Dean and Sanjay Ghemawat.
2008.
MapReduce:Simplified Data Processing on Large Clusters.
Com-mun.
ACM, 51(1):107?113.Gregory Druck and Andrew McCallum.
2010.
High-Performance Semi-Supervised Learning using Dis-criminatively Constrained Generative Models.
In Pro-ceedings of the International Conference on MachineLearning (ICML 2010), pages 319?326.John Duchi and Yoram Singer.
2009.
Efficient On-line and Batch Learning Using Forward BackwardSplitting.
Journal of Machine Learning Research,10:2899?2934.Terry Koo and Michael Collins.
2010.
Efficient Third-Order Dependency Parsers.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1?11.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple Semi-supervised Dependency Parsing.
In Pro-ceedings of ACL-08: HLT, pages 595?603.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
InProceedings of the International Conference on Ma-chine Learning (ICML 2001), pages 282?289.Dekang Lin and Xiaoyun Wu.
2009.
Phrase Cluster-ing for Discriminative Learning.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP, pages1030?1038.Dong C. Liu and Jorge Nocedal.
1989.
On the LimitedMemory BFGS Method for Large Scale Optimization.Math.
Programming, Ser.
B, 45(3):503?528.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,and Mario Figueiredo.
2010.
Turbo Parsers: Depen-dency Parsing by Approximate Variational Inference.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 34?44.Lev Ratinov and Dan Roth.
2009.
Design Challengesand Misconceptions in Named Entity Recognition.
InProceedings of the Thirteenth Conference on Compu-tational Natural Language Learning (CoNLL-2009),pages 147?155.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedSequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.
In Proceedings of ACL-08: HLT, pages 665?673.Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichaelCollins.
2009.
An Empirical Study of Semi-supervised Structured Conditional Models for Depen-dency Parsing.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Process-ing, pages 551?560.Erik Tjong Kim Sang and Fien De Meulder.
2003.
Intro-duction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.
In Proceed-ings of CoNLL-2003, pages 142?147.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic Gradient Descent Trainingfor L1-regularized Log-linear Models with Cumula-tive Penalty.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 477?485.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word Representations: A Simple and GeneralMethod for Semi-Supervised Learning.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 384?394.641
