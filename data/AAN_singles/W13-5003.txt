Proceedings of the TextGraphs-8 Workshop, pages 11?19,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsMerging Word SensesSumit Bhagwani, Shrutiranjan Satapathy, Harish KarnickComputer Science and EngineeringIIT Kanpur, Kanpur - 208016, India{sumitb,sranjans,hk}@cse.iitk.ac.inAbstractWordNet, a widely used sense inventory forWord Sense Disambiguation(WSD), is oftentoo fine-grained for many Natural Languageapplications because of its narrow sense dis-tinctions.
We present a semi-supervised ap-proach to learn similarity between WordNetsynsets using a graph based recursive sim-ilarity definition.
We seed our frameworkwith sense similarities of all the word-sensepairs, learnt using supervision on human-labelled sense clusterings.
Finally we discussour method to derive coarse sense invento-ries at arbitrary granularities and show that thecoarse-grained sense inventory obtained sig-nificantly boosts the disambiguation of nounson standard test sets.1 IntroductionWith different applications requiring different levelsof word sense granularity, producing sense clusteredinventories with the requisite level of sense granu-larity has become important.
The subtleties of sensedistinctions captured by WordNet(Miller, 1995) arehelpful for language learners (Snow et al 2007)and in machine translation of languages as diverseas Chinese and English (Ng et al 2003).
On theother hand, for tasks like Document Categorizationand Information Retrieval (Buitelaar, 2000), it maybe sufficient to know if a given word belongs to acoarsely defined class of WordNet senses.
Usingthe fine grained sense inventory of WordNet may bedetrimental to the performance of these applications.Thus developing a framework which can generatesense inventories with different granularities can im-prove the performance of many applications.To generate a coarse sense inventory, many re-searchers have focused on generating coarse sensesfor each word by merging the fine-grained senses(Chugur et al 2002) (Navigli, 2006).
This approachhas two problems.
First, it requires a stopping crite-rion for each word ?
for example the number offinal classes.
The right number of classes for eachword cannot usually be predetermined even if theapplication is known.
So such systems cannot beused to derive coarse senses for all the words.
Sec-ond, inconsistent sense clusters are obtained becausecoarse senses are independently generated for eachword.
This leads to transitive closure errors and sug-gests that for deriving consistent coarse senses, in-stead of clustering senses for each word separatelywe should cluster synsets.We propose a framework that derives a coarsesense inventory by learning a synset similarity met-ric.
We focus on coarsening the noun synsets ofWordNet and show that the obtained coarse-grainedsense inventory greatly improves the noun sensedisambiguation.
Our approach closely resembles(Snow et al 2007) for supervised learning of synsetsimilarity.
But to learn similarity between synsetpairs which do not share a word we use a variantof the SimRank framework (Jeh and Widom, 2002)and avoid giving them zero similarity.
Thus the sim-ilarity learnt is more than a binary decision and isreflective of a more comprehensive semantic simi-larity between the synsets.
The use of SimRank forlearning synset similarity is inspired by the successof graph-centrality algorithms in WSD.
We do notmodify the WordNet ontology, unlike (Snow et al2007), as it may introduce spurious relations and re-move some manually encoded information.11In section 2, we discuss past work in sense clus-tering.
In section 3 and 4, we describe our frame-work of learning synset similarity using SimRank.In section 5, we discuss our methodology of produc-ing coarse senses using the learnt similarity metric.Section 6 describes the experimental setup and eval-uates the framework described.
Section 7 containsconclusions and discusses the directions for futurework.2 Related WorkA wide variety of automatic methods have been pro-posed for coarsening fine-grained inventories.
Theearliest attempt on WordNet include (Mihalcea andMoldovan, 2001) which merged synsets on seman-tic principles like sharing a pertainym, antonym orverb group.
We discuss some of the ideas whichare related to our work.
Though promising, many ofthese techniques are severely limited by the amountof available manually annotated data.
(Chugur et al 2002) constructed sense similaritymatrices using translation equivalences in four lan-guages.
With the advent of WordNets being devel-oped in multiple languages1 as well as multilingualontologies like BabelNet (Navigli and Ponzetto,2012), this seems a promising area.
(McCarthy, 2006) estimated sense similarities us-ing a combination of word-to-word distributionalsimilarity combined with the JCN WordNet basedsimilarity measure (Jiang and Conrath, 1997).
Theyintroduce a more relaxed notion of sense relatednesswhich allows the user to control the granularity forthe application in hand.
(Navigli, 2006) produced a fixed set sense clustersby mapping WordNet word senses to Oxford En-glish Dictionary(OED) word senses exploiting sim-ilarities in glosses and semantic relationships in thesense inventories.
It is expected that the differentWordNet senses that are semantically close mappedto the same sense in the other ontology via an ef-ficient mapping that is able to capture the semanticsimilarity between the concepts in both the ontolo-1GlobalWordNet lists the WordNets available in the pub-lic domains: http://www.globalwordnet.org/gwa/wordnet table.html.gies.
The drawback of this method is the generationof inconsistent sense clusters.
(Snow et al 2007) presented a novel supervisedapproach in which they train a Support Vector Ma-chine(SVM) using features derived from WordNetand other lexical resources, whose predictions serveas a distance measure between synsets.
Assumingzero similarity between synset pairs with no com-mon words, they cluster synsets using average linkagglomerative clustering and the synset similaritymodel learnt.3 SimRankSimRank (Jeh and Widom, 2002) is a graph basedsimilarity measure applicable in any domain withobject-to-object relationships.
It uses the intuitionthat ?two objects are similar if they are related tosimilar objects?.
Since SimRank has a recursivestructure, the base cases play an important role.Let us denote the SimRank similarity between ob-jects ?
and ?
by s(?, ?).
It is defined as 1 if ?
= ?,otherwise it is given by:s(?, ?)
=C|I(?)||I(?)||I(?)|?i=1|I(?)|?j=1s(Ii(?
), Ij(?
))(1)where C ?
(0, 1) is a constant decay factor andI(v) is the set consisting of in-neighbours of node v,whose individual members are referred to as Ij(v),1 ?
j ?
|I(v)|.3.1 Solution and its Properties(Jeh and Widom, 2002) proved that a solution s(?, ?
)to the SimRank equations always exists and isunique.
For a graphG(V,E), the solution is reachedby iteration to a fixed-point.
For each iteration k, wekeep |V |2 entries Sk(?, ?
), where Sk(?, ?)
is the es-timate of similarity between ?
and ?
at the kth iter-ation.
We start with S0(?, ?)
which is 1 for single-ton nodes like (x, x), 0 otherwise.
We successivelycompute Sk+1(?, ?)
based on Sk(?, ?)
using equa-tion 1.Regarding the convergence of the above computa-tion process, (Lizorkin et al 2010) proved that thedifference between the SimRank theoretical scores12and iterative similarity scores decreases exponen-tially in the number of iterations and uniformly forevery pair of nodes i.e.s(?, ?)?
Sk(?, ?)
?
Ck+1 ?
?, ?
?
V ; k = 0, 1, 2 .
.
.
(2)3.2 Personalizing SimRankIn many scenarios we do not have complete informa-tion about the objects and thus have similarities foronly some pairs of objects.
These similarities maybe independently learnt and may not directly con-form with the underlying graph.
In such situations,we would like to get a more complete and consis-tent similarity metric between objects while simul-taneously using the existing information.
For thiswe propose a personalized framework for SimRankwhere we bias the SimRank by changing the initial-ization.
If we know similarities of some pairs, wefix them in our set of equations and let the rest of thevalues be automatically learnt by the system.Let us call the map of node pairs to their similarityvalues as InitStore.
It also contains all the single-ton nodes like (x, x) which have values equal to 1.For other node pairs, the system of equations is thesame as equation 1.
In the personalized framework,we have no constraints on the initialization as longas all values initialized are in the range [0, C].3.3 Learning Synset Similarity using SimRankThe Personalized SimRank framework requires anunderlying graph G(V,E), where V is the set ofobjects to be clustered and E is the set of seman-tic links connecting these objects and an InitStorecontaining the similarity values over some pairsfrom V ?
V learnt or known otherwise.
Note thatthe values in the InitStore have an upper bound ofC.For learning synset similarity, V is the set ofsynsets to be clustered and E is the set of Word-Net relations connecting these synsets.
We use theHypernymy, Hyponymy, Meronymy and Holonymyrelations of WordNet as the semantic links.
Themethod for seeding the InitStore is described insection 4 and can be summed up as follows:?
We train the SVMs from synset-merging datafrom OntoNotes (Hovy et al 2006) to pre-dict the similarity values of all the synset pairswhich share at least one word.?
We estimate the posterior probabilities from theSVM predictions by approximating the poste-rior by a sigmoid function, using the methoddiscussed in (Lin et al 2003).?
We scale the posterior probabilities obtained torange between [0, C] by linear scaling, whereC is the SimRank decay parameter.4 Seeding SimRank with supervision4.1 OutlineWe learn semantic similarity between differentsenses of a word using supervision, which allowsus to intelligently combine and weigh the differentfeatures and thus give us an insight into how hu-mans relate word senses.
We obtain pairs of synsetswhich human-annotators have labeled as ?merged?or ?not merged?
and describe each pair as a featurevector.
We learn a synset similarity measure by us-ing an SVM on this extracted dataset, where positiveexamples are the pairs which were merged and neg-ative examples are the ones which were not mergedby the annotators.
We then calculate the posteriorprobability using the classifier score which is usedas an estimate of the similarity between synsets con-stituting the pair.4.2 Gold standard sense clustering datasetSince our methodology depends upon the availabil-ity of labelled judgements of synset relatedness, adataset with a high Inter-Annotator agreement is re-quired.
We use the manually labelled mappingsfrom the Omega ontology2 (Philpot et al 2005)to the WordNet senses, provided by the OntoNotesproject (Hovy et al 2006).The OntoNotes dataset creation involved a rigor-ous iterative annotation process producing a coarsesense inventory which guarantees at least 90% Inter-Tagger agreement on the sense-tagging of the sam-ple sentences used in the annotation process.
Thuswe expect the quality of the final clustering of sensesand the derived labelled judgements to be reasonablyhigh.2http://omega.isi.edu/13We use OntoNotes Release 3.0 3 for extractingWordNet sense clusters.4.
The dataset consists ofsenses for selected words in sense files.
The sensesin OntoNotes are mapped to WordNet senses, if agood mapping between senses exists.
The steps in-volved in extraction are as follows:1.
OntoNotes has mappings to 4 WordNet ver-sions: 1.7, 2.0, 2.1 and 3.0.
We mapped allthe senses5 to WordNet 3.0.2.
Validating clusters on WN3.0:?
We removed the sense files which did notcontain all the senses of the word i.e.
theclustering was not complete.?
We removed the sense files in which theclusters had a clash i.e.
one sense be-longed to multiple clusters.3.
We removed instances that were present in bothpositive and negative examples.
This situa-tion arises because the annotators were work-ing with word senses and there were inconsis-tent sense clusters.Statistics Nouns Verbs# of Word Sense File Before Processing 2033 2156# of Word Sense Files After Processing 1680 1951Distinct Offsets encountered 4930 6296Positive Examples 1214 6881Negative Examples 11974 20899Percentage of Positive examples 9.20 24.76Table 1: Statistics of Pairwise Classification Dataset ob-tained from OntoNotes4.3 Feature EngineeringIn this section, we describe the feature space con-struction.
We derive features from the structure ofWordNet and other available lexical resources.
Ourfeatures can be broadly categorized into two parts:derived from WordNet and derived from other cor-pora.
Many of the listed features are motivated by(Snow et al 2007) and (Mihalcea and Moldovan,2001).3 http://www.ldc.upenn.edu/Catalog/docs/LDC2009T24/OntoNotes-Release-3.0.pdf4The OntoNotes groupings will be available through theLDC at http://www.ldc.upenn.edu5We dropped WN1.7 as there were very few senses and themapping from WN1.7 to WN3.0 was not easily available.4.3.1 Features derived from WordNetWordNet based features are further subdividedinto similarity measures and features.
Among theWordNet similarity measures, we used Path BasedSimilarity Measures: WUP (Wu and Palmer, 1994),LCH (Leacock et al 1998); Information ContentBased Measures: RES (Resnik, 1995), JCN (Jiangand Conrath, 1997), LIN (Lin, 1998); Gloss BasedHeuristics (variants of Lesk (Lesk, 1986)): AdaptedLesk (Banerjee and Pedersen, 2002), Adapted LeskTanimoto and Adapted Lesk Tanimoto without hy-ponyms6Other synset and sense based features includenumber of lemmas common in two synsets, SenseC-ount: maximum polysemy degree among the lem-mas shared by the synsets, SenseNum: number oflemmas having maximum polysemy degree amongthe lemmas shared by the synsets, whether twosynsets have the same lexicographer file, number ofcommon hypernyms, autohyponymy: whether thetwo synsets have a hyponym-hypernym relation be-tween them and merging heuristics by (Mihalceaand Moldovan, 2001).74.3.2 Features derived from External Corpora?
eXtended WordNet Domains Project (Gonza?lezet al 2012) provides us the score of a synsetwith respect to 169 hierarchically organizeddomain-labels(excluding factotum label).
Weobtain a representation of a synset in the do-main label space and use cosine similarity, L1distance and L2 distance computed over theweight representations of the synsets as fea-tures.?
BabelNet (Navigli and Ponzetto, 2012) pro-vides us with the translation of noun wordsenses in 6 languages namely: English, Ger-man, Spanish, Catalan, Italian and French andthe mapping of noun synsets to DBpedia8 en-tries.
For features we use counts of common6We call the lesk variants as AdapLesk, AdapLeskTani andAdapLeskTaniNoHypo.7We divide mergeSP1 2 into two features: The strict heuris-tic checks whether all the hypernyms are shared or not whereasthe relaxed heuristic checks if the synsets have at least 1 com-mon hypernym.8http://dbpedia.org/About14lemmas in all 6 languages and count of com-mon DBpedia entries.?
SentiWordNet (Baccianella et al 2010) pro-vides us with a mapping from a synset to a triadof three weights.
The weights correspond to thescore given to a synset based on its objectivityand subjectivity(positive and negative).
We usecosine similarity, L1 distance and L2 distanceof the weight representations of the synsets asfeatures.?
We use the sense clusterings produced by map-ping WordNet senses to OED senses by theorganizers of the coarse-grained AW task inSemEval-20079 (Navigli et al 2007).
For eachpair of synsets, we check if there are senses inthe synsets that belong to the same cluster inthe OED mapping.4.4 Classifier and TrainingWe train SVMs using the features above on thesynset pairs extracted from OntoNotes, where ev-ery synset pair is given either a ?merged?
or ?not-merged?
label.
Because of the skewed class distribu-tion in the dataset, we randomly generated balanceddatasets (equal number of positive and negative in-stances) and then divided them in a ratio of 7:3 fortraining and testing respectively.
We repeated theprocess multiple number of times and report the av-erage.To train the SVMs we used an implementation by(Joachims, 1998), whose java access is provided byJNI-SVMLight 10 library.
For all experiments re-ported, we use the linear kernel with the default pa-rameters provided by the library.
11We scale the ranges of all the features to a com-mon range [-1,1].
The main advantage offered byscaling is that it prevents domination of attributeswith smaller numeric ranges by those with greaternumeric ranges.
It also avoids numerical difficultieslike overflow errors caused by large attribute values.Note that both training and testing data should bescaled with the same parameters.9 http://lcl.uniroma1.it/coarse-grained-aw/10JNI-SVMLight: http://adrem.ua.ac.be/?tmartin/11We also tested our system with an RBF kernel but the bestresults were obtained with the linear kernel(Bhagwani, 2013)4.5 Estimating Posterior Probabilities fromSVM ScoresFor seeding SimRank, we need an estimate of theposterior probability Pr(y = +1|x) instead of theclass label.
(Platt, 1999) proposed approximatingthe posterior by a sigmoid functionPr(y = +1|x) ?
PA,B(f(x)) ?11+exp(Af(x)+B)We use the method described in (Lin et al 2003),as it avoids numerical difficulties faced by (Platt,1999).5 Coarsening WordNetWe construct an undirected graph G(V,E) wherethe vertex set V contains the synsets of WordNet andedge set E comprises of edges obtained by thresh-olding the similarity metric learnt using the person-alized SimRank model (see section 3.2).
On varyingthe threshold, we obtain different graphs which dif-fer in the number of edges.
On these graphs, we findconnected components12, which gives us a partitionover synsets.
All the senses of a word occurring inthe same component are grouped as a single coarsesense.
We call our approach Connected ComponentsClustering(CCC).For lower thresholds, we obtain denser graphsand thus fewer connected components.
This smallnumber of components translates into more coarsersenses.
Therefore, using this threshold as a param-eter of the system, we can control the granularity ofthe coarse senses produced.6 Experimental Setup and Evaluation6.1 Feature AnalysisWe analyze the feature space used for SVMs in twoways.
We evaluate Information Gain(IG) and GainRatio(GR) functions over the features and do a fea-ture ablation study.
The former tries to capture thediscrimination ability of the feature on its own andthe latter measures how a feature corroborates withother features in the feature space.12a connected component of an undirected graph is a sub-graph in which any two vertices are connected to each other bypaths, and which is connected to no additional vertices in thesupergraph.15We extracted all the features over the completeOntoNotes dataset without any normalization andevaluated them using IG and GR functions.
We re-port the top 7 features of both the evaluators in table213.Feature GR IGLCH 0.0129 0.0323WUP 0.0148 0.0290JCN 0.0215 0.0209AdapLesk 0.0169 0.0346AdapLeskTani 0.0231 0.0360AdapLeskTaniNoHypo 0.0168 0.0301mergeSP1 2 strict 0.0420 0.0010mergeSP1 2 relaxed 0.0471 0.0012number of Common Hypernyms 0.0883 0.0096Domain-Cosine Similarity 0.0200 0.0442OED 0.0326 0.0312Table 2: Information Gain and Gain Ratio Based Evalua-tionWe divide our features into 6 broad categories andreport the average F-Score of both the classes ob-served by removing that category of features fromour feature space.
The SVMs are trained with fea-tures normalized using MinMax Normalization forthis study.Features Removed FScore Pos FScore NegWordNet Similarity Measures 0.6948 0.6784WordNet Based Features 0.7227 0.7092BabelNet Features 0.7232 0.7127Domain Similarity Features 0.6814 0.6619OED Feature 0.6957 0.7212SentiWordNet Features 0.7262 0.7192Without Removing Features 0.7262 0.7192Table 3: Feature Ablation StudyFrom tables 2 and 3, we observe that the most sig-nificant contributors in SVM performance are Word-Net similarity measures and domain cosine similar-ity.
The former highlights the importance of the on-tology structure and the gloss definitions in Word-Net.
The latter stresses the fact that approximatelymatching the domain of two senses is a strong cueabout whether the two senses are semantically re-lated enough to be merged.13Table lists only 11 features as 3 features are common in top7 features of both the evaluatorsOther notable observations are the effectivenessof the OED feature and the low Information Gainand Gain Ratio of multilingual features.
Wealso found that SentiWordNet features were non-discriminatory as most of the noun synsets were de-scribed as objective concepts.6.2 Estimating Posterior Probabilities fromSVM ScoresWe learn parameters A and B of the sigmoid thattransforms SVM predictions to posterior probabili-ties (see section 4.5).
Since using the same data setthat was used to train the model we want to calibratewill introduce unwanted bias we calibrate on an in-dependently generated random balanced subset fromOntoNotes.The values of A and B obtained are -1.1655 and0.0222 respectively.
Using these values, the SVMprediction of value 0 gets mapped to 0.4944.6.3 Semi-Supervised Similarity LearningWe learn similarity models using the SimRank vari-ant described in section 3.
(Jeh and Widom, 2002)use C = 0.8 and find that 5-6 iterations are enough.
(Lizorkin et al 2010) suggest lower values of C ormore number of iterations.
We vary the values for Cbetween 0.6, 0.7 and 0.8 and we run all systems for10 iterations to avoid convergence issues.6.4 Coarsening WordNetWe assess the effect of automatic synset clusteringon the English all-words task at Senseval-3 (Snyderand Palmer, 2004) 14.
The task asked WSD systemsto select the apt sense for 2,041 content words inrunning texts comprising of 351 sentences.
Sincethe BabelNet project provided multilingual equiva-lences for only nouns, we focussed on nouns andused the 890 noun instances.We consider the three best performing WSD sys-tems: GAMBL (Decadt et al 2004), SenseLearner(Mihalcea and Faruque, 2004) and Koc University(Yuret, 2004) - and the best unsupervised system:IRST-DDD (Strapparava et al 2004) submitted inthe task.
The answer by the system is given full14This evaluation is similar to the evaluation used by (Nav-igli, 2006) and (Snow et al 2007)16C System F-Score Threshold CCC Random Improvement0.6GAMBL 0.7116 0.36 0.9031 0.8424 0.0607SenseLearner 0.7104 0.37 0.8824 0.8305 0.0518KOC University 0.7191 0.37 0.8924 0.8314 0.0610IRST-DDD 0.6367 0.35 0.8731 0.8013 0.07180.7GAMBL 0.7116 0.52 0.8453 0.7864 0.0589SenseLearner 0.7104 0.49 0.8541 0.8097 0.0444KOC University 0.7191 0.52 0.8448 0.7911 0.0538IRST-DDD 0.6367 0.49 0.7970 0.7402 0.05680.8GAMBL 0.7116 0.59 0.8419 0.7843 0.0577SenseLearner 0.7104 0.56 0.8439 0.7984 0.0455KOC University 0.7191 0.59 0.8414 0.7879 0.0535IRST-DDD 0.6367 0.47 0.8881 0.8324 0.0557Table 4: Improvement in Senseval-3 WSD performance using Connected Component Clustering Vs Random Cluster-ing at the same granularitycredit if it belongs to the cluster of the correct an-swer.Observe that any clustering will only improve theWSD performance.
Therefore to assess the improve-ment obtained because of our clustering, we calcu-late the expected F-Score, the harmonic mean of ex-pected precision and expected recall, for a randomclustering at the same granularity and study the im-provement over the random clustering.Let the word to be disambiguated have N senses,each mapped to a unique synset.
Let the clusteringof these N synsets on a particular granularity giveus k clusters C1, .
.
.
Ck.
The expectation that an in-correctly chosen sense and the actual correct sensewould belong to same cluster is?ki=1|Ci|(|Ci|?1)N(N ?
1)(3)We experiment with C = 0.6, 0.7 and 0.8.
TheSVM probability boundaries when scaled to [0, C]for these values are 0.30, 0.35 and 0.40.
To find thethreshold giving the best improvement against therandom clustering baseline, we use the search space[C ?
0.35, C].
The performance of the systems atthese thresholds for different values of C is reportedin table 4.Commenting theoretically about the impact of Con the performance is tough as by changing C weare changing all the |V |2 simultaneous equations tobe solved.
Empirically, we observe that across allsystems improvements over the baseline keep de-creasing as C increases.
This might be due to theslow convergence of SimRank for higher values ofC.Figure 1 shows that by varying thresholds the im-provement of the Connected Components Cluster-ing over the random clustering baseline at the samegranularity first increases and then decreases.
Thisbehaviour is shared by both supervised and unsuper-vised systems.
Similar figures are obtained for othervalues of C (0.7 and 0.8), but are omitted because oflack of space.Across supervised and unsupervised systems, weobserve higher improvements for unsupervised sys-tems.
This could be because the unsupervised sys-tem was underperforming compared to the super-vised systems in the fine grained WSD task setting.7 Conclusions and Future WorkWe presented a model for learning synset similarityutilizing the taxonomy information and informationlearnt from manually obtained sense clustering.
Theframework obtained is generic and can be applied toother parts of speech as well.
For coarsening senses,we used one of the simplest approaches to clustersenses but the generic nature of the similarity givesus the flexibility to use other clustering algorithms170.60.650.70.750.80.850.90.9510.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65FScoreThresholdConnected Components ClusteringRandom Clustering(a)0.60.650.70.750.80.850.90.9510.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75FScoreThresholdConnected Components ClusteringRandom Clustering(b)Figure 1: Improvement in (a) average performance of best 3 Supervised Systems and (b) performance of best Unuper-vised System in Senseval-3 using Connected Component Clustering Vs Random Clustering at the same granularitywith C = 0.6for experimentation.
We show that the clustering ob-tained by partitioning synsets in connected compo-nents gives us a maximum improvement of 5.78%on supervised systems and 7.18% on an unsuper-vised system.
This encourages us to study graphbased similarity learning methods further as they al-low us to employ available wide-coverage knowl-edge bases.We use the WordNet relations Hypernymy, Hy-ponymy, Meronymy and Holonymy without any dif-ferentiation.
If we can grade the weights of the rela-tions based on their relative importance we can ex-pect an improvement in the system.
These weightscan be obtained by annotator feedback from cogni-tive experiments or in a task based setting.
In ad-dition to the basic WordNet relations, we can alsoenrich our relation set using the Princeton WordNetGloss Corpus15, in which all the WordNet glosseshave been sense disambiguated.
Any synset occur-ing in the gloss of a synset is directly related to thatsynset via the gloss relation.
This relation helpsmake the WordNet graph denser and richer by cap-turing the notion of semantic relatedness, rather thanjust the notion of semantic similarity captured by thebasic WordNet relations.15http://wordnet.princeton.edu/glosstag.shtmlAcknowledgmentsThe authors would like to thank the anonymous re-viewers for their valuable comments and sugges-tions to improve the quality of the paper.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InProceedings of LREC.Satanjeev Banerjee and Ted Pedersen.
2002.
An adaptedlesk algorithm for word sense disambiguation usingwordnet.
In Proceedings of CICLing 2002.Sumit Bhagwani.
2013.
Merging word senses.
Master?sthesis, Indian Institute of Technology Kanpur.Paul Buitelaar.
2000.
Reducing lexical semantic com-plexity with systematic polysemous classes and un-derspecification.
In NAACL-ANLP 2000 Workshop:Syntactic and Semantic Complexity in Natural Lan-guage Processing Systems, pages 14?19.
Associationfor Computational Linguistics.Irina Chugur, Julio Gonzalo, and Felisa Verdejo.
2002.Polysemy and sense proximity in the senseval-2 testsuite.
In Proceedings of the ACL 2002 WSD workshop.Bart Decadt, Ve?ronique Hoste, Walter Daelemans, andAntal Van den Bosch.
2004.
Gambl, genetic algo-rithm optimization of memory-based wsd.
In Proceed-ings of ACL/SIGLEX Senseval-3.18Aitor Gonza?lez, German Rigau, and Mauro Castillo.2012.
A graph-based method to improve wordnet do-mains.
In Proceedings of CICLing 2012.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Proceedings of HLT-NAACL2006.Glen Jeh and Jennifer Widom.
2002.
Simrank: A mea-sure of structural-context similarity.
In KDD, pages538?543.Jay J. Jiang and David W. Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical taxonomy.In Proceedings of ROCLING?97.Thorsten Joachims.
1998.
Making large-scale supportvector machine learning practical.Claudia Leacock, George A. Miller, and MartinChodorow.
1998.
Using corpus statistics and wordnetrelations for sense identification.
Comput.
Linguist.,24(1):147?165, March.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In Proceedings of SIG-DOC 1986.Hsuan-tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2003.A note on platt?s probabilistic outputs for support vec-tor machines.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of ICML 1998.Dmitry Lizorkin, Pavel Velikhov, Maxim Grinev, and De-nis Turdakov.
2010.
Accuracy estimate and optimiza-tion techniques for simrank computation.
The VLDBJournal, 19(1):45?66, February.Diana McCarthy.
2006.
Relating wordnet senses forword sense disambiguation.
Making Sense of Sense:Bringing Psycholinguistics and Computational Lin-guistics Together, 17.Rada Mihalcea and Ehsanul Faruque.
2004.
Sense-learner: Minimally supervised word sense disam-biguation for all words in open text.
In Proceedingsof ACL/SIGLEX Senseval-3.Rada Mihalcea and Dan Moldovan.
2001.
Ez.wordnet:principles for automatic generation of a coarse grainedwordnet.
In Proceedings of Flairs 2001, pages 454?459.George A Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Roberto Navigli and Simone Paolo Ponzetto.
2012.
Ba-belNet: The automatic construction, evaluation andapplication of a wide-coverage multilingual semanticnetwork.
Artificial Intelligence, 193:217?250.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
Semeval-2007 task 07: Coarse-grainedenglish all-words task.
In Proceedings of SemEval-2007, pages 30?35.
Association for ComputationalLinguistics, June.Roberto Navigli.
2006.
Meaningful clustering of senseshelps boost word sense disambiguation performance.In Proceedings of COLING-ACL, pages 105?112.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.
Ex-ploiting parallel texts for word sense disambiguation:an empirical study.
In Proceedings of ACL 2003.Andrew Philpot, Eduard Hovy, and Patrick Pantel.
2005.The omega ontology.
In Proceedings of the ONTOLEXWorkshop at IJCNLP 2005.John C. Platt.
1999.
Probabilistic outputs for supportvector machines and comparisons to regularized like-lihood methods.
In ADVANCES IN LARGE MARGINCLASSIFIERS, pages 61?74.
MIT Press.Philip Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proceedingsof IJCAI 1995.Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-drew Y. Ng.
2007.
Learning to Merge Word Senses.In Proceedings of EMNLP-CoNLL, pages 1005?1014,June.Benjamin Snyder and Martha Palmer.
2004.
The en-glish all-words task.
In Proceedings of ACL/SIGLEXSenseval-3, pages 41?43.Carlo Strapparava, Alfio Gliozzo, and Claudiu Giuliano.2004.
Pattern abstraction and term similarity for wordsense disambiguation: Irst at senseval-3.
In Proceed-ings of ACL/SIGLEX Senseval-3.Zhibiao Wu and Martha Palmer.
1994.
Verbs semanticsand lexical selection.
In Proceedings of ACL 1994.Deniz Yuret.
2004.
Some experiments with a naivebayes wsd system.
In Proceedings of ACL/SIGLEXSenseval-3.19
