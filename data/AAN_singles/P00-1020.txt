An Empirical Study of the Influence of Argument Conciseness onArgument EffectivenessGiuseppe CareniniIntelligent Systems ProgramUniversity of Pittsburgh,Pittsburgh, PA 15260, USAcarenini@cs.pitt.eduJohanna D. MooreThe Human Communication Research Centre,University of Edinburgh,2 Buccleuch Place, Edinburgh EH8 9LW, UK.jmoore@cogsci.ed.ac.ukAbstractWe have developed a system that generatesevaluative arguments that are tailored to theuser, properly arranged and concise.
We havealso developed an evaluation framework inwhich the effectiveness of evaluative argumentscan be measured with real users.
This paperpresents the results of a formal experiment wehave performed in our framework to verify theinfluence of argument conciseness on argumenteffectiveness1 IntroductionEmpirical methods are critical to gauge thescalability and robustness of proposedapproaches, to assess progress and to stimulatenew research questions.
In the field of naturallanguage generation, empirical evaluation hasonly recently become a top research priority(Dale, Eugenio et al 1998).
Some empiricalwork has been done to evaluate models forgenerating descriptions of objects and processesfrom a knowledge base (Lester and Porter March1997), text summaries of quantitative data(Robin and McKeown 1996), descriptions ofplans (Young to appear) and concise causalarguments (McConachy, Korb et al 1998).However, little attention has been paid to theevaluation of systems generating evaluativearguments, communicative acts that attempt toaffect the addressee?s attitudes (i.e.
evaluativetendencies typically phrased in terms of like anddislike or favor and disfavor).The ability to generate evaluative arguments iscritical in an increasing number of onlinesystems that serve as personal assistants,advisors, or shopping assistants1.
For instance, ashopping assistant may need to compare twosimilar products and argue why its current usershould like one more than the other.1See for instance www.activebuyersguide.comIn the remainder of the paper, we first describe acomputational framework for generatingevaluative arguments at different levels ofconciseness.
Then, we present an evaluationframework in which the effectiveness ofevaluative arguments can be measured with realusers.
Next, we describe the design of anexperiment we ran within the framework toverify the influence of argument conciseness onargument effectiveness.
We conclude with adiscussion of the experiment?s results.2 Generating concise evaluativeargumentsOften an argument cannot mention all theavailable evidence, usually for the sake ofbrevity.
According to argumentation theory, theselection of what evidence to mention in anargument should be based on a measure of theevidence strength of support (or opposition) tothe main claim of the argument (Mayberry andGolden 1996).
Furthermore, argumentationtheory suggests that for evaluative arguments themeasure of evidence strength should be based ona model of the intended reader?s values andpreferences.Following argumentation theory, we havedesigned an argumentative strategy forgenerating evaluative arguments that areproperly arranged and concise (Carenini andMoore 2000).
In our strategy, we assume thatthe reader?s values and preferences arerepresented as an additive multiattribute valuefunction (AMVF), a conceptualization based onmultiattribute utility theory (MAUT)(Clemen1996).
This allows us to adopt and extend ameasure of evidence strength proposed inprevious work on explaining decision theoreticadvice based on an AMVF (Klein1994).Figure 1 Sample additive multiattribute value function (AMVF)The argumentation strategy has beenimplemented as part of a complete argumentgenerator.
Other modules of the generatorinclude a microplanner, which performsaggregation, pronominalization and makesdecisions about cue phrases and scalaradjectives, along with a sentence realizer, whichextends previous work on realizing evaluativestatements (Elhadad 1995).2.1 Background on AMVFAn AMVF is a model of a person?s values andpreferences with respect to entities in a certainclass.
It comprises a value tree and a set ofcomponent value functions, one for eachprimitive attribute of the entity.
A value tree is adecomposition of the value of an entity into ahierarchy of aspects of the entity2, in which theleaves correspond to the entity primitiveattributes (see Figure 1 for a simple value tree inthe real estate domain).
The arcs of the tree areweighted to represent the importance of thevalue of an objective in contributing to the valueof its parent in the tree (e.g., in Figure 1 locationis more than twice as important as size indetermining the value of a house).
Note that thesum of the weights at each level is equal to 1.
Acomponent value function for an attributeexpresses the preferability of each attributevalue as a number in the [0,1] interval.
Forinstance, in Figure 1 neighborhood n2 haspreferability 0.3, and a distance-from-park of 1mile has preferability (1 - (1/5 * 1))=0.8).2In decision theory these aspects are calledobjectives.
For consistency with previous work, wewill follow this terminology in the remainder of thepaper.Formally, an AMVF predicts the value )(ev ofan entity e as follows:v(e) = v(x1,?,xn) = ?wi vi(xi), where- (x1,?,xn) is the vector of attribute values for anentity e- ?attribute i, vi is the component value function,which maps the least preferable xi to 0, the mostpreferable to 1, and the other xi to values in [0,1]- wi is the weight for attribute i, with 0?
wi ?1and ?wi =1- wi is equal to the product of all the weightsfrom the root of the value tree to the attribute iA function vo(e) can also be defined for eachobjective.
When applied to an entity, thisfunction returns the value of the entity withrespect to that objective.
For instance, assumingthe value tree shown in Figure 1, we have:))(6.0())(4.0()(evevevparkfromDistodNeighborhoLocation??
?+?==Thus, given someone?s AMVF, it is possible tocompute how valuable an entity is to thatindividual.
Furthermore, it is possible tocompute how valuable any objective (i.e., anyaspect of that entity) is for that person.
All ofthese values are expressed as a number in theinterval [0,1].2.2 A measure of evidence strengthGiven an AMVF for a user applied to an entity(e.g., a house), it is possible to define a precisemeasure of an objective strength in determiningthe evaluation of its parent objective for thatentity.
This measure is proportional to twofactors: (A) the weight of the objective?
+??
?k =1k = -1k = 0compellingnessFigure 2 Sample population of objectivesrepresented by dots and ordered by theircompellingness(which is by itself a measure of importance), (B)a factor that increases equally for high and lowvalues of the objective, because an objective canbe important either because it is liked a lot orbecause it is disliked a lot.
We call this measures-compellingness and provide the followingdefinition:s-compellingness(o, e, refo) = (A)?
(B) == w(o,refo)?
max[[vo(e)]; [1 ?
vo(e)]],  where?
o is an objective, e is an entity, refo is anancestor of o in the value tree?
w(o,refo) is the product of the weights of allthe links from o to refo?
vo is the component value function for leafobjectives (i.e., attributes), and it is therecursive evaluation over children(o) fornonleaf objectivesGiven a measure of an objective's strength, apredicate indicating whether an objective shouldbe included in an argument (i.e., worthmentioning) can be defined as follows:s-notably-compelling?
(o,opop,e, refo) ?
?s-compellingness(o, e, refo)?>?x+k?x , where?
o, e, and refo are defined as in the previousDef; opop is an objective population (e.g.,siblings(o)), and ?opop?>2?
p?
opop; x?X = ?s-compellingness(p, e,refo)??
?x is the mean of X, ?x  is the standarddeviation and k is a user-defined constantSimilar measures for the comparison of twoentities are defined and extensively discussed in(Klein 1994).2.3 The constant kIn the definition of s-notably-compelling?, theconstant k determines the lower bound of s-compellingness for an objective to be includedin an argument.
As shown in Figure 2, for k=0only objectives with s-compellingness greaterFigure 3 Arguments about the same house,tailored to the same subject but with k rangingfrom 1 to ?1than the average s-compellingness in apopulation are included in the argument (4 in thesample population).
For higher positive valuesof k less objectives are included (only 2, whenk=1), and the opposite happens for negativevalues (8 objectives are included, when k=-1).Therefore, by setting the constant k to differentvalues, it is possible to control in a principledway how many objectives (i.e., pieces ofevidence) are included in an argument, thuscontrolling the degree of conciseness of thegenerated arguments.Figure 3 clearly illustrates this point by showingseven arguments generated by our argumentgenerator in the real-estate domain.
Thesearguments are about the same house, tailored tothe same subject, for k ranging from 1 to ?1.3 The evaluation frameworkIn order to evaluate different aspects of theargument generator, we have developed anevaluation framework based on the task efficacyevaluation method.
This method allowsFigure 4 The evaluation framework architecturethe experimenter to evaluate a generation modelby measuring the effects of its output on user?sbehaviors, beliefs and attitudes in the context ofa task.Aiming at general results, we chose a ratherbasic and frequent task that has been extensivelystudied in decision analysis: the selection of asubset of preferred objects (e.g., houses) out of aset of possible alternatives.
In the evaluationframework that we have developed, the userperforms this task by using a computerenvironment (shown in Figure 5) that supportsinteractive data exploration and analysis (IDEA)(Roth, Chuah et al 1997).
The IDEAenvironment provides the user with a set ofpowerful visualization and direct manipulationtechniques that facilitate the user?s autonomousexploration of the set of alternatives and theselection of the preferred alternatives.Let?s examine now how an argument generatorcan be evaluated in the context of the selectiontask, by going through the architecture of theevaluation framework.3.1 The evaluation framework architectureFigure 4 shows the architecture of the evaluationframework.
The framework consists of threemain sub-systems: the IDEA system, a UserModel Refiner and the Argument Generator.
Theframework assumes that a model of the user?spreferences (an AMVF) has been previouslyacquired from the user, to assure a reliable initialmodel.At the onset, the user is assigned the task toselect from the dataset the four most preferredalternatives and to place them in a Hot List (seeFigure 5, upper right corner) ordered bypreference.
The IDEA system supports the userin this task (Figure 4 (1)).
As the interactionunfolds, all user actions are monitored andcollected in the User?s Action History (Figure 4(2a)).
Whenever the user feels that the task isaccomplished, the ordered list of preferredalternatives is saved as her Preliminary Decision(Figure 4 (2b)).
After that, this list, the User?sAction History and the initial Model of User?sPreferences are analysed by the User ModelRefiner (Figure 4 (3)) to produce a RefinedModel of the User?s Preferences (Figure 4 (4)).At this point, the stage is set for argumentgeneration.
Given the Refined Model of theUser?s Preferences, the Argument Generatorproduces an evaluative argument tailored to themodel (Figure 4 (5-6)), which is presented to theuser by the IDEA system (Figure 4 (7)).Theargument goal is to introduce a new alternative(not included in the dataset initially presented tothe user) and to persuade the user that thealternative is worth being considered.
The newalternative is designed on the fly to be preferablefor the user given her preference model.3-26HotListNewHouse 3-26Figure 5 The IDEA environment display at the end of the interactionAll the information about the new alternative isalso presented graphically.
Once the argument ispresented, the user may (a) decide immediatelyto introduce the new alternative in her Hot List,or (b) decide to further explore the dataset,possibly making changes to the Hot List addingthe new instance to the Hot List, or (c) donothing.
Figure 5 shows the display at the end ofthe interaction, when the user, after reading theargument, has decided to introduce the newalternative in the Hot List first position (Figure5, top right).Whenever the user decides to stop exploring andis satisfied with her final selections, measuresrelated to argument?s effectiveness can beassessed (Figure 4  (8)).
These measures areobtained either from the record of the userinteraction with the system or from user self-reports in a final questionnaire (see Figure 6 foran example of self-report) and include:- Measures of behavioral intentions and attitudechange: (a) whether or not the user adopts thenew proposed alternative, (b) in which positionin the Hot List she places it and (c) how muchshe likes the new alternative and the otherobjects in the Hot List.- A measure of the user?s confidence that she hasselected the best for her in the set of alternatives.- A measure of argument effectiveness derivedby explicitly questioning the user at the end ofthe interaction about the rationale for herdecision (Olso and Zanna 1991).
This canprovide valuable information on what aspects ofthe argument were more influential (i.e., betterunderstood and accepted by the user).- An additional measure of argumenteffectiveness is to explicitly ask the user at theend of the interaction to judge the argument withrespect to several dimensions of quality, such ascontent, organization, writing style andconvincigness.
However, evaluations based onFigure 6 Self -report on user?s satisfaction withhouses in the HotListFigure 7 Hypotheses on experiment outcomesjudgements along these dimensions are clearlyweaker than evaluations measuring actualbehavioural and attitudinal changes (Olso andZanna 1991).To summarize, the evaluation framework justdescribed supports users in performing arealistic task at their own pace by interactingwith an IDEA system.
In the context of this task,an evaluative argument is generated andmeasurements related to its effectiveness can beperformed.We now discuss an experiment that we haveperformed within the evaluation framework4 The ExperimentThe argument generator has been designed tofacilitate testing the effectiveness of differentaspects of the generation process.
Theexperimenter can easily control whether thegenerator tailors the argument to the currentuser, the degree of conciseness of the argument(by varying k as explained in Section 2.3), andwhat microplanning tasks the generatorperforms.
In the experiment described here, wefocused on studying the influence of argumentconciseness on argument effectiveness.
Aparallel experiment about the influence oftailoring is described elsewhere.We followed a between-subjects design withthree experimental conditions:No-Argument - subjects are simply informed thata new house came on the market.Tailored-Concise - subjects are presented withan evaluation of the new house tailored to theirpreferences and at a level of conciseness that wehypothesize to be optimal.
To start ourinvestigation, we assume that an effectiveargument (in our domain) should containslightly more than half of the available evidence.By running the generator with different valuesfor k on the user models of the pilot subjects, wefound that this corresponds to k=-0.3.
In fact,with k=-0.3 the arguments contained on average10 pieces of evidence out of the 19 available.Tailored-Verbose - subjects are presented withan evaluation of the new house tailored to theirpreferences, but at a level of conciseness that wehypothesize to be too low (k=-1, whichcorresponds on average, in our analysis of thepilot subjects, to 16 pieces of evidence out of thepossible 19).In the three conditions, all the information aboutthe new house is also presented graphically, sothat no information is hidden from the subject.Our hypotheses on the outcomes of theexperiment are summarized in Figure 7.
Weexpect arguments generated for the Tailored-Concise condition to be more effective thanarguments generated for the Tailored-Verbosecondition.
We also expect the Tailored-Concisecondition to be somewhat better than the No-Argument condition, but to a lesser extent,because subjects, in the absence of anyargument, may spend more time furtherexploring the dataset, thus reaching a moreinformed and balanced decision.
Finally, we donot have strong hypotheses on comparisons ofargument effectiveness between the No-Argument and Tailored-Verbose conditions.The experiment is organized in two phases.
Inthe first phase, the subject fills out aquestionnaire on the Web.
The questionnaireimplements a method form decision theory toacquire an AMVF model of the subject?spreferences (Edwards and Barron 1994).
In thesecond phase of the experiment, to control forpossible confounding variables (includingsubject?s argumentativeness (Infante and Rancer1982), need for cognition (Cacioppo, Petty et al1983), intelligence and self-esteem), the subjectTailoredConciseTailoredVerboseNo-Argument>>> ?a) How would you judge the houses in your Hot List?The more you like the house the closer you shouldput a cross to ?good choice?1st housebad choice  : __:__:__:__ :__:__:__:__:__: good choice2nd housebad choice  : __:__:__:__ :__:__:__:__:__: good choice3rd housebad choice  : __:__:__:__ :__:__:__:__:__: good choice4th housebad choice  : __:__:__:__ :__:__:__:__:__: good choiceFigure 8 Sample filled-out self-report on user?ssatisfaction with houses in the Hot List3is randomly assigned to one of the threeconditions.Then, the subject interacts with the evaluationframework and at the end of the interactionmeasures of the argument effectiveness arecollected, as described in Section 3.1.After running the experiment with 8 pilotsubjects to refine and improve the experimentalprocedure, we ran a formal experiment involving30 subjects, 10 in each experimental condition.5 Experiment Results5.1 A precise measure of satisfactionAccording to literature on persuasion, the mostimportant measures of arguments effectivenessare the ones of behavioral intentions and attitudechange.
As explained in Section 3.1, in ourframework such measures include (a) whether ornot the user adopts the new proposed alternative,(b) in which position in the Hot List she placesit, (c) how much she likes the proposed newalternative and  the other objects in the Hot List.Measures (a) and (b) are obtained from therecord of the user interaction with the system,whereas measures in (c) are obtained from userself-reports.A closer analysis of the above measuresindicates that the measures in (c) are simply amore precise version of measures (a) and (b).
Infact, not only they assess the same informationas measures (a) and (b), namely a preferenceranking among the new alternative and theobjects in the Hot List, but they also offer twoadditional critical advantages:3If the subject does not adopt the new house, she isasked to express her satisfaction with the new housein an additional self-report.
(i) Self-reports allow a subject to expressdifferences in satisfaction more precisely thanby ranking.
For instance, in the self-reportshown in Figure 8, the subject was able tospecify that the first house in the Hot List wasonly one space (unit of satisfaction) better thenthe house preceding it in the ranking, while thethird house was two spaces better than the housepreceding it.
(ii) Self-reports do not force subjects to expressa total order between the houses.
For instance, inFigure 8 the subject was allowed to express thatthe second and the third house in the Hot Listwere equally good for her.Furthermore, measures of satisfaction obtainedthrough self-reports can be combined in a single,statistically sound measure that conciselyexpress how much the subject liked the newhouse with respect to the other houses in the HotList.
This measure is the z-score of the subject?sself-reported satisfaction with the new house,with respect to the self-reported satisfaction withthe houses in the Hot List.
A z-score is anormalized distance in standard deviation unitsof a measure xi from the mean of a population X.Formally:xi?
X; z-score( xi ,X) = [xi - ?
(X)] / ?
(X)For instance, the satisfaction z-score for the newinstance, given the sample self-reports shown inFigure 8, would be:[7 - ?
({8,7,7,5})] /  ?
({8,7,7,5}) = 0.2The satisfaction z-score precisely and conciselyintegrates all the measures of behavioralintentions and attitude change.
We have usedsatisfaction z-scores as our primary measure ofargument effectiveness.5.2 ResultsAs shown in Figure 9, the satisfaction z-scoresobtained in the experiment confirmed ourhypotheses.
Arguments generated for theTailored-Concise condition were significantlymore effective than arguments generated forTailored-Verbose condition.
The Tailored-Concise condition was also significantly betterthan the No-Argument condition, but to a lesserextent.
Logs of the interactions suggest that thishappened because subjects in the No-Argumentcondition spent significantly more time furtherexploring the dataset.
Finally, there was nosignificant difference in argument effectivenessa)How would you judge the houses in your Hot List?The more you like the house the closer you shouldput a cross to ?good choice?1st housebad choice  : __:__:__:__ :__:__:__:X :__: good choice2nd house(New house)bad choice  : __:__:__:__ :__:__:X :__:__: good choice3rd housebad choice  : __:__:__:__ :__:__:X :__:__: good choice4th housebad choice  : __:__:__:__ :X :__:__:__:__: good choiceFigure 9Results for satisfaction z-scores.
Theaverage z-scores for the three conditions areshown in the grey boxes and the p-values arereported beside the linksbetween the No-Argument and Tailored-Verbose conditions.With respect to the other measures of argumenteffectiveness mentioned in Section 3.1, we havenot found any significant differences among theexperimental conditions.6 Conclusions and Future WorkArgumentation theory indicates that effectivearguments should be concise, presenting onlypertinent and cogent information.
However,argumentation theory does not tell us what is themost effective degree of conciseness.
As apreliminary attempt to answer this question forevaluative arguments, we have compared in aformal experiment the effectiveness ofarguments generated by our argument generatorat two different levels of conciseness.
Theexperiment results show that argumentsgenerated at the more concise level aresignificantly better than arguments generated atthe more verbose level.
However, furtherexperiments are needed to determine what is theoptimal level of conciseness.AcknowledgementsOur thanks go to the members of the Autobriefproject: S. Roth, N. Green, S. Kerpedjiev and J.Mattis.
We also thank C. Conati for commentson drafts of this paper.
This work was supportedby grant number DAA-1593K0005 from theAdvanced Research Projects Agency (ARPA).ReferencesCacioppo, J. T., R. E. Petty, et al (1983).
?Effects ofNeed for Cognition on Message Evaluation, Recall,and Persuasion.?
Journal of Personality and SocialPsychology 45(4): 805-818.Carenini, G. and J. Moore (2000).
A Strategy forGenerating Evaluative Arguments.
InternationalConference on Natural Language Generation,Mitzpe Ramon, Israel.Clemen, R. T. (1996).
Making Hard Decisions: anintroduction to decision analysis.
Belmont,California, Duxbury Press.Dale, R., B. d. Eugenio, et al (1998).
?Introduction tothe Special Issue on Natural LanguageGeneration.?
Computational Linguistics 24(3):345-353.Edwards, W. and F. H. Barron (1994).
?SMARTSand SMARTER: Improved Simple Methods forMulti-attribute Utility Measurements.
?Organizational Behavior and Human DecisionProcesses 60: 306-325.Elhadad, M. (1995).
?Using argumentation in textgeneration.?
Journal of Pragmatics 24: 189-220.Infante, D. A. and A. S. Rancer (1982).
?AConceptualization and Measure ofArgumentativeness.?
Journal of PersonalityAssessment 46: 72-80.Klein, D. (1994).
Decision Analytic IntelligentSystems: Automated Explanation and KnowledgeAcquisition, Lawrence Erlbaum Associates.Lester, J. C. and B. W. Porter (March 1997).
?Developing and Empirically Evaluating RobustExplanation Generators: The KNIGHTExperiments.?
Computational Linguistics 23(1):65-101.Mayberry, K. J. and R. E. Golden (1996).
ForArgument's Sake: A Guide to Writing EffectiveArguments, Harper Collins, College Publisher.McConachy, R., K. B. Korb, et al (1998).
DecidingWhat Not to Say: An Attentional-ProbabilisticApproach to Argument Presentation.
CognitiveScience Conference.Olso, J. M. and M. P. Zanna (1991).
Attitudes andbeliefs ; Attitude change and attitude-behaviorconsistency.
Social Psychology.
R. M. Baron andW.
G. Graziano.Robin, J. and K. McKeown (1996).
?EmpiricallyDesigning and Evaluating a New Revision-BasedModel for Summary Generation.?
ArtificialIntelligence journal 85: 135-179.Roth, S. F., M. C. Chuah, et al (1997).
Towards anInformation Visualization Workspace: CombiningMultiple Means of   Expression.
Human-ComputerInteraction Journal.Young, M. R. ?Using Grice's Maxim of Quantity toSelect the Content of Plan Descriptions.?
ArtificialIntelligence Journal, to appear.TailoredConciseTailoredVerboseNo-Argument>?0.880.050.250.02>>0.030.31
