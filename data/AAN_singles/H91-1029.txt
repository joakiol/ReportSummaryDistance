COLLECTION AND ANALYSIS OF DATA FROM REAL USERS:IMPLICATIONS FOR SPEECH RECOGNITION/UNDERSTANDINGSYSTEMSJudith Spitz and the Artificial Intelligence Speech Technology GroupNYNEX Science and Technology, Research and Development500 Westchester AvenueWhite Plains, New York 10604ABSTRACTPerformance estimates given for speech recognition/understandingsystems are typically based on the assumption that users will behave inways similar to the observed behavior of laboratory volunteers.
Thisincludes the acoustic/phonetic characteristics of the spcech they produce aswell as their willingness and ability to constrain their input o the deviceaccording to instructions.
Since speech recognition devices often do notperform as well in the field as they do in the laboratory, analyses of realuser behavior have been undertaken.
The results of several field trialssuggest that real user compliance with instructions is dramatically affectedby the particular details of the prompts supplied to the user.
A significantamount of real user speech data has been collected uring these trials(34,000 uuerances, 29hours of data).
These speech databases are describedalong with the results of an experiment comparing the performance of aspeech recognition system on real user vs. laboratory speech.INTRODUCTIONSpeech recognition/understanding systems will ultimatelyestablish their usefulness by working well under eal applicationconditions.
Success in the field will depend not only on thetechnology itself but also on the behavior of real users.
Real userbehavior can be characterized interms of 1. what people say and 2.how they say it.What people say: Real user complianceUntil the advent of a high performance continuous speech,unconstrained vocabulary/grammar, interactive speechunderstanding system, users must constrain their spokeninteractions with speech recognition/understanding systems.Constraints may require speaking words in isolation, conformingto a limited vocabulary or grammar, restricting queries to aparticular knowledge domain, etc.
The users' willingness tocomply with instructions pecifying these constraints willdetermine the success of the technology.
If users are willing oreven able to confine themselves toone of two words (e.g., yes orno), a two-word speech recognition system may succeed.
If usersare non-compliant (e.g., say the target words embedded inphrases,say synonyms of the target words, reject he service as a result ofthe constraining instructions), the technology will fail in the field;despite high accuracy laboratory performance.How compliant are real users?
The answer may be application-specific, dependent on particulars such as 1. frequency of repeatusage of the system, 2. motivation of the users, 3. cost of an error,4.
nature of the constraint, etc.
It would be useful to understand thefactors that predict compliance, and to know whethergeneralizations can be made across applications.
In addition, itwould be useful to have a better understanding of how to maximizeuser compliance.Moreover, there is value in analyzing non-compliant behavior.To the extent hat non-compliance takes the form of choosingsynonyms of the target words, the recognizer's vocabulary must beexpanded.
If non-compliance takes the form of embedding the targetword in a phrase, word spotting or continuous speech recognitionis required.
If non-compliance is manifested by the userconsistently wandering outside the knowledge domain of the speechrecognition/understanding system, better instructions may berequired.
Data from real users should provide researchers anddevelopers with the information ecessary to both specify anddevelop the technology required for successful deployment ofspeech recognition/understanding systems.How people speak: Real user speechIt seems intuitively obvious that to maximize the probabilityof successfully automating an application with speech recognition,a recognizer should be trained and tested on real user speech.
Thisrequires the collection of data from casual users interacting with anautomated or pseudo-automated system, thereby producingspontaneous goal-directed speech under application conditions.These databases can be difficult and expensive tocollect and so it isnot surprising that speech recognition systems are most typicallytrained and tested on speech data collected under laboratoryconditions.
Laboratory databases can be gathered relatively quicklyand inexpensively b  recording speech produced by cooperativevolunteers who are aware that they are participating in a datacollection exercise.
But these databases typically have relativelyfew talkers and speech that is recited rather than spontaneously-produced.Potential differences between real user and laboratory speechdatabases would be of little interest if speech recognition systemswere performing as well in field applications as they are in thelaboratory.
However, there is data to suggest that this is not thecase; systems performing well in the laboratory often achievesignificantly poorer esults when confronted with real user data\[1,2\].A number of features that differentiate r al user from laboratorydatabase collection procedures may have an impact on theperformance of speech recognition systems.
One that has receivedspecific attention i the literature is that of spontaneously-producedvs.
read speech.
Jelinek et al \[3\] compared the performance of aspeech recognition system when tested on pre-recorded, read andspontaneous speech produced by five talkers.
Results indicatedecreasing performance for the three sets of test material (98.0%,96.9% and 94.3% correct, respectively).
Rudnicky et al \[4\]evaluated their speech recognition system on both read andspontaneous speech produced by four talkers and found thatperformance was roughly equal for the two data sets (94.0% vs.94.9% correct, respectively).
It is important to note, however, thatthe spontaneous speech used for this comparison was "live clean164speech" defined as "only those utterances that both contain ointerjected material (e.g., audible non-speech) and that aregrammatical".
Degradation i performance was indeed seen whenthe test set included all of the "live speech" (92.7%).
Zue etal.
\[5\]also evaluated their speech recognition system on read andspontaneous speech samples.
Word and sentence accuracy weresimilar for the two data sets.
For each of these studies, 'real user'speech samples were recorded under wideband application-likeconditions.
For at least wo of the studies (\[4\], \[5\]), the 'real users'were apparently aware that they were participating in anexperiment.It has not been possible to collect databases that are matchedwith respect to speakers for telephone speech, probably because theanonymity of the users of telephone services makes it difficult oobtain read versions of spontaneously-produced sp ech from thesame set of talkers.
Therefore, there is little published ata on theeffects of read vs. spontaneous speech on the performance ofrecognition systems for telephone applications.
Differences inspeakers not withstanding, there is recent data to suggest hatrecognition performance an be significantly poorer when testingon real user telephone speech as compared to tests using telephonespeech collected under laboratory conditions (\[1\], \[2\]).In summary, laboratory and real user behavior can becharacterized along at least wo important dimensions: complianceand speech characteristics.
To gain a better understanding of how toimprove the field performance of speech recognition/understandingsystems, we have been collecting and analyzing both laboratoryand real user data.
The goal of this paper is to summarize ourwork in the analysis of 1. real user compliance for telephoneapplications and 2. laboratory vs. real user speech data for thedevelopment of speech recognition/understanding systems.REAL USER DATABASE COLLECTIONPROCEDURESThree real user telephone speech databases have been collectedby pseudo-automating telephone operator functions and digitallyrecording the speech produced by users as they interacted with theservices.
In each case, experimental equipment was attached to atraditional telephone operator workstation and was capable of : 1.automatically detecting the presence of a call, 2. playing one of aset of pre-recorded prompts to the user, 3. recording user speech, 4.automatically detecting a user hang-up and 5. storing data aboutcall conditions associated with a given speech file (e.g., time ofday, prompt condition, etc.).
The three operator services understudy were 1.
Intercept Services (IS) 2.
Directory Assistance CallCompletion (DACC) and 3.
Directory Assistance (DA).
In additionto collecting data for several automated dialogues, recordings weremade of traditional 'operator-handled' calls for the services underinvestigation.Each of these databases was collected in a real serviee-providingenvironment.
That is, users were unaware that they wereparticipating in an experiment.
The identity of the speakers wasnot known, so a precise description of dialectal distribution isdifficult.
Calls reached the trial position through randomassignment of calls to operator positions, a task performed by anetwork component known as an Automatic Call Distributor(ACD).
Therefore, for each of the databases, it is assumed that henumber of utterances corresponds tothe number of speakers.
Wehave so far collected nearly 29 hours of real user speech: 34,000utterances (presumably from that many different speakers).REAL USER COMPLIANCEFor the IS trial, users were asked what telephone number theyhad just dialed.
For the DACC trial, users were asked to accept orreject he call completion service.
For the DA trial, users wereasked for the city name corresponding to their directory request.The 'target' responses, therefore, were digit strings, yes/noresponses and isolated city names, respectively.
Users werepresented with different automated prompts varying along a numberof dimensions.
Their responses were analyzed to determine theeffects of dialogue condition on real user compliance (frequencywith which users provided the target response).Intercept Services: 'Simple' Digit RecognitionOne problem with digit recognition is that users may say morethan just digits.
The target response for the IS trial was a digitstring.
The automated prompts to the users varied with respect tothe 1. presence/absence of an introductory greeting which informedusers that hey were interacting with an automated system 2. speedwith which the prompts were spoken (fast, slow), and 3. theexplicitness of the prompts (wordy, concise).
In addition, data wererecorded under an operator-handled condition.
During operator-handled intercept calls, operators ask users, "What number did youdial?
".A total of 3794 utterances were recorded: 2223 were in theautomated-prompt conditions and 1571 were in the operator-handledcondition.
'Non-target' words were defined as anything other thanthe digits '0' through '9' and the word 'oh'.
Results showed thatonly 13.6% of the utterances in the automated conditions wereclassified as non-target, while 40.6% of the utterances in theoperator-handled condition fell into the non-target category.Non-target utterances were further classified as '100-type'utterances (that is, utterances inwhich the user said the digit stringas "992-4-one-hundred, tc.)
and 'extra verbiage' utterances (that is,utterances in which the user said more than just the digit stringsuch as, "I think the number is ...", or "oh, urn, 992 ...").
Forboth automated and operator-handled calls, users produce more extraverbiage utterances than 100-type utterances.
Both types of non-target responses occurred more than twice as often in the operator-handled condition compared to the automated conditions.The speed and wordiness of the automated prompts did notaffect user compliance.
However, contrary to our expectations, thedata suggest hat the proportion of non-target responses issubstantially reduced when the user is not given an introductorygreeting which explains the automated service (19.2% vs. 4.9%non-target responses for the greeting vs. no-greeting conditions,respectively).
Instead, giving users an immediate directive to saythe dialed number results in the highest proportion of responseswhich are restricted to the desired vocabulary.
It appears that evenuntrained users are immediately attuned to the fact that they areinteracting with an automated service and modify their instinctualresponse in ways beneficial to speech recognition automation.
Atleast for this application, brevity is best.
For more information onthis trial, see \[6\].D i rec tory  Ass i s tance  Ca l l  Complet ion :  Yes /NoRecognitionThe target response for the DACC trial was an isolated 'yes' or'no' response.
Successful recognition of these words would havemany applications, but the problem for a two-word recognizer is165that users sometimes say more than the desired two words.
Datawere collected under fl~ree automated prompt conditions and oneoperator-lmndled condition.
The operator asked, ?Would you like usto automatically dial that call for an additional charge of__ cents?
"The three automated prompts were as follows: 1. a recorded versionof the operator prompt 2. a prompt which explicitly asked for a'yes' or 'no' response and 3. a oromot hat asked for a 'yes' or hangup response.A total of 3394 responses were recorded; 1781 were operator-handled calls, while 1613 were calls handled by automatedprompts.
Figure 1 shows the percentage of 'yes' responses amongthe affirmative responses as a function of dialogue condition.Results again indicate that variations in the prompt can have asizable ffect on user compliance and that there are considerabledifferences between user behavior with a human operator vs. anautomated system.i P100 ' |90 1 so706040o3020o !
I !
IOP.
Pr.
1 Pr.
2 Pr.
3PromptsFigure 1: Percentage of affirmative responses that were the targetresponse ('yes') as a function of dialogue condition.Non-target affirmative r sponses were categorized as 'yes, please','sure' and 'other'.
A response was categorized as 'other' if itaccounted for less than 5% of the data for any prompt condition.The frequency of occurrence of these non-target responses as afunction of dialogue condition is shown in Table 1.
'yes, please''sure'otherOperatorhandled221653Prompt1161026Prompt23012~ompt3107Table 1: Percentage of users' affirmative responses as a functionof prompt condition.The operator-handled condition exhibited the greatest range ofvariability, with 53% of the affirmative responses falling into the'other' category.
For more information on the DACC trial, see \[7\],"Directory assistance, what city please?
"The target response for the Directory Assistance trial was anisolated city name.
Data were collected under four automatedprompt conditions and one operator-handled condition.
DirectoryAssistance operators typically ask users "What city, please?".
Oneautomated prompt used the same wording as the operator; the otherthree were worded to encourage users to say an isolated city name.Recording was initiated automatically at the offset of a beep tonethat prompted users to respond.
Recording was terminated by ahuman observer who determined that the user had finishedresponding tothe automated request for information.A total of 26,946 utterances were collected under automatedconditions.
Operator-handled calls were collected uring a separatetrial \[8\] and only 100 of these utterances were available foranalysis.
Figure 2 shows the percentage of target responses as afunction of dialogue condition.1009o70 Isolated City Narr~6o5040300 ' u !OP.
Pr.
1 Pr.
2 Pr.
3 Pr.
4PromptsFigure 2: Percentage of all responses that were isolated citynames as a function of dialogue condition.As in the other two trials, user behavior was quite different foroperator-handled vs. automated calls.
On average, users werealmost four times more likely to say an isolated city name inresponse to an automated prompt than to an operator query.Moreover, the wording of the automated prompt had a large effecton user compliance.
Superficially minor variations in promptwording increased user compliance by a factor of four (15.0% vs.64.0% compfiance for prompt 1 vs. 4, respectively).Very few users either did not reply or replied without a cityname in response to an operator prompt.
For the automatedconditions, between 14% and 23% of the users simply did notrespond.
Between 3% and 23% responded without including acityname.
To interpret these results, we point out that in contrast tothe users oflS and DACC services, Directory Assistance users tendto be repeat callers with well-rehearsed scripts in mind.
When thefamiliar interaction is unexpectedly disrupted, some of these usersappear to be unsure of how to respond.Of particular interest was the effect of dialogue condition on thefrequency of occurrence of city names embedded in longerutterances.
These results appear in Figure 3.1661009080o 50?
403o20" ,0m 10-0 I I \[ I IOP.
Pr.
1 Pr.
2 Pr.
3 Pr.
4PromptsFigure 3: Percentage of all responses that were embedded citynames as a function of dialogue condition.It is clear that embedded responses are most ypical during user-operator interactions.
To allow for this response mode, a recognizerwould have to be able to 'find' the city name in such utterances.This could be accomplished with a word spotting system or with acontinuous speech recognition/understanding system.
To considerthe difficulty of the former, embedded city name responses werefurther categorized as simple vs. complex; assuming that theformer would be relatively easy to 'spot'.
A 'simple' embedded cityname was operationally defined as a city name surrounded byapproximately one word (for example, "Boston, please", "urn,Boston", "Boston, thank you").
The proportion of embeddedutterances classified as 'simple' as a function of prompt is shownin Figure 4.100 .~80 'Simple' Embedded70403020OP.
Pr.
1 Pr.
2 Pr.
3 Pr.
4PromptsFigure 4: Percentage of embedded city names that werecategorized as 'simple' as a function of dialogue condition.It is interesting tonote that prompts 3 and 4, which elicited thehighest proportion of isolated city names, also elicited a higherproportion of 'simple' embedded city names.
It seems that usersinterpreted prompts 3 and 4 as the most constraining, even whenthey did not fully comply.DISCUSSIONThe results of this series of experiments on real usercompliance suggest that this aspect of user behavior issignificantly different when interacting with a live operator thanwhen interacting with an automated system.
The lesson is thatfeasibility projections made on the basis of observing operator-handled transactions will significantly underestimate automationpotential.
In addition, the precise wording of the prompts used in aspeech recognition/understanding application significantly affectsuser compliance and therefore the likelihood of recognition success.Users seem to know immediately that they are interacting with anautomated service and explicitly infotraing them of this fact doesnot improve (in fact, decreases) user compliance.
Prompts houldbe brief and the tasks should not be too unnatural.
Although notdiscussed above; informal analysis of the data suggests that veryfew users attempted to interrupt the prompts with their verbalresponses.
While this would suggest that 'barge-in' technology isnot a high priority, it should be noted that the users underinvestigation were all first-time users of the automated service.
Itseems likely that their desire to interrupt the prompt will increasewith experience, as has been found for Touch-Tone applications.Although each of the applications under investigation wasdifferent with respect to the degree of repeat usage, the motivationof the user, the cost of an error, etc., the trials were similar in thatthere was no opportunity for learning on the part of the user.
Thisis an important factor in the success of many speechrecognition/understanding systems and is an area of future researchfor the group.LABORATORY DATABASE COLLECTIONPROCEDURESWhile real user speech databases provide value to theresearcher/developer, they present limitations as well.
Mostnotably, the a priori probabilities for the vocabulary items underinvestigation are typically quite skewed.
It is rare, in a realapplication, that any one user response is as likely as any other.The DA data collection gathered almost 27,000 utterances, yetthere are less than 10 instances of particular cities and,correspondingly, less than 10 exemplars of certain phones.
If thesedata are to be used for training speech recognition/understandingsystems, they must be supplemented with laboratory data?
To thisend, as well as for the purposes of comparing real user tolaboratory data, application-specific and standardized laboratorytelephone speech data collection efforts were undertaken.Application-specific laboratory speechdatabase collectionA laboratory city name database was collected by havingvolunteers call a New York-based laboratory from their NewEngland-based home or office telephones.
Talkers were originallyfrom the New England area and so were assumed to be familiarwith the pronunciation f the target city names.When a speaker called, the system asked him/her to speak thecity names, waiting for a prompt before saying the next city name(the order of the city names was randomized soas to minimize listeffects).
10,900 utterances from over 400 speakers have beencollected in this way.This kind of database provides ome of the characteristics of areal user database (a sample of telephone network connections and167telephone sets).
The speech, however, is read rather thanspontaneously-produced and the speakers are aware that they areparticipating in a data collection exercise.
This database has beencompared to the DA corpus just described.
Results are reportedbelow.Standardized telephone speech data collectionThe TIM1T database r flects the general nature of speech and isnot designed for any particular application \[10\].
It is well knownthat the telephone network creates both linear and nonlineardistortions of the speech signal during transmission.
In thedevelopment of a telephone speech recognition/understandingsystem, it is desirable to have a database with the advantages of theTIMIT database, coupled with the effects introduced by thetelephone network.
Towards this end, a data collection system hasbeen developed tocreate a telephone network version of the TIM1Tdatabase (as well as other standardized wideband speech databases).The system is capable of 1. systematically controlling thetelephone network and 2. retaining the original time-alignedphonetic transcriptions.Figure 5 shows the hardware configuration used in thecollection of the NTIMIT (Network TIM1T) database.
The TIMITutterance is transmitted in an acoustically isolated room through anartificial mouth.
A telephone handset is held by a telephone testframe mounting device.
Taken together, this equipment isdesignedto approximate he acoustic oupling between the human mouthand the telephone handset.
To allow transmission of utterances tovarious locations, "loopback" devices in remote central officeswere used.
'el":eTransmitting Computer \[ \] ReceivingComputer \]Figure 5: Hardware configuration for NTIMIT database collection.The choice of where to send the ~ utterances was carefullydesigned to ensure geographic coverage as well as to keep thedistribution of speaker gender and dialect for each geographic arearoughly equivalent to the distribution i the entire TIMIT database.To obtain information about ransmission characteristics such asfrequency response, loss, etc., two calibration signals (sweepfrequency and 1000 Hz pure tone signals) were sent along with theTIMIT utterances.
NTIM1T utterances were automatically alignedwith the original TIM1T transcriptions.The NTIMIT database is currently being used to train atelephone network speech recognition system.
Performance will becompared to that of a system trained on a band-limited version ofthe TIMIT database to determine the effects of a 'real' vs. simulatedtelephone network on recognition results.
In addition, we areevaluating the performance of a recognizer trained on acombination of material from real user speech databases andNTIM1T.For more information on NTIMIT, see \[9\].
The NTIMITdatabase is being prepared for public distribution through NIST.LABORATORY VS.  REAL  USER SPEECHFOR TRAIN ING AND TEST ING SPEECHRECOGNIT ION SYSTEMSThe laboratory and real user city name databases describedabove allowed us to evaluate the performance of a speakerindependent, isolated word, telephone network speech recognitionsystem when tested on laboratory vs. real user data.
Two trainingscenarios were included: 1. trained on laboratory data and 2. trainedon real user data.
To equate the number of training and testingsamples for each of the 15 city names under investigation, only asubset of each database was used (and only isolated city names wereused from the real user database).Each database was divided into a training and testing set,consisting of 90% and 10% of the databases, respectively.
Aphonetically-based speaker independent isolated word telephonenetwork speech recognition system was used for this experiment.The recognizer, developed as part of an MIT-NYNEX jointdevelopment project, was built upon a system developed at MIT(for more details on the MIT recognizer, see \[11\]).
The system wastrained on each training set and then tested on each testing set.
Thisresulted in four training/testing conditions.Results revealed that performance hanged little as a function oflaboratory vs. user training databases when tested on laboratoryspeech (95.9% vs. 91.1% for laboratory and user trainingdatabases, respectively).
In contrast, performance changeddramatically as a function of training database when tested on realuser speech (52.0% vs. 87.7% for laboratory and user trainingdatabases, respectively).
Two points of interest here are: 1.
Therecognizer that was trained and tested on laboratory speechperformed almost 9% better than the recognizer t ained and testedon real user speech (95.9% vs. 87.7% respectively).
Apparently,recognizing real user speech is an inherently more difficultproblem.
2.
Performance of the laboratory-trained system on realuser speech was 43.9% poorer than the same system tested onlaboratory speech.
A number of experiments were conducted tobetter understand these results.It is assumed that the performance of the real user-trainedsystem on real user speech (87.7%) represents optimalperformance.
Therefore, the performance discrepancy tobe exploredis the difference between 52.0% (the lab-trained system on real userspeech) and 87.7%.
Each one of the recognizer's componentsinvolved in the training was considered for analysis.
This included1.
phonetic acoustic models, 2. silence acoustic models 3. lexicaltransition weights and 4. lexieal arc deletion weights.
A series ofexperiments were done in which each of these components fromthe real user-trained recognizer was systematically substituted forits counterpart in the laboratory-trained r cognizer.
The resultinghybrid recognizer was evaluated ateach stage.Results revealed that an overwhelming majority of theperformance difference could be accounted for by the acousticmodels for silence.
A recognizer t ained on laboratory speech whichused silence acoustic models trained on real user speech achieved82% accuracy when tested on real user speech.
An acousticanalysis of the two databases revealed that they were quite similarwith respect o the frequency characteristics of the non-speechportions of the signal and the signal-to-noise ratios.
Rather, it was168the mean duration and variability in duration of the non-speechsignal prior to the onset of the speech that accounts for this effect.It is important to note that the silence surrounding the laboratory-collected city names was artificially controlled by both the datacollection procedures (talkers knew they had only a limited amountof time to speak before hearing the prompt o read the next cityname) and subsequent hand editing.
The real user data were notsince a field recognizer will not see controlled or hand edited ata.While these results may appear to be artifactual, they point out thelimitations imposed on the researcher/developer in only beingexposed to laboratory data.
Further experimentation revealed thatusing real user-trained phonetic acoustic models accounts for mostof the remaining 6%, with decreasing importance attributable toreal user-trained lexical transition weights and real user-trainedlexical arc deletion weights.DISCUSSION AND CONCLUSIONSThe longer term goal of the work summarized above is todevelop speech recognition/understanding systems that maintainhigh accuracy performance in real telephone applications.Comparisons between a real user and a laboratory speechdatabase highlight how apparently superficial differences betweenthe two database types can result in dramatic differences inrecognition performance.
Having accounted for this kind of effect,there appears to be an approximately 6% performance differencethat can be attributed todifferences in the speech signal itself, evenfor a small vocabulary isolated-word ecognition task.
This is thesubject of further investigation.
The differences reported in theliterature when comparing laboratory to field performance forcontinuous peech recognition typically exceeds 6% (\[1\], \[2\]).
Itseems likely that differences between read and spontaneous speechare minimized in the production of isolated words.
Future researchwill include continued study of the differences between real user andlaboratory speech and its effects on recognition/understandingperformance for a continuous speech task.ACKNOWLEDGMENTSI would like to gratefully acknowledge the members of the AISpeech Group: Sara Basson, Ben Chigier, Charles Jankowski,Ashok Kalyanswamy, Suzi Levas, David Lubensky, KennethNawyn, John Pitrelli, Lisa Russell, Kim Silverman and DinaYashchin for their participation i the projects described and fortheir editorial assistance in writing this paper; the MIT SpokenLanguage Systems Group who developed the base recognitionsystem and assisted in various ways towards the completion of thiswork; the NYNEX Systems Analysis Laboratory, Maryann Cavan,John Mitchell, Erik Urdang and the employees of New EnglandTelephone Operator Services who made the collection of thesedatabases possible.REFERENCES\[1\] Yashchin, D., Basson, S., Lauritzen, N., Levas, S., Loring, A.,Rubin-Spitz, J.
(1989) Performance of Speech Recognition Devices:Evaluating Speech Produced Over the Telephone Network, Proceedings ofICASSP, Vol.
1, S10b.10, p. 552-555, May 1989.\[2\] Bounds, A., Pmsak, M. (1989) Implementing Speech Recognitionin an Operational Environment, Proceedings ofAVIOS 1989, p. 3-7 andviewgraph presentation.\[3\] Jelinek, F., Speech Recognition Group (1985) A Real-Time,Isolated-Word, Speech Recognition System for Dictation Transcription,Proceedings ofICASSP, Vol.
2, 23.5.1, p. 858-861, March, 1985.\[4\] Rudnicky, A.I., Sakamoto, M. and Polifroni, J. H. (1990) SpokenLanguage Interaction i  a Goal-Directed Task, Proceedings of ICASSP,Vol.
1, $2.2, p. 45-48, April, 1990.\[5\] Zue, V., Daly, N., Glass, J., Goodine, D., Leung, H., Phillips,M., Polffroni, J., Seneff, S. and Soclof, M. (1989) Preliminary Evaluationof the Voyager Spoken Language System, Proceedings of the SecondDARPA Speech and Natural Language Workshop, October, 1989.\[6\] Rubin-Spitz, J., Yashchin, D. (1989) Effects of Dialogue Designon Customer Responses in Automated Operator Services, Proceedings ofSpeech Tech, 1989.\[7\] Basson, S., Christie, O., Levas, S., Spitz, J.
(1989) EvaluatingSpeech Recognition Potential in Automating Directory Assistance CallCompletion, 1989 AVIOS Proceedings.\[8\] Altom, M.J., Velius, G. (1990) Estimating Directory AssistanceSavings with Automatic City Name Recognition, Belleore TechnicalMemorandum TM -ARH015286.\[9\] Jankowski, C., Kalyanswamy, A., Basson, S., Spitz, J. NTIMIT:A Phonetically Balanced, Continuous Speech, Telephone BandwidthSpeech Database (1990), Proceedings of ICASSP, Vol.
1, $2.19.
p. 109-112, April, 1990.\[10\] Fisher, W., Doddington, G.R., Gondie-Marshall, M (1986) TheDARPA Speech Recognition Research Database: Specifications and Status,Proceedings ofthe DARPA Workshop on Speech Recognition, February,1986.\[11\] Zue, V., Glass, J., Phillips, M., Seneff, S. (1989) The MITSUMMIT Speech Recognition System: A Progress Report, Proceedingsof the First DARPA Speech and Natural Language Workshop, .
178-189.169
