Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12?16,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsLarge-scale CCG Induction from the Groningen Meaning BankSebastian Beschke?, Yang Liu?and Wolfgang Menzel?
?Department of Informatics, University of Hamburg, Germany{beschke,menzel}@informatik.uni-hamburg.de?State Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and Technology, Tsinghua University, Beijing, Chinaliuyang2011@tsinghua.edu.cnAbstractIn present CCG-based semantic parsingsystems, the extraction of a semanticgrammar from sentence-meaning exam-ples poses a computational challenge.
Animportant factor is the decomposition ofthe sentence meaning into smaller parts,each corresponding to the meaning of aword or phrase.
This has so far limitedsupervised semantic parsing to small, spe-cialised corpora.
We propose a set ofheuristics that render the splitting of mean-ing representations feasible on a large-scale corpus, and present a method forgrammar induction capable of extracting asemantic CCG from the Groningen Mean-ing Bank.1 IntroductionCombinatory Categorial Grammar (CCG) formsthe basis of many current approaches to semanticparsing.
It is attractive for semantic parsing dueto its unified treatment of syntax and semantics,where the construction of the meaning representa-tion directly follows the syntactic analysis (Steed-man, 2001).
However, the supervised induction ofsemantic CCGs?the inference of a CCG from acorpus of sentence-meaning pairs?has so far onlybeen partially solved.
While approaches are avail-able that work on small corpora focused on spe-cific domains (such as Geoquery and Freebase QAfor question answering (Zelle and Mooney, 1996;Cai and Yates, 2013)), we are not aware of anyapproach that allows the extraction of a seman-tic CCG from a wide-coverage corpus such as theGroningen Meaning Bank (GMB) (Basile et al.,2012).
This work attempts to fill this gap.Analogous to the work of Kwiatkowski et al.
(2010), we view grammar induction as a series ofsplitting steps, each of which essentially reversesa CCG derivation step.
However, we divergefrom their approach by applying novel heuristicsfor searching the space of possible splits.
Thecombination of alignment consistency and single-branching recursion turns out to produce a man-ageable number of lexical items for most sen-tences in the GMB, while statistical measuresand manual inspection suggest that many of theseitems are also plausible.2 Searching the space of CCGderivationsOur search heuristics are embedded into a verygeneral splitting algorithm, Algorithm 1.
Givena sentence-meaning pair, it iterates over all possi-ble sentence-meaning splits in two steps.
First, asplit index in the sentence is chosen along with abinary CCG-combinator to be reversed (the syn-tactic split).
Then, the meaning representation issplit accordingly to reverse the application of theselected combinator (the semantic split).
E. g., forthe forward application combinator, the meaningrepresentation z is split into f, g so that z = fg(modulo ?, ?, ?
conversions).
By identifying fwith the left half l of the sentence and g with theright half r, we obtain two new phrase-meaningpairs, which are then split recursively.This algorithm combines two challengingsearch problems.
Recursive syntactic splittingsearches the space of syntactic CCG derivationsthat yield the sentence, which is exponential in thelength of the sentence.
Semantic splitting, giventhe flexibility of ?-calculus, has infinitely manysolutions.
The crucial question is how to prunethe parts of the search space that are unlikely tolead to good results.Our strategy to address this problem is to applyheuristics that constrain the results returned by se-mantic splitting.
By yielding no results on certaininputs, this at the same time constrains the syntac-tic search space.
The following descriptions there-12fore relate to the implementation of the SEMSPLITfunction.Algorithm 1 A general splitting algorithm.
C isthe set of binary CCG combinators.
The SEM-SPLIT function returns possible splits of a meaningrepresentation according to the reverse applicationof a combinator.function SPLIT(x, z)if |x| = 1 thenreturn {(x, z)}elseG?
?for 0 < i ?
|x| ?
1 and c ?
C dol?
x0.
.
.
xi?1r ?
xi.
.
.
x|x|?1S ?
SEMSPLIT(c, z)for (f, g) ?
S doG?
G ?
SPLIT(l, f)?
SPLIT(r, g)end forend forreturn Gend ifend function2.1 Alignment consistencyThe first heuristic we introduce is borrowed fromthe field of statistical machine translation.
There,alignments between words of two languages areused to identify corresponding phrase pairs, asin the well-known GHKM algorithm (Galley etal., 2004).
In order to apply the same strategyto meaning representations, we represent them astheir abstract syntax trees.
Following Li et al.
(2013), we can then align words in the sentenceand nodes in the meaning representation to iden-tify components that correspond to each other.This allows us to impose an extra constraint onthe generation of splits: We require that nodes in fnot be aligned to any words in the right sentence-half r, and conversely, that nodes in g not bealigned to words in l.Alignment consistency helps the search to fo-cus on more plausible splits by grouping elementsof the meaning representation with the words thatevoked them.
However, by itself it does not signif-icantly limit the search space, as it is still possibleto extract infinitely many semantic splits from anysentence at any splitting index.Example: Given the word-to-meaning?x?y?mia(y)love(x, y)vincent(x)lovesVincent MiaFigure 1: An example word-to-meaning align-ment.
Splits across any of the alignment edges areprohibited.
E. g., we cannot produce a split whosemeaning representation contains both vincent andmia.alignment from Figure 1, a split that isexcluded by the alignment criterion is:(Vincent : ?g.?x.
?y.vincent(x) ?
love(x, y) ?g(y)), (loves Mia : ?y.mia(y)).
This is becausethe node ?love?
(in f ) is aligned to the word?loves?
(in r).2.2 Single-branching recursive splittingThe second heuristic is best described as a searchstrategy over possible semantic splits.
In the fol-lowing presentation, we presume that alignmentconsistency is being enforced.
Again, it is helpfulto view the meaning representation as an abstractsyntax tree.Recall that our goal is to find two expressionsf, g to be associated with the sentence halves l, r.In a special case, this problem is easily solved: Ifwe can find some split node X which governs allnodes aligned to words in r, but no nodes alignedto words in l, we can simply extract the sub-treerooted at X and replace it with a variable.
E. g.,z = a(bc) can be split into f = ?x.a(xc) andg = b, which can be recombined by application.However, requiring the existence of exactly twosuch contiguous components can be overly restric-tive, as Figure 2 illustrates.
Instead, we say that wedecompose z into a hierarchy of components, witha split node at the root of each component.
Thesecomponents are labelled as f - and g-componentsin an alternating fashion.In this hierarchy, the members of an f -component are not allowed to have alignments towords in l. A corresponding requirement holds for13@@ed@@cbax0x1x2x3x4Figure 2: Illustration of single-branching recur-sion: Assume that the leaves of the meaning rep-resentation a(bc)(de) are aligned as given to thewords x0.
.
.
x4, and that we wish to split the sen-tence at index 2.
The indicated split partitions themeaning representation into three hierarchicallynested components and yields f = ?x.xc(de) andg = ?y.a(by), which can be recombined using ap-plication.g-components.The single-branching criterion states that allsplit nodes lie on a common path from the root,or in other words, every component is the parentof at most one sub-component.In comparison to more flexible strategies,single-branching recursive splitting has the advan-tage of requiring a minimum of additionally gen-erated structure.
For every component, we onlyneed to introduce one new bound variable for thebody plus one for every variable that occurs freeunder the split node.Together with the alignment consistency cri-terion, single-branching recursive splitting limitsthe search space sufficiently to make a full searchtractable in many cases.2.3 Other heuristicsThe following heuristics seem promising but areleft to be explored in future work.Min-cut splitting In this strategy, we place norestriction on which split nodes are chosen.
In-stead, we require that the overall count of splitnodes is minimal, which is equivalent to sayingthat the edges cut by the split form a minimum cutseparating the nodes aligned to the left and righthalves of the sentence, respectively.
This strat-egy has the advantage of being able to handle anyalignment/split point combination, but requires amore complex splitting pattern and thus more ad-ditional structure than single-branching recursion.Syntax-driven splitting Since CCG is basedon the assumption that semantic and syntacticderivations are isomorphic, we might use syntac-tic annotations to guide the search of the deriva-tion space and only consider splits along con-stituent boundaries.
Syntactic annotations mightbe present in the data or generated by standardtools.
However, initial tests have shown that thisrequirement is too restrictive when combined withour two main heuristics.Obviously, an effective combination of heuris-tics needs to be found.
One particular configura-tion which seems promising is alignment consis-tency combined with min-cut splitting (which ismore permissive than single-branching recursion)and syntax-driven splitting (which adds an extrarestriction).3 DiscussionWe present some empirical observations about thebehaviour of the above-mentioned heuristics.
Ourobservations are based on a grammar extractedfrom the GMB.
A formal evaluation of our systemin the context of a full semantic parsing system isleft for future work.3.1 ImplementationCurrently, our system implements single-branching recursive splitting along with alignmentconsistency.
We extracted the word-to-meaningalignments from the CCG derivations annotatedin the GMB, but kept only alignment edgesto predicate nodes.
Sentence grammars wereextracted by generating an initial item for eachsentence and feeding it to the SPLIT procedure.In addition to alignment consistency and single-branching recursion, we enforce three simple cri-teria to rule out highly implausible items: Thecount of arrows in an extracted meaning represen-tation?s type is limited to eight, the number of splitnodes is limited to three, and the number of freevariables in extracted components is also limitedto three.A major limitation of our implementation is thatit currently only considers the application combi-nator during splitting.
We take this as a main rea-son for the limited granularity we observe in ouroutput.
Generalisation of the splitting implemen-tation to other combinators such as composition istherefore necessary before performing any seriousevaluation.143.2 Manual inspectionManual inspection of the generated grammarsleads to two general observations.Firstly, many single-word items present in theCCG annotations of the GMB are recovered.While this behaviour is not required, it is en-couraging, as these items exhibit a relatively sim-ple structure and would be expected to generalisewell.At the same time, many multi-word phrasesremain in the data that cannot be split further,and are therefore unlikely to generalise well.
Wehave identified two likely causes for this phe-nomenon: The missing implementation of a com-position combinator, and coarse alignments.Composition splits would enable the splitting ofitems which do not decompose well (i. e., do notpass the search heuristics in use) under the appli-cation combinator.
Since composition occurs fre-quently in GMB derivations, it is to be expectedthat its lack noticeably impoverishes the quality ofthe extracted grammar.The extraction of alignments currently in use inour implementation works by retracing the CCGderivations annotated in the GMB, and thus es-tablishing a link between a word and the set ofmeaning representation elements introduced by it.However, our current implementation only han-dles the most common derivation nodes and oth-erwise cuts this retracing process short, makingalignments to the entire phrase governed by an in-termediate node.
This may cause the correspond-ing part of the search to be pruned due to a searchspace explosion.
We plan to investigate using astatistical alignment tool instead, possibly usingsupplementary heuristics for determining alignedwords and nodes.
As an additional advantage, thiswould remove the need for annotated CCG deriva-tions in the data.3.3 Statistical observationsFrom the total 47 230 sentences present in theGMB, our software was able to extract a sentencegrammar for 43 046 sentences.
Failures occurredeither because processing took longer than 20 min-utes, because the count of items extracted for asingle sentence surpassed 10 000, or due to pro-cessing errors.On average, 825 items were extracted per sen-tence with a median of 268.
After removing dupli-cate items, the combined grammar for the wholeGMB consisted of about 32 million items.
Whilethe running time of splitting is still exponentialand gets out of hand on some examples, most sen-tences are processed within seconds.Single-word items were extracted for 46% ofword occurrences.
Ideally, we would like to ob-tain single-word items for as many words as pos-sible, as those items have the highest potential togeneralise to unseen data.
For those occurrenceswhere no single-word item was extracted, the me-dian length of the smallest extracted item was 12,with a maximum of 49.4 ConclusionWe have presented a method for bringing the in-duction of semantic CCGs to a larger scale thanhas been feasible so far.
Using the heuristics ofalignment consistency and single-branching recur-sive splitting, we are able to extract a grammarfrom the full GMB.
Our observations suggest amixed outcome: We obtain desirable single-worditems for only about half of all word occurrences.However, due to the incompleteness of the im-plementation and the lack of a formal evaluation,these observations do not yet permit any conclu-sions.
In future work, we will address both ofthese shortcomings.5 Final remarksThe software implementing the presented func-tionality is available for download1.This work has been supported by the Ger-man Research Foundation (DFG) as part of theCINACS international graduate research group.ReferencesValerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012.
Developing a large semanticallyannotated corpus.
In Proceedings of LREC?12, Is-tanbul, Turkey.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextension.
In Proceedings of ACL 2013, Sofia, Bul-garia.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT-NAACL 2004, Boston, Mas-sachusetts, USA.1http://nats-www.informatik.uni-hamburg.de/User/SebastianBeschke15Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilis-tic CCG grammars from logical form with higher-order unification.
In Proceedings of EMNLP 2010,Cambridge, Massachusetts, USA.Peng Li, Yang Liu, and Maosong Sun.
2013.
An ex-tended GHKM algorithm for inducing ?-scfg.
InProceedings of AAAI 2013, Bellevue, Washington,USA.Mark Steedman.
2001.
The Syntactic Process.
MITPress, January.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of AAAI-96, Portland,Oregon, USA.16
