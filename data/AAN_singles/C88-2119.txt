A New Strategy for Providing DefinitionsIn Task-Oriented DialoguesMargaret H. SARNERDepartment of Computer ScienceUniversity of DelawareNewark, Delaware 19716 U.S.A.Sandra CARBERRYDepartment of Computer ScienceUniversity of DelawareNewark, Delaware 19716 U.S.A.AbstractDefinitions may be made up of one or more components,which correspond to strategic predicates.
The selection of whichcomponents o use in giving a definition in a task-oriented dialoguedepends heavily on the needs of the user.
The selection strategywe present involves weighting possible strategic predicates and thepropositiomL used to fill them at multiple points throughout anongoing dialogue and at the actual time of giving the definition.Weighting will he influenced by a model of the user's domain kimwl-edge~ task-related plans and goals, and receptivity to the differentkinds of intormation that could he presented.
An utteraalce canthen be produced that incorporates the most important informa-tion while adhering to common rhetorical practices.1 IntroductionIn the course of ongoing task-oriented expert-consultationdialogues, many occasions arise in which the expert must providea definition.
In this paper we will present a new strategy for acomputer expert to use in giving definitions in a way that is mosthelpful to the individual user.The strategy relies on a dynamically inferred model of theuser's dom~dn knowledge, task-related plans and goals, and recep-tivity to different kinds of information.
It constructs a definition byweighting both the strategic predicates that might comprise a def-inition and the propositions that might be used to fill the strategicpredicates.
These weights are used to order what migh t be said ac-cording to its anticipated usefulness to the user.
Rules can then beused to produce an utterance that incorporates the most importantinformatlo~t while adhering to common rhetorical practices.
Thisstrategy rellects our overall hypothesis that beliefs about the appro-priate content of a definition should guide selection of a rhetoricalstrategy, instead of the choice of a rhetorical strategy determiningcontent.Section 2 describes ituations in task-oriented dialogues inwhich definitions are called for.
Section 3 identifies three charac-teristics that differentiate definitions provided by experts duringtask-oriented dialogues from those provided in response to isolatedrequests for definitions, and argues that the choice of a rhetoricalstrategy should be made on the basis of being able to include inthe definition those features deemed most important.
Section 4proposes a Principle of Usefulness as a guideline for selecting infor-mation to include in definitions.
Section 5 discusses strategic pred-icates.
Section 6 presents an overview of our strategy for weightingpredicates and propositions and then ranking what might be saidaccording to its usefulness to the user.2 Definition SituationsIn its simplest form, a definition-giving dialogue consists ofan information-seeker asking "What is an Xf" and an information-provider saying "An X /s  a .
.
.
.  "
In actual practice, however, thereare many ways a definition can he requested and many ways therequest cmt be responded to by the information-provider.
In or-der to identify the characteristics of definitign-giving dialogues, wehave analyzed transcripts of novice-expert dialogues from a varietyof domains, including student/advisor dialogues, recipe-providingdialogues, taxpayer/tax-agent dialogues, and radio talk shows in!which callers 8ought expert advice on investments and real estate.
1This section describes definition-glving situations identified in thisstudy.An expert may give a definition either in response to a user'srequest or spontaneously.
Occasions for providing definitions arisemost obviously when the user asks a question of the form "What is...
?"
or "What is the significance of .
.
.
? "
The question doesn'thave to be explicit, however, as illustrated in the exchange below,which is an excerpt from a money-management talk show tran-script:E: "I'd llke to see you put that into two different South-ern utilities.
"U: "Southern utilities?
"As shown in \[Carberry 1985\], such elliptical fragments are oftenintended to elicit clarification and explanation of the repeated term.In addition to giving definitions in response to a request bythe user, the expert may provide a definition as part of correctinga user misconception \[McCoy 1986\], or may generate definitionsspontaneously.
There are several reasons an expert may give spon-taneous definitions.
He may provide additional definitional infor-mation to justify use of a concept, tie may think it likely that theuser doesn't know about the entity being introduced.
The expertmay want to ensure that he and the user are working with the samedefinition.
The statement below is an example of a spontaneousdefinition from a recipe-giving dialogue:E: "You use a spring-form pan - -  the kind that ,allowsyou to separate the bottom and the sides once youhave prepared your dish.
"3 Def in i t ions  in  Task -Or iented  D ia loguesMcKeown \[McKeown 1985\] studied definitions in the cou-text of requests for information about the objects modeled by adatabase system.
She claimed that humans have mutually knownconventions for organizing information and providing definitions,and that a natural language system should make use of these strate*gies in producing explanations.
Given a definition request, herTEXT system selected a rhetorical strategy based on the infor-mation available.
The rhetorical strategy was then responsible forselecting the information to he incorporated into the definition.TEXT treated requests for definitions as isolated queries, whereaswe are interested in definitions generated in the course of ongoingtask-oriented dialogues.Our analysis of transcripts of naturally occurring interac-tions indicates that definitions generated in task-oriented dialoguesdiffer significantly .from those generated statically or as a result ofisolated efinition requests.
The differences appear to be the resultof several factors:tThese trmascrlpts were provided by the Computer Science Departments ofthe University of Pen~ylvania and the University of Delaware.5671.
In task-oriented dialogues, the information-provider knowssomething about what the information-seeker is trying to ac-complish, and will generate definitions that help the informa-tion-seeker achieve his goals.
For example, the first responsebelow would be an appropriate definition of baking soda ifthe information-seeker is baking a cake, whereas the secondwould be appropriate if he is trying to relieve indigestion.E: "Baking soda is an ingredient hat, whenheated, releases carbon dioxide, thereby caus-ing the mixture to expand in size.
"E: "Baking soda is a substance that, when dis-solved in water,produces a chemically basic so-lution that will counteract acidity."2.
Whereas tatic definitions or responses to one-shot requestsfor defiuit|ons must assume a generic model for the informa-tion-seeker, esponses to definition requests during an ongo-ing dialogue can take into account acquired beliefs about theinformation-seeker's specific domain knowledge.
For exam-ple, the information-provider might include an analogy to anentity that the information-seeker is already familiar with, asin the following definition of the course CS106:E: "CS106 is like CS105, except hat it uses For-tran instead of Pascal and emphasizes scientificapplications of computing."3.
Whereas tatic definitions and responses to one-shot requestsfor definitions must be generated all at once, dialogue allowsthe information-provider to produce what he thinks will bean acceptable definition and analyze the information-seeker'sresponse to determine whether to elaborate on the definition.For example, in the following dialogue with a veterinarianabout treating a cat with a hyperthyroid condition, the vet-erinarian (E) provides a definition that he believes will Sat-isfy the information-seeker's needs, then must elaborate on itwhen the information-seeker's response reveals multiple goals:to improve the condition of the cat and to have medicationthat is easy to administer.E: "Tapazole is a drug that decreases the functionof the thyroid.
"U: "How large are the pills?"
"H a system carrying on a task-oriented dialogue is te beviewed by the information-seeker as cooperative, intelligent, andnatural, it must take the above factors into account.
Otherwise, itwill not appear to be directed toward the user's goals (uncoopera-tive), will not appear to make use of what the user already knows(unintelligent), and will not appear to take advantage of the factthat the interaction is ongoing, as opposed to one-shot (unnatural).Our hypothesis that, instead of using a rhetorical strategyto determine the content of a definition, the system should reasonabout the user's plans and goals and speclli?
domain knowledge todecide the importance of incorporating individual propositions intothe final definition.
For this purpose a user model, preferably adynamically constructed user model, is essential.
The  choice of arhetorical strategy should be made on the basis of being able toinclude into the definition those features deemed most important.Thus beliefs about the appropiiate content of the definition shouldguide selection of a rhetorical strategy, instead of the choice of arhetorical strategy determining content.McKeown, Wish, and Matthews \[McKeown et al 1985\] ad-dressed some of these issues in their work on an expert lystemthat could provide explanations tailored to users.
They describeda method for using a model of the user's goals along with p~bui l tperspectives on the knowledge base to generate appropriate expla-nations.
While they touched on some of the issues that concernus, they took a different approach from the one we are proposing.568Their perspectives were built into the domain knowledge base, andtheir system did not make much use of informaticm available fromthe system's model of the user's plans and goals.
Also, they wereconcerned with answering can and should questions, whereas we areinterested in definition explanations.4 Appropr iate Content  of a DefinitionOur analysis of naturally occurring consultation dialoguesindicates that definitions can take many forms.
They may bemade up of one or more of a set of components, which correspondto rhetorical predicates described in \[Grimes 1975, Williams 1893,McKeown 1985\].
These predicates will be discussed further in Sec-tion 5.Since we are studying cooperative dialogues in which the ex-pert's goal is to help the information-seeker solve his problem, wehypothesize that the expert's overriding concern in selecting theinformation to include is that the response be as useful as possi-ble to the individual user.
Intuitively, to be truly useful to theuser, the information must be something he doesn't already knowbut something relevant hat he can understand.
Our hypothesis,which appears to explain the definitions occurring in our dialoguetranscripts, uggests the following Principle of Usefulness:Pr inciple of  Usefulness1.
The response should be made at a high enough level that itis meaningful to the user.
(a) Don't say something the user won't understand.
(b) Don't give information that addresses more detailed as-pects of the user's task-related plan than is appropriatefor his current focus of attention.2.
The response should be made at a low enough level that it ishelpful to the user.
(a) Don't inform the user of something he already knows.
(b) Don't give information that is unrelated to the user'sgoals and task-related plan, or is too general for his cur-rent focus of attention in the plan.Grice \[(\]rice 1975\] stated that contributions should be asinformative as required for the exchange, but not more informativethan required.
Paris \[Paris 1988\] suggested that an answer to aquestion should be both informative and understandable to theuser, based on the user's level of knowledge about the domain ofdiscourse.
The Principle of Usefulness formalizes and extends theseguidelines for definitions by selecting the appropriate l vel both inknowledge-related issues (la, 2a) and in plans and goals (lb, 2b).This Principle will be used whenever a selection of appropriate l velof information to fill a predicate is called for.For example, consider a plant classification hierarchy.THING\[ isaPLANT\] isaFLOWERING PLANT\[ isaARUM\[ isaCUCKOOPINTTo descrlbe a Cuckoopint as an arum would have no meantm84fo aninformation-seeker who has never heard of an arnm, while definingit as a thing is too general.
The useful evel of explanation for theinformation-seeker with no special knowledge of botany is defininga cuckoopint as a flowering plant.
In task-odanted dialogues, ad-dltional care must be taken to avoid providing extra informationthat is unrelated t0; or too detailed for, the user's current needs.Otherwise, the extra information may lead the user to erroneouslyassume that the system believes the distinguishing characteristicsare important or that the system has mls-identified the aspect ofhis task on which the user is currently focused.The term rhetorical predicate has taken on several meanlugs in the literature of linguistics and coutputationM linguistics.It ha.s been used to describe relationships ranting from structuralto conceptual in uature.
Grinms \[Grimes 1.975\] described rhetoricalpredicates i.hat "relate ~he kinds of informatio~t communica*ed i~tdiscourse with each other."
One of his predicates was ~he Attributive predica.te which "adds qualities or color to sa~other predicaieas center."
Ilobbs \[tIobbs 1979\] chose to use the term coherence *~.lution in pn;ference to rhetorical predicate to place tile emphasis onthe coherence between sentential units.
McKeown's description ofrhetoricM vredicatcs \[McKeown 1985\] imtdied ~ut association withsentential s~ructure, but ia practice the predicates he used, sucha~ Constitsency, dealt more with conceptuM relationships.Wc :n'e using predicates to chara?terize the componeni;s ofdefiuitio~s i~a terms of relationships between conceptual uuits.
Ourpredicates relate information M)out the entity being defined to theentity itself.
This relationship is datuMs-independent mid content-independent.
For exarnple, our Identification predicate is instanti-axed by fiuding iu a generalization hierarchy an entity which is artancestor of the entity being defined.
This usage is close to MeKe.own's, but because of the apparent ambiguity of the term rhctori..cal pmdicales, we prefer to call the predicates strttte#ic predicates,putting emghasis on the motivation of g~fining ant end (in this case,conveying useful information to the user) ratber than on style.l,?om our study of definitions occurring in actual dialogues,we have identified fourteen distiuct predicates that relate to deft--nixies content.
Each predicate corresponds to a different type ofiah)rntatio~ that can be put into a definition.
Although we donot claim lltat the list is complete or unique, we do believe it issutllcient to generate appropriate definitions in an expert consul-tatlon system.
Some of our predicates (ldeutification, Properties,Analogy, Components) are similar to McKeowu's.
Others (Effect,Prerequisites) are paxticular to a task--orieuted environment.Associated witit each predicate alie semantics ~hat indicatehow to inst ~utiate it.
Foc example, efl~ct information will i~e tbundiu the system's library of plans ~ud ,,~o'ds, aud property informationwill be f~a,~d ill the generalization bieliarchy.
\[a either case, the~;ystem m;t *'casaba about, the paFt icH\]ar_  ' usea++s plans mid goals in,~rder to deternfinc a propositiou's relewntce to what the user is~xyiug h) a.:contplish.
When au occasion !br a definition a~iscs, agiven predicate laity be lilled one or c, tore times.
The propositiotmtiros prod,~ced at'e caudidates for inclusion in the detluitio~,.
Siuceour goal i.~ to selecl; !~he informatiou thai; i~; l,tost important to th,'user, we as~;ociate a me'tsnrc at" signiftcauce with each proposition~The sigailia:aa:ce metrics will be described in Section 6.In the rent,-finder o\[" this sectiou we will look at three, typesof definitio:,t components in some detail to illustrate how the usermodel influences election.,%1 :tfde~tifieationMany naturally occurring definitions contain au Identifi-cation component, identification consists (ff ideutifying the entitybeiug described as a member (d a generic class in a hierarchicMlystructured knowledge base ~- for example,E: "Amaretto is a liqueur.
"Th~ system's model of the user dictates what superclass fromthe generalizaLioa hierarchy to use ia au identification.
In order tbridentificati,m to I)e h@fful to the user, it is necessary that theuser have knowledge of the pareut category used in making theidentitication.
Ttds condition corresponds to the first part of tilePrirtcipk ~, ,,; Usefldness.
Knowledge of a parent category may not besuhici,mt, however, to cause that parent category to be given in thedefinition.
If the systemh beliefs indicate that the pareut categoryis w.lated ~,.~ the u~er's pin, Is end goals, then there is stronger reasonto mention it.
In t\],e cane iu which the entity ha8 severM parentsthat the n~:e~" haJ; kuowledge of, plans and goals should be u.qed to~elect he one (or ones) most appropriate lo mention.
Suppose,lbr exampl% that a digital systems course is cross-listed as both aCmapu*er Science and an Elec~ricM Engineering course.U: "What is Digital Systems?
"E: "It is a Computer Science course .
.
.
"orF,: "It is an F, lectrieal Engineering course .
.
.
"The choice of answer depends on whether tim user model indicatesthat the user is trying to satisfy Coolputer Science or EleetriceflFingineering requirements.
A third ~dternative isF: "It is both a Computer Scieu(:e course mid an ElectricalEngineering course .
.
.
"This response might be given if the model indicates laoth parentcategories play a role in tim user's plans and goals.Following tile Principle of Usefulness, the appropriate super.class is the lowest level parent category that would have meaning tothe user and be relevant to what the system believes al.Ie the user'splans and goals.IDIXThe user knows what A, B, C areTim user doesn't know about i)Tim user asks "What i.~ X ?
"\]n the cm;e illustrated above, the expert's taleutification a:n.sw~rmigllt be "X  is a C." The efl'eci; of an.uwering "X  is a D" wo01dbe to caaJse the user to ask "What is a D?"
.
r  give up withottt get-ting meaningful info~'mation.
The an~Jwm' "X i'.~ a 11" would misstile distinguishing features hared lay C :uld ;( but not lay B.
If the~edistinguishing features ~Lre not important to the tJ.~el ~ii\[1 wol.l!d \[;i-V(!the false impression that tlle system believes they are.
a<W(~v:;~i ttile user's task, however~ a higher hwel thnu C shouhi b.
'~ selected.5.2 P roper t iesA Properties response consists o!
naminv~ characteristicsof tile entity.
These are often expl~ssed i, descriptions Kiwm byhumans a~q "adjectival phrases attached to the ldentitlcati(m of theentity.E: "A no-load fired is a mutual fired with no sales charge.
"E: "Amaretti are crisp Italian almond-flavored macaroons.
"In the TEXT systenl \[McKeown 1985\], attributes whose v;A-ues distinguish one sub-type from another axe marked in ~;he knowl-edge base.
In task-oriented dialogues, however, an entity's mo~qt important distinguishing attributes are not always static but inul.eadmay vary depending on tile inhxrmation..seeker'.q plans and goals.For example, the coarse Computer, Ethics and Society may haveseveral distinguishing properties, including its content, its sub:;tan-tial writing component, its lack of programmiug projects, and itt+scheduling at night through continuing education.
An information.seeker whose objective is to earn a\]IA degree at night while holdinga full-time job would consider its schedtding property of interest ilidifferentiating it from other computer science courses, whereas aa~electrical engineering major seeking a technical elective would prob-ably consider its lack of programming projects of particular siguif.icance.
Titus, although the properties of an entity are found in thegeneralization hierarchy, the system's beliefs about the user's plaJlsand goals should play a major role in determining which propertiesof the entity are most appropriate to iuclude in a (lefiuititm.5695.3 Operat ionAn Operation response consists of a description of howsomething works.
Paris \[Paris 1988\] has demonstrated that expla-nations given novices in a domain often take the form of processtraces.
An Operation definition may take the form of process infor-mation or steps in implementation.
The difference between the twois that the process information is essentially a chain of cause-and-effect occurrences, while the steps in implementation are sequential,but not necessarily causal, as is shown in the example:U: "Can yon tell me what the money market is?
"E: " A money market fund is a group of people gettingtogether - -  put their money together in a pool and it isinvested by professional investors.
"As with the Properties predicate, the system's beliefs aboutthe user's plans and goals must be taken into consideration.
Theexpert might identify the need for an Operation explanation i  atask-oriented dialogue when the entity being explained appears in astep in a plan the user must carry out to meet a goal.
For example,if the user is a traveler asking the expert for help planning a cartrip and the expert advises the user to follow a "Trip Tik," theexpert should explain how a Trip Tik works if the model of the userindicates lack of familiarity with it.
The definitions of baking sodagiven earlier illustrate a case in which the appropriate Operationexplanation depends on the use to which the entity will be put bythe information-seeker.6 Se lect ing  Def in i t ion  ContentOur strategy assumes a knowledge base consisting of a gen-eralization hierarchy containing domain knowledge, a plan library,and a lexicon.
The user model has three components:1. a model of the user's domain knowledge in the form of mark-ings on the knowledge base showing the pieces with which theuser is familiar \[Kass 1987\],2. a model of the user's underlying t/ak-related plan and cur-rent focus of attention in the plan, given by a context ree\[Carberry 1988\],3. a model of how receptive the user is to various kinds of infor-mation, given by weightings on strategic predicates.The first two components will be dynamically updated uring thedialogue as shown in \[Kass 1987\] and \[Carberry 1988\].
The thirdcomponent will also be updated dynamically in response to theuser's receptivity to types of definitions and his own usage of strate-gic predicates.6.1 Weight ing  Pred icatesWhen a definition occasion arises, a local predicate recep-tivity model is created.
Starting with a copy of the current globalweights representing the user's general receptivity to the kinds of in-formation represented by the strategic predicates, as inferred fromthe preceding dialogue, further adjustments may be made to reflectthe appropriateness of the predicates in the particular situation.The question itself and the level of local domain expertisemay cause further weighting of predicates.
For example, if the userasks "What is XP' where X is an object, the Identification predicatewould be more heavily weighted.
If X is an action, the Operationpredicate would be more heavily weighted.
The level of local do-main expertise can be ascertained when a definition is requested bylooking at the parts of the plan library and generalization hierarchythat contain references to the entity in question.
If they are heavilymarked with things the user knows, the user can be considered tohave a high level of expertise; otherwise, the user will be consideredto be a novice.
The weights for predicates that have been deter-mined to be appropriate for expert and novice users will then beincreased \[Paris 1988\].hO.,8 -,6 -,4 -I13 " '2 d " ''+Figure 1: Graph of Relevance Formula6.2 we ight ing  Propos i t ionsAfter predicate weighting has been determined, predicatesare filled with information from the knowledge base (generaliza-tion hierarchy, lexicon, plans and goals) relevant o the conceptbeing defined.
The semantics of each individual predicate dictatewhere to find the information to fill the predicate.
For instance, theIdentification and Properties predicates are filled with informationfound in the generalization hierarchy, and Necessity propositionsare drawn from the plans of the user.
Some predicates may pro-duce several propositions.
For example, an entity may have severalproperties.
For others there might not be any corresponding propo-sitions available.Selection of propositions depends on both the weights of thepossible predicates and a measure of significance of the informa-tion that could be used to fill them.
Significance reflects wherethe proposition fits into the system's model of the user's goals andpossible plans for accomplishing them (relevance) and what infor-mation in the generalization hierarchy has been marked as knownby the user (familiarity).The system's beliefs about the user's underlying task-relatedplan, as dynamically inferred from the preceding dialogue, are rep+resented in a tree structure called a context model \[Carberry 1988\].Each node in this tree represents a goal that the user has investi-gated achieving.
Except for the root, each goal in the context modelis a descendant of a higher-level goal whose associated plan, foundin the system's plan library, contains the lower-level goal.
One nodein the tree is marked as the current focus of attention and indicatesthat aspect of the task on which the user's attention is currentlycentered.
The context model may be expanded to arbitrarily ma~ylevels of detail by repeatedly replacing non-prlmitive suhgoals withassociated plans which themselves contain constituent subgoals.If pursuing a subgoal in a plan represents a significant shiftin focus, it is marked in the plan library as introducing a new focusdomain~;~, Within the context model, a focus domain of subgoalsthat are at approximately the same level of focus is generated byexpanding'the plan associated with a subgoai that introduces thefocus domain.
As long as this plan is expanded by substitutingplans for just those subgoals that do not introduce another newfocus domain, the subgoals appearing in the expanded plan arepart of the same focus domain.Our estimate of relevance is based on distance of the part ofthe context model in which the definition information is found fromthe current focus of attention in the context model.
This distanceis measured as the number of shifts in focus domains.
If the plan isat the focus of attention, the information derived from it is of veryhigh relevance.
If it is in the immediately surrounding focus domain(one shift), the information is still of high relevance.
As the numberof focus domain shifts increases, the relevance of information i  theplans begins to fall off, but as long as a plan has been activatedthe information found in it is of some relevance.
This situation inwhich relevance remains high close to the focus of attention, butdrops off more rapidly as the distance increases, is modeled by aninverse xponential function, as shown in Figure 1.
The equationd2 r = e - ( , )  ,where r is the relevance rating and d is the number of shifts fromthe current focus of attention, captures the desired features.570i .....13Figure 2: Graph of Familiarity FormulaCurrently, our relevance metric treats all shifts ~xaong focusdomains equally.
It may be the case, however, that information i  ahigber-level plan h that led to the current focus of attention is moreappropriate to include in a'defiuition than is information extractedfrom a subplan s appearing in an expansion of the current focusedplan, even if the two plans, h and s, represent the same number ofshifts from the current focus of attention in the context model.
Thecurrent fecund plan is part of an expansion of h, so we know thatthe user is concerned with accomplishing h; therefore, informationrelevant o h may be more significant to the user than informationrelevant o details of carrying out the current focused plan.
This isan issue that we plan to investigate further.Our measure of familiarity is based on the knowledge theexpert believes the user has about the objects, properties, or con-cepts that could be used in a definition.
We are assuming a variantof the user modeling system described by Kass \[Kass&Fiuin 1987\],modified so that each node in the knowledge base is marked with abellef factor~ ranging in value from O to 1, giving the system's levelof belief that the user is familiar with the entity.
Because of theimportance of giving a definition in terms of something the personreceiving the.
definition will understand, an entity known to havemeaning to the user (belief factor = 1) should be treated as poten-tially useful to include, even if it is not germane to the hypothesizedgoals.
If it is not believed strongly that the person is fandllar withthe entity, however, it is less useful to tie the definition to that en-tity.
Note that since the dialogues under consideration are ongoing,as opposed to one-shot, a definition can include items that the sys-tem believes the user is probably familiar with, mad the system canwait for the user's response to decide whether the definition wassuccessful.
The heuristic described here is modeled by the functionshown in Figure 2.
The formulae 6b(2-b) - -  1f= e e -  1 'where f is the familiarity rating and b is the belief factor, exhibitsan appropriate amount of curvature to reflect the rapid drop-off inusefulness a~ the belief factor decreases.The \]ast step in computing a measure of significance for apiece of information is to form a weighted combination of the rele-vance rating and the familiarity rating.
Since our primary goal is toprovide information that will help the user accomplish a task, ouribrmula for combining the two measures weights ignificance twiceas heavily ~ familiarity.
Our significance metric, then, is2r + f.3where S is significance, r is the relevance rating, and f is the famil-iarity rating.The following example from a hypothetical travel domainifiustrates how propo~itions are weighted according to significance.The dialogue pertains to planning a trip abroad.U: "I need to have enough money with me to pay foranything I buy.
"E: "You can carry as much as you like in travelerschecks.
"U: "Travelers checks?
"The first statement causes the have-money plants beinfocas.
Thehave-moneyplan has subgoalshave-convartlble-funda ((_agent: person)(_amountl: funds))hart_currency ((_agent:  person)(_country: country)(_amount2: funds)).Suppose that the user's elliptical fragment is interpreted as a re-quest for a definition.
Figure 3 shows part of the context model.
Asa result of the expert's preceding response, the focus of attention isnow on the have-convertible-funds plan.
Suppose further that theother plans shown are in a focus domain at a distance of 1 from thefocus of attention.Figure 3: A Portion of the Context ModelThe Operation predicate produces the candidate propositionformed from components of the use-travelers-checks subplazt (notshown) equivalent to the statement"You can buy travelers checks at a bank here and cash themin the currency of the country.
"The information comes from the body of the use*travelers-checkssubplan, which is at distance d=l  from the focus of attention.
As-suming that the expert believes that the user is familiar with theconcepts of buying, banks, currency, and cashing things in, we haver = e-(}) 2 = e-('z) 2 -- .939e ~(2-b)  - -  1 e s(1)  - -  1J - e s _ ~ -  eS_ l  =1S = - -=2r+f  .9593571The Analogy predicate is filled by a reference to a siblingwith similar properties, equivalent to"Travelers checks are like personal checks.
"Suppose the belief factor for personal checks is .9 - -  that is, theexpert believes it very likely but is not absolutely certain that theuser knows about personal checks.
Suppose further that the prop-erties of travelers checks that are similar to those of personal checksappear in plans at a distance of two shifts of focus domain from thefocus of attention.
Iu this case we computer = e-(~) 2 - -  e-(~) 2 = .779e sb(2-b) -- 1 e s'4(l't) -- 1f = e 6 -- -1 -- e 6 -- 1 .942S - 2 r+f_ .8333The fact that the first definition component has higher com-puted significance than the second oes not necessarily mean that itwill be preferred, however.
Recall that weights of candidate propo-sitions must reflect both significance of the information and predi-cate receptivity.Once weights have been assigned to the candidate proposi-tions, they are then ranked according to weight and put into cate-gories.
There are four categories:Must SaySay if ConvenientSay if Needed for CoherenceDo Not SayThe higher weight categories receive the higher-weighted propo-sitions; the lower-weighted propositions go into the lower weightcategories.
Some categories may be empty.When all category assignments have been made, the result-ing four groups of propositions axe passed to an answer generator.Construction of this answer generator is a future project.
The gen-erator will take the classes of propositions, find a way to say all ofthe Must Say propositions a~ld as many as possible of the Say ifConvenient propositions, using Say if Needed for Coherence propo-sitions whenever they help the construction of the response.
Wepropose to do this task using rules of combination developed toproduce an utterance that adheres to common rhetorical practicesthat people appear to follow.7 A ComparisonOur strategy will produce different responses tban wouldcurrent definition systems.
For example, consider a request for adefinition of amaretti.
McKeown's TEXT system would identify theentity and include all based database and distinguishing databaseattributes, and would produce a definition resembling"Amaretti are macaroons.
They are made from apricot ker-nels, have ahnond flavor, are of Italian origin, and have crisptexture.
The most popular brand is made by Lazzaroni andCompany.
"Our definition module would attempt o pick information appro-priate to the individual user.
If the user is selecting food items tosell in an international bazaar, it would say"Amaretti are Italian macaroons.
The most popular brandis made by Lazzaxoni and Company.
"If the user is making Amaretti Amaretto Chocolate Cheesecake, forwhich amaretti are an ingredient, however, it would say"Amaretti are crisp almond-flauored macaroons.
"8 .Future WorkOur continuing research will work out additional details ofour strategy for providing definitions in task-oriented dialogues.
Weneed to investigate a strategy for dynamically weighting strategicpredicates according to the user's perceived receptivity to differentkinds of information, and putting this weighting together with ore'measure of significance for propositions.
An answer generator thatcombines propositions, giving emphasis to including those proposi..tions deemed most important o say, must be designed.
This taskincludes ranking the candidate propositions by weight and combin-ing the most heavily weighted ones in a way that will produce acoherent utterance.
Finally, the system must be implemented totest and demonstrate he utility of our definition strategy.9 SummaryWe claim that determining the most important hings tosay for the individual user is the most significant task in providingdefinitions in task-oriented dialogues.
In thls paper we prasent anew strategy for generating definitions, using a weighting strategythat draws on a dynamically inferred model of the user's domainknowledge, task-related plans, and receptivity to different kindsof information.
This strategy reflects our over-all hypothesis thatbeliefs about the appropriate content of a definition should guideselection of a rhetorical strategy, instead of the choice of a rhetor-ical strategy determining content.
This approach will produce asystem that exhibits cooperative, intelligent behavior by providingdefinitions tailored to the needs of the individual user.ReferencesCarberry, Sa~dra.
1985.
A Pragmatics Based Approach toUnderstanding Intersentential El ipsis.
In: t'roceedings of the23rd Annual Meeting of the Association for Computation Lin..gaistics, 188'-197.Carberry, Sandra.
1988.
Modeling the User's Plans and Coals.Computational Linguistics Journal, To Appear.Grice, H. Paul.
1975.
Logic and Conversation.
In: P. Coleand J. L. Morgan, Eds., Syntax and Semantics II\[: SpeechActs, Academic Press, N.Y.: 41-58.Grimes, J. E. 1975.
The Thread of Discourse.
Mouton.lIobbs, Jerry R. 1979.
Coherence and Coreferenee.
CognitiveScience, 3:67-90.Kass, Robert.
1987.
Implicit Acquisition of User Models inCooperative Advisory Systems.
Technical Report MS-CIS-87-05, Department of Computer and Information Science, Uni-versity of Pennsylvania, Philadelphia, PA.Kass, Robert and Finin, Tim.
1987.
Rules for the ImplicitAcquisition of Knowledge About the User.
Proceedings of theSixth National Conference on Artificial Intelligencc, 295-30{}.McCoy, Kathleen F. 1986.
The ROMPEI~ System: Respond-ing to Object-Related Misconceptions Using Perspective.
Pro~.ceedings of the 24th Annual Meeting of the Association forComputational Linguistics, 97-105.McKeown, Kathleen IL 1985.
Text Generation.
CambridgeUniversity Press.McKeown, K., Wish, M., and Matthews, K. 1985. l~lorirlgExplanations for the User.
In: Proceedings of the 1985 Con.-ference, Int'l Joint Conference on Artificial Intelligence, LosAngeles CA.Paris, Cecile L. 1988.
Tailoring Object Descriptions to aUser's Level of Expertise.
Computational Linguistics Journal.Williams, W. 1893.
Composition and Rhetoric.
Heath andCompany.572
