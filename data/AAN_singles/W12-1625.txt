Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 179?188,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsProbabilistic Dialogue Models with Prior Domain KnowledgePierre LisonDepartment of InformaticsUniversity of Oslo, Norwayplison@ifi.uio.noAbstractProbabilistic models such as Bayesian Net-works are now in widespread use in spokendialogue systems, but their scalability to com-plex interaction domains remains a challenge.One central limitation is that the state spaceof such models grows exponentially with theproblem size, which makes parameter esti-mation increasingly difficult, especially fordomains where only limited training data isavailable.
In this paper, we show how to cap-ture the underlying structure of a dialogue do-main in terms of probabilistic rules operatingon the dialogue state.
The probabilistic rulesare associated with a small, compact set of pa-rameters that can be directly estimated fromdata.
We argue that the introduction of this ab-straction mechanism yields probabilistic mod-els that are easier to learn and generalise bet-ter than their unstructured counterparts.
Weempirically demonstrate the benefits of suchan approach learning a dialogue policy for ahuman-robot interaction domain based on aWizard-of-Oz data set.1 IntroductionSpoken dialogue systems increasingly rely on prob-abilistic models at various stages of their pipeline.Statistical methods have notably been applied totasks such as disfluency detection (Lease et al,2006), semantic parsing (Erdogan et al, 2002; Heand Young, 2005), dialogue act recognition (Stol-cke et al, 2000; Lan et al, 2008), dialogue man-agement (Frampton and Lemon, 2009; Young et al,2010), natural language generation (Oh and Rud-nicky, 2002; Lemon, 2011) and speech synthesis(Zen et al, 2009).There are two compelling reasons for this grow-ing interest in statistical approaches: first, spokendialogue is pervaded with noise and uncertainty(due to e.g.
speech recognition errors, linguisticand pragmatic ambiguities, and unknown user in-tentions), which must be dealt with at all processingstages.
Second, a decisive advantage of probabilis-tic models lies in their ability to be automaticallyoptimised from data, enabling statistically-based di-alogue systems to exhibit conversational behavioursthat are often more robust, flexible and adaptive thanhand-crafted systems (Lemon and Pietquin, 2007).Despite their success, the use of probabilisticmodels also presents a number of challenges.
Themost pressing issue is the paucity of appropriate datasets.
Stochastic models often require large amountsof training data to estimate their parameters ?
ei-ther directly (Henderson et al, 2008) or indirectlyby way of a user simulator (Schatzmann et al, 2007;Cuaya?huitl et al, 2010).
Unfortunately, real interac-tion data is scarce, expensive to acquire, and difficultto transfer from one domain to another.
Moreover,many dialogue domains are inherently open-ended,which means they are not limited to the completionof a single task with predefined features but have torepresent a varying number of tasks, complex usermodels and a rich, dynamic environment.
Exam-ples of such domains include human-robot interac-tion (Kruijff et al, 2010), cognitive assistants andcompanions (Nguyen, 2005; Cavazza et al, 2010),and tutoring systems (Litman and Silliman, 2004;Eskenazi, 2009).
In such settings, the dialogue sys-tem might need to track a large number of variablesin the course of the interaction, which quickly leadsto a combinatorial explosion of the state space.There is an extensive body of work in the machine179learning and planning literature that shows how toaddress this issue by relying on more expressive rep-resentations, able to capture relevant aspects of theproblem structure in a compact manner.
By takingadvantage of hierarchical or relational abstractions,system developers can leverage their domain knowl-edge to yield probabilistic models that are easier tolearn (due to a reduced number of parameters) andmore efficient to use (since the structure can be ex-ploited by the inference algorithm).The contributions of this paper are twofold.
Wefirst present a new framework for encoding priorknowledge in probabilistic dialogue models, basedon the concept of probabilistic rules.
The frame-work is very general and can accommodate a widespectrum of domains and learning tasks, from fullystatistical models with virtually no prior knowledgeto manually designed models with only a hand-ful of parameters.
Second, we demonstrate howthis framework can be exploited to learn stochas-tic dialogue policies with limited data sets using aBayesian learning approach.The following pages spell out the approach inmore detail.
In Section 2, we provide the generalbackground on probabilistic models and their use inspoken dialogue systems.
We describe in Section 3how to encode such models via probabilistic rulesand estimate their parameters from data.
In Sec-tion 4, we detail the empirical evaluation of our ap-proach in a human-robot interaction domain, givensmall amounts of data collected in Wizard-of-Oz ex-periments.
Finally, we discuss and compare our ap-proach to related work in Section 5.2 Background2.1 Bayesian NetworksThe probabilistic models used in this paper are ex-pressed as directed graphical models, also known asBayesian Networks.
Let X1...Xn denote a set ofrandom variables.
Each variable Xi is associatedwith a range of mutually exclusive values.
In dia-logue models, this range is often discrete and can beexplicitly enumerated: V al(Xi) = {x1i , ..., xmi }.A Bayesian Network defines the joint probabil-ity distribution P (X1...Xn) via conditional depen-dencies between variables, using a directed graphwhere each node corresponds to a variable Xi.
EachACBDEValue for B: P(B)T 0.6F 0.4Value for A: P(A)T 0.3F 0.7Value for C P(C)T1.0 if (A=T ?
B=T)0.0 otherwiseF0.0 if (A=T ?
B=T)1.0 otherwiseValuefor D:P(D|C)C=T C=FT 0.2 0.99F 0.8 0.01Valuefor E:P(E|C)C=T C=FT 0.5 0.4F 0.5 0.6Figure 1: Example of Bayesian network with 5 nodes.The double circles denote a deterministic node.
Asan example, the query P (A|D=T) gives the resultP (A=T|D=T) ?
0.18 and P (A=F|D=T) ?
0.82.edge Xi ?
Xj denotes a conditional dependencebetween the two nodes, in which case Xi is said tobe a parent of Xj .
A conditional probability distri-bution P (Xi|Par(Xi)) is associated with each nodeXi, where Par(Xi) denotes the parents of Xi.Conditional probability distributions (CPDs) canbe defined in various ways, from look-up tablesto deterministic distributions (Koller and Friedman,2009).
Together with the directed graph, the CPDsfully determine the joint probability distribution ofthe Bayesian Network.
The network can be used forinference by querying the distribution of a subset ofvariables, often given some additional evidence, asillustrated by the example in Figure 1.2.2 Dialogue ModelsA dialogue state s is usually decomposed into a setof state variables s = {s1, ...sn} representing rel-evant features of the interaction.
For instance, thestate variables for a human-robot interaction sce-nario might be composed of tasks to accomplish, theinteraction history, past events, as well as objects,spatial locations and agents in the environment.Given the uncertainty present in spoken dialogue,many variables are only partially observable.
Wethus encode our knowledge of the current state ina distribution b(s) = P (s1, ..., sn) called the be-lief state, which can be conveniently expressed asa Bayesian Network (Thomson and Young, 2010).This belief state b is regularly updated as new infor-180SpeechrecognitionSpeechunderstandingGenerationSpeechsynthesisExtra-linguistic environmentUserinput speech signal(user utterance)Recognitionhypotheses uuUtterance tosynthesise umoutput speech signal(machine utterance)Interpretedutterance ?uIntendedresponse am~Belief state bBeliefupdateActionselectionDialogue managementFigure 2: Dialogue system architecture schema.mation becomes available.
As illustrated in Figure2, the whole system pipeline can be formalised interms of inference steps over this belief state:1.
Upon detection of a new utterance, the speechrecogniser generates the N-best list of recogni-tion hypotheses u?u = P (uu|o);2.
Speech understanding then searches for themost likely dialogue act(s) realised in the ut-terance: a?u = P (au|u?u,b);3.
The belief state is updated with the new inter-preted dialogue act: b?
= P (s?|a?u,b);4.
Based on the updated belief state, the action se-lection searches for the optimal system actionto perform: a?m = arg maxam Q(am|b);5.
The system action is then realised in an utter-ance um, which is again framed as a search foru?m = arg maxum Q(um|b, am);6.
Finally, the dialogue state is re-updated giventhe system action: b?
= P (s?|am,b).The models defined above use P (x|b) as a nota-tional convenience for?si?V al(s) P (x|s=si)b(si).The same holds for the estimated values u?u and a?u:P (x|y?)
=?yi?V al(y?)
P (x|y=yi)P (y=yi).3 ApproachThe starting point of our approach is the observationthat dialogue often exhibits a fair amount of internalstructure.
This structure can take several forms.We can first note that the probability or utilityof a given output variable often depends on only asmall subset of input variables, although the num-ber and identity of these variables might naturallydiffer from action to action.
The state variable en-coding the physical location of a mobile robot is forinstance relevant for answering a user requesting itslocation, but not for responding to a greeting act.Moreover, the values of the dependent variablescan often be grouped into partitions yieldingsimilar outcomes, thereby reducing the problemdimensionality.
The partitions can generally beexpressed via logical conditions on the variablevalues.
As illustration, consider a dialogue wherethe user can ask yes/no questions pertaining to thecolour of specific objects (e.g.
?Is the ball red??
).The utility of the system action Confirm dependson two variables: the user dialogue act, for instanceau= VerifyColour(ball, red), and the object colour,such as ball.colour = blue.
The combination ofthese two variables can take a wide range of values,but the utility of Confirm only depends on two par-titions: (VerifyColour(x, y) ?
x.colour=y),in which case the utility is positive, and(VerifyColour(x, y) ?
x.colour 6=y), in whichcase it is negative.We outline below a generic description frame-work for expressing this internal structure, based onthe concept of probabilistic rules.
The rules ex-press the distribution of a dialogue model in terms ofstructured mappings between input and output vari-ables.
At runtime, the rules are then combined toperform inference on the dialogue state, i.e.
to com-pute the distribution of the output variables given theinput variables.
As we shall see, this is done by in-stantiating the rules and their associated variablesto construct an equivalent Bayesian Network usedfor inference.
The probabilistic rules thus functionas high-level templates for a classical probabilisticmodel.
The major benefit of this approach is that therule structure is described in exponentially fewer pa-rameters than its plain counterpart, and is thus mucheasier to learn and to generalise to unseen data.3.1 DefinitionsA probabilistic rule is defined as a condition-effectmapping, where each condition is mapped to a setof alternative effects, each being assigned a distinct181probability.
The list of conditions is ordered andtakes the form of a ?if ... then ... else?
case express-ing the distribution of the output variables dependingon the inputs.Formally, a rule r is defined as an ordered listof cases ?c1, ...cn?, where each case ci is associatedwith a condition ?i and a distribution over stochas-tic effects {(?1i , p1i ), ..., (?ki , pki )}, where ?ji is astochastic effect and probability pji = P (?ji |?i),where p1...ki satisfy the usual probability axioms.The rule reads as such:if (?1) then{P (?11) = p11, ... P (?k1 ) = pk1}...else if (?n) then{P (?1n) = p1n, ... P (?mn ) = pmn }A final else case is implicitly added to the bottom ofthe list, and holds if no other condition applies.
Ifnot overridden, the default effect associated to thislast case is void ?
i.e.
it causes no changes to thedistribution over the output variables.ConditionsThe rule conditions are expressed as logical for-mulae grounded in the input variables.
They can bearbitrarily complex formulae connected by conjunc-tion, disjunction and negation.
The conditions onthe input variables can be seen as providing a com-pact partitioning of the state space to mitigate thedimensionality curse.
Without this partitioning inalternative conditions, a rule ranging over m vari-ables each of size n would need to enumerate nmpossible assignments.
The partitioning with condi-tions reduces this number to p mutually exclusivepartitions, where p is usually small.EffectsThe rule effects are defined similarly: given a con-dition holding on a set of input variables, the asso-ciated effects define specific value assignments forthe output variables.
The effects can be limited toa single variable or range over several output vari-ables.
For action selection, effects can also take theform of assignments of utility values for a particularaction, i.e.
Q(am = x) = y, where y is the scalarvalue for the utility of action x.Each effect is assigned a probability, and severalalternative stochastic effects can be defined for thesame case.
If a unique effect is specified, it is thenimplicitly assumed to hold with probability 1.0.
Theprobabilities of stochastic effects and the action util-ities are treated as parameters, which can be eitherhand-coded or estimated from data.ExampleThe rules r1 and r2 below express the utilities oftwo actions: the physical action ExecuteMov(X)(with X representing the movement type), and theclarification request AskRepeat.r1 : if (au= RequestMov(X)) then{Q(am= ExecuteMov(X)) = ?
(1)r1 }r2 : if (au 6= ?
?
am 6= AskRepeat) then{Q(am= AskRepeat) = ?
(1)r2 }else if (au 6= ?)
then{Q(am= AskRepeat) = ?
(2)r2 }Rule r1 specifies that, if the last user action au isequal to RequestMov(X) (i.e.
requesting the robotto execute a particular movement X), the utility as-sociated with ExecuteMov(X) is equal to the pa-rameter ?1r1 .
Similarly, the rule r2 specifies the util-ity of the clarification request AskRepeat, providedthat the last user action au is assigned to a value (i.e.is different than ?).
Two cases are distinguished inr2, depending on whether the previous system ac-tion was already AskRepeat.
This partitioning en-ables us to assign a distinct utility to the clarificationrequest if one follows the other, in order to e.g.
pe-nalise for the repeated clarification.As illustration, assume that ?
(1)r1 = 2.0, ?
(1)r2 =1.3, ?
(2)r2 = 1.1, and that the belief state contains astate variable au with the following distribution:P (au = RequestMov(LiftBothArms)) = 0.7P (au = RequestMov(LiftLeftArm)) = 0.2P (au = ?)
= 0.1The optimal system action in this case is there-fore ExecuteMov(LiftBothArms) with utility 1.4,followed by AskRepeat with utility 1.17, andExecuteMov(LiftLeftArm) with utility 0.4.1823.2 InferenceGiven a belief state b, we perform inference by con-structing a Bayesian Network corresponding to theapplication of the rules.
Algorithm 1 describes theconstruction procedure, which operates as follows:1.
We initialise the Bayesian Network with thevariables in the belief state;2.
For every rule r in the rule set, we create a con-dition node ?r and include the conditional de-pendencies with its input variables;3.
We create an effect node ?r conditioned on ?r,expressing the possible effects of the rule;4.
Finally, we create the (chance or value) nodescorresponding to the output variables of therule, as specified in the effects.Rule r2 described in the previous section wouldfor instance be translated into a condition node ?r2with 3 values (corresponding to the specified con-ditions and a default else condition if none applies)and an effect node ?r2 also containing 3 values (thetwo specified effects and a void effect associatedwith the default condition).
Figure 3 illustrates theapplication of rules r1 and r2.Once the Bayesian network is constructed,queries can be evaluated using any standard algo-rithm for exact or approximate inference.
The proce-dure is an instance of ground inference (Getoor andTaskar, 2007), since the rule structure is grounded ina standard Bayesian Network.3.3 Parameter LearningThe estimation of the rule parameters can be per-formed using a Bayesian approach by adding param-eter nodes ?
= ?1...?k to the Bayesian Network,auam?r1?r1?r2?r2?r2?r1rule r1rule r2Q(am)amFigure 3: Bayesian Network with the rules r1 and r2.and updating their distribution given a collection oftraining data.
Each data sample d is a pair (bd, td),where bd is the belief state for the specific sample,and td the target value.
The target value depends onthe model to learn ?
for learning dialogue policies,it corresponds to the selected action am.Algorithm 1 : NETWORKCONSTRUCTION (b,R)Require: b: Current belief stateRequire: R: Set of probabilistic rules1: B ?
b2: for all rule r ?
R do3: Ir ?
INPUTNODES(r)4: ?r ?
CONDITIONNODE(r)5: Add ?r and dependencies Ir ?
?r to B6: ?r ?
EFFECTNODE(r)7: Add ?r and dependency ?r ?
?r to B8: Or ?
OUTPUTNODES(r)9: for all output variable o ?
Or do10: Add/modify node o and dep.
?r ?
o to B11: end for12: end for13: return BAlgorithm 2 : PARAMETERLEARNING (R,?,D)Require: R: Set of probabilistic rulesRequire: ?
: Parameters with prior distributionRequire: D: Training sample1: for all data d ?
D do2: B ?
NETWORKCONSTRUCTION(bd,R)3: Add parameters nodes ?
to B4: for all ?i ?
?
do5: P (?
?i|d) = ?
P (td|bd, ?i) P (?i)6: end for7: end for8: return ?To estimate the parameters ?, we start from aninitial prior distribution.
Then, for each sample din the training data, we construct the correspond-ing Bayesian Network from its belief state bd andthe rules, including nodes corresponding to the un-known rule parameters.
Then, for each parameter ?i,we compute its posterior distribution given the data(Koller and Friedman, 2009):P (?
?i|d) = ?
P (td|bd, ?i) P (?i) (1)183Given the number of parameters in our example do-main and their continuous range, we used approxi-mate inference to calculate the posterior efficiently,via direct sampling from a set of parameter values.The constant ?
serves as a normalisation factor overthe sampled parameter values for ?i.
The procedureis repeated for every sample, as shown in Algorithm2.
The parameter distribution will thus progressivelynarrow down its spread to the values providing thebest fit for the training data.4 EvaluationWe evaluated our approach in the context of a dia-logue policy learning task for a human-robot inter-action scenario.
The main question we decided toaddress is the following: how much does the rulestructure contribute to the parameter estimation ofa given probabilistic model, especially for domainswith limited amounts of available data?
The objec-tive of the experiment was to learn the rule param-eters corresponding to the policy model Q(am|s)from a Wizard-of-Oz data collection.
In this partic-ular case, the parameters correspond to the utilitiesof the various actions.
The policy model used in theexperiment included a total of 14 rules.We compared our approach with two baselineswhich are essentially ?flattened?
or rolled-out ver-sions of the rule-based model.
The input and outputvariables remain identical, but they are directly con-nected, without the ?
and ?
nodes serving as inter-mediate structures.
The two baselines are (1) a plainmultinomial model and (2) a linear model of the in-put variables.
We are thus comparing three versionsof the Q(am|s) model: two baselines where am isdirectly dependent on the state variables, and our ap-proach where the dependency is realised indirectlythrough condition and effect nodes.4.1 Experimental SetupThe scenario for the Wizard-of-Oz experiment in-volved a human user and a Nao robot1 (see Figure4).
The user was instructed to teach the robot a se-quence of basic movements (lift the left arm, stepforward, kneel down, etc.)
using spoken commands.The interaction included various dialogue acts such1A programmable humanoid robot developed by AldebaranRobotics, http://www.aldebaran-robotics.com.Figure 4: Human user interacting with the Nao robot.as clarification requests, feedbacks, acknowledge-ments, corrections, etc.
Short examples of recordeddialogues are provided in the appendix.In addition to the policy model, the dialogue sys-tem include a speech recognizer (Vocon 3200 fromNuance) connected to the robot microphones, shal-low components for dialogue act recognition andgeneration, a text-to-speech module, and compo-nents for planning the robot movements and control-ling its motors in real-time.
All components are con-nected to the shared belief state, and read/write to itas they process their data flow.We collected a total of 20 interactions with 7users and one wizard playing the role of the pol-icy model, for a total of 1020 system turns, sum-ming to around 1h of interaction.
All the inter-actions were performed in English.
The wizardonly had access to the N-best list output from thespeech recogniser, and could then select which ac-tion to perform from a list of 14 alternatives (suchas AskRepeat, DemonstrateMove, UndoMove,AskForConfirmation, etc).
Each selected actionwas recorded along with the belief state (includingthe full probability distribution for every state vari-able) in effect at the time of the selection.4.2 AnalysisThe data set was split into training (75% of the sys-tem turns) and test data (remaining 25%) used tomeasure the accuracy of our policies.
The accuracyis defined as the percentage of actions correspondingto the gold standard action selected by the wizard.The parameter distributions are initialised with uni-form priors, and are progressively refined as moredata points are processed.
We calculated the accu-racy by sampling over the parameters, performinginference over the resulting models, and finally av-eraging over the inference results.18402550751000 152 304 456 608 760Accuracyontestingset(in%)Number of training samplesRule-structured modelLinear modelPlain model(a) Linear scale02550751000 2 11 47 190 760Accuracyontestingset(in%)Number of training samplesRule-structured modelLinear modelPlain model(b) Log-2 scaleFigure 5: Learning curves for the overall accuracy of the learned dialogue policy, on a held-out test set of 255 actions,depending on the size of the training sample.
The accuracy results are given for the plain, linear and rule-structuredpolicy models, using linear (left) and logarithmic scales (right).Table 1 provides the accuracy results.
The dif-ferences between our model and the baselines arestatistically significant using Bonferroni-correctedpaired t-tests, with p-value < 0.0001.
The 17% ofactions labelled as incorrect are mainly due to thehigh degree of noise in the data set, and the some-times inconsistent or unpredictable behaviour of thewizard (regarding e.g.
clarification requests).It is instructive to analyse the learning curve ofthe three models, shown in Figure 5.
Given itssmaller number of parameters, the rule-structuredmodel is able to converge to near-optimal values af-ter observing only a small fraction of the trainingset.
As the figure shows, the baseline models do alsoimprove their accuracies over time, but at a muchslower rate.
The linear model is comparatively fasterthan the plain model, but levels off towards the end,possibly due to the non-linearity of some dialoguestrategies.
The plain model continues its conver-gence and would probably reach an accuracy simi-lar to the rule-structured model if given much largeramounts of training data.
Note that since the pa-rameters are initially uniformly distributed, the ac-curacy is already non-zero before learning, since arandom assignment of parameters has a low but non-zero chance of leading to the right action.5 Discussion and Related WorkThe idea of using structural knowledge in proba-bilistic models has been explored in many direc-Type of model Accuracy (in %)Plain model 67.35Linear model 61.85Rule-structured model 82.82Table 1: Accuracy results for the three action selectionmodels on a test set, using the full training set.tions, both in the fields of decision-theoretic plan-ning and of reinforcement learning (Hauskrecht etal., 1998; Pineau, 2004; Lang and Toussaint, 2010;Otterlo, 2012) and in statistical relational learning(Jaeger, 2001; Richardson and Domingos, 2006;Getoor and Taskar, 2007).
The introduced struc-ture may be hierarchical, relational, or both.
As inour approach, most of these frameworks rely on theuse of expressive representations as templates forgrounded probabilistic models.In the dialogue management literature, moststructural approaches rely on a clear-cut task decom-position into goals and sub-goals (Allen et al, 2000;Steedman and Petrick, 2007; Bohus and Rudnicky,2009), where the completion of each goal is assumedto be fully observable, discarding any remaining un-certainty.
Information-state approaches to dialoguemanagement (Larsson and Traum, 2000; Bos et al,2003) also rely on a shared state updated accordingto a rich repository of rules, but contrary to the ap-proach presented here, these rules are generally de-terministic and do not include learnable parameters.185The literature on dialogue policy optimisationwith reinforcement learning also contains severalapproaches dedicated to dimensionality reductionfor large state-action spaces, such as function ap-proximation (Henderson et al, 2008), hierarchicalreinforcement learning (Cuaya?huitl et al, 2010) andsummary POMDPs (Young et al, 2010).
Most ofthese approaches rely on large but weakly struc-tured state spaces (generally encoded as large listsof features), which are suited for slot-filling dia-logue applications but are difficult to transfer tomore open-ended or relational domains.
The idea ofstate space partitioning, implemented here via high-level conditions, has also been explored in recent pa-pers (Williams, 2010; Crook and Lemon, 2010).
Fi-nally, Cuaya?huitl (2011) describes a closely-relatedapproach using logic-based representations of thestate-action space for relational MDPs.
His ap-proach is however based on reinforcement learningwith a user simulator, while the learning procedurepresented here relies on supervised learning from alimited data set.
He also reduced his belief stateto fully observable variables, whereas we retain thepartial observability associated with each variable.An important side benefit of structured repre-sentations in probabilistic models is their improvedreadability for human designers, who are able touse these powerful abstractions to encode their priorknowledge of the dialogue domain in the form ofpragmatic rules, generic background knowledge, ortask-specific constraints.
There has been previ-ous work on integrating expert knowledge into di-alogue policy learning, using finite-state policies orad-hoc constraints to filter a plain statistical model(Williams, 2008; Henderson et al, 2008).
The ap-proach presented in this paper is however more gen-eral since it does not rely on an external filteringmechanism but directly incorporates prior domainknowledge into the statistical model.6 ConclusionsWe showed in this paper how to represent the under-lying structure of probabilistic models for dialogueusing probabilistic rules.
These rules are defined asstructured mappings over variables of the dialoguestate, specified using high-level conditions and ef-fects.
These rules can include parameters such aseffect probabilities or action utilities.
Probabilisticrules allow the system designer to exploit power-ful generalisations in the dialogue domain specifi-cation without sacrificing the probabilistic nature ofthe model.
The framework is very general and canexpress a wide spectrum of models, from classicalmodels fully estimated from data to ones incorpo-rating rich prior knowledge.
The choice of modelwithin this spectrum is therefore essentially a designdecision dependent on the relative availabilities oftraining data and domain knowledge.We have also presented algorithms for construct-ing Bayesian Networks corresponding to the appli-cation of the rules and for estimating their parame-ters from data using Bayesian inference.
The pre-sented approach has been implemented in a spo-ken dialogue system for human-robot interaction,and validated on a policy learning task based on aWizard-of-Oz data set.
The empirical results haveshown that the rule structure enables the learning al-gorithm to converge faster and with better generali-sation performance.We are currently working on extending this ap-proach in two directions.
First, we would like to ex-tend our parameter estimation method to Bayesianmodel-based reinforcement learning.
The currentimplementation operates in a supervised learningmode, which requires expert data.
Alternatively,one could estimate the model parameters in a fullyonline fashion, without any supervisory input, byincorporating model uncertainty into the inferenceand continuously adapting the parameter distribu-tion from (real or simulated) interaction experience,using the same Bayesian approach we have outlinedin this paper (Ross et al, 2011).The second direction is the extension of our workto tasks other than action selection.
The frameworkwe have presented is not confined to dialogue pol-icy learning but can be used to structure any proba-bilistic model2.
It is therefore possible to use proba-bilistic rules as a unifying framework for all modelsdefined in a given architecture, and exploit it to per-form joint optimisation of dialogue understanding,action selection and generation.2In fact, the dialogue understanding and generation modelsused for the evaluation were already structured with probabilis-tic rules, but with fixed, hand-crafted parameters.186AcknowledgementsThe author would like to thank Stephan Oepen, ErikVelldal and Amanda Stent for useful comments onan earlier version of this paper.ReferencesJ.
Allen, D. Byron, M. Dzikovska, G. Ferguson,L.
Galescu, and A. Stent.
2000.
An architecture fora generic dialogue shell.
Natural Language Engineer-ing, 6:213?228.D.
Bohus and A. I. Rudnicky.
2009.
The RavenClawdialog management framework: Architecture and sys-tems.
Computer Speech & Language, 23:332?361.J.
Bos, E. Klein, O.
Lemon, and T. Oka.
2003.
DIPPER:Description and formalisation of an information-stateupdate dialogue system architecture.
In 4th SIGdialWorkshop on Discourse and Dialogue, pages 115?124.M.
Cavazza, R. Santos de la Camara, M. Turunen,J.
Relan?o-Gil, J. Hakulinen, N. Crook, and D. Field.2010.
How was your day?
an affective companionECA prototype.
In Proceedings of the 11th SIGDIALMeeting on Discourse and Dialogue, pages 277?280.P.
A. Crook and O.
Lemon.
2010.
Representing uncer-tainty about complex user goals in statistical dialoguesystems.
In Proceedings of the 11th SIGDIAL meetingon Discourse and Dialogue, pages 209?212.H.
Cuaya?huitl, S. Renals, O.
Lemon, and H. Shimodaira.2010.
Evaluation of a hierarchical reinforcementlearning spoken dialogue system.
Computer Speech& Language, 24:395?429.H.
Cuaya?huitl.
2011.
Learning Dialogue Agents withBayesian Relational State Representations.
In Pro-ceedings of the IJCAI Workshop on Knowledge andReasoning in Practical Dialogue Systems (IJCAI-KRPDS), Barcelona, Spain.H.
Erdogan, R. Sarikaya, Y. Gao, and M. Picheny.
2002.Semantic structured language models.
In Proceedingsof the 7th International Conference on Spoken Lan-guage Processing (ICSLP), Denver, USA.M.
Eskenazi.
2009.
An overview of spoken languagetechnology for education.
Speech Commununications,51:832?844.M.
Frampton and O.
Lemon.
2009.
Recent research ad-vances in reinforcement learning in spoken dialoguesystems.
Knowledge Engineering Review, 24(4):375?408.L.
Getoor and B. Taskar.
2007.
Introduction to StatisticalRelational Learning.
The MIT Press.M.
Hauskrecht, N. Meuleau, L. P. Kaelbling, T. Dean, andC.
Boutilier.
1998.
Hierarchical solution of markovdecision processes using macro-actions.
In Proceed-ings of the 14th Conference on Uncertainty in Artifi-cial Intelligence (UAI), pages 220?229.Y.
He and S. Young.
2005.
Semantic processing usingthe hidden vector state model.
Computer Speech &Language, 19(1):85?106.J.
Henderson, O.
Lemon, and K. Georgila.
2008.
Hybridreinforcement/supervised learning of dialogue poli-cies from fixed data sets.
Computational Linguistics,34:487?511.M.
Jaeger.
2001.
Complex probabilistic modeling withrecursive relational bayesian networks.
Annals ofMathematics and Artificial Intelligence, 32(1-4):179?220.D.
Koller and N. Friedman.
2009.
Probabilistic Graphi-cal Models: Principles and Techniques.
MIT Press.G.-J.
M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson,Hendrik Zender, and Ivana Kruijff-Korbayova?, 2010.Situated Dialogue Processing for Human-Robot Inter-action, chapter 8.
Springer Verlag, Heidelberg, Ger-many.K.
C. Lan, K. S. Ho, R. W. Pong Luk, and H. Va Leong.2008.
Dialogue act recognition using maximum en-tropy.
Journal of the American Society for InformationScience and Technology (JASIST), pages 859?874.T.
Lang and M. Toussaint.
2010.
Planning with noisyprobabilistic relational rules.
Journal of Artificial In-telligence Research, 39:1?49.S.
Larsson and D. R. Traum.
2000.
Information state anddialogue management in the TRINDI dialogue moveengine toolkit.
Natuarl Language Engineering, 6(3-4):323?340, September.M.
Lease, M. Johnson, and E. Charniak.
2006.
Rec-ognizing disfluencies in conversational speech.
IEEETransactions on Audio, Speech & Language Process-ing, 14(5):1566?1573.O.
Lemon and O. Pietquin.
2007.
Machine Learning forSpoken Dialogue Systems.
In Proceedings of the 10thEuropean Conference on Speech Communication andTechnologies (Interspeech?07), pages 2685?2688.O.
Lemon.
2011.
Learning what to say and how to sayit: Joint optimisation of spoken dialogue managementand natural language generation.
Computer Speech &Language, 25:210?221.D.
J. Litman and S. Silliman.
2004.
ITSPOKE: an in-telligent tutoring spoken dialogue system.
In Proceed-ings of the Conference of the North American Chapterof the Association of Computational Linguistics (HLT-NAACL 2004), pages 5?8.A.
Nguyen.
2005.
An agent-based approach to dialoguemanagement in personal assistants.
In Proceedings ofthe 2005 International conference on Intelligent UserInterfaces (IUI), pages 137?144.
ACM Press.187A.
Oh and A. I. Rudnicky.
2002.
Stochastic naturallanguage generation for spoken dialog systems.
Com-puter Speech & Language, 16(3-4):387?407.M.
Otterlo.
2012.
Solving relational and first-order log-ical markov decision processes: A survey.
In Rein-forcement Learning, volume 12 of Adaptation, Learn-ing, and Optimization, pages 253?292.
Springer BerlinHeidelberg.J.
Pineau.
2004.
Tractable Planning Under Uncertainty:Exploiting Structure.
Ph.D. thesis, Robotics Institute,Carnegie Mellon University, Pittsburgh, USA.M.
Richardson and P. Domingos.
2006.
Markov logicnetworks.
Machine Learning, 62:107?136.S.
Ross, J. Pineau, B. Chaib-draa, and P. Kreitmann.2011.
A Bayesian Approach for Learning and Plan-ning in Partially Observable Markov Decision Pro-cesses.
Journal of Machine Learning Research,12:1729?1770.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye, andS.
Young.
2007.
Agenda-based user simulation forbootstrapping a POMDP dialogue system.
In HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics, NAACL 2007, pages 149?152.M.
Steedman and R. P. A. Petrick.
2007.
Planning dialogactions.
In Proceedings of the 8th SIGDIAL Meetingon Discourse and Dialogue, pages 265?272, Antwerp,Belgium.A.
Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-tin, and M. Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computional Linguistics, 26:339?373.V.
Thomson and S. Young.
2010.
Bayesian updateof dialogue state: A POMDP framework for spokendialogue systems.
Computer Speech & Language,24:562?588, October.J.
D. Williams.
2008.
The best of both worlds: Unify-ing conventional dialog systems and POMDPs.
In In-ternational Conference on Speech and Language Pro-cessing (ICSLP 2008), Brisbane, Australia.J.
D. Williams.
2010.
Incremental partition recombi-nation for efficient tracking of multiple dialog states.In Proceedings of the IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP),pages 5382?5385.S.
Young, M.
Gas?ic?, S. Keizer, F. Mairesse, J. Schatz-mann, B. Thomson, and K. Yu.
2010.
The hiddeninformation state model: A practical framework forpomdp-based spoken dialogue management.
Com-puter Speech & Language, 24:150?174.H.
Zen, K. Tokuda, and A. W. Black.
2009. Review:Statistical parametric speech synthesis.
Speech Com-munications, 51:1039?1064.A Example of interactionWe present below two recorded excerpts of Wizard-of-Oz interactions.
The user utterances are simpli-fied to show only the hypothesis with highest score(indicated in parenthesis) in the N-best list.Transcript A.1 Wizard-of-Oz interaction 1HUMAN: Hi Lenny (0.74)ROBOT: Hello there!HUMAN: Move your left arm up (0.41)ROBOT: Sure!
[executing movement, waits a few seconds]ROBOT: Was that correct?HUMAN: Yes (0.88)[registers movement as confirmed]HUMAN: Move your right arm up at the sametime (0.38)ROBOT: Let?s see[executing movement]ROBOT: Was that correct?HUMAN: No (0.50)[goes back to previous position]ROBOT: What should I do then?[...
]Transcript A.2 Wizard-of-Oz interaction 2HUMAN: Stand up (0.41)ROBOT: Let?s see[executing movement]HUMAN: Move your right arm forward (0.35)[last move is implicitly grounded]ROBOT: Let?s see[executing movement]HUMAN: Lower the left arm (0.34)ROBOT: Sorry, could you repeat?HUMAN: Lift your left arm (0.37)ROBOT: Sure[executing movement, waits a few seconds]ROBOT: Should I do it like this?[...
]188
