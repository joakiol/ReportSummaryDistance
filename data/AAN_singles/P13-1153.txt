Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1558?1567,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsNamed Entity Recognition using Cross-lingual Resources: Arabic as anExampleKareem DarwishQatar Computing Research InstituteDoha, Qatarkdarwish@qf.org.qaAbstractSome languages lack large knowledge basesand good discriminative features for NameEntity Recognition (NER) that can general-ize to previously unseen named entities.
Onesuch language is Arabic, which: a) lacks acapitalization feature; and b) has relativelysmall knowledge bases, such as Wikipedia.
Inthis work we address both problems by in-corporating cross-lingual features and knowl-edge bases from English using cross-linguallinks.
We show that such features have adramatic positive effect on recall.
We showthe effectiveness of cross-lingual features andresources on a standard dataset as well ason two new test sets that cover both newsand microblogs.
On the standard dataset, weachieved a 4.1% relative improvement in F-measure over the best reported result in theliterature.
The features led to improvementsof 17.1% and 20.5% on the new news and mi-croblogs test sets respectively.1 IntroductionNamed Entity Recognition (NER) is essential for avariety of Natural Language Processing (NLP) ap-plications such as information extraction.
There hasbeen a fair amount of work on NER for a variety oflanguages including Arabic.
To train an NER sys-tem, some of the following feature types are typi-cally used (Benajiba and Rosso, 2008; Nadeau andSekine, 2009):- Orthographic features: These features includecapitalization, punctuation, existence of digits, etc.One of the most effective orthographic features iscapitalization in English, which helps NER to gener-alize to new text of different genres.
However, capi-talization is not very useful in some languages suchas German, and nonexistent in other languages suchas Arabic.
Further, even in English social media,capitalization may be inconsistent.- Contextual features: Certain words are indica-tive of the existence of named entities.
For example,the word ?said?
is often preceded by a named en-tity of type ?person?
or ?organization?.
Sequencelabeling algorithms (ex.
Conditional Random Fields(CRF)) can often identify such indicative words.- Character-level features: These features typ-ically include the leading and trailing letters ofwords.
In some languages, these letters could pre-fixes and suffixes.
Such features can be indicative orcounter-indicative of the existence of named entities.For example, a word ending with ?ing?
is typicallynot a named entity, while a word ending in ?berg?
isoften a named entity.- Part-of-speech (POS) tags and morphologicalfeatures: POS tags indicate (or counter-indicate) thepossible presence of a named entity at word level orat word sequence level.
Morphological features canmostly indicate the absence of named entities.
Forexample, Arabic allows the attachment of pronounsto nouns and verbs.
However, pronouns are rarelyever attached to named entities.- Gazetteers: This feature checks the presence ofa word or a sequence of words in large lists of namedentities.
If gazetteers are small, then they wouldhave low coverage, and if they are very large thentheir entries may be ambiguous.
For example, ?syn-tax?
may refer to sentence construction or the musicband ?Syntax?.Typically, a subset of these features are availablefor different languages.
For example, morpholog-ical, contextual, and character-level features havebeen shown to be effective for Arabic NER (Bena-jiba and Rosso, 2008).
However, Arabic lacks in-dicative orthographic features that generalize to pre-viously unseen named entities.
Also, although some1558of the Arabic gazetteers that were used for NERwere small (Benajiba and Rosso, 2008), there hasbeen efforts to build larger Arabic gazetteers (Attiaet al, 2010).
Since training and test parts of stan-dard datasets for Arabic NER are drawn from thesame genre in relatively close temporal proximity,a named entity recognizer that simply memorizesnamed entities in the training set generally performswell on such test sets.
Thus, the results that are re-ported in the literature are generally high (Abdul-Hamid and Darwish, 2010; Benajiba et al, 2008).We illustrate the limited capacity of existing recog-nizers to generalize to previously unseen named en-tities using two new test sets that include microblogsas well as news texts that cover local and interna-tional politics, economics, health, sports, entertain-ment, and science.
As we will show later, recall iswell below 50% for all named entity types on thenew test sets.To address this problem, we introduce the useof cross-lingual links between a disadvantaged lan-guage, Arabic, and a language with good discrim-inative features and large resources, English, toimprove Arabic NER.
We exploit English?s ortho-graphic features, particularly capitalization, as wellas Arabic and English Wikipedias, including exist-ing annotations from large knowledge sources suchas DBpedia.
We also show how to use transliter-ation mining to improve NER, even when neitherlanguage has a capitalization (or similar) feature.The intuition is that if the translation of a word isin fact a transliteration, then the word is likely anamed entity.
Cross-lingual links are obtained usingWikipedia cross-language links and a large MachineTranslation (MT) phrase table that is true cased,where word casing is preserved during training.
Weshow the effectiveness of these new features on astandard dataset as well as two new test sets.
Thecontributions of this paper are as follows:- Using cross-lingual links to exploit orthographicfeatures in other languages.- Employing transliteration mining to improve NER.- Using cross-lingual links to exploit a large knowl-edge base, namely English DBpedia, to benefitNER.- Introducing two new NER test sets for Arabic thatinclude recent news as well as microblogs.
We planto release these test sets.- Improving over the best reported results in the liter-ature by 4.1% (Abdul-Hamid and Darwish, 2010) bystrictly adding cross-lingual features.
We also showimprovements of 17.1% and 20.5% on the new testsets.The remainder of the paper is organized as fol-lows: Section 2 provides related work; Section 3 de-scribes the baseline system; Section 4 introduces thecross-lingual features and reports on their effective-ness; and Section 5 concludes the paper.2 Related Work2.1 Using cross-lingual FeaturesFor many NLP tasks, some languages may have sig-nificantly more training data, better knowledge re-sources, or more discriminating features than otherlanguages.
If cross-lingual resources are available,such as parallel data, increased training data, betterresources, or superior features can be used to im-prove the processing (ex.
tagging) for other lan-guages (Ganchev et al, 2009; Shi et al, 2010;Yarowsky and Ngai, 2001).
Some work has at-tempted to use bilingual features in NER.
Burkettet al (2010) used bilingual text to improve mono-lingual models including NER models for German,which lacks a good capitalization feature.
They didso by training a bilingual model and then generat-ing more training data from unlabeled parallel data.They showed significant improvement in GermanNER effectiveness, particularly for recall.
In ourwork, there is no need for tagged text that has aparallel equivalent in another language.
Benajiba etal.
(2008) used an Arabic English dictionary fromMADA, an Arabic analyzer, to indicate if a wordis capitalized in English or not.
They reported thatit was the second most discriminating feature thatthey used.
However, there seems to be room for im-provement because: (1) MADA?s dictionary is rela-tively small and would have low coverage; and (2)the use of such a binary feature is problematic, be-cause Arabic names are often common Arabic wordsand hence a word may be translated as a named en-tity and as a common word.
To overcome these twoproblems, we use cross-lingual features to improveNER using large bilingual resources, and we incor-porate confidences to avoid having a binary feature.Richman and Schone (2008) used English linguis-1559tic tools and cross language links in Wikipedia toautomatically annotate text in different languages.Transliteration Mining (TM) has been used to en-rich MT phrase tables or to improve cross languagesearch (Udupa et al, 2009).
Conversely, people haveused NER to determine if a word is to be transliter-ated or not (Hermjakob et al, 2008).
However, weare not aware of any prior work on using TM to de-termine if a sequence is a NE.
Further, we are notaware of prior work on using TM (or transliterationin general) as a cross lingual feature in any annota-tion task.
In our work, we use state-of-the-art TM asdescribed by El-Kahki et al (2011)2.2 Arabic NERMuch work has been done on NER with mul-tiple public evaluation forums.
Nadeau andSekine (Nadeau and Sekine, 2009) surveyed lots ofwork on NER for a variety of languages.
Signifi-cant work has been conducted by Benajiba and col-leagues on Arabic NER (Benajiba and Rosso, 2008;Benajiba et al, 2008; Benajiba and Rosso, 2007;Benajiba et al, 2007).
Benajiba et al (2007) useda maximum entropy classifier trained on a featureset that includes the use of gazetteers and a stop-word list, appearance of a NE in the training set,leading and trailing word bigrams, and the tag of theprevious word.
They reported 80%, 37%, and 47%F-measure for locations, organizations, and personsrespectively on the ANERCORP dataset that theycreated and publicly released.
Benajiba and Rosso(2007) improved their system by incorporating POStags to improve NE boundary detection.
They re-ported 87%, 46%, and 52% F-measure for loca-tions, organizations, and persons respectively.
Be-najiba and Rosso (2008) used CRF sequence label-ing and incorporated many language specific fea-tures, namely POS tagging, base-phrase chunking,Arabic tokenization, and adjectives indicating na-tionality.
They reported that tokenization generallyimproved recall.
Using POS tagging generally im-proved recall at the expense of precision, leadingto overall improvements in F-measure.
Using alltheir suggested features, they reported 90%, 66%,and 73% F-measure for location, organization, andpersons respectively.
In Benajiba et al (2008),they examined the same feature set on the Auto-matic Content Extraction (ACE) datasets using CRFsequence labeling and a Support Vector Machine(SVM) classifier.
They did not report per categoryF-measure, but they reported overall 81%, 75%, and78% macro-average F-measure for broadcast newsand newswire on the ACE 2003, 2004, and 2005datasets respectively.
Huang (2005) used an HMM-based NE recognizer for Arabic and reported 77%F-measure on the ACE 2003 dataset.
Farber etal.
(2008) used POS tags obtained from an Ara-bic tagger to enhance NER.
They reported 70% F-measure on the ACE 2005 dataset.
Shaalan andRaza (2007) reported on a rule-based system thatuses hand crafted grammars and regular expressionsin conjunction with gazetteers.
They reported up-wards of 93% F-measure, but they conducted theirexperiments on non-standard datasets, making com-parison difficult.
Abdul-Hamid and Darwish (2010)used a simplified feature set that relied primarily oncharacter level features, namely leading and trailingletters in a word.
They also experimented with avariety of phrase level features with little success.They reported an F-measure of 76% and 81% forthe ACE2005 and the ANERCorp datasets datasetsrespectively.
We used their simplified features in ourbaseline system.
The different experiments reportedin the literature may not have been done on the sametraining/test splits.
Thus, the results may not becompletely comparable.
Mohit et al (2012) per-formed NER on a different genre from news, namelyArabic Wikipedia articles, and reported recall valuesas low as 35.6%.
They used self training and recalloriented classification to improve recall, typically atthe expense of precision.
McNamee and Mayfield(2002) and Mayfield et al (2003) used thousandsof language independent features such as charactern-grams, capitalization, word length, and position ina sentence, along with language dependent featuressuch as POS tags and BP chunking.
The use of CRFsequence labeling for NER has shown success (Mc-Callum and Li, 2003; Nadeau and Sekine, 2009; Be-najiba and Rosso, 2008).3 Baseline Arabic NER SystemFor the baseline system, we used the CRF++1 im-plementation of CRF sequence labeling with defaultparameters.
We opted to reimplement the most suc-1http://code.google.com/p/crfpp/1560cessful features that were reported by Benajiba etal.
(2008) and Abdul-Hamid and Darwish (2010),namely the leading and trailing 1, 2, 3, and 4 lettersin a word; whether a word appears in the gazetteerthat was created by Benajiba et al (2008), whichis publicly available, but is rather small (less than5,000 entries); and the stemmed form of words (afterremoving coordinating conjunctions, prepositions,and determiners using a rule-based stemmer akinto (Larkey et al, 2002)).
As mentioned earlier, theleading and trailing letters in a word may indicateor counter-indicate the presence of named entities.Stemming is important due to the morphologicalcomplexity of Arabic.
We used the previous andthe next words in their raw and stemmed forms asfeatures.
For training and testing, we used the AN-ERCORP dataset (Benajiba and Rosso, 2007).
Thedataset has approximately 150k tokens and we usedthe 80/20 training/test splits of Abdul-Hamid andDarwish (2010), who graciously provided us withtheir splits of the collection and they achieved thebest reported results on the dataset.
We will re-fer to their results, which are provided in Table 1,as ?baseline-lit?.
Table 2 (a) shows our results onthe ANERCORP dataset.
Our results were slightlylower than their results (Abdul-Hamid and Darwish,2010).
It is noteworthy that 69% of the named enti-ties in the test part were seen during training.We also created two new test sets.
The first testset is composed of news snippets from the RSS feedof the Arabic (Egypt) version of news.google.comfrom Oct. 6, 2012.
The RSS feed contains theheadline and the first 50-100 words in the news ar-ticles.
The set has news from over a dozen differ-ent news sources and covers international and localnews, politics, financial news, health, sports, enter-tainment, and technology.
This set contains roughly15k tokens.
The second set contains a set of 1,423tweets that were randomly selected from tweets au-thored between November 23, 2011 and Novem-ber 27, 2011.
We scraped tweets from Twitter us-ing the query ?lang:ar?
(language=Arabic).
This setcontains approximately 26k tokens.
The test setswill be henceforth be referred to as the NEWS andTWEETS sets respectively.
They were annotated byone person, a native Arabic speaker, using the Lin-guistics Data Consortium tagging guidelines.
Ta-ble 2 (b) and (c) report on the results for the baselinesystem on both test sets.
The results on the NEWStest are substantially lower than those for ANER-CORP.
It is worth noting that only 27% of the namedentities in the NEWS test set were observed in thetraining set (compared to 69% for ANERCORP).
AsTable 3 shows for the ANERCORP dataset, usingonly the tokens as features, where the labeler mainlymemorizes previously seen named entities, yieldshigher results than the baseline results for the NEWSdataset (Table 2 (b)).
The results on the TWEETStest are very poor, with 24% of the named entities inthe test set appearing in the training set.ANERCORP DatasetPrecision Recall F?=1LOC 93 83 88ORG 84 64 73PERS 90 75 82Overall 89 74 81Table 1: ?Baseline-lit?
Results from (Abdul-Hamid andDarwish, 2010)(a) ANERCORP DatasetPrecision Recall F?=1LOC 93.6 83.3 88.1ORG 83.8 61.2 70.8PERS 84.3 64.4 73.0Overall 88.9 72.5 79.9(b) NEWS Test SetPrecision Recall F?=1LOC 84.1 53.2 65.1ORG 73.2 23.2 35.2PERS 74.8 47.1 57.8Overall 78.0 41.9 54.6(c) TWEETS Test SetPrecision Recall F?=1LOC 79.9 27.1 40.4ORG 44.4 9.1 15.1PERS 45.7 27.8 34.5Overall 58.0 23.1 33.1Table 2: Baseline Results for the three test setsANERCORP DatasetPrecision Recall F?=1LOC 95.3 62.7 75.6ORG 86.3 44.7 58.9PERS 85.4 36.4 51.0Overall 91.0 50.0 64.5Table 3: Results of using only tokens as features on AN-ERCORP15614 Cross-lingual FeaturesWe experimented with three different cross-lingualfeatures that used Arabic and English Wikipediacross-language links and a true-cased phrase ta-ble that was generated using Moses (Koehn et al,2007).
True-casing preserves case information dur-ing training.
We used the Arabic Wikipedia snap-shot from September 28, 2012.
The snapshot has348,873 titles including redirects, which are alter-native names to articles.
Of these articles, 254,145have cross-lingual links to English Wikipedia.
Weused DBpedia 3.8 which includes 6,157,591 entriesof Wikipedia titles and their ?types?, such as ?per-son?, ?plant?, or ?device?, where a title can havemultiple types.
The phrase table was trained on a setof 3.69 million parallel sentences containing 123.4million English tokens.
The sentences were drawnfrom the UN parallel data along with a variety ofparallel news data from LDC and the GALE project.The Arabic side was stemmed (by removing just pre-fixes) using the Stanford word segmenter (Green andDeNero, 2012).4.1 Cross-lingual CapitalizationAs we mentioned earlier, Arabic lacks capitalizationand Arabic names are often common Arabic words.For example, the Arabic name ?Hasan?
means good.To capture cross-lingual capitalization, we used theaforementioned true-cased phrase table at word andphrase levels as follows:Input: True-cased phrase table PT , sentence S containing n wordsw0..n, max sequence length l, translations T1..k..m of wi..jfor i = 0?
n doj = min(i + l ?
1, n)if PT contains wi..j & ?
Tk isCaps thenweight(wi..j) =?Tk isCapsP (Tk)?Tk isCapsP (Tk)+?Tk notCapsP (Tk)round weight(wi..j) to first significant figureset tag of wi = B-CAPS-weightset tag for words wi+1..j = I-CAPS-weightelseif j > i thenj- -elsetag of wi = nullend ifend ifend forWhere: PT was the aforementioned phrase ta-ble; l = 4; P (Tk) equaled to the product ofp(source|target) and p(target|source) for a wordsequence; isCaps and notCaps were whether thetranslation was capitalized or not respectively; andthe weights were binned because CRF++ only takesnominal features.
In essence we tried every subse-quence of S of length l or less to see if the translationwas capitalized.
A subsequence can be 1 word long.We tried longer sequences first.
To determine if thecorresponding phrase was capitalized (isCaps), allnon-function words on the English side needed to becapitalized.
As an example, the phrase ?XA??
@ ?Jj??
@(meaning ?Pacific Ocean?)
was translated to a cap-italized phrase 36.7% of the time.
Thus, the word?Jj??
@ was assigned B-CAPS-0.4 and ?XA??
@ wasassigned I-CAPS-0.4.
Using weights avoids usingcapitalization as a binary feature.Table 4 reports on the results of the baselinesystem with the capitalization feature on the threedatasets.
In comparing baseline results in Table 2and cross-lingual capitalization results in Table 4,recall consistently increased for all datasets, par-ticularly for ?persons?
and ?locations?.
For thedifferent test sets, recall increased by 3.1 to 6.1points (absolute) or by 8.4% to 13.6% (relative).This led to an overall improvement in F-measure of1.8 to 3.4 points (absolute) or 4.2% to 5.7% (rela-tive).
Precision dropped overall on the ANERCORPdataset and dropped substantially for the NEWS andTWEETS test sets.
(a) ANERCORP DatasetPrecision Recall F?=1LOC 92.0/-1.6/-1.7 86.8/3.5/4.2 89.3/1.2/1.4ORG 82.8/-1.1/-1.3 63.9/2.7/4.4 72.1/1.4/1.9PERS 86.0/1.7/2.0 75.4/11.0/17.1 80.3/7.3/10.1Overall 88.4/-0.4/-0.5 78.6/6.1/8.4 83.2/3.4/4.2(b) NEWS Test SetPrecision Recall F?=1LOC 82.1/-2.0/-2.4 59.0/5.8/11.0 68.7/3.5/5.4ORG 68.4/-4.9/-6.6 23.2/0.0/0.0 34.6/-0.6/-1.7PERS 70.7/-4.0/-5.4 55.6/8.4/17.9 62.2/4.4/7.6Overall 74.5/-3.5/-4.5 47.0/5.1/12.2 57.7/3.1/5.7(c) TWEETS Test SetPrecision Recall F?=1LOC 76.9/-3.0/-3.7 27.9/0.9/3.2 41.0/0.5/1.4ORG 44.4/0.0/0.0 10.4/1.3/14.3 16.8/1.8/11.6PERS 40.0/-5.7/-12.5 35.0/7.3/26.2 37.3/2.8/8.1Overall 51.8/-6.2/-10.7 26.3/3.1/13.6 34.9/1.8/5.4Table 4: Results with cross-lingual capitalization with/absolute/relative differences compared to baseline15624.2 Transliteration MiningAn alternative to capitalization can be translitera-tion mining.
The intuition is that named entities areoften transliterated, particularly the names of loca-tions and persons.
This feature is helpful if cross-lingual resources do not have capitalization infor-mation, or if the ?helper?
language to be consulteddoes not have a useful capitalization feature.
We per-formed transliteration mining (aka cognate match-ing) at word level for each Arabic word against allits possible translations in the phrase table.
Weused a transliteration miner akin to that of El-Kahkiet al (2011) that was trained using 3,452 parallelArabic-English transliteration pairs.
We aligned theword-pairs at character level using GIZA++ and thephrase extractor and scorer from the Moses machinetranslation package (Koehn et al, 2007).
The align-ment produced mappings between English letters se-quences and Arabic letter sequences with associatedmapping probabilities.
Given an Arabic word, weproduced all its possible segmentations along withtheir associated mappings into English letters.
Weretained valid target sequences that produced trans-lations in the phrase table.Again we used a weight similar to the one forcross-lingual capitalization and we rounded the val-ues of the ratio the significant figure.
The weightswere computed as:?Tk isTransliterationP (Tk)?Tk isTransliterationP (Tk) +?Tk notTransliterationP (Tk)(1)where P (Tk) is probability of the kth translation ofa word in the phrase table.If a word was not found in the phrase table, thefeature value was assigned null.
For example, if thetranslations of the word 	?
?k are ?Hasan?, ?Has-san?, and ?good?, where the first two are transliter-ations and the last not, then the weight of the wordwould be:P (Hasan| 	?
?k) + P (Hassan| 	?
?k)P (Hasan| 	?
?k) + P (Hassan| 	?
?k) + P (good| 	?
?k)(2)In our experiments, the weight of 	?
?k was equalto 0.5 (after rounding).
Table 5 reports on the re-sults using the baseline system with the transliter-ation mining feature.
Like the capitalization fea-ture, transliteration mining slightly lowered preci-sion ?
except for the TWEETS test set where thedrop in precision was significant ?
and positivelyincreased recall, leading to an overall improvementin F-measure for all test sets.
Overall, F-measureimproved by 1.9%, 3.7%, and 3.9% (relative) com-pared to the baseline for the ANERCORP, NEWS,and TWEETS test sets respectively.
The similarityof results between using transliteration mining andword-level cross-lingual capitalization suggests thatperhaps they can serve as surrogates.4.3 Using DBpediaDBpedia2 is a large collaboratively-built knowledgebase in which structured information is extractedfrom Wikipedia (Bizer et al, 2009).
DBpedia 3.8,the release we used in this paper, contains 6,157,591Wikipedia titles belonging to 296 types.
Types varyin granularity with each Wikipedia title having oneor more type.
For example, NASA is assigned thefollowing types: Agent, Organization, and Govern-mentAgency.
In all, DBpedia includes the names of764k persons, 573k locations, and 192k organiza-tions.
Of the Arabic Wikipedia titles, 254,145 haveWikipedia cross-lingual links to English Wikipedia,and of those English Wikipedia titles, 185,531 haveentries in DBpedia.
Since Wikipedia titles may havemultiple DBpedia types, we opted to keep the mostpopular type (by count of how many Wikipedia ti-tles are assigned a particular type) for each title, andwe disregarded the rest.
We also chose not to usethe ?Agent?
and ?Work?
types because they werehighly ambiguous.
We found word sequences inthe manner described in the pseudocode for cross-lingual capitalization.
For translation, we gener-ated two features using two translation resources,namely the aforementioned phrase table and Arabic-English Wikipedia cross-lingual links.
When usingthe phrase table, we used the most likely transla-tion into English that matches an entry in DBpediaprovided that the product of p(source|target) andp(target|source) of translation was above 10?5.We chose the threshold qualitatively using offlineexperiments.
When using Arabic-English Wikipediacross-lingual links, if an entry was found in theArabic Wikipedia, we performed a look up in DB-2http://dbpedia.org1563pedia using the English Wikipedia title that corre-sponds to the Arabic Wikipedia title.
We used Ara-bic Wikipedia page-redirects to improve coverage.For both features (using the two translation meth-ods), for an Arabic word sequence corresponding tothe DBpedia entry, the first word in the sequencewas assigned the feature ?B-?
plus the DBpediatype and subsequent words were assigned the fea-ture ?I-?
plus the DBpedia type.
For example, for???
@ H.Qk (meaning ?Hezbollah?
), the words H. 	Qkand ???
@ were assigned ?B-Organization?
and ?I-Organization?
respectively.
For all other words, thefeature was assigned ?null?.
Using the phrase ta-ble for translation likely yielded improved coverageover using Wikipedia cross-lingual links.
However,Wikipedia cross-lingual links likely led to higherquality translations, because they were manually cu-rated.
Table 6 reports on the results of using thebaseline system with the two DBpedia features.
Us-ing DBpedia consistently improved precision and re-call for named entity types on all test sets, exceptfor a small drop in precision for locations on theANERCORP dataset and for locations and personson the TWEETS test set.
For the different test sets,improvements in recall ranged between 4.4 and 7.5points (absolute) or 6.5% and 19.1% (relative).
Pre-cision improved by 0.9 and 5.5 points (absolute) or1.0% and 7.1% (relative) for the ANERCORP andNEWS test sets respectively.
Overall improvementin F-measure ranged between 3.2 and 7.6 points (ab-solute) or 4.1% and 13.9% (relative).4.4 Putting it All TogetherTable 7 reports on the results of using all aforemen-tioned cross-lingual features together.
Figures 1, 2,and 3 compare the results of the different setups.
Asthe results show, the impact of cross-lingual featureson recall were much more pronounced on the NEWSand TWEETS test sets ?
compared to the ANER-CORP dataset.
Further, the recall values for the AN-ERCORP dataset in the baseline experiments weremuch higher than those for the two other test sets.This confirms our suspicion that the reported valuesin the literature on the standard datasets are unrealis-tically high due to the similarity between the trainingand test sets.
Hence, these high effectiveness resultsmay not generalize to other test sets.
Of all the cross-(a) ANERCORP DatasetPrecision Recall F?=1LOC 92.9/-0.7/-0.7 83.5/0.2/0.3 88.0/-0.2/-0.2ORG 82.9/-0.9/-1.0 61.8/0.6/1.0 70.9/0.1/0.1PERS 84.5/0.3/0.3 71.9/7.5/11.7 77.7/4.7/6.5Overall 88.3/-0.5/-0.6 75.5/2.9/4.1 81.4/1.5/1.9(b) NEWS Test SetPrecision Recall F?=1LOC 84.9/0.7/0.9 53.6/0.5/0.9 65.7/0.6/0.9ORG 67.2/-6.1/-8.3 22.9/-0.3/-1.1 34.2/-1.0/-2.9PERS 72.8/-1.9/-2.6 55.0/7.8/16.7 62.7/4.8/8.4Overall 75.9/-2.1/-2.6 45.0/3.1/7.4 56.6/2.0/3.7(c) TWEETS Test SetPrecision Recall F?=1LOC 79.1/-0.8/-1.0 27.1/0.0/0.0 40.3/-0.1/-0.3ORG 41.8/-2.7/-6.0 9.1/0.0/0.0 14.9/-0.2/-1.1PERS 40.0/-5.7/-12.5 35.5/7.7/27.8 37.6/3.1/8.8Overall 51.7/-6.3/-10.9 25.8/2.6/11.3 34.4/1.3/3.9Table 5: Results with transliteration mining with /abso-lute/relative differences compared to baseline(a) ANERCORP DatasetPrecision Recall F?=1LOC 92.7/-0.9/-0.9 87.1/3.9/4.6 89.9/1.7/1.9ORG 84.6/0.8/0.9 66.6/5.3/8.7 74.5/3.7/5.3PERS 87.8/3.6/4.2 69.9/5.5/8.6 77.8/4.8/6.6Overall 89.8/0.9/1.0 77.2/4.7/6.5 83.0/3.2/4.0(b) NEWS Test SetPrecision Recall F?=1LOC 87.8/3.6/4.3 61.8/8.6/16.2 72.5/7.4/11.3ORG 76.1/2.9/3.9 30.2/7.0/30.1 43.2/8.0/22.7PERS 83.2/8.5/11.3 54.2/7.1/15.0 65.7/7.8/13.6Overall 83.5/5.5/7.1 49.5/7.5/18.0 62.2/7.6/13.9(c) TWEETS Test SetPrecision Recall F?=1LOC 77.4/-2.5/-3.1 30.5/3.5/12.9 43.8/3.4/8.4ORG 57.0/12.5/28.2 15.9/6.8/75.1 24.8/9.8/64.9PERS 40.8/-4.9/-10.6 31.7/4.0/14.3 35.7/1.2/3.4Overall 55.3/-2.6/-4.5 27.5/4.4/19.1 36.8/3.7/11.2Table 6: Results using DBpedia with /absolute/relativedifferences compared to baselinelingual features that we experimented with, the useof DBpedia led to improvements in both precisionand recall (except for precision on the TWEETS testset).
Other cross-lingual features yielded overall im-provements in F-measure, mostly due to gains in re-call, typically at the expense of precision.
Overall,F-measure improved by 5.5%, 17.1%, and 20.5%(relative) compared to the baseline for the ANER-CORP, NEWS, and TWEETS test sets respectively.For the ANERCORP test set, our results improvedover the baseline-lit results (Abdul-Hamid and Dar-wish, 2010) by 4.1% (relative).1564Figure 1: ANERCORP Dataset ResultsFigure 2: NEWS Test Set ResultsWhen using all the features together, one notableresult is that precision dropped significantly for theTWEETS test sets.
We examined the output for theTWEETS test set and here are some of the factorsthat affected precision:- the presence of words that would typically benamed entities in news but would generally be reg-ular words in tweets.
For example, the Arabic word?Mubarak?
is most likely the name of the formerEgyptian president in the context of news, but itwould most likely mean ?blessed?, which is com-mon in expressions of congratulations, in tweets.- the use of dialectic words that may have transliter-ations or a named entity as the most likely transla-tion into English.
For example, the word ???
is typ-ically the dialectic version of the Arabic word Z?
??,meaning something.
However, since the MT sys-tem that we used was trained on modern standardArabic, the dialectic word would not appear in train-ing and would typically be translated/transliteratedto the name ?Che?
(as in Che Guevara).- Since tweets are restricted in length, authors fre-quently use shortened versions of named entities.For example, tweets would mostly have ?Morsi?instead of ?Mohamed Morsi?
and without triggerwords such as ?Dr.?
or ?president?.
The full ver-sion of a name and trigger words are more com-Figure 3: TWEETS Test Set Results(a) ANERCORP DatasetPrecision Recall F?=1LOC 92.3/-1.3/-1.4 87.8/4.6/5.5 90.0/1.9/2.1ORG 81.4/-2.4/-2.9 66.0/4.7/7.7 72.9/2.1/3.0PERS 87.0/2.8/3.3 77.7/13.3/20.7 82.1/9.1/12.5Overall 88.7/-0.2/-0.2 80.3/7.8/10.7 84.3/4.4/5.5(b) NEWS Test SetPrecision Recall F?=1LOC 85.1/1.0/1.2 64.1/11.0/20.6 73.1/8.0/12.3ORG 73.8/0.5/0.7 29.4/6.2/26.9 42.1/6.8/19.4PERS 76.8/2.0/2.7 63.4/16.3/34.5 69.5/11.7/20.2Overall 79.2/1.2/1.6 53.6/11.6/27.7 63.9/9.4/17.1(c) TWEETS Test SetPrecision Recall F?=1LOC 81.4/1.5/1.8 33.5/6.5/23.9 47.5/7.1/17.4ORG 52.1/7.6/17.2 16.2/7.1/78.6 24.7/9.6/64.1PERS 40.5/-5.2/-11.4 39.2/11.5/41.3 39.8/5.3/15.4Overall 54.4/-3.6/-6.2 31.4/8.3/35.9 39.9/6.8/20.5Table 7: Results using all the cross-lingual features with/absolute/relative differences compared to baselinemon in news.
This same problem was present in theNEWS test set, because it was constructed from anRSS feed, and headlines, which are typically com-pact, had a higher representation in the test collec-tion.
We observed the same phenomenon for orga-nization names.
For example, ?the Real?
refers to?Real Madrid?.
Nicknames are also prevalent.
Forexample, ?the Land of the two Sanctuaries?
refers to?Saudi Arabia?.We believe that this problem can be overcome byintroducing new training data that include tweets (orother social text) and performing domain adaptation.New training data would help: identify words andexpressions that are common in conversations, ac-count for common dialectic words, and learn a bet-ter word transition model.
Further, gazetteers thatcover shortened versions of names could be helpfulas well.15655 ConclusionIn this paper, we presented different cross-lingualfeatures that can make use of linguistic propertiesand knowledge bases of other languages for NER.For translation, we used an MT phrase table andWikipedia cross-lingual links.
We used Englishas the ?helper?
language and we exploited the En-glish capitalization feature and an English knowl-edge base, DBpedia.
If the helper language didnot have capitalization, then transliteration miningcould provide some of the benefit of capitalization.Transliteration mining requires limited amounts oftraining examples.
We believe that the proposedcross-lingual features can be used to help NER forother languages, particularly languages that lackgood features that generalize well.
For Arabic NER,the new features yielded an improvement of 5.5%over a strong baseline system on a standard dataset,with 10.7% gain in recall and negligible change inprecision.
We tested on a new news test set, NEWS,which has recent news articles (the same genre asthe standard dataset), and indeed NER effective-ness was much lower.
For the new NEWS test set,cross-lingual features led to a small increase in pre-cision (1.6%) and a very large improvement in re-call (27.7%).
This led to a 17.1% improvementin overall F-measure.
We also tested NER on theTWEETS test set, where we observed substantialimprovements in recall (35.9%).
However, precisiondropped by 6.2% for the reasons we mentioned ear-lier.
For future work, it would be interesting to applycross-lingual features to other language pairs and tomake use of joint cross-lingual models.
Further, wealso plan to investigate Arabic NER on social media,particularly microblogs.ReferencesA.
Abdul-Hamid and K. Darwish.
2010.
Simplified Fea-ture Set for Arabic Named Entity Recognition.
Pro-ceedings of the 2010 Named Entities Workshop, ACL2010, pages 110115.Mohammed Attia, Antonio Toral, Lamia Tounsi, Mon-ica Monachini, and Josef van Genabith.
2010.
An au-tomatically built named entity lexicon for Arabic.
In:LREC 2010 - 7th conference on International Lan-guage Resources and Evaluation, 17-23 May 2010,Valletta, Malta.Y.
Benajiba, M. Diab, and P. Rosso.
2008.
Arabic NamedEntity Recognition using Optimized Feature Sets.
Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 284293,Honolulu, October 2008.Y.
Benajiba and P. Rosso.
2008.
Arabic Named En-tity Recognition using Conditional Random Fields.
InProc.
of Workshop on HLT & NLP within the ArabicWorld, LREC08.Y.
Benajiba, P. Rosso and J. M. Benedi.
2007.
ANER-sys: An Arabic Named Entity Recognition systembased on Maximum Entropy.
In Proc.
of CICLing-2007, Springer-Verlag, LNCS(4394), pp.143-153Y.
Benajiba and P. Rosso.
2007.
ANERsys 2.0: Con-quering the NER task for the Arabic language bycombining the Maximum Entropy with POS-tag infor-mation.
In Proc.
of Workshop on Natural Language-Independent Engineering, IICAI-2007.Christian Bizer, Jens Lehmann, Georgi Kobilarov, SrenAuer, Christian Becker, Richard Cyganiak, SebastianHellmann.
2009.
DBpedia A Crystallization Point forthe Web of Data.
Journal of Web Semantics: Science,Services and Agents on the World Wide Web, Issue 7,Pages 154165, 2009.D.
Burkett, S. Petrov, J. Blitzer, D. Klein.
2010.
LearningBetter Monolingual Models with Unannotated Bilin-gual Text.
Proceedings of the Fourteenth Conferenceon Computational Natural Language Learning, pages46?54.A.
El Kahki, K. Darwish, A. Saad El Din, M. Abd El-Wahab and A. Hefny.
2011.
Improved TransliterationMining Using Graph Reinforcement.
EMNLP-2011.B.
Farber, D. Freitag, N. Habash, and O. Rambow.
2008.Improving NER in Arabic Using a Morphological Tag-ger.
In Proc.
of LREC08.K.
Ganchev, J. Gillenwater, and B. Taskar.
2009.
Depen-dency grammar induction via bitext projection con-straints.
In ACL-2009.Spence Green and John DeNero.
2012.
A Class-BasedAgreement Model for Generating Accurately InflectedTranslations.
In ACL-2012.Ulf Hermjakob, Kevin Knight, and Hal Daum III.
2008.Name translation in statistical machine translation:Learning when to transliterate.
ACL-08: HLT, Pages389-397.F.
Huang.
2005.
Multilingual Named Entity Extractionand Translation from Text and Speech.
Ph.D. Thesis.Pittsburgh: Carnegie Mellon University.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, Evan Herbst, Moses: Open Source Toolkit1566for Statistical Machine Translation, Annual Meeting ofthe Association for Computational Linguistics (ACL),demonstration session, Prague, Czech Republic, June2007.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data, In Proc.
of ICML,pp.282-289, 2001.Leah S. Larkey, Lisa Ballesteros, and Margaret E. Con-nell.
2002.
Improving stemming for Arabic informa-tion retrieval: light stemming and co-occurrence anal-ysis.
SIGIR-2002.J.
Mayfield, P. McNamee, and C. Piatko.
2003.NamedEntity Recognition using Hundreds of Thousands ofFeatures.
HLT-NAACL 2003-Volume 4, 2003.A.
McCallum and W. Li.
2003.
Early Results for NamedEntity Recognition with Conditional Random Fields,Features Induction and Web-Enhanced Lexicons.
InProc.
Conference on Computational Natural LanguageLearning.P.
McNamee and J. Mayfield.
2002.
Entity extractionwithout language-specific.
Proceedings of CoNLL,.2002Behrang Mohit, Nathan Schneider, Rishav Bhowmick,Kemal Oflazer, Noah A. Smith.
2012.
Recall-orientedlearning of named entities in Arabic Wikipedia.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL 2012), pp.
162-173.
2012.D.
Nadeau and S. Sekine.
2009.
A Survey of Named En-tity Recognition and Classification.
Named Entities:Recognition, Classification and Use, ed.
S. Sekine andE.
Ranchhod, John Benjamins Publishing Company.Alexander E. Richman and Patrick Schone.
2008.
Miningwiki resources for multilingual named entity recogni-tion.
Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies.
2008.K.
Shaalan and H. Raza.
2007.
Person Name EntityRecognition for Arabic.
Proceedings of the 5th Work-shop on Important Unresolved Matters, pages 1724,Prague, Czech Republic, June 2007.L.
Shi, R. Mihalcea, M. Tian.
2010.
Cross LanguageText Classification by Model Translation and Semi-supervised Learning.
Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP 2010.Raghavendra Udupa, Anton Bakalov, and Abhijit Bhole.2009.
They Are Out There, If You Know Where toLook: Mining Transliterations of OOV Query Termsfor Cross-Language Information Retrieval.
Advancesin Information Retrieval.
Pages: 437-448.D.
Yarowsky and G. Ngai.
2001.
Inducing MultilingualPOS Taggers and NP Bracketers via Robust Projectionacross Aligned Corpora.
In NAACL-2001.1567
