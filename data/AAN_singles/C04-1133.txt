Automated Induction of Sense in ContextJames PUSTEJOVSKY, Patrick HANKS, Anna RUMSHISKYBrandeis UniversityWaltham, MA 02454, USA{jamesp,patrick,arum}@cs.brandeis.eduAbstractIn this paper, we introduce a model for sense as-signment which relies on assigning senses to thecontexts within which words appear, rather than tothe words themselves.
We argue that word sensesas such are not directly encoded in the lexiconof the language.
Rather, each word is associatedwith one or more stereotypical syntagmatic pat-terns, which we call selection contexts.
Each selec-tion context is associated with a meaning, whichcan be expressed in any of various formal or com-putational manifestations.
We present a formalismfor encoding contexts that help to determine thesemantic contribution of a word in an utterance.Further, we develop a methodology through whichsuch stereotypical contexts for words and phrasescan be identified from very large corpora, and sub-sequently structured in a selection context dictio-nary, encoding both stereotypical syntactic and se-mantic information.
We present some preliminaryresults.1 IntroductionThis paper describes a new model for the acquisi-tion and exploitation of selectional preferences forpredicates from natural language corpora.
Our goalis to apply this model in order to construct a dic-tionary of normal selection contexts for natural lan-guage; that is, a computational lexical database ofrich selectional contexts, associated with proceduresfor assigning interpretations on a probabilistic basisto less normal contexts.
Such a semi-automaticallydeveloped resource promises to have applications fora number of NLP tasks, including word-sense disam-biguation, selectional preference acquisition, as wellas anaphora resolution and inference in specializeddomains.
We apply this methodology to a selectedset of verbs, including a subset of the verbs in theSenseval 3 word sense discrimination task and reportour initial results.1.1 Selectional Preference Acquisition:Current State of the ArtPredicate subcategorization information constitutesan essential part of the computational lexicon entry.In recent years, a number of approaches have beenproposed for dealing computationally with selec-tional preference acquisition (Resnik (1996); Briscoeand Carroll (1997); McCarthy (1997); Rooth et al(1999); Abney and Light (1999); Ciaramita andJohnson (2000); Korhonen (2002)).The currently available best algorithms developedfor the acquisition of selectional preferences for pred-icates are induction algorithms modeling selectionalbehavior as a distribution over words (cf.
Abney andLight (1999)).
Semantic classes assigned to predi-cate arguments in subcategorization frames are ei-ther derived automatically through statistical clus-tering techniques (Rooth et al (1999), Light andGreiff (2002)) or assigned using hand-constructedlexical taxonomies such as the WordNet hierarchy orLDOCE semantic classes.
Overwhelmingly, Word-Net is chosen as the default resource for dealing withthe sparse data problem (Resnik (1996); Abney andLight (1999); Ciaramita and Johnson (2000); Agirreand Martinez (2001); Clark and Weir (2001); Carrolland McCarthy (2000); Korhonen and Preiss (2003)).Much of the work on inducing selectional prefer-ences for verbs from corpora deals with predicates in-discriminately, assuming no differentiation betweenpredicate senses (Resnik (1996); Abney and Light(1999); Ciaramita and Johnson (2000); Rooth et al(1999)).
Those approaches that do distinguish be-tween predicate senses or complementation patternsin acquisition of selectional constraints (Korhonen(2002); Korhonen and Preiss (2003)) do not use cor-pus analysis for verb sense classification.1.2 Word Sense Disambiguation: CurrentState of the ArtPrevious computational concerns for economy ofgrammatical representation have given way to mod-els of language that not only exploit generativegrammatical resources but also have access to largelists of contexts of linguistic items (words), to whichnew structures can be compared in new usages.However, following the work of Yarowsky (1992),Yarowsky (1995), many supervised WSD systemsuse minimal information about syntactic structures,for the most part restricting the notion of con-text to topical and local features.
Topical featurestrack open-class words that appear within a cer-tain window around a target word, and local fea-tures track small N-grams associated with the tar-get word.
Disambiguation therefore relies on wordco-occurrence statistics, rather than on structuralsimilarities.
That remains the case for most systemsthat participated in Senseval-2 (Preiss and Yarowsky(2001)).
Some recent work (Stetina et al (1998);Agirre et al (2002); Yamashita et al (2003)) at-tempts to change this situation and presents a di-rected effort to investigate the impact of using syn-tactic features for WSD learning algorithms.
Agirreet al(2002) and Yamashita et al (2003) report re-sulting improvement in precision.Stevenson and Wilks (2001) propose a somewhatrelated technique to handle WSD, based on inte-grating LDOCE classes with simulated annealing.Although space does not permit discussion here, ini-tial comparisons suggest that our selection contextscould incorporate similar knowledge resources; it isnot clear what role model bias plays in associatingpatterns with senses, however.In this paper we modify the notion of word sense,and at the same time revise the manner in whichsenses are encoded.
The notion of word sense thathas been generally adopted in the literature is anartifact of several factors in the status quo, notablythe availability of lexical resources such as machine-readable dictionaries, in which fine sense distinctionsare not supported by criteria for selecting one senserather than another, and WordNet, where synsetgroupings are taken as defining word sense distinc-tions.
Thus, for instance, Senseval-2 WSD tasks re-quired disambiguation using WordNet senses (see,e.g., discussion in Palmer et al (2004)).
The featuresets used in the supervised WSD algorithms at bestuse only minimal information about the typing of ar-guments.
The approach we adopt, Corpus PatternAnalysis (CPA) (Pustejovsky and Hanks (2001)),incorporates semantic features of the arguments ofthe target word.
Semantic features are expressed interms of a restricted set of shallow types, chosen fortheir prevalence in selection context patterns.
Thistype system is extended with predicate-based nounclustering, in the bootstrapping process describedbelow.1.3 Related Resources: FrameNetIt is necessary to say a few words about the dif-ferences between CPA and FrameNet.
The CPAapproach has its origins in the analysis of largecorpora for lexicographic purposes (e.g.
Cobuild(Sinclair et al, 1987)) and in systemic-functionalgrammar, in particular in Halliday?s notion of ?lexisas a linguistic level?
(Halliday, 1966) and Sin-clair?s empirical approach to collocational anal-ysis (Sinclair, 1991).
FrameNet (freely avail-able online in a beautifully designed data base athttp://www.icsi.berkeley.edu/?framenet/), is an attempt toimplement Fillmore?s 1975 proposal that, instead ofseeking to satisfy a set of necessary and sufficientconditions, the meanings of words in text should beanalyzed by calculating resemblance to a prototype(Fillmore, 1975).CPA (Hanks, 2004) is concerned with establishingprototypical norms of usage for individual words.
Itis possible (and certainly desirable) that CPA normswill be mappable onto FrameNet?s semantic frames(for which see the whole issue of the InternationalJournal of Lexicography for September 2003 (in par-ticular Atkins et al (2003a), Atkins et al (2003b),Fillmore et al (2003a), Baker et al (2003), Fillmoreet al (2003b)).
In frame semantics, the relationshipbetween semantics and syntactic realization is oftenat a comparatively deep level, i.e.
in many sentencesthere are elements that are potentially present butnot actually expressed.
For example, in the sentence?he risked his life?, two semantic roles are expressed(the risker and the valued object ?his life?
that is putat risk).
But at least three other roles are sublim-inally present although not expressed: the possiblebad outcome (?he risked his death?
), the beneficiaryor goal (?he risked his life for her/for a few dollars?
),and the means (?he risked a backward glance?
).CPA, on the other hand, is shallower and morepractical: the objective is to identify, in relation toa given target word, the overt textual clues thatactivate one or more components of its meaningpotential.
There is also a methodological differ-ence: whereas FrameNet research proceeds frame byframe, CPA proceeds word by word.
This meansthat when a word has been analysed in CPA thepatterns are immediately available for disambigua-tion.
FrameNet will be usable for disambiguationonly when all frames have been completely analysed.Even then, FrameNet?s methodology, which requiresthe researchers to think up all possible members ofa Frame a priori, means that important senses ofwords that have been partly analysed are missingand may continue to be missing for years to come.There is no attempt in FrameNet to identify thesenses of each word systematically and contrastively.In its present form, at least, FrameNet has at leastas many gaps as senses.
For example, at the timeof writing toast is shown as part of the Apply Heatframe but not the Celebrate frame.
It is not clearhow or whether the gaps are to be filled systemat-ically.
We do not even know whether there is (oris going to be) a Celebrate frame and if so what itwill be called.
What is needed is a principled fix ?
adecision to proceed from evidence, not frames.
Thisis ruled out by FrameNet for principled reasons: theunit of analysis for FrameNet is the frame, not theword.2 CPA MethodologyThe Corpus Pattern Analysis (CPA) technique usesa semi-automatic bootstrapping process to producea dictionary of selection contexts for predicatesin a language.
Word senses for verbs are distin-guished through corpus-derived syntagmatic pat-terns mapped to Generative Lexicon Theory (Puste-jovsky (1995)) as a linguistic model of interpreta-tion, which guides and constrains the induction ofsenses from word distributional information.
Eachpattern is specified in terms of lexical sets for eachargument, shallow semantic typing of these sets, andother syntagmatically relevant criteria (e.g., adver-bials of manner, phrasal particles, genitives, nega-tives).The procedure consists of three subtasks: (1) themanual discovery of selection context patterns forspecific verbs; (2) the automatic recognition of in-stances of the identified patterns; and (3) automaticacquisition of patterns for unanalyzed cases.
Ini-tially, a number of patterns are manually formulatedby a lexicographer through corpus pattern analysisof about 500 occurrences of each verb lemma.
Next,for higher frequency verbs, the remaining corpus oc-currences are scrutinized to see if any low-frequencypatterns have been missed.
The patterns are thentranslated into a feature matrix used for identifyingthe sense of unseen instances for a particular verb.In the remainder of this section, we describe thesesubtasks in more detail.
The following sections ex-plain the current status of the implementation ofthese tasks.2.1 Lexical DiscoveryNorms of usage are captured in what we call selec-tion context patterns.
For each lemma, contextsof usage are sorted into groups, and a stereotypi-cal CPA pattern that captures the relevant seman-tic and syntactic features of the group is recorded.Many patterns have alternations, recorded in satel-lite CPA patterns.
Alternations are linked tothe main CPA pattern through the same sense-modifying mechanisms as those that allow for ex-ploitations (coercions) of the norms of usage to beunderstood.
For example, here is the set of pat-terns for the verb treat.
Note that these patternsdo not capture all possible uses, and other patternsmay be added, e.g.
if additional evidence is foundin domain-specific corpora.
(1) CPA Pattern set for treat:I.
[[Person 1]] treat [[Person 2]] ({at | in} [[Location]])(for [[Event = Injury | Ailment]]); NO [Adv[Manner]]II.
[[Person 1]] treat [[Person 2]] [Adv[Manner]]IIIa.
[[Person]] treat [[TopType 1]] {{as | like} [[TopType 2]]}IIIb.
[[Person]] treat [[TopType]] {{as if | as though | like}[CLAUSE]}IV.
[[Person 1]] treat [[Person 2]] {to [[Event]]}V. [[Person]] treat [[PhysObj | Stuff 1]] (with [[Stuff 2]])There may be several patterns realizing a singlesense of a verb, as in (IIIa/IIIb) above.
Also, theremay be several equivalent alternations or there maybe a stereotype.
Note that alternations are differentrealizations of the same norm, not exploitations (i.e.,not coercions).
(2) Alternations for treat Pattern 1 :[[Person 1]] treat [[Person 2]] ({at | in} [[Hospital]])(for [[Injury | Ailment]]); NO [Adv[Manner]]Alternation 1:[[Person 1 <--> Medicament | Med-Procedure | Institution]]Alternation 2:[[Person 2 <--> Injury | Ailment | Bodypart]]CPA PatternsA CPA pattern extends the traditional notion of se-lectional context to include a number of other con-textual features, such as minor category parsing andsubphrasal cues.
Accurate identification of the se-mantically relevant aspects of a pattern is not anobvious and straightforward procedure, as has some-times been assumed in the literature.
For example,the presence or absence of an adverbial of manner inthe third valency slot around a verb can dramaticallyalter the verb?s meaning.
Simple syntactic encodingof argument structure, for instance, is insufficient todiscriminate between the two major senses of theverb treat, as illustrated below.
(3) a.
They say their bosses treat them with respect.b.
Such patients are treated with antibiotics.The ability to recognize the shallow semantic typeof a phrase in the context of a predicate is of coursecrucial ?for example, in (3a) recognizing the PP as(a) an adverbial, and (b) an adverbial of manner,rather than an instrumental co-agent (as in (3b)),is crucial for assigning the correct sense to the verbtreat above.In the CPA model, automatic identification ofselection contexts not only captures the argumentstructure of a predicate, but also more delicate fea-tures, which may have a profound effect on thesemantic interpretation of a predicate in context.There are four constraint sets that contribute to thepatterns for encoding selection contexts.
These are:(4) a.
Shallow Syntactic Parsing: Phrase-level recogni-tion of major categories.b.
Shallow Semantic Typing: 50-100 primitive shal-low types, such as Person, Institution, Event, Abstract, Ar-tifact, Location, and so forth.
These are the top types se-lected from the Brandeis Shallow Ontology (BSO), and aresimilar to entities (and some relations) employed in NamedEntity Recognition tasks, such as TREC and ACE.c.
Minor Syntactic Category Parsing: e.g., loca-tives, purpose clauses, rationale clauses, temporal adjuncts.d.
Subphrasal Syntactic Cue Recognition: e.g.,genitives, partitives, bare plural/determiner distinctions,infinitivals, negatives.The notion of a selection context pattern, as pro-duced by a human annotator, is expressed as a BNFspecification in Table 1.1 This specification relieson word order to specify argument position, and iseasily translated to a template with slots allocatedfor each argument.
Within this grammar, a seman-tic roles can be specified for each argument, but thisinformation currently is not used in the automatedprocessing.English contains only about 8,000 verbs, of whichwe estimate that about 30% have only one basic pat-tern.
The rest are evenly split between verbs hav-ing 2-3 patterns and verbs having more than 4 or1Round brackets indicate optional elements of the pattern,and curly brackets indicate syntactic constituents.CPA-Pattern ?
Segment verb-lit Segment | verb-lit Segment | Segment verb-lit | CPA-Pattern ?;?
ElementSegment ?
Element | Segment Segment | ??
Segment ??
| ?(?
Segment ?)?
| Segment ?|?
SegmentElement ?
literal | ?[?
Rstr ArgType ?]?
| ?[?
Rstr literal ?]?
| ?[?
Rstr ?]?
| ?[?
NO Cue ?]?
| ?[?
Cue ?
]?Rstr ?
POS | Phrasal | Rstr ?|?
Rstr | epsilonCue ?
POS | Phrasal | AdvCueAdvCue ?
ADV ?[?
AdvType ?
]?AdvType ?
Manner | Dir | LocationPhrasal ?
OBJ | CLAUSE | VP | QUOTEPOS ?
ADJ | ADV | DET | POSDET | COREF POSDET | REFL-PRON | NEG |MASS | PLURAL | V | INF | PREP | V-ING | CARD | QUANT | CONJArgType ?
?[?
SType ?]?
| ?[?
SType ?=?
SubtypeSpec ?]?
| ArgType ?|?
ArgType | ?[?
SType ArgIdx ?]?
|?[?
SType ArgIdx ?=?
SubtypeSpec ?
]?SType ?
AdvType | TopType | Entity | Abstract | PhysObj | Institution | Asset | Location | Human | Animate |Human Group | Substance | Unit of Measurement | Quality | Event | State of Affairs | ProcessSubtypeSpec ?
SubtypeSpec ?|?
SubtypeSpec | SubtypeSpec ?&?
SubtypeSpec | Role | Polarity | LSetRole ?
Role | Role ?|?
Role | Benficiary | Meronym | Agent | PayerPolarity ?
Negative | PositiveLSet ?
Worker | Pilot | Musician | Competitor | Hospital | Injury | Ailment | Medicament | Medical Procedure |Hour-Measure | Bargain | Clothing | BodyPart | Text | Sewage | Part | Computer | AnimalArgIdx ?
<number> verb-lit ?
<verb-word-form>literal ?
word word ?
<word>CARD ?
<number> NEG ?
notPOSDET ?
my | your | ... INF ?
toQUANT ?
CARD | a lot | longer | more | many | ...Table 1: Pattern grammarmore patterns.
About 20 light verbs have between100 and 200 patterns each.
This is less alarmingthan it sounds, because the majority of light verbpatterns involve selection of just one specific nom-inal head, e.g., take account, take plunge, takephotograph, with few if any alternations.
The pat-tern sets for verbs of different frequency groups differin terms of the number and type of features each pat-tern requires, the number of patterns in a set for agiven verbs, the number of alternations for each pat-tern, and the type of selectional preferences affectingthe verb?s arguments.Brandeis Shallow OntologyThe Brandeis Shallow Ontology (BSO) is a shallowhierarchy of types selected for their prevalence inmanually identified selection context patterns.
Atthe time of writing, there are just 65 types, in termsof which patterns for the first one hundred verbshave been analyzed.
New types are added occasion-ally, but only when all possibilities of using existingtypes prove inadequate.
Once the set of manuallyextracted patterns is sufficient, the type system willbe re-populated and become pattern-driven.The BSO type system allows multiple inheri-tance (e.g.
Document v PhysObj and Document vInformation.
The types currently comprising theontology are listed above.
The BSO contains typeassignments for 20,000 noun entries and 10,000 nom-inal collocation entries.Corpus-driven Type SystemThe acquisition strategy for selectional preferencesfor predicates proceeds as follows:(5) a. Partition the corpus occurrences of apredicate according to the selection contextspattern grammar, distinguished by the fourlevels of constraints mentioned in (4).
Theseare uninterpreted patterns for the predicate.b.
Within a given pattern, promote the statisti-cally significant literal types from the corpus foreach argument to the predicate.
This inducesan interpretation of the pattern, treating thepromoted literal type as the specific binding ofa shallow type from step (a) above.c.
Within a given pattern, coerce all lexicalheads in the same shallow type for an argu-ment, into the promoted literal type, assignedin (b) above.
This is a coercion of a lexicalhead to the interpretation of the promotedliteral type induced from step (b) above.In a sense, (5a) can be seen as a broad multi-level partitioning of the selectional behavior for apredicate according to a richer set of syntactic andsemantic discriminants.
Step (5b) can be seen ascapturing the norms of usage in the corpus, whilestep (5c) is a way of modeling the exploitationof these norms in the language (through coercion,metonymy, and other generative operations).
Toillustrate the way in which CPA discriminates un-interpreted patterns from the corpus, we return tothe verb treat as it is used in the BNC.
Althoughthere are three basic senses for this verb, the twomajor senses, as illustrated in (1) above, emerge ascorrelated with two distinct context patterns, us-ing the discriminant constraints mentioned in (4)above.
For the full specification for this verb, seewww.cs.brandeis.edu/~arum/cpa/treat.html.
(6) a.
[[Person 1]] treat [[Person 2]]; NO [Adv[Manner]]b.
[[Person 1]] treat [[Person 2]] [Adv[Manner]]Given a distinct (contextual) basis on which to an-alyze the actual statistical distribution of the wordsin each argument position, we can promote statisti-cally relevant and significant literal types for thesepositions.
For example, for pattern (a) above, thisinduces Doctor as Person 1, and Patient as boundto Person 2.
This produces the interpreted contextpattern for this sense as shown below.
(7) [[doctor]] treat [[patient]]Promoted literal types are corpus-derived andpredicate-dependent, and are syntactic heads ofphrases that occur with the greatest frequency in ar-gument positions for a given sense pattern; they aresubsequently assumed to be subtypes of the particu-lar shallow type in the pattern.
Step (5c) above thenenables us to bind the other lexical heads in these po-sitions as coerced forms of the promoted literal type.This can be seen below in the concordance sample,where therapies is interpreted as Doctor, and peopleand girl are interpreted as Patient.
(8) a. returned with a doctor who treated the girl till an am-bulance arrived.b.
more than 90,000 people have been treated for cholerasince the epidemic beganc.
nonsurgical therapies to treat the breast cancer, whichmay involveModel BiasThe assumption within GL is that semantic typesin the grammar map systematically to default syn-tactic templates (cf.
Pustejovsky (1995)).
Theseare termed canonical syntactic forms (CSFs).
Forexample, the CSF for the type proposition is atensed S. There are, however, many possible real-izations (such as infinitival S and NP) for this typedue to the different possibilities available from gen-erative devices in a grammar, such as coercion andco-composition.
The resulting set of syntactic formsassociated with a particular semantic type is calleda phrasal paradigm for that type.
The model biasprovided by GL acts to guide the interpretation ofpurely statistically based measures (cf.
Pustejovsky(2000)).2.2 Automatic Recognition of Pattern UseEssentially, this subtask is similar to the traditionalsupervised WSD problem.
Its purpose is (1) to testthe discriminatory power of CPA-derived feature-set, (2) to extend and refine the inventory of featurescaptured by the CPA patterns, and (3) to allow forpredicate-based argument groupings by classifyingunseen instances.
Extension and refinement of theinventory of features should involve feature induc-tion, but at the moment this part has not been im-plemented.
During the lexical discovery stage, lex-ical sets that fill some of the argument slots in thepatterns are instantiated from the training exam-ples.
As more predicate-based lexical sets withinshallow types are explored, the data will permitidentification of the types of features that unite ele-ments in lexical sets.2.3 Automatic Pattern AcquisitionThe algorithm for automatic pattern acquisition in-volves the following steps:(9) a.
Collect all constituents in a particular argu-ment position;b.
Identify syntactic alternations;c. Perform clustering on all nouns that occur ina particular argument position of a given pred-icate;d. For each cluster, measure its relatednessto the known lexical sets, obtained previouslyduring the lexical discovery stage and extendedthrough WSD of unseen instances.
If none ofthe existing lexical sets pass the distance thresh-old, establish the cluster as a new lexical set, tobe used in future pattern specification.Step (9d) must include extensive filtering proce-dures to check for shared semantic features, look-ing for commonality between the members.
Thatis, there must be some threshold overlap betweensubgroups of the candidate lexical set and and theexisting semantic classes.
For instance, checking if,for a certain percentage of pairs in the candidate set,there already exists a set of which both elements aremembers.3 Current ImplementationThe CPA patterns are developed using the BritishNational Corpus (BNC).
The sorted instances areused as a training set for the supervised disambigua-tion.
For the disambiguation task, each pattern istranslated into into a set of preprocessing-specificfeatures.The BNC is preprocessed using the Robust Accu-rate Statistical Parsing system (RASP) and seman-tically tagged with BSO types.
The RASP system(Briscoe and Carroll (2002)) tokenizes, POS-tags,and lemmatizes text, generating a forest of full parsetrees for each sentence and associating a probabilitywith each parse.
For each parse, RASP produces aset of grammatical relations, specifying the relationtype, the headword, and the dependent element.
Allour computations are performed over the single top-ranked tree for the sentences where a full parse wassuccessfully obtained.
Some of the grammatical re-lations identified by RASP are shown in (10).
(10) subjects: ncsubj, clausal (csubj, xsubj)objects: dobj, iobj, clausal complementmodifiers: adverbs, modifiers of event nominalsWe use endocentric semantic typing, i.e., the head-word of each constituent is used to establish its se-mantic type.
The semantic tagging strategy is simi-lar to the one described in Pustejovsky et al (2002).Currently, a subset of 24 BSO types is used for se-mantic tagging.A CPA pattern is translated into a feature set,which in the current implementation uses binary fea-tures.
It is further complemented with other dis-criminant context features which, rather than dis-tinguishing a particular pattern, are merely likely tooccur with a given subset of patterns; that is, the fea-tures that only partially determine or co-determinea sense.
In the future, these should be learned fromthe training set through feature induction from thetraining sample, but at the moment, they are addedmanually.
The resulting feature matrix for each pat-tern contains features such as those in (11) below.Each pattern is translated into a template of 15-25features.
(11) Selected context features:a. obj institution: object belongs to the BSO type ?Insti-tution?b.
subj human group: subject belongs to the BSO type ?Hu-manGroup?c.
mod adv ly: target verb has an adverbial modifier, with a-ly adverbd.
clausal like: target verb has a clausal argument intro-duced by ?like?e.
iobj with: target verb has an indirect object by ?with?f.
obj PRP: direct object is a personal pronoung.
stem VVG: the target verb stem is an -ing formEach feature may be realized by a number of RASPrelations.
For instance, a feature dealing withobjects would take into account RASP relations?dobj?, ?obj2?, and ?ncsubj?
(for passives).
Thefeatures such as (11a)-(11e) are typically taken di-rectly from the pattern specification, while featuressuch as in (11f) and (11g) would typically be addedas co-determining the pattern.4 Results and DiscussionThe experimental trials performed to date are toopreliminary to validate the methodology outlinedabove in general terms for the WSD task.
Our re-sults are encouraging however, and comparable tothe best performing systems reported from Senseval2.
For our experiments, we implemented two ma-chine learning algorithms, instance-based k-NearestNeighbor, and a decision tree algorithm (a versionof ID3).
For these experiments, kNN was run withthe full training set.
Table 2 shows the results on asubset of verbs that have been processed, also listingthe number of patterns in the pattern set for each ofthe verbs.2verb number of training accuracypatterns set ID3 kNNedit 2 100 87% 86%treat 4 200 45% 52%submit 4 100 59% 64%Table 2: Accuracy of pattern identificationFurther experimentation is obviously needed toadequately gauge the effectiveness of the selectioncontext approach for WSD and other NLP tasks.It is already clear, however, that the traditionalsense enumeration approach, where senses are asso-ciated with individual lexical items, must give wayto a model where senses are assigned to the contextswithin which words appear.
Furthermore, becausethe variability of the stereotypical syntagmatic pat-terns that are associated with words appears to berelatively small, such information can be encoded as2Test set size for each lemma is 100 instances, selected outof several randomly chosen segments of BNC, non-overlappingwith the training setlexically-indexed contexts.
A comprehensive dictio-nary of such contexts could prove to be a powerfultool for a variety of NLP tasks.ReferencesS.
Abney and M. Light.
1999.
Hiding a semantic hierarchy in amarkov model.E.
Agirre and D. Martinez.
2001.
Learning class-to-class se-lectional preferences.
In Walter Daelemans and Re?mi Zajac,editors, Proceedings of CoNLL-2001, pages 15?22.
Toulouse,France.E.
Agirre, D. Martinez, and L. Marquez.
2002.
Syntactic featuresfor high precision word sense disambiguation.
COLING 2002.S.
Atkins, C. Fillmore, and C. Johnson.
2003a.
Lexicographicrelevance: Selecting information from corpus evidence.
Inter-national Journal of Lexicography, 16(3):251?280, September.S.
Atkins, M. Rundell, and H. Sato.
2003b.
The contribution ofFramenet to practical lexicography.
International Journal ofLexicography, 16(3):333?357, September.C.
Baker, C. Fillmore, and B. Cronin.
2003.
The structure of theFramenet database.
International Journal of Lexicography,16(3):281?296, September.T.
Briscoe and J. Carroll.
1997.
Automatic extraction of sub-categorization from corpora.
Proceedings of the 5th ANLPConference, Washington DC, pages 356?363.T.
Briscoe and J. Carroll.
2002.
Robust accurate statistical anno-tation of general text.
Proceedings of the Third InternationalConference on Language Resources and Evaluation (LREC2002), Las Palmas, Canary Islands, May 2002, pages 1499?1504.J.
Carroll and D. McCarthy.
2000.
Word sense disambiguationusing automatically acquired verbal preferences.M.
Ciaramita and M. Johnson.
2000.
Explaining away ambiguity:Learning verb selectional preference with Bayesian networks.COLING 2000.S.
Clark and D. Weir.
2001.
Class-based probability estimationusing a semantic hierarchy.
Proceedings of the 2nd Conferenceof the North American Chapter of the ACL.
Pittsburgh, PA.C.
Fillmore, C. Johnson, and M. Petruck.
2003a.
Background toFramenet.
International Journal of Lexicography, 16(3):235?250, September.C.
Fillmore, M. Petruck, J. Ruppenhofer, and A. Wright.
2003b.Framenet in action: The case of attaching.
InternationalJournal of Lexicography, 16(3):297?332, September.C.
Fillmore.
1975.
Santa Cruz Lectures on Deixis.
Indiana Uni-versity Linguistics Club.
Bloomington, IN.M.
A. K. Halliday.
1966.
Lexis as a linguistic level.
In C. E.Bazell, J. C. Catford, M. A. K. Halliday, and R. H. Robins,editors, In Memory of J. R. Firth.
Longman.P.
Hanks.
2004.
The syntagmatics of metaphor (forthcoming).International Journal of Lexicography, 17(3), September.forthcoming.A.
Korhonen and J. Preiss.
2003.
Improving subcategorizationacquisition using word sense disambiguation.
Proceedings ofthe 41st Annual Meeting of the Association for Computa-tional Linguistics.
Sapporo, Japan.A.
Korhonen.
2002.
Subcategorization Acquisition.
PhD thesispublished as Techical Report UCAM-CL-TR-530.
ComputerLaboratory, University of Cambridge.M.
Light and W. Greiff.
2002.
Statistical models for the inductionand use of selectional preferences.
Cognitive Science, Volume26(3), pp.
269- 281.D.
McCarthy.
1997.
Word sense disambiguation for acquisition ofselectional preferences.
In Piek Vossen, Geert Adriaens, Nico-letta Calzolari, Antonio Sanfilippo, and Yorick Wilks, editors,Automatic Information Extraction and Building of LexicalSemantic Resources for NLP Applications, pages 52?60.
As-sociation for Computational Linguistics, New Brunswick, NewJersey.M.
Palmer, H. T. Dang, and C. Fellbaum.
2004.
Making fine-grained and coarse-grained sense distinctions, both manuallyand automatically.
Natural Language Engineering.
Preprint.J.
Preiss and D. Yarowsky, editors.
2001.
Proceedings of the Sec-ond Int.
Workshop on Evaluating WSD Systems (Senseval2).
ACL2002/EACL2001.J.
Pustejovsky and P. Hanks.
2001.
Very Large LexicalDatabases: A tutorial.
ACL Workshop, Toulouse, France.J.
Pustejovsky, A. Rumshisky, and J. Castano.
2002.
RerenderingSemantic Ontologies: Automatic Extensions to UMLS throughCorpus Analytics.
In LREC 2002 Workshop on Ontologiesand Lexical Knowledge Bases.
Las Palmas, Canary Islands,Spain.J.
Pustejovsky.
1995.
Generative Lexicon.
Cambridge (Mass.
):MIT Press.J.
Pustejovsky.
2000.
Lexical shadowing and argument closure.In Y. Ravin and C. Leacock, editors, Lexical Semantics.
Ox-ford University Press.P.
Resnik.
1996.
Selectional constraints: An information-theoretic model and its computational realization.
Cognition,61:127?159.M.
Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil.
1999.Inducing a semantically annotated lexicon via EM?based clus-tering.
In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics (ACL?99), Mary-land.J.
Sinclair, P. Hanks, and et al 1987.
The Collins Cobuild En-glish Language Dictionary.
HarperCollins, 4th (2003) edition.Published as Collins Cobuild Advanced Learner?s English Dic-tionary.J.
M. Sinclair.
1991.
Corpus, Concordance, Collocation.
OxfordUniversity Press.J.
Stetina, S. Kurohashi, and M. Nagao.
1998.
General wordsense disambiguation method based on A full sentential con-text.
In Sanda Harabagiu, editor, Use of WordNet in NaturalLanguage Processing Systems: Proceedings of the Confer-ence, pages 1?8.
Association for Computational Linguistics,Somerset, New Jersey.M.
Stevenson and Y. Wilks.
2001.
The interaction of knowledgesources in word sense disambiguation.
Computational Lin-guistics, 27(3), September.K.
Yamashita, K. Yoshida, and Y. Itoh.
2003.
Word sense dis-ambiguation using pairwise alignment.
ACL2003.D.
Yarowsky.
1992.
Word-sense disambiguation using statisticalmodels of Roget?s categories trained on large corpora.
Proc.COLING92, Nantes, France.D.
Yarowsky.
1995.
Unsupervised word sense disambiguation ri-valing supervised methods.
In Meeting of the Association forComputational Linguistics, pages 189?196.
