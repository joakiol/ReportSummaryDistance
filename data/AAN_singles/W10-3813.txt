Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101?109,COLING 2010, Beijing, August 2010.HMM Word-to-Phrase Alignment with Dependency ConstraintsYanjun Ma Andy WayCentre for Next Generation LocalisationSchool of ComputingDublin City University{yma,away}@computing.dcu.ieAbstractIn this paper, we extend the HMM word-to-phrase alignment model with syntac-tic dependency constraints.
The syn-tactic dependencies between multiplewords in one language are introducedinto the model in a bid to produce co-herent alignments.
Our experimental re-sults on a variety of Chinese?Englishdata show that our syntactically con-strained model can lead to as much asa 3.24% relative improvement in BLEUscore over current HMM word-to-phrasealignment models on a Phrase-BasedStatistical Machine Translation systemwhen the training data is small, anda comparable performance compared toIBM model 4 on a Hiero-style systemwith larger training data.
An intrin-sic alignment quality evaluation showsthat our alignment model with depen-dency constraints leads to improvementsin both precision (by 1.74% relative) andrecall (by 1.75% relative) over the modelwithout dependency information.1 IntroductionGenerative word alignment models includingIBM models (Brown et al, 1993) and HMMword alignment models (Vogel et al, 1996) havebeen widely used in various types of Statisti-cal Machine Translation (SMT) systems.
Thiswidespread use can be attributed to their robust-ness and high performance particularly on large-scale translation tasks.
However, the qualityof the alignment yielded from these models isstill far from satisfactory even with significantamounts of training data; this is particularly truefor radically different languages such as Chineseand English.The weakness of most generative models of-ten lies in the incapability of addressing one tomany (1-to-n), many to one (n-to-1) and manyto many (m-to-n) alignments.
Some research di-rectly addresses m-to-n alignment with phrasealignment models (Marcu and Wong, 2002).However, these models are unsuccessful largelydue to intractable estimation (DeNero and Klein,2008).
Recent progress in better parameteri-sation and approximate inference (Blunsom etal., 2009) can only augment the performance ofthese models to a similar level as the baselinewhere bidirectional word alignments are com-bined with heuristics and subsequently used toinduce translation equivalence (e.g.
(Koehn etal., 2003)).
The most widely used word align-ment models, such as IBM models 3 and 4, canonly model 1-to-n alignment; these models areoften called ?asymmetric?
models.
IBM models3 and 4 model 1-to-n alignments using the notionof ?fertility?, which is associated with a ?defi-ciency?
problem despite its high performance inpractice.On the other hand, the HMM word-to-phrasealignment model tackles 1-to-n alignment prob-lems with simultaneous segmentation and align-ment while maintaining the efficiency of themodels.
Therefore, this model sets a good ex-ample of addressing the tradeoffs between mod-elling power and modelling complexity.
Thismodel can also be seen as a more generalised101case of the HMM word-to-word model (Vogel etal., 1996; Och and Ney, 2003), since this modelcan be reduced to an HMM word-to-word modelby restricting the generated target phrase lengthto one.
One can further refine existing wordalignment models with syntactic constraints (e.g.
(Cherry and Lin, 2006)).
However, most re-search focuses on the incorporation of syntacticconstraints into discriminative alignment mod-els.
Introducing syntactic information into gen-erative alignment models is shown to be morechallenging mainly due to the absence of appro-priate modelling of syntactic constraints and the?inflexibility?
of these generative models.In this paper, we extend the HMM word-to-phrase alignment model with syntactic depen-dencies by presenting a model that can incor-porate syntactic information while maintainingthe efficiency of the model.
This model is basedon the observation that in 1-to-n alignments,the n words bear some syntactic dependencies.Leveraging such information in the model canpotentially further aid the model in producingmore fine-grained word alignments.
The syn-tactic constraints are specifically imposed on then words involved in 1-to-n alignments, whichis different from the cohesion constraints (Fox,2002) as explored by Cherry and Lin (2006),where knowledge of cross-lingual syntactic pro-jection is used.
As a syntactic extension of theopen-source MTTK implementation (Deng andByrne, 2006) of the HMM word-to-phrase align-ment model, its source code will also be releasedas open source in the near future.The remainder of the paper is organised as fol-lows.
Section 2 describes the HMM word-to-phrase alignment model.
In section 3, we presentthe details of the incorporation of syntactic de-pendencies.
Section 4 presents the experimentalsetup, and section 5 reports the experimental re-sults.
In section 6, we draw our conclusions andpoint out some avenues for future work.2 HMM Word-to-Phrase AlignmentModelIn HMM word-to-phrase alignment, a sentencee is segmented into a sequence of consecutivephrases: e = vK1 , where vk represents the kthphrase in the target sentence.
The assumptionthat each phrase vk generated as a translation ofone single source word is consecutive is made toallow efficient parameter estimation.
Similarlyto word-to-word alignment models, a variableaK1 is introduced to indicate the correspondencebetween the target phrase index and a sourceword index: k ?
i = ak indicating a mappingfrom a target phrase vk to a source word fak .
Arandom process ?k is used to specify the num-ber of words in each target phrase, subject to theconstraints J =?Kk=1 ?k, implying that the to-tal number of words in the phrases agrees withthe target sentence length J .The insertion of target phrases that do not cor-respond to any source words is also modelledby allowing a target phrase to be aligned to anon-existent source word f0 (NULL).
Formally,to indicate whether each target phrase is alignedto NULL or not, a set of indicator functions?K1 = {?1, ?
?
?
, ?K} is introduced (Deng andByrne, 2008): if ?k = 0, then NULL ?
vk; if?k = 1, then fak ?
vk.To summarise, an alignment a in an HMMword-to-phrase alignment model consists of thefollowing elements:a = (K,?K1 , aK1 , ?K1 )The modelling objective is to define a condi-tional distribution P (e,a|f) over these align-ments.
Following (Deng and Byrne, 2008),P (e,a|f) can be decomposed into a phrase countdistribution (1) modelling the segmentation of atarget sentence into phrases (P (K|J, f) ?
?Kwith scalar ?
to control the length of the hy-pothesised phrases), a transition distribution (2)modelling the dependencies between the currentlink and the previous links, and a word-to-phrasetranslation distribution (3) to model the degreeto which a word and a phrase are translational toeach other.P (e,a|f) = P (vK1 ,K, aK1 , ?K1 , ?K1 |f)= P (K|J, f) (1)P (aK1 , ?K1 , ?K1 |K,J, f) (2)P (vK1 |aK1 , ?K1 , ?K1 ,K, J, f)(3)102The word-to-phrase translation distribution(3) is formalised as in (4):P (vK1 |aK1 , ?K1 , ?K1 ,K, J, f)=K?k=1pv(vk|?k ?
fak , ?k) (4)Note here that we assume that the translationof each target phrase is conditionally indepen-dent of other target phrases given the individualsource words.If we assume that each word in a target phraseis translated with a dependence on the previ-ously translated word in the same phrase giventhe source word, we derive the bigram transla-tion model as follows:pv(vk|fak , ?k, ?k) = pt1(vk[1]|?k, fak)?k?j=2pt2(vk[j]|vk[j ?
1], ?k, fak)where vk[1] is the first word in phrase vk, vk[j]is the jth word in vk, pt1 is an unigram transla-tion probability and pt2 is a bigram translationprobability.
The intuition is that the first wordin vk is firstly translated by fak and the transla-tion of the remaining words vk[j] in vk from fakis dependent on the translation of the previousword vk[j ?
1] from fak .
The use of a bigramtranslation model can address the coherence ofthe words within the phrase vk so that the qual-ity of phrase segmentation can be improved.3 Syntactically Constrained HMMWord-to-Phrase Alignment Models3.1 Syntactic Dependencies forWord-to-Phrase AlignmentAs a proof-of-concept, we performed depen-dency parsing on the GALE gold-standard wordalignment corpus using Maltparser (Nivre et al,2007).1 We find that 82.54% of the consec-utive English words have syntactic dependen-cies and 77.46% non-consecutive English wordshave syntactic dependencies in 1-to-2 Chinese?English (ZH?EN) word alignment (one Chi-nese word aligned to two English words).
For1http://maltparser.org/English?Chinese (EN?ZH) word alignment, weobserve that 75.62% of the consecutive Chinesewords and 71.15% of the non-consecutive Chi-nese words have syntactic dependencies.
Ourmodel represents an attempt to encode these lin-guistic intuitions.3.2 Component Variables and DistributionsWe constrain the word-to-phrase alignmentmodel with a syntactic coherence model.
Givena target phrase vk consisting of ?k words, weuse the dependency label rk between words vk[1]and vk[?k] to indicate the level of coherence.The dependency labels are a closed set obtainedfrom dependency parsers, e.g.
using Maltparser,we have 20 dependency labels for English and12 for Chinese in our data.
Therefore, we havean additional variable rK1 associated with the se-quence of phrases vK1 to indicate the syntacticcoherence of each phrase, defining P (e,a|f) asbelow:P (rK1 , vK1 ,K, aK1 , ?K1 , ?K1 |f) = P (K|J, f)P (aK1 , ?K1 , ?K1 |K,J, f)P (vK1 |aK1 , ?K1 , ?K1 ,K, J, f)P (rK1 |aK1 , ?K1 , ?K1 , vK1 ,K, J, f) (5)The syntactic coherence distribution (5) issimplified as in (6):P (rK1 |aK1 , ?K1 , ?K1 , vK1 ,K, J, f)=K?k=1pr(rk; ?, fak , ?k) (6)Note that the coherence of each target phraseis conditionally independent of the coherence ofother target phrases given the source words fakand the number of words in the current phrase?k.
We name the model in (5) the SSH model.SSH is an abbreviation of Syntactically con-strained Segmental HMM, given the fact thatthe HMM word-to-phrase alignment model is aSegmental HMM model (SH) (Ostendorf et al,1996; Murphy, 2002).As our syntactic coherence model utilises syn-tactic dependencies which require the presenceof at least two words in target phrase vk, wetherefore model the cases of ?k = 1 and ?k ?
2103separately.
We rewrite (6) as follows:pr(rk; ?, fak , ?k) ={p?k=1(rk; ?, fak ) if ?k = 1p?k?2(rk; ?, fak ) if ?k ?
2where p?k=1 defines the syntactic coherencewhen the target phrase only contains one word(?k = 1) and p?k?2 defines the syntactic co-herence of a target phrase composed of multiplewords (?k ?
2).
We define p?k=1 as follows:p?k=1(rk; ?, fak ) ?
pn(?k = 1; ?, fak )where the coherence of the target phrase (word)vk is defined to be proportional to the probabilityof target phrase length ?k = 1 given the sourceword fak .
The intuition behind this model is thatthe syntactic coherence is strong iff the probabil-ity of the source fak fertility ?k = 1 is high.For p?k?2, which measures the syntactic co-herence of a target phrase consisting of morethan two words, we use the dependency label rkbetween words vk[1] and vk[?k] to indicate thelevel of coherence.
A distribution over the valuesrk ?
R = {SBJ,ADJ, ?
?
? }
(R is the set of de-pendency types for a specific language) is main-tained as a table for each source word associatedwith all the possible lengths ?
?
{2, ?
?
?
,N})of the target phrase it can generate, e.g.
we setN = 4 for ZH?EN alignment and N = 2 forEN?ZH alignment in our experiments.Given a target phrase vk containing ?k(?k ?2) words, it is possible that there are no depen-dencies between the first word vk[1] and the lastword vk[?k].
To account for this fact, we intro-duce a indicator function ?
as in below:?
(vk[1], ?k) =????
?1 if vk[1] and vk[?k]havesyntactic dependencies0 otherwiseWe can thereafter introduce a distribution p?(?
),where p?(?
= 0) = ?
(0 ?
?
?
1) andp?(?
= 0) = 1?
?
, with ?
indicating how likelyit is that the first and final words in a target phrasedo not have any syntactic dependencies.
We canset ?
to a small number to favour target phrasessatisfying the syntactic constraints and to a largernumber otherwise.
The introduction of this vari-able enables us to tune the model towards ourdifferent end goals.
We can now define p?k?2as:p?k?2(rk; ?, fak) = p(rk|?
; ?, fak )p?(?
)where we insist that p(rk|?
; ?, fak ) = 1 if?
= 0 (the first and last words in the targetphrase do not have syntactic dependencies) toreflect the fact that in most arbitrary consecu-tive word sequences the first and last words donot have syntactic dependencies, and otherwisep(rk|?
; ?, fak ) if ?
= 1 (the first and last wordsin the target phrase have syntactic dependen-cies).3.3 Parameter EstimationThe Forward-Backward Algorithm (Baum,1972), a version of the EM algorithm (Dempsteret al, 1977), is specifically designed for unsu-pervised parameter estimation of HMM models.The Forward statistic ?j(i, ?, ?)
in our modelcan be calculated recursively over the trellis asfollows:?j(i, ?, ?)
= {?i?,??,???j??
(i?, ?
?, ??
)pa(i|i?, ?
; I)}pn(?
; ?, fi)?pt1(ej?
?+1|?, fi)j?j?=j??+2pt2(ej?
|ej?
?1, ?, fi)pr(rk; ?, fi, ?
)which sums up the probabilities of every paththat could lead to the cell ?j, i, ??.
Note that thesyntactic coherence term pr(rk; ?, fi, ?)
can ef-ficiently be added into the Forward procedure.Similarly, the Backward statistic ?j(i, ?, ?)
iscalculated over the trellis as below:?j(i, ?, ?)
=?i?,??,???j+??
(i?, ?
?, ??
)pa(i?|i, h?
; I)pn(??
; ?
?, fi?)?pt1(ej+1|?
?, fi?)j+???j?=j+2pt2(ej?
|ej?
?1, ?
?, fi?
)pr(rk; ?
?, fi?
, ??
)Note also that the syntactic coherence termpr(rk; ?
?, fi?
, ??)
can be integrated into the Back-ward procedure efficiently.104Posterior probability can be calculated basedon the Forward and Backward probabilities.3.4 EM Parameter UpdatesThe Expectation step accumulates fractionalcounts using the posterior probabilities for eachparameter during the Forward-Backward passes,and the Maximisation step normalises the countsin order to generate updated parameters.The E-step for the syntactic coherence modelproceeds as follows:c(r?
; f, ??)
=?
(f ,e)?T?i,j,?,fi=f?j(i, ?, ?
= 1)?
(?, ??)?
(?j(e, ?
), r?
)where ?j(i, ?, ?)
is the posterior probability thata target phrase tjj?
?+1 is aligned to source wordfi, and ?j(e, ?)
is the syntactic dependency labelbetween ej?
?+1 and ej .
The M-step performsnormalisation, as below:pr(r?
; f, ??)
=c(r?
; f, ??
)?r c(r; f, ??
)Other component parameters can be estimatedin a similar manner.4 Experimental Setup4.1 DataWe built the baseline word alignment andPhrase-Based SMT (PB-SMT) systems using ex-isting open-source toolkits for the purposes offair comparison.
A collection of GALE data(LDC2006E26) consisting of 103K (2.9 millionEnglish running words) sentence pairs was firstlyused as a proof of concept (?small?
), and FBISdata containing 238K sentence pairs (8 millionEnglish running words) was added to construct a?medium?
scale experiment.
To investigate theintrinsic quality of the alignment, a collectionof parallel sentences (12K sentence pairs) forwhich we have manually annotated word align-ment was added to both ?small?
and ?medium?scale experiments.
Multiple-Translation ChinesePart 1 (MTC1) from LDC was used for Mini-mum Error-Rate Training (MERT) (Och, 2003),and MTC2, 3 and 4 were used as developmenttest sets.
Finally the test set from NIST 2006evaluation campaign was used as the final testset.The Chinese data was segmented using theLDC word segmenter.
The maximum-entropy-based POS tagger MXPOST (Ratnaparkhi, 1996)was used to tag both English and Chinese texts.The syntactic dependencies for both English andChinese were obtained using the state-of-the-artMaltparser dependency parser, which achieved84% and 88% labelled attachment scores forChinese and English respectively.4.2 Word AlignmentThe GIZA++ (Och and Ney, 2003) implementa-tion of IBM Model 4 (Brown et al, 1993) is usedas the baseline for word alignment.
Model 4 isincrementally trained by performing 5 iterationsof Model 1, 5 iterations of HMM, 3 iterationsof Model 3, and 3 iterations of Model 4.
Wecompared our model against the MTTK (Dengand Byrne, 2006) implementation of the HMMword-to-phrase alignment model.
The modeltraining includes 10 iterations of Model 1, 5 it-erations of Model 2, 5 iterations of HMM word-to-word alignment, 20 iterations (5 iterations re-spectively for phrase lengths 2, 3 and 4 with un-igram translation probability, and phrase length4 with bigram translation probability) of HMMword-to-phrase alignment for ZH?EN alignmentand 5 iterations (5 iterations for phrase length2 with uniform translation probability) of HMMword-to-phrase alignment for EN?ZH.
This con-figuration is empirically established as the bestfor Chinese?English word alignment.
To allowfor a fair comparison between IBM Model 4and HMM word-to-phrase alignment models, wealso restrict the maximum fertility in IBM model4 to 4 for ZH?EN and 2 for EN?ZH (the defaultis 9 in GIZA++ for both ZH?EN and EN?ZH).?grow-diag-final?
heuristic described in (Koehnet al, 2003) is used to derive the refined align-ment from bidirectional alignments.4.3 MT systemThe baseline in our experiments is a standardlog-linear PB-SMT system.
With the word align-ment obtained using the method described in105section 4.2, we perform phrase-extraction usingheuristics described in (Koehn et al, 2003), Min-imum Error-Rate Training (MERT) (Och, 2003)optimising the BLEU metric, a 5-gram languagemodel with Kneser-Ney smoothing (Kneser andNey, 1995) trained with SRILM (Stolcke, 2002)on the English side of the training data, andMOSES (Koehn et al, 2007) for decoding.
AHiero-style decoder Joshua (Li et al, 2009) isalso used in our experiments.
All significancetests are performed using approximate randomi-sation (Noreen, 1989) at p = 0.05.5 Experimental Results5.1 Alignment Model TuningIn order to find the value of ?
in the SSH modelthat yields the best MT performance, we usedthree development test sets using a PB-SMT sys-tem trained on the small data condition.
Figure 1shows the results on each development test setusing different configurations of the alignmentmodels.
For each system, we obtain the meanof the BLEU scores (Papineni et al, 2002) onthe three development test sets, and derive theoptimal value for ?
of 0.4, which we use here-after for final testing.
It is worth mentioningthat while IBM model 4 (M4) outperforms othermodels including the HMM word-to-word (H)and word-to-phrase (SH) alignment model in ourcurrent setup, using the default IBM model 4 set-ting (maximum fertility 9) yields an inferior per-formance (as much as 8.5% relative) comparedto other models.0.110.1150.120.1250.130.1350.14M4 H SH SSH-0.05SSH-0.1SSH-0.2SSH-0.3SSH-0.4SSH-0.5SSH-0.6BLEUscorealignment systemsMTC2MTC3MTC4Figure 1: BLEU score on development test setusing PB-SMT systemPB-SMT Hierosmall medium small mediumH 0.1440 0.2591 0.1373 0.2595SH 0.1418 0.2517 0.1372 0.2609SSH 0.1464 0.2518 0.1356 0.2624M4 0.1566 0.2627 0.1486 0.2660Table 1: Performance of PB-SMT using differentalignment models on NIST06 test set5.2 Translation ResultsTable 1 shows the performance of PB-SMT andHiero systems using a small amount of data foralignment model training on the NIST06 test set.For the PB-SMT system trained on the small dataset, using SSH word alignment leads to a 3.24%relative improvement over SH, which is statis-tically significant.
SSH also leads to a slightgain over the HMM word-to-word alignmentmodel (H).
However, when the PB-SMT systemis trained on larger data sets, there are no sig-nificant differences between SH and SSH.
Addi-tionally, both SH and SSH models underperformH on the medium data condition, indicating thatthe performance of the alignment model tunedon the PB-SMT system with small training datadoes not carry over to PB-SMT systems withlarger training data (cf.
Figure 1).
IBM model4 demonstrates stronger performance over othermodels for both small and medium data condi-tions.For the Hiero system trained on a small dataset, no significant differences are observed be-tween SSH, SH and H. On a larger training set,we observe that SSH alignment leads to betterperformance compared to SH.
Both SH and SSHalignments achieved higher translation qualitythan H. Note that while IBM model 4 outper-forms other models on a small data condition, thedifference between IBM model 4 and SSH is notstatistically significant on a medium data condi-tion.
It is also worth pointing out that the SSHmodel yields significant improvement over IBMmodel 4 with the default fertility setting, indicat-ing that varying the fertility limit in IBM model4 has a significant impact on translation quality.In summary, the SSH model which incorpo-rates syntactic dependencies into the SH modelachieves consistently better performance than106ZH?EN EN?ZHP R P RH 0.5306 0.3752 0.5282 0.3014SH 0.5378 0.3802 0.5523 0.3151SSH 0.5384 0.3807 0.5619 0.3206M4 0.5638 0.3986 0.5988 0.3416Table 2: Intrinsic evaluation of the alignment us-ing different alignment modelsSH in both PB-SMT and Hiero systems underboth small and large data conditions.
For aPB-SMT system trained on the small data set,the SSH model leads to significant gains overthe baseline SH model.
The results also en-tail an observation concerning the suitability ofdifferent alignment models for different typesof SMT systems; trained on a large data set,our SSH alignment model is more suitable toa Hiero-style system than a PB-SMT system,as evidenced by a lower performance comparedto IBM model 4 using a PB-SMT system, anda comparable performance compared to IBMmodel 4 using a Hiero system.5.3 Intrinsic EvaluationIn order to further investigate the intrinsic qual-ity of the word alignment, we compute the Preci-sion (P), Recall (R) and F-score (F) of the align-ments obtained using different alignment mod-els.
As the models investigated here are asym-metric models, we conducted intrinsic evalua-tion for both alignment directions, i.e.
ZH?ENword alignment where one Chinese word can bealigned to multiple English words, and EN?ZHword alignment where one English word can bealigned to multiple Chinese words.Table 2 shows the results of the intrinsic eval-uation of ZH?EN and EN?ZH word alignmenton a small data set (results on the medium dataset follow the same trend but are left out dueto space limitations).
Note that the P and Rare all quite low, demonstrating the difficulty ofChinese?English word alignment in the news do-main.
For the ZH?EN direction, using the SSHmodel does not lead to significant gains over SHin P or R. For the EN?ZH direction, the SSHmodel leads to a 1.74% relative improvement inP, and a 1.75% relative improvement in R overthe SH model.
Both SH and SSH lead to gainsover H for both ZH?EN and EN?ZH directions,while gains in the EN?ZH direction appear to bemore pronounced.
IBM model 4 achieves signif-icantly higher P over other models while the gapin R is narrow.Relating Table 2 to Table 1, we observe thatthe HMM word-to-word alignment model (H)can still achieve good MT performance despitethe lower P and R compared to other mod-els.
This provides additional support to previ-ous findings (Fraser and Marcu, 2007b) that theintrinsic quality of word alignment does not nec-essarily correlate with the performance of the re-sulted MT system.5.4 Alignment CharacteristicsIn order to further understand the characteristicsof the alignment that each model produces, weinvestigated several statistics of the alignment re-sults which can hopefully reveal the capabilitiesand limitations of each model.5.4.1 Pairwise ComparisonGiven the asymmetric property of these align-ment models, we can evaluate the quality of thelinks for each word and compare the alignmentlinks across different models.
For example, inZH?EN word alignment, we can compute thelinks for each Chinese word and compare thoselinks across different models.
Additionally, wecan compute the pairwise agreement in align-ing each Chinese word for any two alignmentmodels.
Similarly, we can compute the pairwiseagreement in aligning each English word in theEN?ZH alignment direction.For ZH?EN word alignment, we observe thatthe SH and SSH models reach a 85.94% agree-ment, which is not surprising given the fact thatSSH is a syntactic extension over SH, while IBMmodel 4 and SSH reach the smallest agreement(only 65.09%).
We also observe that there is ahigher agreement between SSH and H (76.64%)than IBM model 4 and H (69.58%).
This can beattributed to the fact that SSH is still a form ofHMM model while IBM model 4 is not.
A simi-lar trend is observed for EN?ZH word alignment.107ZH?EN EN?ZH1-to-0 1-to-1 1-to-n 1-to-0 1-to-1 1-to-ncon.
non-con.
con.
non-con.HMM 0.3774 0.4693 0.0709 0.0824 0.4438 0.4243 0.0648 0.0671SH 0.3533 0.4898 0.0843 0.0726 0.4095 0.4597 0.0491 0.0817SSH 0.3613 0.5092 0.0624 0.0671 0.3990 0.4835 0.0302 0.0872M4 0.2666 0.5561 0.0985 0.0788 0.3967 0.4850 0.0592 0.0591Table 3: Alignment types using different alignment models5.4.2 Alignment TypesAgain, by taking advantage of the asymmet-ric property of these alignment models, we cancompute different types of alignment.
For bothZH?EN (EN?ZH) alignment, we divide the linksfor each Chinese (English) word into 1-to-0where each Chinese (English) word is alignedto the empty word ?NULL?
in English (Chi-nese), 1-to-1 where each Chinese (English) wordis aligned to only one word in English (Chinese),and 1-to-n where each Chinese (English) wordis aligned to n (n ?
2) words in English (Chi-nese).
For 1-to-n links, depending on whetherthe n words are consecutive, we have consecu-tive (con.)
and non-consecutive (non-con.)
1-to-n links.Table 3 shows the alignment types in themedium data track.
We can observe that forZH?EN word alignment, both SH and SSH pro-duce far more 1-to-0 links than Model 4.
It canalso be seen that Model 4 tends to produce moreconsecutive 1-to-n links than non-consecutive 1-to-n links.
On the other hand, the SSH modeltends to produce more non-consecutive 1-to-nlinks than consecutive ones.
Compared to SH,SSH tends to produce more 1-to-1 links than 1-to-n links, indicating that adding syntactic de-pendency constraints biases the model towardsonly producing 1-to-n links when the n wordsfollow coherence constraint, i.e.
the first and lastword in the chunk have syntactic dependencies.For example, among the 6.24% consecutive ZH?EN 1-to-n links produced by SSH, 43.22% ofthem follow the coherence constraint comparedto just 39.89% in SH.
These properties can havesignificant implications for the performance ofour MT systems given that we use the grow-diag-final heuristics to derive the symmetrisedword alignment based on bidirectional asymmet-ric word alignments.6 Conclusions and Future WorkIn this paper, we extended the HMM word-to-phrase word alignment model to handle syntac-tic dependencies.
We found that our model wasconsistently better than that without syntactic de-pendencies according to both intrinsic and ex-trinsic evaluation.
Our model is shown to be ben-eficial to PB-SMT under a small data conditionand to a Hiero-style system under a larger datacondition.As to future work, we firstly plan to investi-gate the impact of parsing quality on our model,and the use of different heuristics to combineword alignments.
Secondly, the syntactic co-herence model itself is very simple, in that itonly covers the syntactic dependency betweenthe first and last word in a phrase.
Accordingly,we intend to extend this model to cover more so-phisticated syntactic relations within the phrase.Furthermore, given that we can construct dif-ferent MT systems using different word align-ments, multiple system combination can be con-ducted to avail of the advantages of different sys-tems.
We also plan to compare our model withother alignment models, e.g.
(Fraser and Marcu,2007a), and test this approach on more data andon different language pairs and translation direc-tions.AcknowledgementsThis research is supported by the Science Foundation Ire-land (Grant 07/CE/I1142) as part of the Centre for NextGeneration Localisation (www.cngl.ie) at Dublin City Uni-versity.
Part of the work was carried out at Cambridge Uni-versity Engineering Department with Dr. William Byrne.The authors would also like to thank the anonymous re-viewers for their insightful comments.108ReferencesBaum, Leonard E. 1972.
An inequality and associ-ated maximization technique in statistical estimation forprobabilistic functions of Markov processes.
Inequali-ties, 3:1?8.Blunsom, Phil, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A gibbs sampler for phrasal synchronousgrammar induction.
In Proceedings of ACL-IJCNLP2009, pages 782?790, Singapore.Brown, Peter F., Stephen A. Della-Pietra, Vincent J. Della-Pietra, and Robert L. Mercer.
1993.
The mathematics ofStatistical Machine Translation: Parameter estimation.Computational Linguistics, 19(2):263?311.Cherry, Colin and Dekang Lin.
2006.
Soft syntactic con-straints for word alignment through discriminative train-ing.
In Proceedings of the COLING-ACL 2006, pages105?112, Sydney, Australia.Dempster, Arthur, Nan Laird, and Donald Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.DeNero, John and Dan Klein.
2008.
The complexity ofphrase alignment problems.
In Proceedings of ACL-08:HLT, Short Papers, pages 25?28, Columbus, OH.Deng, Yonggang and William Byrne.
2006.
MTTK: Analignment toolkit for Statistical Machine Translation.
InProceedings of HLT-NAACL 2006, pages 265?268, NewYork City, NY.Deng, Yonggang and William Byrne.
2008.
HMM wordand phrase alignment for Statistical Machine Transla-tion.
IEEE Transactions on Audio, Speech, and Lan-guage Processing, 16(3):494?507.Fox, Heidi.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proceedings of the EMNLP 2002, pages304?3111, Philadelphia, PA, July.Fraser, Alexander and Daniel Marcu.
2007a.
Getting thestructure right for word alignment: LEAF.
In Pro-ceedings of EMNLP-CoNLL 2007, pages 51?60, Prague,Czech Republic.Fraser, Alexander and Daniel Marcu.
2007b.
Measuringword alignment quality for Statistical Machine Transla-tion.
Computational Linguistics, 33(3):293?303.Kneser, Reinhard and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the IEEE ICASSP, volume 1, pages 181?184, Detroit, MI.Koehn, Philipp, Franz Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceedingsof HLT-NAACL 2003, pages 48?54, Edmonton, AB,Canada.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkit forStatistical Machine Translation.
In Proceedings of ACL2007, pages 177?180, Prague, Czech Republic.Li, Zhifei, Chris Callison-Burch, Chris Dyer, SanjeevKhudanpur, Lane Schwartz, Wren Thornton, JonathanWeese, and Omar Zaidan.
2009.
Joshua: An opensource toolkit for parsing-based machine translation.
InProceedings of the WMT 2009, pages 135?139, Athens,Greece.Marcu, Daniel and William Wong.
2002.
A Phrase-Based,joint probability model for Statistical Machine Transla-tion.
In Proceedings of EMNLP 2002, pages 133?139,Philadelphia, PA.Murphy, Kevin.
2002.
Hidden semi-markov models (seg-ment models).
Technical report, UC Berkeley.Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Ervin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency parsing.Natural Language Engineering, 13(2):95?135.Noreen, Eric W. 1989.
Computer-Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley-Interscience, New York, NY.Och, Franz and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Compu-tational Linguistics, 29(1):19?51.Och, Franz.
2003.
Minimum Error Rate Training in Statis-tical Machine Translation.
In Proceedings of ACL 2003,pages 160?167, Sapporo, Japan.Ostendorf, Mari, Vassilios V. Digalakis, and Owen A. Kim-ball.
1996.
From HMMs to segment models: A uni-fied view of stochastic modeling for speech recognition.IEEE Transactions on Speech and Audio Processing,4(5):360?378.Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of Machine Translation.
In Proceedings of ACL2002, pages 311?318, Philadelphia, PA.Ratnaparkhi, Adwait.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proceedings of EMNLP1996, pages 133?142, Somerset, NJ.Stolcke, Andreas.
2002.
SRILM ?
An extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, CO.Vogel, Stefan, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of ACL 1996, pages 836?841,Copenhagen, Denmark.109
