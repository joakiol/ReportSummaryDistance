Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113?1120,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning to Say It Well:Reranking Realizations by Predicted Synthesis QualityCrystal Nakatsu and Michael WhiteDepartment of LinguisticsThe Ohio State UniversityColumbus, OH 43210 USAfcnakatsu,mwhiteg@ling.ohio-state.eduAbstractThis paper presents a method for adaptinga language generator to the strengths andweaknesses of a synthetic voice, therebyimproving the naturalness of syntheticspeech in a spoken language dialogue sys-tem.
The method trains a discriminativereranker to select paraphrases that are pre-dicted to sound natural when synthesized.The ranker is trained on realizer and syn-thesizer features in supervised fashion, us-ing human judgements of synthetic voicequality on a sample of the paraphrases rep-resentative of the generator?s capability.Results from a cross-validation study indi-cate that discriminative paraphrase rerank-ing can achieve substantial improvementsin naturalness on average, ameliorating theproblem of highly variable synthesis qual-ity typically encountered with today?s unitselection synthesizers.1 IntroductionUnit selection synthesis1?a technique which con-catenates segments of natural speech selected froma database?has been found to be capable of pro-ducing high quality synthetic speech, especiallyfor utterances that are similar to the speech in thedatabase in terms of style, delivery, and coverage(Black and Lenzo, 2001).
In particular, in the lim-ited domain of a spoken language dialogue sys-tem, it is possible to achieve highly natural synthe-sis with a purpose-built voice (Black and Lenzo,2000).
However, it can be difficult to develop1See e.g.
(Hunt and Black, 1996; Black and Taylor, 1997;Beutnagel et al, 1999).a synthetic voice for a dialogue system that pro-duces natural speech completely reliably, and thusin practice output quality can be quite variable.Two important factors in this regard are the label-ing process for the speech database and the direc-tion of the dialogue system?s further development,after the voice has been built: when labels are as-signed fully automatically to the recorded speech,label boundaries may be inaccurate, leading to un-natural sounding joins in speech output; and whenfurther system development leads to the genera-tion of utterances that are less like those in therecording script, such utterances must be synthe-sized using smaller units with more joins betweenthem, which can lead to a considerable dropoff inquality.As suggested by Bulyko and Ostendorf (2002),one avenue for improving synthesis quality in a di-alogue system is to have the system choose whatto say in part by taking into account what is likelyto sound natural when synthesized.
The idea isto take advantage of the generator?s periphrasticability:2 given a set of generated paraphrases thatsuitably express the desired content in the dialoguecontext, the system can select the specific para-phrase to use as its response according to the pre-dicted quality of the speech synthesized for thatparaphrase.
In this way, if there are significantdifferences in the predicted synthesis quality forthe various paraphrases?and if these predictionsare generally borne out?then, by selecting para-phrases with high predicted synthesis quality, thedialogue system (as a whole) can more reliablyproduce natural sounding speech.In this paper, we present an application of dis-2See e.g.
(Iordanskaja et al, 1991; Langkilde and Knight,1998; Barzilay and McKeown, 2001; Pang et al, 2003) fordiscussion of paraphrase in generation.1113criminative reranking to the task of adapting a lan-guage generator to the strengths and weaknessesof a particular synthetic voice.
Our method in-volves training a reranker to select paraphrasesthat are predicted to sound natural when synthe-sized, from the N-best realizations produced bythe generator.
The ranker is trained in super-vised fashion, using human judgements of syn-thetic voice quality on a representative sample ofthe paraphrases.
In principle, the method can beemployed with any speech synthesizer.
Addition-ally, when features derived from the synthesizer?sunit selection search can be made available, fur-ther quality improvements become possible.The paper is organized as follows.
In Section 2,we review previous work on integrating choice inlanguage generation and speech synthesis, and onlearning discriminative rerankers for generation.In Section 3, we present our method.
In Section 4,we describe a cross-validation study whose resultsindicate that discriminative paraphrase rerankingcan achieve substantial improvements in natural-ness on average.
Finally, in Section 5, we con-clude with a summary and a discussion of futurework.2 Previous WorkMost previous work on integrating language gen-eration and synthesis, e.g.
(Davis and Hirschberg,1988; Prevost and Steedman, 1994; Hitzeman etal., 1998; Pan et al, 2002), has focused on howto use the information present in the languagegeneration component in order to specify contex-tually appropriate intonation for the speech syn-thesizer to target.
For example, syntactic struc-ture, information structure and dialogue contexthave all been argued to play a role in improvingprosody prediction, compared to unrestricted text-to-speech synthesis.
While this topic remains animportant area of research, our focus is insteadon a different opportunity that arises in a dialoguesystem, namely, the possibility of choosing the ex-act wording and prosody of a response accordingto how natural it is likely to sound when synthe-sized.To our knowledge, Bulyko and Ostendorf(2002) were the first to propose allowing thechoice of wording and prosody to be jointly deter-mined by the language generator and speech syn-thesizer.
In their approach, a template-based gen-erator passes a prosodically annotated word net-work to the speech synthesizer, rather than a singletext string (or prosodically annotated text string).To perform the unit selection search on this ex-panded input efficiently, they employ weightedfinite-state transducers, where each step of net-work expansion is then followed by minimiza-tion.
The weights are determined by concatena-tion (join) costs, relative frequencies (negative logprobabilities) of the word sequences, and prosodicprediction costs, for cases where the prosody isnot determined by the templates.
In a perceptionexperiment, they demonstrated that by expand-ing the space of candidate responses, their systemachieved higher quality speech output.Following (Bulyko and Ostendorf, 2002), Stoneet al (2004) developed a method for jointly de-termining wording, speech and gesture.
In theirapproach, a template-based generator producesa word lattice with intonational phrase breaks.A unit selection algorithm then searches for alow-cost way of realizing a path through thislattice that combines captured motion sampleswith recorded speech samples to create coherentphrases, blending segments of speech and mo-tion together phrase-by-phrase into extended ut-terances.
Video demonstrations indicate that natu-ral and highly expressive results can be achieved,though no human evaluations are reported.In an alternative approach, Pan and Weng(2002) proposed integrating instance-based real-ization and synthesis.
In their framework, sen-tence structure, wording, prosody and speechwaveforms from a domain-specific corpus are si-multaneously reused.
To do so, they add prosodicand acoustic costs to the insertion, deletion andreplacement costs used for instance-based surfacerealization.
Their contribution focuses on how todesign an appropriate speech corpus to facilitatean integrated approach to instance-based realiza-tion and synthesis, and does not report evaluationresults.A drawback of these approaches to integratingchoice in language generation and synthesis is thatthey cannot be used with most existing speech syn-thesizers, which do not accept (annotated) wordlattices as input.
In contrast, the approach we in-troduce here can be employed with any speechsynthesizer in principle.
All that is required isthat the language generator be capable of produc-ing N-best outputs; that is, the generator must beable to construct a set of suitable paraphrases ex-1114pressing the desired content, from which the topN realizations can be selected for reranking ac-cording to their predicted synthesis quality.
Oncethe realizations have been reranked, the top scor-ing realization can be sent to the synthesizer asusual.
Alternatively, when features derived fromthe synthesizer?s unit selection search can be madeavailable?and if the time demands of the dia-logue system permit?several of the top scoringreranked realizations can be sent to the synthe-sizer, and the resulting utterances can be rescoredwith the extended feature set.Our reranking approach has been inspired byprevious work on reranking in parsing and gen-eration, especially (Collins, 2000) and (Walker etal., 2002).
As in Walker et al?s (2002) method fortraining a sentence plan ranker, we use our gen-erator to produce a representative sample of para-phrases and then solicit human judgements of theirnaturalness to use as data for training the ranker.This method is attractive when there is no suit-able corpus of naturally occurring dialogues avail-able for training purposes, as is often the case forsystems that engage in human-computer dialoguesthat differ substantially from human-human ones.The primary difference between Walker et al?swork and ours is that theirs examines the impacton text quality of sentence planning decisions suchas aggregation, whereas ours focuses on the im-pact of the lexical and syntactic choice at the sur-face realization level on speech synthesis quality,according to the strengths and weaknesses of aparticular synthetic voice.3 Reranking Realizations by PredictedSynthesis Quality3.1 Generating AlternativesOur experiments with integrating language gener-ation and synthesis have been carried out in thecontext of the COMIC3 multimodal dialogue sys-tem (den Os and Boves, 2003).
The COMIC sys-tem adds a dialogue interface to a CAD-like ap-plication used in sales situations to help clients re-design their bathrooms.
The input to the systemincludes speech, handwriting, and pen gestures;the output combines synthesized speech, an ani-mated talking head, deictic gestures at on-screenobjects, and direct control of the underlying appli-cation.3COnversational Multimodal Interaction with Computers,http://www.hcrc.ed.ac.uk/comic/.Drawing on the materials used in (Foster andWhite, 2005) to evaluate adaptive generation inCOMIC, we selected a sample of 104 sentencesfrom 38 different output turns across three dia-logues.
For each sentence in the set, a variant wasincluded that expressed the same content adaptedto a different user model or adapted to a differ-ent dialogue history.
For example, a descriptionof a certain design?s colour scheme for one usermight be phrased as As you can see, the tiles havea blue and green colour scheme, whereas a vari-ant expression of the same content for a differentuser could be Although the tiles have a blue colourscheme, the design does also feature green, if theuser disprefers blue.In COMIC, the sentence planner uses XSLT togenerate disjunctive logical forms (LFs), whichspecify a range of possible paraphrases in a nestedfree-choice form (Foster and White, 2004).
Suchdisjunctive LFs can be efficiently realized us-ing the OpenCCG realizer (White, 2004; White,2006b; White, 2006a).
Note that for the experi-ments reported here, we manually augmented thedisjunctive LFs for the 104 sentences in our sam-ple to make greater use of the periphrastic capa-bilities of the COMIC grammar; it remains for fu-ture work to augment the COMIC sentence plan-ner produce these more richly disjunctive LFs au-tomatically.OpenCCG includes an extensible API for inte-grating language modeling and realization.
To se-lect preferred word orders, from among all thoseallowed by the grammar for the input LF, we useda backoff trigram model trained on approximately750 example target sentences, where certain wordswere replaced with their semantic classes (e.g.MANUFACTURER, COLOUR) for better general-ization.
For each of the 104 sentences in our sam-ple, we performed 25-best realization from the dis-junctive LF, and then randomly selected up to 12different realizations to include in our experimentsbased on a simulated coin flip for each realization,starting with the top-scoring one.
We used thisprocedure to sample from a larger portion of theN-best realizations, while keeping the sample sizemanageable.Figure 1 shows an example of 12 paraphrasesfor a sentence chosen for inclusion in our sample.Note that the realizations include words with pitchaccent annotations as well as boundary tones asseparate, punctuation-like words.
Generally the1115 thisHdesignHuses tiles from Villeroy and BochH?s Funny DayHcollection LL% . thisHdesignHis based on the Funny DayHcollec-tion by Villeroy and BochHLL% . thisHdesignHis based on Funny DayHLL% , byVilleroy and BochHLL% . thisHdesignHdraws from the Funny DayHcollec-tion by Villeroy and BochHLL% . thisHone draws from Funny DayHLL% , byVilleroy and BochHLL% . hereL+HLH% we have a design that is based onthe Funny DayHcollection by Villeroy and BochHLL% . thisHdesignHdraws from Villeroy and BochH?sFunny DayHseries LL% . here is a design that draws from Funny DayHLL% ,by Villeroy and BochHLL% . thisHone draws from Villeroy and BochH?sFunny DayHcollection LL% . thisHdraws from the Funny DayHcollection byVilleroy and BochHLL% . thisHone draws from the Funny DayHcollection byVilleroy and BochHLL% . here is a design that draws from Villeroy and BochH?s Funny DayHcollection LL% .Figure 1: Example of sampled periphrastic alter-natives for a sentence.quality of the sampled paraphrases is very high,only occasionally including dispreferred word or-ders such as We here have a design in the familystyle, where here is in medial position rather thanfronted.43.2 Synthesizing UtterancesFor synthesis, OpenCCG?s output realizations areconverted to APML,5 a markup language whichallows pitch accents and boundary tones to bespecified, and then passed to the Festival speechsynthesis system (Taylor et al, 1998; Clark et al,2004).
Festival uses the prosodic markup in thetext analysis phase of synthesis in place of thestructures that it would otherwise have to predictfrom the text.
The synthesiser then uses the con-text provided by the markup to enforce the selec-4In other examples medial position is preferred, e.g.
Thisdesign here is in the family style.5Affective Presentation Markup Language; seehttp://www.cstr.ed.ac.uk/projects/festival/apml.html.tion of suitable units from the database.A custom synthetic voice for the COMIC sys-tem was developed, as follows.
First, a domain-specific recording script was prepared by select-ing about 150 sentences from the larger set of tar-get sentences used to train the system?s n-grammodel.
The sentences were greedily selected withthe goals of ensuring that (i) all words (includingproper names) in the target sentences appeared atleast once in the record script, and (ii) all bigramsat the level of semantic classes (e.g.
MANUFAC-TURER, COLOUR) were covered as well.
For thecross-validation study reported in the next section,we also built a trigram model on the words in thedomain-specific recording script, without replac-ing any words with semantic classes, so that wecould examine whether the more frequent occur-rence of the specific words and phrases in this partof the script is predictive of synthesis quality.The domain-specific script was augmented witha set of 600 newspaper sentences selected for di-phone coverage.
The newspaper sentences makeit possible for the voice to synthesize words out-side of the domain-specific script, though notnecessarily with the same quality.
Once thesescripts were in place, an amateur voice talent wasrecorded reading the sentences in the scripts dur-ing two recording sessions.
Finally, after thespeech files were semi-automatically segmentedinto individual sentences, the speech database wasconstructed, using fully automatic labeling.We have found that the utterances synthesizedwith the COMIC voice vary considerably in theirnaturalness, due to two main factors.
First, thesystem underwent further development after thevoice was built, leading to the addition of a va-riety of new phrases to the system?s repertoire, aswell as many extra proper names (and their pro-nunciations); since these names and phrases usu-ally require going outside of the domain-specificpart of the speech database, they often (though notalways) exhibit a considerable dropoff in synthe-sis quality.6 And second, the boundaries of the au-tomatically assigned unit labels were not alwaysaccurate, leading to problems with unnatural joinsand reduced intelligibility.
To improve the reliabil-ity of the COMIC voice, we could have recordedmore speech, or manually corrected label bound-6Note that in the current version of the system, propernames are always required parts of the output, and thus thediscriminative reranker cannot learn to simply choose para-phrases that leave out problematic names.1116aries; the goal of this paper is to examine whetherthe naturalness of a dialogue system?s output canbe improved in a less labor-intensive way.3.3 Rating Synthesis QualityTo obtain data for training our realization reranker,we solicited judgements of the naturalness of thesynthesized speech produced by Festival for theutterances in our sample COMIC corpus.
Twojudges (the first two authors) provided judgementson a 1?7 point scale, with higher scores represent-ing more natural synthesis.
Ratings were gatheredusing WebExp2,7 with the periphrastic alternativesfor each sentence presented as a group in a ran-domized order.
Note that for practical reasons,the utterances were presented out of the dialoguecontext, though both judges were familiar with thekinds of dialogues that the COMIC system is ca-pable of.Though the numbers on the seven point scalewere not assigned labels, they were roughly takento be ?horrible,?
?poor,?
?fair,?
?ok,?
?good,?
?verygood?
and ?perfect.?
The average assigned ratingacross all utterances was 4.05 (?ok?
), with a stan-dard deviation of 1.56.
The correlation betweenthe two judges?
ratings was 0.45, with one judge?sratings consistently higher than the other?s.Some common problems noted by the judgesincluded slurred words, especially the sometimessounding like ther or even their; clipped words,such as has shortened at times to the point ofsounding like is, or though clipped to unintelligi-bility; unnatural phrasing or emphasis, e.g.
occa-sional pauses before a possessive ?s, or words suchas style sounding emphasized when they shouldbe deaccented; unnatural rate changes; ?choppy?speech from poor joins; and some unintelligibleproper names.3.4 RankingWhile Collins (2000) and Walker et al (2002)develop their rankers using the RankBoost algo-rithm (Freund et al, 1998), we have instead cho-sen to use Joachims?
(2002) method of formu-lating ranking tasks as Support Vector Machine(SVM) constraint optimization problems.8 Thischoice has been motivated primarily by conve-nience, as Joachims?
SVMlight package is easy to7http://www.hcrc.ed.ac.uk/web exp/8See (Barzilay and Lapata, 2005) for another applicationof SVM ranking in generation, namely to the task of rankingalternative text orderings for local coherence.use; we leave it for future work to compare theperformance of RankBoost and SVMlight on ourranking task.The ranker takes as input a set of paraphrasesthat express the desired content of each sentence,optionally together with synthesized utterancesfor each paraphrase.
The output is a ranking ofthe paraphrases according to the predicted natu-ralness of their corresponding synthesized utter-ances.
Ranking is more appropriate than classifi-cation for our purposes, as naturalnesss is a gradedassessment rather than a categorical one.To encode the ranking task as an SVM con-straint optimization problem, each paraphrase jof a sentence i is represented by a feature vector(sij) = hf1(sij); : : : ; fm(sij)i, where m is thenumber of features.
In the training data, the fea-ture vectors are paired with the average value oftheir corresponding human judgements of natural-ness.
From this data, ordered pairs of paraphrases(sij; sik) are derived, where sijhas a higher nat-uralness rating than sik.
The constraint optimiza-tion problem is then to derive a parameter vector~w that yields a ranking score function ~w  (sij)which minimizes the number of pairwise rank-ing violations.
Ideally, for every ordered pair(sij; sik), we would have ~w (sij) > ~w (sik);in practice, it is often impossible or intractable tofind such a parameter vector, and thus slack vari-ables are introduced that allow for training errors.A parameter to the algorithm controls the trade-offbetween ranking margin and training error.In testing, the ranker?s accuracy can be deter-mined by comparing the ranking scores for ev-ery ordered pair (sij; sik) in the test data, anddetermining whether the actual preferences areborne out by the predicted preference, i.e.
whether~w  (sij) > ~w  (sik) as desired.
Note thatthe ranking scores, unlike the original ratings, donot have any meaning in the absolute sense; theirimport is only to order alternative paraphrases bytheir predicted naturalness.In our ranking experiments, we have usedSVMlight with all parameters set to their defaultvalues.3.5 FeaturesTable 1 shows the feature sets we have investigatedfor reranking, distinguished by the availability ofthe features and the need for discriminative train-ing.
The first row shows the feature sets that are1117Table 1: Feature sets for reranking.DiscriminativeAvailability no yesRealizer NGRAMS WORDSSynthesizer COSTS ALLavailable to the realizer.
There are two n-grammodels that can be used to directly rank alterna-tive realizations: NGRAM-1, the language modelused in COMIC, and NGRAM-2, the languagemodel derived from the domain-specific recordingscript; for feature values, the negative logarithmsare used.
There are also two WORDS featuresets (shown in the second column): WORDS-BI,which includes NGRAMS plus a feature for everypossible unigram and bigram, where the value ofthe feature is the count of the unigram or bigramin a given realization; and WORDS-TRI, whichincludes all the features in WORDS-BI, plus afeature for every possible trigram.
The secondrow shows the feature sets that require informa-tion from the synthesizer.
The COSTS feature setincludes NGRAMS plus the total join and targetcosts from the unit selection search.
Note that aweighted sum of these costs could be used to di-rectly rerank realizations, in much the same wayas relative frequencies and concatenation costs areused in (Bulyko and Ostendorf, 2002); in ourexperiments, we let SVMlight determine how toweight these costs.
Finally, there are two ALL fea-ture sets: ALL-BI includes NGRAMS, WORDS-BI and COSTS, plus features for every possi-ble phone and diphone, and features for everyspecific unit in the database; ALL-TRI includesNGRAMS, WORDS-TRI, COSTS, and a featurefor every phone, diphone and triphone, as well asspecific units in the database.
As with WORDS,the value of a feature is the count of that feature ina given synthesized utterance.4 Cross-Validation StudyTo train and test our ranker on our feature sets,we partitioned the corpus into 10 folds and per-formed 10-fold cross-validation.
For each fold,90% of the examples were used for training theranker and the remaining unseen 10% were usedfor testing.
The folds were created by randomlychoosing from among the sentence groups, result-ing in all of the paraphrases for a given sentenceoccurring in the same fold, and each occurring ex-Table 2: Comparison of results for differing fea-ture sets, topline and baseline.Features Mean Score SD Accuracy (%)BEST 5.38 1.11 100.0WORDS-TRI 4.95 1.24 77.3ALL-BI 4.95 1.24 77.9ALL-TRI 4.90 1.25 78.0WORDS-BI 4.86 1.28 76.8COSTS 4.69 1.27 68.2NGRAM-2 4.34 1.38 56.2NGRAM-1 4.30 1.29 53.3RANDOM 4.11 1.22 50.0actly once in the testing set as a whole.We evaluated the performance of our rankerby determining the average score of the bestranked paraphrase for each sentence, under eachof the following feature combinations: NGRAM-1, NGRAM-2, COSTS, WORDS-BI, WORDS-TRI, ALL-BI, and ALL-TRI.
Note that since weused the human ratings to calculate the score ofthe highest ranked utterance, the score of the high-est ranked utterance cannot be higher than thatof the highest human-rated utterance.
Therefore,we effectively set the human ratings as the topline(BEST).
For the baseline, we randomly chose anutterance from among the alternatives, and usedits associated score.
In 15 tests generating the ran-dom scores, our average scores ranged from 3.88?4.18.
We report the median score of 4.11 as theaverage for the baseline, along with the mean ofthe topline and each of the feature subsets, in Ta-ble 2.We also report the ordering accuracy of eachfeature set used by the ranker in Table 2.
As men-tioned in Section 3.4, the ordering accuracy of theranker using a given feature set is determined byc=N , where c is the number of correctly orderedpairs (of each paraphrase, not just the top rankedone) produced by the ranker, and N is the totalnumber of human-ranked ordered pairs.As Table 2 indicates, the mean of BEST is 5.38,whereas our ranker using WORDS-TRI featuresachieves a mean score of 4.95.
This is a differenceof 0.42 on a seven point scale, or only a 6% dif-ference.
The ordering accuracy of WORDS-TRIis 77.3%.We also measured the improvement of ourranker with each feature set over the random base-line as a percentage of the maximum possiblegain (which would be to reproduce the humantopline).
The results appear in Figure 2.
As the1118010203040506070NGRAM-1NGRAM-2COSTSWORDS-BIALL-TRIALL-BIWORDS-TRIFigure 2: Improvement as a percentage of themaximum possible gain over the random baseline.figure indicates, the maximum possible gain ourranker achieves over the baseline is 66% (using theWORDS-TRI or ALL-BI feature set) .
By com-parison, NGRAM-1 and NGRAM-2 achieve lessthan 20% of the possible gain.To verify our main hypothesis that our rankerwould significantly outperform the baselines,we computed paired one-tailed t-tests betweenWORDS-TRI and RANDOM (t = 2:4, p <8:9x10 13), and WORDS-TRI and NGRAM-1(t = 1:4, p < 4:5x10 8).
Both differences werehighly significant.
We also performed seven post-hoc comparisons using two-tailed t-tests, as wedid not have an a priori expectation as to whichfeature set would work better.
Using the Bonfer-roni adjustment for multiple comparisons, the p-value required to achieve an overall level of signif-icance of 0.05 is 0.007.
In the first post-hoc test,we found a significant difference between BESTand WORDS-TRI (t = 8:0,p < 1:86x10 12),indicating that there is room for improvement ofour ranker.
However, in considering the top scor-ing feature sets, we did not find a significant dif-ference between WORDS-TRI and WORDS-BI(t = 2:3, p < 0:022), from which we infer that thedifference among all of WORDS-TRI, ALL-BI,ALL-TRI and WORDS-BI is not significant also.This suggests that the synthesizer features haveno substantial impact on our ranker, as we wouldexpect ALL-TRI to be significantly higher thanWORDS-TRI if so.
However, since COSTS doessignificantly improve upon NGRAM2 (t = 3:5,p < 0:001), there is some value to the use of syn-thesizer features in the absence of WORDS.
Wealso looked at the comparison for the WORDSmodels and COSTS.
While WORDS-BI did notperform significantly better than COSTS ( t =2:3, p < 0:025), the added trigrams in WORDS-TRI did improve ranker performance significantlyover COSTS (t = 3:7, p < 3:29x10 4).
SinceCOSTS ranks realizations in the much the sameway as (Bulyko and Ostendorf, 2002), the fact thatWORDS-TRI outperforms COSTS indicates thatour discriminative reranking method can signifi-cantly improve upon their non-discriminative ap-proach.5 ConclusionsIn this paper, we have presented a method foradapting a language generator to the strengthsand weaknesses of a particular synthetic voice bytraining a discriminative reranker to select para-phrases that are predicted to sound natural whensynthesized.
In contrast to previous work onthis topic, our method can be employed with anyspeech synthesizer in principle, so long as fea-tures derived from the synthesizer?s unit selec-tion search can be made available.
In a casestudy with the COMIC dialogue system, we havedemonstrated substantial improvements in the nat-uralness of the resulting synthetic speech, achiev-ing two-thirds of the maximum possible gain, andraising the average rating from ?ok?
to ?good.?
Wehave also shown that in this study, our discrimina-tive method significantly outperforms an approachthat performs selection based solely on corpus fre-quencies together with target and join costs.In future work, we intend to verify the resultsof our cross-validation study in a perception ex-periment with na?
?ve subjects.
We also plan to in-vestigate whether additional features derived fromthe synthesizer can better detect unnatural pausesor changes in speech rate, as well as F0 contoursthat fail to exhibit the targeting accenting pattern.Finally, we plan to examine whether gains in qual-ity can be achieved with an off-the-shelf, generalpurpose voice that are similar to those we have ob-served using COMIC?s limited domain voice.AcknowledgementsWe thank Mary Ellen Foster, Eric Fosler-Lussierand the anonymous reviewers for helpful com-ments and discussion.ReferencesRegina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.
In Pro-1119ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, Ann Arbor.Regina Barzilay and Kathleen McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Proc.ACL/EACL.M.
Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou,and A. Syrdal.
1999.
The AT&T Next-Gen TTSsystem.
In Joint Meeting of ASA, EAA, and DAGA.Alan Black and Kevin Lenzo.
2000.
Limited domainsynthesis.
In Proceedings of ICSLP2000, Beijing,China.Alan Black and Kevin Lenzo.
2001.
Optimal dataselection for unit selection synthesis.
In 4th ISCASpeech Synthesis Workshop, Pitlochry, Scotland.Alan Black and Paul Taylor.
1997.
Automatically clus-tering similar units for unit selection in speech syn-thesis.
In Eurospeech ?97.Ivan Bulyko and Mari Ostendorf.
2002.
Efficient in-tegrated response generation from multiple targetsusing weighted finite state transducers.
ComputerSpeech and Language, 16:533?550.Robert A.J.
Clark, Korin Richmond, and Simon King.2004.
Festival 2 ?
build your own general pur-pose unit selection speech synthesiser.
In 5th ISCASpeech Synthesis Workshop, pages 173?178, Pitts-burgh, PA.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proc.
ICML.James Raymond Davis and Julia Hirschberg.
1988.Assigning intonational features in synthesized spo-ken directions.
In Proc.
ACL.Els den Os and Lou Boves.
2003.
Towards ambientintelligence: Multimodal computers that understandour intentions.
In Proc.
eChallenges-03.Mary Ellen Foster and Michael White.
2004.
Tech-niques for Text Planning with XSLT.
In Proc.
4thNLPXML Workshop.Mary Ellen Foster and Michael White.
2005.
As-sessing the impact of adaptive generation in theCOMIC multimodal dialogue system.
In Proc.IJCAI-05 Workshop on Knowledge and Representa-tion in Practical Dialogue Systems.Y.
Freund, R. Iyer, R. E. Schapire, and Y.
Singer.
1998.An efficient boosting algorithm for combining pref-erences.
In Machine Learning: Proc.
of the Fif-teenth International Conference.Janet Hitzeman, Alan W. Black, Chris Mellish, JonOberlander, and Paul Taylor.
1998.
On the use ofautomatically generated discourse-level informationin a concept-to-speech synthesis system.
In Proc.ICSLP-98.A.
Hunt and A.
Black.
1996.
Unit selection in aconcatenative speech synthesis system using a largespeech database.
In Proc.
ICASSP-96, Atlanta,Georgia.Lidija Iordanskaja, Richard Kittredge, and AlainPolgu?ere.
1991.
Lexical selection and paraphrasein a meaning-text generation model.
In Ce?cile L.Paris, William R. Swartout, and William C. Mann,editors, Natural Language Generation in ArtificialIntelligence and Computational Linguistics, pages293?312.
Kluwer.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proc.
KDD.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProc.
COLING-ACL.Shimei Pan and Wubin Weng.
2002.
Designing aspeech corpus for instance-based spoken languagegeneration.
In Proc.
of the International NaturalLanguage Generation Conference (INLG-02).Shimei Pan, Kathleen McKeown, and Julia Hirschberg.2002.
Exploring features from natural languagegeneration for prosody modeling.
Computer Speechand Language, 16:457?490.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations:Extracting paraphrases and generating new sen-tences.
In Proc.
HLT/NAACL.Scott Prevost and Mark Steedman.
1994.
Specify-ing intonation from context for speech synthesis.Speech Communication, 15:139?153.Matthew Stone, Doug DeCarlo, Insuk Oh, ChristianRodriguez, Adrian Stere, Alyssa Lees, and ChrisBregler.
2004.
Speaking with hands: Creating ani-mated conversational characters from recordings ofhuman performance.
ACM Transactions on Graph-ics (SIGGRAPH), 23(3).P.
Taylor, A.
Black, and R. Caley.
1998.
The architec-ture of the the Festival speech synthesis system.
InThird International Workshop on Speech Synthesis,Sydney, Australia.Marilyn A. Walker, Owen C. Rambow, and Monica Ro-gati.
2002.
Training a sentence planner for spo-ken dialogue using boosting.
Computer Speech andLanguage, 16:409?433.Michael White.
2004.
Reining in CCG Chart Realiza-tion.
In Proc.
INLG-04.Michael White.
2006a.
CCG chart realization fromdisjunctive logical forms.
In Proc.
INLG-06.
To ap-pear.Michael White.
2006b.
Efficient Realization of Coor-dinate Structures in Combinatory Categorial Gram-mar.
Research on Language & Computation, on-line first, March.1120
