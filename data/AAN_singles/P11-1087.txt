Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 865?874,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Hierarchical Pitman-Yor Process HMMfor Unsupervised Part of Speech InductionPhil BlunsomDepartment of Computer ScienceUniversity of OxfordPhil.Blunsom@cs.ox.ac.ukTrevor CohnDepartment of Computer ScienceUniversity of SheffieldT.Cohn@dcs.shef.ac.ukAbstractIn this work we address the problem ofunsupervised part-of-speech inductionby bringing together several strands ofresearch into a single model.
We develop anovel hidden Markov model incorporatingsophisticated smoothing using a hierarchicalPitman-Yor processes prior, providing anelegant and principled means of incorporatinglexical characteristics.
Central to ourapproach is a new type-based samplingalgorithm for hierarchical Pitman-Yor modelsin which we track fractional table counts.In an empirical evaluation we show that ourmodel consistently out-performs the currentstate-of-the-art across 10 languages.1 IntroductionUnsupervised part-of-speech (PoS) induction haslong been a central challenge in computationallinguistics, with applications in human languagelearning and for developing portable languageprocessing systems.
Despite considerable researcheffort, progress in fully unsupervised PoS inductionhas been slow and modern systems barely improveover the early Brown et al (1992) approach(Christodoulopoulos et al, 2010).
One popularmeans of improving tagging performance is toinclude supervision in the form of a tag dictionaryor similar, however this limits portability andalso comprimises any cognitive conclusions.
Inthis paper we present a novel approach to fullyunsupervised PoS induction which uniformlyoutperforms the existing state-of-the-art across allour corpora in 10 different languages.
Moreover, theperformance of our unsupervised model approachesthat of many existing semi-supervised systems,despite our method not receiving any human input.In this paper we present a Bayesian hiddenMarkov model (HMM) which uses a non-parametricprior to infer a latent tagging for a sequence ofwords.
HMMs have been popular for unsupervisedPoS induction from its very beginnings (Brownet al, 1992), and justifiably so, as the mostdiscriminating feature for deciding a word?s PoS isits local syntactic context.Our work brings together several strands ofresearch including Bayesian non-parametric HMMs(Goldwater and Griffiths, 2007), Pitman-Yorlanguage models (Teh, 2006b; Goldwater etal., 2006b), tagging constraints over word types(Brown et al, 1992) and the incorporation ofmorphological features (Clark, 2003).
The resultis a non-parametric Bayesian HMM which avoidsoverfitting, contains no free parameters, andexhibits good scaling properties.
Our model usesa hierarchical Pitman-Yor process (PYP) prior toaffect sophisicated smoothing over the transitionand emission distributions.
This allows themodelling of sub-word structure, thereby capturingtag-specific morphological variation.
Unlike manyexisting approaches, our model is a principledgenerative model and does not include any handtuned language specific features.Inspired by previous successful approaches(Brown et al, 1992), we develop a new type-level inference procedure in the form of anMCMC sampler with an approximate method forincorporating the complex dependencies that arisebetween jointly sampled events.
Our experimentalevaluation demonstrates that our model, particularlywhen restricted to a single tag per type, produces865state-of-the-art results across a range of corpora andlanguages.2 BackgroundPast research in unsupervised PoS induction haslargely been driven by two different motivations: atask based perspective which has focussed on induc-ing word classes to improve various applications,and a linguistic perspective where the aim is toinduce classes which correspond closely to anno-tated part-of-speech corpora.
Early work was firmlysitutated in the task-based setting of improving gen-eralisation in language models.
Brown et al (1992)presented a simple first-order HMM which restrictedword types to always be generated from the sameclass.
Though PoS induction was not their aim, thisrestriction is largely validated by empirical analysisof treebanked data, and moreover conveys the sig-nificant advantage that all the tags for a given wordtype can be updated at the same time, allowing veryefficient inference using the exchange algorithm.This model has been popular for language mod-elling and bilingual word alignment, and an imple-mentation with improved inference called mkcls(Och, 1999)1 has become a standard part of statis-tical machine translation systems.The HMM ignores orthographic information,which is often highly indicative of a word?s part-of-speech, particularly so in morphologically richlanguages.
For this reason Clark (2003) extendedBrown et al (1992)?s HMM by incorporating acharacter language model, allowing the modellingof limited morphology.
Our work draws from thesemodels, in that we develop a HMM with a oneclass per tag restriction and include a characterlevel language model.
In contrast to these previousworks which use the maximum likelihood estimate,we develop a Bayesian model with a rich prior forsmoothing the parameter estimates, allowing us tomove to a trigram model.A number of researchers have investigated a semi-supervised PoS induction task in which a tag dictio-nary or similar data is supplied a priori (Smith andEisner, 2005; Haghighi and Klein, 2006; Goldwaterand Griffiths, 2007; Toutanova and Johnson, 2008;Ravi and Knight, 2009).
These systems achieve1Available from http://fjoch.com/mkcls.html.much higher accuracy than fully unsupervised sys-tems, though it is unclear whether the tag dictionaryassumption has real world application.
We focussolely on the fully unsupervised scenario, which webelieve is more practical for text processing in newlanguages and domains.Recent work on unsupervised PoS induction hasfocussed on encouraging sparsity in the emissiondistributions in order to match empirical distribu-tions derived from treebank data (Goldwater andGriffiths, 2007; Johnson, 2007; Gao and Johnson,2008).
These authors took a Bayesian approachusing a Dirichlet prior to encourage sparse distri-butions over the word types emitted from each tag.Conversely, Ganchev et al (2010) developed a tech-nique to optimize the more desirable reverse prop-erty of the word types having a sparse posterior dis-tribution over tags.
Recently Lee et al (2010) com-bined the one class per word type constraint (Brownet al, 1992) in a HMM with a Dirichlet prior toachieve both forms of sparsity.
However this workapproximated the derivation of the Gibbs sampler(omitting the interdependence between events whensampling from a collapsed model), resulting in amodel which underperformed Brown et al (1992)?sone-class HMM.Our work also seeks to enforce both forms ofsparsity, by developing an algorithm for type-levelinference under the one class constraint.
This workdiffers from previous Bayesian models in that weexplicitly model a complex backoff path using ahierachical prior, such that our model jointly infersdistributions over tag trigrams, bigrams and uni-grams and whole words and their character levelrepresentation.
This smoothing is critical to ensureadequate generalisation from small data samples.Research in language modelling (Teh, 2006b;Goldwater et al, 2006a) and parsing (Cohn etal., 2010) has shown that models employingPitman-Yor priors can significantly outperform themore frequently used Dirichlet priors, especiallywhere complex hierarchical relationships existbetween latent variables.
In this work we applythese advances to unsupervised PoS tagging,developing a HMM smoothed using a Pitman-Yorprocess prior.8663 The PYP-HMMWe develop a trigram hidden Markov model whichmodels the joint probability of a sequence of latenttags, t, and words, w, asP?
(t,w) =L+1?l=1P?
(tl|tl?1, tl?2)P?
(wl|tl) ,where L = |w| = |t| and t0 = t?1 = tL+1 = $ areassigned a sentinel value to denote the start or end ofthe sentence.
A key decision in formulating such amodel is the smoothing of the tag trigram and emis-sion distributions, which would otherwise be too dif-ficult to estimate from small datasets.
Prior workin unsupervised PoS induction has employed simplesmoothing techniques, such as additive smoothingor Dirichlet priors (Goldwater and Griffiths, 2007;Johnson, 2007), however this body of work has over-looked recent advances in smoothing methods usedfor language modelling (Teh, 2006b; Goldwater etal., 2006b).
Here we build upon previous work bydeveloping a PoS induction model smoothed witha sophisticated non-parametric prior.
Our modeluses a hierarchical Pitman-Yor process prior for boththe transition and emission distributions, encodinga backoff path from complex distributions to suc-cesssively simpler ones.
The use of complex dis-tributions (e.g., over tag trigrams) allows for richexpressivity when sufficient evidence is available,while the hierarchy affords a means of backing offto simpler and more easily estimated distributionsotherwise.
The PYP has been shown to generatedistributions particularly well suited to modellinglanguage (Teh, 2006a; Goldwater et al, 2006b), andhas been shown to be a generalisation of Kneser-Neysmoothing, widely recognised as the best smoothingmethod for language modelling (Chen and Good-man, 1996).The model is depicted in the plate diagram in Fig-ure 1.
At its centre is a standard trigram HMM,which generates a sequence of tags and words,tl|tl?1, tl?2, T ?
Ttl?1,tl?2wl|tl, E ?
Etl .UBjTijEjCjkw1t1w2t2w3t3...DjFigure 1: Plate diagram representation of the trigramHMM.
The indexes i and j range over the set of tagsand k ranges over the set of characters.
Hyper-parametershave been omitted from the figure for clarity.The trigram transition distribution, Tij , is drawnfrom a hierarchical PYP prior which backs off to abigram Bj and then a unigram U distribution,Tij |aT , bT , Bj ?
PYP(aT , bT , Bj)Bj |aB, bB, U ?
PYP(aB, bB, U)U |aU , bU ?
PYP(aU , bU ,Uniform) ,where the prior over U has as its base distribition auniform distribution over the set of tags, while thepriors for Bj and Tij back off by discarding an itemof context.
This allows the modelling of trigramtag sequences, while smoothing these estimates withtheir corresponding bigram and unigram distribu-tions.
The degree of smoothing is regulated bythe hyper-parameters a and b which are tied acrosseach length of n-gram; these hyper-parameters areinferred during training, as described in 3.1.The tag-specific emission distributions, Ej , arealso drawn from a PYP prior,Ej |aE , bE , C ?
PYP(aE , bE , Cj) .We consider two different settings for the base distri-bution Cj : 1) a simple uniform distribution over thevocabulary (denoted HMM for the experiments insection 4); and 2) a character-level language model(denoted HMM+LM).
In many languages morpho-logical regularities correlate strongly with a word?spart-of-speech (e.g., suffixes in English), which wehope to capture using a basic character languagemodel.
This model was inspired by Clark (2003)867The big dog5 23 23 7b r o w nFigure 2: The conditioning structure of the hierarchicalPYP with an embedded character language models.who applied a character level distribution to the sin-gle class HMM (Brown et al, 1992).
We formu-late the character-level language model as a bigrammodel over the character sequence comprising wordwl,wlk|wlk?1, tl, C ?
Ctlwlk?1Cjk|aC , bC , Dj ?
PYP(aC , bC , Dj)Dj |aD, bD ?
PYP(aD, bD,Uniform) ,where k indexes the characters in the word and,in a slight abuse of notation, the character itself,w0 and is set to a special sentinel value denotingthe start of the sentence (ditto for a final end ofsentence marker) and the uniform base distributionranges over the set of characters.
We expect thatthe HMM+LM model will outperform the uniformHMM as it can capture many consistent morpholog-ical affixes and thereby better distinguish betweendifferent parts-of-speech.
The HMM+LM is shownin Figure 2, illustrating the decomposition of the tagsequence into n-grams and a word into its compo-nent character bigrams.3.1 TrainingIn order to induce a tagging under this model weuse Gibbs sampling, a Markov chain Monte Carlo(MCMC) technique for drawing samples from theposterior distribution over the tag sequences givenobserved word sequences.
We present two differentsampling strategies: First, a simple Gibbs samplerwhich randomly samples an update to a single taggiven all other tags; and second, a type-level sam-pler which updates all tags for a given word under aone-tag-per-word-type constraint.
In order to extracta single tag sequence to test our model against thegold standard we find the tag at each site with maxi-mum marginal probability in the sample set.Following standard practice, we performinference using a collapsed sampler wherebythe model parameters U,B, T,E and C aremarginalised out.
After marginalisation theposterior distribution under a PYP prior is describedby a variant of the Chinese Restaurant Process(CRP).
The CRP is based around the analogy ofa restaurant with an infinite number of tables,with customers entering one at a time and seatingthemselves at a table.
The choice of table isgoverned byP (zl = k|z?l) =??
?n?k ?al?1+b 1 ?
k ?
K?K?a+bl?1+b k = K?
+ 1(1)where zl is the table chosen by the lth customer, z?lis the seating arrangement of the l?
1 previous cus-tomers, n?k is the number of customers in z?l whoare seated at table k, K?
= K(z?l) is the total num-ber of tables in z?l, and z1 = 1 by definition.
Thearrangement of customers at tables defines a cluster-ing which exhibits a power-law behavior controlledby the hyperparameters a and b.To complete the restaurant analogy, a dish is thenserved to each table which is shared by all the cus-tomers seated there.
This corresponds to a drawfrom the base distribution, which in our case rangesover tags for the transition distribution, and wordsfor the observation distribution.
Overall the PYPleads to a distribution of the formP T (tl = i|z?l, t?l) =1n?h + bT?
(2)(n?hi ?K?hiaT +(K?h aT + bT)PB(i|z?l, t?l)),illustrating the trigram transition distribution, wheret?l are all previous tags, h = (tl?2, tl?1) is the con-ditioning bigram, n?hi is the count of the trigram hiin t?l, n?h the total count over all trigrams beginningwith h, K?hi the number of tables served dish i andPB(?)
is the base distribution, in this case the bigramdistribution.A hierarchy of PYPs can be formed by making thebase distribution of a PYP another PYP, following a868semantics whereby whenever a customer sits at anempty table in a restaurant, a new customer is alsosaid to enter the restaurant for its base distribution.That is, each table at one level is equivalent to a cus-tomer at the next deeper level, creating the invari-ants: K?hi = n?ui andK?ui = n?i , where u = tl?1indicates the unigram backoff context of h. Therecursion terminates at the lowest level where thebase distribution is static.
The hierarchical settingallows for the modelling of elaborate backoff pathsfrom rich and complex structure to successively sim-pler structures.Gibbs samplers Both our Gibbs samplers performthe same calculation of conditional tag distributions,and involve first decrementing all trigrams and emis-sions affected by a sampling action, and then rein-troducing the trigrams one at a time, conditioningtheir probabilities on the updated counts and tableconfigurations as we progress.The first local Gibbs sampler (PYP-HMM)updates a single tag assignment at a time, in asimilar fashion to Goldwater and Griffiths (2007).Changing one tag affects three trigrams, withposteriorP (tl|z?l, t?l,w) ?
P (tl?2, wl|z?l?2, t?l?2) ,where l?2 denotes the range l?2, l?1, l, l+1, l+2.The joint distribution over the three trigrams con-tained in tl?2 can be calculated using the PYP for-mulation.
This calculation is complicated by the factthat these events are not independent; the counts ofone trigram can affect the probability of later ones,and moreover, the table assignment for the trigrammay also affect the bigram and unigram counts, ofparticular import when the same tag occurs twice ina row such as in Figure 2.Many HMMs used for inducing word classes forlanguage modelling include the restriction that alloccurrences of a word type always appear with thesame class throughout the corpus (Brown et al,1992; Och, 1999; Clark, 2003).
Our second sampler(PYP-1HMM) restricts inference to taggings whichadhere to this one tag per type restriction.
Thisrestriction permits efficient inference techniques inwhich all tags of all occurrences of a word type areupdated in parallel.
Similar techniques have beenused for models with Dirichlet priors (Liang et al,2010), though one must be careful to manage thedependencies between multiple draws from the pos-terior.The dependency on table counts in the conditionaldistributions complicates the process of drawingsamples for both our models.
In the non-hierarchicalmodel (Goldwater and Griffiths, 2007) thesedependencies can easily be accounted for byincrementing customer counts when such adependence occurs.
In our model we would need tosum over all possible table assignments that resultin the same tagging, at all levels in the hierarchy:tag trigrams, bigrams and unigrams; and also words,character bigrams and character unigrams.
To avoidthis rather onerous marginalisation2 we instead useexpected table counts to calculate the conditionaldistributions for sampling.
Unfortunately weknow of no efficient algorithm for calculating theexpected table counts, so instead develop a novelapproximationEn+1 [Ki] ?
En [Ki] +(aUEn [K] + bU )P0(i)(n?
En [Ki] bU ) + (aUEn [K] + bU )P0(i), (3)where Ki is the number of tables for the tag uni-gram i of which there are n + 1 occurrences, En [?
]denotes an expectation after observing n items andEn [K] =?j En [Kj ].
This formulation definesa simple recurrence starting with the first customerseated at a table, E1 [Ki] = 1, and as each subse-quent customer arrives we fractionally assign themto a new table based on their conditional probabilityof sitting alone.
These fractional counts are thencarried forward for subsequent customers.This approximation is tight for small n, and there-fore it should be effective in the case of the localGibbs sampler where only three trigrams are beingresampled.
For the type based resampling wherelarge numbers of n are involved (consider resam-pling the), this approximation can deviate from theactual value due to errors accumulated in the recur-sion.
Figure 3 illustrates a simulation demonstratingthat the approximation is a close match for small aand n but underestimates the true value for high a2Marginalisation is intractable in general, i.e.
for the 1HMMwhere many sites are sampled jointly.8690 20 40 60 80 10024681012number of customersexpectedtablesa=0.9a=0.8a=0.5a=0.1Figure 3: Simulation comparing the expected tablecount (solid lines) versus the approximation under Eq.
3(dashed lines) for various values of a.
This data was gen-erated from a single PYP with b = 1, P0(i) = 14 andn = 100 customers which all share the same tag.and n. The approximation was much less sensitiveto the choice of b (not shown).To resample a sequence of trigrams we start byremoving their counts from the current restaurantconfiguration (resulting in z?).
For each tag wesimulate adding back the trigrams one at a time,calculating their probability under the given z?
plusthe fractional table counts accumulated by Equation3.
We then calculate the expected table count con-tribution from this trigram and add it to the accu-mulated counts.
The fractional table count from thetrigram then results in a fractional customer enteringthe bigram restaurant, and so on down to unigrams.At each level we must update the expected countsbefore moving on to the next trigram.
After per-forming this process for all trigrams under consider-ation and for all tags, we then normalise the resultingtag probabilities and sample an outcome.
Once atag has been sampled, we then add all the trigramsto the restaurants sampling their tables assignmentsexplicitly (which are no longer fractional), recordedin z.
Because we do not marginalise out the tablecounts and our expectations are only approximate,this sampler will be biased.
We leave to future workproperly accounting for this bias, e.g., by devising aMetropolis Hastings acceptance test.Sampling hyperparameters We treat thehyper-parameters {(ax, bx) , x ?
(U,B, T,E,C)}as random variables in our model and infer theirvalues.
We place prior distributions on the PYPdiscount ax and concentration bx hyperparamtersand sample their values using a slice sampler.
Forthe discount parameters we employ a uniformBeta distribution (ax ?
Beta(1, 1)), and forthe concentration parameters we use a vaguegamma prior (bx ?
Gamma(10, 0.1)).
All thehyper-parameters are resampled after every 5thsample of the corpus.The result of this hyperparameter inference is thatthere are no user tunable parameters in the model,an important feature that we believe helps explain itsconsistently high performance across test settings.4 ExperimentsWe perform experiments with a range of corporato both investigate the properties of our proposedmodels and inference algorithms, as well as to estab-lish their robustness across languages and domains.For our core English experiments we report resultson the entire Penn.
Treebank (Marcus et al, 1993),while for other languages we use the corpora madeavailable for the CoNLL-X Shared Task (Buchholzand Marsi, 2006).
We report results using the many-to-one (M-1) and v-measure (VM) metrics consid-ered best by the evaluation of Christodoulopouloset al (2010).
M-1 measures the accuracy of themodel after mapping each predicted class to its mostfrequent corresponding tag, while VM is a variantof the F-measure which uses conditional entropyanalogies of precision and recall.
The log-posteriorfor the HMM sampler levels off after a few hundredsamples, so we report results after five hundred.
The1HMM sampler converges more quickly so we usetwo hundred samples for these models.
All reportedresults are the mean of three sampling runs.An important detail for any unsupervisedlearning algorithm is its initialisation.
We usedslightly different initialisation for each of ourinference strategies.
For the unrestricted HMM werandomly assigned each word token to a class.
Forthe restricted 1HMM we use a similar initialiser to870Model M-1 VMPrototype meta-model (CGS10) 76.1 68.8MEMM (BBDK10) 75.5 -mkcls (Och, 1999) 73.7 65.6MLE 1HMM-LM (Clark, 2003)?
71.2 65.5BHMM (GG07) 63.2 56.2PR (Ganchev et al, 2010)?
62.5 54.8Trigram PYP-HMM 69.8 62.6Trigram PYP-1HMM 76.0 68.0Trigram PYP-1HMM-LM 77.5 69.7Bigram PYP-HMM 66.9 59.2Bigram PYP-1HMM 72.9 65.9Trigram DP-HMM 68.1 60.0Trigram DP-1HMM 76.0 68.0Trigram DP-1HMM-LM 76.8 69.8Table 1: WSJ performance comparing previous workto our own model.
The columns display the many-to-1accuracy and the V measure, both averaged over 5 inde-pendent runs.
Our model was run with the local sampler(HMM), the type-level sampler (1HMM) and also withthe character LM (1HMM-LM).
Also shown are resultsusing Dirichlet Process (DP) priors by fixing a = 0.
Thesystem abbreviations are CGS10 (Christodoulopoulos etal., 2010), BBDK10 (Berg-Kirkpatrick et al, 2010) andGG07 (Goldwater and Griffiths, 2007).
Starred entriesdenote results reported in CGS10.Clark (2003), assigning each of the k most frequentword types to its own class, and then randomlydividing the rest of the types between the classes.As a baseline we report the performance ofmkcls (Och, 1999) on all test corpora.
This modelseems not to have been evaluated in prior work onunsupervised PoS tagging, which is surprising givenits consistently good performance.First we present our results on the most frequentlyreported evaluation, the WSJ sections of the Penn.Treebank, along with a number of state-of-the-artresults previously reported (Table 1).
All of thesemodels are allowed 45 tags, the same number of tagsas in the gold-standard.
The performance of ourmodels is strong, particularly the 1HMM.
We alsosee that incorporating a character language model(1HMM-LM) leads to further gains in performance,improving over the best reported scores under bothM-1 and VM.
We have omitted the results for theHMM-LM as experimentation showed that the localGibbs sampler became hopelessly stuck, failing to0 10 20 30 40 50024681012141618 x 104Tags sorted by frequencyFrequencyGold tag distribution1HMM1HMM?LMMKCLSFigure 4: Sorted frequency of tags for WSJ.
The goldstandard distribution follows a steep exponential curvewhile the induced model distributions are more uniform.mix due to the model?s deep structure (its peak per-formance was ?
55%).To evaluate the effectiveness of the PYP prior weinclude results using a Dirichlet Process prior (DP).We see that for all models the use of the PYP pro-vides some gain for the HMM, but diminishes forthe 1HMM.
This is perhaps a consequence of theexpected table count approximation for the type-sampled PYP-1HMM: the DP relies less on the tablecounts than the PYP.If we restrict the model to bigrams we seea considerable drop in performance.
Note thatthe bigram PYP-HMM outperforms the closelyrelated BHMM (the main difference being thatwe smooth tag bigrams with unigrams).
It is alsointeresting to compare the bigram PYP-1HMM tothe closely related model of Lee et al (2010).
Thatmodel incorrectly assumed independence of theconditional sampling distributions, resulting in aaccuracy of 66.4%, well below that of our model.Figures 4 and 5 provide insight into the behaviorof the sampling algorithms.
The former shows thatboth our models and mkcls induce a more uniformdistribution over tags than specified by the treebank.It is unclear whether it is desirable for models toexhibit behavior closer to the treebank, which ded-icates separate tags to very infrequent phenomenawhile lumping the large range of noun types intoa single category.
The graph in Figure 5 showsthat the type-based 1HMM sampler finds a goodtagging extremely quickly and then sticks with it,8710 50 100 1501020304050607080Number of samplesM?1Accuracy (%)PYP?1HMMPYP?1HMM?LMPYP?HMMPYP?HMM?LMFigure 5: M-1 accuracy vs. number of samples.NNINNNPDTJJNNS,.CDRBVBDVBCCTOVBZVBNPRPVBGVBPMDPOSPRP$$????:WDTJJRRPNNPSWPWRBJJSRBR?RRB??LRB?EXRBSPDTFWWP$#UHSYMNNINNNPDTJJNNS,.CDRBVBDVBCCTOVBZVBNPRPVBGVBPMDPOSPRP$$????:WDTJJRRPNNPSWPWRBJJSRBR?RRB?
?LRB?EXRBSPDTFWWP$#UHSYMFigure 6: Cooccurence between frequent gold (y-axis)and predicted (x-axis) tags, comparing mkcls (top) andPYP-1HMM-LM (bottom).
Both axes are sorted in termsof frequency.
Darker shades indicate more frequent cooc-curence and columns represent the induced tags.save for the occasional step change demonstrated bythe 1HMM-LM line.
The locally sampled model isfar slower to converge, rising slowly and plateauingwell below the other models.In Figure 6 we compare the distributions overWSJ tags for mkcls and the PYP-1HMM-LM.
Onthe macro scale we can see that our model induces asparser distribution.
With closer inspection we canidentify particular improvements our model makes.In the first column for mkcls and the third columnfor our model we can see similar classes with sig-nificant counts for DTs and PRPs, indicating a classthat the models may be using to represent the startof sentences (informed by start transitions or capi-talisation).
This column exemplifies the sparsity ofthe PYP model?s posterior.We continue our evaluation on the CoNLLmultilingual corpora (Table 2).
These results showa highly consistent story of performance for ourmodels across diverse corpora.
In all cases thePYP-1HMM outperforms the PYP-HMM, whichare both outperformed by the PYP-1HMM-LM.The character language model provides largegains in performance on a number of corpora,in particular those with rich morphology (Arabic+5%, Portuguese +5%, Spanish +4%).
We againnote the strong performance of the mkcls model,significantly beating recently published state-of-the-art results for both Dutch and Swedish.
Overall ourbest model (PYP-1HMM-LM) outperforms boththe state-of-the-art, where previous work exists, aswell as mkcls consistently across all languages.5 DiscussionThe hidden Markov model, originally developed byBrown et al (1992), continues to be an effectivemodelling structure for PoS induction.
We havecombined hierarchical Bayesian priors with a tri-gram HMM and character language model to pro-duce a model with consistently state-of-the-art per-formance across corpora in ten languages.
How-ever our analysis indicates that there is still room forimprovement, particularly in model formulation anddeveloping effective inference algorithms.Induced tags have already proven their usefulnessin applications such as Machine Translation, thus itwill prove interesting as to whether the improve-ments seen from our models can lead to gains indownstream tasks.
The continued successes of mod-els combining hierarchical Pitman-Yor priors withexpressive graphical models attests to this frame-work?s enduring attraction, we foresee continuedinterest in applying this technique to other NLPtasks.872Language mkcls HMM 1HMM 1HMM-LM Best pub.
Tokens Tag typesArabic 58.5 57.1 62.7 67.5 - 54,379 20Bulgarian 66.8 67.8 69.7 73.2 - 190,217 54Czech 59.6 62.0 66.3 70.1 - 1,249,408 12cDanish 62.7 69.9 73.9 76.2 66.7?
94,386 25Dutch 64.3 66.6 68.7 70.4 67.3?
195,069 13cHungarian 54.3 65.9 69.0 73.0 - 131,799 43Portuguese 68.5 72.1 73.5 78.5 75.3?
206,678 22Spanish 63.8 71.6 74.7 78.8 73.2?
89,334 47Swedish 64.3 66.6 67.0 68.6 60.6?
191,467 41Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best publishedresult (?Berg-Kirkpatrick et al (2010) and ?Lee et al (2010)).
This data was taken from the CoNLL-X shared tasktraining sets, resulting in listed corpus sizes.
Fine PoS tags were used for evaluation except for items marked with c,which used the coarse tags.
For each language the systems were trained to produce the same number of tags as thegold standard.ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless unsu-pervised learning with features.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 582?590, Los Angeles,California, June.
Association for Computational Lin-guistics.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist., 18:467?479, December.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning, CoNLL-X ?06, pages 149?164, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of the 34th annual meetingon Association for Computational Linguistics, pages310?318, Morristown, NJ, USA.
Association for Com-putational Linguistics.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedPOS induction: How far have we come?
In Proceed-ings of the 2010 Conference on Empirical Methods inNatural Language Processing, pages 575?584, Cam-bridge, MA, October.
Association for ComputationalLinguistics.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the tenth Annual Meeting of theEuropean Association for Computational Linguistics(EACL), pages 59?66.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
Journalof Machine Learning Research, pages 3053?3096.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of MachineLearning Research, 99:2001?2049, August.Jianfeng Gao and Mark Johnson.
2008.
A comparison ofbayesian estimators for unsupervised hidden markovmodel pos taggers.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?08, pages 344?352, Morristown, NJ,USA.
Association for Computational Linguistics.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In Proc.
of the 45th Annual Meeting of the ACL(ACL-2007), pages 744?751, Prague, Czech Republic,June.Sharon Goldwater, Tom Griffiths, and Mark Johnson.2006a.
Contextual dependencies in unsupervisedword segmentation.
In Proc.
of the 44th Annual Meet-ing of the ACL and 21st International Conferenceon Computational Linguistics (COLING/ACL-2006),Sydney.Sharon Goldwater, Tom Griffiths, and Mark Johnson.2006b.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, Advances in Neural873Information Processing Systems 18, pages 459?466.MIT Press, Cambridge, MA.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe main conference on Human Language Technol-ogy Conference of the North American Chapter of theAssociation of Computational Linguistics, pages 320?327, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Mark Johnson.
2007.
Why doesnt EM find goodHMM POS-taggers?
In Proc.
of the 2007 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP-2007), pages 296?305, Prague, CzechRepublic.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2010.
Simple type-level unsupervised pos tagging.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 853?861, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.P.
Liang, M. I. Jordan, and D. Klein.
2010.
Type-basedMCMC.
In North American Association for Compu-tational Linguistics (NAACL).Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn treebank.
ComputationalLinguistics, 19(2):313?330.Franz Josef Och.
1999.
An efficient method for deter-mining bilingual word classes.
In Proceedings of theninth conference on European chapter of the Asso-ciation for Computational Linguistics, pages 71?76,Morristown, NJ, USA.
Association for ComputationalLinguistics.Sujith Ravi and Kevin Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In Proceed-ings of the Joint Conferenceof the 47th Annual Meet-ing of the Association for Computational Linguisticsand the 4th International Joint Conference on Natu-ral Language Processing of the Asian Federation ofNatural Language Processing (ACL-IJCNLP), pages504?512.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 354?362, Ann Arbor, Michigan, June.Y.
W. Teh.
2006a.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages985?992.Yee Whye Teh.
2006b.
A hierarchical bayesian languagemodel based on pitman-yor processes.
In Proceedingsof the 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, ACL-44, pages985?992, Morristown, NJ, USA.
Association for Com-putational Linguistics.Kristina Toutanova and Mark Johnson.
2008.
A bayesianlda-based model for semi-supervised part-of-speechtagging.
In J.C. Platt, D. Koller, Y.
Singer, andS.
Roweis, editors, Advances in Neural InformationProcessing Systems 20, pages 1521?1528.
MIT Press,Cambridge, MA.874
