Proceedings of the Workshop on Information Extraction Beyond The Document, pages 20?28,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatic Extraction of Definitions from German Court DecisionsStephan WalterDepartment of Computational LinguisticsUniversit?t des Saarlandes66123 Saarbr?cken, Germanystwa@coli.uni-saarland.deManfred PinkalDepartment of Computational LinguisticsUniversit?t des Saarlandes66123 Saarbr?cken, Germanypinkal@coli.uni-saarland.deAbstractThis paper deals with the use of computa-tional linguistic analysis techniques forinformation access and ontology learningwithin the legal domain.
We present arule-based approach for extracting andanalysing definitions from parsed textand evaluate it on a corpus of about 6000German court decisions.
The results areapplied to improve the quality of a textbased ontology learning method on thiscorpus.11 MotivationMethods like ontology based knowledge man-agement and information access through concep-tual search have become active research topics inthe general research community, with practicalapplications in many areas.
However the use ofIT in legal practice (at least in German speakingcountries) is up to now mainly restricted todocument preparation and management or Boo-lean keyword search on full-text collections.
Le-gal ontologies have been proposed in variousresearch projects, but they focus on an upperlevel of concepts and are, with only a few excep-tions, small knowledge repositories that werehand-made by experts (for a summary of existinglegal ontologies, cf.
(Valente 2004)).It is clear that realistically large knowledge-based applications in the legal domain will needmore comprehensive ontologies incorporatinge.g.
up-to-date knowledge from court decisions.For this purpose an expert-based approach has to1 This paper describes research within the projectCORTE funded by the German Science Foundation,DFG PI 154/10-1(http://www.coli.uni-saarland.de/projects/corte/)be supplemented by automatic acquisition me-thods.
The same is true for large-scale advancedinformation access: Extensive conceptual indexa-tion of even a fraction of all court decisions pub-lished in one year seems hardly possible withoutautomatic support.
However there has been rela-tively little research on the use of natural lan-guage processing for this purpose (exceptions are(Lame 2005) and (Saias and Quaresma 2005)).In this paper we look at the use of computa-tional linguistic analysis techniques for informa-tion access and ontology learning within the le-gal domain.
We present a rule-based method forextracting and analyzing definitions from parsedtext, and evaluate this method on a corpus ofabout 6000 German court decisions within thefield of environmental law.
We then report on anexperiment exploring the use of our extractionresults to improve the quality of text-based on-tology learning from noun-adjective bigrams.
Wewill start however with a general discussion ofthe role that definitions play in legal language.2 Definitions in Legal LanguageTwo central kinds of knowledge contained in thestatutes of a code law system are normativeknowledge, connecting legal consequences todescriptions of certain facts and situations, andterminological knowledge, consisting in defini-tions of some of the concepts used in these de-scriptions (Valente and Breuker 1994).Normative content is exemplified by (1), parts ofsection 324a of the German criminal law.
Thelegal consequence consisting in the specifiedpunishment is connected to the precondition ofsoil pollution:(1) Whoever (?)
allows to penetrate or re-leases substances into the soil and thereby pol-lutes it or otherwise detrimentally alters it:201. in a manner that is capable of harming (?
)property of significant value or a body of wa-ter (?
)shall be punished with imprisonment for notmore than five years or a fine.Terminological knowledge consists in definitionsof concepts used to describe the sanctioned facts.E.g., soil is defined in article 2 of the Germansoil protection law as follows:(2) Soil within the meaning of this Act is theupper layer of the earth's crust (?)
including itsliquid components (soil solution) and gaseouscomponents (soil air), except groundwater andbeds of bodies of water.If the definitions contained in statutes wouldfully specify how the relevant concepts are to beapplied, cases could be solved (once the relevantstatutes have been identified) by mechanicallychecking which of some given concepts apply,and then deriving the appropriate legal conse-quences in a logical conclusion.
However such asimple procedure is never possible in reality.Discussions in courts (and consequently in alllegal texts that document court decisions) are inlarge parts devoted to pinning down whether cer-tain concepts apply.
Controversies often arisebecause not all relevant concepts are defined atall within statutes, and because the terms used inlegal definitions are often in need of clarificationthemselves.
For instance it may be unclear insome cases what exactly counts as the bed of abody of water mentioned in Example (2).
Addi-tionally, reality is complex and constantly chang-ing, and these changes also pertain to the appli-cability of formerly clear-cut concepts.
Whilethis is especially true of social reality, ratherphysical concepts may also be affected.
An oftencited example is a case where the GermanReichsgericht had to decide whether electricitywas to be counted as a thing.At the heart of these difficulties lies the factthat statutes are written in natural language, notin a formalized or a strongly restricted special-ized language.
It is widely assumed in the phi-losophical literature that most natural languageconcepts do not lend themselves to definitionsfixing all potential conditions of applicability apriori.
From the point of view of legal theory thisopen-textured character of natural language con-cepts is often seen as essential for the functioningof any legal system (the term open texture wasintroduced into this discussion by (Hart 1961)).The use of natural language expressions allowsfor a continuous re-adjustment of the balancebetween precision and openness.
This possibilityis needed to provide regulations that are on theone hand reliable and on the other hand flexibleenough to serve as a common ground for allkinds of social interaction.
For the solution ofconcrete cases, the concepts made availablewithin statute texts are supplemented by furtherdefinitions (in a wide sense, covering all kinds ofmodification and adaptation of concepts) givenin the courts?
decisions (in particular within thereasons for judgement).
Such definitions for in-stance fix whether a certain stretch of sandcounts as the bed of a body of water or if some-thing is of significant value in the case at hand.These definitions are generally open for lateramendment or revision.
Still they almost alwaysremain binding beyond the case at hand.Easy access to definitions in decisions is there-fore of great importance to the legal practitioner.Sections 3 and 4 show how computational lin-guistic analysis helps answering this need byenabling an accurate search for definitions in alarge collection of court decisions.
Accuratedefinition extraction is a prerequisite to buildingup an information system that allows for con-cept-centred access to the interpretational knowl-edge spread over tens of thousands of documentsproduced by courts every year.Definitions are however not only of directvalue as a source of information in legal practice.They also provide contexts that contain particu-larly much relevant terminology, and are there-fore a good place to search for concepts to beintegrated in a domain ontology.
Given the im-portance and frequency of definitions in legaltext, such an approach seems particularly prom-ising for this domain.
Section 5 describes howautomatically extracted definitions improve theresults of a standard ontology learning method.3 Structure of DefinitionsOur current work is based on a collection ofmore than 6000 verdicts in environmental law.As a starting point however we conducted a sur-vey based on a random selection of 40 verdictsfrom various legal fields (none of them is in ourpresent test set), which contained 130 definitions.Inspection of these definitions has shown a rangeof common structural elements, and has allowedus to identify typical linguistic realizations of21these structural elements.
We will illustrate thiswith the example definition given in (3):(3) [4 Bei einem Einfamilienreihenhaus] [3 liegt]ein [1 mangelhafter Schallschutz] [5 dann] [3 vor,wenn] [2 die Haustrennwand einschalig errichtetwurde] (?).
(One-family row-houses have insufficient noiseinsulation if the separating wall is one-layered.
)This definition contains:1.
The definiendum, i.e.
the element that is de-fined (unzureichender Schallschutz - insufficientnoise insulation).2.
The definiens, i.e.
the element that fixes themeaning to be given to the definiendum (dieHaustrennwand einschalig errichtet wurde - theseparating wall is  one-layered).Apart from these constitutive parts, it contains:3.
A connector, indicating the relation betweendefiniendum and definiens (liegt?vor, wenn,have?, if).4.
A qualification specifying a domain area ofapplicability, i.e.
a restriction in terms of the partof reality that the regulation refers to (bei Einfa-milienreihenh?usern - one-family row-houses).5.
Signal words that cannot be assigned any clearfunction with regard to the content of the sen-tence, but serve to mark it as a definition (dann -?
).The connector normally contains at least thepredicate of the main clause, often together withfurther material (subjunction, relative pronoun,determiner).
It not only indicates the presence ofa definition.
It also determines how definiens anddefiniendum are realized linguistically and oftencontains information about the type of the givendefinition (full, partial, by examples?).
The lin-guistic realization of definiendum and definiensdepends on the connector.
One common patternrealizes the definiendum as the subject, and thedefiniens within a subclause.
The domain area isoften specified by a PP introduced by bei (?inthe field of?, for), as seen in the example.
Furtherpossibilities are other PPs or certain subclauses.Signal words are certain particles (dann in theexample), adverbs (e.g.
begrifflich - conceptu-ally) or nominal constructions containing thedefiniendum (e.g.
der Begriff des?, the conceptof?
).Of course many definitions also contain fur-ther structural elements that are not present inExample (3).
For instance certain adverbials ormodal verbs modify the force, validity or degreeof commitment to a definition (e.g.
only for typi-cal cases).
The field of law within which thegiven definition applies is often specified as a PPcontaining a formal reference to sections of stat-utes or simply the name of a statute, document,or even a complete legal field (e.g.
Umweltrecht- environmental law).
Citation information fordefinitions is standardly included in brackets as areference to another verdict by date, court, andreference number.4 Automatic extraction of definitionsThe corpus based pilot study discussed in thelast section has on the one hand shown a broadlinguistic variation among definitions in reasonsfor judgement.
No simple account, for instancein terms of keyword spotting or pattern match-ing, will suffice to extract the relevant informa-tion from a significant amount of occurrences.On the other hand our survey has shown a rangeof structural uniformities across these formula-tions.
This section discusses computational lin-guistic analysis techniques that are useful toidentify and segment definitions based on theseuniformities.4.1 Linguistic AnalysisOur current work is based on a collection ofmore than 6000 verdicts in environmental lawthat were parsed using the Preds-parser (Predsstands for partially resolved dependency struc-ture), a semantically-oriented parsing system thathas been developed in the Saarbr?cken Computa-tional Linguistics Department within the projectCOLLATE.
It was used there for informationextraction from newspaper text (Braun 2003,Fliedner 2004).
The Preds-parser balances depthof linguistic analysis with robustness of theanalysis process and is therefore able to providerelatively detailed linguistic information even forlarge amounts of syntactically complex text.It generates a semantic representation for its in-put by a cascade of analysis components.
Start-ing with a topological analysis of the input sen-tence, it continues by applying a phrase chunkerand a named entity recognizer to the contents ofthe topological fields.
The resulting extendedtopological structure is transformed to a semanticrepresentation (called Preds, see above) by a se-ries of heuristic rules.
The Preds-format encodessemantic dependencies and modification rela-tions within a sentence using abstract categories22such as deep subject and deep object.
This way itprovides a common normalized structure forvarious surface realizations of the same content(e.g.
in active or passive voice).The Preds-parser makes use of syntactic under-specification to deal with the problem of ambigu-ity.
It systematically prefers low attachment incase of doubt and marks the affected parts of theresult as default-based.
Later processing steps areenabled to resolve ambiguities based on furtherinformation.
But this is not necessary in general.Common parts of multiple readings can be ac-cessed without having to enumerate and searchthrough alternative representations.
Figure 1shows the parse for the definition in Example(3).2 The parser returns an XML-tree that con-tains this structure together with the full linguis-tic information accumulated during the analysisprocess.4.2 Search and processingThe structures produced by the Preds parser pro-vide a level of abstraction that allows us to turntypical definition patterns into declarative extrac-tion rules.
Figure 2 shows one such extractionrule.
It specifies (abbreviated) XPath-expressionsdescribing definitions such as Example (3).
Thefield query contains an expression characterisinga sentence with the predicate vorliegen and asubclause that is introduced by the subjunctionwenn (if).
This expression is evaluated on thePreds of the sentences within our corpus to iden-tify definitions.
Other fields determine the loca-tions containing the structural elements (such asdefiniendum, definiens and domain area) withinthe Preds of the identified definitions.2 Figure 1 and Figure 3 were generated using the SALSA-Tool (Burchardt et al 2006)<pattern>description=liegt vor + wenn-Nebensatzquery=sent/parse/preds/word[@stem="vorliegen"and INDPRES and WENN]filters=definitedefiniendum=DSubdefiniens=WENN/arg/wordarea=PPMOD{PREP%bei}</pattern>Figure 2.
Extraction rule.The field filters specifies a set of XSLT-scriptsused to filter out certain results.
In the examplewe exclude definienda that are either pronominal(because we do not presently resolve anaphoricreferences) or definite (because these are oftenalso anaphoric, or indicate that the sentence athand is valid for that particular case only).
Figure3 shows how the definition in Example (3) isanalyzed by this rule.4.3 EvaluationWe currently use 33 such extraction rulesbased on the connectors identified in our pilotstudy, together with various kinds of filters.When applied to the reasons for judgement in all6000 decisions (containing 237935 sentences) inour environmental law corpus, these rules yield5461 hits before filtering (since not all patternsare mutually exclusive, these hits are all within4716 sentences).
After exclusion of pronominaland in some cases definite definienda (seeabove), as well as definienda containing stop-words (certain very common adjectives andnouns) the number of remaining hits decreases to1486 (in 1342 sentences).A selection of 492 hits (in 473 sentences; allhits for rules with less than 20, at least 20 hits forothers) was checked for precision by two annota-tors.
The evaluation was based on a very inclu-sive concept of definition, covering many casesof doubt such as negative applicability condi-tions, legal preconditions or elaborations on theuse of evaluative terms.
Clear ?no?-judgementsFigure 1.
Grammatical structure for Example (3).23Figure 3.
Structural elements of the definition in Example (3).were e.g.
given for statements referring only toone particular case without any general elements,and for purely contingent statements.
The overallagreement of the judgements given was rela-tively high, with an overall ?
of 0.835.Total33 rules 1486 hits (1342 / 237935 sent)Annotator 1 Good: 211/473  (p = 44.6 %)Annotator 2 Good: 230/473  (p = 48.6 %)Best rules onlyAnnotator 117 rules 749 hits (749 / 1342 sent)Good: 176/245  (p = 71.8 %)Annotator 218 rules 764 hits (633 / 1342 sent)Good: 173/230  (p = 75.2 %)Table 1.
Annotation results.Precision values within the checked hits varyconsiderably.
However in both cases more than50 % of all hits are by patterns that together stillreach a precision of well above 70 % (Table 1).4.4 DiscussionSo far, our focus in selecting rules and filtershas been on optimizing precision.
As our presentresults show, it is possible to extract definitionsat an interesting degree of precision and stillachieve a reasonable number of hits.
Howeverwe have not addressed the issue of recall system-atically yet.
The assessment of recall posesgreater difficulties than the evaluation of the pre-cision of search patterns.
To our knowledge noreference corpus with annotated definitions ex-ists.
Building up such a corpus is time intensive,in particular because of the large amount of textthat has to be examined for this purpose.
Withinthe 3500 sentences of the 40 decisions examinedin our pilot study mentioned above, we foundonly about 130 definitions.
While this amount issignificant from the perspective of informationaccess, it is quite small from the annotator?spoint of view.
Moreover it has become clear inour pilot study that there is a considerableamount of definitions that cannot be identified bypurely linguistic features, and that many of theseare unclear cases of particular difficulty for theannotator.
The proportion of such problematiccases will obviously be much higher in free textannotation than in the evaluation of our extrac-tion results, which were generated by looking forclear linguistic cues.Taking the ratio observed in our pilot study(130 definitions in 3500 sentences) as an orienta-tion, the set of rules we are currently using isclearly far from optimal in terms of recall.
Itseems that a lot of relatively simple improve-ments can be made in this respect.
A variety ofobvious good patterns are still missing in ourworking set.
We are currently testing a boot-strapping approach based on a seed of variousnoun-combinations taken from extracted defini-tions in order to acquire further extraction pat-terns.
We hope to be able to iterate this proce-dure in a process of mutual bootstrapping similarto that described in (Riloff and Jones 1999).Moreover all presently employed rules usepatterns that correspond to the connector-parts(cf.
Section 3) of definitions.
Accumulations ofe.g.
certain signals and modifiers may turn out toindicate definitions with equal precision.
We24identified a range of adverbial modifiers that arehighly associated with definitions in the corpusof our pilot study, but we have not yet evaluatedthe effect of integrating them in our extractionpatterns.We also assume that there is great potential formore fine-grained and linguistically sensitivefiltering, such that comparable precision isachieved without losing so many results.Even with all of the discussed improvementshowever, the problem of definitions withoutclear linguistic indicators will remain.
Heuristicsbased on domain specific information, such ascitation and document structure (e.g.
the firstsentence of a paragraph is often a definition),may be of additional help in extending recall ofour method to such cases.Apart from integrating further features in ourextractors and using bootstrapping techniques foridentifying new patterns, another option is totrain classifiers for the identification of defini-tions based on parse features, such as depend-ency paths.
This approach has for instance beenused successfully for hypernym discovery (cf.Snow et al, 2005).
For this task, WordNet couldbe used as a reference in the training and evalua-tion phase.
The fact that no comparable referenceresource is available in our case presents a greatdifficulty for the application of machine learningmethods.5 Ontology ExtractionOccurrence of a concept within a definition islikely to indicate that the concept is important forthe text at hand.
Moreover in court decisions, agreat deal of the important (legal as well as sub-ject domain) concepts will in fact have at leastsome occurrences within definitions.
This can beassumed because legal argumentation (as dis-cussed in Section 2) characteristically proceedsby adducing explicit definitions for all relevantconcepts.
Definition extraction therefore seemsto be a promising step for identifying concepts,in particular within legal text.
This section dis-cusses how extracted definitions can be used toimprove the quality of text-based ontology learn-ing from court decisions.
For this purpose wefirst examine the results of a standard method ?identification of terms and potential class-subclass-relations through weighted bigrams ?and then look at the effect of combining thismethod with a filter based on occurrence withindefinitions.5.1 Bigram ExtractionAdjective-noun-bigrams are often taken as astarting point in text based ontology extractionbecause in many cases they contain two conceptsand one relation (see e.g.
Buitelaar et al 2004).The nominal head represents one concept, whileadjective and noun together represent anotherconcept that is subordinate to the first one.
Thereare however obvious limits to the applicability ofthis concept-subconcept-rule:(1) It may happen that the bigram or even al-ready the nominal head on its own do not corre-spond to relevant concepts, i.e.
that one or bothof the denoted classes are of no particular rele-vance for the domain.
(2) Not all adjective-noun-bigrams refer to asubclass of the class denoted by the head noun.Adjectives may e.g.
be used redundantly, makingexplicit a part of the semantics of the head noun,or the combination may be non-compositionaland therefore relatively unrelated to the classreferred to by the head noun.For these reasons, extracted bigrams generallyneed to be hand-checked before correspondingconcepts can be integrated into an ontology.
Thistime-intensive step can be facilitated by provid-ing a relevance-ranking of the candidates to beinspected.
Such rankings use association meas-ures known from collocation discovery (like ?2,pointwise mutual information or log-likelihood-ratios).
But while the elements of a collocationare normally associated in virtue of their mean-ing, they do not necessarily correspond to a do-main concept just by this fact.
Moreover, manycollocations are non-compositional.
An associa-tion based ranking therefore cannot solve Prob-lem (2) just mentioned, and only partially solvesProblem (1).
However it seems likely that thedefiniendum in a definition is a domain concept,and for the reasons discussed in Section 2, it canbe assumed that particularly many concepts willin fact occur within definitions in the legal do-main.
In order to investigate this hypothesis, weextracted all head-modifier pairs with nominalhead and adjectival modifier from all parsed sen-tences in our corpus.
We then restricted this listto only those bigrams occurring within at leastone identified definiendum, and compared theproportion of domain concepts following theconcept-subconcept-rule on both lists.5.2 Unfiltered Extraction and AnnotationWe found a total 165422 bigram-occurrencesof 73319 types (in the following we use bigrams25to refer to types, not to occurrences) within thefull corpus.
From this list we deleted combina-tions with 53 very frequent adjectives that aremainly used to establish uniqueness for definitereference (such as vorgenannt ?
mentionedabove).
All types with more than 5 occurrenceswere then ranked by log-likelihood of observedcompared to independent occurrence of the bi-gram elements.3 The resulting list contains 4371bigrams on 4320 ranks.
Each bigram on the first600 ranks of this list (601 bigrams, two bigramsshare rank 529) was assigned one of the follow-ing five categories:1.
Environmental domain: Bigrams encodingconcepts from the environmental domain (e.g.unsorted construction waste).
These occur be-cause our corpus deals with environmental law.2.
Legal domain: Bigrams encoding conceptsfrom the legal domain.
These range from con-cepts that are more or less characteristic of envi-ronmental law (e.g.
various kinds of town-planning schemes) to very generic legal concepts(such as statutory prerequisite)3.
No subconcept: Bigrams that would becategorized as 1. or 2., but (typically for one ofthe reasons explained above) do not encode asubconcept of the concept associated with thehead noun.
An example is ?ffentliche Hand(?public hand?, i.e.
public authorities ?
a non-compositional collocation).4.
No concept: All bigrams that - as a bigram- do not stand for a domain concept (although thenominal head alone may stand for a concept).5.
Parser error: Bigrams that were obviouslymisanalysed due to parser errors.Figure 4 shows the distribution of categoriesamong the 600 top-ranked bigrams, as well aswithin an additionally annotated 100 ranks to-wards the end of the list (ranks 3400-3500).20 41 94 118 1736 61 134 1632211 3374 81031 62 186 223562 3 12 16 60%10%20%30%40%50%60%70%80%90%100%Top100Top200Top500Top6003400-3500Parser errorNo conceptNo subconceptLegalEnvironmentalFigure 4.
Results of log-likelihood ranking.3 The ranking was calculated by the Ngram StatisticsPackage described in (Bannerjee and Pedersen 2003)For selecting the two categories of central in-terest, namely those of legal and environmentalconcepts to which the concept-subconcept ruleapplies, the ranking is most precise on the firstfew hundred ranks, and looses much of its effecton lower ranks.
The percentage of such conceptsdecreases from 56% among the first 100 ranks to51% among the first 200, but is roughly the samewithin the first 500 and 600 ranks (with even aslight increase, 45.6% compared to 46.8%).
Eventhe segment from rank 3400 to 3500 still con-tains 39% of relevant terminology.
There are nobigrams of the ?no subconcept?
category withinthis final segment.
The explanation for this factis probably that such bigrams (especially thenon-compositional ones) are mostly establishedcollocations and therefore show a particularlyhigh degree of association.It must be noted that the results of our annota-tion have to be interpreted cautiously.
They havenot yet been double-checked and during the an-notation process there turned out to be a certaindegree of uncertainty especially in the subclassi-fication of the various categories of concepts (1,2 and 3).
A further category for concepts withgeneric attributes (e.g.
permissible, combiningwith a whole range of one-word terms) wouldprobably cover many cases of doubt.
The binarydistinction between concepts and non-conceptsin contrast was less difficult to make, and it issurely safe to conclude about general tendenciesbased on our annotation.5.3 Filtering and Combined ApproachBy selecting only those bigrams that occurwithin defienda, the 4371 items on the originallist were were reduced to 227 (to allow for com-parison, these were kept in the same order andannotated with their ranks as on the original list).Figure 5 shows how the various categories aredistributed within the items selected from the topsegments of the original list, as well as within thecomplete 227 filtering results.7 14 24 26 45121731 35100235 5104 417 17650 0 1 1 70%20%40%60%80%100%Top 100(25 items)Top 200(38 items)Top 500(78 items)Top 600(84 items)Complete(227 it.
)Parser errorNo conceptNo subconceptLegalEnvironmentalFigure 5.
Filtered results26The proportion of interesting concepts reachesabout 80% and is higher than 60% on the com-plete selection.
This is still well above the 56%precision within the top 100-segment of theoriginal list.
However the restriction to a total of227 results on our filtered list (of which only 145are useful) means a dramatic loss in recall.
Thisproblem can be alleviated by leaving a top seg-ment of the original list in place (e.g.
the top 200or 500 ranks, where precision is still at a tolera-bly high level) and supplementing it with thelower ranks from the filtered list until the desirednumber of items is reached.
Another option is toapply the filtering to the complete list of ex-tracted bigrams, not only to those that occurmore than 5 times.
We assume that a concept thatis explicitly defined is likely to be of particularrelevance for the domain regardless of its fre-quency.
Hence our definition-based filter shouldstill work well on concept candidates that are tooinfrequent to be considered at all in a log-likelihood ranking, and allow us to include suchcandidates in our selection, too.We investigated the effect of a combination ofboth methods just described.
For this purpose,we first extracted all noun-adjective bigrams oc-curring within any of the identified definienda,regardless of their frequency within the corpus.After completing the annotation on the 627 re-sulting bigrams they were combined with varioustop segments of our original unfiltered list.Figure 6 shows the distribution of the anno-tated categories among the 627 bigrams fromdefinienda, as well as on two combined lists.Cutoff 200/750 is the result of cutting the originallist at rank 200 and filling up with the next 550items from the filtered list.
For cutoff 500/1000we cut the original list at rank 500 and filled upwith the following 500 items from the filteredone.
The distribution of categories among theoriginal top 200 is repeated for comparison.Figure 6.
Log-likelihood and filtering combined.Precision among the 627 filtering results ishigher than among the original top 200 (almost56% compared to 51%), and only slightlysmaller even for the 1000 results in the cutoff500/1000 setting.
Using definition extraction asan additional knowledge source, the top 1000results retrieved are thus of a quality that canotherwise only be achieved for the top 200 re-sults.6 ConclusionIn this paper we argued that definitions are animportant element of legal texts and in particularof court decisions.
We provided a structuralsegmentation scheme for definitions and dis-cussed a method of applying computational lin-guistic analysis techniques for their text-basedextraction and automatic segmentation.
Weshowed that a large number of definitions can infact be extracted at high precision using thismethod, but we also pointed out that there is stillmuch room for improvement in terms of recall,e.g.
through the inclusion of further definitionpatterns.Our future work in this area will focus on theintegration of extraction results across docu-ments (e.g.
recognizing and collecting comple-mentary definitions for the same concept) and ona user interface for structured access to this data.For this work we have access to a corpus of sev-eral million verdicts provided to us by the com-pany juris GmbH, Saarbr?cken.
We also demon-strated how the identification of definitions canimprove the results of text-driven ontology learn-ing in the legal domain.
When looking for noun-adjective bigrams encoding relevant concepts, itleads to a considerable increase in precision torestrict the search to definienda only.
Thismethod is more precise than selecting the topranks of a log-likelihood ranking.
Its great disad-vantage is the very low total number of results,leading to poor recall.
However by combining alog-likelihood ranking with definition-basedconcept extraction, recall can be improved whilestill achieving better precision than with a log-likelihood ranking alone.
Moreover this com-bined method also retrieves concepts that are tooinfrequent to be included at all in a log-likelihood ranking.There is however another, maybe even morerelevant reason to look for definitions in ontol-ogy learning.
Definitions in legal text often veryexplicitly and precisely determine all kinds ofrelational knowledge about the defined concept.For instance they specify explicit subordinations(as in the classical definitio per genus et differen-116 140 181 41231 269 320 6125 51 9033230 278 3766225 27 33 30%10%20%30%40%50%60%70%80%90%100%Filtered(627)Cutoff 200/750Cutoff 500/1000Top 200Parser ErrorsNo conceptNo subconceptLegalEnvironmental27tiam), introduce restrictions on roles inheritedfrom a superconcept, determine the constitutiveparts of the definiendum, or contain informationabout its causal relations to other concepts.
Asone focus of our future work we plan to investi-gate how such rich ontological knowledge can beextracted automatically.ReferencesSatanjeev Banerjee and Ted Pedersen.
2003.
The De-sign, Implementation, and Use of the Ngram Statis-tics Package.
CICLing 2003: 370-381Christian Braun.
2003.
Parsing German text for syn-tacto-semantic structures.
In Prospects and Ad-vances in the Syntax/Semantics Interface, Lorraine-Saarland Workshop Series, Nancy, France:99-102Paul Buitelaar, Daniel Olejnik, Michael Sintek.
2004.A Prot?g?
Plug-In for Ontology Extraction fromText Based on Linguistic Analysis In: Proceedingsof the 1st European Semantic Web Symposium(ESWS), Heraklion, GreeceAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski and Sebastian Pad?.
2006.
SALTO -- AVersatile Multi-Level Annotation Tool.
Proceed-ings of LREC 2006, Genoa, Italy.Gerhard Fliedner.
2004.
Deriving FrameNet Repre-sentations: Towards Meaning-Oriented QuestionAnswering.
Proceedings of the International Con-ference on Applications of Natural Language to In-formation Systems (NLDB).
Salford, UK.
LNCS3136/2004.
Springer.
64?75.Herbert L.A. Hart.
1961.
The concept of Law.
OxfordUniversity Press, London, UKGuiraude Lame.
2005.
Using NLP Techniques toIdentify Legal Ontology Components: Conceptsand Relations, Lecture Notes in Computer Science,Volume 3369:169 ?
184Ellen Riloff and Rosie Jones.
1999.
Learning Diction-aries for Information Extraction Using Multi-levelBoot-strapping, Proceedings of AAAI-99, 474 - 479Jos?
Saias and Paulo Quaresma.
2005.
A Methodol-ogy to Create Legal Ontologies in a Logic Pro-gramming Information Retrieval System.
LectureNotes in Computer Science, Volume 3369:185 -200Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.2005.
Learning syntactic patterns for automatic hy-pernym discovery, Proceedings of NIPS 2004,Vancouver, Canada.Andre Valente.
2005.
Types and Roles of Legal On-tologies.
Lecture Notes in Computer Science, Vol-ume 3369:65 - 76.Andre Valente and Jost Breuker.
1994.
A functionalontology of law.
Towards a global expert system inlaw.
CEDAM Publishers, Padua, Italy28
