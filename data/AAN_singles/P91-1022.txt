AL IGNING SENTENCES IN PARALLEL  CORPORAPeter F. Brown, Jennifer C. Lai, a, nd Robert L. MercerIBM Thomas J. Watson Research CenterP.O.
Box 704Yorktown Heights, NY 10598ABSTRACTIn this paper we describe a statistical tech-nique for aligning sentences with their translationsin two parallel corpora.
In addition to certainanchor points that are available in our da.ta, theonly information about the sentences that we usefor calculating alignments i  the number of tokensthat they contain.
Because we make no use of thelexical details of the sentence, the alignment com-putation is fast and therefore practical for appli-cation to very large collections of text.
We haveused this technique to align several million sen-tences in the English-French Hans~trd corpora ndhave achieved an accuracy in excess of 99% in arandom selected set of 1000 sentence pairs that wechecked by hand.
We show that even without hebenefit of anchor points the correlation betweenthe lengths of aligned sentences i strong enoughthat we should expect o achieve an accuracy ofbetween 96% and 97%.
Thus, the technique maybe applicable to a wider variety of texts than wehave yet tried.INTRODUCTIONRecent work by Brown et al, \[Brown etal., 1988, Brown et al, 1990\] has quickenedanew the long dormant idea of using statisticaltechniques to carry out machine translationfrom one natural language to another.
Thelynchpin of their approach is a. large collectionof pairs of sentences that.
are mutual transla-tions.
Beyond providing grist to the sta.tisti-cal mill, such pairs of sentences are valuableto researchers in bilingual exicography \[I(la.-va.ns and Tzoukerma.nn, 1990, Warwick andRussell, 1990\] and may be usefifl in other ap-proaches to machine translation \[Sadler, 1989\].In this paper, we consider the problem ofextra.cting from pa.raJlel French and F, nglishcorpora pairs sentences that are translationsof one another.
The task is not trivial becauseat times a single sentence in one language istranslated as two or more sentences in theother language.
At other times a sentence,or even a whole passage, may be missing fromone or the other of the corpora.If a person is given two parallel texts andasked to match up the sentences in them, it isna.tural for him to look at the words in the sen-tences.
Elaborating this intuitively appealinginsight, researchers at Xerox and at ISSCO\[Kay, 1991, Catizone et al, 1989\] have devel-oped alignment Mgodthms that pair sentencesaccording to the words that they contain.
Anysuch algorithm is necessarily slow and, despitethe potential for highly accurate alignment,may be unsuitable for very large collectionsof text.
Our algorithm makes no use of thelexical details of the corpora, but deals onlywith the number of words in each sentence.Although we have used it only to align paral-lel French and English corpora from the pro-ceedings of the Canadian Parliament, we ex-pect that our technique wouhl work on otherFrench and English corpora and even on otherpairs of languages.
The work of Gale andChurch , \[Gale and Church, 1991\], who usea very similar method but measure sentencelengths in characters rather than in words,supports this promise of wider applica.bility.T I IE  HANSARD CORPORABrown el al., \[Brown et al, 1990\] describethe process by which the proceedings of theCa.nadian Parliament are recorded.
In Canada,these proceedings are re\[erred to as tta.nsards.169Our Hansard corpora consist of the llansardsfrom 1973 through 1986.
There are two filesfor each session of parliament: one Englishand one French.
After converting the obscuretext markup language of the raw data.
to TEX ,we combined all of the English files into a sin-gle, large English corpus and all of the Frenchfiles into a single, large French corpus.
Wethen segmented the text of each corpus intotokens and combined the tokens into groupsthat we call sentences.
Generally, these con-form to the grade-school notion of a sentence:they begin with a capital letter, contain a.verb, and end with some type of sentence-finalpunctuation.
Occasionally, they fall short ofthis ideal and so each corpus contains a num-ber of sentence fragments and other groupingsof words that we nonetheless refer to as sen-tences.
With this broad interpretation, theEnglish corpus contains 85,016,286 tokens in3,510,744 sentences, and the French corpuscontains 97,857,452 tokens in 3,690,425 sen-tences.
The average English sentence has 24.2tokens, while the average French sentence isabout 9.5% longer with 26.5 tokens.The left-hand side of Figure 1 shows theraw data for a portion of the English corpus,and the right-hand side shows the same por-tion after we converted it to TEX and dividedit up into sentences.
The sentence numbers donot advance regularly because we have editedthe sample in order to display a variety of phe-no lnena .In addition to a verbatim record of theproceedings and its translation, the ttansardsinclude session numbers, names of speakers,time stamps, question numbers, and indica-tions of the original language in which eachspeech was delivered.
We retain this auxiliaryinformation in the form of comments prin-kled throughout the text.
Each comment hasthe form \SCM{} .
.
.
\ECM{} as shownon the right-hand side of Figure 1.
\]n ad-dition to these comments, which encode in-formation explicitly present in the data, weinserted Paragraph comments as suggested bythe space command of which we see aa exam-ple in the eighth line on the left-hand side ofFigure 1.We mark the beginning of a parliamentarysession with a Document comment as shownin Sentence 1 on the right-hand side of Fig-ure 1.
Usually, when a member addresses theparliament, his name is recorded and we en-code it in an Author comment.
We see an ex-ample of this in Sentence 4.
If the presidentspeaks, he is referred to in the English cor-pus as Mr. Speaker and in the French corpusas M. le Prdsideut.
If several members peakat once, a shockingly regular occurrence, theyare referred to as Some Hon.
Members in theEnglish and as Des Voix in the French.
Timesare recorded either ~ exact times on a.
24-hourbasis as in $entencc 8\], or as inexact imes ofwhich there are two forms: Time = Later,and Time = Recess.
These are rendered inFrench as Time = Plus Tard and Time = Re-cess.
Other types of comments that appearare shown in Table 1.AL IGNING ANCHOR POINTSAfter examining the Hansard corpora, werealized that the comments laced throughoutwould serve as uscflll anchor points in anyalignment process.
We divide the commentsinto major and minor anchors as follows.
Thecomments Author  = Mr. Speaker, Author  =ill.
le Pr(sident, Author = Some Hon.
Mem-bers, and Author  = Des Voix are called minoranchors.
All other comments are called majoranchors with the exception of the Paragraphcomment which is not treated as an anchor atall.
The minor anchors are much more com-mon than any particular major anchor, mak-ing an alignment based on them less robustagainst deletions than one based on the ma-jor anchors.
Therefore, we have carried outthe alignment of anchor points in two passes,first aligning the major anchors and then theminor anchors.Usually, the major anchors appear in bothlanguages.
Sometimes, however, through inat-tentlon on the part of the translator or othermisa.dvel~ture, the tla.me of a speaker may begarbled or a comment may be omitted.
In thefirst alignment pass, we assign to alignments170/*START_COMMENT* Beginning file = 048101 H002-108 script A *END_COMMENT*/.TB 029 060 090 099.PL 060.LL 120.NFThe House met at 2 p.m..SP*boMr.
Donald MacInnis (Cape Breton-East Richmond):*ro Mr. Speaker,I rise on a question of privilege af-fecting the rights and prerogativesof parliamentary committees and onewhich reflects on the word of twoministers..SP*boMr.
Speaker: *roThe hon.
member'smotion is proposed to theHouse under the terms of StandingOrder 43.
Is there unanimous consent?.SP*boSome hon.
Members: *roAgreed.s*itText*ro)Question No.
17--*boMr.
Mazankowski:*toI.
For the period April I, 1973 toJ anuary  31, 1974, what amount ofmoney was expended on the  operat ionand maintenance of the PrimeMinister's residence at HarringtonLake, Quebec?.SP(1415)s* i tLater : * ro ).SP*boMr.
Coss i t t : * ro  Mr. Speaker ,  I r i seon a po in t  of o rder  to  ask fo rc la r i f i ca t ion  by the  par l iamentarysecretary.1.
\SCM{} Document  = 048 101 H002-108script A \ECM{)2.
The House met a t  2 p.m.3.
\SCM{} Paragraph \ECM{}4.
\SCM{} Author = Mr. Donald MacInnis(Cape Breton-East Richmond) \ECM{}5.
Mr. Speaker, I rise on a question ofprivilege affecting the rights andprerogatives of parliamentarycommittees and one which reflects onthe word of two ministers.21.
\SCM{} Paragraph \ECM{}22.
\SCM{} Author = Mr. Speaker \ECM{}23.
The hon.
member's motion is proposedto the House under the terms ofStanding Order 43.44.
Is there unanimous consent?45.
\SCM{} Paragraph \ECM{)46.
\SCM{-} Author = Some hon.
Members\ECM{}47.
Agreed.61.
\SCM{} Source = Text \ECM{}62.
\SCM{} Question = 17 \ECM{}63.
\SCM{} Author = Mr. Mazankowski\ECMO64.
I.65.
For the period April I, 1973 toJ anuary  31, 1974, .hat  amount ofmoney was expended on the  operat ionand maintenance  of  the  Pr imeMin is ter ' s  res idence  at  Har r ingtonLake,  Quebec?66.
\SCM{} Paragraph \ECN{}81.
\SCM{) Time = (1415) \ECM{}82.
\SCM{) Time = Later \ECM{)83.
\SCM{} Paragraph \ECM{}84.
\SCM{} Author = Mr. Cossitt \ECM{}85.
Mr. Speaker, I rise on a point oforder to ask for clarification bythe parliamentary secretary.F igure  1: A sample of text before and after cleanup171a cost that favors exact matches and penalizesomissions or garbled matches.
Thus, for ex-ample, we assign a cost of 0 to the pair T ime= Later  and T ime = P lus Tard, but a costof 10 to the pair T ime = Later  and Author= Mr.  Bateman.
We set the cost of a dele-tion at 5.
For two names, we set the cost bycounting the number of insertions, deletions,and substitutions necessary to transform onename, letter by letter, into the other.
Thisvalue is then reduced to the range 0 to 10.Given the costs described above, it is astandard problem in dynamic programmingto find that alignment of the major anchorsin the two corpora with the least total cost\[Bellman, 1957\].
In theory, the time and spacerequired to find this alignment grow as theproduct of the lengths of the two sequencesto be aligned.
In practice, however, by usingthresholds and the partial traceback techniquedescribed by Brown, Spohrer, Hochschild, andBaker , \[Brown et al, 1982\], the time requiredcan be made linear in the length of the se-quences, and the space can be made constant.Even so, the computational demand is severesince, in places, the two corpora are out ofalignment by as many as 90,000 sentences ow-ing to mislabelled or missing files.This first pass renders the data as a se-quence of sections between aligned major an-chors.
In the second pass, we accept or rejecteach section in turn according to the popula-tion of minor anchors that it contains.
Specifi-cally, we accept a section provided that, withinthe section, both corpora contain the samenumber of minor anchors in the same order.Otherwise, we reject the section.
Altogether,we reject about 10% of the data in each cor-pus.
The minor anchors serve to divide theremaining sections into subsections thai.
rangein size from one sentence to several thousandsentences and average about ten sentences.AL IGNING SENTENCES ANDPARAGRAPH BOUNDARIESWe turn now to the question of aligningthe individual sentences in a subsection be-tween minor anchors.
Since the number ofEngl ishSource = EnglishSource = TranslationSource = TextSource = List ItemSource = QuestionSource = AnswerFren(;hSource = TraductionSource = FrancaisSource = TexteSource = List ItemSource = QuestionSource = ReponseTable 1: Examples of commentssentences in the French corpus differs from thenumber in the English corpus, it is clear thatthey cannot be in one-to-one correspondencethroughout.
Visual inspection of the two cor-pora quickly reveals that although roughly 90%of the English sentences correspond to singleFrench sentences, there are many instanceswhere a single sentence in one corpus is rep-resented by two consecutive sentences in theother.
Rarer, but still present, are examplesof sentences that appear in one corpus butleave no trace in the other.
If one is moder-ately well acquainted with both English andFrench, it is a simple matter to decide how thesentences hould be aligned.
Unfortunately,the sizes of our corpora make it impracticalfor us to obtain a complete set of alignmentsby hand.
Rather, we must necessarily employsome automatic scheme.It is not surprising and further inspectionverifies that tile number of tokens in sentencesthat are translations of one another are corre-lated.
We looked, therefore, at the possibilityof obtaining alignments solely on the basis ofsentence lengths in tokens.
From this point ofview, each corl)us is simply a sequence of sen-tence lengths punctuated by occasional para-graph markers.
Figure 2 shows the initial por-tion of such a pair of corpora.
We have circledgroups of sentence lengths to show the cor-rect alignment.
We call each of the groupingsa bead.
In this example, we have an el-beadfollowed by an eft-bead followed by an e-beadfollowed by a ?~?l-bead.
An alignment, hen,is simply a sequence of beads that accountsfor the observed sequences of sentence lengthsand paragraph markers.
We imagine the sen-tences in a subsection to have been generatedby a pa.ir of random processes, the first pro-172Figure 2: Division of aligned corpora into beadsBeade/,fee/eft?!
?o?tTextone English sentenceone French sentenceone English and one French sentencetwo English and one French sentenceone English and two French sentencesone English paragraphone French paragraphone English and one French paragraphTable 2: Alignment Beadsducing a sequence of beads and the secondchoosing the lengths of the sentences in eachbead.Figure 3 shows the two-state Markov modelthat we use for generating beads.
-We assumethat a single sentence in one language lines upwith zero, one, or two sentences in the otherand that paragraph markers may be deleted.Thus, we allow any of the eight beads shown inTable 2.
We also assume that Pr (e) = Pr ( f) ,Pr (e f t )= er (ee/), and Pr (??)
= Pr(?t) .BEAD.
.
.
.
.
.
s-L-?--P- .
.
.
.
.
.
.
; !
: : :OFigure 3: Finite state model for generating beadsGiven a bead, we determine the lengths ofthe sentences it contains as follows.
We a.s-sume the probability of an English sentenceof length g~ given an e-bead to be the sameas the probability of an English sentence oflength ee in the text as a whole.
We denotethis probability by Pr(ee).
Similarly, we as-sume the probability of a French sentence oflength g!
given an f-bead to be Pr (gY)" For anel-bead, we assume that the English sentencehas length e, with probability Pr (~e) and thatlog of the ratio of length of the French sen-tence to the length of the English sentence isuormMly distributed with mean /t and vari-ance a 2.
Thus, if r = log(gt/ge), we assumethater(ts\[e, ) = c exp\[-(r- (1)with 0?
chosen so that the sum of Pr(tllt, )over positive values of gI is equal to unity.
Foran eel-bead, we assume that each of the En-glish sentences is drawn independently fromthe distribution Pr(t.) and that the log ofthe ratio of the length of the French sentenceto the sum of the lengths of the English sen-tences is normally distributed with the samemean and variance as for an el-bead.
Finally,for an eft-bead, we assume that the length ofthe English sentence is drawn from the distri-bution Pr (g,) and that the log of the ratio ofthe sum of the lengths of the French sentencesto the length of the English sentence is nor-mally distributed asbefore.
Then, given thesum of the lengths of the French sentences,we assume that tile probability of a particularpair of lengths,/~11 and ~12, is proportional toVr (ef,) Pr (~S~) .Together, these two random processes forma hidden Markov model \[Baum, 1972\] for thegeneration of aligned pairs of corpora.. We de-termined the distributions, Pr (g,) and Pr (aS),front the relative frequencies of various sen-tence lengths in our data.
Figure 4 shows foreach language a. histogram of these for sen-tences with fewer than 81 tokens.
Except forlengths 2 and 4, which include a large num-ber of formulaic sentences in both the Frenchand the English, the distributions are verysmooth.For short sentences, the relative frequencyis a reliable estimate of the corresponding prob-ability since for both French and English wehave more than 100 sentences of each lengthless tha.n 8\].
We estimated the probabilities173I 80mentenee length1 80.entenea lengthFigure 4: Histograms of French (top) and English (bottom) sentence l ngths174of greater lengths by fitting the observed fre-quencies of longer sentences to the tail of aPoisson distribution.We determined M1 of the other parametersby applying the EM algorithm to a large sam-pie of text \[Baum, 1972, Dempster et al, 1977\].The resulting values are shown in Table 3.From these parameters, we can see that 91%of the English sentences and 98% of the En-glish paragraph markers line up one-to-onewith their French counterparts.
A randomvariable z, the log of which is normMly dis-tributed with mean # and variance o ~, hasmean value exp(/t + a2/2).
We can also see,therefore, that the total length of the Frenchtext in an el-, eel-, or eft-bead should be about9.8% greater on average than the total lengthof the corresponding English text.
Since mostsentences belong to el-beads, this is close tothe value of 9.5% given in Section 2 for theamount by which the length of the averageFrench sentences exceeds that of the averageEnglish sentence.We can compute from the parameters inTable 3 that the entropy of the bead produc-tion process is 1.26 bits per sentence.
Us-ing the parameters # and (r 2, we can combinethe observed istribution of English sentencelengths shown in Figure 4 with the conditionaldistribution of French sentence lengths givenEnglish sentence lengths in Equation (1) toobtain the joint distribution of French andEnglish sentences lengths in el-, eel-, and eft-beads.
From this joint distribution, we cancompute that the mutual information betweenFrench and English sentence lengths in thesebeads is 1.85 bits per sentence.
We see there-fore that, even in the absence of the anchorpoints produced by the first two pa.sses, thecorrela.tion in sentence lengths is strong enoughto allow alignment with an error rate thatis asymptotically less than 100%.
lh;arten-ing though such a result may be to the theo-retician, this is a sufficiently coarse bound onthe error rate to warrant further study.
Ac-cordingly, we wrote a program to Simulate thealignment process that we had in mind.
UsingPr(e?
), Pr((?
), and the parameters from Ta-Parameter Estimateer (e),  P r ( / )  .007Pr (e/) .690Pr (eel),  Pr (eft) .020Pr (?~), Pr (?
f )  .005It.
.072tr 2 .043Table 3: P~rameter stimatesble 3, we generated an artificial pair of alignedcorpora.
We then determined the most prob-able alignment for the data.
We :recordedthe fraction of el-beads in the most probablealignment hat did not correspond to el-beadsin the true Mignment as the error rate for theprocess.
We repeated this process many thou-sands of times and found that we could ex-pect an error rate of about 0.9% given thefrequency of anchor points from the first twopa,sses.By varying the parameters of the hiddenMarkov model, we explored the effect of an-chor points and paragraph ma.rkers on the ac-curacy of alignment.
We found that with para-graph markers but no ~tnchor points, we couldexpect an error rate of 2.0%, with anchor pointsbut no l)~tra.graph markers, we could expect anerror rate of 2.3%, and with neither anchorpoints nor pa.ragraph markers, we could ex-pect an error rate of 3.2%.
Thus, while anchorpoints and paragraph markers are important,alignment is still feasible without them.
Thisis promising since it suggests that one maybe able to apply the same technique to datawhere frequent anchor points are not avail-able.RESULTSWe aplflied the alignment algorithm of Sec-t.ions 3 and 4 to the Ca.na.dian Hansa.rd datadescribed in Section 2.
The job ran for l0clays on au IBM Model 3090 mainframe un-der an operating system that permitted ac-cess to 16 mega.bytes of virtual memory.
Themost probable alignment contained 2,869,041el-beads.
Some of our colleagues helped us175And love and kisses to you, too.... mugwumps who sit on the fence withtheir mugs on one side and theirwumps on the other side and do notknow which side to come down on.At first reading, she may have.Pareillelnent.... en voulant m&lager la ch~vre t le chouxils n'arrivent 1)as k prendre patti.Elle semble en effet avoir un grief tout afait valable, du moins au premierabord.Table 4: Unusual but correct alignmentsexamine a random sample of 1000 of thesebeads, and we found 6 in which sentences werenot translations of one another.
This is con-sistent with the expected error rate ol 0.9%mentioned above.
In some cases, the algo-rithm correctly aligns sentences with very dif-ferent lengths.
Table 4 shows some interestingexamples of this.REFERENCES\[Baum, 1972\] Baum, L. (1972).
An inequalityand associated maximization technique instatistical estimation of probabilistic func-tions of a Markov process.
Inequalities, 3:1-8.\[Bellman, 1957\] Bellman, R. (1957).
Dy-namic Programming.
Princeton UniversityPress, Princeton N.J.\[Brown et al, 1982\] Brown, P., Spohrer, J.,Hochschild, P., and Baker, J.
(1982).
Par-tial traceback and dynamic programming.In Proceedings of the IEEE InternationalConference on Acoustics, Speech and SignalProcessing, pages 1629-1632, Paris, France.\[Brown et ai., 1990\] Brown, P. F., Cocke, J.,DellaPietra, S. A., DellaPietra, V. J., Je-linek, F., Lafferty, J. D., Mercer, R. L.,and Roossin, P. S. (1990).
A statisticM ap-proach to machine translation.
Computa-tional Linguistics, 16(2):79-85.\[Brown et al, 1988\] Brown, P. F., Cocke, J.,DellaPietra, S. A., DellaPietra., V. J., .le-linek, F., Mercer, R. L., and Roossin, P. S.(1988).
A statistical approach to languagetranslation.
In Proceedings of the I2th In-ternational Conference on ComputationalLinguisticsl Budapest, Hungary.\[Catizone t al., 1989\] Catizone, R., Russell,G., and Warwick, S. (1989).
Deriving trans-lation data \[rom bilingual texts.
In Proceed-ings of the First International AcquisitionWorkshop, Detroit, Michigan.\[Dempster t al., \]977\] Dempster, A., Laird,N., and Rubin, D. (1977).
Maximum likeli-hood from incomplete data via the EM al-gorithm.
Journal of the Royal StatisticalSociety, 39(B):1-38.\[Gale and Church, 1991\] Gale, W. A. andChurch, K. W. (1991).
A program for align-ing sentences in bilingual corpora.
In Pro-ceedings of the 2gth Annual Meeting of theA ssociation for Computational Linguistics,Berkeley, California.\[Kay, \]991\] Kay, M. (1991).
Text-translationalignment.
In ACII/ALLC '91: "Mak-in.q Connections" Conference Handbook,Tempe, Arizona.\[Klavans and Tzoukermann, 1990\]Kiavans, .l.
and Tzoukermann, E. (1990).The bicord system.
\]n COLING-90, pages174-179, Ilelsinki, Finland.\[Sadler, 19~9\] Sadler, V. (1989).
The Bilin-gual Knowledge Bank- A New ConceptualBasis for MT.
BSO/Research, Utrecht.\[Warwick and Russell, 1990\] Wa.rwick, S. andRussell, G. (1990).
Bilingual concordancingand bilingnM lexicography.
In EURALEX4th International Congress, M~ilaga, Spain.176
