Multiword Unit Hybrid ExtractionGa?l DiasCentre of MathematicsBeira Interior UniversityCovilh?, Portugalddg@di.ubi.ptAbstractThis paper describes an original hybrid systemthat extracts multiword unit candidates frompart-of-speech tagged corpora.
While classicalhybrid systems manually define local part-of-speech patterns that lead to the identificationof well-known multiword units (mainly com-pound nouns), our solution automaticallyidentifies relevant syntactical patterns fromthe corpus.
Word statistics are then combinedwith the endogenously acquired linguistic in-formation in order to extract the most relevantsequences of words.
As a result, (1) human in-tervention is avoided providing total flexibil-ity of use of the system and (2) differentmultiword units like phrasal verbs, adverbiallocutions and prepositional locutions may beidentified.
The system has been tested on theBrown Corpus leading to encouraging results.1 IntroductionMultiword units (MWUs) include a large range of lin-guistic phenomena, such as compound nouns (e.g.
inte-rior designer), phrasal verbs (e.g.
run through),adverbial locutions (e.g.
on purpose), compound deter-minants (e.g.
an amount of), prepositional locutions(e.g.
in front of) and institutionalized phrases (e.g.
concarne).
MWUs are frequently used in everyday lan-guage, usually to precisely express ideas and conceptsthat cannot be compressed into a single word.
As a con-sequence, their identification is a crucial issue for appli-cations that require some degree of semantic processing(e.g.
machine translation, summarization, informationretrieval).In recent years, there has been a growing awareness inthe Natural Language Processing (NLP) community ofthe problems that MWUs pose and the need for theirrobust handling.
For that purpose, syntactical (DidierBourigault, 1993), statistical (Frank Smadja, 1993; TedDunning, 1993; Ga?l Dias, 2002) and hybrid syntaxico-statistical methodologies (B?atrice Daille, 1996; Jean-Philippe Goldman et al 2001) have been proposed.In this paper, we propose an original hybrid systemcalled HELAS1 that extracts MWU candidates frompart-of-speech tagged corpora.
Unlike classical hybridsystems that manually pre-define local part-of-speechpatterns of interest (B?atrice Daille, 1996; Jean-PhilippeGoldman et al 2001), our solution automatically identi-fies relevant syntactical patterns from the corpus.
Wordstatistics are then combined with the endogenously ac-quired linguistic information in order to extract the mostrelevant sequences of words i.e.
MWU candidates.Technically, we conjugate the Mutual Expectation (ME)association measure with the acquisition process calledGenLocalMaxs (Ga?l Dias, 2002) in a five step process.First, the part-of-speech tagged corpus is divided intotwo sub-corpora: one containing words and one contain-ing part-of-speech tags.
Each sub-corpus is then seg-mented into a set of positional ngrams i.e.
orderedvectors of textual units.
Third, the ME independentlyevaluates the degree of cohesiveness of each positionalngram i.e.
any positional ngram of words and any posi-tional ngram of part-of-speech tags.
A combination ofboth MEs is then used to evaluate the global degree ofcohesiveness of any sequence of words associated withits respective part-of-speech tag sequence.
Finally, theGenLocalMaxs retrieves all the MWU candidates byevidencing local maxima of association measure valuesthus avoiding the definition of global thresholds.
Theoverall architecture can be seen in Figure 1.Compared to existing hybrid systems, the benefits ofHELAS are clear.
By avoiding human intervention inthe definition of syntactical patterns, it provides total1 HELAS stands for Hybrid Extraction of Lexical ASsocia-tions.flexibility of use.
Indeed, the system can be used for anylanguage without any specific tuning.
HELAS also al-lows the identification of various MWUs like phrasalverbs, adverbial locutions, compound determinants,prepositional locutions and institutionalized phrases.Finally, it responds to some extent to the affirmation ofBeno?t Habert and Christian Jacquemin (1993) thatclaim that ?existing hybrid systems do not sufficientlytackle the problem of the interdependency between thefiltering stage [the definition of syntactical patterns]and the acquisition process [the scoring and the electionof relevant sequences of words] as they propose thatthese two steps should be independent?.Figure 1: Global architecture of HELASThe article is divided into five main sections: (1) weintroduce the related work; (2) we present the text cor-pus segmentation into positional ngrams; (3) we definethe Mutual Expectation and a new combined associationmeasure; (4) we propose the GenLocalMaxs algorithmas the acquisition process; Finally, in (5), we presentsome results over the Brown Corpus.2 Related WorkFor the purpose of MWU extraction, syntactical, statis-tical and hybrid syntaxico-statistical methodologieshave been proposed.
On one hand, purely linguistic sys-tems (Didier Bourigault, 1993) propose to extract rele-vant MWUs by using techniques that analyse specificsyntactical structures in the texts.
However, these meth-odologies suffer from their monolingual basis as thesystems require highly specialised linguistic techniquesto identify clues that isolate possible MWU candidates.On the other hand, purely statistical systems (FrankSmadja, 1993; Ted Dunning, 1993; Ga?l Dias, 2002)extract discriminating MWUs from text corpora bymeans of association measure regularities.
As they useplain text corpora and only require the information ap-pearing in texts, such systems are highly flexible andextract relevant units independently from the domainand the language of the input text.
However, thesemethodologies can only identify textual associations inthe context of their usage.
As a consequence, manyrelevant structures can not be introduced directly intolexical databases as they do not guarantee adequate lin-guistic structures for that purpose.Finally, hybrid syntactico-statistical systems (B?atriceDaille, 1996; Jean-Philippe Goldman et al 2001) defineco-occurrences of interest in terms of syntactical pat-terns and statistical regularities.
Thus, such systemsreduce the searching space to groups of words that cor-respond to a priori defined syntactical patterns (e.g.Adj+Noun, Noun+Prep+Noun) and apply statisticalscores to identify the most relevant sequences of words.One major drawback of such systems is that they do notdeal with a great proportion of interesting MWUs (e.g.phrasal verbs, prepositional locutions).
Moreover, theylack flexibility as the syntactical patterns have to berevised whenever the targeted language changes.In order to overcome these difficulties, we propose anoriginal architecture that combines word statistics withendogenously acquired linguistic information.
We baseour study on two assumptions.
On one hand, a great dealof studies in lexicography and terminology assess thatmost of the MWUs evidence well-known morpho-syntactic structures (Gaston Gross, 1996).
On the otherhand, MWUs are recurrent combinations of words.
In-deed, according to Beno?t Habert and Christian Jacque-min (1993), the MWUs may represent a fifth of theoverall surface of a text.
Consequently, it is reasonableto think that the syntactical patterns embodied by theMWUs may be endogenously identified by using statis-tical scores over texts of part-of-speech tags exactly inthe same manner as word dependencies are identified incorpora of words.
So, the global degree of cohesivenessof any sequence of words may be evaluated by a combi-nation of its degree of cohesiveness of words and thedegree of cohesiveness of its associated part-of-speechtag sequence (See Figure 1).Compared to existing systems, the benefits of our archi-tecture are clear.
By avoiding human intervention in thedefinition of syntactical patterns, (1) HELAS providestotal flexibility of use being independent of the targetedInput TaggedTextText of Words Text of TagsWord ngrams Tag ngramsME (word ngram)ME (word ngram)?xME (tag ngram)1- ?ME (tag ngram)GenLocalMaxsMWU candidateslanguage and (2) it allows the identification of variousMWUs like phrasal verbs, adverbial locutions, com-pound determinants, prepositional locutions and institu-tionalized phrases.3 Text SegmentationPositional ngrams are nothing more than ordered vec-tors of textual units which principles are introduced inthe next subsection.3.1 Positional NgramsThe original idea of the positional ngram model (Ga?lDias, 2002) comes from the lexicographic evidence thatmost lexical relations associate words separated by atmost five other words (John Sinclair, 1974).
As a con-sequence, lexical relations such as MWUs can be con-tinuous or discontinuous sequences of words in acontext of at most eleven words (i.e.
5 words to the leftof a pivot word, 5 words to the right of the same pivotword and the pivot word itself).
In general terms, aMWU can be defined as a specific continuous or dis-continuous sequence of words in a (2.F+1)-word sizewindow context (i.e.
F words to the left of a pivot word,F words to the right of the same pivot word and thepivot word itself).
This situation is illustrated in Figure2 for the multiword unit Ngram Statistics that fits in thewindow context of size 2.3+1=7.Figure 2: 7-word size window contextThus, any substring (continuous or discontinuous) thatfits inside the window context and contains the pivotword is called a positional word ngram.
For instance,[Ngram Statistics] is a positional word ngram as is thediscontinuous sequence [Ngram ___ from] where the gaprepresented by the underline stands for any word occur-ring between Ngram and from (in this case, Statistics).More examples are given in Table 1.Positional word 2grams Positional word 3grams[Ngram Statistics] [Ngram Statistics from][Ngram ___ from] [Ngram Statistics ___ Large][Ngram ___ ___ Large] [Ngram ___ from Large][to ___ Ngram] [to ___ Ngram ___ from]Table 1: Possible positional ngramsGenerically, any positional word ngram may be definedas a vector of words [p11 u1 p12 u2 ?
p1n un] where uistands for any word in the positional ngram and p1irepresents the distance that separates words u1 and ui2.Thus, the positional word ngram [Ngram Statisitcs] wouldbe rewritten as [0 Ngram +1 Statistics].
More examples aregiven in Table 2.Positional word ngrams Algebraic notation[Ngram ___ from] [0 Ngram +2 from][Ngram ___ ___ Large] [0 Ngram +3 Large][to ___ Ngram] [0 to +2 Ngram][Ngram Statistics ___ Large] [0 Ngram +1 Statisitcs +3 Large]Table 2: Algebraic NotationHowever, in a part-of-speech tagged corpus, each wordis associated to a unique part-of-speech tag.
As a conse-quence, each positional word ngram is linked to a corre-sponding positional tag ngram.
A positional tag ngramis nothing more than an ordered vector of part-of-speechtags exactly in the same way a positional word ngram isan ordered vector of words.
Let?s exemplify this situa-tion.
Let?s consider the following portion of a part-of-speech tagged sentence following the Brown tag set:Virtual /JJ Approach /NN to /IN Deriving /VBG Ngram /NN Statistics /NNfrom /IN Large /JJ Scale /NN Corpus /NNIt is clear that the corresponding positional tag ngram ofthe positional word ngram [0 Ngram +1 Statisitcs] is thevector [0 /NN +1 /NN].
More examples are in Table 3.Generically, any positional tag ngram may be defined asa vector of part-of-speech tags [p11 t1 p12 t2 ?
p1n tn]where ti stands for any part-of-speech tag in the posi-tional tag ngram and p1i represents the distance thatseparates the part-of-speech tags t1 and ti.Positional word ngrams Positional tag ngrams[0 Ngram +2 from] [0 /NN +2 /IN][0 Ngram +3 Large] [0 /NN +3 /JJ][0 to +2 Ngram] [0 /IN +2 /NN][0 Ngram +1 Statisitcs +3 Large] [0 /NN +1 /NN +3 /JJ]Table 3: Positional tag ngramsSo, any sequence of words, in a part-of-speech taggedcorpus, is associated to a positional word ngram and acorresponding positional tag ngram.
In order to intro-duce the part-of-speech tag factor in any sequence ofwords of part-of-speech tagged corpus, we present analternative notation of positional ngrams called posi-tional word-tag ngrams.In order to represent a sequence of words with its asso-ciated part-of-speech tags, a positional ngram may berepresented by the following vector of words and part-2 By statement, any pii is equal to zero.Virtual   Approach to Deriving   Ngram  Statistics from Large   ScalepivotF=3 F=3of-speech tags [p11 u1 t1 p12 u2 t2?
p1n un tn] where uistands for any word in the positional ngram, ti stands forthe part-of-speech tag of the word ui and p1i representsthe distance that separates words u1 and ui.
Thus, thepositional ngram [Ngram Statistics] can be represented bythe vector [0 Ngram /NN +1 Statistics /NN] given the textcorpus in section (3.1).
More examples are given in Ta-ble 4.Positional ngrams Alternative notation[Ngram ___ from] [0 Ngram /NN +2 from /IN][Ngram ___ ___ Large] [0 Ngram /NN +3 Large /JJ][to ___ Ngram] [0 to /IN +2 Ngram /NN]Table 4: Alternative NotationThis alternative notation will allow us to defining, withelegance, our combined association measure, introducedin the next section.3.2 Data PreparationSo, the first step of our architecture deals with segment-ing the input text corpus into positional ngrams.
First,the part-of-speech tagged corpus is divided into twosub-corpora: one sub-corpus of words and one sub-corpus of part-of-speech tags.
The word sub-corpus isthen segmented into its set of positional word ngramsexactly in the same way the tagged sub-corpus is seg-mented into its set of positional tag ngrams.In parallel, each positional word ngram is associated toits corresponding positional tag ngram in order to fur-ther evaluate the global degree of cohesiveness of anysequence of words in a part-of-speech tagged corpus.Our basic idea is to evaluate the degree of cohesivenessof each positional ngram independently (i.e.
the posi-tional word ngrams on one side and the positional tagngrams on the other side) in order to calculate the globaldegree of cohesiveness of any sequence in the part-of-speech tagged corpus by combining its respective de-grees of cohesiveness i.e.
the degree of cohesiveness ofits sequence of words and the degree of cohesiveness ofits sequence of part-of-speech tags.In order to evaluate the degree of cohesiveness of anysequence of textual units, we use the association meas-ure called Mutual Expectation.4 Cohesiveness EvaluationThe Mutual Expectation (ME) has been introduced byGa?l Dias (2002) and evaluates the degree of cohesive-ness that links together all the textual units contained ina positional ngram (?n, n ?
2) based on the concept ofNormalized Expectation and relative frequency.4.1 Normalized ExpectationThe basic idea of the Normalized Expectation (NE) is toevaluate the cost, in terms of cohesiveness, of the loss ofone element in a positional ngram.
Thus, the NE is de-fined in Equation 1 where the function k(.)
returns thefrequency of any positional ngram3.
[ ]( )[ ]( )[ ]( ) ?????????????????????
?+=?=nikknkNE2n1n^i^1i1 11n2n i 2i2 22n1ni1i1 11n1ni1i1 11u p ... u  p ... upup ... up ... up1u p ... u ...p upu p ... u ...p upEquation 1: Normalized ExpectationIn order to exemplify the NE formula, we present inEquation 2 its development for the given positionalngram [0 A +2 C +3 D +4 E] where each letter may repre-sent a word or a part-of-speech tag.
[ ]( ) [ ]( )[ ]( )[ ]( )[ ]( )[ ]( ) ?????????????
?+++=E 2 ,D 1 ,C0E 4 ,D 3 ,A0E 4 ,C 2 ,A0D 3 ,C 2 ,A041E 4 ,D 3 ,C 2 ,A0E 4 D, 3 ,C 2 ,A0kkkkkNEEquation 2: Normalized Expectation exampleHowever, evaluating the average cost of the loss of anelement is not enough to characterize the degree of co-hesiveness of a sequence of textual units.
The MutualExpectation is introduced to solve this insufficiency.4.2 Mutual ExpectationMany applied works in Natural Language Processinghave shown that frequency is one of the most relevantstatistics to identify relevant textual associations.
Forinstance, in the context of multiword unit extraction,(John Justeson and Slava Katz, 1995; B?atrice Daille,1996) assess that the comprehension of a multiword unitis an iterative process being necessary that a unit shouldbe pronounced more than one time to make its compre-hension possible.
G?el Dias (2002) believes that thisphenomenon can be enlarged to part-of-speech tags.From this assumption, they pose that between two posi-tional ngrams with the same NE, the most frequent posi-tional ngram is more likely to be a relevant sequence.So, the Mutual Expectation of any positional ngram isdefined in Equation 3 based on its NE and its relativefrequency embodied by the function p(.
).3 The "^" corresponds to a convention used in Algebra thatconsists in writing a "^" on the top of the omitted term of agiven succession indexed from 1 to n.[ ]( )[ ]( ) [ ]( )n1ni1i1 11n1ni1i1 11n1ni1i1 11u p ... u ...p upu p ... u ...p upu p ... u ...p upNEpME?=Equation 3: Mutual ExpectationWe will note that the ME shows interesting properties.One of them is the fact that it does not sub-evaluate in-terdependencies when frequent individual textual unitsare present.
In particular, this allows us to avoid the useof lists of stop words.
Thus, when calculating all thepositional ngrams, all the words and part-of-speech tagsare used.
This fundamentally participates to the flexibil-ity of use of our system.As we said earlier, the ME is going to be used to calcu-late the degree cohesiveness of any positional wordngram and any positional tag ngram.
The way we calcu-late the global degree of cohesiveness of any sequenceof words associated to its part-of-speech tag sequence,based on its two MEs, is discussed in the next subsec-tion.4.3 Combined Association MeasureThe drawbacks shown by the statistical methodologiesevidence the lack of linguistic information.
Indeed,these methodologies can only identify textual associa-tions in the context of their usage.
As a consequence,many relevant structures can not be introduced directlyinto lexical databases as they do not guarantee adequatelinguistic structures for that purpose.In this paper, we propose a first attempt to solve thisproblem without pre-defining syntactical patterns ofinterest that bias the extraction process.
Our idea is sim-ply to combine the strength existing between words in asequence and the evidenced interdependencies betweenits part-of-speech tags.
We could summarize this idea asfollows: the more cohesive the words of a sequence andthe more cohesive its part-of-speech tags, the morelikely the sequence may embody a multiword unit.This idea can only be supported due to two assumptions.On one hand, a great deal of studies in lexicography andterminology assess that most of the MWUs evidencewell-known morpho-syntactic structures (Gaston Gross,1996).
On the other hand, MWUs are recurrent combi-nations of words capable of representing a fifth of theoverall surface of a text (Beno?t Habert and ChristianJacquemin, 1993).
Consequently, it is reasonable tothink that the syntactical patterns embodied by theMWUs may endogenously be identified by using statis-tical scores over texts of part-of-speech tags exactly inthe same manner as word dependencies are identified incorpora of words.
So, the global degree of cohesivenessof any sequence of words may be evaluated by a combi-nation of its own ME and the ME of its associated part-of-speech tag sequence.
The degree of cohesiveness ofany positional ngram based on a part-of-speech taggedcorpus can then be evaluated by the combined associa-tion measure (CAM) defined in Equation 4 where ?stands as a parameter that tunes the focus whether onwords or on part-of-speech tags.
[ ]( )[ ]( ) [ ]( ) ??
?
?=1n1ni1i1 11n1ni1i1 11nn 1nii1i11 11tp ...  t...p tpu p ... u ...p uptu p ... tu ...p tupMEMECAMEquation 4: Combined Association MeasureWe will see in the final section of this paper that differ-ent values of ?
lead to fundamentally different sets ofmultiword unit candidates.
Indeed, ?
can go from a totalfocus on part-of-speech tags (i.e.
the relevance of aword sequence is based only on the relevance of its part-of-speech sequence) to a total focus on words (i.e.
therelevance of a word sequence is defined only by itsword dependencies).
Before going to experimentation,we need to introduce the used acquisition process whichobjective is to extract the MWUs candidates.5 The Acquisition ProcessThe GenLocalMaxs (Ga?l Dias, 2002) proposes a flexi-ble and fine-tuned approach for the selection process asit concentrates on the identification of local maxima ofassociation measure values.
Specifically, the GenLo-calMaxs elects MWUs from the set of all the valuedpositional ngrams based on two assumptions.
First, theassociation measures show that the more cohesive agroup of words is, the higher its score will be.
Second,MWUs are localized associated groups of words.
So, wemay deduce that a positional word-tag ngram is a MWUif its combined association measure value is higher orequal than the combined association measure values ofall its sub-groups of (n-1) words and if it is strictlyhigher than the combined association measure values ofall its super-groups of (n+1) words.
Let cam be thecombined association measure, W a positional word-tagngram, ?n-1 the set of all the positional word-tag (n-1)-grams contained in W, ?n+1 the set of all the positionalword-tag (n+1)-grams containing W and sizeof(.)
a func-tion that returns the number of words of a positionalword-tag ngram.
The GenLocalMaxs is defined as:?x ?
?n-1 , ?y ?
?n+1 ,   W  is a MWU if(sizeof(W)=2  ?
cam(W) > cam(y) )?
(sizeof(W)?2  ?
cam(W) ?
cam(x)  ?
cam(W) > cam(y))Definition 1: GenLocalMaxs AlgorithmAmong others, the GenLocalMaxs shows one importantproperty: it does not depend on global thresholds.
Adirect implication of this characteristic is the fact that, asno tuning needs to be made in order to acquire the set ofall the MWU candidates, the use of the system remainsas flexible as possible.
Finally, we show the results ob-tained by applying HELAS over the Brown Corpus.6 The ExperimentsIn order to test our architecture, we have conducted anumber of experiments with 11 different values of ?
fora portion of the Brown Corpus containing 249 578words i.e.
249 578 words plus its 249 578 part-of-speech tags.
The limited size of our corpus is mainlydue to the space complexity of our system.
Indeed, thenumber of computed positional ngrams is huge even fora small corpus.
For instance, 21 463 192 positionalngrams are computed for this particular corpus for a 7-word size window context.
As a consequence, computa-tion is hard.
For this experiment, HELAS has beentested on a personal computer with 128 Mb of RAM, 20Gb of Hard Disk and an AMD 1.4 Ghz processor underLinux Mandrake 7.2.
On average, each experiment (i.e.for a given ?)
took 4 hours and 20 minutes.
Knowingthat our system increases proportionally with the size ofthe corpus, it was unmanageable, for this particular ex-periment, to test our architecture over a bigger corpus.Even though, the whole processing stage lasted almost48 hours4.We will divide our experiment into two main parts.First, we will do a quantitative analysis and then we willlead a qualitative analysis.
All results will only tacklecontiguous multiword units although non-contiguoussequences may be extracted.
This decision is due to thelack of space.6.1 Quantitative AnalysisIn order to understand, as deeply as possible, the inter-action between word cohesiveness and part-of-speechtag cohesiveness, we chose eleven different values for?, i.e.
?
?
{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}, goingfrom total focus on words (?
= 1) to total focus on part-of-speech tags (?
= 0).First, we show the number of extracted contiguousMWU candidates by ?
in table 5.
The total results arenot surprising.
Indeed, with ?
= 0, the focus is exclu-sively on part-of-speech tags.
It means that any wordsequence, with an identified relevant part-of-speechsequence, is extracted independently of the words itcontains.
For instance, all the word sequences with thepattern [/JJ /NN] (i.e.
Adjective + Noun) may be ex-4 We are already working on an efficient implementation ofHELAS using suffix-arrays and the concept of masks.tracted independently of their word dependencies!
Thisobviously leads to an important number of extractedsequences.
The inclusion of the word factor, by increas-ing the value of ?, progressively leads to a decreasingnumber of extracted positional ngrams.
In fact, the wordsequences with relevant syntactical structures are beingfiltered out depending on their word statistics.
Finally,with ?
= 1, the focus is exclusively on words.
The im-pact of the syntactical structure is null and the positionalngrams are extracted based on their word associations.In this case, the word sequences do not form classes ofmorpho-syntactic structures being the reason why lesspositional ngrams are extracted.alpha 0 0.1 0.2 0.3 0.4 0.52gram 23146 21890 20074 17689 15450 134613gram 297 467 567 351 1188 16934gram 86 108 127 163 225 3265gram 79 81 81 82 77 826gram 62 57 56 57 56 58TOTAL 23670 22603 20905 18342 16996 15620alpha 0.6 0.7 0.8 0.9 1.02gram 11531 9950 9114 8650 84653gram 2147 2501 2728 2828 26514gram 428 557 679 740 4845gram 93 112 128 161 1456gram 58 58 60 64 60TOTAL 14257 13178 12709 12443 11805Table 5: Number of extracted MWU candidatesA deeper analysis of table 5 reveals interesting results.The smaller the values of ?, the more positional 2gramsare extracted.
This situation is illustrated in Figure 3.#  of  e x t r a c t e d ngr a ms by  a l pha05 0 0 010 0 0 015 0 0 02 0 0 0 02 5 0 0 00 0 .
1 0 .
2 0 .
3 0 .
4 0 .
5 0 .
6 0 .
7 0 .
8 0 .
9 1al pha2gr am 3gr am 4gr am 5gr am 6gr amFigure 3: Number of extracted MWU candidatesOnce again these results are not surprising.
The MutualExpectation tends to give more importance to frequentsequences of textual units.
While it performs reasonablywell on word sequences, it tends to over-evaluate thepart-of-speech tag sequences.
Indeed, sequences of twopart-of-speech tags are much more frequent than othertypes of sequences and, as a consequence, tend to beover-evaluated in terms of cohesiveness.
As small val-ues of ?
focus on syntactical structures, it is clear that inthis case, small sequences of words are preferred overlonger sequences.By looking at Figure 3 and Table 5, we may think that agreat number of extracted sequences are common toeach experiment.
However, this is not true.
In order toassess this affirmation, we propose, in Table 6, thesummary of the identical ratio.alphas 0 0.1 0.2 0.3 0.4 0.50  14.64 5.74 2.99 1.73 1.170.1   9.99 3.77 2.08 1.350.2    6.2 2.83 1.690.3     4.89 2.360.4      5.310.5alphas 0.6 0.7 0.8 0.9 1.00 0.83 0.63 0.54 0.49 0.470.1 0.93 0.70 0.59 0.54 0.520.2 1.11 0.81 0.68 0.61 0.590.3 1.42 0.98 0.81 0.72 0.690.4 2.34 1.44 1.13 0.97 0.900.5 4.77 2.26 1.62 1.33 1.170.6  5.06 2.82 2.10 1.730.7   7.21 3.99 2.810.8    9.45 4.500.9     7.711.0Table 6: Identical RatioThe identical ratio calculates, for two values of ?, thequotient between the number of identical extracted se-quences and the number of different extracted se-quences.
Thus, the first value of the first row of table 6,represents the identical ratio for ?=0 and ?=0.1, andmeans that there are 14.64 times more identical ex-tracted sequences than different sequences between bothexperiments.Taking ?=0 and ?=1, it is interesting to see that there aremuch more different sequences than identical sequencesbetween both experiments (identical ratio = 0.47).
Infact, this phenomenon progressively increases as theword factor is being introduced in the combined asso-ciation measure to reach ?=1.
This was somewhat unex-pected.
Nevertheless, this situation can be partlydecrypted from Figure 3.
Indeed, figure 3 shows thatlonger sequences are being preferred as ?
increases.
Infact, what happens is that short syntactically well-founded sequences are being replaced by longer wordsequences that may lack linguistic information.
For in-stance, the sequence [Blue Mosque] was extracted with?=0, although the longer sequence [the Blue Mosque] waspreferred with ?=1 as whenever [Blue Mosque] appears inthe text, the determinant [the] precedes it.Finally, a last important result concerns the frequency ofthe extracted sequences.
Table 7 gives an overview ofthe situation.
The figures are clear.
Most of the ex-tracted sequences occur only twice in the input text cor-pus.
This result is rather encouraging as most knownextractors need high frequencies in order to decidewhether a sequence is a MWU or not.
This situation ismainly due to the GenLocalMaxs algorithm.alpha 0 0.1 0.2 0.3 0.4 0.5Freq=2 13555 13093 12235 11061 10803 10458Freq=3 4203 3953 3616 3118 2753 2384Freq=4 1952 1839 1649 1350 1166 960Freq=5 1091 1019 917 743 608 511Freq>2 2869 2699 2488 2070 1666 1307TOTAL 23670 22603 20905 18342 16996 15620alpha 0.6 0.7 0.8 0.9 1.0Freq=2 10011 9631 9596 9554 9031Freq=3 2088 1858 1730 1685 1678Freq=4 766 617 524 485 468Freq=5 392 276 232 202 189Freq>2 1000 796 627 517 439TOTAL 14257 13178 12709 12443 11805Table 7: Number of extracted MWUs by frequency6.2 Qualitative AnalysisAs many authors assess (Frank Smadja, 1993; JohnJusteson and Slava Katz, 1995), deciding whether a se-quence of words is a multiword unit or not is a trickyproblem.
For that purpose, different definitions of mul-tiword unit have been proposed.
One of the most suc-cessful attempts can be attributed to Gaston Gross(1996) that classifies multiword units into six groupsand provides techniques to determine their belonging.As a consequence, we intend as multiword unit anycompound noun (e.g.
interior designer), compound deter-minant (e.g.
an amount of), verbal locution (e.g.
runthrough), adverbial locution (e.g.
on purpose), adjectivallocution (e.g.
dark blue) or prepositional locution (e.g.
infront of).The analysis of the results has been done intramurosalthough we are aware that an external independentcross validation would have been more suited.
How-ever, it was not logistically possible to do so and byusing Gaston Gross?s classification and methodology,we narrow the human error evaluation as much as pos-sible.
Technically, we have randomly extracted and ana-lysed 200 positional 2grams, 200 positional 3grams and100 positional 4grams for each value of ?.
For the spe-cific case of positional 5grams and 6grams, all the se-quences have been analysed.Precision results of this analysis are given in table 8 andshow that word dependencies and part-of-speech tagdependencies may both play an important role in theidentification of relevant sequences.
Indeed, values of ?between 0.4 and 0.5 seem to lead to optimum results.Knowing that most extracted sequences are positional2grams or positional 3grams, the global precision resultsapproximate the results given by 2grams and 3grams.
Inthese conditions, the best results are for ?=0.5 reachingan average precision of 62 %.
This would mean thatword dependencies and part-of-speech tags contributeequally to multiword unit identification.alpha 0 0.1 0.2 0.3 0.4 0.52gram 29 % 22 % 30 % 44 % 53 % 60 %3gram 52 % 77 % 74 % 73 % 80 % 85 %4gram 38 % 32 % 32 % 46 % 47 % 41 %5gram 34 % 28 % 29 % 31 % 33 % 34 %6gram 29 % 22 % 18 % 24 % 31 % 38 %alpha 0.6 0.7 0.8 0.9 1.02gram 45 % 23 % 25 % 18 % 30 %3gram 43 % 35 % 46 % 51 % 36 %4gram 41 % 45 % 39 % 44 % 37 %5gram 27 % 27 % 29 % 38 % 38 %6gram 32 % 37 % 26 % 29 % 29 %Table 8: Precision in % by alphaA deeper look at the results evidences interesting regu-larities as shown in figure 4.
Indeed, the curves for4grams, 5grams and 6grams are reasonably steady alongthe X axis evidencing low results.
This means, to someextent, that that our system does not seem to be able totackle successfully multiword units with more thanthree words.
In fact, neither a total focus on words or onpart-of-speech tags seems to change the extraction re-sults.
However, the importance of these results must beweakened as they represent a small proportion of theextracted structures.Precision by alpha and ngram0%20%40%60%80%100%0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0alphaprecision(%)2gram 3gram 4gram 5gram 6gramFigure 4: Precision by alpha and ngramOn the other hand, the curves for 2grams and 3gramsshow different behaviours.
For the 3gram case, it seemsthat the syntactical structure plays an important role inthe identification process.
Indeed, precision falls downdrastically when the focus passes to word dependencies.This is mainly due to the extraction of recurrent se-quences of words that do not embody multiword unitsyntactical structures like [been able to] or [can still be].As 2grams are concerned, the situation is different.
Infact, it seems that too much focus on either words orpart-of-speech tags leads to unsatisfactory results.
In-deed, optimum results are obtained for a balance be-tween both criteria.
This result can be explained by thefact that there exist many recurrent sequences of twowords in a corpus.
However, most of them are not mul-tiword units like [of the] or [can be].
For that reason, onlya balanced weight on part-of-speech tag and word de-pendencies may identify relevant two word sequences.However, not-so-high precision results show that two-word sequences still remain a tricky problem for ourextractor as it is difficult to filter out very frequent pat-terns that embody meaningless syntactical structures.7 ConclusionThis paper describes an original hybrid system that ex-tracts multiword unit candidates by endogenously iden-tifying relevant syntactical patterns from the corpus andby combining word statistics with the acquired linguis-tic information.
As a result, by avoiding human inter-vention in the definition of syntactical patterns, (1)HELAS provides total flexibility of use being independ-ent of the targeted language and (2) it allows the identi-fication of various MWUs like compound nouns,compound determinants, verbal locutions, adverbiallocutions, prepositional locutions and adjectival locu-tions without defining any threshold or using lists ofstop words.
The system has been tested on the BrownCorpus leading to encouraging results evidenced by aprecision score of 62 % for the best configuration.
Thesystem will soon be available on http://helas.di.ubi.pt.ReferencesB?atrice Daille.
1996.
Study and Implementation of Combined Tech-niques for Automatic Extraction of Terminology.
The balancingact combining symbolic and statistical approaches to language,MIT Press, 49-66.Beno?t Habert and Chistian Jacquemin.
1993.
Noms compos?s, termes,d?nominations complexes: probl?matiques linguistiques et traite-ments automatiques.
Traitement Automatique des Langues, vol.34(2), 5-41.Didier Bourigault.
1993.
Analyse syntaxique locale pour le rep?ragede termes complexes dans un texte.
Traitement Automatique desLangues, vol.
34 (2), 105-117.Frank Smadja.
1993.
Retrieving collocations from text: XTRACT.Computational Linguistics, vol.
19(1), 143-177.Ga?l Dias.
2002.
Extraction Automatique d?Associations Lexicales ?partir de Corpora.
PhD Thesis.
DI/FCT New University of Lisbon(Portugal) and LIFO University of Orl?ans (France).Gaston Gross.
1996.
Les expressions fig?es en fran?ais.
Paris, Ophrys.Jean-Philippe Goldman, Luka Nerima and Eric Wehrli.
2001.
Collo-cation Extraction using a Syntactic Parser.
Workshop of the 39thAnnual Meeting of the Association for Computational Linguisticson Collocation: Computational Extraction, Analysis and Exploita-tion, Toulouse, France, 61-66.John Justeson and Slava Katz.
1995.
Technical Terminology: somelinguistic properties and an algorithm for identification in text.Natural Language Engineering, vol.
1, 9-27.John Sinclair.
1974.
English Lexical Collocations: A study in compu-tational linguistics.
Singapore, reprinted as chapter 2 of Foley, J.A.
(ed).
1996, John Sinclair on Lexis and Lexicography, Uni Press.Ted Dunning.
1993.
Accurate Methods for the Statistics of Surpriseand Coincidence.
Computational Linguistics, vol.
19(1), 61-74.
