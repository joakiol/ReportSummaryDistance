Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 56?64,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsDistributed Parse MiningScott A. Waterman, PhDMicrosoft Live Search/Powerset475 Brannan St.San Francisco, USAwaterman@acm.orgAbstractWe describe the design and implementation ofa system for data exploration over dependencyparses and derived semantic representationsin a large-scale NLP-based search system atpowerset.com.
Because of the distributednature of the document repository and the pro-cessing infrastructure, and also the complexrepresentations of the corpus data, standardtext analysis tools such as grep or awk orlanguage modeling toolkits are not applicable.This paper explores the challenges of extract-ing statistical information and of building lan-guage models in such a distributed NLP envi-ronment, and introduces a corpus analysis sys-tem, Oceanography, that simplifies the writ-ing of analysis code and transparently takesadvantage of existing distributed processinginfrastructure.1 IntroductionIn computational linguistics we deal with large cor-pora and vast amounts of data from which we wouldlike to extract useful information.
The size of thetext resources, derived linguistic analyses, and thecomplexity of their representations is often a stum-bling block on the way to understanding the statisti-cal and linguistic behavior within the corpus.
Sim-ple software tools suffice for small or simple anal-ysis problems, or for building models of easily rep-resented relations.
However, as the size of data, theintricacy of relations to be analyzed, and the com-plexity of the representation grow, so too does thetechnical difficulty of conducting the analysis.Software is our given means of escape from thisescalation of complexity.
However, as ?computa-tional linguists,?
we often find ourselves spendingmore time and attention building software to per-form the required computations than we do on un-derstanding the linguistics.Even once a suitable set of NLP tools (e.g.
tag-gers, chunkers, parsers, etc.)
has been chosen, anal-ysis software, in the CL world, often consists of?throw away?
scripts.
Small, ad hoc programs areoften the norm, often with no assurance (via strictdesign or testing) of correctness or completeness.1.1 OceanographyOur goal is to ensure that analysis is not so prob-lematic.
Powerset is a group within the MicrosoftLive Search team focused on using semantic NLPto improve web search.
We face many problemswith the scale and integration of our NLP compo-nents, and are approaching solving them by applyingsound software design and abstraction principles tocorpus processing.
By generalizing tools to fit theprocessing environment, and the nature of the prob-lems at hand, we enable flexible processing whichscales with the size of the platform and the data.The Oceanography software environment is de-signed to address two important needs in large cor-pus analysis.
The first is to simplify the actual pro-gramming of analysis code to reduce developmenttime and increase reliability, and the second is to usethe available distributed computing resources to re-duce running time and provide for rapid testing andexperimental turnaraound.561.2 Linguistic and Diagnostic data analysisThere are two separate kinds of analysis we wantto support over this processed corpus.
The first islinguistic modeling.
In order to achieve the best se-mantic interpretation of each source document, weseek to understand the linguistic behavior withinthe corpus.
Probabilistic parsing, entity extraction,sense disambiguation, and argument role assign-ment are all informed by structured, statistical mod-els of word behavior within the corpus.
Some mod-els can be built from simple tokenized text, whileother models need to incorporate parse dependen-cies or real-word knowledge of entities.
Some ofthese tasks are exploratory and underspecified (e.g.selectional restrictions), while others, such as nametagging, have a well-developed literature and a num-ber of almost standard methodologies.The second kind of analysis is aimed at character-izing and improving system behavior.
For example,distributions of POS-tags or preposition attachmentscan serve as regression indicators for parser perfor-mance.
In order to perform error analysis, we needto selectively sample various types of label assign-ments or parse structures.
So summarization andsampling from the various intermediate NL analysesare very important processes to support.2 Generalizing Text MiningWe have found that most of these analysis and datamodeling tasks share certain higher order steps thatallow us to generalize them all into a single pro-gramming framework.
All involve identifying somephenomena in one of the NLP outputs, represent-ing it in some characteristic form, and then sum-ming or comparing distributions.
These generalsteps apply to many corpus tasks, including buildingn-gram data, learning sentence breaks, identifyingselectional preferences, or building role mappingsfor verb nominalizations.The Oceanography system generalizes these stepsinto a declarative language for stating the selectionof data, and the form of output, in a way that avoidsrepetitive and error prone boilerplate code for filetraversal, regular expression matching, and statisticsprogramming.
By matching a declarative syntax tothe general analysis steps, these common functionscan be relegated to library code, or wrapped into theexecutable in the compilation step.
The less timespent in describing a task, or in coding and debug-ging the implementation, the more time and atten-tion can be spent in understanding the results andmodeling the linguistic processes that underly thedata.This sort of abstraction away from the detailsof file representation, storage architecture, and pro-cessing model fits a general trend toward data min-ing, or text mining (Feldman and Dagan, 1995).
Indata mining or KDD systems (Fayyad et al, 1996),the goal is to separate the tasks of creative anal-ysis and theorizing from the mundane aspects oftraversing the data collection and computing statis-tics.
These are much the same goals emphasizedby Tukey (1977) ?
exploration of the data and in-teractions in order to understand which hypotheses,and which models of interaction, would be fruitfulto explore.
For our needs in analyzing collections oftext, parses, and semantic representations, we haveachieved a very practical step toward these goals.2.1 Matching process to conceptionWe have found four steps that map very closely toour conception of the data analysis problem, whichat the same time are easily translated to implemen-tations that can be run on both small local data setsand on very large distributed corpora.1.
Pattern matching ?
find the interesting phe-nomena among the mass of data, by declaring aset of desired properties to be met.
In Oceanog-raphy, these are matched per-sentence.2.
Transformation ?
rewrite the raw occurrencedata to identify the interesting part, and isolateit from the background3.
Aggregation ?
group together instances of thesame kind4.
Statistics ?
compute statistics for counts, relativefrequency, conditional distributions, distribu-tional comparisons, etc.In the following sections we describe the nature ofeach step in more detail, map these steps to a declar-ative data analysis language, give some motivatingexamples, and describe how these steps are typically57accomplished in an exploratory setting for NLP in-vestigations.Later, in section 4, we describe how the stepsare mapped to processing operations within the NLPpipeline architecture.
Following that, we give exam-ples of how this framework maps to specific prob-lems, of both the exploratory and the diagnostictype.2.2 Pattern MatchingThe first step is to identify the specific phenomena ofinterest within the source data.
If the data is a com-plex structure, it is helpful to express the patterns ina logical representation of the structure, rather thanmatching the representation directly.Pattern matching in Oceanography for depen-dency parse structures is handled using a domainspecific language (DSL) built explicitly for pattern-based manipulation of parse trees and semantic rep-resentations generated by the XLE linguistic compo-nents (Crouch et al, 2006).
This Transfer language(Crouch, 2006) is normally used in the regular lin-guistic processing pipeline to incrementally rewriteLFG dependency parses into a role-labeled seman-tic representations (semreps) of entities, events, andrelations in the text.
Transfer matches pattern rulesto a current set of parse dependencies or semanticfacts, and writes alternate expressions derived fromthe matched components.
Variables in these expres-sions are bound via Prolog-style unification (Huet,1975).For example, in figure 1, the first expressionword(?
?
?)
will match word forms in a parse thatare ?verb?s, and bind %VerbSk variable to aunique occurrence id and %VerbWord to the verblemma.
The second pattern finds the node in thedependency graph that fills the ob (object) role forthat verb, and extracts its lemmas.
(The %%?s areplaceholder variables in the pattern, needed to matchthe arity of the expression.)
Below, in the samefigure, is a representation of the verb and objectfrom a parse of the phrase ?determined the struc-ture?.
On matching these facts, the VerbWord andObjLemma variables would be bound to the stringsdetermine and structure.In a simpler environment, with more basic textualrepresentations, this pattern matching step would bewritten with regular expressions, for example usingthe familiar grep command.
The balance providedby grep between the simplicity of its operationalmodel (a transform from stdin to stdout) and the ex-pressiveness of the regular expressions allows grepto be a workhorse for data analysis over text.However, except for simple cases such as wordcooccurrence models, the typical need in deep lin-guistic analysis is not well served by regular expres-sions over strings.
Anyone in the NLP field whohas written regular expressions to match, say, part-of-speech labeled text knows the difficulties of hav-ing a pattern language which differs from the logicalstructures being matched.
Another typical solutionis to write a short program in a scripting language(e.g.
perl, python, SNOBOL) which combines regu-lar expressions to provide a simple structure parser.Tgrep (Pito, 1993) is a one such program which ex-tends this regular expression notion to patterns overtrees, and can output subtrees matching those ex-pressions, but only provided they are represented astext in the LDC TreeBank format.2.3 TransformationOnce the items of the pattern have been identified intheir original context, it is often necessary to isolatethem from that context, and remove the extraneous,irrelevant information.
For instance, if one is do-ing a simple word count, the tokenized words of textmust be separated from any annotation and countedindependently.
For more complicated tasks, such asfinding a verb?s distribution of occurrence with di-rect objects, the verb and object need to be isolatedfrom the remainder of the parse tree, perhaps as thesimple tuple (verb, object), or in a more complexstructure, with additional dependent information.In our case, we express the transformed output ofeach pattern match with an expression built from theunification variables bound to the match.
In figure 2,we construct a vo pair of (verb, object).
This newconstruct is simply added to the collection of factsand representations already present.
All other pre-existing facts in the NL analysis of the sentence alsoremain in context, potentially available for aggrega-tion and counting.==> vo_pair(%VerbWord, %ObjLemma).Figure 2: Transforming the matched pattern58word(%VerbSk, %VerbWord, verb, verb, %%, %%, %%, %% ),in_context(%%, role(hier(ob, %%), %VerbSk, %ObjLemma:%%))word(determine:n(41,3),determine,verb,verb, ....)in_context(t,role(hier(ob,[[ob,root],..]),determine:n(82,3),structure:n(91,3))))Figure 1: Pattern matching using TransferIn shallower text mining, this might be accom-plished using regex matching in a perl program.
An-other common approach is to use command-line texttools such as awk or sed.
Awk (Aho et al, 1977)is designed especially for text mining, but is limitedto plain text files, on single machines, and doesn?textend easily to structured graph representations ordistributed processing.
(But see, e.g.
Sawzall (Pikeet al, 2005) for a scalable awk-like language.
)2.4 AggregationThe aggregation step collects the extracted instancesand groups them by type and by key.
Ratherthan have the matched, transformed results simplydumped out in some enormous file or database intheir order of occurrence in the data set (as onewould get e.g.
from grep), it is quite useful even inthe simplest of cases to aggregate all similar outputitems.
This condenses the mass of data selected, andallows one to see the extent and diversity of the itemsthat are found by the patterns.
This simple countingis often enough for diagnostic tasks, and sometimesfor exploratory tasks when a statistical judgement isnot yet desired.
The aggregation key might be, forvarious kinds of extraction: the head noun of an NN-compound, or the error type for parse errors, or thecontrolling verb of a relative clause.In Oceanography, we require a declaration of thedata that will be aggregated, in order to separate itfrom the remainder which will be discarded.
Thesedeclarations take the form of familiar static type dec-larations, in the style of C++ or Java.
Figure 3shows the simple declaration for our vo pair type,where both fields are declared as strings.
Thesenamed fields also provide a handle to refer to struc-ture members in later statements.In the command line text world, aggregationmight be accomplished by using the unix pipelinevo_pair :: {verb::String, object::String }Figure 3: Declaring aggregation typescommand sort | uniq -c , to organize the out-put by the appropriate key.
If using a small programto do this kind of analysis, one would use a dictio-nary or hash-table and sorting routines to organizethe data before output.2.5 StatisticsWith the matched and extracted data, one can buildup a statistical picture of the data and its interrela-tions.
In our practice, and in the computational NLPliterature, we have found a few fundamental statisti-cal operations that are frequently used to make senseof the corpus data.
Primary among these are sim-ple class counts: the number of occurrences of agiven phenomena.
For instance, the count of part-of-speech tags, or of head nouns with adjective mod-ifiers, or the counts of (verb,object) pairs.
Thesecounts can be computed easily by summing the oc-currences in the aggregated groups.Other statistics are more complicated, requiringcombinations of the simple counts and sums ?normalizing distributions by the total occurrencecounts, for instance, as in the conditional occurrenceof a part-of-speech label relative to the frequencyof the token.
Estimation of log-likelihood ratiosor Pearson?s Chi-square test for pairwise correlationalso falls in this category.
These kinds of computa-tions are used heavily for building classifiers and fordiagnostic purposes.Higher order functions of the counts are also in-teresting, in which various distributions compared.These include computing KL distance between con-ditional distributions for similarity measurements,59clustering over similarity, and building predictive orclassification models for model corpus behavior.3 Data Parallel Document Processing atPowersetTo simplify the processing of large web documentcollections, and flexibly include new processingmodules, we have built a single consistent pro-cessing architecture for the natural language doc-ument pipeline, which allows us to process mil-lions of documents and handle terabytes of analy-sis data effectively.
Coral is the name of the dis-tributed, document-parallel NLP pipeline at Power-set.
Coral provides both a process and a data man-agement framework in order to smoothly execute themulti-step linguistic analysis of all content indexedfor Powerset?s search.Coral controls a multi-step pipeline for deep lin-guistic processing of documents indexed for search.A partial list of the steps every web document un-dergoes includes: HTML destructuring, sentencebreaking, name tagging, parsing, semantic inter-pretation, anaphora resolution, and indexing.
Thepipeline is similar to the UIMA (Ferrucci and Lally,2004) architecture in that each step adds interme-diate data ?
tagged spans, dependency trees, co-referent expressions ?
that can be used in subse-quent steps.
Each step adds a different kind of datato the set, with its own labels and meanings.
Theoutput of all these steps is a daunting amount of in-formation, all of which is valuable for understandingthe linguistic relations within the text, and also thebehavior and effectiveness of the NLP pipeline.Documents are processed in a data-parallel fash-ion.
Multiple documents are processed indepen-dently, across multiple processes on multiple com-pute nodes within a clustered environment.
The doc-ument processing model is sequential, with multi-ple steps run in a fixed sequence for each documentin the index.
All processing for a single documentis typically performed on a single compute node.The steps of the pipeline communicate through in-termediate data writen to the local filesystem in be-tween steps, where each step is free to consume dataproduced earlier.
Output from the stages is check-pointed to backing storage at various points alongthe way, and the final index fragments are merged atthe end.This kind of data-parallel process lends itself wellto a map/reduce programming infrastructure (Deanand Ghemawat, 2004).
Map/reduce divides process-ing into two classes: data-parallel ?map?
operations,and commutative ?reduce?
operations, in which allmap output aggregated under a particular key is pro-cessed together.
In map/reduce terms, the entirelinguistic processing runs as a sequence of ?map?steps (there is no inter-document communication),with a final ?reduce?
step to collect index fragmentsand construct a unified search index.
Coral uses theopen-source hadoop implementation of map/reduce(Cutting, ) as the central task control and distribu-tion mechanism for assigning NLP pipeline jobs todocuments in the input data, and it has full controlof the map/reduce processing layer.3.1 Difficulties for data mining in CoralAll of the intermediate processing output of thepipeline, the name tags, parses, semantic representa-tions, etc., are are retained by this complex process.Unfortunately, they are retained in an unfriendlyformat: small document-addressed chunks scatteredacross a large distributed filesystem, on hundreds ofmachines.
There is no operational way to collectthese chunks in any single file, or to traverse themefficiently from any single point.
Traditional script-ing techniques, even if scalable to the terabytes ofdata, are not applicable to the distributed organiza-tion of the underlying data.3.2 Re-using processing infrastructure forminingHowever, we can re-use the same Coral process anddata management for the problems of data analy-sis.
The breakdown of parse-mining steps presentedearlier, in addition to providing a coherent modelfor data analysis, also maps very cleanly to thedistributed map/reduce computational model.
Bytranslating the four steps of any analysis into corre-sponding map/reduce operations across the linguis-tic pipeline data, we can efficiently translate the cor-pus analytics to an arbitrarily large data setting.
Fur-ther, because we can rely on the Coral process anddata management infrastructure to handle the datamovement and traversal, we allow the researcher orlanguage engineer to concentrate on specifying the60patterns and relations to be investigated, rather thanburdening them with complex yet necessary details.4 Oceanography - a compiled data mininglanguageOceanography has a compiler that transforms shortanalysis programs into multiple map/reduce stepsthat will operate over the corpus of text and deep lin-guistic analyses.
These multiple sub-operations arethen farmed out through the distributed cluster envi-ronment, managed by the Coral job controller.
Thedata flow and dependencies between these jobs arecompiled to a Coral-specific job control language.An oceanography program (cf.
figure 4) is asingle-file description of the data analysis task.
Itcontains specifications for each of the four oper-ations: pattern matching, transformation, aggrega-tion, and statistics.
The program style is declarative?
there are no instructions for iterating over files,summing distributions, or parsing the dependencygraph representations.We find that this matches our intuitions and con-ception of the parse mining task.
A statement ofthe end-product of the analysis is natural: e.g.
findthe conditional distribution of object head nouns forverbs, or symbolically p(obj|verb).
The style of theoceanography program matches this well, where thestatistics statement such asdist triple.object cond on triple.verbstates the desired output, and the preceding pat-tern match and type declarations serve as definitionsto specify precisely what is meant by the variablenames.In the following sections, we will follow the stepsof the Oceanography program in the listing in fig-ure 4.
The example analysis presented is a simpleone ?
to find all verbs with both subject and objectroles, i.e.
triples of (subject, object, verb), and re-port some counts and relative frequencies of verbs,subjects, and objects.4.1 Step 1: Pattern MatchingThe pattern matching rules are similar to thosepresented above in sec.
2.2.
The first linematches a verb term, and the next two linesrequire the presence of terms in both the sub-ject (role(hier(sb, %%))) and objectrole(hier(ob, %%)) roles.
Following theexplicit pattern expression, we add negative checksto ensure that neither the subject or object are PROelements, which have no surface form.4.2 TransformationThe transformation expressed in figure 4 is almosttrivial.
We capture the verb-subject-object triple in asimple three place predicate.
Recall that the valuesof the triple:(%VerbWord, %SubjLemma, %ObjLemma)are bound by unification to the terms matched in thepattern, above.Although we have only one pattern and onematching transformation in this example, we are notin general limited in the number of patterns or out-put expressions we might use.
Multiple transforms,from multiple patterns, can be used.During compilation, these Transfer rules are com-piled into a binary object module, then distributedat runtime to the compute nodes where they will beexecuted in the proper sequence by the Coral jobcontroller.
Output from the transformation step, andbetween all the steps, is encoded as a set of hierar-chically structured objects using JSON (Crockford,2006).
Because JSON provides a simple structuralencoding with named fields, and many programmingenvironments can handle the JSON format, it pro-vides a flexible and self-describing interchange for-mat between steps in the Oceanography runtime.4.3 AggregationThe third section of the Oceanography program de-clares the types of objects to be aggregated follow-ing the transform step.
The type declarations inthis section serve two purposes.
First, they spec-ify exactly what types of data from the match-ing/transformation phase should be carried forward.Recall that all of the source data is available for pro-cessing, but we are likely only interested in a smallportion of it.
Secondly, the declarations serve as typehints to the compiler so that operations and data stor-age are performed correctly in the later phases (e.g.adding strings vs. integers).4.4 StatisticsThe simplest statistic we can compute is the countof a type that has been aggregated.
For example,61## Step 1: pattern matchingrules {word(%VerbSk, %VerbWord, verb, verb, %%Pos, %%SentNum, %%Context, %%LexicalInfo ),in_context(%%, role(hier(sb, %%), %VerbSk, %SubjLemma:%%)),in_context(%%, role(hier(ob, %%), %VerbSk, %ObjLemma:%%)),{ \+memberchk( %SubjLemma, [group_object, null_pro, agent_pro]),\+memberchk( %ObjLemma, [group_object, null_pro, agent_pro]) }## Step 2: Transformation==> triple(%VerbWord, %SubjLemma, %ObjLemma).
}## Step 3: Aggregationtriple :: {verb :: String,subject :: String,object :: String}## Step 4: Statisticscount triplecount triple.verbcount triple.verb, triple.subjectdist triple.object cond on triple.verbFigure 4: A complete Oceanography programcount triple.verbwill result in occurrence counts of each verb seenin the parses.
We can combine primitive types intotuples, in order to count n-grams (which are not nec-essarily adjacent), e.g.count triple.verb, triple.subjectto give occurrence counts for all (verb,subject) pairs.The dist X cond on Y statement is used toproduce the conditional distribution p(x|y).
Themap/reduce framework collates all occurrences witha given value yi to a single reduce function, whichsums the conditional counts of x, and normalizes bythe total.Other statistics require multiple map/reduce op-erations.
Computing the probability for the verbunigrams requires knowing the total number of oc-currences, which, in this kind of data-parallel pro-cessing architecture, is not available until the out-put of all occurrence counts is known.
So, a probtriple.verb statistic must implicitly computecount triple.verb, sum all occurrences, andnormalize across the set.
For a good type-drivenanalysis of information flow during various stagesof a map/reduce computations, see La?mmel (2008).4.5 OutputOutput is given two forms.
For ease of interpreta-tions, human-readable tab delimited files are writ-ten, in which each record is preceded by the type,as given in the argument to the statistics declaration.To simplify later offline computation, the record canalso be written out in a JSON encoded structure withnamed fields corresponding to the type.5 Development and testing inOceanographyRapid turnaround and testing in exploratory corpusanalytics is essential to understanding the nature ofthe data, and the performance and behavior of one?sprogram.
Because the tools on which Oceanogra-phy is built are modular, we can compile an anal-ysis program for a local, single machine target aseasily as we can for a cluster of arbitrarily manycompute nodes.
The resulting compiled programsdiffer somewhat in the ways they traverse the data,and in the control structures for the Coral processingsteps.
However, it was an important design require-ment that we could compile and test using small dataon a single machine as easily as on a muti-terabytecorpus on a distributed cluster.62The same source program is compiled for eithersingle machine or cluster execution.
The user mustspecify a different type of store location for inputand output data, depending on environment.
Compi-lation is done using a command line program, whichtakes as input the Oceanography program, and pro-duces a set of executable outputs, corresponding tothe tasks in the map/reduce process.
These can alsobe run immediately in the single machine setting,with results going to stdout.5.1 Some sample tasksAlthough these tools have been available at Power-set only a few months, we have already used themto great advantage in diagnostic and linguistic anal-ysis tasks.
Diagnostically, it is important to un-derstand the failure modes of the various linguisticpipeline components.
For instance, the morpholog-ical analysis component of the XLE parser will onoccasion encounter tokens it cannot analyze.
Hand-examining a few hundred parses (which starts to ex-ceed the mental fatigue threshold), one can find nu-merous examples.
But one has no idea of the rel-ative frequency of any given type of error, or theircombined effect on the parse output.
Oceanographyenables a very simple single pattern match rule to beused to find the frequency distribution of unknowntokens over 100M sentences as easily as 100, and thegrammar engineers can use this information to pri-oritize their effort.
Other diagnostics on the parse,such as the frequency of certain rare grammaticalconstructs (e.g.
reduced relatives), or the prevalenceof unparseable fragments, or relative frequencies oftransitive v. intransitive use, are immensely impor-tant for understanding the nature of the corpus andthe behavior of the parser.The S-V-O triples used as an example also havepractical import.
By identifying the most commonverb expressions, we can, just as in a keyword stoplist, eliminate or downweight some of the less mean-ingful relations in our semantic index.
For example,in the Wikipedia corpus, one of the most common S-V-O triples comes from the phrase ?this article needsreferences.
?We are also beginning a series of lexical seman-tic studies, looking at selectional preferences andtheir dependence on surface form.
Correspondencebetween prepositional adjunct roles and other sur-face realizations is also an active area.
Additionally,Oceanography is being used to analyze feature datafrom the parses in order to experiment with an unsu-pervised word sense disambiguation project.6 ConclusionWe have presented a methodology for understandinga certain class of linguistic data analysis problems,which identifies the steps of pattern matching, datatransformation, aggregation, and statistics.
We havealso presented a programming system, Oceanogra-phy, which by following this breakdown simplifiesthe programming of these tasks while at the sametime enabling us to take advantage of existing largescale distributed processing infrastructure.AcknowledgmentsI would like to thank Jim Firby, creator of the Coraldocument processing pipeline at Powerset, and DickCrouch, creator of the XLE Transfer system, fortheir foundational work which makes these presentdevelopments possible.63ReferencesAlfred V. Aho, Peter J. Weinberger, and Brian W.Kernighan.
1977. awk.D.
Crockford.
2006.
The application/json Media Typefor JavaScript Object Notation (JSON).
RFC 4627 (In-formational), July.Richard S. Crouch, Mary Dalrymple, Ronald M. Kaplan,Tracy Holloway King, John Maxwell, and P. Newman.2006.
XLE documentation.Richard S. Crouch.
2006.
Packed rewriting for mappingtext to semantics and KR.Doug Cutting.
Apache Hadoop Project.http://hadoop.apache.org/.Jeffrey Dean and Sanjay Ghemawat.
2004.
MapRe-duce: simplified data processing on large clusters.
InOSDI?04: Proceedings of the 6th conference on Sym-posium on Opearting Systems Design & Implementa-tion, Berkeley, CA, USA.
USENIX Association.Usama M. Fayyad, David Haussler, and Paul E. Stolorz.1996.
KDD for Science Data Analysis: Issues andExamples.
In KDD, pages 50?56.Ronen Feldman and Ido Dagan.
1995.
Knowledge Dis-covery in Textual Databases (KDT).
In KDD, pages112?117.David Ferrucci and Adam Lally.
2004.
UIMA: an archi-tectural approach to unstructured information process-ing in the corporate research environment.
Nat.
Lang.Eng., 10(3-4):327?348.Grard P. Huet.
1975.
A unification algorithm for typedlambda-calculus.
Theor.
Comput.
Sci, 1:27.Ralf La?mmel.
2008.
Google?s MapReduce programmingmodel - Revisited.
Sci.
Comput.
Program., 70(1):1?30.Rob Pike, Sean Dorward, Robert Griesemer, and SeanQuinlan.
2005.
Interpreting the data: Parallel analy-sis with Sawzall.
Scientific Programming, 13(4):277?298.Richard Pito.
1993.
Tgrep.John Wilder Tukey.
1977.
Exploratory Data Analysis.Addison-Wesley, New York.64
