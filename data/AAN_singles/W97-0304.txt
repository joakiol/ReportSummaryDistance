Text Segmentation Using Exponential Models*Doug Beeferman Adam Berger John LaffertySchool  of  Computer  Sc ienceCarneg ie  Mel lon  Un ivers i tyAbst ractThis paper introduces a new statistical ap-proach to partitioning text automaticallyinto coherent segments.
Our approach en-lists both short-range and long-range lan-guage models to help it sniff out likely sitesof topic changes in text.
To aid its search,the system consults a set of simple lexicalhints it has learned to associate with thepresence of boundaries through inspectionof a large corpus of annotated ata.
Wealso propose a new probabilistically mo-tivated error metric for use by the natu-ral language processing and information re-trieval communities, intended to supersedeprecision and recall for appraising segmen-tation algorithms.
Qualitative assessmentof our algorithm as well as evaluation usingthis new metric demonstrate he effective-ness of our approach in two very differentdomains, Wall Street Journal articles andthe TDT Corpus, a collection of newswirearticles and broadcast news transcripts.1 In t roduct ionThe task we address in this paper might seem on theface of it rather elementary: identify where one re-gion of text ends and another begins.
This work wasmotivated by the observations that such a seeminglysimple problem can actually prove quite difficult toautomate, and that a tool for partitioning a streamof undifferentiated text (or multimedia) into coher-ent regions would be of great benefit o a number ofexisting applications.The task itself is ill-defined: what exactly is meantby a "region" of text?
We confront this issue by*Research supported in part by NSF grant IRI-9314969, DARPA AASERT award DAAH04-95-1-0475,and the ATR Interpreting Telecommunications ResearchLaboratories.adopting an empirical definition of segment.
At ourdisposal is a collection of online data (38 millionwords of Wall Street Journal archives and another150 million words from selected news broadcasts)annotated with the boundaries between regions--articles or news reports, respectively.
Given this in-put, the task of constructing a segmenter may becast as a problem in machine learning: glean fromthe data a set of hints about where boundaries occur,and use these hints to inform a decision on where toplace breaks in unsegmented data.A general-purpose tool for partitioning expositorytext or multimedia data into coherent regions wouldhave a number of immediate practical uses.
In fact,this research was inspired by a problem in informa-tion retrieval: given a large unpartitioned collectionof expository text and a user's query, return a collec-tion of coherent segments matching the query.
Lack-ing a segmenting tool, an II:t application may be ableto locate positions in its database which are strongmatches with the user's query, but be unable to de-termine how much of the surrounding data to pro-vide to the user.
This can manifest itself in quiteunfortunate ways.
For example, a video-on-demandapplication (such as the one described in (Christelet al, 1995)) responding to a query about a recentnews event may provide the user with a news cliprelated to the event, followed or preceded by part ofan unrelated story or even a commercial.Document summarization is another fertile areafor an automatic segmenter.
Summarization toolsoften work by breaking the input into "topics" andthen summarizing each topic independently.
A seg-mentation tool has obvious applications to the firstof these tasks.The output of a segmenter could also serve asinput to various language-modeling tools.
For in-stance, one could envision segmenting a corpus, clas-sifying the segments by topic, and then construct-ing topic-dependent language models from the gen-erated classes.The paper will proceed as follows.
In Section 2 we35very briefly review some previous approaches to thetext segmentation problem.
In Section 3 we describeour model, including the type of linguistic clues itlooks for in deciding when placing a partition is ap-propriate.
In Section 4 we describe a feature induc-tion algorithm that automatically constructs a set ofthe most informative clues.
Section 5 shows exam-ples of the feature induction algorithm in action.
InSection 6 we introduce a new, probabilistically mo-tivated way to evaluate a text segmenter.
Finally, inSection 7 we demonstrate our model's effectivenesson two distinct domains.2 Some Previous WorkIn this section we very briefly discuss ome previousapproaches to the text segmentation problem.2.1 Text ti l ingThe Te~ctTiling algorithm, introduced by Hearst(Hearst, 1994), segments expository texts into mul-tiple paragraphs of coherent discourse units.
A co-sine measure is used to gauge the similarity betweenconstant-size blocks of morphologically analyzed to-kens.
First-order rates of change of this measure arethen calculated to decide the placement of bound-aries between blocks, which are then adjusted to co-incide with the paragraph segmentation, providedas input to the algorithm.
This approach leveragesthe observation that text segments are dense withrepeated content words.
Relying on this fact, how-ever, may limit precision because the repetition ofconcepts within a document ismore subtle than canbe recognized by only a "bag of words" tokenizerand morphological filter.Word pairs other than "self-triggers," for exam-ple, can be discovered automatically from train-ing data using the techniques of mutual informa-tion employed by our language model.
Furthermore,Hearst's approach segments at the paragraph level,which may be too coarse for applications like in-formation retrieval on transcribed or automaticallyrecognized spoken documents, in which paragraphboundaries are not known.2.2 Lexical cohesion(Kozima, 1993) employs a "lexical cohesion profile"to keep track of the semantic ohesiveness of wordsin a text within a fixed-length window.
In con-trast to Hearst's focus on strict repetition, Kozimauses a semantic network to provide knowledge aboutrelated word pairs.
Lexical cohesiveness betweentwo words is calculated in the network by "acti-vating" the node for one word and observing the"activity value" at the other word after some num-ber of iterations of "spreading activation" betweennodes.
The network is trained automatically using alanguage-specific knowledge source (a dictionary ofdefinitions).
Kozima generalizes lexical cohesivenessto apply to a window of text, and plots the cohe-siveness of successive text windows in a document,identifying the valleys in the measure as segmentboundaries.A graphically motivated segmentation techniquecalled dotplotting is offered in (Reynar, 1994).
Thistechnique uses a simplified notion of lexical cohe-sion, depending exclusively on word repetition tofind tight regions of topic similarity.2.3 Decision trees(Litman and Passonneau, 1995) presents an algo-rithm that uses decision trees to combine multiplelinguistic features extracted from corpora of spokentext, including prosodic and lexical cues.
The deci-sion tree algorithm, like ours, chooses from a spaceof candidate features, some of which are similar toour vocabulary questions.
The set of candidate ques-tions in Litman and Passonneu's approach, however,is lacking in features related to "lexical cohesion.
"In our work we incorporate such features by using apair of language models, as described below.3 A Feature-Based ApproachOur attack on the segmentation problem is based ona statistical framework that we call feature inductionfor random fields and exponential models (Berger,Della Pietra, and Della Pietra, 1996; Della Pietra,Della Pietra, and Lafferty, 1997).
The idea is toconstruct a model which assigns to each position inthe data stream a probability that a boundary be-longs at that position.
This probability distributionarises by incrementally building a log-linear modelthat weighs different "features" of the data.
For sim-plicity, we assume that the features are binary ques-tions.To illustrate (and to show that our approach isin no way restricted to text), consider the task ofpartitioning a stream of multimedia data containingaudio, text and video.
In this setting, the featuresmight include questions such as:?
Does the phrase COMING UP appear in the last ut-terance of the decoded speech??
Is there a sharp change in the video stream in thelast 20 frames??
Does the language model degrade in performance inthe next two utterances??
Is there a "match" between the spectrum of thecurrent image and an image near the last segmentboundary??
Are there blank video frames nearby??
Is there a sharp change in the audio stream in thenext utterance?36The idea of using features is a natural one, andindeed other recent work on segmentation, such as(Litman and Passonneau, 1995), adopts this ap-proach.
We take a unique approach to incorporat-ing the information inherent in various features, us-ing the statistical framework of exponential modelsto choose the best features and combine them in aprincipled manner.3.1 A shor t - range model  of  languageCentral to our approach to segmenting is a pair oftools: a short- and long-range model of language.Monitoring the relative behavior of these two mod-els goes a long way towards helping our segmentersniff out natural breaks in the text.
In this sectionand the next, we describe these language models andexplain their utility in identifying segments.The trigram models Ptri(W \]w-2, W-l) we em-ploy use the Katz backoff scheme (Katz, 19877) forsmoothing.
We trained trigram models on two differ-ent corpora.
The Wall Street Journal corpus (WSJ)is a 38-million word corpus of articles from the news-paper.
The model was constructed using a set },V ofthe approximately 20,000 most frequently occurringwords in the corpus.
Another model was constructedon the Broadcast News corpus (BN), made up of ap-proximately 150 million words (four and a half years)of transcripts of various news broadcasts, includingCNN news, political roundtables, NPR broadcasts,and interviews.By restricting the conditioning information to theprevious two words, the trigram model is making thesimplifying assumption--clearly false--that the useof language one finds in television, radio, and news-paper can be modeled by a second-order Markov pro-cess.
Although words prior to w-2 certainly bear onthe identity of w, higher-order models are impracti-cal: the number of parameters in an n-gram modelis O(\[ W \]~), and finding the resources to computeand store all these parameters becomes a hopelesstask for n > 3.
Usually the lexical myopia of thetrigram model is a hindrance; however, we will seehow a segmenter can in fact make positive use of thisshortsightedness.3.2 A long-range mode l  of  languageOne of the fundamental characteristics of language,viewed as a stochastic process, is that it is highlynonstationary.
Throughout a written documentand during the course of spoken'conversation, thetopic evolves, affecting local statistics on word oc-currences.
A model which could adapt to its recentcontext would seem to offer much over a stationarymodel such as the trigram model.
For example, anadaptive model might, for some period of time afterseeing a word like HOMERUN, boost the probabilitiesof the words {HOMERUN, PITCHER, FIELDER, ER-ROR, BATTER, TRIPLE, OUT}.
For an empirically-driven example, we provide an excerpt from theBN corpus.
Emphasized words mark where a long-range language model might reasonably be expectedto outperform (assign higher probabilities than) ashort-range model:Some doctors are more ski l led at doingthe procedure  than others so it's recom-mended that pat ients  ask doctors  abouttheir track record.
People at high r isk ofstroke include those over age 55 with afamily h is tory  or high b lood pressure,diabetes and smokers.
We urge them tobe evaluated by their family physiciansand this can be done by a very simple pro-cedure simply by having them test  with as tethoscope for symptoms of blockage.One means of injecting long-range awareness intoa language model is by retaining a cache of themost recently seen n-grams which is smoothed to-gether (typically by linear interpolation) with thestatic model; see for example (Jelinek et al, 1991;Kuhn and de Mori, 1990).
Another approach, usingmaximum entropy methods, introduces a parameterfor trigger pairs of mutually informative words, sothat the occurrence of certain words in recent con-text boosts the probability of the words that theytrigger (Lau, Rosenfeld, and Roukos, 1993).The method we use here, described in (Beefer-man, Berger, and Lafferty, 1997), employs a statictrigram model as a "prior," or default distribution,and adds certain features to a family of conditionalexponential models to capture some of the nonsta-tionary features of text.
The features are simpletrigger pairs of words chosen on the basis of mutualinformation.
Figure 1 provides a small sample of the(s,t) trigger pairs used in most of the experimentswe will describe.To incorporate triggers into a long-range lan-guage model, we begin by constructing a standard,static backoff trigram model Ptri (w \] w_ 2, w_ 1 ) as de-scribed in 3.1.
We then build a family of conditionalexponential models of the general formpexp(W I H) =Z(H) exp Aifi(H,w) Ptri(W I w-2, w-1)where H~ W-N,W-N+l,...,w-x is the word his-tory (the N words preceding w in the text), andZ(H) is the normalization constantZ(H) =w EI,'V37($, t) e ARESIDUES, CARCINOGENS 2.3CHARLESTON, SHIPYARDS 4.0MICROSCOPIC, CUTICLE 4.1DEFENSE, DEFENSE 8.4TAX, TAX 10.5KURDS, ANKARA 14.8VLADIMIR, GENNADY 19.6STEVE, STEVE 20.7EDUCATION, EDUCATION 22.2MUSIC, MUSIC 22.4INSURANCE, INSURANCE 23.0PULITZER, PRIZEWINNING 23.6YELTSIN, YELTSIN 23.7RUSSIAN, RUSSIAN 26.
ISAUCE, TEASPOON 27.1FLOWER, PETALS 32.3CASINOS, HARRAH'S 42.8DRUG, DRUG 47.7CLAIRE, CLAIRE 80.9PICKET, SCAB 103.1Table 1: A sample of the 84,694 word pairs fromthe BN domain.
Roughly speaking, after seeing an"s" word, the empirical probability of witnessing thecorresponding "t" in the next N words is boosted bythe factor in the third column.
In the experimentsdescribed herein, N = 500.
A separate set of (s, t)pairs were extracted from the WSJ  corpus.The functions fi, which depend both on the wordhistory H and the word being predicted, are the fea-tures; each fl is assigned a weight A?.
In the modelsthat we built, feature fi is an indicator function,testing for the occurrence of a trigger pair (si,tl):1 i f s iEHandw=t if i(H,w)= 0 otherwise.The above equations reveal that the probability ofa word t involves a sum over all words s such thats E H (s appeared in the past 500 words) and (s, t)is a trigger pair.
One propitious manner of view-ing this model is to imagine that, when assigningprobability to a word w following a history of wordsH, the model "consults" a cache of words which ap-peared in H and which are the left half of some (s, t)trigger pair.
In general, the cache consists of con-tent words s which promote the probability of theirmate t, and correspondingly demote the probabilityof other words.
As described in (Beeferman, Berger,and Lafferty, 1997), for each (s,t) trigger pair therecorresponds a real-valued parameter A; the proba-bility of t is boosted by a factor of e x for W wordsfollowing the occurrence of si.The training algorithm we use for estimating theA values is the Improved Iterative Scaling algorithmof (Della Pietra, Della Pietra, and Lafferty, 1997),which is a scheme for solving the maximum like-lihood problem that is "dual" to a correspondingmaximum entropy problem.
Assuming robust esti-mates for the A parameters, the resulting model isessentially guaranteed to be superior to the trigrammodel.For a concrete example, if si-~-VLADIMIR andti =GENNADY, then fi = 1 if and only if VLADIMIRappeared in the past N words and the current wordw is GENNADY.
Consulting Table 1, we see that inthe BN corpus, the presence of VLADIMIR will boostthe probability of GENNADY by a factor of 19.6 forthe next N = 500 words.3.3 Language mode l  "relevance" featuresA long-range language model such as that describedin Section 3.2 uses selected words from the past ten,twenty or more sentences to inform its decision onthe possible identity of the next word.
This is likelyto help if all of these sentences are in the same docu-ment as the current word, for in that case the modelhas presumably begun to adapt to the idiosyncra-cies of the current document.
In the case of the trig-ger model described above, the cache will be filledwith "relevant" words.
In this setting, one would ex-pect a long-range model to outperform a trigram (orother short-range) model, which doesn't avail itselfof long-range information.On the other hand, if the present document hasjust recently begun, the long-range model is wronglyconditioning its decision on information from adifferent--and presumably unrelated--document.
Asoap commercial, for instance, doesn't benefit along-range model in assigning probabilities to thewords in the news segment following the commercial.Often a long-range model will actually be misled bysuch irrelevant context; in this case, the myopia ofthe trigram model is actually helpful.By monitoring the long- and short-range mod-els, one might be more inclined towards a parti-tion when the long-range model suddenly shows adip in performance--a lower assigned probability tothe observed words--compared to the short-rangemodel.
Conversely, when the long-range model isconsistently assigning higher probabilities to the ob-served words, a partition is less likely.This motivates a quantitative measure of "rele-vance," which we define as the logarithm of the ratioof the probability the exponential model assigns tothe next word (or sentence) to that assigned by theshort-range trigram model:a(H,w)=-log( Pexp(wlH)~kPtri(W I W-2W-1) J  "When the exponential model outperforms the tri-gram model, R > 0.38If we observe the behavior of R as a function ofthe position of the word within a segment, we findthat on average R slowly increases from below zeroto well above zero.
Figure 1 gives a striking graphi-cal illustration of this phenomenon.
The figure plotsthe average value of R as a function of relative po-sition in the segment, with position zero indicatingthe beginning of a segment.
This plot shows thatwhen a segment boundary is crossed the predictionsof the adaptive model undergo a dramatic and sud-den degradation, and then steadily become more ac-curate as relevant content words for the new segmentare encountered and added to the cache.
(The fewvery high points to the left of a segment boundaryare primarily a consequence of the word CNN--whichis a trigger word and often appears at the beginningand end of a broadcast news segment.
)This observed behavior is consistent with our ear-lier intuition: the cache of the long-range model isdestructive arly in a document, when the new con-tent words bear little in common with the contentwords from the previous article.
Gradually, as thecache fills with words drawn from the current article,the long-range model gains steam and R improves.While Figure 1 shows that this behavior is very pro-nounced as a "law of large numbers," our feature in-duction results indicate that relevance is also a verygood predictor of boundaries for individual events.In the experiments we report in this paper, we as-sume that sentence boundaries are provided in theannotation, and so the questions we ask are actu-ally about the relevance score assigned to entire sen-tences normalized by sentence length, a geometricmean of language model ratios.3.4 Vocabu lary  featuresIn addition to the estimate of "topicality" that rele-vance features provide, we included features pertain-ing to the identity of words before and after potentialsegment boundaries as candidates in our exponentialmodel.
The set of candidate word-based features weuse are simple questions of the form?
Does the word appear up to 1 sentence in thefuture?2 sentences?
3?
5??
Does the word appear up to 1 sentence in the past?sentences ?
3?
5??
Does the word appear up to 5 sentences in the pastbut not 5 sentences in the future??
Does the word appear up to 5 sentences in the futurebut not 5 sentences in the past??
Does the word appear up to 1 word in the future?
5words ??
Does the word appear up to 1 word in the past?
5words ??
Does the word begin the preceding sentence?0.3 I ?
?0.25 r0,, ?0.05I "I I I I I~ -400 -~ ~ 800 1000Figure 1: Near the beginning of a segment, an adap-tive, long-range language model is on average l ss ac-curate than a static trigram model.
The figure plotsthe average value of the logarithm of the ratio of theadaptive language model to the static trigram modelas a function of relative position in the segment, withposition zero indicating the beginning of a segment.The statistics were collected over the roughly sevenmillion words of mixed broadcast news and Reutersdata comprising the TDT corpus (see Section 5).4 Feature  Induct ionTo cast the problem of determining segment bound-aries in statistical terms, we set as our goal the con-struction of a probability distribution q(b i w), whereb E {YES, NO} is a random variable describing thepresence of a segment boundary in context w. Weconsider distributions in the linear exponential fam-ily Q( f  , qo) given by { 1 }Q(f ,  qo)-- q(b lo J ) -  Zx~w)e x't('?)
q0(blw)where q0(blw ) is a prior or default distribution onthe presence of a boundary, and A- f(w) is a linearcombination of binary features f i (w) E {0, 1} withreal-valued feature parameters )ti:)t. f(w) = )tlfl(w) + )t2f2 (w) -I-.. ")tnfn(w) ?The normalization constantsZx(w) = 1 + e x'f(?~)insure that this is indeed a family of conditionalprobability distributions.
(This family of models isclosely related to the class of sigmoidal belief net-works (Neal, 1992).
)Our judgment of the merit of a model q E Q(f ,  qo)relative to a reference distribution p ~ Q(f ,  qo) dur-ing training is made in terms of the Kullback-Leiblerdivergence,~ea be{YES,NO} qL?
I w) "39Thus, when p is chosen to be the empirical distribu-tion of a sample of training events { (w, b)}, we areusing the maximum likelihood criterion for modelselection.
Under certain mild regularity conditions,the maximum likelihood solutionq* = argmin D(pll q)qE ~(\],qo )exists and is unique.
To find this solution, weuse the iterative scaling algorithm presented in(Della Pietra, Della Pietra, and Lafferty, 1997).This explains how a model is chosen once we knowthe features f l , - .
.
,  fn, but how are these features tobe found?
The procedure that we follow is a greedyalgorithm akin to growing a decision tree.
Given aninitial distribution q and a set of candidate featuresC, we consider the one-parameter family of distribu-tions {q~,g}aeR = Q(g' q) for each g E C. The gainof the candidate feature g is defined to beCq(g) = argmaxa (D(~ II q) - D(~ II qc,,.f)) ?This is the improvement to the model that wouldresult from adding the feature g and adjusting itsweight to the best value.
After calculating the gainof each candidate feature, the one with the largestgain is chosen to be added to the model, and all ofthe model's parameters are then adjusted using iter-ative scaling.
In this manner, an exponential modelis incrementally built up using the most informativefeatures.Having concluded our discussion of our overall ap-proach, we present in Figure 2 a schematic view ofthe steps involved in building a segmenter using thisapproach.D~aTrldniag~Ttaiaing~Tr~g~I p(w I w aw.
i  ) ~w I H)1 !
lI'  IFigure 2: Data flow in training the exponential seg-mentation model5 Feature Induct ion in Ac t ionThis section provides a peek at the construction ofsegmenters for two different domains.
Inspecting the40sequence of features elected by the induction algo-rithm reveals much about feature induction in gen-eral, and how it applies to the segmenting task inparticular.
We emphasize that the process of fea-ture selection is completely automatic once the setof candidate features has been selected.The first segmenter was built on the WSJ  cor-pus.
The second was built on the Topic Detectionand Tracking Corpus (Allan, to appear).
The TDTcorpus is a mixed collection of newswire articles andbroadcast news transcripts adapted from text cor-pora previously released by the Linguistic Data Con-sortium; in particular, portions of data were ex-tracted from the 1995 and 1996 Language Modeltext collections published by the LDC in support ofthe DARPA Continuous Speech Recognition project.The extracts used for TDT include material fromthe Reuters newswire service, and from the PrimarySource Media CD-ROM publications of transcriptsfor news programs that appeared on the ABC, CNN,NPR and PBS broadcast networks; the size of thecorpus is roughly 7.5 million words.
The TDT cor-pus was constructed as part of a DARPA-sponsoredproject intended to study methods for detecting newtopics or events and tracking their reappearance andevolution over time.5.1 WSJ  featuresFor the WSJ  experiments, which we describe first,a total of 300,000 candidate features were availableto the induction program.
Though the trigram priorwas trained on 38 million words, the trigger param-eters were only trained on a one million word subsetof this data.Figure 3 shows the first several features that wereselected by the feature induction algorithm.
Thisshows the word or relevance score for each featuretogether with the value of e x for the feature af-ter iterative scaling is complete for the final model.The ~-- -~ figures indicate features that are ac-tive over a range of sentences.
Thus, the symbolMR.
+1 I, 0.07 ,t represents he feature "Does the wordMR.
appear in the next sentence?"
which, if true,contributes a factor of e x = 0.07 to the exponen-tial model.
Similarly, the ~ ~ figures representfeatures that are active over a range of words.
ForHE +5 example, the figure ?
0.08 ?
represents thequestion "Does the word HE appear in the next fivewords?"
which is assigned a weight of 0.08.
Thesymbol \]5 -~ SAm +5 SAID :'.= 2 7 ,I stands for afeature which asks "Does the" word SAID appear inthe previous five sentences but not in the next fivesentences?"
and contributes a factor of 2.7 if theanswer is "yes.
"Most of the features in Figure 3 make a good deal-4 -tSAIDCURRENTPOSITION +1 +2 +'3 +'4 +'5INCORPORATED~-" 4.5-0.50 < R~..~05.3CORPORATION" 31.6 "SAYS" 0.39 "MR.~- 0.07 "-~CLOSED~-" 27.6 "~SEE"94.8SAID~- 2.9 -'~FEDERAL~-" 6.8SAID" 2.7THE?
0.36POINT~- 4.5,, , Ri _ ~ 0 ~,4.5NAMED~'- 14.2 --~HE?
0.082~.< Ri < 0.056.1MAY2.0 -~ALSO~-- 0.07 "-~Figure 3: First several features induced for the WSJcorpus, presented in order of selection, with e x fac-tors underneath.
The length of the bars indicateactive range of the feature, in words or sentences,relative to the current word.of sense.
The first selected feature, for instance, is astrong hint that an article may have just begun; ar-ticles in the WSJ  corpus often concern companies,and typically the full name of the company (ACMEINCORPORATED, for instance) only appears once atthe beginning of the article, and subsequently in ab-breviated form (ACME).
Thus the appearance ofINCORPORATED is a strong indication that a newarticle may have recently begun.The second feature uses the relevance statistic t.1 For the WSJ  experiments,  we modified the languagemodel relevance stat ist ic by adding a weight to each wordposit ion depending only on its tr igram history w-2,  w-1.A l though our results require fur ther  analysis, we do notbelieve that  this makes a significant difference in the fea-If the trigger model performs poorly relative to thetrigram model in the following sentence, this feature(roughly speaking) boosts the probability of a seg-ment at this location by a factor of 5.3.The fifth feature concerns the presence of the wordMR.
In hindsight, we can explain this feature bynoting that in WSJ  data the style is to introduce aperson in the beginning of an article by writing, forexample, WILE E.  COYOTE, PRESIDENT OF ACMEINCORPORATED... and then later in the article us-ing a shortened form of the name: MR. COYOTECITED A LACK OF EXPLOSIVES...
Thus, the pres-ence of MR. in the following sentence discounts theprobability of an article boundary by 0.07, a factorof roughly 14.The sixth feature which boosts the probabilityof a segment if the previous sentence contained theword CLOSED--is another artifact of the WSJ  do-main, where articles often end with a statement ofa company's performance on the stock market dur-ing the day of the story of interest.
Similarly, theend of an article is often made with an invitation tovisit a related story; hence a sentence beginning withSEE boosts the probability of a segment boundaryby a large factor of 94.8.
Since a personal pronountypically requires an antecedent, he presence of HEamong the first words is a sign that the current posi-tion is not near an article boundary, and this featuretherefore has a discounting factor of 0.082.5.2 TDT featuresFor the TDT experiments, a larger vocabulary androughly 800,000 candidate features were available tothe induction program.
Though the trigram priorwas trained on approximately 150 million words, thetrigger parameters were trained on a 10 million wordsubset of the BN corpus.Figure 4 reveals the first several features chosenby the induction algorithm.
The letter c. appearsamong several of the first features.
This is becauseof the fact that the data is tokenized for speech pro-cessing (whence c. N. N. rather than CNN), andthe network identification information is often givenat the end and beginning of news segments (c.
N.N.
'S RICHARD BLYSTONE IS HERE TO TELL US...).The first feature asks if the letter c. appears in theprevious five words; if so, the probability of a seg-ment boundary is boosted by a factor of 9.0.
Thepersonal pronoun I appears as the second feature; ifthis word appears in the following three sentencesthen the probability of a segment boundary is dis-counted.The language model relevance statistic appearsfor the first time in the sixth feature.
The wordturps chosen by the algorithm, or the quantitative per-formance of the resulting segmenter.41-~ -,4 -~ -7 -~tC.?
9 .0CURRENTPOSIY~ION +1 +2 +3 +'4 +'5I'" 0.61 "FRIDAY""- 9.93 -'~JOINS" 2.26 "A"-" 3.10--0.1 < R~ < 03.4J.7.76HAITI~- 5.95 "~C.~-- 7.91 "~L< R~ < 0.0_51.85C.
'~' 2.06 *'IT ~SJ '4 I i  I0.55IN~-- 1.7 --~is~-  0.59 --~Z.0.10FROM2.22HERE'" 0.45CLINTON~'- 3.04 "-~HE" 0.12C.?
0.29AGENCY" 3.88 "'Figure 4: First several features induced for the TDTcorpus, presented in order of selection, with e ~ fac-tors underneath.J.
that the seventh and fifteenth features ask aboutcan be attributed to the large number of news sto-ries in the data having to do with the O.J.
Simp-son trial.
The nineteenth feature asks if the termFROM appears among the previous five words, andif the answer is "yes" raises the probability of asegment boundary by more than a factor of two.This feature makes sense in light of the "sign-off"conventions that news reporters and anchors follow(THIS IS WOLF  BLITZER REPORTING LIVE FROMTHE WHITE HOUSE).
Similar explanations of many42of the remaining features are easy to guess from aperusal of Figure 4.6 A Probab i l i s t i c  Er ror  Met r i cPrecision and recall statistics are commonly usedin natural language processing and information re-trieval to assess the quality of algorithms.
For thesegmentation task they might be used to gauge howfrequently boundaries actually occur when they arehypothesized and vice versa.
Although they havesnuck into the literature in this disguise, we believethey are unwelcome guests.A useful error metric should somehow correlatewith the utility of the instrumented procedure in areM application.
In almost any conceivable appli-cation, a segmenting tool that consistently comesclose--off by a sentence, say-- is  preferable to onethat places boundaries willy-nilly.
Yet an algorithmthat places a boundary a sentence away from theactual boundary every time actually receives worseprecision and recall scores than an algorithm thathypothesizes a boundary at every position.
It isnatural to expect that in a segmenter, close shouldcount for something.A useful metric should Mso be robust with respectto the scale (words, sentences, paragraphs, for in-stance) at which boundaries are determined.
How-ever, precision and recall are scale-dependent quan-tities.
(Reynar, 1994) uses an error window thatredefines "correct" to mean hypothesized withinsome constant window of units away from a refer-ence boundary, but this approach still suffers fromoverdiscretizing error, drawing all-or-nothing linesinsensitive to gradations of correctness.Finally, for many purposes it is useful to havea metric that is a single number.
A commonlycited flaw of the precision/recall figures is their com-plementary nature: hypothesizing more boundariesraises precision at the expense of recall, allowing analgorithm designer to tweak parameters to trade pre-cision for recall.
One proposed work-around is toemploy dynamic time warping to come up with anexplicit alignment between the segments proposedby the algorithm and the reference segments, andthen to combine insertion, deletion, and substitutionerrors into an overall penalty.
This error metric, incommon use in speech recognition, can be achievedby a similar Viterbi search.
A string edit distancesuch as this is useful and reasonable for applicationslike speech or spelling correction partly because itmeasures how much work a user would have to do tocorrect the output of the machine.
For many of theapplications we envision for segmentation, however,the user will not correct the output but will ratherbrowse the returned text to extract information.Our proposed metric satisfies the listed desiderata.It formalizes in a probabilistic manner the effect ofdocument co-occurrence on goodness, in which it isdeemed esirable for related units of information toappear in the same document and unrelated units toappear in separate documents.6.1 The  new metr i cSegmentation, whether at the word or sentence level,is about identifying boundaries between successiveunits of information in a text corpus.
Two suchunits are either related or unrelated by the intentof the document author.
A natural way to reasonabout developing a segmentation algorithm is there-fore to optimize the likelihood that two such unitsare correctly labeled as being related or being unre-lated.
Our error metric P~, is simply the probabilitythat two sentences drawn randomly from the corpusare correctly identified as belonging to the same doc-ument or not belonging to the same document.
Moreformally, given two segmentations re f  and hyp for acorpus n sentences long,P, ( ref ,hyp)  = ~ D~( i , j )  Sref( i , j )  ~$hyp( i , j )l<i<j<nHere ~ref is an indicator function which is 1 if thetwo corpus indices specified by its parameters belongin the same document, and 0 otherwise; similarly,~hyp is 1 if the two indices are hypothesized to be-long in the same document, and 0 otherwise.
Theoperator is the XNOR function ("both or neither")on its two operands.
The function D,  is a distanceprobability distribution over the set of possible dis-tances between sentences chosen randomly from thecorpus, and will in general depend on certain pa-rameters # such as the average spacing between sen-tences.
If D~ is uniform over the length of the text,then the metric represents the probability that anytwo sentences drawn from the corpus are correctlyidentified as being in the same document or not.Consider the implications of this for informationretrieval.
Suppose there is precisely one sentencein a target corpus that satisfies our information de-mands.
For some applications it may be sufficientfor the system to return only that sentence, but ingeneral we desire that it return as many sentencesdirectly related to the target sentence as possible,without returning too many unrelated sentences.
Ifwe assume "related" to mean "contained in the samedocument", then our error metric judges algorithmsbased on how often this happens.In practice letting D~, be the uniform distribu-tion is unreasonable, since for large corpora mostrandomly drawn pairs of sentences are in differentdocuments and are correctly identified as such byeven the most naive algorithms.
We instead adopta distribution that focuses on small distances.
Inparticular, we choose D~ to be an exponential dis-tribution with mean l /p ,  a parameter that we fixat the approximate mean document length for thedomain:Dt~(i, J) = 7t~ e-~l i - j l  .In the above, 7t, is a normalization chosen so thatD~, is a probability distribution over the range ofdistances it can accept.There are several sanity checks that validate theuse of our metric.
The measure is a probability andtherefore a real number between 0 and 1.
We ex-pect 1 to represent perfection; indeed, an algorithmscores 1 with respect to some data if and only if itpredicts its segmentation exactly.
It captures thenotion of nearness in a principled way, gently penal-izing algorithms that hypothesize boundaries thataren't quite right, and scaling down with the algo-r ithm's degradation.
Furthermore, it is not possibleto "cheat" and obtain a high score with this met-ric: spurious behavior such as never hypothesizingboundaries and hypothesizing nothing but bound-aries are penalized.
We refer to Section 7 for sampleresults on how these trivial algorithms core.One weakness of the metric as we have presentedit here is that there is no principled way of specify-ing the distance distribution Du.
We plan to give amore detailed analysis of this problem and presenta method for choosing the parameters ~ in a futurepaper.7 Exper imenta l  Resu l ts7.1 Quant i ta t ive  resu l t sAfter feature induction was carried out (as de-scribed in Section 5), a simple decision procedurewas used for actually placing boundaries: a segmentboundary was placed at each position for which themodel probability was above a fixed threshold or,with boundaries required to be separated by a mini-mum number of sentences e. The threshold and min-imum separation were determined on heldout datain order to maximize the probability P~, and turnedout to be a = 0.20 and e = 2 for the WSJ  model,and ot = 0.14 and e = 5 for the TDT models.The quantiative results for the WSJ  and TDTmodels are collected in Tables 5 and 6 respectively.For the WSJ  model, the probabilistic metric P~, was0.83 when evaluated on 325K words of test data,and the precision and recall for exact matches ofboundaries were 56% and 54%, for an F-measureof 55.
As a simple baseline we compared this per-formance to that obtained by four simple defaultmethods for assigning boundaries: choosing bound-aries randomly, assigning every possible boundary,43modelfeature  induct ionrandomallreference hypoth.segments segments P~ precision757 792 83% 56%757 757 67% 17%757 13540 53% 5%none 757 0 52% 0%even 757 753 68% 17%recall F.measure54% 5516% 17100% 100%17% 17Table 5: Quantitative results for WSJ  segmentation.
The WSJ  model was trained on 325K words of data,and tested on a similarly sized portion of unseen text.
The top 70 features were selected.
The mean segmentlength in the training and test data was 1/p = 18 sentences.
As a basis of comparison, the figures for severalbaseline models are given.
The figures in the random row were calculated by randomly generating a numberof segments equal to the number appearing in the test data.
The all and none  rows include the figures formodels which hypothesize all possible segment boundaries and no boundaries, respectively.
The even  rowshows the results of simply hypothesizing a segment boundary every 18 sentences.reference hypoth.model segments segments P~ precisionfeature  induct ion(Model B) 9984 9543 88% 60%feature  i nduct ion(Model A) 9984 9449 82% 47%random 9984 9984 68% 12%all 9984 219,099 59% 5%none 9984 0 43% 0%even 9984 9980 74% 14%recall F-measure57% 5845% 4612% 12100% 90%12% 13Table 6: Quantitative results for TDT segmentation.
The TDT models were trained on 2M words andtested on 4.3M words of previously unseen TDT data.
Model A was trained on 2M words of broadcast newsdata from 1992-1993, not included in TDT corpus, and the top 100 features were selected.
Model B wastrained on the first 2M words of TDT corpus which is made up of a mix of CNN transcripts and Reutersnewswire, and again the top 100 features were selected.
The mean document length was 1/p = 25 sentences.assigning no boundaries, and deterministically plac-ing a segment boundary every 1/p sentences.
It isinstructive to compare the values of P ,  with preci-sion and recall for these default algorithms in orderto obtain some intuition for the new error metric.Two separate models were built to segment heTDT corpus.
The first, which we shall refer to sim-ply as Model A, was trained using two million wordsfrom the BN corpus from the 1992-1993 time pe-riod.
This data contains CNN transcripts, but noReuters newswire data.
Model B was trained on thefirst two million words of the TDT corpus.
Bothmodels were tested on the last 4.3 million words ofthe TDT corpus.
We expect Model A to be infe-rior to Model B for two reasons: the lack of Reutersdata in it's training set and the difference of betweenone and two years in the dates of the stories in the44training and test sets.
The difference is quantifiiedin Table 6, which shows that P~, = 0.82 for Model Awhile P ,  = 0.88 for Model B.7.2 Qua l i ta t ive  resu l tsWe now present graphical examples of the segmen-tation algorithm at work on previously unseen testdata.
Figure 7 shows the performance of the WSJsegmenter on a typical collection of test data, inblocks of 300 contiguous entences.
In these figuresthe reference segmentation is shown below the hori-zontal line as a vertical ine at the position betweensentences where the article boundary occurred.
Thedecision made by the automatic segmenter is shownas a verticle line above the horzontal ine at theappropriate position.
The fluctuating curve is theprobability assigned by the exponential model con-h l J  L ~ .
- ^ .
A^ , -I. i: !
!I.~  .
J^Figure 7: Typical segmentations of WSJ  test data.The lower verticle lines indicate reference segmenta-tions ("truth").
The upper verticle lines are bound-aries placed by the algorithm.
The fluctuating curveis the probability of a segment boundary accordingto the exponential model after 70 features were in-duced.structed using feature induction.
Notice that inthis domain many of the segments are quite short,adding special difficulties for the segmentation prob-lem.
Figure 8 shows the performance of the TDTsegmenter (Model B) on five randomly chosen blocksof 200 sentences from the TDT test data.We hasten to add that these results were obtainedFigure 8: Randomly chosen segmentations of TDTtest data, in 200 sentence blocks, using Model B.with no smoothing or pruning of any kind, and withno more than 100 features induced from the candi-date set of several hundred thousand.
Unlike manyother machine learning methods, feature inductionfor exponential models is quite robust to overfittingsince the features act in concert to assign probabil-ity to events rather than splitting the event spaceand assigning probability using relative counts.
Weexpect that significantly better results can be ob-tained by simply training on much more data, andby allowing a more sophisticated set of features.458 ConclusionsWe have presented and evaluated a new statisticalmodel for segmenting unpartitioned text into coher-ent fragments.
We leverage long- and short-rangelanguage models, as well as automatic feature induc-tion techniques, in the design of this model.
In thiswork we rely exclusively on simple lexical features,including a topicality measure called relevance anda number of vocabulary features that are inducedfrom a large space of candidate features.We have proposed a new probabilistically moti-vated error metric for the assessment of segmenta-tion algorithms.
Qualitative assessment as well asthe evaluation of our algorithm with this new metricdemonstrates its effectiveness in two very differentdomains, Wall Street Journal articles and broadcastnews transcripts.Our immediate application of this model will be tothe video-on-demand application called Informedia(Christel et al, 1995).
We intend to mix simple au-dio and video features uch as statistics from pauses,black frames, and color histograms with our lexicalfeatures in order to segment news broadcasts intocomponent stories.
Other applications that we havenot explored in this paper include automatic infer-ence of subtopic structure for information retrieval,document summarization, and improved languagemodeling.AcknowledgementsWe thank Michael Witbrock and Alex Hauptmannfor discussions on the segmentation problem withinthe context of the Inforrnedia project.
We also thankJalme Carbonell and Yiming Yang for their input,and for encouraging us to build segmentation modelson the TDT corpus.
Participants in the TDT pilotstudy, including James Allan, Rich Schwartz, JonYamron, and especially George Doddington, pro-vided invaluable feedback on the probabilistic eval-uation metric.ReferencesAllan, J.
To appear.
Topic Detection and TrackingCorpus, Linguistic Data Consortium, Universityof Pennsylvania.Beeferman, D., A. Berger, and J. Lafferty.
1997.A model of lexical attraction and repulsion.
InProceedings of the 35th Annual Meeting of theACL, Madrid, Spain.Berger, A., S. Della Pietra, and V. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39-71.46Christel, M., T. Kanade, M. Mauldin, It.
Iteddy,M.
Sirbu, S. Stevens, and H. Wactlar.
1995.
In-formedia digital video library.
Communications ofthe ACM, 38(4):57-58.Della Pietra, S., V. Della Pietra, and J. Lafferty.1997.
Inducing features of random fields.
IEEETrans.
on Pattern Analysis and Machine Intelli-gence, 19(4):380-393, April.Hearst, M.A.
1994.
Multi-paragraph segmentationof expository text.
In Proceedings of the 32ndAnnual Meeting of the ACL, Las Cruces, NM.Jelinek, F., B. Merialdo, S. Roukos, and M. Strauss.1991.
A dynamic language model for speech recog-nition.
In Proceedings of the DARPA Speech andNatural Language Workshop, pp.
293-295, Febru-ary.Katz, S. 1987.
Estimation of probabilities fromsparse data for the langauge model componentof a speech recognizer.
IEEE Transactions onAcoustics, Speech and Signal Processing, ASSP-35(3):400-401, March.Kozima, H. 1993.
Text segmentation based on sim-ilarity between words, in Proceedings of the 31stAnnual Meeting of the ACL, Columbus, OH, pp.286-288.Kozima, H. and T. Furugori.
1994.
Segmenting nar-rative text into coherent scenes.
Literary and Lin-guistic Computing, 9:13-19.Kuhn, It.
and R. de Mori.
1990.
A cache-based nat-ural language model for speech recognition.
IEEETrans.
on Pattern Analysis and Machine Intelli-gence, 12:570-583.Lau, R., It.
Rosenfeld, and S. Roukos.
1993.
Adap-tive language modeling using the maximum en-tropy principle.
In Proceedings of the ARPA Hu-man Language Technology Workshop, pages 108-113.
Morgan Kaufman Publishers.Litman, D. J. and R. J. Passonneau.
1995.
Com-bining multiple knowledge sources for discoursesegmentation.
In Proceedings of the 33rd AnnualMeeting of the ACL, Cambridge, MA.Neal, R. 1992.
Connectionist learning of belief net-works.
Artificial Intelligence, 56:71-113.Reynar, J. C. 1994.
In Proceedings of the 32ndAnnual Meeting of the ACL, student session, LasCruces, NM.Youmans, G. 1991.
A new tool for discourse anal-ysis: The vocabulary-management profile.
Lan-guage, 67:763-789.
