Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 315?324,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsAutomatic Discovery of Manner Relations and its ApplicationsEduardo Blanco and Dan MoldovanHuman Language Technology Research InstituteThe University of Texas at DallasRichardson, TX 75080 USA{eduardo,moldovan}@hlt.utdallas.eduAbstractThis paper presents a method for the auto-matic discovery of MANNER relations fromtext.
An extended definition of MANNER isproposed, including restrictions on the sortsof concepts that can be part of its domain andrange.
The connections with other relationsand the lexico-syntactic patterns that encodeMANNER are analyzed.
A new feature set spe-cialized on MANNER detection is depicted andjustified.
Experimental results show improve-ment over previous attempts to extract MAN-NER.
Combinations of MANNER with othersemantic relations are also discussed.1 IntroductionExtracting semantic relations from text is an impor-tant step towards understanding the meaning of text.Many applications that use no semantics, or onlyshallow semantics, could benefit by having availablemore text semantics.
Recently, there is a growing in-terest in text semantics (Ma`rquez et al, 2008; Davi-dov and Rappoport, 2008).An important semantic relation for many appli-cations is the MANNER relation.
Broadly speaking,MANNER encodes the mode, style, way or fashionin which something is done or happened.
For ex-ample, quick delivery encodes a MANNER relation,since quick is the manner in which the delivery hap-pened.An application of MANNER detection is QuestionAnswering, where many how questions refer to thisparticular relation.
Consider for example the ques-tion How did the President communicate his mes-sage?, and the text Through his spokesman, Obamasent a strong message [.
.
.
].
To answer such ques-tions, it is useful to identify first the MANNER rela-tions in text.MANNER occurs frequently in text and it isexpressed by a wide variety of lexico-syntacticpatterns.
For example, PropBank annotates8,037 ARGM-MNR relations (10.7%) out of 74,980adjunct-like arguments (ARGMs).
There are verbsthat state a particular way of doing something, e.g.,to limp implicitly states a particular way of walk-ing.
Adverbial phrases and prepositional phrasesare the most productive patterns, e.g., The nation?sindustrial sector is now growing very slowly if atall and He started the company on his own.
Con-sider the following example: The company saidMr.
Stronach will personally direct the restructur-ing assisted by Manfred Gingl, [.
.
.
]1.
There aretwo MANNER relations in this sentence: the under-lined chunks of text encode the way in which Mr.Stronach will direct the restructuring.2 Previous WorkThe extraction of semantic relations in general hascaught the attention of several researchers.
Ap-proaches to detect semantic relations usually focuson particular lexical and syntactic patterns.
Thereare both unsupervised (Davidov et al, 2007; Turney,2006) and supervised approaches.
The SemEval-2007 Task 04 (Girju et al, 2007) aimed at relationsbetween nominals.
Work has been done on detect-ing relations within noun phrases (Nulty, 2007),1Penn TreeBank, file wsj 0027, sentence 10.315named entities (Hirano et al, 2007), clauses (Sz-pakowicz et al, 1995) and syntax-based comma res-olution (Srikumar et al, 2008).
There have beenproposals to detect a particular relation, e.g., CAUSE(Chang and Choi, 2006), INTENT (Tatu, 2005),PART-WHOLE (Girju et al, 2006) and IS-A (Hearst,1992).MANNER is a frequent relation, but besides the-oretical studies there is not much work on detect-ing it.
Girju et al (2003) propose a set of fea-tures to extract MANNER exclusively from adverbialphrases and report a precision of 64.44% and re-call of 68.67%.
MANNER is a semantic role, and allthe works on the extraction of roles (Gildea and Ju-rafsky, 2002; Giuglea and Moschitti, 2006) extractsMANNER as well.
However, these approaches treatMANNER as any other role and do not use any spe-cific features for its detection.
As we show in thispaper, MANNER has its own unique characteristicsand identifying them improves the extraction accu-racy.
The two most used semantic role annotationresources, FrameNet (Baker et al, 1998) and Prop-Bank (Palmer et al, 2005), include MANNER.The main contributions of this paper are: (1) em-pirical study of MANNER and its semantics;(2) analysis of the differences with other relations;(3) lexico-syntactic patterns expressing MANNER;(4) a set of features specialized on the detection ofMANNER; and (5) the way MANNER combines withother semantic relations.3 The Semantics of MANNER RelationTraditionally, a semantic relation is defined by stat-ing the kind of connection linking two concepts.For example, MANNER is loosely defined by thePropBank annotation guidelines2 as manner adverbsspecify how an action is performed [.
.
. ]
mannershould be used when an adverb be an answer toa question starting with ?how??.
We find this kindof definition weak and prone to confusion (Section3.2).
Nonetheless, to the best of our knowledge,semantic relations have been mostly defined statingonly a vague definition.Following (Helbig, 2005), we propose an ex-tended definition for semantic relations, includ-2http://verbs.colorado.edu/?mpalmer/projects/ace/PBguidelines.pdf, page 26.ing semantic restrictions for its domain and range.These restrictions help deciding which relationholds between a given pair of concepts.
A relationshall not hold between two concepts unless they be-long to its domain and range.
These restrictions arebased on theoretical and empirical grounds.3.1 MANNER DefinitionFormally, MANNER is represented as MNR(x, y),and it should be read x is the manner in whichy happened.
In addition, DOMAIN(MNR) andRANGE(MNR) are the sets of sorts of concepts thatcan be part of the first and second argument.RANGE(MNR), namely y, is restricted to situa-tions, which are defined as anything that happensat a time and place.
Situations include events andstates and can be expressed by verbs or nouns, e.g.,conference, race, mix and grow.
DOMAIN(MNR),namely x, is restricted to qualities (ql), non temporalabstract objects (ntao) and states (st).
Qualities rep-resent characteristics that can be assigned to otherconcepts, such as slowly and abruptly.
Non tempo-ral abstract objects are intangible entities.
Theyare somehow product of human reasoning and arenot palpable.
They do not encode periods or pointsof time, such as week, or yesterday.
For example,odor, disease, and mile are ntao; book and car arenot because they are tangible.
Unlike events, statesare situations that do not imply a change in the con-cepts involved.
For example, standing there or hold-ing hands are states; whereas walking to the parkand pinching him are events.
For more details aboutthese semantic classes, refer to (Helbig, 2005).These semantic restrictions on MANNER come af-ter studying previous definitions and manual exami-nation of hundreds of examples.
Their use and ben-efits are described in Section 4.3.2 MANNER and Other Semantic RelationsMANNER is close in meaning with several other rela-tions, specifically INSTRUMENT, AT-LOCATION andAT-TIME.Asking how does not identify MANNER in manycases.
For example, given John broke the window[with a hammer], the question how did John breakthe window?
can be answered by with the hammer,and yet the hammer is not the MANNER but the IN-STRUMENT of the broke event.
Other relations that316may be confused as MANNER include AT-LOCATIONand AT-TIME, like in [The dog jumped]x [over thefence]y and [John used to go]x [regularly]y.A way of solving this ambiguity is by prioritiz-ing the semantic relations among the possible can-didates for a given pair of concepts.
For exam-ple, if both INSTRUMENT and MANNER are possi-ble, the former is extracted.
In a similar fashion, AT-LOCATION and AT-TIME could have higher prioritythan MANNER.
This idea has one big disadvantage:the correct detection of MANNER relies on the detec-tion of several other relations, a problem which hasproven difficult and thus would unnecessarily intro-duce errors.Using the proposed extended definition one maydiscard the false MANNER relations above.
Hammeris not a quality, non temporal abstract object or state(hammers are palpable objects), so by definition arelation of the form MNR(the hammer, y) shall nothold.
Similarly, fence and week do not fulfill thedomain restriction, so MNR(over the fence, y) andMNR(every other week, y) are not valid either.MANNER also relates to CAUSE.
Again, ask-ing how?
does not resolve the ambiguity.
GivenThe legislation itself noted that it [was introduced]x?by request,?
[.
.
. ]
(wsj 0041, 47), we believethe underlined PP indicates the CAUSE and not theMANNER of x because the introduction of the leg-islation is the effect of the request.
Using the ex-tended definition, since request is an event (it im-plies a change), MNR(by request, y) is discardedbased on the domain and range restrictions.4 Argument ExtractionIn order to implement domain and range restrictions,one needs to map words to the four proposed se-mantic classes: situations (si), states (st), qualities(ql) and non temporal abstract objects (ntao).
Theseclasses are the ones involved in MNR; work has beendone to define in a similar way more relations, butwe do not report on that in this paper.First, the head word of a potential argument isidentified.
Then, the head is mapped into a seman-tic class using three sources of information: POStags, WordNet hypernyms and named entity (NE)types.
Table 1 presents the rules that define the map-ping.
We obtained them following a data-driven ap-proach using a subset of MANNER annotation fromPropBank and FrameNet.
Intermediate classes aredefined to facilitate legibility; intermediate classesending in -NE only involve named entity types.Words are automatically POS tagged using amodified Brill tagger.
We do not perform word sensedisambiguation because in our experiments it did notbring any improvement; all senses are consideredfor each word.
isHypo(x) for a given word w in-dicates if any of the senses of w is a hyponym of xin WordNet 2.0.
An in-house NE recognizer is usedto assign NE types.
It detects 90 types organizedin a hierarchy with an accuracy of 92% and it hasbeen used in a state-of-the-art Question Answeringsystem (Moldovan et al, 2007).
As far as the map-ping is concerned, only the following NE types areused: human, organization, country, town, province,other-loc, money, date and time.
The mapping alsouses an automatically built list of verbs and nounsthat encode events (verb events and noun events).The procedure to map words into semanticclasses has been evaluated on a subset of PropBankwhich was not used to define the mapping.
First,we selected 1,091 sentences which contained a totalof 171 MANNER relations.
We syntactically parsedthe sentences using Charniak?s parser and then per-formed argument detection by matching the trees tothe syntactic patterns depicted in Section 5.
52,612arguments pairs were detected as potential MAN-NER.
Because of parsing errors, 146 (85.4%) of the171 MANNER relations are in this set.After mapping and enforcing domain and rangeconstraints, the argument pairs were reduced to11,724 (22.3%).
The filtered subset includes 140(81.8%) of the 171 MANNER relations.
The filteringdoes make mistakes, but the massive pruning mainlyfilters out potential relations that do not hold: it fil-ters 77.7% of argument pairs and it only misclassi-fies 6 pairs.5 Lexico-Syntactic Patterns ExpressingMANNERMANNER is expressed by a wide variety of lexico-syntactic patterns, implicitly or explicitly.Table 2 shows the syntactic distribution of MAN-NER relation in PropBank.
We only consider rela-tions between a single node in the syntactic tree and317Class Rulesituation state || eventstate POStag=verb || isHypo(state.n.4)event (POStag=verb && in(verb events)) || (POStag=noun &&!animate object && (isHypo(phenomenon.n.1) || isHypo(event.n.1)|| in(noun events)))animate object livingNE || (POStag=noun && ((isHypo(entity.n.1) &&!isHypo(thing.n.9) && !isHypo(anticipation.n.4)) ||isHypo(social group.n.1)))livingNE neType=(human | organization | country | town | province |other-loc)quality POStag=(adverb | gerund) || headPP=(with | without)non temporal abstract object abstract object && !temporalabstract object neType=money || isHypo(thing.n.9) || (!isHypo(social group.n.1)&& (isHypo(abstraction.n.6 | psychological feature.n.1 |possession.n.2 | event.n.1 | state.n.4 | group.n.1 | act.n.2)))temporal TemporalNE || isHypo(time period.n.1) || isHypo(time.n.5)temporalNE ne-type=(date | time)Table 1: Mapping for the semantic classes used for defining DOMAIN(MNR) and RANGE(MNR)..Synt.#Occ.
%Occ.Examplepattern File, #sent SentenceADVP 3559 45.3% wsj 0039, 24 This story line might [resonate]y [more strongly]ADVP if Mr. Lanehad as strong a presence in front of the camera as he does behind it.PP 3499 44.6% wsj 2451, 0 NBC may yet find a way to [take]y a passive, minority interest in aprogram-maker [without violating the rules]PP.RB 286 3.6% wsj 0052, 3 Backe is [a [[closely]RB [held]y]ADJP media firm]NP run by formerCBS Inc. President Jon Backe.S 148 1.9% wsj 1217, 25 Salomon [posted]y an unexpectedly big gain in quarterly earnings,[aided by its securities trading and investments banking activities]S.NP 120 1.5% wsj 0100, 21 [.
.
. ]
he [graduated]y [Phi Beta Kappa]NP from the University ofKentucky at age 18, after spending only 2 1/2 years in college.Other 240 3.1% wsj 1337, 0 Tokyo stocks [closed]y [firmer]ADJP Monday, with the Nikkei indexmaking its fifth consecutive daily gain.Table 2: Syntactic patterns encoding MANNER in PropBank, number of occurrences, and examples.
A total of 7,852MANNER relations are encoded in PropBank between a single node in the syntactic tree and a verb.
In all examples,MNR(x, y) holds, where x is the text underlined.
Syntactic annotation comes straight from the Penn TreeBank.a verb; MANNER relations expressed by trace chainsidentifying coreference and split arguments are ig-nored.
This way, we consider 7,852 MANNER outof the total of the 8,037 PropBank annotates.
Be-cause ADVPs and PPs represent 90% of MANNERrelations, in this paper we focus exclusively on thesetwo phrases.For both ADVP and PP the most common directancestor is either a VP or S, although examples arefound that do not follow this rule.
Table 3 shows thenumber of occurrences for several parent nodes andexamples.
Only taking into account phrases whoseancestor is either a VP or S yields a coverage of 98%and thus those are the focus of this work.5.1 Ambiguities of MANNERBoth ADVPs and PPs are highly ambiguous whenthe task is to identify their semantics.
The PropBankauthors (Palmer et al, 2005) report discrepanciesbetween annotators mainly with AT-LOCATION andsimply no relation, i.e., when a phrase does not en-code a role at all.
In their annotation, 22.2% of AD-VPs encode MANNER (30.3% AT-TIME), whereasonly 4.6% of PPs starting with in and 6.1% start-318Parent #Occ.ExamplePhrase File, #sent SentenceVP3306 ADVP wsj 2341, 23 The company [was [officially]ADVP [merged]y with Bristol-MyersCo.
earlier this month]VP.3107 PP wsj 2320, 7 This is something P&G [would [do]y [with or without Kao]PP]VP,says Mr. Zurkuhlen.S215 ADVP wsj 0044, 6 [[Virtually word by word]ADVP, the notes [matched]y questions andanswers on the social-studies section of the test the student wastaking.
]S339 PP wsj 2454, 9 [[Under the laws of the land]PP, the ANC [remains]y an illegal or-ganization, and its headquarters are still in Lusaka, Zambia.
]SADJP17 ADVP wsj 1057, 85 [.
.
. ]
ABC touted ?Call to Glory,?
but the military drama was[[missing]y [in action]PP]ADJP within weeks.4 PP wsj 2431, 14 Two former ministers [were]y [[so heavily]ADVP implicated]ADJP inthe Koskotas affair that PASOK members of Parliament voted [.
.
.
]PP9 ADVP wsj 1249, 24 In Japan, by contrast, companies tend to develop their talent and[promote]y [from [within]PP]PP.9 PP wsj 1505, 30 London share prices were [influenced]y [[largely]ADVP by declineson Wall Street and weakness in the British pound]PP.Table 3: Examples of ADVPs and PPs encoding MANNER with different nodes as parents.
In all examples, MNR(x, y)holds, where x is the underlined phrase.
Syntactic annotation comes straight from the Penn TreeBank.ing with at encode MANNER.
The vast majority ofPPs encode either a AT-TIME or AT-LOCATION.MANNER relations expressed by ADVPs are eas-ier to detect since the adverb is a clear signal.
Ad-verbs ending in -ly are more likely to encode a MAN-NER.
Not surprisingly, the verb they attach to alsoplays an important role.
Section 6.2 depicts the fea-tures used.PPs are more complicated since the prepositionper se is not a signal of whether or not the phraseencodes a MANNER.
Even prepositions such as un-der and over can introduce a MANNER.
For ex-ample, A majority of an NIH-appointed panel rec-ommended late last year that the research con-tinue under carefully controlled conditions, [.
.
.
](wsj 0047, 9) and [.
.
. ]
bars where Japanese rev-elers sing over recorded music, [.
.
. ]
(wsj 0300, 3).Note that in both cases, the head of the NP containedin the PP encoding MANNER (conditions and music)belongs to ntao (Section 4).
Other prepositions, likewith and like are more likely to encode a MANNER,but again it is not guaranteed.6 ApproachWe propose a supervised learning approach, whereinstances are positive and negative MANNER exam-ples.
Due to their intrinsic difference, we build dif-ferent models for ADVPs and PPs.6.1 Building the CorpusThe corpus building procedure is as follows.
First,all ADVPs and PPs whose parent node is a VP orS and encode a MANNER according to PropBankare extracted, yielding 3559 and 3499 positive in-stances respectively.
Then, 10,000 examples of AD-VPs and another 10,000 of PPs from the Penn Tree-Bank not encoding a MANNER according to Prop-Bank are added.
These negative instances must haveas their parent node either VP or S as well and areselected randomly.The total number of instances, 13,559 for ADVPsand 13,499 for PPs, are then divided into training(60%), held-out (20%) and test (20%).
The held-outportion is used to tune the feature set and the finalresults provided are the ones obtained with the testportion, i.e., instances that have not been used in anyway to learn the models.
Because PropBank adds se-mantic role annotation on top of the Penn TreeBank,we have gold syntactic annotation for all instances.6.2 Selecting featuresSelected features are derived from previous workson detecting semantic roles, namely (Gildea andJurafsky, 2002) and the participating systems in319No.
Feature Values Explanation1 parent-node {S, VP} syntactic node of ADVP?s parent2 num-leaves N number of words in ADVP3 adverb {often, strongly, .
.
. }
main adverb of ADVP4 dictionary {yes, no} is adverb is in dictionary?5 ends-with-ly {yes, no} does adverb end in -ly?6 POS-tag-bef POStags POS tag word before adverb7 POS-tag-aft POStags POS tag word after adverb8 verb {assigned, go, .
.
. }
main verb the ADVP attaches to9 distance N number of words between adverb and verbTable 4: Features used for extracting MANNER from ADVPs, their values and explanation.
Features 4 and 5 arespecialized on MANNER detection.No.
Feature Values Explanation1 parent-node {S, VP} syntactic node of PP?s parent2 next-node {NP, SBAR, , .
.
. }
syntactic node of sibling to the right of PP3 num-pp-bef N number of sibling before PP which are PP4 num-pp-aft N number of sibling after PP which are PP5 first-word {with, after, .
.
. }
first word in PP6 first-POS-tag POStags first POS tag in PP7 first-prep {by, on, .
.
. }
first preposition in PP8 POS-tag-bef POStags POS tag before first-word9 POS-tag-aft POStags POs tag after first-word10 word-aft {one, their, .
.
. }
word after first-word11 has-rb {yes, no} does the PP contain an adverb?12 has-quotes {yes, no} does the PP have any quotes?13 head-np-lemma {amount, year, .
.
. }
head of the NP whose parent is the PP14 head-is-last {yes, no} is head-np the last word of the sentence?15 head-has-cap {yes, no} does the PP have a capitalized word?16 verb {approved, fly, .
.
. }
verb the PP attaches to17 verb-lemma {approve, be, .
.
. }
verb lemma the PP attaches to18 verb-pas {yes, no} is verb in passive voice?Table 5: Features used for extracting MANNER from PPs, their values and explanation.
Features in bold letters are newand specialized on detecting MANNER from PPs.CoNLL-2005 Shared Task (Carreras and Ma`rquez,2005), combined with new, manner-specific featuresthat we introduce.
These new features bring a signif-icant improvement and are dependent on the phrasepotentially encoding a MANNER.
Experimentationhas shown that MANNER relations expressed by anADVP are easier to detect than the ones expressedby a PP.Adverbial Phrases The feature set used is depictedin Table 4.
Some features are typical of semanticrole labeling, but features adverb, dictionaryand ends-with-ly are specialized to MANNERextraction from ADVPs.
These three additional fea-tures bring a significant improvement (Section 7).We only provide details for the non-obvious fea-tures.The main adverb and verb are retrieved by select-ing the last adverb or verb of a sequence.
For exam-ple, in more strongly, the main adverb is strongly,and in had been rescued the main verb is rescued.Dictionary tests the presence of theadverb in a custom built dictionary whichcontains all lemmas for adverbs in WordNetwhose gloss matches the regular expression ina .
* (manner|way|fashion|style).
For example,more.adv.1: used to form the comparative of someadjectives and adverbs does not belong to thedictionary, and strongly.adv.1: with strength or in a320strong manner does.
This feature is an extension ofthe dictionary presented in (Girju et al, 2003).Given the sentence [.
.
. ]
We [work[damn hard]ADVP at what we do for damn lit-tle pay]VP, and [.
.
. ]
(wsj 1144, 128), the featuresare: {parent-node:VP, num-leaves:2, adverb:hard,dictionary:no, ends-with-ly:no, POS-tag-bef:RB,POS-tag-aft:IN, verb:work, distance:1}, and it is apositive instance.Prepositional Phrases PPs are known to be highlyambiguous and more features need to be added.
Thecomplete set is depicted in Table 5.Some features are typical of semantic role detec-tion; we only provide a justification for the newfeatures added.
Num-pp-bef and num-pp-aftcaptures the number of PP siblings before and afterthe PP.
The relative order of PPs is typically MAN-NER, AT-LOCATION and AT-TIME (Hawkins, 1999),and this feature captures this idea without requiringtemporal or local annotation.PPs having quotes are more likely to en-code a MANNER, the chunk of text betweenquotes being the manner.
For example, usein ?very modest amounts?
(wsj 0003, 10) and re-ward with ?page bonuses?
(wsj 0012, 8).head-np indicates the head noun of the NPthat attaches to the preposition to form the PP.
Itis retrieved by selecting the last noun in the NP.Certain nouns are more likely to indicate a MAN-NER than others.
This feature captures the do-main restriction.
For nouns, only non temporalabstract objects and states can encode a MAN-NER.
Some examples of positive instances arehaul in the guests?
[honor], lift in two [stages], winat any [cost], plunge against the [mark] and easewith little [fanfare].
However, counterexamples canbe found as well: say through his [spokesman] anddo over the [counter].Verb-pas indicates if a verb is in passivevoice.
In that case, a PP starting with by is muchmore likely to encode an AGENT than a MAN-NER.
For example, compare (1) ?When the fruit isripe, it [falls]y from the tree [by itself]PP,?
he says.
(wsj 0300, 23); and (2) Four of the planes [werepurchased]y [by International Lease]PP from Singa-pore Airlines in a [.
.
. ]
transaction (wsj 0243, 3).In the first example a MANNER holds; in the secondan AGENT.Given the sentence Kalipharma is a New Jersey-based pharmaceuticals concern that [sells products[under the Purepac label]PP]VP.
(wsj 0023, 1), thefeatures are: {parent-node:VP, next-node:-, num-pp-bef:0, num-pp-aft:0, first-word:under, first-POS-tag:IN, first-prep:under, POS-tag-bef:NNS, POS-tag-aft:DT, word-aft:the, has-rb:no, has-quotes:no,head-np-lemma:label, head-is-last:yes, head-has-cap:yes, verb:sells, verb-lemma:sell, verb-pas:no},and it is a positive instance.7 Learning Algorithm and Results7.1 Experimental ResultsAs a learning algorithm we use a Naive Bayes clas-sifier, well known for its simplicity and yet good per-formance.
We trained our models with the trainingcorpus using 10-fold cross validation, and used theheld-out portion to tune the feature set and adjustparameters.
More features than the ones depictedwere tried, but we only report the final set.
For ex-ample, named entity recognition and flags indicat-ing the presence of AT-LOCATION and AT-TIME re-lations for the verb were tried, but they did not bringany significant improvement.Table 6 summarizes the results obtained.
We re-port results only on the test corpus, which corre-sponds to instances not seen before and thereforethey are a honest estimation of the performance.The improvement brought by subsets of featuresand statistical significance tests are also reported.We test the significance of the difference in per-formance between two feature sets i and j on aset of ins instances with the Z-score test, wherez = abs(erri,errj)?d , errk is the error made using setk, and ?d =?erri(1?erri)ins +errj(1?errj)ins .ADVPs The full set of features yields a F-measureof 0.815.
The three specialized features (3, 4 and5) are responsible for an improvement of .168 in theF-measure.
This difference in performance yields aZ-score of 7.1, which indicates that it is statisticallysignificant.PPs All the features proposed yield a F-measure of0.693.
The novel features specialized in MANNERdetection from PPs (in bold letters in Table 5) bringan improvement of 0.059, which again is significant.321Phrase #MNR Feat.
Set #MNR retrieved #MNR correct P R FADVP 6781,2,6-9 908 513 .565 .757 .647all 757 585 .773 .863 .815PP 7051,2,5,6,8-10,16,17 690 442 .641 .627 .634all 713 491 .689 .696 .693Table 6: Results obtained during testing for different sets of features.The Z-score is 2.35, i.e., the difference in perfor-mance is statistically significant with a confidencegreater than 97.5%.
Thus, adding the specializedfeatures is justified.7.2 Error AnalysisThe mapping of words to semantic classes isdata-driven and decisions were taken so that theoverall accuracy is high.
However, mistakesare made.
Given We want to [see]y the mar-ket from the inside, the underlined PP encodes aMANNER and the mapping proposed (Table 1)does not map inside to ntao.
Similarly, givenLike their cohorts in political consulting, the litiga-tion advisers [encourage]y their clients [.
.
.
], theunderlined text encodes a MANNER and yet cohortsis subsumed by social group.n.1 and therefore is notmapped to ntao.The model proposed for MANNER detectionmakes mistakes as well.
For ADVPs, if the mainadverb has not been seen during training, chances ofdetecting MANNER are low.
For example, the classi-fier fails to detect the following MANNER relations:[.
.
. ]
which together own about [.
.
. ]
(wsj 0671, 1);and who has ardently supported [.
.
. ]
(wsj 1017,26) even though ardently is present in the dictionaryand ends in -ly;For PPs, some errors are due to the Prop-Bank annotation.
For example, in ShearsonLehman Hutton began its coverage of the companywith favorable ratings.
(wsj 2061, 57), the under-lined text is annotated as ARG2, even though itdoes encode a MANNER.
Our model correctly de-tects a MANNER but it is counted as a mistake.Manners encoded by under and at are rarely de-tected, as in that have been consolidated in fed-eral court under U.S. District Judge Milton Pollack(wsj 1022.mrg, 10).8 Comparison with Previous WorkTo the best of our knowledge, there have not beenmuch efforts to detect MANNER alone.
Girju et al(2003), present a supervised approach for ADVPsimilar to the one reported in this paper, yieldinga F-measure of .665.
Our augmented feature setobtains a F-measure of .815, clearly outperformingtheir method (Z-test, confidence > 97.5%).
More-over, ADVPs only represent 45.3% of MANNER as asemantic role in PropBank.
We also have presenteda model to detect MANNER encoded by a PP, theother big chunk of MANNER (44.6%) in PropBank.Complete systems for Semantic Role Labelingperform poorly when detecting MANNER; the Top-10 systems in CoNLL-2005 shared task3 obtainedF-measures ranging from .527 to .592.
We havetrained our models using the training data providedby the task organizers (using the Charniak parsersyntactic information), and tested with the providedtest set (test.wsj).
Our models yield a Precision of.759 and Recall of .626 (F-measure .686), bringing asignificant improvement over those systems (Z-test,confidence > 97.5%).
When calculating recall, wetake into account all MANNER in the test set, notonly ADVPs and PPs whose fathers are S or VP (i.e.not only the ones our models are able to detect).
Thisevaluation is done with exactly the same data pro-vided from the task organizers for both training andtest.Unlike typical semantic role labelers, our featuresdo not include rich syntactic information (e.g.
syn-tactic path from verb to the argument).
Instead, weonly require the value of the parent and in the case ofPPs, the sibling node.
When repeating the CoNLL-2005 Shared Task training and test using gold syn-tactic information, the F-measure obtained is .714,very close to the .686 obtained with Charniak syn-tactic trees (not significant, confidence > 97.5%).3http://www.lsi.upc.es/?srlconll/st05/st05.html322Even though syntactic parsers achieve a good perfor-mance, they make mistakes and the less our modelsrely on them, the better.9 Composing MANNER with PURPOSEMANNER can combine with other semantic rela-tions in order to reveal implicit relations that oth-erwise would be missed.
The basic idea is to com-pose MANNER with other relations in order to in-fer another MANNER.
A necessary condition forcombining MANNER with another relation R is thecompatibility of RANGE(MNR) with DOMAIN(R) orRANGE(R) with DOMAIN(MNR).
The extended def-inition (Section 3) allows to quickly determine if tworelations are compatible (Blanco et al, 2010).The new MANNER is automatically inferredby humans when reading, but computers needan explicit representation.
Consider the follow-ing example: [.
.
. ]
the traders [place]y orders[via computers]MNR [to buy the basket of stocks.
.
.
]PRP (wsj 0118, 48).
PropBank states the basicannotation between brackets: via computers is theMANNER and to buy the basket [.
.
. ]
the PURPOSEof the place orders event.
We propose to combinethese two relations in order to come up with the newrelation MNR(via computers, buy the basket [.
.
. ]
).This relation is obvious when reading the sentence,so it is omitted by the writer.
However, any seman-tic representation of text needs as much semantics aspossible explicitly stated.This claim is supported by several PropBankexamples: (1) The classics have [zoomed]y[in price]MNR [to meet the competition]PRP,and .
.
.
(wsj 0071, 9) and (2) .
.
.
the govern-ment [curtailed]y production [with land-idlingprograms]MNR [to reduce price-depressingsurpluses]PRP (wsj 0113, 12).
In both exam-ples, PropBank encodes the MANNER and PURPOSEfor event y indicated with brackets.
After com-bining both relations, two new MANNER arise:MNR(in price, meet the competition) and MNR(withland-idling programs, reduce price-depressingsurpluses).Out of 237 verbs having in PropBank both PUR-POSE and MANNER annotation, the above inferencemethod yields 189 new valid MANNER not presentin PropBank (Accuracy .797).MANNER and other relations.
MANNER doesnot combine with relations such as CAUSE, AT-LOCATION or AT-TIME.
For example, given Andthey continue [anonymously]x,MNR [attacking]y CIADirector William Webster [for being too accom-modating to the committee]z,CAU (wsj 0590, 27),there is no relation between x and z. Similarly,given [In the tower]x,LOC, five men and women[pull]y [rhythmically]z,MNR on ropes attached to[.
.
. ]
(wsj 0089, 5) and [In May]x,TMP, the twocompanies, [through their jointly owned holdingcompany]z,MNR, Temple, [offered]y [.
.
. ]
(wsj 0063,3), no connection exists between x and z.10 ConclusionsWe have presented a supervised method for the au-tomatic discovery of MANNER.
Our approach issimple and outperforms previous work.
Our mod-els specialize in detecting the most common patternencoding MANNER.
By doing so we are able to spe-cialize our feature sets and outperform previous ap-proaches that followed the idea of using dozens offeatures, most of them potentially useless, and let-ting a complicated machine learning algorithm de-cide the actual useful features.We believe that each relation or role has its ownunique characteristics and capturing them improvesperformance.
We have shown this fact for MANNERby examining examples, considering the kind of ar-guments that can be part of the domain and range,and considering theoretical works (Hawkins, 1999).We have shown performance using both gold syn-tactic trees and the output from the Charniak parser,and there is not a big performance drop.
This ismainly due to the fact that we do not use deep syn-tactic information in our feature sets.The combination of MANNER and PURPOSEopens up a novel paradigm to perform semantic in-ference.
We envision a layer of semantics using asmall set of basic semantic relations and inferencemechanisms on top of them to obtain more seman-tics on demand.
Combining semantic relations inorder to obtain more relation is only one of the pos-sible inference methods.323ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of the 17th international conference on Computa-tional Linguistics, Montreal, Canada.Eduardo Blanco, Hakki C. Cankaya, and Dan Moldovan.2010.
Composition of Semantic Relations: Model andApplications.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (COLING2010), Beijing, China.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 shared task: semantic role label-ing.
In CONLL ?05: Proceedings of the Ninth Confer-ence on Computational Natural Language Learning,pages 152?164, Morristown, NJ, USA.Du S. Chang and Key S. Choi.
2006.
Incremen-tal cue phrase learning and bootstrapping method forcausality extraction using cue phrase and word pairprobabilities.
Information Processing & Management,42(3):662?678.Dmitry Davidov and Ari Rappoport.
2008.
Unsuper-vised Discovery of Generic Relationships Using Pat-tern Clusters and its Evaluation by Automatically Gen-erated SAT Analogy Questions.
In Proceedings ofACL-08: HLT, pages 692?700, Columbus, Ohio.Dmitry Davidov, Ari Rappoport, and Moshe Koppel.2007.
Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining.
In Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 232?239, Prague,Czech Republic.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic La-beling Of Semantic Roles.
Computational Linguistics,28:245?288.Roxana Girju, Manju Putcha, and Dan Moldovan.
2003.Discovery of Manner Relations and Their Applicabil-ity to Question Answering.
In Proceedings of the ACL2003 Workshop on Multilingual Summarization andQuestion Answering, pages 54?60, Sapporo, Japan.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2006.
Automatic Discovery of Part-Whole Relations.Computational Linguistics, 32(1):83?135.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 Task 04: Classification of SemanticRelations between Nominals.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 13?18, Prague, CzechRepublic.Ana M. Giuglea and Alessandro Moschitti.
2006.
Se-mantic role labeling via FrameNet, VerbNet and Prop-Bank.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 929?936, Morristown, NJ, USA.John A. Hawkins.
1999.
The relative order of prepo-sitional phrases in English: Going beyond Manner-Place-Time.
Language Variation and Change,11(03):231?266.Marti A. Hearst.
1992.
Automatic Acquisition of Hy-ponyms from Large Text Corpora.
In Proceedings ofthe 14th International Conference on ComputationalLinguistics, pages 539?545.Hermann Helbig.
2005.
Knowledge Representation andthe Semantics of Natural Language.
Springer.Toru Hirano, Yoshihiro Matsuo, and Genichiro Kikui.2007.
Detecting Semantic Relations between NamedEntities in Text Using Contextual Features.
In Pro-ceedings of the 45th Annual Meeting of the Associa-tion for Computational Linguistics, Demo and PosterSessions, pages 157?160, Prague, Czech Republic.Llu?
?s Ma`rquez, Xavier Carreras, Kenneth C. Litkowski,and Suzanne Stevenson.
2008.
Semantic Role Label-ing: An Introduction to the Special Issue.
Computa-tional Linguistics, 34(2):145?159.Dan Moldovan, Christine Clark, and Mitchell Bowden.2007.
Lymba?s PowerAnswer 4 in TREC 2007.
InProceedings of the Sixteenth Text REtrieval Confer-ence (TREC 2007).Paul Nulty.
2007.
Semantic Classification of NounPhrases Using Web Counts and Learning Algorithms.In Proceedings of the ACL 2007 Student ResearchWorkshop, pages 79?84, Prague, Czech Republic.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.Vivek Srikumar, Roi Reichart, Mark Sammons, Ari Rap-poport, and Dan Roth.
2008.
Extraction of EntailedSemantic Relations Through Syntax-Based CommaResolution.
In Proceedings of ACL-08: HLT, pages1030?1038, Columbus, Ohio.Barker Szpakowicz, Ken Barker, and Stan Szpakowicz.1995.
Interactive semantic analysis of Clause-LevelRelationships.
In Proceedings of the Second Confer-ence of the Pacific Association for Computational Lin-guistics, pages 22?30.Marta Tatu.
2005.
Automatic Discovery of Intentions inText and its Application to Question Answering.
InProceedings of the ACL Student Research Workshop,pages 31?36, Ann Arbor, Michigan.Peter D. Turney.
2006.
Expressing Implicit SemanticRelations without Supervision.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 313?320, Syd-ney, Australia.324
