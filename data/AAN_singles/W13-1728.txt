Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 216?223,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsImproving Native Language Identification with TF-IDF WeightingBinyam Gebrekidan Gebre1, Marcos Zampieri2, Peter Wittenburg1, Tom Heskes31Max Planck Institute for Psycholinguistics2University of Cologne3Radboud Universitybingeb@mpi.nl,mzampier@uni-koeln.de,peter.wittenburg@mpi.nl,t.heskes@science.ru.nlAbstractThis paper presents a Native Language Iden-tification (NLI) system based on TF-IDFweighting schemes and using linear classi-fiers - support vector machines, logistic re-gressions and perceptrons.
The system wasone of the participants of the 2013 NLI SharedTask in the closed-training track, achieving0.814 overall accuracy for a set of 11 nativelanguages.
This accuracy was only 2.2 per-centage points lower than the winner?s perfor-mance.
Furthermore, with subsequent evalua-tions using 10-fold cross-validation (as givenby the organizers) on the combined trainingand development data, the best average accu-racy obtained is 0.8455 and the features thatcontributed to this accuracy are the TF-IDF ofthe combined unigrams and bigrams of words.1 IntroductionNative Language Identification (NLI) is the task ofautomatically identifying the native language of awriter based on the writer?s foreign language pro-duction.
The task is modeled as a classification taskin which automatic methods have to assign class la-bels (native languages) to objects (texts).
NLI is byno means trivial and it is based on the assumptionthat the mother tongue influences Second LanguageAcquisition (SLA) and production (Lado, 1957).When an English native speaker hears someonespeaking English, it is not difficult for him/her toidentify if this person is a native speaker or not.Moreover, it is, to some extent, possible to assertthe mother tongue of non-native speakers by his/herspronunciation patterns, regardless of their languageproficiency.
In NLI, the same principle that seemsintuitive for spoken language, is applied to text.
Ifit is true that the mother tongue of an individual in-fluences speech production, it should be possible toidentify these traits in written language as well.NLI methods are particularly relevant for lan-guages with a significant number of foreign speak-ers, most notably, English.
It is estimated thatthe number of non-native speakers of English out-numbers the number of native speakers by two toone (Lewis et al 2013).
The written productionof non-native speakers is abundant on the Internet,academia, and other contexts where English is usedas lingua franca.This study presents the system that participated inthe 2013 NLI Shared Task (Tetreault et al 2013)under the name Cologne-Nijmegen.
The novel as-pect of the system is the use of TF-IDF weightingschemes.
For this study, we experimented with anumber of algorithms and features.
Linear SVM andlogistic regression achieved the best accuracies onthe combined features of unigrams and bigrams ofwords.
The rest of the paper will explain in detailthe features, methods and results achieved.2 MotivationThere are two main reasons to study NLI.
On onehand, there is a strong linguistic motivation, particu-larly in the field of SLA and on the other hand, thereis the practical relevance of the task and its integra-tion to a number of computational applications.The linguistic motivation of NLI is the possibil-ity of using classification methods to study the inter-216play between native and foreign language acquisi-tion and performance (Wong and Dras, 2009).
Oneof the SLA theories that investigate these phenom-ena is contrastive analysis, which is used to explainwhy some structures of L2 are more difficult to ac-quire than others (Lado, 1957).Contrastive analysis postulates that the difficultyin mastering L2 depends on the differences betweenL1 and L2.
In the process of acquiring L2, lan-guage transfer (also known as L1 interference) oc-curs and speakers apply knowledge from their na-tive language to a second language, taking advan-tage of their similarities.
Computational methodsapplied to L2 written production can function as acorpus-driven method to level out these differencesand serve as a source of information for SLA re-searchers.
It can also be used to provide more tar-geted feedback to language learners about their er-rors.NLI is also a relevant task in computational lin-guistics and researchers have turned their attentionto it in the last few years.
The task is often regardedas a part of a broader task of authorship profiling,which consists of the application of automatic meth-ods to assert information about the writer of a giventext, such as age, gender as well native language.Authorship profiling is particularly useful for foren-sic linguistics.Automatic methods of NLI may be integrated inNLP applications such as spam detection or machinetranslation.
NLP tasks such as POS tagging andparsing might also benefit from NLI, as these re-sources are trained on standard language written bynative speakers.
These tools can be more accurate totag non-native speaker?s text if trained with L2 cor-pora.3 Related WorkIn the last years, a couple of attempts at identifyingnative language have been described in the literature.Tomokiyo and Jones (2001) uses a Naive Bayes al-gorithm to classify transcribed data from three nativelanguages: Chinese, Japanese and English.
The al-gorithm reached 96% accuracy when distinguishingnative from non-native texts and 100% when distin-guishing English native speakers from Chinese na-tive speakers.Koppel et al(2005) used machine learning toidentify the native languages of non-native Englishspeakers with five different mother tongues (Bul-garian, Czech, French, Russian, and Spanish), us-ing data retrieved from the International Corpus ofLearner English (ICLE) (Granger et al 2009).
Thefeatures used in this study were function words,character n-grams, and part-of-speech (POS) bi-grams.Tsur and Rappoport (2007) investigated the influ-ence of the phonology of a writer?s mother tonguethrough native language syllables modelled by char-acter bigrams.
Estival et al(2007) addressed NLI aspart of authorship profiling.
Authors aim to attribute10 different characteristics of writers by analysinga set of English e-mails.
The study reports around84% accuracy in distinguishing e-mails written byEnglish Arabic and Spanish L1 speakers.SVM, the algorithm that achieved the best resultsin our experiments, was also previously used in NLI(Kochmar, 2011).
In this study, the author identi-fied error types that are typical for speakers of differ-ent native languages.
She compiled a set of featuresbased on these error types to improve the classifica-tion?s performance.Recently, the TOEFL11 corpus was compiled toserve as an alternative to the ICLE corpus (Tetreaultet al 2012).
Authors argue that TOEFL11 is moresuitable to NLI than ICLE.
This study also experi-mented with different features to increase results inNLI and reports best accuracy results of 90.1% onICLE and 80.9% on TOEFL11.4 MethodsWe approach the task of native language identifica-tion as a kind of text classification.
In text classifica-tion, decisions and choices have to be made at threelevels.
First, how do we use the training and devel-opment data?
Second, what features do we extractand how do we select the most informative ones?Third, which machine learning algorithms performbest and which parameters can we tune under theconstraints of memory and time?
In the followingsubsections, we answer these questions.2174.1 Dataset: TOEFL11The dataset used for the shared task is calledTOEFL11 (Blanchard et al 2013).
It consists of12,100 English essays (about 300 to 400 words long)from the Test of English as a Foreign Language(TOEFL).
The essays are written by 11 native lan-guage speakers (L1).
Table 1 shows the 11 na-tive languages.
Each essay is labelled with an En-glish language proficiency level (high, medium, orlow) based on the judgments of human assessmentspecialists.
We used 9,900 essays for training dataand 1,100 for development (parameter tuning).
Theshared task organizers kept 1,100 essays for testing.Table 1: TOEFL11L1 languages Arabic, Chinese,French, German,Hindi, Italian,Japanese, Korean,Spanish, Telugu,Turkish# of essays per L1900 for training100 for validating100 for testing4.2 FeaturesWe explored different kinds and combinations offeatures that we assumed to be different for differentL1 speakers and that are also commonly used in theNLI literature (Koppel et al 2005; Tetreault et al2012).
Table 2 shows the sources of the features weconsidered.
Unigrams and bigrams of words are ex-plored separately and in combination.
One throughfour grams of part of speech tags have also been ex-plored.
For POS tagging of the essays, we appliedthe default POS tagger from NLTK (Bird, 2006).Spelling errors have also been treated as features.We used the collection of words in Peter Norvig?swebsite1 as a reference dictionary.
The collectionconsists of about a million words.
It is a concate-nation of several public domain books from ProjectGutenberg and lists of most frequent words fromWiktionary and the British National Corpus.Character n-grams have also been explored forboth the words in the essays and for words with1http://norvig.com/spell-correct.htmlspelling errors.
The maximum n-gram size consid-ered is six.All features, consisting of either characters orwords or part-of-speech tags or their combinations,are mapped into normalized numbers (norm L2).For the mapping, we use TF-IDF, a weighting tech-nique popular in information retrieval but which isalso finding its use in text classification.
Featuresthat occurred in less than 5 of the essays or thosethat occurred in more than 50% of the essays areremoved (all characters are in lower case).
Thesecut-off values are experimentally selected.Table 2: A summary of features used in our experimentsWord n-grams Unigrams and bigrams ofwords present in the es-says.POS n-grams One up to four grams ofPOS tags present in theessays; tagging is doneusing default NLTK tag-ger (Bird, 2006).Character n-grams One up to six grams ofcharacters in each essay.Spelling errors All words that are notfound in the dictionaryof Peter Norvig?s spellingcorrector.4.2.1 Term Frequency (TF)Term Frequency refers to the number of times aparticular term appears in an essay.
In our experi-ments, terms are n-grams of characters, words, part-of-speech tags or any combination of them.
Theintuition is that a term that occurs more frequentlyidentifies/specifies the essay better than another termthat occurs less frequently.
This seems a usefulheuristic but what is the relationship between the fre-quency of a term and its importance to the essay?From among many relationships, we selected a log-arithmic relationship (sublinear TF scaling) (Man-ning et al 2008):wft,e ={1 + log(tft,e) if tft,e > 00 otherwise(1)218where wft,e refers to weight and tft,e refers to thefrequency of term t in essay e.The wft,e weight tells us the importance of a termin an essay based on its frequency.
But not all termsthat occur more frequently in an essay are equallyimportant.
The effective importance of a term alsodepends on how infrequent the term is in other es-says and this intuition is handled by Inverse Docu-ment Frequency(IDF).4.2.2 Inverse Document Frequency(IDF)Inverse Document Frequency (IDF) quantifies theintuition that a term which occurs in many essaysis not a good discriminator, and should be givenless weight than one which occurs in fewer essays.In mathematical terms, IDF is the log of the in-verse probability of a term being found in any essay(Salton and McGill, 1984):idf(ti) = logNni, (2)where N is the number of essays in the corpus,and term ti occurs in ni of them.
IDF gives a newweight when combined with TF to form TF-IDF.4.2.3 TF?IDFTF?IDF combines the weights of TF and IDFby multiplying them.
TF gives more weight to afrequent term in an essay and IDF downscales theweight if the term occurs in many essays.
Equation3 shows the final weight that each term of an essaygets before normalization.wi,e = (1 + log(tft,e))?
log(N/ni) (3)Essay lengths are usually different and this has animpact on term weights.
To abstract from differentessay lengths, each essay feature vector is normal-ized to unit length.
After normalization, the result-ing essay feature vectors are fed into classifiers.4.3 ClassifiersWe experimented with three linear classifiers - lin-ear support vector machines, logistic regression andperceptrons - all from scikit-learn (Pedregosa et al2011).
These algorithms are suitable for high dimen-sional and sparse data (text data is high dimensionaland sparse).
In the following paragraphs, we brieflydescribe the algorithms and the parameter values weselected.SVMs have been explored systematically for textcategorization (Joachims, 1998).
An SVM classi-fier finds a hyperplane that separates examples intotwo classes with maximal margin (Cortes and Vap-nik, 1995) (Multi-classes are handled by multi one-versus-rest classifiers).
Examples that are not lin-early separable in the feature space are mapped to ahigher dimension using kernels.
In our experiments,we used a linear kernel and a penalty parameter ofvalue 1.0.In its various forms, logistic regression is alsoused for text classification (Zhang et al 2003;Genkin et al 2007; Yu et al 2011) and nativelanguage identification (Tetreault et al 2012).
Lo-gistic regression classifies data by using a decisionboundary, determined by a linear function of the fea-tures.
For the implementation of the algorithm, weused the LIBLINEAR open source library (Fan etal., 2008) from scikit-learn (Pedregosa et al 2011)and we fixed the regularization parameter to 100.0.For baseline, we used a perceptron classifier(Rosenblatt, 1957).
Perceptron (or single layer net-work) is the simplest form of neural network.
It isdesigned for linear separation of data and works wellfor text classification.
The number of iterations ofthe training algorithm is fixed to 70 and the rest ofparameters are left with their default values.5 Results and DiscussionFor each classifier, we ran ten-fold cross-validationexperiments.
We divided the training and develop-ment data into ten folds using the same fold splittingids as requested by the shared task organizers andalso as used in (Tetreault et al 2012).
Nine of thefolds were used for training and the tenth for test-ing the trained model.
This was repeated ten timeswith each fold being held out for testing.
The per-formance of the classifiers on different features arepresented in terms of average accuracy.Table 3 gives the average accuracies based onthe TF-IDF of word and character n-grams.
Lin-ear SVM gives the highest accuracy of 84.55% us-ing features extracted from unigrams and bigramsof words.
Logistic regression also gives comparableaccuracy of 84.45% on the same features.219Table 3: Cross-validation results; accuracy in %N-gram LinearSVMLogisticRegression PerceptronWords1 74.73 74.18 65.452 80.91 80.27 75.451 and 2 84.55 84.45 78.82(1 and 2)* 83.36 83.27 78.73* minus country and language namesCharacters1 18.45 19.27 9.092 43.27 40.82 10.363 71.36 68.00 36.914 80.36 79.91 59.645 83.09 82.64 73.916 84.09 84.00 76.45The size of the feature vector of unigrams and bi-grams of words is 73,6262.
For each essay, only afew of the features have non-zero values.
Whichfeatures are active and most discriminating in theclassifiers?
Table 4 shows the ten most informativefeatures for the 10th run in the cross-validation (aspicked up linear SVM).Table 4: Ten most informative features for each L1ARA many reasons / from / self / advertisment / , and /statment / any / thier / alot of / alotCHI in china / hold / china / time on / may / taiwan / just /still / , the / .
takeFRE french / conclude , / even if / in france / france / toconclude / indeed , / ... / .
indeed / indeedGER special / furthermore / might / germany / , because /have to / .
but / - / often / , thatHIN which / and concept / various / hence / generation / &/ towards / then / its / as comparedITA in italy / , for / infact / that a / italy / i think / in fact /italian / think that / :JPN , and / i disagree / is because / .
it / .
if / i think /japan , / japanese / in japan / japanKOR .
however / however , / even though / however / thesedays / various / korea , / korean / in korea / koreaSPA an specific / because is / moment / , etc / going to / ,is / necesary / , and / diferent / , butTELmay not / the statement / every one / days / the above/ where as / with out / when compared / i conclude /and alsoTUR ages / istanbul / addition to / conditions / enough / inturkey / the life / ; / .
because / turkeyThe ten most informative features include coun-2features that occur less than 5 times or that occur in morethan 50% of the essays are removed from the vocabularytry and language names.
For example, for Japaneseand Korean L1s, four of the ten top features includeKorea or Korean in the unigrams or bigrams.
Howwould the classification accuracy decrease if we re-moved mentions of country or language names?We made a list of the 11 L1 language names andthe countries where they are mainly spoken (for ex-ample, German, Germany, French, France, etc.).
Weconsidered this list as stop words (i.e.
removed themfrom corpus) and ran the whole classification exper-iments.
The new best accuracy is 83.36% ( a loss ofjust 1.2% ).
Table 3 shows the new accuracies for allclassifiers.
The new top ten features mostly consistof function words and some spelling errors.
Table 5shows all of the new top ten features.The spelling errors seem to have been influencedby the L1 languages, especially for French andSpanish languages.
The English words exampleand developed have similar sounding/looking equiv-alents in French (exemple and de?veloppe?)
.
Simi-larly, the English words necessary and different havesimilar sounding/looking words in Spanish (nece-sario and diferente).
These spelling errors made itto the top ten features.
But how discriminating arethey on their own?Table 5: Ten most informative features (minus countryand language names) for each L1ARA many reasons / from / self / advertisment / , and /statment / any / thier / alot of / alotCHI and more / hold / more and / time on / taiwan / may /just / still / .
take / , theFRE conclude / exemple / developped / conclude , / evenif / to conclude / indeed , / ... / .
indeed / indeedGER has to / special / furthermore / might / , because /have to / .
but / - / often / , thatHIN and concept / which / various / hence / generation / &/ towards / then / its / as comparedITA possibility / probably / particular / , for / infact / thata / i think / in fact / think that / &JPN i agree / the opinion / tokyo / two reasons / is because/ , and / i disagree / .
it / .
if / i thinkKOR creative / , many / ?s / .
also / .
however / even though/ however , / various / however / these daysSPA activities / an specific / moment / , etc / going to / , is/ necesary / , and / diferent / , butTELmay not / the statement / every one / days / the above/ where as / when compared / with out / i conclude /and alsoTUR enjoyable / being / ages / addition to / istanbul /enough / conditions / the life / ; / .
becauseWe ran experiments with features extracted from220Table 6: Confusion matrix: Best accuracy is for German (95%) and the worst is for Hindi (72%)ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TURARA 83 1 4 1 1 3 1 2 3 1 0CHI 0 88 2 0 2 0 2 5 1 0 0FRE 3 0 88 2 1 2 0 1 2 0 1GER 2 0 1 95 0 0 0 0 1 0 1HIN 2 1 1 1 72 0 0 0 2 18 3ITA 0 0 6 3 0 84 0 0 6 0 1JPN 1 2 0 1 1 0 84 10 0 0 1KOR 0 3 0 2 3 0 8 81 1 1 1SPA 6 2 5 2 0 4 0 0 79 0 2TEL 0 0 0 0 16 0 1 0 0 83 0TUR 1 1 0 1 3 0 0 0 1 0 93only spelling errors.
For comparison, we also ranexperiments with POS tags with and without theirwords.
None of these experiments beat the best ac-curacy obtained using unigram and bigram of words- not even the unigram and bigram of POS taggedwords.
See table 7 for the obtained results.Table 7: Cross-validation results; accuracy in %N-gram LinearSVMLogisticRegression PerceptronPOS1 17.00 17.09 9.092 43.45 40.00 11.183 55.27 53.55 35.364 56.09 56.18 48.64POS + Word1 75.09 74.18 64.092 80.45 80.64 76.181 and 2 83.00 83.36 79.09Spelling errors - characters1 20.36 21.00 9.092 34.09 32.64 9.733 47.00 44.64 26.824 50.82 48.09 41.641?4 51.82 48.27 34.18words 42.73 39.45 28.73All our reported results so far have been globalclassification results.
Table 6 shows the confusionmatrix for each L1.
The best accuracy is 95% forGerman and the worst is for Hindi (72%).
Hindiis classified as Telugu (18%) of the times and Tel-ugu is classified as Hindi 16% of the times andonly one Telugu essay is classified as any other thanHindi.
More generally, the confusion matrix seemsto suggest that geographically closer countries aremore confused with each other: Hindi and Telugu,Japanese and Korean, Chinese and Korean.The best accuracy (84.55%) obtained in our ex-periments is higher than the state-of-the-art accuracyreported in (Tetreault et al 2012) (80.9%).
But thefeatures we used are not different from those com-monly used in the literature (Koppel et al 2005;Tetreault et al 2012) (n-grams of characters orwords).
The novel aspect of our system is the useof TF-IDF weighting on all of the features includingon unigrams and bigrams of words.TF-IDF weighting has already been used in na-tive language identification (Kochmar, 2011; Ahn,2011).
But its importance has not been fully ex-plored.
Experiments in Kochmar (2011) were lim-ited to character grams and in a binary classifica-tion scenario.
Experiments in Ahn (2011) appliedTF-IDF weighting to identify content words andshowed how their removal decreased performance(Ahn, 2011).
By contrast, in this paper, we appliedTF-IDF weighting consistently to all features - sametype features (e.g.
unigrams) or combined features(e.g.
unigram and bigrams).How would the best accuracy change if TF-IDFweighting is not applied?
Table 8 shows the changesto the best average accuracies with and withoutTF/IDF weighting for the three classifiers.Table 8: The importance of TF-IDF weightingTF IDF SVM LR PerceptronYes Yes 84.55 84.45 78.82Yes No 80.82 80.73 63.18No Yes 82.36 82.27 78.82No No 79.18 78.55 56.362216 ConclusionsThis paper has presented the system that participatedin the 2013 NLI Shared Task in the closed-trainingtrack.
Cross-validation testing on the TOEFL11 cor-pus showed that the system could achieve an accu-racy of about 84.55% in categorizing unseen essaysinto one of the eleven L1 languages.The novel aspect of the system is the useof TF-IDF weighting schemes on features ?which could be any or combination of n-gramwords/characters/POS tags.
The feature combina-tion that gave the best accuracy is the TF-IDF ofunigrams and bigrams of words.
The next best fea-ture class is the TF-IDF of 6-gram characters , whichachieved 84.09%, very close to 84.55%.
Both lin-ear support vector machines and logistic regressionclassifiers have performed almost equally.To improve performance in NLI, future workshould examine new features that can classify ge-ographically or typologically related languages suchas Hindi and Telugu.
Future work should also ana-lyze the information obtained in NLI experiments toquantify and investigate differences in the usage offoreign language lexicon or grammar according tothe individual?s mother tongue.AcknowledgmentsThe research leading to these results has re-ceived funding from the European Commissions7th Framework Program under grant agreement no238405 (CLARA).
The authors would like to thankthe organizers of the NLI Shared Task 2013 for pro-viding prompt reply to all our inquiries and for coor-dinating a very interesting and fruitful shared task.ReferencesCharles S. Ahn.
2011.
Automatically detecting authors?native language.
Ph.D. thesis, Monterey, California.Naval Postgraduate School.Steven Bird.
2006.
NLTK: the natural language toolkit.In Proceedings of the COLING/ACL on Interactivepresentation sessions, pages 69?72.
Association forComputational Linguistics.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine learning, 20(3):273?297.Dominique Estival, Tanja Gaustad, Son Bao Pham, WillRadford, and Ben Hutchinson.
2007.
Author Profilingfor English Emails.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics (PACLING), pages 263?272.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
Liblinear: A libraryfor large linear classification.
The Journal of MachineLearning Research, 9:1871?1874.Alexander Genkin, David D Lewis, and David Madigan.2007.
Large-scale bayesian logistic regression for textcategorization.
Technometrics, 49(3):291?304.Sylviane Granger, Estelle Dagneaux, and Fanny Meu-nier.
2009. International Corpus of Learner English.Presses Universitaires de Louvain, Louvain-la-Neuve.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
Springer.Ekaterina Kochmar.
2011.
Identification of a writer?s na-tive language by error analysis.
Master?s thesis, Uni-versity of Cambridge, United Kingdom.Moshe Koppel, Jonathan Schler, and Kfir Zigon.
2005.Automatically determining an anonymous author?s na-tive language.
Lecture Notes in Computer Science,3495:209?217.Robert Lado.
1957.
Applied Linguistics for LanguageTeachers.
University of Michigan Press.Paul Lewis, Gary Simons, and Charles Fennig.
2013.Ethnologue: Languages of the World, Seventeeth Edi-tion.
SIL International.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to information re-trieval, volume 1.
Cambridge University Press Cam-bridge.Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-cent Dubourg, Jake Vanderplas, Alexandre Passos,David Cournapeau, Matthieu Brucher, Matthieu Per-rot, and E?douard Duchesnay.
2011.
Scikit-learn: Ma-chine learning in Python.
Journal of Machine Learn-ing Research, 12:2825?2830.Frank Rosenblatt.
1957.
The perceptron, a perceiv-ing and recognizing automaton Project Para.
CornellAeronautical Laboratory.Gerard Salton and Michael McGill.
1984.
Introductionto Modern Information Retrieval.
McGraw-Hill BookCompany.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost andfound: Resources and empirical evaluations in native222language identification.
In Proceedings of COLING2012, pages 2585?2602, Mumbai, India, December.The COLING 2012 Organizing Committee.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.Summary report on the first shared task on native lan-guage identification.
In Proceedings of the EighthWorkshop on Building Educational Applications Us-ing NLP, Atlanta, GA, USA, June.
Association forComputational Linguistics.Laura Mayfield Tomokiyo and Rosie Jones.
2001.You?re not from ?round here, are you?
: Naive bayesdetection of non-native utterance text.
In Proceedingsof the second meeting of the North American Chap-ter of the Association for Computational Linguisticson Language technologies (NAACL ?01).Oren Tsur and Ari Rappoport.
2007.
Using classifier fea-tures for studying the effect of native language on thechoice of written second language words.
In Proceed-ings of the Workshop on Cognitive Aspects of Compu-tational Language Acquisition, pages 9?16.Sze-Meng Jojo Wong and Mark Dras.
2009.
Contrastiveanalysis and native language identification.
In Pro-ceedings of the Australasian Language Technology As-sociation Workshop, pages 53?61.
Citeseer.Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.2011.
Dual coordinate descent methods for logisticregression and maximum entropy models.
MachineLearning, 85(1-2):41?75.Jian Zhang, Rong Jin, Yiming Yang, and Alexander G.Hauptmann.
2003.
Modified logistic regression: Anapproximation to svm and its applications in large-scale text categorization.
In ICML, pages 888?895.223
