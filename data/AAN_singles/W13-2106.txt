Proceedings of the 14th European Workshop on Natural Language Generation, pages 51?60,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsUsing Integer Linear Programming for Content Selection, Lexicalization,and Aggregation to Produce Compact Texts from OWL OntologiesGerasimos Lampouras and Ion AndroutsopoulosDepartment of InformaticsAthens University of Economics and BusinessPatission 76, GR-104 34 Athens, Greecehttp://nlp.cs.aueb.gr/AbstractWe present an Integer Linear Program-ming model of content selection, lexical-ization, and aggregation that we devel-oped for a system that generates texts fromOWL ontologies.
Unlike pipeline archi-tectures, our model jointly considers theavailable choices in these three text gen-eration stages, to avoid greedy decisionsand produce more compact texts.
Experi-ments with two ontologies confirm that itleads to more compact texts, compared toa pipeline with the same components, withno deterioration in the perceived quality ofthe generated texts.
We also present an ap-proximation of our model, which allowslonger texts to be generated efficiently.1 IntroductionConcept-to-text natural language generation(NLG) generates texts from formal knowledgerepresentations (Reiter and Dale, 2000).
With theemergence of the Semantic Web (Berners-Lee etal., 2001; Shadbolt et al 2006; Antoniou andvan Harmelen, 2008), interest in concept-to-textNLG has been revived and several methods havebeen proposed to express axioms of OWL ontolo-gies (Grau et al 2008), a form of descriptionlogic (Baader et al 2002), in natural language(Bontcheva, 2005; Mellish and Sun, 2006; Gala-nis and Androutsopoulos, 2007; Mellish and Pan,2008; Schwitter et al 2008; Schwitter, 2010;Liang et al 2011; Williams et al 2011).NLG systems typically employ a pipeline archi-tecture.
They usually start by selecting the logicalfacts (axioms, in the case of an OWL ontology) tobe expressed.
The purpose of the next stage, textplanning, ranges from simply ordering the facts tobe expressed to making more complex decisionsabout the rhetorical structure of the text.
Lexical-ization then selects the words and syntactic struc-tures that will realize each fact, specifying howeach fact can be expressed as a single sentence.Sentence aggregation may then combine shortersentences to form longer ones.
Another compo-nent generates appropriate referring expressions,and surface realization produces the final text.Each stage of the pipeline is treated as a lo-cal optimization problem, where the decisions ofthe previous stages cannot be modified.
This ar-rangement produces texts that may not be optimal,since the decisions of the stages have been shownto be co-dependent (Danlos, 1984; Marciniak andStrube, 2005; Belz, 2008).
For example, deci-sions made during content selection may maxi-mize importance measures, but may produce factsthat are difficult to turn into a coherent text; also,content selection and lexicalization may lead tomore or fewer sentence aggregation opportunities.Some of these problems can be addressed by over-generating at each stage (e.g., producing severalalternative sets of facts at the end of content selec-tion, several alternative lexicalizations etc.)
andemploying a final ranking component to select thebest combination (Walker et al 2001).
This over-generate and rank approach, however, may alsofail to find an optimal solution, and it generates anexponentially large number of candidate solutionswhen several components are pipelined.In this paper, we present an Integer Linear Pro-gramming (ILP) model that combines content se-lection, lexicalization, and sentence aggregation.Our model does not consider directly text plan-ning, nor referring expression generation, whichwe hope to include in future work, but it is com-bined with an external simple text planner and anexternal referring expression generation compo-nent; we also do not discuss surface realization.Unlike pipeline architectures, our model jointlyexamines the possible choices in the three NLGstages it considers, to avoid greedy local decisions.51Given an individual (entity) or class of an OWLontology and a set of facts (axioms) about the in-dividual or class, we aim to produce a compacttext that expresses as many facts in as few wordsas possible.
This is desirable when space is lim-ited or expensive, e.g., when displaying productdescriptions on smartphones, or when includingadvertisements in Web search results.
If an impor-tance score is available for each fact, our modelcan take it into account to prefer expressing im-portant facts, again using as few words as possi-ble.
The model itself, however, does not produceimportance scores, i.e., we assume that the scoresare produced by a separate process (Barzilay andLapata, 2005; Demir et al 2010), not included inour content selection.
In the experiments of thisarticle, we treat all the facts as equally important.Although the search space of our model is verylarge and ILP problems are in general NP-hard, off-the-shelf ILP solvers can be used, which can bevery fast in practice and guarantee finding a globaloptimum.
Experiments with two ontologies showthat our ILP model outperforms, in terms of ex-pressed facts per word, an NLG system that usesthe same components connected in a pipeline, withno deterioration in perceived text quality; the ILPmodel may actually lead to texts of higher quality,compared to those of the pipeline, when there aremany facts to express.
We also present an approx-imation of our ILP model, which is more efficientwhen larger numbers of facts need to be expressed.Section 2 discusses previous related work.
Sec-tion 3 defines our ILP model.
Section 4 presentsour experimentals.
Section 5 concludes.2 Related workMarciniak and Strube (2005) propose a generalILP approach for language processing applicationswhere the decisions of classifiers that considerparticular, but co-dependent, subtasks need to becombined.
They also show how their approachcan be used to generate multi-sentence route di-rections, in a setting with very different inputs andprocessing stages than the ones we consider.Barzilay and Lapata (2005) treat content selec-tion as an optimization problem.
Given a pool offacts and scores indicating their importance, theyselect the facts to express by formulating an op-timization problem similar to energy minimiza-tion.
The problem is solved by applying a minimalcut partition algorithm to a graph representing thepool of facts and the importance scores.
The im-portance scores of the facts are obtained via super-vised machine learning (AdaBoost) from a datasetof (sports) facts and news articles expressing them.In other work, Barzilay and Lapata (2006) con-sider sentence aggregation.
Given a set of factsthat a content selection stage has produced, aggre-gation is viewed as the problem of partitioning thefacts into optimal subsets.
Sentences expressingfacts of the same subset are aggregated to form alonger sentence.
The optimal partitioning maxi-mizes the pairwise similarity of the facts in eachsubset, subject to constraints that limit the numberof subsets and the number of facts in each sub-set.
A Maximum Entropy classifier predicts thesemantic similarity of each pair of facts, and anILP model is used to find the optimal partitioning.Althaus et al(2004) show that ordering a set ofsentences to maximize local coherence is equiva-lent to the traveling salesman problem and, hence,NP-complete.
They also show an ILP formulationof the problem, which can be solved efficiently inpractice using branch-and-cut with cutting planes.Kuznetsova et al(2012) use ILP to generate im-age captions.
They train classifiers to detect theobjects in each image.
Having identified the ob-jects of a given image, they retrieve phrases fromthe captions of a corpus of images, focusing onthe captions of objects that are similar (color, tex-ture, shape) to the ones in the given image.
Toselect which objects of the image to report andin what order, Kuznetsova et almaximize (viaILP) the mean of the confidence scores of the ob-ject detection classifiers and the sum of the co-occurrence probabilities of the objects that will bereported in adjacent positions in the caption.
Hav-ing decided which objects to report and their order,Kuznetsova et aluse a second ILP model to decidewhich phrases to use for each object and to orderthe phrases.
The second ILP model maximizes theconfidence of the phrase retrieval algorithm andthe local cohesion between subsequent phrases.Joint optimization ILP models have also beenused in multi-document text summarization andsentence compression (McDonald, 2007; Clarkeand Lapata, 2008; Berg-Kirkpatrick et al 2011;Galanis et al 2012; Woodsend and Lapata, 2012),where the input is text, not formal knowledge rep-resetations.
Statistical methods to jointly performcontent selection, lexicalization, and surface real-ization have also been proposed in NLG (Liang et52al., 2009; Konstas and Lapata, 2012a; Konstas andLapata, 2012b), but they are currently limited togenerating single sentences from flat records, asopposed to ontologies.
Our method is the first oneto consider content selection, lexicalization, andsentence aggregation as an ILP joint optimizationproblem in the context of multi-sentence concept-to-text generation.3 Our ILP model of NLGLet F = {f1, .
.
.
, fn} be the set of all the facts fi(OWL axioms) about the individual or class to bedescribed.
OWL axioms can be represented as setsof RDF triples of the form ?S,R,O?, where S is anindividual or class, O is another individual, class,or datatype value, and R is a relation (property)that connects S to O.1 Hence, we can assume thateach fact fi is a triple ?Si, Ri, Oi?.2For each fact fi, a set Pi = {pi1, pi2, .
.
.
}of alternative sentence plans is available.
Eachsentence plan pik specifies how to express fi =?Si, Ri, Oi?
as an alternative single sentence.
Inour work, a sentence plan is a sequence of slots,along with instructions specifying how to fill theslots in; and each sentence plan is associatedwith the relations it can express.
For example,?exhibit12,foundIn,athens?
could be ex-pressed using a sentence plan like ?
[ref (S)][findpast] [in] [ref (O)]?, where square bracketsdenote slots, ref (S) and ref (O) are instructionsrequiring referring expressions for S and O inthe corresponding slots, and ?findpast?
requires thesimple past form of ?find?.
In our example, thesentence plan would lead to a sentence like ?Ex-hibit 12 was found in Athens?.
We call elementsthe slots with their instructions, but with ?S?and ?O?
accompanied by the individuals, classes,or datatype values they refer to; in our exam-ple, the elements are ?
[ref (S: exhibit12)]?,?
[findpast]?, ?
[in]?, ?
[ref (O: athens)]?.Different sentence plans may lead to more orfewer aggregation opportunities; e.g., sentenceswith the same verb are easier to aggregate.
Weuse aggregation rules similar to those of Dalianis(1999), which operate on sentence plans and usu-ally lead to shorter texts, as in the example below.Bancroft Chardonnay is a kind of Chardonnay.
It is1See www.w3.org/TR/owl2-mapping-to-rdf/.2We actually convert the RDF triples to simpler messagetriples, so that each message triple can be easily expressed bya simple sentence, but we do not discuss this conversion here.made in Bancroft.
?
Bancroft Chardonnay is a kindof Chardonnay made in Bancroft.Let s1, .
.
.
, sm be disjoint subsets of F , eachcontaining 0 to n facts, with m < n. A singlesentence is generated for each subset sj by aggre-gating the sentences (more precisely, the sentenceplans) expressing the facts of sj .3 An empty sjgenerates no sentence, i.e., the resulting text canbe at most m sentences long.
Let us also define:ai ={1, if fact fi is selected0, otherwise (1)likj =??
?1, if sentence plan pik is used to expressfact fi, and fi is in subset sj0, otherwise(2)btj ={1, if element et is used in subset sj0, otherwise (3)and let B be the set of all the distinct elements (noduplicates) from all the available sentence plansthat can express the facts of F .
The length of anaggregated sentence resulting from a subset sj canbe roughly estimated by counting the distinct el-ements of the sentence plans that have been cho-sen to express the facts of sj ; elements that occurmore than once in the chosen sentence plans of sjare counted only once, because they will probablybe expressed only once, due to aggregation.Our objective function (4) maximizes the to-tal importance of the selected facts (or simply thenumber of selected facts, if all facts are equallyimportant), and minimizes the number of distinctelements in each subset sj , i.e., the approximatelength of the corresponding aggregated sentence;an alternative explanation is that by minimizingthe number of distinct elements in each sj , we fa-vor subsets that aggregate well.
By a and b wejointly denote all the ai and btj variables.
Thetwo parts of the objective function are normalizedto [0, 1] by dividing by the total number of avail-able facts |F | and the number of subsets m timesthe total number of distinct elements |B|.
We as-sume that the importance scores imp(fi) are pro-vided by a separate component (Barzilay and La-pata, 2005; Demir et al 2010) and range in [0, 1].The parameters ?1, ?2 are used to tune the prior-ity given to expressing many important facts vs.3All the sentences of every possible subset sj can be ag-gregated, because all the sentences share the same subject,the class or individual being described.
If multiple aggrega-tion rules apply, we use the one that leads to a shorter text.53generating shorter texts; we set ?1 + ?2 = 1.maxa,b?1 ?|F |?i=1ai ?
imp(fi)|F |?
?2 ?m?j=1|B|?t=1btjm ?
|B|(4)subject to:ai =m?j=1|Pi|?k=1likj , for i = 1, .
.
.
, n (5)?et?Bikbtj ?
|Bik| ?
likj , fori = 1, .
.
.
, nj = 1, .
.
.
,mk = 1, .
.
.
, |Pi|(6)?pik?P (et)likj ?
btj , fort = 1, .
.
.
, |B|j = 1, .
.
.
,m (7)|B|?t=1btj ?
Bmax, for j = 1, .
.
.
,m (8)|Pi|?k=1likj +|Pi?
|?k?=1li?k?j ?
1, forj = 1, .
.
.
,m, i = 2, .
.
.
, ni?
= 1, .
.
.
, n?
1; i 6= i?section(fi) 6= section(f ?i)(9)Constraint 5 ensures that for each selected fact,only one sentence plan in only one subset is se-lected; if a fact is not selected, no sentence planfor the fact is selected either.
|?| denotes the car-dinality of a set ?.
In constraint 6, Bik is the set ofdistinct elements et of the sentence plan pik.
Thisconstraint ensures that if pik is selected in a subsetsj , then all the elements of pik are also present insj .
If pik is not selected in sj , then some of its el-ements may still be present in sj , if they appear inanother selected sentence plan of sj .In constraint 7, P (et) is the set of sentence plansthat contain element et.
If et is used in a subset sj ,then at least one of the sentence plans of P (et)must also be selected in sj .
If et is not used in sj ,then no sentence plan of P (et) may be selected insj .
Lastly, constraint 8 limits the number of ele-ments that a subset sj can contain to a maximumallowed number Bmax, in effect limiting the max-imum length of an aggregated sentence.We assume that each relation R has been man-ually mapped to a single topical section; e.g., re-lations expressing the color, body, and flavor ofa wine may be grouped in one section, and rela-tions about the wine?s producer in another.
Thesection of a fact fi = ?Si, Ri, Oi?
is the sectionof its relation Ri.
Constraint 9 ensures that factsfrom different sections will not be placed in thesame subset sj , to avoid unnatural aggregations.4 ExperimentsWe used NaturalOWL (Galanis and Androutsopou-los, 2007; Galanis et al 2009; Androutsopou-los et al 2013), an NLG system for OWL on-tologies that relies on a pipeline of content selec-tion, text planning, lexicalization, aggregation, re-ferring expression generation, and surface realiza-tion components.4 We modified the content selec-tion, lexicalization, and aggregation componentsto use our ILP model, maintaining the aggrega-tion rules of the original system.
For referring ex-pressions and surface realization, the new system,called ILPNLG, invokes the corresponding compo-nents of the original system.
We use branch-and-cut to solve the ILP problems.5The original system, hereafter called PIPELINE,assumes that each relation has been mapped to atopical section, as in ILPNLG.
It also assumes thata manually specified order of the sections and therelations of each section is available, which is usedby the text planner to order the selected facts (bytheir relations).
The subsequent components of thepipeline are not allowed to change the order of thefacts, and aggregation operates only on sentenceplans of adjacent facts from the same section.
InILPNLG, the manually specified order of sectionsand relations is used to order the sentences of eachsubset sj (before aggregating them), the aggre-gated sentences in each section (each aggregatedsentence inherits the minimum order of its con-stituents), and the sections (with their sentences).4.1 Experiments with the Wine OntologyIn a first set of experiments, we used the Wine On-tology, which had also been used in previous ex-periments with PIPELINE (Androutsopoulos et al2013).
The ontology contains 63 wine classes, 52wine individuals, a total of 238 classes and indi-viduals (including wineries, regions, etc.
), and 14properties.6 We kept the 2 topical sections, theordering of sections and relations, and the sen-tence plans of the previous experiments, but weadded more sentence plans to ensure that 3 sen-tence plans were available per relation.
We gen-erated English texts for the 52 wine individuals4All the software and data that we used will befreely available from http://nlp.cs.aueb.gr/software.html.
We use version 2 of NaturalOWL.5We use the branch-and-cut implementation of GLPK withmixed integer rounding, mixed cover, and clique cuts; seesourceforge.net/projects/winglpk/.6See www.w3.org/TR/owl-guide/wine.rdf.54of the ontology; we did not experiment with textsdescribing classes, because we could not think ofmultiple alternative sentence plans for many oftheir axioms.
For each wine individual, there were5 facts on average and a maximum of 6 facts.
Weset the importance scores imp(fi) of all the factsfi to 1, to make the decisions of PIPELINE andILPNLG easier to understand; both systems use thesame importance scores.
PIPELINE does not pro-vide any mechanism to estimate the importancescores, assuming that they are provided manually.PIPELINE has a parameter M specifying themaximum number of facts it is allowed to reportper text.
When M is smaller than the number ofavailable facts (|F |) and all the facts are treatedas equally important, as in our experiments, it se-lects randomly M of the available facts.
We re-peated the generation of PIPELINE?s texts for the52 individuals for M = 2, 3, 4, 5, 6.
For each M ,the texts of PIPELINE for the 52 individuals weregenerated three times, each time using one of thedifferent alternative sentence plans of each rela-tion.
We also generated the texts using a variant ofPIPELINE, dubbed PIPELINESHORT, which alwaysselects the shortest (in elements) sentence planamong the available ones.
In all cases, PIPELINEand PIPELINESHORT were allowed to form ag-gregated sentences containing up to Bmax = 22distinct elements, which was the number of dis-tinct elements of the longest aggregated sentencein the previous experiments (Androutsopoulos etal., 2013), where PIPELINE was allowed to aggre-gate up to 3 original sentences.7With ILPNLG, we repeated the generation of thetexts of the 52 individuals using different valuesof ?1 (?2 = 1 ?
?1), which led to texts express-ing from zero to all of the available facts.
We setthe maximum number of fact subsets to m = 3,which was the maximum number of (aggregated)sentences in the texts of PIPELINE and PIPELI-NESHORT.
Again, we set Bmax = 22.We compared ILPNLG to PIPELINE and PIPELI-NESHORT by measuring the average number offacts they reported divided by the average textlength (in words).
Figure 1 shows this ratio as afunction of the average number of reported facts,along with 95% confidence intervals (of samplemeans).
PIPELINESHORT achieved better resultsthan PIPELINE, but the differences were small.For ?1 < 0.2, ILPNLG produces empty texts,7We modified the two pipeline systems to count elements.Figure 1: Facts/words of Wine Ontology texts.because it focuses on minimizing the number ofdistinct elements of each text.
For ?1 ?
0.225,it performs better than the other systems.
For?1 ?
0.3, it obtains the highest fact/words ratioby selecting the facts and sentence plans that leadto the most compressive aggregations.
For greatervalues of ?1, it selects additional facts whose sen-tence plans do not aggregate that well, which iswhy the ratio declines.
For small numbers of facts,the two pipeline systems select facts and sentenceplans that offer few aggregation opportunities; asthe number of selected facts increases, some moreaggregation opportunities arise, which is why thefacts/words ratio of the two systems improves.
Inall the experiments, the ILP solver was very fast(average: 0.08 sec, worst: 0.14 sec per text).We show below texts produced by PIPELINE(M = 4) and ILPNLG (?1 = 0.3).PIPELINE: This is a strong Sauternes.
It is made from Semil-lon grapes and it is produced by Chateau D?ychem.ILPNLG: This is a strong Sauternes.
It is made from Semillongrapes by Chateau D?ychem.PIPELINE: This is a full Riesling and it has moderate flavor.It is produced by Volrad.ILPNLG: This is a full sweet moderate Riesling.In the first pair, PIPELINE uses different verbs forthe grapes and producer, whereas ILPNLG uses thesame verb, which leads to a more compressive ag-gregation; both texts describe the same wine andreport 4 facts.
In the second pair, ILPNLG has cho-sen to express the sweetness instead of the pro-ducer, and uses the same verb (?be?)
for all thefacts, leading to a shorter sentence; again bothtexts describe the same wine and report 4 facts.In both examples, some facts are not aggregatedbecause they belong in different sections.We also wanted to investigate the effect that thehigher facts/words ratio of ILPNLG has on the per-ceived quality of the generated texts, comparedto the texts of the pipeline.
We were concernedthat the more compressive aggregations of ILPNLG55Criteria PIPELINESHORT ILPNLGSentence fluency 4.75 ?
0.21 4.85 ?
0.10Text structure 4.94 ?
0.06 4.88 ?
0.14Clarity 4.77 ?
0.18 4.75 ?
0.15Overall 4.52 ?
0.20 4.60 ?
0.18Table 1: Human scores for Wine Ontology texts.might lead to sentences that sound less fluent orunnatural, though aggregation often helps producemore natural texts.
We were also concerned thatthe more compact texts of ILPNLG might be per-ceived as being more difficult to understand (lessclear) or less well-structured.
To investigate theseissues, we showed the 52 ?
2 = 104 texts ofPIPELINESHORT (M = 4) and ILPNLG (?1 = 0.3)to 6 computer science students not involved in thework of this article; they were all fluent, thoughnot native, English speakers.
Each one of the 104texts was given to exactly one student.
Each stu-dent was given approximately 9 randomly selectedtexts of each system.
The OWL statements that thetexts were generated from were not shown, and thestudents did not know which system had generatedeach text.
Each student was shown all of his/hertexts in random order, regardless of the system thatgenerated them.
The students were asked to scoreeach text by stating how strongly they agreed ordisagreed with statements S1?S3 below.
A scalefrom 1 to 5 was used (1: strong disagreement, 3:ambivalent, 5: strong agreement).
(S1) Sentence fluency: The sentences of the text are fluent,i.e., each sentence on its own is grammatical and sounds nat-ural.
When two or more smaller sentences are combined toform a single, longer sentence, the resulting longer sentenceis also grammatical and sounds natural.
(S2) Text structure: The order of the sentences is appro-priate.
The text presents information by moving reasonablyfrom one topic to another.
(S3) Clarity: The text is easy to understand, provided thatthe reader is familiar with basic wine terms.The students were also asked to provide an over-all score (1?5) per text.
We did not score referringexpressions, since both systems use the same com-ponent to generate them.Table 1 shows the average scores of the twosystems with 95% confidence intervals (of sam-ple means).
For each criterion, the best score isshown in bold.
The sentence fluency and over-all scores of ILPNLG are slightly higher than thoseof PIPELINESHORT, whereas PIPELINESHORT ob-tained a slightly higher score for text structure andclarity.
The differences, however, are very small,especially in clarity, and there is no statisticallysignificant difference between the two systems inany of the criteria.8 Hence, there was no evidencein these experiments that the highest facts/wordsratio of ILPNLG comes at the expense of lower per-ceived text quality.
We investigated these issuesfurther in a second set of experiments, discussednext, where the generated texts were longer.4.2 Consumer Electronics experimentsIn the second set of experiments, we used theConsumer Electronics Ontology, which had alsobeen used in previous work with PIPELINE.
Theontology comprises 54 classes and 441 individ-uals (e.g., printer types, paper sizes), but no in-formation about particular products.9 In previ-ous work, 30 individuals (10 digital cameras, 10camcorders, 10 printers) were added to the ontol-ogy; they were randomly selected from a publiclyavailable dataset of 286 digital cameras, 613 cam-corders, and 58 printers, whose instances complywith the Consumer Electronics Ontology.10 Wekept the 6 topical sections, the ordering of sec-tions and relations, and the sentence plans of theprevious work, but we added more sentence plansto ensure that 3 sentence plans were available foralmost every relation; for some relations we couldnot think of enough sentence plans.
Again, we setthe importance scores of all the facts to 1.We generated texts with PIPELINE and PIPELI-NESHORT for the 30 individuals, for M =3, 6, 9, .
.
.
, 21.
Again for each M , the texts ofPIPELINE were generated three times, each timeusing one of the different alternative sentenceplans of each relation.
PIPELINE and PIPELI-NESHORT were allowed to form aggregated sen-tences containing up to Bmax = 39 distinct ele-ments, which was the number of distinct elementsof the longest aggregated sentence in the previouswork with this ontology, where PIPELINE was al-lowed to aggregate up to 3 original sentences.
Wealso set Bmax = 39 in ILPNLG.There are 14 facts (F ) on average and a max-imum of 21 facts for each one of the 30 individ-uals, compared to the 5 facts on average and themaximum of 6 facts of the experiments with theWine Ontology.
Hence, the texts of the Consumer8The confidence intervals do not overlap, and we also per-formed paired two-tailed t-tests (?
= 0.05) to check for sta-tistical significance.
In previous work, where judges wereasked to score texts using the same criteria, inter-annotatoragreement was strong (sample Pearson correlation r ?
0.91).9Ontology available from www.ebusiness-unibw.org/ontologies/consumerelectronics/v1.10See rdf4ecommerce.esolda.com/.56Figure 2: Average solver times for ILPNLG for dif-ferent maximum numbers of fact subsets (m).Electronics Ontology are much longer, when theyreport all the available facts.
To generate texts forthe 30 individuals with ILPNLG, we would haveto set the maximum number of fact subsets tom = 10, which was the maximum number of (ag-gregated) sentences in the texts of PIPELINE andPIPELINESHORT.
The number of variables of ourILP model, however, grows exponentially tom andthe number of available facts |F |.
Figure 2 showsthe average time the ILP solver took for differentvalues ofm in the experiments with the ConsumerElectronics ontology; the results are also averagedfor ?1 = 0.4, 0.5, 0.6 (?2 = 1 ?
?1).
For m = 4,the solver took 1 minute and 47 seconds on av-erage per text; recall that |F | is also much largernow, compared to the experiments of the previoussection.
Form = 5, the solver was so slow that weaborted the experiment.
Figure 3 shows the aver-age solver time for different numbers of availablefacts |F |, for m = 3; in this case, we modifiedthe set of available facts (F ) of every individualto contain 3, 6, 9, 12, 15, 18, 21 facts; the resultsare averaged for ?1 = 0.4, 0.5, 0.6.
Althoughthe times of Fig.
3 also grow exponentially, theyremain under 4 seconds, showing that the mainproblem for ILPNLG is m, the number of fact sub-sets, which is also the maximum allowed numberof (aggregated) sentences of each text.To be able to efficiently generate texts withlarger m values, we use a variant of ILPNLG,called ILPNLGAPPROX, which considers each factsubset separately.
ILPNLGAPPROX starts with thefull set of available facts (F ) and uses our ILPmodel (Section 3) with m = 1 to produce the first(aggregated) sentence of the text.
It then removesthe facts expressed by the first (aggregated) sen-tence from F , and uses the ILP model, again withFigure 3: Average solver times for ILPNLG for dif-ferent numbers of available facts (|F |) andm = 3.Figure 4: Avg.
solver times for ILPNLGAPPROXfor different max.
numbers of fact subsets (m).m = 1, to produce the second (aggregated) sen-tence etc.
This process is repeated until we pro-duce the maximum number of allowed aggregatedsentences, or until we run out of facts.
ILPNLGAP-PROX is an approximation of ILPNLG, in the sensethat it does not consider all the fact subsets jointlyand, hence, does not guarantee finding a globallyoptimal solution for the entire text.
Figures 4?5show the average solver times of ILPNLGAPPROXfor different values of m and |F |; all the other set-tings are as in Figures 2?3.
The solver times ofILPNLGAPPROX grow approximately linearly tomand |F | and are under 0.3 seconds in all cases.Figure 6 shows the average facts/words ratio ofILPNLGAPPROX (m = 10), PIPELINE and PIPELI-NESHORT, along with 95% confidence intervals(of sample means), for the texts of the 30 individu-als.
Again, PIPELINESHORT achieves slightly bet-ter results than PIPELINE, but the differences arenow smaller (cf.
Fig.
1).
ILPNLGAPPROX behavesvery similarly to ILPNLG in the Wine Ontology ex-periments (cf.
Fig.
1); for ?1 ?
0.35, it producesempty texts, while for ?1 ?
0.4 it performs betterthan the other systems.
ILPNLGAPPROX obtains57Figure 5: Avg.
solver times for ILPNLGAPPROXfor different |F | values and m = 3.Figure 6: Facts/words for Consumer Electronics.the highest facts/words ratio for ?1 = 0.45, whereit selects the facts and sentence plans that lead tothe most compressive aggregations.
For greatervalues of ?1, it selects additional facts whose sen-tence plans do not aggregate that well, which iswhy the ratio declines.
The two pipeline systemsselect facts and sentence plans that offer very fewaggregation opportunities; as the number of se-lected facts increases, some more aggregation op-portunities arise, which is why the facts/words ra-tio of the two systems improves slightly, thoughthe improvement is now hardly noticeable.We show below two example texts produced byPIPELINE (M = 6) and ILPNLGAPPROX (?1 =0.45).
Both texts report 6 facts, but ILPNLGAP-PROX has selected facts and sentence plans thatallow more compressive aggregations.
Recall thatwe treat all the facts as equally important.PIPELINE: Sony DCR-TRV270 requires minimum illumina-tion of 4.0 lux and its display is 2.5 in.
It features a sportsscene mode, it includes a microphone and an IR remote con-trol.
Its weight is 780.0 grm.ILPNLGAPPROX: Sony DCR-TRV270 has a microphone andan IR remote control.
It is 98.0 mm high, 85.0 mm wide,151.0 mm deep and it weighs 780.0 grm.We showed the 30 ?
2 = 60 texts of PIPELI-NESHORT (M = 6) and ILPNLGAPPROX (?1 =Criteria PIPELINESHORT ILPNLGAPPROXSentence fluency 4.50 ?
0.30 4.87 ?
0.12Text structure 4.33 ?
0.36 4.73 ?
0.22Clarity 4.53 ?
0.29 4.97 ?
0.06Overall 4.10 ?
0.31 4.73 ?
0.16Table 2: Human scores for Consumer Electronics.0.45) to the same 6 students, as in Section 4.1.Again, each text was given to exactly one student.Each student was given approximately 5 randomlyselected texts of each system.
The OWL statementsthat the texts were generated from were not shown,and the students did not know which system hadgenerated each text.
Each student was shown allof his/her texts in random order, regardless of thesystem that generated them.
The students wereasked to score each text by stating how stronglythey agreed or disagreed with statements S1?S3,as in Section 4.1.
They were also asked to providean overall score (1?5) per text.Table 2 shows the average scores of the twosystems with 95% confidence intervals (of sam-ple means).
For each criterion, the best score isshown in bold; the confidence interval of the bestscore is also shown in bold, if it does not overlapwith the confidence interval of the other system.Unlike the Wine Ontology experiments, the scoresof our ILP approach are now higher than those ofthe pipeline in all of the criteria, and the differ-ences are also larger, though the differences arestatistically significant only for clarity and over-all quality.11 We attribute these differences to thefact that the texts are now longer and the sentenceplans more varied, which often makes the texts ofthe pipeline sound verbose and, hence, more diffi-cult to follow, compared to the more compact textsof ILPNLGAPPROX, which sound more concise.Overall, the human scores of the experimentswith the two ontologies suggest that the higherfacts/words ratio of our ILP approach does notcome at the expense of lower perceived text qual-ity.
On the contrary, the texts of the ILP approachmay be perceived as clearer and overall better thanthose of the pipeline, when the texts are longer.5 ConclusionsWe presented an ILP model of content selection,lexicalization, and aggregation that jointly con-siders the possible choices in the three stages, to11When two confidence intervals do not overlap, the dif-ference is statistically significant.
When they overlap, thedifference may still be statistically significant; we performedadditional paired two-tailed t-tests (?
= 0.05) in those cases.58avoid greedy local decisions and produce morecompact texts.
The model has been embedded inNaturalOWL, a NLG system for OWL ontologies,which used a pipeline architecture in its originalform.
Experiments with two ontologies confirmedthat our approach leads to expressing more factsper word, with no deterioration in the perceivedtext quality; the ILP approach may actually lead totexts perceived as clearer and overall better, com-pared to the pipeline, when there are many factsto express.
We also presented an approximationof our ILP model, which allows longer texts tobe generated efficiently.
We plan to extend ourmodel to include text planning, referring expres-sion generation, and mechanisms to obtain impor-tance scores.AcknowledgmentsThis research has been co-financed by the Euro-pean Union (European Social Fund ?
ESF) andGreek national funds through the Operational Pro-gram ?Education and Lifelong Learning?
of theNational Strategic Reference Framework (NSRF)?
Research Funding Program: Heracleitus II.
In-vesting in knowledge society through the Euro-pean Social Fund.ReferencesE.
Althaus, N. Karamanis, and A. Koller.
2004.
Com-puting locally coherent discourses.
In 42nd AnnualMeeting of ACL, pages 399?406, Barcelona, Spain.I.
Androutsopoulos, G. Lampouras, and D. Gala-nis.
2013.
Generating natural language descrip-tions from OWL ontologies: the NaturalOWL sys-tem.
Technical report, Natural Language ProcessingGroup, Department of Informatics, Athens Univer-sity of Economics and Business.G.
Antoniou and F. van Harmelen.
2008.
A SemanticWeb primer.
MIT Press, 2nd edition.F.
Baader, D. Calvanese, D.L.
McGuinness, D. Nardi,and P.F.
Patel-Schneider, editors.
2002.
The De-scription Logic Handbook.
Cambridge Univ.
Press.R.
Barzilay and M. Lapata.
2005.
Collective contentselection for concept-to-text generation.
In HLT-EMNLP, pages 331?338, Vancouver, BC, Canada.R.
Barzilay and M. Lapata.
2006.
Aggregation viaset partitioning for natural language generation.
InHLT-NAACL, pages 359?366, New York, NY.A.
Belz.
2008.
Automatic generation of weatherforecast texts using comprehensive probabilisticgeneration-space models.
Natural Language Engi-neering, 14(4):431?455.T.
Berg-Kirkpatrick, D. Gillick, and D. Klein.
2011.Jointly learning to extract and compress.
In 49thMeeting of ACL, pages 481?490, Portland, OR.T.
Berners-Lee, J. Hendler, and O. Lassila.
2001.
TheSemantic Web.
Scientific American, May:34?43.K.
Bontcheva.
2005.
Generating tailored textual sum-maries from ontologies.
In 2nd European SemanticWeb Conf., pages 531?545, Heraklion, Greece.J.
Clarke and M. Lapata.
2008.
Global inference forsentence compression: An integer linear program-ming approach.
Journal of Artificial Intelligence Re-search, 1(31):399?429.H.
Dalianis.
1999.
Aggregation in natural languagegeneration.
Comput.
Intelligence, 15(4):384?414.L.
Danlos.
1984.
Conceptual and linguistic decisionsin generation.
In 10th COLING, pages 501?504,Stanford, CA.S.
Demir, S. Carberry, and K.F.
McCoy.
2010.A discourse-aware graph-based content-selectionframework.
In 6th Int.
Nat.
Lang.
Generation Con-ference, pages 17?25, Trim, Co. Meath, Ireland.D.
Galanis and I. Androutsopoulos.
2007.
Generatingmultilingual descriptions from linguistically anno-tated OWL ontologies: the NaturalOWL system.
In11th European Workshop on Natural Lang.
Genera-tion, pages 143?146, Schloss Dagstuhl, Germany.D.
Galanis, G. Karakatsiotis, G. Lampouras, and I. An-droutsopoulos.
2009.
An open-source natural lan-guage generator for OWL ontologies and its use inProte?ge?
and Second Life.
In 12th Conf.
of the Euro-pean Chapter of ACL (demos), Athens, Greece.D.
Galanis, G. Lampouras, and I. Androutsopoulos.2012.
Extractive multi-document summarizationwith ILP and Support Vector Regression.
In COL-ING, pages 911?926, Mumbai, India.B.C.
Grau, I. Horrocks, B. Motik, B. Parsia, P. Patel-Schneider, and U. Sattler.
2008.
OWL 2: The nextstep for OWL.
Web Semantics, 6:309?322.I.
Konstas and M. Lapata.
2012a.
Concept-to-text gen-eration via discriminative reranking.
In 50th AnnualMeeting of ACL, pages 369?378, Jeju Island, Korea.I.
Konstas and M. Lapata.
2012b.
Unsupervisedconcept-to-text generation with hypergraphs.
InHLT-NAACL, pages 752?761, Montre?al, Canada.P.
Kuznetsova, V. Ordonez, A. Berg, T. Berg, andY.
Choi.
2012.
Collective generation of natural im-age descriptions.
In 50th Annual Meeting of ACL,pages 359?368, Jeju Island, Korea.59P.
Liang, M. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
In47th Meeting of ACL and 4th AFNLP, pages 91?99,Suntec, Singapore.S.F.
Liang, R. Stevens, D. Scott, and A. Rector.
2011.Automatic verbalisation of SNOMED classes usingOntoVerbal.
In 13th Conf.
AI in Medicine, pages338?342, Bled, Slovenia.T.
Marciniak and M. Strube.
2005.
Beyond thepipeline: Discrete optimization in NLP.
In 9th Con-ference on Computational Natural Language Learn-ing, pages 136?143, Ann Arbor, MI.R.
McDonald.
2007.
A study of global inference al-gorithms in multi-document summarization.
In Eu-ropean Conference on Information Retrieval, pages557?564, Rome, Italy.C.
Mellish and J.Z.
Pan.
2008.
Natural language di-rected inference from ontologies.
Artificial Intelli-gence, 172:1285?1315.C.
Mellish and X.
Sun.
2006.
The Semantic Web as alinguistic resource: opportunities for nat.
lang.
gen-eration.
Knowledge Based Systems, 19:298?303.E.
Reiter and R. Dale.
2000.
Building Natural Lan-guage Generation Systems.
Cambridge Univ.
Press.R.
Schwitter, K. Kaljurand, A. Cregan, C. Dolbear, andG.
Hart.
2008.
A comparison of three controllednat.
languages for OWL 1.1.
In 4th OWL Experi-ences and Directions Workshop, Washington DC.R.
Schwitter.
2010.
Controlled natural languages forknowledge representation.
In 23rd COLING, pages1113?1121, Beijing, China.N.
Shadbolt, T. Berners-Lee, and W. Hall.
2006.The Semantic Web revisited.
IEEE Intell.
Systems,21:96?101.M.A.
Walker, O. Rambow, and M. Rogati.
2001.
Spot:A trainable sentence planner.
In 2nd Annual Meet-ing of NAACL, pages 17?24, Pittsburgh, PA.S.
Williams, A.
Third, and R. Power.
2011.
Levelsof organization in ontology verbalization.
In 13thEuropean Workshop on Natural Lang.
Generation,pages 158?163, Nancy, France.K.
Woodsend and M. Lapata.
2012.
Multiple as-pect summarization using ILP.
In EMNLP-CoNLL,pages 233?243, Jesu Island, Korea.60
