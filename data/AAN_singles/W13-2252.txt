Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 408?413,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsAre ACT?s scores increasing with better translation quality?Najeh HajlaouiIdiap Research InstituteRue Marconi 19CH-1920 Martigny SwitzerlandNajeh.Hajlaoui@idiap.chAbstractThis paper gives a detailed description ofthe ACT (Accuracy of Connective Trans-lation) metric, a reference-based metricthat assesses only connective translations.ACT relies on automatic word-level align-ment (using GIZA++) between a sourcesentence and respectively the referenceand candidate translations, along withother heuristics for comparing translationsof discourse connectives.
Using a dictio-nary of equivalents, the translations arescored automatically or, for more accu-racy, semi-automatically.
The accuracy ofthe ACT metric was assessed by humanjudges on sample data for English/French,English/Arabic, English/Italian and En-glish/German translations; the ACT scoresare within 2-5% of human scores.The actual version of ACT is availableonly for a limited language pairs.
Conse-quently, we are participating only for theEnglish/French and English/German lan-guage pairs.
Our hypothesis is that ACTmetric scores increase with better transla-tion quality in terms of human evaluation.1 IntroductionDiscourse connectives should preserve their senseduring translation, as they are often ambiguousand may convey more than one sense dependingon the inter-sentential relation (causality, conces-sion, contrast or temporal).
For instance, sincein English can express temporal simultaneity, butalso a causal sense.In this paper, we present results of different Ma-chine Translation systems for English-to-Frenchand English-to-German pairs.
More specifically,we measure the quality of machine translationsof eight English discourse connectives: although,even though, meanwhile, since, though, while,however, and yet, adopting different approaches.This quality is measured using a dedicated met-ric named ACT (Accuracy of Connective Transla-tion), a reference-based metric that assesses onlyconnective translations.The paper is organized as follows.
In Section 2,we present the ACT metric and its error rate.
Insection 3, we compare the ACT metric to previousmachine translation evaluation metrics.
Finally,we present the results of the different English-to-German and English-to-French MT systems (Sec-tion 4).2 ACT MetricWe described the ACT metric in (Hajlaoui andPopescu-Belis, 2013) and (Hajlaoui and Popescu-Belis, 2012).
Its main idea is to detect, for a givenexplicit source discourse connective, its transla-tion in a reference translation and in a candidatetranslation.
ACT then compares and scores thesetranslations.
To identify the translations, ACT firstuses a dictionary of possible translations of eachdiscourse connective type, collected from trainingdata and validated by humans.
If a reference or acandidate translation contains more than one pos-sible translation of the source connective, align-ment information is used to detect the correct con-nective translation.
If the alignment information isirrelevant (not equal to a connective), it then com-pares the word position (word index) of the sourceconnective alignment with the index in the trans-lated sentence (candidate or reference) and the setof candidate connectives to disambiguate the con-nective?s translation.
Finally, the nearest connec-tive to the alignment is taken.ACT proceeds by checking whether the refer-ence translation contains one of the possible trans-lations of the connective in question.
After that, itsimilarly checks if the candidate translation con-tains a possible translation of the connective.
Fi-408nally, it checks if the reference connective foundis equal (case 1), synonymous (case 2) or incom-patible 1(case 3) to the candidate connective.
Dis-course relations can be implicit in the candidate(case 4), or in the reference (case 5) translation orin both of them (case 6).
These different compar-isons can be represented by the following 6 cases:?
Case 1: same connective in the reference(Ref) and candidate translation (Cand).?
Case 2: synonymous connective in Ref andCand.?
Case 3: incompatible connective in Ref andCand.?
Case 4: source connective translated in Refbut not in Cand.?
Case 5: source connective translated in Candbut not in Ref.?
Case 6: the source connective neither trans-lated in Ref nor in Cand.Based on the connective dictionary categorisedby senses, ACT gives one point for identical (case1) and equivalent translations (case 2), otherwisezero.
ACT proposes a semi-automatic option bymanually checking instances of case 5 and case62.ACT returns the ratio of the total number ofpoints to the number of source connectives ac-cording to the three versions: (1) ACTa countsonly case 1 and case 2 as correct and all otherscases as wrong, (2) ACTa5+6 excludes case 5 andcase 6 and (3) ACTm considers the correct transla-tions found by manual scoring of case 5 and case 6noted respectively case5corr and case6corr to bet-ter consider these implicit cases.ACTa = (| case1 | + | case2 |)/6?i=1| casei | (1)ACTa5 + 6 = (| case1 | + | case2 |)/4?i=1| casei | (2)ACTm = ACTa + (| case5corr | + | case6corr | /6?i=1| casei |)(3)1In terms of connective sense.2We do not check manually case 4 because we observedthat its instances propose generally explicit translations thatdo not belong to our dictionary, it means the SMT systemtends to learn explicit translations for explicit source connec-tive.2.1 Configurations of ACT metricAs shown in Figure 1, ACT can be configured touse an optional disambiguation module.
Two ver-sions of this disambiguation module can be used:(1) without training, which means without sav-ing an alignment model and only using GIZA++as alignment tool; (2) with training and savingan alignment model using MGIZA++ (a multi-threaded version of GIZA++) trained on an exter-nal corpus to align the (Source, Reference) and the(Source, Candidate) data.Figure 1: ACT architectureACT is more accurate using the disambiguationmodule.
We encourage to use the version withouttraining since it only requires the installation ofthe GIZA++ tool.
Based on its heuristics and onits connective dictionaries categorised by senses,ACT has a higher precision to detect the right con-nective when more than one translation is possible.The following example illustrates the usefulnessof the disambiguation module when we have morethan one possible translation of the source con-nective.
Without disambiguation, ACT detects thesame connective si in both target sentences (wrongcase 1), while the right translation of the sourceconnective although is bien que and me?me si re-spectively in the reference and the candidate sen-tence (case 2).Without disambiguation, case 1: Csrc= although,Cref = si, Ccand = siWith disambiguation, case 2: Csrc= although(concession), Cref = bien que, Ccand = me?me si?
SOURCE: we did not have it so bad in irelandthis time although we have had many seriouswind storms on the atlantic .409?
REFERENCE: cette fois-ci en irlande .
ce n?e?tait pas si grave .
bien que de nombreusestempe?tes violentes aient se?vi dans l?
atlan-tique .?
CANDIDATE: nous n?
e?tait pas si mauvaiseen irlande .
cette fois .
me?me si nous avonseu vent de nombreuses graves tempe?tes surles deux rives de l?
atlantique .In the following experiments, we used the rec-ommended configuration of ACT (without train-ing).2.2 Error rate of the ACT metricACT is a free open-source Perl script licensed un-der GPL v33.
It has a reasonable and accept-able error score when comparing its results tohuman judgements (Hajlaoui and Popescu-Belis,2013).
Its accuracy was assessed by human judgeson sample data for English-to-French, English-to-Arabic, English-to-Italian and English-to-Germantranslations; the ACT scores are within 2-5% ofhuman scores.2.3 Multilingual architecture of ACT MetricThe ACT architecture is multilingual: it was ini-tially developed for the English-French languagepair, then ported to English-Arabic, English-Italian and English-German.The main resource needed to port the ACT met-ric to another language pair is the dictionary ofconnectives matching possible synonyms and clas-sifying connectives by sense.
To find these pos-sible translations of a given connective, we pro-posed an automatic method based on a large cor-pus analysis (Hajlaoui and Popescu-Belis, 2012).This method can be used for any language pair.Estimating the effort that would have to be takento port the ACT metric to new language pairs fo-cusing on the same linguistic phenomena mainlydepends on the size of parallel data sets contain-ing the given source connective.
The classifi-cation by sense depends also on the number ofpossible translations detected for a given sourceconnective.
This task is sometimes difficult, assome translations (target connectives) can be asambiguous as the source connective.
Native lin-guistic knowledge of the target language is there-fore needed in order to complete a dictionary withthe main meanings and senses of the connectives.3Available from https://github.com/idiap/act.We think that the same process and the sameeffort can be taken to adapt ACT to new linguisticphenomena (verbs, pronouns, adverbs, etc).3 Related worksACT is different from existing MT metrics.
TheMETEOR metric (Denkowski and Lavie, 2011)uses monolingual alignment between two trans-lations to be compared: a system translation anda reference one.
METEOR performs a mappingbetween unigrams: every unigram in each trans-lation maps to zero or one unigram in the othertranslation.
Unlike METEOR, the ACT metricuses a bilingual alignment (between the source andthe reference sentences and between the sourceand the candidate sentences) and the word posi-tion information as additional information to dis-ambiguate the connective situation in case there ismore than one connective in the target (referenceor candidate) sentence.
ACT may work withoutthis disambiguation.The evaluation metric described in (Max et al2010) indicates for each individual source wordwhich systems (among two or more systems orsystem versions) correctly translated it accordingto some reference translation(s).
This allows car-rying out detailed contrastive analyses at the wordlevel, or at the level of any word class (e.g.
partof speech, homonymous words, highly ambiguouswords relative to the training corpus, etc.).
TheACT metric relies on the independent compari-son of one system?s hypothesis with a reference.An automatic diagnostics of machine translationand based on linguistic checkpoints (Zhou et al2008), (Naskar et al 2011) constitute a differentapproach from our ACT metric.
The approach es-sentially uses the BLEU score to separately eval-uate translations of a set of predefined linguis-tic checkpoints such as specific parts of speech,types of phrases (e.g., noun phrases) or phraseswith a certain function word.
A different ap-proach was proposed by (Popovic and Ney, 2011)to study the distribution of errors over five cate-gories (inflectional errors, reordering errors, miss-ing words, extra words, incorrect lexical choices)and to examine the number of errors in each cat-egory.
This proposal was based on the calcu-lation of Word Error Rate (WER) and Position-independent word Error Rate (PER), combinedwith different types of linguistic knowledge (baseforms, part-of-speech tags, name entity tags, com-410pound words, suffixes, prefixes).
This approachdoes not allow checking synonym words havingthe same meaning like the case of discourse con-nectives.4 ACT-based comparative evaluationWe used the ACT metric to assess connectivetranslations for 21 English-German systems and23 English-French systems.
It was computed ontokenized and lower-cased text using its secondconfiguration ?without training?
(Hajlaoui andPopescu-Belis, 2013).Table 1 shows only ACTa scores for theEnglish-to-German translation systems sinceACTa5+6 gives the same rank as ACTa.
Table 2present the same for the English-to-French sys-tems.
We are not presenting ACTm either becausewe didn?t check manually case 5 and case 6.Metric System Value Avg SDACTacu-zeman.2724 0.772rbmt-3 0.772TUBITAK.2633 0.746KITprimary.2663 0.737StfdNLPG.2764 0.733JHU.2888 0.728LIMSI-N-S-p.2589 0.720online-G 0.720Shef-wproa.2748 0.720RWTHJane.2676 0.711 0.697 0.056uedin-wmt13.2638 0.707UppslaUnv.2698 0.707online-A 0.698rbmt-1 0.694online-B 0.677uedin-syntax.2611 0.672online-C 0.664FDA.2842 0.664MES-reorder.2845 0.664PROMT.2789 0.621rbmt-4 0.513Table 1: Metric scores for all En-De systems:ACTa and ACTa5+6 scores give the same rank;ACT V1.7.
SD is the Standard Deviation.5 ConclusionThe connective translation accuracy of the can-didate systems cannot be measured correctly bycurrent MT metrics such as BLEU and NIST.
Wetherefore developed a new distance-based metric,ACT, to measure the improvement in connectivetranslation.
ACT is a reference-based metric thatonly compares the translations of discourse con-nectives.
It is intended to capture the improvementof an MT system that can deal specifically withdiscourse connectives.Metric System Value Avg SDACTacu-zeman.2724 0.772online-B 0.647LIMSI-N-S.2587 0.647MES.2802 0.647FDA.2890 0.638KITprimary.2656 0.638cu-zeman.2728 0.634online-G 0.634PROMT.2752 0.634uedin-wmt13.2884 0.634MES-infl-pr.2672 0.629StfdNLPGPTP.2765 0.629 0.608 0.04DCUprimary.2827 0.625JHU.2683 0.625online-A 0.621OmniFTEn-to-Fr.2647 0.616RWTHph-Janepr.2639 0.612OFlTEnFr.2645 0.591rbmt-1 0.586Its-LATL.2667 0.565rbmt-3 0.565rbmt-4 0.543Its-LATL.2652 0.543online-C 0.500Table 2: Metric scores for all En-Fr systems:ACTa and ACTa5+6 scores give the same rank;ACT V1.7.
SD is the Standard Deviation.ACT can be also used semi-automatically.
Con-sequently, the scores reflect more accurately theimprovement in translation quality in terms of dis-course connectives.Theoretically, a better system should preservethe sense of discourse connectives.
Our hypothe-sis is thus that ACT scores are increasing with bet-ter translation quality.
We need access the humanrankings of this task to validate if ACT?s scoresindeed correlate with overall translation qualityrankings.AcknowledgmentsWe are grateful to the Swiss National Sci-ence Foundation for its support through theCOMTIS Sinergia Project, n. CRSI22 127510(see www.idiap.ch/comtis/).ReferencesMarine Carpuat and Dekai Wu.
2005.
Word sense dis-ambiguation vs. statistical machine translation.
InProceedings of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics (ACL), pages387?394, Sydney, Australia.Marine Carpuat, Yihai Shen, Xiaofeng Yu, and DekaiWu.
2006.
Toward integrating word sense and en-tity disambiguation into statistical machine transla-411tion.
In Proceedings of the 3rd International Work-shop on Spoken Language Translation (IWSLT),pages 37?44, Kyoto, Japan.Bruno Cartoni and Thomas Meyer.
2012.
ExtractingDirectional and Comparable Corpora from a Multi-lingual Corpus for Translation Studies.
In Proceed-ings of the eighth international conference on Lan-guage Resources and Evaluation (LREC), Istanbul,Turkey.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word sense disambiguation improves statisti-cal machine translation.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics (ACL), pages 33?40, Prague, Czech Re-public.Jonathan Clark, Chris Dyer, Alon Lavie, and NoahSmith.
2011.
Better hypothesis testing for statis-tical machine translation: Controlling for optimizerinstability.
In Proceedings of ACL-HLT 2011 (46thAnnual Meeting of the ACL: Human Language Tech-nologies), Portland, OR.Laurence Danlos and Charlotte Roze.
2011.
Traduc-tion (automatique) des connecteurs de discours.
InActes de la 18e` Confe?rence sur le Traitement Au-tomatique des Langues Naturelles (TALN), Montpel-lier, France.Laurence Danlos, Die?go Antolinos-Basso, Chloe?Braud, and Charlotte Roze.
2012.
Vers lefdtb : French discourse tree bank.
In Actes dela confe?rence conjointe JEP-TALN-RECITAL 2012,volume 2: TALN, pages 471?478, Grenoble, France.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation, Edinburgh, UK.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In Proceed-ings of Interspeech, Brisbane, Australia.Najeh Hajlaoui and Andrei Popescu-Belis.
2012.Translating english discourse connectives into ara-bic: A corpus-based analysis and an evaluatoin met-ric.
In Proceedings of the 4th Workshop on Com-putational Approaches to Arabic Script-based Lan-guages (CAASL) at AMTA 2012, San Diego, CA.Najeh Hajlaoui and Andrei Popescu-Belis.
2013.
As-sessing the accuracy of discourse connective trans-lations: Validation of an automatic metric.
In Pro-ceedings of the 14th International Conference onIntelligent Text Processing and Computational Lin-guistics, Samos, Greece.Hugo Hernault, Danushka Bollegala, and Ishizuka Mit-suru.
2010a.
A semi-supervised approach to im-prove classification of infrequent discourse relationsusing feature vector extension.
In Proceedings ofthe 2010 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 399?409,Cambridge, MA.Hugo Hernault, Helmut Prendinger, David A. duVerle,and Mitsuru Ishizuka.
2010b.
HILDA: A discourseparser using Support Vector Machine classification.Dialogue and Discourse, 3(1):1?33.Alistair Knott and Chris Mellish.
1996.
A feature-based account of the relations signalled by sentenceand clause connectives.
Language and Speech,39(2?3):143?183.Philipp Koehn and Hieu Hoang.
2007.
FactoredTranslation Models.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing (EMNLP) and ComputationalNatural Language Learning (CONLL), pages 868?876, Prague, Czech Republic.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbs.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In Proceedings of 45th Annual Meeting of theAssociation for Computational Linguistics (ACL),Demonstration Session, pages 177?180, Prague,Czech Republic.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofMT Summit X, pages 79?86, Phuket, Thailand.S.
Kolachina, R. Prasad, D. Sharma, and A. Joshi.2012.
Evaluation of discourse relation annotation inthe hindi discourse relation bank.
In Proceedings ofthe eighth international conference on Language Re-sources and Evaluation (LREC), Instanbul, Turkey.A.
Max, J. M. Crego, and Yvon F. 2010.
Contrastivelexical evaluation of machine translation.
In Pro-ceedings of the International Conference on Lan-guage Resources and Evaluation (LREC), Valletta,Malta.K.
Naskar, S., A. Toral, F. Gaspari, and A.
Way.
2011.A framework for diagnostic evaluation of mt basedon linguistic checkpoints.
In Proceedings of MTSummit XIII, Xiamen, China.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics (ACL), pages 160?167,Sapporo, Japan.M.
Popovic and H. Ney.
2011.
Towards automaticerror analysis of machine translation output.
Com-putational Linguistics, 37(4):657?688.M.
Zhou, B. Wang, S. Liu, M. Li, D. Zhang, andT.
Zhao.
2008.
Diagnostic evaluation of ma-chine translation systems using automatically con-structed linguistic check-points.
In Proceedings412of the 22rd International Conference on Compu-tational Linguistics (COLING), pages 1121?1128,Manchester, UK.413
