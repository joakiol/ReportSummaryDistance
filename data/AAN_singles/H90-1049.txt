SPOKEN LANGUAGE UNDERSTANDINGFOR PERSONAL COMPUTERSGeorge M. WhiteDavid NagelApple Computer Inc20525 Mariani AvenueCupertino, CA 95014ABSTRACTAutomatic speech recognition technology will soonallow users to converse with their computers.
Thispaper describes an approach to improve the humaninterface through speech recognition; describes howour research benefits the DARPA Spoken LanguageResearch program; and describes ome researchresults in the area of merging Hidden Markov Models(HMM), and Artificial Neural Nets (ANN).We apply ANN techniques as a post-process toHMMrecognizers such as Sphinx.
We show that treatingANNs as a post-process topartially recognized speechfrom an HMM pre-processor is superior to usingANNs alone or to hybrid systems applying ANNsbefore HMM processing.
A theory explaining theadvantages of applying ANNs as a post-process ispresented along with preliminary results.IMPROVING THE HUMAN INTERFACE WITHSPEECH RECOGNITIONApple's approach is distinguished by its emphasis onconversational communication with personalcomputers as distinct from dictation or command andcontrol only.
It is further distinguished by integrationof speech recognition i to the visual "desk top"metaphor of personal computers.
We believe thatspeech recognition will impact personal computingsooner and more effectively if it is integrated withother I/O modalities such as the mouse, keyboard,visual icons, dialog boxes and perhaps peech output.We expect o bring such integrated systems to marketin the 1990's.Our approach is similar in spirit to notions of AlanSears in his SLIP (speech, language icons, andpointing) paradigm but with some distinctivedifferences.
We will use task domain constraintsprovided by particular application packages onpersonal computers to create constrained naturallanguage understanding.
Furthermore we willimplement interactive voice and text responsemechanisms such as dialog boxes and speechsynthesis to respond to the users input.
We willprovide a conversational natural languageunderstanding within narrow task domains onpersonal computers in which speech is augmentedwith pointing, typing, and mousing around.SPEECH UNDERSTANDING ON PERSONALCOMPUTERSA perennial problem confronting the speechrecognition community has been lack of adequatecomputing power to perform real time recognition andunderstanding.
This shortcoming is being solved, notso much to serve speech interests as it is to serve thecomputing needs of society at large.
It is the naturalprogression of VLSI, economies of scale of massproduced personal computers, and computinginfi'astructures.For personal computer users, speech recognition isparticularly useful in areas where the user isconfronted with too many options to easily managewith function keys or a small number of shift-keycombinations.
The current solution is to use pulldown or pop up menus but these are fast becomingless convenient by shear weight of numbers ofoptions.
Sub-directories of sub-directories arebecoming common.
The arm motion simply to get theinitial menu, and then each submenu, is a limitationon ease-of-use.
Speech recognition can cut throughthe branches of the menu tree to speed throughput as241long as the speech recognition is fast and accurateenough.Speech recognition offers other advantages to the userinterface by allowing many words and phrases tomean the same thing.
If a user forgets a command ordoes not know if one exists, speech recognitionsystems can partially solve this problem by supportingsynonyms and paraphrase.
In addition, when userdefined scripts and macros become numerous, theyare difficult o manage with function keys and shiftkey commands.
Speech recognition allows users toinvoke these macros and scripts with a distinctivename or phrase and avoids function keys altogether.We expect o employ speech in interfaces toeducational programs, tandard computer applications(spreadsheet, word processing, etc.
), multimediasystems, and telephone access ystems.Automated language l arning is another area ofparticular interest to Apple that seems to be yieldingto DARPA sponsored research.
Speech recognitiontechniques are becoming ood enough to time alignknown utterances totemplates for the words in thespeech.
Words that are poorly pronounced can bespotted and students can be directed to repeatoffending words to mimic correctly pronouncedwords from the computer.COMMERCIAL APPLICATIONS OF DARPATECHNOLOGYOur philosophy at Apple is to leverage the efforts ofother companies and researchers by providing them aplatform through which they can commerciallyaddress the needs of personal computer users.
Forexample, we stay out of certain business areas uch asselling application software in order to encourageindependent developers todevelop roducts in theseareas.
In the research area, we stand ready to adoptsystems developed by DARPA contractors and offerthem along side our internally developed systems tocommercial outlets.Apple encourages outside vendors to produce ASRsystems to be promoted or sold by Apple.
We preferto work with those DARPA contractors that maketheir research freely available, but we will alsoconsider licensing technology from the outside if it isbetter than internally developed technology.
Weactively seek partners to supply components tobeused in our own internal ASR systems.For example, we currently have SPHINX working ona MAC which we call MACSPHINX.
This is notcurrently scheduled tobe shipped as a product, but aproduct may be based on MACSPHINX at a latertime.As our contribution tothe underlying technology, weintend to extend Sphinx to give it a speaker dependentmode in which it can learn new words "on the fly".We will initially do this by augmenting Sphinx withANNs as described below.As another example of partnering, we expect o beginbuilding on Victor Zue's work with VOYAGER.
Wewill receive VOYAGER from MIT in a month or tworunning on a MAC platform.
We expect o modify itto run faster and with an order of magnitude l sscomputing power.THE PLUS SPEECH ACCELERATOR PROJECT:In order to make it easier for DARPA contractors touse MACINTOSH computers, and to build speechrecognition systems that would control applicationson MACINTOSHs, we have supported andencouraged Roberto Bisiani to design a "speechaccelerator" for more than a year.
The goal was toallow a MAC to have intimate control over anaccelerator p ocessing unit that would offer between50 and 200 MIPS economically and with broad baseof software support.
This was achieved in an externalbox, named PLUS by its designer Roberto Bisiani,which has a SCSI interface as well as higher speedNU BUS connection toa MAC.
The SCSI interfaceallows the box to be programmed byother computerssuch as SUN computers as well as using a MAC.However, the high speed NU BUS interface to theMAC will allow tighter integration with the MACthan other computers.
The box itself containsMotorola 88000s, one to ten in a single box; and theboxes may be daisy chained.
We hope many of theDARPA contractors in attendance h re will use theaccelerator box to make their spoken languagecommunication systems available to MAC242The identity of the ~ Secondphrase, word, or ,,j..,, Stepphoneme is verif iedby the outp?t / TANNOutputLayerANNHiddenLayerANNInputLayerHMMPHRASE orWORD orPHONEMETemplate10 ms timetime barsSpeechTimeSeriesANN/HMM 1I I I I Ii ~._/ ~_/ ~/  Ste)pI I I1I I IFIG.
1 Nodes in a canonical HMM topology pointing to time intervals in speechtime series and also to the input nodes in an ANN.
The pointers to the timeintervals are established using well known techniques as part of the HMMprocessing in step 1.
Standard ANN techniques are then applied in step 2 to thespeech which has now been time aligned to fixed structure of the HMM.applications.
Development of this box is currentlyfunded by DARPA and will probably be availablelater this year.ANN POST PROCESS TO HMMThe Hidden Markov Model (HMM) approach is thedominant speech recognition paradigm in the ASRfield today.
Millions of dollars have been spent indozens of institutions to explore the contributions ofHMM techniques tospeech recognition.
ArtificialNeural Net (ANN) technology is newer, but it hasalso become heavily funded and widely investigated.It has been only within the last year or two that thepossibility and need to combine techniques from thesetwo fields has emerged.
It is very likely thatnumerous proposals for merging HMMs and ANNswill be presented in the next few years.George White has proposed a new and previouslyunpublished, technique for combining HMMs andANNs.
We refer to this technique as the ANN"postprocessing technique" by which we mean that243//00 0000 000 ?
O0  ?~ / /~ooooo ,o , ,  , ,oo+ I ~~A ???? '
' ?
?? '
' ' ?
l~OO+O+ooo+o..o- -  \.1ooooooooooooo~ 0 0  0000 0000 O0 0 0?0000000000 ?
O?
0000 ?00000(9  ooT%o?o<~w~_eo op%~_9_~_~o_9_o.. ~ ..~c-- ~ ~.
.
\ .
.
.
.
.
.
.
.
.
.
.~ ~ .
.
.
.
.
.
.
.
.
.
.
.i i~ 00000000000000mem,~m?
M 0~mMo~~Z~ offo~o ~4 ~244ANNs should be applied to speech "after" HMMshave been applied.
The HMM processing determineswhere in time words should be located in the acousticinput data.
More to the point, input nodes in an ANNstructure may be connected to states inside the finitestate machines that form HMMs, which are thenconnected to time intervals in the unknown speech.The HMMs accomplish non-linear time warping,otherwise known as "dynamic time warping," of timedomain acoustic information to properly match therigid structure of a neural net template (see Fig 1).We postulate that there is great significance tobringing inputs from HMM states that span severaltime units to provide input to the neural nets.The fundamental postulate of HMM or DTW(Dynamic Time Warping) is that the speech soundsimilarity scores in adjacent time intervals may besimply summed up provide a global match score.This is rationalized by assuming that the probabilityof global match over all time intervals,P(tl,t2,t3 .... tn), is equal to the product of theprobabilities of the matches for each individual timeinterval.In other words, the fundamental assumption behindHMM or DTW is:P(tl,t2,t3,...tn)=P(tl)P(t2)P(t3)...P(tn) Eq.1This may be acceptable when there is no practicalalternative but it is not accurate and can lead torecognition errors when when subtle differencesbetween words matter.ANNs can circumvent this problem if they are trainedon the global unit spanning tl,t2,t3 .... tn.
Thefundamental motivation behind our approach tomerging ANNs and HMMs is thatANNs compute P(tl,t2,t3,...tn) directlyand thus avoid the error of Eq 1.For example, the HMM approach to scoring wordsized units sums scores for phonemes which in turnsum scores over elemental time units, typically 10 msin duration, which assumes tatistical independencebetween the phonemes and also between the 10 msdomain units.
Since these units are usually notstatistically independent, some are typically over-weighted.
ANNs spanning word sized units overcomesome of these limitations.Previous work on the general subject of mergingHMMs and ANNs includes, "Speaker-IndependentWord Recognition Using Dynamic ProgrammingNeural Networks", by Sakoe, Isotani, and Yoshida(Readings in Speech Recognition, edited by AlexWaibel & Kai-Fu Lee).
Other work includes"Merging Multilayer Perceptrons and Hidden MarkovModels: Some Experiments in Continuous SpeechRecognition" (ICSI, TR-89-033, July 1989), whichevidently applies MLP (a form of ANN) to individualstates inside HMM models.
While this is a merger ofANN and HMM techniques, it falls short of the powerof an ANN post process which overcomes the lack ofstatistical independence b tween adjacent timeintervals.Other work includes "Speaker-IndependentRecognition of Connected Utterances UsingRecurrent and Non-recurrent Neural Networks"(IJCNN, June 1989).
This work, like the onementioned above, doesn't propose to achieve timealignment by HMM techniques as a precursor toapplications of ANNs which is the basis of ourproposal.
Instead, it proposes to apply ANNtechnology first and then apply HMM techniques.This necessarily precludes the beneficial effects ofHMM guided dynamic time warping from beingrealized by the inputs to the ANNs.Other related work in this area may be considered asspecial cases of one of the two above mentionedapproaches.GENERAL COMMENTARY ON ANNWhile we advocate the use of ANN in conjunctionwith HMM or DTW, we do not at all endorse thenotion that ANNs should be used alone, without DTWor HMM or other segment spotting approaches.Internal time variability in word pronunciation imultiple pronunciations of the same word must bemanaged and ANNs have no easy way to handletemporal variability without extraordinaryrequirements for silicon area.To handle the time variability problem with245accuracies competitive with HM, neural net structuresmust store intermediate r sults inside the neral netstructures for each time interval.
For problems ascomplex as speech recognition, this is not practical onsilicon because of the number of interconnections islimited by the two dimensional nature of the surfacesof silicon chips.
Trying to simulate the neededstructures on Von Neumann machines, with DSPs forexample, will result in optimal solutions imilar to theViterbi search currently used in HMM systems.
Inother words, as long as we are restricted to twodimensional chips and Von Neumann architectures,ANN simulations will necessarily need to employsearch strategies that are already core technologies inthe speech community.
It would be misguided for theANN community to ignore these refined searchtechniques.
It is not likely that the need searchstrategies can be circumvented as long as the dynamicallocation of cpu operations i needed and it will beneeded until we achieve three dimensionalinterconnections between ANNs.
We should expectthat hybrid combinations of HMMs (or DTW basedapproaches) and ANNs will be superior to pure ANNsystems until an entirely new process for producingthree dimensional integrated circuits is invented, andthis will probably be a long time.RESULTSThese ideas have been adapted by Parfitt at Apple forapplication to Sphinx.
Parfitt modified the Sphinxrecognition system to generate input to a three layerperception, a type of ANN as shown in Figures 1 and2.
The following describes his implementation: TheSphinx system is initially trained in the traditionalmanner using the forward/backward algorithm.However, during training and recognition, a modifiedViterbi/beam search is used.
A record withbackpointers i  maintained for all nodes that arevisited during the search.
When the last speechinterval is processed, the best final node identifies theoptimum path through the utterance.
The mapping ofthe speech data to the optimum path is used toestablish word boundaries and derive a set of timealigned paramaters topass to the ANN.A separate ANN is used for each word in thevocabulary.
Each word in the vocabulary isrepresented by one or more triphone models.Although each triphone has seven states, only threeare unique states.
Because the HMM models containskip arcs, the speech can skip one, two or all three ofthe states.
The model also contains elf-loop arcs foreach of the states.
The speech may match a givenstate an arbitrary number of times.
When severalspeech samples match a given state, the middlesample is used to supply input to the ANN.
When aneven number of samples match, the left middle sampleis used.
When no speech samples match a given state,zero is used as input to the ANN.The ANN uses different input parameters than theHMM triphone in SPHINX.
The SPHINX recognizerworks on three VQ symbols per window.
Thewindows are twenty milliseconds wide and areadvanced by ten millisecond increments.
The VQs arederived from three sets of parameters, twelve CepstralCoefficients, twelve Delta Cepstral Coefficients, andPower/Delta-Power.
However, the ANNs do notreceive VQs.
Instead, they receive the same three setsof parameters before they are vector quantized exceptthat each parameter is linearly scaled to range betweenminus one and plus one.As shown in Figurel, the ANN models have onehidden layer and a single output node.
Words areconstructed from a sequence of triphones.
Forexample, for the word "zero", there are four inputtriphones.
Each triphone has three unique HMMnodes and each node has twenty-six input parametersfor a total of 78 inputs per triphone.
Hence, the word"zero" has 312 inputs to the ANN.
The 78 inputs perphone are fully interconnected to 25 nodes in thehidden layer.
All the hidden layer nodes are fullyinterconnected to the single output node.ANN TRAINING: Each word ANN is trained fromtime aligned ata produced by the modified SPHINX.Two sets of data are used to train the ANNs.
One setof data represents he "in class" utterances.
The ANNis trained to be plus one when this first set of data ispresented.
The second set of data represents "out ofclass" utterances and is composed of other words.
Forthis case, the output of the ANN is trained to be minusone.
The ratio of "out of class" utterances to "in class"utterances i 3.5.
The ANNs tend to converge to100% accuracy on the training data after about 300passes through the data.
Back propagation is used fortraining.246ANN RECOGNITION: The time normalized data foreach word from the utterance is fed as input into eachof the neural nets.
If the best path word is shorter thana given neural nets' input, additional data is takenfrom the rest of the best path.
Silence is alwaysskipped.
If the end of the utterance is reached beforeenough data is collected, nulls are input to the neuralnet.
For recognition, the individual neural nets areconnected together and the output which is most "on"is used to indicate what word.The system was tested on TI Connected DigitsDatabase.
Six male speakers from two differentdialects were used for training.
Three males, MKR,MRD, and MIN were taken from the Little Rock, ARdialect.
The other three male speakers, MBN, MBH,MIB, were taken from the Rochester, NY, dialect.The current modifications toSphinx only producepointers to the best candidate words duringrecognition.
There are three classes of errors:insertions, deletions, and substitutions.
When theHMM scores correctly, the ANN was tested and is inagreement 100% of the time.
For the three classes oferrors, only substitution errors have been tested withthe ANN.
From a set of 385 utterances, the Rochestermale speakers, nine substitution errors were made bySphinx.
The ANNs corrected four of the nine errors.CONCLUSIONS: A larger set of data needs to betested before any strong conclusions can be drawn.The initial reduction in error rate by 44% of analready highly tuned system is promising.247T1~:~ ?-~"~~?...OQOOlIOOOltIO OO00QOOOOOlIO~/ QOOOOO0000OOO~, Q QQQQO000000~e m. .~~OOl tOOOt l lO JQQQQQQQOOOIIQ~~OOOOOOOOOOOO~OOOOQO000OOO0~O000QO0000~tO~O000000000OO000QOOOOOQO0OOOQIOOI00001OOOgIOQSOQOiCOOQOIOQ~I~IC0000000000~(O000QO000000CDOOOQO0000000n i n~oa.(Io~Im?~?
-- ~ ~e .~Z0 ~o o248
