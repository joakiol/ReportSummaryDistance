Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821?831,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsLessons Learned in Part-of-Speech Tagging of Conversational SpeechVladimir Eidelman?, Zhongqiang Huang?, and Mary Harper??
?Laboratory for Computational Linguistics and Information ProcessingInstitute for Advanced Computer StudiesUniversity of Maryland, College Park, MD?Human Language Technology Center of ExcellenceJohns Hopkins University, Baltimore, MD{vlad,zhuang,mharper}@umiacs.umd.eduAbstractThis paper examines tagging models for spon-taneous English speech transcripts.
We ana-lyze the performance of state-of-the-art tag-ging models, either generative or discrimi-native, left-to-right or bidirectional, with orwithout latent annotations, together with theuse of ToBI break indexes and several meth-ods for segmenting the speech transcripts (i.e.,conversation side, speaker turn, or human-annotated sentence).
Based on these studies,we observe that: (1) bidirectional models tendto achieve better accuracy levels than left-to-right models, (2) generative models seem toperform somewhat better than discriminativemodels on this task, and (3) prosody improvestagging performance of models on conversa-tion sides, but has much less impact on smallersegments.
We conclude that, although the useof break indexes can indeed significantly im-prove performance over baseline models with-out them on conversation sides, tagging ac-curacy improves more by using smaller seg-ments, for which the impact of the break in-dexes is marginal.1 IntroductionNatural language processing technologies, such asparsing and tagging, often require reconfigurationwhen they are applied to challenging domains thatdiffer significantly from newswire, e.g., blogs, twit-ter text (Foster, 2010), or speech.
In contrast totext, conversational speech represents a significantchallenge because the transcripts are not segmentedinto sentences.
Furthermore, the transcripts are of-ten disfluent and lack punctuation and case informa-tion.
On the other hand, speech provides additionalinformation, beyond simply the sequence of words,which could be exploited to more accurately assigneach word in the transcript a part-of-speech (POS)tag.
One potentially beneficial type of informationis prosody (Cutler et al, 1997).Prosody provides cues for lexical disambigua-tion, sentence segmentation and classification,phrase structure and attachment, discourse struc-ture, speaker affect, etc.
Prosody has been foundto play an important role in speech synthesis sys-tems (Batliner et al, 2001; Taylor and Black, 1998),as well as in speech recognition (Gallwitz et al,2002; Hasegawa-Johnson et al, 2005; Ostendorf etal., 2003).
Additionally, prosodic features such aspause length, duration of words and phones, pitchcontours, energy contours, and their normalized val-ues have been used for speech processing tasks likesentence boundary detection (Liu et al, 2005).Linguistic encoding schemes like ToBI (Silver-man et al, 1992) have also been used for sentenceboundary detection (Roark et al, 2006; Harper et al,2005), as well as for parsing (Dreyer and Shafran,2007; Gregory et al, 2004; Kahn et al, 2005).
Inthe ToBI scheme, aspects of prosody such as tone,prominence, and degree of juncture between wordsare represented symbolically.
For instance, Dreyerand Shafran (2007) use three classes of automati-cally detected ToBI break indexes, indicating majorintonational breaks with a 4, hesitation with a p, andall other breaks with a 1.Recently, Huang and Harper (2010) found thatthey could effectively integrate prosodic informa-821tion in the form of this simplified three class ToBIencoding when parsing spontaneous speech by us-ing a prosodically enriched PCFG model with latentannotations (PCFG-LA) (Matsuzaki et al, 2005;Petrov and Klein, 2007) to rescore n-best parsesproduced by a baseline PCFG-LA model withoutprosodic enrichment.
However, the prosodically en-riched models by themselves did not perform sig-nificantly better than the baseline PCFG-LA modelwithout enrichment, due to the negative effect thatmisalignments between automatic prosodic breaksand true phrase boundaries have on the model.This paper investigates methods for using state-of-the-art taggers on conversational speech tran-scriptions and the effect that prosody has on taggingaccuracy.
Improving POS tagging performance ofspeech transcriptions has implications for improvingdownstream applications that rely on accurate POStags, including sentence boundary detection (Liuet al, 2005), automatic punctuation (Hillard et al,2006), information extraction from speech, parsing,and syntactic language modeling (Heeman, 1999;Filimonov and Harper, 2009).
While there havebeen several attempts to integrate prosodic informa-tion to improve parse accuracy of speech transcripts,to the best of our knowledge there has been littlework on using this type of information for POS tag-ging.
Furthermore, most of the parsing work hasinvolved generative models and rescoring/rerankingof hypotheses from the generative models.
In thiswork, we will analyze several factors related to ef-fective POS tagging of conversational speech:?
discriminative versus generative POS taggingmodels (Section 2)?
prosodic features in the form of simplified ToBIbreak indexes (Section 4)?
type of speech segmentation (Section 5)2 ModelsIn order to fully evaluate the difficulties inherent intagging conversational speech, as well as the possi-ble benefits of prosodic information, we conductedexperiments with six different POS tagging mod-els.
The models can be broadly separated into twoclasses: generative and discriminative.
As the firstof our generative models, we used a Hidden MarkovModel (HMM) trigram tagger (Thede and Harper,1999), which serves to establish a baseline and togauge the difficulty of the task at hand.
Our sec-ond model, HMM-LA, was the latent variable bi-gram HMM tagger of Huang et al (2009), whichachieved state-of-the-art tagging performance by in-troducing latent tags to weaken the stringent Markovindependence assumptions that generally hinder tag-ging performance in generative models.For the third model, we implemented a bidirec-tional variant of the HMM-LA (HMM-LA-Bidir)that combines evidence from two HMM-LA tag-gers, one trained left-to-right and the other right-to-left.
For decoding, we use a product model (Petrov,2010).
The intuition is that the context informationfrom the left and the right of the current positionis complementary for predicting the current tag andthus, the combination should serve to improve per-formance over the HMM-LA tagger.Since prior work on parsing speech with prosodyhas relied on generative models, it was necessaryto modify equations of the model in order to incor-porate the prosodic information, and then performrescoring in order to achieve gains.
However, it isfar simpler to directly integrate prosody as featuresinto the model by using a discriminative approach.Hence, we also investigate several log-linear mod-els, which allow us to easily include an arbitrarynumber and varying kinds of possibly overlappingand non-independent features.First, we implemented a Conditional RandomField (CRF) tagger, which is an attractive choice dueto its ability to learn the globally optimal labelingfor a sequence and proven excellent performance onsequence labeling tasks (Lafferty et al, 2001).
Incontrast to an HMM which optimizes the joint like-lihood of the word sequence and tags, a CRF opti-mizes the conditional likelihood, given by:p?
(t|w) =exp?j ?jFj(t, w)?t exp?j ?jFj(t, w)(1)where the ?
?s are the parameters of the model to es-timate and F indicates the feature functions used.The denominator in (1) is Z?
(x), the normalizationfactor, with:Fj(t, w) =?ifj(t, w, i)822Class Model Name Latent Variable Bidirectional N-best-Extraction Markov OrderGenerativeTrigram HMM?2ndHMM-LA?
?1stHMM-LA-Bidir?
?1stDiscriminativeStanford Bidir?2ndStanford Left5 2ndCRF 2ndTable 1: Description of tagging modelsThe objective we need to maximize then becomes :L =?n??
?j?jFj(tn, wn)?
logZ?(xn)?????
?22?2where we use a spherical Gaussian prior to pre-vent overfitting of the model (Chen and Rosen-feld, 1999) and the wide-spread quasi-NewtonianL-BFGS method to optimize the model parame-ters (Liu and Nocedal, 1989).
Decoding is per-formed with the Viterbi algorithm.We also evaluate state-of-the-art Maximum En-tropy taggers: the Stanford Left5 tagger (Toutanovaand Manning, 2000) and the Stanford bidirectionaltagger (Toutanova et al, 2003), with the former us-ing only left context and the latter bidirectional de-pendencies.Table 1 summarizes the major differences be-tween the models along several dimensions: (1) gen-erative versus discriminative, (2) directionality ofdecoding, (3) the presence or absence of latent anno-tations, (4) the availability of n-best extraction, and(5) the model order.In order to assess the quality of our models, weevaluate them on the section 23 test set of the stan-dard newswire WSJ tagging task after training allmodels on sections 0-22.
Results appear in Ta-ble 2.
Clearly, all the models have high accuracyon newswire data, but the Stanford bidirectional tag-ger significantly outperforms the other models withthe exception of the HMM-LA-Bidir model on thistask.11Statistically significant improvements are calculated usingthe sign test (p < 0.05).Model AccuracyTrigram HMM 96.58HMM-LA 97.05HMM-LA-Bidir 97.16Stanford Bidir 97.28Stanford Left5 97.07CRF 96.81Table 2: Tagging accuracy on WSJ3 Experimental SetupIn the rest of this paper, we evaluate the tag-ging models described in Section 2 on conver-sational speech.
We chose to utilize the PennSwitchboard (Godfrey et al, 1992) and Fisher tree-banks (Harper et al, 2005; Bies et al, 2006) becausethey provide gold standard tags for conversationalspeech and we have access to corresponding auto-matically generated ToBI break indexes provided by(Dreyer and Shafran, 2007; Harper et al, 2005)2.We utilized the Fisher dev1 and dev2 sets contain-ing 16,519 sentences (112,717 words) as the primarytraining data and the entire Penn Switchboard tree-bank containing 110,504 sentences (837,863 words)as an additional training source3.
The treebankswere preprocessed as follows: the tags of auxiliaryverbs were replaced with the AUX tag, empty nodes2A small fraction of words in the Switchboard treebank donot align with the break indexes because they were producedbased on a later refinement of the transcripts used to producethe treebank.
For these cases, we heuristically added break *1*to words in the middle of a sentence and *4* to words that enda sentence.3Preliminary experiments evaluating the effect of trainingdata size on performance indicated using the additional Switch-board data leads to more accurate models, and so we use thecombined training set.823and function tags were removed, words were down-cased, punctuation was deleted, and the words andtheir tags were extracted.
Because the Fisher tree-bank was developed using the lessons learned whendeveloping Switchboard, we chose to use its evalportion for development (the first 1,020 tagged sen-tences containing 7,184 words) and evaluation (theremaining 3,917 sentences with 29,173 words).We utilize the development set differently for thegenerative and discriminative models.
Since the EMalgorithm used for estimating the parameters in thelatent variable models introduces a lot of variabil-ity, we train five models with a different seed andthen choose the best one based on dev set perfor-mance.
For the discriminative models, we tunedtheir respective regularization parameters on the devset.
All results reported in the rest of this paper areon the test set.4 Integration of Prosodic InformationIn this work, we use three classes of automaticallygenerated ToBI break indexes to represent prosodicinformation (Kahn et al, 2005; Dreyer and Shafran,2007; Huang and Harper, 2010): 4, 1, and p.Consider the following speech transcription exam-ple, which is enriched with ToBI break indexes inparentheses and tags: i(1)/PRP did(1)/VBDn?t(1)/RB you(1)/PRP know(4)/VBPi(1)/PRP did(1)/AUX n?t(1)/RB...The speaker begins an utterance, and then restartsthe utterance.
The automatically predicted break 4associated with know in the utterance compellinglyindicates an intonational phrase boundary and couldprovide useful information for tagging if we canmodel it appropriately.To integrate prosody into our generative models,we utilize the method from (Dreyer and Shafran,2007) to add prosodic breaks.
As Figure 1 shows,ToBI breaks provide a secondary sequence of ob-servations that is parallel to the sequence of wordsthat comprise the sentence.
Each break bi in the sec-ondary sequence is generated by the same tag ti asthat which generates the corresponding wordwi, andso it is conditionally independent of its correspond-ing word given the tag:P (w, b|t) = P (w|t)P (b|t)PRPi1VBDdid1RBn?t1VBPknow4Figure 1: Parallel generation of words and breaks for theHMM modelsThe HMM-LA taggers are then able to split tags tocapture implicit higher order interactions among thesequence of tags, words, and breaks.The discriminative models are able to utilizeprosodic features directly, enabling the use of con-textual interactions with other features to further im-prove tagging accuracy.
Specifically, in addition tothe standard set of features used in the tagging lit-erature, we use the feature templates presented inTable 3, where each feature associates the break bi,word wi, or some combination of the two with thecurrent tag ti4.Break and/or word values Tag valuebi=B ti = Tbi=B & bi?1=C ti = Twi=W & bi=B ti = Twi+1=W & bi=B ti = Twi+2=W & bi=B ti = Twi?1=W & bi=B ti = Twi?2=W & bi=B ti = Twi=W & bi=B & bi?1=C ti = TTable 3: Prosodic feature templates5 Experiments5.1 Conversation side segmentationWhen working with raw speech transcripts, we ini-tially have a long stream of unpunctuated words,which is called a conversation side.
As the averagelength of conversation side segments in our data isapproximately 630 words, it poses quite a challeng-ing tagging task.
Thus, we hypothesize that it is onthese large segments that we should achieve the most4We modified the Stanford taggers to handle these prosodicfeatures.8249393.393.693.994.294.5HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRFBaseline Prosody OracleBreak OracleBreak+Sent OracleSent OracleBreak-Sent RescoringFigure 2: Tagging accuracy on conversation sidesimprovement from the addition of prosodic informa-tion.In fact, as the baseline results in Figure 2 show,the accuracies achieved on this task are much lowerthan those on the newswire task.
The trigram HMMtagger accuracy drops to 92.43%, while all the othermodels fall to within the range of 93.3%-94.12%,a significant departure from the 96-97.3% range onnewswire sentences.
Note that the Stanford bidi-rectional and HMM-LA tagger perform very simi-larly, although the HMM-LA-Bidir tagger performssignificantly better than both.
In contrast to thenewswire task on which the Stanford bidirectionaltagger performed the best, on this genre, it is slightlyworse than the HMM-LA tagger, albeit the differ-ence is not statistically significant.With the direct integration of prosody into thegenerative models (see Figure 2), there is a slight butstatistically insignificant shift in performance.
How-ever, integrating prosody directly into the discrimi-native models leads to significant improvements inthe CRF and Stanford Left5 taggers.
The gain inthe Stanford bidirectional tagger is not statisticallysignificant, however, which suggests that the left-to-right models benefit more from the addition ofprosody than bidirectional models.5.2 Human-annotated sentencesGiven the lack-luster performance of the taggingmodels on conversation side segments, even with thedirect addition of prosody, we chose to determine theperformance levels that could be achieved on thistask using human-annotated sentences, which wewill refer to as sentence segmentation.
Figure 3 re-ports the baseline tagging accuracy on sentence seg-ments, and we see significant improvements acrossall models.
The HMM Trigram tagger performanceincreases to 93.00%, while the increase in accuracyfor the other models ranges from around 0.2-0.3%.The HMM-LA taggers once again achieve the bestperformance, with the Stanford bidirectional closebehind.
Although the addition of prosody has verylittle impact on either the generative or discrimina-tive models when applied to sentences, the base-line tagging models (i.e., not prosodically enriched)significantly outperform all of the prosodically en-riched models operating on conversation sides.At this point, it would be apt to suggest us-ing automatic sentence boundary detection to cre-ate shorter segments.
Table 4 presents the resultsof using baseline models without prosodic enrich-ment trained on the human-annotated sentences totag automatically segmented speech5.
As can beseen, the results are quite similar to the conversationside segmentation performances, and thus signifi-cantly lower than when tagging human-annotatedsentences.
A caveat to consider here is that we breakthe standard assumption that the training and test setbe drawn from the same distribution, since the train-ing data is human-annotated and the test is automat-ically segmented.
However, it can be quite challeng-ing to create a corpus to train on that represents thebiases of the systems that perform automatic sen-tence segmentation.
Instead, we will examine an-5We used the Baseline Structural Metadata System de-scribed in Harper et al (2005) to predict sentence boundaries.8259393.393.693.994.294.5HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRFBaseline Prosody OracleBreak RescoringFigure 3: Tagging accuracy on human-annotated segmentsother segmentation method to shorten the segmentsautomatically, i.e., by training and testing on speakerturns, which preserves the train-test match, in Sec-tion 5.5.Model AccuracyHMM-LA 93.95HMM-LA-Bidir 94.07Stanford Bidir 93.77Stanford Left5 93.35CRF 93.29Table 4: Baseline tagging accuracy on automatically de-tected sentence boundaries5.3 Oracle Break InsertionAs we believe one of the major roles that prosodiccues serve for tagging conversation sides is as aproxy for sentence boundaries, perhaps the efficacyof the prosodic breaks can, at least partially, be at-tributed to errors in the automatically induced breakindexes themselves, as they can misalign with syn-tactic phrase boundaries, as discussed in Huang andHarper (2010).
This may degrade the performanceof our models more than the improvement achievedfrom correctly placed breaks.
Hence, we conducta series of experiments in which we systematicallyeliminate noisy phrase and disfluency breaks andshow that under these improved conditions, prosodi-cally enriched models can indeed be more effective.To investigate to what extent noisy breaks are im-peding the possible improvements from prosodicallyenriched models, we replaced all 4 and p breaks inthe training and evaluation sets that did not alignto the correct phrase boundaries as indicated by thetreebank with break 1 for both the conversation sidesand human-annotated sentences.
The results fromusing Oracle Breaks on conversation sides can beseen in Figure 2.
All models except Stanford Left5and HMM-LA-Bidir significantly improve in accu-racy when trained and tested on the Oracle Breakmodified data.
On human-annotated sentences, Fig-ure 3 shows improvements in accuracies across allmodels, however, they are statistically insignificant.To further analyze why prosodically enrichedmodels achieve more improvement on conversationsides than on sentences, we conducted three moreOracle experiments on conversation sides.
For thefirst, OracleBreak-Sent, we further modified the datasuch that all breaks corresponding to a sentenceending in the human-annotated segments were con-verted to break 1, thus effectively only leaving in-side sentence phrasal boundaries.
This modificationresults in a significant drop in performance, as canbe seen in Figure 2.For the second, OracleSent, we converted allthe breaks corresponding to a sentence end in thehuman-annotated segmentations to break 4, and allthe others to break 1, thus effectively only leavingsentence boundary breaks.
This performed largelyon par with OracleBreak, suggesting that the phrase-aligned prosodic breaks seem to be a stand-in forsentence boundaries.Finally, in the last condition, OracleBreak+Sent,we modified the OracleBreak data such that allbreaks corresponding to a sentence ending in thehuman-annotated sentences were converted to break8269393.393.693.994.294.5HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRFBaseline Prosody RescoringFigure 4: Tagging accuracy on speaker turns4 (essentially combining OracleBreak and Oracle-Sent).
As Figure 2 indicates, this modification re-sults in the best tagging accuracies for all the mod-els.
All models were able to match or even improveupon the baseline accuracies achieved on the humansegmented data.
This suggests that when we havebreaks that align with phrasal and sentence bound-aries, prosodically enriched models are highly effec-tive.5.4 N-best RescoringBased on the findings in the previous section and thefindings of (Huang and Harper, 2010), we next ap-ply a rescoring strategy in which the search spaceof the prosodically enriched generative models is re-stricted to the n-best list generated from the base-line model (without prosodic enrichment).
In thismanner, the prosodically enriched model can avoidpoor tag sequences produced due to the misalignedbreak indexes.
As Figure 2 shows, using the base-line conversation side model to produce an n-bestlist for the prosodically enriched model to rescoreresults in significant improvements in performancefor the HMM-LA model, similar to the parsing re-sults of (Huang and Harper, 2010).
The size of then-best list directly impacts performance, as reducingto n = 1 is akin to tagging with the baseline model,and increasing n ?
?
amounts to tagging with theprosodically enriched model.
We experimented witha number of different sizes for n and chose the bestone using the dev set.
Figure 3 presents the resultsfor this method applied to human-annotated sen-tences, where it produces only marginal improve-ments6.5.5 Speaker turn segmentationThe results presented thus far indicate that if wehave access to close to perfect break indexes, wecan use them effectively, but this is not likely to betrue in practice.
We have also observed that taggingaccuracy on shorter conversation sides is greaterthan longer conversation sides, suggesting that post-processing the conversation sides to produce shortersegments would be desirable.We thus devised a scheme by which we couldautomatically extract shorter speaker turn segmentsfrom conversation sides.
For this study, speakerturns, which effectively indicate speaker alterna-tions, were obtained by using the metadata in thetreebank to split the sentences into chunks based onspeaker change.
Every time a speaker begins talk-ing after the other speaker was talking, we start anew segment for that speaker.
In practice, this wouldneed to be done based on audio cues and automatictranscriptions, so these results represent an upperbound.Figure 4 presents tagging results on speaker turnsegments.
For most models, the difference in accu-racy achieved on these segments and that of human-annotated sentences is statistically insignificant.
Theonly exception is the Stanford bidirectional tagger,6Rescoring using the CRF model was also performed, butled to a performance degradation.
We believe this is due tothe fact that the prosodically enriched CRF model was able todirectly use the break index information, and so restricting it tothe baseline CRF model search space limits the performance tothat of the baseline model.8270100200300400NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD INNumberof ErrorsConv Baseline Conv Rescore Conv OracleBreak Sent Baseline(a) Number of errors by part of speech category for the HMM-LA model with and without prosody0100200300400NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD INNumberof ErrorsConv Baseline Conv Prosody Conv OracleBreak Sent Baseline(b) Number of errors by part of speech category for the CRF model with and without prosodyFigure 5: Error reduction for prosodically enriched HMM-LA (a) and CRF (b) modelswhich performs worse on these slightly longer seg-ments.
With the addition of break indexes, we seemarginal changes in most of the models; only theCRF tagger receives a significant boost.
Thus, mod-els achieve performance gains from tagging shortersegments, but at the cost of limited usefulness of theprosodic breaks.
Overall, speaker turn segmenta-tion is an attractive compromise between the originalconversation sides and human-annotated sentences.6 DiscussionAcross the different models, we have found that tag-gers applied to shorter segments, either sentences orspeaker turns, do not tend to benefit significantlyfrom prosodic enrichment, in contrast to conversa-tion sides.
To analyze this further we broke downthe results by part of speech for the two modelsfor which break indexes improved performance themost: the CRF and HMM-LA rescoring models,which achieved an overall error reduction of 2.8%and 2.1%, respectively.
We present those categoriesthat obtained the greatest benefit from prosody inFigure 5 (a) and (b).
For both models, the UH cate-gory had a dramatic improvement from the additionof prosody, achieving up to a 10% reduction in error.For the CRF model, other categories that saw im-pressive error reductions were NN and VB, with10% and 5%, respectively.
Table 5 lists the prosodicfeatures that received the highest weight in the CRFmodel.
These are quite intuitive, as they seem to rep-resent places where the prosody indicates sentenceor clausal boundaries.
For the HMM-LA model,the VB and DT tags had major reductions in errorof 13% and 10%, respectively.
For almost all cat-egories, the number of errors is reduced by the ad-dition of breaks, and further reduced by using theOracleBreak processing described above.Weight Feature2.2212 wi=um & bi=4 & t=UH1.9464 wi=uh & bi=4 & t=UH1.7965 wi=yes & bi=4 & t=UH1.7751 wi=and & bi=4 & t=CC1.7554 wi=so & bi=4 & t=RB1.7373 wi=but & bi=4 & t=CCTable 5: Top break 4 prosody features in CRF prosodymodelTo determine more precisely the effect that thesegment size has on tagging accuracy, we extractedthe oracle tag sequences from the HMM-LA andCRF baseline and prosodically enriched modelsacross conversation sides, sentences, and speakerturn segments.
As the plot in Figure 6 shows, aswe increase the n-best list size to 500, the ora-cle accuracy of the models trained on sentences in-828929496981002 5 10 20 50 100 200 500AccuracyN -Best sizeSentencesSpeaker  tunsConversation sidesFigure 6: Oracle comparison: solid lines for sentences,dashed lines for speaker turns, and dotted lines for con-versation sidescreases rapidly to 99%; whereas, the oracle accu-racy of models on conversation sides grow slowlyto between 94% and 95%.
The speaker turn trainedmodels, however, behave closely to those using sen-tences, climbing rapidly to accuracies of around98%.
This difference is directly attributable to thelength of the segments.
As can be seen in Table 6,the speaker turn segments are more comparable inlength to sentences.Train EvalConv 627.87 ?
281.57 502.98 ?
151.22Sent 7.52?
7.86 7.45 ?
8.29Speaker 15.60?
29.66 15.27?
21.01Table 6: Length statistics of different data segmentationsNext, we return to the large performance degrada-tion when tagging speech rather than newswire textto examine the major differences among the mod-els.
Using two of our best performing models, theStanford bidirectional and HMM-LA, in Figure 7we present the categories for which performancedegradation was the greatest when comparing per-formance of a tagger trained on WSJ to a taggertrained on spoken sentences and conversation sides.The performance decrease is quite similar acrossboth models, with the greatest degradation on theNNP, RP, VBN, and RBS categories.Unsurprisingly, both the discriminative and gen-erative bidirectional models achieve the most im-pressive results.
However, the generative HMM-LA and HMM-LA-Bidir models achieved the bestresults across all three segmentations, and the bestoverall result, of 94.35%, on prosodically enrichedsentence-segmented data.
Since the Stanford bidi-rectional model incorporates all of the features thatproduced its state-of-the-art performance on WSJ,we believe the fact that the HMM-LA outperformsit, despite the discriminative model?s more expres-sive feature set, is indicative of the HMM-LA?s abil-ity to more effectively adapt to novel domains duringtraining.
Another challenge for the discriminativemodels is the need for regularization tuning, requir-ing additional time and effort to train several mod-els and select the most appropriate parameter eachtime the domain changes.
Whereas for the HMM-LA models, although we also train several models,they can be combined into a product model, such asthat described by Petrov (2010), in order to furtherimprove performance.Since the prosodic breaks are noisier features thanthe others incorporated in the discriminative models,it may be useful to set their regularization param-eter separately from the rest of the features, how-ever, we have not explored this alternative.
Our ex-periments used human transcriptions of the conver-sational speech; however, realistically our modelswould be applied to speech recognition transcripts.In such a case, word error will introduce noise in ad-dition to the prosodic breaks.
In future work, we willevaluate the use of break indexes for tagging whenthere is lexical error.
We would also apply the n-best rescoring method to exploit break indexes in theHMM-LA bidirectional model, as this would likelyproduce further improvements.7 ConclusionIn this work, we have evaluated factors that are im-portant for developing accurate tagging models forspeech.
Given that prosodic breaks were effectiveknowledge sources for parsing, an important goalof this work was to evaluate their impact on vari-ous tagging model configurations.
Specifically, wehave examined the use of prosodic information fortagging conversational speech with several differentdiscriminative and generative models across threedifferent speech transcript segmentations.
Our find-8290%10%20%30%40%50%NNP VBN WP CD RP EX WRB WDT JJR POS JJS RBSErrorRateWSJ (Stanford-Bidir) WSJ (HMM-LA)Sent (Stanford-Bidir) Sent (HMM-LA)Conv (Stanford-Bidir) Conv (HMM-LA)Figure 7: Comparison of error rates between the Standford Bidir and HMM-LA models trained on WSJ, sentences,and conversation sidesings suggest that generative models with latent an-notations achieve the best performance in this chal-lenging domain.
In terms of transcript segmenta-tion, if sentences are available, it is preferable to usethem.
In the case that no such annotation is avail-able, then using automatic sentence boundary detec-tion does not serve as an appropriate replacement,but if automatic speaker turn segments can be ob-tained, then this is a good alternative, despite the factthat prosodic enrichment is less effective.Our investigation also shows that in the event thatconversation sides must be used, prosodic enrich-ment of the discriminative and generative modelsproduces significant improvements in tagging accu-racy (by direct integration of prosody features forthe former and by restricting the search space andrescoring with the latter).
For tagging, the most im-portant role of the break indexes appears to be as astand in for sentence boundaries.
The oracle breakexperiments suggest that if the accuracy of the au-tomatically induced break indexes can be improved,then the prosodically enriched models will performas well, or even better, than their human-annotatedsentence counterparts.8 AcknowledgmentsThis research was supported in part by NSF IIS-0703859 and the GALE program of the DefenseAdvanced Research Projects Agency, Contract No.HR0011-06-2-001.
Any opinions, findings, and rec-ommendations expressed in this paper are those ofthe authors and do not necessarily reflect the viewsof the funding agency or the institutions where thework was completed.ReferencesAnton Batliner, Bernd Mo?bius, Gregor Mo?hler, AntjeSchweitzer, and Elmar No?th.
2001.
Prosodic models,automatic speech understanding, and speech synthesis:toward the common ground.
In Eurospeech.Ann Bies, Stephanie Strassel, Haejoong Lee, KazuakiMaeda, Seth Kulick, Yang Liu, Mary Harper, andMatthew Lease.
2006.
Linguistic resources for speechparsing.
In LREC.Stanley F. Chen and Ronald Rosenfeld.
1999.
A Gaus-sian prior for smoothing maximum entropy models.Technical report, Technical Report CMU-CS-99-108,Carnegie Mellon University.Anne Cutler, Delphine Dahan, and Wilma v an Donselaar.1997.
Prosody in comprehension of spoken language:A literature review.
Language and Speech.Markus Dreyer and Izhak Shafran.
2007.
Exploitingprosody for PCFGs with latent annotations.
In Inter-speech.Denis Filimonov and Mary Harper.
2009.
A jointlanguage model with fine-grain syntactic tags.
InEMNLP.Jennifer Foster.
2010.
?cba to check the spelling?
: Inves-tigating parser performance on discussion forum posts.In NAACL-HLT.Florian Gallwitz, Heinrich Niemann, Elmar No?th, andVolker Warnke.
2002.
Integrated recognition of wordsand prosodic phrase boundaries.
Speech Communica-tion.John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel.
1992.
SWITCHBOARD: Telephone speechcorpus for research and development.
In ICASSP.Michelle L. Gregory, Mark Johnson, and Eugene Char-niak.
2004.
Sentence-internal prosody does not helpparsing the way punctuation does.
In NAACL.Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark,Izhak Shafran, Matthew Lease, Yang Liu, MatthewSnover, Lisa Yung, Anna Krasnyanskaya, and RobinStewart.
2005.
2005 Johns Hopkins Summer Work-shop Final Report on Parsing and Spoken Structural830Event Detection.
Technical report, Johns HopkinsUniversity.Mark Hasegawa-Johnson, Ken Chen, Jennifer Cole,Sarah Borys, Sung suk Kim, Aaron Cohen, TongZhang, Jeung yoon Choi, Heejin Kim, Taejin Yoon,and Ra Chavarria.
2005.
Simultaneous recognitionof words and prosody in the boston university radiospeech corpus.
speech communication.
Speech Com-munication.Peter A. Heeman.
1999.
POS tags and decision trees forlanguage modeling.
In EMNLP.Dustin Hillard, Zhongqiang Huang, Heng Ji, Ralph Gr-ishman, Dilek Hakkani-Tur, Mary Harper, Mari Os-tendorf, and Wen Wang.
2006.
Impact of automaticcomma prediction on POS/name tagging of speech.
InICASSP.Zhongqiang Huang and Mary Harper.
2010.
Appropri-ately handled prosodic breaks help PCFG parsing.
InNAACL.Zhongqiang Huang, Vladimir Eidelman, and MaryHarper.
2009.
Improving a simple bigram hmm part-of-speech tagger by latent annotation and self-training.In NAACL-HLT.Jeremy G. Kahn, Matthew Lease, Eugene Charniak,Mark Johnson, and Mari Ostendorf.
2005.
Effectiveuse of prosody in parsing conversational speech.
InEMNLP-HLT.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In ICML.D.
C. Liu and Jorge Nocedal.
1989.
On the limited mem-ory BFGS method for large scale optimization.
Math-ematical Programming.Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and MaryHarper.
2005.
Using conditional random fields forsentence boundary detection in speech.
In ACL.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InACL.Mari Ostendorf, Izhak Shafran, and Rebecca Bates.2003.
Prosody models for conversational speechrecognition.
In Plenary Meeting and Symposium onProsody and Speech Processing.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL.Slav Petrov.
2010.
Products of random latent variablegrammars.
In HLT-NAACL.Brian Roark, Yang Liu, Mary Harper, Robin Stewart,Matthew Lease, Matthew Snover, Izhak Shafran, Bon-nie Dorr, John Hale, Anna Krasnyanskaya, and LisaYung.
2006.
Reranking for sentence boundary detec-tion in conversational speech.
In ICASSP.Kim Silverman, Mary Beckman, John Pitrelli, Mari Os-tendorf, Colin Wightman, Patti Price, Janet Pierrehum-bert, and Julia Hirshberg.
1992.
ToBI: A standard forlabeling English prosody.
In ICSLP.Paul Taylor and Alan W. Black.
1998.
Assigningphrase breaks from part-of-speech sequences.
Com-puter Speech and Language.Scott M. Thede and Mary P. Harper.
1999.
A second-order hidden markov model for part-of-speech tag-ging.
In ACL.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In EMNLP.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In NAACL.831
