Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 375?379, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsTJP: Using Twitter to Analyze the Polarity of ContextsTawunrat Chalothorn Jeremy EllmanUniversity of Northumbria at Newcastle University of Northumbria at NewcastlePandon Building, Camden Street Pandon Building, Camden StreetNewcastle Upon Tyne, NE2 1XE, UK Newcastle Upon Tyne, NE2 1XE, UKTawunrat.chalothorn@unn.ac.uk Jeremy.ellman@unn.ac.ukAbstractThis paper presents our system, TJP, whichparticipated in SemEval 2013 Task 2 part A:Contextual Polarity Disambiguation.
The goalof this task is to predict whether marked con-texts are positive, neutral or negative.
Howev-er, only the scores of positive and negativeclass will be used to calculate the evaluationresult using F-score.
We chose to work as?constrained?, which used only the providedtraining and development data without addi-tional sentiment annotated resources.
Our ap-proach considered unigram, bigram andtrigram using Na?ve Bayes training modelwith the objective of establishing a simple-approach baseline.
Our system achieved F-score 81.23% and F-score 78.16% in the re-sults for SMS messages and Tweets respec-tively.1 IntroductionNatural language processing (NLP) is a researcharea comprising various tasks; one of which is sen-timent analysis.
The main goal of sentiment analy-sis is to identify the polarity of natural languagetext (Shaikh et al 2007).
Sentiment analysis canbe referred to as opinion mining, as study peoples?opinions, appraisals and emotions towards entitiesand events and their attributes (Pang and Lee,2008).
Sentiment analysis has become a popularresearch area in NLP with the purpose of identify-ing opinions or attitudes in terms of polarity.This paper presents TJP, a system submitted toSemEval 2013 for Task 2 part A: Contextual Polar-ity Disambiguation (Wilson et al 2013).
TJP wasfocused on the ?constrained?
task, which used onlytraining and development data provided.
Thisavoided both resource implications and potentialadvantages implied by the use of additional datacontaining sentiment annotations.
The objectivewas to explore the relative success of a simple ap-proach that could be implemented easily withopen-source software.The TJP system was implemented using the Py-thon Natural Language Toolkit (NLTK, Bird et al2009).
We considered several basic approaches.These used a preprocessing phase to expand con-tractions, eliminate stopwords, and identify emoti-cons.
The next phase used supervised machinelearning and n-gram features.
Although we hadtwo approaches that both used n-gram features, wewere limited to submitting just one result.
Conse-quently, we chose to submit a unigram based ap-proach followed by naive Bayes since thisperformed better on the data.The remainder of this paper is structured as fol-lows: section 2 provides some discussion on therelated work.
The methodology of corpus collec-tion and data classification are provided in section3.
Section 4 outlines details of the experiment andresults, followed by the conclusion and ideas forfuture work in section 5.2 Related WorkThe micro-blogging tool Twitter is well-knownand increasingly popular.
Twitter allows its usersto post messages, or ?Tweets?
of up to 140 charac-ters each time, which are available for immediate375download over the Internet.
Tweets are extremelyinteresting to marketing since their rapid publicinteraction can either indicate customer success orpresage public relations disasters far more quicklythan web pages or traditional media.
Consequently,the content of tweets and identifying their senti-ment polarity as positive or negative is a currentactive research topic.Emoticons are features of both SMS texts, andtweets.
Emoticons such as :) to represent a smile,allow emotions to augment the limited text in SMSmessages using few characters.
Read (2005) usedemoticons from a training set that was downloadedfrom Usenet newsgroups as annotations (positiveand negative).
Using the machine learning tech-niques of Na?ve Bayes and Support Vector Ma-chines Read (2005) achieved up to 70 % accuracyin determining text polarity from the emoticonsused.Go et al(2009) used distant supervision to clas-sify sentiment of Twitter, as similar as in (Read,2005).
Emoticons have been used as noisy labels intraining data to perform distant supervised learning(positive and negative).
Three classifiers wereused: Na?ve Bayes, Maximum Entropy and Sup-port Vector Machine, and they were able to obtainmore than 80% accuracy on their testing data.Aisopos et al(2011) divided tweets in to threegroups using emoticons for classification.
If tweetscontain positive emoticons, they will be classifiedas positive and vice versa.
Tweets without posi-tive/negative emoticons will be classified as neu-tral.
However, tweets that contain both positiveand negative emoticons are ignored in their study.Their task focused on analyzing the contents ofsocial media by using n-gram graphs, and the re-sults showed that n-gram yielded high accuracywhen tested with C4.5, but low accuracy with Na-?ve Bayes Multinomial (NBM).3 Methodology3.1 CorpusThe training data set for SemEval was built usingTwitter messages training and development data.There are more than 7000 pieces of context.
Usersusually use emoticons in their tweets; therefore,emoticons have been manually collected and la-beled as positive and negative to provide somecontext (Table 1), which is the same idea as in Ai-sopos et al(2011).Negative emoticons :( :-( :d :< D: :\ /: etc.Positive emoticons:) ;) :-) ;-) :P ;P (: (; :D;D etc.Table 1: Emoticon labels as negative and positiveFurthermore, there are often features that havebeen used in tweets, such as hashtags, URL links,etc.
To extract those features, the following pro-cesses have been applied to the data.1.
Retweet (RT), twitter username (@panda),URL links (e.g.
y2u.be/fiKKzdLQvFo),and special punctuation were removed.2.
Hashtags have been replaced by the fol-lowing word (e.g.
# love was replaced bylove, # exciting was replaced by exciting).3.
English contraction of ?not?
was convertedto full form (e.g.
don?t -> do not).4.
Repeated letters have been reduced and re-placed by 2 of the same character (e.g.happpppppy will be replaced by happy,coollllll will be replaced by cooll).3.2 ClassifierOur system used the NLTK Na?ve Bayes classifiermodule.
This is a classification based on Bayes?srule and also known as the state-of-art of the Bayesrules (Cufoglu et al 2008).
The Na?ve Bayesmodel follows the assumption that attributes withinthe same case are independent given the class label(Hope and Korb, 2004).Tang et al(2009) considered that Na?ve Bayesassigns a context   (represented by a vector) tothe class    that maximizesby applyingBayes?s rule, as in (1).
(  |)(1)whereis a randomly selected context  .
Therepresentation of vector is.
is the randomselect context that is assigned to class  .To classify the term, features inwere assumed as    from         as in (2).376(  |)?
(2)There are many different approaches to lan-guage analysis using syntax, semantics, and se-mantic resources such as WordNet.
That may beexploited using the NLTK (Bird et al2009).
How-ever, for simplicity we opted here for the n-gramapproach where texts are decomposed into termsequences.
A set of single sequences is a unigram.The set of two word sequences (with overlapping)are bigrams, whilst the set of overlapping threeterm sequences are trigrams.
The relative ad-vantage of the bi-and trigram approaches are thatcoordinates terms effectively disambiguate sensesand focus content retrieval and recognition.N-grams have been used many times in contentsclassification.
For example, Pang et al(2002) usedunigram and bigram to classify movie reviews.
Theresults showed that unigram gave better resultsthan bigram.
Conversely, Dave et al(2003) re-ported gaining better results from trigrams ratherthan bigram in classifying product reviews.
Conse-quently, we chose to evaluate unigrams, bigramsand trigrams to see which will give the best resultsFigure 1: Comparison of Twitter messages from two approachesFigure 2: Comparison of SMS messages from two approachesUnigram Bigram TrigramPos 1 84.46 82.09 80.8Neg 1 71.08 59.53 52.91Pos 2 84.62 83.31 83.25Neg 2 71.70 65.00 64.34505560657075808590F-score(%) Pos 1Neg 1Pos 2Neg 2Unigram Bigram TrigramPos 1 76.23 73.89 72.02Neg 1 82.61 76.04 71.19Pos 2 77.81 75.69 75.42Neg 2 84.66 79.94 79.37505560657075808590F-score(%)Pos 1Neg 1Pos 2Neg 2377in the polarity classification.
Our results are de-scribed in the next section.4 Experiment and ResultsIn this experiment, we used the distributed datafrom Twitter messages and the F-measure for sys-tem evaluation.
As at first approach, the corporawere trained directly in the system, while stop-words (e.g.
a, an, the) were removed before train-ing using the python NLTK for the secondapproach.
The approaches are demonstrated on asample context in Table 2 and 3.After comparing both approaches (Figure 1), wewere able to obtain an F-score 84.62% of positiveand 71.70% of negative after removing stopwords.Then, the average F-score is 78.16%, which wasincreased from the first approach by 0.50%.
Theresults from both approaches showed that, unigramachieved higher scores than either bigrams or tri-grams.Moreover, these experiments have been testedwith a set of SMS messages to assess how well oursystem trained on Twitter data can be generalizedto other types of message data.
The second ap-proach still achieved the better scores (Figure 2),where we were able to obtain an F-score of 77.81%of positive and 84.66% of negative; thus, the aver-age F-score is 81.23%.The results of unigram from the second ap-proach submitted to SemEval 2013 can be found inFigure 3.
After comparing them using the averageF-score from positive and negative class, the re-sults showed that our system works better for SMSmessaging than for Twitter.gonna miss some of my classes.Unigram Bigram Trigramgonnamisssomeofmyclassesgonna missmiss somesome ofof mymy classesgonna miss somemiss some ofsome of myof my classesTable 2: Example of context from first approachgonna miss (some of) my classes.Unigram Bigram Trigramgonnamissmyclassesgonna missmiss mymy classesgonna miss mymiss my classesTable 3: Example of context from second approach.Note ?some?
and ?of?
are listed in NLTK stopwords.Figure 3: Results of unigram of Twitter and SMS in thesecond approach5 Conclusion and Future WorkA system, TJP, has been described that participatedin SemEval 2013 Task 2 part A: Contextual Polari-ty Disambiguation (Wilson et al 2013).
The sys-tem used the Python NLTK (Bird et al009) NaiveBayes classifier trained on Twitter data.
Further-more, emoticons were collected and labeled as pos-itive and negative in order to classify contexts withemoticons.
After analyzing the Twitter messageand SMS messages, we were able to obtain an av-erage F-score of 78.16% and 81.23% respectivelyduring the SemEval 2013 task.
The reason that, oursystem achieved better scores with SMS messagethen Twitter message might be due to our use ofTwitter messages as training data.
However this isstill to be verified experimentally.The experimental performance on the tasksdemonstrates the advantages of simple approaches.This provides a baseline performance set to whichmore sophisticated or resource intensive tech-niques may be compared.Pos Neg AverageTwitter 84.62 71.70 78.16SMS 77.81 84.66 81.23657075808590F-score(%)378For future work, we intend to trace back to theroot words and work with the suffix and prefix thatimply negative semantics, such as ?dis-?, ?un-?, ?-ness?
and ?-less?.
Moreover, we would like to col-lect more shorthand texts than that used commonlyin microblogs, such as gr8 (great), btw (by theway), pov (point of view), gd (good) and ne1 (any-one).
We believe these could help to improve oursystem and achieve better accuracy when classify-ing the sentiment of context from microblogs.ReferencesAlec Go, Richa Bhayani and Lei Huang.
2009.
Twittersentiment classification using distant supervision.CS224N Project Report, Stanford, 1-12.Ayse Cufoglu, Mahi Lohi and Kambiz Madani.
2008.Classification accuracy performance of Naive Bayes-ian (NB), Bayesian Networks (BN), Lazy Learning ofBayesian Rules (LBR) and Instance-Based Learner(IB1) - comparative study.
Paper presented at theComputer Engineering & Systems, 2008.
ICCES2008.
International Conference on.Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
Paper presented at theProceedings of the ACL-02 conference on Empiricalmethods in natural language processing - Volume 10.Fotis Aisopos, George Papadakis and TheodoraVarvarigou.
2011.
Sentiment analysis of social mediacontent using N-Gram graphs.
Paper presented at theProceedings of the 3rd ACM SIGMM internationalworkshop on Social media, Scottsdale, Arizona,USA.Huifeng Tang, Songbo Tan and Xueqi Cheng.
2009.
Asurvey on sentiment detection of reviews.
Expert Sys-tems with Applications, 36(7), 10760-10773.Jonathon.
Read.
2005.
Using emoticons to reduce de-pendency in machine learning techniques for senti-ment classification.
Paper presented at theProceedings of the ACL Student Research Work-shop, Ann Arbor, Michigan.Kushal Dave, Steve Lawrence and David M. Pennock.2003.
Mining the peanut gallery: opinion extractionand semantic classification of product reviews.
Paperpresented at the Proceedings of the 12th internationalconference on World Wide Web, Budapest, Hungary.Lucas R. Hope and Kevin B. Korb.
2004.
A bayesianmetric for evaluating machine learning algorithms.Paper presented at the Proceedings of the 17th Aus-tralian joint conference on Advances in Artificial In-telligence, Cairns, Australia.Mostafa Al Shaikh, Helmut Prendinger and IshizukaMitsuru.
2007.
Assessing Sentiment of Text by Se-mantic Dependency and Contextual Valence Analy-sis.
Paper presented at the Proceedings of the 2nd in-ternational conference on Affective Computing andIntelligent Interaction, Lisbon, Portugal.Pang Bo and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Found.
Trends Inf.
Retr., 2(1-2),1-135.Steven Bird, Ewan Klein and Edward Loper.
2009.
Nat-ural language processing with Python: O'Reilly.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,Sara Rosenthal, Veselin Stoyanov and Alan Ritter.2013.
SemEval-2013 Task 2: Sentiment Analysis inTwitter Proceedings of the 7th International Work-shop on Semantic Evaluation: Association for Com-putational Linguistics.379
