Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 16?20,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsTwo Multivariate Generalizations of Pointwise Mutual InformationTim Van de CruysRCEALUniversity of CambridgeUnited Kingdomtv234@cam.ac.ukAbstractSince its introduction into the NLP community,pointwise mutual information has proven to bea useful association measure in numerous nat-ural language processing applications such ascollocation extraction and word space models.In its original form, it is restricted to the anal-ysis of two-way co-occurrences.
NLP prob-lems, however, need not be restricted to two-way co-occurrences; often, a particular prob-lem can be more naturally tackled when for-mulated as a multi-way problem.
In this pa-per, we explore two multivariate generaliza-tions of pointwise mutual information, and ex-plore their usefulness and nature in the extrac-tion of subject verb object triples.1 IntroductionMutual information (Shannon and Weaver, 1949) isa measure of mutual dependence between two ran-dom variables.
The measure ?
and more specificallyits instantiation for specific outcomes called point-wise mutual information (PMI) ?
has proven to be auseful association measure in numerous natural lan-guage processing applications.
Since its introduc-tion into the NLP community (Church and Hanks,1990), it has been used in order to tackle or im-prove upon several NLP problems, including col-location extraction (ibid.)
and word space mod-els (Pantel and Lin, 2002).
In its original form, it isrestricted to the analysis of two-way co-occurrences.NLP problems, however, need not be restricted totwo-way co-occurrences; often, a particular prob-lem can be more naturally tackled when formulatedas a multi-way problem.
Notably, the frameworkof tensor decomposition, that has recently perme-ated into the NLP community (Turney, 2007; Ba-roni and Lenci, 2010; Giesbrecht, 2010; Van deCruys, 2010), analyzes language issues as multi-way co-occurrences.
Up till now, little attention hasbeen devoted to the weighting of such multi-way co-occurrences (which, for the research cited above, re-sults either in using no weighting at all, or in apply-ing an ad-hoc weighting solution without any theo-retical underpinnings).In this paper, we explore two possible generaliza-tions of pointwise mutual information for multi-wayco-occurrences from a theoretical point of view.
Insection 2, we discuss some relevant related work,mainly in the field of information theory.
In sec-tion 3 the two generalizations of PMI are laid out inmore detail, based on their global multivariate coun-terparts.
Section 4 then discusses some applicationsin the light of NLP, while section 5 concludes andhints at some directions for future research.2 Previous workResearch into the generalization of mutual informa-tion was pioneered in two seminal papers.
The firstone to explore the interaction of multiple randomvariables in the scope of information theory wasMcGill (1954).
McGill described a first generaliza-tion of mutual information based on the notion ofconditional entropy.
This first generalization, calledinteraction information, is described in section 3.2.1below.
A second generalization, solely based onthe commonalities of the random variables, was de-scribed by Watanabe (1960).
This generalization,16called total correlation is presented in section 3.2.2.3 Theory3.1 Mutual informationMutual information is a measure of the amount ofinformation that one random variable contains aboutanother random variable.
It is the reduction inthe uncertainty of one random variable due to theknowledge of the other.I(X;Y ) =?x?X?y?Yp(x, y) logp(x, y)p(x)p(y)(1)Pointwise mutual information is a measure of as-sociation that looks at particular instances of the tworandom variablesX and Y .
More specifically, point-wise mutual information measures the difference be-tween the probability of their co-occurrence giventheir joint distribution and the probability of theirco-occurrence given the marginal distributions of Xand Y (thus assuming the two random variables areindependent).pmi(x, y) = logp(x, y)p(x)p(y)(2)Note that mutual information (equation 1) yieldsthe expected PMI value over all possible instances ofrandom variables X and Y .Ep(X,Y )[pmi(X,Y )] (3)Furthermore, note that PMI may be positive ornegative, but its expected outcome over all events(i.e.
the global mutual information) is always non-negative.3.2 Multivariate mutual informationIn this section, the two generalizations for multivari-ate distributions are presented.
For both generaliza-tions, we examine their standard form (which looksat the interaction between the random variables as awhole) and their specific instantiation (that looks atparticular outcomes of the random variables).
Anal-ogously to PMI, it is these specific instantiationsof the measures that are able to weigh specific co-occurrences according to their importance in the cor-pus.
As with PMI, the value for the global case oughtto be the expected value for all the instantiations ofthe specific measure.3.2.1 Interaction informationInteraction information (McGill, 1954) ?
alsocalled co-information (Bell, 2003) ?
is based on thenotion of conditional mutual information.
Condi-tional mutual information is the mutual informationof two random variables conditioned on a third one.I(X;Y |Z)=?x?X?y?Y?z?Zp(x, y, z) logp(x, y|z)p(x|z)p(y|z)(4)which can be rewritten as?x?X?y?Y?z?Zp(x, y, z) logp(z)p(x, y, z)p(x, z)p(y, z)(5)For the case of three variables, the interaction in-formation is then defined as the conditional mutualinformation subtracted by the standard mutual infor-mation.I1(X;Y ;Z) = I(X;Y |Z)?
I(X;Y )= I(X;Z|Y )?
I(X;Z)= I(Y ;Z|X)?
I(Y ;Z) (6)Expanded, this gives the following equation:I1(X;Y ;Z)=?x?X?y?Y?z?Zp(x, y, z) logp(z)p(x, y, z)p(x, z)p(y, z)?
?x?X?y?Yp(x, y) logp(x, y)p(x)p(y)(7)We can now define specific interaction informa-tion as follows1:1Note that ?
compared to equation 7 ?
the two subparts inthe right-hand side of the equation have been swapped.
For thethree-variable case, this gives exactly the same outcome exceptfor a change in sign.
The swap is necessary in order to ensure aproper set-theoretic measure (Fano, 1961; Reza, 1994).17SI1(x, y, z) = logp(x, y)p(x)p(y)?
logp(z)p(x, y, z)p(x, z)p(y, z)= logp(x, y)p(y, z)p(x, z)p(x)p(y)p(z)p(x, y, z)(8)Interaction information ?
as well as specific in-teraction information ?
can equally be defined forn > 3 variables.3.2.2 Total correlationTotal correlation (Watanabe, 1960) ?
also calledmulti-information (Studeny?
and Vejnarova?, 1998)quantifies the amount of information that is sharedamong the different random variables, and thus ex-presses how related a particular group of randomvariables are.I2(X1, X2, .
.
.
, Xn)=?x1?X1,x2?X2,...xn?Xnp(x1, x2, .
.
.
, xn) logp(x1, x2, .
.
.
, xn)?ni=1p(xi)(9)Analogously to the definition of pointwise mu-tual information, we can straightforwardly define thecorrelation for specific instances of the random vari-ables, which we coin specific correlation.SI2(x1, x2, .
.
.
, xn) = logp(x1, x2, .
.
.
, xn)?ni=1p(xi)(10)For the case of three variables, this gives the follow-ing equation:SI2(x, y, z) = logp(x, y, z)p(x)p(y)p(z)(11)Note that this measure has been used in NLP tasksbefore, notably for collocation extraction (VilladaMoiro?n, 2005).4 ApplicationIn this section, we explore the performance of themeasures defined above in an NLP context, viz.
theextraction of salient subject verb object triples.
Thisresearch has been carried out for Dutch.
The TwenteNieuws Corpus (Ordelman, 2002), a 500M Dutchword corpus, has been automatically parsed withthe Dutch dependency parser ALPINO (van Noord,2006), and all subject verb object triples with fre-quency f ?
3 have been extracted.
Next, a ten-sor T of size I ?
J ?
K has been constructed,containing the three-way co-occurrence frequenciesof the I most frequent subjects by the J most fre-quent verbs by the K most frequent objects, withI = 10000, J = 1000,K = 10000.
Finally, twonew tensors U and V have been constructed, suchthat Uijk = SI1(Tijk) and Vijk = SI2(Tijk), i.e.tensor U has been weighted using specific interac-tion information (equation 8) and tensor V has beenweighted using specific correlation (equation 11).Table 1 shows the top five subject verb objecttriples that received the highest specific interactioninformation score, while table 2 gives the top fivesubject verb object triples that gained the highestspecific correlation score (both with f > 30).Note that both methods are able to extract salientsubject verb object triples, such as prototypical svocombinations (peiling geeft opinie weer ?poll repre-sents opinion?, helikopter vuurt raket af ?helicopterfires rocket?)
and fixed expressions (Dutch proverbssuch as de wal keert het schip ?the circumstanceschange the course?
and de vlag dekt de lading ?thecontent corresponds to the title?
).subject verb object SI1peiling geef weer opinie 18.20?poll?
?represent?
?opinion?helikopter vuur af raket 17.57?helicopter?
?fire?
?rocket?Man bijt hond 17.15?man?
?bite?
?dog?verwijt snijd hout 17.10?reproach?
?cut?
?wood?wal keer schip 17.01?quay?
?turn?
?ship?Table 1: Top five subject verb object triples with highestspecific interaction information scoreComparing both methods, the results seem to in-dicate that the extracted triples are similar for bothweightings.
This, however, is not consistently thecase: the results can differ significantly for partic-18subject verb object SI2verwijt snijd hout 8.05?reproach?
?cut?
?wood?helikopter vuur af raket 7.75?helicopter?
?fire?
?rocket?peiling geef weer opinie 7.64?poll?
?represent?
?opinion?vlag dek lading 7.21?flag?
?cover?
?load?argument snijd hout 7.17?argument?
?cut?
?wood?Table 2: Top five subject verb object triples with highestspecific correlation scoreular instances.
This becomes apparent when com-paring table 3 and table 4, which for each methodcontain the top five combinations for the Dutch verbspeel ?play?.Table 3 indicates that specific interaction informa-tion picks up on prototypical svo combinations (ork-est speelt symfonie ?orchestra plays symphony?
; alsonote the 4 other triples that come from bridge gamedescriptions).
Specific correlation (table 4), on theother hand, picks up on the expression een rol spe-len ?play a role?, and extracts salient subjects that gowith the expression.subject verb object SI1orkest speel symfonie 11.65?orchestra?
?play?
?symphony?leider speel ruiten 10.29?leader?
?play?
?diamonds?leider speel harten 10.20?leader?
?play?
?hearts?leider speel schoppen 10.01?leader?
?play?
?spades?leider speel klaveren 9.89?leader?
?play?
?clubs?Table 3: Top five combinations with highest specific in-teraction information scores for verb speelIn order to quantitatively assess the aptness of thetwo methods for the extraction of salient svo triples,we performed a small-scale manual evaluation of the100 triples that scored the highest for each measure.subject verb object SI2nationaliteit speel rol 4.12?nationality?
?play?
?role?afkomst speel rol 4.06?descent?
?play?
?role?toeval speel rol 4.04?coincidence?
?play?
?role?motief speel rol 4.04?motive?
?play?
?role?afstand speel rol 4.02?distance?
?play?
?role?Table 4: Top five combinations with highest specific cor-relation scores for verb speelA triple is considered salient when it is made up ofa fixed (multi-word) expression, or when it consistsof a fixed expression combined with a salient sub-ject or object (e.g.
argument snijd hout ?argumentcut wood?).
The bare frequency tensor (without anyweighting) was used as a baseline.
The results arepresented in table 5.measure precisionbaseline .00SI1 .24SI2 .31Table 5: Manual evaluation results for the extraction ofsalient svo triplesThe results indicate that both measures are able toextract a significant number of salient triples com-pared to the frequency baseline, which is not ableto extract any salient triples at all.
Comparing bothmeasures, specific correlation clearly performs best(.31 versus .24 for specific interaction information).Additionally, we computed Kendall?s ?b to com-pare the rankings yielded by the two different meth-ods (over all triples).
The correlation between bothrankings is ?b = 0.21, indicating that the resultsyielded by both methods ?
though correlated ?
differto a significant extent.These are, of course, preliminary results, and amore thorough evaluation is necessary to confirm thetendencies that emerge.195 ConclusionIn this paper, we presented two multivariate gen-eralizations of mutual information, as well as theirinstantiated counterparts specific interaction infor-mation and specific correlation, that are useful forweighting multi-way co-occurrences in NLP tasks.The main goal of this paper is to show that there isnot just one straightforward generalization of point-wise mutual information for the multivariate case,and NLP researchers that want to exploit multi-wayco-occurrences in an information-theoretic frame-work should take this fact into account.Moreover, we have applied the two different mea-sures to the extraction of subject verb object triples,and demonstrated that the results may differ signif-icantly.
It goes without saying that these are justexploratory and rudimentary observations; more re-search into the exact nature of both generalizationsand their repercussions for NLP ?
as well as a properquantitative evaluation ?
are imperative.This brings us to some avenues for future work.More research needs to be carried with regard to theexact nature of the dependencies that both measurescapture.
Preliminary results show that they extractdifferent information, but it is not clear what theexact nature of that information is.
Secondly, wewant to carry out a proper quantitative evaluationon different multi-way co-occurrence (factorization)tasks, in order to indicate which measure works best,and which measure might be more suitable for a par-ticular task.AcknowledgementsA number of anonymous reviewers provided fruitfulremarks and comments on an earlier draft of this pa-per, from which the current version has significantlybenefited.ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):1?48.Anthony J.
Bell.
2003.
The co-information lattice.
InProceedings of the Fifth International Workshop on In-dependent Component Analysis and Blind Signal Sep-aration: ICA 2003.Kenneth W. Church and Patrick Hanks.
1990.
Word as-sociation norms, mutual information & lexicography.Computational Linguistics, 16(1):22?29.Robert Fano.
1961.
Transmission of information.
MITPress, Cambridge, MA.Eugenie Giesbrecht.
2010.
Towards a matrix-based dis-tributional model of meaning.
In Proceedings of theNAACL HLT 2010 Student Research Workshop, pages23?28.
Association for Computational Linguistics.William J. McGill.
1954.
Multivariate information trans-mission.
Psychometrika, 19(2):97?116.R.J.F.
Ordelman.
2002.
Twente Nieuws Corpus (TwNC),August.
Parlevink Language Techonology Group.University of Twente.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of ACM Confer-ence on Knowledge Discovery and Data Mining, pages613?619, Edmonton, Canada.Fazlollah M. Reza.
1994.
An introduction to informationtheory.
Dover Publications.Claude Shannon and Warren Weaver.
1949.
The math-ematical theory of communication.
University of Illi-nois Press, Urbana, Illinois.M.
Studeny?
and J. Vejnarova?.
1998.
The multiinforma-tion function as a tool for measuring stochastic depen-dence.
In Proceedings of the NATO Advanced StudyInstitute on Learning in graphical models, pages 261?297, Norwell, MA, USA.
Kluwer Academic Publish-ers.Peter D. Turney.
2007.
Empirical evaluation of four ten-sor decomposition algorithms.
Technical Report ERB-1152, National Research Council, Institute for Infor-mation Technology.Tim Van de Cruys.
2010.
A non-negative tensor fac-torization model for selectional preference induction.Natural Language Engineering, 16(4):417?437.Gertjan van Noord.
2006.
At Last Parsing Is Now Op-erational.
In Piet Mertens, Cedrick Fairon, Anne Dis-ter, and Patrick Watrin, editors, TALN06.
Verbum ExMachina.
Actes de la 13e conference sur le traite-ment automatique des langues naturelles, pages 20?42, Leuven.Begon?a Villada Moiro?n.
2005.
Data-driven identifica-tion of fixed expressions and their modifiability.
Ph.D.thesis, University of Groningen, The Netherlands.Satosi Watanabe.
1960.
Information theoretical analysisof multivariate correlation.
IBM Journal of Researchand Development, 4:66?82.20
