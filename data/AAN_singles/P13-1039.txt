Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392?401,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsProbabilistic Domain Modelling With Contextualized DistributionalSemantic VectorsJackie Chi Kit CheungUniversity of Toronto10 King?s College Rd., Room 3302Toronto, ON, Canada M5S 3G4jcheung@cs.toronto.eduGerald PennUniversity of Toronto10 King?s College Rd., Room 3302Toronto, ON, Canada M5S 3G4gpenn@cs.toronto.eduAbstractGenerative probabilistic models have beenused for content modelling and templateinduction, and are typically trained onsmall corpora in the target domain.
Incontrast, vector space models of distribu-tional semantics are trained on large cor-pora, but are typically applied to domain-general lexical disambiguation tasks.
Weintroduce Distributional Semantic HiddenMarkov Models, a novel variant of a hid-den Markov model that integrates thesetwo approaches by incorporating contex-tualized distributional semantic vectorsinto a generative model as observed emis-sions.
Experiments in slot induction showthat our approach yields improvements inlearning coherent entity clusters in a do-main.
In a subsequent extrinsic evalua-tion, we show that these improvements arealso reflected in multi-document summa-rization.1 IntroductionDetailed domain knowledge is crucial to manyNLP tasks, either as an input for language un-derstanding, or as the goal itself, to acquire suchknowledge.
For example, in information extrac-tion, a list of slots in the target domain is givento the system, and in natural language generation,content models are trained to learn the contentstructure of texts in the target domain for infor-mation structuring and automatic summarization.Generative probabilistic models have been onepopular approach to content modelling.
An impor-tant advantage of this approach is that the structureof the model can be adapted to fit the assumptionsabout the structure of the domain and the natureof the end task.
As this field has progressed, theformal structures that are assumed to represent adomain have increased in complexity and becomemore hierarchical.
Earlier work assumes a flat setof topics (Barzilay and Lee, 2004), which are ex-pressed as states of a latent random variable in themodel.
Later work organizes topics into a hierar-chy from general to specific (Haghighi and Van-derwende, 2009; Celikyilmaz and Hakkani-Tur,2010).
Recently, Cheung et al (2013) formalizeda domain as a set of frames consisting of proto-typical sequences of events, slots, and slot fillersor entities, inspired by classical AI work such asSchank and Abelson?s (1977) scripts.
We adoptmuch of this terminology in this work.
For exam-ple, in the CRIMINAL INVESTIGATIONS domain,there may be events such as a murder, an investi-gation of the crime, an arrest, and a trial.
Thesewould be indicated by event heads such as kill, ar-rest, charge, plead.
Relevant slots would includeVICTIM, SUSPECT, AUTHORITIES, PLEA, etc.One problem faced by this line of work is that,by their nature, these models are typically trainedon a small corpus from the target domain, on theorder of hundreds of documents.
The small size ofthe training corpus makes it difficult to estimate re-liable statistics, especially for more powerful fea-tures such as higher-order N-gram features or syn-tactic features.By contrast, distributional semantic models aretrained on large, domain-general corpora.
Thesemethods model word meaning using the contextsin the training corpus in which the word appears.The most popular approach today is a vector spacerepresentation, in which each dimension corre-sponds to some context word, and the value at thatdimension corresponds to the strength of the as-sociation between the context word and the targetword being modelled.
A notion of word similarityarises naturally from these models by comparingthe similarity of the word vectors, for example byusing a cosine measure.
Recently, these modelshave been extended by considering how distribu-392tional representations can be modified dependingon the specific context in which the word appears(Mitchell and Lapata, 2008, for example).
Con-textualization has been found to improve perfor-mance in tasks like lexical substitution and wordsense disambiguation (Thater et al, 2011).In this paper, we propose to inject contextual-ized distributional semantic vectors into genera-tive probabilistic models, in order to combine theircomplementary strengths for domain modelling.There are a number of potential advantages thatdistributional semantic models offer.
First, theyprovide domain-general representations of wordmeaning that cannot be reliably estimated from thesmall target-domain corpora on which probabilis-tic models are trained.
Second, the contextualiza-tion process allows the semantic vectors to implic-itly encode disambiguated word sense and syntac-tic information, without further adding to the com-plexity of the generative model.Our model, the Distributional Semantic HiddenMarkov Model (DSHMM), incorporates contextu-alized distributional semantic vectors into a gen-erative probabilistic model as observed emissions.We demonstrate the effectiveness of our model intwo domain modelling tasks.
First, we apply it toslot induction on guided summarization data overfive different domains.
We show that our modeloutperforms a baseline version of our method thatdoes not use distributional semantic vectors, aswell as a recent state-of-the-art template inductionmethod.
Then, we perform an extrinsic evaluationusing multi-document summarization, wherein weshow that our model is able to learn event and slottopics that are appropriate to include in a sum-mary.
From a modelling perspective, these resultsshow that probabilistic models for content mod-elling and template induction benefit from distri-butional semantics trained on a much larger cor-pus.
From the perspective of distributional seman-tics, this work broadens the variety of problems towhich distributional semantics can be applied, andproposes methods to perform inference in a prob-abilistic setting beyond geometric measures suchas cosine similarity.2 Related WorkProbabilistic content models were proposed byBarzilay and Lee (2004), and related models havesince become popular for summarization (Fungand Ngai, 2006; Haghighi and Vanderwende,2009), and information ordering (Elsner et al,2007; Louis and Nenkova, 2012).
Other relatedgenerative models include topic models and struc-tured versions thereof (Blei et al, 2003; Gruberet al, 2007; Wallach, 2008).
In terms of domainlearning in the form of template induction, heuris-tic methods involving multiple clustering stepshave been proposed (Filatova et al, 2006; Cham-bers and Jurafsky, 2011).
Most recently, Cheunget al (2013) propose PROFINDER, a probabilis-tic model for frame induction inspired by contentmodels.
Our work is similar in that we assumemuch of the same structure within a domain andconsequently in the model as well (Section 3), butwhereas PROFINDER focuses on finding the ?cor-rect?
number of frames, events, and slots with anonparametric method, this work focuses on in-tegrating global knowledge in the form of distri-butional semantics into a probabilistic model.
Weadopt one of their evaluation procedures and use itto compare with PROFINDER in Section 5.Vector space models form the basis of moderninformation retrieval (Salton et al, 1975), but onlyrecently have distributional models been proposedthat are compositional (Mitchell and Lapata, 2008;Clark et al, 2008; Grefenstette and Sadrzadeh,2011, inter alia), or that contextualize the meaningof a word using other words in the same phrase(co-compositionality) (Erk and Pado?, 2008; Dinuand Lapata, 2010; Thater et al, 2011).
We re-cently showed how such models can be evaluatedfor their ability to support semantic inference foruse in complex NLP tasks like question answeringor automatic summarization (Cheung and Penn,2012).Combining distributional information and prob-abilistic models has actually been explored in pre-vious work.
Usually, an ad-hoc clustering stepprecedes training and is used to bias the initializa-tion of the probabilistic model (Barzilay and Lee,2004; Louis and Nenkova, 2012), or the clusteringis interleaved with iterations of training (Fung etal., 2003).
By contrast, our method better modu-larizes the two, and provides a principled way totrain the model.
More importantly, previous ad-hoc clustering methods only use distributional in-formation derived from the target domain itself;initializing based on domain-general distributionalinformation can be problematic because it can biastraining towards a local optimum that is inappro-priate for the target domain, leading to poor per-393?1?1?1?1?1???????????.
.
.??
?????
??????
??
?????
?????
?Figure 1: Graphical representation of our model.Distributions that generate the latent variables andhyperparameters are omitted for clarity.formance.3 Distributional Semantic HiddenMarkov ModelsWe now describe the DSHMM model.
This modelcan be thought of as an HMM with two layersof latent variables, representing events and slotsin the domain.
Given a document consisting ofa sequence of T clauses headed by propositionalheads ~H (verbs or event nouns), and argumentnoun phrases ~A, a DSHMM models the joint prob-ability of observations ~H , ~A, and latent randomvariables ~E and ~S representing domain events andslots respectively; i.e., P ( ~H, ~A, ~E, ~S).The basic structure of our model is similar toPROFINDER.
Each timestep in the model gener-ates one clause in the document.
More specifi-cally, it generates the event heads and argumentswhich are crucial in identifying events and slots.We assume that event heads are verbs or eventnouns, while arguments are the head words of theirsyntactically dependent noun phrases.
We also as-sume that the sequence of clauses and the clause-internal syntactic structure are fixed, for exampleby applying a dependency parser.
Within eachclause, a hierarchy of latent and observed variablesmaps to corresponding elements in the clause (Ta-ble 1), as follows:Event Variables At the top-level, a categoricallatent variable Et with NE possible states repre-sents the event that is described by clause t. Itsvalue is conditioned on the previous time step?sevent variable, following the standard, first-orderMarkov assumption (PE(Et|Et?1), or PEinit(E1)Node Component Textual unitEt Event ClauseSta Slot Noun phraseHt Event head Verb/event nounAta Event argument Noun phraseTable 1: The correspondence between nodes in ourgraphical model, the domain components that theymodel, and the related elements in the clause.for the first clause).
The internal structure of theclause is generated by conditioning on the state ofEt, including the head of the clause, and the slotsfor each argument in the clause.Slot Variables Categorical latent variables withNS possible states represent the slot that an argu-ment fills, and are conditioned on the event vari-able in the clause, Et (i.e., PS(Sta|Et), for theath slot variable).
The state of Sta is then used togenerate an argument Ata.Head and Argument Emissions The head ofthe clause Ht is conditionally dependent on Et,and each argument Ata is likewise conditioned onits slot variable Sta.
Unlike in most applications ofHMMs in text processing, in which the represen-tation of a token is simply its word or lemma iden-tity, tokens in DSHMM are also associated with avector representation of their meaning in contextaccording to a distributional semantic model (Sec-tion 3.1).
Thus, the emissions can be decomposedinto pairs Ht = (lemma(Ht), sem(Ht)) andAta = (lemma(Ata), sem(Ata)), where lemmaand sem are functions that return the lemma iden-tity and the semantic vector respectively.
Theprobability of the head of a clause is thus:PH(Ht|Et) = PHlemm(lemma(Ht)|Et) (1)?
PHsem(sem(Ht)|Et),and the probability of a clausal argument is like-wise:PA(Ata|Sta) = PAlemm(lemma(Ata)|Sta) (2)?
PAsem(sem(Ata)|Sta).All categorical distributions are smoothed usingadd-?
smoothing (i.e., uniform Dirichlet priors).Based on the independence assumptions describedabove, the joint probability distribution can be fac-394tored into:P ( ~H, ~A, ~E, ~S) = PEinit(E1) (3)?T?t=2PE(Et|Et?1)T?t=1PH(Ht|Et)?T?t=1Ct?a=1PS(Sta|Et)PA(Ata|Sta).3.1 Vector Space Models of SemanticsIn this section, we describe several methods forproducing the semantic vectors associated witheach event head or argument; i.e., the functionsem.
We chose several simple, but widely studiedmodels, to investigate whether they can be effec-tively integrated into DSHMM.
We start with a de-scription of the training of a basic model withoutany contextualization, then describe several con-textualized models based on recent work.Simple Vector Space Model In the basic ver-sion of the model (SIMPLE), we train a term-context matrix, where rows correspond to targetwords, and columns correspond to context words.Training begins by counting context words that ap-pear within five words of the target word, ignor-ing stopwords.
We then convert the raw countsto positive pointwise mutual information scores,which has been shown to improve word similaritycorrelation results (Turney and Pantel, 2010).
Weset thresholds on the frequencies of words for in-clusion as target and context words (given in Sec-tion 4).
Target words which fall below the thresh-old are modelled as UNK.
All the methods belowstart from this basic vector representation.Component-wise Operators Mitchell and Lap-ata (2008) investigate using component-wise op-erators to combine the vectors of verbs and theirintransitive subjects.
We use component-wise op-erators to contextualize our vectors, but by com-bining with all of the arguments, and regardlessof the event head?s category.
Let event head hbe the syntactic head of a number of argumentsa1, a2, ...am, and ~vh, ~va1 , ~va2 , ...~vam be their re-spective vector representations according to theSIMPLE method.
Then, their contextualized vec-tors ~cM&Lh ,~cM&La1 , ...~cM&Lam would be:~cM&Lh = ~vh  (m?i=1~vam) (4)~cM&Lai = ~vai  ~vh,?i = 1...m, (5)where  represents a component-wise operator,addition or multiplication, and ?
represents itsrepeated application.
We tested component-wiseaddition (M&L+) and multiplication (M&L?
).Selectional Preferences Erk and Pado?
(2008)(E&P) incorporate inverse selectional preferencesinto their contextualization function.
The intu-ition is that a word should be contextualized suchthat its vector representation becomes more sim-ilar to the vectors of other words that its depen-dency neighbours often take in the same syntacticposition.
For example, suppose catch is the headof the noun ball, in the relation of a direct object.Then, the vector for ball would be contextualizedto become similar to the vectors for other frequentdirect objects of catch, such as baseball, or cold.Likewise, the vector for catch would be contextu-alized to become similar to the vectors for throw,hit, etc.
Formally, let h take a as its argument inrelation r. Then:~cE&Ph = ~vh ?m?i=1?w?Lfreq(w, r, ai) ?
~vw, (6)~cE&Pa = ~va ?
?w?Lfreq(h, r, w) ?
~vw, (7)where freq(h, r, a) is the frequency of h occur-ring as the head of a in relation r in the train-ing corpus, L is the lexicon, and ?
representscomponent-wise multiplication.Dimensionality Reduction and Vector EmissionAfter contextualization, we apply singular valuedecomposition (SVD) for dimensionality reduc-tion to reduce the number of model parameters,keeping the k most significant singular values andvectors.
In particular, we apply SVD to the m-by-n term-context matrix M produced by the SIM-PLE method, resulting in the truncated matricesM ?
Uk?kV Tk , where Uk is a m-by-k matrix, ?kis k-by-k, and Vk is n-by-k.
This takes place af-ter contextualization, so the component-wise op-erators apply in the original semantic space.
Af-terwards, the contextualized vector in the originalspace, ~c, can be transformed into a vector in thereduced space, ~cR, by ~cR = ?
?1k V Tk ~c.Distributional semantic vectors are traditionallycompared by measures which ignore vector mag-nitudes, such as cosine similarity, but a multivari-ate Gaussian is sensitive to magnitudes.
Thus, thefinal step is to normalize ~cR into a unit vector bydividing it by its L2 norm, ||~cR||.395We model the emission of these contextualizedvectors in DSHMM as multivariate Gaussian dis-tributions, so the semantic vector emissions can bewritten as PHsem, PAsem ?
N (?,?
), where ?
?
Rkis the mean and ?
?
Rk?k is the covariancematrix.
To avoid overfitting, we regularize thecovariance using its conjugate prior, the Inverse-Wishart distribution.
We follow the ?neutral?
set-ting of hyperparameters given by Ormoneit andTresp (1995), so that the MAP estimate for the co-variance matrix for (event or slot) state i becomes:?i =?j rij(xj ?
?i)(xj ?
?i)T + ?I?j rij + 1, (8)where j indexes all the relevant semantic vectorsxj in the training set, rij is the posterior respon-sibility of state i for vector xj , and ?
is the re-maining hyperparameter that we tune to adjust theamount of regularization.
To further reduce modelcomplexity, we set the off-diagonal entries of theresulting covariance matrix to zero.3.2 Training and InferenceInference in DSHMM is accomplished by the stan-dard Inside-Outside and tree-Viterbi algorithms,except that the tree structure is fixed, so thereis no need to sum over all possible subtrees.Model parameters are learned by the Expectation-Maximization (EM) algorithm.
We tune the hy-perparameters (NE , NS , ?, ?, k) and the numberof EM iterations by two-fold cross-validation1.3.3 Summary and Generative ProcessIn summary, the following steps are applied totrain a DSHMM:1.
Train a distributional semantic model on alarge, domain-general corpus.2.
Preprocess and generate contextualized vec-tors of event heads and arguments in thesmall corpus in the target domain.3.
Train the DSHMM using the EM algorithm.The formal generative process is as follows:1.
Draw categorical distributions PEinit;PE , PS , PHlemm (one per event state);PAlemm (one per slot state) from Dirichletpriors.2.
Draw multivariate Gaussians PHsem, PAsem foreach event and slot state, respectively.1The topic cluster splits and the hyperparameter set-tings are available at http://www.cs.toronto.edu/?jcheung/dshmm/dshmm.html.3.
Generate the documents, clause by clause.Generating a clause at position t consists ofthese steps:1.
Generate the event state Et ?
PE (or PEinit).2.
Generate the event head componentslemm(Ht) ?
PHlemm, sem(Ht) ?
PHsem.3.
Generate a number of slot states Sta ?
PS .4.
For each slot, generate the argument compo-nents lemm(Ata) ?
PAlemm, sem(Ata) ?PAsem.4 ExperimentsWe trained the distributional semantic models us-ing the Annotated Gigaword corpus (Napoles etal., 2012), which has been automatically prepro-cessed and is based on Gigaword 5th edition.
Thiscorpus contains almost ten million news articlesand more than 4 billion tokens.
We used those ar-ticles marked as ?stories?
?
the vast majority ofthem.
We modelled the 50,000 most common lem-mata as target words, and the 3,000 most commonlemmata as context words.We then trained DSHMM and conducted ourevaluations on the TAC 2010 guided summa-rization data set (Owczarzak and Dang, 2010).Lemmatization and extraction of event heads andarguments are done by preprocessing with theStanford CoreNLP tool suite (Toutanova et al,2003; de Marneffe et al, 2006).
This data set con-tains 46 topic clusters of 20 articles each, groupedinto five topic categories or domains.
For exam-ple, one topic cluster in the ATTACK category isabout the Columbine Massacre.
Each topic clustercontains eight human-written ?model?
summaries(?model?
here meaning a gold standard).
Half ofthe articles and model summaries in a topic clusterare used in the guided summarization task, and therest are used in the update summarization task.We chose this data set because it allows usto conduct various domain-modelling evaluations.First, templates for the domains are provided, andthe model summaries are annotated with slotsfrom the template, allowing for an intrinsic eval-uation of slot induction (Section 5).
Second, itcontains multiple domain instances for each of thedomains, and each domain instance comes anno-tated with eight model summaries, allowing for anextrinsic evaluation of our system (Section 6).3965 Guided Summarization Slot InductionWe first evaluated our models on their ability toproduce coherent clusters of entities belonging tothe same slot, adopting the experimental proce-dure of Cheung et al (2013).As part of the official TAC evaluation proce-dure, model summaries were manually segmentedinto contributors, and labelled with the slot inthe TAC template that the contributor expresses.For example, a summary fragment such as On 20April 1999, a massacre occurred at ColumbineHigh School is segmented into the contributors:(On 20 April 1999, WHEN); (a massacre oc-curred, WHAT); and (at Columbine High School,WHERE).In the slot induction evaluation, this annotationis used as follows.
First, the maximal noun phrasesare extracted from the contributors and clusteredbased on the TAC slot of the contributor.
Theseclusters of noun phrases then become the goldstandard clusters against which automatic systemsare compared.
Noun phrases are considered to bematched if the lemmata of their head words are thesame and they are extracted from the same sum-mary.
This accounts for the fact that human an-notators often only label the first occurrence of aword that belongs to a slot in a summary, and fol-lows the standard evaluation procedure in previ-ous information extraction tasks, such as MUC-4.Pronouns and demonstratives are ignored.
Thisextraction process is noisy, because the meaningof some contributors depends on an entire verbphrase, but we keep this representation to allowa direct comparison to previous work.Because we are evaluating unsupervised sys-tems, the clusters produced by the systems are notlabelled, and must be matched to the gold stan-dard clusters.
This matching is performed by map-ping to each gold cluster the best system clusteraccording to F1.
The same system cluster may bemapped multiple times, because several TAC slotscan overlap.
For example, in the NATURAL DIS-ASTERS domain, an earthquake may fit both theWHAT slot as well as the CAUSE slot, because itgenerated a tsunami.We trained a DSHMM separately for each of thefive domains with different semantic models, tun-ing hyperparameters by two-fold cross-validation.We then extracted noun phrase clusters from themodel summaries according to the slot labels pro-duced by running the Viterbi algorithm on them.Method P R F1HMM w/o semantics 13.8 64.1 22.6*DSHMM w/ SIMPLE 20.9 27.5 23.7DSHMM w/ E&P 20.7 27.9 23.8PROFINDER 23.7 25.0 24.3DSHMM w/ M&L+ 19.7 36.3 25.6*DSHMM w/ M&L?
22.1 33.2 26.5*Table 2: Slot induction results on the TAC guidedsummarization data set.
Asterisks (*) indicatethat the model is statistically significantly differ-ent from PROFINDER in terms of F1 at p < 0.05.Results We compared DSHMM to two base-lines.
Our first baseline is PROFINDER, a state-of-the-art template inducer which Cheung et al(2013) showed to outperform the previous heuris-tic clustering method of Chambers and Jurafsky(2011).
Our second baseline is our DSHMMmodel, without the semantic vector component,(HMM w/o semantics).
To calculate statisticalsignificance, we use the paired bootstrap method,which can accommodate complex evaluation met-rics like F1 (Berg-Kirkpatrick et al, 2012).Table 2 shows that performance of the mod-els.
Overall, PROFINDER significantly outper-forms the HMM baseline, but not any of theDSHMM models by F1.
DSHMM with contextu-alized semantic vectors achieves the highest F1s,and are significantly better than PROFINDER.
Allof the differences in precision and recall betweenPROFINDER and the other models are significant.The baseline HMM model has highly imbalancedprecision and recall.
We think this is because themodel is unable to successfully produce coher-ent clusters, so the best-case mapping procedureduring evaluation picked large clusters that havehigh recall.
PROFINDER has slightly higher preci-sion, which may be due to its non-parametric split-merge heuristic.
We plan to investigate whetherthis learning method could improve DSHMM?sperformance further.
Importantly, the contextual-ization of the vectors seems to be beneficial, atleast with the M&L component-wise operators.In the next section, we show that the improve-ment from contextualization transfers to multi-document summarization results.3976 Multi-document Summarization: AnExtrinsic EvaluationWe next evaluated our models extrinsically in thesetting of extractive, multi-document summariza-tion.
To use the trained DSHMM for extractivesummarization, we need a decoding procedure forselecting sentences in the source text to include inthe summary.
Inspired by the KLSUM and HI-ERSUM methods of Haghighi and Vanderwende(2009), we develop a criterion based on Kullback-Leibler (KL) divergence between distributions es-timated from the source text, and those estimatedfrom the summary.
The assumption here is thatthese distributions should match in a good sum-mary.
We describe two methods to use this crite-rion: a basic unsupervised method (Section 6.1),and a supervised variant that makes use of in-domain summaries to learn the salient slots andevents in the domain (Section 6.2).6.1 A KL-based CriterionThere are four main component distributions fromour model that should be considered during extrac-tion: (1) the distribution of events, (2) the distri-bution of slots, (3) the distribution of event heads,and (4) the distribution of arguments.
We estimate(1) as the context-independent probability of beingin a certain event state, which can be calculatedusing the Inside-Outside algorithm.
Given a col-lection of documents D which make up the sourcetext, the distribution of event topics P?E(E) is es-timated as:P?E(E = e) = 1Z?d?D?tInt(e)Outt(e)P (d) , (9)where Int(e) and Outt(e) are the values of theinside and outside trellises at timestep t for someevent state e, and Z is a normalization constant.The distribution for a set of sentences in a can-didate summary, Q?E(E), is identical, except thesummation is over the clauses in the candidatesummary.
Slot distributions P?S(S) and Q?S(S) (2)are defined analogously, where the summation oc-curs along all the slot variables.For (3) and (4), we simply use the MLE es-timates of the lemma emissions, where the esti-mates are made over the source text and the can-didate summary instead of over the entire train-ing set.
All of the candidate summary distribu-tions (i.e., the ?Q?
distributions?)
are smoothed bya small amount, so that the KL-divergence is al-ways finite.
Our KL criterion combines the abovecomponents linearly, weighting the lemma distri-butions by the probability of their respective eventor slot state:KLScore = (10)DKL(P?E ||Q?E) +DKL(P?S ||Q?S)+NE?e=1P?E(e)DKL(P?H(H|e)||Q?H(H|e))+NS?s=1P?S(s)DKL(P?A(A|s)||Q?A(A|s))To produce a summary, sentences from thesource text are greedily added such thatKLScoreis minimized at each step, until the desired sum-mary length is reached, discarding sentences withfewer than five words.6.2 Supervised LearningThe above unsupervised method results in sum-maries that closely mirror the source text in termsof the event and slot distributions, but this ig-nores the fact that not all such topics should beincluded in a summary.
It also ignores genre-specific, stylistic considerations about character-istics of good summary sentences.
For example,Woodsend and Lapata (2012) find several factorsthat indicate sentences should not be included inan extractive summary, such as the presence ofpersonal pronouns.
Thus, we implemented a sec-ond method, in which we modify the KL criterionabove by estimating P?E and P?S from other modelsummaries that are drawn from the same domain(i.e.
topic category), except for those summariesthat are written for the specific topic cluster to beused for evaluation.6.3 Method and ResultsWe used the best performing models from the slotinduction task and the above unsupervised and su-pervised methods based on KL-divergence to pro-duce 100-word summaries of the guided summa-rization source text clusters.
We did not com-pare against PROFINDER, as its structure is dif-ferent and would have required a different proce-dure than the KL-criterion we developed above.As shown in the previous evaluation, however, theHMM baseline without semantics and DSHMMwith SIMPLE perform similarly in terms of F1,398Method ROUGE-1 ROUGE-2 ROUGE-SU4unsup.
sup.
unsup.
sup.
unsup.
sup.Leading baseline 28.0 ?
5.39 ?
8.6 ?HMM w/o semantics 32.3 32.7 6.45 6.49 10.1 10.2DSHMM w/ SIMPLE 32.1 32.7 5.81 6.50 9.8 10.2DSHMM w/ M&L+ 32.1 33.4 6.27 6.82 10.0 10.6DSHMM w/ M&L?
32.4 34.3* 6.35 7.11?
10.2 11.0*DSHMM w/ E&P 32.8 33.8* 6.38 7.31* 10.3 10.8*Table 3: TAC 2010 summarization results by three settings of ROUGE.
Asterisks (*) indicate that themodel is statistically significantly better than the HMM model without semantics at a 95% confidenceinterval, a caret ?
indicates that the value is marginally so.so we consider these competitive baselines.
Wedid not evaluate with the update summarizationtask, because our method has not been adapted toit.
For the evaluation measure, we used the stan-dard ROUGE suite of automatic evaluation mea-sures (Lin, 2004).
Note that the evaluation con-ditions of TAC 2010 are different, and thus thoseresults are not directly comparable to ours.
For in-stance, top performing systems in TAC 2010 makeuse of manually constructed lists of entities knownto fit the slots in the provided templates and sam-ple topic statements, which our method automat-ically learns.
We include the leading baseline re-sults from the competition as a point of reference,as it is a well-known and non-trivial one for newsarticles.
This baseline summary consists of theleading sentences from the most recent documentin the source text cluster up to the word lengthlimit.Table 3 shows the summarization results for thethree most widely-used settings of ROUGE.
Allof our models outperform the leading baseline bya large margin, demonstrating the effective of theKL-criterion.
In terms of unsupervised perfor-mance, all of our models perform similarly.
Be-cause the unsupervised method mimics the distri-butions in the source text at all levels, the methodmay negate the benefit of learning and simply pro-duce summaries that match the source text in theword distributions, thus being an approximationof KLSUM.
Looking at the supervised results,however, the semantic vector models show cleargains in ROUGE, whereas the baseline methoddoes not obtain much benefit from supervision.
Asin the previous evaluation, the models with con-textualized semantic vectors provide the best per-formance.
M&L?
performs very well, as in slotinduction, but E&P also performs well, unlike inthe previous evaluation.
This result reinforces theimportance of the contextualization procedure fordistributional semantic models.Analysis To better understand what is gained bysupervision using in-domain summaries, we ana-lyzed the best performing M&L?
model?s outputsummaries for one document cluster from eachdomain.
For each event state, we calculated theratio P?Esumm(e)/P?Esource(e), for the probability ofan event state e as estimated from the trainingsummaries and the the source text respectively.Likewise, we calculated P?Ssumm(s)/P?Ssource(s) forthe slot states.
This ratio indicates the change instate?s probability after supervision; the greater theratio, the more preferred that state becomes aftertraining.
We selected the most preferred and dis-preferred event and slot for each document clus-ter, and took the three most probable lemmatafrom the associated lemma distribution (Table 4).It seems that supervision is beneficial because itpicks out important event heads and arguments inthe domain, such as charge, trial, and murder inthe TRIALS domain.
It also helps the summarizeravoid semantically generic words (be or have),pronouns, quotatives, and common but irrelevantwords (home, city, restaurant in TRIALS).7 ConclusionWe have shown that contextualized distributionalsemantic vectors can be successfully integratedinto a generative probabilistic model for domainmodelling, as demonstrated by improvements inslot induction and multi-document summariza-tion.
The effectiveness of our model stems fromthe use of a large domain-general corpus to trainthe distributional semantic vectors, and the im-plicit syntactic and word sense information pro-399Domain Event Heads Slot Arguments+ ?
+ ?ATTACKS say2, cause,doctor say2, be, have attack, hostage,troops he, it, theyTRIALS charge, trial,accuse say, be, haveprison, murder,chargehome, city, restau-rantRESOURCES reduce, increase,university say, be, havegovernment,effort, program he, they, itDISASTERS flood, strengthen,engulf say, be, haveproduction,statoil, barrel he, it, theyHEALTH be, department,have say, do, makefood, product,meat she, people, wayTable 4: Analysis of the most probable event heads and arguments in the most preferred (+) and dispre-ferred (?)
events and slots after supervised training.vided by the contextualization process.
Our ap-proach is modular, and allows principled train-ing of the probabilistic model using standard tech-niques.
While we have focused on the overall clus-tering of entities and the distribution of event andslot topics in this work, we would also like to in-vestigate discourse modelling and content struc-turing.
Finally, our work shows that the applica-tion of distributional semantics to NLP tasks neednot be confined to lexical disambiguation.
Wewould like to see modern distributional semanticmethods incorporated into an even greater varietyof applications.AcknowledgmentsThis work is supported by the Natural Sciencesand Engineering Research Council of Canada.ReferencesRegina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedingsof the Human Language Technology Conference ofthe North American Chapter of the Association forComputational Linguistics: HLT-NAACL 2004.Taylor Berg-Kirkpatrick, David Burkett, and DanKlein.
2012.
An empirical investigation of statisti-cal significance in NLP.
In Proceedings of the 2012Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, Jeju Island, Korea, July.
Asso-ciation for Computational Linguistics.2The event head say happens to appear in both the mostpreferred and dispreferred events in the ATTACKS domain.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
The Journal ofMachine Learning Research, 3.Asli Celikyilmaz and Dilek Hakkani-Tur.
2010.
A hy-brid hierarchical model for multi-document summa-rization.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 815?824, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Nathanael Chambers and Dan Jurafsky.
2011.Template-based information extraction without thetemplates.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 976?986, Portland, Oregon, USA, June.
Association forComputational Linguistics.Jackie Chi Kit Cheung and Gerald Penn.
2012.
Evalu-ating distributional models of semantics for syntacti-cally invariant inference.
In Proceedings of the 13thConference of the European Chapter of the Associ-ation for Computational Linguistics, pages 33?43,Avignon, France, April.
Association for Computa-tional Linguistics.Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-derwende.
2013.
Probabilistic frame induction.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies.Stephen Clark, Bob Coecke, and MehrnooshSadrzadeh.
2008.
A compositional distribu-tional model of meaning.
In Proceedings of theSecond Quantum Interaction Symposium (QI-2008).Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InIn LREC 2006.Georgiana Dinu and Mirella Lapata.
2010.
Measur-ing distributional similarity in context.
In Proceed-400ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 1162?1172.Micha Elsner, Joseph Austerweil, and Eugene Char-niak.
2007.
A unified local and global model fordiscourse coherence.
In Human Language Tech-nologies 2007: The Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics; Proceedings of the Main Conference,Rochester, New York, April.
Association for Com-putational Linguistics.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 897?906.Elena Filatova, Vasileios Hatzivassiloglou, and Kath-leen McKeown.
2006.
Automatic creation ofdomain templates.
In Proceedings of the COL-ING/ACL 2006 Main Conference Poster Sessions,pages 207?214, Sydney, Australia, July.
Associationfor Computational Linguistics.Pascale Fung and Grace Ngai.
2006.
One story, oneflow: Hidden markov story models for multilin-gual multidocument summarization.
ACM Transac-tions on Speech and Language Processing (TSLP),3(2):1?16.Pascale Fung, Grace Ngai, and Chi-Shun Cheung.2003.
Combining optimal clustering and hiddenmarkov models for extractive summarization.
InProceedings of the ACL 2003 Workshop on Multilin-gual Summarization and Question Answering, pages21?28, Sapporo, Japan, July.
Association for Com-putational Linguistics.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 1394?1404, Edinburgh, Scotland, UK., July.
Associationfor Computational Linguistics.Amit Gruber, Michael Rosen-Zvi, and Yair Weiss.2007.
Hidden topic markov models.
Artificial In-telligence and Statistics (AISTATS).Aria Haghighi and Lucy Vanderwende.
2009.
Ex-ploring content models for multi-document summa-rization.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 362?370, Boulder, Col-orado, June.
Association for Computational Linguis-tics.Chin Y. Lin.
2004.
ROUGE: A package for automaticevaluation of summaries.
In Stan Szpakowicz andMarie-Francine Moens, editors, Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associa-tion for Computational Linguistics.Annie Louis and Ani Nenkova.
2012.
A coherencemodel based on syntactic patterns.
In Proceedingsof the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244.1992.
Proceedings of the Fourth Message Understand-ing Conference (MUC-4).
Morgan Kaufmann.Courtney Napoles, Matthew Gormley, and BenjaminVan Durme.
2012.
Annotated gigaword.
In Pro-ceedings of the NAACL-HLT Joint Workshop on Au-tomatic Knowledge Base Construction & Web-scaleKnowledge Extraction (AKBC-WEKEX), pages 95?100.Dirk Ormoneit and Volker Tresp.
1995.
Improvedgaussian mixture density estimates using bayesianpenalty terms and network averaging.
In Advancesin Neural Information Processing, pages 542?548.Karolina Owczarzak and Hoa T. Dang.
2010.
TAC2010 guided summarization task guidelines.Gerard Salton, Anita Wong, and Chung-Shu Yang.1975.
A vector space model for automatic indexing.Communications of the ACM, 18(11):613?620.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,Plans, Goals, and Understanding: An Inquiry IntoHuman Knowledge Structures.
Lawrence Erlbaum,July.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and ef-fective vector model.
In Proceedings of IJCNLP.Kristina Toutanova, Dan Klein, Christoper D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, page 180.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Hanna M. Wallach.
2008.
Structured topic models forlanguage.
Doctoral dissertation, University of Cam-bridge.Kristian Woodsend and Mirella Lapata.
2012.
Mul-tiple aspect summarization using integer linear pro-gramming.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, Jeju Island, Korea, July.
Association forComputational Linguistics.401
