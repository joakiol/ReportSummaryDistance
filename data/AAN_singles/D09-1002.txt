Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11?20,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPGraph Alignment for Semi-Supervised Semantic Role LabelingHagen F?urstenauDept.
of Computational LinguisticsSaarland UniversitySaarbr?ucken, Germanyhagenf@coli.uni-saarland.deMirella LapataSchool of InformaticsUniversity of EdinburghEdinburgh, UKmlap@inf.ed.ac.ukAbstractUnknown lexical items present a majorobstacle to the development of broad-coverage semantic role labeling systems.We address this problem with a semi-supervised learning approach which ac-quires training instances for unseen verbsfrom an unlabeled corpus.
Our method re-lies on the hypothesis that unknown lexicalitems will be structurally and semanticallysimilar to known items for which annota-tions are available.
Accordingly, we rep-resent known and unknown sentences asgraphs, formalize the search for the mostsimilar verb as a graph alignment prob-lem and solve the optimization using inte-ger linear programming.
Experimental re-sults show that role labeling performancefor unknown lexical items improves withtraining data produced automatically byour method.1 IntroductionSemantic role labeling, the task of automaticallyidentifying the semantic roles conveyed by sen-tential constituents, has recently attracted much at-tention in the literature.
The ability to express therelations between predicates and their argumentswhile abstracting over surface syntactic configu-rations holds promise for many applications thatrequire broad coverage semantic processing.
Ex-amples include information extraction (Surdeanuet al, 2003), question answering (Narayananand Harabagiu, 2004), machine translation (Boas,2005), and summarization (Melli et al, 2005).Much progress in the area of semantic role la-beling is due to the creation of resources likeFrameNet (Fillmore et al, 2003), which documentthe surface realization of semantic roles in realworld corpora.
Such data is paramount for de-veloping semantic role labelers which are usuallybased on supervised learning techniques and thusrequire training on role-annotated data.
Examplesof the training instances provided in FrameNet aregiven below:(1) a.
If [you]Agent[carelessly]Mannerchance going back there, youdeserve what you get.b.
Only [one winner]Buyerpurchased[the paintings]Goodsc.
[Rachel]Agentinjured [herfriend]Victim[by closing the cardoor on his left hand]Means.Each verb in the example sentences evokes a framewhich is situation-specific.
For instance, chanceevokes the Daring frame, purchased the Com-merce buy frame, and injured the Cause harmframe.
In addition, frames are associated withsemantic roles corresponding to salient entitiespresent in the situation evoked by the predicate.The semantic roles for the frame Daring are Agentand Manner, whereas for Commerce buy these areBuyer and Goods.
A system trained on largeamounts of such hand-annotated sentences typi-cally learns to identify the boundaries of the argu-ments of the verb predicate (argument identifica-tion) and label themwith semantic roles (argumentclassification).A variety of methods have been developed forsemantic role labeling with reasonably good per-formance (F1measures in the low 80s on standardtest collections for English; we refer the interestedreader to the proceedings of the SemEval-2007shared task (Baker et al, 2007) for an overviewof the state-of-the-art).
Unfortunately, the relianceon training data, which is both difficult and highlyexpensive to produce, presents a major obstacleto the widespread application of semantic role la-beling across different languages and text gen-res.
The English FrameNet (version 1.3) is not11a small resource ?
it contains 502 frames cov-ering 5,866 lexical entries and 135,000 annotatedsentences.
Nevertheless, by virtue of being un-der development it is incomplete.
Lexical items(i.e., predicates evoking existing frames) are miss-ing as well as frames and annotated sentences(their number varies greatly across lexical items).Considering how the performance of supervisedsystems degrades on out-of-domain data (Bakeret al, 2007), not to mention unseen events, semi-supervised or unsupervised methods seem to offerthe primary near-term hope for broad coverage se-mantic role labeling.In this work, we develop a semi-supervisedmethod for enhancing FrameNet with additionalannotations which could then be used for clas-sifier training.
We assume that an initial set oflabeled examples is available.
Then, faced withan unknown predicate, i.e., a predicate that doesnot evoke any frame according to the FrameNetdatabase, we must decide (a) which frames it be-longs to and (b) how to automatically annotateexample sentences containing the predicate.
Wesolve both problems jointly, using a graph align-ment algorithm.
Specifically, we view the taskof inferring annotations for new verbs as an in-stance of a structural matching problem and fol-low a graph-based formulation for pairwise globalnetwork alignment (Klau, 2009).
Labeled and un-labeled sentences are represented as dependency-graphs; we formulate the search for an optimalalignment as an integer linear program where dif-ferent graph alignments are scored using a func-tion based on semantic and structural similarity.We evaluate our algorithm in two ways.
We assesshow accurate it is in predicting the frame for anunknown verb and also evaluate whether the an-notations we produce are useful for semantic rolelabeling.In the following section we provide an overviewof related work.
Next, we describe our graph-alignment model in more detail (Section 3) andpresent the resources and evaluation methodologyused in our experiments (Section 4).
We concludethe paper by presenting and discussing our results.2 Related WorkMuch previous work has focused on creatingFrameNet-style annotations for languages otherthan English.
A common strategy is to exploitparallel corpora and transfer annotations fromEnglish sentences onto their translations (Pad?oand Lapata, 2006; Johansson and Nugues, 2006).Other work attempts to automatically augment theEnglish FrameNet in a monolingual setting eitherby extending its coverage or by creating additionaltraining data.There has been growing interest recently indetermining the frame membership for unknownpredicates.
This is a challenging task, FrameNetcurrently lists 502 frames with example sentenceswhich are simply too many (potentially related)classes to consider for a hypothetical system.Moreover, predicates may have to be assigned tomultiple frames, on account of lexical ambiguity.Previous work has mainly used WordNet (Fell-baum, 1998) to extend FrameNet.
For example,Burchardt et al (2005) apply a word sense dis-ambiguation system to annotate predicates witha WordNet sense and hyponyms of these predi-cates are then assumed to evoke the same frame.Johansson and Nugues (2007) treat this problemas an instance of supervised classification.
Usinga feature representation based also on WordNet,they learn a classifier for each frame which decideswhether an unseen word belongs to the frame ornot.
Pennacchiotti et al (2008) create ?distribu-tional profiles?
for frames.
Each frame is repre-sented as a vector, the (weighted) centroid of thevectors representing the meaning of the predicatesit evokes.
Unknown predicates are then assignedto the most similar frame.
They also propose aWordNet-based model that computes the similar-ity between the synsets representing an unknownpredicate and those activated by the predicates ofa frame.All the approaches described above are type-based.
They place more emphasis on extendingthe lexicon rather than the annotations that comewith it.
In our earlier work (F?urstenau and Lapata,2009) we acquire new training instances, by pro-jecting annotations from existing FrameNet sen-tences to new unseen ones.
The proposed methodis token-based, however, it only produces annota-tions for known verbs, i.e., verbs that FrameNetlists as evoking a given frame.In this paper we generalize the proposals ofPennacchiotti et al (2008) and F?urstenau and Lap-ata (2009) in a unified framework.
We create train-ing data for semantic role labeling of unknownpredicates by projection of annotations from la-beled onto unlabeled data.
This projection is con-12ceptualized as a graph alignment problem wherewe seek to find a globally optimal alignment sub-ject to semantic and structural constraints.
Insteadof predicting the same frame for each occurence ofan unknown predicate, we consider a set of candi-date frames and allow projection from any labeledpredicate that can evoke one of these frames.
Thisallows us to make instance-based decisions andthus account for predicate ambiguity.3 Graph Alignment MethodOur approach acquires annotations for an un-known frame evoking verb by selecting sen-tences featuring this verb from a large unlabeledcorpus (the expansion corpus).
The choice isbased upon a measure of similarity between thepredicate-argument structure of the unknown verband those of similar verbs in a manually labeledcorpus (the seed corpus).
We formulate the prob-lem of finding the most similar verbs as the searchfor an optimal graph alignment (we representlabeled and unlabeled sentences as dependencygraphs).
Conveniently, this allows us to create la-beled training instances for the unknown verb byprojecting role labels from the most similar seedinstance.
The annotations can be subsequentlyused for training a semantic role labeler.Given an unknown verb, the first step is to nar-row down the number of frames it could poten-tially evoke.
FrameNet provides definitions formore than 500 frames, of which we entertain onlya small number.
This is done using a method sim-ilar to Pennacchiotti et al (2008).
Each frameis represented in a semantic space as the cen-troid of the vectors of all its known frame evokingverbs.
For an unknown verb we then consider asframe candidates the k closest frames according toa measure of distributional similarity (which wecompute between the unknown verb?s vector andthe frame centroid vector).
We provide details ofthe semantic space we used in our experiments inSection 4.Next, we compare each sentence featuring theunknown verb in question to labeled sentences fea-turing known verbs which according to FrameNetevoke any of the k candidate frames.
If sufficientlysimilar seeds exist, the unlabeled sentence is anno-tated by projecting role labels from the most sim-ilar one.
The similarity score of this best match isrecorded as a measure of the quality (or reliability)of the new instance.
After carrying out this pro-Body movementFEE?
?~~~~~~~~~Agent_eikmpruyBody part~|}andSUBJxxqqqqqqqqqqCONJCONJ''OOOOOOOOOOOOOHerkimerMODblinkDOBJnodMODOldeyeDETwiselyhisFigure 1: Annotated dependency graph for thesentenceOld Herkimer blinked his eye and noddedwisely.
The alignment domain is indicated in boldface.
Labels in italics denote frame roles, whereasgrammatical roles are rendered in small capitals.The verb blink evokes the frame Body Movement.cedure for all sentences in the expansion corpusfeaturing an unknown verb, we collect the highestscoring new instances and add them back to ourseed corpus as new training items.
In the follow-ing we discuss in more detail how the similarity ofpredicate-argument structures is assessed.3.1 Alignment ScoringLet s be a semantically labeled dependency graphin which node nFEErepresents the frame evokingverb.
Here, we use the term ?labeled?
to indi-cate that the graph contains semantic role labelsin addition to grammatical role labels (e.g., sub-ject or object).
Let g be an unlabeled graphand ntargeta verbal node in it.
The ?unlabeled?graph contains grammatical roles but no semanticroles.
We wish to find an alignment between thepredicate-argument structures of nFEEand ntarget,respectively.
Such an alignment takes the form ofa function ?
from a set M of nodes of s (the align-ment domain) to a set N of nodes of g (the align-ment range).
These two sets represent the rele-vant predicate-argument structures within the twographs; nodes that are not members of these setsare excluded from any further computations.If there were no mismatches between (frame)semantic arguments and syntactic arguments, wewould expect all roles in s to be instantiated bysyntactic dependents in nFEE.
This is usually thecase but not always.
We cannot therefore sim-13ply define M as the set of direct dependents ofthe predicate, but also have to consider complexpaths between nFEEand role bearing nodes.
Anexample is given in Figure 1, where the role Agentis filled by a node which is not dominated by theframe evoking verb blink ; instead, it is connectedto blink by the complex path (CONJ?1, SUBJ).
Fora given seed s we build a list of all such complexpaths and also include all nodes of s connectedto nFEEby one of these paths.
We thus define thealignment domain M as:1. the predicate node nFEE2.
all direct dependents of nFEE, except auxil-iaries3.
all nodes on complex paths originatingin nFEE4.
single direct dependents of any preposition orconjunction node which is in (2) or end-pointof a complex path covered in (3)The last rule ensures that the semantic headsof prepositional phrases and conjunctions are in-cluded in the alignment domain.The alignment range N is defined in a similarway.
However, we cannot extract complex pathsfrom the unlabeled graph g, as it does not con-tain semantic role information.
Therefore, we usethe same list of complex paths extracted from s.Note that this introduces an unavoidable asymme-try into our similarity computation.An alignment is a function ?
: M ?
N?{?
}which is injective for all values except ?,i.e., ?
(n1) = ?
(n2) 6= ?
?
n1= n2.
We score thesimilarity of two subgraphs expressed by an align-ment function ?
by the following term:?n?M?(n)6=?sem(n,?(n))+?
??(n1,n2)?E(M)(?(n1),?(n2))?E(N)syn(rn1n2,r?(n1)?
(n2))(2)Here, sem represents a semantic similarity mea-sure between graph nodes and syn a syntactic sim-ilarity measure between the grammatical role la-bels of graph edges.
E(M) and E(N) are the setsof all graph edges between nodes of M and nodesof N, respectively, and rn1n2denotes the grammati-cal relation between nodes n1and n2.Equation (2) expresses the similarity betweentwo predicate-argument structures in terms of thesum of semantic similarity scores of aligned graphnodes and the sum of syntactic similarity scores ofaligned graph edges.
The relative weight of thesetwo sums is determined by the parameter ?.
Fig-ure 2 shows an example of an alignment betweentwo dependency graphs.
Here, the aligned nodepairs thud and thump, back and rest, against andagainst, as well as wall and front contribute se-mantic similarity scores, while the three edge pairsSUBJ and SUBJ, IOBJ and IOBJ, as well as DOBJand DOBJ contribute syntactic similarity scores.We normalize the resulting score so that it al-ways falls within the interval [0,1].
To take intoaccount unaligned nodes in both the alignment do-main and the alignment range, we divide Equa-tion (2) by:?|M| ?
|N|+?
?|E(M)| ?
|E(N)| (3)A trivial alignment of a seed with itself where allsemantic and syntactic scores are 1 will thus re-ceive a score of:|M| ?1+?
?
|E(M)| ?1?|M|2+?
?E(M)2= 1 (4)which is the largest possible similarity score.
Thelowest possible score is obviously 0, assuming thatthe semantic and syntactic scores cannot be nega-tive.Considerable latitude is available in selectingthe semantic and syntactic similarity measures.With regard to semantic similarity, WordNet is aprime contender and indeed has been previouslyused to acquire new predicates in FrameNet (Pen-nacchiotti et al, 2008; Burchardt et al, 2005; Jo-hansson and Nugues, 2007).
Syntactic similaritymay be operationalized in many ways, for exam-ple by taking account a hierarchy of grammaticalrelations (Keenan and Comrie, 1977).
Our experi-ments employed relatively simple instantiations ofthese measures.
We did not make use of Word-Net, as we were interested in exploring the set-ting where WordNet is not available or has limitedcoverage.
Therefore, we approximate the seman-tic similarity between two nodes via distributionalsimilarity.
We present the details of the semanticspace model we used in Section 4.If n and n?are both nouns, verbs or adjectives,we set:sem(n,n?)
:= cos(~vn,~vn?)
(5)where ~vnand ~vn?are the vectors representing thelemmas of n and n?respectively.
If n and n?14ImpactFEEOOImpactor_iwImpactee_VJ9.
($!thud((SUBJzzvvvvvvvvvvIOBJ%%KKKKKKKKKKthumpSUBJ{{vvvvvvvvvIOBJ%%KKKKKKKKKbackDET''againstDOBJ66restDETIOBJ$$HHHHHHHHHHagainstDOBJhis wallDET77the ofDOBJfrontDETIOBJ$$IIIIIIIIIIthebodyDETthe ofDOBJhiscageDETtheFigure 2: The dotted arrows show aligned nodes in the graphs for the two sentences His back thuddedagainst the wall.
and The rest of his body thumped against the front of the cage.
(Graph edges are alsoaligned to each other.)
The alignment domain and alignment range are indicated in bold face.
The verbthud evokes the frame Impact.are identical prepositions or conjunctions we setsem(n,n?)
:= 1.
In all other cases sem(n,n?)
:= 0.As far as syntactic similarity is concerned, wechose the simplest metric possible and set:syn(r,r?
):={1 if r = r?0 otherwise(6)3.2 Alignment SearchThe problem of finding the best alignment ac-cording to the scoring function presented in Equa-tion (2) can be formulated as an integer linear pro-gram.
Let the binary variables xikindicate whethernode niof graph s is aligned to node nkof graph g.Since it is not only nodes but also graph edgesthat must be aligned we further introduce binaryvariables yi jkl, where yi jkl= 1 indicates that theedge between nodes niand njof graph s is alignedto the edge between nodes nkand nlof graph g.This follows a general formulation of the graphalignment problem based on maximum structuralmatching (Klau, 2009).
In order for the xikandyi jklvariables to represent a valid alignment, thefollowing constraints must hold:1.
Each node of s is aligned to at most one nodeof g:?kxik?
12.
Each node of g is aligned to at most one nodeof s:?ixik?
13.
Two edges may only be aligned if theiradjacent nodes are aligned: yi jkl?
xikandyi jkl?
xjlThe scoring function then becomes:?i,ksem(ni,nk)xik+?
?
?i, j,k,lsyn(rninj,rnknl)yi jkl(7)We solve this optimization problem with a ver-sion of the branch-and-bound algorithm (Landand Doig, 1960).
In general, this graph align-ment problem is NP-hard (Klau, 2009) and usuallysolved approximately following a procedure simi-lar to beam search.
However, the special structureof constraints 1 to 3, originating from the requiredinjectivity of the alignment function, allows us tosolve the optimization exactly.
Our implementa-tion of the branch-and-bound algorithm does notgenerally run in polynomial time, however, wefound that in practice we could efficiently com-pute optimal alignments in almost all cases (lessthan 0.1% of alignment pairs in our data could notbe solved in reasonable time).
This relatively be-nign behavior depends crucially on the fact thatwe do not have to consider alignments between15full graphs, and the number of nodes in the alignedsubgraphs is limited.4 Experimental DesignIn this section we present our experimental set-upfor assessing the performance of our method.
Wegive details on the data sets we used, describe thebaselines we adopted for comparison with our ap-proach, and explain how our system output wasevaluated.Data Our experiments used annotated sentencesfrom FrameNet as a seed corpus.
These wereaugmented with automatically labeled sentencesfrom the BNC which we used as our expan-sion corpus.
FrameNet sentences were parsedwith RASP (Briscoe et al, 2006).
In addi-tion to phrase structure trees, RASP delivers adependency-based representation of the sentencewhich we used in our experiments.
FrameNet roleannotations were mapped onto those dependencygraph nodes that corresponded most closely to theannotated substring (see F?urstenau (2008) for a de-tailed description of the mapping algorithm).
BNCsentences were also parsed with RASP (Andersenet al, 2008).We randomly split the FrameNet corpus1into 80% training set, 10% test set, and 10% de-velopment set.
Next, all frame evoking verbs inthe training set were ordered by their number ofoccurrence and split into two groups, seen and un-seen.
Every other verb from the ordered list wasconsidered unseen.
This quasi-random split coversa broad range of predicates with a varying numberof annotations.
Accordingly, the FrameNet sen-tences in the training and test sets were dividedinto the sets train seen, train unseen, test seen,and test unseen.
As we explain below, this wasnecessary for evaluation purposes.The train seen dataset consisted of 24,220 sen-tences, with 1,238 distinct frame evoking verbs,whereas train unseen contained 24,315 sentenceswith the same number of frame evoking verbs.Analogously, test seen had 2,990 sentences and817 unique frame evoking verbs; the numberof sentences in test unseen was 3,064 (with847 unique frame evoking verbs).Model Parameters The alignment model pre-sented in Section 3 crucially relies on the similar-1Here, we consider only FrameNet example sentencesfeaturing verbal predicates.ity function that scores potential alignments (seeEquation (2)).
This function has a free parameter,the weight ?
for determining the relative contri-bution of semantic and syntactic similarity.
Wetuned ?
using leave-one-out cross-validation onthe development set.
For each annotated sentencein this set we found its most similar other sentenceand determined the best alignment between thetwo dependency graphs representing them.
Sincethe true annotations for each sentence were avail-able, it was possible to evaluate the accuracy of ourmethod for any ?
value.
We did this by compar-ing the true annotation of a sentence to the anno-tation its nearest neighbor would have induced byprojection.
Following this procedure, we obtainedbest results with ?
= 0.2.The semantic similarity measure relies on a se-mantic space model which we built on a lemma-tized version of the BNC.
Our implementation fol-lowed closely the model presented in F?urstenauand Lapata (2009) as it was used in a similartask and obtained good results.
Specifically, weused a context window of five words on eitherside of the target word, and 2,000 vector dimen-sions.
These were the common context words inthe BNC.
Their values were set to the ratio of theprobability of the context word given the targetword to the probability of the context word over-all.
Semantic similarity was measured using thecosine of the angle between the vectors represent-ing any two words.
The same semantic space wasused to create the distributional profile of a frame(which is the centroid of the vectors of its verbs).For each unknown verb, we consider the k mostsimilar frame candidates (again similarity is mea-sured via cosine).
Our experiments explored dif-ferent values of k ranging from 1 to 10.Evaluation Our evaluation assessed the perfor-mance of a semantic frame and role labeler withand without the annotations produced by ourmethod.
The labeler followed closely the im-plementation described in Johansson and Nugues(2008).
We extracted features from dependencyparses corresponding to those routinely used inthe semantic role labeling literature (see Bakeret al (2007) for an overview).
SVM classifierswere trained2with the LIBLINEAR library (Fanet al, 2008) and learned to predict the framename, role spans, and role labels.
We followed2The regularization parameterC was set to 0.1.16Figure 3: Frame labeling accuracy on high,medium and low frequency verbs, before and af-ter applying our expansion method; the labeler de-cides among k = 1, .
.
.
,10 candidate frames.the one-versus-one strategy for multi-class classi-fication (Friedman, 1996).Specifically, the labeler was trained on thetrain seen data set without any access to traininginstances representative of the ?unknown?
verbs intest unseen.
We then trained the labeler on a largerset containing train seen and new training exam-ples obtained with our method.
To do this, we usedtrain seen as the seed corpus and the BNC as theexpansion corpus.
For each ?unknown?
verb intrain unseen we obtained BNC sentences with an-notations projected from their most similar seeds.The quality of these sentences as training instancesvaries depending on their similarity to the seed.In our experiments we added to the training setthe 20 highest scoring BNC sentences per verb(adding less or more instances led to worse per-formance).The average number of frames which can beevoked by a verb token in the set test unseenwas 1.96.
About half of them (1,522 instances)can evoke only one frame, 22% can evoke twoframes, and 14 instances can evoke up to 11 differ-ent frames.
Finally, there are 120 instances (4%)in test unseen for which the correct frame is notannotated on any sentence in train seen.Figure 4: Role labeling F1for high, medium, andlow frequency verbs (roles of mislabeled framesare counted as wrong); the labeler decides amongk = 1, .
.
.
,10 candidate frames.5 ResultsWe first examine how well our method performsat frame labeling.
We partitioned the frame evok-ing verbs in our data set into three bands (High,Medium, and Low) based on an equal divisionof the range of their occurrence frequency in theBNC.
As frequency is strongly correlated withpolysemy, the division allows us to assess howwell our method is performing at different degreesof ambiguity.
Figure 3 summarizes our results forHigh, Medium, and Low frequency verbs.
Thenumber of verbs in each band are 282, 282, and283, respectively.
We compare the frame accuracyof a labeler trained solely on the annotations avail-able in FrameNet (Without expansion) against alabeler that also uses annotations created with ourmethod (After expansion).
Both classifiers wereemployed in a setting where they had to decideamong k candidate frames.
These were the k mostsimilar frames to the unknown verb in question.We also show the accuracy of a simple baselinelabeler, which randomly chooses one of the k can-didate frames.The graphs in Figure 3 show that for verbs in theMedium and Low frequency bands, both classi-fiers (with and without expansion) outperform thebaseline of randomly choosing among k candidateframes.
Interestingly, rather than defaulting to themost similar frame (k = 1), we observe that ac-17Figure 5: Hybrid frame labeling accuracy (k = 1for High frequency verbs).curacy improves when frame selection is viewedas a classification task.
The classifier trained onthe expanded training set consistently outperformsthe one trained on the original training set.
Whilethis is also true for the verbs in the High frequencyband, labeling accuracy peaks at k = 1 and doesnot improve when more candidate frames are con-sidered.
This is presumably due to the skewedsense distributions of high frequency verbs, anddefaulting to the most likely sense achieves rela-tively good performance.Next, we evaluated our method on role label-ing, again by comparing the performance of ourrole labeler on the expanded and original train-ing set.
Since role and frame labeling are inter-dependent, we count all predicted roles of an in-correctly predicted frame as wrong.
This unavoid-ably results in low role labeling scores, but allowsus to directly compare performance across differ-ent settings (e.g., different number of candidateframes, with or without expansion).
Figure 4 re-ports labeled F1for verbs in the High, Mediumand Low frequency bands.
The results are simi-lar to those obtained for frame labeling; the rolelabeler trained on the the expanded training setconsistently outperforms the labeler trained on theunexpanded one.
(There is no obvious baselinefor role labeling, which is a complex task involv-ing the prediction of frame labels, identification ofthe role bearing elements, and assignment of rolelabels.)
Again, for High frequency verbs simplydefaulting to k = 1 performs best.Taken together, our results on frame and rolelabeling indicate that our method is not very effec-tive for High frequency verbs (which in practiceshould be still annotated manually).
We there-Figure 6: Hybrid role labeling F1(k = 1 for Highfrequency verbs).fore also experimented with a hybrid approachthat lets the classifier choose among k candi-dates for Medium and Low frequency verbs anddefaults to the most similar candidate for Highfrequency verbs.
Results for this approach areshown in Figures 5 and 6.
All differences be-tween the expanded and the unexpanded classi-fier when choosing between the same k > 1 can-didates are significant according to McNemar?stest (p < .05).
The best frame labeling accu-racy (26.3%) is achieved by the expanded classi-fier when deciding among k = 6 candidate frames.This is significantly better (p < .01) than the bestperformance of the unexpanded classifier (25.0%),which is achieved at k = 2.
Role labeling resultsfollow a similar pattern.
The best expanded classi-fier (F1=14.9% at k = 6) outperforms the best un-expanded one (F1=14.1% at k = 2).
The differencein performance as significant at p < 0.05, usingstratified shuffling (Noreen, 1989).6 ConclusionsThis paper presents a novel semi-supervised ap-proach for reducing the annotation effort involvedin creating resources for semantic role labeling.Our method acquires training instances for un-known verbs (i.e., verbs that are not evoked byexisting FrameNet frames) from an unlabeled cor-pus.
A key assumption underlying our work isthat verbs with similar meanings will have sim-ilar argument structures.
Our task then amountsto finding the seen instances that resemble the un-seen instances most, and projecting their annota-tions.
We represent this task as a graph alignmentproblem, and formalize the search for an optimalalignment as an integer linear program under an18objective function that takes semantic and struc-tural similarity into account.Experimental results show that our method im-proves frame and role labeling accuracy, espe-cially for Medium and Low frequency verbs.
Theoverall frame labeling accuracy may seem low.There are at least two reasons for this.
Firstly, theunknown verb might have a frame for which nomanual annotation exists.
And secondly, many er-rors are due to near-misses, i.e., we assign the un-known verb a wrong frame which is neverthelessvery similar to the right one.
In this case, accuracywill not give us any credit.An obvious direction for future work concernsimproving our scoring function.
Pennacchiottiet al (2008) show that WordNet-based similaritymeasures outperform their simpler distributionalalternatives.
An interesting question is whether theincorporation of WordNet-based similarity wouldlead to similar improvements in our case.
Alsonote that currently our method assigns unknownlexical items to existing frames.
A better alterna-tive would be to decide first whether the unknownitem can be classified at all (because it evokes aknown frame) or whether it represents a genuinelynovel frame for which manual annotation must beprovided.Acknowledgments The authors acknowledgethe support of DFG (IRTG 715) and EPSRC (grantGR/T04540/01).
We are grateful to Richard Jo-hansson for his help with the re-implementation ofhis semantic role labeler.
Special thanks to Man-fred Pinkal for valuable feedback on this work.References?istein E. Andersen, Julien Nioche, Ted Briscoe,and John Carroll.
2008.
The BNC Parsed withRASP4UIMA.
In Proceedings of the 6th Interna-tional Language Resources and Evaluation Confer-ence, pages 865?869, Marrakech, Morocco.Collin F. Baker, Michael Ellsworth, and Katrin Erk.2007.
SemEval-2007 Task 19: Frame SemanticStructure Extraction.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations,pages 99?104, Prague, Czech Republic.Hans C. Boas.
2005.
Semantic frames as interlingualrepresentations for multilingual lexical databases.International Journal of Lexicography, 18(4):445?478.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The Second Release of the RASP System.
In Pro-ceedings of the COLING/ACL 2006 Interactive Pre-sentation Sessions, pages 77?80, Sydney, Australia.Aljoscha Burchardt, Katrin Erk, and Anette Frank.2005.
A WordNet Detour to FrameNet.
In Proceed-ings of the GLDV 200Workshop GermaNet II, Bonn,Germany.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR: ALibrary for Large Linear Classification.
Journal ofMachine Learning Research, 9:1871?1874.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.Charles J. Fillmore, Christopher R. Johnson, andMiriam R. L. Petruck.
2003.
Background toFrameNet.
International Journal of Lexicography,16:235?250.Jerome H. Friedman.
1996.
Another approach to poly-chotomous classification.
Technical report, Depart-ment of Statistics, Stanford University.Hagen F?urstenau and Mirella Lapata.
2009.
Semi-supervised semantic role labeling.
In Proceedingsof the 12th Conference of the European Chapterof the Association for Computational Linguistics,pages 220?228, Athens, Greece.Hagen F?urstenau.
2008.
Enriching frame semantic re-sources with dependency graphs.
In Proceedings ofthe 6th Language Resources and Evaluation Confer-ence, pages 1478?1484, Marrakech, Morocco.Richard Johansson and Pierre Nugues.
2006.
AFrameNet-based semantic role labeler for Swedish.In Proceedings of the COLING/ACL 2006 MainConference Poster Sessions, pages 436?443, Syd-ney, Australia.Richard Johansson and Pierre Nugues.
2007.
UsingWordNet to extend FrameNet coverage.
In RichardJohansson and Pierre Nugues, editors, FRAME2007: Building Frame Semantics Resources forScandinavian and Baltic Languages, pages 27?30,Tartu, Estonia.Richard Johansson and Pierre Nugues.
2008.
The ef-fect of syntactic representation on semantic role la-beling.
In Proceedings of the 22nd InternationalConference on Computational Linguistics, pages393?400, Manchester, UK.E.
Keenan and B. Comrie.
1977.
Noun phrase acces-sibility and universal grammar.
Linguistic Inquiry,8:62?100.Gunnar W. Klau.
2009.
A new graph-based methodfor pairwise global network alignment.
BMC Bioin-formatics, 10 (Suppl 1).A.H.
Land and A.G. Doig.
1960.
An automaticmethod for solving discrete programming problems.Econometrica, 28:497?520.19Gabor Melli, Yang Wang, Yurdong Liu, Mehdi M.Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,and Fred Popowich.
2005.
Description ofSQUASH, the SFU question answering summaryhandler for the duc-2005 summarization task.
InProceedings of the HLT/EMNLP Document Under-standing Workshop, Vancouver, Canada.Srini Narayanan and Sanda Harabagiu.
2004.
Ques-tion answering based on semantic structures.
InProceedings of the 20th International Conference onComputational Linguistics, pages 693?701, Geneva,Switzerland.E.
Noreen.
1989.
Computer-intensive Methods forTesting Hypotheses: An Introduction.
John Wileyand Sons Inc.Sebastian Pad?o and Mirella Lapata.
2006.
Optimalconstituent alignment with edge covers for seman-tic projection.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Com-putational Linguistics, pages 1161?1168, Sydney,Australia.Marco Pennacchiotti, Diego De Cao, Roberto Basili,Danilo Croce, and Michael Roth.
2008.
Automaticinduction of FrameNet lexical units.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, pages 457?465, Honolulu,Hawaii.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argumentstructures for information extraction.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 8?15, Sap-poro, Japan.20
