Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 640?647,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsCorpus Effects on the Evaluation of Automated Transliteration SystemsSarvnaz Karimi Andrew Turpin Falk ScholerSchool of Computer Science and Information TechnologyRMIT University, GPO Box 2476V, Melbourne 3001, Australia{sarvnaz,aht,fscholer}@cs.rmit.edu.auAbstractMost current machine transliteration sys-tems employ a corpus of known source-target word pairs to train their system, andtypically evaluate their systems on a similarcorpus.
In this paper we explore the perfor-mance of transliteration systems on corporathat are varied in a controlled way.
In partic-ular, we control the number, and prior lan-guage knowledge of human transliteratorsused to construct the corpora, and the originof the source words that make up the cor-pora.
We find that the word accuracy of au-tomated transliteration systems can vary byup to 30% (in absolute terms) depending onthe corpus on which they are run.
We con-clude that at least four human transliteratorsshould be used to construct corpora for eval-uating automated transliteration systems;and that although absolute word accuracymetrics may not translate across corpora, therelative rankings of system performance re-mains stable across differing corpora.1 IntroductionMachine transliteration is the process of transform-ing a word written in a source language into a wordin a target language without the aid of a bilingualdictionary.
Word pronunciation is preserved, as faras possible, but the script used to render the targetword is different from that of the source language.Transliteration is applied to proper nouns and out-of-vocabulary terms as part of machine translationand cross-lingual information retrieval (CLIR) (Ab-dulJaleel and Larkey, 2003; Pirkola et al, 2006).Several transliteration methods are reported in theliterature for a variety of languages, with their per-formance being evaluated on multilingual corpora.Source-target pairs are either extracted from bilin-gual documents or dictionaries (AbdulJaleel andLarkey, 2003; Bilac and Tanaka, 2005; Oh and Choi,2006; Zelenko and Aone, 2006), or gathered ex-plicitly from human transliterators (Al-Onaizan andKnight, 2002; Zelenko and Aone, 2006).
Some eval-uations of transliteration methods depend on a singleunique transliteration for each source word, whileothers take multiple target words for a single sourceword into account.
In their work on transliteratingEnglish to Persian, Karimi et al (2006) observedthat the content of the corpus used for evaluatingsystems could have dramatic affects on the reportedaccuracy of methods.The effects of corpus composition on the evalua-tion of transliteration systems has not been specif-ically studied, with only implicit experiments orclaims made in the literature such as introduc-ing the effects of different transliteration mod-els (AbdulJaleel and Larkey, 2003), language fam-ilies (Linde?n, 2005) or application based (CLIR)evaluation (Pirkola et al, 2006).
In this paper, we re-port our experiments designed to explicitly examinethe effect that varying the underlying corpus used inboth training and testing systems has on translitera-tion accuracy.
Specifically, we vary the number ofhuman transliterators that are used to construct thecorpus; and the origin of the English words used inthe corpus.Our experiments show that the word accuracy ofautomated transliteration systems can vary by up to30% (in absolute terms), depending on the corpusused.
Despite the wide range of absolute values640in performance, the ranking of our two translitera-tion systems was preserved on all corpora.
We alsofind that a human?s confidence in the language fromwhich they are transliterating can affect the corpusin such a way that word accuracy rates are altered.2 BackgroundMachine transliteration methods are divided intographeme-based (AbdulJaleel and Larkey, 2003;Linde?n, 2005), phoneme-based (Jung et al, 2000;Virga and Khudanpur, 2003) and combined tech-niques (Bilac and Tanaka, 2005; Oh and Choi,2006).
Grapheme-based methods derive transforma-tion rules for character combinations in the sourcetext from a training data set, while phoneme-basedmethods use an intermediate phonetic transforma-tion.
In this paper, we use two grapheme-basedmethods for English to Persian transliteration.
Dur-ing a training phase, both methods derive rules fortransforming character combinations (segments) inthe source language into character combinations inthe target language with some probability.During transliteration, the source word si is seg-mented and rules are chosen and applied to each seg-ment according to heuristics.
The probability of aresulting word is the product of the probabilities ofthe applied rules.
The result is a list of target wordssorted by their associated probabilities, Li.The first system we use (SYS-1) is an n-gramapproach that uses the last character of the previ-ous source segment to condition the choice of therule for the current source segment.
This system hasbeen shown to outperform other n-gram based meth-ods for English to Persian transliteration (Karimi etal., 2006).The second system we employ (SYS-2) makesuse of some explicit knowledge of our chosen lan-guage pair, English and Persian, and is also onthe collapsed-vowel scheme presented by Karimi etal.
(2006).
In particular, it exploits the tendency forruns of English vowels to be collapsed into a singlePersian character, or perhaps omitted from the Per-sian altogether.
As such, segments are chosen basedon surrounding consonants and vowels.
The full de-tails of this system are not important for this paper;here we focus on the performance evaluation of sys-tems, not the systems themselves.2.1 System EvaluationIn order to evaluate the list Li of target words pro-duced by a transliteration system for source word si,a test corpus is constructed.
The test corpus con-sists of a source word, si, and a list of possible targetwords {ti j}, where 1 ?
j ?
di, the number of dis-tinct target words for source word si.
Associatedwith each ti j is a count ni j which is the number ofhuman transliterators who transliterated si into ti j.Often the test corpus is a proportion of a largercorpus, the remainder of which has been used fortraining the system?s rule base.
In this work weadopt the standard ten-fold cross validation tech-nique for all of our results, where 90% of a corpusis used for training and 10% for testing.
The pro-cess is repeated ten times, and the mean result taken.Forthwith, we use the term corpus to refer to the sin-gle corpus from which both training and test sets aredrawn in this fashion.Once the corpus is decided upon, a metric to mea-sure the system?s accuracy is required.
The appro-priate metric depends on the scenario in which thetransliteration system is to be used.
For example,in a machine translation application where only onetarget word can be inserted in the text to represent asource word, it is important that the word at the topof the system generated list of target words (by def-inition the most probable) is one of the words gen-erated by a human in the corpus.
More formally,the first word generated for source word si, Li1, mustbe one of ti j,1 ?
j ?
di.
It may even be desirablethat this is the target word most commonly used forthis source word; that is, Li1 = ti j such that ni j ?
nik,for all 1 ?
k ?
di.
Alternately, in a CLIR appli-cation, all variants of a source word might be re-quired.
For example, if a user searches for an En-glish term ?Tom?
in Persian documents, the searchengine should try and locate documents that containboth ?
AK?
(3 letters: H- -) and ???'?
(2 letters: H-),two possible transliterations of ?Tom?
that would begenerated by human transliterators.
In this case, ametric that counts the number of ti j that appear inthe top di elements of the system generated list, Li,might be appropriate.In this paper we focus on the ?Top-1?
case, whereit is important for the most probable target word gen-erated by the system, Li1 to be either the most pop-641ular ti j (labeled the Majority, with ties broken ar-bitrarily), or just one of the ti j?s (labeled Uniformbecause all possible transliterations are equally re-warded).
A third scheme (labeled Weighted) is alsopossible where the reward for ti j appearing as Li1is ni j/?dij=1 ni j; here, each target word is given aweight proportional to how often a human translit-erator chose that target word.
Due to space consid-erations, we focus on the first two variants only.In general, there are two commonly used met-rics for transliteration evaluation: word accuracy(WA) and character accuracy (CA) (Hall and Dowl-ing, 1980).
In all of our experiments, CA basedmetrics closely mirrored WA based metrics, andso conclusions drawn from the data would be thesame whether WA metrics or CA metrics were used.Hence we only discuss and report WA based metricsin this paper.For each source word in the test corpus of Kwords, word accuracy calculates the percentage ofcorrectly transliterated terms.
Hence for the major-ity case, where every source word in the corpus onlyhas one target word, the word accuracy is defined asMWA = |{si|Li1 = ti1,1 ?
i ?
K}|/K,and for the Uniform case, where every target variantis included with equal weight in the corpus, the wordaccuracy is defined asUWA = |{si|Li1 ?
{ti j},1 ?
i ?
K,1 ?
j ?
di}|/K.2.2 Human EvaluationTo evaluate the level of agreement between translit-erators, we use an agreement measure based on Munand Eye (2004).For any source word si, there are di differenttransliterations made by the ni human translitera-tors (ni = ?dij=1 ni j, where ni j is the number of timessource word si was transliterated into target wordti j).
When any two transliterators agree on thesame target word, there are two agreements beingmade: transliterator one agrees with transliteratortwo, and vice versa.
In general, therefore, the to-tal number of agreements made on source word si is?dij=1 ni j(ni j ?
1).
Hence the total number of actualagreements made on the entire corpus of K words isAact =K?i=1di?j=1ni j(ni j ?1).The total number of possible agreements (that is,when all human transliterators agree on a single tar-get word for each source word), isAposs =K?i=1ni(ni ?1).The proportion of overall agreement is thereforePA =AactAposs.2.3 CorporaSeven transliterators (T1, T2, .
.
., T7: all native Per-sian speakers from Iran) were recruited to transliter-ate 1500 proper names that we provided.
The nameswere taken from lists of names written in English onEnglish Web sites.
Five hundred of these names alsoappeared in lists of names on Arabic Web sites, andfive hundred on Dutch name lists.
The transliteratorswere not told of the origin of each word.
The en-tire corpus, therefore, was easily separated into threesub-corpora of 500 words each based on the originof each word.
To distinguish these collections, weuse E7, A7 and D7 to denote the English, Arabic andDutch sub-corpora, respectively.
The whole 1500word corpus is referred to as EDA7.Dutch and Arabic were chosen with an assump-tion that most Iranian Persian speakers have littleknowledge of Dutch, while their familiarity withArabic should be in the second rank after English.All of the participants held at least a Bachelors de-gree.
Table 1 summarizes the information aboutthe transliterators and their perception of the giventask.
Participants were asked to scale the difficultyof the transliteration of each sub-corpus, indicatedas a scale from 1 (hard) to 3 (easy).
Similarly, theparticipants?
confidence in performing the task wasrated from 1 (no confidence) to 3 (quite confident).The level of familiarity with second languages wasalso reported based on a scale of zero (not familiar)to 3 (excellent knowledge).The information provided by participants con-firms our assumption of transliterators knowledgeof second languages: high familiarity with English,some knowledge of Arabic, and little or no priorknowledge of Dutch.
Also, the majority of themfound the transliteration of English terms of mediumdifficulty, Dutch was considered mostly hard, andArabic as easy to medium.642Second Language Knowledge Difficulty,ConfidenceTransliterator English Dutch Arabic Other English Dutch Arabic1 2 0 1 - 1,1 1,2 2,32 2 0 2 - 2,2 2,3 3,33 2 0 1 - 2,2 1,2 2,24 2 0 1 - 2,2 2,1 3,35 2 0 2 Turkish 2,2 1,1 3,26 2 0 1 - 2,2 1,1 3,37 2 0 1 - 2,2 1,1 2,2Table 1: Transliterator?s language knowledge (0=not familiar to 3=excellent knowledge), perception ofdifficulty (1=hard to 3=easy) and confidence (1=no confidence to 3=quite confident) in creating the corpus.E7 D7 A7 EDA7Corpus020406080100WordAccuracy(%)UWA (SYS-2)UWA (SYS-1)MWA (SYS-2)MWA (SYS-1)Figure 1: Comparison of the two evaluation metricsusing the two systems on four corpora.
(Lines wereadded for clarity, and do not represent data points.
)0 20 40 60 80 100Corpus020406080100WordAccuracy(%)UWA (SYS-2)UWA (SYS-1)MWA (SYS-2)MWA (SYS-1)Figure 2: Comparison of the two evaluation metricsusing the two systems on 100 randomly generatedsub-corpora.3 ResultsFigure 1 shows the values of UWA and MWA forE7, A7, D7 and EDA7 using the two transliterationsystems.
Immediately obvious is that varying thecorpora (x-axis) results in different values for wordaccuracy, whether by the UWA or MWA method.
Forexample, if you chose to evaluate SYS-2 with theUWA metric on the D7 corpus, you would obtain aresult of 82%, but if you chose to evaluate it with theA7 corpus you would receive a result of only 73%.This makes comparing systems that report resultsobtained on different corpora very difficult.
Encour-agingly, however, SYS-2 consistently outperformsthe SYS-1 on all corpora for both metrics exceptMWA on E7.
This implies that ranking system per-formance on the same corpus most likely yields asystem ranking that is transferable to other corpora.To further investigate this, we randomly extracted100 corpora of 500 word pairs from EDA7 and ranthe two systems on them and evaluated the resultsusing both MWA and UWA.
Both of the measuresranked the systems consistently using all these cor-pora (Figure 2).As expected, the UWA metric is consistentlyhigher than the MWA metric; it allows for the toptransliteration to appear in any of the possible vari-ants for that word in the corpus, unlike the MWAmetric which insists upon a single target word.
Forexample, for the E7 corpus using the SYS-2 ap-proach, UWA is 76.4% and MWA is 47.0%.Each of the three sub-corpora can be further di-vided based on the seven individual transliterators,in different combinations.
That is, construct a sub-corpus from T1?s transliterations, T2?s, and so on;then take all combinations of two transliterators,then three, and so on.
In general we can construct7Cr such corpora from r transliterators in this fash-ion, all of which have 500 source words, but mayhave between one to seven different transliterationsfor each of those words.Figure 3 shows the MWA for these sub-corpora.The x-axis shows the number of transliterators usedto form the sub-corpora.
For example, when x = 3,the performance figures plotted are achieved on cor-pora when taking all triples of the seven translitera-tor?s transliterations.From the boxplots it can be seen that performancevaries considerably when the number of transliter-ators used to determine a majority vote is varied.6431 2 3 4 5 6 72030405060D7                                                                                                                                                                                    1 2 3 4 5 6 72030405060Number of TransliteratorsEDA71 2 3 4 5 6 72030405060Word Accuracy (%)E7                                                                                                                                                                            1 2 3 4 5 6 72030405060Number of TransliteratorsWord Accuracy (%)A7Figure 3: Performance on sub-corpora derived by combining the number of transliterators shown on the x-axis.
Boxes show the 25th and 75th percentile of the MWA for all 7Cx combinations of transliterators usingSYS-2, with whiskers showing extreme values.However, the changes do not follow a fixed trendacross the languages.
For E7, the range of accuraciesachieved is high when only two or three translitera-tors are involved, ranging from 37.0% to 50.6% inSYS-2 method and from 33.8% to 48.0% in SYS-1(not shown) when only two transliterators?
data areavailable.
When more than three transliterators areused, the range of performance is noticeably smaller.Hence if at least four transliterators are used, then itis more likely that a system?s MWA will be stable.This finding is supported by Papineni et al (2002)who recommend that four people should be used forcollecting judgments for machine translation exper-iments.The corpora derived from A7 show consistent me-dian increases as the number of transliterators in-creases, but the median accuracy is lower than forother languages.
The D7 collection does not showany stable results until at least six transliterator?s areused.The results indicate that creating a collection usedfor the evaluation of transliteration systems, basedon a ?gold standard?
created by only one humantransliterator may lead to word accuracy results thatcould show a 10% absolute difference compared toresults on a corpus derived using a different translit-E7 D7 A7 EDA7Corpus0204060WordAccuracy(%)T1T2T3T4T5T6T7SYS-2Figure 4: Word accuracy on the sub-corpora usingonly a single transliterator?s transliterations.erator.
This is evidenced by the leftmost box in eachpanel of the figure which has a wide range of results.Figure 4 shows this box in more detail for eachcollection, plotting the word accuracy for eachuser for all sub-corpora for SYS-2.
The accuracyachieved varies significantly between translitera-tors; for example, for E7 collections, word accuracyvaries from 37.2% for T1 to 50.0% for T5.
Thisvariance is more obvious for the D7 dataset wherethe difference ranges from 23.2% for T 1 to 56.2%for T 3.
Origin language also has an effect: accuracyfor the Arabic collection (A7) is generally less thanthat of English (E7).
The Dutch collection (D7),shows an unstable trend across transliterators.
Inother words, accuracy differs in a narrower range forArabic and English, but in wider range for Dutch.644This is likely due to the fact that most transliteratorsfound Dutch a difficult language to work with, asreported in Table 1.3.1 Transliterator ConsistencyTo investigate the effect of invididual transliteratorconsistency on system accuracy, we consider thenumber of Persian characters used by each transliter-ator on each sub-corpus, and the average number ofrules generated by SYS-2 on the ten training sets de-rived in the ten-fold cross validation process, whichare shown in Table 2.
For example, when translit-erating words from E7 into Persian, T3 only everused 21 out of 32 characters available in the Persianalphabet; T7, on the other hand, used 24 differentPersian characters.
It is expected that an increase innumber of characters or rules provides more ?noise?for the automated system, hence may lead to loweraccuracy.
Superficially the opposite seems true forrules: the mean number of rules generated by SYS-2 is much higher for the EDA7 corpus than for the A7corpus, and yet Figure 1 shows that word accuracyis higher on the EDA7 corpus.
A correlation test,however, reveals that there is no significant relation-ship between either the number of characters used,nor the number of rules generated, and the result-ing word accuracy of SYS-2 (Spearman correlation,p = 0.09 (characters) and p = 0.98 (rules)).A better indication of ?noise?
in the corpus maybe given by the consistency with which a translit-erator applies a certain rule.
For example, a largenumber of rules generated from a particular translit-erator?s corpus may not be problematic if many ofthe rules get applied with a low probability.
If, onthe other hand, there were many rules with approx-imately equal probabilities, the system may havedifficulty distinguishing when to apply some rules,and not others.
One way to quantify this effectis to compute the self entropy of the rule distribu-tion for each segment in the corpus for an indi-vidual.
If pi j is the probability of applying rule1 ?
j ?
m when confronted with source segmenti, then Hi = ?
?mj=1 pi j log2 pi j is the entropy of theprobability distribution for that rule.
H is maximizedwhen the probabilities pi j are all equal, and mini-mized when the probabilities are very skewed (Shan-non, 1948).
As an example, consider the rules:t ?< H,0.5 >, t ?<?,0.3 > and t ?<X,0.2 >; forwhich Ht = 0.79.The expected entropy can be used to obtain a sin-gle entropy value over the whole corpus,E = ?R?i=1fiS Hi,where Hi is the entropy of the rule probabilities forsegment i, R is the total number of segments, fi isthe frequency with which segment i occurs at anyposition in all source words in the corpus, and S isthe sum of all fi.The expected entropy for each transliterator isshown in Figure 5, separated by corpus.
Compar-ison of this graph with Figure 4 shows that gen-erally transliterators that have used rules inconsis-tently generate a corpus that leads to low accuracyfor the systems.
For example, T1 who has the low-est accuracy for all the collections in both methods,also has the highest expected entropy of rules forall the collections.
For the E7 collection, the max-imum accuracy of 50.0%, belongs to T 5 who hasthe minimum expected entropy.
The same appliesto the D7 collection, where the maximum accuracyof 56.2% and the minimum expected entropy bothbelong to T 3.
These observations are confirmedby a statistically significant Spearman correlationbetween expected rule entropy and word accuracy(r = ?0.54, p = 0.003).
Therefore, the consistencywith which transliterators employ their own internalrules in developing a corpus has a direct effect onsystem performance measures.3.2 Inter-Transliterator Agreement andPerceived DifficultyHere we present various agreement proportions (PAfrom Section 2.2), which give a measure of consis-tency in the corpora across all users, as opposed tothe entropy measure which gives a consistency mea-sure for a single user.
For E7, PA was 33.6%, forA7 it was 33.3% and for D7, agreement was 15.5%.In general, humans agree less than 33% of the timewhen transliterating English to Persian.In addition, we examined agreement amongtransliterators based on their perception of the taskdifficulty shown in Table 1.
For A7, agreementamong those who found the task easy was higher(22.3%) than those who found it in medium level645E7 D7 A7 EDA7Char Rules Char Rules Char Rules Char RulesT1 23 523 23 623 28 330 31 1075T2 22 487 25 550 29 304 32 956T3 21 466 20 500 28 280 31 870T4 23 497 22 524 28 307 30 956T5 21 492 22 508 28 296 29 896T6 24 493 21 563 25 313 29 968T7 24 495 21 529 28 299 30 952Mean 23 493 22 542 28 304 30 953Table 2: Number of characters used and rules generated using SYS-2, per transliterator.(18.8%).
PA is 12.0% for those who found theD7 collection hard to transliterate; while the sixtransliterators who found the E7 collection difficultymedium had PA = 30.2%.
Hence, the harder par-ticipants rated the transliteration task, the lower theagreement scores tend to be for the derived corpus.Finally, in Table 3 we show word accuracy resultsfor the two systems on corpora derived from translit-erators grouped by perceived level of difficulty onA7.
It is readily apparent that SYS-2 outperformsSYS-1 on the corpus comprised of human translit-erations from people who saw the task as easy withboth word accuracy metrics; the relative improve-ment of over 50% is statistically significant (pairedt-test on ten-fold cross validation runs).
However,on the corpus composed of transliterations that wereperceived as more difficult, ?Medium?, the advan-tage of SYS-2 is significantly eroded, but is stillstatistically significant for UWA.
Here again, usingonly one transliteration, MWA, did not distinguishthe performance of each system.4 DiscussionWe have evaluated two English to Persian translit-eration systems on a variety of controlled corporausing evaluation metrics that appear in previoustransliteration studies.
Varying the evaluation cor-pus in a controlled fashion has revealed several in-teresting facts.We report that human agreement on the Englishto Persian transliteration task is about 33%.
The ef-fect that this level of disagreement on the evalua-tion of systems has, can be seen in Figure 4, whereword accuracy is computed on corpora derived fromsingle transliterators.
Accuracy can vary by up to30% in absolute terms depending on the translitera-tor chosen.
To our knowledge, this is the first paperE7 D7 A7 EDA7Corpus0.00.20.40.6EntropyT1T2T3T4T5T6T7Figure 5: Entropy of the generated segments basedon the collections created by different transliterators.to report human agreement, and examine its effectson transliteration accuracy.In order to alleviate some of these effects on thestability of word accuracy measures across corpora,we recommend that at least four transliterators areused to construct a corpus.
Figure 3 shows that con-structing a corpus with four or more transliterators,the range of possible word accuracies achieved isless than that of using fewer transliterators.Some past studies do not use more than a sin-gle target word for every source word in the cor-pus (Bilac and Tanaka, 2005; Oh and Choi, 2006).Our results indicate that it is unlikely that these re-sults would translate onto a corpus other than theone used in these studies, except in rare cases wherehuman transliterators are in 100% agreement for agiven language pair.Given the nature of the English language, an En-glish corpus can contain English words from a vari-ety of different origins.
In this study we have usedEnglish words from an Arabic and Dutch origin toshow that word accuracy of the systems can vary byup to 25% (in absolute terms) depending on the ori-gin of English words in the corpus, as demonstratedin Figure 1.In addition to computing agreement, we also in-646RelativePerception SYS-1 SYS-2 Improvement (%)UWA Easy 33.4 55.4 54.4 (p < 0.001)Medium 44.6 48.4 8.52 (p < 0.001)MWA Easy 23.2 36.2 56.0 (p < 0.001)Medium 30.6 37.4 22.2 (p = 0.038)Table 3: System performance when A7 is split into sub-corpora based on transliterators perception of thetask (Easy or Medium).vestigated the transliterator?s perception of difficultyof the transliteration task with the ensuing word ac-curacy of the systems.
Interestingly, when using cor-pora built from transliterators that perceive the taskto be easy, there is a large difference in the wordaccuracy between the two systems, but on corporabuilt from transliterators who perceive the task to bemore difficult, the gap between the systems narrows.Hence, a corpus applied for evaluation of transliter-ation should either be made carefully with translit-erators with a variety of backgrounds, or should belarge enough and be gathered from various sourcesso as to simulate different expectations of its ex-pected non-homogeneous users.The self entropy of rule probability distributionsderived by the automated transliteration system canbe used to measure the consistency with which in-dividual transliterators apply their own rules in con-structing a corpus.
It was demonstrated that whensystems are evaluated on corpora built by transliter-ators who are less consistent in their application oftransliteration rules, word accuracy is reduced.Given the large variations in system accuracy thatare demonstrated by the varying corpora used in thisstudy, we recommend that extreme care be takenwhen constructing corpora for evaluating translitera-tion systems.
Studies should also give details of theircorpora that would allow any of the effects observedin this paper to be taken into account.AcknowledgmentsThis work was supported in part by the Australiangovernment IPRS program (SK).ReferencesNasreen AbdulJaleel and Leah S. Larkey.
2003.
Statisticaltransliteration for English-Arabic cross-language informa-tion retrieval.
In Conference on Information and KnowledgeManagement, pages 139?146.Yaser Al-Onaizan and Kevin Knight.
2002.
Machine translit-eration of names in Arabic text.
In Proceedings of the ACL-02 workshop on Computational approaches to semitic lan-guages, pages 1?13.Slaven Bilac and Hozumi Tanaka.
2005.
Direct combinationof spelling and pronunciation information for robust back-transliteration.
In Conference on Computational Linguisticsand Intelligent Text Processing, pages 413?424.Patrick A. V. Hall and Geoff R. Dowling.
1980.
Approximatestring matching.
ACM Computing Survey, 12(4):381?402.Sung Young Jung, Sung Lim Hong, and Eunok Paek.
2000.
AnEnglish to Korean transliteration model of extended Markovwindow.
In Conference on Computational Linguistics, pages383?389.Sarvnaz Karimi, Andrew Turpin, and Falk Scholer.
2006.
En-glish to Persian transliteration.
In String Processing and In-formation Retrieval, pages 255?266.Krister Linde?n.
2005.
Multilingual modeling of cross-lingualspelling variants.
Information Retrieval, 9(3):295?310.Eun Young Mun and Alexander Von Eye, 2004.
AnalyzingRater Agreement: Manifest Variable Methods.
LawrenceErlbaum Associates.Jong-Hoon Oh and Key-Sun Choi.
2006.
An ensemble oftransliteration models for information retrieval.
InformationProcessing Management, 42(4):980?1002.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation ofmachine translation.
In The 40th Annual Meeting of Associ-ation for Computational Linguistics, pages 311?318.Ari Pirkola, Jarmo Toivonen, Heikki Keskustalo, and KalervoJa?rvelin.
2006.
FITE-TRT: a high quality translation tech-nique for OOV words.
In Proceedings of the 2006 ACMSymposium on Applied Computing, pages 1043?1049.Claude Elwood Shannon.
1948.
A mathematical theory ofcommunication.
Bell System Technical Journal, 27:379?423.Paola Virga and Sanjeev Khudanpur.
2003.
Transliteration ofproper names in cross-language applications.
In ACM SIGIRConference on Research and Development on InformationRetrieval, pages 365?366.Dmitry Zelenko and Chinatsu Aone.
2006.
Discriminativemethods for transliteration.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural Language Process-ing, pages 612?617.647
