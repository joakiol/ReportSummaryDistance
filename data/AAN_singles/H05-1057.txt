Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 451?458, Vancouver, October 2005. c?2005 Association for Computational LinguisticsMatching Inconsistently Spelled Names in Automatic Speech RecognizerOutput for Information RetrievalHema Raghavan and James AllanDepartment of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA 01003, USA{hema,allan}@cs.umass.eduAbstractMany proper names are spelled inconsis-tently in speech recognizer output, posinga problem for applications where locatingmentions of named entities is critical.
Wemodel the distortion in the spelling of aname due to the speech recognizer as theeffect of a noisy channel.
The models fol-low the framework of the IBM translationmodels.
The model is trained using a par-allel text of closed caption and automaticspeech recognition output.
We also test astring edit distance based method.
The ef-fectiveness of these models is evaluated ona name query retrieval task.
Our methodsresult in a 60% improvement in F1.
Wealso demonstrate why the problem has notbeen critical in TREC and TDT tasks.1 IntroductionProper names are key to our understanding of topicsin news.
For example, to determine that a newsstory is on the 2004 elections in the United States,the words President Bush, John Kerry and USAare necessary features of the story.
In other words,names of people, places and organizations are keyentities of a news story.
For many tasks, like intopic detection and tracking (TDT), the entitiesform an important feature for distinguishing topicsfrom one another.
For example, it is the peoplethat distinguish stories on the 2004 election fromstories on the 2000 U.S election.
Names, especiallyrare and foreign ones are a problem for automaticspeech recognition (ASR) systems as they are oftenout of vocabulary (OOV) i.e., they do not exist inthe lexicon of the ASR system.
An OOV word isreplaced with the most similar word in the lexiconof the speech recognizer.
Sometimes, even if aname is in the lexicon of the speech recognizer, itmay have multiple spelling variants.
The followingis a sample ASR snippet from the TDT3 1 corpusthat demonstrates how the same entity may havedifferent spellings even within the same snippet ofASR text....newspaper quotes qaddafi is saying they?llturn them over but only if they?re allowed ..leadermoammar gadhafi says he doesn?t want an interna-tional confrontation over the suspects in the..In this work, we aim to find methods by which tocluster or group names in ASR text.
We evaluatea variety of techniques that range from a simplestring-edit distance model to generative modelsusing both intrinsic and extrinsic evaluations.
Weget statistically significant improvements in resultsfor ad-hoc retrieval when the query is just the nameof a person.
We also explain why the problemof misspelled proper names in ASR has not beenan issue in the TREC spoken document retrieval(SDR) track or in topic detection and tracking(TDT).
We demonstrate how the problem would beof significance when the query is short, containingmainly names with little or no context.1http:///www.ldc.upenn.edu/projects/tdt3/4512 Related WorkThat names can be spelled differently is a prob-lem that has been addressed by the database com-munity in great detail.
They found that the prob-lem was rising in significance with the increasinginterest in reconciling different databases.
Differ-ences in names due to spelling errors, spelling vari-ants and transliteration errors have been dealt withby different kinds of approximate string matchingtechniques like Soundex, Phonix, and String Editdistance (James C. French, 1997; Zobel and Dart,1996).
The nature of the problem is identical whenthe domain consists of databases of documents butin order to apply techniques that were developed fornames by the database community one would haveto first detect names in the corpus, and then normal-ize them to some canonical form.
This is the ap-proach taken by Raghavan and Allan (Raghavan andAllan, 2004) who showed that normalizing namesusing Soundex codes resulted in a 10% improvementon the TDT3 Story Link Detection Task.
They testedtheir method on newswire stories only.
Their diffi-culty in applying Soundex to the ASR documentswas that detecting names in ASR is too error pronefor their methods to be useful (Miller et al, 2000).Spoken document retrieval was a track at theTREC-6,7 and 8 (Voorhees and Harman, 1997;Voorhees and Harman, 1998; Voorhees and Harman,1999) conferences.
At the TREC-8 SDR track theconclusion was that ASR is not really an issue forad hoc retrieval.
However, the queries in those trackswere not centered on any entity.
The TREC-8 pro-ceedings also acknowledge that mean average preci-sion dropped as named entity word error rate (NE-WER) increased.
A typical speech recognizer has alexicon of about 60K and for this size of a lexicon,about 10% of the person names are out of vocabu-lary (OOV).The problem of alternate spellings of names hasalso been explored by the cross lingual informationretrieval community (Virga and Khudanpur, 2003;AbdulJaleel and Larkey, 2003).
The problem withnames in machine translated text is quite similar tothe problem with names in ASR text, except that theerrors caused by a speech recognizer are often pho-netic confusions, which is not necessarily the casefor machine translation errors.
Spelling errors ofnames in machine translated text are typically con-sistent.
A given word in the source language alwaystranslates to the same word in the target language fora given machine translation system.
As seen earlier,ASR systems do not exhibit such consistency.Another problem that resembles the one we areaddressing in this paper is that of spelling correc-tion.
Spelling correction has been tackled in severaldifferent ways (Durham et al, 1983), in some caseswith the use of contextual cues (Golding and Roth,1999) and in some cases it has been modeled as a?noisy channel problem?
(Kernighan et al, 1990).The latter approach is similar to ours because wealso approach the problem of spelling variations dueto speech recognizer errors as analogous to the er-rors caused by a noisy channel.
However, spellingcorrection methods must rectify human errors (ty-pographic errors and common confusions) whereasspeech recognizer errors are different.Additionally, the argument that Jon Smith andJohn Smythe may genuinely be different people andshould not be considered to be the same entity ismore of a cross-document co-reference problem.The problem we are attempting to solve in thispaper is one of grouping names that ?sound like?each other together, without considering the prob-lem of cross document co-reference.
For example,the name Lewinsky has 199 occurrences in the TDT3corpus, and also appears as Lewinski (1324 times),and Lewenskey (171 times).
Most of these occur-rences refer to Monica Lewinsky.
The aim is togroup all these variants together, without taking intoconsideration which ones refer to the same person.We then measure the effectiveness of our methodson various retrieval tasks.Perhaps the most similar work from the point ofview of the task is work in word spotting in audiooutput (Amir et al, 2001).
The queries are singlewords and the task is to locate their mention in au-dio.
The starting point in that work is however, aphonetic transcript of the audio signal and the em-phasis is not on locating names.
Our starting pointis automatic speech recognizer output, and we aimto locate names in particular.3 Our ApproachesIn this section we explain the techniques by whichwe group names together.
One method uses stringedit distance to group names that are variants of each452other.
The other techniques are some of the possiblegenerative models suitable to this task.An equivalence class is defined as a group ofnames such that any two names in that class are vari-ants of each other and such that there exist no twonames from different equivalence classes that arevariants of each other.
An equivalence class is rep-resented as a set of names enclosed in curly bracesas {name-1 name-2 ...}Four of our models are trained on a parallel textof ASR and manual transcripts (or closed captiondepending on availability) in order to learn a proba-bilistic model of ASR errors.
The parallel text con-sists of pairs of sentences: sentences from the ASRoutput and the corresponding manual transcripts.This is a common technique in machine translationfor which the IBM translation models are popularmethods (Brown et al, 1993).As a convention, we use uppercase letters to de-note ASR output and lowercase for manual tran-scriptions.
Given an input of parallel text of ASRand manual transcriptions, the model learns a prob-abilistic dictionary.
The dictionary contains pairsof closed caption and ASR words and the probabil-ity that the closed caption word is generated from agiven word in ASR.
Thus, the model might learn ahigh probability for P(CAT|kate).3.1 Overview of MethodsWe generate equivalence classes of names by clus-tering a list of names.
The algorithm draws links be-tween pairs of words and then clusters the words intoequivalence classes such that if a and b are linkedand b and c are linked then a, b and c are in the sameequivalence class.
Links between words are gener-ated in five different ways described below.In the first of our methods we align manual tran-scripts and ASR sentences using the IBM transla-tion model (Brown et al, 1993) to obtain a proba-bilistic dictionary.
We give details of the translationmodel in section 3.2.
Names are grouped such thatif P(CAT|kate) is high (above some threshold) thenthere is a link between CAT and kate.
This is calledthe Simple Aligned method.
Some sample pairs ofwords obtained by this technique are shown in fig-ure 1.We can also ask a human to create a list of equiv-alence classes of names.
We describe our methodafrican AFRICA albania ALBANIANalex ALEC cardoso CARDOZOann ANNE ching CHIANGFigure 1: Example of pairs of words obtained bySimple Alignedof obtaining such a list in section 4.
This method iscalled the Supervised method.Given a list of equivalence classes, pairs of namesthat go together can easily be generated such that foreach pair, both words are obtained from the sameequivalence class.
In this way equivalence classesof names obtained from the Simple Aligned and Su-pervised methods can be used to create a list of pairsof names that form parallel text to train a charac-ter level machine translation model.
We would ex-pect this model to learn a high probability for simi-lar sounding alphabets, e.g., a high probability forP (C|k).
Depending on where the training set ofpairs of names for this method comes from, we gettwo possible systems.
These are called the Gener-ative Unsupervised method and Generative Super-vised method respectively.
Note that the Genera-tive Unsupervised method is not completely unsu-pervised; we still need the parallel text of ASR andmanual transcripts, but we don?t need a human todo the added grouping of names into equivalenceclasses.
A character level translation model helpsus generalize better to unseen words.We also grouped together names that differ by astring edit distance of one, giving a fifth system.
Inparticular, we use the Levenshtein distance (Lev-enshtein, 1966), that is the number of insertions,deletions and substitutions needed to convert onestring to the other.
Many methods employed by thedatabase community build on string edit distance.The method works well but has some disadvantages.Consider a user who types in a query containing aname such that the spelling, as typed by the user,never occurs in the corpus.
To employ string editdistance, one would have to compare the query nameagainst all the words in the vocabulary of the cor-pus to find the most similar strings.
With a gener-ative model, only the query needs to be expandedusing the translation model, thereby speeding up thesearch process.
The string edit distance model on the453other hand, is completely unsupervised and needs notraining in the form of parallel text.
Both methodshave their advantages and disadvantages, and the useof one method over the other is situation dependent.3.2 DetailsTo learn alignments, translation probabilities, etc inthe first method we used work that has been done instatistical machine translation (Brown et al, 1993),where the translation process is considered to beequivalent to a corruption of the source language textto the target language text due to a noisy channel.We can similarly consider that an ASR system cor-rupts the spelling of a name as a result of a noisychannel.
To obtain the closed caption word c, of anASR word a, we want to find the string for whichthe probability P (c|a) is highest.
This is modeled asP (c|a) = P (c)P (a|c)P (a) (1)For a given name a, since P (a) is constant, theproblem reduces to one of maximizing P (c)P (a|c).P (c) is called the language model.
We needto model P (a|c) as opposed to directly modelingP (c|a) so that our model assigns more probabilityto well formed English names.Given a pair of sentences (c, a), an alignmentA(c, a) is defined as the mapping from the wordsin c to the words in a.
If there are l closed captionwords and m ASR words, there are 2lm alignmentsin A(c, a).
l ?
A(c, a) can be denoted as a serieslm1 = l1, l2...lm where lj = i means that a word inposition j of the ASR string is aligned with a word inposition i of the closed caption string.
Then P (a|c)is computed as follows:P (a|c) =?lP (a, l|c)P (a, l|c) = P (m|c)m?jP (lj |lj?11 , aj?11 , m, e)?P (aj |lj1, aj?11 , m, c) (2)where aj is a word in position j of the string a, andaj1 is the series a1...aj .
The model is generative inthe following way: we first choose for each word inthe closed caption string the number of ASR wordsthat will be connected to it, then we pick the identityof those ASR words and finally we pick the actualpositions that these words will occupy.
There arefive different IBM translation models (Brown et al,1993).
Models 3 and 4 build on the above equations,and also incorporate the notion of fertility.
Fertilitytakes into account that a given word in closed cap-tion may be omitted by an ASR system, or one wordmay result in two or more, like Iraq?
I ROCK (Thisis a true example).
The models are trained using Ex-pectation Maximization.
Further details are in theoriginal paper (Brown et al, 1993).The IBM models have shown good performancein machine translation, and especially so within cer-tain families of languages, for example in translatingbetween French and English or between Sinhaleseand Tamil (Brown et al, 1993; Weerasinghe, 2004).Pairs of closed caption and ASR sentences or words(as the case may be) are akin to a pair of closely re-lated languages.For the Generative Unsupervised and GenerativeSupervised methods, we use the same models, but inthis case the training set consists of pairs of wordsobtained from the ASR and closed caption text asopposed to sentences.
In other words, the place ofwords in the previous case is taken by characters.Modeling fertility, etc, again fits very well in thiscase.
For example the terminal character e is oftendropped in ASR, and a single o in closed captionmay result in a double o in ASR or vice versa.4 Experimental Set Up4.1 CorporaFor experiments in this paper we used the TREC-6and TREC-7 SDR track data (Voorhees and Harman,1998).
We also used the TDT2 and TDT3 corpora.For TREC-6 we had the ASR output provided byNIST (WER 34%).
The TREC-7 corpus consists ofthe output of the Dragon systems speech recognizer(WER 29.5%).
For the TDT sources we had theASR output of the BBN Byblos Speech recognizerprovided by the LDC.
NIST provides human gener-ated transcripts for the TREC corpora and LDC pro-vides closed caption quality transcripts with a WERof 14.5% for the TDT corpora.
There are 3943,23282, 1819 and 2866 ASR documents in the TDT2TDT3, TREC-6 and TREC-7 corpora respectively.4544.2 Intrinsic EvaluationThe Paice evaluation (Paice, 1996) for stemming al-gorithms (algorithms that reduce a word to its mor-phological root), attempts to compare the equiva-lence classes generated by our methods with humanjudgments.The Paice evaluation measures the performanceof a stemmer based on its understemming and over-stemming indices (UI and OI respectively).
UImeasures the total number of missed links betweenwords and OI measures the total number of falsealarm links.
A perfect stemmer would have a UI andOI value of zero.We obtained a list of names to be grouped intoequivalence classes in the following way.
We didnot use a named entity tagger on the corpus becausenamed entity taggers typically have very high worderror rates for ASR text (Bikel et al, 1999).
Insteadwe ran the Unix spell command on the corpus andused the list of rejected words as the list of namesfor the annotators to group into equivalence classes.These 296 OOV words are taken to correspond tothe names in the corpus.
We then obtained the set ofground-truth equivalence classes by a method simi-lar to Paice.A group of undergraduate students was hired.
Thelist of names was provided to each student in a texteditor in alphabetical order.
The purpose as ex-plained to them was to group together names thatwere alternate spellings of similar sounding namestogether.
The student was instructed to go throughthe list systematically, and for each word to lookat the previous 10 words, as well as the following10 words to see if there were any other variants.
Ifthere was a word or a group where the current wordwas likely to fit in, they were asked to cut the wordand paste it into the appropriate group.
In this way,groups were created such that no word could belongto more than one group.
The annotators were alsoasked to mark the words that were indeed names.
Ofthe 296 OOV words, 292 were found to be actualnames.4.3 Extrinsic evaluationIn addition to the Paice evaluation we propose twoextrinsic or task based evaluations for our methods.In the first task, given a name as a query, we aim toQuery Equivalence class1: {christy christie}2: {christina christine}3: {toney toni}4: {michelle michel mitchell}5: {columbia colombia colombian}Figure 2: Some sample query equivalence classesfind all documents that have a mention of that nameor any of its variants.
In order to obtain queriesand relevance judgments for this task we arbitrar-ily chose 35 groups of names from the ground-truthset of equivalence classes.
The TDT3 corpus waschosen to be the test corpus for this task.
Hence weeliminated those words that had no occurrence in theTDT3 corpus from the 35 groups of names giving atotal of 76 names.
Each of the 76 words formed aquery.
For each name query we consider all docu-ments that contain a mention of any of the names inthe equivalence class of the query as relevant to thatquery.
In this way we obtained relevance judgmentsfor the name query task.
Some sample queries areshown in figure 2.
We use F1 (harmonic mean of theprecision and recall) as a measure of performance.Our extrinsic evaluation is spoken document re-trieval.
The queries on the TREC-6 and TREC-7corpora are standard TREC spoken document re-trieval track queries.
For the TDT2 corpus we useone randomly chosen document from each topic asthe query.
This document is like a long query withplenty of entities and plenty of contextual informa-tion.
For the TDT3 corpus we use the topic de-scriptions as provided by the LDC as the queries.The LDC topic descriptions discuss the events thatdescribe a topic and the key entities and locationsinvolved in the event.
These are representative ofshorter queries, rich in entities.
LDC has providedrelevance judgments for both the TDT2 and TDT3corpora.
Mean average precision was used as themeasure of evaluation.4.4 Implementation DetailsWe use GIZA++ (Och and Ney, 2003) to train themachine translation system and the ISI ReWriteDecoder (ISI, 2001) to do the actual translations.The decoder takes as input the models learned by455GIZA++ and a sentence from the foreign language.It can output the top n translations of the input sen-tence.
The ReWrite decoder can translate using IBMModel-3 or Model-4.
We found Model 3 to havelower perplexity and hence chose it for our experi-ments.
In order to build the language model P (c),we used the CMU Language Modeling toolkit 2.All retrieval experiments were performed using theLEMUR 3 toolkit, and using the traditional vectorspace model.
In the traditional vector space modelqueries and documents are represented as vectors ofwords.
Each word in the vector is weighted usinga product of term frequency and inverse documentfrequency.
The similarity between a query and adocument is measured using the cosine of the anglebetween the query and document vectors.The Simple Aligned and Generative Unsuper-vised methods require a parallel corpus of ASR andclosed caption for training.
For the name query taskwe used TDT2, TREC-6 and TREC-7 to train thesemethods and TDT3 as the test corpus.The Supervised and Generative Supervised meth-ods require a human to provide pairs of words thatare variants of each other.
We filtered out thosewords from the human generated list of equivalenceclasses that occurred exclusively in the test corpusand in no other corpus.
This is equivalent to askinga human to group words in the training corpus.
Sim-ilarly we trained the Simple Aligned and GenerativeUnsupervised models using ASR and closed captiontext from all other sources except those in the testset.The models were trained similarly for the SDRexperiments.
The models were tested on each ofthe four corpora in turn, and in each case they weretrained on everything but the test corpus.5 Results5.1 Intrinsic ExperimentsTable 1 shows how the different methods perform onthe intrinsic evaluation.
We also show the UI and OIvalues for methods that use string edit distances of2, 3, 4 and 5.
Note that the Supervised method is theground truth for this evaluation, and hence it has a UIand OI value of zero.
A string edit distance of 1 has2http://mi.eng.cam.ac.uk/prc14/toolkit documentation.html3http://www.cs.cmu.edu/lemurMethod UI OISimple Aligned 0.236 0.004Supervised 0 0Gen Sup 0.393 0.023Gen Uns 0.351 0.003Str.
Ed.
(1) 0.229 0.000Str.
Ed.
(2) 0.083 0.003Str.
Ed.
(3) 0.039 0.001Str.
Ed.
(4) 0.031 0.124Str.
Ed.
(5) 0.023 0.336Table 1: Understemming and Overstemming indicesfor each of the methods (lower is better)the lowest OI value, meaning there are very few falsealarms.
Higher string edit distances have lower UIvalues, with an increase in OI.
We will interpret theUI and OI values again after observing performanceon the retrieval tasks, so as to interpret the impact ofmissed links and false alarm links for retrieval.5.2 Name Query Retrieval experimentsThe results of our experiments on the name querytask are given in table 2.
We report both Macroand Micro averaged (averaged over the equivalenceclasses of the queries) F1 measures.
They do not dif-fer much since the equivalence classes have almostthe same number (2-3) of names.From table 2, all methods improve the baselineF1 score significantly (statistical significance mea-sured using a two tailed t-test with a confidence of95%).
In general, the Simple Aligned, GenerativeUnsupervised and string edit distance methods arethe best performing for this task.
The string editdistance improves the baseline by over 60%.
TheSupervised method is also not as good as the otherfour of our methods as it does not generalize well tonames that occur exclusively in the test set.String edit distance performs very well on cer-tain equivalence classes of names.
For example, onthe equivalence class {Seigal, Segal, Siegal, Siegel}the precision and recall are 100% each since all ofthe words in the equivalence class differ from eachother by a string edit distance of one.
In the case ofthe equivalence class {Lewenskey Lewinski Lewin-sky}, the term Lewenskey has a string edit distanceof 2 (greater than one) from the other two members,456Method Micro avg Micro avg Micro Macro avg Macro avg MacroRecall Precision F1 Recall Precision F1Baseline 0.401 1 0.573 0.400 1 0.571Simple Aligned 0.632 0.933 0.754 0.608 0.925 0.734Sup 0.477 0.961 0.638 0.463 0.960 0.625Gen Sup 0.530 0.937 0.677 0.517 0.938 0.667Gen Uns 0.590 0.921 0.720 0.576 0.913 0.706Str.
Ed 0.752 0.867 0.806 0.751 0.871 0.807Table 2: Results on the Name Query Retrieval taskLewinsky and Lewinski.
The equivalence class of{John Jon Joan} has very low precision and recall.This is because both John and Jon differ by a stringedit distance of one from so many other names in thecorpus, such as Jong, resulting in lowered precision.The Simple Aligned method fails on names it hasnot seen in the training set.
However, for caseslike {Greensborough Greensboro} the link betweenthese two names is detected using the simple alignedmethod and by no other.
The generative methods candetect variations in spelling due to similar soundingalphabets.
For example it can detect the link be-tween Sydney and Sidney.
The generative modelswere also able to learn that c and k are substitutablefor each other.
Therefore these models could detectthe links between the words in the equivalence class{Katherine Kathryn Catherine}.The Simple Aligned model performs well on theextrinsic evaluations although it has a high OI value.The intrinsic evaluations use judgments by humans.The Simple Aligned method would conflate Kofi andCopy into one class if that was a genuine ASR errorand the alignment was correct, but these two wordswould not be conflated into the same equivalenceclass by our annotators and would actually countas a false alarm on the intrinsic evaluations.
There-fore, although the OI is high for the Simple AlignedMethod, on closer examination we found that someof the false alarms were actually representative ofASR errors.5.3 Spoken Document RetrievalWe now move on to discuss results on the SDR task.For TDT3 we got statistically significant improve-ments (an improvement in mean average precisionfrom 0.715 to 0.757) over the baseline using stringedit distance.
On the remaining corpora we got littleor no improvement by our methods.
We proceed toexplain why this is the case for each of the corpora.The TREC-7 corpus has only 5 queries with amention of a name resulting in hardly any gainsoverall.
Similar was the case for TREC-6.
Againin the case of the TDT2 corpus, since we used en-tire documents as stories, there are enough words inthe query that a few recognition errors can be toler-ated and therefore traditional retrieval is good for thetask.
There is evidence from previous TREC tracks(Voorhees and Harman, 1999) that shorter queriesresult in a decrease in retrieval performance andhence we see some improvements for TDT3.
Be-sides, the TDT3 queries were rich in names.We wanted to check how our methods performedon outputs of different ASR systems.
Spoken doc-ument retrieval on the TREC-7 data with the out-put of Dragon systems, which has a word error rateof 29.5%, results in an improvement of 6% usingthe Simple Aligned method.
The NIST-B2 systemwith a higher WER (46.6%) has an improvement inMean Average Precision of 6.5%.
Similarly with theCUHTK (WER 35.6%) and NIST-B1 (WER 33.8%)and Sheffield (WER 24.6 %) systems we obtainedimprovements of 1.6%, 0.39% and 0.05% respec-tively using the Simple Aligned method.
Thus, withincreasing WER, the named entity word error rateincreases significantly, and therefore the benefits ofour method are more apparent in such situations.6 Discussion and ConclusionsWe showed (both intrinsically and extrinsically) thatstring edit distance is an effective technique for lo-cating name variants.
We also developed a set ofgenerative models and showed that they are almost457as effective at name finding and document retrieval,but are probably more efficient than string edit dis-tance.
The generative models need to be trained onparallel text and therefore require human effort fortraining the models.
The advantage of one methodover the other is dependent on the size of the corpusand the availability of resources.The problem has not been of significance in previ-ous TREC tasks or in TDT, because we have alwaysescaped the problem of misspelled names by virtueof the nature of those tasks.
In the TREC tasks veryfew queries are centered on an entity.
In all the TDTtasks, one is usually required to compare entire sto-ries with each other.
A story is long enough thatthere are enough words that are in the vocabulary(just like a very long query) or that are correctly rec-ognized, that the ASR errors do not really matter.Therefore, the TDT tasks also do not suffer as a re-sult of these ASR errors.We can improve and apply our methods to otherdomains like Switchboard data (Godfrey et al,1992).
Our methods also generalize well across lan-guages since there are no language specific tech-niques employed.7 AcknowledgementsThis work was supported in part by the Centerfor Intelligent Information Retrieval and in part bySPAWARSYSCEN-SD grant number N66001-02-1-8903.
Any opinions, findings and conclusions orrecommendations expressed in this material are theauthor(s) and do not necessarily reflect those of thesponsor.ReferencesNasreen AbdulJaleel and Leah S. Larkey.
2003.
Statisticaltransliteration for english-arabic cross language informationretrieval.
In Proceedings of the 12th CIKM conference,pages 139?146.
ACM Press.Arnon Amir, Alon Efrat, and Savitha Srinivasan.
2001.
Ad-vances in phonetic word spotting.
In CIKM ?01: Proceed-ings of the tenth international conference on Information andknowledge management, pages 580?582, New York, NY,USA.
ACM Press.Daniel M. Bikel, Richard L. Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?s in aname.
Machine Learning, 34(1-3):211?231.P.
F. Brown, Steven A. Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
ComputationalLingustics, 19(2):263?311.Ivor Durham, David A. Lamb, and James B. Saxe.
1983.Spelling correction in user interfaces.
Commun.
ACM,26(10):764?773.J.
Godfrey, E. Holiman, and J. McDaniel.
1992.
Switchboard:Telephone speech corpus for research and development.
InProceedings of the International Conference on Acoustics,Speech and Signa Processing pp.
I-517-520, 1992, pages517?520.Andrew R. Golding and Dan Roth.
1999.
A winnow-basedapproach to context-sensitive spelling correction.
MachineLearning, 34(1-3):107?130.2001.
ISI rewrite decoder, http://www.isi.edu/licensed-sw/rewrite-decoder/.Allison L. Powell James C. French.
1997.
Applications of ap-proximate word matching in information retrieval.
In Pro-ceedings of the Sixth CIKM Conference.Mark D. Kernighan, Kenneth W. Church, , and William A. Gale.1990.
A spelling correction program based on a noisy chan-nel model.
In Proceedings of COLING-90, pages 205?210.V.
I. Levenshtein.
1966.
Binary codes capable of correcingdeletions,insertions and reversals.
Phs.
Dokl., 6:707?710.David Miller, Richard Schwartz, Ralph Weischedel, and Re-becca Stone.
2000.
Named entity extraction from broadcastnews.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.Chris D. Paice.
1996.
Method for evaluation of stemming al-gorithms based on error counting.
JASIS, 47(8):632?649.Hema Raghavan and James Allan.
2004.
Using soundex codesfor indexing names in asr documents.
In Proceedings of theHLT NAACL Workshop on Interdisciplinary Approaches toSpeech Indexing and Retrieval.Paola Virga and Sanjeev Khudanpur.
2003.
Transliteration ofproper names in cross-language applications.
In Proceed-ings of the 26th ACM SIGIR conference, pages 365?366.ACM Press.E.
M. Voorhees and D. K. Harman, editors.
1997.
The SixthText REtrieval Conference (TREC 6).
NIST.E.
M. Voorhees and D. K. Harman, editors.
1998.
The SeventhText REtrieval Conference (TREC 7).
NIST.E.
M. Voorhees and D. K. Harman, editors.
1999.
The EighthText REtrieval Conference (TREC 8).
NIST.Ruvan Weerasinghe.
2004.
A statistical machine translationapproach to Sinhala Tamil language translation.
In SCALLA2004.Justin Zobel and Philip W. Dart.
1996.
Phonetic string match-ing: Lessons from information retrieval.
In Proceedings ofthe 19th ACM SIGIR Conference,(Special Issue of the SIGIRForum), pages 166?172.458
