A Corpus-Based Approach for Building Semantic LexiconsE l len  R i lo f f  and  Jess ica  ShepherdDepar tment  of  Computer  Sc ienceUn ivers i ty  of  UtahSalt  Lake City,  UT  84112riloff?cs, utah.
eduAbst ractSemantic knowledge can be a great asset tonatural language processing systems, butit is usually hand-coded for each applica-tion.
Although some semantic informationis available in general-purpose knowledgebases such as WordNet and Cyc, many ap-plications require domain-specific lexiconsthat represent words and categories for aparticular topic.
In this paper, we presenta corpus-based method that can be usedto build semantic lexicons for specific cat-egories.
The input to the system is a smallset of seed words for a category and a rep-resentative text corpus.
The output is aranked list of words that are associatedwith the category.
A user then reviews thetop-ranked words and decides which onesshould be entered in the semantic lexicon.In experiments with five categories, userstypically found about 60 words per cate-gory in 10-15 minutes to build a core se-mantic lexicon.1 In t roduct ionSemantic information can be helpful in almost allaspects of natural language understanding, includ-ing word sense disambiguation, selectional restric-tions, attachment decisions, and discourse process-ing.
Semantic knowledge can add a great deal ofpower and accuracy to natural language processingsystems.
But semantic information is difficult to ob-tain.
In most cases, semantic knowledge is encodedmanually for each application.There have been a few large-scale fforts to cre-ate broad semantic knowledge bases, such as Word-Net (Miller, 1990) and Cyc (Lenat, Prakash, andShepherd, 1986).
While these efforts may be use-ful for some applications, we believe that they willnever fully satisfy the need for semantic knowledge.Many domains are characterized by their own sub-language containing terms and jargon specific tothe field.
Representing all sublanguages in a singleknowledge base would be nearly impossible.
Fur-thermore, domain-specific semantic lexicons are use-ful for minimizing ambiguity problems.
Within thecontext of a restricted domain, many polysemouswords have a strong preference for one word sense,so knowing the most probable word sense in a do-main can strongly constrain the ambiguity.We have been experimenting with a corpus-based method for building semantic lexicons semi-automatically.
Our system uses a text corpus anda small set of seed words for a category to identifyother words that also belong to the category.
Thealgorithm uses simple statistics and a bootstrappingmechanism to generate a ranked list of potential cat-egory words.
A human then reviews the top wordsand selects the best ones for the dictionary.
Our ap-proach is geared toward fast semantic lexicon con-struction: given a handful of seed words for a cate-gory and a representative t xt corpus, one can builda semantic lexicon for a category in just a few min-utes.In the first section, we describe the statisticalbootstrapping algorithm for identifying candidatecategory words and ranking them.
Next, we describeexperimental results for five categories.
Finally, wediscuss our experiences with additional categoriesand seed word lists, and summarize our results.2 Generat ing  a Semant ic  Lex iconOur work is based on the observation that categorymembers are often surrounded by other categorymembers in text, for example in conjunctions (lionsand tigers and bears), lists (lions, tigers, bears...),appositives (the stallion, a white Arabian), and nom-inal compounds (Arabian stallion; tuna fish).
Givena few category members, we wondered whether it117would be possible to collect surrounding contextsand use statistics to identify other words that alsobelong to the category.
Our approach was moti-vated by Yarowsky's word sense disambiguation al-gorithm (Yarowsky, 1992) and the notion of statis-tical salience, although our system uses somewhatdifferent statistical measures and techniques.We begin with a small set of seed words for acategory.
We experimented with different numbersof seed words, but were surprised to find that only5 seed words per category worked quite well.
As anexample, the seed word lists used in our experimentsare shown below.Energy :  fuel gas gasoline oil powerF inanc ia l :  bank banking currency dollar moneyMi l i ta ry :  army commander infantry soldiertroopVehicle:  airplane car jeep plane truckWeapon:  bomb dynamite xplosives gun rifleFigure 1: Initial Seed Word ListsThe input to our system is a text corpus and aninitial set of seed words for each category.
Ideally,the text corpus should contain many references tothe category.
Our approach is designed for domain-specific text processing, so the text corpus should bea representative sample of texts for the domain andthe categories hould be semantic lasses associatedwith the domain.
Given a text corpus and an initialseed word list for a category C, the algorithm forbuilding a semantic lexicon is as follows:1.
We identify all sentences in the text corpus thatcontain one of the seed words.
Each sentence isgiven to our parser, which segments the sen-tence into simple noun phrases, verb phrases,and prepositional phrases.
For our purposes, wedo not need any higher level parse structures.2.
We collect small context windows surroundingeach occurrence of a seed word as a head nounin the corpus.
Restricting the seed words tobe head nouns ensures that the seed word isthe main concept of the noun phrase.
Also,this reduces the chance of finding different wordsenses of the seed word (though multiple nounword senses may still be a problem).
We use avery narrow context window consisting of onlytwo words, the first noun to the word's rightand the first noun to its left.
We collected onlynouns under the assumption that most, if notall, true category members would be nouns31 Of course, this may depend on the target  categories.118.The context windows do not cut across sen-tence boundaries.
Note that our context win-dow is much narrower than those used by otherresearchers (Yarowsky, 1992).
We experimentedwith larger window sizes and found that the nar-row windows more consistently included wordsrelated to the target category.Given the context windows for a category, wecompute a category score for each word, whichis essentially the conditional probability thatthe word appears in a category context.
Thecategory score of a word W for category C isdefined as:?corefW ?7~ - /reg.
o/ w in O's context windows v /  freq.
o\] W in corpus..Note that this is not exactly a conditional prob-ability because a single word occurrence can be-long to more than one context window.
Forexample, consider the sentence: I bought anAK-~7 gun and an M-16 rifle.
The word M-16would be in the context windows for both gunand rifle even though there was just one occur-rence of it in the sentence.
Consequently, thecategory score for a word can be greater than 1.Next, we remove stopwords, numbers, and anywords with a corpus frequency < 5.
We useda stopword list containing about 30 generalnouns, mostly pronouns (e.g., /, he, she, they)and determiners (e.g., this, that, those).
Thestopwords and numbers are not specific to anycategory and are common across many domains,so we felt it was safe to remove them.
The re-maining nouns are sorted by category score andranked so that the nouns most strongly associ-ated with the category appear at the top.The top five nouns that are not already seedwords are added to the seed word list dynam-ically.
We then go back to Step 1 and repeatthe process.
This bootstrapping mechanism dy-namically grows the seed word list so that eachiteration produces a larger category context.
Inour experiments, the top five nouns were addedautomatically without any human intervention,but this sometimes allows non-category wordsto dilute the growing seed word list.
A few in-appropriate words are not likely to have muchimpact, but many inappropriate words or a fewhighly frequent words can weaken the feedbackprocess.
One could have a person verify thateach word belongs to the target category be-fore adding it to the seed word list, but thiswould require human interaction at each itera-tion of the feedback cycle.
We decided to seehow well the technique could work without thisadditional human interaction, but the potentialbenefits of human feedback still need to be in-vestigated.After several iterations, the seed word list typi-cally contains many relevant category words.
Butmore importantly, the ranked list contains many ad-ditional category words, especially near the top.
Thenumber of iterations can make a big difference inthe quality of the ranked list.
Since new seed wordsare generated ynamically without manual review,the quality of the ranked list can deteriorate rapidlywhen too many non-category words become seedwords.
In our experiments, we found that abouteight iterations usually worked well.The output of the system is the ranked list ofnouns after the final iteration.
The seed word listis thrown away.
Note that the original seed wordswere already known to be category members, andthe new seed words are already in the ranked listbecause that is how they were selected.
~Finally, a user must review the ranked list andidentify the words that are true category members.How one defines a "true" category member is sub-jective and may depend on the specific application,so we leave this exercise to a person.
Typically, thewords near the top of the ranked list are highly asso-ciated with the category but the density of categorywords decreases as one proceeds down the list.
Theuser may scan down the list until a sufficient numberof category words is found, or as long as time per-mits.
The words selected by the user are added toa permanent semantic lexicon with the appropriatecategory label.Our goal is to allow a user to build a semanticlexicon for one or more categories using only a smallset of known category members as seed words and atext corpus.
The output is a ranked list of potentialcategory words that a user can review to create a se-mantic lexicon quickly.
The success of this approachdepends on the quality of the ranked list, especiallythe density of category members near the top.
Inthe next section, we describe experiments to evalu-ate our system.21t is possible that a word may be near the top ofthe ranked list during one iteration (and subsequentlybecome a seed word) but become buried at the bottomof the ranked list during later iterations.
However, wehave not observed this to be a problem so far.3 Exper imenta l  Resu l tsWe performed experiments with five categories toevaluate the effectiveness and generality of our ap-proach: energy, financial, military, vehicles, andweapons.
The MUC-4 development corpus (1700texts) was used as the text corpus (MUC-4 Pro-ceedings, 1992).
We chose these five categories be-cause they represented relatively different semanticclasses, they were prevalent in the MUC-4 corpus,and they seemed to be useful categories.For each category, we began with the seed wordlists shown in Figure 1.
We ran the bootstrappingalgorithm for eight iterations, adding five new wordsto the seed word list after each cycle.
After the finaliteration, we had ranked lists of potential categorywords for each of the five categories.
The top 45words 3 from each ranked list are shown in Figure 2.While the ranked lists are far from perfect, onecan see that there are many category members nearthe top of each list.
It is also apparent that a few ad-ditional heuristics could be used to remove many ofthe extraneous words.
For example, our number pro-cessor failed to remove numbers with commas (e.g.,2,000).
And the military category contains severalordinal numbers (e.g., lOth 3rd 1st) that could beeasily identified and removed.
But the key questionis whether the ranked list contains many true cate-gory members.
Since this is a subjective question,we set up an experiment involving human judges.For each category, we selected the top 200 wordsfrom its ranked list and presented them to a user.We presented the words in random order so thatthe user had no idea how our system had rankedthe words.
This was done to minimize contextualeffects (e.g., seeing five category members in a rowmight make someone more inclined to judge the nextword as relevant).
Each category was judged by twopeople independently.
4The judges were asked to rate each word on a scalefrom 1 to 5 indicating how strongly it was associ-ated with the category.
Since category judgementscan be highly subjective, we gave them guidelinesto help establish uniform criteria.
The instructionsthat were given to the judges are shown in Figure 3.We asked the judges to rate the words on a scalefrom 1 to 5 because different degrees of categorymembership might be acceptable for different appli-cations.
Some applications might require strict cat-3Note that some of these words are not nouns, such asboardedand U.S.-made.
Our parser tags unknown wordsas nouns, so sometimes unknown words are mistakenlyselected for context windows.4 The judges were members of our research group butnot the authors.119Energy:  Limon-Covenas  oligarchs pill staplespoles Limon Barrancabermeja Covenas 200,000barrels oil Bucaramanga pipeline prices electricpipelines towers Cano substation transmissionrates pylons pole infrastructure transfer gas fuelsale lines companies power tower price gasolineindustries insurance Arauca stretch inc industryforum nationalization supply electricity controlsF inancia l :  monetary fund nationalizationattractive circulation suit gold branches managerbank advice invested banks bomb_explosioninvestment invest announcements contentmanagers insurance dollar savings productemployee accounts goods currency reservesamounts money shops farmers maintenanceItagui economies companies foundationmoderation promotion annually cooperativesempire loans industry possessionMi l i tary :  infantry 10th 3rd 1st brigade techni-cian 2d 3d moran 6th 4th Gaspar 5th 9th Amil-car regiment sound 13th Pineda brigades Anayadivision Leonel contra anniversary ranksUzcategui brilliant Aristides escort dispatched8th Tablada employee skirmish puppetRolando columns (FMLN) deserter troopsNicolas Aureliano Montes FuentesVehicle: C-47 license A-37 crewmen plateplates crash push tank pickup Cessna air-craft cargo passenger boarded Boeing_727 luxuryAvianca dynamite_sticks hostile passengers acci-dent sons airplane light plane flight U.S.-madeweaponry truck airplanes gunships fighter carrierapartment schedule flights observer tanks planesLa._Aurora b fly helicopters helicopter poleWeapon:  fragmentation sticks cartridge AK-47M-16 carbines AR-15 movie clips knapsacks cal-ibers TNT rifles cartridges theater 9-mm 40,000quantities grenades machineguns dynamite kgammunition revolvers FAL rifle clothing bootsmaterials ubmachineguns M-60 pistols pistol M-79 quantity assault powder fuse grenade calibersquad mortars explosives gun 2,000"Limon-Covenas refers to an oil pipehne.bLa_Aurora refers to an airport.Figure 2: The top-ranked words for each categoryCRITERIA: On a scale of 0 to 5, rate each word'sstrength of association with the given category usingthe following criteria.
We'll use the category ANI-MAL as an example.5: CORE MEMBER OF THE CATEGORY:If a word is clearly a member of the category,then it deserves a 5.
For example, dogs andsparrows are members of the ANIMAL cate-gory.4: SUBPART OF MEMBER OF THECATEGORY:If a word refers to a part of something that isa member of the category, then it deserves a4.
For example, feathers and tails are parts ofANIMALS.3: STRONGLY ASSOCIATED WITH THECATEGORY:If a word refers to something that is stronglyassociated with members of the category, butis not actually a member of the category itself,then it deserves a 3.
For example, zoos andnests are strongly associated with ANIMALS.2: WEAKLY ASSOCIATED WITH THECATEGORY:If a word refers to something that can be as-sociated with members of the category, but isalso associated with many other types of things,then it deserves a 2.
For example, bowls andparks are weakly associated with ANIMALS.1: NO ASSOCIATION WITH THE CATEGORY:If a word has virtually no association with thecategory, then it deserves a 1.
For example,tables and moons have virtually no associationwith ANIMALS.0: UNKNOWN WORD:If you do not know what a word means, then itshould be labeled with a 0.IMPORTANT!
Many words have several distinctmeanings.
For example, the word "horse" can re-fer to an animal, a piece of gymnastics equipment,or it can mean to fool around (e.g., "Don't horsearound!").
If a word has ANY meaning associatedwith the given category, then only consider thatmeaning when assigning numbers.
For example, theword "horse" would be a 5 because one of its mean-ings refers to an ANIMAL.Figure 3: Instructions to human judges120egory membership, for example only words like gun,rifle, and bomb should be labeled as weapons.
Butfrom a practical perspective, subparts of categorymembers might also be acceptable.
For example, ifa cartridge or trigger is mentioned in the contextof an event, then one can infer that a gun was used.And for some applications, any word that is stronglyassociated with a category might be useful to in-clude in the semantic lexicon.
For example, wordslike ammunition or bullets are highly suggestive of aweapon.
In the UMass/MUC-4 information extrac-tion system (Lehnert et al, 1992), the words ammu-nition and bullets were defined as weapons, mainlyfor the purpose of selectional restrictions.The human judges estimated that it took them ap-proximately 10-15 minutes, on average, to judge the200 words for each category.
Since the instructionsallowed the users to assign a zero to a word if theydid not know what it meant, we manually removedthe zeros and assigned ratings that we thought wereappropriate.
We considered ignoring the zeros, butsome of the categories would have been severelyimpacted.
For example, many of the legitimateweapons (e.g., M-16 and AR-15) were not knownto the judges.
Fortunately, most of the unknownwords were proper nouns with relatively unambigu-ous semantics, so we do not believe that this processcompromised the integrity of the experiment.Finally, we graphed the results from the humanjudges.
We counted the number of words judgedas 5's by either judge, the number of words judgedas 5's or 4's by either judge, the number of wordsjudged as 5's, 4's, or 3's by either judge, and thenumber of words judged as either 5's, 4's, 3's, or 2's.We plotted the results after each 20 words, step-ping down the ranked list, to see whether the wordsnear the top of the list were more highly associatedwith the category than words farther down.
We alsowanted to see whether the number of category wordsleveled off or whether it continued to grow.
The re-sults from this experiment are shown in Figures 4-8.With the exception of the Energy category, wewere able to find 25-45 words that were judged as4's or 5's for each category.
This was our strictesttest because only true category members (or sub-parts of true category members) earned this rating.Although this might not seem like a lot of categorywords, 25-45 words is enough to produce a reason-able core semantic lexicon.
For example, the wordsjudged as 5's for each category are shown in Figure 9.Figure 9 illustrates an important benefit of thecorpus-based approach.
By sifting through a largetext corpus, the algorithm can find many relevantcategory words that a user would probably not en-lO0.9o,180, !7o!
'~ 6oi50 ) , .
.
.~ 40 ,!
.o".
?
,30,: ""20 " ~-- ~ - - '10 !
'~  s--  ~ '.
.
.
.
.
.
.
.
.
.i .
.
.
?0 20 40 60 80 100120140160180200Words ReviewedFigure 4: Energy Results, - , , , -  54&53&4&5.
.
.
.
.
2&3&4&5I70.i3oi|OOL9oi801 i "?"?..~.-?"?
!60:  .-"50) ?"
\] ,~40!
?."
~ ~..,, r "2o!
.
"L~/ -~" -~"  ~ =lO I ~ , .
-0 ' :1 , ,  , i , , ,  i , ,  , i ,  , .
i , ,  ,1 ,  , ,  i ,  .
, i .
, , i , , ,  i , ,  , I0 20 40 60 80 100120140160180200Words Reviewedo?, - - - -  54&..53&4&5.
.
.
.
.
2&3&4&5Figure 5: Financial Resultster in a semantic lexicon on their own.
For exam-ple, suppose a user wanted to build a dictionary ofVehicle words.
Most people would probably definewords such as car, truck, plane, and automobile.
Butit is doubtful that most people would think of wordslike gunships, fighter, carrier, and ambulances.
Thecorpus-based algorithm is especially good at identi-fying words that are common in the text corpus eventhough they might not be commonly used in general.As another example, specific types of weapons (e.g.,M-16, AR-15, M-60, or M-79) might not even beknown to most users, but they are abundant in theMUC-4 corpus.If we consider all the words rated as 3's, 4's, or5's, then we were able to find about 50-65 wordsfor every category except Energy.
Many of thesewords would be useful in a semantic dictionary forthe category.
For example, some of the words ratedas 3's for the Vehicle category include: flight, flights,aviation, pilot, airport, and highways.Most of the words rated as 2's are not specificto the target category, but some of them might beuseful for certain tasks.
For example, some wordsjudged as 2's for the Energy category are: spill, pole,1211009080770,\[ , r  ??
"?
?
?
\ ]" / .
_~ i o50 ;'" / .
.
.~2o. ""
/~ jO" " '1 ' ' '1 ' ' '1 ' ' '1 ' ' '1 ' ' '1 ' ' '1 ' ' '1 ' ' '1 ' ' '10 20 40 60 80 100120140160180200Words ReviewedFigure 6: Military Results54&53&4&5.
.
.
.
.
2&3&4&5100-90" :  - !so.!
!
-"I 7oi j"E 60.: ?
.
.
, ' " "50.: .. "'" ? '
"  ~?
.
J ~ l40.:.
- / J  I5oi , .
/ , .
-  !0 i /~  i i ?
.
.
.
.
.
.
.
.
.
.
.
I .
.
.
.
.
.
.
.
.
t ' ' ' 1  .
.
.
.
.
I0 20 40 60 80 100120140160180200Words Reviewed54&53&4&5.
.
.
.
.
2&3&4&5Figure 7: Vehicle Resultstower, and fields.
These words may appear in manydifferent contexts, but in texts about Energy topicsthese words are likely to be relevant and probablyshould be defined in the dictionary?
Therefore weexpect that a user would likely keep some of thesewords in the semantic lexicon but would probablybe very selective.Finally, the graphs show that most of the acquisi-tion curves displayed positive slopes even at the endof the 200 words.
This implies that more categorywords would likely have been found if the users hadreviewed more than 200 words.
The one exception,again, was the Energy category, which we will dis-cuss in the next section?
The size of the ranked listsranged from 442 for the financial category to 919 forthe military category, so it would be interesting toknow how many category members would have beenfound if we had given the entire lists to our judges?4 Se lec t ing  Categor ies  and  SeedWordsWhen we first began this work, we were unsureabout what types of categories would be amenable tothis approach.
So we experimented with a number100-.9o.1 = \[8o.1 I7o-i ' .. "'".
6o!
.
.
.
.50 i.
-- -"4o: , " -0 20 40 60 80 100120140160180200Words  Reviewed54&53&4&5.
.
.
.
.
2&3&4&5Figure 8: Weapon Resultsof different categories?
Fortunately, most of themworked fairly well, but some of them did not.
Wedo not claim to understand exactly what types ofcategories will work well and which ones will not,but our early experiences did shed some light on thestrengths and weaknesses of this approach.In addition to the previous five categories, we alsoexperimented with categories for Location, Commer-cial, and Person.
The Location category performedvery well using seed words such as city, town, andprovince.
We didn't formally evaluate this categorybecause most of the category words were propernouns and we did not expect hat our judges wouldknow what they were.
But it is worth noting thatthis category achieved good results, presumably be-cause location names often cluster together in ap-positives, conjunctions, and nominal compounds?For the Commercial category, we chose seed wordssuch as store, shop, and market?
Only a few newcommercial words were identified, such as hotel andrestaurant?
In retrospect, we realized that there wereprobably few words in the MUC-4 corpus that re-ferred to commercial establishments.
(The MUC-4corpus mainly contains reports of terrorist and mil-itary events?)
The relatively poor performance ofthe Energy category was probably due to the sameproblem?
If a category is not well-represented in?
the corpus then it is doomed because inappropriatewords become seed words in the early iterations andquickly derail the feedback loop.The Person category produced mixed results?Some good category words were found, such asrebel, advisers, criminal, and citizen?
But many ofthe words referred to Organizations (e.g., FMLN),groups (e.g., forces), and actions (e.g., attacks).Some of these words seemed reasonable, but it washard to draw a line between specific references topeople and concepts like organizations and groupsthat may or may not consist entirely of people?
The122Energy: oil electric gas fuel power gasoline lec-tricity petroleum energy CELFinancial: monetary fund gold bank investedbanks investment invest dollar currency moneyeconomies loans billion debts millions IMF com-merce wealth inflation million market funds dol-lars debtMil itary: infantry brigade regiment brigadesdivision ranks deserter troops commander cor-poral GN Navy Bracamonte soldier units patrolscavalry detachment officer patrol garrisons armyparatroopers Atonal garrison battalion unit mili-tias lieutenantVehicle: C-47 A-37 tank pickup Cessna air-craft Boeing_727 airplane plane truck airplanesgunships fighter carrier tanks planes La_Aurorahelicopters helicopter automobile j ep car boatstrucks motorcycles ambulances train buses hipscars bus ship vehicle vehiclesWeapon: AK-47 M-16 carbines AR-15 TNT ri-fles 9-mm grenades machineguns dynamite re-volvers rifle submachineguns M-60 pistols pistolM-79 grenade mortars gun mortar submachine-gun cannon RPG-7 firearms guns bomb ma-chinegun weapons car_bombs car_bomb artillerytanks armsFigure 9: Words judged as 5's for each categorylarge proportion of action words also diluted thelist.
More experiments are needed to better under-stand whether this category is inherently difficult orwhether a more carefully chosen set of seed wordswould improve performance.More experiments are also needed to evaluate dif-ferent seed word lists.
The algorithm is clearly sen-sitive to the initial seed words, but the degree of sen-sitivity is unknown.
For the five categories reportedin this paper, we arbitrarily chose a few words thatwere central members of the category.
Our initialseed words worked well enough that we did not ex-periment with them very much.
But we did performa few experiments varying the number of seed words.In general, we found that additional seed words tendto improve performance, but the results were notsubstantially different using five seed words or usingten.
Of course, there is also a law of diminishing re-turns: using a seed word list containing 60 categorywords is almost like creating a semantic lexicon forthe category by hand!5 Conclus ionsBuilding semantic lexicons will always be a subjec-tive process, and the quality of a semantic lexiconis highly dependent on the task for which it willbe used.
But there is no question that semanticknowledge is essential for many problems in natu-ral language processing.
Most of the time semanticknowledge is defined manually for the target applica-tion, but several techniques have been developed forgenerating semantic knowledge automatically.
Somesystems learn the meanings of unknown words us-ing expectations derived from other word definitionsin the surrounding context (e.g., (Granger, 1977;Carbonell, 1979; Jacobs and Zernik, 1988; Hast-ings and Lytinen, 1994)).
Other approaches useexample or case-based methods to match unknownword contexts against previously seen word contexts(e.g., (Berwick, 1989; Cardie, 1993)).
Our task ori-entation is a bit different because we are trying toconstruct a semantic lexicon for a target category,instead of classifying unknown or polysemous wordsin context.To our knowledge, our system is the first oneaimed at building semantic lexicons from raw textwithout using any additional semantic knowledge.The only lexical knowledge used by our parser isa part-of-speech dictionary for syntactic processing.Although we used a hand-crafted part-of-speech dic-tionary for these xperiments, statistical nd corpus-based taggers are readily available (e.g., (Brill, 1994;Church, 1989; Weischedel et al, 1993)).Our corpus-based approach is designed to sup-port fast semantic lexicon construction.
A user onlyneeds to supply a representative text corpus and asmall set of seed words for each target category.
Ourexperiments suggest that a core semantic lexicon canbe built for each category with only 10-15 minutesof human interaction.
While more work needs to bedone to refine this procedure and characterize thetypes of categories it can handle, we believe that thisis a promising approach for corpus-based semanticknowledge acquisition.6 AcknowledgmentsThis research was funded by NSF grant IRI-9509820and the University of Utah Research Committee.We would like to thank David Bean, Jeff Lorenzen,and Kiri Wagstaff for their help in judging our cat-egory lists.123ReferencesBerwick, Robert C. 1989.
Learning Word Mean-ings from Examples.
In Semantic Structures: Ad-vances in Natural Language Processing.
LawrenceErlbaum Associates, chapter 3, pages 89-124.Brill, E. 1994.
Some Advances in Rule-based Part ofSpeech Tagging.
In Proceedings of the Twelfth Na-tional Conference on Artificial Intelligence, pages722-727.
AAAI Press/The MIT Press.Carbonell, J. G. 1979.
Towards a Self-ExtendingParser.
In Proceedings of the 17th Annual Meetingof the Association for Computational Linguistics,pages 3-7.Cardie, C. 1993.
A Case-Based Approach toKnowledge Acquisition for Domain-Specific Sen-tence Analysis.
In Proceedings of the Eleventh Na-tional Conference on Artificial Intelligence, pages798-803.
AAAI Press/The MIT Press.Church, K. 1989.
A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text.
In Pro-ceedings off the Second Conference on Applied Nat-ural Language Processing.Granger, R. H. 1977.
FOUL-UP: A Program thatFigures Out Meanings of Words from Context.
InProceedings of the Fifth International Joint Con-ference on Artificial Intelligence, pages 172-178.Hastings, P. and S. Lytinen.
1994.
The Ups andDowns of Lexical Acquisition.
In Proceedings ofthe Twelfth National Conference on Artificial In-telligence, pages 754-759.
AAAI Press/The MITPress.Jacobs, P. and U. Zernik.
1988.
Acquiring LexicalKnowledge from Text: A Case Study.
In Pro-ceedings of the Seventh National Conference onArtificial Intelligence, pages 739-744.Lehnert, W., C. Cardie, D. Fisher, J. McCarthy,E.
Riloff, and S. Soderland.
1992.
Univer-sity of Massachusetts: Description of the CIR-CUS System as Used for MUC-4.
In Proceedingsof the Fourth Message Understanding Conference(MUC-~), pages 282-288, San Mateo, CA.
Mor-gan Kaufmann.Lenat, D. B., M. Prakash, and M. Shepherd.
1986.CYC: Using Common Sense Knowledge to Over-come Brittleness and Knowledge-Acquisition Bot-tlenecks.
AI Magazine, 6:65-85.Miller, G. 1990.
Wordnet: An On-line LexicalDatabase.
International Journal of Lexicography,3(4).MUC-4 Proceedings.
1992.
Proceedings ofthe Fourth Message Understanding Conference(MUC-4).
Morgan Kaufmann, San Mateo, CA.Weischedel,R., M. Meteer, R. Schwartz, L. Ramshaw, andJ.
Palmucci.
1993.
Coping with Ambiguity andUnknown Words through Probabilistic Models.Computational Linguistics, 19(2):359-382.Yarowsky, D. 1992.
Word sense disambiguation us-ing statistical models of Roget's categories trainedon large corpora.
In Proceedings of the FourteenthInternational Conference on Computational Lin-guistics (COLING-92), pages 454-460.124
