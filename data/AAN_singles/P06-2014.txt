Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 105?112,Sydney, July 2006. c?2006 Association for Computational LinguisticsSoft Syntactic Constraints for Word Alignmentthrough Discriminative TrainingColin CherryDepartment of Computing ScienceUniversity of AlbertaEdmonton, AB, Canada, T6G 2E8colinc@cs.ualberta.caDekang LinGoogle Inc.1600 Amphitheatre ParkwayMountain View, CA, USA, 94043lindek@google.comAbstractWord alignment methods can gain valu-able guidance by ensuring that their align-ments maintain cohesion with respect tothe phrases specified by a monolingual de-pendency tree.
However, this hard con-straint can also rule out correct alignments,and its utility decreases as alignment mod-els become more complex.
We use a pub-licly available structured output SVM tocreate a max-margin syntactic aligner witha soft cohesion constraint.
The resultingaligner is the first, to our knowledge, to usea discriminative learning method to trainan ITG bitext parser.1 IntroductionGiven a parallel sentence pair, or bitext, bilin-gual word alignment finds word-to-word connec-tions across languages.
Originally introduced as abyproduct of training statistical translation modelsin (Brown et al, 1993), word alignment has be-come the first step in training most statistical trans-lation systems, and alignments are useful to a hostof other tasks.
The dominant IBM alignment mod-els (Och and Ney, 2003) use minimal linguistic in-tuitions: sentences are treated as flat strings.
Thesecarefully designed generative models are difficultto extend, and have resisted the incorporation ofintuitively useful features, such as morphology.There have been many attempts to incorporatesyntax into alignment; we will not present a com-plete list here.
Some methods parse two flat stringsat once using a bitext grammar (Wu, 1997).
Othersparse one of the two strings before alignment be-gins, and align the resulting tree to the remainingstring (Yamada and Knight, 2001).
The statisti-cal models associated with syntactic aligners tendto be very different from their IBM counterparts.They model operations that are meaningful at asyntax level, like re-ordering children, but ignorefeatures that have proven useful in IBM models,such as the preference to align words with simi-lar positions, and the HMM preference for links toappear near one another (Vogel et al, 1996).Recently, discriminative learning technologyfor structured output spaces has enabled severaldiscriminative word alignment solutions (Liu etal., 2005; Moore, 2005; Taskar et al, 2005).
Dis-criminative learning allows easy incorporation ofany feature one might have access to during thealignment search.
Because the features are han-dled so easily, discriminative methods use featuresthat are not tied directly to the search: the searchand the model become decoupled.In this work, we view synchronous parsing onlyas a vehicle to expose syntactic features to a dis-criminative model.
This allows us to include theconstraints that would usually be imposed by atree-to-string alignment method as a feature in ourmodel, creating a powerful soft constraint.
Weadd our syntactic features to an already strongflat-string discriminative solution, and we showthat they provide new information resulting in im-proved alignments.2 Constrained AlignmentLet an alignment be the complete structure thatconnects two parallel sentences, and a link beone of the word-to-word connections that makeup an alignment.
All word alignment methodsbenefit from some set of constraints.
These limitthe alignment search space and encourage com-petition between potential links.
The IBM mod-els (Brown et al, 1993) benefit from a one-to-many constraint, where each target word has ex-105the tax causes unrestl' imp?t cause le malaiseFigure 1: A cohesion constraint violation.actly one generator in the source.
Methods likecompetitive linking (Melamed, 2000) and maxi-mum matching (Taskar et al, 2005) use a one-to-one constraint, where words in either sentence canparticipate in at most one link.
Throughout this pa-per we assume a one-to-one constraint in additionto any syntax constraints.2.1 Cohesion ConstraintSuppose we are given a parse tree for one of thetwo sentences in our sentence pair.
We will re-fer to the parsed language as English, and theunparsed language as Foreign.
Given this infor-mation, a reasonable expectation is that Englishphrases will move together when projected ontoForeign.
When this occurs, the alignment is saidto maintain phrasal cohesion.Fox (2002) measured phrasal cohesion in goldstandard alignments by counting crossings.
Cross-ings occur when the projections of two disjointphrases overlap.
For example, Figure 1 shows ahead-modifier crossing: the projection of the thetax subtree, impo?t .
.
.
le, is interrupted by the pro-jection of its head, cause.
Alignments with nocrossings maintain phrasal cohesion.
Fox?s exper-iments show that cohesion is generally maintainedfor French-English, and that dependency trees pro-duce the highest degree of cohesion among thetested structures.Cherry and Lin (2003) use the phrasal cohesionof a dependency tree as a constraint on a beamsearch aligner.
This constraint produces a sig-nificant reduction in alignment error rate.
How-ever, as Fox (2002) showed, even in a languagepair as close as French-English, there are situa-tions where phrasal cohesion should not be main-tained.
These include incorrect parses, systematicviolations such as not ?
ne .
.
.
pas, paraphrases,and linguistic exceptions.We aim to create an alignment system thatobeys cohesion constraints most of the time, butcan violate them when necessary.
Unfortunately,Cherry and Lin?s beam search solution does notlend itself to a soft cohesion constraint.
The im-perfect beam search may not be able to find theoptimal alignment under a soft constraint.
Further-more, it is not clear what penalty to assign to cross-ings, or how to learn such a penalty from an iter-ative training process.
The remainder of this pa-per will develop a complete alignment search thatis aware of cohesion violations, and use discrimi-native learning technology to assign a meaningfulpenalty to those violations.3 Syntax-aware Alignment SearchWe require an alignment search that can find theglobally best alignment under its current objectivefunction, and can account for phrasal cohesion inthis objective.
IBM Models 1 and 2, HMM (Vo-gel et al, 1996), and weighted maximum matchingalignment all conduct complete searches, but theywould not be amenable to monitoring the syntac-tic interactions of links.
The tree-to-string modelsof (Yamada and Knight, 2001) naturally considersyntax, but special modeling considerations areneeded to allow any deviations from the providedtree (Gildea, 2003).
The Inversion TransductionGrammar or ITG formalism, described in (Wu,1997), is well suited for our purposes.
ITGs per-form string-to-string alignment, but do so througha parsing algorithm that will allow us to inform theobjective function of our dependency tree.3.1 Inversion Transduction GrammarAn ITG aligns bitext through synchronous pars-ing.
Both sentences are decomposed into con-stituent phrases simultaneously, producing a wordalignment as a byproduct.
Viewed generatively, anITG writes to two streams at once.
Terminal pro-ductions produce a token in each stream, or a tokenin one stream with the null symbol ?
in the other.We will use standard ITG notation: A ?
e/f in-dicates that the token e is produced on the Englishstream, while f is produced on the Foreign stream.To allow for some degree of movement duringtranslation, non-terminal productions are allowedto be either straight or inverted.
Straight pro-ductions, with their non-terminals inside squarebrackets [.
.
.
], produce their symbols in the sameorder on both streams.
Inverted productions, in-dicated by angled brackets ?.
.
.
?, have their non-terminals produced in the given order on the En-glish stream, but this order is reversed in the For-eign stream.106the Canadian agriculture industryl' industrie agricole CanadienneFigure 2: An example of an ITG alignment.
Ahorizontal bar across an arc indicates an inversion.An ITG chart parser provides a polynomial-time algorithm to conduct a complete enumerationof all alignments that are possible according to itsgrammar.
We will use a binary bracketing ITG, thesimplest interesting grammar in this formalism:A?
[AA] | ?AA?
| e/fThis grammar enforces its own weak cohesionconstraint: for every possible alignment, a corre-sponding binary constituency tree must exist forwhich the alignment maintains phrasal cohesion.Figure 2 shows a word alignment and the corre-sponding tree found by an ITG parser.
Wu (1997)provides anecdotal evidence that only incorrectalignments are eliminated by ITG constraints.
Inour French-English data set, an ITG rules outonly 0.3% of necessary links beyond those alreadyeliminated by the one-to-one constraint (Cherryand Lin, 2006).3.2 Dependency-augmented ITGAn ITG will search all alignments that conformto a possible binary constituency tree.
We wishto confine that search to a specific n-array depen-dency tree.
Fortunately, Wu (1997) provides amethod to have an ITG respect a known partialstructure.
One can seed the ITG parse chart so thatspans that do not agree with the provided structureare assigned a value of??
before parsing begins.The result is that no constituent is ever constructedwith any of these invalid spans.In the case of phrasal cohesion, the invalid spanscorrespond to spans of the English sentence thatinterrupt the phrases established by the provideddependency tree.
To put this notion formally, wefirst define some terms: given a subtree T[i,k],where i is the left index of the leftmost leaf in T[i,k]and k is the right index of its rightmost leaf, we sayany index j ?
(i, k) is internal to T[i,k].
Similarly,any index x /?
[i, k] is external to T[i,k].
An in-valid span is any span for which our provided treeT[i,k]x1 i j k x2j'TFigure 3: Illustration of invalid spans.
[j?, j] and[j, k] are legal, while [x1, j] and [j, x2] are not.the tax causes unrestFigure 4: The invalid spans induced by a depen-dency tree.has a subtree T[i,k] such that one endpoint of thespan is internal to T[i,k] while the other is externalto it.
Figure 3 illustrates this definition, while Fig-ure 4 shows the invalid spans induced by a simpledependency tree.With these invalid spans in place, the ITG canno longer merge part of a dependency subtree withanything other than another part of the same sub-tree.
Since all ITG movement can be explainedby inversions, this constrained ITG cannot in-terrupt one dependency phrase with part of an-other.
Therefore, the phrasal cohesion of the in-put dependency tree is maintained.
Note that thiswill not search the exact same alignment spaceas a cohesion-constrained beam search; instead ituses the union of the cohesion constraint and theweaker ITG constraints (Cherry and Lin, 2006).Transforming this form of the cohesion con-straint into a soft constraint is straight-forward.Instead of overriding the parser so it cannot useinvalid English spans, we will note the invalidspans and assign the parser a penalty should ituse them.
The value of this penalty will be de-termined through discriminative training, as de-scribed in Section 4.
Since the penalty is avail-able within the dynamic programming algorithm,the parser will be able to incorporate it to find aglobally optimal alignment.4 Discriminative TrainingTo discriminatively train our alignment systems,we adopt the Support Vector Machine (SVM) for107Structured Output (Tsochantaridis et al, 2004).We have selected this system for its high degree ofmodularity, and because it has an API freely avail-able1.
We will summarize the learning mechanismbriefly in this section, but readers should refer to(Tsochantaridis et al, 2004) for more details.SVM learning is most easily expressed as a con-strained numerical optimization problem.
All con-straints mentioned in this section are constraintson this optimizer, and have nothing to do with thecohesion constraint from Section 2.4.1 SVM for Structured OutputTraditional SVMs attempt to find a linear sepa-rator that creates the largest possible margin be-tween two classes of vectors.
Structured outputSVMs attempt to separate the correct structurefrom all incorrect structures by the largest possiblemargin, for all training instances.
This may soundlike a much more difficult problem, but with a fewassumptions in place, the task begins to look verysimilar to a traditional SVM.As in most discriminative training methods, webegin by assuming that a candidate structure y,built for an input instance x, can be adequately de-scribed using a feature vector ?
(x, y).
We also as-sume that our ?
(x, y) decomposes in such a waythat the features can guide a search to recover thestructure y from x.
That is:struct(x; ~w) = argmaxy?Y ?~w,?
(x, y)?
(1)is computable, where Y is the set of all possiblestructures, and ~w is a vector that assigns weightsto each component of ?
(x, y).
~w is the parametervector we will learn using our SVM.Now the learning task begins to look straight-forward: we are working with vectors, and thetask of building a structure y has been recast asan argmax operator.
Our learning goal is to find a~w so that the correct structure is found:?i, ?y ?
Y \ yi : ?~w,?i(yi)?
> ?~w,?i(y)?
(2)where xi is the ith training example, yi is itscorrect structure, and ?i(y) is short-hand for?
(xi, y).
As several ~w will fulfill (2) in a linearlyseparable training set, the unique max-margin ob-jective is defined to be the ~w that maximizes theminimum distance between yi and the incorrectstructures in Y .1At http://svmlight.joachims.org/svm struct.htmlThis learning framework also incorporates a no-tion of structured loss.
In a standard vector clas-sification problem, there is 0-1 loss: a vector iseither classified correctly or it is not.
In the struc-tured case, some incorrect structures can be bet-ter than others.
For example, having the argmaxselect an alignment missing only one link is bet-ter than selecting one with no correct links and adozen wrong ones.
A loss function ?
(yi, y) quan-tifies just how incorrect a particular structure y is.Though Tsochantaridis et al (2004) provide sev-eral ways to incorporate loss into the SVM ob-jective, we will use margin re-scaling, as it corre-sponds to loss usage in another max-margin align-ment approach (Taskar et al, 2005).
In marginre-scaling, high loss structures must be separatedfrom the correct structure by a larger margin thanlow loss structures.To allow some misclassifications during train-ing, a soft-margin requirement replaces our max-margin objective.
A slack variable ?i is introducedfor each training example xi, to allow the learnerto violate the margin at a penalty.
The magnitudeof this penalty to determined by a hand-tuned pa-rameter C. After a few transformations (Tsochan-taridis et al, 2004), the soft-margin learning ob-jective can be formulated as a quadratic program:min~w,?12||~w||2 +Cnn?i=1?i, s.t.
?i?i ?
0 (3)?i, ?y ?
Y \ yi : (4)?~w,?i(yi)??i(y)?
?
?
(yi, y)?
?iNote how the slack variables ?i allow some in-correct structures to be built.
Also note that theloss ?
(yi, y) determines the size of the margin be-tween structures.Unfortunately, (4) provides one constraint forevery possible structure for every training exam-ple.
Enumerating these constraints explicitly is in-feasible, but in reality, only a subset of these con-straints are necessary to achieve the same objec-tive.
Re-organizing (4) produces:?i,?y ?
Y \ yi :?i ?
?
(yi, y)?
?~w,?i(yi)??i(y)?
(5)which is equivalent to:?i : ?i ?
maxy?Y\yicosti(y; ~w) (6)where costi is defined as:costi(y; ~w) = ?
(yi, y)?
?~w,?i(yi)?
?i(y)?108Provided that the max cost structure can be foundin polynomial time, we have all the componentsneeded for a constraint generation approach to thisoptimization problem.Constraint generation places an outer looparound an optimizer that minimizes (3) repeatedlyfor a growing set of constraints.
It begins by min-imizing (3) with an empty constraint set in placeof (4).
This provides values for ~w and ~?.
The maxcost structurey?
= argmaxy?Y\yicosti(y; ~w)is found for i = 1 with the current ~w.
If the re-sulting costi(y?
; ~w) is greater than the current valueof ?i, then this represents a violated constraint2 inour complete objective, and a new constraint ofthe form ?i ?
costi(y?
; ~w) is added to the con-straint set.
The algorithm then iterates: the opti-mizer minimizes (3) again with the new constraintset, and solves the max cost problem for i = i+ 1with the new ~w, growing the constraint set if nec-essary.
Note that the constraints on ?
change with~w, as cost is a function of ~w.
Once the end ofthe training set is reached, the learner loops backto the beginning.
Learning ends when the entiretraining set can be processed without needing toadd any constraints.
It can be shown that thiswill occur within a polynomial number of itera-tions (Tsochantaridis et al, 2004).With this framework in place, one need only fillin the details to create an SVM for a new struc-tured output space:1.
A ?
(x, y) function to transform instance-structure pairs into feature vectors2.
A search to find the best structure given aweight vector: argmaxy ?~w,?
(x, y)?.
Thishas no role in training, but it is necessary touse the learned weights.3.
A structured loss function ?
(y, y?)4.
A search to find the max cost structure:argmaxycosti(y;w)4.2 SVMs for AlignmentUsing the Structured SVM API, we have createdtwo SVM word aligners: a baseline that usesweighted maximum matching for its argmax op-erator, and a dependency-augmented ITG that will2Generally the test to see if ?i > costi(y?
; ~w) is approxi-mated as ?i > costi(y?
; ~w) + ?
for a small constant ?.satisfy our requirements for an aligner with a softcohesion constraint.
Our x becomes a bilingualsentence-pair, while our y becomes an alignment,represented by a set of links.4.2.1 Weighed Maximum MatchingGiven a bipartite graph with edge values, theweighted maximum matching algorithm (West,2001) will find the matching with maximumsummed edge values.
To create a matching align-ment solution, we reproduce the approach of(Taskar et al, 2005) within the framework de-scribed in Section 4.1:1.
We define a feature vector ?
for each poten-tial link l in x, and ?
in terms of y?s compo-nent links: ?
(x, y) =?l?y ?(l).2.
Our structure search is the matching algo-rithm.
The input bipartite graph has an edgefor each l. Each edge is given the valuev(l)?
?~w, ?(l)?.3.
We adopt the weighted Hamming loss in de-scribed (Taskar et al, 2005):?
(y, y?)
= co|y ?
y?|+ cc|y?
?
y|where co is an omission penalty and cc is acommission penalty.4.
Our max cost search corresponds to theirloss-augmented matching problem.
The in-put graph is modified to prefer costly links:?l /?
y : v(l)?
?~w, ?
(l)?+ cc?l ?
y : v(l)?
?~w, ?(l)?
?
coNote that our max cost search could not have beenimplemented as loss-augmented matching had weselected one of the other loss objectives presentedin (Tsochantaridis et al, 2004) in place of marginrescaling.We use the same feature representation ?
(l) as(Taskar et al, 2005), with some small exceptions.Let l = (Ej , Fk) be a potential link between thejth word of English sentence E and the kth wordof Foreign sentence F .
To measure correlation be-tween Ej and Fk we use conditional link proba-bility (Cherry and Lin, 2003) in place of the Dicecoefficient:cor(Ej , Fk) =#links(Ej , Fk)?
d#cooccurrences(Ej , Fk)where the link counts are determined by word-aligning 50K sentence pairs with another match-ing SVM that uses the ?2 measure (Gale and109Church, 1991) in place of Dice.
The ?2 measurerequires only co-occurrence counts.
d is an abso-lute discount parameter as in (Moore, 2005).
Also,we omit the IBM Model 4 Prediction features, aswe wish to know how well we can do without re-sorting to traditional word alignment techniques.Otherwise, the features remain the same,including distance features that measureabs(j|E| ?k|F |); orthographic features; wordfrequencies; common-word features; a bias termset alays to 1; and an HMM approximationcor(Ej+1, Fk+1).4.2.2 Soft Dependency-augmented ITGBecause of the modularity of the structured out-put SVM, our SVM ITG re-uses a large amountinfrastructure from the matching solution.
Weessentially plug an ITG parser in the place ofthe matching algorithm, and add features to takeadvantage of information made available by theparser.
x remains a sentence pair, and y becomesan ITG parse tree that decomposes x and speci-fies an alignment.
Our required components are asfollows:1.
We define a feature vector ?T on instancesof production rules, r. ?
is a function ofthe decomposition specified by y: ?
(x, y) =?r?y ?T (r).2.
The structure search is a weighted ITG parserthat maximizes summed production scores.Each instance of a production rule r is as-signed a score of ?~w, ?T (r)?3.
Loss is unchanged, defined in terms of thealignment induced by y.4.
A loss-augmented ITG is used to find the maxcost.
Productions of the form A ?
e/fthat correspond to links have their scores aug-mented as in the matching system.The ?T vector has two new features in addition tothose present in the matching system?s ?.
Thesefeatures can be active only for non-terminal pro-ductions, which have the formA?
[AA] | ?AA?.One feature indicates an inverted production A?
?AA?, while the other indicates the use of an in-valid span according to a provided English depen-dency tree, as described in Section 3.2.
Theseare the only features that can be active for non-terminal productions.A terminal production rl that corresponds to alink l is given that link?s features from the match-ing system: ?T (rl) = ?(l).
Terminal productionsr?
corresponding to unaligned tokens are givenblank feature vectors: ?T (r?)
= ~0.The SVM requires complete ?
vectors for thecorrect training structures.
Unfortunately, ourtraining set contains gold standard alignments, notITG parse trees.
The gold standard is divided intosure and possible link sets S and P (Och and Ney,2003).
Links in S must be included in a correctalignment, while P links are optional.
We createITG trees from the gold standard using the follow-ing sorted priorities during tree construction:?
maximize the number of links from S?
minimize the number of English dependencyspan violations?
maximize the number of links from P?
minimize the number of inversionsThis creates trees that represent high scoring align-ments, using a minimal number of invalid spans.Only the span and inversion counts of these treeswill be used in training, so we need not achieve aperfect tree structure.
We still evaluate all methodswith the original alignment gold standard.5 Experiments and ResultsWe conduct two experiments.
The first teststhe dependency-augmented ITG described in Sec-tion 3.2 as an aligner with hard cohesion con-straints.
The second tests our discriminative ITGwith soft cohesion constraints against two strongbaselines.5.1 Experimental setupWe conduct our experiments using French-EnglishHansard data.
Our ?2 scores, link probabilitiesand word frequency counts are determined using asentence-aligned bitext consisting of 50K sentencepairs.
Our training set for the discriminative align-ers is the first 100 sentence pairs from the French-English gold standard provided for the 2003 WPTworkshop (Mihalcea and Pedersen, 2003).
Forevaluation we compare to the remaining 347 goldstandard pairs using the alignment evaluation met-rics: precision, recall and alignment error rate orAER (Och and Ney, 2003).
SVM learning param-eters are tuned using the 37-pair development setprovided with this data.
English dependency treesare provided by Minipar (Lin, 1994).110Table 1: The effect of hard cohesion constraints ona simple unsupervised link score.Search Prec Rec AERMatching 0.723 0.845 0.231ITG 0.764 0.860 0.200D-ITG 0.830 0.873 0.1535.2 Hard Constraint PerformanceThe goal of this experiment is to empirically con-firm that the English spans marked invalid bySection 3.2?s dependency-augmented ITG provideuseful guidance to an aligner.
To do so, wecompare an ITG with hard cohesion constraints,an unconstrained ITG, and a weighted maximummatching aligner.
All aligners use the same sim-ple objective function.
They maximize summedlink values v(l), where v(l) is defined as followsfor an l = (Ej , Fk):v(l) = ?2(Ej , Fk)?
10?5abs(j|E|?k|F |)All three aligners link based on ?2 correlationscores, breaking ties in favor of closer pairs.
Thisallows us to evaluate the hard constraints outsidethe context of supervised learning.Table 1 shows the results of this experiment.We can see that switching the search methodfrom weighted maximum matching to a cohesion-constrained ITG (D-ITG) has produced a 34% rel-ative reduction in alignment error rate.
The bulkof this improvement results from a substantial in-crease in precision, though recall has also gone up.This indicates that these cohesion constraints are astrong alignment feature.
The ITG row shows thatthe weaker ITG constraints are also valuable, butthe cohesion constraint still improves on them.5.3 Soft Constraint PerformanceWe now test the performance of our SVM ITGwith soft cohesion constraint, or SD-ITG, whichis described in Section 4.2.2.
We will test againsttwo strong baselines.
The first baseline, matchingis the matching SVM described in Section 4.2.1,which is a re-implementation of the state-of-the-art work in (Taskar et al, 2005)3.
The secondbaseline, D-ITG is an ITG aligner with hard co-hesion constraints, but which uses the weights3Though it is arguably lacking one of its strongest fea-tures: the output of GIZA++ (Och and Ney, 2003)Table 2: The performance of SVM-trained align-ers with various degrees of cohesion constraint.Method Prec Rec AERMatching 0.916 0.860 0.110D-ITG 0.940 0.854 0.100SD-ITG 0.944 0.878 0.086trained by the matching SVM to assign link val-ues.
This is the most straight-forward way to com-bine discriminative training with the hard syntacticconstraints.The results are shown in Table 2.
The first thingto note is that our Matching baseline is achievingscores in line with (Taskar et al, 2005), which re-ports an AER of 0.107 using similar features andthe same training and test sets.The effect of the hard cohesion constraint hasbeen greatly diminished after discriminative train-ing.
Matching and D-ITG correspond to the theentries of the same name in Table 1, only with amuch stronger, learned value function v(l).
How-ever, in place of a 34% relative error reduction, thehard constraints in the D-ITG produce only a 9%reduction from 0.110 to 0.100.
Also note that thistime the hard constraints result in a reduction inrecall.
This indicates that the hard cohesion con-straint is providing little guidance not provided byother features, and that it is actually eliminatingmore sure links than it is helping to find.The soft-constrained SD-ITG, which has accessto the D-ITG?s invalid spans as a feature duringSVM training, is fairing substantially better.
ItsAER of 0.086 represents a 22% relative error re-duction compared to the matching system.
Theimproved error rate is caused by gains in both pre-cision and recall.
This indicates that the invalidspan feature is doing more than just ruling outlinks; perhaps it is de-emphasizing another, lessaccurate feature?s role.
The SD-ITG overrides thecohesion constraint in only 41 of the 347 test sen-tences, so we can see that it is indeed a soft con-straint: it is obeyed nearly all the time, but it can bebroken when necessary.
The SD-ITG achieves byfar the strongest ITG alignment result reported onthis French-English set; surpassing the 0.16 AERreported in (Zhang and Gildea, 2004).Training times for this system are quite low; un-supervised statistics can be collected quickly overa large set, while only the 100-sentence training111set needs to be iteratively aligned.
Our match-ing SVM trains in minutes on a single-processormachine, while the SD-ITG trains in roughly onehour.
The ITG is the bottleneck, so training timecould be improved by optimizing the parser.6 Related WorkSeveral other aligners have used discriminativetraining.
Our work borrows heavily from (Taskaret al, 2005), which uses a max-margin approachwith a weighted maximum matching aligner.
(Moore, 2005) uses an averaged perceptron fortraining with a customized beam search.
(Liu etal., 2005) uses a log-linear model with a greedysearch.
To our knowledge, ours is the first align-ment approach to use this highly modular struc-tured SVM, and the first discriminative method touse an ITG for the base aligner.
(Gildea, 2003) presents another aligner with asoft syntactic constraint.
This work adds a cloningoperation to the tree-to-string generative model in(Yamada and Knight, 2001).
This allows subtreesto move during translation.
As the model is gen-erative, it is much more difficult to incorporate awide variety of features as we do here.
In (Zhangand Gildea, 2004), this model was tested on thesame annotated French-English sentence pairs thatwe divided into training and test sets for our exper-iments; it achieved an AER of 0.15.7 ConclusionWe have presented a discriminative, syntacticword alignment method.
Discriminative trainingis conducted using a highly modular SVM forstructured output, which allows code reuse be-tween the syntactic aligner and a maximum match-ing baseline.
An ITG parser is used for the align-ment search, exposing two syntactic features: theuse of inverted productions, and the use of spansthat would not be available in a tree-to-string sys-tem.
This second feature creates a soft phrasal co-hesion constraint.
Discriminative training allowsus to maintain all of the features that are useful tothe maximum matching baseline in addition to thenew syntactic features.
We have shown that thesefeatures produce a 22% relative reduction in errorrate with respect to a strong flat-string model.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2):263?312.C.
Cherry and D. Lin.
2003.
A probability model to improveword alignment.
In Meeting of the Association for Com-putational Linguistics, pages 88?95, Sapporo, Japan, July.C.
Cherry and D. Lin.
2006.
A comparison of syntacti-cally motivated word alignment spaces.
In Proceedingsof EACL, pages 145?152, Trento, Italy, April.H.
J.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proceedings of EMNLP, pages 304?311.W.
A. Gale and K. W. Church.
1991.
Identifying word cor-respondences in parallel texts.
In 4th Speech and NaturalLanguage Workshop, pages 152?157.
DARPA.D.
Gildea.
2003.
Loosely tree-based alignment for machinetranslation.
In Meeting of the Association for Computa-tional Linguistics, pages 80?87, Sapporo, Japan.D.
Lin.
1994.
Principar - an efficient, broad-coverage,principle-based parser.
In Proceedings of COLING, pages42?48, Kyoto, Japan.Y.
Liu, Q. Liu, and S. Lin.
2005.
Log-linear models for wordalignment.
In Meeting of the Association for Computa-tional Linguistics, pages 459?466, Ann Arbor, USA.I.
D. Melamed.
2000.
Models of translational equivalenceamong words.
Computational Linguistics, 26(2):221?249.R.
Mihalcea and T. Pedersen.
2003.
An evaluation exer-cise for word alignment.
In HLT-NAACL Workshop onBuilding and Using Parallel Texts, pages 1?10, Edmon-ton, Canada.R.
Moore.
2005.
A discriminative framework for bilingualword alignment.
In Proceedings of HLT-EMNLP, pages81?88, Vancouver, Canada, October.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
Computational Lin-guistics, 29(1):19?52, March.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005.
A discrimi-native matching approach to word alignment.
In Proceed-ings of HLT-EMNLP, pages 73?80, Vancouver, Canada.I.
Tsochantaridis, T. Hofman, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interdependentand structured output spaces.
In Proceedings of ICML,pages 823?830.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Proceedingsof COLING, pages 836?841, Copenhagen, Denmark.D.
West.
2001.
Introduction to Graph Theory.
Prentice Hall,2nd edition.D.
Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
ComputationalLinguistics, 23(3):377?403.K.
Yamada and K. Knight.
2001.
A syntax-based statisti-cal translation model.
In Meeting of the Association forComputational Linguistics, pages 523?530.H.
Zhang and D. Gildea.
2004.
Syntax-based alignment:Supervised or unsupervised?
In Proceedings of COLING,Geneva, Switzerland, August.112
