First Joint Conference on Lexical and Computational Semantics (*SEM), pages 430?434,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsZhijun Wu: Chinese Semantic Dependency Parsing with Third-OrderFeaturesZhijun Wu Xuan Wang Xinxin LiComputer Application Research CenterSchool of Computer Science and TechnologyHarbin Institute of Technology Shenzhen Graduate SchoolShenzhen 518055, P.R.Chinamattwu305@gmail.com wangxuan@insun.hit.edu.cn  lixin2@gmail.comAbstractThis paper presents our system participated onSemEval-2012 task: Chinese Semantic De-pendency Parsing.
Our system extends thesecond-order MST model by adding twothird-order features.
The two third-order fea-tures are grand-sibling and tri-sibling.
In thedecoding phase, we keep the k best results foreach span.
After using the selected third-orderfeatures, our system presently achieves LASof 61.58% ignoring punctuation tokens whichis 0.15% higher than the result of purelysecond-order model on the test dataset.1 IntroductionRecently, semantic role labeling (SRL) has been ahot research topic.
CoNLL shared tasks for jointparsing for syntactic and semantic dependenciesboth in the year 2008 and 2009, cf.
(Surdeanu et al,2008; Haji?
et al, 2009; Bohnet, 2009).
Sameshared tasks in SemEval-2007 (Sameer S., 2007).The SRL is traditionally implemented as two sub-tasks, argument identification and classification.However, there are some problems for the seman-tic representation method used by the semantic rolelabeling.
For example, the SRL only considers thepredicate-argument relations and ignores the rela-tions between a noun and its modifier, the meaningof semantic roles is related with special predicates.In order to overcome those problems, semanticdependency parsing (SDP) is introduced.
Semanticdependencies express semantic links between pre-dicates and arguments and represent relations be-tween entities and events in text.
The SDP is a kindof dependency parsing, and its task is to build adependency structure for an input sentence and tolabel the semantic relation between a word and itshead.
However, semantic relations are differentfrom syntactic relations, such as position indepen-dent.
Table 1 shows the position independent ofsemantic relations for the sentence XiaoMing hitXiaoBai with a book today.Today, XiaoMing hit XiaoBai with a book.XiaoBai was hit by XiaoMing today with a book.With a book, XiaoMing hit XiaoBai today.XiaoMing hit XiaoBai with a book today.Table 1: An example position dependencyAlthough semantic relations are different fromsyntactic relations, yet they are identical in the de-pendency tree.
That means the methods used insyntactic dependency parsing can also be appliedin SDP.Two main approaches to syntactic dependencyparing are Maximum Spanning Tree (MST) baseddependency parsing and Transition based depen-dency parsing (Eisner, 1996; Nivre et al, 2004;McDonald and Pereira, 2006).
The main idea of430MSTParser is to take dependency parsing as aproblem of searching a maximum spanning tree(MST) in a directed graph (Dependency Tree).
Wesee MSTParser a better chance to improve theparsing speed and MSTParser provides the state-of-the-art performance for both projective and non-projective tree banks.
For the reasons above, wechoose MSTParser as our SemEval-2012 sharedtask participating system basic framework.2 System ArchitectureOur parser is based on the projective MSTParserusing all the features described by (McDonald etal., 2006) as well as some third-order features de-scribed in the following sections.
Semantic depen-dency paring is introduced in Section 3.
Weexplain the reasons why we choose projectiveMSTParser in Section 4 which also contains theexperiment result analysis in various conditions.Section 5 gives our conclusion and future work.3 Semantic Dependency parsers3.1 First-Order ModelDependency tree parsing as the search for the max-imum spanning tree in a directed graph was pro-posed by McDonald et al (2005c).
Thisformulation leads to efficient parsing algorithmsfor both projective and non-projective dependencytrees with the Eisner algorithm (Eisner, 1996) andthe Chu-Liu-Edmonds algorithm (Chu and Liu,1965; Edmonds, 1967) respectively.
The formula-tion works by defining in McDonald et al(2005a).The score of a dependency tree y for sentence x is( )( , ), ( , ) ( , )i j ys x y s i j w f i j?= = ??
?f(i, j) is a multidimensional feature vector repre-sentation of the edge from node i to node j.
We setthe value of f(i, j) as 1 if there an edge from node ito node j. w is the corresponding weight vectorbetween the two nodes that will be learned duringtraining.
Hence, finding a dependency tree withhighest score is equivalent to finding a maximumspanning tree.
Obviously, the scores are restrictedto a single edge in the dependency tree, thus wecall this first-order dependency parsing.
This is astandard linear classifier.
The features used in thefirst-order dependency parser are based on thoselisted in (Johansson, 2008).
Table 2 shows the fea-tures we choose in the first-order parsing.
We usesome shorthand notations in order to simplify thefeature representations: h is the abbreviation forhead, d for dependent, s for nearby nodes (may notbe siblings), f for form, le for the lemmas, pos forpart-of-speech tags, dir for direction, dis for dis-tance, ?+1?
and ?-1?
for right and left position re-spectively.
Additional features are built by addingthe direction and the distance plus the direction.The direction is left if the dependent is left to itshead otherwise right.
The distance is the number ofwords minus one between the head and the depen-dent in a certain sentence, if ?
5, 5 if > 5, 10 if >10.
?
means  that previous part is built once andthe additional part follow ?
together with the pre-vious part is built again.Head and Dependenth-f, h-l, d-pos ?dir(h, d) ?dis(h, d)h-l, h-pos, d-f ?dir(h, d) ?dis(h, d)h-pos, h-f, d-l ?dir(h, d) ?dis(h, d)h-f, d-l, d-pos ?dir(h, d)  ?dis(h, d)h-f, d-f, d-l  ?dir(h, d) ?dis(h, d)h-f, h-l, d-f, d-l  ?dir(h, d) ?dis(h, d)h-f, h-l, d-f, d-pos ?dir(h, d) ?dis(h, d)h-f, h-pos, d-f, d-pos ?dir(h, d) ?dis(h, d)h-l, h-pos, d-l, d-pos ?dir(h, d) ?dis(h, d)Dependent and Nearbyd-pos-1, d-pos, s-pos ?dir(d, s) ?dis(d, s)d-pos-1, s-pos, s-pos+1 ?dir(d, s) ?dis(d, s)d-pos-1, d-pos, s-pos+1 ?dir(d, s) ?dis(d, s)d-pos, s-pos, s-pos+1 ?dir(d, s) ?dis(d, s)d-pos, d-pos+1, s-pos-1 ?dir(d, s) ?dis(d, s)d-pos-1, d-pos, s-pos-1 ?dir(d, s) ?dis(d, s)d-pos, d-pos+1, s-pos ?dir(d, s) ?dis(d, s)d-pos, s-pos-1, s-pos ?dir(d, s) ?dis(d, s)d-pos+1, s-pos-1, s-pos ?dir(d, s) ?dis(d, s)d-pos-1, d-pos, s-pos-1, s-pos ?
dir(d, s) ?dis(d, s)d-pos, d-pos+1, s-pos-1, s-pos ?dir(d, s) ?dis(d, s)d-pos-1, d-pos, s-pos, s-pos+1 ?dir(d, s) ?dis(d, s)Table 2: Selected features in first order parsing4313.2 Second-Order ModelA second order model proposed by McDonald(McDonald and Pereira, 2006) alleviates some ofthe first order factorization limitations.
Because thefirst order parsing restricts scores to a single edgein a dependency tree, the procedure is sufficient.However, in the second order parsing scenariowhere more than one edge are considered by theparsing algorithm, combinations of two edgesmight be more accurate which will be described inthe Section 4.
The second-order parsing can bedefined as below:( )( , ), ( , , )i j ys x y s i k j?= ?where k and j are adjacent,  same-side children of iin the tree y.
The shortcoming of this definition isthat it restricts i on the same side of its sibling.
Inour system, we extend this restriction by addingthe feature that as long as i is another child of k or j.In that case, i may be the child or grandchild of kor j which is shown in Figure 1.k  i  j ?
k  i jFigure 1: Sibling and grand-child relations.Siblingsc1-pos, c2-pos?dir(c1, c2)?dis(c1, c2)c1-f, c2-f?dir(c1, c2)c1-f, c2-pos?dir(c1, c2)c1-pos, c2-f?dir(c1, c2)Parent and Two Childrenp-pos, c1-pos, c2-pos?dir(c1, c2)?dis(c1, c2)p-f, c1-pos, c2-pos?dir(c1, c2)?dis(c1, c2)p-f, c1-f, c2-pos?dir(c1, c2) ?dis(c1, c2)p-f, c1-f, c2-f ?dir(c1, c2) ?dis(c1, c2)p-pos, c1-f, c2-f?dir(c1, c2) ?dis(c1, c2)p-pos, c1-f, c2-pos?dir(c1, c2) ?dis(c1, c2)p-pos, c1-pos, c2-f?dir(c1, c2) ?dis(c1, c2)Table 3: Selected features in second-order parsingShorthand notations are almost the same with theSection 3.1 except for that we use c1 and c2 torepresent the two children and p for parent.
Insecond-order parsing?
the features selected areshown in Table 3.
We divide the dependency dis-tance into six parts which are 1 if > 1, 2 if > 2, ?
,5 if  > 5, 10 if > 10.3.3 Third-Order FeaturesThe order of parsing is defined according to thenumber of dependencies it contains (Koo and Col-lins, 2010).
Collins classifies the third-order as twomodels, Model 1 is all grand-siblings, and Model 2is grand-siblings and tri-siblings.
A grand-siblingis a 4-tuple of indices (g, h, m, s) where g is grand-father.
(h, m, s) is a sibling part and (g, h, m) is agrandchild part as well as (g, h, s).
A tri-siblingpart is also a 4-tuple of indices (h, m, s, t).
Both (h,m, s) and (h, s, t) are siblings.
Figure 2 clearlyshows these relations.g h  s  m ?h t  s mFigure 2: Grand-siblings and tri-siblings dependency.Collins and Koo implement an efficient third-order dependency parsing algorithm, but still timeconsuming compared with the second-order(McDonald, 2006).
For that reason, we only addthird-order relation features into our system insteadof implementing the third-order dependency pars-ing model.
These features shown in Table 4 aregrand-sibling and tri-sibling described above.Shorthand notations are almost the same with theSection 3.1 and 3.2 except that we use c3 for thethird sibling and g represent the grandfather.
Weattempt to add features of words form and parts-of-speech as well as directions into our system, whichis used both in first-order and second-order as fea-tures, but result shows that these decrease the sys-tem performance.Tri-Siblingc1-pos, c2-pos, c3-pos?dir(c1, c2)Grandfather and Two Childreng-pos, c1-pos, c2-pos?dir(c1, c2)g-pos, p-pos, c1-pos, c2-pos?dir(c1, c2)Table 4: Third-order features.4324 Experiment result analysisAs we all know that projective dependency parsingusing edge based factorization can be processed bythe Einster algorithm (Einster, 1996).
The corpusgiven by SemEval-2012 is consists of 10000 sen-tences converting into dependency structures fromChinese Penn Treebank randomly.
We find thatnone of non-projective sentence existing by testingthe 8301 sentences in training data.
For this reason,we set the MSTParser into projective parsing mode.We perform a number of experiments where wecompare the first-order, second-order and second-order by adding third-order features proposed inthe previous sections.
We train the model on thefull training set which contains 8301 sentences to-tally.
We use 10 training iterations and projectivedecoding in the experiments.
Experimental resultsshow that 10 training iterations are better than oth-ers.
After adjusting the features of third-order, ourbest result reaches the labeled attachment score of62.48% on the developing dataset which ignorespunctuation.
We submitted our currently best resultto SemEval-2012 which is 61.58% on the test data-set.
The results in Table 5 show that by addingthird-order features to second-order model, we im-prove the dependency parsing accuracies by 1.21%comparing to first-order model and 0.15% compar-ing to second-order model.Models LAS UASFirst-Order 61.26 80.18Second-Order 62.33 81.40Second-Order+ 62.48 81.43Table 5: Experimental results.
Second-Order+ meanssecond-order model by adding third-order features.Results are tested under the developping dataset whichcontains the heads and semantic relations given byorganizer.5 Conclusion and Future WorkIn this paper, we have presented the semantic de-pendency parsing and shown it works on the first-order model, second-order model and second-ordermodel by adding third-order features.
Our experi-mental results show more significant improve-ments than the conventional approaches of third-order model.In the future, we firstly plan to implement thethird-order model by adding higher-order features,such as forth-order features.
We have found thatboth in the first-order and second-order model ofMSTParser, words form and lemmas are recog-nized as two different features.
These features areessential in languages that have different grid,however, which are the same in Chinese in the giv-en dataset.
Things are the same in POS (part-of-speech tags) and CPOS (fine-grid POS) which areviewed as different features.
For the applications ofsyntactic and semantic parsing, the parsing timeand memory footprint are very important.
There-fore, secondly, we decide to remove these repeatedfeatures in order to reduce to training time as wellas the space if it does not lower the system perfor-mance.AcknowledgmentsThe authors would like to thank the reviewers fortheir helpful comments.ReferencesMihai Surdeanu, Richard Johansson, Adam Meyers,Llu?s M?rquez, and JoakimNivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic andse-mantic dependencies.
In Proceedings of the 12thJan Haji?, Massimiliano Ciaramita, Richard Johansson,Daisuke Kawahara, Maria Antonia Mart?, Llu?isM?rquez, Adam Meyers, Joakim Nivre, SebastianPa-do, Jan ?tep?nek, Pavel Stran?k, Miahi Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 Shared Task: Syntactic and Semantic Depen-dencies in Multiple Languages.
In Proceedings of the13th CoNLL-2009, June 4-5, Boulder, Colorado,USA.CoNLL-2008.Bohnet, Bernd.
2009.
Efficient parsing of syntactic andsemantic dependency structures.
In Proceedings ofCoNLL-09.Ryan McDonald.
2006.
Discriminative Learning andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.Ryan McDonald and Fernando Pereira.
2006.
OnlineLearning of Approximate Dependency Parsing Al-grithms.
In In Proc.
of EACL, pages 81?88.Ryan.
McDonald, K. Crammer, and F. Pereira.
2005a.Online large-margin training of dependency parsers.In Proc.
of the 43rd Annual Meeting of the ACL.Ryan.
McDonald, F. Pereira, K. Ribarov, and J. Haji?c.2005c.
Non-projective dependency parsing usingspanning tree algorithms.
In Proc.
HLT-EMNLP.Richard Johansson.
2008.
Dependency-based SemanticAnalysis of Natural-language Text.
Ph.D. thesis,Lund University.433Jason Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proceedingsof the 16th International Conference on Computa-tional Linguistics (COLING-96), pages 340?345,Copenhaen.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-Based Dependency Parsing.
In Proceedingsof the 8th CoNLL, pages 49?56, Boston, Massachu-setts.Terry Koo, Michael Collins.
2010.
Efficient Third-orderDependency Parsers.
Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 1?11.434
