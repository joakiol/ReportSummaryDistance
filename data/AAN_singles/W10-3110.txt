Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 51?59,Uppsala, July 2010.What?s Great and What?s Not: Learning to Classify the Scope of Negationfor Improved Sentiment AnalysisIsaac G. CouncillGoogle, Inc.76 Ninth AvenueNew York, NY 10011icouncill@google.comRyan McDonaldGoogle, Inc.76 Ninth AvenueNew York, NY 10011ryanmcd@google.comLeonid VelikovichGoogle, Inc.76 Ninth AvenueNew York, NY 10011leonidv@google.comAbstractAutomatic detection of linguistic negationin free text is a critical need for many textprocessing applications, including senti-ment analysis.
This paper presents a nega-tion detection system based on a condi-tional random field modeled using fea-tures from an English dependency parser.The scope of negation detection is limitedto explicit rather than implied negationswithin single sentences.
A new negationcorpus is presented that was constructedfor the domain of English product reviewsobtained from the open web, and the pro-posed negation extraction system is eval-uated against the reviews corpus as wellas the standard BioScope negation corpus,achieving 80.0% and 75.5% F1 scores, re-spectively.
The impact of accurate nega-tion detection on a state-of-the-art senti-ment analysis system is also reported.1 IntroductionThe automatic detection of the scope of linguisticnegation is a problem encountered in wide varietyof document understanding tasks, including butnot limited to medical data mining, general fact orrelation extraction, question answering, and senti-ment analysis.
This paper describes an approachto negation scope detection in the context of sen-timent analysis, particularly with respect to sen-timent expressed in online reviews.
The canoni-cal need for proper negation detection in sentimentanalysis can be expressed as the fundamental dif-ference in semantics inherent in the phrases, ?thisis great,?
versus, ?this is not great.?
Unfortunately,expressions of negation are not always so syntac-tically simple.Linguistic negation is a complex topic: thereare many forms of negation, ranging from the useof explicit cues such as ?no?
or ?not?
to muchmore subtle linguistic patterns.
At the higheststructural level, negations may occur in two forms(Givo?n, 1993): morphological negations, whereword roots are modified with a negating prefix(e.g., ?dis-?, ?non-?, or ?un-?)
or suffix (e.g., ?-less?
), and syntactic negation, where clauses arenegated using explicitly negating words or othersyntactic patterns that imply negative semantics.For the purposes of negation scope detection, onlysyntactic negations are of interest, since the scopeof any morphological negation is restricted to anindividual word.
Morphological negations arevery important when constructing lexicons, whichis a separate but related research topic.Tottie (1991) presents a comprehensive taxon-omy of clausal English negations, where eachform represents unique challenges for a negationscope detection system.
The top-level negationcategories ?
denials, rejections, imperatives, ques-tions, supports, and repetitions ?
can be describedas follows:?
Denials are the most common form and aretypically unambiguous negations of a partic-ular clause, such as, ?There is no questionthat the service at this restaurant is excellent,?or, ?The audio system on this television is notvery good, but the picture is amazing.??
Rejections often occur in discourse, whereone participant rejects an offer or sugges-tion of another, e.g., ?Can I get you any-thing else?
No.?
However, rejections may ap-pear in expository text where a writer explic-itly rejects a previous supposition or expec-tation, for instance, ?Given the poor reputa-tion of the manufacturer, I expected to be dis-appointed with the device.
This was not thecase.??
Imperatives involve directing an audience51away from a particular action, e.g., ?Do notneglect to order their delicious garlic bread.??
Questions, rhetorical or otherwise, can indi-cate negations often in the context of surpriseor bewilderment.
For example, a reviewer ofa desk phone may write, ?Why couldn?t theyinclude a decent speaker in this phone?
?, im-plying that the phone being reviewed does nothave a decent speaker.?
Supports and Repetitions are used to ex-press agreement and add emphasis or clar-ity, respectively, and each involve multipleexpressions of negation.
For the purpose ofnegation scope detection, each instance ofnegation in a support or repetition can be iso-lated and treated as an independent denial orimperative.Tottie also distinguishes between intersenten-tial and sentential negation.
In the case of inter-sentential negation, the language used in one sen-tence may explicitly negate a proposition or impli-cation found in another sentence.
Rejections andsupports are common examples of intersententialnegation.
Sentential negation, or negations withinthe scope of a single sentence, are much morefrequent; thus sentential denials, imperatives, andquestions are the primary focus of the work pre-sented here.The goal of the present work is to develop a sys-tem that is robust to differences in the intendedscope of negation introduced by the syntactic andlexical features in each negation category.
In par-ticular, as the larger context of this research in-volves sentiment analysis, it is desirable to con-struct a negation system that can correctly identifythe presence or absence of negation in spans of textthat are expressions of sentiment.
It so follows thatin developing a solution for the specific case of thenegation of sentiment, the proposed system is alsoeffective at solving the general case of negationscope identification.This rest of this paper is organized as follows.
?2 presents related work on the topic of auto-matic detection of the scope of linguistic nega-tions.
The annotated corpora used to evaluatethe proposed negation scope identification methodare presented in ?3, including a new data set de-veloped for the purpose of identifying negationscopes in the context of online reviews.
?4 de-scribes the proposed negation scope detection sys-tem.
The novel system is evaluated in ?5 interms of raw results on the annotated negation cor-pora as well as the performance improvement onsentiment classification achieved by incorporatingthe negation system in a state-of-the-art sentimentanalysis pipeline.
Lessons learned and future di-rections are discussed in ?6.2 Related workNegation and its scope in the context of senti-ment analysis has been studied in the past (Moila-nen and Pulman, 2007).
In this work we focuson explicit negation mentions, also called func-tional negation by Choi and Cardie (2008).
How-ever, others have studied various forms of nega-tion within the domain of sentiment analysis, in-cluding work on content negators, which typi-cally are verbs such as ?hampered?, ?lacked?, ?de-nied?, etc.
(Moilanen and Pulman, 2007; Choiand Cardie, 2008).
A recent study by Danescu-Niculescu-Mizil et al (2009) looked at the prob-lem of finding downward-entailing operators thatinclude a wider range of lexical items, includ-ing soft negators such as the adverbs ?rarely?
and?hardly?.With the absence of a general purpose corpusannotating the precise scope of negation in sen-timent corpora, many studies incorporate nega-tion terms through heuristics or soft-constraints instatistical models.
In the work of Wilson et al(2005), a supervised polarity classifier is trainedwith a set of negation features derived from alist of cue words and a small window aroundthem in the text.
Choi and Cardie (2008) com-bine different kinds of negators with lexical polar-ity items through various compositional semanticmodels, both heuristic and machine learned, to im-prove phrasal sentiment analysis.
In that work thescope of negation was either left undefined or de-termined through surface level syntactic patternssimilar to the syntactic patterns from Moilanenand Pulman (2007).
A recent study by Nakagawaet al (2010) developed an semi-supervised modelfor sub-sentential sentiment analysis that predictspolarity based on the interactions between nodesin dependency graphs, which potentially can in-duce the scope of negation.As mentioned earlier, the goal of this work is todefine a system that can identify exactly the scopeof negation in free text, which requires a robust-ness to the wide variation of negation expression,52both syntactic and lexical.
Thus, this work is com-plimentary to those mentioned above in that weare measuring not only whether negation detec-tion is useful for sentiment, but to what extent wecan determine its exact scope in the text.
Towardsthis end in we describe both an annotated nega-tion span corpus as well as a negation span detec-tor that is trained on the corpus.
The span detec-tor is based on conditional random fields (CRFs)(Lafferty, McCallum, and Pereira, 2001), which isa structured prediction learning framework com-mon in sub-sentential natural language process-ing tasks, including sentiment analysis (Choi andCardie, 2007; McDonald et al, 2007)The approach presented here resembles work byMorante and Daelemans (2009), who used IGTreeto predict negation cues and a CRF metalearnerthat combined input from k-nearest neighbor clas-sification, a support vector machine, and anotherunderlying CRF to predict the scope of nega-tions within the BioScope corpus.
However, ourwork represents a simplified approach that re-places machine-learned cue prediction with a lex-icon of explicit negation cues, and uses only a sin-gle CRF to predict negation scopes, with a morecomprehensive model that includes features froma dependency parser.3 Data setsOne of the only freely available resources for eval-uating negation detection performance is the Bio-Scope corpus (Vincze et al, 2008), which consistsof annotated clinical radiology reports, biologicalfull papers, and biological abstracts.
Annotationsin BioScope consist of labeled negation and spec-ulation cues along with the boundary of their as-sociated text scopes.
Each cue is associated withexactly one scope, and the cue itself is consideredto be part of its own scope.
Traditionally, negationdetection systems have encountered the most dif-ficulty in parsing the full papers subcorpus, whichcontains nine papers and a total of 2670 sentences,and so the BioScope full papers were held out as abenchmark for the methods presented here.The work described in this paper was part of alarger research effort to improve the accuracy ofsentiment analysis in online reviews, and it wasdetermined that the intended domain of applica-tion would likely contain language patterns thatare significantly distinct from patterns common inthe text of professional biomedical writings.
Cor-rect analysis of reviews generated by web usersrequires robustness in the face of ungrammaticalsentences and misspelling, which are both exceed-ingly rare in BioScope.
Therefore, a novel cor-pus was developed containing the text of entirereviews, annotated according to spans of negatedtext.A sample of 268 product reviews were obtainedby randomly sampling reviews from Google Prod-uct Search1 and checking for the presence of nega-tion.
The annotated corpus contains 2111 sen-tences in total, with 679 sentences determined tocontain negation.
Each review was manually an-notated with the scope of negation by a single per-son, after achieving inter-annotator agreement of91% with a second person on a smaller subset of20 reviews containing negation.
Inter-annotatoragreement was calculated using a strict exact spancriteria where both the existence and the left/rightboundaries of a negation span were required tomatch.
Hereafter the reviews data set will be re-ferred to as the Product Reviews corpus.The Product Reviews corpus was annotated ac-cording to the following instructions:1.
Negation cues: Negation cues (e.g., thewords ?never?, ?no?, or ?not?
in it?s variousforms) are not included the negation scope.For example, in the sentence, ?It was not X?only ?X?
is annotated as the negation span.2.
General Principles: Annotate the minimalspan of a negation covering only the portionof the text being negated semantically.
Whenin doubt, prefer simplicity.3.
Noun phrases: Typically entire nounphrases are annotated as within the scopeof negation if a noun within the phrase isnegated.
For example, in the sentence, ?Thiswas not a review?
the string ?a review?
is an-notated.
This is also true for more complexnoun phrases, e.g., ?This was not a reviewof a movie that I watched?
should be anno-tated with the span ?a review of a movie thatI watched?.4.
Adjectives in noun phrases: Do not anno-tate an entire noun phrase if an adjective is allthat is being negated - consider the negationof each term separately.
For instance, ?Not1http://www.google.com/products/53top-drawer cinema, but still good...?
: ?top-drawer?
is negated, but ?cinema?
is not, sinceit is still cinema, just not ?top-drawer?.5.
Adverbs/Adjective phrases:(a) Case 1: Adverbial comparatives like?very,?
?really,?
?less,?
?more?, etc., an-notate the entire adjective phrase, e.g.,?It was not very good?
should be anno-tated with the span ?very good?.
(b) Case 2: If only the adverb is directlynegated, only annotate the adverb it-self.
E.g., ?Not only was it great?, or?Not quite as great?
: in both cases thesubject still ?is great?, so just ?only?and ?quite?
should be annotated, respec-tively.
However, there are cases wherethe intended scope of adverbial negationis greater, e.g., the adverb phrase ?just asmall part?
in ?Tony was on stage for theentire play.
It was not just a small part?.
(c) Case 3: ?as good as X?.
Try to identifythe intended scope, but typically the en-tire phrase should be annotated, e.g., ?Itwas not as good as I remember?.
Notethat Case 2 and 3 can be intermixed,e.g., ?Not quite as good as I remem-ber?, in this case follow 2 and just anno-tate the adverb ?quite?, since it was stillpartly ?as good as I remember?, just notentirely.6.
Verb Phrases: If a verb is directly negated,annotate the entire verb phrase as negated,e.g., ?appear to be red?
would be marked in?It did not appear to be red?.For the case of verbs (or adverbs), we made nospecial instructions on how to handle verbs thatare content negators.
For example, for the sen-tence ?I can?t deny it was good?, the entire verbphrase ?deny it was good?
would be marked as thescope of ?can?t?.
Ideally annotators would alsomark the scope of the verb ?deny?, effectively can-celing the scope of negation entirely over the ad-jective ?good?.
As mentioned previously, there area wide variety of verbs and adverbs that play sucha role and recent studies have investigated meth-ods for identifying them (Choi and Cardie, 2008;Danescu-Niculescu-Mizil et al, 2009).
We leavethe identification of the scope of such lexical itemshardly lack lacking lacksneither nor never nonobody none nothing nowherenot n?t aint cantcannot darent dont doesntdidnt hadnt hasnt havnthavent isnt mightnt mustntneednt oughtnt shant shouldntwasnt wouldnt withoutTable 1: Lexicon of explicit negation cues.and their interaction with explicit negation as fu-ture work.The Product Reviews corpus is different fromBioScope in several ways.
First, BioScope ignoresdirect adverb negation, such that neither the nega-tion cue nor the negation scope in the the phrase,?not only,?
is annotated in BioScope.
Second,BioScope annotations always include entire adjec-tive phrases as negated, where our method distin-guishes between the negation of adjectives and ad-jective targets.
Third, BioScope includes nega-tion cues within their negation scopes, whereasour corpus separates the two.4 System descriptionAs the present work focuses on explicit negations,the choice was made to develop a lexicon of ex-plicit negation cues to serve as primary indicatorsof the presence of negation.
Klima (1964) was thefirst to identify negation words using a statistics-driven approach, by analyzing word co-occurrencewith n-grams that are cues for the presence ofnegation, such as ?either?
and ?at all?.
Klima?slexicon served as a starting point for the presentwork, and was further refined through the inclu-sion of common misspellings of negation cues andthe manual addition of select cues from the ?Neg?and ?Negate?
tags of the General Inquirer (Stoneet al, 1966).
The final list of cues used for theevaluations in ?5 is presented in Table 1.
The lex-icon serves as a reliable signal to detect the pres-ence of explicit negations, but provides no meansof inferring the scope of negation.
For scope de-tection, additional signals derived from surfaceand dependency level syntactic structure are em-ployed.The negation scope detection system is built asan individual annotator within a larger annotationpipeline.
The negation annotator relies on two dis-54tinct upstream annotators for 1) sentence boundaryannotations, derived from a rule-based sentenceboundary extractor and 2) token annotations froma dependency parser.
The dependency parser is animplementation of the parsing systems describedin Nivre and Scholz (2004) and Nivre et al (2007).Each annotator marks the character offsets for thebegin and end positions of individual annotationranges within documents, and makes the annota-tions available to downstream processes.The dependency annotator controls multiplelower-level NLP routines, including tokenizationand part of speech (POS) tagging in addition toparsing sentence level dependency structure.
Theoutput that is kept for downstream use includesonly POS and dependency relations for each to-ken.
The tokenization performed at this stage is re-cycled when learning to identify negation scopes.The feature space of the learning problem ad-heres to the dimensions presented in Table 2,and negation scopes are modeled using a first or-der linear-chain conditional random field (CRF)2,with a label set of size two indicating whether atoken is within or outside of a negation span.
Thefeatures include the lowercased token string, tokenPOS, token-wise distance from explicit negationcues, POS information from dependency heads,and dependency distance from dependency headsto explicit negation cues.
Only unigram featuresare employed, but each unigram feature vector isexpanded to include bigram and trigram represen-tations derived from the current token in conjunc-tion with the prior and subsequent tokens.The distance measures can be explained as fol-lows.
Token-wise distance is simply the numberof tokens from one token to another, in the orderthey appear in a sentence.
Dependency distance ismore involved, and is calculated as the minimumnumber of edges that must be traversed in a de-pendency tree to move from one node (or token)to another.
Each edge is considered to be bidi-rectional.
The CRF implementation used in oursystem employs categorical features, so both inte-ger distances are treated as encodings rather thancontinuous values.
The number 0 implies that atoken is, or is part of, an explicit negation cue.The numbers 1-4 encode step-wise distance froma negation cue, and the number 5 is used to jointlyencode the concepts of ?far away?
and ?not appli-cable?.
The maximum integer distance is 5, which2Implemented with CRF++: http://crfpp.sourceforge.net/Feature DescriptionWord The lowercased token string.POS The part of speech of a token.Right Dist.
The linear token-wise distance tothe nearest explicit negation cueto the right of a token.Left Dist.
The linear token-wise distance tothe nearest explicit negation cueto the left of a token.Dep1 POS The part of speech of the the firstorder dependency of a token.Dep1 Dist.
The minimum number of depen-dency relations that must be tra-versed to from the first order de-pendency head of a token to anexplicit negation cue.Dep2 POS The part of speech of the the sec-ond order dependency of a token.Dep2 Dist.
The minimum number of depen-dency relations that must be tra-versed to from the second orderdependency head of a token to anexplicit negation cue.Table 2: Token features used in the conditionalrandom field model for negation.was determined empirically.The negation annotator vectorizes the tokensgenerated in the dependency parser annotator andcan be configured to write token vectors to an out-put stream (training mode) or load a previouslylearned conditional random field model and ap-ply it by sending the token vectors directly to theCRF decoder (testing mode).
The output annota-tions include document-level negation span rangesas well as sentence-level token ranges that includethe CRF output probability vector, as well as thealpha and beta vectors.5 ResultsThe negation scope detection system was evalu-ated against the data sets described in ?3.
Thenegation CRF model was trained and testedagainst the Product Reviews and BioScope biolog-ical full papers corpora.
Subsequently, the practi-cal effect of robust negation detection was mea-sured in the context of a state-of-the-art sentimentanalysis system.55Corpus Prec.
Recall F1 PCSReviews 81.9 78.2 80.0 39.8BioScope 80.8 70.8 75.5 53.7Table 3: Results of negation scope detection.5.1 Negation Scope DetectionTo measure scope detection performance, theautomatically generated results were comparedagainst each set of human-annotated negation cor-pora in a token-wise fashion.
That is, precisionand recall were calculated as a function of the pre-dicted versus actual class of each text token.
To-kens made up purely of punctuation were consid-ered to be arbitrary artifacts of a particular tok-enization scheme, and thus were excluded fromthe results.
In keeping with the evaluation pre-sented by Morante and Daelemans (2009), thenumber of perfectly identified negation scopes ismeasured separately as the percentage of correctscopes (PCS).
The PCS metric is calculated as thenumber of correct spans divided by the number oftrue spans, making it a recall measure.Only binary classification results were consid-ered (whether a token is of class ?negated?
or ?notnegated?)
even though the probabilistic nature ofconditional random fields makes it possible to ex-press uncertainty in terms of soft classificationscores in the range 0 to 1.
Correct predictions ofthe absence of negation are excluded from the re-sults, so the reported measurements only take intoaccount correct prediction of negation and incor-rect predictions of either class.The negation scope detection results for boththe Product Reviews and BioScope corpora arepresented in Table 3.
The results on the ProductReviews corpus are based on seven-fold cross vali-dation, and the BioScope results are based on five-fold cross validation, since the BioScope data setis smaller.
For each fold, the number of sentenceswith and without negation were balanced in bothtraining and test sets.The system was designed primarily to supportthe case of negation scope detection in the openweb, and no special considerations were taken toimprove performance on the BioScope corpus.
Inparticular, the negation cue lexicon presented inTable 1 was not altered in any way, even thoughBioScope contains additional cues such as ?ratherthan?
and ?instead of?.
This had a noticeable ef-fect on on recall in BioScope, although in severalCondition Prec.
Recall F1 PCSBioScope,trained onReviews72.2 42.1 53.5 52.2Reviews,trained onBioscope58.8 68.8 63.4 45.7Table 4: Results for cross-trained negation mod-els.
This shows the results for BioScope witha model trained on the Product Reviews corpus,and the results for Product Reviews with a modeltrained on the BioScope corpus.cases the CRF was still able to learn the missingcues indirectly through lexical features.In general, the system performed significantlybetter on the Product Reviews corpus than on Bio-Scope, although the performance on BioScope fullpapers is state-of-the-art.
This can be accountedfor at least partially by the differences in the nega-tion cue lexicons.
However, significantly morenegation scopes were perfectly identified in Bio-Scope, with a 23% improvement in the PCS metricover the Product Reviews corpus.The best reported performance to date on theBioScope full papers corpus was presented byMorante and Daelemans (2009), who achieved anF1 score of 70.9 with predicted negation signals,and an F1 score of 84.7 by feeding the manuallyannotated negation cues to their scope finding sys-tem.
The system presented here compares favor-ably to Morante and Daelemans?
fully automaticresults, achieving an F1 score of 75.5, which isa 15.8% reduction in error, although the resultsare significantly worse than what was achieved viaperfect negation cue information.5.2 Cross trainingThe degree to which models trained on eachcorpus generalized to each other was also mea-sured.
For this experiment, each of the two mod-els trained using the methods described in ?5.1was evaluated against its non-corresponding cor-pus, such that the BioScope-trained corpus wasevaluated against all of Product Reviews, and themodel derived from Product Reviews was evalu-ated against all of BioScope.The cross training results are presented in Ta-ble 4.
Performance is generally much worse, asexpected.
Recall drops substantially in BioScope,56which is almost certainly due to the fact that notonly are several of the BioScope negation cuesmissing from the cue lexicon, but the CRF modelhas not had the opportunity to learn from the lex-ical features in BioScope.
The precision in Bio-Scope remains fairly high, and the percentage ofperfectly labeled scopes remains almost the same.For Product Reviews, an opposing trend can beseen: precision drops significantly but recall re-mains fairly high.
This seems to indicate that thescope boundaries in the Product Reviews corpusare generally harder to predict.
The percentageof perfectly labeled scopes actually increases forProduct Reviews, which could also indicate thatscope boundaries are less noisy in BioScope.5.3 Effect on sentiment classificationIn addition to measuring the raw performance ofthe negation scope detection system, an experi-ment was conducted to measure the effect of thefinal negation system within the context of a largersentiment analysis system.The negation system was built into a senti-ment analysis pipeline consisting of the followingstages:1.
Sentence boundary detection.2.
Sentiment detection.3.
Negation scope detection, applying the sys-tem described in ?4.4.
Sentence sentiment scoring.The sentiment detection system in stage 2 findsand scores mentions of n-grams found in a largelexicon of sentiment terms and phrases.
The sen-timent lexicon is based on recent work using labelpropagation over a very large distributional simi-larity graph derived from the web (Velikovich etal., 2010), and applies positive or negative scoresto terms such as ?good?, ?bad?, or ?just what thedoctor ordered?.
The sentence scoring system instage 4 then determines whether any scored senti-ment terms fall within the scope of a negation, andflips the sign of the sentiment score for all negatedsentiment terms.
The scoring system then sums allsentiment scores within each sentence and com-putes overall sentence sentiment scores.A sample of English-language online reviewswas collected, containing a total of 1135 sen-tences.
Human raters were presented with consec-utive sentences and asked to classify each sentence0 0.1 0.2 0.3 0.4 0.5 0.6 0.70.40.50.60.70.80.91RecallPrecisionWith Negation DetectionWithout Negation DetectionFigure 1: Precision-recall curve showing the effectof negation detection on positive sentiment predic-tion.as expressing one of the following types of sen-timent: 1) positive, 2) negative, 3) neutral, or 4)mixed positive and negative.
Each sentence wasreviewed independently by five separate raters,and final sentence classification was determinedby consensus.
Of the original 1135 sentences 216,or 19%, were found to contain negations.The effect of the negation system on sentimentclassification was evaluated on the smaller subsetof 216 sentences in order to more precisely mea-sure the impact of negation detection.
The smallernegation subset contained 73 sentences classifiedas positive, 114 classified as negative, 12 classifiedas neutral, and 17 classified as mixed.
The num-ber of sentences classified as neutral or mixed wastoo small for a useful performance measurement,so only sentences classified as positive or negativesentences were considered.Figures 1 and 2 show the precision-recall curvesfor sentences predicted by the sentiment analysissystem to be positive and negative, respectively.The curves indicate relatively low performance,which is consistent with the fact that sentimentpolarity detection is notoriously difficult on sen-tences with negations.
The solid lines show per-formance with the negation scope detection sys-tem in place, and the dashed lines show perfor-mance with no negation detection at all.
Fromthe figures, a significant improvement is immedi-ately apparent at all recall levels.
It can also beinferred from the figures that the sentiment analy-sis system is significantly biased towards positivepredictions: even though there were significantlymore sentences classified by human raters as neg-570 0.05 0.1 0.15 0.2 0.25 0.3 0.350.650.70.750.80.850.90.951RecallPrecisionWith Negation DetectionWithout Negation DetectionFigure 2: Precision-recall curve showing the ef-fect of negation detection on negative sentimentprediction.Metric w/o Neg.
w/ Neg.
% Improv.Positive SentimentPrec.
44.0 64.1 35.9Recall 54.8 63.7 20.0F1 48.8 63.9 29.5Negative SentimentPrec.
68.6 83.3 46.8Recall 21.1 26.3 6.6F1 32.3 40.0 11.4Table 5: Sentiment classification results, show-ing the percentage improvement obtained from in-cluding negation scope detection (w/ Neg.)
overresults obtained without including negation scopedetection (w/o Neg.
).ative, the number of data points for positive pre-dictions far exceeds the number of negative pre-dictions, with or without negation detection.The overall results are presented in Table 5, sep-arated by positive and negative class predictions.As expected, performance is improved dramati-cally by introducing negation scope detection.
Theprecision of positive sentiment predictions sees thelargest improvement, largely due to the inherentbias in the sentiment scoring algorithm.
F1 scoresfor positive and negative sentiment predictions im-prove by 29.5% and 11.4%, respectively.6 ConclusionsThis paper presents a system for identifying thescope of negation using shallow parsing, by meansof a conditional random field model informed bya dependency parser.
Results were presented onthe standard BioScope corpus that compare favor-ably to the best results reported to date, using asoftware stack that is significantly simpler than thebest-performing approach.A new data set was presented that targets thedomain of online product reviews.
The product re-view corpus represents a departure from the stan-dard BioScope corpus in two distinct dimensions:the reviews corpus contains diverse common andvernacular language patterns rather than profes-sional prose, and also presents a divergent methodfor annotating negations in text.
Cross-training bylearning a model on one corpus and testing on an-other suggests that scope boundary detection in theproduct reviews corpus may be a more difficultlearning problem, although the method used to an-notate the reviews corpus may result in a moreconsistent representation of the problem.Finally, the negation system was built into astate-of-the-art sentiment analysis system in orderto measure the practical impact of accurate nega-tion scope detection, with dramatic results.
Thenegation system improved the precision of positivesentiment polarity detection by 35.9% and nega-tive sentiment polarity detection by 46.8%.
Errorreduction on the recall measure was less dramatic,but still significant, showing improved recall forpositive polarity of 20.0% and improved recall fornegative polarity of 6.6%.Future research will include treatment of im-plicit negation cues, ideally by learning to predictthe presence of implicit negation using a prob-abilistic model that generates meaningful confi-dence scores.
A related topic to be addressedis the automatic detection of sarcasm, which isan important problem for proper sentiment anal-ysis, particularly in open web domains where lan-guage is vernacular.
Additionally, we would liketo tackle the problem of inter-sentential negations,which could involve a natural extension of nega-tion scope detection through co-reference resolu-tion, such that negated pronouns trigger negationsin text surrounding their pronoun antecedents.AcknowledgmentsThe authors would like to thank Andrew Hogueand Kerry Hannan for useful discussions regardingthis work.58ReferencesYejin Choi and Claire Cardie.
2007.
Structured Lo-cal Training and Biased Potential Functions for Con-ditional Random Fields with Application to Coref-erence Resolution.
Proceedings of The 9th Con-ference of the North American Chapter of the As-sociation for Computational Linguistics, ACL,Rochester, NY.Yejin Choi and Claire Cardie.
2008.
Learning withCompositional Semantics as Structural Inference forSubsentential Sentiment Analysis.
Proceedings ofthe Conference on Empirical Methods on NaturalLanguage Processing.
ACL, Honolulu, HI.Cristian Danescu-Niculescu-Mizil, Lillian Lee, andRichard Ducott.
2008.
Without a ?doubt??
Un-supervised discovery of downward-entailing opera-tors.
Proceedings of The 10th Annual Conference ofthe North American Chapter of the Association forComputational Linguistics.
ACL, Boulder, CO.Talmy Givo?n.
1993.
English Grammer: A Function-Based Introduction.
Benjamins, Amsterdam, NL.Edward S. Klima.
1964.
Negation in English.
Read-ings in the Philosophy of Language.
Ed.
J.
A. Fodorand J. J. Katz.
Prentice Hall, Englewood Cliffs, NJ:246-323.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random elds: Prob-abilistic models for segmenting and labeling se-quence data.
Proceedings of the International Con-ference on Machine Learning.
Morgan Kaufmann,Williamstown, MA.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured Models forFine-to-Coarse Sentiment Analysis.
Proceedings ofthe Annual Meeting of the Association for Computa-tional Linguistics.
Prague, Czech Republic.Karo Moilanen and Stephen Pulman 2007.
SentimentComposition.
Proceedings of the Recent Advancesin Natural Language Processing International Con-ference Borovets, BulgariaRoser Morante and Walter Daelemans.
2009.
Ametalearning approach to processing the scope ofnegation.
Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL).
ACM, Boulder, CO.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency Tree-based Sentiment Classifi-cation using CRFs with Hidden Variables.
Proceed-ings of The 11th Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics ACL, Los Angeles, CA.Joakim Nivre and Mario Scholz.
2004.
DeterministicDependency Parsing of English Text.
Proceedingsof the 20th International Conference on Computa-tional Linguistics.
ACM, Geneva, Switzerland.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gulsen Eryigit Sandra Kubler, SvetoslavMarinov and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing Natural Language Engineering13(02):95?135Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,Daniel M. Ogilvie.
1966.
The General Inquirer:A Computer Approach to Content Analysis.
MITPress, Cambridge, MA.Gunnel Tottie.
1991.
Negation in English Speechand Writing: A Study in Variation Academic, SanDiego, CA.Leonid Velikovich, Sasha Blair-Goldensohn, KerryHannan, and Ryan McDonald.
2010.
The viabil-ity of web-derived polarity lexicons.
Proceedings ofThe 11th Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics.
ACL, Los Angeles, CA.Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9(Suppl 11):S9.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
Proceedings of the Confer-ence on Human Language Technology and Empiri-cal Methods in Natural Language Processing Van-couver, Canada.59
