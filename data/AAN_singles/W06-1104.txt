Proceedings of the Workshop on Linguistic Distances, pages 16?24,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatically creating datasets for measures of semantic relatednessTorsten Zesch and Iryna GurevychDepartment of TelecooperationDarmstadt University of TechnologyD-64289 Darmstadt, Germany{zesch,gurevych} (at) tk.informatik.tu-darmstadt.deAbstractSemantic relatedness is a special form oflinguistic distance between words.
Eval-uating semantic relatedness measures isusually performed by comparison with hu-man judgments.
Previous test datasets hadbeen created analytically and were limitedin size.
We propose a corpus-based systemfor automatically creating test datasets.1Experiments with human subjects showthat the resulting datasets cover all de-grees of relatedness.
As a result of thecorpus-based approach, test datasets coverall types of lexical-semantic relations andcontain domain-specific words naturallyoccurring in texts.1 IntroductionLinguistic distance plays an important role inmany applications like information retrieval, wordsense disambiguation, text summarization orspelling correction.
It is defined on different kindsof textual units, e.g.
documents, parts of a docu-ment (e.g.
words and their surrounding context),words or concepts (Lebart and Rajman, 2000).2Linguistic distance between words is inverse totheir semantic similarity or relatedness.Semantic similarity is typically defined via thelexical relations of synonymy (automobile ?
car)and hypernymy (vehicle ?
car), while semanticrelatedness (SR) is defined to cover any kind oflexical or functional association that may exist be-1In the near future, we are planning to make the softwareavailable to interested researchers.2In this paper, word denotes the graphemic form of a to-ken and concept refers to a particular sense of a word.tween two words (Gurevych, 2005).3 Dissimilarwords can be semantically related, e.g.
via func-tional relationships (night ?
dark) or when theyare antonyms (high ?
low).
Many NLP applica-tions require knowledge about semantic related-ness rather than just similarity (Budanitsky andHirst, 2006).A number of competing approaches for comput-ing semantic relatedness of words have been de-veloped (see Section 2).
A commonly acceptedmethod for evaluating these approaches is to com-pare their results with a gold standard based onhuman judgments on word pairs.
For that pur-pose, relatedness scores for each word pair haveto be determined experimentally.
Creating testdatasets for such experiments has so far been alabor-intensive manual process.We propose a corpus-based system to automat-ically create test datasets for semantic relatednessexperiments.
Previous datasets were created ana-lytically, preventing their use to gain insights intothe nature of SR and also not necessarily reflectingthe reality found in a corpus.
They were also lim-ited in size.
We provide a larger annotated test setthat is used to better analyze the connections anddifferences between the approaches for computingsemantic relatedness.The remainder of this paper is organized as fol-lows: we first focus on the notion of semantic re-latedness and how it can be evaluated.
Section 3reviews related work.
Section 4 describes our sys-tem for automatically extracting word pairs from acorpus.
Furthermore, the experimental setup lead-ing to human judgments of semantic relatedness3Nevertheless the two terms are often (mis)used inter-changeably.
We will use semantic relatedness in the remain-der of this paper, as it is the more general term that subsumessemantic similarity.16is presented.
Section 5 discusses the results, andfinally we draw some conclusions in Section 6.2 Evaluating SR measuresVarious approaches for computing semantic re-latedness of words or concepts have been pro-posed, e.g.
dictionary-based (Lesk, 1986),ontology-based (Wu and Palmer, 1994; Leacockand Chodorow, 1998), information-based (Resnik,1995; Jiang and Conrath, 1997) or distributional(Weeds and Weir, 2005).
The knowledge sourcesused for computing relatedness can be as differentas dictionaries, ontologies or large corpora.According to Budanitsky and Hirst (2006),there are three prevalent approaches for evaluatingSR measures: mathematical analysis, application-specific evaluation and comparison with humanjudgments.Mathematical analysis can assess a measurewith respect to some formal properties, e.g.whether a measure is a metric (Lin, 1998).4 How-ever, mathematical analysis cannot tell us whethera measure closely resembles human judgments orwhether it performs best when used in a certainapplication.The latter question is tackled by application-specific evaluation, where a measure is testedwithin the framework of a certain application,e.g.
word sense disambiguation (Patwardhan etal., 2003) or malapropism detection (Budanitskyand Hirst, 2006).
Lebart and Rajman (2000) ar-gue for application-specific evaluation of similar-ity measures, because measures are always usedfor some task.
But they also note that evaluatinga measure as part of a usually complex applica-tion only indirectly assesses its quality.
A certainmeasure may work well in one application, but notin another.
Application-based evaluation can onlystate the fact, but give little explanation about thereasons.The remaining approach - comparison with hu-man judgments - is best suited for applicationindependent evaluation of relatedness measures.Human annotators are asked to judge the related-ness of presented word pairs.
Results from theseexperiments are used as a gold standard for eval-uation.
A further advantage of comparison withhuman judgments is the possibility to gain deeper4That means, whether it fulfills some mathematical crite-ria: d(x, y) ?
0; d(x, y) = 0 ?
x = y; d(x, y) = d(y, x);d(x, z) ?
d(x, y) + d(y, z).insights into the nature of semantic relatedness.However, creating datasets for evaluation has sofar been limited in a number of respects.
Onlya small number of word pairs was manually se-lected, with semantic similarity instead of related-ness in mind.
Word pairs consisted only of noun-noun combinations and only general terms wereincluded.
Polysemous and homonymous wordswere not disambiguated to concepts, i.e.
humansannotated semantic relatedness of words ratherthan concepts.3 Related workIn the seminal work by Rubenstein and Goode-nough (1965), similarity judgments were obtainedfrom 51 test subjects on 65 noun pairs written onpaper cards.
Test subjects were instructed to orderthe cards according to the ?similarity of meaning?and then assign a continuous similarity value (0.0 -4.0) to each card.
Miller and Charles (1991) repli-cated the experiment with 38 test subjects judg-ing on a subset of 30 pairs taken from the original65 pairs.
This experiment was again replicated byResnik (1995) with 10 subjects.
Table 1 summa-rizes previous experiments.A comprehensive evaluation of SR measures re-quires a higher number of word pairs.
However,the original experimental setup is not scalable asordering several hundred paper cards is a cum-bersome task.
Furthermore, semantic relatednessis an intuitive concept and being forced to assignfine-grained continuous values is felt to overstrainthe test subjects.
Gurevych (2005) replicated theexperiment of Rubenstein and Goodenough withthe original 65 word pairs translated into German.She used an adapted experimental setup where testsubjects had to assign discrete values {0,1,2,3,4}and word pairs were presented in isolation.
Thissetup is also scalable to a higher number of wordpairs (350) as was shown in Gurevych (2006).Finkelstein et al (2002) annotated a larger set ofword pairs (353), too.
They used a 0-10 range ofrelatedness scores, but did not give further detailsabout their experimental setup.
In psycholinguis-tics, relatedness of words can also be determinedthrough association tests (Schulte im Walde andMelinger, 2005).
Results of such experiments arehard to quantify and cannot easily serve as the ba-sis for evaluating SR measures.Rubenstein and Goodenough selected wordpairs analytically to cover the whole spectrum of17CORRELATIONPAPER LANGUAGE PAIRS POS REL-TYPE SCORES # SUBJECTS INTER INTRAR/G (1965) English 65 N sim continuous 0?4 51 - .850M/C (1991) English 30 N sim continuous 0?4 38 - -Res (1995) English 30 N sim continuous 0?4 10 .903 -Fin (2002) English 353 N, V, A relat continuous 0?10 16 - -Gur (2005) German 65 N sim discrete {0,1,2,3,4} 24 .810 -Gur (2006) German 350 N, V, A relat discrete {0,1,2,3,4} 8 .690 -Z/G (2006) German 328 N, V, A relat discrete {0,1,2,3,4} 21 .478 .647Table 1: Comparison of previous experiments.
R/G=Rubenstein and Goodenough, M/C=Miller andCharles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevychsimilarity from ?not similar?
to ?synonymous?.This elaborate process is not feasible for a largerdataset or if domain-specific test sets should becompiled quickly.
Therefore, we automaticallycreate word pairs using a corpus-based approach.We assume that due to lexical-semantic cohesion,texts contain a sufficient number of words re-lated by means of different lexical and semanticrelations.
Resulting from our corpus-based ap-proach, test sets will also contain domain-specificterms.
Previous studies only included generalterms as opposed to domain-specific vocabulariesand therefore failed to produce datasets that canbe used to evaluate the ability of a measure to copewith domain-specific or technical terms.
This is animportant property if semantic relatedness is usedin information retrieval where users tend to usespecific search terms (Porsche) rather than generalones (car).Furthermore, manually selected word pairsare often biased towards highly related pairs(Gurevych, 2006), because human annotators tendto select only highly related pairs connected by re-lations they are aware of.
Automatic corpus-basedselection of word pairs is more objective, leadingto a balanced dataset with pairs connected by allkinds of lexical-semantic relations.
Morris andHirst (2004) pointed out that many relations be-tween words in a text are non-classical (i.e.
otherthan typical taxonomic relations like synonymy orhypernymy) and therefore not covered by seman-tic similarity.Previous studies only considered semantic re-latedness (or similarity) of words rather than con-cepts.
However, polysemous or homonymouswords should be annotated on the level of con-cepts.
If we assume that bank has two meanings(?financial institution?
vs. ?river bank?
)5 and it ispaired with money, the result is two sense quali-5WordNet lists 10 meanings.fied pairs (bankfinancial ?
money) and (bankriver?
money).
It is obvious that the judgments on thetwo concept pairs should differ considerably.
Con-cept annotated datasets can be used to test the abil-ity of a measure to differentiate between senseswhen determining the relatedness of polysemouswords.
To our knowledge, this study is the first toinclude concept pairs and to automatically gener-ate the test dataset.In our experiment, we annotated a high numberof pairs similar in size to the test sets by Finkel-stein (2002) and Gurevych (2006).
We used the re-vised experimental setup (Gurevych, 2005), basedon discrete relatedness scores and presentation ofword pairs in isolation, that is scalable to thehigher number of pairs.
We annotated semanticrelatedness instead of similarity and included alsonon noun-noun pairs.
Additionally, our corpus-based approach includes domain-specific techni-cal terms and enables evaluation of the robustnessof a measure.4 Experiment4.1 System architectureFigure 1 gives an overview of our automaticcorpus-based system for creating test datasets forevaluating SR measures.In the first step, a source corpus is preprocessedusing tokenization, POS-tagging and lemmatiza-tion resulting in a list of POS-tagged lemmas.Randomly generating word pairs from this listwould result in too many unrelated pairs, yieldingan unbalanced dataset.
Thus, we assign weights toeach word (e.g.
using tf.idf-weighting).
The mostimportant document-specific words get the high-est weights and due to lexical cohesion of the doc-uments many related words can be found amongthe top rated.
Therefore, we randomly generatea user-defined number of word pairs from the rwords with the highest weights for each document.18CorpusTokenizationPOS-taggingLemmatizationTermweightingWord-conceptmappingconcept pairswith glossesWordpairgeneratorPreprocessingWordpairfiltertf.idfWordsensedictionaryAbbreviationsStoplistother userdefined filtersPOScombinationsFigure 1: System architecture for extraction ofconcept pairs.In the next step, user defined filters are appliedto the initial list of word pairs.
For example, a fil-ter can remove all pairs containing only uppercaseletters (mostly acronyms).
Another filter can en-force a certain fraction of POS combinations to bepresent in the result set.As we want to obtain judgment scores for se-mantic relatedness of concepts instead of words,we have to include all word sense combinations ofa pair in the list.
An external dictionary of wordsenses is necessary for this step.
It is also used toadd a gloss for each word sense that enables testsubjects to distinguish between senses.If differences in meaning between senses arevery fine-grained, distinguishing between them ishard even for humans (Mihalcea and Moldovan,2001).6 Pairs containing such words are not suit-able for evaluation.
To limit their impact on theexperiment, a threshold for the maximal numberof senses can be defined.
Words with a number ofsenses above the threshold are removed from thelist.The result of the extraction process is a list ofsense disambiguated, POS-tagged pairs of con-cepts.6E.g.
the German verb ?halten?
that can be translated ashold, maintain, present, sustain, etc.
has 26 senses in Ger-maNet.4.2 Experimental setup4.2.1 Extraction of concept pairsWe extracted word pairs from three differentdomain-specific corpora (see Table 2).
This ismotivated by the aim to enable research in infor-mation retrieval incorporating SR measures.
Inparticular, the ?Semantic Information Retrieval?project (SIR Project, 2006) systematically investi-gates the use of lexical-semantic relations betweenwords or concepts for improving the performanceof information retrieval systems.The BERUFEnet (BN) corpus7 consists of de-scriptions of 5,800 professions in Germany andtherefore contains many terms specific to profes-sional training.
Evaluating semantic relatednesson a test set based on this corpus may reveal theability of a measure to adapt to a very special do-main.
The GIRT (German Indexing and RetrievalTestdatabase) corpus (Kluck, 2004) is a collec-tion of abstracts of social science papers.
It is astandard corpus for evaluating German informa-tion retrieval systems.
The third corpus is com-piled from 106 arbitrarily selected scientific Pow-erPoint presentations (SPP).
They cover a widerange of topics from bio genetics to computer sci-ence and contain many technical terms.
Due tothe special structure of presentations, this corpuswill be particularly demanding with respect to therequired preprocessing components of an informa-tion retrieval system.The three preprocessing steps (tokenization,POS-tagging, lemmatization) are performed us-ing TreeTagger (Schmid, 1995).
The resultinglist of POS-tagged lemmas is weighted using theSMART ?ltc?8 tf.idf-weighting scheme (Salton,1989).We implemented a set of filters for word pairs.One group of filters removed unwanted wordpairs.
Word pairs are filtered if they contain atleast one word that a) has less than three letters b)contains only uppercase letters (mostly acronyms)or c) can be found in a stoplist.
Another fil-ter enforced a specified fraction of combinationsof nouns (N), verbs (V) and adjectives (A) to bepresent in the result set.
We used the following pa-rameters: NN = 0.5, NV = 0.15, NA = 0.15,V V = 0.1, V A = 0.05, AA = 0.05.
That means50% of the resulting word pairs for each corpus7http://berufenet.arbeitsagentur.de8l=logarithmic term frequency, t=logarithmic inverse doc-ument frequency, c=cosine normalization.19CORPUS # DOCS # TOKENS DOMAINBN 9,022 7,728,501descriptionsof professionsGIRT 151,319 19,645,417abstracts of socialscience papersSPP 106 144,074scientific .pptpresentationsTable 2: Corpus statistics.were noun-noun pairs, 15% noun-verb pairs andso on.Word pairs containing polysemous wordsare expanded to concept pairs using Ger-maNet (Kunze, 2004), the German equivalent toWordNet, as a sense inventory for each word.
Itis the most complete resource of this type for Ger-man.GermaNet contains only a few conceptualglosses.
As they are required to enable test sub-jects to distinguish between senses, we use artifi-cial glosses composed from synonyms and hyper-nyms as a surrogate, e.g.
for brother: ?brother,male sibling?
vs. ?brother, comrade, friend?
(Gurevych, 2005).
We removed words which hadmore than three senses.Marginal manual post-processing was neces-sary, since the lemmatization process introducedsome errors.
Foreign words were translated intoGerman, unless they are common technical termi-nology.
We initially selected 100 word pairs fromeach corpus.
11 word pairs were removed be-cause they comprised non-words.
Expanding theword list to a concept list increased the size of thelist.
Thus, the final dataset contained 328 automat-ically created concept pairs.4.2.2 Graphical User InterfaceWe developed a web-based interface to obtainhuman judgments of semantic relatedness for eachautomatically generated concept pair.
Test sub-jects were invited via email to participate in theexperiment.
Thus, they were not supervised dur-ing the experiment.Gurevych (2006) observed that some annotatorswere not familiar with the exact definition of se-mantic relatedness.
Their results differed particu-larly in cases of antonymy or distributionally re-lated pairs.
We created a manual with a detailedintroduction to SR stressing the crucial points.The manual was presented to the subjects beforethe experiment and could be re-accessed at anytime.Figure 2: Screenshot of the GUI.
Polysemouswords are defined by means of synonyms and re-lated words.During the experiment, one concept pair at atime was presented to the test subjects in randomordering.
Subjects had to assign a discrete related-ness value {0,1,2,3,4} to each pair.
Figure 2 showsthe system?s GUI.In case of a polysemous word, synonyms orrelated words were presented to enable test sub-jects to understand the sense of a presented con-cept.
Because this additional information can leadto undesirable priming effects, test subjects wereinstructed to deliberately decide only about the re-latedness of a concept pair and use the gloss solelyto understand the sense of the presented concept.Since our corpus-based approach includesdomain-specific vocabulary, we could not assumethat the subjects were familiar with all words.Thus, they were instructed to look up unknownwords in the German Wikipedia.9Several test subjects were asked to repeat theexperiment with a minimum break of one day.
Re-sults from the repetition can be used to measureintra-subject correlation.
They can also be usedto obtain some hints on varying difficulty of judg-ment for special concept pairs or parts-of-speech.5 Results and discussion21 test subjects (13 males, 8 females) participatedin the experiment, two of them repeated it.
Theaverage age of the subjects was 26 years.
Mostsubjects had an IT background.
The experimenttook 39 minutes on average, leaving about 7 sec-onds for rating each concept pair.The summarized inter-subject correlation be-tween 21 subjects was r=.478 (cf.
Table 3), which9http://www.wikipedia.de20CONCEPTS WORDSINTER INTRA INTER INTRAall .478 .647 .490 .675BN .469 .695 .501 .718GIRT .451 .598 .463 .625SPP .535 .649 .523 .679AA .556 .890 .597 .887NA .547 .773 .511 .758NV .510 .658 .540 .647NN .463 .620 .476 .661VA .317 .318 .391 .212VV .278 .494 .301 .476Table 3: Summarized correlation coefficients forall pairs, grouped by corpus and grouped by POScombinations.is statistically significant at p < .05.
This correla-tion coefficient is an upper bound of performancefor automatic SR measures applied on the samedataset.Resnik (1995) reported a correlation ofr=.9026.10 The results are not directly compara-ble, because he only used noun-noun pairs, wordsinstead of concepts, a much smaller dataset, andmeasured semantic similarity instead of semanticrelatedness.
Finkelstein et al (2002) did notreport inter-subject correlation for their largerdataset.
Gurevych (2006) reported a correlationof r=.69.
Test subjects were trained students ofcomputational linguistics, and word pairs wereselected analytically.Evaluating the influence of using concept pairsinstead of word pairs is complicated because wordlevel judgments are not directly available.
There-fore, we computed a lower and an upper boundfor correlation coefficients.
For the lower bound,we always selected the concept pair with higheststandard deviation from each set of correspondingconcept pairs.
The upper bound is computed byselecting the concept pair with the lowest standarddeviation.
The differences between correlation co-efficient for concepts and words are not signifi-cant.
Table 3 shows only the lower bounds.Correlation coefficients for experiments mea-suring semantic relatedness are expected to belower than results for semantic similarity, since theformer also includes additional relations (like co-occurrence of words) and is thus a more compli-cated task.
Judgments for such relations stronglydepend on experience and cultural background ofthe test subjects.
While most people may agree10Note that Resnik used the averaged correlation coeffi-cient.
We computed the summarized correlation coefficientusing a Fisher Z-value transformation.01234 050100150200250300350Concept pairSemantic relatedness scoreFigure 3: Distribution of averaged human judg-ments.01234 01020304050Concept pairSemantic relatedness scoresFigure 4: Distribution of averaged human judg-ments with standard deviation < 0.8.that (car ?
vehicle) are highly related, a strongconnection between (parts ?
speech) may only beestablished by a certain group.
Due to the corpus-based approach, many domain-specific conceptpairs are introduced into the test set.
Therefore,inter-subject correlation is lower than the resultsobtained by Gurevych (2006).In our experiment, intra-subject correlation wasr=.670 for the first and r=.623 for the second in-dividual who repeated the experiment, yieldinga summarized intra-subject correlation of r=.647.Rubenstein and Goodenough (1965) reported anintra-subject correlation of r=.85 for 15 subjectsjudging the similarity of a subset (36) of the orig-inal 65 word pairs.
The values may again not becompared directly.
Furthermore, we cannot gen-eralize from these results, because the number ofparticipants which repeated our experiment wastoo low.The distribution of averaged human judgmentson the whole test set (see Figure 3) is almost bal-anced with a slight underrepresentation of highlyrelated concepts.
To create more highly re-lated concept pairs, more sophisticated weightingschemes or selection on the basis of lexical chain-2100.30.60.91.21.51.8 01234AveragedjudgmentStandard deviationFigure 5: Averaged judgments and standard devia-tion for all concept pairs.
Low deviations are onlyobserved for low or high judgments.ing could be used.
However, even with the presentsetup, automatic extraction of concept pairs per-forms remarkably well and can be used to quicklycreate balanced test datasets.Budanitsky and Hirst (2006) pointed out thatdistribution plots of judgments for the word pairsused by Rubenstein and Goodenough display anempty horizontal band that could be used to sepa-rate related and unrelated pairs.
This empty bandis not observed here.
However, Figure 4 shows thedistribution of averaged judgments with the high-est agreement between annotators (standard devi-ation < 0.8).
The plot clearly shows an empty hor-izontal band with no judgments.
The connectionbetween averaged judgments and standard devia-tion is plotted in Figure 5.When analyzing the concept pairs with lowestdeviation there is a clear tendency for particularlyhighly related pairs, e.g.
hypernymy: Universit?t?
Bildungseinrichtung (university ?
educationalinstitution); functional relation: T?tigkeit ?
aus-f?hren (task ?
perform); or pairs that are obviouslynot connected, e.g.
logisch ?
Juni (logical ?
June).Table 4 lists some example concept pairs alongwith averaged judgments and standard deviation.Concept pairs with high deviations betweenjudgments often contain polysemous words.
Forexample, Quelle (source) was disambiguated toWasserquelle (spring) and paired with Text(text).
The data shows a clear distinction be-tween one group that rated the pair low (0) andanother group that rated the pair high (3 or 4).
Thelatter group obviously missed the point that tex-tual source was not an option here.
High devia-tions were also common among special technicalterms like (Mips ?Core), proper names (Georg ?August ?
two common first names in German) orfunctionally related pairs (agieren ?
mobil).
Hu-man experience and cultural background clearlyinfluence the judgment of such pairs.The effect observed here and the effect notedby Budanitsky and Hirst is probably caused by thesame underlying principle.
Human agreement onsemantic relatedness is only reliable if two wordsor concepts are highly related or almost unrelated.Intuitively, this means that classifying word pairsas related or unrelated is much easier than numeri-cally rating semantic relatedness.
For an informa-tion retrieval task, such a classification might besufficient.Differences in correlation coefficients for thethree corpora are not significant indicating that thephenomenon is not domain-specific.
Differencesin correlation coefficients for different parts-of-speech are significant (see Table 3).
Verb-verb andverb-adjective pairs have the lowest correlation.A high fraction of these pairs is in the problem-atic medium relatedness area.
Adjective-adjectivepairs have the highest correlation.
Most of thesepairs are either highly related or not related at all.6 ConclusionWe proposed a system for automatically creatingdatasets for evaluating semantic relatedness mea-sures.
We have shown that our corpus-based ap-proach enables fast development of large domain-specific datasets that cover all types of lexical andsemantic relations.
We conducted an experimentto obtain human judgments of semantic related-ness on concept pairs.
Results show that averagedhuman judgments cover all degrees of relatednesswith a slight underrepresentation of highly relatedconcept pairs.
More highly related concept pairscould be generated by using more sophisticatedweighting schemes or selecting concept pairs onthe basis of lexical chaining.Inter-subject correlation in this experiment islower than the results from previous studies dueto several reasons.
We measured semantic relat-edness instead of semantic similarity.
The for-mer is a more complicated task for annotators be-cause its definition includes all kinds of lexical-semantic relations not just synonymy.
In addition,concept pairs were automatically selected elimi-nating the bias towards strong classical relationswith high agreement that is introduced into thedataset by a manual selection process.
Further-more, our dataset contains many domain-specific22PAIRGERMAN ENGLISH CORPUS AVG ST-DEVUniversit?t ?
Bildungseinrichtung university ?
educational institution GIRT 3.90 0.30T?tigkeit ?
ausf?hren task ?
to perform BN 3.67 0.58strafen ?
Paragraph to punish ?
paragraph GIRT 3.00 1.18Quelle ?
Text spring ?
text GIRT 2.43 1.57Mips ?
Core mips ?
core SPP 2.10 1.55elektronisch ?
neu electronic ?
new GIRT 1.71 1.15verarbeiten ?
dichten to manipulate ?
to caulk BN 1.29 1.42Leopold ?
Institut Leopold ?
institute SPP 0.81 1.25Outfit ?
Strom outfit ?
electricity GIRT 0.24 0.44logisch ?
Juni logical ?
June SPP 0.14 0.48Table 4: Example concept pairs with averaged judgments and standard deviation.
Only one sense islisted for polysemous words.
Conceptual glosses are omitted due to space limitations.concept pairs which have been rated very differ-ently by test subjects depending on their expe-rience.
Future experiments should ensure thatdomain-specific pairs are judged by domain ex-perts to reduce disagreement between annotatorscaused by varying degrees of familiarity with thedomain.An analysis of the data shows that test sub-jects more often agreed on highly related or unre-lated concept pairs, while they often disagreed onpairs with a medium relatedness value.
This resultraises the question whether human judgments ofsemantic relatedness with medium scores are re-liable and should be used for evaluating seman-tic relatedness measures.
We plan to investigatethe impact of this outcome on the evaluation ofsemantic relatedness measures.
Additionally, forsome applications like information retrieval it maybe sufficient to detect highly related pairs ratherthan accurately rating word pairs with mediumvalues.There is also a significant difference betweenthe correlation coefficient for different POS com-binations.
Further investigations are needed to elu-cidate whether these differences are caused by thenew procedure for corpus-based selection of wordpairs proposed in this paper or are due to inherentproperties of semantic relations existing betweenword classes.AcknowledgmentsWe would like to thank Sabine Schulte im Waldefor her remarks on experimental setups.
We aregrateful to the Bundesagentur f?r Arbeit for pro-viding the BERUFEnet corpus.
This work wascarried out as part of the ?Semantic InformationRetrieval?
(SIR) project funded by the GermanResearch Foundation.ReferencesAlexander Budanitsky and Graeme Hirst.
2006.
EvaluatingWordNet-based Measures of Semantic Distance.
Compu-tational Linguistics, 32(1).Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, EhudRivlin, Zach Solan, and Gadi Wolfman.
2002.
PlacingSearch in Context: The Concept Revisited.
ACM Trans-actions on Information Systems, 20(1):116?131.Iryna Gurevych.
2005.
Using the Structure of a ConceptualNetwork in Computing Semantic Relatedness.
In Pro-ceedings of the 2nd International Joint Conference on Nat-ural Language Processing, pages 767?778, Jeju Island,Republic of Korea.Iryna Gurevych.
2006.
Computing Semantic RelatednessAcross Parts of Speech.
Technical report, Darmstadt Uni-versity of Technology, Germany, Department of ComputerScience, Telecooperation.Jay J. Jiang and David W. Conrath.
1997.
Semantic Similar-ity Based on Corpus Statistics and Lexical Taxonomy.
InProceedings of the 10th International Conference on Re-search in Computational Linguistics.Michael Kluck.
2004.
The GIRT Data in the Evaluation ofCLIR Systems - from 1997 Until 2003.
Lecture Notes inComputer Science, 3237:376?390, January.Claudia Kunze, 2004.
Lexikalisch-semantische Wortnetze,chapter Computerlinguistik und Sprachtechnologie, pages423?431.
Spektrum Akademischer Verlag.Claudia Leacock and Martin Chodorow, 1998.
WordNet: AnElectronic Lexical Database, chapter Combining LocalContext and WordNet Similarity for Word Sense Identi-fication, pages 265?283.
Cambridge: MIT Press.Ludovic Lebart and Martin Rajman.
2000.
Computing Sim-ilarity.
In Robert Dale, editor, Handbook of NLP.
Dekker:Basel.Michael Lesk.
1986.
Automatic Sense Disambiguation Us-ing Machine Readable Dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of the 5thAnnual International Conference on Systems Documenta-tion, pages 24?26, Toronto, Ontario, Canada.Dekang Lin.
1998.
An Information-Theoretic Definition ofSimilarity.
In Proceedings of International Conference onMachine Learning, Madison, Wisconsin.23Rada Mihalcea and Dan Moldovan.
2001.
Automatic Gen-eration of a Coarse Grained WordNet.
In Proceedingsof NAACL Workshop on WordNet and Other Lexical Re-sources, Pittsburgh, PA, June.George A. Miller and Walter G. Charles.
1991.
ContextualCorrelates of Semantic Similarity.
Language and Cogni-tive Processes, 6(1):1?28.Jane Morris and Graeme Hirst.
2004.
Non-Classical LexicalSemantic Relations.
In Workshop on Computational Lex-ical Semantics, Human Language Technology Conferenceof the North American Chapter of the ACL, Boston.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-sen. 2003.
Using Measures of Semantic Relatednessfor Word Sense Disambiguation.
In Proceedings of theFourth International Conference on Intelligent Text Pro-cessing and Computational Linguistics, Mexico City.Philip Resnik.
1995.
Using Information Content to Evalu-ate Semantic Similarity.
In Proceedings of the 14th Inter-national Joint Conference on Artificial Intelligence, pages448?453, Montreal, Canada.Herbert Rubenstein and John B. Goodenough.
1965.
Con-textual Correlates of Synonymy.
Communications of theACM, 8(10):627?633.Gerard Salton.
1989.
Automatic Text Processing: the Trans-formation, Analysis, and Retrieval of Information by Com-puter.
Addison-Wesley Longman Publishing, Boston,MA, USA.Helmut Schmid.
1995.
Probabilistic Part-of-Speech TaggingUsing Decision Trees.
In International Conference onNew Methods in Language Processing, Manchester, UK.Sabine Schulte im Walde and Alissa Melinger.
2005.
Iden-tifying Semantic Relations and Functional Properties ofHuman Verb Associations.
In Proceedings of the JointConference on Human Language Technology and Empiri-cal Methods in NLP, pages 612?619, Vancouver, Canada.SIR Project.
2006.
Project ?Semantic InformationRetrieval?.
URL http://www.cre-elearning.tu-darmstadt.de/elearning/sir/.Julie Weeds and David Weir.
2005.
Co-occurrence Retrieval:A Flexible Framework For Lexical Distributional Similar-ity.
Computational Linguistics, 31(4):439?475, Decem-ber.Zhibiao Wu and Martha Palmer.
1994.
Verb Semantics andLexical Selection.
In 32nd Annual Meeting of the ACL,pages 133?138, NewMexico State University, Las Cruces,New Mexico.24
