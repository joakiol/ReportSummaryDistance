Empirical Acquisition of Differentiating Relations from DefinitionsTom O?HaraDepartment of Computer ScienceNew Mexico State UniversityLas Cruces, NM 88003tomohara@cs.nmsu.eduJanyce WiebeDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260wiebe@cs.pitt.eduAbstractThis paper describes a new automatic approach forextracting conceptual distinctions from dictionarydefinitions.
A broad-coverage dependency parser isfirst used to extract the lexical relations from the def-initions.
Then the relations are disambiguated usingassociations learned from tagged corpora.
This con-trasts with earlier approaches using manually devel-oped rules for disambiguation.1 IntroductionLarge-scale lexicons for computational semantics of-ten lack sufficient distinguishing information for theconcepts serving to define words.
For example,WordNet (Miller, 1990) recently introduced new re-lations for domain category and location in Version2.0, along with 6,000+ instances; however, about38% of the noun synsets are still not explicitly dis-tinguished from sibling synsets.Work on the Extended WordNet project(Harabagiu et al, 1999) is achieving substan-tial progress in making the information in WordNetmore explicit.
The main goal is to transform thedefinitions into a logical form representation suit-able for drawing inferences; in addition, the contentwords in the definitions are being disambiguated.
Inthe logical form representation, separate predicatesare used for each preposition, as well as for someother functional words (e.g., conjunctions); thus,ambiguity in the underlying relations implicit inthe definitions is not being resolved.
The workdescribed here automates the process of relationdisambiguation.
This can be used to further thetransformation of WordNet into an explicit lexicalknowledge base.Earlier approaches to differentia extraction havepredominantly relied upon manually constructedpattern matching rules for extracting relations fromdictionary definitions (Vanderwende, 1996; Barrie`re,1997; Rus, 2002).
These rules can be very precise,but achieving broad-coverage can be difficult.
Herea broad coverage dependency parser is first used todetermine the syntactic relations that are presentamong the constituents in the sentence.
Then thesyntactic relations between sentential constituentsare converted into semantic relations between theunderlying concepts using statistical classification.Isolating the disambiguation step from the extrac-tion step in this manner allows for greater flexibil-ity over earlier approaches.
For example, differentparsers can be incorporated without having to re-work the disambiguation process.This paper is organized as follows: Section 2 de-tails the steps in extracting the initial relations fromthe definition parse.
Section 3 illustrates the disam-biguation process, the crucial part of this approach.Section 4 presents an evaluation of the relations thatare extracted from the WordNet definitions.
Lastly,Section 5 compares the approach to previous ap-proaches that have been tried.2 Differentia ExtractionThe approach to differentia extraction is entirely au-tomated.
This starts with using the Link GrammarParser (Sleator and Temperley, 1993), a dependencyparser, to determine the syntactic lexical relationsthat occur in the sentence.
Dictionary definitions areoften given in the form of sentence fragments withthe headword omitted.
For example, the definitionfor the beverage sense of ?wine?
is ?fermented juice(of grapes especially).?
Therefore, prior to runningthe definition analysis, the definitions are convertedinto complete sentences, using simple templates foreach part of speech.After parsing, a series of postprocessing steps isperformed prior to the extraction of the lexical rela-tions.
For the Link Parser, this mainly involves con-version of the binary dependencies into relational tu-ples and the realignment of the tuples around func-tion words.
The Link Parser outputs syntactic de-pendencies among words, punctuation, and sentenceboundary markers.
The parser uses quite specializedsyntactic relations, so these are converted into gen-eral ones prior to the extraction of the relational tu-ples.
For example, the relation A, which is used forpre-noun adjectives, is converted into modifies.
Fig-ure 1 illustrates the syntactic relations that wouldbe extracted, along with the original parser output.The syntactic relationships are first convertedinto relational tuples using the format ?source-word,relation-word , target-word?.
This conversion is per-formed by following the dependencies involving thecontent words, ignoring cases involving non-word el-ements (e.g., punctuation).
For example, the firsttuple extracted from the parse would be ?n:wine,Definition sentence:Wine is fermented juice (of grapes especially).Link Grammar parse:?/////, Wd, 1.
n:wine?
?/////, Xp, 10.
.??1.
n:wine, Ss, 2.
v:is??10.
., RW, 11.
/////??2.
v:is, Ost, 4.
n:juice??3.
v:fermented, A, 4.
n:juice??4.
n:juice, MXs, 6.
of??5.
(, Xd, 6.
of??6.
of, Jp, 7.
n:grapes??6.
of, Xc, 9.
)?Extracted relations:?1.
n:wine, 2. v:is, 4.
n:juice??3.
v:fermented, modifies-3-4, 4.
n:juice??4.
n:juice, 6. of, 7. n:grapes?Figure 1: Example for relation extraction.v:is , n:juice?.
Certain types of dependencies aretreated specially by converting the syntactic rela-tionships directly into a relational tuple involving aspecial relation-indicating word (e.g., ?modifies?
).The relational tuples extracted from the parseform the basis for the lexical relations derived fromthe definition.
Structural ambiguity resolution is notaddressed here, so the first parse returned is used.The remaining optional step assigns weights to therelations that are extracted.When using the relations in applications, it is de-sirable to have a measure of how relevant the re-lations are to the associated concepts.
One suchmeasure would be the degree to which the relationapplies to the concept being described as opposedto sibling concepts.
To account for this, cue validi-ties are used, borrowing from cognitive psychology(Smith and Medin, 1981).
Cue validities can be in-terpreted as probabilities indicating the degree towhich features apply to a given concept versus sim-ilar concepts (i.e., P (C|F )).Cue validities are estimated by calculating thepercentage of times that the feature is associatedwith a concept versus the total associations of con-trasting concepts.
This requires a means of deter-mining the set of contrasting concepts for a givenconcept.
The simplest way of doing this would be tojust select the set of sibling concepts (e.g., synsetssharing a common parent in WordNet).
However,due to the idiosyncratic way concepts are special-ized in knowledge bases, this likely would not includeconcepts intuitively considered as contrasting.To alleviate this problem the most-informative an-cestor will be used instead of the parent.
This isdetermined by selecting the ancestor that best bal-ances frequency of occurrence in a tagged corpuswith specificity.
This is similar to Resnik?s (1995)notion of most-informative subsumer for a pair ofconcepts.
In his approach, estimated frequencies forsynsets are percolated up the hierarchy, so that thefrequency always increases as one proceeds up thehierarchy.
Therefore the first common ancestor fora pair is the most-informative subsumer (i.e., hasmost information content).
Here attested frequen-cies from SemCor (Miller et al, 1994) are used, so allancestors are considered.
Specificity is accounted forby applying a scaling factor to the frequencies thatdecreases as one proceeds up the hierarchy.
Thus,?informative?
is used more in an intuitive sense ratherthan technical.More details on the extraction process and thesubsequent disambiguation can be found in (O?Hara,forthcoming).3 Differentia DisambiguationAfter the differentia properties have been extractedfrom a definition, the words for the relation sourceand object terms are disambiguated to order to re-duce vagueness in the relationships.
In addition, therelation types are converted from surface-level rela-tions (e.g., object) or relation-indicating words (e.g.,prepositions) into the underlying semantic relation.Since WordNet serves as the knowledge base be-ing targeted, term disambiguation involves select-ing the most appropriate synset for both the sourceand target terms.
The WordNet definitions have re-cently been sense-tagged as part of the ExtendedWordNet (Novischi, 2002), so these annotations areincorporated.
For other dictionaries, use of tradi-tional word-sense disambiguation algorithms wouldbe required.With the emphasis on corpus analysis in computa-tional linguistics, there has been shift away from re-lying on explicitly coded knowledge towards the useof knowledge inferred from naturally occurring text,in particular text that has been annotated by hu-mans to indicate phenomena of interest.
The PennTreebank version II (Marcus et al, 1994) providedthe first large-scale set of case role annotations forgeneral-purpose text.
These are very general rolesakin to Fillmore?s (1968) case roles.
The BerkeleyFrameNet (Fillmore et al, 2001) project providesthe most recent large-scale annotation of semanticroles.
These are at a much finer granularity thanthose in Treebank, so they should prove quite use-ful for applications learning detailed semantics fromcorpora.
O?Hara and Wiebe (2003) explain howboth inventories can be used for preposition disam-biguation.The goal of relation disambiguation is to deter-mine the underlying semantic role indicated by par-ticular words in a phrase or by word order.
Forrelations indicated directly by function words, thedisambiguation can be seen as a special case of word-sense disambiguation (WSD).
As an example, refin-ing the relationship ?
?dog?, ?with?
, ?ears??
into ?
?dog?,has-part , ?ears?
?, is equivalent to disambiguating thepreposition ?with,?
given that the senses are the dif-Local-context featuresPOS: part of speech of target wordPOS?i: part-of-speech of ith word to leftPOS+i: part-of-speech of ith word to rightWord: target wordform as isWord?i: ith word to the leftWord+i: ith word to the rightCollocational featuresWordColli: word collocation for sense iClass-based collocational featuresHyperColls: hypernym collocation for sense iFigure 2: Features for preposition classifier.ferent relations it can indicate.
For relations that areindicated implicitly (e.g., adjectival modification),other classification techniques would be required, re-flecting the more syntactic nature of the task.A straightforward approach for preposition disam-biguation would be to use standard WSD features,such as the parts-of-speech of surrounding wordsand, more importantly, collocations (e.g., lexical as-sociations).
Although this can be highly accurate,it tends to overfit the data and to generalize poorly.The latter is of particular concern here as the train-ing data is taken from a different genre (e.g., news-paper text rather than dictionary definitions).
Toovercome these problems, a class-based approach isused for the collocations, with WordNet high-levelsynsets as the source of the word classes.
Figure 2lists the features used for the classifier.For the application to differentia disambiguation,the classifiers learned over Treebank and FrameNetneed to be combined.
This can be done readily in acascaded fashion with the classifier for the most spe-cific relation inventory (i.e., FrameNet) being usedfirst and then the other classifiers being applied inturn whenever the classification is inconclusive.
Thishas the advantage that new resources can be in-tegrated into the combined relation classifier withminimal effort.
However, the resulting role inven-tory will likely be heterogeneous and might be proneto inconsistent classifications.
In addition, the roleinventory could change whenever new annotation re-sources are incorporated, making the differentia dis-ambiguation system less predictable.Alternatively, the annotations can be convertedinto a common inventory, and a separate relationclassifier induced over the resulting data.
This hasthe advantage that the target relation-type inven-tory remains stable whenever new sources of relationannotations are introduced.
The drawback howeveris that annotations from new resources must first bemapped into the common inventory before incorpo-ration.
The latter approach is employed here.
Thecommon inventory incorporates some of the generalrelation types defined by Gildea and Jurafsky (2002)for their experiments in classifying semantic rela-tions in FrameNet using a reduced inventory.Relation FrequencyTheme 0.316Goal 0.116Ground 0.080Category 0.069Agent 0.069Cause 0.061Manner 0.058Recipient 0.053Medium 0.039Characteristic 0.022Resource 0.021Means 0.021Source 0.019Path 0.017Experiencer 0.017Accompaniment 0.011Area 0.010Direction 0.001Table 1: Frequency of relations extracted.4 EvaluationThe evaluation discussed here assesses the quality ofthe information that would be added to the lexiconswith respect to relation disambiguation, which is thefocus of the research.
An application-oriented evalu-ation is discussed in (O?Hara, forthcoming), showinghow using the extracted information improves word-sense disambiguation.All the definitions from WordNet 1.7.1 were runthrough the differentia-extraction process.
This in-volved 111,223 synsets, of which 10,810 had prepro-cessing or parse-related errors leading to no relationsbeing extracted.
Table 1 shows the frequency of therelations in the output from the differentia extrac-tion process.
The most common relation used isTheme, which occurs four times as much comparedto the annotations.
It is usually annotated as thesense for ?of,?
which also occurs with roles Source,Category, Ground , Agent , Characteristic, and Expe-riencer .
Some of these represent subtle distinctions,so it is likely that the difference in the text genre iscausing the classifier to use the default more often.Four human judges were recruited to evaluate ran-dom samples of the relations that were extracted.
Toallow for inter-coder reliability analysis, each evalua-tor evaluated some samples that were also evaluatedby the others, half as part of a training phase andhalf after training.
In addition, they also evaluateda few samples that were manually corrected before-hand.
This provides a baseline against which theuncorrected results can be measured against.
Be-cause the research only addresses relations indicatedby prepositional phrases, the evaluation is restrictedto these cases.
Specifically, the judges rate the as-signment of relations to the prepositional phrases ona scale from 1 to 5, with 5 being an exact match.The evaluation is based on averaging the assess-Corrected#Cases 10#Scores 40Mean 3.225stdev 1.625Score 0.60Uncorrected#Cases 15#Scores 60Mean 3.033stdev 1.551Score 0.58Table 2: Mean assessment score for all ex-tracted relationships.
25 relationships were eachevaluated by 4 judges.
Mean gives the mean of theassessment ratings (from 1 to 5).
Score gives ratingsrelative to scale from 0 to 1.ment scores over the relationships.
Table 2 showsthe results from this evaluation, including the man-ually corrected as well as the uncorrected subsetsof the relationships.
For the corrected output, themean assessment value was 3.225, which translatesinto an overall score of 0.60.
For the uncorrected sys-tem output, the mean assessment value was 3.033,which translates into an overall score of 0.58.
Al-though the absolute score is not high, the system?soutput is generally acceptable, as the score for theuncorrected set of relationships is close to that of themanually corrected set.5 Related workMost of the work addressing differentia extractionhas relied upon manually constructed extractionrules (Vanderwende, 1996; Barrie`re, 1997; Rus,2002).
Here the emphasis is switched from transfor-mation patterns for extracting relations into statis-tical classification for relation disambiguation, giventagged corpora with examples.
This allows for bet-ter coverage at the expense of precision.
Note thatrelation disambiguation is not yet addressed in Ex-tended WordNet (Rus, 2002); for example, preposi-tions are treated as predicates in the logical formrepresentation.
Their extraction process is alsoclosely tied into the specifics of the parser, as a trans-formation rule is developed for each grammar rule.This work addresses the acquisition of conceptualdistinctions.
In principle, it can handle any levelof granularity given sufficient training data; how-ever, addressing distinctions at the level of near-synonyms (Edmonds and Hirst, 2002) might requirecustomized analysis for each cluster of nearly syn-onymous words.
Inkpen and Hirst (2001) discusshow this can be automated by analyzing specializedsynonymy dictionaries.
Decision lists of indicativekeywords are learned for the broad types of prag-matic distinctions, and these are then manually splitinto decision lists for more-specific distinctions.6 ConclusionWe have presented an empirical methodology forextracting information from dictionary definitions.This differs from previous approaches by using data-driven relation disambiguation, using FrameNet se-mantic roles annotations mapped into a reduced in-ventory.
All the definitions from WordNet 1.7.1 wereanalyzed using this process, and the results evalu-ated by four human judges.
The overall results werenot high, but the evaluation was comparable to re-lations that were manually corrected before coding.ReferencesC.
Barrie`re.
1997.
From Machine Readable Dictio-naries to a Lexical Knowledge Base of ConceptualGraphs.
Ph.D. thesis, Simon Fraser University.P.
Edmonds and G. Hirst.
2002.
Near-synonymyand lexical choice.
Computational Linguistics,28(2):105?144.C.
Fillmore, C. Wooters, and C. Baker.
2001.
Build-ing a large lexical databank which provides deepsemantics.
In Proc.
PACLIC-01.C.
Fillmore.
1968.
The case for case.
In E. Bachand R. Harms, editors, Universals in LinguisticTheory.
Holt, Rinehart and Winston, New York.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.S.
Harabagiu, G. Miller, and D. Moldovan.
1999.WordNet 2?A morphologically and semanticallyenhanced resource.
In Proc.
SIGLEX Workshop.D.
Inkpen and G. Hirst.
2001.
Building a lexicalknowledge-base of near-synonym differences.
InProc.
WordNet and Other Lexical Resources.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIn-tyre, et al 1994.
The Penn Treebank: Annotat-ing predicate argument structure.
In Proc.
ARPAHuman Language Technology Workshop.G.
Miller, M. Chodorow, S. Landes, C. Leacock, andR.
Thomas.
1994.
Using a semantic concordancefor sense identification.
In Proc.
ARPA HumanLanguage Technology Workshop.G.
Miller.
1990.
Introduction.
International Jour-nal of Lexicography, 3(4).A.
Novischi.
2002.
Accurate semantic annotationsvia pattern matching.
In Proc.
FLAIRS 2002.T.
O?Hara and J. Wiebe.
2003.
Preposition se-mantic classification via Penn Treebank andFrameNet.
In Proc.
CoNLL-03.T.
O?Hara.
forthcoming.
Empirical acquisition ofconceptual distinctions via dictionary definitions.Ph.D.
thesis, New Mexico State University.P.
Resnik.
1995.
Disambiguating noun groupingswith respect to WordNet senses.
In Proc.
WVLC.V.
Rus.
2002.
Logic Forms for WordNet Glosses.Ph.D.
thesis, Southern Methodist University.D.
Sleator and D. Temperley.
1993.
Parsing Englishwith a link grammar.
In Proc.
Workshop on Pars-ing Technologies.E.
Smith and D. Medin.
1981.
Categories and Con-cepts.
Harvard University Press, Cambridge, MA.L.
Vanderwende.
1996.
Understanding Noun Com-pounds using Semantic Information Extractedfrom On-Line Dictionaries.
Ph.D. thesis, George-town University.
