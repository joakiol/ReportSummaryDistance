Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?9,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsWhitepaper of NEWS 2012 Shared Task on Machine Transliteration?Min Zhang?, Haizhou Li?, Ming Liu?, A Kumaran?
?Institute for Infocomm Research, A*STAR, Singapore 138632{mzhang,hli,mliu}@i2r.a-star.edu.sg?Multilingual Systems Research, Microsoft Research IndiaA.Kumaran@microsoft.comAbstractTransliteration is defined as phonetictranslation of names across languages.Transliteration of Named Entities (NEs)is necessary in many applications, suchas machine translation, corpus alignment,cross-language IR, information extractionand automatic lexicon acquisition.
Allsuch systems call for high-performancetransliteration, which is the focus ofshared task in the NEWS 2012 workshop.The objective of the shared task is to pro-mote machine transliteration research byproviding a common benchmarking plat-form for the community to evaluate thestate-of-the-art technologies.1 Task DescriptionThe task is to develop machine transliteration sys-tem in one or more of the specified language pairsbeing considered for the task.
Each language pairconsists of a source and a target language.
Thetraining and development data sets released foreach language pair are to be used for developinga transliteration system in whatever way that theparticipants find appropriate.
At the evaluationtime, a test set of source names only would bereleased, on which the participants are expectedto produce a ranked list of transliteration candi-dates in another language (i.e.
n-best translitera-tions), and this will be evaluated using commonmetrics.
For every language pair the participantsmust submit at least one run that uses only thedata provided by the NEWS workshop organisersin a given language pair (designated as ?standard?run, primary submission).
Users may submit more?stanrard?
runs.
They may also submit several?non-standard?
runs for each language pair that?http://translit.i2r.a-star.edu.sg/news2012/use other data than those provided by the NEWS2012 workshop; such runs would be evaluated andreported separately.2 Important DatesResearch paper submission deadline 25 March 2012Shared taskRegistration opens 18 Jan 2012Registration closes 11 Mar 2012Training/Development data release 20 Jan 2012Test data release 12 Mar 2012Results Submission Due 16 Mar 2012Results Announcement 20 Mar 2012Task (short) Papers Due 25 Mar 2012For all submissionsAcceptance Notification 20 April 2012Camera-Ready Copy Deadline 30 April 2012Workshop Date 12/13/14 July 20123 Participation1.
Registration (18 Jan 2012)(a) NEWS Shared Task opens for registra-tion.
(b) Prospective participants are to register tothe NEWS Workshop homepage.2.
Training & Development Data (20 Jan 2012)(a) Registered participants are to obtaintraining and development data from theShared Task organiser and/or the desig-nated copyright owners of databases.
(b) All registered participants are requiredto participate in the evaluation of at leastone language pair, submit the results anda short paper and attend the workshop atACL 2012.3.
Test data (12 March 2012)1(a) The test data would be released on 12March 2012, and the participants have amaximum of 5 days to submit their re-sults in the expected format.
(b) One ?standard?
run must be submit-ted from every group on a given lan-guage pair.
Additional ?standard?
runsmay be submitted, up to 4 ?standard?runs in total.
However, the partici-pants must indicate one of the submit-ted ?standard?
runs as the ?primary sub-mission?.
The primary submission willbe used for the performance summary.In addition to the ?standard?
runs, more?non-standard?
runs may be submitted.In total, maximum 8 runs (up to 4 ?stan-dard?
runs plus up to 4 ?non-standard?runs) can be submitted from each groupon a registered language pair.
The defi-nition of ?standard?
and ?non-standard?runs is in Section 5.
(c) Any runs that are ?non-standard?
mustbe tagged as such.
(d) The test set is a list of names in sourcelanguage only.
Every group will pro-duce and submit a ranked list of translit-eration candidates in another languagefor each given name in the test set.Please note that this shared task is a?transliteration generation?
task, i.e.,given a name in a source language oneis supposed to generate one or moretransliterations in a target language.
Itis not the task of ?transliteration discov-ery?, i.e., given a name in the source lan-guage and a set of names in the targetlanguage evaluate how to find the ap-propriate names from the target set thatare transliterations of the given sourcename.4.
Results (20 March 2012)(a) On 20 March 2012, the evaluation re-sults would be announced and will bemade available on the Workshop web-site.
(b) Note that only the scores (in respectivemetrics) of the participating systems oneach language pairs would be published,and no explicit ranking of the participat-ing systems would be published.
(c) Note that this is a shared evaluation taskand not a competition; the results aremeant to be used to evaluate systems oncommon data set with common metrics,and not to rank the participating sys-tems.
While the participants can cite theperformance of their systems (scores onmetrics) from the workshop report, theyshould not use any ranking informationin their publications.
(d) Furthermore, all participants shouldagree not to reveal identities of otherparticipants in any of their publicationsunless you get permission from the otherrespective participants.
By default, allparticipants remain anonymous in pub-lished results, unless they indicate oth-erwise at the time of uploading their re-sults.
Note that the results of all systemswill be published, but the identities ofthose participants that choose not to dis-close their identity to other participantswill be masked.
As a result, in this case,your organisation name will still appearin the web site as one of participants, butit will not be linked explicitly to your re-sults.5.
Short Papers on Task (25 March 2012)(a) Each submitting site is required to sub-mit a 4-page system paper (short paper)for its submissions, including their ap-proach, data used and the results on ei-ther test set or development set or by n-fold cross validation on training set.
(b) The review of the system papers will bedone to improve paper quality and read-ability and make sure the authors?
ideasand methods can be understood by theworkshop participants.
We are aimingat accepting all system papers, and se-lected ones will be presented orally inthe NEWS 2012 workshop.
(c) All registered participants are requiredto register and attend the workshop tointroduce your work.
(d) All paper submission and review will bemanaged electronically through https://www.softconf.com/acl2012/news2012/.24 Language PairsThe tasks are to transliterate personal names orplace names from a source to a target language assummarised in Table 1.
NEWS 2012 Shared Taskoffers 14 evaluation subtasks, among them ChEnand ThEn are the back-transliteration of EnCh andEnTh tasks respectively.
NEWS 2012 releasestraining, development and testing data for each ofthe language pairs.
NEWS 2012 continues all lan-guage pairs that were evaluated in NEWS 2011.
Insuch cases, the training and development data inthe release of NEWS 2012 are the same as thosein NEWS 2011.
However, the test data in NEWS2012 are entirely new.Please note that in order to have an accuratestudy of the research progress of machine transla-tion technology, different from previous practice,the test/reference sets of NEWS 2011 are not re-leased to the research community.
Instead, weuse the test sets of NEWS 2011 as progress testsets in NEWS 2012.
NEWS 2012 participants arerequested to submit results on the NEWS 2012progress test sets (i.e., NEWS 2011 test sets).
Bydoing so, we would like to do comparison studiesby comparing the NEWS 2012 and NEWS 2011results on the progress test sets.
We hope that wecan have some insightful research findings in theprogress studies.The names given in the training sets for Chi-nese, Japanese, Korean, Thai and Persian lan-guages are Western names and their respectivetransliterations; the Japanese Name (in English)?
Japanese Kanji data set consists only of nativeJapanese names; the Arabic data set consists onlyof native Arabic names.
The Indic data set (Hindi,Tamil, Kannada, Bangla) consists of a mix of In-dian and Western names.Examples of transliteration:English ?
ChineseTimothy ??
?English ?
Japanese KatakanaHarrington ?????
?English ?
Korean HangulBennett ?
?
?Japanese name in English ?
Japanese KanjiAkihiro ??
?English ?
HindiSan Francisco ?
???????????????
?English ?
TamilLondon ?
?????
?English ?
KannadaTokyo ?
?????
?Arabic ?
Arabic name in English?Khalid???
?5 Standard DatabasesTraining Data (Parallel)Paired names between source and target lan-guages; size 7K ?
37K.Training Data is used for training a basictransliteration system.Development Data (Parallel)Paired names between source and target lan-guages; size 1K ?
2.8K.Development Data is in addition to the Train-ing data, which is used for system fine-tuningof parameters in case of need.
Participantsare allowed to use it as part of training data.Testing DataSource names only; size 1K ?
2K.This is a held-out set, which would be usedfor evaluating the quality of the translitera-tions.Progress Testing DataSource names only; size 0.6K ?
2.6K.This is the NEWS 2011 test set, it is held-outfor progress study.1.
Participants will need to obtain licenses fromthe respective copyright owners and/or agreeto the terms and conditions of use that aregiven on the downloading website (Li et al,2004; MSRI, 2010; CJKI, 2010).
NEWS2011 will provide the contact details of eachindividual database.
The data would be pro-vided in Unicode UTF-8 encoding, in XMLformat; the results are expected to be sub-mitted in UTF-8 encoding in XML format.The XML formats details are available in Ap-pendix A.2.
The data are provided in 3 sets as describedabove.3.
Name pairs are distributed as-is, as providedby the respective creators.3Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Progress Test 2012 TestWestern English Chinese Institute for Infocomm Research 37K 2.8K 2K 1K EnChWestern Chinese English Institute for Infocomm Research 28K 2.7K 2.2K 1K ChEnWestern English Korean Hangul CJK Institute 7K 1K 609 1K EnKoWestern English Japanese Katakana CJK Institute 26K 2K 1.8K 1K EnJaJapanese English Japanese Kanji CJK Institute 10K 2K 571 1K JnJkArabic Arabic English CJK Institute 27K 2.5K 2.6K 1K ArEnMixed English Hindi Microsoft Research India 12K 1K 1K 1K EnHiMixed English Tamil Microsoft Research India 10K 1K 1K 1K EnTaMixed English Kannada Microsoft Research India 10K 1K 1K 1K EnKaMixed English Bangla Microsoft Research India 13K 1K 1K 1K EnBaWestern English Thai NECTEC 27K 2K 2K 1K EnThWestern Thai English NECTEC 25K 2K 1.9K 1K ThEnWestern English Persian Sarvnaz Karimi / RMIT 10K 2K 2K 1K EnPeWestern English Hebrew Microsoft Research India 9.5K 1K 1K 1K EnHeTable 1: Source and target languages for the shared task on transliteration.
(a) While the databases are mostly man-ually checked, there may be still in-consistency (that is, non-standard usage,region-specific usage, errors, etc.)
or in-completeness (that is, not all right varia-tions may be covered).
(b) The participants may use any method tofurther clean up the data provided.i.
If they are cleaned up manually, weappeal that such data be providedback to the organisers for redistri-bution to all the participating groupsin that language pair; such sharingbenefits all participants, and furtherensures that the evaluation providesnormalisation with respect to dataquality.ii.
If automatic cleanup were used,such cleanup would be considered apart of the system fielded, and hencenot required to be shared with allparticipants.4.
Standard Runs We expect that the partici-pants to use only the data (parallel names)provided by the Shared Task for translitera-tion task for a ?standard?
run to ensure a fairevaluation.
One such run (using only the dataprovided by the shared task) is mandatory forall participants for a given language pair thatthey participate in.5.
Non-standard Runs If more data (either par-allel names data or monolingual data) wereused, then all such runs using extra data mustbe marked as ?non-standard?.
For such ?non-standard?
runs, it is required to disclose thesize and characteristics of the data used in thesystem paper.6.
A participant may submit a maximum of 8runs for a given language pair (including themandatory 1 ?standard?
run marked as ?pri-mary submission?
).6 Paper FormatPaper submissions to NEWS 2012 should followthe ACL 2012 paper submission policy, includ-ing paper format, blind review policy and title andauthor format convention.
Full papers (researchpaper) are in two-column format without exceed-ing eight (8) pages of content plus two (2) extrapage for references and short papers (task paper)are also in two-column format without exceedingfour (4) pages content plus two (2) extra page forreferences.
Submission must conform to the offi-cial ACL 2012 style guidelines.
For details, pleaserefer to the ACL 2012 website2.7 Evaluation MetricsWe plan to measure the quality of the translitera-tion task using the following 4 metrics.
We acceptup to 10 output candidates in a ranked list for eachinput entry.Since a given source name may have multiplecorrect target transliterations, all these alternativesare treated equally in the evaluation.
That is, anyof these alternatives are considered as a correcttransliteration, and the first correct transliterationin the ranked list is accepted as a correct hit.2http://www.ACL2012.org/4The following notation is further assumed:N : Total number of names (sourcewords) in the test setni : Number of reference transliterationsfor i-th name in the test set (ni ?
1)ri,j : j-th reference transliteration for i-thname in the test setci,k : k-th candidate transliteration (systemoutput) for i-th name in the test set(1 ?
k ?
10)Ki : Number of candidate transliterationsproduced by a transliteration system1.
Word Accuracy in Top-1 (ACC) Alsoknown as Word Error Rate, it measures correct-ness of the first transliteration candidate in the can-didate list produced by a transliteration system.ACC = 1 means that all top candidates are cor-rect transliterations i.e.
they match one of the ref-erences, and ACC = 0 means that none of the topcandidates are correct.ACC =1NN?i=1{1 if ?
ri,j : ri,j = ci,1;0 otherwise}(1)2.
Fuzziness in Top-1 (Mean F-score) Themean F-score measures how different, on average,the top transliteration candidate is from its closestreference.
F-score for each source word is a func-tion of Precision and Recall and equals 1 when thetop candidate matches one of the references, and0 when there are no common characters betweenthe candidate and any of the references.Precision and Recall are calculated based on thelength of the Longest Common Subsequence be-tween a candidate and a reference:LCS(c, r) =12(|c|+ |r| ?
ED(c, r)) (2)where ED is the edit distance and |x| is the lengthof x.
For example, the longest common subse-quence between ?abcd?
and ?afcde?
is ?acd?
andits length is 3.
The best matching reference, thatis, the reference for which the edit distance hasthe minimum, is taken for calculation.
If the bestmatching reference is given byri,m = argminj(ED(ci,1, ri,j)) (3)then Recall, Precision and F-score for i-th wordare calculated asRi =LCS(ci,1, ri,m)|ri,m|(4)Pi =LCS(ci,1, ri,m)|ci,1|(5)Fi = 2Ri ?
PiRi + Pi(6)?
The length is computed in distinct Unicodecharacters.?
No distinction is made on different charactertypes of a language (e.g., vowel vs. conso-nants vs. combining diereses etc.)3.
Mean Reciprocal Rank (MRR) Measurestraditional MRR for any right answer produced bythe system, from among the candidates.
1/MRRtells approximately the average rank of the correcttransliteration.
MRR closer to 1 implies that thecorrect answer is mostly produced close to the topof the n-best lists.RRi ={minj 1j if ?ri,j , ci,k : ri,j = ci,k;0 otherwise}(7)MRR =1NN?i=1RRi (8)4.
MAPref Measures tightly the precision in then-best candidates for i-th source name, for whichreference transliterations are available.
If all ofthe references are produced, then the MAP is 1.Let?s denote the number of correct candidates forthe i-th source word in k-best list as num(i, k).MAPref is then given byMAPref =1NN?i1ni(ni?k=1num(i, k))(9)8 Contact UsIf you have any questions about this share task andthe database, please email toMr.
Ming LiuInstitute for Infocomm Research (I2R),A*STAR1 Fusionopolis Way#08-05 South Tower, ConnexisSingapore 138632mliu@i2r.a-star.edu.sg5Dr.
Min ZhangInstitute for Infocomm Research (I2R),A*STAR1 Fusionopolis Way#08-05 South Tower, ConnexisSingapore 138632mzhang@i2r.a-star.edu.sgReferences[CJKI2010] CJKI.
2010.
CJK Institute.http://www.cjk.org/.
[Li et al2004] Haizhou Li, Min Zhang, and Jian Su.2004.
A joint source-channel model for machinetransliteration.
In Proc.
42nd ACL Annual Meeting,pages 159?166, Barcelona, Spain.
[MSRI2010] MSRI.
2010.
Microsoft Research India.http://research.microsoft.com/india.6A Training/Development Data?
File Naming Conventions:NEWS12 train XXYY nnnn.xmlNEWS12 dev XXYY nnnn.xmlNEWS12 test XXYY nnnn.xmlNEWS11 test XXYY nnnn.xml(progress test sets)?
XX: Source Language?
YY: Target Language?
nnnn: size of parallel/monolingualnames (?25K?, ?10000?, etc)?
File formats:All data will be made available in XML for-mats (Figure 1).?
Data Encoding Formats:The data will be in Unicode UTF-8 encod-ing files without byte-order mark, and in theXML format specified.B Submission of Results?
File Naming Conventions:You can give your files any name you like.During submission online you will need toindicate whether this submission belongs toa ?standard?
or ?non-standard?
run, and if itis a ?standard?
run, whether it is the primarysubmission.?
File formats:All data will be made available in XML for-mats (Figure 2).?
Data Encoding Formats:The results are expected to be submitted inUTF-8 encoded files without byte-order markonly, and in the XML format specified.7<?xml version="1.0" encoding="UTF-8"?><TransliterationCorpusCorpusID = "NEWS2012-Train-EnHi-25K"SourceLang = "English"TargetLang = "Hindi"CorpusType = "Train|Dev"CorpusSize = "25000"CorpusFormat = "UTF8"><Name ID=fl1fl><SourceName>eeeeee1</SourceName><TargetName ID="1">hhhhhh1_1</TargetName><TargetName ID="2">hhhhhh1_2</TargetName>...<TargetName ID="n">hhhhhh1_n</TargetName></Name><Name ID=fl2fl><SourceName>eeeeee2</SourceName><TargetName ID="1">hhhhhh2_1</TargetName><TargetName ID="2">hhhhhh2_2</TargetName>...<TargetName ID="m">hhhhhh2_m</TargetName></Name>...<!-- rest of the names to follow -->...</TransliterationCorpus>Figure 1: File: NEWS2012 Train EnHi 25K.xml8<?xml version="1.0" encoding="UTF-8"?><TransliterationTaskResultsSourceLang = "English"TargetLang = "Hindi"GroupID = "Trans University"RunID = "1"RunType = "Standard"Comments = "HMM Run with params: alpha=0.8 beta=1.25"><Name ID="1"><SourceName>eeeeee1</SourceName><TargetName ID="1">hhhhhh11</TargetName><TargetName ID="2">hhhhhh12</TargetName><TargetName ID="3">hhhhhh13</TargetName>...<TargetName ID="10">hhhhhh110</TargetName><!-- Participants to provide theirtop 10 candidate transliterations --></Name><Name ID="2"><SourceName>eeeeee2</SourceName><TargetName ID="1">hhhhhh21</TargetName><TargetName ID="2">hhhhhh22</TargetName><TargetName ID="3">hhhhhh23</TargetName>...<TargetName ID="10">hhhhhh110</TargetName><!-- Participants to provide theirtop 10 candidate transliterations --></Name>...<!-- All names in test corpus to follow -->...</TransliterationTaskResults>Figure 2: Example file: NEWS2012 EnHi TUniv 01 StdRunHMMBased.xml9
