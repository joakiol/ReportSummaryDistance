Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 133?136,New York, June 2006. c?2006 Association for Computational LinguisticsUsing Phrasal Patterns to Identify Discourse RelationsManami SaitoNagaoka University ofTechnologyNiigata, JP 9402188saito@nlp.nagaokaut.ac.jpKazuhide YamamotoNagaoka University ofTechnologyNiigata, JP 9402188yamamoto@fw.ipsj.or.jpSatoshi SekineNew York UniversityNew York, NY 10003sekine@cs.nyu.eduAbstractThis paper describes a system whichidentifies discourse relations between twosuccessive sentences in Japanese.
On topof the lexical information previouslyproposed, we used phrasal patterninformation.
Adding phrasal informationimproves the system's accuracy 12%,from 53% to 65%.1 IntroductionIdentifying discourse relations is important formany applications, such as text/conversationunderstanding, single/multi-documentsummarization and question answering.
(Marcuand Echihabi 2002) proposed a method to identifydiscourse relations between text segments usingNa?ve Bayes classifiers trained on a huge corpus.They showed that lexical pair informationextracted from massive amounts of data can have amajor impact.We developed a system which identifies thediscourse relation between two successivesentences in Japanese.
On top of the lexicalinformation previously proposed, we added phrasalpattern information.
A phrasal pattern includes atleast three phrases (bunsetsu segments) from twosentences, where function words are mandatoryand content words are optional.
For example, if thefirst sentence is ?X should have done Y?
and thesecond sentence is ?A did B?, then we found itvery likely that the discourse relation isCONTRAST (89% in our Japanese corpus).2 Discourse Relation DefinitionsThere have been many definitions of discourserelation, for example (Wolf 2005) and (Ichikawa1987) in Japanese.
We basically used Ichikawa?sclasses and categorized 167 cue phrases in theChaSen dictionary (IPADIC, Ver.2.7.0), as shownin Table 1.
Ambiguous cue phrases werecategorized into multiple classes.
There are 7classes, but the OTHER class will be ignored in thefollowing experiment, as its frequency is verysmall.Table 1.
Discourse relationsDiscourserelationExamples of cue phrase(English translation)Freq.
incorpus [%]ELABORATION and, also, then, moreover 43.0CONTRAST although, but, while 32.2CAUSE-EFFECTbecause, and so, thus,therefore 12.1EQUIVALENCE in fact, alternatively, similarly 6.0CHANGE-TOPICby the way, incidentally,and now, meanwhile, well 5.1EXAMPLE for example, for instance 1.5OTHER most of all, in general 0.23 Identification using Lexical InformationThe system has two components; one is to identifythe discourse relation using lexical information,described in this section, and the other is toidentify it using phrasal patterns, described in thenext section.A pair of words in two consecutive sentencescan be a clue to identify the discourse relation ofthose sentences.
For example, the CONTRASTrelation may hold between two sentences which133have antonyms, such as ?ideal?
and ?reality?
inExample 1.
Also, the EXAMPLE relation mayhold when the second sentence has hyponyms of aword in the first sentence.
For example, ?gift shop?,?department store?, and ?supermarket?
arehyponyms of ?store?
in Example 2.Ex1)a.
It is ideal that people all over the worldaccept independence and associate on anequal footing with each other.b.
(However,) Reality is not that simple.Ex2)a.
Every town has many stores.b.
(For example,) Gift shops, departmentstores, and supermarkets are the mainstores.In our experiment, we used a corpus from theWeb (about 20G of text) and 38 years ofnewspapers.
We extracted pairs of sentences inwhich an unambiguous discourse cue phraseappears at the beginning of the second sentence.We extracted about 1,300,000 sentence pairs fromthe Web and about 150,000 pairs from newspapers.300 pairs (50 of each discourse relation) were setaside as a test corpus.3.1 Extracting Word PairsWord pairs are extracted from two sentences; i.e.one word from each sentence.
In order to reducenoise, the words are restricted to common nouns,verbal nouns, verbs, and adjectives.
Also, the wordpairs are restricted to particular kinds of POScombinations in order to reduce the impact of wordpairs which are not expected to be useful indiscourse relation identification.
We confined thecombinations to the pairs involving the same partof speech and those between verb and adjective,and between verb and verbal noun.All of the extracted word pairs are used in baseform.
In addition, each word is annotated with apositive or negative label.
If a phrase segmentincludes negative words like ?not?, the words inthe same segment are annotated with a negativelabel.
Otherwise, words are annotated with apositive label.
We don?t consider double negatives.In Example 1-b, ?simple?
is annotated with anegative, as it includes ?not?
in the same segment.3.2 Score CalculationAll possible word pairs are extracted from thesentence pairs and the frequencies of pairs arecounted for each discourse relation.
For a new(test) sentence pair, two types of score arecalculated for each discourse relation based on allof the word pairs found in the two sentences.
Thescores are given by formulas (1) and (2).
HereFreq(dr, wp) is the frequency of word pair (wp) inthe discourse relation (dr).
Score1 is the fraction ofthe given discourse relation among all the wordpairs in the sentences.
Score2 incorporates anadjustment based on the rate (RateDR) of thediscourse relation in the corpus, i.e.
the thirdcolumn in Table 1.
The score actually comparesthe ratio of a discourse relation in the particularword pairs against the ratio in the entire corpus.
Ithelps the low frequency discourse relations getbetter scores.
( )( )?
?=wpdrwpwpdrFreqwpDRFreqDRScore,1 ),(,(1)( )( )DRwpdrwpRatewpdrFreqwpDRFreqDRScore ?= ?
?,2 ),(,(2)4 Identification using Phrasal PatternWe can sometimes identify the discourse relationbetween two sentences from fragments of the twosentences.
For example, the CONTRAST relationis likely to hold between the pair of fragments ?
?should have done ?.?
and ??
did ?.
?, and theEXAMPLE relation is likely to hold between thepair of fragments ?There is??
and ?Those are ?and so on.?.
Here ???
represents any sequence ofwords.
The above examples indicate that thediscourse relation between two sentences can berecognized using fragments of the sentences evenif there are no clues based on the sort of contentwords involved in the word pairs.
Accumulatingsuch fragments in Japanese, we observe that thesefragments actually form a phrasal pattern.
A phrase(bunsetsu) in Japanese is a basic component ofsentences, and consists of one or more contentwords and zero or more function words.
We134specify that a phrasal pattern contain at least threesubphrases, with at least one from each sentence.Each subphrase contains the function words of thephrase, and may also include accompanyingcontent words.
We describe the method to createpatterns in three steps using an example sentencepair (Example 3) which actually has theCONTRAST relation.Ex3)a.
?kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai.?
(No one knows whatfeeling she had in her mind.)b.
?sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai.?
(I think that she must haveneeded courage.
)1) Deleting unnecessary phrasesNoun modifiers using ?no?
(a typical particle for anoun modifier) are excised from the sentences, asthey are generally not useful to identify a discourserelation.
For example, in the compound phrase?kanozyo-no (her) kokoro (mind)?
in Example 3,the first phrase (her), which just modifies a noun(mind), is excised.
Also, all of the phrases whichmodify excised phrases, and all but the last phrasein a conjunctive clause are excised.2) Restricting phrasal patternIn order to avoid meaningless phrases, we restrictthe phrase participants to components matching thefollowing regular expression pattern.
Here, noun-xmeans all types of nouns except common nouns, i.e.verbal nouns, proper nouns, pronouns, etc.?
(noun-x | verb | adjective)?
(particle | auxiliaryverb | period)+$?, or ?adverb$?3) Combining phrases and selecting words in aphraseAll possible combinations of phrases including atleast one phrase from each sentence and at leastthree phrases in total are extracted from a pair ofsentences in order to build up phrasal patterns.
Foreach phrase which satisfies the regular expressionin 2), the subphrases to be used in phrasal patternsare selected based on the following four criteria (Ato D).
In each criterion, a sample of the resultpattern (using all the phrases in Example 3) isexpressed in bold face.
Note that it is quite difficultto translate those patterns into English as manyfunction words in Japanese are encoded as aposition in English.
We hope readers understandthe procedure intuitively.A) Use all components in each phrasekanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai.sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai.B) Remove verbal noun and proper nounkanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai.sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai.C) In addition, remove verb and adjectivekanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai.sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai.D) In addition, remove adverb and remaining nounkanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai.sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai.4.1 Score CalculationBy taking combinations of 3 or more subphrasesproduced as described above, 348 distinct patternscan be created for the sentences in Example 3; allof them are counted with frequency 1 for theCONTRAST relation.
Like the score calculationusing lexical information, we count the frequencyof patterns for each discourse relation over theentire corpus.
Patterns appearing more than 1000times are not used, as those are found not useful todistinguish discourse relations.The scores are calculated replacing Freq(dr,wp) in formulas (1) and (2) by Freq(dr, pp).
Here,pp is a phrasal pattern and Freq(dr, pp) is thenumber of times discourse relation dr connectssentences for which phrasal pattern pp is matched.These scores will be called Score3 and Score4,respectively.5 EvaluationThe system identifies one of six discourse relations,described in Table 1, for a test sentence pair.
Usingthe 300 sentence pairs set aside earlier (50 of eachdiscourse relation type), we ran two experimentsfor comparison purposes: one using only lexicalinformation, the other using phrasal patterns aswell.
In the experiment using only lexicalinformation, the system selects the relationmaximizing Score2 (this did better than Score1).
Inthe other, the system chooses a relation as follows:if one relation maximizes both Score1 and Score2,135choose that relation; else, if one relation maximizesboth Score3 and Score4, choose that relation; elsechoose the relation maximizing Score2.Table 2 shows the result.
For all discourse relations,the results using phrasal patterns are better or thesame.
When we consider the frequency ofdiscourse relations, i.e.
43% for ELABORATION,32% for CONTRAST etc., the weighted accuracywas 53% using only lexical information, which iscomparable to the similar experiment by (Marcuand Echihabi 2002) of 49.7%.
Using phrasalpatterns, the accuracy improves 12% to 65%.
Notethat the baseline accuracy (by always selecting themost frequent relation) is 43%, so the improvementis significant.Table 2.
The resultDiscourse relation Lexical info.
OnlyWith phrasalpatternELABORATION 44% (22/50) 52% (26/50)CONTRAST 62% (31/50) 86% (43/50)CAUSE-EFFECT 56% (28/50) 56% (28/50)EQUIVALENCE 58% (29/50) 58% (29/50)CHANGE-TOPIC 66% (33/50) 72% (36/50)EXAMPLE 56% (28/50) 60% (30/50)Total 57% (171/300) 64% (192/300)Weighted accuracy 53% 65%Since they are more frequent in the corpus,ELABORATION and CONTRAST are morelikely to be selected by Score1 or Score3.
Butadjusting the influence of rate bias using Score2and Score4, it sometimes identifies the otherrelations.The system makes many mistakes, but peoplealso may not be able to identify a discourserelation just using the two sentences if the cuephrase is deleted.
We asked three human subjects(two of them are not authors of this paper) to dothe same task.
The total (un-weighted) accuraciesare 63, 54 and 48%, which are about the same oreven lower than the system performance.
Note thatthe subjects are allowed to annotate more than onerelation (Actually, they did it for 3% to 30% of thedata).
If the correct relation is included amongtheir N choices, then 1/N is credited to the accuracycount.
We measured inter annotator agreements.The average of the inter-annotator agreements is69%.
We also measured the system performanceon the data where all three subjects identified thecorrect relation, or two of them identified thecorrect relation and so on (Table 3).
We can seethe correlation between the number of subjectswho answered correctly and the system accuracy.In short, we can observe from the result and theanalyses that the system works as well as a humandoes under the condition that only two sentencescan be read.Table 3.
Accuracy for different agreements# of  subjects correct 3 2 1 0System accuracy 71% 63% 60% 47%.6 ConclusionIn this paper, we proposed a system whichidentifies discourse relations between twosuccessive sentences in Japanese.
On top of thelexical information previously proposed, we usedphrasal pattern information.
Using phrasalinformation improves accuracy 12%, from 53% to65%.
The accuracy is comparable to humanperformance.
There are many future directions,which include 1) applying other machine learningmethods, 2) analyzing discourse relationcategorization strategy, and 3) including a longercontext beyond two sentences.AcknowledgementsThis research was partially supported by theNational Science Foundation under Grant IIS-00325657.
This paper does not necessarily reflectthe position of the U.S. Government.
We wouldlike to thank Prof. Ralph Grishman, New YorkUniversity, who provided useful suggestions anddiscussions.ReferencesDaniel Marcu and Abdessamad Echihabi.
2002.
AnUnsupervised Approach to Recognizing DiscourseRelations, Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, 368-375.Florian Wolf and Edward Gibson.
2005.
RepresentingDiscourse Coherence: A Corpus-Based Study,Computational Linguistics, 31(2):249-287.Takashi Ichikawa.
1978.
Syntactic Overview forJapanese Education, Kyo-iku publishing, 65-67 (inJapanese).136
