Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318?1327,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning Word-Class Lattices for Definition and Hypernym ExtractionRoberto Navigli and Paola VelardiDipartimento di InformaticaSapienza Universita` di Roma{navigli,velardi}@di.uniroma1.itAbstractDefinition extraction is the task of au-tomatically identifying definitional sen-tences within texts.
The task has provenuseful in many research areas includingontology learning, relation extraction andquestion answering.
However, current ap-proaches ?
mostly focused on lexico-syntactic patterns ?
suffer from both lowrecall and precision, as definitional sen-tences occur in highly variable syntacticstructures.
In this paper, we propose Word-Class Lattices (WCLs), a generalization ofword lattices that we use to model tex-tual definitions.
Lattices are learned froma dataset of definitions from Wikipedia.Our method is applied to the task of def-inition and hypernym extraction and com-pares favorably to other pattern general-ization methods proposed in the literature.1 IntroductionTextual definitions constitute a fundamentalsource to look up when the meaning of a term issought.
Definitions are usually collected in dictio-naries and domain glossaries for consultation pur-poses.
However, manually constructing and up-dating glossaries requires the cooperative effort ofa team of domain experts.
Further, in the presenceof new words or usages, and ?
even worse ?
newdomains, such resources are of no help.
Nonethe-less, terms are attested in texts and some (usuallyfew) of the sentences in which a term occurs aretypically definitional, that is they provide a formalexplanation for the term of interest.
While it is notfeasible to manually search texts for definitions,this task can be automatized by means of MachineLearning (ML) and Natural Language Processing(NLP) techniques.Automatic definition extraction is useful notonly in the construction of glossaries, but alsoin many other NLP tasks.
In ontology learning,definitions are used to create and enrich conceptswith textual information (Gangemi et al, 2003),and extract taxonomic and non-taxonomic rela-tions (Snow et al, 2004; Navigli and Velardi,2006; Navigli, 2009a).
Definitions are also har-vested in Question Answering to deal with ?whatis?
questions (Cui et al, 2007; Saggion, 2004).In eLearning, they are used to help students as-similate knowledge (Westerhout and Monachesi,2007), etc.Much of the current literature focuses on the useof lexico-syntactic patterns, inspired by Hearst?s(1992) seminal work.
However, these methodssuffer both from low recall and precision, as defi-nitional sentences occur in highly variable syntac-tic structures, and because the most frequent def-initional pattern ?
X is a Y ?
is inherently verynoisy.In this paper we propose a generalized form ofword lattices, called Word-Class Lattices (WCLs),as an alternative to lexico-syntactic pattern learn-ing.
A lattice is a directed acyclic graph (DAG), asubclass of non-deterministic finite state automata(NFA).
The lattice structure has the purpose ofpreserving the salient differences among distinctsequences, while eliminating redundant informa-tion.
In computational linguistics, lattices havebeen used to model in a compact way many se-quences of symbols, each representing an alter-native hypothesis.
Lattice-based methods differin the types of nodes (words, phonemes, con-cepts), the interpretation of links (representing ei-ther a sequential or hierarchical ordering betweennodes), their means of creation, and the scor-ing method used to extract the best consensusoutput from the lattice (Schroeder et al, 2009).In speech processing, phoneme or word lattices(Campbell et al, 2007; Mathias and Byrne, 2006;Collins et al, 2004) are used as an interface be-tween speech recognition and understanding.
Lat-1318tices are adopted also in Chinese word segmenta-tion (Jiang et al, 2008), decompounding in Ger-man (Dyer, 2009), and to represent classes oftranslation models in machine translation (Dyer etal., 2008; Schroeder et al, 2009).
In more com-plex text processing tasks, such as information re-trieval, information extraction and summarization,the use of word lattices has been postulated but isconsidered unrealistic because of the dimension ofthe hypothesis space.To reduce this problem, concept lattices havebeen proposed (Carpineto and Romano, 2005;Klein, 2008; Zhong et al, 2008).
Here links repre-sent hierarchical relations, rather than the sequen-tial order of symbols like in word/phoneme lat-tices, and nodes are clusters of salient words ag-gregated using synonymy, similarity, or subtreesof a thesaurus.
However, salient word selectionand aggregation is non-obvious and furthermoreit falls into word sense disambiguation, a notori-ously AI-hard problem (Navigli, 2009b).In definition extraction, the variability of pat-terns is higher than for ?traditional?
applicationsof lattices, such as translation and speech, how-ever not as high as in unconstrained sentences.The methodology that we propose to align patternsis based on the use of star (wildcard *) charac-ters to facilitate sentence clustering.
Each clus-ter of sentences is then generalized to a lattice ofword classes (each class being either a frequentword or a part of speech).
A key feature of ourapproach is its inherent ability to both identify def-initions and extract hypernyms.
The method istested on an annotated corpus of Wikipedia sen-tences and a large Web corpus, in order to demon-strate the independence of the method from theannotated dataset.
WCLs are shown to general-ize over lexico-syntactic patterns, and outperformwell-known approaches to definition and hyper-nym extraction.The paper is organized as follows: Section 2discusses related work, WCLs are introduced inSection 3 and illustrated by means of an examplein Section 4, experiments are presented in Section5.
We conclude the paper in Section 6.2 Related WorkDefinition Extraction.
A great deal of workis concerned with definition extraction in severallanguages (Klavans and Muresan, 2001; Storrerand Wellinghoff, 2006; Gaudio and Branco, 2007;Iftene et al, 2007; Westerhout and Monachesi,2007; Przepio?rkowski et al, 2007; Dego?rski etal., 2008).
The majority of these approaches usesymbolic methods that depend on lexico-syntacticpatterns or features, which are manually craftedor semi-automatically learned (Zhang and Jiang,2009; Hovy et al, 2003; Fahmi and Bouma, 2006;Westerhout, 2009).
Patterns are either very sim-ple sequences of words (e.g.
?refers to?, ?is de-fined as?, ?is a?)
or more complex sequences ofwords, parts of speech and chunks.
A fully au-tomated method is instead proposed by Borg etal.
(2009): they use genetic programming to learnsimple features to distinguish between definitionsand non-definitions, and then they apply a geneticalgorithm to learn individual weights of features.However, rules are learned for only one categoryof patterns, namely ?is?
patterns.
As we alreadyremarked, most methods suffer from both low re-call and precision, because definitional sentencesoccur in highly variable and potentially noisy syn-tactic structures.
Higher performance (around 60-70% F1-measure) is obtained only for specific do-mains (e.g., an ICT corpus) and patterns (Borg etal., 2009).Only few papers try to cope with the general-ity of patterns and domains in real-world corpora(like the Web).
In the GlossExtractor web-basedsystem (Velardi et al, 2008), to improve precisionwhile keeping pattern generality, candidates arepruned using more refined stylistic patterns andlexical filters.
Cui et al (2007) propose the useof probabilistic lexico-semantic patterns, calledsoft patterns, for definitional question answeringin the TREC contest1.
The authors describe twosoft matching models: one is based on an n-gramlanguage model (with the Expectation Maximiza-tion algorithm used to estimate the model param-eter), the other on Profile Hidden Markov Mod-els (PHMM).
Soft patterns generalize over lexico-syntactic ?hard?
patterns in that they allow a par-tial matching by calculating a generative degreeof match probability between the test instance andthe set of training instances.
Thanks to its gen-eralization power, this method is the most closelyrelated to our work, however the task of defini-tional question answering to which it is applied isslightly different from that of definition extraction,so a direct performance comparison is not possi-1Text REtrieval Conferences: http://trec.nist.gov1319ble2.
In fact, the TREC evaluation datasets cannotbe considered true definitions, but rather text frag-ments providing some relevant fact about a targetterm.
For example, sentences like: ?Bollywood isa Bombay-based film industry?
and ?700 or morefilms produced by India with 200 or more fromBollywood?
are both ?vital?
answers for the ques-tion ?Bollywood?, according to TREC classifica-tion, but the second sentence is not a definition.Hypernym Extraction.
The literature on hy-pernym extraction offers a higher variability ofmethods, from simple lexical patterns (Hearst,1992; Oakes, 2005) to statistical and machinelearning techniques (Agirre et al, 2000; Cara-ballo, 1999; Dolan et al, 1993; Sanfilippo andPoznan?ski, 1992; Ritter et al, 2009).
One of thehighest-coverage methods is proposed by Snow etal.
(2004).
They first search sentences that con-tain two terms which are known to be in a taxo-nomic relation (term pairs are taken from Word-Net (Miller et al, 1990)); then they parse the sen-tences, and automatically extract patterns from theparse trees.
Finally, they train a hypernym clas-sifer based on these features.
Lexico-syntactic pat-terns are generated for each sentence relating aterm to its hypernym, and a dependency parser isused to represent them.3 Word-Class Lattices3.1 PreliminariesNotion of definition.
In our work, we rely ona formal notion of textual definition.
Specifically,given a definition, e.g.
: ?In computer science, aclosure is a first-class function with free variablesthat are bound in the lexical environment?, we as-sume that it contains the following fields (Storrerand Wellinghoff, 2006):?
The DEFINIENDUM field (DF): this part ofthe definition includes the definiendum (thatis, the word being defined) and its modifiers(e.g., ?In computer science, a closure?);?
The DEFINITOR field (VF): it includes theverb phrase used to introduce the definition(e.g., ?is?
);2In the paper, a 55% recall and 34% precision is achievedwith the best experiment on TREC-13 data.
Furthermore, theclassifier of Cui et al (2007) is based on soft patterns but alsoon a bag-of-word relevance heuristic.
However, the relativeinfluence of the two methods on the final performance is notdiscussed.?
The DEFINIENS field (GF): it includes thegenus phrase (usually including the hyper-nym, e.g., ?a first-class function?);?
The REST field (RF): it includes additionalclauses that further specify the differentia ofthe definiendum with respect to its genus(e.g., ?with free variables that are bound inthe lexical environment?
).Further examples of definitional sentences an-notated with the above fields are shown in Table1.
For each sentence, the definiendum (that is, theword being defined) and its hypernym are markedin bold and italic, respectively.
Given the lexico-syntactic nature of the definition extraction mod-els we experiment with, training and test sentencesare part-of-speech tagged with the TreeTagger sys-tem, a part-of-speech tagger available for manylanguages (Schmid, 1995).Word Classes and Generalized Sentences.
Wenow introduce our notion of word class, on whichour learning model is based.
Let T be the setof training sentences, manually bracketed with theDF, VF, GF and RF fields.
We first determine theset F of words in T whose frequency is above athreshold ?
(e.g., the, a, is, of, refer, etc.).
In ourtraining sentences, we replace the term being de-fined with ?TARGET?, thus this frequent token isalso included in F .We use the set of frequent words F to generalizewords to ?word classes?.
We define a word classas either a word itself or its part of speech.
Givena sentence s = w1, w2, .
.
.
, w|s|, where wi is thei-th word of s, we generalize its words wi to wordclasses ?i as follows:?i ={wi if wi ?
FPOS(wi) otherwisethat is, a word wi is left unchanged if it occursfrequently in the training corpus (i.e., wi ?
F )or is transformed to its part of speech (POS(wi))otherwise.
As a result, we obtain a general-ized sentence s?
= ?1, ?2, .
.
.
, ?|s|.
For instance,given the first sentence in Table 1, we obtain thecorresponding generalized sentence: ?In NN, a?TARGET?
is a JJ NN?, where NN and JJ indicatethe noun and adjective classes, respectively.3.2 AlgorithmWe now describe our learning algorithm basedon Word-Class Lattices.
The algorithm consists ofthree steps:1320[In arts, a chiaroscuro]DF [is]VF [a monochrome picture]GF.
[In mathematics, a graph]DF [is]VF [a data structure]GF [that consists of .
.
.
]REST.
[In computer science, a pixel]DF [is]VF [a dot]GF [that is part of a computer image]REST.Table 1: Example definitions (defined terms are marked in bold face, their hypernyms in italic).?
Star patterns: each sentence in the trainingset is pre-processed and generalized to a starpattern.
For instance, ?In arts, a chiaroscurois a monochrome picture?
is transformed to?In *, a ?TARGET?
is a *?
(Section 3.2.1);?
Sentence clustering: the training sentencesare then clustered based on the star patternsto which they belong (Section 3.2.2);?
Word-Class Lattice construction: for eachsentence cluster, a WCL is created by meansof a greedy alignment algorithm (Section3.2.3).We present two variants of our WCL model,dealing either globally with the entire sentence orseparately with its definition fields (Section 3.2.4).The WCL models can then be used to classify anyinput sentence of interest (Section 3.2.5).3.2.1 Star PatternsLet T be the set of training sentences.
In this step,we associate a star pattern ?
(s) with each sentences ?
T .
To do so, let s ?
T be a sentence such thats = w1, w2, .
.
.
, w|s|, where wi is its i-th word.Given the set F of most frequent words in T (cf.Section 3.1), the star pattern ?
(s) associated withs is obtained by replacing with * all the wordswi 6?
F , that is all the tokens that are non-frequentwords.
For instance, given the sentence ?In arts,a chiaroscuro is a monochrome picture?, the cor-responding star pattern is ?In *, a ?TARGET?
is a*?, where ?TARGET?
is the defined term.Note that, here and in what follows, we discardthe sentence fragments tagged with the REST field,which is used only to delimit the core part of defi-nitional sentences.3.2.2 Sentence ClusteringIn the second step, we cluster the sentences in ourtraining set T based on their star patterns.
For-mally, let ?
= (?1, .
.
.
, ?m) be the set of starpatterns associated with the sentences in T .
Wecreate a clustering C = (C1, .
.
.
, Cm) such thatCi = {s ?
T : ?
(s) = ?i}, that is Ci contains allthe sentences whose star pattern is ?i.As an example, assume ?3 = ?In *, a?TARGET?
is a *?.
The sentences reported in Ta-ble 1 are all grouped into cluster C3.
We note thateach cluster Ci contains sentences whose degreeof variability is generally much lower than for anypair of sentences in T belonging to two differentclusters.3.2.3 Word-Class Lattice ConstructionFinally, the third step consists of the constructionof a Word-Class Lattice for each sentence cluster.Given such a cluster Ci ?
C, we apply a greedyalgorithm that iteratively constructs the WCL.Let Ci = {s1, s2, .
.
.
, s|Ci|} and considerits first sentence s1 = w11, w12, .
.
.
, w1|s1|(wjidenotes the i-th token of the j-th sentence).We first produce the corresponding general-ized sentence s?1 = ?11, ?12, .
.
.
, ?1|s1|(cf.
Sec-tion 3.1).
We then create a directed graphG = (V,E) such that V = {?11, .
.
.
, ?1|s1|} andE = {(?11, ?12), (?12, ?13), .
.
.
, (?1|s1|?1, ?1|s1|)}.Next, for the subsequent sentences in Ci, thatis, for each j = 2, .
.
.
, |Ci|, we determine thealignment between the sentence sj and eachsentence sk ?
Ci such that k < j based on thefollowing dynamic programming formulation(Cormen et al, 1990, pp.
314?319):Ma,b = max {Ma?1,b?1 +Sa,b,Ma,b?1,Ma?1,b}where a ?
{1, .
.
.
, |sk|} and b ?
{1, .
.
.
, |sj |},Sa,b is a score of the matching between the a-thtoken of sk and the b-th token of sj , and M0,0,M0,b and Ma,0 are initially set to 0 for all a and b.The matching score Sa,b is calculated on thegeneralized sentences s?k of sk and s?j of sj as fol-lows:Sa,b ={1 if ?ka = ?jb0 otherwisewhere ?ka and ?jb are the a-th and b-th word classesof s?k and s?j , respectively.
In other words, thematching score equals 1 if the a-th and the b-thtokens of the two original sentences have the sameword class.Finally, the alignment score between sk and sjis given by M|sk|,|sj |, which calculates the mini-1321InartssciencemathematicsNN1NN4computer, a ?TARGET?pixelgraphchiaroscurois amonochromeJJ NN2structurepicturedotNN3dataFigure 1: The Word-Class Lattice for the sentences in Table 1.
The support of each word class is reportedbeside the corresponding node.mal number of misalignments between the two to-ken sequences.
We repeat this calculation for eachsentence sk (k = 1, .
.
.
, j ?
1) and choose theone that maximizes its alignment score with sj .We then use the best alignment to add sj to thegraph G. Such alignment is obtained by meansof backtracking from M|sk|,|sj | to M0,0.
We addto the set of vertices V the tokens of the gen-eralized sentence s?j for which there is no align-ment to s?k and we add to E the edges (?j1, ?j2),.
.
.
, (?j|sj |?1, ?j|sj |).
Furthermore, in the final lat-tice, nodes associated with the hypernym words inthe learning sentences are marked as hypernymsin order to be able to determine the hypernym of atest sentence at classification time.3.2.4 Variants of the WCL ModelSo far, we have assumed that our WCL modellearns lattices from the training sentences intheir entirety (we call this model WCL-1).
Wenow propose a second model that learns separateWCLs for each field of the definition, namely:the DEFINIENDUM (DF), DEFINITOR (VF) andDEFINIENS (GF) fields (see Section 3.1).
We re-fer to this latter model as WCL-3.
Rather than ap-plying the WCL algorithm to the entire sentence,the very same method is applied to the sentencefragments tagged with one of the three definitionfields.
The reason for introducing the WCL-3model is that, while definitional patterns are highlyvariable, DF, VF and GF individually exhibit alower variability, thus WCL-3 should improve thegeneralization power.3.2.5 ClassificationOnce the learning process is over, a set of WCLs isproduced.
Given a test sentence s, the classifica-tion phase for the WCL-1 model consists of deter-mining whether it exists a lattice that matches s. Inthe case of WCL-3, we consider any combinationof DEFINIENDUM, DEFINITOR and DEFINIENSlattices.
While WCL-1 is applied as a yes-no clas-sifier as there is a single WCL that can possiblymatch the input sentence, WCL-3 selects, if any,the combination of the three WCLs that best fitsthe sentence.
In fact, choosing the most appro-priate combination of lattices impacts the perfor-mance of hypernym extraction.
The best combi-nation of WCLs is selected by maximizing the fol-lowing confidence score:score(s, lDF, lVF, lGF) = coverage ?
log(support)where s is the candidate sentence, lDF, lVF and lGFare three lattices one for each definition field, cov-erage is the fraction of words of the input sentencecovered by the three lattices, and support is thesum of the number of sentences in the star patternscorresponding to the three lattices.Finally, when a sentence is classified as a def-inition, its hypernym is extracted by selecting thewords in the input sentence that are marked as ?hy-pernyms?
in the WCL-1 lattice (or in the WCL-3GF lattice).4 ExampleAs an example, consider the definitions in Table1.
As illustrated in Section 3.2.2, their star pat-tern is ?In *, a ?TARGET?
is a *?.
The corre-sponding WCL is built as follows: the first part-of-speech tagged sentence, ?In/IN arts/NN , a/DT?TARGET?/NN is/VBZ a/DT monochrome/JJ pic-ture/NN?, is considered.
The corresponding gen-eralized sentence is ?In NN , a ?TARGET?
is aJJ NN?.
The initially empty graph is thus popu-lated with one node for each word class and oneedge for each pair of consecutive tokens, as shownin Figure 1 (the central sequence of nodes in thegraph).
Note that we draw the hypernym tokenNN2 with a rectangle shape.
We also add to the1322graph a start node ?
and an end node ?
?, and con-nect them to the corresponding initial and finalsentence tokens.
Next, the second sentence, ?Inmathematics, a graph is a data structure that con-sists of...?, is aligned to the first sentence.
Thealignment of the generalized sentence is perfect,apart from the NN3 node corresponding to ?data?.The node is added to the graph together with theedges a?
NN3 and NN3 ?
NN2 .
Finally, thethird sentence in Table 1, ?In computer science, apixel is a dot that is part of a computer image?,is generalized as ?In NN NN , a ?TARGET?
isa NN?.
Thus, a new node NN4 is added, corre-sponding to ?computer?
and new edges are added:In?NN4 and NN4?NN1.
Figure 1 shows the re-sulting WCL-1 lattice.5 Experiments5.1 Experimental SetupDatasets.
We conducted experiments on twodifferent datasets:?
A corpus of 4,619 Wikipedia sentences, thatcontains 1,908 definitional and 2,711 non-definitional sentences.
The former were ob-tained from a random selection of the firstsentences of Wikipedia articles3.
The de-fined terms belong to different Wikipediadomain categories4, so as to capture arepresentative and cross-domain sample oflexical and syntactic patterns for defini-tions.
These sentences were manually an-notated with DEFINIENDUM, DEFINITOR,DEFINIENS and REST fields by an expertannotator, who also marked the hypernyms.The associated set of negative examples(?syntactically plausible?
false definitions)was obtained by extracting from the sameWikipedia articles sentences in which thepage title occurs.?
A subset of the ukWaC Web corpus (Fer-raresi et al, 2008), a large corpus of the En-glish language constructed by crawling the.uk domain of the Web.
The subset includesover 300,000 sentences in which occur anyof 239 terms selected from the terminologyof four different domains (COMPUTER SCI-3The first sentence of Wikipedia entries is, in the largemajority of cases, a definition of the page title.4en.wikipedia.org/wiki/Wikipedia:Cate-goriesENCE, ASTRONOMY, CARDIOLOGY, AVIA-TION).The reason for using the ukWaC corpus is that, un-like the ?clean?
Wikipedia dataset, in which rel-atively simple patterns can achieve good results,ukWaC represents a real-world test, with manycomplex cases.
For example, there are sentencesthat should be classified as definitional accordingto Section 3.1 but are rather uninformative, like?dynamic programming was the brainchild of anamerican mathematician?, as well as informativesentences that are not definitional (e.g., they do nothave a hypernym), like ?cubism was characterisedby muted colours and fragmented images?.
Evenmore frequently, the dataset includes sentenceswhich are not definitions but have a definitionalpattern (?A Pacific Northwest tribe?s saga refers toa young woman who [..]?
), or sentences with verycomplex definitional patterns (?white body cellsare the body?s clean up squad?
and ?joule is alsoan expression of electric energy?).
These cases canbe correctly handled only with fine-grained pat-terns.
Additional details on the corpus and a morethorough linguistic analysis of complex cases canbe found in Navigli et al (2010).Systems.
For definition extraction, we experi-ment with the following systems:?
WCL-1 and WCL-3: these two classifiersare based on our Word-Class Lattice model.WCL-1 learns from the training set a latticefor each cluster of sentences, whereas WCL-3 identifies clusters (and lattices) separatelyfor each sentence field (DEFINIENDUM,DEFINITOR and DEFINIENS) and classifies asentence as a definition if any combinationfrom the three sets of lattices matches (cf.Section 3.2.4, the best combination is se-lected).?
Star patterns: a simple classifier based onthe patterns learned as a result of step 1 of ourWCL learning algorithm (cf.
Section 3.2.1):a sentence is classified as a definition if itmatches any of the star patterns in the model.?
Bigrams: an implementation of the bigramclassifier for soft pattern matching proposedby Cui et al (2007).
The classifier selects asdefinitions all the sentences whose probabil-ity is above a specific threshold.
The proba-bility is calculated as a mixture of bigram and1323Algorithm P R F1 AWCL-1 99.88 42.09 59.22 76.06WCL-3 98.81 60.74 75.23 83.48Star patterns 86.74 66.14 75.05 81.84Bigrams 66.70 82.70 73.84 75.80Random BL 50.00 50.00 50.00 50.00Table 2: Performance on the Wikipedia dataset.unigram probabilities, with Laplace smooth-ing on the latter.
We use the very same set-tings of Cui et al (2007), including thresholdvalues.
While the authors propose a secondsoft-pattern approach based on Profile HMM(cf.
Section 2), their results do not show sig-nificant improvements over the bigram lan-guage model.For hypernym extraction, we compared WCL-1 and WCL-3 with Hearst?s patterns, a systemthat extracts hypernyms from sentences based onthe lexico-syntactic patterns specified in Hearst?sseminal work (1992).
These include (hypernymin italic): ?such NP as {NP ,} {(or | and)} NP?,?NP {, NP} {,} or other NP?, ?NP {,} includ-ing { NP ,} {or | and} NP?, ?NP {,} especially {NP ,} {or | and} NP?, and variants thereof.
How-ever, it should be noted that hypernym extractionmethods in the literature do not extract hypernymsfrom definitional sentences, like we do, but ratherfrom specific patterns like ?X such as Y?.
There-fore a direct comparison with these methods is notpossible.
Nonetheless, we decided to implementHearst?s patterns for the sake of completeness.
Wecould not replicate the more refined approach bySnow et al (2004) because it requires the annota-tion of a possibly very large dataset of sentencefragments.
In any case Snow et al (2004) re-ported the following performance figures on a cor-pus of dimension and complexity comparable withukWaC: the recall-precision graph indicates preci-sion 85% at recall 10% and precision 25% at re-call of 30% for the hypernym classifier.
A variantof the classifier that includes evidence from coor-dinate terms (terms with a common ancestor in ataxonomy) obtains an increased precision of 35%at recall 30%.
We see no reasons why these figuresshould vary dramatically on the ukWaC.Finally, we compare all systems with the ran-dom baseline, that classifies a sentence as a defi-nition with probability 12 .Algorithm P R?WCL-1 98.33 39.39WCL-3 94.87 56.57Star patterns 44.01 63.63Bigrams 46.60 45.45Random BL 50.00 50.00Table 3: Performance on the ukWaC dataset (?
Re-call is estimated).Measures.
To assess the performance of oursystems, we calculated the following measures:?
precision ?
the number of definitional sen-tences correctly retrieved by the system overthe number of sentences marked by the sys-tem as definitional.?
recall ?
the number of definitional sen-tences correctly retrieved by the system overthe number of definitional sentences in thedataset.?
the F1-measure ?
a harmonic mean of preci-sion (P) and recall (R) given by 2PRP+R .?
accuracy ?
the number of correctly classi-fied sentences (either as definitional or non-definitional) over the total number of sen-tences in the dataset.5.2 Results and DiscussionDefinition Extraction.
In Table 2 we reportthe results of definition extraction systems on theWikipedia dataset.
Given this dataset is also usedfor training, experiments are performed with 10-fold cross validation.
The results show very highprecision for WCL-1, WCL-3 (around 99%) andstar patterns (86%).
As expected, bigrams and starpatterns exhibit a higher recall (82% and 66%, re-spectively).
The lower recall of WCL-1 is due toits limited ability to generalize compared to WCL-3 and the other methods.
In terms of F1-measure,star patterns and WCL-3 achieve 75%, and arethus the best systems.
Similar performance is ob-served when we also account for negative sen-tences ?
that is we calculate accuracy (with WCL-3 performing better).
All the systems perform sig-nificantly better than the random baseline.From our Wikipedia corpus, we learned over1,000 lattices (and star patterns).
Using WCL-3, we learned 381 DF, 252 VF and 395 GF lat-tices, that then we used to extract definitions from1324Algorithm Full SubstringWCL-1 42.75 77.00WCL-3 40.73 78.58Table 4: Precision in hypernym extraction on theWikipedia datasetthe ukWaC dataset.
To calculate precision on thisdataset, we manually validated the definitions out-put by each system.
However, given the large sizeof the test set, recall could only be estimated.
Tothis end, we manually analyzed 50,000 sentencesand identified 99 definitions, against which recallwas calculated.
The results are shown in Table 3.On the ukWaC dataset, WCL-3 performs best, ob-taining 94.87% precision and 56.57% recall (wedid not calculate F1, as recall is estimated).
In-terestingly, star patterns obtain only 44% preci-sion and around 63% recall.
Bigrams achieveeven lower performance, namely 46.60% preci-sion, 45.45% recall.
The reason for such badperformance on ukWaC is due to the very dif-ferent nature of the two datasets: for example, inWikipedia most ?is a?
sentences are definitional,whereas this property is not verified in the realworld (that is, on the Web, of which ukWaC isa sample).
Also, while WCL does not need anyparameter tuning5, the same does not hold for bi-grams6, whose probability threshold and mixtureweights need to be best tuned on the task at hand.Hypernym Extraction.
For hypernym extrac-tion, we tested WCL-1, WCL-3 and Hearst?s pat-terns.
Precision results are reported in Tables 4and 5 for the two datasets, respectively.
The Sub-string column refers to the case in which the cap-tured hypernym is a substring of what the annota-tor considered to be the correct hypernym.
Noticethat this is a complex matter, because often the se-lection of a hypernym depends on semantic andcontextual issues.
For example, ?Fluoroscopy isan imaging method?
and ?the Mosaic was an in-teresting project?
have precisely the same genuspattern, but (probably depending on the vaguenessof the noun in the first sentence, and of the adjec-tive in the second) the annotator selected respec-5WCL has only one threshold value ?
to be set for deter-mining frequent words (cf.
Section 3.1).
However, no tuningwas made for choosing the best value of ?.6We had to re-tune the system parameters on ukWaC,since with the original settings of Cui et al (2007) perfor-mance was much lower.Algorithm Full SubstringWCL-1 86.19 (206) 96.23 (230)WCL-3 89.27 (383) 96.27 (413)Hearst 65.26 (62) 88.42 (84)Table 5: Precision in hypernym extraction on theukWaC dataset (number of hypernyms in paren-theses).tively imaging method and project as hypernyms.For the above reasons it is difficult to achieve highperformance in capturing the correct hypernym(e.g.
40.73% with WCL-3 on Wikipedia).
How-ever, our performance of identifying a substringof the correct hypernym is much higher (around78.58%).
In Table 4 we do not report the preci-sion of Hearst?s patterns, as only one hypernymwas found, due to the inherently low coverage ofthe method.On the ukWaC dataset, the hypernyms returnedby the three systems were manually validated andprecision was calculated.
Both WCL-1 and WCL-3 obtained a very high precision (86-89% and 96%in identifying the exact hypernym and a substringof it, respectively).
Both WCL models are thusequally robust in identifying hypernyms, whereasWCL-1 suffers from a lack of generalization indefinition extraction (cf.
Tables 2 and 3).
Also,given that the ukWaC dataset contains sentencesin which any of 239 domain terms occur, WCL-3extracts on average 1.6 and 1.7 full and substringhypernyms per term, respectively.
Hearst?s pat-terns also obtain high precision, especially whensubstrings are taken into account.
However, thenumber of hypernyms returned by this method ismuch lower, due to the specificity of the patterns(62 vs. 383 hypernyms returned by WCL-3).6 ConclusionsIn this paper, we have presented a lattice-based ap-proach to definition and hypernym extraction.
Thenovelty of our approach is:1.
The use of a lattice structure to generalizeover lexico-syntactic definitional patterns;2.
The ability of the system to jointly identifydefinitions and extract hypernyms;3.
The generality of the method, which appliesto generic Web documents in any domain andstyle, and needs no parameter tuning;13254.
The high performance as compared with thebest-known methods for both definition andhypernym extraction.
Our approach outper-forms the other systems particularly wherethe task is more complex, as in real-worlddocuments (i.e., the ukWaC corpus).Even though definitional patterns are learnedfrom a manually annotated dataset, the dimensionand heterogeneity of the training dataset ensuresthat training needs not to be repeated for specificdomains7, as demonstrated by the cross-domainevaluation on the ukWaC corpus.The datasets used in our experiments are avail-able from http://lcl.uniroma1.it/wcl.We also plan to release our system to the researchcommunity.
In the near future, we aim to apply theoutput of our classifiers to the task of automatedtaxonomy building, and to test the WCL approachon other information extraction tasks, like hyper-nym extraction from generic sentence fragments,as in Snow et al (2004).ReferencesEneko Agirre, Ansa Olatz, Xabier Arregi, Xabier Ar-tola, Arantza Daz de Ilarraza Snchez, Mikel Ler-sundi, David Martnez, Kepa Sarasola, and RubenUrizar.
2000.
Extraction of semantic relations froma basque monolingual dictionary using constraintgrammar.
In Proceedings of Euralex.Claudia Borg, Mike Rosner, and Gordon Pace.
2009.Evolutionary algorithms for definition extraction.
InProceedings of the 1st Workshop on Definition Ex-traction 2009 (wDE?09).William M. Campbell, M. F. Richardson, and D. A.Reynolds.
2007.
Language recognition with wordlattices and support vector machines.
In Proceed-ings of the IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP 2007),pages 989?992, Honolulu, HI.Sharon A. Caraballo.
1999.
Automatic constructionof a hypernym-labeled noun hierarchy from text.
InProceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages120?126, Maryland, USA.Claudio Carpineto and Giovanni Romano.
2005.
Us-ing concept lattices for text retrieval and mining.
InB.
Ganter, G. Stumme, and R. Wille, editors, FormalConcept Analysis, pages 161?179.Christopher Collins, Bob Carpenter, and Gerald Penn.2004.
Head-driven parsing for word lattices.
In Pro-ceedings of the 42nd Meeting of the Association for7Of course, it would need some additional work if appliedto languages other than English.
However, the approach doesnot need to be adapted to the language of interest.Computational Linguistics (ACL?04), Main Volume,pages 231?238, Barcelona, Spain, July.Thomas H. Cormen, Charles E. Leiserson, andRonald L. Rivest.
1990.
Introduction to algorithms.the MIT Electrical Engineering and Computer Sci-ence Series.
MIT Press, Cambridge, MA.Hang Cui, Min-Yen Kan, and Tat-Seng Chua.
2007.Soft pattern matching models for definitional ques-tion answering.
ACM Transactions on InformationSystems, 25(2):8.?ukasz Dego?rski, Micha?
Marcinczuk, and AdamPrzepio?rkowski.
2008.
Definition extraction us-ing a sequential combination of baseline grammarsand machine learning classifiers.
In Proceedings ofthe Sixth International Conference on Language Re-sources and Evaluation (LREC 2008), Marrakech,Morocco.William Dolan, Lucy Vanderwende, and Stephen D.Richardson.
1993.
Automatically deriving struc-tured knowledge bases from on-line dictionaries.
InProceedings of the First Conference of the PacificAssociation for Computational Linguistics, pages 5?14.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice translation.In Proceedings of the Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2008),pages 1012?1020, Columbus, Ohio, USA.Christopher Dyer.
2009.
Using a maximum en-tropy model to build segmentation lattices for mt.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (HLT-NAACL 2009), pages 406?414, Boul-der, Colorado, USA.Ismail Fahmi and Gosse Bouma.
2006.
Learning toidentify definitions using syntactic features.
In Pro-ceedings of the EACL 2006 workshop on LearningStructured Information in Natural Language Appli-cations, pages 64?71, Trento, Italy.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukwac, a very large Web-derived corpus of english.In Proceedings of the 4th Web as Corpus Workshop(WAC-4), Marrakech, Morocco.Aldo Gangemi, Roberto Navigli, and Paola Velardi.2003.
The OntoWordNet project: Extension and ax-iomatization of conceptual relations in WordNet.
InProceedings of the International Conference on On-tologies, Databases and Applications of SEmantics(ODBASE 2003), pages 820?838, Catania, Italy.Rosa Del Gaudio and Anto?nio Branco.
2007.
Auto-matic extraction of definitions in portuguese: A rule-based approach.
In Proceedings of the TeMa Work-shop.Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceed-ings of the 14th International Conference on Com-putational Linguistics (COLING), pages 539?545,Nantes, France.1326Eduard Hovy, Andrew Philpot, Judith Klavans, UlrichGermann, and Peter T. Davis.
2003.
Extendingmetadata definitions by automatically extracting andorganizing glossary definitions.
In Proceedings ofthe 2003 Annual National Conference on DigitalGovernment Research, pages 1?6.
Digital Govern-ment Society of North America.Adrian Iftene, Diana Trandaba?, and Ionut Pistol.
2007.Natural language processing and knowledge repre-sentation for elearning environments.
In Proc.
ofApplications for Romanian.
Proceedings of RANLPworkshop, pages 19?25.Wenbin Jiang, Haitao Mi, and Qun Liu.
2008.
Wordlattice reranking for chineseword segmentation andpart-of-speech tagging.
In Proceedings of the 22ndInternational Conference on Computational Lin-guistics (COLING 2008), pages 385?392, Manch-ester, UK.Judith Klavans and Smaranda Muresan.
2001.
Eval-uation of the DEFINDER system for fully auto-matic glossary construction.
In Proc.
of the Amer-ican Medical Informatics Association (AMIA) Sym-posium.Michael Tully Klein.
2008.
Understanding Englishwith Lattice-Learning, Master thesis.
MIT, Cam-bridge, MA, USA.Lambert Mathias and William Byrne.
2006.
Statis-tical phrase-based speech translation.
In Proceed-ings of the IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP 2006),Toulouse, France.George A. Miller, R.T. Beckwith, Christiane D. Fell-baum, D. Gross, and K. Miller.
1990.
WordNet:an online lexical database.
International Journal ofLexicography, 3(4):235?244.Roberto Navigli and Paola Velardi.
2006.
Ontologyenrichment through automatic semantic annotationof on-line glossaries.
In Proceedings of the 15th In-ternational Conference on Knowledge Engineeringand Knowledge Management (EKAW 2006), pages126?140, Podebrady, Czech Republic.Roberto Navigli, Paola Velardi, and Juana Mar?
?a Ruiz-Mart??nez.
2010.
An annotated dataset for extract-ing definitions and hypernyms from the Web.
InProceedings of the 7th International Conference onLanguage Resources and Evaluation (LREC 2010),Valletta, Malta.Roberto Navigli.
2009a.
Using cycles and quasi-cyclesto disambiguate dictionary glosses.
In Proceed-ings of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL 2009), pages 594?602, Athens, Greece.Roberto Navigli.
2009b.
Word Sense Disambiguation:A survey.
ACM Computing Surveys, 41(2):1?69.Michael P. Oakes.
2005.
Using hearst?s rules forthe automatic acquisition of hyponyms for mining apharmaceutical corpus.
In Proceedings of the Work-shop Text Mining Research.Adam Przepio?rkowski, Lukasz Dego?rski, BeataWo?jtowicz, Miroslav Spousta, Vladislav Kubon?,Kiril Simov, Petya Osenova, and Lothar Lemnitzer.2007.
Towards the automatic extraction of defini-tions in slavic.
In Proceedings of the Workshopon Balto-Slavonic Natural Language Processing (inACL ?07), pages 43?50, Prague, Czech Republic.Association for Computational Linguistics.Alan Ritter, Stephen Soderland, and Oren Etzioni.2009.
What is this, anyway: Automatic hypernymdiscovery.
In Proceedings of the 2009 AAAI SpringSymposium on Learning by Reading and Learningto Read, pages 88?93.Horacio Saggion.
2004.
Identifying denitions in textcollections for question answering.
In Proceedingsof the Fourth International Conference on LanguageResources and Evaluation (LREC 2004), Lisbon,Portugal.Antonio Sanfilippo and Victor Poznan?ski.
1992.
Theacquisition of lexical knowledge from combinedmachine-readable dictionary sources.
In Proceed-ings of the third Conference on Applied Natural Lan-guage Processing, pages 80?87.Helmut Schmid.
1995.
Improvements in part-of-speech tagging with an application to german.
InProceedings of the ACL SIGDAT-Workshop, pages47?50.Josh Schroeder, Trevor Cohn, and Philipp Koehn.2009.
Word lattices for multi-source translation.
InProceedings of the European Chapter of the Asso-ciation for Computation Linguistics (EACL 2009),pages 719?727, Athens, Greece.Rion Snow, Dan Jurafsky, and Andrew Y. Ng.
2004.Learning syntactic patterns for automatic hypernymdiscovery.
In Proceedings of Advances in NeuralInformation Processing Systems, pages 1297?1304.Angelika Storrer and Sandra Wellinghoff.
2006.
Auto-mated detection and annotation of term definitions ingerman text corpora.
In Proceedings of the Fifth In-ternational Conference on Language Resources andEvaluation (LREC 2006), Genova, Italy.Paola Velardi, Roberto Navigli, and PierluigiD?Amadio.
2008.
Mining the Web to createspecialized glossaries.
IEEE Intelligent Systems,23(5):18?25.Eline Westerhout and Paola Monachesi.
2007.
Extrac-tion of dutch definitory contexts for eLearning pur-poses.
In Proceedings of CLIN.Eline Westerhout.
2009.
Definition extraction usinglinguistic and structural features.
In Proceedingsof the RANLP 2009 Workshop on Definition Extrac-tion, pages 61?67.Chunxia Zhang and Peng Jiang.
2009.
Automatic ex-traction of definitions.
In Proceedings of 2nd IEEEInternational Conference on Computer Science andInformation Technology, pages 364?368.Zhao-man Zhong, Zong-tian Liu, and Yan Guan.
2008.Precise information extraction from text based ontwo-level concept lattice.
In Proceedings of the2008 International Symposiums on Information Pro-cessing (ISIP ?08), pages 275?279, Washington,DC, USA.1327
