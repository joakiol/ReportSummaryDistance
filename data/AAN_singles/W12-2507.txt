Workshop on Computational Linguistics for Literature, pages 54?58,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsMining wisdomAnders S?gaardCenter for Language TechnologyUniversity of CopenhagenDK-2300 Copenhagen Ssoegaard@hum.ku.dkAbstractSimple text classification algorithms performremarkably well when used for detecting fa-mous quotes in literary or philosophical text,with f-scores approaching 95%.
We comparethe task to topic classification, polarity classi-fication and authorship attribution.1 IntroductionMark Twain famously said that ?the difference be-tween the right word and the almost-right word isthe difference between lightning and a lightningbug.?
Twain?s quote is also about the importanceof quotes.
A great quote can come in handy whenyou are looking to inspire people, make them laughor persuade people to believe in a particular point ofview.
Quotes are emblems that serve to remind us ofphilosophical or political stand-points, world views,perspectives that comfort or entertain us.
Famousquotes such as ?Cogito ergo sum?
(Descartes) and?God is dead?
(Nietzsche) occur millions of timeson the Internet.The importance of quotes has motivated publish-ing houses to create and publish large collections ofquotes.
In this process, the editor typically spendsyears reading philosophy books, literature, and in-terviews to find good quotes, but this process is bothexpensive and cumbersome.
In this paper, we con-sider the possibility of automatically learning whatis a good quote, and what is not.1.1 Related workWhile there seems to have been no previous workon identifying quotes, the task is very similar towidely studied tasks such as topic classification, po-larity classification, (lexical sample) word sense dis-ambiguation (WSD) and authorship attribution.
Inmost of these applications, texts are represented asbags-of-words, i.e.
a text is represented as a vectorx = ?x1, .
.
.
, xN ?
where each xi encodes the pres-ence and possibly the frequency of an n-gram.
It iscommon to exclude stop words or closed class itemssuch as pronouns and adpositions from the set of n-grams when constructing the bags-of-words.
Some-times lemmatization or word clustering is also usedto avoid data sparsity.Topic classification is the classic problem in textclassification of distinguishing articles on a partic-ular topic from other articles on other topics, saysports from international politics and letters to theeditor.
Several resources exist for evaluating topicclassifiers such as Reuters 20 Newsgroups.
Com-mon baselines are Naive Bayes, logistic regression,or SVM classifiers trained on bag-of-words repre-sentations of n-grams with stop words removed.While newspaper articles typically consist of tensor hundreds of sentences, famous quotes typicallyconsist of one or two sentences, and it is interest-ing to compare quotation mining to work on apply-ing topic classification techniques to short texts orsentences (Cohen et al, 2003; Wang et al, 2005;Khoo et al, 2006).
Cohen et al (2003) and Khoo etal.
(2006) classify sentences in email wrt.
their rolein discourse.
Khoo et al (2006) argue that extend-ing a bag-of-words representation with frequencycounts is meaningless in small text and restrict them-selves to binary representations.
They show empir-ically that excluding stop words and lemmatization54both lead to impoverished results.
We also observethat stop words are extremely useful for quotationmining.Polarity classification is the task of determiningwhether an opinionated text about a particular topic,say a user review of a product, is positive or neg-ative.
Polarity classification is different from quo-tation mining in that there is a small set of strongpredictors of polarity (pivot features) (Wang et al,2005; Blitzer et al, 2007), e.g.
the polarity wordslisted in subjectivity lexica, including opinionatedadjectives such as good or awful.
The meaning ofpolarity words is context-sensitive, however, so con-text is extremely important when modeling polarity.Some quotes are expressions of opinion, and therehas been some previous research on polarity classifi-cation in direct quotations (not famous quotes).
Bal-ahur et al (2009) present work on polarity classifica-tion of newspaper quotations, for example.
They usean SVM classifier on a bag-of-words representationof direct quotes in the news, but using only wordstaken from subjectivity lexica as features.
Drury etal.
(2011) present a strategy for polarity classifica-tion of direct quotations from financial news.
Theyuse a Naive Bayes classifier on a bag-of-words mod-els of unigrams, but learn group-specific models foranalysts and CEOs.WSD.
The lexical sample task in WSD is the taskof determining the meaning of a specific target wordin context.
Mooney (1996) argues that Naive Bayesclassification and perceptron classifiers are particu-larly fit for lexical sample word sense disambigua-tion problems, because they combine weighted evi-dence from all features rather than select a subset offeatures for early discrimination.
This of course alsoholds for logistic regression and SVMs.
Whethera sentence is a good quotation or not also dependson many aspects of the sentence, and experimentson held-out data comparing Naive Bayes with deci-sion tree-based learning algorithms, also mentionedin Sect.
5, clearly demonstrated that early discrimi-nation based on single features is a bad idea.
In thisrespect, quotation mining is more similar to lexicalsample WSD than to topic and polarity classificationwhere there is a small set of pivot features.Authorship attribution is the task of determin-ing which of a given set of authors wrote a particulartext.
One of the insights from authorship attributionPositivesTwo lives that once part are as ships that divide.My appointed work is to awaken the divine nature that is within.Discussion in America means dissent.NegativesThe business was finished, and Harriet safe.But how shall I do?
What shall I say?I am quite determined to refuse him.Figure 1: Examples.is that stop words are important when you want tolearn stylistic differences.
Stylistic differences canbe identified from the distribution of closed classwords (Arun et al, 2009).
As already mentioned,we observe the same holds for quotation mining.In conclusion, early-discrimination learning algo-rithms do not seem motivated for applications suchas mining quotes where pivot features are hard tochoose a priori.
Furthermore, we hypothesize thatit is better not to exclude stop words.
Quotationmining can thus in our view be thought of as an ap-plication that is similar to sentence classification inthat famous quotes are relatively small, and similarto authorship attribution in that style is an importantpredictor of whether a sentence is a famous quote.2 DataWe obtain the database of famous quotes from apopular on-line collection of quotes1 and use philo-sophical and literary text sampled from the Guten-berg corpus as negative data.
In particular we usethe portion of Gutenberg documents that is dis-tributed in the corpora collection at NLTK.2 Thisgives us a total of 44,385 positive data points (fa-mous quotes) and 247,115 negative data points (or-dinary sentences).
In our experiments we use thetop 4,000 data points in each sample, i.e.
a total of8,000 data points, except for when we derive a learn-ing curve later on, which uses up to 2?
20, 000 datapoints.
Some sample data points are presented inFigure 1.3 ExperimentEach data point is represented as a binary bag-of-words - or bag-of-n-grams, really.
Our initial hy-pothesis was to include stop words and keep infor-1http://quotationsbook.com2http://nltk.org55mation about case (capital letters).
Stop words areextremely important to distinguish between literarystyles, and we speculated that quotes can be dis-tinguished from ordinary text in part by their style.We also speculated that there would be a tendencyto capitalize some words in quotes, e.g.
?God?, ?theOther?, or ?the World?.
Finally, we hypothesized thatincluding more context would be beneficial.
Our in-tuition was that sometimes larger chunks such as ?Hewho?
may indicate that a sentence is a quote withoutthe component words being indicative of that in anyway.To evaluate these hypotheses we considered a lo-gistic regression classifier over bag-of-word repre-sentations of the quotes and our neutral sentences.We used a publicly available implementation3 oflimited memory L-BFGS to find the weights thatmaximize the log-likelihood of the training data:w?
= argmaxw?iy(i) log11 + e?w?x+ (1?
y(i))loge?w?x1 + e?w?xwhere w ?
x is the dot product of weights and bi-nary features in the usual way.
We prefer logistic re-gression over Naive Bayes, since logistic regressionis more resistant to possible dependencies betweenvariables.
The conditional likelihood maximizationin logistic regression will adjust its parameters tomaximize the fit even when the resulting parametersare inconsistent with the Naive Bayes assumption.Finally, logistic regression is less sensitive to param-eter tuning than SVMs, so to avoid expensive param-eter optimization we settled for logistic regression.To test the importance of case, we did experi-ments with and without lowercasing of all words.To test the importance of stop words, we did experi-ments where stop words had been removed from thetexts in advance.
We also considered models withbigrams and trigrams to test the impact of biggerunits of text (context).
Finally, we varied the sizeof the dataset to obtain a learning curve suggestinghow our model would perform in the limit.3http://mallet.cs.umass.edu/1.0 1.5 2.0 2.5 3.0n-grams (n<=x)8486889092F1(positives)logregrlogregr(case)logregr(nostop)Figure 2: Results with n-grams of different sizes w/olower-casing and w/o stop words.4 ResultsWe report f-scores obtained by 10-fold cross-validation over a balanced 8,000 data points in Fig-ure 2.
The green line is our hypothesis model us-ing n-grams of up to different lengths (1, 2 and 3).In this model features are not lower-cased (case ispreserved), and stop words are included.
This cor-responds to our hypotheses about what would workbest for quotation mining.
The green line tells usthat our unigram model is considerably better thanour bigram and trigram models.
This is probablybecause the bigrams and trigrams are too sparselydistributed in our data selection.The blue line represents results with lowercasedfeatures.
This means that features will be less sparse,and we now see that the bigram model is slightlybetter than the unigram model.The red line represents results where stop wordshave been removed.
This would be a typical modelfor topic classification.
We see that this performsradically worse than the other two models, suggest-ing that our hypothesis about the usefulness of stopwords for quotation mining was correct.
The obser-vation that the bigram and trigram models withoutstop words are much worse than the unigram modelwithout stop words is most likely due to the extrasparsity introduced by open class trigrams.Our main result is that with sufficient training datathe f-score for detecting famous quotes in philosoph-ical and literary text approaches 95%.
The learningcurves in Figure 3 are the results of our hypothesis56Source QuoteBill Clinton?s Inaugural 1992 Powerful people maneuver for position and worry endlessly about who is in and who is out,who is up and who is down, forgetting those people whose toil and sweat sends us here andpaves our way.Bill Clinton?s Inaugural 1997 But let us never forget : The greatest progress we have made, and the greatest progress wehave yet to make, is in the human heart.PTB CoNLL 2007 test When the dollar is in a free-fall , even central banks can?t stop it .Europarl 01-17-00 Our citizens can not accept that the European Union takes decisions in a way that is, at leaston the face of it, bureaucratic .Europarl 01-18-00 If competition policy is to be made subordinate to the aims of social and environmentalpolicy , real efficiency and economic growth will remain just a dream .Europarl 01-19-00 For Europe to become the symbol of peace and fraternity , we need a bold and generouspolicy to come to the aid of the most disadvantaged .Figure 4: The sentence with highest probability of being a quote in each corpus according to our 20K logistic regressionunigram model).0 5000 10000 15000 20000 25000 30000 35000 40000data points91.091.592.092.593.093.594.094.595.0F1(positives)unigramsbigramsFigure 3: Learning curves for unigram and bigram mod-els without lower-casing and with stop words.model (green line in Figure 2) obtained with vary-ing amounts of training data, from 4,000 to 40,000data points.
The learning curves also confirm thatthe bigram model was suffering from sparsity withsmaller data selections, and we observe that the bi-gram model becomes superior to the unigram modelwith about 30,000 data points.
The learning curvesshow that F-scores for positive class approach 95%as we add more training data.5 DiscussionTo confirm Mooney?s hypothesis that it is better tocombine weighted evidence from all features ratherthan select a subset of features for early discrimi-nation, also in the case of mining quotes, we ran adecision tree algorithm on the same data sets usedabove.
The f-score for detecting quotes was consis-tently below 65%.The decision tree algorithm tries to find good fea-tures for early discrimination.
Interestingly, one ofthe most discriminative features picked up by thedecision tree from trigram data with case preservedwas the bigram ?He who?.
This feature was usedto split 500 sentences, leaving only 11 in the minor-ity class.
Other discriminative features include ?Peo-ple?, ?we are?, ?if you have?, and ?Nothing is more?.Similarly, we can observe remarkable differencesin marginal distributions by considering the mostfrequent words in positive and negative texts.
Wordssuch as ?who?, ?all?, ?word?, and ?things?
occurmuch more frequently in quotes than in more bal-anced literary philosophical text.
Interestingly ??
?is also a very good predictor of a sentence being apotential quote.Finally, we ran a model on other corpora to iden-tify novel candidates of famous quotes (Figure 4).We ran it on texts where you would expect to findpotential famous quotes (e.g.
inaugurals), as well ason texts where you would not expect that.6 ConclusionSimple text classification algorithms perform re-markably well when used for detecting famousquotes in literary or philosophical text, with f-scoresapproaching 95%.
We compare the task to topicclassification, polarity classification and authorshipattribution and observe that unlike in topic classifi-cation, stop words are extremely useful for quotationmining.57ReferencesR Arun, R Saradha, V Suresh, M Murty, and C Madha-van.
2009.
Stopwords and stylometry: a latent Dirich-let alocation approach.
In NIPS workshop on Appli-cations for Topic Models.Alexandra Balahor, Ralf Steinberger, Erik van derGoot, Bruno Pouliquen, and Mijail Kabadjov.
2009.Opinion mining on newspaper quotations.
InIEEE/WIC/ACM Web Intelligence.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InACL.William Cohen, Vitor Carvalho, and Tom Mitchell.
2003.Learning to classify email into ?speech acts?.
InEMNLP.Brett Drury, Gae?l Dias, and Luis Torgo.
2011.
A con-textual classification strategy for polarity analysis ofdirect quotations from financial news.
In RANLP.Anthony Khoo, Yuval Marom, and David Albrecht.2006.
Experiments with sentence classification.
InALTW.Raymond Mooney.
1996.
Comparative experiments ondisambiguating word senses.
In EMNLP.Chao Wang, Jie Lu, and Guangquan Zhang.
2005.
Asemantic classification approach for online product re-views.
In IEEE/WIC/ACM Web Intelligence.58
