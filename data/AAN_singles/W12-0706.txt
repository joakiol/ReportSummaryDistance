Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 44?54,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsRobust Induction of Parts-of-Speech in Child-Directed Language byCo-Clustering of Words and ContextsRichard E. Leibbrandt David M W PowersSchool of Computer Science, Engineeringand MathematicsSchool of Computer Science, Engineeringand MathematicsFlinders University Flinders Universityrichard.leibbrandt@flinders.edu.audavid.powers@flinders.edu.auAbstractWe introduce Conflict-Driven Co-Clustering,a novel algorithm for data co-clustering, andapply it to the problem of inducing parts-of-speech in a corpus of child-directed spokenEnglish.
Co-clustering is preferable tounidimensional clustering as it takes intoaccount both item and context ambiguity.
Weshow that the categorization performance ofthe algorithm is comparable with the co-clustering algorithm of Leibbrandt andPowers (2008), but out-performs thatalgorithm in robustly pruning less-usefulclusters and merging them into categoriesstrongly corresponding to the three main openclasses of English.1 IntroductionThe problem of unsupervised part-of-speechinduction has received considerable attention incomputational linguistics (for a recentcomparison of several influential models, seeChristodoulopoulos, Goldwater & Steedman,2010).
A common approach is to estimate theparameters of a generative model given thenatural language data, with the model usually avariant of a Hidden Markov Model (e.g.Goldwater & Griffiths, 2007; Berg-Kirkpatrick,C?t?, De Nero &  Klein, 2010; Moon, Erk &Baldridge, 2010).
These models are oftenevaluated on corpora of formal, written English,such as the Penn Treebank, rather than onnatural, spoken language, and typically the aimof these studies is to improve the state-of-the-artof POS induction using various techniques frommachine learning, with an implicit focus ondevising techniques that can be used in practicalapplications.In the current paper, on the other hand, ourfocus is on part-of-speech induction mechanismsthat children might use when learning their firstlanguage.
Hence, we are interested in models thatare motivated by psychological considerations,rather than by a more abstract mathematical orstatistical grounding.
In language acquisitionresearch, a typical approach to part-of-speechinduction is to make use of clustering.
We willreview this work and argue for the particularutility of two-mode clustering or co-clusteringapproaches, before presenting two novel co-clustering techniques and evaluating theirperformance in part-of-speech tagging on acorpus of child-directed English.1.1 Clustering and co-clustering approachesto part-of-speech induction in languageacquisition researchSingle-mode clustering approachesClustering algorithms operate on a two-dimensional matrix where the rows and columnsin this context represent words and the linguisticcontexts in which they appear, taken from acorpus of natural language, and the cells of thematrix contain frequency counts of how often aword occurs in a particular context.
It has oftenbeen proposed that children might make use ofinformation about the contextual distribution ofusage of words to induce the parts-of-speech oftheir native language (e.g.
Maratsos & Chalkley,1980), and work by, e.g., Redington, Chater &Finch (1998) and Clark (2000), showed thatparts-of-speech can indeed be induced by44clustering together words that are used in similarcontexts in a corpus.
Clustering word typestogether does not take into account the fact thatthe part-of-speech of a word type may changedepending on the context in which it is used.
Oneof the most influential models in part-of-speechinduction in language acquisition, the FrequentFrames model of Mintz (2003), addresses thisissue by forming clusters of the contextualframes in which words are used, rather than thewords themselves.
The idea is that the contextsdefine the part-of-speech, rather than the wordsthemselves.
This model attains high, but notperfect results in part-of-speech tagging forEnglish child-directed speech; part of the reasonis that even frames are sometimes ambiguous inthe parts-of-speech that they can accommodate,and Erkelens (2008) has shown that this problemis more pronounced when the Frequent Framesapproach is applied to Dutch material.
In general,however the set of frame contexts is chosen, theproblem of contextual ambiguity is likely topresent itself.
Hence, an approach is needed inwhich both words and contexts can be associatedwith multiple categories.
Techniques of co-clustering, also called biclustering or two-modeclustering, (see Madeira & Oliveira, 2004, VanMechelen et al, 2004, for reviews), represent onesuch approach.Co-clustering approachesSingle-mode clustering forms clusters ofelements in one dimension of the matrix (eitherrows or columns) by grouping together elementson the basis of similar co-occurrence withelements of the other dimension.
Co-clusteringtechniques, on the other hand, form clusters onthe basis of similarity between rows andsimilarity between columns simultaneously.
Co-clustering is therefore able to assign row andcolumn elements to the same clusters.
We candistinguish between row-column clusteringmethods which assign each row and each columnto a particular cluster, and data clusteringmethods which assign each individual non-emptycell of the matrix to a cluster.
Some co-clusteringmethods allow for overlapping clusters, i.e.
inrow-column methods by allowing rows andcolumns to belong to more than one cluster, or indata clustering methods by allowing cells in thematrix to belong to more than one cluster.
Co-clustering algorithms have been shown to beuseful in many applications, notably in theanalysis of gene expression data (Madeira &Oliveira, 2004).There are good reasons to prefer a co-clustering approach over a single-modecategorization approach in part-of-speechinduction.
In natural language, including child-directed speech, there are many cases where aword appears in a context that does not specifythe part-of-speech exactly, but allows severalpossibilities, while at the same time, the word isalso ambiguous  in its part-of-speech.
Co-clustering is able to deal with part-of-speechambiguity at the level of word and framesimultaneously.
For example, a common framein child-directed speech in English is ?That?sX.
?, where the word that fills the X slot could bea noun (?That?s ice-cream.?)
or an adjective(?That?s pretty.?).
Simultaneously, the word?mean?
can be used as either a verb or anadjective (the nominal usage is rare in child-directed speech).
A single-mode clusteringalgorithm that aims to assign a part-of-speech tothe word ?mean?
in ?That?s mean?
will beunable to decide between the allowed parts-of-speech for the frame, if frames were clustered,and between the allowed parts-of-speech for thewords, if words were clustered.
However, a co-clustering approach that assigned ?That?s X?
toboth the categories noun and adjective, and?mean?
to the categories verb and adjective,would be able to deduce that the only categorythat the word and the frame have in common isadjective, and therefore that this is the correctcategory.
In this way, co-clustering is better ableto deal with linguistic ambiguity.Even apart from its practical utility in part-of-speech induction, co-clustering is broadlycompatible with a psychological outlook thatconceives of part-of-speech development interms of associative learning (see e.g.
Shanks,1995).
Under this view, parts-of-speech aremental categories that are formed by repeatedexposure to words used in context, incombination with whatever semantic construalthe language-learning child places on theutterances she hears.Only a few studies have applied co-clusteringto part-of-speech induction with child-directedlanguage (but see Freitag, 2004, for part-of-speech induction with co-clustering on adult-directed language in the Penn Treebank).
Thepioneering work in this regard was the EMILEsystem of Adriaans and colleagues (Adriaans,1992), which formed co-clusters of word-contextcombinations as a step in the process of inducing45rules for a categorial grammar.
While thegrammars formed in this way perform well,EMILE typically produces large, overlappingcategories which  do not correspond to the parts-of-speech of English (Adriaans, 1999).
Hence, itis difficult to evaluate the accuracy of EMILE?spart-of-speech tagging against a gold standard.Leibbrandt & Powers (2008) applied co-clustering to a corpus of English child-directedspeech, yielding accuracy comparable to thatobtained by the Frequent Frames model of Mintz(2003).
This approach was also able tooutperform Frequent Frames in tagging child-directed data in Dutch (Leibbrandt & Powers,2010).In this paper, we extend the work ofLeibbrandt & Powers (2008, 2010) by describingand evaluating a novel co-clustering techniquefor part-of-speech induction.
In Section 2 wepresent the Conflict-Driven Co-Clusteringalgorithm, and in Section 3 we evaluate itsperformance in part-of-speech tagging of acorpus of child-directed speech.
We show thatthe algorithm delivers performance comparableto that of both the Frequent Frames model ofMintz (2003) and the co-clustering work byLeibbrandt & Powers (2008, 2010), and is morerobust than the earlier work in automaticallydiscovering the main English open classes ofnoun, verb and adjective, discarding smaller andless-easily interpretable categories.
In Section 4we consider reasons for these results and point tofuture directions for this work.2 Conflict-Driven Co-ClusteringThe Conflict-Driven Co-Clustering (CDCC)algorithm is a row-column-based co-clusteringalgorithm.
It creates an initial clustering of wordsinto a set of clusters, and a simultaneousclustering of frames into the same set of clusters.Only a few word and frame types are clustered tostart with, and hence this initial clustering isinadequate to account for the empirical co-occurrence data (as explained below).
From thisstarting point, the CDCC algorithm iterativelyadds frames and clusters to the clusters, until allof the co-occurrence data is accounted for.We make the assumption that there exist anumber of parts-of-speech in the target language,and that a particular word used in a particularframe context belongs to only one part-of-speech1.
We also assume that the word type is acue to the part-of-speech, and that the same istrue of the frame type.
Finally, each word typeand frame type is presumed to have the potentialto be associated with more than one part-of-speech.Suppose, then, that we (in this case, the co-clustering algorithm, but also, potentially, a childlearning the target language) already have somenotion of the parts-of-speech to which aparticular frame type f ?belongs?, and the parts-of-speech to which a word type w belongs.
Thenwhen we encounter an instance (i.e.
a token) ofthe word type w used in the context of the frametype f, and wish to assign a part-of-speech to thisinstance, the only viable candidates (based onour knowledge at the time) are those  parts-of-speech that both w and f have in common.Should there be multiple such candidates, a part-of-speech tagging algorithm might resort tocombining information about the probabilities off and w belonging to each candidate in order toselect a ?winner?.
However, when there is nosuch candidate (word and frame have no part-of-speech in common), this presents a problem forpart-of-speech tagging.
Such a situation is aninstance of the ?conflicts?
from which CDCCderives its name.More concretely, we can represent the clustermembership of each of the J words underconsideration as a J?K matrix W, where K is thenumber of clusters, and Wjk = 1 if word j is amember of cluster k, and 0 otherwise.
Similarly,the cluster membership of each of the I frames isrepresented by the I?K matrix F, where Fik = 1 ifframe i belongs to cluster k, and 0 otherwise.The I?J matrix D represents the co-occurrencedata obtained from the corpus, where Dij = 1 ifword j occurs in the context of frame i in thecorpus, and 0 otherwise.
Then a conflict existswhenever Dij = 1 and the dot-product Wj ?
Fi = 0.We can think of the possibilities described bythe cluster membership matrices W and F asaccounting for the word-frame co-occurrencesdescribed in D: if a word and frame can occurtogether, there must be at least one part-of-speech to which they both belong.
Conflictsoccur where cells in the D matrix are not yetaccounted for in this way.
The problem to besolved in this case, therefore, is to remove all1 There are examples, even in the corpus used in thisexperiment, for which this assumption does not seem tohold; however, these examples are relatively infrequentenough to warrant its use as a useful heuristic.46instances of conflict.
Because the D matrix isempirically given, the only way to removeconflict is to modify the F and W matrices so thatall co-occurrences in D can be accounted for.Figure 1 illustrates some cases of conflict andresolved conflict between word and frame.Initially, the utterance ?Shall I brush it?
?contains a conflict, because the frame ?Shall I Xit??
is allocated to the Verb category, but ?brush?is not yet alocated to any category.
The conflictmight be resolved by adding ?brush?
to the Verbcategory.
Later, when we consider the utterance?There?s your brush?, a conflict would occur if?brush?
was allocated to Verb only and ?There?syour X?
was allocated to Noun only.
Supposethat the conflict was resolved correctly by alsoadding ?brush?
to the category Noun (in additionto already being allocated to Verb).
Then whenthe utterance ?Don?t brush it?
is encountered,there is no conflict, as both ?Don?t X it?
and?brush?
are allocated to the Verb cluster, andhence the allocations are compatible.Shall I brush it?
N V Abrush 0 0 0Shall I X it?
0 1 0There?s your brush.
N V Abrush 0 1 0There?s your X.
1 0 0Don?t brush it.
N V Abrush 1 1 0Don?t X it.
0 1 0Figure 1.
Three instances of conflict and non-conflict.
In the top example, brush and Shall I Xit?
are in conflict, in the middle example, brushand There?s your X are in conflict, and in thelower example there is no conflict.
(N = Noun, V= Verb, A = Adjective)An open problem is then how best to calculatethe cluster membership matrices W and F so as toremove all conflicts.
One obvious ?solution?would be to simply add membership of everycluster to every word and frame.
While thiswould remove all conflicts, it is clearly not auseful basis for part-of-speech tagging, andviolates our sense that not every word or contextcan belong to every part-of-speech.A better approach might be to start with a verysparse pair of initial matrices for W and F, whichgreatly under-determine the co-occurrencematrix D, and then add cluster memberships toindividual frames and words (changing 0s to 1sin F and W) if adding them would help to solveconflicts.We still need to decide which clustermemberships to add, and a useful principle mightbe to add memberships parsimoniously, i.e.
to tryto minimize the number of new membershipsadded to F and W. The CDCC algorithm takes agreedy approach to this problem.
On eachiteration, it simply adds the single clustermembership (word or frame) that would resolvethe largest number of conflicts existing at thattime.
The set of remaining conflicts is thenrecalculated, and the cluster membership thatagain resolves the greatest number of conflicts isadded, with the process being repeated until allconflicts have been resolved.The only remaining point to specify is how thealgorithm gets started, i.e.
how the W and Fmatrices are initialized.
It would be desirable tobegin with just a small number of ?groundtruths?, i.e.
a small number of categorymemberships, for only a few frames and words,that are well-established in advance.
The rest ofthe values in the membership matrices are thenbootstrapped from this starting point by referringto the co-occurrence matrix.The initial values with which W and F are?seeded?
can come from any source: for instance,they may be the result of a process of semanticcategory formation (e.g.
Macnamara, 1982;Pinker, 1984), so that words that refer to physicalobjects are flagged as belonging to one category,and words for actions marked as belonging toanother category (bear in mind that this does notpreclude these words from later also beingassigned to other categories).
This process mightalso be extended to frames that reliably containwords referring to objects, actions, physicalproperties, etc.
In computational work onlanguage acquisition, proxies for these categoriesmight be obtained from lists of early-acquiredwords, possibly in combination with norms onword imageability.
In less acquisition-orientedwork, seeds may be obtained from manuallyannotated examples, so that this becomes a semi-supervised approach to part-of-speech tagging.In the experiment reported here, we decided toobtain our seed information entirely from thesame word-frame co-occurrence matrix D usedlater to expand the W and F matrices, and we didso along the same lines as followed byLeibbrandt & Powers (2008, 2010).Consequently, our results are prone to some ofthe shortcomings of the earlier work, as47discussed later.
We emphasize that the choice ofseeding algorithm is not part of the CDCCalgorithm proper, and informal experimentationhas shown that the performance of CDCC ishighly dependent on the accuracy of the initialseed information.2.1 CDCC AlgorithmThe conflict-driven co-clustering algorithm(pseudo-code is presented in Box 2) attempts tofind a conflict-free allocation of categories towords and frames.
It does so by repeatedlyremoving the largest existing conflict until noconflicts remain.In what follows, we use the term ?co-item?
torefer to those items with which an item (word orframe) co-occurs in D, i.e.
the co-items of  aword type are the frame types in which it hasoccurred, and the co-items of a frame type arethe word types that have occurred in it.
Conflictsbetween items and their co-items are removed bysimply allocating those additional categories toitems that they would need in order to no longerbe in conflict with the co-items.
Conflicts are notresolved in random order; instead, the conflictresolution option that would resolve the largestnumber of conflicts is chosen at every step.
Inthis way, the membership vectors for each of thewords and frames are adjusted so as to convergeonto the ?correct?
allocation.
When no morechanges can be made to the membership vectors,the algorithm halts.The algorithm works in batch mode,considering the entire data matrix at once.
Forevery item (whether word or frame), the set ofco-items that are currently in conflict with theitem is collected.
Using the current membershipmatrices W and F, the algorithm allows each co-item to cast one vote for every category to whichit is currently allocated (i.e.
co-items cast votesto have particular categories added to the item?sallocations).
Per definition, these are categoriesthat the target item does not have in itsmembership vector, so that adding that categoryto the item?s membership vector would resolvethe conflict between the item and that particularco-item; however, the point of voting is to findthe single change that would result in the largestnumber of conflict resolutions at once.
Thenumber of votes for each category is determinedin this way for every target item (every word andevery frame).
The suggested category allocationthat has received the largest number of votesover all words and all frames is designated the?winner?, and the category in question is addedto the membership vector of the item in question.CDCC:D: co-occurrence matrix of frames and wordsF, W: membership matrices describing thecategories to which each of the frames andwords may belong.
F and W are initializedprior to running CDCC, for instance usingunsupervised clustering as in Box 2.
F[ k ][ i ]= 1 if frame i is able to belong to cluster k, and0 otherwise, and similarly for W.repeat until convergence (see text)for i = 1 to Ifor j = 1 to Jif D[ i ][ j ] = 1conflict = truefor k = 1 to Kif (F [ k ][ i ] = 1and W [ k ][ j ] = 1)conflict =falseif conflicttallyVotes(i, j)find k1 such that FrameVotes[ k1 ][ i ] =max cell in FrameVotesfind k2 such that WordVotes[ k2 ][ j ] =max cell in WordVotesif FrameVotes[k1][ i ] > WordVotes[k2][ j ]F [ k1 ][ i ] = 1elseW [ k2 ][ j ] = 1tallyVotes(i, j):for k = 1 to Kif (F[ k ][ i ] = 0 and W [ k ][ j ] = 1)FrameVotes[ k ][ i ] += 1else if (F [ k ][ i ] = 1 and W [ k ][ j ] = 0)WordVotes[ k ][ j ] += 1Box 1.
Conflict-Driven Co-ClusteringAlgorithm.One of the benefits of the voting system is thatit is self-correcting.
If an item which is, say, aNoun, is incorrectly not assigned to the clustercorresponding to Nouns, then it will cast oneincorrect vote each time to change the allocationof each of its co-items.
However, the co-itemsare likely to be Nouns in most cases, and hence48to occur in other Noun frames, which will inmost cases lend them the Noun allocation, so thatthey will vote en masse to change the allocationof the incorrectly allocated item to Noun.The product of the CDCC algorithm is a fairlyconservative allocation of (potentially multiple)clusters to each of the words and frames.3 Evaluation of the algorithmsThe CDCC algorithm was applied to a corpusof child-directed speech, after which individualtokens of word-frame co-occurrences werecategorized into one of the co-clusters producedby the algorithm, as described below.3.1 Data SetThe data set used was the same as in Leibbrandt& Powers (2008), namely the child-directedportion of the Manchester corpus (Theakston,Lieven, Pine & Rowland, 2001) obtained fromthe CHILDES project (MacWhinney, 2000).
Thiscorpus is supplied with a manual part-of-speechtagging, which was used as the ?gold standard?correct tagging against which the categorizationproduced by CDCC was evaluated.3.2 Extraction of Contextual FramesContextual frames were extracted from thecorpus following the method in Leibbrandt &Powers (2008).
Frames were formed fromutterances in the corpus by replacing all but themost frequently-occurring words in the corpuswith a placeholder symbol, turning corpusutterances into lexically-based schematictemplate sentences with slots that can be filled byinserting single words (for example, ?Don?t Xit?, ?That?s your X?, ?It?s very X?).
Frequencycounts were collected of the number ofoccurrences of each word in each of thecontextual frames, and the resulting data matrixwas filtered to contain only those elements thatattained a certain level of support, i.e.
frames thatoccurred with 5 or more distinct word types, andwords that occurred in 5 or more frame types.The resulting data matrix was used to obtain seedcategory membership information for selectedwords and frames, as described in the nextsection.3.3 Seed InformationThe first step in obtaining ?ground truth?
seedinformation for running the CDCC algorithm(pseudocode shown in Box 2) is to perform aD: co-occurrence matrix, such that D [ i ][ j ] = 1if word j has co-occurred with frame I, 0otherwise.Allocation: Cluster membership vector forframes, obtained from hard clustering algorithm,such that Allocation[ i ] = k if frame i is allocatedto cluster k.Initialize ClusterCoocc[ K ][ J ] to all zeroes.for i = 1 to Ifor j = 1 to Jif D[ i ][ j ]ClusterCoocc [ Allocation[ i ] ] [ j ] += 1for k = 1 to Ksum = sum(ClusterCoocc [ k ])for j = 1 to JDistribution[ k ][ j ].index  = jDistribution[ k ][ j ].value =ClusterCoocc [ k ][ j ] / sumSort Distribution[ k ] by value (descending)cumulativeProportion = 0;  j = 0repeat until cumulativeProportion ?
?j += 1index = Distribution[ k ][ j ].indexvalue = Distribution[ k ][ j ].valueSeedWords[ k ] [index]  = 1cumulativeProportion += valuefor each pair (SeedWords[a], SeedWords[b]),a ?
bRemove all words that occur in bothSeedWords[a] and SeedWords[b]for i = 1 to Ifor j = 1 to Jif D [ i ][ j ]for k = 1 to Kif SeedWords[ k ][ j ] = 1SeedFrames[ k ][ i ] = 1for each pair (SeedFrames[a], SeedFrames[b]),a ?
bRemove all frames that occur in bothSeedFrames[a] and SeedFrames[b]Box 2.
Seed frame and word selection algorithm.standard one-mode clustering of the (L2-normalized) frame vectors of the co-occurrencematrix D, producing clusters of contextual49frames (hierarchical clustering with averagelinkage was used in this experiment).Next, we select sets of words that areparticularly distinctive of each of the frameclusters.
The assumption is that words that occurin a large number of frame types from aparticular cluster are good representatives of thatcluster.
Hence, for each cluster, words are rankedin order of the number of distinct frame typesfrom the cluster in which each word hasoccurred, and are added one-by-one to the seed-word set for the cluster, until the cumulativeproportion of total distinct-frame countsaccounted for exceeds a threshold (set to 0.25 inthis experiment).
Once all seed-word sets havebeen collected in this way, seed-words whichoccur in the sets of more than one cluster arediscarded.Next, a seed-frame set is created for eachcluster, consisting of all frames which occurredwith seed-words from that cluster and did notoccur with a seed-word from any other cluster.The resulting seed sets are arguably the wordsand frames that are the most distinctly associatedwith each cluster.
The process described abovecan be considered to produce similar results to apsychological process of association betweenclusters and words, where the strength ofassociation between the cluster and the word isstrengthened each time the word is used in aframe that is strongly associated with that clusteralready.
Each distinct frame is considered tocontribute an equal amount of activation strengthto the word, regardless of its own frequency ofoccurrence in the input, so that this associationprocess is sensitive to the type frequency offrames co-occurring with the word in question,rather than to the token frequency.
A wider rangeof co-occurring frames constitutes more robustevidence that the word does indeed belong withthe cluster (and most likely possesses many ofthe semantic attributes that are associated withthe cluster).
For evidence that the type frequencyof words occurring in a frame aidsgeneralization, see Bybee (1985, 2006).The algorithm maintains a binary-valuedallocation vector for each frame and each wordof length K, where K is the number of clusters.The k?th value in the allocation vector is 1 if theword or frame can belong to cluster k, and 0 ifnot.
In this way, the algorithms deal with theambiguity of both words and frames, by allowingan item to belong to more than one cluster.
Forevery cluster k, the k?th value of the allocationvector of every seed word and every seed frameof cluster k is initialized to 1, and all other valuesare set to 0.3.4 CategorizationFor the purpose of evaluation, we categorizeeach of the instances of word-frame co-occurrences in the data matrix D by combiningthe word and frame cluster information containedin the membership matrices W and F. Whenclassifying a particular instance of word w usedin frame f, if there exists a unique a cluster c suchthat w and f  have both been allocated to c (in amajority of cases in this experiment, there wassuch a unique cluster), then the word-framecombination is classified as belonging to thecluster in question.
In cases where the word andframe have more than one cluster in common, wefall back on estimating the amount of evidencethat the word and frame separately belong toeach of the clusters.
The fallback values for eachword and frame are calculated as the proportionof co-items of the word or frame that areallocated to each cluster.
The fallback value ofthe word is multiplied by the fallback value ofthe frame, for each cluster separately, and thecluster with the highest product is selected as thecategory to which the frame-word combination isassigned.3.5 Evaluation MeasuresResults are reported in terms of standardmeasures of precision, recall and F-score, withrandom baselines in parentheses.
These measureswere calculated, as is customary in unsupervisedcategorization, by a pair counting approach thatconstructs a confusion matrix based on whetherpairs of elements are assigned to the samecategory in the gold-standard, and also in theclustering model (see e.g.
Mintz, Newport &Bever, 2002).
Because of several well-knownshortcomings of precision and recall (e.g.Powers, 2003; Rosenberg & Hirschberg, 2007),we also report the Informedness measure(Powers, 2003), which corresponds to theprobability that the predictions made by thealgorithm  are informed, in the sense of makingcorrect use of information.For a 2?2 contingency table with the symbolsa, b, c and d respectively indicating the numberof true positives, false positives, false negativesand true negatives, Informedness is given by?
=??
+ ????
+ ?.50Informedness can thus be expressed as Recallfor a particular cluster, discounted by theproportion of all non-category items that occur inthat cluster.
Informedness is equivalent to thewell-known delta-P formula expressingassociation strength in human associativelearning (e.g.
Shanks, 1995).
For a supervisedclassification problem, with a table of arbitrarydimensions m?m, Informedness is calculated forthe 2?2 contingency table of each category inturn, and the Informedness values for allcategories are combined in a weighted sum,where the weight for each category is theproportion of word tokens assigned to thatcategory by the algorithm (i.e.
the algorithm?sbias to assign instances to the category).
Inunsupervised cases, it is not obvious how toassociate clusters with gold-standard categories.In this case, weighted Informedness values arecalculated for every possible 1-to-1 mappingbetween gold standard categories and clusters,and the highest of these Informedness values isselected.For evaluation, we made use of only thosetokens that were assigned to one of the threemajor open-class categories (nouns, verbs andadjectives).HC CDCC LP08 FreqFPrecision0.844(0.559)0.888(0.559)0.900(0.559)0.90Recall0.774(0.513)0.911(0.574)0.886(0.551)0.91F0.808(0.535)0.899(0.566)0.893(0.555)0.90I 0.708 0.800 0.814 n/aTable 1.
Performance of clustering-based part-of-speech induction methods.
Random baselinevalues in italics.
Baseline value for Informednessis zero.
HC = Hierarchical Clustering (one-dimensional); CDCC = Conflict-Driven Co-Clustering; LP08 = replication of Leibbrandt &Powers (2008); FreqF = Frequent Frames(results from Mintz, 2006, baseline andInformedness scores unknown).3.6 ResultsThe results of categorization according to theCDCC algorithm is shown in Table 1.
Forcomparison, we have also shown the results ofcategorization with three other algorithms,namely: LP08, a replication of Leibbrandt &Powers (2008); FreqF, the results from Mintz(2003) for the Frequent Frames model applied tothe same corpus as used here; and HC, the resultsfrom categorizing a word-frame combinationaccording to the cluster of the frame only, wherethe frame clusters are the ones derived in theone-way clustering step that produced the seedinformation for CDCC.The results show that CDCC is competitive inits categorization performance with both theLP08 and FreqF approaches.
ComparingInformedness and F-scores against their randombaselines, the performance of LP08 is onlyslightly better than that of the two newalgorithms (random baseline values were notreported by Mintz, 2003).
Importantly, CDCC(as well as LP08) performs much better than thehard clustering HC from which it derives its seedinformation, showing that co-clustering improvescategorization.3.7 Robustness of induced parts-of-speechWe have not yet said much about the numberof clusters formed by the co-clusteringalgorithms.
This number could conceivably beinfluenced by the number of clusters formed bythe initial one-way clustering algorithm, which isoften (as it was in our experiment) a parameterunder control of the experimenter.
However, thenumber of parts-of-speech produced by a part-of-speech induction algorithm should be relativelyimmune to manipulations of algorithmicparameters.
A related issue is that the parts-of-speech produced by clustering approaches areoften unsatisfactory from a linguistic point ofview, as they don?t correspond exactly to theexpected parts-of-speech of the target language(see also Sch?tze, 1995).
We regard it asdesirable for a part-of-speech induction methodto account for at least the main open-class parts-of-speech of English (nouns, verbs, adjectivesand adverbs), and to be able to produce thesewithout undue coercion.Therefore, it is of interest to consider how thenumber of parts-of-speech produced by the co-clustering algorithms is affected by the numberof clusters in the original one-way clusteringfrom which they start.
These results are shown inTable 2.
The table shows the number of parts-of-speech produced by LP08 versus CDCC whenstarted off with varying numbers of hard clusters51in the range 3 to 18.
For each algorithm, the tableshows (under Any) the number of distinct parts-of-speech (clusters) to which at least one word-frame occurrence was assigned during thecategorization reported above, and also (under1%) the number of parts-of-speech such that atleast one percent of the total number of word-frame combinations were assigned to that part-of-speech.
The results under Any show that, asLP08 CDCCK Any 1% Any 1%3 3 3 3 36 5 3 4 39 9 4 5 312 12 6 9 315 15 6 10 318 18 7 9 3Table 2.
Number of parts-of-speech used duringcategorization for three co-clustering algorithms,for varying K = number of clusters produced ininitial one-way clustering.
Any = number ofparts-of-speech that account for at least oneframe-word instance; 1% = number of parts-of-speech that account for at least 1% of instances.LP08 = replication of Leibbrandt & Powers(2008); CDCC = Conflict-Driven Co-Clustering.the number of initial clusters grew, so too did thenumber of clusters that were used at least onceduring categorization, so that the algorithms wererather badly prone to proliferation of parts-of-speech when started with a large number ofinitial clusters, although CDCC was moreconservative than LP08, and managed to discardmany of the original clusters.
However, theresults for 1% are more encouraging.
Bothalgorithms, even when started with severalcandidate clusters in the one-way clustering,managed to eliminate the minor clusters to someextent, and redistribute their members into thelarger parts-of-speech.
It is particularlynoteworthy that for CDCC, only three clusterswere used for more than 1% of all instances.Inspection of the details of categorizationshowed that the CDCC algorithm managed todiscover three clusters that seemed to correspondclosely to the three major English parts-of-speech of Noun, Verb and Adjective.
Thesecategories appeared to be such a salient featureof the data for CDCC that they were able to ?self-organize?
during runs of the algorithm fromvarious one-way clustering starting points.
Thisrobust induction of the main English parts-of-speech is a striking advantage of CDCC overLP08.It may be argued that the number of classesproduced by the algorithm are too few to providea basis for part-of-speech induction.
To someextent this is a consequence of the seedingalgorithm chosen.
The frames used byLeibbrandt & Powers (2008, 2010) tended tosupport mostly open-class word fillers; nouns,verbs and adjectives made up respectively 52%,25% and 10% of the total number of tokens thatserved as fillers in their frames, for a total of87%.
Arguably, this may be seen as desirable:for a child learning a language, knowledge of theopen classes is more useful for learning novelwords than knowledge of the closed classes.
Onthe other hand, the lack of a category of adverbsmay be regarded as a shortcoming of the originalwork by Leibbrandt & Powers.
Nevertheless, theCDCC algorithm was able to robustly identifythe main classes represented in the co-occurrencematrix.4 DiscussionThe CDCC algorithm has been shown toachieve similar categorization performance tosome earlier models of part-of-speech induction.The most striking advantage has been that CDCCis able to ?hone in?
on the three main parts-of-speech.
We suggest that this is due to theconservative nature of conflict resolution: bytallying the strength of evidence for a particularcategory in terms of the number of votes itreceives, weaker categories are not able to castsufficient numbers of votes to change word orframe allocations.
Importantly, this means that insubsequent iterations, when conflicts arerecalculated and votes cast once more,allocations of particular words or frames to theseminor categories are more likely  to be swampedby the additional allocations previously added tothe major categories, so that the initially strongercategories become stronger as the algorithmexecutes while the weaker categories all butdisappear.
This is an important feature of thealgorithm, because the original clustering stepfrom which both CDCC and Leibbrandt &Powers (2008) begin is unconstrained in thenumber of clusters it produces; this is aparameter of the system, but it is a relativelyunimportant one in the case of CDCC becausethe algorithm self-organizes around the majorcategories.52While the CDCC algorithm performs similarlyto other established work while taking a radicallydifferent approach, several issues remain to beinvestigated.
One of the potential strengths ofCDCC is that it treats category membership in adiscrete or symbolic way, rather than graded, asin Leibbrandt & Powers (2008).
It remains to beseen whether such a treatment provides specificbenefits in resolving ambiguity when dealingwith words or frames that can belong to multiplecategories.CDCC has been formulated here as combiningdistributional information about the word typeand the frame type in order to produce a part-of-speech allocation.
However, the algorithm can beviewed more generally as a method to combineor fuse more than one source of informationtogether, and hence can be applied todistributional, phonological, semantic or anyother forms of linguistic information.As it has been formulated here as a batchprocess, the CDCC algorithm can be regarded asaddressing only the computational level of theproblem of part-of-speech induction in languageacquisition.
Additional work would be requiredto attempt to address the algorithmic orimplementational levels by turning the algorithminto a fully incremental learner (e.g., Parisien,Fazly & Stevenson, 2008; Chrupala & Alishahi,2010).
A simple variant of the CDCC algorithmcould be one that simply processes the corpus inorder, and in the case of a conflict between wordand frame, stores the occurrence as evidence thatthe membership of either the word or frameshould be altered, and in what way.
When theaccumulated evidence for a specific change ofmembership exceeds a threshold (e.g.
when acertain number of votes have been cast to addmembership of a particular cluster to a word orframe), the membership is added.
It wouldremain to be determined empirically whether thisiterative variant is still able to exhibit the samecategorization performance and the property ofrobustness shown above for the batch CDCCalgorithm.ReferencesAdriaans, P. (1992).
Language Learning from aCategorial Perspective.
Unpublished PhD thesis,University of Amsterdam.Adriaans, P. (1999).
Learning Shallow Context-FreeLanguages under Simple Distributions (TechnicalReport No.
PP-1999-13): Institute for Logic,Language and Computation, University ofAmsterdam.Berg-Kirkpatrick, T., C?t?, A.B., DeNero, J.
& Klein,D.
(2010).
Painless unsupervised learning withfeatures.
Proceedings of NAACL 2010, 582?590.Bybee, J. L. (1985).
Morphology: a study of therelation between meaning and form: JohnBenjamins.Bybee J. L. (2006).
From usage to grammar: the mind?sresponse to repetition.
Language, 82,711?733.Christodoulopoulos, C., Goldwater, S. & Steedman,M.
(2010).
Two Decades of Unsupervised POSinduction: How far have we come?
Proceedings ofEMNLP 2010, 575-584.Chrupala, G. &  Alishahi, A.
(2010).
Online Entropy-based Model of Lexical Category Acquisition.Proceedings of the 14th Conference onComputational Natural Language Learning(CoNLL-2010).Clark, A.
(2000).
Inducing syntactic categories bycontext distribution clustering.
Proceedings of theConference on Natural Language Learning(CONLL-2000), 91?94.Erkelens, M. (2008).
Restrictions of frequent framesas cues to categories: the case of Dutch.Supplement to the Proceedings of the 32nd BostonUniversity Conference on Language Development(BUCLD 32).Freitag, D. (2004).
Toward unsupervised whole-corpus tagging.
Proceedings of COLING-04, 357-363.Goldwater, S. & Griffiths, T. (2007).
A fullyBayesian approach to unsupervised part-of-speechtagging.
Proceedings of ACL 2007, 744?751,Leibbrandt, R. E., & Powers, D. M. W. (2008).Grammatical category induction using lexically-based templates.
Supplement to the Proceedings ofthe 32nd Boston University Conference onLanguage Development (BUCLD 32).Leibbrandt, R.E.
& Powers, D.M.
(2010).
FrequentFrames as Cues to Part-of-Speech in Dutch: WhyFiller Frequency Matters.
Proceedings of the 32ndAnnual Conference of the Cognitive ScienceSociety, 2680-2685.MacWhinney, B.
(2000).
The CHILDES Project:Tools for analyzing talk.
(3rd ed.
Vol.
2: Thedatabase).
Mahwah, NJ: Lawrence Erlbaum.Macnamara, J.
(1982).
Names for things: a study ofchild language.
Cambridge, MA: MIT Press.Madeira, S. C., & Oliveira, A. L. (2004).
Biclusteringalgorithms for biological data analysis: A survey.IEEE Transactions on Computational Biology andBioinformatics, 1(1), 24-45.53Maratsos, M. P., & Chalkley, M. A.
(1980).
Theinternal language of children's syntax: Theontogenesis and representation of syntacticcategories.
In K. E. Nelson (Ed.
), Children'sLanguage (Vol.
2).
New York: Gardner Press.Mintz, T. H. (2003).
Frequent frames as a cue forgrammatical categories in child directed speech.Cognition, 90(1), 91-117.Mintz, T. H., Newport, E. L., & Bever, T. G. (2002).The distributional structure of grammaticalcategories in speech to young children.
CognitiveScience, 26, 393-424.Moon, T., Erk, K. & Baldridge, J.
(2010) CrouchingDirichlet, Hidden Markov Model: UnsupervisedPOS Tagging with Context Local Tag Generation.Proceedings of EMNLP 2010, 196-206.Parisien, C., Fazly, A.
& Stevenson, S. (2008).
Anincremental Bayesian model for learning syntacticcategories.
Proceedings of the 12th Conference onComputational Natural Language Learning,(CONLL-2008).Pinker, S. (1984).
Language learnability andlanguage development.
Cambridge, MA: HarvardUniversity Press.Powers, D. M. W. (2003).
Recall and precision versusthe Bookmaker.
Paper presented at the 4thInternational Conference on Cognitive Science(ICCS).Redington, M., Chater, N., & Finch, S. (1998).Distributional information: A powerful cue foracquiring syntactic categories.
Cognitive Science,22(4), 425-469.Rosenberg, A.
& Hirschberg, J.
(2007).
V-Measure: AConditional Entropy-Based External ClusterEvaluation Measure.
Proceedings of the 2007 JointConference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL-2007), pp.410-420.Sch?tze, H. (1995) Distributional part-of-speechtagging.
Proceedings of EACL-95.Shanks, D.R.
(1995).
The psychology of associativelearning.
Cambridge University Press.St.
Clair, M.C., Monaghan, P. & Christiansen, M. H.(2010).
Learning grammatical categories fromdistributional cues: Flexible frames for languageacquisition.
Cognition, 116, 341-360.Theakston, A. L., Lieven, E., Pine, J. M., & Rowland,C.
F. (2001).
The role of performance limitations inthe acquisition of verb-argument structure: analternative account.
Journal of Child Language, 28,127-152.Van Mechelen, I.
& De Boeck, P. (2004).
Two-modeclustering methods: a structured overview.Statistical Methods in Medical Research, 13, 363-394.54
