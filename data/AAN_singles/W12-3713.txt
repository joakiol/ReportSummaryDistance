Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 84?88,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsPOLITICAL-ADS: An annotated corpus of event-level evaluativityKevin ReschkeDepartment of Computer ScienceStanford UniversityPalo Alto, CA 94305 USAreschkek@gmail.comPranav AnandDepartment of LinguisticsUniversity of California, Santa CruzSanta Cruz, CA 95064 USApanand@ucsc.eduAbstractThis paper presents a corpus targeting eval-uative meaning as it pertains to descriptionsof events.
The corpus, POLITICAL-ADS isdrawn from 141 television ads from the 2008U.S.
presidential race and contains 3945 NPsand 1549 VPs annotated for scalar sentimentfrom three different perspectives: the narra-tor, the annotator, and general society.
Weshow that annotators can distinguish these per-spectives reliably and that correlation betweenthe annotator?s own perspective and that of ageneric individual is higher than those withthe narrator.
Finally, as a sample application,we demonstrate that a simple compositionalmodel built off of lexical resources outper-forms a lexical baseline.1 IntroductionIn the past decade, the semantics of evaluative lan-guage has received renewed attention in both formaland computational linguistics (Martin and White,2005; Potts, 2005; Pang and Lee, 2008; Jackend-off, 2007).
This work has focused on evaluativityat either the lexical level or the phrasal/event levelstance, without bridging between the two.
A par-allel tradition of compositional event polarity ((Na-sukawa and Yi, 2003; Moilanen and Pulman, 2007;Choi and Cardie, 2008; Neviarouskaya et al, 2010))has grown up analogous to approaches to composi-tionality in formal semantics: event predicates arenot of constant polarity, but provide functions fromthe polarities of their arguments to event polarities.Little work exists assessing the relative advantagesof a compositional account, in part because no re-source annotating both NP level polarity and event-level polarity in context exists.
This paper intro-duces such a corpus, POLITICAL-ADS, a collec-tion of 2008 U.S. presidential race television adswith scalar sentiment annotations at the NP and VPlevel.
After describing the corpus creation and char-acteristics in sections 3 and 4, in section 5, we showthat a compositional system achieves an accuracy of84.2%, above a lexical baseline of 65.1%.2 BackgroundWhile many sentiment models handle negationquasi-compositionally (Pang and Lee, 2008; Polanyiand Zaenen, 2005), Nasukawa & Yi (Nasukawa andYi, 2003) first noted that predicates like preventare ?flippers?, conveying that their subject and ob-ject have opposite polarity ?
since trouble is nega-tive, something that prevents trouble is good.
Re-cent work has expanded that idea into a fully com-positional system (Moilanen and Pulman, 2007;Neviarouskaya et al, 2010).
Moilanen and Pulmanconstruct a system of compositional rules that buildspolarityin terms of a hand-built lexicon of predicatesas flippers or preservers.
However, this system con-flates two different assessment perspectives, that ofthe Narrator and of some mentioned NP (NP-to-NPperspective).
The latter include psychological pred-icates such as love and hate, and those of admira-tion or censure (e.g., admonish, praise).
Thus, theywould mark John dislikes scary movies as negative, acorrect NP-to-NP claim, but not necessarily correctfor the Narrator.
Recognizing this, Neviarouskayaet al (Neviarouskaya et al, 2010) develop a pair of84Announcer: In tough times, who will help Michigan?sauto industry?
Barack Obama favors loan guarantees tohelp Detroit retool and revitalize.
But John McCain re-fused to support loan guarantees for the auto industry.Now he?s just paying lip service.
Not talking straight.And McCain voted repeatedly for tax breaks for compa-nies that ship jobs overseas, selling out American anno-tators.
We just can?t afford more of the same.Figure 1: Transcript of POLITICAL-ADS ad #5714Figure 5: Snapshot of Mechanical Turk form for Transcript #57 (Dem.
)Figure 6: Instruction for completing annotation form.
Our Goals: The purpose of this HIT is to help us document the words people use to persuade others.
Overview: We ask you to read transcripts of political ads from the 2008 US presidential campaign (you can watch videos of the ads as well).
Then you will answer questions about different highlighted portions of the ad.
The questions are designed to determine how different pieces of text contribute to the overall message of the ad.
You will answer the same four questions for each highlighted portion: 1.
How does the narrator want you to feel about the highlighted expression?
2.
How do you to feel about the highlighted expression?
3.
In your opinion, how controversial is the highlighted expression in American society?Figure 2: POLITICAL-ADS annotation interfacecompositional rules over both perspectives.
Impor-tantly, neither of these appr ache have been vali-dated against a suffici ntly nuanced dataset.
M ila-nen and Pulman test against the SemEval-07 Head-lines Corpus, which asks annotators to give an over-all impression of sentiment.
This approach allows aheadline such as Outcry in N Korea ?nuclear test?to be arked negative, even though outcry overmilitary provocations is arguably good.
Similarly,Neviarouskaya et al evaluate only against NP-to-NP data as well.
While the MPQA corpus (Wiebeet al, 2005), which annotates the source of eachsentiment annotation, separates these two sentimentsources, work trained on it has not (Choi and Cardie,2008; Moilanen et al, 2010).
In addition, existingannotation schemes are not designed to tease apartperspectival differences.
For example, MPQA in-cludes a notion of Narrator-oriented evaluativity, butit does not include the perspectives of you and thegeneral public.3 The corpusPOLITICAL-ADS, is drawn from politics, a richand recently evolving domain for evaluativity re-search that we hypothesized would involve a highvolume of sentiment claims subject to perspecti-val differences.
POLITICAL-ADS is a collec-tion of 141 television ads that ran during the 2008U.S.
presidential race between Democratic candi-date Barack Obama and Republican candidate JohnMcCain.
The collection consists of 81 ads fromDemocratic side and 60 ads from Republican side.Figure 1 provides a sample transcript.Each transcript was parsed using the StanfordParser and all NPs and VPs excluding those headedby auxiliaries were extracted.
VP annotations wereassumed to represent phrasal/event-level polarityand NP ones argument-level polarity.
The annota-tion interface is shown in Figure 2.
Annotators wereshown a transcript and a movie clip, and navigatedthrough the NPs and VPs within the document.
Ateach point they were asked to rate their responseon a [-1,1] scale for the following four questionsabout the highlighted expression: 1) how the nar-rator wants them to feel; 2) how they feel; 3) howpeople in general feel; 4) how controversial the is-sue is (included to test the whether sense of contro-versy yields sharper differences between the variousassessment perspectives).
Finally, because phraseswere not prefiltered, a ?Doesn?t Make Sense?
buttonwas provided for each question.206 annotators on Mechanical Turk completed985 transcripts at $0.40 per transcript; each tran-script was annotated by an average of 4.8 differentannotators living in the U.S. We then filtered anno-tators by 200 phrases we deemed relatively uncon-troversial in 20 randomly selected transcripts.
To dothis, we scored each annotator in terms of the ab-solute difference between their mean response andthe median (each annotator?s scores were first nor-malized by mean absolute value) in the Narratorquestion.
We found when we thresholded annota-tors at a score above 0.5, agreement with our goldstandard was 83.5% and dropped substantially after-wards.
This threshold excluded 74 annotators, leav-ing 132 high-quality, or HQ, annotators (the full datais available in the corpus).The corpus consists of 5494 phrases (1549 VPsand 3945 NPs) annotated 6.3 times on average, fora total of 34, 692 annotations (9800 VP and 24892NP).
Each phrase was annotated by at least 3 HQannotators (average 3.9 annotators), and such an-notators contributed 5960 VP and 15238 NP an-85notations.
Of these, 12.1% HQ NP and 5.4% ofHQ VP responses were marked as ?Doesn?t MakeSense?
(DMS) for the narrator question.
In general,controversy and narrator questions had the highestand lowest rates of DMS, respectively; NPs showedhigher response rates than VPs; and HQ annotatorshad higher rates of button presses.1 In sections 4 and5, we will ignore the DMS responses.4 Corpus FindingsTable 1 provides summary statistics for the corpus.Across the board, the three perspective questions av-eraged close to 0, and in general HQ annotators arecloser to 0 (non-HQ annotators tended to providepositive responses).
VPs had slightly higher vari-ance than NPs, at marginal probability (p < .04),suggesting that VP responses were more extremethan NP ones.
You and Generic assessments arehighly correlated (Pearson?s ?
= 0.85), but Narra-tor is less so (?
= .76/.74).
All three are weaklycorrelated with Controversy (?
= .25/.26/.29 forNarr., You, Gen., respectively).
Narrator has thehighest standard deviations for the raw data, but thelowest for the normed data.
In the raw data, manyannotators recognized the narrators intensely parti-san views and rated accordingly (|x| > 0.8), butwere more tempered when providing their perspec-tive (|x| ?
0.35), leading to lower ?.
This intensitydifference is factored out in normalization, yieldingthe opposite pattern.The response data was collected from our anno-tators in scalar form, but applications (e.g., evalu-ative polarity classification) it is the polarity of theresponse that matters.
Ignoring magnitude, Table 3shows the polarity breakdown for all HQ phrasal an-notations.
Positive responses are the dominant classacross the board.
Neutral responses are less frequentfor Narrator than for the other types.
NPs have fewernegatives and more neutrals than VPs.Table 2 shows average standard deviations (i.e.,agreement) by worker, question, and XP type.
Noteboth that NPs show less variance than VPs and thatnon-HQ annotators less than HQ annotators (non-HQ annotators gave more 0 responses).1In a QUESTION + PHRASE TYPE + QUESTION + ANNOTA-TOR TYPE linear model with annotator as a random effect, allof the above effects are significant.
This was the simplest modelCOND ALL HQ ONLYRAW RAW NORMEDNarr.
.10 (.45) .05 (.62) .08 (.87)You .10 (.34) .06 (.46) .09 (.85)Gen. .10 (.33) .05 (.45) .08 (.86)Contr.
.17 (.22) .13 (.30) .17 (.60)Table 1: Mean response by category and worker typeCOND HQ ANNOTATORSRAW NORMEDALL VP NP ALL VP NPNarr.
.69 .75 .67 .96 1.06 .93You .57 .63 .55 .99 1.12 .94Gen.
.53 .58 .51 .99 1.13 .94Contr.
.53 .58 .51 1.01 1.15 .96ALL ANNOTATORSALL VP NPNarr.
.63 .68 .62You .54 .59 .53Gen.
.52 .56 .51Contr.
.54 .56Table 2: Average Standard Deviations For HQ and allannotators5 Comparing lexical and compositionaltreatmentsWhile compositional models of event-level evalua-tivity are logically defensible, the extent to whichthese models apply in the wild is an open ques-tion.
Because other compositional lexicons are notfreely available, we used the system described in(Reschke and Anand, 2011), which induces flippersand preservers from the MPQA subjectivity lexi-con and FrameNet (Ruppenhofer et al, 2005).
TheMPQA lexicon is a collection of over 8,000 wordsmarked for polarity.
Our functor lexicon uses thefollowing heuristic: verbs marked positive in MPQAare preservers; verbs marked negative are flippers.For example, dislike has negative MPQA polarity;therefore, it is marked as a flipper in our lexicon.This gives us 1249 predicates: 869 flippers and 380preservers.
329 additional verbs were added fromFrameNet according to their membership in five en-according to ?w model comparison.86COND POL VP NPNarr.
+ 2874 (51%) 6877 (51%)- 2654 (47%) 5590 (42%)0 111 (2%) 932 (7%)You + 2714 (49%) 6573 (50%)- 2466 (45%) 4967 (38%)0 337 (6%) 1575 (12%)Gen. + 2615 (48%) 6350 (49%)- 2541 (48%) 5125 (39%)0 332 (6%) 1558 (12%)Contr.
+ 3095 (57%) 6522 (51%)- 1755 (32%) 4159 (33%)0 558 (10%) 2051 (16%)Table 3: Polarity breakdowns for HQ annotationstailment classes (Reschke and Anand, 2011): verbsof injury/destruction, lacking, benefit, creation, andhaving.
124 frames across these classes were identi-fied, and then verbs of benefit, creation, and having(aid, generate, have) were marked as preservers andthe complement set (forget, arrest, lack) as flippers.As a lexical baseline, the MPQA polarity of eachverb was used ?
flippers correspond to baseline neg-ative events and preservers to positive ones.A 635 VP test subset of POLITICAL-ADS wasconstructed by omitting intransitive VPs and VPswith non-NP complements.
Gold standard labelswere determined from average normed HQ annota-tor data.
This yielded 329 positive, 284 negative,and 2 neutral events.
NPs, determined similarly, di-vided into 393 positive, 230 negative, and 12 neutral.Of the 635 VPs in the test set, only 272 (43.5%)are in our FrameNet/MPQA lexicon and we hencecompare the two systems on this subset.
On thissubset, the compositional system has an accuracy of84.2%, while the lexical baseline has an accuracyof 65.1%; there were 72 instances where the com-positional model outperformed the lexical baselineand 22 where the lexical outperformed the composi-tional.
Typical examples where the compositionalsystem won involve MPQA negatives like break,cut, and hate and positives like want and trust.
Thelexical model marks VPs like breaks the grip of for-eign oil and want a massive government as negativeand positive, respectively ?
because the NPs in ques-tion are negative, the answers should be reversed.
Incontrast, the lexical model wins on cases like growthe economy and reform Wall Street correct.
Theseexemplify a robust pattern in the errors: cases wherethe event is marked positive while the NP is markednegative.
In examples like grow Washington, theidea that grow is a preserver is reasonable.
However,in grow the economy, the negativity of the economyis arguably measuring the state of some constant en-tity.
While reform is marked positive in MPQA, itis arguably a reverser; this shows the problems withour lexicon induction.At an intuitive level, we expect agent evalu-ativity to mirror event-level evaluativity becausepositive/negative entities tend to commit posi-tive/negative acts, and this is borne out.
For flip-pers or preservers, the average VP evaluativity iscorrelated with the average subject evaluativity.
Forflippers the correlation is 0.57; for preservers it is0.52.
Although our model ignored subject evalua-tivity, we performed a generalized linear regressionwith subject and object evaluativity as predictorsand event-level evaluativity as outcome.
For flip-pers the regression coefficients were 0.52 for subject(p < 4e?
4) and?0.52 for object (p < 1e?
5).
Forpreservers the coefficients were 0.27 (p < 1e?5) forsubject and 0.93 for object (p < 2e?
7).
Thus, sub-ject polarity is an important factor for flipper events(e.g., the hero/villain defeated the enemy, but less sofor preservers (e.g.
the hero/villain helped the en-emy.
).6 ConclusionIn this paper we have presented POLITICAL-ADS,a new resource for investigating the relationships be-tween NP sentiment and VP sentiment systemati-cally.
We have demonstrated that annotators can re-liably annotate political data with sentiment at thephrasal level from multiple perspectives.
We havealso shown that in the present data set that self-reporting and judging generic positions are highlycorrelated, while correlation with narrators is ap-preciably weaker, as narrators are seen as more ex-treme.
We have also shown that the controversy of aphrase does not correlate with annotators?
disagree-ments with the narrator.
Finally, as a sample appli-cation, we demonstrated that a simple compositionalmodel built off of lexical resources outperforms apurely lexical baseline.87ReferencesY.
Choi and C Cardie.
2008.
Learning with compo-sitional semantics as structural inference for subsen-tential sentiment analysis.
In Proceedings of EMNLP2008.Ray Jackendoff.
2007.
Language, consciousness, cul-ture.
MIT Press.J.
R. Martin and P. R. R. White.
2005.
Language of Eval-uation: Appraisal in English.
Palgrave Macmillan.Karo Moilanen and Stephen Pulman.
2007.
Sentimentcomposition.
In Proceedings of RANLP 2007.K.
Moilanen, S. Pulman, and Y Zhang.
2010.
Packedfeelings and ordered sentiments: Sentiment pars-ing with quasi-compositional polarity sequencing andcompression.
In Proceedings of WASSA 2010, EACI2010.T.
Nasukawa and J. Yi.
2003.
Sentiment analysis: Cap-turing favorability using natural language processing.In Proceedings of the 2nd international conference onKnowledge capture.A.
Neviarouskaya, H. Prendinger, , and M. Ishizuka.2010.
Recognition of affect, judgment, and appreci-ation in text.
In Proceedings of COLING 2010.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.L.
Polanyi and A. Zaenen.
2005.
Contextual valenceshifters.
in computing attitude and affect in text.
InJanyce Wiebe James G. Shanahan, Yan Qu, editor,Computing Attitude and Affect in Text: Theory andApplication.
Springer Verlag, Dordrecht, The Nether-lands.Chris Potts.
2005.
The Logic of Conventional Implica-ture.
Oxford University Press.K.
Reschke and P. Anand.
2011.
Extracting contextualevaluativity.
In Proceedings of ICWS 2011.Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.Petruck, and Christopher R. Johnson.
2005.
Framenetii: Extended theory and practice.
Technical report,ICSI Technical Report.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.
InProceedings of LREC 2005.88
