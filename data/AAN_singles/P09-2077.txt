Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 305?308,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPExtending a Surface Realizer to Generate Coherent DiscourseEva BanikThe Open UniversityMilton Keynes, UKe.banik@open.ac.ukAbstractWe present a discourse-level Tree Adjoin-ing Grammar which tightly integrates syn-tax and discourse levels, including a repre-sentation for discourse entities.
We showthat this technique makes it possible toextend an optimisation algorithm used innatural language generation (polarity fil-tering) to the discourse level.
We imple-mented the grammar in a surface realizerand show that this technique can be usedto reduce the search space by filtering outreferentially incoherent solutions.1 IntroductionA fundamental problem that microplanners andsurface realizers face in natural language gener-ation is how to restrict the search space of possi-ble solutions.
A traditional solution to this compu-tational complexity problem is to divide the gen-eration process into tractable sub-problems, eachrepresented as a module in a pipeline, where everydecision made by a module restricts the number ofoptions available to others further down the line.Though such pipeline architectures are computa-tionally efficient, they severely restrict the flexibil-ity of the system and the quality of the generatedoutput.
Most systems with pipeline architecturesgenerate relatively simple, domain-specific out-put.
Systems that produce more complex linguis-tic constructions typically achieve this by addingmore modules to the pipeline (e.g.
a revision mod-ule (Robin, 1994) or aggregation (Shaw, 2002)).Since complex linguistic constructions often re-quire interaction between modules, adding them tothe repertoire of pipelined NLG systems becomesan engineering and programming task.Integrated NLG systems have a simpler archi-tecture because they do not need to model in-teractions between modules.
However, they stillface the problem of computational complexitythat was originally solved by the pipeline model.Strategies that have been introduced to reducethe search space in integrated systems includegreedy/incremental search algorithms (Stone etal., 2003), constructing a dependency graph for aflat semantic input and converting it into a deriva-tion tree (Koller and Striegnitz, 2002), using plan-ning algorithms (Appelt, 1985; Koller and Stone,2007), polarity filtering (Kow, 2007) and usingunderspecified g-derivation trees (G-TAG, Danlos(2000)).
Despite all these efforts, most systemsstill don?t attempt to go above the sentence levelor generate very complex sentences.
In this pa-per we present a new technique for designing anintegrated grammar for natural language genera-tion.
Using this technique it is possible to use lin-guistic constraints on referential coherence to au-tomatically reduce the search space ?
which inturn makes it possible to generate longer and morecoherent texts.First we extend the grammar of a surface real-izer to produce complex, multi-sentential output.Then we add a representation for discourse refer-ents to the grammar, inspired by Centering The-ory?s notion of a backward looking center and pre-ferred center.
Having done this, we show that byintegrating discourse-level representations into asyntactic grammar we can extend an optimizationtechnique ?
polarity filtering (Kow, 2007; Gar-dent and Kow, 2006) ?
from syntactic realizationto the discourse level.2 The Problem of Referential CoherenceReferential coherence is the phenomenon which isresponsible for the contrast in (1), in the sense thatthe example in (1b) is perceived to be more coher-ent than (1a).
(1) a Elixir is approved by the FDA.
Viralskin disorders are relieved by305Aliprosan.
Elixir is a white cream.Aliprosan is an ingredient of Elixir.b Elixir is a white cream.
Elixir isapproved by the FDA.
Elixir containsAliprosan.
Aliprosan relieves viral skindisorders.Centering Theory (Grosz et al, 1995) is a fre-quently used framework for modeling referentialcoherence in discourse.
It is based on the no-tion that for each utterance in a discourse thereis a set of entities which are the centers of atten-tion and which serve to link that utterance to otherutterances in the same discourse segment.
Enti-ties mentioned by an utterance (the set of forwardlooking centers) form a partially ordered list calledthe Cf list where roughly, subjects are ranked high-est, followed by objects, indirect objects and otherarguments or adjuncts.
The backward lookingcenter of Un is said to be the most highly rankedelement on the Cf list of Un-1 mentioned in theprevious utterance.Centering Theory has been adapted to NLG byKibble (1999; 2001), and implemented in Kib-ble and Power (2004).
Rather than using the no-tion of centering transitions as defined by Grosz etal.
(1995), in these papers centering theory is re-defined as constraints on salience and cohesion.These constraints state that there is a preferencefor consecutive utterances to keep the same centerand that there is a preference for the center of Unto be realized as the highest ranked entity on theCf list of Un.
Kibble and Power (2004) show howthese constraints can be used to drive text plan-ning, sentence planning and pronominalization inan integrated fashion.
Our approach is similar toKibble and Power (2004) in that we don?t use theconcept of centering transitions.
However, ourmethod is more efficient in that Kibble and Power(2004) use centering transitions to rank the set ofgenerated solutions (some of which are incoher-ent), whereas we encode centering constraints inelementary trees to reduce the search space of pos-sible solutions before we start computing them.3 GenI and Polarity FilteringThe grammar described in the next section wasimplemented in the GenI surface realizer (Kow,2007), which uses a lexicalized feature-based TreeAdjoining Grammar to generate all possible para-phrases for a given flat semantic input.
GenI im-plements an optimization technique called polar-h1:white-cream(e)DcHHHSHHNP?[idx:e]VPHHVisNPcream[idx:e]Punct.h2:contain(e,a)Dc[c:e]HHHDc?[c:e]DcHHHSHHNP?[idx:e]VPHHVcontainsNP?
[idx:a]Punct.Figure 1: Elementary syntax/discourse treesity filtering to constrain the effects of lexical am-biguity.
The basic idea of polarity filtering is toassociate elementary trees with a set of polarities.When these polarities don?t ?cancel each otherout?, it means that it is not possible to combinethe set of trees selected for a given input.
This isa quick way to check whether the number of ar-gument slots is the same as the number of poten-tial arguments.
For example, if the lexical selec-tion consists of two trees for a given input, one ofwhich provides an NP (-NP) and one of which ex-pects two NPs (-2NP) then the sum of polaritieswill be -NP and therefore the generator will notattempt to combine the trees.Values for polarities are defined as follows: ev-ery initial tree is assigned a -cat polarity for eachsubstitution node of category cat and a +cat po-larity if its root node is of category cat.
Auxiliarytrees are assigned a -cat polarity for each substi-tution node only.Polarity filtering is a very powerful optimiza-tion technique, because it allows the generator toreduce the search space early on in the process,before it attempts to combine any trees.4 An Integrated Syntax-DiscourseGrammarIn order to generate mutisentential text, we firstdefine a discourse-level Tree Adjoining Gram-mar.
The trees in the grammar tightly integratesyntax and discourse representations in the sensethat sentence-level elementary trees include oneor more discourse-level nodes.
The elementarytrees in Fig.
1 illustrate what we mean by this:every lexical item that would normally project asentence in a syntactic grammar (i.e., an S-rooted306+e +a -v +e -e +a -eDc[c:e]HHSHHNP?
[arg:e]VPHHVapproved byNP?[arg:f].Dc[c:a]HHHHDc?[c:v]SHHNP?
[arg:v]VPHHVrelieved byNP?[arg:a].Dc[c:e]HHHHDc?[c:e]SHHNP?
[arg:e]VPHHVisNPa cream.Dc[c:a]HHHHDc?[c:e]SHHHHNP?
[arg:a]VPHHVis ingredientofNP?
[arg:e].h3:approve(f,e)h6:relieve(a,v)h0:cream(e)h4:contain(e,a)+2a -vElixir is approved by the FDA.
Viral skin disorders are relieved by Aliprosan.
Elixir is a white cream.Aliprosan is an ingredient of Elixir.Figure 2: Discourse-level polarities for (1a) sum up to +2a -v+e -e +a -a +e +a -eDc[c:e]HHHHDc?[c:e]SHHNP?
[arg:e]VPHHVapproved byNP?[arg:f].Dc[c:a]HHHHDc?[c:a]SHHNP?[arg:a]VPHHVrelievesNP?[arg:v].Dc[c:e]HHSHHNP?
[arg:e]VPHHVisNPa cream.Dc[c:a]HHHHDc?[c:e]SHHHHNP?[arg:e]VPHHVcontainsNP?
[arg:a].h3:approve(f,e)h6:relieve(a,v)h0:cream(e)h4:contain(e,a)+aElixir is a white cream.
Elixir is approved by the FDA.
Elixir contains Aliprosan.
Aliprosan relievesviral skin disorders.Figure 3: Discourse-level polarities for (1b) sum up to +atree) here projects a discourse clause (i.e., a Dcrooted tree).
Every predicate that projects a dis-course clause is assigned two kinds of elementarytrees: a discourse initial tree (Fig.
1a) and a dis-course continuing tree (Fig.
1b), which takes thepreceding discourse clause as an argument.We model referential coherence by associatinga discourse entity with every root- and substitutionnode of category Dc.
A discourse entity on a rootnode is ?exported?
by the elementary tree to bethe center of attention in the next sentence.
Thisroughly corresponds to Centering Theory?s notionof a forward looking center.
A discourse entity ona substitution node is the entity expected by thesentence to have been the center of attention inthe previous utterance, roughly corresponding tothe notion of backward looking center in Center-ing Theory.For example, the tree on the left in Fig.
1. ex-ports the discourse entity representing its subject(?e?)
as its ?forward looking center?.
The tree onthe right in Fig.
1. is looking for a discourse en-tity called ?e?
as its ?backward looking center?
andexports the same discourse entity as its ?forwardlooking center?.
The combination of these twotrees therefore yields a coherent discourse, whichis expected to be continued with an utterance cen-tered on ?e?.5 Polarity Filtering on Discourse EntitiesBy treating discourse entities on Dcnodes as anadditional polarity key we can apply the polarityfiltering technique on the discourse level.
Thismeans we can filter out lexical selections thatwouldn?t lead to a coherent discourse the sameway as those lexical selections are filtered outwhich won?t lead to a syntactically well formedsentence.
To give an example, given the semanticrepresentation in Figure 4 potential realizations bya generator which is not aware of discourse coher-ence would include both of the examples in (1).As an experiment, we generated the above ex-ample using the same input but two differentgrammars.
In the first case we used a grammarwhich consists of discourse-level trees but no an-notations for discourse entities.
The realizer pro-307h0:white cream(e)h1:elixir(e)h2:fda(f)h3:approve(f e)h4:contain(e a)h5:aliprosan(a)h6:relieve(a v)h7:viral skin disorders(v)Figure 4: Input for the sentences in (1)duced 192 solutions, including many incoherentones such as (1a).
In the second case, we useda grammar with the same trees, but annotated withdiscourse referents.
In this case the realizer pro-duced only 16 solutions, all of which maintainedreferential coherence.
In the first case, the gram-mar provided 128 ways to associate trees with theinput (tree sets), and the 192 solutions includedall possible sentence orders.
Since for most treesin the grammar there are more than one ways toannotate them with discourse referents, in the sec-ond case the grammar contained more trees (dif-fering only in their discourse referent asignments).In this case there were 1536 tree sets selected forthe same input.
Of these, 1320 were discarded bypolarity filtering on discourse entities.
Of the re-maining 216 tree sets 200 were ruled out by fea-ture unification when the trees were combined.Figures 2 and 3 illustrate two sets of trees thatwere selected by the realizer, corresponding to theexamples in (1).
Discourse-level polarity filteringin this example (for the input in (4)) discards alltree sets whose polarities don?t sum up to one ofthe discourse entities, i.e., +e, +a, +f or +v.
Thepolarity of the tree set in Fig.2 is +2a -v so thetree set is discarded.
For the tree set in Fig.3 thepolarities sum up to +e and the realizer attemptsto combine the trees, which in this case leads to areferentially coherent solution (1b).The search space of the realizer can be furtherrestricted by only allowing tree sets whose polari-ties sum up to a specific discourse entity.
In thiscase the realizer will produce paragraphs wherethe center of attention in the last sentence is thediscourse entity used for polarity filtering.6 ConclusionsWe have described a discourse-level extension ofTree Adjoining Grammar which tightly integratessyntax with discourse and includes a representa-tion of discourse entities.
We have shown that in-cluding discourse entities in the grammar of a sur-face realizer improves the coherence of the gener-ated text and that these variables can also be usedin a very efficient optimization technique, polarityfiltering, to filter out referentially incoherent solu-tions.ReferencesD.E.
Appelt.
1985.
Planning English sentences.
Cam-bridge University Press, Cambridge.L.
Danlos.
2000.
G-TAG: A lexicalized formalism fortext generation inspired by Tree Adjoining Gram-mar.
In A. Abeille and O. Rambow, editors, TreeAdjoining Grammars: Formalisms, linguistic analy-sis and processing, pages 343?370.
CSLI, Stanford,CA.C.
Gardent and E. Kow.
2006.
Three reasons to adoptTAG-based surface realisation.
In Proceedings ofTAG+8), Sydney/Australia.B.J.
Grosz, A.K.
Joshi, and S Weinstein.
1995.
Cen-tering: a framework for modelling the local co-herence of discourse.
Computational Linguistics,21(2):203?225.R.
Kibble and R. Power.
2004.
Optimizing referentialcoherence in text generation.
Computational Lin-guistics, 30(4):401?416.R.
Kibble.
1999.
Cb or not Cb?
centering theory ap-plied to NLG.
In ACL workshop on Discourse andReference Structure, pages 72?81.R.
Kibble.
2001.
A reformulation of rule 2 of centeringtheory.
Comput.
Linguist., 27(4):579?587.A.
Koller and M. Stone.
2007.
Sentence generation asplanning.
In Proceedings of ACL.A.
Koller and K. Striegnitz.
2002.
Generation as de-pendency parsing.
In Proceedings of ACL.E.
Kow.
2007.
Surface realisation: ambiguity anddeterminism.
Ph.D. thesis, Universite de HenriPoincare - Nancy 1.J.
Robin.
1994.
Revision-based generation of Natu-ral Language Summaries providing historical Back-ground.
Ph.D. thesis, Columbia University.J.
Shaw.
2002.
Clause Aggregation: An approachto generating concise text.
Ph.D. thesis, ColumbiaUniversity.M.
Stone, C. Doran, B. Webber, T. Bleam, andM.
Palmer.
2003.
Microplanning with communica-tive intentions: The SPUD system.
ComputationalIntelligence, 19(4):311?381.308
