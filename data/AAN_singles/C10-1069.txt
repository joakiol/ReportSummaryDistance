Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608?616,Beijing, August 2010Phrase Clustering for Smoothing TM Probabilities ?
or, How toExtract Paraphrases from Phrase Tables1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford1National Research Council of Canada2University of Waterloo1First.Last@nrc.gc.ca; 2evan.stratford@gmail.comAbstractThis paper describes how to cluster to-gether the phrases of a phrase-based sta-tistical machine translation (SMT) sys-tem, using information in the phrase tableitself.
The clustering is symmetric andrecursive: it is applied both to source-language and target-language phrases,and the clustering in one language helpsdetermine the clustering in the other.
Thephrase clusters have many possible uses.This paper looks at one of these uses:smoothing the conditional translationmodel (TM) probabilities employed bythe SMT system.
We incorporatedphrase-cluster-derived probability esti-mates into a baseline loglinear featurecombination that included relative fre-quency and lexically-weighted condition-al probability estimates.
In Chinese-English (C-E) and French-English (F-E)learning curve experiments, we obtaineda gain over the baseline in 29 of 30 tests,with a maximum gain of 0.55 BLEUpoints (though most gains were fairlysmall).
The largest gains came with me-dium (200-400K sentence pairs) ratherthan with small (less than 100K sentencepairs) amounts of training data, contraryto what one would expect from the pa-raphrasing literature.
We have only be-gun to explore the original smoothingapproach described here.1 Introduction and Related WorkThe source-language and target-language ?phras-es?
employed by many statistical machine trans-lation (SMT) systems are anomalous: they arearbitrary sequences of contiguous words ex-tracted by complex heuristics from a bilingualcorpus, satisfying no formal linguistic criteria.Nevertheless, phrase-based systems perform bet-ter than word-based systems (Koehn 2010, pp.127-129).
In this paper, we look at what happenswhen we cluster together these anomalous butuseful entities.Here, we apply phrase clustering to obtain bet-ter estimates for ?backward?
probability P(s|t)and ?forward?
probability P(t|s), where s is asource-language phrase, t is a target-languagephrase, and phrase pair (s,t) was seen at leastonce in training data.
The current work is thusrelated to work on smoothing P(s|t) and P(t|s) ?see (Foster et al, 2006).
The relative frequencyestimates for P(s|t) and P(t|s) arettstsPRF /#),(#)|( = and stsstPRF /#),(#)|( = ,where #(s,t) denotes the number of times phrasepair (s,t) was observed, etc.
These estimates aretypically smoothed with ?lexical?
estimatesfound by breaking phrases s and t into words.We adopt a different idea, that of smoothingPRF(s|t) and PRF(t|s) with estimates obtained fromphrases that have similar meanings to s and t. Inour experiments, the two methods were com-bined, yielding an improvement over lexicalsmoothing alone ?
this indicates they providecomplementary information.
E.g., lexical esti-mates don?t work well for non-compositionalphrases like ?kick the bucket?
- our methodmight cluster this phrase with ?die?
and ?expire?and thus provide better smoothing.
The researchthat comes closest to ours is the work ofSchwenk et al (2007) on continuous space N-gram models, where a neural network is em-ployed to smooth translation probabilities.
How-ever, both Schwenk et al?s smoothing technique608and the system to which it is applied are quitedifferent from ours.Phrase clustering is also somewhat related towork on paraphrases for SMT.
Key papers in thisarea include (Bannard and Callison-Burch, 2005),which pioneered the extraction of paraphrasesfrom bilingual parallel corpora, (Callison-Burchet al, 2006) which showed that paraphrase gen-eration could improve SMT performance, (Calli-son-Burch, 2008) and (Zhao et al, 2008) whichshowed how to improve the quality of paraphras-es, and (Marton et al, 2009) which derived pa-raphrases from monolingual data using distribu-tional information.
Paraphrases typically helpSMT systems trained on under 100K sentencepairs the most.The phrase clustering algorithm in this paperoutputs groups of source-language and target-language phrases with similar meanings: paraph-rases.
However, previous work on paraphrasesfor SMT has aimed at finding translations forsource-language phrases in the system?s inputthat weren?t seen during system training.
Ourapproach is completely useless in this situation:it only generates new information for target orsource phrases that are already in the system?sphrase table.
Thus, we find paraphrases for manyof the source and target phrases that are in thephrase table, while the work cited above looksfor paraphrases of source phrases that are not inthe phrase table.Our work also differs from most work on pa-raphrases in that information is extracted notfrom sources outside the SMT system (e.g., pivotlanguages or thesauri) but from the system?sphrase table.
In this respect if no other, it is simi-lar to Chiang?s classic work on hierarchicalphrase-based systems (Chiang, 2005), thoughChiang was mining a very different type of in-formation from phrase tables.Because of all these differences between workon paraphrasing and the phrase clustering ap-proach, both in terms of the input informationand where they are best applied, we did not expe-rimentally compare the two approaches.2 Deriving Conditional Probabilitiesfrom Phrase ClustersGiven phrase clusters in the source and targetlanguages, how would one derive estimates forconditional probabilities P(s|t) and P(t|s)?
Weassume that the clustering is ?hard?
: each sourcephrase s belongs to exactly one cluster C(s), andeach target phrase t belongs to exactly onecluster C(t).
Some of these clusters will containsingleton phrases, and others will contain morethan one phrase.
Let ?#?
denote the total numberof observations in the training data associatedwith a phrase or phrase cluster.
E.g., suppose theEnglish cluster CS contains the three phrases?red?, ?dark red?, and ?burgundy?, with 50, 25,and 10 observations in the training datarespectively ?
then #(CS) = 85.
Also, let #(CS,CT)be the number of co-occurrences in the trainingdata of source-language cluster CS and target-language cluster CT.The phrase-cluster-based probabilities PPC are:)(#))(),((#)(#)(#))(|)(())(|()|(tCtCsCsCstCsCPsCsPtsPPC?=?=(1)and)(#))(),((#)(#)(#))(|)(())(|()|(sCtCsCtCtsCtCPtCtPstPPC?=?=(2)Note that the PPC will often be non-zero wherethe corresponding relative frequency estimatesPRF were zero (the opposite can?t happen).
Also,the PPC will be most useful where the phrase be-ing conditioned on was seldom seen in the train-ing data.
If t was seen 1,000 times during train-ing, the PRF(s|t) are reliable and don?t needsmoothing; but if t was seen 6 times,  PPC(s|t)may yield valuable extra information.
The samekind of argument applies to estimation of P(t|s).3 Clustering PhrasesWe used only information ?native?
to phrasetables to cluster phrases.
Two types of similaritymetric between phrases or phrase clusters wereemployed: count-based metrics and edit-basedmetrics.
The former are based on phrase co-occurrence counts; the latter are based on theword sequences that make up the phrases.
Eachhas its advantages.
Count-based metrics can de-duce from the similar translations of two phrasesthat they have similar meanings, despite dissimi-larity between the two word sequences ?
e.g.,they can deduce that ?red?
and ?burgundy?
be-long in the same cluster.
However, these metricsare unreliable when total counts are low, sincephrase co-occurrences are determined by a noisyalignment process.
Edit-based metrics are inde-pendent of how often phrases were observed.However, sometimes they can be fooled byphrases that have similar word sequences butdifferent meanings (e.g., ?the dog bit the man?609and ?the man bit the dog?, or ?walk on thebeach?
and ?don?t walk on the beach?).
In ourexperiments, we used a combination of count-based and edit-based metrics to cluster phrases(by simply multiplying the metrics together).However, we invested most of our effort in per-fecting the count-based component: our edit-based metric was fairly na?ve.If we rely mainly on count-based similaritybetween phrases to cluster them, and this kind ofsimilarity is most reliable when phrases havehigh counts, yet we need phrase-cluster-basedestimates most for phrases with low counts,aren?t we carrying out clustering on the phrasesthat need it least?
Our hope was that there is aclass of phrases with intermediate counts (e.g.,with 3-15 observations in the training data) thatcan be clustered reliably, but still benefit fromphrase-cluster-based probability estimates.3.1 Count-based clustering: overviewFigure 1 shows count-based phrase clustering.One first arbitrarily picks a language (eithersource or target) and then clusters together someof the phrases in that language.
One then switch-es to the other language and clusters phrases inthat language, then switches back to the first one,and so on until enough clustering has taken place.Each phrase or phrase cluster is represented bythe vector of its co-occurrence counts.
To calcu-late the similarity between two phrase clusters,one first normalizes their count vectors.
At thetop of Figure 1, source phrase s1 occurred 9times: 7 times aligned with target phrase t1, 2times aligned with t4.
For source similarity com-putation, the entry for (s1,t1) is normalized to 7/9= 0.78 and the entry for (s1,t4) is normalized to2/9 = 0.22 (these normalized values are shown inbrackets and italics after the counts).The two most similar normalized vectors atthe top of Figure 1 are those associated withphrases s1 and s2.
These phrases are merged byadding corresponding counts, yielding a newvector associated with the new phrase cluster {s1,s2}.
In real life, one would now do more source-language clustering on the source language side;in this example, we immediately proceed to tar-get-language clustering (carried out in target lan-guage space).
Note that the target similarity cal-culations are affected by the previous sourceclustering (because s1 and s2 are nowrepresented by the same coordinate, t3 and t4 arenow closer than they were in the initial table).
Inthis manner, we can iterate back and forth be-tween the two languages.
The final output is atable of joint phrase cluster counts, which is usedto estimate the PPC (see previous section).3.2 Count-based clustering: detailsCount-based similarity is computed as follows:1.
Phrase alignment is a noisy process, sowe first apply a transformation analogousto tf-idf in information retrieval (Saltonand McGill, 1986) to phrase clusterFigure 1: Example of phrase clustering610counts.
For source similarity computation,each co-occurrence count #(CS,CT) be-tween source cluster CS and target clusterCT is multiplied by a factor that reflectsthe information content of CT. Let#diff(CS) be number of clusters on thesource side, and let #[CT>0] for a par-ticular target cluster CT be the number ofsource clusters CS that co-occur with CT.Then let])0[/#)(log(#),(#),('# >?= TSTSTS CCdiffCCCC .Similarly, for target similarity computa-tion, let])0[/#)(log(#),(#),('# >?= STTSTS CCdiffCCCC .E.g., in source similarity computation, ifCT co-occurs with all source clusters, itscontribution will be set to zero (becauseit carries little information).2.
We normalize by dividing each vector oftf-idf counts ),('# TS CC  by the total num-ber of observations in the vector.3.
We compute the similarity between eachpair of tf-idf vectors using either the co-sine measure (Salton and McGill, 1986)or one of a family of probabilistic metricsdescribed below.4.
We cluster together the most similar vec-tors; this involves summing the unmodi-fied counts #(CS,CT) of the vectors (i.e.,the tf-idf transformation is only appliedfor the purposes of similarity calculationand is not retained).Now, we?ll describe the probabilistic metricswe considered.
For a count vector of dimensionD, u= (u1, u2, ?, uD), define a function)/log(...)/log()( 11 ??
?++?= i iDDi i uuuuuuI u .I(u) is a measure of how well the data in u aremodeled by the normalized vector (u1/?iui,  ?,uD/?iui).
Thus, when two count vectors u and vare merged (by adding them) we have the follow-ing measure of the loss in modeling accuracy:Probability Loss (PL):)()()(),( vuvuvu +?+= IIIPL .
(3)However, if we choose merges with the lowestPL, we will usually merge only vectors withsmall counts.
We are more interested in the aver-age impact of a merge, so we defineAverage Probability Loss (APL):)/())()()((),( ??
++?+= i ii i vuIIIAPL vuvuvu .
(4)In our initial experiments, APL worked betterthan PL.
However, APL had a strange side-effect.Most of the phrase clusters it induced made intui-tive sense, but there were typically three or fourclusters with large numbers of observations onboth language sides that grouped together phras-es with wildly disparate meanings.
Why doesAPL induce these ?monster clusters?
?Consider two count vectors u and v. If ?iui isvery big and ?ivi is small, then I(u) and I(u + v)will be very similar, and APL will be approx-imately I(v) /[?iui + ?ivi ] which will be close tozero.
Thus, the decision will probably be made tomerge u and v, even if they have quite differentsemantics.
The resulting cluster, whose countsare represented by u + v, is now even bigger andeven more likely to swallow up other small countvectors in the next rounds of merging: it becomesa kind of black hole.To deal with this problem, we devised anothermetric.
Let)/log(...)/log()|( 11 ??
?++?= i iDDi i vvuvvuI vu .This is a measure of how well the counts in vpredict the distribution of counts in u.
Then letMaximum Average Probability Loss (MAPL):))|()(,)|()(max(),( ?
?+?+?=i ii ivIIuIIMAPL vuvvvuuuvu.
(5)The first term inside the maximum indicates theaverage probability loss for an observation in uwhen it is modeled by u+v instead of u; similarly,the second term indicates the average probabilityloss for an observation in v. If we merge vectorpairs with the lowest values of MAPL, we willnever merge vectors in a way that will cause alarge loss to either of the two parents.In practice, we found that all these metricsworked better when multiplied by the Dice coef-ficient based distance.
For u and v, this is||||||21),(vuvuvu+??
?=Dice , where ?|u|?
meansthe number of non-zero count entries in u, and?| vu ?
|?
is the number of count entries that arenon-zero in u and v.3.3 Edit-based similarityIn most of our experiments, count-based metricswere combined with edit-based metrics; we putlittle effort into optimizing the edit metrics.
LetMCWS stand for ?maximum common word se-quence?.
For phrases p1 and p2, we define611)()()),((21),(212121 plenplenppMCWSlenppEdit+?
?= .
(6)where len() returns the number of  words.
Thismetric doesn?t take word identities into account;in future work, we may weight differences in-volving content words more heavily.We also defined an edit-based metric for dis-tance between phrase clusters.
Let cluster 1 havephrases ?red?
(10); ?burgundy?
(5); ?resemblingscarlet?
(2) and cluster 2 have ?dark burgundy?
(7); ?scarlet?
(3) (number of observations inbrackets).
What is the edit distance between clus-ters 1 and 2?
We defined the distance as that be-tween the two phrases with the most observa-tions in each cluster.
Thus, distance betweenclusters 1 and 2 would be Edit(?red?, ?dark bur-gundy?)=1.0.
Other definitions are possible.3.4 Examples of phrase clustersFigure 2 shows an English phrase cluster learnedduring C-E experiments by a metric combiningcount-based and edit-based information.
Eachphrase is followed by its count in brackets; wedon?t show phrases with low counts.
Since ouredit distance sees words as atoms (it doesn?tknow about morphology), the phrases containing?emancipating?
were clustered with phrases con-taining ?emancipation?
based on count informa-tion, rather than because of the common stem.Figure 3 shows part of a French phrase clusterlearned during F-E experiments by the samemixed metric.
The surface forms are quite varied,but most of the phrases mean ?to assure or toguarantee that something will happen?.
An inter-esting exception is ?pas faire?
?
it means not todo something (?pas?
is negative).
This illustrateswhy we need a better edit distance that heavilyweights negative words.emancipating (247), emancipate(167), emancipate our (73), emanci-pating thinking (67), emancipateour minds (46), further emancipate(45), emancipate the (38), emanci-pate the mind (38), emancipatingminds (33), emancipate their (32),emancipate their minds (27), eman-cipating our minds (24), emancipat-ing our (21), emancipate our mind(21), further emancipate our (19),emancipate our thinking (14), fur-ther emancipate their (11), emanci-pating the minds (9), emancipatethinking (8), unfettering (8) ...Figure 2: partial English phrase clustergarantir que (64), assurer que(46), veiller ?
ce que (27), afinde garantir (24), faire en sorte(19), de garantir que (16), afin degarantir que (14), faire des (14),de veiller ?
ce (14), s' assurerque (13), de veiller ?
ce que (13),pour garantir que (13), de faire ensorte (8), de faire en sorte que(7), ?
garantir que (6), pas faire(5), de veiller (5)?Figure 3:  partial French phrase cluster4 ExperimentsWe carried out experiments on a standard one-pass phrase-based SMT system with a phrasetable derived from merged counts of symme-trized IBM2 and HMM alignments; the systemhas both lexicalized and distance-based distor-tion components (there is a 7-word distortionlimit) and employs cube pruning (Huang andChiang, 2007).
The baseline is a loglinear featurecombination that includes language models, thedistortion components, relative frequency esti-mators PRF(s|t) and PRF(t|s) and lexical weightestimators PLW(s|t) and PLW(t|s).
The PLW() com-ponents are based on (Zens and Ney, 2004); Fos-ter et al (2006) found this to be the most effec-tive lexical smoothing technique.
The phrase-cluster-based components PPC(s|t) and PPC(t|s)are incorporated as additional loglinear featurefunctions.
Weights on feature functions arefound by lattice MERT (Macherey et al, 2008).4.1 DataWe evaluated our method on C-E and F-E tasks.For each pair, we carried out experiments ontraining corpora of different sizes.
C-E data werefrom the NIST1 2009 evaluation; all the allowedbilingual corpora except the UN corpus, HongKong Hansard and Hong Kong Law corpus wereused to estimate the translation model.
For C-E,we trained two 5-gram language models: the firston the English side of the parallel data, and thesecond on the English Gigaword corpus.Our C-E development set is made up mainlyof data from the NIST 2005 test set; it also in-cludes some balanced-genre web-text from theNIST training material.
Evaluation was per-formed on the NIST 2006 and 2008 test sets.Table 1 gives figures for training, developmentand test corpora for C-E tasks; |S| is the numberof sentences, and |W| is the number of words.There are four references for dev and test sets.1http://www.nist.gov/speech/tests/mt612Chi EngAll parallelTrain|S| 3.3M|W| 68.2M 66.5MDev |S| 1,506 1,506?4Test NIST06 |S| 1,664 1,664?4NIST08 |S| 1,357 1,357?4Gigaword |S| - 11.7MTable 1: Statistics for Chinese-to-English tasks.Fre EngTrain Europarl |S| 1.6M|W| 51.3M 46.6MDev 2008 |S| 2,051Test 2009 |S| 2,5252010 |S| 2,489GigaFrEn |S| - 22.5MTable 2: Statistics for French-to-English tasks.Lang (#sent) C-E (3.3M) F-E (1.6M)#count-1  #other  #count-1  #otherSrcBeforeclustering11.3M 5.7M 28.1M 21.2MAfterclustering11.3M 5.3M 28.1M 19.3M#clustered 0 0.4M 0 1.9MTgtBeforeclustering11.9M 6.0M 25.6M 20.4MAfterclustering11.9M 5.6M 25.6M 18.5M#clustered 0 0.4M 0 1.9MTable 3: # phrase classes before & after clustering.For F-E tasks, we used WMT 20102 F-E trackdata sets.
Parallel Europarl data are used fortraining; WMT Newstest 2008 set is the dev set,and WMT Newstest 2009 and 2010 are the testsets.
One reference is provided for each sourceinput sentence.
Two language models are used inthis task: one is the English side of the paralleldata, and the second is the English side of theGigaFrEn corpus.
Table 2 summarizes the train-ing, development and test corpora for F-E tasks.4.2 Amount of clustering and metricFor both C-E and E-F, we assumed that phrasesseen only once in training data couldn?t be clus-tered reliably, so we prevented these ?count 1?phrases from participating in clustering.
The key2http://www.statmt.org/wmt10/clustering parameter is the number of merge op-erations per iteration, given as a percentage ofthe number of potential same-language phrasepairs satisfying a simple criterion (some overlapin translations to the other language).
Prelimi-nary tests involving the FBIS corpus (about 8%of the C-E data) caused us to set this parameter at5%.
For C-E, we first clustered Chinese with this5% value, then English with the same amount.For F-E, we first clustered French, then English,using 5% in both cases.Table 3 shows the results.
Only 2-4% of thetotal phrases in each language end up in a cluster(that?s 6.5-9% of eligible phrases, i.e., of phrasesthat aren?t ?count 1?).
However, about 20-25%of translation probabilities are smoothed for bothlanguage pairs.
Based on these preliminary tests,we decided to use MAPLDiceEdit ??
( DMAPLEdit ? )
as our metric (thoughCosineEdit ?
was a close runner-up).4.3 Results and discussionOur evaluation metric is IBM BLEU (Papineni etal., 2002), which performs case-insensitivematching of n-grams up to n = 4.
Our first expe-riment evaluated the effects of the phrase cluster-ing features given various amounts of trainingdata.
Figure 4 gives the BLEU score improve-ments for the two language pairs, with results foreach pair averaged over two test sets (trainingdata size shown as #sentences).
The improve-ment is largest for medium amounts of trainingdata.
Since the F-E training data has more wordsper sentence than C-E, the two peaks would havebeen closer together if we?d put #words on the xaxis: improvements for both tasks peak around 6-8 M English words.
For more details, refer toTable 4 and Table 5.
The biggest improvementis 0.55 BLEU for the NIST06 test.
More impor-tantly, cluster features yield gains in 29 of 30experiments.
Surprisingly, a reviewer asked ifwe?d done significance tests on the individualresults shown in Tables 4 and 5.
Most likely,many of these individual results are insignificant,but so what?
Based on the tables, the probabilityof the null hypothesis that our method has noeffect is equivalent to that of tossing a fair coin30 times and getting 29 heads (if we adopt anindependence approximation).In the research on paraphrases cited earlier,paraphrases tend to be most helpful for smallamounts of training data.
By contrast, ourapproach seems to be most helpful for mediumamounts of training data (200-400K sentence613pairs).
We attribute this to the properties ofcount-based clustering.
When there is littletraining data, clustering is unreliable; when thereis much data, clustering is reliable but unneeded,because most relative frequencies are well-estimated.
In between, phrase cluster probabilityestimates are both reliable and useful.Figure 4: Average BLEU improvement for C-E andF-E tasks (each averaged over two tests) vs. #trainingsent.Finally, we carried out experiments to see ifsome of our earlier decisions were correct.
Werewe right to use DMAPL instead of cosine as thecount-based component of our metric?
Experi-ments with DMAPLEdit ?
vs.CosineEdit ?
on 400K C-E (tested on NIST06and NIST08) and on 200K F-E (tested on News-test2009 and 2010) showed a tiny advantage forDMAPLEdit ?
of about 0.06 BLEU.
So weprobably didn?t make the wrong decision here(though it doesn?t matter much).
Were we rightto include the Edit component?
Carrying out ana-logous experiments with DMAPLEdit ?
vs.DMAPL, we found that dropping Edit caused aloss of 0.1-0.2 BLEU for all four test sets.
Hereagain, we made the right decision.In a final experiment, we allowed ?count 1?phrases to participate in clustering (usingDMAPLEdit ?
).
The resulting C-E system hadsomewhat more clustered phrases than the pre-vious one (for both Chinese and English, about3.5% of phrases were in clusters compared to2.5% in the previous system).
To our surprise,this led to a slight improvement in BLEU: the400K C-E system now yielded 30.25 on NIST06(up 0.09) and 23.88 on NIST08 (up 0.13).
The F-E system where ?count 1?
clustering is allowedalso had more phrases in clusters than the systemwhere it?s prohibited (the former has just under10% of French and English phrases in clusters vs.Data sizeNist06 Nist08Baseline +phrase-clustering Improv.
Baseline +phrase-clustering Improv.25K 21.66 21.88 0.22 15.80 15.99 0.1950K 23.23 23.43 0.20 17.69 17.84 0.15100K 25.83 26.24 0.41 20.08 20.27 0.19200K 27.80 28.26 0.46 21.28 21.58 0.30400K 29.61 30.16 0.55 23.37 23.75 0.38800K 30.87 31.17 0.30 24.41 24.65 0.241.6M 32.94 33.10 0.16 25.61 25.72 0.113.3M 33.59 33.64 0.05 26.84 26.85 0.01Table 4: BLEU(%) scores for C-E with the various training corpora, including baseline results, results for withphrase clustering, and the absolute improvements.
Corpus size is measured in sentences.Data sizeNewstest2009 Newstest2010Baseline +phrase-clustering Improv.
Baseline +phrase-clustering Improv.25K 20.21 20.37 0.16 20.54 20.73 0.1950K 21.25 21.44 0.19 21.95 22.11 0.16100K 22.56 22.86 0.30 23.44 23.69 0.25200K 23.67 24.02 0.35 24.31 24.71 0.40400K 24.36 24.50 0.14 25.28 25.46 0.18800K 24.92 24.97 0.05 25.80 25.90 0.101.6M 25.47 25.47 0.00 26.35 26.37 0.02Table 5: BLEU(%) scores for F-E with the various training corpora, including baseline results without phraseclustering feature, results for phrase clustering, and the absolute improvements.6144% for the latter).
For F-E, the 200K system al-lowing ?count 1?
clustering again yielded aslightly higher BLEU: 24.07 on Newstest2009and 24.76 on Newstest2010 (up 0.05 in both cas-es).
Thus, our decision not to allow ?count 1?phrases to participate in clustering in the Table 4and 5 experiments appears to have been a mis-take.
We suspect we can greatly improve han-dling of ?count 1?
phrases ?
e.g., by weightingthe Edit component of the similarity metric moreheavily when assigning these phrases to clusters.5 Conclusion and Future WorkWe have shown that source-language and target-language phrases in the phrase table can be clus-tered, and that these clusters can be used tosmooth ?forward?
and ?backward?
estimatesP(t|s) and P(s|t), yielding modest but consistentBLEU gains over a baseline that included lexicalsmoothing.
Though our experiments were doneon a phrase-based system, this method could alsobe applied to hierarchical phrase-based SMT andsyntactic SMT systems.
There are several possi-bilities for future work based on new applica-tions for phrase clusters:?
In the experiments above, we usedphrase clusters to smooth P(t|s) and P(s|t)when the pair (s,t) was observed in train-ing data.
However, the phrase clustersoften give non-zero probabilities for P(t|s)and P(s|t) when s and t were both in thetraining data, but didn?t co-occur.
Wecould allow the decoder to consider such?invented?
phrase pairs (s,t).?
Phrase clusters could be used to con-struct target language models (LMs) inwhich the basic unit is a phrase clusterrather than a word.
For instance, a tri-cluster model would estimate the proba-bility of phrase p at time i as a functionof its phrase cluster, Ci(p), and the twopreceding phrase clusters Ci-1 and Ci-2:)|())(|()( 21 ??
?= iiii CCCfCfP ppp.?
Lexicalized distortion models could bemodified so as to condition distortionevents on phrase clusters.?
We could build SMT grammars in whichthe terminals are phrases and the parentsof terminals are phrase clusters.The phrase clustering algorithm describedabove could be improved in several ways:?
In the above, the edit distance betweenphrases and between phrase clusters wascrudely defined.
If we improve edit dis-tance, it will have an especially largeimpact on ?count 1?
phrases, for whichcount-based metrics are unreliable andwhich are a large proportion of all phras-es.
The edit distance between two phras-es weighted all words equally: preferably,weights for word substitution, insertion,or deletion would be learned from purelycount-derived phrase clusters (contentwords and negative words might haveheavier weights than other words).
Theedit distance between two phrase clusterswas defined as the edit distance betweenthe phrases with the most observations ineach cluster.
E.g., distance to the phrasecluster in Figure 2 is defined as thephrase edit distance to ?emancipating?.Instead, one could allow a cluster to becharacterized by (e.g.)
up to three phras-es, and let distance between two clustersbe the minimum or average pairwise editdistance between these characteristicphrases.?
To cluster phrases, we only used infor-mation derived from phrase tables.
In fu-ture, we could also use the kind of in-formation used in work on paraphrases,such as the context surrounding phrasesin monolingual corpora, entries in the-sauri, and information from pivot lan-guages.?
The phrase clustering above was ?hard?
:each phrase in either language belongs toexactly one cluster.
We could modifyour algorithms to carry out ?soft?
clus-tering.
For instance, we could interpolatethe probabilities associated with a phrasewith probabilities from its neighbours.?
Clustering is a primitive way of findinglatent structure in the table of jointphrase counts.
One could apply principalcomponent analysis or a related algo-rithm to this table.ReferencesC.
Bannard and C. Callison-Burch.
?Paraphrasingwith Bilingual Parallel Corpora?.
Proc.
ACL, pp.597-604, Ann Arbor, USA, June 2005.C.
Callison-Burch, P. Koehn, and M. Osborne.
?Im-proved Statistical Machine Translation Using Pa-raphrases?.
Proc.
HLT/NAACL, pp.
17-24, NewYork City, USA, June 2006.615C.
Callison-Burch.
?Syntactic Constraints on Paraph-rases Extracted from Parallel Corpora?.
Proc.EMNLP, pp.
196-205, Honolulu, USA, October2008.D.
Chiang.
?A hierarchical phrase-based model forstatistical machine translation?.
Proc.
ACL, pp.263-270, Ann Arbor, USA, June 2005.G.
Foster, R. Kuhn, and H. Johnson.
?Phrasetablesmoothing for statistical machine translation?.Proc.
EMNLP, pp.
53-61, Sydney, Australia, July2006.L.
Huang and D. Chiang.
?Forest Rescoring: FasterDecoding with Integrated Language Models?.Proc.
ACL, pp.
144-151, Prague, Czech Republic,June 2007.P.
Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, Cambridge, UK.W.
Macherey, F. Och, I. Thayer, and J.
Uszkoreit.
?Lattice-based Minimum Error Rate Training forStatistical Machine Translation?.
Proc.
EMNLP,pp.
725-734, Honolulu, USA, October 2008.Y.
Marton, C. Callison-Burch, and Philip Resnik.
?Improved Statistical Machine Translation UsingMonolingually-Derived Paraphrases?.
Proc.EMNLP, pp.
381-390, Singapore, August 2009.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
?Bleu:a method for automatic evaluation of machinetranslation?.
Proc.
ACL, pp.
311?318, Philadel-phia, July 2002.G.
Salton and M. McGill.
1986.
Introduction to Mod-ern Information Retrieval.
McGraw-Hill Inc., NewYork, USA.H.
Schwenk, M.
Costa-juss?, and J.
Fonollosa.
?Smooth Bilingual N-gram Translation?.
Proc.Joint EMNLP/CoNLL, pp.
430-438, Prague, CzechRepublic, June 2007.R.
Zens and H. Ney.
?Improvements in phrase-basedstatistical machine translation?.
Proc.
ACL/HLT,pp.
257-264, Boston, USA, May 2004.S.
Zhao, H. Wang, T. Liu, and S. Li.
?Pivot Approachfor Extracting Paraphrase Patterns from BilingualCorpora?.
Proc.
ACL/HLT, pp.
780-788, Colum-bus, USA, June 2008.616
