Proceedings of the 12th Conference of the European Chapter of the ACL, pages 69?76,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsIncremental Parsing with Parallel Multiple Context-Free GrammarsKrasimir AngelovChalmers University of TechnologyGo?teborg, Swedenkrasimir@chalmers.seAbstractParallel Multiple Context-Free Grammar(PMCFG) is an extension of context-freegrammar for which the recognition problem isstill solvable in polynomial time.
We describea new parsing algorithm that has the advantageto be incremental and to support PMCFGdirectly rather than the weaker MCFG formal-ism.
The algorithm is also top-down whichallows it to be used for grammar based wordprediction.1 IntroductionParallel Multiple Context-Free Grammar (PMCFG)(Seki et al, 1991) is one of the grammar formalismsthat have been proposed for the syntax of natural lan-guages.
It is an extension of context-free grammar(CFG) where the right hand side of the production ruleis a tuple of strings instead of only one string.
Using tu-ples the grammar can model discontinuous constituentswhich makes it more powerful than context-free gram-mar.
In the same time PMCFG has the advantage to beparseable in polynomial time which makes it attractivefrom computational point of view.A parsing algorithm is incremental if it reads the in-put one token at the time and calculates all possibleconsequences of the token, before the next token isread.
There is substantial evidence showing that hu-mans process language in an incremental fashion whichmakes the incremental algorithms attractive from cog-nitive point of view.If the algorithm is also top-down then it is possibleto predict the next word from the sequence of preced-ing words using the grammar.
This can be used forexample in text based dialog systems or text editors forcontrolled language where the user might not be awareof the grammar coverage.
In this case the system cansuggest the possible continuations.A restricted form of PMCFG that is still strongerthan CFG is Multiple Context-Free Grammar (MCFG).In Seki and Kato (2008) it has been shown thatMCFG is equivalent to string-based Linear Context-Free Rewriting Systems and Finite-Copying TreeTransducers and it is stronger than Tree AdjoiningGrammars (Joshi and Schabes, 1997).
Efficient recog-nition and parsing algorithms for MCFG have been de-scribed in Nakanishi et al (1997), Ljunglo?f (2004) andBurden and Ljunglo?f (2005).
They can be used withPMCFG also but it has to be approximated with over-generating MCFG and post processing is needed to fil-ter out the spurious parsing trees.We present a parsing algorithm that is incremental,top-down and supports PMCFG directly.
The algo-rithm exploits a view of PMCFG as an infinite context-free grammar where new context-free categories andproductions are generated during parsing.
It is trivial toturn the algorithm into statistical by attaching probabil-ities to each rule.In Ljunglo?f (2004) it has been shown that the Gram-matical Framework (GF) formalism (Ranta, 2004) isequivalent to PMCFG.
The algorithm was implementedas part of the GF interpreter and was evaluated with theresource grammar library (Ranta, 2008) which is thelargest collection of grammars written in this formal-ism.
The incrementality was used to build a help sys-tem which suggests the next possible words to the user.Section 2 gives a formal definition of PMCFG.
Insection 3 the procedure for ?linearization?
i.e.
thederivation of string from syntax tree is defined.
Thedefinition is needed for better understanding of the for-mal proofs in the paper.
The algorithm introductionstarts with informal description of the idea in section4 and after that the formal rules are given in section5.
The implementation details are outlined in section 6and after that there are some comments on the evalua-tion in section 7.
Section 8 gives a conclusion.2 PMCFG definitionDefinition 1 A parallel multiple context-free grammaris an 8-tuple G = (N,T, F, P, S, d, r, a) where:?
N is a finite set of categories and a positive integerd(A) called dimension is given for each A ?
N .?
T is a finite set of terminal symbols which is dis-joint with N .?
F is a finite set of functions where the arity a(f)and the dimensions r(f) and di(f) (1 ?
i ?a(f)) are given for every f ?
F .
For every posi-tive integer d, (T ?
)d denote the set of all d-tuples69of strings over T .
Each function f ?
F is a to-tal mapping from (T ?
)d1(f) ?
(T ?
)d2(f) ?
?
?
?
?
(T ?
)da(f)(f) to (T ?
)r(f), defined as:f := (?1, ?2, .
.
.
, ?r(f))Here ?i is a sequence of terminals and ?k; l?pairs, where 1 ?
k ?
a(f) is called argumentindex and 1 ?
l ?
dk(f) is called constituentindex.?
P is a finite set of productions of the form:A?
f [A1, A2, .
.
.
, Aa(f)]where A ?
N is called result category,A1, A2, .
.
.
, Aa(f) ?
N are called argument cat-egories and f ?
F is the function symbol.
Forthe production to be well formed the conditionsdi(f) = d(Ai) (1 ?
i ?
a(f)) and r(f) = d(A)must hold.?
S is the start category and d(S) = 1.We use the same definition of PMCFG as is used bySeki and Kato (2008) and Seki et al (1993) with theminor difference that they use variable names like xklwhile we use ?k; l?
to refer to the function arguments.As an example we will use the anbncn language:S ?
c[N ]N ?
s[N ]N ?
z[]c := (?1; 1?
?1; 2?
?1; 3?
)s := (a ?1; 1?, b ?1; 2?, c ?1; 3?
)z := (, , )Here the dimensions are d(S) = 1 and d(N) = 3 andthe arities are a(c) = a(s) = 1 and a(z) = 0.
 is theempty string.3 DerivationThe derivation of a string in PMCFG is a two-step pro-cess.
First we have to build a syntax tree of a categoryS and after that to linearize this tree to string.
The defi-nition of a syntax tree is recursive:Definition 2 (f t1 .
.
.
ta(f)) is a tree of category A ifti is a tree of category Bi and there is a production:A?
f [B1 .
.
.
Ba(f)]The abstract notation for ?t is a tree of category A?is t : A.
When a(f) = 0 then the tree does not havechildren and the node is called leaf.The linearization is bottom-up.
The functions in theleaves do not have arguments so the tuples in their defi-nitions already contain constant strings.
If the functionhas arguments then they have to be linearized and theresults combined.
Formally this can be defined as afunction L applied to the syntax tree:L(f t1 t2 .
.
.
ta(f)) = (x1, x2 .
.
.
xr(f))where xi = K(L(t1),L(t2) .
.
.L(ta(f))) ?iand f := (?1, ?2 .
.
.
?r(f)) ?
FThe function uses a helper function K which takes thealready linearized arguments and a sequence ?i of ter-minals and ?k; l?
pairs and returns a string.
The stringis produced by simple substitution of each ?k; l?
withthe string for constituent l from argument k:K ?
(?1?k1; l1?
?2?k2; l2?
.
.
.
?n) = ?1?k1l1?2?k2l2 .
.
.
?nwhere ?i ?
T ?.
The recursion in L terminates when aleaf is reached.In the example anbncn language the function z doesnot have arguments and it corresponds to the base casewhen n = 0.
Every application of s over another treet : N increases n by one.
For example the syntax tree(s (s z)) will produce the tuple (aa, bb, cc).
Finally theapplication of c combines all elements in the tuple ina single string i.e.
c (s (s z)) will produce the stringaabbcc.4 The IdeaAlthough PMCFG is not context-free it can be approx-imated with an overgenerating context-free grammar.The problem with this approach is that the parser pro-duces many spurious parse trees that have to be filteredout.
A direct parsing algorithm for PMCFG shouldavoid this and a careful look at the difference betweenPMCFG and CFG gives an idea.
The context-free ap-proximation of anbncn is the language a?b?c?
withgrammar:S ?
ABCA?
 | aAB ?
 | bBC ?
 | cCThe string ?aabbcc?
is in the language and it can bederived with the following steps:S?
ABC?
aABC?
aaABC?
aaBC?
aabBC?
aabbBC?
aabbC?
aabbcC?
aabbccC?
aabbcc70The grammar is only an approximation because thereis no enforcement that we will use only equal numberof reductions for A, B and C. This can be guaranteedif we replace B and C with new categories B?
and C ?after the derivation of A:B?
?
bB??
C ?
?
cC ??B??
?
bB???
C ??
?
cC ???B???
?
 C ???
?
In this case the only possible derivation from aaB?C ?is aabbcc.The PMCFG parser presented in this paper workslike context-free parser, except that during the parsingit generates fresh categories and rules which are spe-cializations of the originals.
The newly generated rulesare always versions of already existing rules wheresome category is replaced with new more specializedcategory.
The generation of specialized categories pre-vents the parser from recognizing phrases that are oth-erwise withing the scope of the context-free approxi-mation of the original grammar.5 ParsingThe algorithm is described as a deductive process inthe style of (Shieber et al, 1995).
The process derivesa set of items where each item is a statement about thegrammatical status of some substring in the input.The inference rules are in natural deduction style:X1 .
.
.
XnY< side conditions on X1, .
.
.
, Xn >where the premises Xi are some items and Y is thederived item.
We assume that w1 .
.
.
wn is the inputstring.5.1 Deduction RulesThe deduction system deals with three types of items:active, passive and production items.Productions In Shieber?s deduction systems thegrammar is a constant and the existence of a given pro-duction is specified as a side condition.
In our case thegrammar is incrementally extended at runtime, so theset of productions is part of the deduction set.
The pro-ductions from the original grammar are axioms and areincluded in the initial deduction set.Active Items The active items represent the partialparsing result:[kjA?
f [ ~B]; l : ?
?
?]
, j ?
kThe interpretation is that there is a function f with acorresponding production:A?
f [ ~B]f := (?1, .
.
.
?l?1, ?
?, .
.
.
?r(f))such that the tree (f t1 .
.
.
ta(f)) will produce the sub-string wj+1 .
.
.
wk as a prefix in constituent l for anyINITIAL PREDICTS ?
f [ ~B][00S ?
f [ ~B]; 1 : ??
]S - start category, ?
= rhs(f, 1)PREDICTBd ?
g[~C] [kjA?
f [ ~B]; l : ?
?
?d; r?
?
][kkBd ?
g[~C]; r : ??]?
= rhs(g, r)SCAN[kjA?
f [ ~B]; l : ?
?
s ?
][k+1j A?
f [ ~B]; l : ?
s ?
?
]s = wk+1COMPLETE[kjA?
f [ ~B]; l : ??
]N ?
f [ ~B] [kjA; l;N ]N = (A, l, j, k)COMBINE[ujA?
f [ ~B]; l : ?
?
?d; r?
?]
[kuBd; r;N ][kjA?
f [ ~B{d := N}]; l : ?
?d; r?
?
?
]Figure 1: Deduction Rulessequence of arguments ti : Bi.
The sequence ?
is thepart that produced the substring:K(L(t1),L(t2) .
.
.L(ta(f))) ?
= wj+1 .
.
.
wkand ?
is the part that is not processed yet.Passive Items The passive items are of the form:[kjA; l;N ] , j ?
kand state that there exists at least one production:A?
f [ ~B]f := (?1, ?2, .
.
.
?r(f))and a tree (f t1 .
.
.
ta(f)) : A such that the constituentwith index l in the linearization of the tree is equal towj+1 .
.
.
wk.
Contrary to the active items in the passivethe whole constituent is matched:K(L(t1),L(t2) .
.
.L(ta(f))) ?l = wj+1 .
.
.
wkEach time when we complete an active item, a pas-sive item is created and at the same time we cre-ate a new category N which accumulates all produc-tions forA that produce thewj+1 .
.
.
wk substring fromconstituent l. All trees of category N must producewj+1 .
.
.
wk in the constituent l.There are six inference rules (see figure 1).The INITIAL PREDICT rule derives one item spanningthe 0 ?
0 range for each production with the start cat-egory S on the left hand side.
The rhs(f, l) functionreturns the constituent with index l of function f .In the PREDICT rule, for each active item with dot be-fore a ?d; r?
pair and for each production for Bd, a newactive item is derived where the dot is in the beginningof constituent r in g.When the dot is before some terminal s and s is equalto the current terminal wk then the SCAN rule derives anew item where the dot is moved to the next position.71When the dot is at the end of an active item then itis converted to passive item in the COMPLETE rule.
Thecategory N in the passive item is a fresh category cre-ated for each unique (A, l, j, k) quadruple.
A new pro-duction is derived for N which has the same functionand arguments as in the active item.The item in the premise of COMPLETE was at somepoint predicted in PREDICT from some other item.
TheCOMBINE rule will later replace the occurence A in theoriginal item (the premise of PREDICT) with the special-ization N .The COMBINE rule has two premises: one active itemand one passive.
The passive item starts from positionu and the only inference rule that can derive items withdifferent start positions is PREDICT.
Also the passiveitem must have been predicted from active item wherethe dot is before ?d; r?, the category for argument num-ber d must have been Bd and the item ends at u. Theactive item in the premise of COMBINE is such an itemso it was one of the items used to predict the passiveone.
This means that we can move the dot after ?d; r?and the d-th argument is replaced with its specializationN .If the string ?
contains another reference to the d-thargument then the next time when it has to be predictedthe rule PREDICT will generate active items, only forthose productions that were successfully used to parsethe previous constituents.
If a context-free approxima-tion was used this would have been equivalent to unifi-cation of the redundant subtrees.
Instead this is done atruntime which also reduces the search space.The parsing is successful if we had derived the[n0S; 1;S?]
item, where n is the length of the text, S isthe start category and S?
is the newly created category.The parser is incremental because all active itemsspan up to position k and the only way to move to thenext position is the SCAN rule where a new symbol fromthe input is consumed.5.2 SoundnessThe parsing system is sound if every derivable item rep-resents a valid grammatical statement under the inter-pretation given to every type of item.The derivation in INITIAL PREDICT and PREDICT issound because the item is derived from existing pro-duction and the string before the dot is empty so:K ?
 = The rationale for SCAN is that ifK ?
?
= wj?1 .
.
.
wkand s = wk+1 thenK ?
(?
s) = wj?1 .
.
.
wk+1If the item in the premise is valid then it is based onexisting production and function and so will be the itemin the consequent.In the COMPLETE rule the dot is at the end of thestring.
This means that wj+1 .
.
.
wk will be not justa prefix in constituent l of the linearization but the fullstring.
This is exactly what is required in the semanticsof the passive item.
The passive item is derived froma valid active item so there is at least one productionfor A.
The category N is unique for each (A, l, j, k)quadruple so it uniquely identifies the passive item inwhich it is placed.
There might be many productionsthat can produce the passive item but all of them shouldbe able to generate wj+1 .
.
.
wk and they are exactlythe productions that are added to N .
From all this ar-guments it follows that COMPLETE is sound.The COMBINE rule is sound because from the activeitem in the premise we know that:K ?
?
= wj+1 .
.
.
wufor every context ?
built from the trees:t1 : B1; t2 : B2; .
.
.
ta(f) : Ba(f)From the passive item we know that every productionforN produces thewu+1 .
.
.
wk in r. From that followsthatK ??
(?
?d; r?)
= wj+1 .
.
.
wkwhere ??
is the same as ?
except that Bd is replacedwithN .
Note that the last conclusion will not hold if wewere using the original context because Bd is a moregeneral category and can contain productions that doesnot derive wu+1 .
.
.
wk.5.3 CompletenessThe parsing system is complete if it derives an itemfor every valid grammatical statement.
In our case wehave to prove that for every possible parse tree the cor-responding items will be derived.The proof for completeness requires the followinglemma:Lemma 1 For every possible syntax tree(f t1 .
.
.
ta(f)) : Awith linearizationL(ft1 .
.
.
ta(f)) = (x1, x2 .
.
.
xd(A))where xl = wj+1 .
.
.
wk, the system will derive an item[kjA; l;A?]
if the item [kjA ?
f [ ~B]; l : ?
?l] was pre-dicted before that.
We assume that the function defini-tion is:f := (?1, ?2 .
.
.
?r(f))The proof is by induction on the depth of the tree.If the tree has only one level then the function f doesnot have arguments and from the linearization defini-tion and from the premise in the lemma it follows that?l = wj+1 .
.
.
wk.
From the active item in the lemma72by applying iteratively the SCAN rule and finally theCOMPLETE rule the system will derive the requesteditem.If the tree has subtrees then we assume that thelemma is true for every subtree and we prove it for thewhole tree.
We know thatK ?
?l = wj+1 .
.
.
wkSince the function K does simple substitution it is pos-sible for each ?d; s?
pair in ?l to find a new range in theinput string j??k?
such that the lemma to be applicablefor the corresponding subtree td : Bd.
The terminals in?l will be processed by the SCAN rule.
Rule PREDICTwill generate the active items required for the subtreesand the COMBINE rule will consume the produced pas-sive items.
Finally the COMPLETE rule will derive therequested item for the whole tree.From the lemma we can prove the completeness ofthe parsing system.
For every possible tree t : S suchthat L(t) = (w1 .
.
.
wn) we have to prove that the[n0S; 1;S?]
item will be derived.
Since the top-levelfunction of the tree must be from production for S theINITIAL PREDICT rule will generate the active item inthe premise of the lemma.
From this and from the as-sumptions for t it follows that the requested passiveitem will be derived.5.4 ComplexityThe algorithm is very similar to the Earley (1970) algo-rithm for context-free grammars.
The similarity is evenmore apparent when the inference rules in this paperare compared to the inference rules for the Earley al-gorithm presented in Shieber et al (1995) and Ljunglo?f(2004).
This suggests that the space and time complex-ity of the PMCFG parser should be similar to the com-plexity of the Earley parser which is O(n2) for spaceand O(n3) for time.
However we generate new cate-gories and productions at runtime and this have to betaken into account.Let theP(j) function be the maximal number of pro-ductions generated from the beginning up to the statewhere the parser has just consumed terminal numberj.
P(j) is also the upper limit for the number of cat-egories created because in the worst case there will beonly one production for each new category.The active items have two variables that directly de-pend on the input size - the start index j and the endindex k. If an item starts at position j then there are(n ?
j + 1) possible values for k because j ?
k ?
n.The item also contains a production and there are P(j)possible choices for it.
In total there are:n?j=0(n?
j + 1)P(j)possible choices for one active item.
The possibilitiesfor all other variables are only a constant factor.
TheP(j) function is monotonic because the algorithm onlyadds new productions and never removes.
From thatfollows the inequality:n?j=0(n?
j + 1)P(j) ?
P(n)n?i=0(n?
j + 1)which gives the approximation for the upper limit:P(n)n(n+ 1)2The same result applies to the passive items.
The onlydifference is that the passive items have only a categoryinstead of a full production.
However the upper limitfor the number of categories is the same.
Finally theupper limit for the total number of active, passive andproduction items is:P(n)(n2 + n+ 1)The expression for P(n) is grammar dependent butwe can estimate that it is polynomial because the setof productions corresponds to the compact representa-tion of all parse trees in the context-free approximationof the grammar.
The exponent however is grammar de-pendent.
From this we can expect that asymptotic spacecomplexity will be O(ne) where e is some parameterfor the grammar.
This is consistent with the results inNakanishi et al (1997) and Ljunglo?f (2004) where theexponent also depends on the grammar.The time complexity is proportional to the numberof items and the time needed to derive one item.
Thetime is dominated by the most complex rule which inthis algorithm is COMBINE.
All variables that dependon the input size are present both in the premises andin the consequent except u.
There are n possible valuesfor u so the time complexity is O(ne+1).5.5 Tree ExtractionIf the parsing is successful we need a way to extract thesyntax trees.
Everything that we need is already in theset of newly generated productions.
If the goal item is[n0S; 0;S?]
then every tree t of category S?
that can beconstructed is a syntax tree for the input sentence (seedefinition 2 in section 3 again).Note that the grammar can be erasing; i.e., theremight be productions like this:S ?
f [B1, B2, B3]f := (?1; 1?
?3; 1?
)There are three arguments but only two of them areused.
When the string is parsed this will generate anew specialized production:S?
?
f [B?1, B2, B?3]Here S,B1 and B3 are specialized to S?, B?1 and B?3but the B2 category is still the same.
This is correct73because actually any subtree for the second argumentwill produce the same result.
Despite this it is some-times useful to know which parts of the tree were usedand which were not.
In the GF interpreter such un-used branches are replaced by meta variables.
In thiscase the tree extractor should check whether the cate-gory also exists in the original set of categories N inthe grammar.Just like with the context-free grammars the parsingalgorithm is polynomial but the chart can contain ex-ponential or even infinite number of trees.
Despite thisthe chart is a compact finite representation of the set oftrees.6 ImplementationEvery implementation requires a careful design of thedata structures in the parser.
For efficient access the setof items is split into four subsets: A, Sj , C and P. Ais the agenda i.e.
the set of active items that have to beanalyzed.
Sj contains items for which the dot is beforean argument reference and which span up to position j.C is the set of possible continuations i.e.
a set of itemsfor which the dot is just after a terminal.
P is the setof productions.
In addition the set F is used internallyfor the generatation of fresh categories.
The sets C,Sj and F are used as association maps.
They containassociations like k 7?
v where k is the key and v is thevalue.
All maps except F can contain more than onevalue for one and the same key.The pseudocode of the implementation is given infigure 2.
There are two procedures Init and Compute.Init computes the initial values of S, P and A. Theinitial agenda A is the set of all items that can be pre-dicted from the start category S (INITIAL PREDICT rule).Compute consumes items from the current agendaand applies the SCAN, PREDICT, COMBINE or COMPLETErule.
The case statement matches the current itemagainst the patterns of the rules and selects the properrule.
The PREDICT and COMBINE rules have twopremises so they are used in two places.
In both casesone of the premises is related to the current item and aloop is needed to find item matching the other premis.The passive items are not independent entities butare just the combination of key and value in the set F.Only the start position of every item is kept because theend position for the interesting passive items is alwaysthe current position and the active items are either inthe agenda if they end at the current position or theyare in the Sj set if they end at position j.
The activeitems also keep only the dot position in the constituentbecause the constituent definition can be retrieved fromthe grammar.
For this reason the runtime representationof the items is [j;A ?
f [ ~B]; l; p] where j is the startposition of the item and p is the dot position inside theconstituent.The Compute function returns the updated S and Psets and the set of possible continuations C. The set ofcontinuations is a map indexed by a terminal and theLanguage Productions ConstituentsBulgarian 3516 75296English 1165 8290German 8078 21201Swedish 1496 8793Table 1: GF Resource Grammar Library size in numberof PMCFG productions and discontinuous constituents0200400600800100012001 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39Number of TokensmsGerman Bulgarian Swedish EnglishFigure 3: Parser performance in miliseconds per tokenvalues are active items.
The parser computes the set ofcontinuations at each step and if the current terminal isone of the keys the set of values for it is taken as anagenda for the next step.7 EvaluationThe algorithm was evaluated with four languages fromthe GF resource grammar library (Ranta, 2008): Bul-garian, English, German and Swedish.
These gram-mars are not primarily intended for parsing but as aresource from which smaller domain dependent gram-mars are derived for every application.
Despite this, theresource grammar library is a good benchmark for theparser because these are the biggest GF grammars.The compiler converts a grammar written in thehigh-level GF language to a low-level PMCFG gram-mar which the parser can use directly.
The sizes ofthe grammars in terms of number of productions andnumber of unique discontinuous constituents are givenon table 1.
The number of constituents roughly cor-responds to the number of productions in the context-free approximation of the grammar.
The parser per-formance in terms of miliseconds per token is shown infigure 3.
In the evaluation 34272 sentences were parsedand the average time for parsing a given number of to-kens is drawn in the chart.
As it can be seen, althoughthe theoretical complexity is polynomial, the real-timeperformance for practically interesting grammars tendsto be linear.8 ConclusionThe algorithm has proven useful in the GF system.
Itaccomplished the initial goal to provide suggestions74procedure Init() {k = 0Si = ?, for every iP = the set of productions P in the grammarA = ?forall S ?
f [ ~B] ?
P do // INITIAL PREDICTA = A+ [0;S ?
f [ ~B]; 1; 0]return (S,P,A)}procedure Compute(k, (S,P,A)) {C = ?F = ?while A 6= ?
do {let x ?
A, x ?
[j;A?
f [ ~B]; l; p]A = A?
xcase the dot in x is {before s ?
T ?
C = C+ (s 7?
[j;A?
f [ ~B]; l; p+ 1]) // SCANbefore ?d; r?
?
if ((Bd, r) 7?
(x, d)) 6?
Sk then {Sk = Sk + ((Bd, r) 7?
(x, d))forall Bd ?
g[~C] ?
P do // PREDICTA = A+ [k;Bd ?
g[~C]; r; 0]}forall (k;Bd, r) 7?
N ?
F do // COMBINEA = A+ [j;A?
f [ ~B{d := N}]; l; p+ 1]at the end ?
if ?N.
((j, A, l) 7?
N ?
F) then {forall (N, r) 7?
(x?, d?)
?
Sk do // PREDICTA = A+ [k;N ?
f [ ~B]; r; 0]} else {generate fresh N // COMPLETEF = F+ ((j, A, l) 7?
N)forall (A, l) 7?
([j?;A?
?
f ?
[ ~B?
]; l?
; p?
], d) ?
Sj do // COMBINEA = A+ [j?;A?
?
f ?
[ ~B?
{d := N}]; l?
; p?
+ 1]}P = P+ (N ?
f [ ~B])}}return (S,P,C)}Figure 2: Pseudocode of the parser implementation75in text based dialog systems and in editors for con-trolled languages.
Additionally the algorithm has prop-erties that were not envisaged in the beginning.
Itworks with PMCFG directly rather that by approxima-tion with MCFG or some other weaker formalism.Since the Linear Context-Free Rewriting Systems,Finite-Copying Tree Transducers and Tree AdjoiningGrammars can be converted to PMCFG, the algorithmpresented in this paper can be used with the convertedgrammar.
The approach to represent context-dependentgrammar as infinite context-free grammar might be ap-plicable to other formalisms as well.
This will make itvery attractive in applications where some of the otherformalisms are already in use.ReferencesHa?kan Burden and Peter Ljunglo?f.
2005.
Parsinglinear context-free rewriting systems.
In Proceed-ings of the Ninth International Workshop on ParsingTechnologies (IWPT), pages 11?17, October.Jay Earley.
1970.
An efficient context-free parsing al-gorithm.
Commun.
ACM, 13(2):94?102.Aravind Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In Grzegorz Rozenberg andArto Salomaa, editors, Handbook of Formal Lan-guages.
Vol 3: Beyond Words, chapter 2, pages 69?123.
Springer-Verlag, Berlin/Heidelberg/New York.Peter Ljunglo?f.
2004.
Expressivity and Complexity ofthe Grammatical Framework.
Ph.D. thesis, Depart-ment of Computer Science, Gothenburg Universityand Chalmers University of Technology, November.Ryuichi Nakanishi, Keita Takada, and Hiroyuki Seki.1997.
An Efficient Recognition Algorithm for Mul-tiple ContextFree Languages.
In Fifth Meetingon Mathematics of Language.
The Association forMathematics of Language, August.Aarne Ranta.
2004.
Grammatical Framework: AType-Theoretical Grammar Formalism.
Journal ofFunctional Programming, 14(2):145?189, March.Aarne Ranta.
2008.
GF Resource Grammar Library.digitalgrammars.com/gf/lib/.Hiroyuki Seki and Yuki Kato.
2008.
On the Genera-tive Power of Multiple Context-Free Grammars andMacro Grammars.
IEICE-Transactions on Info andSystems, E91-D(2):209?221.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,and Tadao Kasami.
1991.
On multiple context-free grammars.
Theoretical Computer Science,88(2):191?229, October.Hiroyuki Seki, Ryuichi Nakanishi, Yuichi Kaji,Sachiko Ando, and Tadao Kasami.
1993.
Par-allel Multiple Context-Free Grammars, Finite-StateTranslation Systems, and Polynomial-Time Recog-nizable Subclasses of Lexical-Functional Grammars.In 31st Annual Meeting of the Association for Com-putational Linguistics, pages 130?140.
Ohio StateUniversity, Association for Computational Linguis-tics, June.Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and Implementation ofDeductive Parsing.
Journal of Logic Programming,24(1&2):3?36.76
