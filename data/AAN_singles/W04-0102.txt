Non-locality all the way through:Emergent Global Constraints in the Italian Morphological LexiconVito PirrelliIstituto di Linguistica ComputazionaleCNR, Pisa, Italyvito.pirrelli@ilc.cnr.itBasilio CalderoneLaboratorio di LinguisticaScuola Normale Superiore, Pisa, Italyb.calderone@sns.itIvan HerrerosIstituto di Linguistica ComputazionaleCNR, Pisa, Italyivan.herreros@ilc.cnr.itMichele VirgilioDipartimento di FisicaUniversit?
degli Studi di Pisa, Italyvirgilio@df.unipi.itAbstractThe paper reports on the behaviour of a Koho-nen map of the mental lexicon, monitoredthrough different phases of acquisition of theItalian verb system.
Reported experiments ap-pear to consistently reproduce emergent globalordering constraints on memory traces of in-flected verb forms, developed through princi-ples of local interactions between parallelprocessing neurons.1 IntroductionOver the last 15 years, considerable evidence hasaccrued on the critical role of paradigm-based rela-tions as an order-principle imposing a non-localorganising structure on word forms memorised inthe speaker?s mental lexicon, facilitating their re-tention, accessibility and use, while permitting thespontaneous production and analysis of novelwords.
A number of theoretical models of the men-tal lexicon have been put forward to deal with therole of these global constraints in i) setting an up-per bound on the number of possible forms aspeaker is ready to produce (Stemberger and Car-stairs, 1988), ii) accounting for reaction times inlexical decision and related tasks (Baayen et al1997; Orsolini and Marslen-Wilson, 1997 and oth-ers), iii) explaining production errors by bothadults and children (Bybee and Slobin, 1982; By-bee and Moder; 1983; Orsolini et al, 1998) and iv)accounting for human acceptability judgementsand generalisations over nonce verb stems (Sayand Clahsen, 2001).
While most of these modelsshare some core assumptions, they appear tolargely differ on the role played by lexical relationsin word storage, access and processing.
Accordingto the classical view (e.g.
Taft, 1988) the relation-ship between regularly inflected forms is directlyencoded as lexical procedures linking inflectionalaffixation to separately encoded lexical roots.
Ir-regular word forms, on the other hand, are storedin full (Prasada and Pinker, 1993).
In contrast tothis view, associative models of morphologicalprocessing claim that words in the mental lexiconare always listed as full forms, establishing an in-terconnected network of largely redundant linguis-tic data reflecting similarities in meaning and form(Bybeee, 1995).Despite the great deal of experimental evidencenow available, however, we still seem to know toolittle of the dynamic interplay between morpho-logical learning and the actual working of thespeaker?s lexicon to draw conclusive inferencesfrom experimental findings.
Associative models,for example, are generally purported to be unableto capture morpheme-based effects of morphologi-cal storage and access.
Thus, if humans are shownto access the mental lexicon through morphemes,so the argument goes, then associative models ofthe mental lexicon cannot be true.
In fact, if asso-ciative models can simulate emergent morpheme-based effects of lexical organisation through stor-age of full forms, then this conclusion is simplyunwarranted.We believe that computer simulations of mor-phology learning can play a role in this dispute.However, there have been comparatively few at-tempts to model the way global ordering principlesof lexical organisation interact with (local) proc-essing strategies in morphology learning.
In thepresent paper, we intend to simulate a biologically-inspired process of paradigm-based self-organisation of inflected verb forms in a Kohonenmap of the Italian mental lexicon, built on the basisof local processes of memory access and updating.Before we go into that, we briefly overview rele-vant machine learning work from this perspective.Barcelona, July 2004Association for Computations LinguisticsACL Special Interest Group on Computational Phonology (SIGPHON)Proceedings of the Workshop of the2 BackgroundLazy learning methods such as the nearestneighbour algorithm (van den Bosch et al, 1996)or the analogy-based approach (Pirrelli andFederici, 1994; Pirrelli and Yvon, 1999) requirefull storage of supervised data, and make on-lineuse of them with no prior or posterior lexical struc-turing.
This makes this class of algorithms flexibleand efficient, but comparatively noise-sensitiveand rather poor in simulating emergent learningphenomena.
There is no explicit sense in which thesystem learns how to map new exemplars to al-ready memorised ones, since the mapping functiondoes not change through time and the only incre-mental pay-off lies in the growing quantity of in-formation stored in the exemplar data-base.Decision tree algorithms (Quinlan, 1986), on theother hand, try to build the shortest hierarchicalstructure that best classifies the training data, usinga greedy heuristics to select the most discrimina-tive attributes near the root of the hierarchy.
Asheuristics are based on a locally optimal splittingof all training data, adding new training data maylead to a dramatic reorganisation of the hierarchy,and nothing is explicitly learned from having builta decision tree at a previous learning stage (Lingand Marinov, 1993).To tackle the issue of word structure moresquarely, there has been a recent upsurge of inter-est in global paradigm-based constraints on mor-phology learning, as a way to minimise the rangeof inflectional or derivational endings heuristicallyinferred from raw training data (Goldsmith, 2001;Gaussier, 1999; Baroni, 2000).
It should be noted,however, that global, linguistically-inspired con-straints of this sort do not interact with morphologylearning in any direct way.
Rather, they are typi-cally used as global criteria for optimal conver-gence on an existing repertoire of minimally re-dundant sets of paradigmatically related mor-phemes.
Candidate morpheme-like units are ac-quired independently of paradigm-based con-straints, solely on the basis of local heuristics.Once more, there is no clear sense in which globalconstraints form integral part of learning.Of late, considerable attention has been paid toaspects of emergent morphological structure andcontinuous compositionality in multi-layered per-ceptrons.
Plaut et al (1996) show how a neuralnetwork comes to be sensitive to degrees of com-positionality on the basis of exposure to examplesof inputs and outputs from a word-reading task.Systematic input-output pairs tend to establish aclear one-to-one correlation between parts of inputand parts of output representations, thus develop-ing strongly compositional analyses.
By the sametoken, a network trained on inputs with gradedmorphological structure develops representationswith corresponding degrees of compositionality(Rueckl and Raveh, 1999).
It must be appreciatedthat most such approaches to incremental com-postionality are task-oriented and highly super-vised.
Arguably, a better-motivated and more ex-planatory approach should be based on self-organisation of input tokens into morphologicallynatural classes and their time-bound specialisationas members of one such class, with no external su-pervision.
Kohonen?s Self-Organising Maps(SOMs) (Kohonen, 1995) simulate self-organisation by structuring input knowledge on a(generally) two-dimensional grid of neurons,whose activation values can be inspected by theresearcher both instantaneously and through time.In the remainder of this paper we show that we canuse SOMs to highlight interesting aspects of globalmorphological organisation in the learning of Ital-ian conjugation, incrementally developed throughlocal interactions between parallel processing neu-rons.3 SOMsSOMs can project input tokens, represented asdata points of an n-dimensional input space, onto agenerally two-dimensional output space (the mapgrid) where similar input tokens are mapped ontonearby output units.
Each output unit in the map isassociated with a distinct prototype vector, whosedimensionality is equal to the dimensionality of in-put vectors.
As we shall see, a prototype vector isan approximate memory trace of recurring inputs,and plays the role of linking its corresponding out-put unit to a position in the input space.
Accord-ingly, each output unit takes two positions: one inthe input space (through its prototype vector) andone in the output space (its co-ordinates on themap grid).SOMs were originally conceived of as computermodels of somatotopic brain maps.
This explainswhy output units are also traditionally referred toas neurons.
Intuitively, a prototype vector repre-sents the memorised input pattern to which its as-sociated neuron is most sensitive.
Through learn-ing, neurons gradually specialise in selectively be-ing associated with specific input patterns.
More-over, memorised input patterns tend to cluster onthe map grid so as to reflect natural classes in theinput space.These interesting results are obtained through it-erative unsupervised exposure to input tokens.
Ateach learning step, a SOM is exposed to a singleinput token and goes through the following twostages: a) competitive neuron selection, and b)adaptive adjustment of prototype vectors.
As weshall see in more detail in the remainder of thissection, both stages are local and incremental insome crucial respects.13.1 Stage 1: competitive selectionLet vx be the n-dimension vector representationof the current input.
At this stage, the distance be-tween each prototype vector and vx is computed.The output unit b that happens to be associatedwith the prototype vector vb closest to vx is selectedas the best matching unit.
More formally:{ }ixbx vvvv ???
min ,where   is also known as the quantization errorscored by vb relative to vx.
Intuitively, this is to saythat, although b is the map neuron reacting mostsensitively to the current stimulus, b is not (yet)perfectly attuned to vx.Notably, the quantization error is a local distancefunction, as it involves two vector representationsat a time.
Hence, competitive selection is blind togeneral structural properties of the input space,such as the comparative role of each dimension indiscriminating input tokens.
This makes competi-tive selection prone to errors due to accidental orspurious similarity between the input vector andSOM prototype vectors.3.2 Stage 2: adaptive adjustmentAfter the winner unit b is selected at time t, theSOM locally adapts prototype vectors to the cur-rent stimulus.
Vector adaptation applies locally,within a kernel area of radius r, centred on the po-sition of b on the map grid.
Both vb(t) (vb at time t)and the prototype vectors associated with b?s ker-nel units are adjusted to make them more similar tovx(t) (vx at time t).
In particular, for each prototypevector vi in b?s kernel and the input vector vx, thefollowing adaptive function is used[ ])()()()1( tvtvhtvtv ixbiii ?+=+  ,where hbi is the neighbourhood kernel centredaround the winner unit b at time t, a non-increasingfunction of both time and the distance between theinput vi and the winner vector vb.
As learning timeprogresses, however, hbi decreases, and prototypevector updates become less sensitive to input con-ditions, according to the following:1 This marks a notable difference between SOMs andother classical projection techniques such as VectorAnalysis or Multi-dimensional Scaling, which typicallywork on the basis of global constraints on the overalldistribution of input data (e.g.
by finding the space pro-jection that maximizes data variance/co-variance).
)(),()( ttllhth ibbi ??
?= ,where lb and li are, respectively, the position of band its kernel neurons on the map grid, and ?
(t) isthe learning rate at time t, a monotonically decreas-ing function of t. Interaction of these functionssimulates effects of memory entrenchment andproto-typicality of early input data.3.3 SummaryThe dynamic interplay between locality and in-crementality makes SOMs plausible models ofneural computation and data compression.
Theirsensitivity to frequency effects in the distributionof input data allows the researcher to carefully testtheir learning behaviour in different time-boundconditions.
Learning makes output units increas-ingly more reactive to already experienced stimuliand thus gradually more competitive for selection.If an output unit is repeatedly selected by system-atically occurring input tokens, it becomes associ-ated with a more and more faithful vector represen-tation of a stimulus or class of stimuli, to becomean attractor for its neighbouring area on the map.As a result, the most parsimonious global organisa-tion of input data emerges that is compatible witha) the size of the map grid, b) the dimensionality ofoutput units and c) the distribution of input data.This intriguing dynamics persuaded us to useSOMs to simulate the emergence of non-local lexi-cal constraints from local patterns of interconnec-tivity between vector representations of full wordforms.
The Italian verb system offers a particularlyrich material to put this hypothesis to the challeng-ing test of a computer simulation.4 The Italian Verb SystemThe Italian conjugation is a complex inflectionalsystem, with a considerable number of classes ofregular, subregular and irregular verbs exhibitingdifferent probability densities (Pirrelli, 2000; Pir-relli and Battista, 2000).
Traditional descriptivegrammars (e.g.
Serianni, 1988) identify three mainconjugation classes (or more simply conjugations),characterised by a distinct thematic vowel (TV),which appears between the verb root and the in-flectional endings.
First conjugation verbs have theTV -a- (parl-a-re 'speak'), second conjugationverbs have the TV -e- (tem-e-re 'fear'), and thirdconjugation verbs -i- (dorm-i-re 'sleep').
The firstconjugation is by far the largest class of verbsTYPE EXAMPLE ENGLISH GLOSS[isk]-insertion + palatalization fi"nisko/fi"niSSi/fi"njamo (I)/(you)/(we) end[g]-insertion + diphthongization "vEngo/"vjEni/ve"njamo (I)/(you)/(we) comeablauting + velar palatalization "Esko/"ESSi/uS"Samo (I)/(you)/(we) go out[r]-drop + diphthongization "mwojo/"mwori/mo"rjamo (I)/(you)/(we) dieTable 1.
Variable stem alternations in the Italian present indicative.
(73% of all verbs listed in De Mauro et al, 1993),almost all of which are regular.
Only very few 1stconjugation verbs have irregularly inflected verbforms: andare 'go', dare 'give', stare 'stay' and fare?do, make?.
It is also the only truly productiveclass.
Neologisms and foreign loan words all fallinto it.
The second conjugation has far fewermembers (17%), which are for the most part ir-regular (around 95%).
The third conjugation is thesmallest class (10%).
It is mostly regular (around10% of its verbs are irregular) and only partiallyproductive.Besides this macro-level of paradigmatic or-ganisation, Italian subregular verbs also exhibitubiquitous patterns of stem alternations, wherebya change in paradigm slot triggers a simultaneouschange of verb stem and inflectional ending, asillustrated in Table 1 for the present indicative ac-tive.
Pirrelli and Battista (2000) show that phe-nomena of Italian stem alternation, far from beingaccidental inconsistencies of the Italian morpho-phonology, define stable and strikingly conver-gent patterns of variable stem formation (Aronoff,1994) throughout the entire verb system.
The pat-terns partition subregular Italian verbs intoequivalence micro-classes.
In turn, this can be in-terpreted as suggesting that inter-class consistencyplays a role in learning and may have exerted aconvergent pressure in the history of the Italianverb system.
If a speaker has heard a verb only inambiguous inflections (i.e.
inflections that are in-dicators of more than one verb micro-class), (s)hewill need to guess, in order to produce unambigu-ous forms.
Guesses are made on the basis of fre-quently attested verb micro-classes (Albright,2002).5 Computer simulationsThe present experiments were carried out usingthe SOM toolbox (Vesanto et al, 2000), devel-oped at the Neural Networks Research Centre ofHelsinki University of Technology.
The toolboxpartly forced some standard choices in the trainingprotocol, as discussed in more detail in the follow-ing sections.
In particular, we complied with Ko-honen?s view of SOM training as consisting oftwo successive phases: a) rough training and b)fine-tuning.
The implications of this view will bediscussed in more detail later in the paper.5.1 Input dataOur input data are inflected verb forms writtenin standard Italian orthography.
Since Italian or-thography is, with a handful of exceptions, consis-tently phonological, we expect to replicate thesame results with phonologically transcribed verbforms.Forms are incrementally sampled from a train-ing data set, according to their probability densi-ties in a free text corpus of about 3 million words.Input data cover a fragment of Italian verb inflec-tion, including, among others, present indicativeactive, future indicative active, infinitive and pastparticiple forms, for a total of 10 different inflec-tions.
The average length of training forms is 8.5,with a max value of 18.Following Plunkett and Marchman (1993), weassume than the map is exposed to a graduallygrowing lexicon.
At epoch 1, the map learns in-flected forms of the 5 most frequent verb types.
Ateach ensuing epoch, five more verb types areadded to the training data, according to their rankin a list of decreasingly frequent verb types.
As anoverall learning session consists of 100 epochs,the map is eventually exposed to a lexicon of 500verb types, each seen in ten different inflections.Although forms are sampled according to theircorpus distributions, we hypothesise that the rangeof inflections in which verb tokens are seen by themap remains identical across verb types.
This isdone to throw paradigmatic effects in sharper re-lief and responds to the (admittedly simplistic) as-sumption that the syntactic patterns forming thelinguistic input to the child do not vary acrossverb types.Each input token is localistically encoded as an8*16 matrix of values drawn from the set {1, -1}.Column vectors represent characters, and rowsgive the random encoding of each character, en-suring maximum independence of character vec-tor representations.
The first eight columns in thematrix represent the first left-aligned characters ofthe form in question.
The remaining eight col-umns stand for the eight (right-aligned) final char-acters of the input form.a) b)Figure 1.
Early self-organisation of a SOM for roots (a) and endings (b) of Italian verbs (epoch 10).a) b)Figure 2.
Late self-organization of a SOM for roots (a) and endings (b) of Italian verbs (epoch 100).5.2 Training protocolAt each training epoch, the map is exposed to atotal of 3000 input tokens.
As the range ofdifferent inflected forms from which input tokensare sampled is fairly limited (especially at earlyepochs), forms are repeatedly shown to the map.Following Kohonen (1995), a learning epochconsists of two phases.
In the first rough trainingphase, the SOM is exposed to the first 1500tokens.
In this phase, values of ?
(the learningrate) and neighbourhood kernel radius r are madevary as a linear decreasing function of the timeepoch, from max ?
= 0.1 and r = 20 (epoch 1), to?
= 0.02 and r = 10 (epoch 100).
In the secondfine-tuning phase of each epoch, on the otherhand, ?
is kept to 0.02 and r = 3.5.3 Simulation 1: Critical transitions in lexi-cal organisationFigures 1 and 2 contain snapshots of the Italianverb map taken at the beginning and the end oftraining (epochs 1 and 100).
The snapshots areUnified distance matrix (U-matrix, Ultsch andSiemon, 1990) representations of the Italian SOM.They are used to visualise distances between neu-rons.
In a U-matrix representation, the distancebetween adjacent neurons is calculated and pre-sented with different colourings between adjacentpositions on the map.
A dark colouring betweenneurons signifies that their corresponding proto-type vectors are close to each other in the inputspace.
Dark colourings thus highlight areas of themap whose units react consistently to the samestimuli.
A light colouring between output units, onthe other hand, corresponds to a large distance (agap) between their corresponding prototype vec-tors.
In short, dark areas can be viewed as clusters,and light areas as chaotically reacting clusterseparators.
This type of pictorial presentation isuseful when one wants to inspect the state ofknowledge developed by the map through learn-ing.For each epoch, we took two such snapshots: i)one of prototype vector dimensions representingthe initial part of a verb form (approximately itsverb root, Figures 1.a and 2.a), and ii) one of pro-totype vector dimensions representing the verb fi-nal part (approximately, its inflectional endings,Figure 1.b and 2.b).5.3.1 DiscussionData storage on a Kohonen map is a dynamicprocess whereby i) output units tend to consis-tently become more reactive to classes of inputdata, and ii) vector prototypes which are adjacentin the input space tend to cluster in topologicallyconnected subareas of the map.Self-organisation is thus an emergent property,based on local (both in time and space) principlesof prototype vector adaptation.
At the outset, themap is a tabula rasa, i.e.
it has no notion whatso-ever of Italian inflectional morphology.
This hastwo implications.
First, before training sets in,output units are associated with randomly initial-ised sequences of characters.
Secondly, prototypevectors are randomly associated with map neu-rons, so that two contiguous neurons on the mapmay be sensitive to very different stimulus pat-terns.Figure 1 shows that, after the first training ep-och, the map started by organising memorised in-put patterns lexically, grouping them around their(5) roots.
Each root is an attractor of lexically re-lated stimuli, that nonetheless exhibit fairly het-erogeneous endings (see Figure 1.b).At learning epoch 100, on the other hand, thetopological organisation of the verb map is themirror image of that at epoch 10 (Figures 2.a and2.b).
In the course of learning, root attractors aregradually replaced by ending attractors.
Accord-ingly, vector prototypes that used to clusteraround their lexical root appear now to stick to-gether by morpho-syntactic categories such astense, person and number.
One can conceive ofeach connected dark area of map 2.b as a slot inan abstract inflectional paradigm, potentially as-sociated with many forms that share an inflec-tional ending but differ in their roots.rootendingFigure 3.
Average quantization error for an increasing number of input verbsThe main reason for this morphological organisa-tion to emerge at a late learning stage rests in thedistribution of training data.
At the beginning, themap is exposed to a small set of verbs, each ofwhich is inflected in 10 different forms.
Formswith the same ending tend to be fewer than formswith the same root.
As the verb vocabulary grows(say of the order of about 50 different verbs),however, the principles of morphological (as op-posed to lexical) organisation allow for morecompact and faithful data storage, as reflected bya significant reduction in the map average quanti-zation error (Figure 3).
Many different forms canbe clustered around comparatively few endings,and the latter eventually win out as local paradig-matic attractors.Figure 4 (overleaf) is a blow-up of the map areaassociated with infinitive and past participle end-ings.
The map shows the content of the last threecharacters of each prototype vector.
Since pastparticiple forms occur in free texts more oftenthan infinitives, they have a tendency to take aproportionally larger area of the map (due to theso-called magnification factor).
Interestinglyenough, past participles ending in -ato occupy onethird of the whole picture, witnessing the promi-nent role played by regular first conjugation verbsin the past participle inflection.Another intriguing feature of the map is the waythe comparatively connected area of the past par-ticiple is carved out into tightly interconnectedmicro-areas, corresponding to subregular verbforms (e.g.
corso ?run?, scosso ?shaken?
and chie-sto ?answered?).
Rather than lying outside of themorpho-phonological realm (as exceptions to the?TV + to?
default rule), subregular forms of thiskind seem here to draw the topological borders ofthe past participle domain, thus defining a con-tinuous chain of morphological family resem-blances.
Finally, by analogy-based continuity, themap comes to develop a prototype vector for thenon existing (but paradigmatically consistent) pastparticiple ending -eto.2  This ?spontaneous?
over-generalization is the by-product of graded, over-lapping morpheme-based memory traces.In general, stem frequency may have had a re-tardatory effect on the critical transition from alexical to a paradigm-based organisation.
For thesame reason, high-frequency forms are eventuallymemorised as whole words, as they can success-fully counteract the root blurring effect producedby the chaotic overlay of past participle forms ofdifferent verbs, which are eventually attracted tothe same map area.
This turns out to be the casefor very frequent past participles such as stato?been?
and fatto ?done?.
As a final point, a moredetailed analysis of memory traces in the past par-ticiple area of the map is likely to highlight sig-nificant stem patterns in the subregular micro-classes.
If confirmed, this should provide freshevidence supporting the existence of prototypicalmorphonological stem patterns consistently select-ing specific subregular endings (Albright, 2002).5.4 Simulation 2: Second level mapA SOM projects n-dimensional data points ontogrid units of reduced dimensionality (usually 2).We can take advantage of this data compression totrain a new SOM with complex representationsconsisting of the output units of a previouslytrained SOM.
The newly trained SOM is a secondlevel projection of the original data points.To test the consistency of the paradigm-basedorganisation of the map in Figure 2, we trained a2 While Italian regular 1st  and 3rd conjugation verbspresent a thematic vowel in their past participle end-ings (-ato and  -ito respectively), regular 2 conjugationpast participles (TV -e-) end, somewhat unexpectedly,in -uto.novel SOM with verb type vectors.
Each suchvector contains all 10 inflected forms of the sameverb type, encoded through the co-ordinates oftheir best-matching units in the map grid of Figure2.
The result of the newly trained map is given inFigure 5.Figure 4.
The past participle and infinitive areas5.4.1 DiscussionFigure 5 consistently pictures the three-foldmacrostructure of the Italian verb system (section2) as three main horizontal areas going across themap top-to-bottom.Figure 5: A second level mapBesides, we can identify other micro-areas,somewhat orthogonal to the main ones.The mostsignificant such micro-class (circled by a dottedline) contains so-called [g]-inserted verbs (Pirrelli,2000; Fanciullo, 1998), whose forms exhibit acharacteristic [g]/0 stem alternation, as invengo/venite ?I come, you come (plur.)?
andtengo/tenete ?I have/keep, you have/keep (plur.
)?.The class straddles the 2nd and 3rd conjugationareas, thus pointing to a convergent phenomenonaffecting a portion of the verb system (the presentindicative and subjunctive) where the distinctionbetween 2nd and 3rd conjugation inflections isconsiderably (but not completely) blurred.
All inall, Italian verbs appear to fall not only intoequivalence classes based on the selection ofinflectional endings (traditional conjugations), butalso into homogeneous micro-classes reflectingprocesses of variable stem formation.Identification of the appropriate micro-class is acrucial problem in Italian morphology learning.Our map appears to be in a position to tackle itreliably.Note finally the very particular position of theverb stare ?stay?
on the grid.
Although stare is a1st conjugation verb, it selects some 2nd conjuga-tion endings (e.g.
stessimo ?that we stayed (subj.
)?and stette ?
(s)he stayed?).
This is captured in themap, where the verb is located halfway betweenthe 1st and 2nd conjugation areas.6 Conclusion and future workThe paper offered a series of snapshots of thedynamic behaviour of a Kohonen map of the men-tal lexicon taken in different phases of acquisitionof the Italian verb system.
The snapshots consis-tently portray the emergence of global orderingconstraints on memory traces of inflected verbforms, at different levels of linguistic granularity.Our simulations highlight not only morphologi-cally natural classes of input patterns (reminiscentof the hierarchical clustering of perceptron inputunits on the basis of their hidden layer activationvalues) and selective specialisation of neurons andprototype vector dimensions in the map, but alsoother non-trivial aspects of memory organisation.We observe that the number of neighbouring unitsinvolved in the memorisation of a specific mor-phological class is proportional to both type fre-quency of the class and token frequency of itsmembers.
Token frequency also affects the en-trenchment of memory areas devoted to storingindividual forms, so that highly frequent forms arememorised in full, rather than forming part of amorphological cluster.In our view, the solid neuro-physiological basisof SOMs?
processing strategies and the consider-able psycho-linguistic and linguistic evidence infavour of global constraints in morphology learn-ing make the suggested approach an interestingmedium-scale experimental framework, mediatingbetween small-scale neurological structures andlarge-scale linguistic evidence.
In the end, itwould not be surprising if more in-depth computa-tional analyses of this sort will give strong indica-tions that associative models of the morphologicallexicon are compatible with a ?realistic?
interpre-tation of morpheme-based decomposition and ac-cess of inflected forms in the mental lexicon.
Ac-cording to this view, morphemes appear to play atruly active role in lexical indexing, as they ac-quire an increasingly dominant position as localattractors through learning.
This may sound trivialto the psycholinguistic community.
Nonetheless,only very few computer simulations of morphol-ogy learning have so far laid emphasis on the im-portance of incrementally acquiring structure frommorphological data (as opposed ?
say ?
to simplymemorising more and more input examples) andon the role of acquired structure in lexical organi-sation.
Most notably for our present concerns, theglobal ordering constraints imposed by morpho-logical structure in a SOM are the by-product ofpurely local strategies of memory access, process-ing and updating, which are entirely compatiblewith associative models of morphological learn-ing.
After all, the learning child is not a linguistand it has no privileged perspective on all relevantdata.
It would nonetheless be somewhat reassur-ing to observe that its generalisations and orderingconstraints come very close to a linguist?s ontol-ogy.The present work also shows some possiblelimitations of classical SOM architectures.
Thepropensity of SOMs to fully memorise input dataonly at late learning stages (in the fine-tuningphase) is not fully justified in our context.
Like-wise, the hypothesis of a two-staged learningprocess, marked by a sharp discontinuity at thelevel of kernel radius length, has little psycholin-guistic support.
Furthermore, multiple classifica-tions are only minimally supported by SOMs.
Aswe saw, a paradigm-based organisation actuallyreplaces the original lexical structure.
This is notentirely desirable when we deal with complexlanguage tasks.
In order to tackle these potentialproblems, the following changes are currently be-ing implemented:?
endogenous modification of radius length asa function of the local distance between thebest matching prototype vector and the cur-rent stimulus; the smaller the distance thesmaller the effect of adaptive updating onneighbouring vectors?
adaptive vector-distance function; as a neu-ron becomes more sensitive to an input pat-tern, it also develops a sensitivity to specificinput dimensions; differential sensitivity,however, is presently not taken into accountwhen measuring the distance between twovectors; we suggest weighting vector di-mensions, so that distances on some dimen-sions are valued higher than distances onother dimensions?
?self-feeding?
SOMs for multiple classifi-cation tasks; when an incoming stimulushas been matched by the winner unit onlypartially, the non matching part of the samestimulus is fed back to the map; this is in-tended to allow ?recognition?
of more thanone morpheme within the same input form?
more natural input representations, address-ing the issue of time and space-invariantfeatures in character sequences.ReferencesAlbright, Adam.
2002.
Islands of reliability for regularmorphology: Evidence from Italian.
Language,78:684-709.Aronoff, Mark.
1994.
Morphology by Itself.
M.I.T.Press, Cambridge, USA.Baayen, Harald, Ton Dijkstra and Robert Schreuder.1997.
Singulars and Plurals in Dutch: Evidence for aParallel Dual Route Model.
Journal of Memory andLanguage, 36:94-117.Baroni, Marco.
2000.
Distributional cues in morphemediscovery: A computational model and empiricalevidence.
Ph.D. dissertation, UCLA.Bosch van den, Antal, Walter Daelemans, Ton Wei-jters.
1996.
Morphological Analysis as Classifica-tion: an Inductive-learning approach.
In Proceedingsof NEMLAP II , K. Oflazer and H. Somers, eds.,pages 79-89, Ankara.Bybee, Joan.
1995.
Regular Morphology and the Lexi-con.
Language and Cognitive Processes, 10 (5):425-455.Bybee, Joan and Dan I. Slobin.
1982.
Rules and Sche-mas in the Development and Use of the English PastTense.
Language, 58:265-289.Bybee, Joan and Carol Lynn Moder.
1983.
Morpholo-gical Classes as Natural Categories.
Language,59:251-270.De Mauro, Tullio, Federico Mancini, Massimo Vedo-velli and Miriam Voghera.
1993.
Lessico di frequen-za dell'italiano parlato.
Etas Libri, Milan.Fanciullo, Franco.
1998.
Per una interpretazione deiverbi italiani a ?inserto?
velare.
Archivio Glottologi-co Italiano, LXXXIII(II):188-239.Gaussier, Eric.
1999.
Unsupervised learning of deriva-tional morphology from inflectional lexicons.
InProceedings of the Workshop on UnsupervisedLearning in Natural Language Processing, pages24-30, University of Maryland.Goldsmith, John.
2001.
Unsupervised Learning of theMorphology of a Natural Language.
ComputationalLinguistics, 27(2):153-198.Kohonen, Teuvo.
1995.
Self-Organizing Maps.Springer, Berlin.Ling, Charles X. and Marin Marinov.
1993.
Answeringthe Connectionist Challenge: a Symbolic Model ofLearning the Past Tense of English Verbs.
Cogni-tion, 49(3):235-290.Orsolini, Margherita and William Marslen-Wilson.1997.
Universals in Morphological Representations:Evidence from Italian.
Language and CognitiveProcesses, 12(1):1-47.Orsolini, Margherita, Rachele Fanari and Hugo Bo-wles.
1998.
Acquiring regular and irregular inflec-tion in a language with verbal classes.
Languageand Cognitive Processes, 13(4):452-464.Pirrelli, Vito.
2000.
Paradigmi in Morfologia.
IstitutiEditoriali e Poligrafici Internazionali, Pisa.Pirrelli, Vito and Federici Stefano.
1994.
"Deriva-tional" Paradigms in Morphonology.
In Proceedingsof Coling 94, pages 234-240,  Kyoto.Pirrelli, Vito and Fran?ois Yvon.
1999.
The hidden di-mension: a paradigmatic view of data driven NLP.Journal of Experimental and Theoretical ArtificialIntelligence, 11:391-408.Pirrelli, Vito and Marco Battista.
2000.
The Paradig-matic Dimension of Stem Allomorphy in Italian In-flection.
Italian Journal of Linguistics, 12(2):307-380.Plaut, David C., James L. McClelland, Mark S. Sei-denberg and Karalyn Patterson.
1996.
Understand-ing Normal and Impaired Word Reading: Computa-tional Principles in Quasi-regular Domains.
Psycho-logical Review , 103:56-115.Plunkett, Kim and Virginia Marchman.
1993.
Fromrote learning to system building: Acquiring verbmorphology in children and connectionist nets.Cognition, 48:21-69.Prasada, Sandeep and Steven Pinker.
1993.
Generaliza-tions of regular and irregular morphology.
Languageand Cognitive Processes, 8:1-56.Rueckl, Jay G. and Michal Raveh.
1999.
The Influenceof Morphological Regularities on the Dynamics ofConnectionist Networks.
Brain and Language,68:110-117.Say, Tessa and Harald Clahsen.
2001.
Words, Rulesand Stems in the Italian Mental Lexicon.
In ?Storageand computation in the language faculty?, S. Noote-boom, F. Weerman and  F. Wijnen, eds., pages 75-108, Kluwer Academic Publishers, Dordrecht.Serianni, Luca.
1988.
Grammatica italiana: italianocomune e lingua letteraria.
UTET, Turin.Stemberger, Joseph P. and Andrew Carstairs.
1988.
AProcessing Constraint on Inflectional Homonymy.Linguistics, 26:601-61.Taft, Marcus.
1988.
A morphological-decompositionmodel of lexical representation.
Linguistics, 26:657-667.Ultsch, Alfred and H. Peter Siemon.
1990.
Kohonen'sSelf-Organizing Feature Maps for Exploratory DataAnalysis.
In ?Proceedings of INNC'90.
InternationalNeural Network Conference1990?, pages 305-308,DordrechtVesanto, Juha, Johan Himberg, Esa Alhoniemi, andJuha Parhankangas.
2000.
SOM Toolbox for Matlab5 .
Report A57, Helsinki University of Technology,Neural Networks Research Centre, Espoo, Finland.
