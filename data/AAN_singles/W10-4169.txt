1Chinese Word Sense Induction based on Hierarchical ClusteringAlgorithmKe Cai, Xiaodong Shi, Yidong Chen?Zhehuang Huang, Yan GaoCognitive Science Department, Xiamen University, Xiamen, 361005, ChinaAbstractSense induction seeks to automatically identify word senses of polysemous wordsencountered in a corpus.
Unsupervised word sense induction can be viewed as a clusteringproblem.
In this paper, we used the Hierarchical Clustering Algorithm as the classifier forword sense induction.
Experiments show the system can achieve 72% F-score abouttrain-corpus and 65% F-score about test-corpus.1.
IntroductionWord sense induction is a central problem in many natural language processing tasks suchas information extraction, information retrieval, and machine translation [Vickrey et al,2005].Clp 2010 launches totally 4 tasks for evaluation exercise, these are: Chinese wordsegmentation, Chinese parsing, Chinese Personal Name disambiguation and Chinese WordSense Induction.
We participated in task 4, which is Chinese Word Sense Induction..Because the contents surround an ambiguous word is related to its meaning, we solve thesense problem by grouping the instances of the target word into the supposed number ofclusters according to the similarity of contexts of the instance.
In this paper we used thehierarchical clustering algorithm to accomplish the problem.The task can be defined as two stage process: Feature selection and word clustering.Researchers have proposed much approach to the sense induction task which involved the useof basic word co-occurrence features and application of classical clustering algorithms.Because the meanings of unknown words can be inferred from the contexts in which theyappear, Pantel and Lin (2002) map the senses to WordNet.
More recently, the mapping hasbeen used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004;Niu et al, 2005).However, this approach does not generalize to multiple-sense words.
Each sense of apolysemous word can appear in a different context, there have been many attempts in recentyears to apply classical clustering algorithms to this problem.Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen,2004), to agglomerative clustering (Sch?utze, 1998), and the Information Bottleneck (Niu etal., 2007).
Senses are induced by identifying highly dense subgraphs (hubs) in theco-occurrence graph (V?eronis, 2004).The sIB algorithm was used to estimate clusterstructure, which measures the similarity of contexts of instances according to the similarity oftheir feature conditional distribution(Slonim, et al,2002).
Each algorithm treats words asfeature vectors, using the same similarity function based on context information.The remainder of this paper is organized as follows.
In section 2 the Featured set andword similarity definition is introduced.
The hierarchical clustering algorithm is presented insection 3.
Section 4 provides the experimental results and conclusion is drawn in section 5.22.
Feature Selection and Word Similarity Definition2.1 Feature SelectionA feature set is used designed to capture both immediate local context in ourexperiment, wider context and syntactic context.
Specifically, we experimented with severalfeature categories: ?5-word window (5w), ?3-word window (3w), part-of speech n-grams anddependency relations.
These features have been widely adopted in various word senseinduction algorithms.
The overall best scores are achieved with local (5 words) contextwindows.. 2.2 Similarity DefinitionWe treat the context words as feature vectors, using the same similarity function.Suppose1 2( , )i i i inC w w w?
is the contexts set of sentence iS , and 1 2( , )j j j jnC w w w?
is thecontexts set of sentencejS .Then we defined ( , ) ( , )jk ijl ji j kl ik jlW CW Csim S S w sim w w???
?
, here klw  is variable weight,Where( , )( , )ik jl ik jlsim w wdis w w???
?, ?
is an adjustable parameter with a value of 1.2, and( , )ik jlDis w w  is the path length between ikw  and jlw  based on the semantic tree structureused for TongYiCi CiLin (?????).3.
The Hierarchical Clustering Algorithm Used In Word Sense InductionSense induction is viewed as an unsupervised clustering problem where to group aword?s contexts into different classes, each representing a word sense.
In this paper, we usethe bottom-up clumping approach, which begin with n singleton clusters and successivelymerge clusters to produce the other ones.Table1: Hierarchical Clustering Algorithm:1. initialize number of senses n  ?
number of clusters mand clusters 1 2( , ), 1,2i i iC w w i m??
?2.
Set k n?3.
Set  1k k?
?34.
Find the nearest clusters iC and jC  , Merge iC and jC5.
If  k m?
,  go to step 3, otherwise go to step 6;6.  return m  clustersThe merging of the two clusters in step 4 simply corresponds to adding an edge betweenthe nearest pair of nodes in iC and jC .To find the nearest clusters, the following clusteringsimilarity function is used:( , ) ( , )jk ijl ji j kl ik jlW CW Csim S S w sim w w???
?.Our model incorporates features based on lexical information and parts of speech.So we propose a improved hierarchical clustering algorithm based on parts of speech.Table2: improved algorithm based on parts of speech.1.
initialize number of senses n  ?
number of clusters mand clusters 1 2( , ), 1,2i i iC w w i m??
?2.
Part of Speech Tagging on the corpus3.
Divided n  senses into nn  classes base on the information of parts ofspeech.4.
If nn m?
, return m  clusters5.
If nn m?
, invoke hierarchical clustering algorithm in differentclasses?merge clusters into m cluster.6.if nn m?
, invoke hierarchical clustering algorithm in differenttagging, merge clusters into m  cluster.7.
return m  clusters4.
Experimental ResultsThe test data includes totally 100 ambiguous Chinese words,  every word have 504untagged instances.
Table3 show the best/worst/average F-Score of our system abouttrain-corpus and test-corpus.Best word Worst word  All  wordsTrain-corpus 0.98  0.5 0.73Test-corpus ------ ----- 0.65Table 3 Model performance with deferent corpusTable 4 shows the performance of our model about train-corpus when using 3w and 5wword windows, which represent more immediate, local context.Best word Worst word  All  words3w(?3-wordwindow)0.98  0.5 0.735w(?5-wordwindow)0.92 0.52 0.72Table 4 Model performance with deferent windowsTable 5 summarizes the F-score in our system about train-corpus when using deferentsimilarity definition.Best word Worst word  All  wordsThis article 0.98  0.5 0.73Qun LIU  0.99 0.59 0.78Table 5 Model performance with deferent similarity definitionExperimental results show that the Hierarchical Clustering Algorithm can be applied tosense induction.
Considering words to be feature vectors and applying clustering algorithmcan improve accuracy of the task.
A significant gap still exists between the results of thesetechniques and the gold standard of manually compiled word sense dictionaries.5.
ConclusionsSense induction is treated as an unsupervised clustering problem.
In this paper we adopthierarchical clustering algorithm to accomplish the problem.
Generate context wordsaccording to this distribution of key words and formalize the induction problem in agenerative mode.
Experiments show the system can achieved 72% F-score about train-corpusand 65% F-score about test-corpus.
The basic cluster algorithm can sorts the word sense intoclusters corresponding to the context.ReferencesBoyd-Graber, Jordan, David Blei, and Xiaojin Zhu.
2007.A topic model for word sensedisambiguation.
In Proceedings of the EMNLP-CoNLL.
Prague, Czech Republic,pages1024?1033.David Vickrey, Luke Biewald, Marc Teyssler, and Daphne Koller.
Word-sensedisambiguation for machine translation.
In Proceedings of the conference on HumanLanguage Technology and Empirical Methods in Natural Language Processing, page5771-778, 2005.Qun LIU , Sujian LI.
Word Similarity Computing Based on How-net.
ComputationalLinguistics and Chinese Language ProcessingNiu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan.
2007.
I2r: Three systems for wordsense discrimination, chineseword sense disambiguation, and english word sensedisambiguation.
In Proceedings of the Fourth International Workshop on SemanticEvaluations (SemEval-2007).
Association for Computational Linguistics, Prague, CzechRepublic, pages 177?182.Niu, Z.Y., Ji, D.H., & Tan, C.L.
2005.
Word Sense Disambiguation Using LabelPropagation Based Semi-Supervised Learning.
Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics.Pantel, Patrick and Dekang Lin.
2002.
Discovering word senses from text.
In Proceedingsof the 8th KDD.
New York, NY, pages 613?619.Pedersen, Ted.
2007.
Umnd2 : Senseclusters applied to the sense induction task ofsenseval-4.
In Proceedings of SemEval-2007.
Prague, Czech Republic, pages 394?397.Purandare, Amruta and Ted Pedersen.
2004.
Word sense discrimination by clusteringcontexts in vector and similarity spaces.
In Proceedings of the CoNLL.
Boston, MA, pages41?48V?eronis, Jean.
2004.
Hyperlex: lexical cartography for information retrieval.
ComputerSpeech & Language.
18(3):223?252.
