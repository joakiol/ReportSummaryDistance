Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 626?635,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsDiscriminative Sample Selection for Statistical Machine Translation?Sankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard, and Prem NatarajanRaytheon BBN Technologies10 Moulton StreetCambridge, MA, U.S.A.{sanantha,rprasad,stallard,prem}@bbn.comAbstractProduction of parallel training corpora for thedevelopment of statistical machine translation(SMT) systems for resource-poor languagesusually requires extensive manual effort.
Ac-tive sample selection aims to reduce the la-bor, time, and expense incurred in produc-ing such resources, attaining a given perfor-mance benchmark with the smallest possibletraining corpus by choosing informative, non-redundant source sentences from an availablecandidate pool for manual translation.
Wepresent a novel, discriminative sample selec-tion strategy that preferentially selects batchesof candidate sentences with constructs thatlead to erroneous translations on a held-out de-velopment set.
The proposed strategy supportsa built-in diversity mechanism that reducesredundancy in the selected batches.
Simu-lation experiments on English-to-Pashto andSpanish-to-English translation tasks demon-strate the superiority of the proposed approachto a number of competing techniques, suchas random selection, dissimilarity-based se-lection, as well as a recently proposed semi-supervised active learning strategy.1 IntroductionResource-poor language pairs present a significantchallenge to the development of statistical machinetranslation (SMT) systems due to the latter?s depen-dence on large parallel texts for training.
Bilingualhuman experts capable of producing the requisite?Distribution Statement ?A?
(Approved for Public Release,Distribution Unlimited)data resources are often in short supply, and the taskof preparing high-quality parallel corpora is labori-ous and expensive.
In light of these constraints, anattractive strategy is to construct the smallest pos-sible parallel training corpus with which a desiredperformance benchmark may be achieved.Such a corpus may be constructed by selecting themost informative instances from a large collectionof source sentences for translation by a human ex-pert, a technique often referred to as active learn-ing.
A SMT system trained with sentence pairs thusgenerated is expected to perform significantly betterthan if the source sentences were chosen using, say,a na?
?ve random sampling strategy.Previously, Eck et al (2005) described a selec-tion strategy that attempts to maximize coverage bychoosing sentences with the highest proportion ofpreviously unseen n-grams.
Depending on the com-position of the candidate pool with respect to thedomain, this strategy may select irrelevant outliers.They also described a technique based on TF-IDF tode-emphasize sentences similar to those that have al-ready been selected, thereby encouraging diversity.However, this strategy is bootstrapped by randominitial choices that do not necessarily favor sentencesthat are difficult to translate.
Finally, they workedexclusively with the source language and did not useany SMT-derived features to guide selection.Haffari et al (2009) proposed a number of fea-tures, such as similarity to the seed corpus, transla-tion probability, n-gram and phrase coverage, etc.,that drive data selection.
They also proposed amodel in which these features combine linearly topredict a rank for each candidate sentence.
The626top-ranked sentences are chosen for manual transla-tion.
However, this approach requires that the poolhave the same distributional characteristics as thedevelopment sets used to train the ranking model.Additionally, batches are chosen atomically.
Sincesimilar or identical sentences in the pool will typi-cally meet the selection criteria simultaneously, thiscan have the undesired effect of choosing redundantbatches with low diversity.The semi-supervised active learning strategy pro-posed by Ananthakrishnan et al (2010) uses multi-layer perceptrons (MLPs) to rank candidate sen-tences based on various features, including domainrepresentativeness, translation difficulty, and batchdiversity.
A greedy, incremental batch constructiontechnique encourages diversity.
While this strat-egy was shown to be superior to random as wellas n-gram based dissimilarity selection, its coarsegranularity (reducing a candidate sentence to a low-dimensional feature vector for ranking) makes it un-suitable for many situations.
In particular, it is seento have little or no benefit over random selectionwhen there is no logical separation of the candidatepool into ?in-domain?
and ?out-of-domain?
subsets.This paper introduces a novel, active sample se-lection technique that identifies translation errors ona held-out development set, and preferentially se-lects candidate sentences with constructs that areincorrectly translated in the former.
A discrimina-tive pairwise comparator function, trained on theranked development set, is used to order candidatesentences and pick sentences that provide maximumpotential reduction in translation error.
The featurefunctions that power the comparator are updated af-ter each selection to encourage batch diversity.
Inthe following sections, we provide details of the pro-posed sample selection approach, and describe sim-ulation experiments that demonstrate its superiorityover a number of competing strategies.2 Error-Driven Active LearningTraditionally, unsupervised selection strategies havedominated the active learning literature for naturallanguage processing (Hwa, 2004; Tang et al, 2002;Shen et al, 2004).
Sample selection for SMT hasfollowed a similar trend.
The work of Eck et al(2005) and most of the techniques proposed by Haf-fari et al (2009) fall in this category.
Notable ex-ceptions include the linear ranking model of Haf-fari et al (2009) and the semi-supervised selectiontechnique of Ananthakrishnan et al (2010), both ofwhich use one or more held-out development sets totrain and tune the sample selector.
However, whilethe former uses the posterior translation probabilityand the latter, a sentence-level confidence score aspart of the overall selection strategy, current activelearning techniques for SMT do not explicitly targetthe sources of error.Error-driven active learning attempts to choosecandidate instances that potentially maximize errorreduction on a reference set (Cohn et al, 1996;Meng and Lee, 2008).
In the context of SMT, thisinvolves decoding a held-out development set withan existing baseline (seed) SMT system.
The selec-tion algorithm is then trained to choose, from thecandidate pool, sentences containing constructs thatgive rise to translation errors on this set.
Assum-ing perfect reference translations and word align-ment in subsequent SMT training, these sentencesprovide maximum potential reduction in translationerror with respect to the seed SMT system.
It is a su-pervised approach to sample selection.
We assumethe following are available.?
A seed parallel corpus S for training the initialSMT system.?
A candidate pool of monolingual source sen-tences P from which samples must be selected.?
A held-out development set D for training theselection algorithm and for tuning the SMT.?
A test set T for evaluating SMT performance.We further make the following reasonable as-sumptions: (a) the development set D and the testset T are drawn from the same distribution and (b)the candidate pool P consists of both in- and out-of-domain source sentences, as well as an allowablelevel of redundancy (similar or identical sentences).Using translation errors on the development set todrive sample selection has the following advantagesover previously proposed active learning strategiesfor SMT.?
The seed training corpus S need not be derivedfrom the same distribution as D and T. The seedSMT system can be trained with any available627parallel corpus for the specified language pair.This is very useful if, as is often the case, lit-tle or no in-domain training data is available tobootstrap the SMT system.
This removes a criti-cal restriction present in the semi-supervised ap-proach of Ananthakrishnan et al (2010).?
Sentences chosen are guaranteed to be relevantto the domain, because selection is based on n-grams derived from the development set.
Thisalleviates potential problems with approachessuggested by Eck et al (2005) and several tech-niques used by Haffari et al (2009), where ir-relevant outliers may be chosen simply becausethey contain previously unseen n-grams, or aredeemed difficult to translate.?
The proposed technique seeks to minimizeheld-out translation error rather than maximizetraining-set coverage.
This is the more intuitive,direct approach to sample selection for SMT.?
Diversity can be encouraged by preventing n-grams that appear in previously selected sen-tences from playing a role in choosing subse-quent sentences.
This provides an efficient alter-native to the cumbersome ?batch diversity?
fea-ture proposed by Ananthakrishnan et al (2010).The proposed implementation of error-driven ac-tive learning for SMT, discriminative sample selec-tion, is described in the following section.3 Discriminative Sample SelectionThe goal of active sample selection is to induce anordering of the candidate instances that satisfies anobjective criterion.
Eck et al (2005) ordered can-didate sentences based on the frequency of unseenn-grams.
Haffari et al (2009) induced a rankingbased on unseen n-grams, translation difficulty, etc.,as well as one that attempted to incrementally max-imize BLEU using two held-out development sets.Ananthakrishnan et al (2010) attempted to order thecandidate pool to incrementally maximize source n-gram coverage on a held-out development set, sub-ject to difficulty and diversity constraints.In the case of error-driven active learning, we at-tempt to learn an ordering model based on errorsobserved on the held-out development set D. Weachieve this in an innovative fashion by casting theranking problem as a pairwise sentence compari-son problem.
This approach, inspired by Ailon andMohri (2008), involves the construction of a binaryclassifier functioning as a relational operator that canbe used to order the candidate sentences.
The pair-wise comparator is trained on an ordering of D thatranks constituent sentences in decreasing order ofthe number of translation errors.
The comparator isthen used to rank the candidate pool in decreasingorder of potential translation error reduction.3.1 Maximum-Entropy Pairwise ComparatorGiven a pair of source sentences (u, v), we define,adopting the notation of Ailon and Mohri (2008), thepairwise comparator h(u, v) as follows:h(u, v) ={1, u < v0, u >= v(1)In Equation 1, the binary comparator h(u, v)plays the role of the ?less than?
(?<?)
relational op-erator, returning 1 if u is preferred to v in an or-dered list, and 0 otherwise.
As detailed in Ailon andMohri (2008), the comparator must satisfy the con-straint that h(u, v) and h(v, u) be complementary,i.e.
h(u, v) + h(v, u) = 1 to avoid ambiguity.
How-ever, it need not satisfy the triangle inequality.We implement h(u, v) as a combination of dis-criminative maximum entropy classifiers triggeredby feature functions drawn from n-grams of u and v.We define p(u, v) as the conditional posterior prob-ability of the Bernoulli event u < v given (u, v) asshown in Equation 2.p(u, v) = Pr(u < v | u, v) (2)In our implementation, p(u, v) is the output ofa binary maximum-entropy classifier trained on thedevelopment set.
However, this implementationposes two problems.First, if we use constituent n-grams of u and vas feature functions to trigger the classifier, there isno way to distinguish between (u, v) and (v, u) asthey will trigger the same feature functions.
Thiswill result in identical values for p(u, v) and p(v, u),a contradiction.
We resolve this issue by intro-ducing a set of ?complementary?
feature functions,which are formed by simply appending a recogniz-able identifier to the existing n-gram feature func-628u: how are youv: i am goingf(u) = {how:1, are:1, you:1, how*are:2, are*you:2, how*are*you:3}f(v) = {i:1, am:1, going:1, i*am:2, am*going:2, i*am*going:3}f ?
(u) = {!how:1, !are:1, !you:1, !how*are:2, !are*you:2, !how*are*you:3}f ?
(v) = {!i:1, !am:1, !going:1, !i*am:2, !am*going:2, !i*am*going:3}Table 1: Standard and complementary trigram feature functions for a source pair (u, v).tions.
Then, to evaluate p(u, v), for instance, weinvoke the classifier with standard feature functionsfor u and complementary feature functions for v.Similarly, p(v, u) is evaluated by triggering comple-mentary feature functions for u and standard featurefunctions for v. Table 1 illustrates this with a simpleexample.Note that each feature function is associated witha real value, whose magnitude is an indicator of itsimportance.
In our implementation, an n-gram fea-ture function (standard or complementary) receivesa value equal to its length.
This is based on our intu-ition that longer n-grams play a more important rolein dictating SMT performance.Second, the introduction of complementary trig-gers implies that evaluation of p(u, v) and p(v, u)now involves disjoint sets of feature functions.
Thus,p(u, v) is not guaranteed to satisfy the complemen-tarity condition imposed on h(u, v), and thereforecannot directly be used as the binary pairwise com-parator.
We resolve this by normalizing across thetwo possible permutations, as follows:h?
(u, v) = p(u, v)p(u, v) + p(v, u) (3)h?
(v, u) = p(v, u)p(u, v) + p(v, u) (4)Since h?
(u, v) + h?
(v, u) = 1, the complemen-tarity constraint is now satisfied, and h(u, v) is justa binarized (thresholded) version of h?
(u, v).
Thus,the binary pairwise comparator can be constructedfrom the permuted classifier outputs.3.2 Training the Pairwise ComparatorTraining the maximum-entropy classifier for thepairwise comparator requires a set of target labelsand input feature functions, both of which are de-rived from the held-out development set D. We be-gin by decoding the source sentences in D with theseed SMT system, followed by error analysis usingthe Translation Edit Rate (TER) measure (Snoveret al, 2006).
TER measures translation quality bycomputing the number of edits (insertions, substitu-tions, and deletions) and shifts required to transforma translation hypothesis to its corresponding refer-ence.
We then rank D in decreasing order of thenumber of post-shift edits, i.e.
the number of in-sertions, substitutions, and deletions after the shiftoperation is completed.
Since shifts are often due toword re-ordering issues within the SMT decoder (es-pecially for phrase-based systems), we do not con-sider them as errors for the purpose of ranking D.Sentences at the top of the ordered list D?
containthe maximum number of translation errors.For each pair of sentences (u, v) : u < v in D?,we generate two training entries.
The first, signify-ing that u appears before v in D?, assigns the labeltrue to a trigger list consisting of standard featurefunctions derived from u, and complementary fea-ture functions derived from v. The second, reinforc-ing this observation, assigns the label false to a trig-ger list consisting of complementary feature func-tions from u, and standard feature functions from v.The labeled training set (feature:label pairs) for thecomparator can be expressed as follows:?
(u, v) ?
D?
: u < v,{f(u) f ?
(v)} : true{f ?
(u) f(v)} : falseThus, if there are d sentences in D?, we obtain atotal of d(d?
1) labeled examples to train the com-parator.
We use the standard L-BFGS optimization629algorithm (Liu and Nocedal, 1989) to estimate theparameters of the maximum entropy model.3.3 Greedy Discriminative SelectionThe discriminatively-trained pairwise comparatorcan be used as a relational operator to sort the candi-date pool P in decreasing order of potential transla-tion error reduction.
A batch of pre-determined sizeK can then be selected from the top of this list toaugment the existing SMT training corpus.
Assum-ing the pool contains N candidate sentences, andgiven a fast sorting algorithm such as Quicksort, thecomplexity of this strategy is O(N logN).
Batchescan be selected iteratively until a specified perfor-mance threshold is achieved.A potential downside of this approach reveals it-self when there is redundancy in the candidate pool.Since the batch is selected in a single atomic opera-tion from the sorted candidates, and because similaror identical sentences will typically occupy the samerange in the ordered list, it is likely that this approachwill result in batches with low diversity.
Whereaswe desire diverse batches for better coverage and ef-ficient use of manual translation resources.
This is-sue was previously addressed in Shen et al (2004) inthe context of named-entity recognition, where theyused a two-step procedure to first select the most in-formative and representative samples, followed by adiversity filter.
Ananthakrishnan et al (2010) used agreedy, incremental batch construction strategy withan integrated, explicit batch diversity feature as partof the ranking model.
Based on these ideas, we de-sign a greedy selection strategy using the discrimi-native relational operator.Rather than perform a full sort on P, we sim-ply invoke the minh(u,v)(?
?
? )
function to find thesentence that potentially minimizes translation er-ror.
The subscript indicates that our implementationof this function utilizes the discriminative relationaloperator trained on the development set D. The bestchoice sentence s is then added to our batch at thecurrent position (we begin with an empty batch).
Wethen remove the standard and complementary fea-ture functions f(s) and f ?
(s) triggered by s from theglobal pool of feature functions obtained from D,so that they do not play a role in the selection ofsubsequent sentences for the batch.
Subsequently,a candidate sentence that is similar or identical toAlgorithm 1 Greedy Discriminative SelectionB?
()for k = 1 to K dos?
minh(u,v)(P)B(k)?
sP?
P?
{s}f(D)?
f(D)?
f(s)f ?(D)?
f ?(D)?
f ?
(s)end forreturn Bs will not be preferred, because the feature func-tions that previously caused it to rank highly willno longer trigger.
Algorithm 1 summarizes our se-lection strategy in pseudocode.
Since each call tominh(u,v)(?
?
? )
is O(N), the overall complexity ofgreedy discriminative selection is O(K ?N).4 Experiments and ResultsWe conduct a variety of simulation experimentswith multiple language pairs (English-Pashto andSpanish-English) and different data configurationsin order to demonstrate the utility of discrimina-tive sample selection in the context of resource-poorSMT.
We also compare the performance of the pro-posed strategy to numerous competing active andpassive selection methods as follows:?
Random: Source sentences are uniformly sam-pled from the candidate pool P.?
Similarity: Choose sentences from P with thehighest fraction of n-gram overlap with the seedcorpus S.?
Dissimilarity: Select sentences from P with thehighest proportion of n-grams not seen in theseed corpus S (Eck et al, 2005; Haffari et al,2009).?
Longest: Pick the longest sentences from thecandidate pool P.?
Semi-supervised: Semi-supervised active learn-ing with greedy incremental selection (Anan-thakrishnan et al, 2010).?
Discriminative: Choose sentences that po-tentially minimize translation error using amaximum-entropy pairwise comparator (pro-posed method).630Identical low-resource initial conditions are ap-plied to each selection strategy so that they may beobjectively compared.
A very small seed corpus S issampled from the available parallel training data; theremainder serves as the candidate pool.
Followingthe literature on active learning for SMT, our simula-tion experiments are iterative.
A fixed-size batch ofsource sentences is constructed from the candidatepool using one of the above selection strategies.
Wethen look up the corresponding translations from thecandidate targets (simulating an expert human trans-lator), augment the seed corpus with the selecteddata, and update the SMT system with the expandedtraining corpus.
The selected data are removed fromthe candidate pool.
This select-update cycle is thenrepeated for either a fixed number of iterations oruntil a specified performance benchmark is attained.At each iteration, we decode the unseen test set Twith the most current SMT configuration and eval-uate translation performance in terms of BLEU aswell as coverage (defined as the fraction of untrans-latable source words in the target hypotheses).We use a phrase-based SMT framework similar toKoehn et al (2003) for all experiments.4.1 English-Pashto SimulationOur English-Pashto (E2P) data originates from atwo-way collection of spoken dialogues, and con-sists of two parallel sub-corpora: a directional E2Pcorpus and a directional Pashto-English (P2E) cor-pus.
Each sub-corpus has its own independent train-ing, development, and test partitions.
The direc-tional E2P training, development, and test sets con-sist of 33.9k, 2.4k, and 1.1k sentence pairs, respec-tively.
The directional P2E training set consists of76.5k sentence pairs.
The corpus was used as-is, i.e.no length-based filtering or redundancy-reduction(i.e.
removal of duplicates, if any) was performed.The test-set BLEU score with the baseline E2P SMTsystem trained from all of the above data was 9.5%.We obtained a seed training corpus by randomlysampling 1,000 sentence pairs from the directionalE2P training partition.
The remainder of this set, andthe entire reversed P2E training partition were com-bined to create the pool (109.4k sentence pairs).
Inthe past, we have observed that the reversed direc-tional P2E data gives very little performance gainin the E2P direction even though its vocabulary issimilar, and can be considered ?out-of-domain?
asfar as the E2P translation task is concerned.
Thus,our pool consists of 30% in-domain and 70% out-of-domain sentence pairs, making for a challeng-ing active learning problem.
A pool training set of10k source sentences is sampled from this collectionfor the semi-supervised selection strategy, leaving uswith 99.4k candidate sentences, which we use for allcompeting techniques.
The data configuration usedin this simulation is identical to Ananthakrishnan etal.
(2010), allowing us to compare various strategiesunder the same conditions.
We simulated a total of20 iterations with batches of 200 sentences each; theoriginal 1,000 sample seed corpus grows to 5,000sentence pairs and the end of our simulation.Figure 1(a) illustrates the variation in BLEUscores across iterations for each selection strategy.The proposed discriminative sample selection tech-nique performs significantly better at every iterationthan random, similarity, dissimilarity, longest, andsemi-supervised active selection.
At the end of 20iterations, the BLEU score gained 3.21 points, a rel-ative improvement of 59.3%.
This was followed bysemi-supervised active learning, which improved by2.66 BLEU points, a 49.2% relative improvement.Table 2 summarizes the total number of words se-lected by each strategy, as well as the total areaunder the BLEU curve with respect to the base-line.
The latter, labeled BLEUarea and expressed inpercent-iterations, is a better measure of the over-all performance of each strategy across all iterationsthan comparing BLEU scores at the final iteration.Figure 1(b) shows the variation in coverage (per-centage of untranslatable source words in targethypotheses) for each selection technique.
Here,discriminative sample selection was better than allother approaches except longest-sentence selection.4.2 Spanish-English SimulationThe Spanish-English (S2E) training corpus wasdrawn from the Europarl collection (Koehn, 2005).To prevent length bias in selection, the corpus wasfiltered to only retain sentence pairs whose sourceranged between 7 and 15 words (excluding punc-tuation).
Additionally, redundancy was reduced byremoving all duplicate sentence pairs.
After thesesteps, we obtained approximately 253k sentencepairs for training.
The WMT10 held-out develop-631(a) Variation in BLEU (E2P)(b) Variation in coverage (E2P)Figure 1: Simulation results for E2P data selection.632(a) Variation in BLEU (S2E)(b) Variation in coverage (S2E)Figure 2: Simulation results for S2E data selection.633Method E2P size E2P BLEUarea S2E size S2E BLEUareaRandom 58.1k 26.4 26.5k 45.0Similarity 30.7k 21.9 24.7k 13.2Dissimilarity 39.2k 12.4 24.2k 54.9Longest 173.0k 27.5 39.6k 48.3Semi-supervised 80.0k 34.1 27.6k 45.6Discriminative 109.1k 49.6 31.0k 64.5Table 2: Source corpus size (in words) and BLEUarea after 20 sample selection iterations.ment and test sets (2k and 2.5k sentence pairs, re-spectively) were used to tune our system and eval-uate performance.
Note that this data configurationis different from that of the E2P simulation in thatthere is no logical separation of the training data into?in-domain?
and ?out-of-domain?
sets.
The baselineS2E SMT system trained with all available data gavea test-set BLEU score of 17.2%.We randomly sampled 500 sentence pairs fromthe S2E training partition to obtain a seed train-ing corpus.
The remainder, after setting aside an-other 10k source sentences for training the semi-supervised strategy, serves as the candidate pool.
Weagain simulated a total of 20 iterations, except inthis case, we used batches of 100 sentences in an at-tempt to obtain smoother performance trajectories.The training corpus grows from 500 sentence pairsto 2,500 as the simulation progresses.Variation in BLEU scores and coverage for theS2E simulation are illustrated in Figures 2(a) and2(b), respectively.
Discriminative sample selectionoutperformed all other selection techniques acrossall iterations of the simulation.
After 20 iterations,we obtained a 4.51 point gain in BLEU, a rela-tive improvement of 142.3%.
The closest com-petitor was dissimilarity-based selection, which im-proved by 4.38 BLEU points, a 138.1% relativeimprovement.
The proposed method also outper-formed other selection strategies in improving cov-erage, with significantly better results especially inthe early iterations.
Table 2 summarizes the numberof words chosen, and BLEUarea, for each strategy.5 Conclusion and Future DirectionsBuilding SMT systems for resource-poor languagepairs requires significant investment of labor, time,and money for the development of parallel trainingcorpora.
We proposed a novel, discriminative sam-ple selection strategy that can help lower these costsby choosing batches of source sentences from a largecandidate pool.
The chosen sentences, in conjunc-tion with their manual translations, provide signifi-cantly better SMT performance than numerous com-peting active and passive selection techniques.Our approach hinges on a maximum-entropy pair-wise comparator that serves as a relational operatorfor comparing two source sentences.
This allows usto rank the candidate pool in decreasing order of po-tential reduction in translation error with respect toan existing seed SMT system.
The discriminativecomparator is coupled with a greedy, incremental se-lection technique that discourages redundancy in thechosen batches.
The proposed technique divergesfrom existing work on active sample selection forSMT in that it uses machine learning techniques inan attempt to explicitly reduce translation error bychoosing sentences whose constituents were incor-rectly translated in a held-out development set.While the performance of competing strategiesvaried across language pairs and data configurations,discriminative sample selection proved consistentlysuperior under all test conditions.
It provides a pow-erful, flexible, data selection front-end for rapid de-velopment of SMT systems.
Unlike some selectiontechniques, it is also platform-independent, and canbe used as-is with a phrase-based, hierarchical, syn-tactic, or other SMT framework.We have so far restricted our experiments to simu-lations, obtaining expert human translations directlyfrom the sequestered parallel corpus.
We are nowactively exploring the possibility of linking the sam-ple selection front-end to a crowd-sourcing back-end, in order to obtain ?non-expert?
translations us-ing a platform such as the Amazon Mechanical Turk.634ReferencesNir Ailon and Mehryar Mohri.
2008.
An efficient reduc-tion of ranking to classification.
In COLT ?08: Pro-ceedings of the 21st Annual Conference on LearningTheory, pages 87?98.Sankaranarayanan Ananthakrishnan, Rohit Prasad, DavidStallard, and Prem Natarajan.
2010.
A semi-supervised batch-mode active learning strategy forimproved statistical machine translation.
In CoNLL?10: Proceedings of the 14th International Conferenceon Computational Natural Language Learning, pages126?134, July.David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-dan.
1996.
Active learning with statistical models.Journal of Artificial Intelligence Research, 4(1):129?145.Matthias Eck, Stephan Vogel, and Alex Waibel.
2005.Low cost portability for statistical machine translationbased in N-gram frequency and TF-IDF.
In Proceed-ings of IWSLT, Pittsburgh, PA, October.Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.2009.
Active learning for statistical phrase-based ma-chine translation.
In NAACL ?09: Proceedings of Hu-man Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, pages 415?423,Morristown, NJ, USA.
Association for ComputationalLinguistics.Rebecca Hwa.
2004.
Sample selection for statisticalparsing.
Computational Linguistics, 30:253?276.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL?03: Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 48?54, Morristown, NJ, USA.
Association forComputational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In MT Summit X:Proceedings of the 10th Machine Translation Summit,pages 79?86.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
Math.Program., 45(3):503?528.Qinggang Meng and Mark Lee.
2008.
Error-drivenactive learning in growing radial basis function net-works for early robot learning.
Neurocomputing, 71(7-9):1449?1461.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan.
2004.
Multi-criteria-based active learningfor named entity recognition.
In ACL ?04: Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, pages 589?596, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings AMTA, pages 223?231, August.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2002.
Ac-tive learning for statistical natural language parsing.In ACL ?02: Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages120?127, Morristown, NJ, USA.
Association for Com-putational Linguistics.635
