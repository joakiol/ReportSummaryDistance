Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832?841,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPSelf-Training PCFG Grammars with Latent AnnotationsAcross LanguagesZhongqiang Huang11Laboratory for Computational Linguisticsand Information ProcessingInstitute for Advanced Computer StudiesUniversity of Maryland, College Parkzqhuang@umiacs.umd.eduMary Harper1,22Human Language TechnologyCenter of ExcellenceJohns Hopkins Universitymharper@umiacs.umd.eduAbstractWe investigate the effectiveness of self-training PCFG grammars with latent anno-tations (PCFG-LA) for parsing languageswith different amounts of labeled trainingdata.
Compared to Charniak?s lexicalizedparser, the PCFG-LA parser was more ef-fectively adapted to a language for whichparsing has been less well developed (i.e.,Chinese) and benefited more from self-training.
We show for the first time thatself-training is able to significantly im-prove the performance of the PCFG-LAparser, a single generative parser, on bothsmall and large amounts of labeled train-ing data.
Our approach achieves state-of-the-art parsing accuracies for a singleparser on both English (91.5%) and Chi-nese (85.2%).1 IntroductionThere is an extensive research literature on build-ing high quality parsers for English (Collins, 1999;Charniak, 2000; Charniak and Johnson, 2005;Petrov et al, 2006), however, models for parsingother languages are less well developed.
Take Chi-nese for example; there have been several attemptsto develop accurate parsers for Chinese (Bikel andChiang, 2000; Levy and Manning, 2003; Petrovand Klein, 2007), but the state-of-the-art perfor-mance, around 83% F measure on Penn ChineseTreebank (achieved by the Berkeley parser (Petrovand Klein, 2007)) falls far short of performanceon English (?90-92%).
As pointed out in (Levyand Manning, 2003), there are many linguistic dif-ferences between Chinese and English, as well asstructural differences between their correspondingtreebanks, and some of these make it a harder taskto parse Chinese.
Additionally, the fact that theavailable treebanked Chinese materials are morelimited than for English also increases the chal-lenge of building high quality Chinese parsers.Many of these differences would also tend to applyto other less well investigated languages.In this paper, we focus on English and Chinesebecause the former is a language for which ex-tensive parsing research has been conducted whilethe latter is a language that has been less exten-sively studied.
We adapt and improve the Berke-ley parser, which learns PCFG grammars with la-tent annotations, and show through comparativestudies that this parser significantly outperformsCharniak?s parser, which was initially developedfor English and subsequently ported to Chinese.We focus on answering two questions: how welldoes a parser perform across languages and howmuch does it benefit from self-training?The first question is of special interest whenchoosing a parser that is designed for one languageand adapting it to another less studied language.We improve the PCFG-LA parser by adding alanguage-independent method for handling rarewords and adapt it to another language, Chinese,by creating a method to better model Chinese un-known words.
Our results show that the PCFG-LA parser performs significantly better than Char-niak?s parser on Chinese, and is also somewhatmore accurate on English, although both parsershave high accuracy.The second question is important because la-beled training data is often quite limited, espe-cially for less well investigated languages, whileunlabeled data is ubiquitous.
Early investigationson self-training for parsing have had mixed re-sults.
Charniak (1997) reported no improvementsfrom self-training a PCFG parser on the standardWSJ training set.
Steedman et al (2003) re-ported some degradation using a lexicalized treeadjoining grammar parser and minor improve-ment using Collins lexicalized PCFG parser; how-ever, this gain was obtained only when the parser832was trained on a small labeled set.
Reichart andRappoport (2007) obtained significant gains us-ing Collins lexicalized parser with a different self-training protocol, but again they only looked atsmall labeled sets.
McClosky et al (2006) effec-tively utilized unlabeled data to improve parsingaccuracy on the standard WSJ training set, but theyused a two-stage parser comprised of Charniak?slexicalized probabilistic parser with n-best pars-ing and a discriminative reranking parser (Char-niak and Johnson, 2005), and thus it would be bet-ter categorized as ?co-training?
(McClosky et al,2008).
It is worth noting that their attempts at self-training Charniak?s lexicalized parser directly re-sulted in no improvement.
There are other suc-cessful semi-supervised training approaches fordependency parsing, such as (Koo et al, 2008;Wang et al, 2008), and it would be interestingto investigate how they could be applied to con-stituency parsing.We show in this paper, for the first time, thatself-training is able to significantly improve theperformance of the PCFG-LA parser, a single gen-erative parser, on both small and large amounts oflabeled training data, for both English and Chi-nese.
With self-training, a fraction of the WSJor CTB6 treebank training data is sufficient totrain a PCFG-LA parser that is able to achieve oreven beat the accuracies obtained using a singleparser trained on the entire treebank without self-training.
We conjecture based on our comparisonof the PCFG-LA parser to Charniak?s parser thatthe addition of self-training data helps the formerparser learn more fine-grained latent annotationswithout over-fitting.The rest of this paper is organized as follows.We describe the PCFG-LA parser and several en-hancements in Section 2, and discuss self-trainingin Section 3.
We then outline the experimentalsetup in Section 4, describe the results in Sec-tion 5, and present a detailed analysis in Section 6.The last section draws conclusions and describesfuture work.2 Parsing ModelThe Berkeley parser (Petrov et al, 2006; Petrovand Klein, 2007) is an efficient and effective parserthat introduces latent annotations (Matsuzaki etal., 2005) to refine syntactic categories to learnbetter PCFG grammars.
In the example parse treein Figure 1(a), each syntactic category is split intomultiple latent subcategories, and accordingly theoriginal parse tree is decomposed into many parsetrees with latent annotations.
Figure 1(b) depictsone of such trees.
The grammar and lexical rulesare split accordingly, e.g., NP?PRP is split intodifferent NP-i?PRP-j rules.
The expansion prob-abilities of these split rules are the parameters of aPCFG-LA grammar.SShePRPNP VPVBDheard DTNPNNthe noise...NP?2VBD?5PRP?3She heard DT?2the noiseNN?6NP?6.
?1S?1VP?4(a) (b)Figure 1: (a) original treebank tree, (b) after latentannotation.The objective of training is to learn a grammarwith latent annotations that maximizes the like-lihood of the training trees, i.e., the sum of thelikelihood of all parse trees with latent annota-tions.
Since the latent annotations are not avail-able in the treebank, a variant of the EM algo-rithm is utilized to learn the rule probabilities forthem.
The Berkeley parser employs a hierarchi-cal split-merge method that gradually increases thenumber of latent annotations and adaptively allo-cates them to different treebank categories to bestmodel the training data.
In this paper, we call agrammar trained after n split-merge steps an n-th order grammar.
The order of a grammar is astep (not continuous) function of the number of la-tent annotations because the split-merge algorithmfirst splits each latent annotation into two and thenmerges some of the splits back based on their abil-ity to increase training likelihood.For this paper, we implemented1our own ver-sion of Berkeley parser.
Updates include betterhandling of rare words across languages, as wellas unknown Chinese words.
The parser is ableto process difficult sentences robustly using adap-tive beam expansion.
The training algorithm wasupdated to support a wide range of self-trainingexperiments (e.g., posterior-weighted unlabeleddata, introducing self-training in later iterations)and to make use of multiple processors to paral-lelize EM training.
The parallelization is crucial1A major motivation for this implementation was to sup-port some algorithms we are developing.
Most of our en-hancements will be merged with a future release of the Berke-ley parser.833for training a model with large volumes of data ina reasonable amount of time2.We next describe the language-independentmethod to handle rare words, which is impor-tant for training better PCFG-LA grammars es-pecially when the training data is limited in size,and our unknown Chinese word handling method,highlighting the importance of utilizing language-specific features to enhance parsing performance.As we will see later, both of these methods signif-icantly improve parsing performance.2.1 Rare Word HandlingWhereas rule expansions are frequently observedin the treebank, word-tag co-occurrences aresparser and more likely to suffer from over-fitting.Although the lexicon smoothing method in theBerkeley parser is able to make the word emis-sion probabilities of different latent states of aPOS tag more alike, the EM training algorithmstill strongly discriminates among word identities.Suppose word tag pairs ?w1, t?
and ?w2, t?
bothappear the same number of times in the trainingdata.
In a PCFG grammar without latent annota-tions, the probabilities of emitting these two wordsgiven tag t would be the same, i.e., p(w1|t) =p(w2|t).
After introducing latent annotation x totag t, the emission probabilities of these two wordsgiven a latent state txmay no longer be the samebecause p(w1|tx) and p(w2|tx) are two indepen-dent parameters that the EM algorithm optimizeson.
It is beneficial to learn subcategories of POStags to model different types of words, especiallyfor frequent words; however, it is not desirable tostrongly discriminate among rare words because itcould distract the model from learning about com-mon phenomena.To handle this problem, the probability of a la-tent state txgenerating a rare word w is forcedto be proportional to the emission probability ofword w given the surface tag t. This is achievedby mapping all words with frequency less thanthreshold3?
to the unk symbol, and for each la-tent state txof a POS tag t, accumulating the wordtag statistics of these rare words to cr(tx, unk) =?w:c(w)<?c(tx, w), and then redistributing themamong the rare words to estimate their emission2The parallel version is able to train our largest grammaron a 8-core machine within a week, while the non-parallelversion is not able to finish even after 3 weeks.3The value of ?
is tuned on the development set.probabilities:c(tx, w) = cr(tx, unk) ?c(t, w)cr(t, unk)p(w|tx) = c(tx, w)/?wc(tx, w)2.2 Chinese Unknown Word HandlingThe Berkeley parser utilizes statistics associatedwith rare words (e.g., suffix, capitalization) to esti-mate the emission probabilities of unknown wordsat decoding time.
This is adequate for for English,however, only a limited number of classes of un-known words, such as digits and dates, are handledfor Chinese.
In this paper, we develop a character-based unknown word model inspired by (Huanget al, 2007) that reflects the fact that characters inany position (prefix, infix, or suffix) can be predic-tive of the part-of-speech (POS) type for Chinesewords.
In our model, the word emission proba-bility, p(w|tx), of an unknown word w given thelatent state txof POS tag t is estimated by the ge-ometric average of the emission probability of thecharacters ckin the word:P (w|tx) =n?
?ck?w,P (ck|t)6=0P (ck|t)where n = |{ck?
w|P (ck|t) 6= 0}|.
Charactersnot seen in the training data are ignored in thecomputation of the geometric average.
We backoff to use the rare word statistics regardless ofword identity when the above equation cannot beused to compute the emission probability.3 Parser Self-TrainingOur hypothesis is that combining automatically la-beled parses with treebank trees will help the EMtraining of the PCFG-LA parser to make more in-formed decisions about latent annotations and thusgenerate more effective grammars.
In this section,we discuss how self-training is applied to train aPCFG-LA parser.There are several ways to automatically labelthe data.
A fairly standard method is to parse theunlabeled sentences with a parser trained on la-beled training data, and then combine the result-ing parses with the treebank training data to re-train the parser.
This is the approach we chosefor self-training.
An alternative approach is to runEM directly on the labeled treebank trees and theunlabeled sentences, without explicit parse treesfor the unlabeled sentences.
However, because the834brackets would need to be determined for the un-labeled sentences together with the latent annota-tions, this would increase the running time fromlinear in the number of expansion rules to cubic inthe length of the sentence.Another important decision is how to weightthe gold standard and automatically labeled datawhen training a new parser model.
Errors in theautomatically labeled data could limit the accu-racy of the self-trained model, especially whenthere is a much greater quantity of automaticallylabeled data than the gold standard training data.To balance the gold standard and automaticallylabeled data, one could duplicate the treebankdata to match the size of the automatically la-beled data; however, the training of the PCFG-LA parser would result in redundant applicationsof EM computations over the same data, increas-ing the cost of training.
Instead we weight theposterior probabilities computed for the gold andautomatically labeled data, so that they contributeequally to the resulting grammar.
Our preliminaryexperiments show that balanced weighting is ef-fective, especially for Chinese (about 0.4% abso-lute improvement) where the automatic parse treeshave a relatively lower accuracy.The training procedure of the PCFG-LA parsergradually introduces more latent annotations dur-ing each split-merge stage, and the self-labeleddata can be introduced at any of these stages.
In-troduction of the self-labeled data in later stages,after some important annotations are learned fromthe treebank, could result in more effective learn-ing.
We have found that a middle stage introduc-tion (after 3 split-merge iterations) of the automat-ically labeled data has an effect similar to balanc-ing the weights of the gold and automatically la-beled trees, possibly due to the fact that both meth-ods place greater trust in the former than the latter.In this study, we introduce the automatically la-beled data at the outset and weight it equally withthe gold treebank training data in order to focusour experiments to support a deeper analysis.4 Experimental SetupFor the English experiments, sections from theWSJ Penn Treebank are used as labeled trainingdata: section 2-19 for training, section 22 for de-velopment, and section 23 as the test set.
We alsoused 210k4sentences of unlabeled news articles inthe BLLIP corpus for English self-training.For the Chinese experiments, the Penn ChineseTreebank 6.0 (CTB6) (Xue et al, 2005) is usedas labeled data.
CTB6 includes both news articlesand transcripts of broadcast news.
We partitionedthe news articles into train/development/test setsfollowing Huang et al (2007).
The broadcast newssection is added to the training data because itshares many of the characteristics of newswire text(e.g., fully punctuated, contains nonverbal expres-sions such as numbers and symbols).
In addi-tion, 210k sentences of unlabeled Chinese newsarticles are used for self-training.
Since the Chi-nese parsers in our experiments require word-segmented sentences as input, the unlabeled sen-tences need to be word-segmented first.
As shownin (Harper and Huang, 2009), the accuracy of au-tomatic word segmentation has a great impact onChinese parsing performance.
We chose to usethe Stanford segmenter (Chang et al, 2008) inour experiments because it is consistent with thetreebank segmentation and provides the best per-formance among the segmenters that were tested.To minimize the discrepancy between the self-training data and the treebank data, we normalizeboth CTB6 and the self-training data using UWDecatur (Zhang and Kahn, 2008) text normaliza-tion.Table 1 summarizes the data set sizes usedin our experiments.
We used slightly modi-fied versions of the treebanks; empty nodes andnonterminal-yield unary rules5, e.g., NP?VP, aredeleted using tsurgeon (Levy and Andrew, 2006).Train Dev Test UnlabeledEnglish39.8k 1.7k 2.4k 210k(950.0k) (40.1k) (56.7k) (5,082.1k)Chinese24.4k 1.9k 2.0k 210k(678.8k) (51.2k) (52.9k) (6,254.9k)Table 1: The number of sentences (and words inparentheses) in our experiments.We trained parsers on 20%, 40%, 60%, 80%,and 100% of the treebank training data to evaluate4This amount was constrained based on both CPU andmemory.
We plan to investigate cloud computing to exploitmore unlabeled data.5As nonterminal-yield unary rules are less likely to beposited by a statistical parser, it is common for parsers trainedon the standard Chinese treebank to have substantially higherprecision than recall.
This gap between bracket recall andprecision is alleviated without loss of parse accuracy by delet-ing the nonterminal-yield unary rules.
This modification sim-ilarly benefits both parsers we study here.835the effect of the amount of labeled training data onparsing performance.
We also compare how self-training impacts the models trained with differentamounts of gold-standard training data.
This al-lows us to simulate scenarios where the languagehas limited human-labeled resources.We compare models trained only on the goldlabeled training data with those that utilize ad-ditional unlabeled data.
Self-training (PCFG-LAor Charniak) proceeds in two steps.
In the firststep, the parser is first trained on the allocated la-beled training data (e.g., 40%) and is then usedto parse the unlabeled data.
In the second step,a new parser is trained on the weighted combina-tion6of the allocated labeled training data and theadditional automatically labeled data.
The devel-opment set is used in each step to select the gram-mar order with the best accuracy for the PCFG-LAparser and to tune the smoothing parameters forCharniak?s parser.5 ResultsIn this section, we first present the effect of un-known and rare word handling for the PCFG-LAparser, and then compare and discuss the perfor-mance of the PCFG-LA parser and Charniak?sparser across languages with different amountsof labeled training, either with or without self-training.5.1 Rare and Unknown Word HandlingTable 2 reports the effect of unknown and rareword handing for the PCFG-LA parser trained on100%7of the labeled training data.
The rare wordhandling improves the English parser by 0.68%and the Chinese parser by 0.56% over the Berke-ley parser.
The Chinese unknown word handlingmethod alone improves the Chinese parser by0.47%.
The rare and unknown handling methodstogether improve the Chinese parser by 0.92%.
Allthe improvements are statistically significant8.We found that the rare word handling methodbecomes more effective as the number of latent an-notations increases, especially when there is not a6We balance the size of manually and automatically la-beled data by posterior weighting for the PCFG-LA parsersand by duplication for Charniak?s parser.7Greater improvements are obtained using smalleramounts of labeled training data.8We use Bikel?s randomized parsing evaluation compara-tor to determine the significance (p < 0.05) of differencebetween two parsers?
output.English ChinesePCFG-LA 89.95 83.23+R 90.63 83.79+U N/A 83.70+R+U N/A 84.15Table 2: Effects of rare word handling (+R) andChinese unknown handling (+U) on the test set.sufficient amount of labeled training data.
Shar-ing statistics of the rare words during training re-sults in more robust grammars with better pars-ing performance.
The unknown word handlingmethod also gives greater improvements on gram-mars trained on smaller amounts of training data,suggesting that it is quite helpful for modeling un-seen words at decoding time.
However, it tends tobe less effective when the number of latent anno-tations increases, probably because the probabilityestimation of unseen words based on surface tagsis less reliable for finer-gained latent annotations.5.2 Labeled Data OnlyWhen comparing the two parsers on both lan-guages in Figure 2 with treebank training, it isclear that they perform much better on Englishthan Chinese.
While this is probably due in partto the years of research on English, Chinese stillappears to be more challenging than English.
Thecomparison between the two parsing approachesprovides two interesting conclusions.First, the PCFG-LA parser always performs sig-nificantly better than Charniak?s parser on Chi-nese, although both model English well.
Ad-mittedly Charniak?s parser has not been opti-mized9on Chinese, but neither has the PCFG-LA parser10.
The lexicalized model in Charniak?sparser was first optimized for English and requiredsophisticated smoothing to deal with sparseness;however, the lexicalized model developed for Chi-nese works less well.
In contrast, the PCFG-LAparser learns the latent annotations from the data,without any specification of what precisely shouldbe modeled and how it should be modeled.
Thisflexibility may help it better model new languages.Second, while both parsers benefit from in-creased amounts of gold standard training data,the PCFG-LA parser gains more.
The PCFG-LAparser is initially poorer than Charniak?s parser9The Chinese port includes modification of the head table,implementation of a Chinese punctuation model, etc.10The PCFG-LA parser without the unknown word han-dling method still outperforms Charniak?s parser on Chinese.8368788899091920.2  0.4  0.6  0.8  1F scoreNumber of Labeled WSJ Training Treesx 39,832PCFG-LAPCFG-LA.ST CharniakCharniak.ST(a) English7678808284860.2  0.4  0.6  0.8  1F scoreNumber of Labeled CTB Training Treesx 24,416(b) ChineseFigure 2: The performance of the PCFG-LAparser and Charniak?s parser evaluated on the testset, trained with different amounts of labeled train-ing data, with and without self-training (ST).when trained on 20% WSJ training data, proba-bly because the training data is too small for it tolearn fine-grained annotations without over-fitting.As more labeled training data becomes avail-able, the performance of the PCFG-LA parser im-proves quickly and finally outperforms Charniak?sparser significantly.
Moreover, performance of thePCFG-LA parser continues to grow when more la-beled training data is available, while the perfor-mance of Charniak?s parser levels out at around80% of the labeled data.
The PCFG-LA parser im-proves by 3.5% when moving from 20% to 100%training data, compared to a 2.21% gain for Char-niak?s parser.
Similarly for Chinese, the PCFG-LA parser also gains more (4.48% vs 3.63%).5.3 Labeled + Self-LabeledThe PCFG-LA parser is also able to benefit morefrom self-training than Charniak?s parser.
On theWSJ data set, Charniak?s parser benefits from self-training initially when there is little labeled train-ing data, but the improvement levels out quicklyas more labeled training trees become available.In contrast, the PCFG-LA parser benefits consis-tently from self-training11, even when using 100%11One may notice that the self-trained PCFG-LA parserwith 100% labeled WSJ data has a slightly lower test accu-of the labeled training set.
Similar trends are alsofound for Chinese.It should be noted that the PCFG-LA parsertrained on a fraction of the treebank training dataplus a large amount of self-labeled training data,which comes with little or no cost, performs com-parably or even better than grammars trained withadditional labeled training data.
For example, theself-trained PCFG-LA parser with 60% labeleddata is able to outperform the grammar trainedwith 100% labeled training data alone for both En-glish and Chinese.
With self-training, even 40%labeled WSJ training data is sufficient to train aPCFG-LA parser that is comparable to the modeltrained on the entire WSJ training data alone.
Thisis of significant importance, especially for lan-guages with limited human-labeled resources.One might conjecture that the PCFG-LA parserbenefits more from self-training than Charniak?sparser because its self-labeled data has higher ac-curacy.
However, this is not true.
As shown in Fig-ure 2 (a), the PCFG-LA parser trained with 40%of the WSJ training set alne has a much lowerperformance (88.57% vs 89.96%) than Charniak?sparser trained on the full WSJ training set.
Withthe same amount of self-training data (labeled byeach parser), the resulting PCFG-LA parser ob-tains a much higher F score than the self-trainedCharniak?s parser (90.52% vs 90.18%).
Similarpatterns can also be found for Chinese.English ChinesePCFG-LA 90.63 84.15+ Self-training 91.46 85.18Table 3: Final results on the test set.Table 3 reports the final results on the test setwhen trained on the entire WSJ or CTB6 trainingset.
For English, self-training contributes 0.83%absolute improvement to the PCFG-LA parser,which is comparable to the improvement obtainedfrom using semi-supervised training with the two-stage parser in (McClosky et al, 2006).
Note thattheir improvement is achieved with the additionof 2,000k unlabeled sentences using the combi-nation of a generative parser and a discriminativereranker, compared to using only 210k unlabeledsentences with a single generative parser in ourapproach.
For Chinese, self-training results in aracy than the self-trained PCFG-LA parser with 80% labeledWSJ data.
This is due to the variance in parser performancewhen initialized with different seeds and the fact that the de-velopment set is used to pick the best model for evaluation.8378788899091929394959697980.2  0.4  0.6  0.8  1F scoreNumber of Labeled WSJ Training Treesx 39,832TestTest.ST TrainTrain.ST60657075808590951000  1  2  3  4  5  6  7 103104105106107F scoreNumberof Rules(logscale)Split-Merge Roundsfewer latent states more latent statesTestTest.ST TrainTrain.ST RulesRules.ST87888990919293949596970.2  0.4  0.6  0.8  1F scoreNumber of Labeled WSJ Training Treesx 39,832TestTest.ST TrainTrain.ST56 6 6 66 7 7 7 7(a) Charniak (b) PCFG-LA (20% WSJ) (c) PCFG-LAFigure 3: (a) The training/test accuracy of Charniak?s parser trained on varying amounts of labeledWSJ training data, with and without self-training (ST).
(b) The training/test accuracy and the numberof nonzero rules of the PCFG-LA grammars trained on 20% of the labeled WSJ training data, w/ andw/o ST. (c) The training/test accuracy of the PCFG-LA parser trained on varying amount of labeled WSJtraining data, w/ and w/o ST; the numbers along the training curves indicate the order of the grammars.767880828486889092940.2  0.4  0.6  0.8  1F scoreNumber of Labeled CTB Training Treesx 24,416TestTest.ST TrainTrain.ST5560657075808590951000  1  2  3  4  5  6  7 103104105106107F scoreNonzeroRules(log scale)Split-Merge Roundsfewer latent states more latent statesTestTest.ST TrainTrain.ST RulesRules.ST7880828486889092940.2  0.4  0.6  0.8  1F scoreNumber of Labeled CTB Training TreesTrain/Test Performance of the PCFG-LA Parser (CTB)x 24,416TestTest.ST TrainTrain.ST45 56 66667 7(a) Charniak (b) PCFG-LA (20% CTB) (c) PCFG-LAFigure 4: (a) The training/test accuracy of Charniak?s parser trained on varying amounts of labeledCTB training data, with and without self-training (ST).
(b) The training/test accuracy and the numberof nonzero rules of the PCFG-LA grammars trained on 20% of the labeled CTB training data, w/ andw/o ST. (c) The training/test accuracy of the PCFG-LA parser trained on varying amount of labeled CTBtraining data, w/ and w/o ST; the numbers along the training curves indicate the order of the grammars.state-of-the-art parsing model with 85.18% accu-racy (1.03% absolute improvement) on a represen-tative test set.
Both improvements are statisticallysignificant.6 AnalysisIn this section, we perform a series of analyses,focusing on English (refer to Figure 3), to investi-gate why the PCFG-LA parser benefits more fromadditional data, most particularly automatically la-beled data, when compared to Charniak?s parser.Similar analyses have been done for Chinese withsimilar results (refer to Figure 4).Charniak?s parser is a lexicalized PCFG parserthat models lexicalized dependencies explicitlyobservable in the training data and relies onsmoothing to avoid over-fitting.
Although it isable to benefit from more training data because ofbroader lexicon and rule coverage and more robustestimation of parameters, its ability to benefit fromthe additional data is limited in the sense that it isnot able to generate additional predictive featuresthat are supported by this data.
As shown in fig-ure 3(a), the parsing accuracy of Charniak?s parseron the test set improves as the amount of labeledtraining data increases; however, the training accu-racy12degrades as more data is added.
Note thatthe training accuracy13of Charniak?s parser also12The parser is tested on the treebank labeled set that theparser is trained on.13The self-training data is combined with the labeled tree-bank trees in a weighted manner; otherwise, the training ac-curacy would be even lower.838decreases after the addition of self-training data.This is expected for models like Charniak?s parserwith fixed model parameters; it is harder to modelmore data with greater diversity.
The addition ofself-labeled data helps on the test set initially but itprovides little gain when the labeled training databecomes relatively large.In contrast, the PCFG-LA grammar is able tomodel the training data with different granulari-ties.
Fewer latent annotations are employed whenthe training set is small.
As the size of the train-ing data increases, it is able to allocate more latentannotations to better model the data.
As shown inFigure 3 (b), for a fixed amount (20%) of labeledtraining data, the accuracy of the model on train-ing data continues to improve as the number of la-tent annotation increases.
Although it is importantto limit the number of latent annotations to avoidover-fitting, the ability to model training data ac-curately given sufficient latent annotations is desir-able when more training data is available.
Whentrained on the labeled data (20%) alone, the 5-thorder grammar achieves its optimal generalizationperformance (based on the development set) andbegins to degrade afterwords.
With the addition ofself-training data, the 5-th order grammar achievesan even greater accuracy on the test set and its per-formance continues to increase14when moving tothe 6-th or even 7-th order grammar.Figure 3 (c) plots the training and test curvesof the English PCFG-LA parser with varyingamounts of labeled training data, with and with-out self-training.
This figure differs substantiallyfrom Figure 3 (a).
First, as mentioned earlier, thePCFG-LA parser benefits much more from self-training than Charniak?s parser with moderate tolarge amounts of labeled training data.
Second, incontrast to Charniak?s parser for which training ac-curacy degrades consistently as the amount of la-beled training data increases, the training accuracyof the PCFG-LA parser sometimes improves whentrained on more labeled training data (e.g., the bestmodel (at order 6) trained on 40%15labeled train-14Although the 20% self-trained grammar has a higher testaccuracy at the 7-th round than the 6-th round, the develop-ment accuracy was better at the 6-th round, and thus we reportthe test accuracy of the 6-th round grammar in Figure 3 (c).15For models trained with greater amounts of labeled train-ing data, although their training accuracy becomes lower (dueto greater diversity) for the grammars (all at order 6) selectedby the development set, their 7-th order grammars (not re-ported in the figure) actually have both higher training andtest accuracies than the 6-th order grammar trained on lesstraining data.ing data alone has a higher training accuracy thanthe best model (at order 5) trained on 20% labeledtraining data).
Third, the addition of self-labeleddata supports more accurate PCFG-LA grammarswith higher orders than those trained without self-training, as evidenced by scores on both the train-ing and test data.
This suggests that the self-trained grammars are able to utilize more latentannotations to learn deeper dependencies.2468101214161820220  2  4  6  8  10  12  14  16 01.22.43.64.86RelativereductionofF error (%)Numberof bracketsSpan Lengthx 1e+4+Labeled +Unlabled #BracketsFigure 5: The relative reduction of bracketing er-rors for different span lengths, evaluated on thetest set.
The baseline model is the PCFG-LAparser trained on 20% of the WSJ training data.The +Unlabeled curve corresponds to the parsertrained with the additional automatically labeleddata and the +Labeled curve corresponds to theparser trained with additional 20% labeled trainingdata.
The counts of the brackets are computed onthe gold reference.
Span length ?0?
is designatedfor the effect on preterminal POS tags to differ-entiate it from the non-terminal brackets spanningonly one word.Figure 5 compares the effect of additional tree-bank labeled and automatically labeled data on therelative reduction of bracketing errors for differentspan lengths.
It is clear from the figure that the im-provement in parsing accuracy from self-trainingis the result of better bracketing across all spanlengths16.
However, even though the automati-cally labeled training data provides more improve-ment than the additional treebank labeled data interms of parsing accuracy, this data is less effectiveat improving tagging accuracy than the additionaltreebank labeled training data.So, how could self-training improve rule esti-mation when training the PCFG-LA parser withmore latent annotations?
One possibility is that theautomatically labeled data smooths the parameter16There is a slight degradation in bracketing accuracy forsome spans longer than 16 words, but the effect is negligibledue to their low counts.839estimates in the EM algorithm, enabling effectivetraining of models with more parameters to learndeeper dependencies.
Let p(a ?
b|e, t) be theposterior probability of expanding subcategories ato b given the event e, which is a rule expansionon a treebank parse tree t. Tland Tuare the setsof gold and automatically labeled parse trees, re-spectively.
The update of the rule expansion prob-ability p(a ?
b) in self-training (with weightingparameter ?)
can be expressed as:Pt?TlPe?tp(a?
b|e, t) + ?Pt?TuPe?tp(a?
b|e, t)Pb(Pt?TlPe?tp(a?
b|e, t) + ?Pt?TuPe?tp(a?
b|e, t))Since the unlabeled data is parsed by a lowerorder grammar (with fewer latent annotations),the expected counts from the automatically la-beled data can be thought of as counts from alower-order grammar17that smooth the higher-order (with more latent annotations) grammar.We observe that many of the rule parameters ofthe grammar trained on WSJ training data alonehave zero probabilities (rules with extremely lowprobabilities are also filtered to zero), as was alsopointed out in (Petrov et al, 2006).
On the onehand, this is what we want because the grammarshould learn to avoid impossible rule expansions.On the other hand, this might also be a sign ofover-fitting of the labeled training data.
As shownin Figure 3 (b), the grammar obtained with the ad-dition of automatically labeled data contains manymore non-zero rules, and its performance contin-ues to improve with more latent annotations.
Sim-ilar patterns also appear when using self-trainingfor other amounts of labeled training data.
As ispartially reflected by the zero probability rules, theaddition of the automatically labeled data enablesthe exploration of a broader parameter space withless danger of over-fitting the data.
Also note thatthe benefit of the automatically labeled data is lessclear in the early training stages (i.e., when thereare fewer latent annotations), as can be seen in Fig-ure 3 (b).
This is probably because there is a smallnumber of free parameters and the treebank data issufficiently large for robust parameter estimation.17We also trained models using only the automatically la-beled data without combining it with human-labeled trainingdata, but they were no more accurate than those trained onthe human-labeled training data alone without self-training.7 ConclusionIn this paper, we showed that PCFG-LA parserscan be more effectively applied to languageswhere parsing is less well developed and that theyare able to benefit more from self-training thanlexicalized generative parsers.
We show for thefirst time that self-training is able to significantlyimprove the performance of a PCFG-LA parser, asingle generative parser, on both small and largeamounts of labeled training data.We conjecture based on our analysis that theEM training algorithm is able to exploit the in-formation available in both gold and automati-cally labeled data with more complex grammarswhile being less affected by over-fitting.
Bet-ter results would be expected by combining thePCFG-LA parser with discriminative rerankingapproaches (Charniak and Johnson, 2005; Huang,2008) for self training.
Self-training should alsobenefit other discriminatively trained parsers withlatent annotations (Petrov and Klein, 2008), al-though training would be much slower comparedto using generative models, as in our case.In future work, we plan to scale up the trainingprocess with more unlabeled training data (e.g.,gigaword) and investigate automatic selection ofmaterials that are most suitable for self-training.We also plan to investigate domain adaptation andapply the model to other languages with modesttreebank resources.
Finally, it is also important toexplore other ways to exploit the use of unlabeleddata.AcknowledgmentsThis material is based upon work supported inpart by the Defense Advanced Research ProjectsAgency (DARPA) under Contract No.
HR0011-06-C-0023 and NSF IIS-0703859.
Any opinions,findings and/or recommendations expressed in thispaper are those of the authors and do not necessar-ily reflect the views of the funding agencies or theinstitutions where the work was completed.ReferencesDaniel M. Bikel and David Chiang.
2000.
Two sta-tistical parsing models applied to the chinese tree-bank.
In Proceedings of the Second Chinese Lan-guage Processing Workshop.Pi-Chuan Chang, Michel Gally, and Christopher Man-ning.
2008.
Optimizing chinese word segmentation840for machine translation performance.
In ACL 2008Third Workshop on Statistical Machine Translation.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In ACL.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In ICAI.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In ACL.Michael John Collins.
1999.
Head-driven statisticalmodels for natural language parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA, USA.Mary Harper and Zhongqiang Huang.
2009.
Chinesestatistical parsing.
To appear in The Gale Book.Zhongqiang Huang, Mary Harper, and Wen Wang.2007.
Mandarin part-of-speech tagging and dis-criminative reranking.
In EMNLP.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In ACL.Terry Koo, Xavier Carrera, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
InACL.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: Tools for querying and manipulating tree datastructures.
In LREC.Roger Levy and Christopher Manning.
2003.
Is itharder to parse chinese, or the chinese treebank.
InACL.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InACL.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InHLT-NAACL.David McClosky, Eugene Charniak, and Mark John-son.
2008.
When is self-training effective for pars-ing?
In COLING.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL.Slav Petrov and Dan Klein.
2008.
Sparse multi-scalegrammars for discriminative latent variable parsing.In EMNLP.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In ACL.Roi Reichart and Ari Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
In ACL.Mark Steedman, Miles Osborne, Anoop Sarkar,Stephen Clark, Rebecca Hwa, Julia Hockenmaier,Paul Ruhlen, Steven Baker, and Jeremiah Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In EACL.Qin Wang, Dale Schuurmans, and Dekang Lin.
2008.Semi-supervised convex training for dependencyparsing.
In ACL.Nianwen Xue, Fei Xia, Fu-dong Chiou, and MartaPalmer.
2005.
The Penn Chinese Treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering.Bin Zhang and Jeremy G. Kahn.
2008.
Evaluation ofdecatur text normalizer for language model training.Technical report, University of Washington.841
