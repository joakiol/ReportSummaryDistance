Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 90?94,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsBaselines and Bigrams: Simple, Good Sentiment and Topic ClassificationSida Wang and Christopher D. ManningDepartment of Computer ScienceStanford UniversityStanford, CA 94305{sidaw,manning}@stanford.eduAbstractVariants of Naive Bayes (NB) and SupportVector Machines (SVM) are often used asbaseline methods for text classification, buttheir performance varies greatly depending onthe model variant, features used and task/dataset.
We show that: (i) the inclusion ofword bigram features gives consistent gains onsentiment analysis tasks; (ii) for short snippetsentiment tasks, NB actually does better thanSVMs (while for longer documents the oppo-site result holds); (iii) a simple but novel SVMvariant using NB log-count ratios as featurevalues consistently performs well across tasksand datasets.
Based on these observations, weidentify simple NB and SVM variants whichoutperform most published results on senti-ment analysis datasets, sometimes providinga new state-of-the-art performance level.1 IntroductionNaive Bayes (NB) and Support Vector Machine(SVM) models are often used as baselines for othermethods in text categorization and sentiment analy-sis research.
However, their performance varies sig-nificantly depending on which variant, features anddatasets are used.
We show that researchers havenot paid sufficient attention to these model selec-tion issues.
Indeed, we show that the better variantsoften outperform recently published state-of-the-artmethods on many datasets.
We attempt to catego-rize which method, which variants and which fea-tures perform better under which circumstances.First, we make an important distinction betweensentiment classification and topical text classifica-tion.
We show that the usefulness of bigram featuresin bag of features sentiment classification has beenunderappreciated, perhaps because their usefulnessis more of a mixed bag for topical text classifica-tion tasks.
We then distinguish between short snip-pet sentiment tasks and longer reviews, showing thatfor the former, NB outperforms SVMs.
Contrary toclaims in the literature, we show that bag of featuresmodels are still strong performers on snippet senti-ment classification tasks, with NB models generallyoutperforming the sophisticated, structure-sensitivemodels explored in recent work.
Furthermore, bycombining generative and discriminative classifiers,we present a simple model variant where an SVM isbuilt over NB log-count ratios as feature values, andshow that it is a strong and robust performer over allthe presented tasks.
Finally, we confirm the well-known result that MNB is normally better and morestable than multivariate Bernoulli NB, and the in-creasingly known result that binarized MNB is bet-ter than standard MNB.
The code and datasets toreproduce the results in this paper are publicly avail-able.
12 The MethodsWe formulate our main model variants as linear clas-sifiers, where the prediction for test case k isy(k) = sign(wTx(k) + b) (1)Details of the equivalent probabilistic formulationsare presented in (McCallum and Nigam, 1998).Let f (i) ?
R|V | be the feature count vector fortraining case i with label y(i) ?
{?1, 1}.
V is the1http://www.stanford.edu/?sidaw90set of features, and f (i)j represents the number of oc-currences of feature Vj in training case i. Definethe count vectors as p = ?
+?i:y(i)=1 f(i) andq = ?
+?i:y(i)=?1 f(i) for smoothing parameter?.
The log-count ratio is:r = log(p/||p||1q/||q||1)(2)2.1 Multinomial Naive Bayes (MNB)In MNB, x(k) = f (k), w = r and b = log(N+/N?
).N+, N?
are the number of positive and negativetraining cases.
However, as in (Metsis et al, 2006),we find that binarizing f (k) is better.
We take x(k) =f?
(k) = 1{f (k) > 0}, where 1 is the indicator func-tion.
p?, q?, r?
are calculated using f?
(i) instead of f (i)in (2).2.2 Support Vector Machine (SVM)For the SVM, x(k) = f?
(k), and w, b are obtained byminimizingwTw +C?imax(0, 1?
y(i)(wT f?
(i) + b))2 (3)We find this L2-regularized L2-loss SVM to workthe best and L1-loss SVM to be less stable.
The LI-BLINEAR library (Fan et al, 2008) is used here.2.3 SVM with NB features (NBSVM)Otherwise identical to the SVM, except we usex(k) = f?
(k), where f?
(k) = r?
?
f?
(k) is the elemen-twise product.
While this does very well for longdocuments, we find that an interpolation betweenMNB and SVM performs excellently for all docu-ments and we report results using this model:w?
= (1?
?)w?
+ ?w (4)where w?
= ||w||1/|V | is the mean magnitude of w,and ?
?
[0, 1] is the interpolation parameter.
Thisinterpolation can be seen as a form of regularization:trust NB unless the SVM is very confident.3 Datasets and TaskWe compare with published results on the followingdatasets.
Detailed statistics are shown in table 1.RT-s: Short movie reviews dataset containing onesentence per review (Pang and Lee, 2005).Dataset (N+, N?)
l CV |V | ?RT-s (5331,5331) 21 10 21K 0.8CR (2406,1366) 20 10 5713 1.3MPQA (3316,7308) 3 10 6299 0.8Subj.
(5000,5000) 24 10 24K 0.8RT-2k (1000,1000) 787 10 51K 1.5IMDB (25k,25k) 231 N 392K 0.4AthR (799,628) 345 N 22K 2.9XGraph (980,973) 261 N 32K 1.8BbCrypt (992,995) 269 N 25K 0.5Table 1: Dataset statistics.
(N+, N?
): number ofpositive and negative examples.
l: average num-ber of words per example.
CV: number of cross-validation splits, or N for train/test split.
|V |: thevocabulary size.
?
: upper-bounds of the differencesrequired to be statistically significant at the p < 0.05level.CR: Customer review dataset (Hu and Liu, 2004)processed like in (Nakagawa et al, 2010).2MPQA: Opinion polarity subtask of the MPQAdataset (Wiebe et al, 2005).3Subj: The subjectivity dataset with subjective re-views and objective plot summaries (Pang andLee, 2004).RT-2k: The standard 2000 full-length movie re-view dataset (Pang and Lee, 2004).IMDB: A large movie review dataset with 50k full-length reviews (Maas et al, 2011).4AthR, XGraph, BbCrypt: Classify pairs ofnewsgroups in the 20-newsgroups dataset withall headers stripped off (the third (18828) ver-sion5), namely: alt.atheism vs. religion.misc,comp.windows.x vs. comp.graphics, andrec.sport.baseball vs. sci.crypt, respectively.4 Experiments and Results4.1 Experimental setupWe use the provided tokenizations when they exist.If not, we split at spaces for unigrams, and we filterout anything that is not [A-Za-z] for bigrams.
We do2http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html3http://www.cs.pitt.edu/mpqa/4http://ai.stanford.edu/?amaas/data/sentiment5http://people.csail.mit.edu/jrennie/20Newsgroups91not use stopwords, lexicons or other resources.
Allresults reported use ?
= 1, C = 1, ?
= 0.25 forNBSVM, and C = 0.1 for SVM.For comparison with other published results, weuse either 10-fold cross-validation or train/test splitdepending on what is standard for the dataset.
TheCV column of table 1 specifies what is used.
Thestandard splits are used when they are available.The approximate upper-bounds on the difference re-quired to be statistically significant at the p < 0.05level are listed in table 1, column ?.4.2 MNB is better at snippets(Moilanen and Pulman, 2007) suggests that while?statistical methods?
work well for datasets withhundreds of words in each example, they cannothandle snippets datasets and some rule-based sys-tem is necessary.
Supporting this claim are examplessuch as not an inhumane monster6, or killing cancerthat express an overall positive sentiment with nega-tive words.Some previous work on classifying snippets in-clude using pre-defined polarity reversing rules(Moilanen and Pulman, 2007), and learning com-plex models on parse trees such as in (Nakagawa etal., 2010) and (Socher et al, 2011).
These worksseem promising as they perform better than manysophisticated, rule-based methods used as baselinesin (Nakagawa et al, 2010).
However, we find thatseveral NB/SVM variants in fact do better than thesestate-of-the-art methods, even compared to meth-ods that use lexicons, reversal rules, or unsupervisedpretraining.
The results are in table 2.Our SVM-uni results are consistent with BoF-noDic and BoF-w/Rev used in (Nakagawa et al,2010) and BoWSVM in (Pang and Lee, 2004).
(Nakagawa et al, 2010) used a SVM with second-order polynomial kernel and additional features.With the only exception being MPQA, MNB per-formed better than SVM in all cases.7Table 2 show that a linear SVM is a weak baselinefor snippets.
MNB (and NBSVM) are much betteron sentiment snippet tasks, and usually better thanother published results.
Thus, we find the hypothe-6A positive example from the RT-s dataset.7We are unsure, but feel that MPQA may be less discrimi-native, since the documents are extremely short and all methodsperform similarly.Method RT-s MPQA CR Subj.MNB-uni 77.9 85.3 79.8 92.6MNB-bi 79.0 86.3 80.0 93.6SVM-uni 76.2 86.1 79.0 90.8SVM-bi 77.7 86.7 80.8 91.7NBSVM-uni 78.1 85.3 80.5 92.4NBSVM-bi 79.4 86.3 81.8 93.2RAE 76.8 85.7 ?
?RAE-pretrain 77.7 86.4 ?
?Voting-w/Rev.
63.1 81.7 74.2 ?Rule 62.9 81.8 74.3 ?BoF-noDic.
75.7 81.8 79.3 ?BoF-w/Rev.
76.4 84.1 81.4 ?Tree-CRF 77.3 86.1 81.4 ?BoWSVM ?
?
?
90.0Table 2: Results for snippets datasets.
Tree-CRF:(Nakagawa et al, 2010) RAE: Recursive Autoen-coders (Socher et al, 2011).
RAE-pretrain: train onWikipedia (Collobert and Weston, 2008).
?Voting?and ?Rule?
: use a sentiment lexicon and hard-codedreversal rules.
?w/Rev?
: ?the polarities of phraseswhich have odd numbers of reversal phrases in theirancestors?.
The top 3 methods are in bold and thebest is also underlined.sis that rule-based systems have an edge for snippetdatasets to be false.
MNB is stronger for snippetsthan for longer documents.
While (Ng and Jordan,2002) showed that NB is better than SVM/logisticregression (LR) with few training cases, we showthat MNB is also better with short documents.
Incontrast to their result that an SVM usually beatsNB when it has more than 30?50 training cases, weshow that MNB is still better on snippets even withrelatively large training sets (9k cases).4.3 SVM is better at full-length reviewsAs seen in table 1, the RT-2k and IMDB datasetscontain much longer reviews.
Compared to the ex-cellent performance of MNB on snippet datasets,the many poor assumptions of MNB pointed outin (Rennie et al, 2003) become more crippling forthese longer documents.
SVM is much strongerthan MNB for the 2 full-length sentiment analy-sis tasks, but still worse than some other publishedresults.
However, NBSVM either exceeds or ap-proaches previous state-of-the art methods, even the92Our results RT-2k IMDB Subj.MNB-uni 83.45 83.55 92.58MNB-bi 85.85 86.59 93.56SVM-uni 86.25 86.95 90.84SVM-bi 87.40 89.16 91.74NBSVM-uni 87.80 88.29 92.40NBSVM-bi 89.45 91.22 93.18BoW (bnc) 85.45 87.8 87.77BoW (b?t?c) 85.8 88.23 85.65LDA 66.7 67.42 66.65Full+BoW 87.85 88.33 88.45Full+Unlab?d+BoW 88.9 88.89 88.13BoWSVM 87.15 ?
90.00Valence Shifter 86.2 ?
?tf.
?idf 88.1 ?
?Appr.
Taxonomy 90.20 ?
?WRRBM ?
87.42 ?WRRBM + BoW(bnc) ?
89.23 ?Table 3: Results for long reviews (RT-2k andIMDB).
The snippet dataset Subj.
is also includedfor comparison.
Results in rows 7-11 are from(Maas et al, 2011).
BoW: linear SVM on bag ofwords features.
bnc: binary, no idf, cosine nor-malization.
?t?
: smoothed delta idf.
Full: thefull model.
Unlab?d: additional unlabeled data.BoWSVM: bag of words SVM used in (Pang andLee, 2004).
Valence Shifter: (Kennedy and Inkpen,2006).
tf.
?idf: (Martineau and Finin, 2009).
Ap-praisal Taxonomy: (Whitelaw et al, 2005).
WR-RBM: Word Representation Restricted BoltzmannMachine (Dahl et al, 2012).ones that use additional data.
These sentiment anal-ysis results are shown in table 3.4.4 Benefits of bigrams depends on the taskWord bigram features are not that commonly usedin text classification tasks (hence, the usual term,?bag of words?
), probably due to their having mixedand overall limited utility in topical text classifica-tion tasks, as seen in table 4.
This likely reflects thatcertain topic keywords are indicative alone.
How-ever, in both tables 2 and 3, adding bigrams alwaysimproved the performance, and often gives betterresults than previously published.8 This presum-ably reflects that in sentiment classification there are8However, adding trigrams hurts slightly.Method AthR XGraph BbCryptMNB-uni 85.0 90.0 99.3MNB-bi 85.1 +0.1 91.2 +1.2 99.4 +0.1SVM-uni 82.6 85.1 98.3SVM-bi 83.7 +1.1 86.2 +0.9 97.7 ?0.5NBSVM-uni 87.9 91.2 99.7NBSVM-bi 87.7 ?0.2 90.7 ?0.5 99.5 ?0.2ActiveSVM ?
90 99DiscLDA 83 ?
?Table 4: On 3 20-newsgroup subtasks, we compareto DiscLDA (Lacoste-Julien et al, 2008) and Ac-tiveSVM (Schohn and Cohn, 2000).much bigger gains from bigrams, because they cancapture modified verbs and nouns.4.5 NBSVM is a robust performerNBSVM performs well on snippets and longer doc-uments, for sentiment, topic and subjectivity clas-sification, and is often better than previously pub-lished results.
Therefore, NBSVM seems to be anappropriate and very strong baseline for sophisti-cated methods aiming to beat a bag of features.One disadvantage of NBSVM is having the inter-polation parameter ?.
The performance on longerdocuments is virtually identical (within 0.1%) for?
?
[?, 1], while ?
= ?
is on average 0.5% betterfor snippets than ?
= 1.
Using ?
?
[?,?]
makesthe NBSVM more robust than more extreme values.4.6 Other resultsMultivariate Bernoulli NB (BNB) usually performsworse than MNB.
The only place where BNB iscomparable to MNB is for snippet tasks using onlyunigrams.
In general, BNB is less stable than MNBand performs up to 10% worse.
Therefore, bench-marking against BNB is untrustworthy, cf.
(McCal-lum and Nigam, 1998).For MNB and NBSVM, using the binarized MNBf?
is slightly better (by 1%) than using the raw countfeature f .
The difference is negligible for snippets.Using logistic regression in place of SVM givessimilar results, and some of our results can beviewed more generally in terms of generative vs.discriminative learning.93ReferencesR.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: Deep neural networkswith multitask learning.
In Proceedings of ICML.George E. Dahl, Ryan P. Adams, and Hugo Larochelle.2012.
Training restricted boltzmann machines onword observations.
arXiv:1202.5695v1 [cs.LG].Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874, June.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In Proceedings ACM SIGKDD,pages 168?177.Alistair Kennedy and Diana Inkpen.
2006.
Sentimentclassification of movie reviews using contextual va-lence shifters.
Computational Intelligence, 22.Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.2008.
DiscLDA: Discriminative learning for dimen-sionality reduction and classification.
In Proceedingsof NIPS, pages 897?904.Andrew L. Maas, Raymond E. Daly, Peter T. Pham, DanHuang, Andrew Y. Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
In Pro-ceedings of ACL.Justin Martineau and Tim Finin.
2009.
Delta tfidf: Animproved feature space for sentiment analysis.
In Pro-ceedings of ICWSM.Andrew McCallum and Kamal Nigam.
1998.
A compar-ison of event models for naive bayes text classification.In AAAI-98 Workshop, pages 41?48.Vangelis Metsis, Ion Androutsopoulos, and GeorgiosPaliouras.
2006.
Spam filtering with naive bayes -which naive bayes?
In Proceedings of CEAS.Karo Moilanen and Stephen Pulman.
2007.
Sentimentcomposition.
In Proceedings of RANLP, pages 378?382, September 27-29.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classificationusing CRFs with hidden variables.
In Proceedings ofACL:HLT.Andrew Y Ng and Michael I Jordan.
2002.
On discrim-inative vs. generative classifiers: A comparison of lo-gistic regression and naive bayes.
In Proceedings ofNIPS, volume 2, pages 841?848.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of ACL.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of ACL.Jason D. Rennie, Lawrence Shih, Jaime Teevan, andDavid R. Karger.
2003.
Tackling the poor assump-tions of naive bayes text classifiers.
In Proceedings ofICML, pages 616?623.Greg Schohn and David Cohn.
2000.
Less is more: Ac-tive learning with support vector machines.
In Pro-ceedings of ICML, pages 839?846.Richard Socher, Jeffrey Pennington, Eric H. Huang, An-drew Y. Ng, and Christopher D. Manning.
2011.Semi-Supervised Recursive Autoencoders for Pre-dicting Sentiment Distributions.
In Proceedings ofEMNLP.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal taxonomies for sentiment anal-ysis.
In Proceedings of CIKM-05.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation, 39(2-3):165?210.94
