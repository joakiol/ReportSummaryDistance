Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455?1464,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDynamic Feature Selection for Dependency ParsingHe He Hal Daume?
IIIDepartment of Computer ScienceUniversity of MarylandCollege Park, MD 20740{hhe,hal}@cs.umd.eduJason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218jason@cs.jhu.eduAbstractFeature computation and exhaustive searchhave significantly restricted the speed ofgraph-based dependency parsing.
We proposea faster framework of dynamic feature selec-tion, where features are added sequentially asneeded, edges are pruned early, and decisionsare made online for each sentence.
We modelthis as a sequential decision-making problemand solve it by imitation learning techniques.We test our method on 7 languages.
Our dy-namic parser can achieve accuracies compara-ble or even superior to parsers using a full setof features, while computing fewer than 30%of the feature templates.1 IntroductionGraph-based dependency parsing usually consists oftwo stages.
In the scoring stage, we score all pos-sible edges (or other small substructures) using alearned function; in the decoding stage, we use com-binatorial optimization to find the dependency treewith the highest total score.Generally linear edge-scoring functions are usedfor speed.
But they use a large set of features, de-rived from feature templates that consider differentconjunctions of the edge?s attributes.
As a result,parsing time is dominated by the scoring stage?computing edge attributes, using them to instanti-ate feature templates, and looking up the weights ofthe resulting features in a hash table.
For example,McDonald et al(2005a) used on average about 120first-order feature templates on each edge, built fromattributes such as the edge direction and length, thetwo words connected by the edge, and the parts ofspeech of these and nearby words.We therefore ask the question: can we use fewerfeatures to score the edges, while maintaining the ef-fect that the true dependency tree still gets a higherscore?
Motivated by recent progress on dynamicfeature selection (Benbouzid et al 2012; He et al2012), we propose to add features one group at atime to the dependency graph, and to use these fea-tures together with interactions among edges (as de-termined by intermediate parsing results) to makehard decisions on some edges before all their fea-tures have been seen.
Our approach has a similarflavor to cascaded classifiers (Viola and Jones, 2004;Weiss and Taskar, 2010) in that we make decisionsfor each edge at every stage.
However, in place ofrelatively simple heuristics such as a global relativepruning threshold, we learn a featurized decision-making policy of a more complex form.
Since eachdecision can affect later stages, or later decisions inthe same stage, we model this problem as a sequen-tial decision-making process and solve it by DatasetAggregation (DAgger) (Ross et al 2011), a recentiterative imitation learning technique for structuredprediction.Previous work has made much progress on thecomplementary problem: speeding up the decodingstage by pruning the search space of tree structures.In Roark and Hollingshead (2008) and Bergsma andCherry (2010), pruning decisions are made locallyas a preprocessing step.
In the recent vine prun-ing approach (Rush and Petrov, 2012), significantspeedup is gained by leveraging structured infor-mation via a coarse-to-fine projective parsing cas-1455cade (Charniak et al 2006).
These approachesdo not directly tackle the feature selection problem.Although pruned edges do not require further fea-ture computation, the pruning step must itself com-pute similar high-dimensional features just to de-cide which edges to prune.
For this reason, Rushand Petrov (2012) restrict the pruning models to asmaller feature set for time efficiency.
We aim to dofeature selection and edge pruning dynamically, bal-ancing speed and accuracy by using only as manyfeatures as needed.In this paper, we first explore standard static fea-ture selection methods for dependency parsing, andshow that even a few feature templates can give de-cent accuracy (Section 3.2).
We then propose anovel way to dynamically select features for eachedge while keeping the overhead of decision mak-ing low (Section 4).
Our present experiments use theMaximum Spanning Tree (MST) parsing algorithm(McDonald et al 2005a; McDonald and Pereira,2006).
However, our approach applies to othergraph-based dependency parsers as well?includingnon-projective parsing, higher-order parsing, or ap-proximations to higher-order parsing that use stack-ing (Martins et al 2008), belief propagation (Smithand Eisner, 2008), or structured boosting (Wang etal., 2007).2 Graph-based Dependency ParsingIn graph-based dependency parsing of an n-word in-put sentence, we must construct a tree y whose ver-tices 0, 1, .
.
.
n correspond to the root node (namely0) and the ordered words of the sentence.
Each di-rected edge of this tree points from a head (parent)to one of its modifiers (child).Following a common approach to structured pre-diction problems, the score of a tree y is definedas a sum of local scores.
That is, s?
(y) = ?
?
?E?y ?
(E) =?E?y ?
?
?
(E), where E rangesover small connected subgraphs of y that can bescored individually.
Here ?
(E) extracts a high-dimensional feature vector from E together with theinput sentence, and ?
denotes a weight vector thathas typically been learned from data.The first-order model decomposes the tree intoedges E of the form ?h,m?, where h ?
[0, n] andm ?
[1, n] (with h 6= m) are a head token and oneof its modifiers.
Finding the best tree requires firstcomputing ???
(E) for each of the n2 possible edges.Since scoring the edges independently in this wayrestricts the parser to a local view of the depen-dency structure, higher-order models can achievebetter accuracy.
For example, in the second-ordermodel of McDonald and Pereira (2006), each localsubgraph E is a triple that includes the head andtwo modifiers of the head, which are adjacent toeach other.
Other methods that use triples includegrandparent-parent-child triples (Koo and Collins,2010), or non-adjacent siblings (Carreras, 2007).Third-order models (Koo and Collins, 2010) usequadruples, employing grand-sibling and tri-siblinginformation.The usual inference problem is to find the high-est scoring tree for the input sentence.
Note that ina valid tree, each token 1, .
.
.
, n must be attachedto exactly one parent (either another token or theroot 0).
We can further require the tree to be pro-jective, meaning that edges are not allowed to crosseach other.
It is well known that dynamic program-ming can be used to find the best projective depen-dency tree in O(n3) time, much as in CKY, for first-order models and some higher-order models (Eis-ner, 1996; McDonald and Pereira, 2006).1 Whenthe projectivity restriction is lifted, McDonald et al(2005b) pointed out that the best tree can be found inO(n2) time using a minimum directed spanning treealgorithm (Chu and Liu, 1965; Edmonds, 1967; Tar-jan, 1977), though only for first-order models.2 Wewill make use of this fast non-projective algorithmas a subroutine in early stages of our system.3 Dynamic Feature SelectionUnlike typical feature selection methods that fix asubset of selected features and use it throughout test-ing, in dynamic feature selection we choose featuresadaptively for each instance.
We briefly introducethis framework below and motivate our algorithmfrom empirical results on MST dependency parsing.1Although the third-order model of Koo and Collins (2010),for example, takes O(n4) time.2The non-projective parsing problem becomes NP-hard forhigher-order models.
One approximate solution (McDonaldand Pereira, 2006) works by doing projective parsing and thenrearranging edges.1456.This time , the firms were ready$ .This time , the firms were ready$ $ This time , the firms were ready .This time , were ready .$ the firmsadd feat.groupprojectivedecoding(a) (b) (c).This time , were ready$ the firms(d)(e).This time , the firms were ready$(f)add feat.groupadd feat.groupadd f eat.groupFigure 1: Dynamic feature selection for dependency parsing.
(a) Start with all possible edges except those filteredby the length dictionary.
(b) ?
(e) Add the next group of feature templates and parse using the non-projective parser.Predicted trees are shown as blue and red edges, where red indicates the edges that we then decide to lock.
Dashededges are pruned because of having the same child as a locked edge; 2-dot-3-dash edges are pruned because of crossingwith a locked edge; fine-dashed edges are pruned because of forming a cycle with a locked edge; and 2-dot-1-dashedges are pruned since the root has already been locked with one child.
(f) Final projective parsing.3.1 Sequential Decision MakingOur work is motivated by recent progress on dy-namic feature selection (Benbouzid et al 2012; Heet al 2012; Grubb and Bagnell, 2012), where fea-tures are added sequentially to a test instance basedon previously acquired features and intermediateprediction results.
This requires sequential decisionmaking.
Abstractly, when the system is in some states ?
S, it chooses an action a = pi(s) from the ac-tion setA using its policy pi, and transitions to a newstate s?, inducing some cost.
In the specific case ofdynamic feature selection, when the system is in agiven state, it decides whether to add some morefeatures or to stop and make a prediction based onthe features added so far.
Usually the sequential de-cision making problem is solved by reinforcementlearning (Sutton and Barto, 1998) or imitation learn-ing (Abbeel and Ng, 2004; Ratliff et al 2004).The dynamic feature selection framework hasbeen successfully applied to supervised classifica-tion and ranking problems (Benbouzid et al 2012;He et al 2012; Gao and Koller, 2010).
Below, wedesign a version that avoids overhead in our struc-tured prediction setting.
As there are n2 possibleedges on a sentence of length n, we wish to avoidthe overhead of making many individual decisionsabout specific features on specific edges, with eachdecision considering the current scores of all otheredges.
Instead we will batch the work of dynamicfeature selection into a smaller number of coarse-grained steps.3.2 StrategyTo speed up graph-based dependency parsing, wefirst investigate time usage in the parsing processon our development set, section 22 of the PennTreebank (PTB) (Marcus et al 1993).
In Fig-ure 2, we observe that (a) feature computation tookmore than 80% of the total time; (b) even thoughnon-projective decoding time grows quadratically interms of the sentence length, in practice it is al-most negligible compared to the projective decodingtime, with an average of 0.23 ms; (c) the second-order projective model is significantly slower dueto higher asymptotic complexity in both the scoringand decoding stages.At each stage of our algorithm, we need to de-cide whether to use additional features to refine theedge scores.
As making this decision separately foreach of the n2 possible edges is expensive, we in-stead propose a version that reduces the number ofdecisions needed.
We show the process for one shortsentence in Figure 1.
The first step is to parse us-ing the current features.
We use the fast first-ordernon-projective parser for this purpose, since givenobservations (b) and (c), we cannot afford to runprojective parsing multiple times.
The single result-ing tree (blue and red edges in Figure 1) has only14570 10 20 30 40 50 60 70sentence length0200400600800100012001400meantime(ms)1st-order scoring O(n2)2nd-order scoring O(n3)proj dec O(n3)non-proj dec O(n2)2nd-order proj dec O(n3)Figure 2: Time comparison of scoring time and decodingtime on English PTB section 22.n edges, and we use a classifier to decide whichof these edges are reliable enough that we should?lock?
them?i.e., commit to including them in thefinal tree.
This is the only decision that our policypi must make.
Locked (red) edges are definitely inthe final tree.
We also do constraint propagation: werule out all edges that conflict with the locked edges,barring them from appearing in the final tree.3 Con-flicts are defined as violation of the projective pars-ing constraints:?
Each word has exactly one parent?
Edges cannot cross each other4?
The directed graph is non-cyclic?
Only one word is attached to the rootFor example, in Figure 1(d), the dashed edges areremoved because they have the same child as one ofthe locked (red) edges.
The 2-dot-3-dash edge time?
firms is removed because it crosses the lockededge (comma)?
were (whereas we ultimately seeka projective parse).
The fine dashed edge were ?
(period) is removed because it forms a cycle withwere ?
(period).
In Figure 1(e), the 2-dot-1-dashedge (root) ?
time is removed since we allow theroot to have only one modifier.3Constraint propagation also automatically locks an edgewhen all other edges with the same child have been ruled out.4A reviewer asks about the cost of finding edges that cross alocked edge.
Naively this is O(n2).
But at most n edges will belocked during the entire algorithm, for a total O(n3) runtime?the same as one call to projective parsing, and far faster in prac-tice.
With cleverness this can even be reduced to O(n2 logn).Once constraint propagation has finished, we visitall edges (gray) whose fate is still unknown, and up-date their scores in parallel by adding the next groupof features.As a result, most edges will be locked in or ruledout without needing to look up all of their features.Some edges may still remain uncertain even after in-cluding all features.
If so, a final iteration (Figure 1(f)) uses the slower projective parser to resolve thestatus of these maximally uncertain edges.
In ourexample, the parser does not figure out the correctparent of time until this final step.
This final, accu-rate parser can use its own set of weighted features,including higher-order features, as well as the pro-jectivity constraint.
But since it only needs to re-solve the few uncertain edges, both scoring and de-coding are fast.If we wanted our parser to be able to produce non-projective trees, then we would skip this final stepor have it use a higher-order non-projective parser.Also, at earlier steps we would not prune edgescrossing the locked edges.4 MethodsOur goal is to produce a faster dependency parser byreducing the feature computation time.
We assumethat we are given three increasingly accurate but in-creasingly slow parsers that can be called as sub-routines: a first-order non-projective parser, a first-order projective parser, and a second-order projec-tive parser.
In all cases, their feature weights havealready been trained using the full set of features,and we will not change these weights.
In generalwe will return the output of one of the projectiveparsers.
But at early iterations, the non-projectiveparser helps us rapidly consider interactions amongedges that may be relevant to our dynamic decisions.4.1 Feature Template RankingWe first rank the 268 first-order feature templates byforward selection.
We start with an empty list of fea-ture templates, and at each step we greedily add theone whose addition most improves the parsing ac-curacy on a development set.
Since some featuresmay be slower than others (for example, the ?be-tween?
feature templates require checking all tokensin-between the head and the modifier), we could in-14580 50 100 150 200 250 300 350 400Number of feature templates used0.50.60.70.80.9Unlabeledattachmentscore(UAS)1st-order non-proj1st-order proj2nd-orderFigure 3: Forward feature selection result using the non-projective model on English PTB section 22.stead select the feature template with the highest ra-tio of accuracy improvement to runtime.
However,for simplicity we do not consider this: after group-ing (see below), minor changes of the ranks within agroup have no effect.
The accuracy is evaluated byrunning the first-order non-projective parser, sincewe will use it to make most of the decisions.
The112 second-order feature templates are then rankedby adding them in a similar greedy fashion (giventhat all first-order features have already been added),evaluating with the second-order projective parser.We then divide this ordered list of feature tem-plates into K groups: {T1, T2, .
.
.
, TK}.
Our parseradds an entire group of feature templates at eachstep, since adding one template at a time would re-quire too many decisions and obviate speedups.
Thesimplest grouping method would be to put an equalnumber of feature templates in each group.
FromFigure 3 we can see that the accuracy increases sig-nificantly with the first few templates and graduallylevels off as we add less valuable templates.
Thus,a more cost-efficient method is to split the rankedlist into several groups so that the accuracy increasesby roughly the same amount after each group isadded.
In this case, earlier stages are fast becausethey tend to have many fewer feature templates thanlater stages.
For example, for English, we use 7groups of first-order feature templates and 4 groupsof second-order feature templates.
The sequence ofgroup sizes is 1, 4, 10, 12, 47, 33, 161 and 35, 29, 31,17 for first- and second-order parsing respectively.4.2 Sequential Feature SelectionSimilar to the length dictionary filter of Rush andPetrov (2012), for each test sentence, we first de-terministically remove edges longer than the maxi-mum length of edges in the training set that have thesame head POS tag, modifier POS tag, and direction.This simple step prunes around 40% of the non-goldedges in our Penn Treebank development set (Sec-tion 6.1) at a cost of less than 0.1% in accuracy.Given a test sentence of length n, we start witha complete directed graph G(V, E), where E ={?h,m?
: h ?
[0, n], m ?
[1, n]}.
After the lengthdictionary pruning step, we compute T1 for all re-maining edges to obtain a pruned weighted directedgraph.
We predict a parse tree using the features sofar (other features are treated as absent, with value0).
Then for each edge in this intermediate tree, weuse a binary linear classifier to choose between twoactions: A = {lock, add}.
The lock action ensuresthat ?h,m?
appears in the final parse tree by prun-ing edges that conflict with ?h,m?.5 If the classi-fier is not confident enough about the parent of m,it decides to add to gather more information.
Theadd action computes the next group of features for?h,m?
and all other competing edges with child m.(Since we classify the edges one at a time, deci-sions on one edge may affect later edges.
To im-prove efficiency and reduce cascaded error, we sortthe edges in the predicted tree and process them asabove in descending order of their scores.
)Now we can continue with the second iteration ofparsing.
Overall, our method runs up to K = K1 +K2 iterations on a given sentence, where we haveK1 groups of first-order features and K2 groups ofsecond-order features.
We run K1 ?
1 iterationsof non-projective first-order parsing (adding groupsT1, .
.
.
, TK1?1), then 1 iteration of projective first-order parsing (adding group TK1), and finally K2 it-erations of projective second-order parsing (addinggroups TK1+1, .
.
.
TK).Before each iteration, we use the result of the pre-vious iteration (as explained above) to prune someedges and add a new group of features to the rest.
We5If the conflicting edge is in the current predicted parse tree(which can happen because of non-projectivity), we forbid themodel to prune it.
Otherwise in rare cases the non-projectiveparser at the next stage may fail to find a tree.1459then run the relevant parser.
Each of the three parsershas a different set of feature weights, so when weswitch parsers on rounds K1 and K1 + 1, we mustalso change the weights of the previously added fea-tures to those specified by the new parsing model.In practice, we can stop as soon as the fate of alledges is known.
Also, if no projective parse treecan be constructed at round K1 using the availableunpruned edges, then we immediately fall back toreturning the non-projective parse tree from roundK1 ?
1.
This FAIL case rarely occurs in our experi-ments (fewer than 1% of sentences).We report results both for a first-order systemwhere K2 = 0 (shown in Figure 1 and Algorithm 1)and for a second-order system where K2 > 0.Algorithm 1 DynFS(G(V, E), pi)E ?
{?h,m?
: |h?m| ?
lenDict(h,m)}Add T1 to all edges in Ey?
?
non-projective decodingfor i = 2 to K doEsort ?
sort unlocked edges {E : E ?
y?}
indescending order of their scoresfor ?h,m?
?
Esort doif pi(?(?h,m?))
== lock thenE ?
E \ {{?h?,m?
?
E : h?
6= h}?{?h?,m??
?
E : crosses ?h,m?}
?{?h?,m??
?
E : cycle with ?h,m?
}}if h == 0 thenE ?
E \ {?0,m??
?
E : m?
6= m}end ifelseAdd Ti to {?h?,m??
?
E : m?
== m}end ifend forif i == K theny?
?
projective decodingelse if i 6= K or FAIL theny?
?
non-projective decodingend ifend forreturn y?5 Policy TrainingWe cast this problem as an imitation learning taskand use Dataset Aggregation (DAgger) Ross et al(2011) to train the policy iteratively.5.1 Imitation LearningIn imitation learning (also called apprenticeshiplearning) (Abbeel and Ng, 2004; Ratliff et al 2004),instead of exploring the environment directed by itsfeedback (reward) as in typical reinforcement learn-ing problems, the learner observes expert demon-strations and aims to mimic the expert?s behavior.The expert demonstration can be represented as tra-jectories of state-action pairs, {(st, at)} where t isthe time step.
A typical approach to imitation learn-ing is to collect supervised data from the expert?strajectories to learn a policy (multiclass classifier),where the input is ?
(s), a feature representation ofthe current state (we call these policy features toavoid confusion with the parsing features), and theoutput is the predicted action (label) for that state.In the sequential feature selection framework, it ishard to directly apply standard reinforcement learn-ing algorithms, as we cannot assign credit to certainfeatures until the policy decides to stop and let usevaluate the prediction result.
On the other hand,knowing the gold parse tree makes it easy to ob-tain expert demonstrations, which enables imitationlearning.5.2 DAggerSince the above approach collects training data onlyfrom the expert?s trajectories, it ignores the fact thatthe distribution of states at training time and that attest time are different.
If the learned policy can-not mimic the expert perfectly, one wrong step maylead to states never visited by the expert due to cu-mulative errors.
This problem of insufficient explo-ration can be alleviated by iteratively learning a pol-icy trained under states visited by both the expertand the learner (Ross et al 2011; Daume?
III et al2009; Ka?a?ria?inen, 2006).Ross et al(2011) proposed to train the policy iter-atively and aggregate data collected from the previ-ous learned policy.
Let pi?
denote the expert?s policyand spii denote states visited by executing pii.
In itssimplest parameter-free form, in each iteration, wefirst run the most recently learned policy pii; then foreach state spii on the trajectory, we collect a trainingexample (?
(spii), pi?
(spii)) by labeling the state withthe expert?s action.
Intuitively, this step intends tocorrect the learner?s mistakes and pull it back to the1460expert?s trajectory.
Thus we can obtain a policy thatperforms well under its own induced state distribu-tion.5.3 DAgger for Feature SelectionIn our case, the expert?s decision is rather straight-forward.
Replace the policy pi in Algorithm 1 byan expert.
If the edge under consideration is a goldedge, it executes lock; otherwise, it executes add.Basically the expert ?cheats?
by knowing the truetree and always making the right decision.
On ourPTB dev set, it can get 96.47% accuracy6 with only2.9% of the first-order features.
This is an upperbound on our performance.We present the training procedure in Algorithm2.
We begin by partitioning the training set intoN folds.
To simulate parsing results at test time,when collecting examples on T i, similar to cross-validation, we use parsers trained on T?
i = T \ T i.Also note that we show only one pass over trainingsentences in Algorithm 2; however, multiple passesare possible in practice, especially when the trainingdata is limited.Algorithm 2 DAgger(T , pi?
)Split the training sentences T into N foldsT 1, T 2, .
.
.
, T NInitialize D ?
?, pi1 ?
pi?for i = 1 to N dofor G(V, E) ?
T i doSample trajectories {(spii , pii(spii))} byDynFS(G(V, E), pii)D ?
D?{(?
(s), pi?
(s)}end forend forTrain policy pii+1 on Dreturn Best pii evaluated on development set5.4 Policy FeaturesOur linear edge classifier uses a feature vector ?
thatconcatenates all previously acquired parsing fea-tures together with ?meta-features?
that reflect con-fidence in the edge.
The classifier?s weights are fixed6The imperfect performance is because the accuracy is mea-sured with respect to the gold parse trees.
The expert onlymakes optimal pruning decisions but the performance dependson the pre-trained parser as well.across iterations, but ?
(edge) changes per iteration.We standardize the edge scores by a sigmoid func-tion.
Let s?
denote the normalized score, definedby s??(?h,m?)
= 1/(1 + exp{?s?(?h,m?)}).
Ourmeta-features for ?h,m?
include?
current normalized score, and normalized scorebefore adding the current feature group?
margin to the highest scoring competing edges,i.e., s?
(w, ?h,m?)?maxh?
s?
(w, ?h?,m?
)where h?
?
[0, n] and h?
6= h?
index of the next feature group to be addedWe also tried more complex meta-features, for ex-ample, mean and variance of the scores of compet-ing edges, and structured features such as whetherthe head of e is locked and how many locked chil-dren it currently has.
It turns out that given all theparsing features, the margin is the most discrimi-native meta-feature.
When it is present, other meta-features we added do not help much, Thus we do notinclude them in our experiments due to overhead.6 Experiment6.1 SetupWe generate dependency structures from the PTBconstituency trees using the head rules of Yamadaand Matsumoto (2003).
Following convention, weuse sections 02?21 for training, section 22 for de-velopment and section 23 for testing.
We also re-port results on six languages from the CoNLL-Xshared task (Buchholz and Marsi, 2006) as sug-gested in (Rush and Petrov, 2012), which cover avariety of language families.
We follow the stan-dard training/test split specified in the CoNLL-Xdata and tune parameters by cross validation whentraining the classifiers (policies).
The PTB test datais tagged by a Stanford part-of-speech (POS) tagger(Toutanova et al 2003) trained on sections 02?21.We use the provided gold POS tags for the CoNLLtest data.
All results are evaluated by the unlabeledattachment score (UAS).
For fair comparison withprevious work, punctuation is included when com-puting parsing accuracy of all CoNLL-X languagesbut not English (PTB).For policy training, we train a linear SVM classi-fier using Liblinear (Fan et al 2008).
For all lan-guages, we run DAgger for 20 iterations and se-1461Language MethodFirst-order Second-orderSpeedup Cost(%) UAS(D) UAS(F) Speedup Cost(%) UAS(D) UAS(F)BulgarianDYNFS 3.44 34.6 91.1 91.3 4.73 16.3 91.6 92.0VINEP 3.25 - 90.5 90.7 7.91 - 91.6 92.0ChineseDYNFS 2.12 42.7 91.0 91.3 2.36 31.6 91.6 91.9VINEP 1.02 - 89.3 89.5 2.03 - 90.3 90.5EnglishDYNFS 5.58 24.8 91.7 91.9 5.27 49.1 92.5 92.7VINEP 5.23 - 91.0 91.2 11.88 - 92.2 92.4GermanDYNFS 4.71 21.0 89.2 89.3 6.02 36.6 89.7 89.9VINEP 3.37 - 89.0 89.2 7.38 - 90.1 90.3JapaneseDYNFS 4.80 15.6 93.7 93.6 8.49 7.53 93.9 93.9VINEP 4.60 - 91.7 92.0 14.90 - 92.1 92.0PortugueseDYNFS 4.36 32.9 87.3 87.1 6.84 40.4 88.0 88.2VINEP 4.47 - 90.0 90.1 12.32 - 90.9 91.2SwedishDYNFS 3.60 37.8 88.8 89.0 5.04 22.1 89.5 89.8VINEP 4.64 - 88.3 88.5 13.89 - 89.4 89.7Table 1: Comparison of speedup and accuracy with the vine pruning cascade approach for six languages.
In the setup,DYNFS means our dynamic feature selection model, VINEP means the vine pruning cascade model, UAS(D) andUAS(F) refer to the unlabeled attachment score of the dynamic model (D) and the full-feature model (F) respectively.For each language, the speedup is relative to its corresponding first- or second-order model using the full set of features.Results for the vine pruning cascade model are taken from Rush and Petrov (2012).
The cost is the percentage offeature templates used per sentence on edges that are not pruned by the dictionary filter.lect the best policy evaluated on the development setamong the 20 policies obtained from each iteration.6.2 Baseline ModelsWe use the publicly available implementation ofMSTParser7 (with modifications to the feature com-putation) and its default settings, so the featureweights of the projective and non-projective parsersare trained by the MIRA algorithm (Crammer andSinger, 2003; Crammer et al 2006).Our feature set contains most features proposedin the literature (McDonald et al 2005a; Koo andCollins, 2010).
The basic feature components in-clude lexical features (token, prefix, suffix), POSfeatures (coarse and fine), edge length and direction.The feature templates consists of different conjunc-tions of these components.
Other than features onthe head word and the child word, we include fea-tures on in-between words and surrounding words aswell.
For PTB, our first-order model has 268 featuretemplates and 76,287,848 features; the second-ordermodel has 380 feature templates and 95,796,140 fea-tures.
The accuracy of our full-feature models is7http://www.seas.upenn.edu/?strctlrn/MSTParser/MSTParser.htmlcomparable or superior to previous results.6.3 Results0 1 2 3 4 5 6Feature selection stage0.00.20.40.60.81.0Time/Accuracy/EdgePercentageruntime %UAS %remaining edge %locked edge %pruned edge %Figure 4: System dynamics on English PTB section 23.Time and accuracy are relative to those of the baselinemodel using full features.
Red (locked), gray (unde-cided), dashed gray (pruned) lines correspond to edgesshown in Figure 1.In Table 1, we compare the dynamic parsing mod-els with the full-feature models and the vine prun-ing cascade models for first-order and second-order14620 10 20 30 40 50 60 70 80Runtime (s)0.500.550.600.650.700.750.800.850.900.95Unlabeledattachmentscore(UAS)staticdynamicFigure 5: Pareto curves for the dynamic and static ap-proaches on English PTB section 23.parsing.
The speedup for each language is defined asthe speed relative to its full-feature baseline model.We take results reported by Rush and Petrov (2012)for the vine pruning model.
As speed comparisonfor parsing largely relies on implementation, we alsoreport the percentage of feature templates chosen foreach sentence.
The cost column shows the averagenumber of feature templates computed for each sen-tence, expressed as a percentage of the number offeature templates if we had only pruned using thelength dictionary filter.From the table we notice that our first-ordermodel?s performance is comparable or superior tothe vine pruning model, both in terms of speedupand accuracy.
In some cases, the model with fewerfeatures even achieves higher accuracy than themodel with full features.
The second-order model,however, does not work as well.
In our experi-ments, the second-order model is more sensitive tofalse negatives, i.e.
pruning of gold edges, due tolarger error propagation than the first-order model.Therefore, to maintain parsing accuracy, the policymust make high-precision pruning decisions and be-comes conservative.
We could mitigate this by train-ing the original parsing feature weights in conjunc-tion with our policy feature weights.
In addition,there is larger overhead during when checking non-projective edges and cycles.We demonstrate the dynamics of our system inFigure 4 on PTB section 23.
We show how the run-time, accuracy, number of locked edges and unde-cided edges change over the iterations in our first-order dynamic projective parsing.
From iterations1 to 6, we obtain parsing results from the non-projective parser; in iteration 7, we run the projectiveparser.
The plot shows relative numbers (percent-age) to the baseline model with full features.
Thenumber of remaining edges drops quickly after thesecond iteration.
From Figure 3, however, we noticethat even with the first feature group which only con-tains one feature template, the non-projective parsercan almost achieve 50% accuracy.
Thus, ideally, ourpolicy should have locked that many edges after thefirst iteration.
The learned policy does not imitatethe expert perfectly, either because our policy fea-tures are not discriminative enough, or because a lin-ear classifier is not powerful enough for this task.Finally, to show the advantage of making dynamicdecisions that consider the interaction among edgeson the given input sentence, we compare our resultswith a static feature selection approach on PTB sec-tion 23.
The static algorithm does no pruning exceptby the length dictionary at the start.
In each iteration,instead of running a fast parser and making deci-sions online, it simply adds the next group of featuretemplates to all edges.
By forcing both algorithmsto stop after each stage, we get the Pareto curvesshown in Figure 5.
For a given level of high accu-racy, our dynamic approach (black) is much fasterthan its static counterpart (blue).7 ConclusionIn this paper we present a dynamic feature selec-tion algorithm for graph-based dependency parsing.We show that choosing feature templates adaptivelyfor each edge in the dependency graph greatly re-duces feature computation time and in some casesimproves parsing accuracy.
Our model also makesit practical to use an even larger feature set, sincefeatures are computed only when needed.
In future,we are interested in training parsers favoring the dy-namic feature selection setting, for example, parsersthat are robust to missing features, or parsers opti-mized for different stages.AcknowledgementsThis work was supported by the National ScienceFoundation under Grant No.
0964681.
We thank theanonymous reviewers for very helpful comments.1463ReferencesP.
Abbeel and A. Y. Ng.
2004.
Apprenticeship learningvia inverse reinforcement learning.
In Proceedings ofICML.D.
Benbouzid, R. Busa-Fekete, and B. Ke?gl.
2012.
Fastclassification using space decision DAGs.
In Proceed-ings of ICML.S.
Bergsma and C. Cherry.
2010.
Fast and accurate arcfiltering for dependency parsing.
In Proceedings ofCOLING.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared taskon multilingual dependency parsing.
In CoNLL.Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL.Eugene Charniak, Mark Johnson, Micha Elsner, JosephAusterweil, David Ellis, Isaac Haxton, Catherine Hill,R.
Shrivaths, Jeremy Moore, Michael Pozar, andTheresa Vu.
2006.
Multilevel coarse-to-fine PCFGparsing.
In Proceedings of ACL.Y.
J. Chu and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
Jour-nal of Machine Learning Research, 3:951?991.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.Hal Daume?
III, John Langford, and Daniel Marcu.
2009.Search-based structured prediction.
Machine Learn-ing Journal (MLJ).J.
Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards,(71B):233?240.Jason Eisner.
1996.
Three new probabilistic models fordependency parsing: an exploration.
In Proceedingsof COLING.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Tianshi Gao and Daphne Koller.
2010.
Active classifi-cation based on value of classifier.
In Proceedings ofNIPS.Alexander Grubb and J. Andrew Bagnell.
2012.SpeedBoost: Anytime prediction with uniform near-optimality.
In AISTATS.He He, Hal Daume?
III, and Jason Eisner.
2012.
Cost-sensitive dynamic feature selection.
In ICML Infern-ing Workshop.Matti Ka?a?ria?inen.
2006.
Lower bounds for reduc-tions.
Talk at the Atomic Learning Workshop (TTI-C),March.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of ACL.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.
InProceedings of EMNLP.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Proceedings of EACL, pages 81?88.Ryan McDonald, K. Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In Proceedings of ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency parsingusing spanning tree algorithms.
In Proc.
of EMNLP.N.
Ratliff, D. Bradley, J.
A. Bagnell, and J. Chestnutt.2004.
Boosting structured prediction for imitationlearning.
In Proceedings of ICML.B.
Roark and K. Hollingshead.
2008.
Classifying chartcells for quadratic complexity context-free inference.In Proceedings of COLING.Ste?phane.
Ross, Geoffrey J. Gordon, and J. Andrew.
Bag-nell.
2011.
A reduction of imitation learning andstructured prediction to no-regret online learning.
InProceedings of AISTATS.Alexander Rush and Slav Petrov.
2012.
Vine pruning forefficient multi-pass dependency parsing.
In Proceed-ings of NAACL.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In EMNLP.Richard S. Sutton and Andrew G. Barto.
1998.
Rein-forcement Learning : An Introduction.
MIT Press.R.
E. Tarjan.
1977.
Finding optimum branchings.
Net-works, 7(1):25?35.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In NAACL.Paul Viola and Michael Jones.
2004.
Robust feal-timeface detection.
International Journal of Computer Vi-sion, 57:137?154.Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007.Simple training of dependency parsers via structuredboosting.
In Proceedings of IJCAI.David Weiss and Ben Taskar.
2010.
Structured predic-tion cascades.
In Proceedings of AISTATS.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with Support Vector Machines.
In Pro-ceedings of IWPT.1464
