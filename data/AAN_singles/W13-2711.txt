Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 84?89,Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational LinguisticsUsing Comparable Collections of Historical Texts for Building aDiachronic Dictionary for Spelling NormalizationMarilisa AmoiaSaarland Universitym.amoial@mx.uni-saarland.deJose Manuel MartinezSaarland Universityj.martinez@mx.uni-saarland.deAbstractIn this paper, we argue that compara-ble collections of historical written re-sources can help overcoming typical chal-lenges posed by heritage texts enhanc-ing spelling normalization, POS-taggingand subsequent diachronic linguistic anal-yses.
Thus, we present a comparable cor-pus of historical German recipes and showhow such a comparable text collection to-gether with the application of innovativeMT inspired strategies allow us (i) to ad-dress the word form normalization prob-lem and (ii) to automatically generate adiachronic dictionary of spelling variants.Such a diachronic dictionary can be usedboth for spelling normalization and for ex-tracting new ?translation?
(word forma-tion/change) rules for diachronic spellingvariants.
Moreover, our approach can beapplied virtually to any diachronic collec-tion of texts regardless of the time spanthey represent.
A first evaluation showsthat our approach compares well withstate-of-art approaches.1 IntroductionThe study of heritage documents has been oneof the regular sources of knowledge in the Hu-manities, specially in history-related disciplines.The last years have witnessed an increased interestin approaches combining NLP and corpus-basedtechniques in the Humanities (Piotrowski, 2012)as they can provide new insights and/or a moreconsistent and reliable account of findings.Until recently, research efforts have been fo-cused on building diachronic corpora (e.g.
OldBailey Online project (Hitchcock et al 2012)and its follow-up, the Old Bailey Corpus (Huber,2007), the Bonn Corpus of Early New High Ger-man (Diel et al 2002) or the GerManC (Scheibleet al 2011b) for German and many others).
Suchresources are generally annotated with shallowmetadata (e.g.
year of publication, author, ge-ographical location) for allowing fast retrieval.However, the annotation of richer linguistic andsemantic information still poses a series of chal-lenges that have to be overcome, such as (i)the noise introduced by deviant linguistic data(spelling/orthography variation, lack of sentenceboundaries, etc.)
typical of this kind of material,due to the lack of standardized writing conventionsin terms of words and punctuation and hence (ii)the higher error rates obtained when applying stan-dard NLP methods.Further, standardization of spelling variation inhistorical texts can be broken down at least intotwo subproblems:1. the old word forms often differ from the mod-ern orthography of the same items.
Consider,for instance, the diachronic variants of thethird person singular of present tense of theverb werden in German (which means ?be-come?
as full verb, or is used as auxiliaryverb to build the future): wirt, wirdt, wirdetvs wird; (Piotrowski, 2012) and2.
the denomination of certain objects may re-sult completely different from that used inthe modern language due to historical reasons(e.g.
adoption of foreign language terms,semantic shift).
Consider, as an example,the German historical/modern variants of theword lemon (e.g.
Limonie/Zitrone) or of theword woman (e.g.
Weib/Frau).Previous approaches to spelling normalizationof historical texts have focused on the first sub-problem.
Two main strategies that have been ap-plied:?
a rule based strategy, in which the transla-tion of historical variants into modern forms84is performed on the ground of manually writ-ten or semi-automatically gathered rules (cf.
(Pilz et al 2008), (Bollmann et al 2011));?
a string similarity strategy, in which a semi-automatic attempt is made to link histori-cal variants with modern dictionary entriesfollowing string similarity (cf.
(Giusti etal., 2007), (Kunstmann and Stein, 2007),(Dipper, 2010), (Hendrickx and Marquilhas,2011), (Gotscharek et al 2011)) or pho-netic conflation strategies (cf.
(Koolen et al2006), (Jurish, 2008) ).These approaches have the disadvantage of end-ing up relying on a time-specific dictionary of vari-ants, e.g.
they can cope with variants realized intexts stemming from the same period of time forwhich they have been created but may result inap-propriate for texts belonging to other time spans.Moreover, to our knowledge, there is currentlyno approach to spelling normalization that can ad-dress successfully the second subproblem statedabove ?
the recognition of paraphrastic variationsrealized as completely different strings or consist-ing of semantic shifts.As it has been often noted, the problem of stan-dardizing diachronic variants can be understood asa translation operation, where instead of translat-ing between two different languages, translationtakes place between two diachronic varieties of thesame language.
Inspired by experiments done forinterlinguistic translation (Rapp et al 2012), theidea is to use diachronic comparable corpora toautomatically produce a dictionary of diachronicspelling variants even including semantic shifts,regardless of the historical variants at stake.In short, we first build a comparable histori-cal corpus made up of recipe repertoires publishedin the German language during the Early ModernAge along with a contemporary comparable cor-pus.
Second, we address the problem of recog-nizing and translating different variants by relay-ing on MT techniques based on string similarity aswell as on semantic similarity measures.
Finally,we automatically extract a diachronic dictionaryof spelling and semantic variants which also pro-vides a canonical form.This paper is organized as follows.
Section2 presents the comparable corpus of Germanrecipes.
Section 3 describes the approach used forgenerating the dictionary of diachronic spellingvariants.
Section 4 shows the results of a prelimi-nary evaluation.
Finally, in Section 5 we concludeby discussing some final remarks.2 The Historical Comparable Corpus ofGerman RecipesThe text collection encoded in our corpus spanstwo hundred years and includes samples from 14cook books written in German between 1569 and1729.
The core of the recipe corpus was compiledas part of a PhD work in the field of TranslationStudies (cf.
(Wurm, 2007)).
This corpus has beenaligned resulting into two comparable corpora:?
a historical comparable dataset alned atrecipe level providing multiple versions ofthe same dish across the time span of the corecorpus;?
a contemporary comparable dataset provid-ing contemporary German versions for eachrecipe.In order to produce the historical comparable com-ponent we proceeded in the following way:?
first, we classified the core recipes by mainingredient and cooking method (e.g.
chicken,roast).
These two parameters form the crite-ria to consider the recipes aligned, then;?
we collected as many as possible diachronicversions/variants of the same recipe by alsosearching online resources providing collec-tions of historical texts.The historical component of the corpus (core andcomparable) contains a total of 430 recipes andabout 45.000 tokens.
This dataset constitutes theobject of study for subsequent research, provid-ing a representative sample of German during theEarly Modern Age in this specific domain.
More-over, language and genre evolution can be tracedthanks to its comparable nature.Regarding the compilation of the contemporaryGerman comparable corpus, we collected a set ofrecipes belonging to the same register but repre-senting contemporary German language.
Theserecipes were collected from Internet sources andfiltered by geographical criteria (only the ones cat-egorized as belonging to the cuisine of Germanspeaking regions were selected).
The corpus con-tains around 1500 recipes and over 500.000 to-kens, which have been also aligned with their85Figure 1: A text excerpt from Wecker 1679.comparable historical counterparts according tothe same parameters explained above.
This sub-set alws not only to compare historical recipeswith their modern versions but also to use them asa reference corpus to extract standard word forms.2.1 Digitization StrategyThe corpus has been manually transcribed.
Thetranscription can be regarded as a diplomatic one,since it tries to preserve as many features of theoriginal as possible.
Some standardization hasbeen performed at punctuation and hyphenationlevel but no spellchecking or word separation hasbeen carried out.
The corpus is encoded in UTF-8and we have used a TEI-compatible XML formatto store both text and metadata.2.2 AnnotationsThe corpus currently includes some shallowsemantic annotation describing text structure(e.g.
recipe, title, and body) and providing abasic classification of recipes based on the mainingredient and recipe type.
The figure 2 belowshows an example of semantic annotation.3 Building a Diachronic Dictionary ofSpelling VariantsOur spelling normalization strategy aims at solv-ing both subproblems discussed in the Introduc-tion.
In order to extract the mapping betweendiachronic variants by also capturing paraphrasesand semantic shifts, we apply two different strate-gies one based on string similarity and the otherbased on semantic similarity measures.Our workflow can be summarized as follows:1.
In a first step, we relay on clustering tech-niques based on string similarity measures<recipe id=?26?
author=?Deckhardt?
year=?1611?language=?german?
ingredient=?Erdbeere?cookingMethod=?Mus?><title> Ein Erdbeermuhs zumachen.
< /title><body> <seg type=?newline?>Nimb Erdbeer</seg><seg type=?newline?/ >treibe es durch mit Weine</seg><seg type=?newline?>thue Zucker darein</seg><seg type=?newline?>darnach man es gerne su?sse haben wil</seg>...< /body>< /recipe>Figure 2: Comparable diachronic corpus: an ex-ample of annotation.to identify a set of diachronic variations ofthe same word form.
In this phase, cluster-ing corresponds to the extraction of ?similarstrings?.2.
In the second step, we address the problem offinding semantic variants, i.e.
those variantsthat are not realized as similar strings by ap-plying paraphrase recognition techniques toidentify different denominations of the sameobject.3.
Finally, we integrate the results of bothphases and generate a dictionary of di-achronic variants, that is used to extract thenormalized spelling for each word in the cor-pus.
We assume that the normalized wordform corresponds to the most modern variantfound in the dictionary.3.1 String SimilarityIn the first step, we extract comparable recipesfrom different decades and from the corpus ofmodern recipes.
Then we apply clustering tech-niques to find spelling variations.
The fact thatwe use comparable texts for clustering, should re-duces the errors as all tokens come from similarterminological fields.We apply agglomerative hierarchical clusteringas implemented in the R statistical programmingenvironment with the average agglomerationmethod.
As a string similarity measure, we usethe standard Levenshtein edit distance as imple-mented in the R package Biostrings.
In order to86build the dictionary, we select clusters that havea string similarity greater than 65%.
Figure 3shows an example of diachronic dictionary entriesgenerated with this approach.Hu?hner: Hu?ner 1574, Hu?nern 1574,hu?ner 1574, Hu?nner 1611und: vnd 1569, vnnd 1569,vnd 1679, und 1698magsts: magst 1574, magstu 1602,magst 1679lasst: lassen 1679, lassets 1682,la?ssets 1715Muscatenblu?h: Muscatblu?
1569, Mus-catenblu?h 1715Figure 3: Diachronic Dictionary.For each list of diachronic variants gathered atthis point, we extracted the most recent variant andused it as normalized form.3.2 Semantic similarityIn order to cluster paraphrastic variants and se-mantic shifts, we apply a slightly modified ver-sion of Lin?s algorithm (Lin, 1998) based on theassumption that words sharing similar contextsshould have similar semantics.
Contrary to Lin, inour approach we do not perform any dependencyanalysis of the corpus data and compute semanticsimilarity between strings simply in terms of themutual information of trigrams.The semantic similarity strategy we imple-mented can be summarized as follows:?
We start by generating a list of trigrams fromthe corpus.?
We assign to each pair of tokens in the corpusa value for their mutual information.?
We assign to each pair of tokens in the corpusa value for their similarity.?
For each token in the corpus, we extract the Nmost similar tokens and take the most modernone as the normalized form.The mutual information I for a pair of tokens t1and t2 is defined as:I(t1, t2) = log ?t1,?,t2???,?,???t1,?,????,?,t2?
, with?
t1, ?, t2 ?
the frequency of the occurrence ofthe trigram t1,*,t2 in the corpus, ?
?, ?, ?
?
thetotal number of trigrams in the corpus, ?
t1, ?, ?
?the number of trigrams with t1 as first token and?
?, ?, t2 ?
the number of trigrams with t2 as lasttoken.Semantic similarity between tokens is definedin terms of their mutual information:sim(t1, t2) =?Tt1?Tt2I(t1,?)+I(t2,?)?I(t1,?)+?I(w2,?
),with Tt1 = {(v, w) : I(t1.w) > 0} and Tt2 ={(v, w) : I(t2.w) > 0}, the sets of token pairsthat form trigrams with t1 or t2 as first elementand such that they have positive mutual informa-tion values.4 EvaluationIn order to evaluate the performance of our nor-malization strategy, we extracted a subset ofrecipes from the corpus for testing purposes.
Thissubcorpus includes 32 comparable recipes on howto roast a chicken that have been written in a timeperiod ranging from 1569 to 1800 reaching a sizeof 7103 words (about 8% of whole corpus).
Wetake as reference the results yielded by TreeTag-ger1 (Schmid, 1994), the state-of-art POS-taggerfor German, regarding lemmatization and POS-tagging.First, we tagged the subcorpus on the non-normalized word forms.
The performance of POS-tagging, in this case, is around 80%, which ishigher than the one observed in similar experi-ments (cf.
(Scheible et al 2011a)) on other his-torical corpora of German.
We believe the reasonfor this is the relative syntactic simplicity of recipetexts in comparison to other kind of texts (dramas,sermons, letters, scientific or legal texts).The tagger?s poor performance is due to the ex-istence of lexical items unknown to the system(around 27%), on the one hand, and the high in-consistency of the spelling, on the other hand.Our strategy to circumvent this problem consistsof providing a modern word form to all histori-cal word variants that we obtained from the pre-viously discussed diachronic dictionary.
We ex-pected, that after the two normalization steps dis-cussed in Section 3, the performance of the tag-ging process should improve.1The TreeTagger was trained on the Tu?Ba-D/Z treebank.Its performance is about 97.4% on newspaper texts and 78%on texts containing unknown words.87Strategy Lemma POSno-norm 73% 80%string-similarity 81% 81.4%semantic similarity 82.5% 82%Table 1: Evaluation Results.Therefore, we repeated the experiment, first, onthe test subcorpus normalized by using the di-achronic dictionary generated with first normaliza-tion strategy, i.e.
the one based on string similaritymeasure and, second, on the normalized versionobtained after using the second strategy based onsemantic similarity.Table 1 summarizes the results of a preliminaryevaluation of our strategy.After string similarity normalization, the taggerwas able to identify all lemmas except for 1358tokens (19% of unknown tokens).
While POS-tagging improved to 81.4%.The semantic similarity step improved the per-formance of lemmatization and POS reaching82.5% and 82% respectively.Despite the fact that our experiments refer tovery few data and to a restricted domain, we be-lieve they are promising and show that our strat-egy, the integration of string similarity and seman-tic similarity measures can lead to a high qualityautomatic spelling normalization and outperformstate-of-art approaches.5 ConclusionIn this paper we have presented a comparablecorpus of historical German recipes and shownthat such comparable resources can help remov-ing sources of noise typical of these text typesthat hinder standard NLP manipulation of suchmaterial.
The old German recipes corpus is, toour knowledge, one of the first attempts2 to builda comparable historical corpus of German.
Thecorpus is accessible through a web interface andallows sophisticated queries according to differ-ent levels of annotation: 1) historical word forms;2) modern normalized forms; 3) lemmas on topof normalized forms; 4) part-of-speech, and, lastbut not least; 5) semantics, namely main ingre-dient and cooking method.
Further, we describean innovative strategy for word form normaliza-2We are aware of only one similar project (Bartsch et al2011) aimed at building a comparable corpus of German textsfor three main periods Old High, Middle High and Early NewHigh German.
However, those corpora are not yet available.tion that integrate string similarity measure withsemantic similarity thereby being able to cope notonly with formal spelling variations but also withparaphrastic variations and semantic shift.
More-over, this method can be applied to any compara-ble diachronic corpus, regardless of the time spanat stake.
A preliminary evaluation has shown thatsuch a strategy may outperform state-of-art ap-proaches.ReferencesNina Bartsch, Stefanie Dipper, Birgit Herbers, SarahKwekkeboom, Klaus-Peter Wegera, Lars Eschke,Thomas Klein, and Elke Weber.
2011.
An-notiertes Referenzkorpus Mittelhochdeutsch (1050-1350).
Poster session at the 33rd annual meeting ofthe German Linguistic Society (DGfS-2011) (Ab-stract, Poster) .Marcel Bollmann, Florian Petran, and Stefanie Dip-per.
2011.
Applying Rule-Based Normalization toDifferent Types of Historical Texts ?
An Evalua-tion.
In Proceedings of the 5th Language & Tech-nology Conference: Human Language Technologiesas a Challenge for Computer Science and Linguis-tics, Poznan?, November.Marcel Diel, Bernhard Fisseni, Winfried Lenders,and Hans-Christian Schmitz.
2002.
XML-Kodierung des Bonner Fru?hneuhochdeutschkorpus.IKP-Arbeitsbericht NF 02, Bonn .Stefanie Dipper.
2010.
Pos-tagging of historicallanguage data: First experiments.
In SemanticApproaches in Natural Language Processing.
Pro-ceedings of the 10th Conference on Natural Lan-guage Processing (KONVENS-10), pages 117?121,Saarbru?cken.Rafael Giusti, Arnaldo Candido Jr, Marcelo Muniz,L?
?via Cucatto, and Sandra Alu??sio.
2007.
Auto-matic Detection of Spelling Variation in HistoricalCorpus : An Application to Build a Brazilian Por-tuguese Spelling Variants Dictionary.
In Proceed-ings of the Corpus Linguistics Conference, pages 1?20.A.
Gotscharek, U. Reffle, C. Ringlstetter, K. U. Schulz,and A. Neumann.
2011.
Towards information re-trieval on historical document collections: the roleof matching procedures and special lexica.
IJDAR,14(2):159?171.Iris Hendrickx and Rita Marquilhas.
2011.
From oldtexts to modern spellings: an experiment in auto-matic normalisation.
Journal for Language Technol-ogy and Computational Linguistics, 26(2):65?76.Tim Hitchcock, Robert Shoemaker, Clive Emsley,Sharon Howard, and Jamie McLaughliin.
2012.The Old Bailey Proceedings Online, 1674-1913(version 7.0).Magnus Huber.
2007.
The Old Bailey Proceed-ings, 1674-1834.
Evaluating and annotating a cor-pus of 18th- and 19th-century spoken English.
In88Meurman-Solin.
Anneli and Arja Nurmi, editors,Annotating Variation and Change, volume 1.
Re-search Unit for Variation, Contacts and Changein English (VARIENG), University of Helsinki,Helsinki.Bryan Jurish.
2008.
Finding canonical formsfor historical German text.
In Angelika Storrer,Alexander Geyken, Alexander Siebert, and Kay-Michael Wu?rzner, editors, Text Resources and Lex-ical Knowledge.
Selected Papers from the 9th Con-ference on Natural Language Processing KONVENS2008, pages 27?38.
Mouton de Gruyter, Berlin /New York.Marijn Koolen, Frans Adriaans, Jaap Kamps, andMaarten de Rijke.
2006.
A cross-language ap-proach to historic document retrieval.
In MouniaLalmas, Andy MacFarlane, Stefan Rueger, Anasta-sios Tombros, Theodora Tsikrika, Alexei Yavlinsky,editor, Advances in Information Retrieval, volume3936, pages 407?419.
Lecture Notes in ComputerScience, Berlin/Heidelberg: Springer.Pierre Kunstmann and Achum Stein.
2007.
LeNouveau Corpus d?Amsterdam.
In Pierre Kunst-mann Achim Stein, editor, Le Nouveau Corpusd?Amsterdam.
Actes de l?atelier de Lauterbad, 23-26 fe?vrier 2006, pages 9?27.
Stuttgart, Germany:Steiner.Vladimir I. Levenshtein.
1965.
Binary codes capa-ble of correcting deletions, insertions, and reversals.Doklady Akademii Nauk SSSR, 163(4):845?848.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.Thomas Pilz, Andrea Ernst-Gerlach, Sebastian Kemp-ken, Paul Rayson, and Dawn Archer.
2008.
TheIdentification of Spelling Variants in English andGerman Historical Texts: Manual or Automatic?Literary and Linguistic Computing, 23(1):65?72,April.Michael Piotrowski.
2012.
Natural Language Pro-cessing for Historical Texts, volume 5.
Morgan &Claypool Publishers, September.Reinhard Rapp, Serge Sharoff, and Bogdan Babych.2012.
Identifying word translations from compa-rable documents without a seed lexicon.
In LREC,pages 460?466.Silke Scheible, Richard J. Whitt, Martin Durrell, andPaul Bennett.
2011a.
Evaluating an f-the-shelfS-tagger on Early Modern German text.
In Proceed-ings of the 5th ACL-HLT Workshop on LanguageTechnology for Cultural Heritage, Social Sciences,and Humanities, number June, pages 19?23, Port-land, Oregon.
Association for Computational Lin-guistics.Silke Scheible, Richard J. Whitt, Martin Durrell, andPaul Bennett.
2011b.
A gold standard corpusof early modern german.
In Linguistic AnnotationWorkshop, pages 124?128.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofInternational Conference on New Methods in Lan-guage Processing, Manchester, UK.Andrea Wurm.
2007.
TranslatorischeWirkung: ein Beitrag zum Versta?ndnis vonU?bersetzungsgeschichte als Kulturgeschichte amBeispiel deutscher U?bersetzungen franzo?sischerKochbu?cher in der Fru?hen Neuzeit.
Ph.D. thesis,Universita?t des Saarlandes, Saarbru?cken.89
