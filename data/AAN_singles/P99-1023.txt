A Second-Order Hidden Markov Model for Part-of-SpeechTaggingScot t  M.  Thede and Mary  P. HarperSchool of Electrical and Computer  Engineering, Purdue UniversityWest Lafayette, IN 47907{ thede, harper} @ecn.purdue.eduAbst rac tThis paper describes an extension to the hiddenMarkov model for part-of-speech tagging usingsecond-order approximations for both contex-tual and lexical probabilities.
This model in-creases the accuracy of the tagger to state ofthe art levels.
These approximations make useof more contextual information than standardstatistical systems.
New methods of smoothingthe estimated probabilities are also introducedto address the sparse data problem.1 I n t roduct ionPart-of-speech tagging is the act of assigningeach word in a sentence a tag that describeshow that word is used in the sentence.
Typ-ically, these tags indicate syntactic ategories,such as noun or verb, and occasionally includeadditional feature information, such as number(singular or plural) and verb tense.
The PennTreebank documentation (Marcus et al, 1993)defines a commonly used set of tags.Part-of-speech tagging is an important re-search topic in Natural Language Processing(NLP).
Taggers are often preprocessors in NLPsystems, making accurate performance spe-cially important.
Much research as been doneto improve tagging accuracy using several dif-ferent models and methods, including: hiddenMarkov models (HMMs) (Kupiec, 1992), (Char-niak et al, 1993); rule-based systems (Brill,1994), (Brill, 1995); memory-based systems(Daelemans et al, 1996); maximum-entropysystems (Ratnaparkhi, 1996); path voting con-straint systems (Tiir and Oflazer, 1998); linearseparator systems (Roth and Zelenko, 1998);and majority voting systems (van Halteren etal., 1998).This paper describes various modificationsto an HMM tagger that improve the perfor-mance to an accuracy comparable to or betterthan the best current single classifier taggers.175This improvement comes from using second-order approximations of the Markov assump-tions.
Section 2 discusses a basic first-orderhidden Markov model for part-of-speech taggingand extensions to that model to handle out-of-lexicon words.
The new second-order HMM isdescribed in Section 3, and Section 4 presentsexperimental results and conclusions.2 H idden Markov  Mode lsA hidden Markov model (HMM) is a statisticalconstruct that can be used to solve classificationproblems that have an inherent state sequencerepresentation.
The model can be visualizedas an interlocking set of states.
These statesare connected by a set of transition probabili-ties, which indicate the probability of travelingbetween two given states.
A process begins insome state, then at discrete time intervals, theprocess "moves" to a new state as dictated bythe transition probabilities.
In an HMM, theexact sequence of states that the process gener-ates is unknown (i.e., hidden).
As the processenters each state, one of a set of output symbolsis emitted by the process.
Exactly which symbolis emitted is determined by a probability distri-bution that is specific to each state.
The outputof the HMM is a sequence of output symbols.2.1 Basic Def in i t ions and Notat ionAccording to (Rabiner, 1989), there are five el-ements needed to define an HMM:1.
N, the number of distinct states in themodel.
For part-of-speech tagging, N isthe number of tags that can be used by thesystem.
Each possible tag for the systemcorresponds to one state of the HMM.2.
M, the number of distinct output symbolsin the alphabet of the HMM.
For part-of-speech tagging, M is the number of wordsin the lexicon of the system.3.
A = {a/j}, the state transition probabil-ity distribution.
The probability aij is theprobability that the process will move fromstate i to state j in one transition.
Forpart-of-speech tagging, the states representthe tags, so aij is the probability that themodel will move from tag ti to tj - -  in otherwords, the probability that tag tj followsti.
This probability can be estimated usingdata from a training corpus.4.
B = {bj(k)), the observation symbol prob-ability distribution.
The probability bj(k)is the probability that the k-th output sym-bol will be emitted when the model is instate j .
For part-of-speech tagging, this isthe probability that the word Wk will beemitted when the system is at tag tj (i.e.,P(wkltj)).
This probability can be esti-mated using data from a training corpus.5.
7r = {Tri}, the initial state distribution.
7riis the probability that the model will startin state i.
For part-of-speech tagging, thisis the probability that the sentence will be-gin with tag ti.When using an HMM to perform part-of-speech tagging, the goal is to determine themost likely sequence of tags (states) that gen-erates the words in the sentence (sequence ofoutput symbols).
In other words, given a sen-tence V, calculate the sequence U of tags thatmaximizes P(VIU ).
The Viterbi algorithm is acommon method for calculating the most likelytag sequence when using an HMM.
This algo-rithm is explained in detail by Rabiner (1989)and will not be repeated here.2.2 Calculating Probabil i t ies forUnknown WordsIn a standard HMM, when a word does notoccur in the training data, the emit probabil-ity for the unknown word is 0.0 in the B ma-trix (i.e., bj(k) = 0.0 if wk is unknown).
Be-ing able to accurately tag unknown words isimportant, as they are frequently encounteredwhen tagging sentences in applications.
Mostwork in the area of unknown words and taggingdeals with predicting part-of-speech informa-tion based on word endings and affixation infor-mation, as shown by work in (Mikheev, 1996),(Mikheev, 1997), (Weischedel et al, 1993), and(Thede, 1998).
This section highlights a methoddevised for HMMs, which differs slightly fromprevious approaches.To create an HMM to accurately tagunknown words, it is necessary to deter-mine an estimate of the probability P(wklti)for use in the tagger.
The probabil-ity P(word contains jl tag is ti) is estimated,where sj is some "suffix" (a more appropri-ate term would be word ending, since the sj'sare not necessarily morphologically significant,but this terminology is unwieldy).
This newprobability is stored in a matrix C = {cj(k)),where cj(k) = P(word has suffix ski tag is tj),replaces bj(k) in the HMM calculations for un-known words.
This probability can be esti-mated by collecting suffix information from eachword in the training corpus.In this work, suffixes of length one to fourcharacters are considered, up to a maximum suf-fix length of two characters less than the lengthof the given word.
An overall count of the num-ber of times each suffix/tag pair appears in thetraining corpus is used to estimate mit prob-abilities for words based on their suffixes, withsome exceptions.
When estimating suffix prob-abilities, words with length four or less are notlikely to contain any word-ending informationthat is valuable for classification, so they areignored.
Unknown words are presumed to beopen-class, so words that are not tagged withan open-class tag are also ignored.When constructing our suffix predictor,words that contain hyphens, are capitalized, orcontain numeric digits are separated from themain calculations.
Estimates for each of thesecategories are calculated separately.
For ex-ample, if an unknown word is capitalized, theprobability distribution estimated from capital-ized words is used to predict its part of speech.However, capitalized words at the beginningof a sentence are not classified in this way--the initial capitalization is ignored.
If a wordis not  capitalized and does not contain a hy-phen or numeric digit, the general distributionis used.
Finally, when predicting the possiblepart of speech for an unknown word, all possiblematching suffixes are used with their predictionssmoothed (see Section 3.2).3 The  Second-Order  Mode l  forPar t -o f -Speech  Tagg ingThe model described in Section 2 is an exam-ple of a first-order hidden Markov model.
Inpart-of-speech tagging, it is called a bigram tag-ger.
This model works reasonably well in part-of-speech tagging, but captures a more limited176amount of the contextual information than isavailable.
Most of the best statistical taggersuse a tr igram model, which replaces the bigramtransition probability aij = P( rp  = tjITp_ 1 -~ti) with a trigram probability aijk : P(7"p =tk lrp_l  = t j ,  rp-2 = ti).
This section describesa new type of tagger that uses trigrams not onlyfor the context probabilities but also for the lex-ical (and suffix) probabilities.
We refer to thisnew model as a fu l l  second-order hidden Markovmodel.3.1 Defining New Probabi l i tyDistr ibut ionsThe full second-order HMM uses a notationsimilar to a standard first-order model for theprobability distributions.
The A matrix con-tains state transition probabilities, the B matrixcontains output symbol distributions, and theC matrix contains unknown word distributions.The rr matrix is identical to its counterpart inthe first-order model.
However, the definitionsof A, B, and C are modified to enable the fullsecond-order HMM to use more contextual in-formation to model part-of-speech tagging.
Inthe following sections, there are assumed to beP words in the sentence with rp and Vp being thep-th tag and word in the sentence, respectively.3.1.1 Contextual  Probabi l i t iesThe A matrix defines the contextual probabil-ities for the part-of-speech tagger.
As in thetrigram model, instead of limiting the contextto a first-order approximation, the A matrix isdefined as follows:A = {a i jk ) ,  where"ai ja= P(rp = tklrp_l = tj, rp-2 = tl), 1 < p < PThus, the transition matrix is now three dimen-sional, and the probability of transitioning toa new state depends not only on the currentstate, but also on the previous tate.
This al-lows a more realistic ontext-dependence for theword tags.
For the boundary cases of p = 1 andp = 2, the special tag symbols NONE and SOSare used.3.1.2 Lexieal and Suffix Probabi l i t iesThe B matrix defines the lexical probabilitiesfor the part-of-speech tagger, while the C ma-trix is used for unknown words.
Similarly to thetrigram extension to the A matrix, the approx-imation for the lexical and suffix probabilitiescan also be modified to include second-order in-formation as follows:B = {bi j(k)) and C = {vii(k)}, where==P(vp  = wklrp = rp -1  = t i )P(vp has suffix sklrp = tj,  rp-1 = tl)fo r l<p<PIn these equations, the probability of the modelemitting a given word depends not only on thecurrent state but also on the previous tate.
Toour knowledge, this approach as not been usedin tagging.
SOS is again used in the p = 1 case.3.2 Smooth ing IssuesWhile the full second-order HMM is a more pre-cise approximation of the underlying probabil-ities for the model, a problem can arise fromsparseness of data, especially with lexical esti-mations.
For example, the size of the B ma-trix is T2W,  which for the WSJ corpus is ap-proximately 125,000,000 possible tag/tag/wordcombinations.
In an attempt o avoid sparsedata estimation problems, the probability esti-mates for each distribution is smoothed.
Thereare several methods of smoothing discussed inthe literature.
These methods include the ad-ditive method (discussed by (Gale and Church,1994)); the Good-Turing method (Good, 1953);the Jelinek-Mercer method (Jelinek and Mercer,1980); and the Katz method (Katz, 1987).These methods are all useful smoothing al-gorithms for a variety of applications.
However,they are not appropriate for our purposes.
Sincewe are smoothing trigram probabilities, the ad-ditive and Good-Turing methods are of limitedusefulness, ince neither takes into account bi-gram or unigram probabilities.
Katz smooth-ing seems a little too granular to be effective inour application--the broad spectrum of possi-bilities is reduced to three options, dependingon the number of times the given event occurs.It seems that smoothing should be based on afunction of the number of occurances.
Jelinek-Mercer accommodates this by smoothing then-gram probabilities using differing coefficients(A's) according to the number of times each n-gram occurs, but this requires holding out train-ing data for the A's.
We have implemented amodel that smooths with lower order informa-tion by using coefficients calculated from thenumber of occurances of each trigram, bigram,and unigram without training.
This method isexplained in the following sections.3.2.1 State Transit ion Probabi l i t iesTo estimate the state transition probabilities,we want to use the most specific information.177However, that information may not always beavailable.
Rather than using a fixed smooth-ing technique, we have developed a new methodthat uses variable weighting.
This method at-taches more weight to triples that occur moreoften.Thetklrp-1P=kaformula for the estimate /3 of P(rp == tj, rp-2 = tl) is:Na + (1 - ka)k2 N2 + (1 - k3)(1 - -  k2).
N:c ,  Yoowhich depends on the following numbers:g l  =N2 --~N3 =Co =C:  - -Co =number of times tk occursnumber of times sequence tjta occursnumber of times sequence titjtk occurstotal number of tags that appearnumber of times tj occursnumber of times sequence titj occurswhere:log(N2 + 1) + 1k~.
= log(Ng.
+ 1) + 2'log(Na + I) + 1and ka = log(Na + 1) + 2The formulas for k2 and k3 are chosen so thatthe weighting for each element in the equationfor/3 changes based on how often that elementoccurs in the training data.
Notice that thesum of the coefficients of the probabilities in theequation for/3 sum to one.
This guarantees thatthe value returned for /3  is a valid probability.After this value is calculated for all tag triples,the values are normalized so that ~ /3 -- 1,tkETcreating a valid probability distribution.The value of this smoothing technique be-comes clear when the triple in question occursvery infrequently, if at all.
Consider calculating/3 for the tag triple CD RB VB.
The informa-tion for this triple is:N1 = 33,277 (number of times VB appears)N2 = 4,335 (number of times RB VB appears)Na = 0 (number of times CD RB VB appears)Co = 1,056,892 (total number of tags)C: = 46,994 (number of times RB appears)C2 = 160 (number of times CD RB appears)Using these values, we calculate the coeffi-cients k2 and k3:log(4,335 + 1) + 1 4.637 k2 = - - - -0 .823log(4,335 + 1) + 2 5.637ka = log(0+l )+ l  =-1 =0.500log(0 + 1) + 2 2Using these values, we calculate the probability/3:15 = k3 ?
~-~-N3 q_ (1 - ka)k2 ?
-~lN?
q_ (1 - k3)(1 - k2) .
NxC.._o= 0.500 ?
0.000 Jr 0.412 ?
0.092 + 0.088 ?
0.031= 0.041If smoothing were not applied, the probabil-ity would have been 0.000, which would createproblems for tagger generalization.
Smoothingallows tag triples that were not encountered inthe training data to be assigned a probability ofoccurance.3.2.2 Lexical  and Suff ix Probabi l i t iesFor the lexical and suffix probabilities, we dosomething somewhat different han for contextprobabilities.
Initial experiments that used aformula similar to that used for the contextualestimates performed poorly.
This poor perfor-mance was traced to the fact that smoothing al-lowed too many words to be incorrectly taggedwith tags that did not occur with that word inthe training data (over-generalization).
As analternative, we calculated the smoothed proba-bil ity/3 for words as follows:(log(N3 + i) + i. N3 1 N2t5 __ "log(N3 + 1) + 2)C-22 + (log(N3 + 1) + 2)C-Twhere:N2 = number of times word wk occurs withtag tjN3 = number of times word wk occurs withtag tj preceded by tag tlC1 = number of times tj occursC2 = number of times sequence titj occursNotice that this method assigns a probabilityof 0.0 to a word/tag pair that does not appearin the training data.
This prevents the taggerfrom trying every possible combination of wordand tag, something which both increases run-ning time and decreases the accuracy.
We be-lieve the low accuracy of the original smoothingscheme emerges from the fact that smoothingthe lexical probabilities too far allows the con-textual information to dominate at the expenseof the lexical information.
A better smooth-ing approach for lexical information could pos-sibly be created by using some sort of word classidea, such as the genotype idea used in (Tzouk-ermann and Radev, 1996), to improve our /5estimate.178In addition to choosing the above approachfor smoothing the C matrix for unknown words,there is an additional issue of choosing whichsuffix to use when predicting the part of speech.There are many possible answers, some of whichare considered by (Thede, 1998): use the longestmatching suffix, use an entropy measure to de-termine the "best" affix to use, or use an av-erage.
A voting technique for c i j (k )  was deter-mined that is similar to that used for contextualsmoothing but is based on different length suf-fixes.Let s4 be the length four suffix of the givenword.
Define s3, s2, and sl to be the lengththree, two, and one suffixes respectively.
If thelength of the word is six or more, these four suf-fixes are used.
Otherwise, suffixes up to lengthn - 2 are used, where n is the length of theword.
Determine the longest suffix of these thatmatches a suffix in the training data, and cal-culate the new smoothed probability:~ / (gk )e~, (sk )  + (1 - -  f(Y*))P~j(sk-,),  1 < k < 4where:log(~+l/+l?
/ (x )  = log( +lj+2?
Ark = the number of times the suffix sk oc-curs in the training data.?
~ i j (Sk)  - -  the estimate of Cij(8k) from theprevious lexical smoothing.After calculating/5, it is normalized.
Thus, suf-fixes of length four are given the most weight,and a suffix receives more weight he more timesit appears.
Information provided by suffixes oflength one to four are used in estimating theprobabilities, however.3.3 The  New V i te rb i  A lgor i thmModification of the lexical and contextualprobabilities is only the first step in defininga full second-order HMM.
These probabilitiesmust also be combined to select the most likelysequence of tags that generated the sentence.This requires modification of the Viterbi algo-rithm.
First, the variables ~ and ?
from (Ra-biner, 1989) are redefined, as shown in Figure1.
These new definitions take into account headded dependencies of the distributions of A,B, and C. We can then calculate the mostlikely tag sequence using the modification of theViterbi algorithm shown in Figure 1.
The run-ning time of this algorithm is O (NT3), where Nis the length of the sentence, and T is the num-ber of tags.
This is asymptotically equivalent tothe running time of a standard trigram taggerthat maximizes the probability of the entire tagsequence.4 Exper iment  and  Conc lus ionsThe new tagging model is tested in severaldifferent ways.
The basic experimental tech-nique is a 10-fold cross validation.
The corpusin question-is randomly split into ten sectionswith nine of the sections combined to train thetagger and the tenth for testing.
The results ofthe ten possible training/testing combinationsare merged to give an overall accuracy mea-sure.
The tagger was tested on two corpora--the Brown corpus (from the Treebank II CD-ROM (Marcus et al, 1993)) and the Wall StreetJournal corpus (from the same source).
Com-paring results for taggers can be difficult, es-pecially across different researchers.
Care hasbeen taken in this paper that, when comparingtwo systems, the comparisons are from experi-ments that were as similar as possible and thatdifferences are highlighted in the comparison.First, we compare the results on each corpusof four different versions of our HMM tagger: astandard (bigram) HMM tagger, an HMM us-ing second-order lexical probabilities, an HMMusing second-order contextual probabilities (astandard trigram tagger), and a full second-order HMM tagger.
The results from both cor-pora for each tagger are given in Table 1.
Asmight be expected, the full second-order HMMhad the highest accuracy levels.
The model us-ing only second-order contextual information (astandard trigram model) was second best, themodel using only second-order lexical informa-tion was third, and the standard bigram HMMhad the lowest accuracies.
The full second-order HMM reduced the number of errors onknown words by around 16% over bigram tag-gers (raising the accuracy about 0.6-0.7%), andby around 6% over conventional trigram tag-gets (accuracy increase of about 0.2%).
Similarresults were seen in the overall accuracies.
Un-known word accuracy rates were increased byaround 2-3% over bigrams.The full second-order HMM tagger is alsocompared to other researcher's taggers in Ta-ble 2.
It is important to note that both SNOW,a linear separator model (Roth and Zelenko,179THE SECOND-ORDER VITERBI ALGORITHMThe variables:?
gp(i,j)= max P(rl, .
.
.
,rp-2, rp-1 =ti, rp=t j ,v l , .
.
.
vp) ,2<p<PTl ~...rTp--2?
Cp(i,j) = arg max P(rl, .
.
.
,rp-2, rp-1 = ti,rp = t j ,vl , .
.
.vp),2 < p < PTl~...iTp--2The procedure:1.
6,(i,j) = { ~ribij(vl), i fv l isknown }?ricij (Vl) , if vl is unknown ,1 _< i, j < N?l(i, j) = O, 1 < i, j  < N{ lma<xN\[Jp-l(i,j)aljk\]bjk(vp), if vp is known }2.
~p(j, k) = m~xN\[Jp_~(i,j)ai~k\]c~k(v,), if vp is unknown ,1 < i,j, k < N, 2 < p < PCp (j, k) = arg l~_ia<_Xg\[Sp_l (i, j)aijk\], 1 < i, j, k < N, 2 g p < P3.
P* = max 6p(i,j) l <i,j<_Nrt~ = argj max 6p(i,j) l <i,j<Nr\],_ 1 = arg i max Jp(i,j)l<_i,j<N4.
r; = Cp+l (r~+l, r;+2),p = P-2 ,  P -3 , .
.
.
,2 ,1Figure 1: Second-Order Viterbi AlgorithmComparison on BrownTagger Type KnownStandard Bigram 95.94%Second-Order Lexical only 96.23%Second-Order Contextual only 96.41%Full Second-Order HMM 96.62%CorpusUnknown Overall80.61% 95.60%81.42% 95.90%82.69% 96.11%83.46% 96.33%Comparison on WSJ  CorpusTagger Type Known UnknownStandard Bigram 96.52% 82.40%Second-Order Lexical only 96.80% 83.63%Second-Order Contextual only 96.90% 84.10%Full Second-Order HMM 97.09% 84.88%Overall96.25%96.54%96.65%96.86%% Error  Reduct ion of Second-Order HMMSystem Type Compared Brown WSJBigram 16.6% 16.3%Lexical Trigrams Only 10.5% 9.2%Contextual Trigrams Only 5.7% 6.3%Table 1: Comparison between Taggers on the Brown and WSJ Corpora1998), and the voting constraint agger (Tiirand Oflazer, 1998) used training data that con-tained full lexical information (i.e., no unknownwords), as well as training and testing data thatdid not cover the entire WSJ corpus.
This use ofa full lexicon may have increased their accuracybeyond what it would have been if the modelwere tested with unknown words.
The stan-dard trigram tagger data is from (Weischedel etal., 1993).
The MBT (Daelemans et al, 1996)180Tagger TypeStandard Trigram(Weischedel t al., 1993)MBT(Daelemans et al, 1996)Rule-based(Brill, 1994)Maximum-Entropy(Ratnaparkhi, 1996)Full Second-Order HMMSNOW(Roth and Zelenko, 1998)Voting Constraints(Tiir and Oflazer, 1998)Full Second-Order HMMKnown Unknown OverallOpen/C losedLexicon?96.7% 85.0% 96.3% open96.7% 90.6% 2 96.4% open82.2% 96.6% open97.1%97.2%85.6%84.9%97.5%96.6%96.9%98.05%openopenclosedclosedclosedTestingMethodfull WSJ 1fixed WSJcross-validationfixedfull WSJ 3fixedfull WSJ 3full WSJcross-validationfixed subsetof WSJ 4subset of WSJcross-validation 5full WSJcross-validationTable 2: Comparison between Full Second-Order HMM and Other Taggersdid not include numbers in the lexicon, whichaccounts for the inflated accuracy on unknownwords.
Table 2 compares the accuracies of thetaggers on known words, unknown words, andoverall accuracy.
The table also contains twoadditional pieces of information.
The first indi-cates if the corresponding tagger was tested us-ing a closed lexicon (one in which all words ap-pearing in the testing data are known to the tag-ger) or an open lexicon (not all words are knownto.the system).
The second indicates whether ahold-out method (such as cross-validation) wasused, and whether the tagger was tested on theentire WSJ corpus or a reduced corpus.Two cross-validation tests with the fullsecond-order HMM were run: the first with anopen lexicon (created from the training data),and the second where the entire WSJ lexiconwas used for each test set.
These two tests al-low more direct comparisons between our sys-tem and the others.
As shown in the table, thefull second-order HMM has improved overall ac-curacies on the WSJ corpus to state-of-the-art1The full WSJ is used, but the paper does not indicatewhether a cross-vaiidation was performed.2MBT did not place numbers in the lexicon, so allnumbers were treated as unknown words.aBoth the rule-based and maximum-entropy modelsuse the full WSJ for training/testing with only a singletest set.4SNOW used a fixed subset of WSJ for training andtesting with no cross-validation.5The voting constraints tagger used a subset of WSJfor training and testing with cross-validation.levels--96.9% is the greatest accuracy reportedon the full WSJ  for an experiment using anopen lexicon.
Finally, using a closed lexicon, thefull second-order HMM achieved an accuracy of98.05%, the highest reported for the WSJ cor-pus for this type of experiment.The accuracy of our system on unknownwords is 84.9%.
This accuracy was achieved bycreating separate classifiers for capitalized, hy-phenated, and numeric digit words: tests on theWall Street Journal corpus with the full second-order HMM show that the accuracy rate on un-known words without separating these types ofwords is only 80.2%.
6 This is below the perfor-mance of our bigram tagger that separates theclassifiers.
Unfortunately, unknown word accu-racy is still below some of the other systems.This may be due in part to experimental dif-ferences.
It should also be noted that some ofthese other systems use hand-crafted rules forunknown word rules, whereas our system usesonly statistical data.
Adding additional rulesto our system could result in comparable per-formance.
Improving our model on unknownwords is a major focus of future research.In conclusion, a new statistical model, the fullsecond-order HMM, has been shown to improvepart-of-speech tagging accuracies over currentmodels.
This model makes use of second-orderapproximations for a hidden Markov model and8Mikheev (1997) also separates uffix probabilitiesinto different estimates, but fails to provide any dataillustrating the implied accuracy increase.181improves the state of the art for taggers with noincrease in asymptotic running time over tra-ditional trigram taggers based on the hiddenMarkov model.
A new smoothing method is alsoexplained, which allows the use of second-orderstatistics while avoiding sparse data problems.ReferencesEric Brill.
1994.
A report of recent progressin transformation-based rror-driven learn-ing.
Proceedings of the Twelfth National Con-ference on Artifical Intelligence, pages 722-727.Eric Brill.
1995.
Transformation-based rror-driven learning and natural anguage process-ing: A case study in part of speech tagging.Computational Linguistics, 21(4):543-565.Eugene Charniak, Curtis Hendrickson, Neil Ja-cobson, and Mike Perkowitz.
1993.
Equa-tions for part-of-speech tagging.
Proceedingsof the Eleventh National Conference on Arti-ficial Intelligence, pages 784-789.Walter Daelemans, Jakub Zavrel, Peter Berck,and Steven Gillis.
1996.
MBT: A memory-based part of speech tagger-generator.
Pro-ceedings of the Fourth Workshop on VeryLarge Corpora, pages 14-27.William A. Gale and Kenneth W. Church.
1994.What's wrong with adding one?
In Corpus-Based Research into Language.
Rodolpi, Am-sterdam.I.
J.
Good.
1953.
The population frequenciesof species and the estimation of populationparameters.
Biometrika, 40:237-264.Frederick Jelinek and Robert L. Mercer.
1980.Interpolated estimation of markov source pa-rameters from sparse data.
Proceedings of theWorkshop on Pattern Recognition in Prac-tice.Salva M. Katz.
1987.
Estimation of probabili-ties from sparse data for the language modelcomponent of a speech recognizer.
IEEETransactions on Acoustics, Speech and SignalProcessing, 35 (3) :400-401.Julian Kupiec.
1992.
Robust part-of-speechtagging using a hidden Markov model.
Com-puter Speech and Language, 6(3):225-242.Mitchell Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313-330.Andrei Mikheev.
1996.
Unsupervised learningof word-category guessing rules.
Proceedingsof the 34th Annual Meeting of the Associationfor Compuatational Linguistics, pages 327-334.Andrei Mikheev.
1997.
Automatic rule induc-tion for unknown-word guessing.
Computa-tional Linguistics, 23 (3) :405-423.Lawrence R. Rabiner.
1989.
A tutorial onhidden Markov models and selected applica-tions in speech recognition.
Proceeding of theIEEE, pages 257-286.Adwait Ratnaparkhi.
1996.
A maximum en-tropy model for part-of-speech tagging.
Pro-ceedings of the Conference on EmpiricalMethods in Natural Language Processing,pages 133-142.Dan Roth and Dmitry Zelenko.
1998.
Part ofspeech tagging using a network of linear sep-arators.
Proceedings of COLING-ACL '98,pages 1136-1142.Scott M. Thede.
1998.
Predicting part-of-speech information about unknown wordsusing statistical methods.
Proceedings ofCOLING-ACL '98, pages 1505-1507.GSkhan Tiir and Kemal Oflazer.
1998.
TaggingEnglish by path voting constraints.
Proceed-ings of COLING-ACL '98, pages 1277-1281.Evelyne Tzoukermann and Dragomir R. Radev.1996.
Using word class for part-of-speechdisambiguation.
Proceedings of the FourthWorkshop on Very Large Corpora, pages 1-13.Hans van Halteren, Jakub Zavrel, and Wal-ter Daelemans.
1998.
Improving data drivenwordclass tagging by system combination.Proceedings of COLING-A CL '98, pages 491-497.Ralph Weischedel, Marie Meeter, RichardSchwartz, Lance Ramshaw, and Jeff Pal-mucci.
1993.
Coping with ambiguity andunknown words through probabilitic models.Computational Linguistics, 19:359-382.182
