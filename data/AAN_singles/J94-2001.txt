Tagging English Text with a ProbabilisticModelBernard  Mer ia ldo  "tInstitut EURECOMIn this paper we present some xperiments on the use of a probabilistic model to tag English text,i.e.
to assign to each word the correct ag (part of speech) in the context of the sentence.
The mainnovelty of these xperiments i  the use of untagged text in the training of the model.
We haveused a simple triclass Marlcov model and are looking for the best way to estimate the parametersof this model, depending on the kind and amount of training data provided.
Two approaches inparticular are compared and combined:?
using text that has been tagged by hand and computing relative frequency counts,?
using text without ags and training the model as a hidden Markov process,according to a Maximum Likelihood principle.Experiments show that the best raining is obtained by using as much tagged text as possible.
Theyalso show that Maximum Likelihood training, the procedure that is routinely used to estimatehidden Markov models parameters from training data, will not necessarily improve the taggingaccuracy.
In fact, it will generally degrade this accuracy, except when only a limited amount ofhand-tagged text is available.1.
IntroductionA lot of effort has been devoted in the past to the problem of tagging text, i.e.
assigningto each word the correct ag (part of speech) in the context of the sentence.
Two mainapproaches have generally been considered:rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen andMartin 1992; Brill et al 1990)probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, andCarstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983;Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988;Marcken 1990; Merialdo 1991; Cutting et al 1992).More recently, some work has been proposed using neural networks (Benello,Mackie, and Anderson 1989; Nakamura nd Shikano 1989).Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.P.
193, 06904Valbonne Cedex France; merialdo@eurecom.fr.t This work was carried out while the author was a visitor of the Continuous Speech Recognition group,IBM T. J. Watson Research Center, Yorktown Heights, NY (USA).
Part of the material included in thiswork has been presented atthe IEEE International Conference on Acoustics, Speech and Signal Processing,Toronto (Canada), May 1991.f~) 1994 Association for Computational LinguisticsComputational Linguistics Volume 20, Number 2Through these different approaches, ome common points have emerged:For any given word, only a few tags are possible, a list of which can befound either in the dictionary or through a morphological nalysis of theword.When a word has several possible tags, the correct ag can generally bechosen from the local context, using contextual rules that define the validsequences of tags.
These rules may be given priorities o that a selectioncan be made even when several rules apply.These kinds of considerations fit nicely inside a probabilistic formulation of theproblem (Beale 1985; Garside and Leech 1985), which offers the following advantages:?
a sound theoretical framework is provided?
the approximations are clear?
the probabilities provide a straightforward way to disambiguate?
the probabilities can be estimated automatically from data.In this paper we present a particular probabilistic model, the triclass model, andresults from experiments involving different ways to estimate its parameters, with theintention of maximizing the ability of the model to tag text accurately.
In particular,we are interested in a way to make the best use of untagged text in the training of themodel.2.
The Problem of TaggingWe suppose that the user has defined a set of tags (attached to words).
Consider asentence W = WlW2...  w,,  and a sequence of tags T -- h t2 .
.
,  t,, of the same length.We call the pair (W, T) an alignment.
We say that word wi has been assigned the tag tiin this alignment.We assume that the tags have some linguistic meaning for the user, so that amongall possible alignments for a sentence there is a single one that is correct from agrammatical point of view.A tagging procedure isa procedure ~that selects asequence of tags (and so definesan alignment) for each sentence.~: W ~ T = ~(W)There are (at least) two measures for the quality of a tagging procedure:?
at sentence l velperfs(~) -- percentage of sentences correctly tagged?
at word levelperfw(~) = percentage of words correctly tagged156Bernard Merialdo Tagging English Text with a Probabilistic ModelIn practice, performance atsentence l vel is generally lower than performance atwordlevel, since all the words have to be tagged correctly for the sentence to be taggedcorrectly.The standard measure used in the literature is performance at word level, and thisis the one considered here.3.
Probabilistic FormulationIn the probabilistic formulation of the tagging problem we assume that the alignmentsare generated by a probabilistic model according to a probability distribution:p(W,T)In this case, depending on the criterion that we choose for evaluation, the optimaltagging procedure is as follows:?
for evaluation at sentence level, choose the most probable sequence oftags for the sentenceargmax argmaxT p(T/W)= T p(W,T)We call this procedure Viterbi tagging.
It is achieved using a dynamicprogramming scheme.for evaluation at word level, choose the most probable tag for each wordin the sentenceargmax argmax~(W)i = t p(ti = t/W) = t ~ p(W, T)T:ti=twhere ~(W)i is the tag assigned to word wi by the tagging procedure ~bin the context of the sentence W, We call this procedure MaximumLikelihood (ML) tagging.It is interesting to note that the most commonly used method is Viterbi tagging(see DeRose 1988; Church 1989) although it is not the optimal method for evaluationat word level.
The reasons for this preference are presumably that:?
Viterbi tagging is simpler to implement than ML tagging and requiresless computation (although they both have the same asymptoticcomplexity)?
Viterbi tagging provides the best interpretation for the sentence, which islinguistically appealing?
ML tagging may produce sequences of tags that are linguisticallyimpossible (because the choice of a tag depends on all contexts takentogether).However, in our experiments, we will show that Viterbi and ML tagging result in verysimilar performance.157Computational Linguistics Volume 20, Number 2Of course, the real tags have not been generated by a probabilistic model and,even if they had been, we would not be able to determine this model exactly be-cause of practical imitations.
Therefore the models that we construct will only beapproximations of an ideal model that does not exist.
It so happens that despite theseassumptions and approximations, these models are still able to perform reasonablywell.4.
The Triclass ModelWe have the mathematical expression:Hp(W~ T)  = I I  P (Wi /Wl t l  .
.
.
w i - l t i - l t i ) .p ( t i /w l t l  .
.
.
W i - l t i -1 )i=1The triclass (or tri-POS \[Derouault 1986\], or tri-Ggram \[Codogno et al 1987\], orHK) model is based on the following approximations:?
The probability of the tag given the past depends only on the last twotagsp( t i /w l t l  .
.
.
w i - l t i -1 )  = h( t i / t i _a t i _ l )?
The probability of the word given the past depends only on its tagp(wi /w l t l  .
.
.
W i - l t i - l t i )  = k (w i / t i )(the name HK model comes from the notation chosen for these probabilities).In order to define the model completely we have to specify the values of all h andk probabilities.
If Nw is the size of the vocabulary and NT the number of different tags,then there are:?
NT .NT .NT  values for the h probabilities?
Nw.NT  values for the k probabilities.Also, since all probability distributions have to sum to one, there are:?
NT .NT  equations to constrain the values for the h probabilities?
NT  equations to constrain the values for the k probabilities.The total number of free parameters i  then:(Nw - 1).NT + (NT  -- 1 ) .NT .NT .Note that this number grows only linearly with respect o the size of the vocabulary,which makes this model attractive for vocabularies of a very large size.The triclass model by itself allows any word to have any tag.
However, if wehave a dictionary that specifies the list of possible tags for each word, we can use thisinformation to constrain the model: if t is not a valid tag for the word w, then we aresure thatk(w/ t )  = O.There are thus at most as many nonzero values for the k probabilities as there arepossible pairs (word, tag) allowed in the dictionary.158Bernard Merialdo Tagging English Text with a Probabilistic Model5.
Training the Triclass ModelWe consider two different ypes of training:?
Relative Frequency (RF) training?
Maximum Likelihood (ML) training which is done via theForward-Backward (FB) algorithm.5.1 Relative Frequency TrainingIf we have some tagged text available we can compute the number of times N(w, t)a given word w appears with the tag t, and the number of times N(h, t2~ t3) the se-quence (tl~ t2~ t3) appears in this text.
We can then estimate the probabilities h and kby computing the relative frequencies of the corresponding events on this data:N(h, t2~ t3) hrf(tB/tl~ t2) =f(tg/tl, t2) - N(tl, t2)N(w,t)krf(W/t)=f(w/t)-  N(t)These estimates assign a probability of zero to any sequence of tags that did notoccur in the training data.
But such sequences may occur if we consider other texts.A probability of zero for a sequence creates problems because any alignment hatcontains this sequence will get a probability of zero.
Therefore, it may happen that,for some sequences of words, all alignments get a probability of zero and the modelbecomes useless for such sentences.To avoid this, we interpolate these distributions with uniform distributions, i.e.we consider the interpolated model defined by:wherehinter(t3/tl~ t2) = ~.hrf(t3/tl~ t2) q- (1 - ),).hunid(t3/h, t2)kinter(W/t) =/~.krf(W/t) q- (1 -/~).kunif(w/t )1hunif(t3/tl, t2) = ~TT1ku,if(w/t) = number of words that have the tag tThe interpolation coefficient ,~ is computed using the deleted interpolation algorithm(Jelinek and Mercer 1980) (it would also be possible to use two coefficients, one forthe interpolation on h, one for the interpolation on k).
The value of this coefficientis expected to increase if we increase the size of the training text, since the rela-tive frequencies should be more reliable.
This interpolation procedure is also called"smoothing.
"Smoothing is performed as follows:Some quantity of tagged text from the training data is not used in thecomputation of the relative frequencies.
It is called the "held-out" data.The coefficient & is chosen to maximize the probability of emission of theheld-out data by the interpolated model.159Computational Linguistics Volume 20, Number 2This maximization can be performed by the standard Forward-Backward(FB) or Baum-Welch algorithm (Baum and Eagon 1967; Jelinek 1976;Bahl, Jelinek, and Mercer 1983; Poritz 1988), by considering ~ and 1 -as the transition probabilities of a Markov model.It can be noted that more complicated interpolation schemes are possible.
Forexample, different coefficients can be used depending on the count of (h, t2), with theintuition that relative frequencies can be trusted more when this count is high.
Anotherpossibilitity is to interpolate also with models of different orders, such as hrf(t3/t2) orhrf(t3).Smoothing can also be achieved with procedures other than interpolation.
Oneexample is the "backing-off" strategy proposed by Katz (1987).5.2 Maximum Likelihood TrainingUsing a triclass model M it is possible to compute the probability of any sequence ofwords W according to this model:= Zp (w, T)Twhere the sum is taken over all possible alignments.
The Maximum Likelihood (ML)training finds the model M that maximizes the probability of the training text:max I I  pM(W)MWwhere the product is taken over all the sentences W in the training text.
This is theproblem of training a hidden Markov model (it is hidden because the sequence of tagsis hidden).
A well-known solution to this problem is the Forward-Backward (FB) orBaum-Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer1983), which iteratively constructs a sequence of models that improve the probabilityof the training data.The advantage of this approach is that it does not require any tagging of the text,but makes the assumption that the correct model is the one in which tags are used tobest predict the word sequence.6.
Tagging AlgorithmsThe Viterbi algorithm is easily implemented using a dynamic programming scheme(Bellman 1957).
The Maximum Likelihood algorithm appears more complex at firstglance, because it involves computing the sum of the probabilities of a large numberof alignments.
However, in the case of a hidden Markov model, these computationscan be arranged in a way similar to the one used during the FB algorithm, so that theoverall amount of computation eeded becomes linear in the length of the sentence(Baum and Eagon 1967).7.
ExperimentsThe main objective of this paper is to compare RF and ML training.
This is done inSection 7.2.
We also take advantage of the environment that we have set up to performother experiments, described in Section 7.3, that have some theoretical interest, but did160Bernard Merialdo Tagging English Text with a Probabilistic ModelTable 1RF training on N sentences, Viterbi tagging.Training data Interpolation Nb of errors % correct(sentences) coefficient )~ (words) tags0 .0 10498 77.0100 .48 4568 90.02000 .77 2110 95.45000 .85 1744 96.210000 .90 1555 96.620000 .92 1419 96.9all .94 1365 97.0not bring any improvement in practice.
One concerns the difference between Viterbiand ML tagging, and the other concerns the use of constraints during training.We shall begin by describing the textual data that we are using, before presentingthe different tagging experiments using these various training and tagging methods.7.1 Text DataWe use the "treebank" data described in Beale (1988).
It contains 42,186 sentences(about one million words) from the Associated Press.
These sentences have been taggedmanually at the Unit for Computer Research on the English Language (University ofLancaster, U.K.), in collaboration with IBM U.K. (Winchester) and the IBM SpeechRecognition group in Yorktown Heights (USA).
In fact, these sentences are not onlytagged but also parsed.
However, we do not use the information contained in theparse.In the treebank 159 different tags are used.
These tags were projected on a smallersystem of 76 tags designed by Evelyne Tzoukermann and Peter Brown (see Appendix).The results quoted in this paper all refer to this smaller system.We built a dictionary that indicates the list of possible tags for each word, bytaking all the words that occur in this text and, for each word, all the tags that areassigned to it somewhere in the text.
In some sense, this is an optimal dictionary forthis data, since a word will not have all its possible tags (in the language), but onlythe tags that it actually had within the text.We separated this data into two parts:a set of 40,186 tagged sentences, the training data, which is used to buildthe modelsa set of 2,000 tagged sentences (45,583 words), the test data, which isused to test the quality of the models.7.2 Basic ExperimentsRF training, Viterbi taggingIn this experiment, we extracted N tagged sentences from the training data.
We thencomputed the relative frequencies on these sentences and built a "smoothed" modelusing the procedure previously described.
This model was then used to tag the 2,000test sentences.
We experimented with different values of N, for each of which weindicate the value of the interpolation coefficient and the number and percentage ofcorrectly tagged words.
Results are indicated in Table 1.161Computational Linguistics Volume 20, Number 2As expected, as the size of the training increases, the interpolation coefficient in-creases and the quality of the tagging improves.When N = 0, the model is made up of uniform distributions.
In this case, allalignments for a sentence are equally probable, so that the choice of the correct agis just a choice at random.
However, the percentage of correct ags is relatively high(more than three out of four) because:?
almost half of the words of the text have a single possible tag, so that nomistake can be made on these words?
about a quarter of the words of the text have only two possible tags sothat, on the average, a random choice is correct every other time.Note that this behavior is obviously very dependent on the system of tags that is used.It can be noted that reasonable results are obtained quite rapidly.
Using 2,000tagged sentences (less than 50,000 words), the tagging error rate is already less than 5%.Using 10 times as much data (20,000 tagged sentences) provides an improvement ofonly 1.5%.ML training, Viterbi taggingIn ML training we take all the training data available (40,186 sentences) but we onlyuse the word sequences, not the associated tags (except o compute the initial model,as will be described later).
This is possible since the FB algorithm is able to train themodel using the word sequence only.In the first experiment we took the model made up of uniform distributions as theinitial one.
The only constraints in this model came from the values k(w/t) that wereset to zero when the tag t was not possible for the word w (as found in the dictionary).We then ran the FB algorithm and evaluated the quality of the tagging.
The resultsare shown in Figure 1.
(Perplexity is a measure of the average branching factor forprobabilistic models.
)This figure shows that ML training both improves the perplexity of the model andreduces the tagging error rate.
However, this error rate remains at a relatively highlevel--higher than that obtained with a RF training on 100 tagged sentences.Having shown that ML training is able to improve the uniform model, we thenwanted to know if it was also able to improve more accurate models.
We thereforetook as the initial model each of the models obtained previously by RF training and,for each one, performed ML training using all of the training word sequences.
Theresults are shown graphically in Figure 2 and numerically in Table 2.These results show that, when we use few tagged data, the model obtained byrelative frequency is not very good and Maximum Likelihood training is able to im-prove it.
However, as the amount of tagged data increases, the models obtained byRelative Frequency are more accurate and Maximum Likelihood training improveson the initial iterations only, but after deteriorates.
If we use more than 5,000 taggedsentences, even the first iteration of ML training degrades the tagging.
(This numberis of course dependent on both the particular system of tags and the kind of text usedin this experiment).These results call for some comments.
ML training is a theoretically sound pro-cedure, and one that is routinely and successfully used in speech recognition to es-timate the parameters of hidden Markov models that describe the relations betweensequences of phonemes and the speech signal.
Although ML training is guaranteedto improve perplexity, perplexity is not necessarily related to tagging accuracy, andit is possible to improve one while degrading the other.
Also, in the case of tagging,162Bernard Merialdo Tagging English Text with a Probabilistic Model24222018161412i006O058056O54052O5OO480460440I I I I5 10  15  20o o o .~ :I I 315  I 41525 30  40I te ra t ionsi , ii0 I t tS 15  20 25I te ra t ions0 0t I30  35  4O 4550Figure 1ML training from uniform distributions.Table 2ML training from various initial points.Number of tagged sentences used for the initial model0 100 2000 5000 10000 20000 allIter Correct tags (% words) after ML on 1M words0 77.0 90.0 95.4 96.2 96.6 96.9 97.01 80.5 92.6 95.8 96.3 96.6 96.7 96.82 81.8 93.0 95.7 96.1 96.3 96.4 96.43 83.0 93.1 95.4 95.8 96.1 96.2 96.24 84.0 93.0 95.2 95.5 95.8 96.0 96.05 84.8 92.9 95.1 95.4 95.6 95.8 95.86 85.3 92.8 94.9 95.2 95.5 95.6 95.77 85.8 92.8 94.7 95.1 95.3 95.5 95.58 86.1 92.7 94.6 95.0 95.2 95.4 95.49 86.3 92.6 94.5 94.9 95.1 95.3 95.310 86.6 92.6 94.4 94.8 95.0 95.2 95.2163Computational Linguistics Volume 20, Number 21210 'I I I IA.... ~ .. .
.
.
.
.
.
.
-~ ...... y.T.~_T .............. ~.-.-...----'~.~:":::-:-.-~:z:-:-:x-.
.
.
.
.
.
.
.
.
e .
.
.
.
.
.
.
.
.  ""
" ' -  .
.
.
.
.
x .
.
.
.
.
.
.
.
.
.
.
.
.
.
~ , ' :2C '? '
.
- : - :~_ - .
- zZ - ' .
'~ : ,~- ' :m-  " - 'Z -  - - -m- ' - - -  .
.
.
.
.
.
.
.I I I I2 4 6 8 i0I terat ionsFigure 2ML training from various initial points (top l ine corresponds to N=IO0,  bottom l ine to N=al l) .the relations between words and tags are much more precise than the relations be-tween phonemes and speech signals (where the correct correspondence is harder todefine precisely).
Some characteristics of ML training, such as the effect of smoothingprobabilities, are probably more suited to speech than to tagging.7.3 Extra ExperimentsViterbi versus ML taggingFor this experiment we considered the initial model built by RF training over the wholetraining data and all the successive models created by the iterations of ML training.For each of these models we performed Viterbi tagging and ML tagging on the sametest data, then evaluated and compared the number of tagging errors produced bythese two methods.
The results are shown in Table 3.The models obtained at different iterations are related, so one should not drawstrong conclusions about the definite superiority of one tagging procedure.
However,the difference in error rate is very small, and shows that the choice of the taggingprocedure is not as critical as the kind of training material.Constrained ML trainingFollowing a suggestion made by E Jelinek, we investigated the effect of constrainingthe ML training by imposing constraints on the probabilities.
This idea comes fromthe observation that the amount of training data needed to properly estimate themodel increases with the number of free parameters of the model.
In the case oflittle training data, adding reasonable constraints on the shape of the models that arelooked for reduces the number of free parameters and should improve the quality ofthe estimates.164Bernard Merialdo Tagging English Text with a Probabilistic ModelTable 3Viterbi vs. ML tagging.Tagging errors out of 45,583 wordsIter.
Viterbi ML Vit.
- ML0 % nb % nb nb0 97.01 1365 97.01 1362 31 96.76 1477 96.75 1480 - 32 96.44 1623 96.47 1607 163 96.23 1718 96.23 1719 - 14 96.00 1824 96.02 1812 125 95.82 1906 95.85 1892 146 95.66 1978 95.68 1970 87 95.51 2046 95.54 2031 158 95.39 2100 95.42 2087 139 95.30 2144 95.31 2140 410 95.21 2183 95.22 2177 6Table 4Standard ML vs. tw-constrained ML training.Tagging errors out of 45,583 wordsIter.
ML tw-c. ML0 % nb % nb0 97.01 1365 97.01 13651 96.76 1477 96.87 14272 96.44 1623 96.71 15013 96.23 1718 96.57 15624 96.00 1824 96.43 16265 95.82 1906 96.36 16616 95.66 1978 96.29 16907 95.51 2046 96.22 17238 95.39 2100 96.18 17419 95.30 2144 96.12 176810 95.21 2183 96.09 1784We tr ied two di f ferent constraints:?
The first one keeps  p(t/w) f ixed if w is a f requent  word ,  in our  case oneof the 1,000 most  f requent  words .
We call it tw-constraint.
The rat ionale isthat  if w is frequent,  the re lat ive f requency prov ides  a good  est imate forp(t/w) and the t ra in ing shou ld  not  change it.?
The second one keeps the marg ina l  d is t r ibut ion  p(t) constant  and isbased  on a s imi lar  reasoning.
We call it t-constraint.tw-constra intThe tw-const ra ined  ML t ra in ing is s imi lar  to the s tandard  ML tra in ing,  except  that theprobabi l i t ies  p(t/w) are not  changed at the end of an iteration.The results  in Table 4 show the number  of tagg ing errors when the mode l  is t ra inedw i th  the s tandard  or tw-const ra ined  ML training.
They show that the tw-const ra inedML t ra in ing stil l degrades  the RF tra in ing,  but  not  as quick ly  as the s tandard  ML.
We165Computational Linguistics Volume 20, Number 2Table 5Standard ML vs. constrained ML training.Tagging errors out of 45,583 words (biclass model)Iter.
ML t-c. ML0 % nb % nb0 96.87 1429 96.87 14291 96.51 1592 96.54 15762 96.18 1743 96.23 17183 96.00 1824 96.03 18104 95.84 1896 95.90 18715 95.67 1972 95.77 19286 95.52 2044 95.59 20097 95.42 2087 95.50 20518 95.33 2129 95.42 20879 95.24 2171 95.34 212610 95.18 2196 95.30 2141have not tested what happens when smaller training data is used to build the initialmodel.t-constraintThis constraint is more difficult to implement than the previous one because the prob-abilities p( t )  are not the parameters of the model, but a combination of these parame-ters.
With the help of R. Polyak we have designed an iterative procedure that allowsthe likelihood to be improved while preserving the values of p( t ) .
We do not havesufficient space to describe this procedure here.
Because of its greater computationalcomplexity, we have only applied it to a biclass model, i.e.
a model wherep( t i /w l t l  .
.
.
W i - l t i -1 )  = h( t i / t i -1 ) .The initial model is estimated by relative frequency on the whole training data andViterbi tagging is used.As in the previous experiment, the results in Table 5 show the number of taggingerrors when the model is trained with the standard or t-constrained ML training.They show that the t-constrained ML training still degrades the RF training, but notas quickly as the standard ML.
Again, we have not tested what happens when smallertraining data is used to build the initial model.8.
Conclus ionThe results presented in this paper show that estimating the parameters of the modelby counting relative frequencies over a very large amount of hand-tagged text lead tothe best tagging accuracy.Maximum Likelihood training is guaranteed to improve perplexity, but will notnecessarily improve tagging accuracy.
In our experiments, ML training degrades theperformance unless the initial model is already very bad.The preceding results suggest hat the optimal strategy to build the best possiblemodel for tagging is the following:?
get as much tagged (by hand) text as you can afford166Bernard Merialdo Tagging English Text with a Probabilistic Modelcompute the relative frequencies from this data to build an initial modelM0get as much untagged text as you can affordstarting from M0, perform the Forward-Backward iterations.
At eachiteration, evaluate the tagging quality of the new model Mi on someheld-out agged text.
Stop either when you have reached a presetnumber of iterations or the model Mi performs worse than Mi-1,whichever occurs first.AcknowledgmentsI would like to thank Peter Brown, FredJelinek, John Lafferty, Robert Mercer, SalimRoukos, and other members of theContinuous Speech Recognition group forthe fruitful discussions I had with themthroughout this work.
I also want to thankone of the referees for his judiciouscomments.ReferencesBahl, Lalit R., and Mercer, Robert L.
(1976).
"Part of speech assignment by a statisticaldecision algorithm."
In IEEE InternationalSymposium on Information Theory, 88-89.Ronneby.Bahl, Lalit R.; Jelinek, Frederick and Mercer,Robert L. (1983).
"A maximum likelihoodapproach to continuous speechrecognition," In IEEE Transactions onPAML 5(2), 179-190.Baum, L. E., and Eagon, J.
A.
(1967).
"Aninequality with application to statisticalestimation for probabilistic functions ofMarkov processes and to a model forecology."
Bulletin of the AmericanMathematicians Society, 73, 360-363.Beale, A. D. (1985).
"A probabilisticapproach to grammatical nalysis ofwritten English by computer."
InProceedings, Second Conference oftheEuropean Chapter of the ACL, Geneva,Switzerland, 159-165.Beale, A. D. (1988).
"Lexicon and grammarin probabilistic tagging of writtenEnglish."
In Proceedings, 26th AnnualMeeting of the Association for ComputationalLinguistics, Buffalo NY: 211-216.Bellman, R. E. (1957).
Dynamic Programming.Princeton University Press.Benello, J.; Mackie, A. W.; and Anderson,J.
A.
(1989).
"Syntactic ategorydisambiguation with neural networks.
"Computer Speech and Language, 3, 203-217.Brill, E.; Magerman, D.; Marcus, M.; andSantorini, B.
(1990).
"Deducing linguisticstructure from the statistics of largecorpora."
In Proceedings, DARPA Speechand Natural Language Workshop, HiddenValley PA. 275-282.Brodda, Benny (1982).
"Problems withtagging and a solution."
Nordic Journal ofLinguistics, 93-116.Church, Kenneth W. (1989).
"A stochasticparts program noun phrase parser forunrestricted text."
In IEEE Proceedings ofthe ICASSP, Glasgow, 695-698.Codogno, M.; Fissore, L.; Martelli, A.; Pirani,G.
; and Volpi, G. (1987).
"Experimentalevaluation of Italian language models forlarge-dictionary speech recognition."
InProceedings, European Conference on SpeechTechnology, Edinburgh, 159-162.Cutting, D.; Kupiec, J.; Pedersen, J.; andSibun, P. (1992).
"A practicalpart-of-speech tagger."
In Proceedings,Third Conference on Applied LanguageProcessing, Trento, Italy, 133-140.Debili, Fathi (1977).
"Traitementssyntaxiques utilisant des matrices deprecedence frequentielles construitesautomatiquement parapprentissage.
"Doctoral dissertation, EngineeringDepartment, Universite Paris 7, France.DeRose, S. (1988).
"Grammatical categorydisambiguation by statisticaloptimization."
Computational Linguistics,14(1), 31-39.Derouault, Anne-Marie, and Merialdo,Bernard (1986).
"Natural anguagemodeling for phoneme-to-texttranscription."
In IEEE Transactions onPattern Analysis and Machine Intelligence,8(6), 742-749.Garside, R., and Leech, F. (1985).
"Aprobabilistic parser."
In Proceedings, SecondConference ofthe European Chapter of theACL, Geneva, Switzerland, 166-170.Jelinek, Frederick (1976).
"Continuousspeech recognition by statisticalmethods."
In Proceedings ofthe IEEE, 64,532-556.Jelinek, Frederick, and Mercer, Robert L.167Computational Linguistics Volume 20, Number 2(1980).
"Interpolated estimation ofMarkov source parameters from sparsedata."
In Proceedings, Workshop on PatternRecognition i  Practice, Amsterdam,381-397.Katz, S. (1987).
"Estimation of probabilitiesfrom sparse data for the language modelcomponent of a speech recognizer."
IEEETransactions on ASSP, 34(3), 400-401.Klein, S., and Simmons, R. E (1963).
"Agrammatical pproach to grammaticalcoding of English words."
JACM, 10,334-347.Leech, G.; Garside, R.; and Atwell, E.
(1983).
"The automatic grammatical tagging ofthe LOB corpus."
Newsletter of theInternational Computer Archive of ModernEnglish, 7, 13-33.de Marcken, C. G. (1990).
"Parsing the LOBcorpus."
In Proceedings, ACL AnnualMeeting, Pittsburg PA, 243-251.Marshall, Ian (1983).
"Choice ofgrammatical word-class without globalsyntactic analysis: Tagging words in theLOB corpus."
Computers and theHumanities, 139-150.Merialdo, Bernard (1991).
"Tagging text witha probabilistic model."
In IEEE Proceedingsof the ICASSP, Toronto, 809-812.Nakamura, M., and Shikano, K. (1989).
"Astudy of English word categoryprediction based on neural networks."
InIEEE Proceedings ofthe ICASSP, Glasgow,731-734.Paulussen, H., and Martin, W.
(1992).
"Dilemma-2: A lemmatizer-tagger formedical abstracts."
In Proceedings, ThirdConference on Applied Language Processing,Trento, Italy, 141-146.Poritz, Alan B.
(1988).
"Hidden Markovmodels: A guided tour."
In IEEEProceedings ofthe ICASSP, New York, 7-13.Stolz, W. S.; Tannenbaum, P. H.; andCarstensen F. V. (1965).
"A stochasticapproach to the grammatical coding ofEnglish."
Communications of the ACM, 8,399-405.168Bernard Merialdo Tagging English Text with a Probabilistic ModelAppendix A: List of Tags Used$* possessive marker ('s, ')APP$* possessive adjectives (my, your, our)AT* article (the, a, no)BOUNDARY_TAG end-of-sentence markerCCF* coordinating conjunction (and, or, but, so, yet, then)CS* subordinating conjunction (/f, because, unless)CT* that or whether as subordinating conjunctionsD* determiner (all, any, enough)D*Q wh-determiner (which, what, whose)D*R comparative plural after-determiner (less, more)D*I determiner singular (this, that, little, much, former)D*2 determiner plural (these, few, several, many)DAT* superlative determiner (least, most)EX* existential thereFW* foreign words (ipso, facto)I* preposition (general)ICS* preposition that can also be used as a conjunction (since, after)IF* the preposition forIO* the preposition ofJ* adjective (small, pretty)J*R comparative adjective (smaller, prettier)J*T superlative adjective (prettiest, nicest)LE* leading coordinator (both, either, neither)M* cardinal numberMD* ordinal number (first, second)N* noun without number (english)N*I singular noun (cat, man)N*2 plural noun (cats, men)NPR* proper noun (paris, fred)NR* noun/adverb of direction (south, west) or time (now, tomorrow, tuesday)P* non-nominative pronoun (none, anyone, oneself)P*Q who, whom, whoever, whomeverPNXI* personal pronoun reflexive (himself)169Computational Linguistics Volume 20, Number 2PNI* indefinite pronoun (anyone, anybody)PP$* possessive pronoun (mine, yours)PP*O personal pronoun object (me, him)PP*S personal pronoun subject (I, you, we)PP*S3 personal pronotm subject 3rd person singular (he, she)PUNCTI* end of sentence (.
!
?
- )PUNCT2* non terminal punctuation (, : ;)QUOT* quoteR* adverb (here, slowly)R*Q wh-adverb (where, when, why, how, whenever, wherever)R*R comparative adverb (better, longer)RG* degree adverb (very, so, too, enough, indeed)RGQ* wh-degree adverb (how)RGR* comparative degree adverb (more, less, worse)RP* adverb that can also serve as a prepositionSIGN* sign ($, c., ct, %)TO* to as pre-infinitiveUH* interjection (gee)VBDR* wereVBDZ* wasVBG* beingVBI* infinitive form of be and imperativeVBM* amVBN* beenVBR* areVBZ* isVDG* doingVDN* past participial form of do (did)VDPAST* past form of do (did)VD0* do as a conjugated form and infinitiveVDOZ* does as a conjugated formVHG* havingVHN* past participial form of have (had)VHPAST* past form of have (had)170Bernard Merialdo Tagging English Text with a Probabilisfic ModelVH0* have as a conjugated formVHOZ* has as a conjugated formVM* modals (can, would, ought, used)VVG* non-aux verb in -ingVVN* past participial form of non-aux verbVVPAST* preterit of non-aux verbVV0* non-third-person-singular form of non-aux verb and infinitiveVVOZ* third-person-singular form of non-aux verbXX* not171
