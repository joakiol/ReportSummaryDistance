Confidence Estimation for Information ExtractionAron CulottaDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003culotta@cs.umass.eduAndrew McCallumDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003mccallum@cs.umass.eduAbstractInformation extraction techniques automati-cally create structured databases from un-structured data sources, such as the Web ornewswire documents.
Despite the successes ofthese systems, accuracy will always be imper-fect.
For many reasons, it is highly desirable toaccurately estimate the confidence the systemhas in the correctness of each extracted field.The information extraction system we evalu-ate is based on a linear-chain conditional ran-dom field (CRF), a probabilistic model whichhas performed well on information extractiontasks because of its ability to capture arbitrary,overlapping features of the input in a Markovmodel.
We implement several techniques to es-timate the confidence of both extracted fieldsand entire multi-field records, obtaining an av-erage precision of 98% for retrieving correctfields and 87% for multi-field records.1 IntroductionInformation extraction usually consists of tagging a se-quence of words (e.g.
a Web document) with semanticlabels (e.g.
PERSONNAME, PHONENUMBER) and de-positing these extracted fields into a database.
Becauseautomated information extraction will never be perfectlyaccurate, it is helpful to have an effective measure ofthe confidence that the proposed database entries are cor-rect.
There are at least three important applications ofaccurate confidence estimation.
First, accuracy-coveragetrade-offs are a common way to improve data integrity indatabases.
Efficiently making these trade-offs requires anaccurate prediction of correctness.Second, confidence estimates are essential for inter-active information extraction, in which users may cor-rect incorrectly extracted fields.
These corrections arethen automatically propagated in order to correct othermistakes in the same record.
Directing the user tothe least confident field allows the system to improveits performance with a minimal amount of user effort.Kristjannson et al (2004) show that using accurate con-fidence estimation reduces error rates by 46%.Third, confidence estimates can improve performanceof data mining algorithms that depend upon databasescreated by information extraction systems (McCallumand Jensen, 2003).
Confidence estimates provide datamining applications with a richer set of ?bottom-up?
hy-potheses, resulting in more accurate inferences.
An ex-ample of this occurs in the task of citation co-referenceresolution.
An information extraction system labels eachfield of a paper citation (e.g.
AUTHOR, TITLE), and thenco-reference resolution merges disparate references to thesame paper.
Attaching a confidence value to each fieldallows the system to examine alternate labelings for lessconfident fields to improve performance.Sound probabilistic extraction models are most con-ducive to accurate confidence estimation because of theirintelligent handling of uncertainty information.
In thiswork we use conditional random fields (Lafferty et al,2001), a type of undirected graphical model, to automat-ically label fields of contact records.
Here, a record is anentire block of a person?s contact information, and a fieldis one element of that record (e.g.
COMPANYNAME).
Weimplement several techniques to estimate both field con-fidence and record confidence, obtaining an average pre-cision of 98% for fields and 87% for records.2 Conditional Random FieldsConditional random fields (Lafferty et al, 2001) are undi-rected graphical models to calculate the conditional prob-ability of values on designated output nodes given val-ues on designated input nodes.
In the special case inwhich the designated output nodes are linked by edges ina linear chain, CRFs make a first-order Markov indepen-dence assumption among output nodes, and thus corre-spond to finite state machines (FSMs).
In this case CRFscan be roughly understood as conditionally-trained hid-den Markov models, with additional flexibility to effec-tively take advantage of complex overlapping features.Let o = ?o1, o2, ...oT ?
be some observed input data se-quence, such as a sequence of words in a document (thevalues on T input nodes of the graphical model).
Let S bea set of FSM states, each of which is associated with a la-bel (such as COMPANYNAME).
Let s = ?s1, s2, ...sT ?
besome sequence of states (the values on T output nodes).CRFs define the conditional probability of a state se-quence given an input sequence asp?
(s|o) =1Zoexp( T?t=1?k?kfk(st?1, st,o, t)),(1)where Zo is a normalization factor over all state se-quences, fk(st?1, st,o, t) is an arbitrary feature func-tion over its arguments, and ?k is a learned weight foreach feature function.
Zo is efficiently calculated usingdynamic programming.
Inference (very much like theViterbi algorithm in this case) is also a matter of dynamicprogramming.
Maximum aposteriori training of thesemodels is efficiently performed by hill-climbing methodssuch as conjugate gradient, or its improved second-ordercousin, limited-memory BFGS.3 Field Confidence EstimationThe Viterbi algorithm finds the most likely state sequencematching the observed word sequence.
The word thatViterbi matches with a particular FSM state is extractedas belonging to the corresponding database field.
We canobtain a numeric score for an entire sequence, and thenturn this into a probability for the entire sequence by nor-malizing.
However, to estimate the confidence of an indi-vidual field, we desire the probability of a subsequence,marginalizing out the state selection for all other partsof the sequence.
A specialization of Forward-Backward,termed Constrained Forward-Backward (CFB), returnsexactly this probability.Because CRFs are conditional models, Viterbi findsthe most likely state sequence given an observation se-quence, defined as s?
= argmaxs p?(s|o).
To avoid anexponential-time search over all possible settings of s,Viterbi stores the probability of the most likely path attime t that accounts for the first t observations and endsin state si.
Following traditional notation, we define thisprobability to be ?t(si), where ?0(si) is the probability ofstarting in each state si, and the recursive formula is:?t+1(si) = maxs?[?t(s?)
exp(?k?kfk(s?, si,o, t))](2)terminating in s?
= argmaxs1?si?sN[?T (si)].The Forward-Backward algorithm can be viewed as ageneralization of the Viterbi algorithm: instead of choos-ing the optimal state sequence, Forward-Backward eval-uates all possible state sequences given the observationsequence.
The ?forward values?
?t+1(si) are recursivelydefined similarly as in Eq.
2, except the max is replacedby a summation.
Thus we have?t+1(si) =?s?[?t(s?)
exp(?k?kfk(s?, si,o, t))].
(3)terminating in Zo =?i ?T (si) from Eq.
1.To estimate the probability that a field is extractedcorrectly, we constrain the Forward-Backward algorithmsuch that each path conforms to some subpath of con-straints C = ?sq .
.
.
sr?
from time step q to r. Here,sq ?
C can be either a positive constraint (the sequencemust pass through sq) or a negative constraint (the se-quence must not pass through sq).In the context of information extraction, C correspondsto an extracted field.
The positive constraints specify theobservation tokens labeled inside the field, and the neg-ative constraints specify the field boundary.
For exam-ple, if we use states names B-TITLE and I-JOBTITLE tolabel tokens that begin and continue a JOBTITLE field,and the system labels observation sequence ?o2, .
.
.
, o5?as a JOBTITLE field, then C = ?s2 = B-JOBTITLE,s3 = .
.
.
= s5 = I-JOBTITLE, s6 6= I-JOBTITLE?.The calculations of the forward values can be made toconform to C by the recursion ?
?q(si) ={Ps?h??q?1(s?)
exp?Pk ?kfk(s?, si, o, t)?iif si ' sq0 otherwisefor all sq ?
C, where the operator si ' sq means siconforms to constraint sq .
For time steps not constrainedby C, Eq.
3 is used instead.If ?
?t+1(si) is the constrained forward value, thenZ ?o =?i ?
?T (si) is the value of the constrained lat-tice, the set of all paths that conform to C. Our confi-dence estimate is obtained by normalizing Z ?o using Zo,i.e.
Z ?o ?
Zo.We also implement an alternative method that uses thestate probability distributions for each state in the ex-tracted field.
Let ?t(si) = p(si|o1, .
.
.
, oT ) be the prob-ability of being in state i at time t given the observationsequence .
We define the confidence measure GAMMAto be?vi=u ?i(si), where u and v are the start and endindices of the extracted field.4 Record Confidence EstimationWe can similarly use CFB to estimate the probability thatan entire record is labeled correctly.
The procedure isthe same as in the previous section, except that C nowspecifies the labels for all fields in the record.We also implement three alternative record confidenceestimates.
FIELDPRODUCT calculates the confidence ofeach field in the record using CFB, then multiplies thesevalues together to obtain the record confidence.
FIELD-MIN instead uses the minimum field confidence as therecord confidence.
VITERBIRATIO uses the ratio of theprobabilities of the top two Viterbi paths, capturing howmuch more likely s?
is than its closest alternative.5 Reranking with Maximum EntropyWe also trained two conditional maximum entropy clas-sifiers to classify fields and records as being labeled cor-rectly or incorrectly.
The resulting posterior probabil-ity of the ?correct?
label is used as the confidence mea-sure.
The approach is inspired by results from (Collins,2000), which show discriminative classifiers can improvethe ranking of parses produced by a generative parser.After initial experimentation, the most informative in-puts for the field confidence classifier were field length,the predicted label of the field, whether or not this fieldhas been extracted elsewhere in this record, and the CFBconfidence estimate for this field.
For the record confi-dence classifier, we incorporated the following features:record length, whether or not two fields were tagged withthe same label, and the CFB confidence estimate.6 Experiments2187 contact records (27,560 words) were collected fromWeb pages and email and 25 classes of data fields werehand-labeled.1 The features for the CRF consist of thetoken text, capitalization features, 24 regular expressionsover the token text (e.g.
CONTAINSHYPHEN), and off-sets of these features within a window of size 5.
We alsouse 19 lexicons, including ?US Last Names,?
?US FirstNames,?
and ?State Names.?
Feature induction is notused in these experiments.
The CRF is trained on 60% ofthe data, and the remaining 40% is split evenly into de-velopment and testing sets.
The development set is usedto train the maximum entropy classifiers, and the testingset is used to measure the accuracy of the confidence es-timates.
The CRF achieves an overall token accuracy of87.32 on the testing data, with a field-level performanceof F1 = 84.11, precision = 85.43, and recall = 82.83.To evaluate confidence estimation, we use three meth-ods.
The first is Pearson?s r, a correlation coefficientranging from -1 to 1 that measures the correlation be-tween a confidence score and whether or not the field(or record) is correctly labeled.
The second is averageprecision, used in the Information Retrieval community1The 25 fields are: FirstName, MiddleName, LastName,NickName, Suffix, Title, JobTitle, CompanyName, Depart-ment, AddressLine, City1, City2, State, Country, PostalCode,HomePhone, Fax, CompanyPhone, DirectCompanyPhone, Mo-bile, Pager, VoiceMail, URL, Email, InstantMessagePearson?s r Avg.
PrecCFB .573 .976MaxEnt .571 .976Gamma .418 .912Random .012 .858WorstCase ?
.672Table 1: Evaluation of confidence estimates for field confi-dence.
CFB and MAXENT outperform competing methods.Pearson?s r Avg.
PrecCFB .626 .863MaxEnt .630 .867FieldProduct .608 .858FieldMin .588 .843ViterbiRatio .313 .842Random .043 .526WorstCase ?
.304Table 2: Evaluation of confidence estimates for record confi-dence.
CFB, MAXENT again perform best.to evaluate ranked lists.
It calculates the precision ateach point in the ranked list where a relevant documentis found and then averages these values.
Instead of rank-ing documents by their relevance score, here we rankfields (and records) by their confidence score, where acorrectly labeled field is analogous to a relevant docu-ment.
WORSTCASE is the average precision obtainedby ranking all incorrect instances above all correct in-stances.
Tables 1 and 2 show that CFB and MAXENT arestatistically similar, and that both outperform competingmethods.
Note that WORSTCASE achieves a high aver-age precision simply because so many fields are correctlylabeled.
In all experiments, RANDOM assigns confidencevalues chosen uniformly at random between 0 and 1.The third measure is an accuracy-coverage graph.
Bet-ter confidence estimates push the curve to the upper-right.Figure 1 shows that CFB and MAXENT dramatically out-perform GAMMA.
Although omitted for space, similarresults are also achieved on a noun-phrase chunking task(CFB r = .516, GAMMA r = .432) and a named-entityextraction task (CFB r = .508, GAMMA r = .480).7 Related WorkWhile there has been previous work using probabilisticestimates for token confidence, and heuristic estimatesfor field confidence, to the best of our knowledge this pa-per is the first to use a sound, probabilistic estimate forconfidence of multi-word fields and records in informa-tion extraction.Much of the work in confidence estimationfor IE has been in the active learning literature.Scheffer et al (2001) derive confidence estimates usinghidden Markov models in an information extractionsystem.
However, they do not estimate the confidenceof entire fields, only singleton tokens.
They estimate0.840.860.880.90.920.940.960.9810  0.2  0.4  0.6  0.8  1accuracycoverage"Optimal""CFB""MaxEnt""Gamma""Random"Figure 1: The precision-recall curve for fields shows that CFBand MAXENT outperform GAMMA.the confidence of a token by the difference betweenthe probabilities of its first and second most likelylabels, whereas CFB considers the full distribution ofall suboptimal paths.
Scheffer et al (2001) also explorean idea similar to CFB to perform Baum-Welch trainingwith partially labeled data, where the provided labelsare constraints.
However, these constraints are again forsingleton tokens only.Rule-based extraction methods (Thompson et al,1999) estimate confidence based on a rule?s coverage inthe training data.
Other areas where confidence estima-tion is used include document classification (Bennett etal., 2002), where classifiers are built using meta-featuresof the document; speech recognition (Gunawardana et al,1998), where the confidence of a recognized word is esti-mated by considering a list of commonly confused words;and machine translation (Gandrabur and Foster, 2003),where neural networks are used to learn the probability ofa correct word translation using text features and knowl-edge of alternate translations.8 ConclusionWe have shown that CFB is a mathematically and empir-ically sound confidence estimator for finite state informa-tion extraction systems, providing strong correlation withcorrectness and obtaining an average precision of 97.6%for estimating field correctness.
Unlike methods marginmaximization methods such as SVMs and M3Ns (Taskaret al, 2003), CRFs are trained to maximize conditionalprobability and are thus more naturally appropriate forconfidence estimation.
Interestingly, reranking by MAX-ENT does not seem to improve performance, despite thebenefit Collins (2000) has shown discriminative rerank-ing to provide generative parsers.
We hypothesize this isbecause CRFs are already discriminative (not joint, gen-erative) models; furthermore, this may suggest that futurediscriminative parsing methods will also have the benefitsof discriminative reranking built-in directly.AcknowledgmentsWe thank the reviewers for helpful suggestions and refer-ences.
This work was supported in part by the Center forIntelligent Information Retrieval, by the Advanced Researchand Development Activity under contract number MDA904-01-C-0984, by The Central Intelligence Agency, the Na-tional Security Agency and National Science Foundation un-der NSF grant #IIS-0326249, and by the Defense AdvancedResearch Projects Agency, through the Department of the Inte-rior, NBC, Acquisition Services Division, under contract num-ber NBCHD030010.ReferencesPaul N. Bennett, Susan T. Dumais, and Eric Horvitz.
2002.Probabilistic combination of text classifiers using reliabilityindicators: models and results.
In Proceedings of the 25thannual international ACM SIGIR conference on Researchand development in information retrieval, pages 207?214.ACM Press.Michael Collins.
2000.
Discriminative reranking for natu-ral language parsing.
In Proc.
17th International Conf.
onMachine Learning, pages 175?182.
Morgan Kaufmann, SanFrancisco, CA.Simona Gandrabur and George Foster.
2003.
Confidence esti-mation for text prediction.
In Proceedings of the Conferenceon Natural Language Learning (CoNLL 2003), Edmonton,Canada.A.
Gunawardana, H. Hon, and L. Jiang.
1998.
Word-basedacoustic confidence measures for large-vocabulary speechrecognition.
In Proc.
ICSLP-98, pages 791?794, Sydney,Australia.Trausti Kristjannson, Aron Culotta, Paul Viola, and AndrewMcCallum.
2004.
Interactive information extraction withconditional random fields.
To appear in Nineteenth NationalConference on Artificial Intelligence (AAAI 2004).John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.Conditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
18th Interna-tional Conf.
on Machine Learning, pages 282?289.
MorganKaufmann, San Francisco, CA.Andrew McCallum and David Jensen.
2003.
A note on theunification of information extraction and data mining usingconditional-probability, relational models.
In IJCAI03 Work-shop on Learning Statistical Models from Relational Data.Tobias Scheffer, Christian Decomain, and Stefan Wrobel.
2001.Active hidden markov models for information extraction.In Advances in Intelligent Data Analysis, 4th InternationalConference, IDA 2001.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003.
Max-margin markov networks.
In Proceedings of Neural Infor-mation Processing Systems Conference.Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.Mooney.
1999.
Active learning for natural language pars-ing and information extraction.
In Proc.
16th InternationalConf.
on Machine Learning, pages 406?414.
Morgan Kauf-mann, San Francisco, CA.
