A CLASS-BASED APPROACH TO LEX ICAL  D ISCOVERYPhi l ip Resnik*Depar tment  of Computer  and In format ion Science, Univers i ty  of Pennsy lvan iaPhi ladelphia,  Pennsy lvan ia  19104, USAInternet: vesnik@linc.cis.upenn.edu1 Int roduct ionIn this paper I propose a generalization of lexicalassociation techniques that is intended to facilitatestatistical discovery of facts involving word classesrather than individual words.
Although defining as-sociation measures over classes (as sets of words) isstraightforward in theory, making direct use of sucha definition is impractical because there are simplytoo many classes to consider.
Rather than consid-ering all possible classes, I propose constraining theset of possible word classes by using a broad-coveragelexical/conceptual hierarchy \[Miller, 1990\].2 Word /Word  Relat ionshipsMutual information is an information-theoretic mea-sure of association frequently used with natural an-guage data to gauge the "relatedness" between twowords z and y.
It is defined as follows:?
Pr(z, y) I(x;y) = log r (1) Pr(z)Pr(y)As an example of its use, consider Itindle's \[1990\]application of mutual information to the discoveryof predicate argument relations.
Hindle investigatesword co-occurrences a mediated by syntactic struc-ture.
A six-million-word sample of Associated Pressnews stories was parsed in order to construct a collec-tion of subject/verb/object instances.
On the basisof these data, Hindle calculates a co-occurrence score(an estimate of mutual information) for verb/objectpairs and verb/subject pairs.
Table 1 shows some ofthe verb/object pairs for the verb drink that occurredmore than once, ranked by co-occurrence score, "ineffect giving the answer to the question 'what can youdrink?'
" \[Hindle, 1990\], p. 270.Word/word relationships have proven useful, butare not appropriate for all applications.
For example,*This work was supported by the following grants: A l toDAAL 03-89-C-0031, DARPA N00014-90-J-1863, NSF IRI 90-16592, Ben Franklin 91S.3078C-1.
I am indebted to EricBrill, Henry Gleitman, Lila Gleitman, Aravind Joshi, Chris-tine Nakatani, and Michael Niv for helpful discussions, and toGeorge Miller and colleagues for making WordNet available.Co-occurrence score \[ verb \[ object11.75 drink tea11.75 drink Pepsi11.75 drink champagne10.53 drink liquid10.20 drink beer9.34 drink wineTable 1: High-scoring verb /ob jec t  pairs fordrink (part of Hindle 1990, Table 2).the selectional preferences of a verb constitute a re-lationship between a verb and a class of nouns ratherthan an individual noun.3 Word /C lass  Relat ionships3.1 A Measure  o f  Assoc ia t ionIn this section, I propose a method for discoveringclass-based relationships in text corpora on the ba-sis of mutual information, using for illustration theproblem of finding "prototypical" object classes forverbs.Let V = {vl,v~,...,vz} andAf = {nl ,n2, .
.
.
,nm}be the sets of verbs and nouns in a vocabulary, andC = {clc C_ Af} the set of noun classes; that is, thepower set of A f. Since the relationship being inves-tigated holds between verbs and classes of their ob-jects, the elementary events of interest are membersof V x C. The joint probability of a verb and a classis estimated asrtEc Pr(v,c)E E(2)u 'EV n~EJV "Given v E V, c E C, define the association scorePr( , c)A(v,c) ~ Pr(cl~ )log Pr(~)Pr(c) (3)= Pr(clv)I(v; c).
(4)The association score takes the mutual informationbetween the verb and a class, and scales it according327to the likelihood that a member of that class willactually appear as the object of the verb.
13.2 Coherent  ClassesA search among a verb's object nouns requires atmost I.A/" I computations of the association score, andcan thus be done exhaustively.
An exhaustive searchamong object classes is impractical, however, sincethe number of classes is exponential.
Clearly someway to constrain the search is needed.
I propose re-stricting the search by imposing a requirement of co-herence upon the classes to be considered.
For ex-ample, among possible classes of objects for open,the class {closet, locker, store} is more coherent than{closet, locker, discourse} on intuitive grounds: ev-ery noun in the former class describes a repositoryof some kind, whereas the latter class has no suchobvious interpretation.The WordNet lexical database \[Miller, 1990\] pro-vides one way to structure the space of noun classes,in order to make the search computationally feasi-ble.
WordNet is a lexical/conceptual database con-structed on psycholinguistic principles by GeorgeMiller and colleagues at Princeton University.
Al-though I cannot judge how well WordNet fares withregard to its psycholinguistic aims, its noun taxon-omy appears to have many of the qualities neededif it is to provide basic taxonomic knowledge for thepurpose of corpus-based research in English, includ-ing broad coverage and multiple word senses.Given the WordNet noun hierarchy, the definitionof "coherent class" adopted here is straightforward.Let words(w) be the set of nouns associated with aWordNet class w. 2Def in i t ion .
A noun class e ?
C is coher-ent iff there is a WordNet class w suchthat words(w) N A/" = c.I A(v,c) l verb \[ object class \[3.58 2.05 I drink drink \] /beverage' \[beverage .
.
.
.
\]~) { ( intoxicant,  \[alcohol .
.
.
.
JTable 2: Ob ject  classes for  drink4 Pre l iminary  Resu l tsAn experiment was performed in order to discover the"prototypical" object classes for a set of 115 commonEnglish verbs.
The counts of equation (2) were cal-culated by collecting a sample of verb/object pairsfrom the Brown corpus.
4 Direct objects were iden-tified using a set of heuristics to extract only thesurface object of the verb.
Verb inflections weremapped down to the base form and plural nounsmapped down to singular.
5 For example, the sen-tence John ate two shiny red apples would yield thepair (eat, apple).
The sentence These are the applesthat John ate would not provide a pair for eat, sinceapple does not appear as its surface object.Given each verb, v, the "prototypical" object classwas found by conducting a best-first search upwardsin the WordNet noun hierarchy, starting with Word-Net classes containing members that appeared as ob-jects of the verb.
Each WordNet class w consid-ered was evaluated by calculating A(v, {n E Afln Ewords(w)}).
Classes having too low a count (fewerthan five occurrences with the verb) were excludedfrom consideration.The results of this experiment are encouraging.Table 2 shows the object classes discovered for theverb drink (compare to Table 1), and Table 3 thehighest-scoring object classes for several other verbs.Recall from the definition in Section 3.2 that eachWordNet class w in the tables appears as an ab-breviation for {n ?
A/'ln ?
words(w)}; for example,( intoxicant,  \[alcohol .
.
.
.
\]) appears as an abbrevi-ation for {whisky, cognac, wine, beer}.As a consequence of this definition, noun classesthat are "too small" or "too large" to be coherent areexcluded, and the problem of search through an ex-ponentially large space of classes is reduced to searchwithin the WordNet hierarchy.
31 Scaling mutual information i  this fashion is often done;see, e.g., \[l:tosenfeld and Huang, 1992\].2Strictly speaking, WordNet as described by \[Miller,1990\] does not have classes, but rather lexical groupingscalled synonym sets.
By "WordNet class" I mean a pair(word, synonym-set ).ZA related possibility being investigated independently bPaul Kogut (personal communication) is assign to each nounand verb a vector of feature/value pairs based upon the word'sclassification i  the WordNet hierarchy, and to classify nounson the basis of their feature-value correspondences.5 Acqu is i t ion  o f  Verb  Proper t iesMore work is needed to improve the performance ofthe technique proposed here.
At the same time, theability to approximate a lexical/conceptual classifica-tion of nouns opens up a number of possible applica-tions in lexical acquisition.
What such applicationshave in common is the use of lexical associations asa window into semantic relationships.
The techniquedescribed in this paper provides a new, hierarchical4The version of the Brown corpus used was the tagged cor-pus found as part of the Penn Treebank.5Nouns outside the scope of WordNet that were tagged asproper names were mapped to the token pname, a subclass ofclasses (someone, \[person\] ) and ( location, \[ location\] ).328I A(v,c) I verb I object class1.94 ask0.16 call2.39 climb3.64 cook0.27 draw3.58 drink1.76 eat0.30 lose1.28 play2.48 pour1.03 pull1.23 push1.18 read2.69 sing(quest ion, \[question .
.
.
.
\] }someone, \[person .
.
.
.
\] }stair ,  \[step .
.
.
.
\] II repast, \[repast .... \] ) cord, \[cord .
.
.
.
\] }(beverage, \[beverage .
.
.
.
\] }<nutrient, \[food .
.
.
.
\] }<sensory-faculty, \[sense .
.
.
.
\] }(part, \[daaracter .
.
.
.
\])<liquid, \[liquid .... \] }(cover, \[coverin~ .
.
.
.
l}(button, \[button .
.
.
.
\]<writt en-mat eriai, \[writ in~ .
.
.
.
\] }(xusic, \ [~ ic  .
.
.
.
\])Table 3: Some "prototypical"  object classessource of semantic knowledge for statistical applica-tions.
This section briefly discusses one area wherethis kind of knowledge might be exploited.Diathesis alternations are variations in the waythat a verb syntactically expresses its arguments\[Levin, 1989\].
For example, l(a,b) shows an in-stance of the indefinite object alternation, and 2(a,b)shows an instance of the causative/inchoative alter-nation.1 a. John ate lunch.b.
John ate.2 a. John opened the door.b.
The door opened.Such phenomena re of particular interest in thestudy of how children learn the semantic and syn-tactic properties of verbs, because they stand at theborder of syntax and lexical semantics.
There are nu-merous possible explanations for why verbs fall intoparticular classes of alternations, ranging from sharedsemantic properties of verbs within a class, to prag-matic factors, to "lexieal idiosyncracy.
"Statistical techniques like the one described in thispaper may be useful in investigating relationships be-tween verbs and their arguments, with the goal ofcontributing data to the study of diathesis alterna-tions, and, ideally, in constructing a computationalmodel of verb acquisition.
For example, in the experi-ment described in Section 4, the verbs participating in"implicit object" alternations 6 appear to have higherassociation scores with their "prototypical" objectclasses than verbs for which implicit objects are dis-allowed.
Preliminary results, in fact, show a statis-tically significant difference between the two groups.eThe indefinite object alternation \[Levin, 1989\] and thespecified object alternation \[Cote, 1992\].Might such shared information-theoretic properties ofverbs play a role in their acquisition, in the same waythat shared semantic properties might?On a related topic, Grim_shaw has recently sug-gested that the syntactic bootstrapping hypothe-sis for verb acquisition \[Gleitman, 1991\] be ex-tended in such a way that alternations such as thecausative/inchoative alternation (e.g.
2(a,b)) arelearned using class information about the observedsubjects and objects of the verb, in addition to sub-categorization i formation.
7 I hope to extend thework on verb/object associations described here toother arguments of the verb in order to explore thissuggestion.6 Conc lus ionsThe technique proposed here provides a way to studystatistical associations beyond the level of individ-ual words, using a broad-coverage lexical/conceptualhierarchy to structure the space of possible nounclasses.
Preliminary results, on the task of discover-ing "prototypical" object classes for a set of commonEnglish verbs, appear encouraging, and applicationsin the study of verb argument structure are appar-ent.
In addition, assuming that the WordNet hier-archy (or some similar knowledge base) proves ap-propriately broad and consistent, he approach pro-posed here may provide a model for importing basictaxonomic knowledge into other corpus-based inves-tigations, ranging from computational lexicographyto statistical language modelling.References\[Cote, 1992\] Sharon Cote.
Discourse functions of twotypes of null objects in English.
Presented at the 66thAnnual Meeting of the Linguistic Society of America,Philadelphia, PA, January 1992.\[Gleitman, 1991\] Lila Gleitman.
The structural sourcesof verb meanings.
Language Acquisition, 1, 1991.\[Hindle, 1990\] Donald Hindle.
Noun classification frompredicate-argument structures.
In Proceedings of the~Sth Annual Meeting of the ACL, 1990.\[Levin, 1989\] Beth Levin.
Towards a lexical organizationof English verbs.
Technical report, Dept.
of Linguistics,Northwestern University, November 1989.\[Miller, 1990\] George Miller.
Wordnet: An on-line lexicaldatabase.
International Journal o\] Lexicography, 4(3),1990.
(Special Issue).\[Rosenfeld and Huang, 1992\] Ronald Rosenfeld and Xue-dong Huang.
Improvements in stochastic languagemodelling.
In Mitch Marcus, editor, Fifth DARPAWorkshop on Speech and Natural Language, February1992.
Arden House Conference Center, Harriman, NY.z Jane Grimshaw, keynote address, Lexicon AcquisitionWorkshop, University of Pennsylvania, J nuary, 1992.329
