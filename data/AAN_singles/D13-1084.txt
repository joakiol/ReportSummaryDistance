Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 857?862,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsWord Level Language Identification in Online Multilingual CommunicationDong Nguyen1 A. Seza Dog?ruo?z23(1) Human Media Interaction, University of Twente, Enschede, The Netherlands(2) Tilburg School of Humanities, Tilburg University, Tilburg, The Netherlands(3) Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USAdong.p.ng@gmail.com, a.s.dogruoz@gmail.comAbstractMultilingual speakers switch between lan-guages in online and spoken communication.Analyses of large scale multilingual data re-quire automatic language identification at theword level.
For our experiments with mul-tilingual online discussions, we first tag thelanguage of individual words using languagemodels and dictionaries.
Secondly, we incor-porate context to improve the performance.We achieve an accuracy of 98%.
Besides wordlevel accuracy, we use two new metrics toevaluate this task.1 IntroductionThere are more multilingual speakers in the worldthan monolingual speakers (Auer and Wei, 2007).Multilingual speakers switch across languages indaily communication (Auer, 1999).
With the in-creasing use of social media, multilingual speakersalso communicate with each other in online environ-ments (Paolillo, 2011).
Data from such resourcescan be used to study code switching patterns and lan-guage preferences in online multilingual conversa-tions.
Although most studies on multilingual onlinecommunication rely on manual identification of lan-guages in relatively small datasets (Danet and Her-ring, 2007; Androutsopoulos, 2007), there is a grow-ing demand for automatic language identification inlarger datasets.
Such a system would also be usefulfor selecting the right parsers to process multilingualdocuments and to build language resources for mi-nority languages (King and Abney, 2013).In this paper, we identify Dutch (NL) en Turkish(TR) at the word level in a large online forum forTurkish-Dutch speakers living in the Netherlands.The users in the forum frequently switch languageswithin posts, for example:<TR> Sariyi ver </TR><NL> Wel mooi doelpunt </NL>So far, language identification has mostly been mod-eled as a document classification problem.
Most ap-proaches rely on character or byte n-grams, by com-paring n-gram profiles (Cavnar and Trenkle, 1994),or using various machine learning classifiers.
WhileMcNamee (2005) argues that language identificationis a solved problem, classification on a more fine-grained level (instead of document level) remains achallenge (Hughes et al 2006).
Furthermore, lan-guage identification is more difficult for short texts(Baldwin and Lui, 2010; Vatanen et al 2010), suchas queries and tweets (Bergsma et al 2012; Carteret al 2012; Ceylan and Kim, 2009).
Tagging in-dividual words (without context) has been done us-ing dictionaries, affix statistics and classifiers us-ing character n-grams (Hammarstro?m, 2007; Got-tron and Lipka, 2010).
Although Yamaguchi andTanaka-Ishii (2012) segmented text by language,their data was artificially created by randomly sam-pling and concatenating text segments (40-160 char-acters) from monolingual texts.
Therefore, the lan-guage switches do not reflect realistic switches asthey occur in natural texts.
Most related to ours isthe work by King and Abney (2013) who labeledlanguages of words in multilingual web pages, butevaluated the task only using word level accuracy.857Our paper makes the following contributions: 1)We explore two new ways to evaluate the task for an-alyzing multilingual communication and show thatonly word accuracy gives a limited view 2) We arethe first to apply this task on a conversational andlarger dataset 3) We show that features using thecontext improve the performance 4) We present anew public dataset to support research on languageidentification.In the rest of the paper, we first discuss the relatedwork and describe our dataset.
Secondly, we presentour experiments.
We finally conclude with a sum-mary and suggestions for future work.2 CorpusOur data1 comes from one of the largest onlinecommunities in The Netherlands for Turkish-Dutchspeakers.
All posts from May 2006 until October2012 were crawled.
Although Dutch and Turkishdominate the forum, English fixed phrases (e.g.
nocomment, come on) are also occasionally observed.Users switch between languages within and acrossposts.
Examples 1 and 2 illustrate switches betweenDutch and Turkish within the same post.
Example 1is a switch at sentence level, example 2 is a switchat word level.Example 1:<NL>Mijn dag kan niet stuk :) </NL><TR> Cok guzel bir haber aldim </TR>Translation: <NL> This made my day:)</NL><TR> I received good news</TR>Example 2:<TR>kahvalti</TR><NL>metvriendinnen by my thuis </NL>Translation: <TR>breakfast </TR><NL> with my girlfriends at my home</NL>The data is highly informal with misspellings,lengthening of characters (e.g.
hotttt), replacementof Turkish characters (kahvalti instead of kahvalt?
)and spelling variations (tankyu instead of thankyou).
Dutch and Turkish sometimes share commonspellings (e.g.
ben is am in Dutch and I in Turkish),making this a challenging task.1Available at http://www.dongnguyen.nl/data-langid-emnlp2013.htmlAnnotationFor this research, we classify words as either Turkishor Dutch.
Since Dutch and English are typologicallymore similar to each other than Turkish, the Englishphrases (less than 1%) are classified as Dutch.
Postswere randomly sampled and annotated by a nativeTurkish speaker who is also fluent in Dutch.
A na-tive Dutch speaker annotated a random set of 100posts (Cohen?s kappa = 0.98).
The following tokenswere ignored for language identification:?
Smileys (as part of the forum markup, as wellas textual smileys such as ?:)?
).?
Numeric tokens and punctuation.?
Forum tags (e.g.
[u] to underline text).?
Links, images, embedded videos etc.?
Turkish and Dutch first names and placenames2.?
Usernames when indicated with special forummarkup.?
Chat words, such as hahaha, ooooh and lol rec-ognized using regular expressions.Posts for which all tokens are ignored, are notincluded in the corpus.StatisticsThe dataset was randomly divided into a training,development and test set.
The statistics are listedin Table 1.
The statistics show that Dutch is themajority language, although the difference betweenTurkish and Dutch is not large.
We also find that thedocuments (i.e.
posts) are short, with on average 18tokens per document.
The data represents realistictexts found in online multilingual communication.Compared to previously used datasets (Yamaguchiand Tanaka-Ishii, 2012; King and Abney, 2013), thedata is noisier and the documents are much shorter.#NL tokens #TR tokens #Posts/(BL%)Train 14900 (54%) 12737 (46%) 1603 (15%)Dev 8590 (51%) 8140 (49%) 728 (19%)Test 5895 (53%) 5293 (47%) 735 (17%)Table 1: Number of tokens and posts for Dutch (NL) andTurkish (TR), including % of bilingual (BL) posts2Based on online name lists and Wikipedia pages8583 Experimental Setup3.1 Training CorporaWe used the following corpora to extract dictionariesand language models.?
GenCor: Turkish web pages (Sak et al 2008).?
NLCOW2012: Dutch web pages (Scha?fer andBildhauer, 2012).?
Blog authorship corpus: English blogs (Schleret al 2006).Each corpus was chunked into large segmentswhich were then selected randomly until 5M tokenswere obtained for each language.
We tokenized thetext and kept the punctuation.3.2 BaselinesAs baselines, we use langid.py3 (Lui and Bald-win, 2012) and van Noord?s TextCat implementa-tion4 of the algorithm by Cavnar and Trenkle (1994).TextCat is based on the comparison of n-gram pro-files and langid.py on Naive Bayes with n-gram fea-tures.
For both baselines, words were entered indi-vidually to each program.
Words for which no lan-guage could be determined were assigned to Dutch.These models were developed to identify the lan-guages of the documents instead of words and wedid not retrain them.
Therefore, these models arenot expected to perform well on this task.3.3 ModelsWe start with models that assign languages based ononly the current word.
Next, we explore models andfeatures that can exploit the context (the other wordsin the post).
Words with the highest probability forEnglish were assigned to Dutch for evaluation.Dictionary lookup (DICT)We extract dictionaries with word frequencies fromthe training corpora.
This approach looks up thewords in the dictionaries and chooses the languagefor which the word has the highest probability.
Ifthe word does not occur in the dictionaries, Dutch ischosen as the language.3https://github.com/saffsd/langid.py4http://www.let.rug.nl/?vannoord/TextCat/Language model (LM)We build a character n-gram language model foreach language (max.
n-gram length is 5).
We useWitten-Bell smoothing and include word boundariesfor calculating the probabilities.Dictionary + Language model (DICT+LM)We first use the dictionary lookup approach (DICT).If the word does not occur in dictionaries, a decisionis made using the language models (LM).Logistic Regression (LR)We use a logistic regression model that incorporatescontext with the following features:?
(Individual word) Label assigned by theDICT+LM model.?
(Context) The results of the LM model based onprevious + current token, and current token +next token (e.g.
the sequence ?ben thuis?
(amhome) as a whole if ben is the current token).This gives the language model more context forestimation.
We compare the use of the assignedlabels (LAB) with the use of the log probabilityvalues (PROB) as feature values.Conditional Random Fields (CRF)We treat the task as a sequence labeling problem andexperiment with linear-chain Conditional RandomFields (Lafferty et al 2001) in three settings:?
(Individual word) A CRF with only the tags as-signed by the DICT+LM to the individual to-kens as a feature (BASE).?
(Context).
CRFs using the LAB or PROB as ad-ditional features (same features as in the logis-tic regression model) to capture additional con-text.3.4 ImplementationLanguage identification was not performed for textswithin quotes.
To handle the alphabetical length-ening (e.g.
lolllll), words are normalized by trim-ming same character sequences of three charactersor more.
We use the Lingpipe5 and Scikit-learn (Pe-dregosa et al 2011) toolkits for our experiments.5http://alias-i.com/lingpipe/859Word classification Fraction Post classificationTR NL MAERun P R P R Acc.
?
All Mono.
BL F1 Acc.Textcat 0.872 0.647 0.743 0.915 0.788 0.739 0.251 0.264 0.188 0.386 0.396LangIDPy 0.954 0.387 0.641 0.983 0.701 0.615 0.364 0.371 0.333 0.413 0.475DICT 0.955 0.733 0.802 0.969 0.858 0.827 0.196 0.200 0.175 0.511 0.531LM 0.950 0.930 0.938 0.956 0.944 0.926 0.074 0.076 0.065 0.699 0.703DICT + LM 0.951 0.934 0.942 0.957 0.946 0.943 0.067 0.067 0.063 0.711 0.717LR + LAB 0.965 0.952 0.958 0.969 0.961 0.917 0.066 0.066 0.068 0.791 0.808LR + PROB 0.956 0.976 0.978 0.959 0.967 0.945 0.048 0.044 0.064 0.826 0.849CRF + BASE 0.973 0.974 0.977 0.976 0.975 0.940 0.043 0.027 0.119 0.858 0.898CRF + LAB 0.964 0.977 0.979 0.967 0.972 0.933 0.046 0.033 0.111 0.855 0.891CRF + PROB 0.970 0.980 0.982 0.973 0.976 0.946 0.039 0.025 0.103 0.853 0.895Table 2: Results of language identification experiments.3.5 EvaluationThe assigned labels can be used for computationalanalysis of multilingual data in different ways.
Forexample, these labels can be used to analyze lan-guage preferences in multilingual communication orthe direction of the switches (from Turkish to Dutchor the other way around).
Therefore, we evaluate themethods from different perspectives.The evaluation at word and post levels is donewith the following metrics:?
Word classification precision (P), recall (R) andaccuracy.
Although this is the most straightfor-ward approach to evaluate the task, it ignoresthe document boundaries.?
Fraction of language in a post: Pearson?s cor-relation (?)
and Mean Absolute Error (MAE) ofproportion of Turkish in a post.
This evaluatesthe measured proportion of languages in a postwhen the actual tags for individual words arenot needed.
For example, such information isuseful for analyzing the language preferencesof users in the online forum.
Besides report-ing the MAE over all posts, we also separatethe performance over monolingual and bilin-gual posts (BL).?
Post classification: Durham (2003) analyzedthe switch between languages in terms of theamount of monolingual and bilingual posts.Our posts are classified as NL, TR or bilingual(BL) if all words are tagged in the particularlanguage or both.
We report F1 and accuracy.4 ResultsThe results are presented in Table 2.
Significancetests were done by comparing the results of the wordand post classification measures using McNemar?stest, and comparing the MAEs using paired t-tests.All runs were significantly different from each otherbased on these tests (p < 0.05), except the MAEs ofthe DICT+LM and LR+LAB runs and the MAEs andpost classification metrics between the CRFs runs.The difficulty of the task is illustrated by exam-ining the coverage of the tokens by the dictionaries.24.6% of the tokens (dev + test set) appear in bothdictionaries, 31.1% only in the Turkish dictionary,30.5% only in the Dutch dictionary and 13.9% innone of the dictionaries.The baselines do not perform well.
This confirmsthat language identification at the word level needsdifferent approaches than identification at the docu-ment level.
Using language models result in a bet-ter performance than dictionaries.
They can han-dle unseen words and are more robust against thenoisy spellings.
The combination of language mod-els and dictionaries is more effective than the indi-vidual models.
The results improve when contextwas added using a logistic regression model, espe-cially with the probability values as feature values.CRFs improve the results but the improvementon the correlation and MAE is less.
More specifi-cally, CRFs improve the performance on monolin-gual posts, especially when a single word is taggedin the wrong language.
However, when the influenceof the context is too high, CRFs reduce the perfor-mance in bilingual posts.860This is also illustrated with the results of the postclassification.
The LR+PROB run has a high recall(0.905), but a low precision (0.559) for bilingualposts, while the CRF+PROB approach has a low re-call (0.611) and a high precision (0.828).The fraction of Dutch and Turkish in posts varieswidely, providing additional challenges to the use ofCRFs for this task.
Classifying posts first as mono-lingual/bilingual and tagging individual words after-wards for bilingual posts might improve the perfor-mance.The evaluation metrics highlight different aspectsof the task whereas word level accuracy gives alimited view.
We suggest using multiple metrics toevaluate this task for future research.Dictionaries versus Language ModelsThe results reported in Table 2 were obtained bysampling 5M tokens of each language.
To study theeffect of the number of tokens on the performanceof the DICT and LM runs, we vary the amount ofdata.
The performance of both methods increasesconsistently with more data (Figure 1).
We alsofind that language models achieve good performancewith only a limited amount of data, and consistentlyoutperform the approach using dictionaries.
This isprobably due to the highly informal and noisy natureof our data.Num.
sampled tokensAccuracy0.60.70.80.91.00 2 ?
1064 ?
106LMDICTFigure 1: Effect of sampling sizePost classificationWe experimented with classifying posts into TR, NLand bilingual using the results of the word level lan-guage identification (Table 2: post classification).Posts were classified as a particular language if allwords were tagged as belonging to that language,and bilingual otherwise.
Runs using CRFs achievedthe best performance.We now experiment with allowing a margin (e.g.a margin of 0.10 classifies posts as TR if at least90% of the words are classified as TR).
Allowinga small margin already increases the results of sim-pler approaches (such as the LR-PROB run, Table 3)by making it more robust against errors.
However,allowing a margin reduces the performance of theCRF runs.Margin 0.0 0.05 0.10 0.15 0.20Accuracy 0.849 0.873 0.876 0.878 0.865Table 3: Effect of margin on post classification(LR-PROB run)Error analysisThe manual analysis of the results revealed threemain challenges: 1) Our data is highly informalwith many spelling variations (e.g.
moimoimoi,goooooooooooolllll) and noise (e.g.
asdfghjfgsha-haha) 2) Words sharing spelling in Dutch and Turk-ish are difficult to identify especially when thereis no context available (e.g.
a post with only oneword).
These words are annotated based on theircontext.
For example, the word super in ?Seyma,super?
is annotated as Turkish since Seyma is also aTurkish word.
3) Named entity recognition is neces-sary to improve the performance of the system anddecrease the noise in evaluation.
Based on precom-piled lists, our system ignores named entities.
How-ever, some names still remain undetected (e.g.
user-names).5 ConclusionWe presented experiments on identifying the lan-guage of individual words in multilingual conversa-tional data.
Our results reveal that language modelsare more robust than dictionaries and adding contextimproves the performance.
We evaluate our methodsfrom different perspectives based on how languageidentification at word level can be used to analyzemultilingual data.
The highly informal spelling inonline environments and the occurrences of namedentities pose challenges.Future work could focus on cases with more thantwo languages, and languages that are typologicallyless distinct from each other or dialects (Trieschnigget al 2012).8616 AcknowledgementsThe first author was supported by the NetherlandsOrganization for Scientific Research (NWO) grant640.005.002 (FACT) and the second author througha postdoctoral research grant in E-Humanities (Digi-tal Humanities) by Tilburg University (NL).
The au-thors would like to thank Marie?t Theune and DolfTrieschnigg for feedback.ReferencesJ.
Androutsopoulos, 2007.
The multilingual internet.Language, Culture and communication online, chapterLanguage choice and code-switching in German-baseddiasporic web-forums., pages 340?361.
Oxford: Ox-ford University Press.P.
Auer and L. Wei.
2007.
Introduction: Multilingual-ism as a problem?
Monolingualism as a problem?
InHandbook of Multilingualism and Multilingual Com-munication, volume 5 of Handbooks of Applied Lin-guistics, pages 1?14.
Mouton de Gruyter.P.
Auer.
1999.
From codeswitching via language mix-ing to fused lects toward a dynamic typology of bilin-gual speech.
International Journal of Bilingualism,3(4):309?332.T.
Baldwin and M. Lui.
2010.
Language identification:the long and the short of the matter.
In Proceedings ofNAACL 2010.S.
Bergsma, P. McNamee, M. Bagdouri, C. Fink, andT.
Wilson.
2012.
Language identification for creat-ing language-specific twitter collections.
In Proceed-ings of the Second Workshop on Language in SocialMedia.S.
Carter, W. Weerkamp, and M. Tsagkias.
2012.
Mi-croblog language identification: Overcoming the limi-tations of short, unedited and idiomatic text.
LanguageResources and Evaluation, pages 1?21.W.B.
Cavnar and J. M. Trenkle.
1994.
N-gram-basedtext categorization.
In Proceedings of Third AnnualSymposium on Document Analysis and InformationRetrieval.H.
Ceylan and Y. Kim.
2009.
Language identification ofsearch engine queries.
In Proceedings of ACL 2009.B.
Danet and S. C. Herring.
2007.
The multilingual In-ternet: Language, culture, and communication online.Oxford University Press Oxford.M.
Durham.
2003.
Language choice on a Swiss mailinglist.
Journal of Computer-Mediated Communication,9(1).T.
Gottron and N. Lipka.
2010.
A comparison of lan-guage identification approaches on short, query-styletexts.
In Proceedings of ECIR 2010.H.
Hammarstro?m.
2007.
A fine-grained model for lan-guage identification.
In Proceedings of iNEWS-07Workshop at SIGIR 2007.B.
Hughes, T. Baldwin, S. Bird, J. Nicholson, andA.
Mackinlay.
2006.
Reconsidering language identifi-cation for written language resources.
In Proceedingsof LREC 2006.B.
King and S. Abney.
2013.
Labeling the languagesof words in mixed-language documents using weaklysupervised methods.
In Proceedings of NAACL 2013.J.
Lafferty, A. McCallum, and F. C. N. Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proceed-ings of ICML 2001.M.
Lui and T. Baldwin.
2012. langid.py: an off-the-shelflanguage identification tool.
In Proceedings of ACL2012.P.
McNamee.
2005.
Language identification: a solvedproblem suitable for undergraduate instruction.
Jour-nal of Computing Sciences in Colleges, 20(3):94?101.J.C.
Paolillo.
2011.
?Conversational?
codeswitching onUsenet and Internet Relay Chat.
Language@Internet,8(3).F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.H.
Sak, T. Gu?ngo?r, and M. Sarac?lar.
2008.
Turkish lan-guage resources: Morphological parser, morphologi-cal disambiguator and web corpus.
In GoTAL 2008,volume 5221 of LNCS, pages 417?427.
Springer.R.
Scha?fer and F. Bildhauer.
2012.
Building large cor-pora from the web using a new efficient tool chain.
InProceedings of LREC 2012.J.
Schler, M. Koppel, S. Argamon, and J. Pennebaker.2006.
Effects of age and gender on blogging.
In Pro-ceedings of 2006 AAAI Spring Symposium on Compu-tational Approaches for Analyzing Weblogs.D.
Trieschnigg, D. Hiemstra, M. Theune, F. Jong, andT.
Meder.
2012.
An exploration of language identifi-cation techniques for the Dutch folktale database.
InAdaptation of Language Resources and Tools for Pro-cessing Cultural Heritage workshop (LREC 2012).T.
Vatanen, J. J. Va?yrynen, and S. Virpioja.
2010.
Lan-guage identification of short text segments with n-gram models.
In Proceedings of LREC 2010.H.
Yamaguchi and K. Tanaka-Ishii.
2012.
Text segmen-tation by language using minimum description length.In Proceedings of ACL 2012.862
