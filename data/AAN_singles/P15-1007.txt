Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 63?73,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsMultiGranCNN: An Architecture for General Matching of Text Chunkson Multiple Levels of GranularityWenpeng Yin and Hinrich Sch?utzeCenter for Information and Language ProcessingUniversity of Munich, Germanywenpeng@cis.uni-muenchen.deAbstractWe present MultiGranCNN, a generaldeep learning architecture for matchingtext chunks.
MultiGranCNN supportsmultigranular comparability of represen-tations: shorter sequences in one chunkcan be directly compared to longer se-quences in the other chunk.
Multi-GranCNN also contains a flexible andmodularized match feature componentthat is easily adaptable to different typesof chunk matching.
We demonstrate state-of-the-art performance of MultiGranCNNon clause coherence and paraphrase iden-tification tasks.1 IntroductionMany natural language processing (NLP) taskscan be posed as classifying the relationship be-tween two TEXTCHUNKS (cf.
Li et al (2012),Bordes et al (2014b)) where a TEXTCHUNK canbe a sentence, a clause, a paragraph or any othersequence of words that forms a unit.Paraphrasing (Figure 1, top) is one task that weaddress in this paper and that can be formalizedas classifying a TEXTCHUNK relation.
The twoclasses correspond to the sentences being (e.g.,the pair <p, q+>) or not being (e.g., the pair<p, q?>) paraphrases of each other.
Anothertask we look at is clause coherence (Figure 1, bot-tom).
Here the two TEXTCHUNK relation classescorrespond to the second clause being (e.g., thepair <x, y+>) or not being (e.g., the pair <x,y?>) a discourse-coherent continuation of thefirst clause.
Other tasks that can be formalizedas TEXTCHUNK relations are question answering(QA) (is the second chunk an answer to the first?
),textual inference (does the first chunk imply thesecond?)
and machine translation (are the twochunks translations of each other?
).pPDC will also almost certainly fan the flames ofspeculation about Longhorn?s release.q+PDC will also almost certainly reignite speculationabout release dates of Microsoft ?s new products.q?PDC is indifferent to the release of Longhorn.x The dollar suffered its worst one-day loss in a month,y+falling to 1.7717 marks .
.
.
from 1.7925 marks yesterday.y?up from 112.78 yen in late New York trading yesterday.Figure 1: Examples for paraphrasing and clausecoherence tasksIn this paper, we present MultiGranCNN, a gen-eral architecture for TEXTCHUNK relation classi-fication.
MultiGranCNN can be applied to a broadrange of different TEXTCHUNK relations.
This isa challenge because natural language has a com-plex structure ?
both sequential and hierarchical ?and because this structure is usually not parallelin the two chunks that must be matched, furtherincreasing the difficulty of the task.
A successfuldetection algorithm therefore needs to capture notonly the internal structure of TEXTCHUNKS, butalso the rich pattern of their interactions.MultiGranCNN is based on two innovationsthat are critical for successful TEXTCHUNK re-lation classification.
First, the architecture is de-signed to ensure multigranular comparability.
Forgeneral matching, we need the ability to matchshort sequences in one chunk with long sequencesin the other chunk.
For example, what is expressedby a single word in one chunk (?reignite?
in q+in the figure) may be expressed by a sequence ofseveral words in its paraphrase (?fan the flamesof?
in p).
To meet this objective, we learn rep-resentations for words, phrases and the entire sen-tence that are all mutually comparable; in particu-lar, these representations all have the same dimen-sionality and live in the same space.Most prior work (e.g., Blacoe and Lapata (2012;Hu et al (2014)) has neglected the need for multi-granular comparability and performed matchingwithin fixed levels only, e.g., only words were63matched with words or only sentences with sen-tences.
For a general solution to the problem ofmatching, we instead need the ability to match aunit on a lower level of granularity in one chunkwith a unit on a higher level of granularity in theother chunk.
Unlike (Socher et al, 2011), ourmodel does not rely on parsing and it can more ex-haustively search the hypothesis space of possiblematchings, including matchings that correspond toconflicting segmentations of the input chunks (seeSection 5).Our second contribution is that MultiGranCNNcontains a flexible and modularized match featurecomponent.
This component computes the ba-sic features that measure how well phrases of thetwo chunks match.
We investigate three differentmatch feature models that demonstrate that a widevariety of different match feature models can beimplemented.
The match feature models can beswapped in and out of MultiGranCNN, dependingon the characteristics of the task to be solved.Prior work that has addressed matching taskshas usually focused on a single task like QA (Bor-des et al, 2014a; Yu et al, 2014) or paraphrasing(Socher et al, 2011; Madnani et al, 2012; Ji andEisenstein, 2013).
The ARC architectures pro-posed by Hu et al (2014) are intended to be moregeneral, but seem to be somewhat limited in theirflexibility to model different matching relations;e.g., they do not perform well for paraphrasing.Different match feature models may also be re-quired by factors other than the characteristics ofthe task.
If the amount of labeled training data issmall, then we may prefer a match feature modelwith few parameters that is robust against overfit-ting.
If there is lots of training data, then a richermatch feature model may be the right choice.This motivates the need for an architecture likeMultiGranCNN that allows selection of the task-appropriate match feature model from a range ofdifferent models and its seamless integration intothe architecture.In remaining parts, Section 2 introduces somerelated work; Section 3 gives an overview of theproposed MultiGranCNN; Section 4 shows how tolearn representations for generalized phrases (g-phrases); Section 5 describes the three matchingmodels: DIRECTSIM, INDIRECTSIM and CON-CAT; Section 6 describes the two 2D poolingmethods: grid-based pooling and phrase-basedpooling; Section 7 describes the match featureCNN; Section 8 summarizes the architecture ofMultiGran CNN; and Section 9 presents experi-ments; finally, Section 10 concludes.2 Related WorkParaphrase identification (PI) is a typical task ofsentence matching and it has been frequently stud-ied (Qiu et al, 2006; Blacoe and Lapata, 2012;Madnani et al, 2012; Ji and Eisenstein, 2013).Socher et al (2011) utilized parsing to model thehierarchical structure of sentences and uses un-folding recursive autoencoders to learn represen-tations for single words and phrases acting as non-leaf nodes in the tree.
The main difference toMultiGranCNN is that we stack multiple convo-lution layers to model flexible phrases and learnrepresentations for them, and aim to address moregeneral sentence correspondence.
Bach et al(2014) claimed that elementary discourse units ob-tained by segmenting sentences play an importantrole in paraphrasing.
Their conclusion also en-dorses (Socher et al, 2011)?s and our work, forboth take interactions between component phrasesinto account.QA is another representative sentence matchingproblem.
Yu et al (2014) modeled sentence rep-resentations in a simplified CNN, finally findingthe match score by projecting question and answercandidates into the same space.
Other relevant QAwork includes (Bordes et al, 2014c; Bordes et al,2014a; Yang et al, 2014; Iyyer et al, 2014)For more general matching, Chopra et al (2005)and Liu (2013) used a Siamese architecture ofshared-weight neural networks (NNs) to modeltwo objects simultaneously, matching their repre-sentations and then learning a specific type of sen-tence relation.
We adopt parts of their architec-ture, but we model phrase representations as wellas sentence representations.Li and Xu (2012) gave a comprehensive intro-duction to query-document matching and arguedthat query and document match at different levels:term, phrase, word sense, topic, structure etc.
Thisalso applies to sentence matching.Lu and Li (2013) addressed matching of shorttexts.
Interactions between the two texts were ob-tained via LDA (Blei et al, 2003) and were thenthe basis for computing a matching score.
Com-pared to MultiGranCNN, drawbacks of this ap-proach are that LDA parameters are not optimizedfor the specific task and that the interactions are64formed on the level of single words only.Gao et al (2014) modeled interestingness be-tween two documents with deep NNs.
Theymapped source-target document pairs to featurevectors in a latent space in such a way that the dis-tance between the source document and its corre-sponding interesting target in that space was min-imized.
Interestingness is more like topic rele-vance, based mainly on the aggregated meaningof keywords, as opposed to more structural rela-tionships as is the case for paraphrasing and clausecoherence.We briefly discussed (Hu et al, 2014)?s ARC inSection 1.
MultiGranCNN is partially inspired byARC, but introduces multigranular comparability(thus enabling crosslevel matching) and supportsa wider range of match feature models.Our unsupervised learning component (Sec-tion 4, last paragraph) resembles word2vecCBOW (Mikolov et al, 2013), but learns repre-sentations of TEXTCHUNKS as well as words.
Italso resembles PV-DM (Le and Mikolov, 2014),but our TEXTCHUNK representation is derived us-ing a hierarchical architecture based on convolu-tion and pooling.3 Overview of MultiGranCNNWe use convolution-plus-pooling in two differ-ent components of MultiGranCNN.
The first com-ponent, the generalized phrase CNN (gpCNN),will be introduced in Section 4.
This componentlearns representations for generalized phrases (g-phrases) where a generalized phrase is a generalterm for subsequences of all granularities: words,short phrases, long phrases and the sentence itself.The gpCNN architecture has L layers of convolu-tion, corresponding (for L = 2) to words, shortphrases, long phrases and the sentence.
We testdifferent values of L in our experiments.
We traingpCNN on large data in an unsupervised mannerand then fine-tune it on labeled training data.Using a Siamese configuration, two copiesof gpCNN, one for each of the two inputTEXTCHUNKS, are the input to the match featuremodel, presented in Section 5.
This model pro-duces s1?
s2matching features, one for each pairof g-phrases in the two chunks, where s1, s2arethe number of g-phrases in the two chunks, respec-tively.The s1?s2match feature matrix is first reducedto a fixed size by dynamic 2D pooling.
The re-sulting fixed size matrix is then the input to thesecond convolution-plus-pooling component, thematch feature CNN (mfCNN) whose output is fedto a multilayer perceptron (MLP) that producesthe final match score.
Section 6 will give details.We use convolution-plus-pooling for both wordsequences and match features because we want tocompute increasingly abstract features at multiplelevels of granularity.
To ensure that g-phrases aremutually comparable when computing the s1?
s2match feature matrix, we impose the constraintthat all g-phrase representations live in the samespace and have the same dimensionality.Figure 2: gpCNN: learning g-phrase representa-tions.
This figure only shows two convolution lay-ers (i.e., L = 2) for saving space.4 gpCNN: Learning Representations forg-PhrasesWe use several stacked blocks, i.e., convolution-plus-pooling layers, to extract increasingly ab-stract features of the TEXTCHUNK.
The input tothe first block are the words of the TEXTCHUNK,represented by CW (Collobert and Weston, 2008)embeddings.
Given a TEXTCHUNK of length |S|,let vector ci?
Rwdbe the concatenated embed-dings of words vi?w+1, .
.
.
, viwhere w = 5 is thefilter width, d = 50 is the dimensionality of CWembeddings and 0 < i < |S| + w. Embeddingsfor words vi, i < 1 and i > |S|, are set to zero.We then generate the representation pi?
Rdofthe g-phrase vi?w+1, .
.
.
, viusing the convolution65matrix Wl?
Rd?wd:pi= tanh(Wlci+ bl) (1)where block index l = 1, bias bl?
Rd.
We usewide convolution (i.e., we apply the convolutionmatrix Wlto words vi, i < 1 and i > |S|) becausethis makes sure that each word vi, 1 ?
i ?
|S|,can be detected by all weights of Wl?
as opposedto only the rightmost (resp.
leftmost) weights forinitial (resp.
final) words in narrow convolution.The configuration of convolution layers in fol-lowing blocks (l > 1) is exactly the same exceptthat the input vectors ciare not words, but the out-put of pooling from the previous layer of convo-lution ?
as we will explain presently.
The con-figuration is the same (e.g., all Wl?
Rd?wd) be-cause, by design, all g-phrase representations havethe same dimensionality d. This also ensures thateach g-phrase representation can be directly com-pared with each other g-phrase representation.We use dynamic k-max pooling to extract the kltop values from each dimension after convolutionin the lthblock and the kLtop values in the finalblock.
We setkl= max(?, dL?
lL|S|e) (2)where l = 1, ?
?
?
, L is the block index, and ?
= 4is a constant (cf.
Kalchbrenner et al (2014)) thatmakes sure a reasonable minimum number of val-ues is passed on to the next layer.
We set kL= 1(not 4, cf.
Kalchbrenner et al (2014)) because ourdesign dictates that all g-phrase representations,including the representation of the TEXTCHUNKitself, have the same dimensionality.
Example: forL = 4, |S| = 20, the kiare [15, 10, 5, 1].Dynamic k-max pooling keeps the most impor-tant features and allows us to stack multiple blocksto extract hiearchical features: units on consec-utive layers correspond to larger and larger partsof the TEXTCHUNK thanks to the subset selectionproperty of pooling.For many tasks, labeled data for traininggpCNN is limited.
We therefore employ unsu-pervised training to initialize gpCNN as shown inFigure 2.
Similar to CBOW (Mikolov et al, 2013),we predict a sampled middle word vifrom the av-erage of seven vectors: the TEXTCHUNK repre-sentation (the final output of gpCNN) and the threewords to the left and to the right of vi.
We usenoise-contrastive estimation (Mnih and Teh, 2012)for training: 10 noise words are sampled for eachtrue example.Figure 3: General illustration of match featuremodel.
In this example, both S1and S2have 10 g-phrases, so the match feature matrix?F ?
Rs1?s2has size 10?
10.5 Match Feature ModelsLet g1, .
.
.
, gskbe an enumeration of the skg-phrases of TEXTCHUNK Sk.
Let Sk?
Rsk?dbethe matrix, constructed by concatenating the fourmatrices of unigram, short phrase, long phrase andsentence representations shown in Figure 2 thatcontain the learned representations from Section 4for these skg-phrases; i.e., row Skiis the learnedrepresentation of gi.The basic design of a match feature model isthat we produce an s1?
s2matrix?F for a pairof TEXTCHUNKS S1and S2, shown in Figure 3.?Fi,jis a score that assesses the relationship be-tween g-phrase giof S1and g-phrase gjof S2with respect to the TEXTCHUNK relation of in-terest (paraphrasing, clause coherence etc).
Thisscore?Fi,jis computed based on the vector repre-sentations S1iand S2jof the two g-phrases.1We experiment with three different featuremodels to compute the match score?Fi,jbecausewe would like our architecture to address a widevariety of different TEXTCHUNK relations.
Wecan model a TEXTCHUNK relation like paraphras-ing as ?for each meaning element in one sentence,there must be a similar meaning element in theother sentence?
; thus, a good candidate for thematch score?Fi,jis simply vector similarity.
Incontrast, similarity is a less promising match scorefor clause coherence; for clause coherence, wewant a score that models how good a continuationone g-phrase is for the other.
These considerationsmotivate us to define three different match featuremodels that we will introduce now.The first match feature model is DIRECTSIM.1In response to a reviewer question, recall that siis thetotal number of g-phrases of Si, so there is only one s1?
s2matrix, not several on different levels of granularity.66Figure 4: CONCAT match feature modelThis model computes the match score of two g-phrases as their similarity using a radial basisfunction kernel:?Fi,j= exp(?||S1i?
S2j||22?)
(3)where we set ?
= 2 (cf.
Wu et al (2013)).DIRECTSIM is an appropriate feature model forTEXTCHUNK relations like paraphrasing becausein that case direct similarity features are helpful inassessing meaning equivalence.The second match feature model is INDIRECT-SIM.
Instead of computing the similarity di-rectly as we do for DIRECTSIM, we first trans-form the representation of the g-phrase in oneTEXTCHUNK using a transformation matrix M ?Rd?d, then compute the match score by innerproduct and sigmoid activation:?Fi,j= ?
(S1iMST2j+ b), (4)Our motivation is that for a TEXTCHUNK rela-tion like clause coherence, the two TEXTCHUNKSneed not have any direct similarity.
However, if wemap the representations of TEXTCHUNK S1intoan appropriate space then we can hope that sim-ilarity between these transformed representationsof S1and the representations of TEXTCHUNK S2do yield useful features.
We will see that this hopeis borne out by our experiments.The third match feature model is CONCAT.
Thisis a general model that can learn any weightedcombination of the values of the two vectors:?Fi,j= ?
(wTei,j+ b) (5)where ei,j?
R2dis the concatenation of S1iandS2j.
We can learn different combination weightsw to solve different types of TEXTCHUNK match-ing.We call this match feature model CONCAT be-cause we implement it by concatenating g-phrasevectors to form a tensor as shown in Figure 4.The match feature models implement multi-granular comparability: they match all units inone TEXTCHUNK with all units in the otherTEXTCHUNK.
This is necessary because a gen-eral solution to matching must match a low-levelunit like ?reignite?
to a higher-level unit like ?fanthe flames of?
(Figure 1).
Unlike (Socher et al,2011), our model does not rely on parsing; there-fore, it can more exhaustively search the hypoth-esis space of possible matchings: mfCNN coversa wide variety of different, possibly overlappingunits, not just those of a single parse tree.6 Dynamic 2D PoolingThe match feature models generate an s1?s2ma-trix.
Since it has variable size, we apply two dif-ferent dynamic 2D pooling methods, grid-basedpooling and phrase-focused pooling, to transformit to a fixed size matrix.6.1 Grid-based poolingWe need to map?F ?
Rs1?s2into a matrix F offixed size s??
s?where s?is a parameter.
Grid-based pooling divides?F into s??
s?nonover-lapping (dynamic) pools and copies the maximumvalue in each dynamic pool to F. This method issimilar to (Socher et al, 2011), but preserves lo-cality better.
?F can be split into equal regions only if both s1and s2are divisible by s?.
Otherwise, for s1> s?and if s1mod s?= b, the dynamic pools in thefirst s??
b splits each have?s1s?
?rows while theremaining b splits each have?s1s?
?+ 1 rows.
InFigure 5, a s1?
s2= 4 ?
5 matrix (left) is splitinto s?
?s?= 3?3 dynamic pools (middle): eachrow is split into [1, 1, 2] and each column is splitinto [1, 2, 2].If s1< s?, we first repeat all rows in batch stylewith size s1until no fewer than s?rows remain.Then the first s?rows are kept and split into s?dynamic pools.
The same principle applies to thepartitioning of columns.
In Figure 5 (right), the ar-eas with dashed lines and dotted lines are repeatedparts for rows and columns, respectively; each cellis its own dynamic pool.6.2 Phrase-focused poolingIn the match feature matrix?F ?
Rs1?s2, row i(resp.
column j) contains all feature values for g-phrase giof S1(resp.
gjof S2).
Phrase-focusedpooling attempts to pick the largest match features67Figure 5: Partition methods in grid-based pooling.
Original matrix with size 4?
5 is mapped into matrixwith size 3?
3 and matrix with size 6?
7, respectively.
Each dynamic pool is distinguished by a borderof empty white space around it.for a g-phrase g on the assumption that they are thebest basis for assessing the relation of g with otherg-phrases.
To implement this, we sort the valuesof each row i (resp.
each column j) in decreasingorder giving us a matrix?Fr?
Rs1?s2with sortedrows (resp.?Fc?
Rs1?s2with sorted columns).Then we concatenate the columns of?Fr(resp.
therows of?Fc) resulting in list Fr= {fr1, .
.
.
, frs1s2}(resp.
Fc= {fc1, .
.
.
, fcs1s2}) where each fr(fc) isan element of?Fr(?Fc).
These two lists are mergedinto a list F by interleaving them so that membersfrom Frand Fcalternate.
F is then used to fill therows of F from top to bottom with each row beingfilled from left to right.27 mfCNN: Match feature CNNThe output of dynamic 2D pooling is further pro-cessed by the match feature CNN (mfCNN) as de-picted in Figure 6. mfCNN extracts increasinglyabstract interaction features from lower-level in-teraction features, using several layers of 2D wideconvolution and fixed-size 2D pooling.We call the combination of a 2D wide convo-lution layer and a fixed-size 2D pooling layer ablock, denoted by index b (b = 1, 2 .
.
.).
In gen-eral, let tensor Tb?
Rcb?sb?sbdenote the fea-ture maps in block b; block b has cbfeature maps,each of size sb?
sb(T1= F ?
R1?s??s?).
LetWb?
Rcb+1?cb?fb?fbbe the filter weights of 2Dwide convolution in block b, fb?fbis then the sizeof sliding convolution regions.
Then the convolu-tion is performed as element-wise multiplication2If?F has fewer cells than F, then we simply repeat thefilling procedure to fill all cells.between Wband Tbas follows:?Tb+1m,i?1,j?1= ?
(?Wbm,:,:,:Tb:,i?fb:i,j?fb:j+bbm)(6)where 0?m<cb+1, 1 ?
i, j < sb+fb, bb?
Rcb+1.Subsequently, fixed-size 2D pooling selectsdominant features from kb?
kbnon-overlappingwindows of?Tb+1to form a tensor as input ofblock b+ 1:Tb+1m,i,j= max(?Tb+1m,ikb:(i+1)kb,jkb:(j+1)kb) (7)where 0 ?
i, j < bsb+fb?1kbc.Hu et al (2014) used narrow convolution whichwould limit the number of blocks.
2D wide convo-lution in this work enables to stack multiple blocksof convolution and pooling to extract higher-levelinteraction features.
We will study the influence ofthe number of blocks on performance below.For the experiments, we set s?= 40, cb=50, fb= 5, kb= 2 (b = 1, 2, ?
?
?
).8 MultiGranCNNWe can now describe the overall architecture ofMultiGranCNN.
First, using a Siamese configu-ration, two copies of gpCNN, one for each ofthe two input TEXTCHUNKS, produce g-phraserepresentations on different levels of abstraction(Figure 2).
Then one of the three match featuremodels (DIRECTSIM, CONCAT or INDIRECTSIM)produces an s1?
s2match feature matrix, eachcell of which assesses the match of a pair of g-phrases from the two chunks.
This match featurematrix is reduced to a fixed size matrix by dy-namic 2D pooling (Section 6).
As shown in Fig-ure 6, the resulting fixed size matrix is the inputfor mfCNN, which extracts interaction features of68Figure 6: mfCNN & MLP for matching score learning.
s?= 10, fb= 5, kb= 2, cb= 4 in this example.increasing complexity from the basic interactionfeatures computed by the match feature model.
Fi-nally, the output of the last block of mfCNN is theinput to an MLP that computes the match score.MultiGranCNN bears resemblance to previouswork on clause and sentence matching (e.g., Huet al (2014), Socher et al (2011)), but it is moregeneral and more flexible.
It learns representa-tions of g-phrases, i.e., representations of parts ofthe TEXTCHUNK at multiple granularities, not justfor a single level such as the sentence as ARC-Idoes (Hu et al, 2014).
MultiGranCNN exploresthe space of interactions between the two chunksmore exhaustively by considering interactions be-tween every unit in one chunk with every otherunit in the other chunk, at all levels of granular-ity.
Finally, MultiGranCNN supports a number ofdifferent match feature models; the correspondingmodule can be instantiated in a way that ensuresthat match features are best suited to support ac-curate decisions on the TEXTCHUNK relation taskthat needs to be addressed.9 Experimental Setup and Results9.1 TrainingSuppose the triple (x,y+,y?)
is given and xmatches y+better than y?.
Then our objectiveis the minimization of the following ranking loss:l(x,y+,y?)
= max(0, 1 + s(x,y?)?
s(x,y+))where s(x,y) is the predicted match score for(x,y).
We use stochastic gradient descent withAdagrad (Duchi et al, 2011), L2regularizationand minibatch training.We set initial learning rate to 0.05, batch size to70, L2weight to 5 ?
10?4.Recall that we employ unsupervised pretrainingof representations for g-phrases.
We can eitherfreeze these representations in subsequent super-vised training; or we can fine-tune them.
We studythe performance of both regimes.9.2 Clause Coherence TaskAs introduced by Hu et al (2014), the clausecoherence task determines for a pair (x,y) ofclauses if the sentence ?xy?
is a coherent sen-tence.
We construct a clause coherence datasetas follows (the set used by Hu et al (2014) is notyet available).
We consider all sentences from En-glish Gigaword (Parker et al, 2009) that consist oftwo comma-separated clauses x and y, with eachclause having between five and 30 words.
For eachy, we choose four clauses y?.
.
.y???
?randomlyfrom the 1000 second clauses that have the highestsimilarity to y, where similarity is cosine similar-ity of TF-IDF vectors of the clauses; restrictingthe alternatives to similar clauses ensures that thetask is hard.
The clause coherence task then is toselect y from the set y,y?, .
.
.
,y???
?as the correctcontinuation of x.
We create 21 million examples,each consisting of a first clause x and five secondclauses.
This set is divided into a training set of19 million and development and test sets of onemillion each.
An example from the training set isgiven in Figure 1.Then, we study the performance variance ofdifferent MultiGranCNN setups from three per-spectives: a) layers of CNN in both unsuper-vised (gpCNN) and supervised (mfCNN) trainingphases; b) different approaches for clause relationfeature modeling; c) dynamic pooling methods forgenerating same-sized feature matrices.Figure 7 (top table) shows that (Hu et al,2014)?s parameters are good choices for our setupas well.
We get best result when both gpCNNand mfCNN have three blocks of convolution and69pooling.
This suggests that multiple layers of con-volution succeed in extracting high-level featuresthat are beneficial for clause coherence.Figure 7 (2nd table) shows that INDIRECTSIMand CONCAT have comparable performance andboth outperform DIRECTSIM.
DIRECTSIM is ex-pected to perform poorly because the contents inthe two clauses usually have little or no overlap-ping meaning.
In contrast, we can imagine thatINDIRECTSIM first transforms the first clause xinto a counterpart and then matches this counter-part with the second clause y.
In CONCAT, eachof s1?s2pairs of g-phrases is concatentated andsupervised training can then learn an unrestrictedfunction to assess the importance of this pair forclause coherence (cf.
Eq.
5).
Again, this is clearlya more promising TEXTCHUNK relation model forclause coherence than one that relies on DIRECT-SIM.accmfCNN0 1 2 3gpCNN0 38.02 44.08 47.81 48.431 40.91 45.31 51.73 52.132 43.10 48.06 54.14 54.863 45.62 51.77 55.97 56.31match feature model accDIRECTSIM 25.40INDIRECTSIM 56.31CONCAT 56.12freeze g-phrase represenations or not accMultiGranCNN (freeze) 55.79MultiGranCNN (fine-tune) 56.31pooling method accdynamic (Socher et al, 2011) 55.91grid-based 56.07phrase-focused 56.31Figure 7: Effect on dev acc (clause coherence) ofdifferent factors: # convolution blocks, match fea-ture model, freeze vs. fine-tune, pooling method.Figure 7 (3rd table) demonstrates that fine-tuning g-phrase representations gives better per-formance than freezing them.
Also, grid-basedand phrase-focused pooling outperform dynamicpooling (Socher et al, 2011) (4th table).
Phrase-focused pooling performs best.Table 1 compares MultiGranCNN to ARC-I andARC-II, the architectures proposed by Hu et al(2014).
We also test the five baseline systemsfrom their paper: DeepMatch, WordEmbed, SEN-MLP, SENNA+MLP, URAE+MLP.
For Multi-GranCNN, we use the best dev set settings: num-ber of convolution layers in gpCNN and mfCNNis 3; INDIRECTSIM; phrase-focused pooling.
Ta-ble 1 shows that MultiGranCNN outperforms allother approaches on clause coherence test set.9.3 Paraphrase Identification TaskWe evaluate paraphrase identification (PI) on thePAN corpus (http://bit.ly/mt-para, (Madnani et al,2012)), consisting of training and test sets of10,000 and 3000 sentence pairs, respectively.
Sen-tences are about 40 words long on average.Since PI is a binary classification task, we re-place the MLP with a logistic regression layer.
Asphrase-focused pooling was proven to be optimal,we directly use phrase-focused pooling in PI taskwithout comparison, assuming that the choice ofdynamic pooling is task independent.For parameter selection, we split the PAN train-ing set into a core training set (core) of size 9000and a development set (dev) of size 1000.
Wethen train models on core and select parametersbased on best performance on dev.
The best re-sults on dev are obtained for the following param-eters: freezing g-phrase representations, DIRECT-SIM, two convolution layers in gpCNN, no convo-lution layers in mfCNN.
We use these parametersettings to train a model on the entire training setand report performance in Table 2.We compare MultiGranCNN to ARC-I/II (Huet al, 2014), and two previous papers reportingperformance on PAN.
Madnani et al (2012) useda combination of three basic MT metrics (BLEU,NIST and TER) and five complex MT met-rics (TERp, METEOR, BADGER, MAXISIM,model accRandom Guess 20.00DeepMatch 34.17WordEmbed 38.28SENMLP 34.57SENNA+MLP 42.09URAE+MLP 27.41ARC-I 45.04ARC-II 50.18MultiGranCNN 56.27Table 1: Performance on clause coherence test set.70SEPIA), computed on entire sentences.
Bach etal.
(2014) applied MT metrics to elementary dis-course units.
We integrate these eight MT metricsfrom prior work.method acc F1ARC-I 61.4 60.3ARC-II 64.9 63.5basic MT metrics 88.6 87.8+ TERp 91.5 91.2+ METEOR 92.0 91.8+ Others 92.3 92.1(Bach et al, 2014) 93.4 93.38MT+MultiGranCNN (fine-tune) 94.1 94.08MT+MultiGranCNN (freeze) 94.9 94.7Table 2: Results on PAN.
?8MT?
= 8 MT metricsTable 2 shows that MultiGranCNN in combina-tion with MT metrics obtains state-of-the-art per-formance on PAN.
Freezing weights learned inunsupervised training (Figure 2) performs betterthan fine-tuning them; also, Table 3 shows that thebest result is achieved if no convolution is usedin mfCNN.
Thus, the best configuration for para-phrase identification is to ?forward?
fixed-size in-teraction matrices as input to the logistic regres-sion, without any intermediate convolution layers.Freezing weights learned in unsupervised train-ing and no convolution layers in mfCNN both pro-tect against overfitting.
Complex deep neural net-works are in particular danger of overfitting whentraining sets are small as in the case of PAN (cf.
Huet al (2014)).
In contrast, fine-tuning weights andseveral convolution layers were the optimal setupfor clause coherence.
For clause coherence, wehave a much larger training set and therefore cansuccessfully train a much larger number of param-eters.Table 3 shows that CONCAT performs badly forPI while DIRECTSIM and INDIRECTSIM performwell.
We can conceptualize PI as the task of deter-mining if each meaning element in S1has a simi-lar meaning element in S2.
The s1?
s2DIRECT-SIM feature model directly models this task andthe s1?s2INDIRECTSIM feature model also mod-els it, but learning a transformation of g-phraserepresentations before applying similarity.
In con-trast, CONCAT can learn arbitrary relations be-tween parts of the two sentences, a model thatseems to be too unconstrained for PI if insufficienttraining resources are available.In contrast, for the clause coherence task, con-catentation worked well and DIRECTSIM workedpoorly and we provided an explanation based onthe specific properties of clause coherence (seediscussion of Figure 7).
We conclude from theseresults that it is dependent on the task what the bestfeature model is for matching two linguistic ob-jects.
Interestingly, INDIRECTSIM performs wellon both tasks.
This suggests that INDIRECTSIM isa general feature model for matching, applicableto tasks with very different properties.10 ConclusionIn this paper, we present MultiGranCNN, a gen-eral deep learning architecture for classifying therelation between two TEXTCHUNKS.
Multi-GranCNN supports multigranular comparabil-ity of representations: shorter sequences in oneTEXTCHUNK can be directly compared to longersequences in the other TEXTCHUNK.
Multi-GranCNN also contains a flexible and modu-larized match feature component that is eas-ily adaptable to different TEXTCHUNK relations.We demonstrated state-of-the-art performance ofMultiGranCNN on paraphrase identification andclause coherence tasks.AcknowledgmentsThanks to CIS members and anonymous re-viewers for constructive comments.
This workwas supported by Baidu (through a Baiduscholarship awarded to Wenpeng Yin) and byDeutsche Forschungsgemeinschaft (grant DFGSCHU 2246/8-2, SPP 1335).F1mfCNN0 1 2 3gpCNN0 92.7 92.9 92.9 93.91 93.2 93.5 93.9 93.52 94.7 94.2 93.7 93.33 94.5 94.0 93.6 92.9match feature model acc F1DIRECTSIM 94.9 94.7INDIRECTSIM 94.7 94.5CONCAT 93.0 92.9Table 3: Effect on dev F1(PI) of different factors:# convolution blocks, match feature model.71ReferencesNgo Xuan Bach, Nguyen Le Minh, and Akira Shi-mazu.
2014.
Exploiting discourse information toidentify paraphrases.
Expert Systems with Applica-tions, 41(6):2832?2841.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for semanticcomposition.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 546?556.
Association for Compu-tational Linguistics.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Antoine Bordes, Sumit Chopra, and Jason Weston.2014a.
Question answering with subgraph embed-dings.
Proceedings of the 2014 Conference on Em-pirical Methods in Natural Language Processing.Antoine Bordes, Xavier Glorot, Jason Weston, andYoshua Bengio.
2014b.
A semantic matching en-ergy function for learning with multi-relational data.Machine Learning, 94(2):233?259.Antoine Bordes, Jason Weston, and Nicolas Usunier.2014c.
Open question answering with weakly su-pervised embedding models.
Proceedings of 2014European Conference on Machine Learning andPrinciples and Practice of Knowledge Discovery inDatabases.Sumit Chopra, Raia Hadsell, and Yann LeCun.
2005.Learning a similarity metric discriminatively, withapplication to face verification.
In Computer Visionand Pattern Recognition, 2005.
CVPR 2005.
IEEEComputer Society Conference on, volume 1, pages539?546.
IEEE.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Jianfeng Gao, Patrick Pantel, Michael Gamon, Xi-aodong He, Li Deng, and Yelong Shen.
2014.
Mod-eling interestingness with deep neural networks.
InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.In Advances in Neural Information Processing Sys-tems, pages 2042?2050.Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,Richard Socher, and Hal Daum?e III.
2014.
A neuralnetwork for factoid question answering over para-graphs.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing, pages 633?644.Yangfeng Ji and Jacob Eisenstein.
2013.
Discrimi-native improvements to distributional sentence sim-ilarity.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 891?896.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics.
Association for ComputationalLinguistics.Quoc V Le and Tomas Mikolov.
2014.
Distributed rep-resentations of sentences and documents.
Proceed-ings of The 31st International Conference on Ma-chine Learning, pages 1188?1196.Hang Li and Jun Xu.
2012.
Beyond bag-of-words:machine learning for query-document matching inweb search.
In Proceedings of the 35th internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 1177?1177.ACM.Xutao Li, Michael K Ng, and Yunming Ye.
2012.Har: Hub, authority and relevance scores in multi-relational data for query search.
In Proceedings ofthe 12th SIAM International Conference on DataMining, pages 141?152.
SIAM.Chen Liu.
2013.
Probabilistic Siamese Network forLearning Representations.
Ph.D. thesis, Universityof Toronto.Zhengdong Lu and Hang Li.
2013.
A deep architec-ture for matching short texts.
In Advances in NeuralInformation Processing Systems, pages 1367?1375.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metricsfor paraphrase identification.
In Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 182?190.
Asso-ciation for Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Andriy Mnih and Yee Whye Teh.
2012.
A fast andsimple algorithm for training neural probabilisticlanguage models.
In Proceedings of the 29th In-ternational Conference on Machine Learning, pages1751?1758.72Robert Parker, Linguistic Data Consortium, et al2009.
English gigaword fourth edition.
LinguisticData Consortium.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2006.Paraphrase recognition via dissimilarity significanceclassification.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 18?26.
Association for Compu-tational Linguistics.Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-pher D Manning, and Andrew Y Ng.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural In-formation Processing Systems, pages 801?809.Pengcheng Wu, Steven CH Hoi, Hao Xia, Peilin Zhao,Dayong Wang, and Chunyan Miao.
2013.
Onlinemultimodal deep similarity learning with applicationto image retrieval.
In Proceedings of the 21st ACMinternational conference on Multimedia, pages 153?162.
ACM.Min-Chul Yang, Nan Duan, Ming Zhou, and Hae-Chang Rim.
2014.
Joint relational embeddings forknowledge-based question answering.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing, pages 645?650.Lei Yu, Karl Moritz Hermann, Phil Blunsom, andStephen Pulman.
2014.
Deep learning for answersentence selection.
NIPS deep learning workshop.73
