Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2090?2095,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning to Capitalize with Character-Level Recurrent Neural Networks:An Empirical StudyRaymond Hendy Susanto?
and Hai Leong Chieu?
and Wei Lu?
?Singapore University of Technology and Design?DSO National Laboratories{raymond susanto,luwei}@sutd.edu.sgchaileon@dso.org.sgAbstractIn this paper, we investigate case restorationfor text without case information.
Previoussuch work operates at the word level.
We pro-pose an approach using character-level recur-rent neural networks (RNN), which performscompetitively compared to language model-ing and conditional random fields (CRF) ap-proaches.
We further provide quantitative andqualitative analysis on how RNN helps im-prove truecasing.1 IntroductionNatural language texts (e.g., automatic speech tran-scripts or social media data) often come in non-standard forms, and normalization would typicallyimprove the performance of downstream natural lan-guage processing (NLP) applications.
This paper in-vestigates a particular sub-task in text normalization:case restoration or truecasing.
Truecasing refers tothe task of restoring case information (uppercase orlowercase) of characters in a text corpus.
Case infor-mation is important for certain NLP tasks.
For ex-ample, Chieu and Ng (2002) used unlabeled mixedcase text to improve named entity recognition (NER)on uppercase text.The task often presents ambiguity: consider theword ?apple?
in the sentences ?he bought an apple?and ?he works at apple?.
While the former refers toa fruit (hence, it should be in lowercase), the latterrefers to a company name (hence, it should be cap-italized).
Moreover, we often need to recover thecase information for words that are previously un-seen by the system.In this paper, we propose the use of character-level recurrent neural networks for truecasing.
Pre-vious approaches for truecasing are based on wordlevel approaches which assign to each word oneof the following labels: all lowercase, all upper-case, initial capital, and mixed case.
For mixedcase words, an additional effort has to be madeto decipher exactly how the case is mixed (e.g.,MacKenzie).
In our approach, we propose a gen-erative, character-based recurrent neural network(RNN) model, allowing us to predict exactly howcases are mixed in such words.Our main contributions are: (i) we show thatcharacter-level approaches are viable compared toword-level approaches, (ii) we show that character-level RNN has a competitive performance comparedto character-level CRF, and (iii) we provide ourquantitative and qualitative analysis on how RNNhelps improve truecasing.2 Related WorkWord-based truecasing The most widely usedapproach works at the word level.
The simplest ap-proach converts each word to its most frequentlyseen form in the training data.
One popular ap-proach uses HMM-based tagging with an N-gramlanguage model, such as in (Lita et al, 2003; Nebhiet al, 2015).
Others used a discriminative tagger,such as MEMM (Chelba and Acero, 2006) or CRF(Wang et al, 2006).
Another approach uses statisti-cal machine translation to translate uncased text intoa cased one.
Interestingly, no previous work oper-ated at the character level.
Nebhi et al (2015) in-vestigated truecasing in tweets, where truecased cor-2090pora are less available.Recurrent neural networks Recent years haveshown a resurgence of interest in RNN, particularlyvariants with long short-term memory (Hochreiterand Schmidhuber, 1997) or gated recurrent units(Cho et al, 2014).
RNN has shown an impressiveperformance in various NLP tasks, such as machinetranslation (Cho et al, 2014; Luong et al, 2015),language modeling (Mikolov et al, 2010; Kim etal., 2016), and constituency parsing (Vinyals et al,2015).
Nonetheless, understanding the mechanismbehind the successful applications of RNN is rarelystudied.
In this work, we take a closer look at ourtrained model to interpret its internal mechanism.3 The Truecasing SystemsIn this section, we describe the truecasing systemsthat we develop for our empirical study.3.1 Word-Level ApproachA word-level approach truecases one word at a time.The first system is a tagger based on HMM (Stol-cke, 2002) that translates an uncased sequence ofwords to a corresponding cased sequence.
An N-gram language model trained on a cased corpus isused for scoring candidate sequences.
For decoding,the Viterbi algorithm (Rabiner, 1989) computes thehighest scoring sequence.The second approach is a discriminative classifierbased on linear chain CRF (Lafferty et al, 2001).In this approach, truecasing is treated as a sequencelabeling task, labelling each word with one of thefollowing labels: all lowercase, all uppercase, initialcapital, and mixed case.
For our experiments, weused the truecaser in Stanford?s NLP pipeline (Man-ning et al, 2014).
Their model includes a rich setof features (Finkel et al, 2005), such as surroundingwords, character N-grams, word shape, etc.Dealing with mixed case Both approaches re-quire a separate treatment for mixed case words.In particular, we need a gazetteer that maps eachword to its mixed case form ?
either manually cre-ated or statistically collected from training data.
Thecharacter-level approach is motivated by this: In-stead of treating them as a special case, we train ourmodel to capitalize a word character by character.3.2 Character-Level ApproachA character-level approach converts each characterto either uppercase or lowercase.
In this approach,mixed case forms are naturally taken care of, andmoreover, such models would generalize better tounseen words.
Our third system is a linear chainCRF that makes character-level predictions.
Simi-lar to the word-based CRF, it includes surroundingwords and character N-grams as features.Finally, we propose a character-level approach us-ing an RNN language model.
RNN is particularlyuseful for modeling sequential data.
At each timestep t, it takes an input vector xt and previous hid-den state ht?1, and produces the next hidden stateht.
Different recurrence formulations lead to differ-ent RNN models, which we will describe below.Long short-term memory (LSTM) is an archi-tecture proposed by Hochreiter and Schmidhuber(1997).
It augments an RNN with a memory cellvector ct in order to address learning long rangedependencies.
The content of the memory cell isupdated additively, mitigating the vanishing gradi-ent problem in vanilla RNNs (Bengio et al, 1994).Read, write, and reset operations to the memory cellare controlled by input gate i, output gate o, and for-get gate f .
The hidden state is computed as:it = ?
(Wiht?1 + Uixt) (1)ot = ?
(Woht?1 + Uoxt) (2)ft = ?
(Wfht?1 + Ufxt) (3)gt = tanh(Wght?1 + Ugxt) (4)ct = ft  ct?1 + it  gt (5)ht = ot  tanh(ct) (6)where ?
and tanh are element-wise sigmoid and hy-perbolic tangent functions, and Wj and Uj are pa-rameters of the LSTM for j ?
{i, o, f, g}.Gated recurrent unit (GRU) is a gating mech-anism in RNN that was introduced by Cho et al(2014).
They proposed a hidden state computationwith reset and update gates, resulting in a simplerLSTM variant:rt = ?
(Wrht?1 + Urxt) (7)zt = ?
(Wzht?1 + Uzxt) (8)h?t = tanh(Wh(rt  ht?1) + Uhxt) (9)ht = (1?
zt) ht?1 + zt  h?t (10)2091EN-Wikipedia EN-WSJ EN-Reuters DE-ECIAcc.
P R F1 Acc.
P R F1 Acc.
P R F1 Acc.
P R F1Word-based ApproachLM (N = 3) 94.94 89.34 84.61 86.91 95.59 91.56 78.79 84.70 94.57 93.49 79.43 85.89 95.67 97.84 87.74 92.51LM (N = 5) 94.93 89.42 84.41 86.84 95.62 91.72 78.79 84.77 94.66 93.92 79.47 86.09 95.68 97.91 87.70 92.53CRF-WORD 96.60 94.96 87.16 90.89 97.64 93.12 90.41 91.75 96.58 93.91 87.19 90.42 96.09 98.41 88.73 93.32Chelba and Acero (2006) n/a 97.10 - - - n/a n/aCharacter-based ApproachCRF-CHAR 96.99 94.60 89.27 91.86 97.00 94.17 84.46 89.05 97.06 94.63 89.12 91.80 98.26 96.95 96.59 96.77LSTM-SMALL 96.95 93.05 90.59 91.80 97.83 93.99 90.92 92.43 97.37 93.08 92.63 92.86 98.70 97.52 97.39 97.46LSTM-LARGE 97.41 93.72 92.67 93.19 97.72 93.41 90.56 91.96 97.76 94.08 93.50 93.79 99.00 98.04 97.98 98.01GRU-SMALL 96.46 92.10 89.10 90.58 97.36 92.28 88.60 90.40 97.01 92.85 90.84 91.83 98.51 97.15 96.96 97.06GRU-LARGE 96.95 92.75 90.93 91.83 97.27 90.86 90.20 90.52 97.12 92.02 92.07 92.05 98.35 96.86 96.79 96.82Table 2: Truecasing performance in terms of precision (P), recall (R), and F1.
All improvements of the best performing character-based systems(bold) over the best performing word-based systems (underlined) are statistically significant using sign test (p < 0.01).
All improvements of thebest performing RNN systems (italicized) over CRF-CHAR are statistically significant using sign test (p < 0.01).At each time step, the conditional probability dis-tribution over next characters is computed by linearprojection of ht followed by a softmax:P (xt = k|x1:t?1) =exp(wkht)?|V |j=1 exp(wjht)(11)where wk is the k-th row vector of a weight matrixW .
The probability of a sequence of characters x1:Tis defined as:P (x1:T ) =T?t=1P (xt|x1:t?1) (12)Similar to the N-gram language modeling approachwe described previously, we need to maximizeEquation 12 in order to decode the most probablecased sequence.
Instead of Viterbi decoding, we ap-proximate this using a beam search.4 Experiments and Results4.1 Datasets and ToolsOur approach is evaluated on English and Germandatasets.
For English, we use a Wikipedia corpusfrom (Coster and Kauchak, 2011), WSJ corpus (Pauland Baker, 1992), and the Reuters corpus from theCoNLL-2003 shared task on named entity recogni-tion (Tjong Kim Sang and De Meulder, 2003).
ForGerman, we use the ECI Multilingual Text Corpusfrom the same shared task.
Each corpus is tok-enized.1 The input test data is lowercased.
Table 1shows the statistics of each corpus split into training,development, and test sets.We use SRILM (Stolcke, 2002) for N-gram lan-guage model training (N ?
{3, 5}) and HMM de-coding.
The word-based CRF models are trained us-ing the CRF implementation in Stanford?s CoreNLP1News headlines, which are all in uppercase, are discarded.Corpus Split #words #charsEN-Wikitrain 2.9M 16.1Mdev 294K 1.6Mtest 32K 176KEN-WSJtrain 1.9M 10.5Mdev 101K 555Ktest 9K 48KEN-Reuterstrain 3.1M 16.8Mdev 49K 264Ktest 44K 231KDE-ECItrain 2.8M 18Mdev 51K 329Ktest 52K 327KTable 1: Statistics of the data.3.6.0 (Finkel et al, 2005).
We use a recommendedconfiguration for training the truecaser.We use CRF-Suite version 0.12 (Okazaki, 2007) to train thecharacter-based CRF model.
Our feature set in-cludes character N-grams (N ?
{1, 2, 3}) and wordN-grams (N ?
{1, 2}) surrounding the current char-acter.
We tune the `2 regularization parameter ?
us-ing a grid search where ?
?
{0.01, 0.1, 1, 10}.We use an open-source character RNN imple-mentation.2 We train a SMALL model with 2 lay-ers and 300 hidden nodes, and a LARGE modelwith 3 layers and 700 hidden nodes.
We also varythe hidden unit type (LSTM/GRU).
The networkis trained using truncated backpropagation for 50time steps.
We use a mini-batch stochastic gradientdescent with batch size 100 and RMSprop update(Tieleman and Hinton, 2012).
We use dropout reg-ularization (Srivastava et al, 2014) with 0.25 prob-ability.
We choose the model with the smallest val-idation loss after 30 epochs.
For decoding, we setbeam size to 10.
The experimental settings are re-ported in more depth in the supplementary materi-als.
Our system and code are publicly available athttp://statnlp.org/research/ta/.2https://github.com/karpathy/char-rnn2092(a) Samples from EN-Wiki(b) Samples from DE-ECIFigure 1: Cells that are sensitive to lowercased and capitalized words.
Text color represents activations (?1 ?
tanh(ct) ?
1): positive is blue,negative is red.
Darker color corresponds to greater magnitude.4.2 ResultsTable 2 shows the experiment results in terms of pre-cision, recall, and F1.
Most previous work did notevaluate their approaches on the same dataset.
Wecompare our work to (Chelba and Acero, 2006) us-ing the same WSJ sections for training and evalua-tion on 2M word training data.
Chelba and Aceroonly reported error rate, and all our RNN and CRFapproaches outperform their results in terms of errorrate.First, the word-based CRF approach gives upto 8% relative F1 increase over the LM approach.Other than WSJ, moving to character level furtherimproves CRF by 1.1-3.7%, most notably on theGerman dataset.
Long compound nouns are com-mon in the German language, which generates manyout-of-vocabulary words.
Thus, we hypothesize thatcharacter-based approach improves generalization.Finally, the best F1 score for each dataset is achievedby the RNN variants: 93.19% on EN-Wiki, 92.43%on EN-WSJ, 93.79% on EN-Reuters, and 98.01% onDE-ECI.We highlight that different features are used inCRF-WORD and CRF-CHAR.
CRF-CHAR onlyincludes simple features, namely character and wordN-grams and sentence boundary indicators.
In con-trast, CRF-WORD contains a richer feature set thatis predefined in Stanford?s truecaser.
For instance,it includes word shape, in addition to neighboringwords and character N-grams.
It also includes morefeature combinations, such as the concatenation ofthe word shape, current label, and previous label.Nonetheless, CRF-CHAR generally performs betterthan CRF-WORD.
Potentially, CRF-CHAR can beimproved further by using larger N-grams.
The de-cision to use simple features is for optimizing thetraining speed.
Consequently, we are able to dedi-cate more time for tuning the regularization weight.Training a larger RNN model generally improvesperformance, but it is not always the case due topossible overfitting.
LSTM seems to work betterthan GRU in this task.
The GRU models have 25%less parameters.
In terms of training time, it took12 hours to train the largest RNN model on a sin-gle Titan X GPU.
For comparison, the longest train-ing time for a single CRF-CHAR model is 16 hours.Training LM and CRF-WORD is much faster: 30seconds and 5.5 hours, respectively, so there is aspeed-accuracy trade-off.5 Analysis5.1 Visualizing LSTM CellsAn interesting component of LSTM is its memorycells, which is supposed to store long range depen-dency information.
Many of these memory cells arenot human-interpretable, but after introspecting ourtrained model, we find a few memory cells that aresensitive to case information.
In Figure 1, we plotthe memory cell activations at each time step (i.e.,tanh(ct)).
We can see that these cells activate differ-ently depending on the case information of a word(towards -1 for uppercase and +1 for lowercase).5.2 Case Category and OOV PerformanceCorpus Lower Cap.
Upper Mixed OOVEN-Wiki 79.91 18.67 0.91 0.51 2.40EN-WSJ 84.28 13.06 2.63 0.03 3.11EN-Reuters 78.36 19.80 1.53 0.31 5.37DE-ECI 68.62 29.15 1.02 1.21 4.01Table 3: Percentage distribution of the case categories and OOV wordsIn this section, we analyze the system perfor-mance on each case category.
First, we report thepercentage distribution of the case categories in eachtest set in Table 3.
For both languages, the most fre-quent case category is lowercase, followed by capi-talization, which generally applies to the first word2093EN-Wiki EN-WSJ EN-Reuters DE-ECI0.20.40.60.8.66 .67.53 .52.18.67.16.04.74.67 .69.87.82.67.8.93accuracy(a) Mixed caseEN-Wiki EN-WSJ EN-Reuters DE-ECI0.80.91.85.77.8.89.89 .9 .89.92.9.83.9.97.93.91.95.99accuracy(b) CapitalizedEN-Wiki EN-WSJ EN-Reuters DE-ECI0.80.850.9.88.9.77.88.91.92.85.92.9 .9.77.88.87.89.82.89accuracy(c) UppercaseLM CRF-Word CRF-Char RNNEN-Wiki EN-WSJ EN-Reuters DE-ECI0.20.40.60.8.33 .32.39.21.55.76.68.37.71 .73.81 .82.82 .84.9 .91accuracy(d) OOVFigure 2: Accuracy on mixed case (a), capitalized (b), uppercase (c), and OOV words (d).in the sentence and proper nouns.
The uppercaseform, which is often found in abbreviations, occursmore frequently than mixed case for English, but theother way around for German.Figure 2 (a) shows system accuracy on mixedcase words.
We choose the best performing LMand RNN for each dataset.
Character-based ap-proaches have a better performance on mixed casewords than word-based approaches, and RNN gen-erally performs better than CRF.
In CRF-WORD,surface forms are generated after label prediction.This is more rigid compared to LM, where the sur-face forms are considered during decoding.In addition, we report system accuracy on capi-talized words (first letter uppercase) and uppercasewords in Figure 2 (b) and (c), respectively.
RNNperforms the best on capitalized words.
On the otherhand, CRF-WORD performs the best on uppercase.We believe this is related to the rare occurrences ofuppercase words during training, as shown in Ta-ble 3.
Although mixed case occurs more rarely ingeneral, there are important clues, such as charac-ter prefix.
CRF-CHAR and RNN have comparableperformance on uppercase.
For instance, there areonly 2 uppercase words in WSJ that were predicteddifferently between CRF-CHAR and RNN.
All sys-tems perform equally well (?99% accuracy) on low-ercase.
Overall, RNN has the best performance.Last, we present results on out-of-vocabulary(OOV) words with respect to the training set.
Thestatistics of OOV words is given in Table 3.
The sys-tem performance across datasets is reported in Fig-ure 2 (d).
We observe that RNN consistently per-forms better than the other systems, which showsthat it generalizes better to unseen words.6 ConclusionIn this work, we conduct an empirical investiga-tion of truecasing approaches.
We have shown thatcharacter-level approaches work well for truecasing,and that RNN performs competitively compared tolanguage modeling and CRF.
Future work includesapplications in informal texts, such as tweets andshort messages (Muis and Lu, 2016).AcknowledgmentsWe would also like to thank the anonymous review-ers for their helpful comments.
This work is sup-ported by MOE Tier 1 grant SUTDT12015008.2094ReferencesYoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gradi-ent descent is difficult.
IEEE Transactions on NeuralNetworks, 5(2):157?166.Ciprian Chelba and Alex Acero.
2006.
Adaptation ofmaximum entropy capitalizer: Little data can help alot.
Computer Speech & Language, 20(4):382?399.Hai Leong Chieu and Hwee Tou Ng.
2002.
Teaching aweaker classifier: Named entity recognition on uppercase text.
In Proceedings of ACL, pages 481?488.Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phrase represen-tations using RNN encoder?decoder for statistical ma-chine translation.
In Proceedings of EMNLP, pages1724?1734.William Coster and David Kauchak.
2011.
Simple En-glish Wikipedia: A new text simplification task.
InProceedings of ACL-HLT, pages 665?669.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In Proceedings of ACL, pages 363?370.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural Computation, 9(8):1735?1780.Andrej Karpathy, Justin Johnson, and Fei-Fei Li.
2016.Visualizing and understanding recurrent networks.
InProceedings of ICLR.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M Rush.
2016.
Character-aware neural languagemodels.
In Proceedings of AAAI.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of ICML, pages 282?289.Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, andNanda Kambhatla.
2003. tRuEcasIng.
In Proceed-ings of ACL, pages 152?159.Minh-Thang Luong, Hieu Pham, and Christopher D.Manning.
2015.
Effective approaches to attention-based neural machine translation.
In Proceedings ofEMNLP, pages 1412?1421.Christopher D Manning, Mihai Surdeanu, John Bauer,Jenny Rose Finkel, Steven Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of ACL Sys-tem Demonstrations, pages 55?60.Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-nocky`, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In Proceedings ofINTERSPEECH, pages 1045?1048.Aldrian Obaja Muis and Wei Lu.
2016.
Weak semi-Markov CRFs for noun phrase chunking in informaltext.
In Proceedings of NAACL.Kamel Nebhi, Kalina Bontcheva, and Genevieve Gorrell.2015.
Restoring capitalization in #tweets.
In Proceed-ings of WWW Companion, pages 1111?1115.Naoaki Okazaki.
2007.
CRFsuite: A fast implementa-tion of conditional random fields (CRFs).Douglas B Paul and Janet M Baker.
1992.
The designfor the Wall Street Journal-based CSR corpus.
In Pro-ceedings of the Workshop on Speech and Natural Lan-guage, pages 357?362.Lawrence R Rabiner.
1989.
A tutorial on hidden Markovmodels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?286.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
Journal of Machine Learning Re-search, 15(1):1929?1958.Andreas Stolcke.
2002.
SRILM-an extensible languagemodeling toolkit.
In Proceedings of ICSLP, pages901?904.Tijmen Tieleman and Geoffrey Hinton.
2012.
Lecture6.5-rmsprop: Divide the gradient by a running aver-age of its recent magnitude.
COURSERA: Neural Net-works for Machine Learning, 4(2).Erik F Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedingsof CoNLL, pages 142?147.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Gram-mar as a foreign language.
In Proceedings of NIPS,pages 2755?2763.Wei Wang, Kevin Knight, and Daniel Marcu.
2006.Capitalizing machine translation.
In Proceedings ofNAACL-HLT, pages 1?8.2095
