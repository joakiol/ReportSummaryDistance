Proceedings of the ACL 2007 Student Research Workshop, pages 55?60,Prague, June 2007. c?2007 Association for Computational LinguisticsAdaptive String Distance Measuresfor Bilingual Dialect Lexicon InductionYves ScherrerLanguage Technology Laboratory (LATL)University of Geneva1211 Geneva 4, Switzerlandyves.scherrer@lettres.unige.chAbstractThis paper compares different measures ofgraphemic similarity applied to the taskof bilingual lexicon induction between aSwiss German dialect and Standard Ger-man.
The measures have been adaptedto this particular language pair by trainingstochastic transducers with the Expectation-Maximisation algorithm or by using hand-made transduction rules.
These adaptivemetrics show up to 11% F-measure improve-ment over a static metric like Levenshteindistance.1 IntroductionBuilding lexical resources is a very important step inthe development of any natural language processingsystem.
However, it is a time-consuming and repeti-tive task, which makes research on automatic induc-tion of lexicons particularly appealing.
In this pa-per, we will discuss different ways of finding lexicalmappings for a translation lexicon between a SwissGerman dialect and Standard German.
The choiceof this language pair has important consequences onthe methodology.
On the one hand, given the so-ciolinguistic conditions of dialect use (diglossia), itis difficult to find written data of high quality; par-allel corpora are virtually non-existent.
These dataconstraints place our work in the context of scarce-resource language processing.
On the other hand,as the two languages are closely related, the lexicalrelations to be induced are less complex.
We arguethat this point alleviates the restrictions imposed bythe scarcity of the resources.
In particular, we claimthat if two languages are close, even if one of them isscarcely documented, we can successfully use tech-niques that require training.Finding lexical mappings amounts to findingword pairs that are maximally similar, with respectto a particular definition of similarity.
Similaritymeasures can be based on any level of linguisticanalysis: semantic similarity relies on context vec-tors (Rapp, 1999), while syntactic similarity is basedon the alignment of parallel corpora (Brown et al,1993).
Our work is based on the assumption thatphonetic (or rather graphemic, as we use writtendata) similarity measures are the most appropriatein the given language context because they requireless sophisticated training data than semantic or syn-tactic similarity models.
However, phonetic simi-larity measures can only be used for cognate lan-guage pairs, i.e.
language pairs that can be tracedback to a common historical origin and that possesshighly similar linguistic (in particular, phonologi-cal and morphological) characteristics.
Moreover,we can only expect phonetic similarity measures toinduce cognate word pairs, i.e.
word pairs whoseforms and significations are similar, as a result of ahistorical relationship.We will present different models of phonetic sim-ilarity that are adapted to the given language pair.
Inparticular, attention has been paid to develop tech-niques requiring little manually annotated data.2 Related WorkOur work is inspired by Mann and Yarowsky(2001).
They induce translation lexicons betweena resource-rich language (typically English) and ascarce resource language of another language fam-ily (for example, Portuguese) by using a resource-55rich bridge language of the same family (for ex-ample, Spanish).
While they rely on existingtranslation lexicons for the source-to-bridge step(English-Spanish), they use string distance models(called cognate models) for the bridge-to-target step(Spanish-Portuguese).
Mann and Yarowsky (2001)distinguish between static metrics, which are suffi-ciently general to be applied to any language pair,and adaptive metrics, which are adapted to a spe-cific language pair.
The latter allow for much finer-grained results, but require more work for the adap-tation.
Mann and Yarowsky (2001) use variants ofLevenshtein distance as a static metric, and a HiddenMarkov Model (HMM) and a stochastic transducertrained with the Expectation-Maximisation (EM) al-gorithm as adaptive metrics.
We will also use Leven-shtein distance as well as the stochastic transducer,but not the HMM, which performed worst in Mannand Yarowsky?s study.The originality of their approach is that they ap-ply models used for speech processing to cognateword pair induction.
In particular, they refer to aprevious study by Ristad and Yianilos (1998).
Ris-tad and Yianilos showed how a stochastic transducercan be trained in a non-supervised manner using theEM algorithm and successfully applied their modelto the problem of pronunciation recognition (sound-to-letter conversion).
Jansche (2003) reviews theirwork in some detail, correcting thereby some errorsin the presentation of the algorithms.Heeringa et al (2006) present several modifica-tions of the Levenshtein distance that approximatelinguistic intuitions better.
These models are pre-sented in the framework of dialectometry, i.e.
theyprovide numerical measures for the classification ofdialects.
However, some of their models can beadapted to be used in a lexicon induction task.
Kon-drak and Sherif (2006) use phonetic similarity mod-els for cognate word identification.Other studies deal with lexicon induction for cog-nate language pairs and for scarce resource lan-guages.
Rapp (1999) extends an existing bilin-gual lexicon with the help of non-parallel cor-pora, assuming that corresponding words share co-occurrence patterns.
His method has been used byHwa et al (2006) to induce a dictionary betweenModern Standard Arabic and the Levantine Arabicdialect.
Although this work involves two closely re-lated language varieties, graphemic similarity mea-sures are not used at all.
Nevertheless, Schafer andYarowsky (2002) have shown that these two tech-niques can be combined efficiently.
They use Rapp?sco-occurrence vectors in combination with Mannand Yarowsky?s EM-trained transducer.3 Two-Stage Models of Lexical InductionFollowing the standard statistical machine transla-tion architecture, we represent the lexicon inductiontask as a two-stage model.
In the first stage, we usethe source word to generate a fixed number of can-didate translation strings, according to a transducerwhich represents a particular similarity measure.
Inthe second stage, these candidate strings are filteredthrough a lexicon of the target language.
Candidatesthat are not words of the target language are thuseliminated.This article is, like previous work, mostly con-cerned with the comparison of different similaritymeasures.
However, we extend previous work byintroducing two original measures (3.3 and 3.4) andby embedding the measures into the proposed two-stage framework of lexicon induction.3.1 Levenshtein DistanceOne of the simplest string distance measures is theLevenshtein distance.
According to it, the distancebetween two words is defined as the least-cost se-quence of edit and identity operations.
All edit oper-ations (insertion of one character, substitution of onecharacter by another, and deletion of one character)have a fixed cost of 1.
The identity operation (keep-ing one character from the source word in the targetword) has a fixed cost of 0.
Levenshtein distance op-erates on single letters without taking into accountcontextual features.
It can thus be implemented ina memoryless (one-state) transducer.
This distancemeasure is static ?
it remains the same for all lan-guage pairs.
We will use Levenshtein distance as abaseline for our experiments.3.2 Stochastic Transducers Trained with EMThe algorithm presented by Ristad and Yianilos(1998) enables one to train a memoryless stochastictransducer with the Expectation-Maximisation (EM)algorithm.
In a stochastic transducer, all transitionsrepresent probabilities (rather than costs or weights).56The transduction probability of a given word pair isthe sum of the probabilities of all paths that gen-erate it.
The goal of using the EM algorithm is tofind the transition probabilities of a stochastic trans-ducer which maximise the likelihood of generatingthe word pairs given in the training stage.
Thisgoal is achieved iteratively by using a training lex-icon consisting of correct word pairs.
The initialtransducer contains uniform probabilities.
It is usedto transduce the word pairs of the training lexicon,thereby counting all transitions used in this process.Then, the transition probabilities of the transducerare reestimated according to the frequency of usageof the transitions counted before.
This new trans-ducer is then used in the next iteration.This adaptive model is likely to perform betterthan the static Levenshtein model.
For example, totransduce Swiss German dialects to Standard Ger-man, inserting n or e is much more likely than in-serting m or i. Language-independent models can-not predict such specific facts, but stochastic trans-ducers learn them easily.
However, these improve-ments come at a cost: a training bilingual lexicon ofsufficient size must be available.
For scarce resourcelanguages, such lexicons often need to be built man-ually.3.3 Training without a Bilingual CorpusIn order to further reduce the data requirements,we developed another strategy that avoided using atraining bilingual lexicon altogether and used otherresources for the training step instead.
The mainidea is to use a simple list of dialect words, and theStandard German lexicon.
In doing this, we assumethat the structure of the lexicon informs us aboutwhich transitions are most frequent.
For example,the dialect word chue ?cow?
does not appear in theStandard German lexicon, but similar words likeKuh ?cow?, Schuh ?shoe?, Schule ?school?, Sache?thing?, K?he ?cows?
do.
Just by inspecting thesemost similar existing words, we can conclude that cmay transform to k (Kuh, K?he), that s is likely tobe inserted (Schuh, Schule, Sache), and that e maytransform to h (Kuh, Schuh ).
But we also concludethat none of the letters c, h, u, e is likely to transformto ?
or f, just because such words do not exist inthe target lexicon.
While such statements are coinci-dental for one single word, they may be sufficientlyreliable when induced over a large corpus.In this model, we use an iterative training algo-rithm alternating two tasks.
The first task is to builda list of hypothesized word pairs by using the di-alect word list, the Standard German lexicon, and atransducer1: for each dialect word, candidate stringsare generated, filtered by the lexicon, and the bestcandidate is selected.
The second task is to train astochastic transducer with EM, as explained above,on the previously constructed list of word pairs.
Inthe next iteration, this new transducer is used in thefirst task to obtain a more accurate list of word pairs,which in turn allows us to build a new transducerin the second task.
This process is iterated severaltimes to gradually eliminate erroneous word pairs.The most crucial step is the selection of the bestcandidate from the list returned by the lexicon filter.We could simply use the word which obtained thehighest transduction probability.
However, prelimi-nary experiments have shown that the iterative algo-rithm tends to prefer deletion operations, so that itwill converge to generating single-letter words only(which turn out to be present in our lexicon).
Toavoid this scenario, the length of the suggested can-didate words must be taken into account.
We there-fore simply selected the longest candidate word.23.4 A Rule-based ModelThis last model does not use learning algorithms.It consists of a simple set of transformation rulesthat are known to be important for the chosen lan-guage pair.
Marti (1985, 45-64) presents a preciseoverview of the phonetic correspondences betweenthe Bern dialect and Standard German.
Contraryto the learning models, this model is implementedin a weighted transducer with more than one state.Therefore, it allows contextual rules too.
For ex-ample, we can state that the Swiss German sequence?ech should be translated to euch.
Each rule is givena weight of 1, no matter how many characters it con-cerns.
The rule set contains about 50 rules.
Theserules are then superposed with a Levenshtein trans-ducer, i.e.
with context-free edit and identity opera-1In the initialization step, we use a Levenshtein transducer.2In fact, we should select the word with the lowest abso-lute value of the length difference.
The suggested simplificationprevents us from being trapped in the single-letter problem andreflects the linguistic reality that Standard German words tendto be longer than dialect words.57tions for each letter.
These additional transitions as-sure that every word can be transduced to its target,even if it does not use any of the language-specificrules.
The identity transformations of the Leven-shtein part weigh 2, and its edit operations weigh3.
With these values, the rules are always preferredto the Levenshtein edit operations.
These weightsare set somewhat arbitrarily, and further adjustmentscould slightly improve the results.4 Experiments and Results4.1 Data and TrainingWritten data is difficult to obtain for Swiss Germandialects.
Most available data is in colloquial styleand does not reliably follow orthographic rules.
Inorder to avoid tackling these additional difficulties,we chose a dialect literature book written in the Berndialect.
From this text, a word list was extracted;each word was manually translated to Standard Ger-man.
Ambiguities were resolved by looking at theword context, and by preferring the alternatives per-ceived as most frequent.3 No morphological analy-sis was performed, so that different inflected formsof the same lemma may occur in the word list.
Theonly preprocessing step concerned the eliminationof morpho-phonological variants (sandhi phenom-ena).
The whole list contains 5124 entries.
Forthe experiments, 393 entries were excluded becausethey were foreign language words, proper nouns orStandard German words.4 From the remaining wordpairs, about 92% were annotated as cognate pairs.5One half of the corpus was reserved for training theEM-based models, and the other half was used fortesting.The Standard German lexicon is a word list con-sisting of 202?000 word forms.
While the lexiconprovides more morphological, syntactic and seman-tic information, we do not use it in this work.3Further quality improvements could be obtained by includ-ing the results of a second annotator, and by allowing multipletranslations.4This last category was introduced because the dialect textcontained some quotations in Standard German.5This annotation was done by the author, a native speakerof both German varieties.
Mann and Yarowsky (2001) considera word pair as cognate if the Levenshtein distance between thetwo words is less than 3.
Their heuristics is very conservative:it detects 84% of the manually annotated cognate pairs of ourcorpus.The test corpus contains 2366 word pairs.
407pairs (17.2 %) consist of identical words (lowerbound).
1801 pairs (76.1%) contain a Standard Ger-man word present in the lexicon, and 1687 pairs(71.3%) are cognate pairs, with the Standard Ger-man word present in the lexicon (upper bound).
Itmay surprise that many Standard German words ofthe test corpus do not exist in the lexicon.
This con-cerns mostly ad-hoc compound nouns, which cannotbe expected to be found in a Standard German lex-icon of a reasonable size.
Additionally, some Berndialect words are expressed by two words in Stan-dard German, such as the sequence ir ?in the (fem.
)?that corresponds to Standard German in der.
For rea-sons of computational complexity, our model onlylooks for single words and will not find such corre-spondences.The basic EM model (3.2) was trained in 50 iter-ations, using a training corpus of 200 word pairs.Interestingly, training on 2000 word pairs did notimprove the results.
The larger training corpus didnot even lead the algorithm to converge faster.6 Themonolingual EM model (3.3) was trained in 10 iter-ations, each of which involved a basic EM trainingwith 50 iterations on a training corpus of 2000 di-alect words.4.2 ResultsAs explained above, the first stage of the model takesthe dialect words given in the test corpus and gen-erates, for each dialect word, the 500 most similarstrings according to the transducer used.
This listis then filtered by the lexicon.
Between 0 and 20candidate words remain, depending on how effectivethe lexicon filter has been.
Thus, each source wordis associated to a candidate list, which is orderedwith respect to the costs or probabilities attributed tothe candidates by the transducer.
Experiments with1000 candidate strings yielded comparable results.Table 1 shows some results for the four models.The table reports the number of times the expectedStandard German words appeared anywhere in thecorresponding candidate lists (List), and the number6This is probably due to the fact that the percentage of iden-tical words is quite high, which facilitates the training.
Anotherreason could be that the orthographical conventions used in thedialect text are quite close to the Standard German ones, so thatthey conceal some phonetic differences.58N L P R FLevenshtein List 840 3.1 18.5 35.5 24.3Top 671 1.1 32.7 28.4 30.4EM bilingual List 1210 4.5 21.4 51.1 30.2Top 794 0.7 52.5 33.6 41.0EM mono- List 1070 5.0 16.6 45.2 24.3lingual Top 700 0.7 47.9 29.6 36.6Rules List 987 3.2 22.8 41.7 29.5Top 909 1.0 45.6 38.4 41.7Table 1: Results.
The table shows the absolute num-bers of correct target words induced (N) and the av-erage lengths of the candidate lists (L).
The threerightmost columns represent percentage values ofprecision (P), recall (R), and F-measure (F).of times they appeared at the best-ranked position ofthe candidate lists (Top).
Precision and recall mea-sures are computed as follows:7precision =|correct target words||unique candidate words|recall =|correct target words||tested words|As Table 1 shows, the three adaptive modelsperform better than the static Levenshtein distancemodel.
This finding is consistent with the resultsof Mann and Yarowsky (2001), although our experi-ments showmore clear-cut differences.
The stochas-tic transducer trained on the bilingual corpus ob-tained similar results to the rule-based system, whilethe transducer trained on a monolingual corpus per-formed only slightly better than the baseline.
Never-theless, its performance can be considered to be sat-isfactory if we take into account that virtually no in-formation on the exact graphemic correspondenceshas been given.
The structure of the lexicon and ofthe source word list suffice to make some generali-sations about graphemic correspondences betweentwo languages.
However, it remains to be shownif this method can be extended to more distant lan-guage pairs.In contrast to Levenshtein distance, the bilingualEM model improves the List statistics a lot, at theexpense of longer candidate lists.
However, whencomparing the Top statistics, the difference betweenthe models is less marked.
The rule-based model7The words that occur in several candidate lists (i.e., fordifferent source words) are counted only once, hence the termunique candidate words.generates rather short candidate lists, but it still out-performs all other models with respect to the wordsproposed in first position.
The rule-based model ob-tains high F-measure values, which means that itsprecision and recall values are better balanced thanin the other models.4.3 DiscussionAll models require only a small amount of trainingor development data.
Such data should be availablefor most language pairs that relate a scarce resourcelanguage to a resource-rich language.
However, theperformances of the rule-based model and the bilin-gual EM model show that building a training corpuswith manually translated word pairs, or alternativelyimplementing a small rule set, may be worthwhile.The overall performances of the presented sys-tems may seem poor.
Looking at the recall valuesof the Top statistics, our models only induce aboutone third of the test corpus, or only about half of thetest words that can be induced by phonetic similar-ity models ?
we cannot expect our models to inducenon-cognate words or words that are not in the lex-icon (see the upper bound values in 4.1).
Using thesame models, Mann and Yarowsky (2001) inducedover 90% of the Spanish-Portuguese cognate vocab-ulary.
One reason for their excellent results lies intheir testing procedure.
They use a small test corpusof 100 word pairs.
For each given word, they com-pute the transduction costs to each of the 100 pos-sible target words, and select the best-ranked candi-date as hypothesized solution.
The list of possibletarget words can thus be explored exhaustively.
Wetested our models withMann and Yarowsky?s testingprocedure and obtained very competitive results (seeTable 2).
Interestingly, the monolingual EM modelperformed much worse in this evaluation, a resultwhich could not be expected in light of the results inTable 1.While Mann and Yarowsky?s procedure is veryuseful to evaluate the performance of different simi-larity measures and the impact of different languagepairs, we believe that it is not representative for thetask of lexicon induction.
Typically, the list of possi-ble target words (the target lexicon) does not contain100 words only, but is much larger (202?000 wordsin our case).
This difference has several implica-tions.
First, the lexicon is more likely to present very59Mann and Yarowsky Our workcognate full cognate fullLevenshtein 92.3 67.9 90.5 85.2EM bilingual 92.3 67.1 92.2 86.5EM monolingual 81.9 76.7Rules 94.1 88.7Table 2: Comparison between Mann andYarowsky?s results on Spanish-Portuguese (68%of the full vocabulary are cognate pairs), and ourresults on Swiss German-Standard German (83%cognate pairs).
The tests were performed on 10corpora of 100 word pairs each.
The numbersrepresent the percentage of correctly induced wordpairs.similar words (for example, different inflected formsof the same lexeme), increasing the probability of?near misses?.
Second, our lexicon is too large to besearched exhaustively.
Therefore, we introduced ourtwo-stage approach, whose first stage is completelyindependent of the lexicon.
The drawback of thisapproach is that for many dialect words, it yieldsno result at all, because the 500 generated candi-dates were all non-words.
The recall rates couldbe increased by generating more candidates, but thiswould lead to longer execution times and lower pre-cision rates.5 Conclusion and PerspectivesThe experiments conducted with various adaptivemetrics of graphemic similarity show that in thecase of closely related language pairs, lexical in-duction performances can be increased compared toa static measure like Levenshtein distance.
Theyalso show that requirements for training data canbe kept rather small.
However, these models alsoshow their limits.
They only use single word in-formation for training and testing, which means thatthe rich contextual information encoded in texts, aswell as the morphologic and syntactic informationavailable in the target lexicon, cannot be exploited.Future research will focus on integrating contextualinformation about the syntactic and semantic prop-erties of the words into our models, still keepingin mind the data restrictions for dialects and otherscarce resource languages.
Such additional informa-tion could be implemented by adding a third step toour two-stage model.AcknowledgementsWe thank Paola Merlo for her precious and usefulcomments on this work.
We also thank Eric Wehrlifor allowing us to use the LATL Standard Germanlexicon.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Computational Linguistics, 19(2):263?311.Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,and John Nerbonne.
2006.
Evaluation of string dis-tance algorithms for dialectology.
In Proceedings ofthe ACL Workshop on Linguistic Distances, pages 51?62, Sydney, Australia.Rebecca Hwa, Carol Nichols, and Khalil Sima?an.
2006.Corpus variations for translation lexicon induction.
InProceedings of AMTA?06, pages 74?81, Cambridge,MA, USA.Martin Jansche.
2003.
Inference of String Mappings forLanguage Technology.
Ph.D. thesis, Ohio State Uni-versity.Grzegorz Kondrak and Tarek Sherif.
2006.
Evaluationof several phonetic similarity algorithms on the taskof cognate identification.
In Proceedings of the ACLWorkshop on Linguistic Distances, pages 43?50, Syd-ney, Australia.Gideon S. Mann and David Yarowsky.
2001.
Multipathtranslation lexicon induction via bridge languages.
InProceedings of NAACL?01, Pittsburgh, PA, USA.Werner Marti.
1985.
Berndeutsch-Grammatik.
FranckeVerlag, Bern, Switzerland.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated English and German cor-pora.
In Proceedings of ACL?99, pages 519?526,Maryland, USA.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learn-ing string-edit distance.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 20(5):522?532.Charles Schafer and David Yarowsky.
2002.
Induc-ing translation lexicons via diverse similarity measuresand bridge languages.
In Proceedings of CoNLL?02,pages 146?152, Taipei, Taiwan.60
