Proceedings of the 8th International Conference on Computational Semantics, pages 140?156,Tilburg, January 2009. c?2009 International Conference on Computational SemanticsAn extended model of natural logicBill MacCartney and Christopher D. ManningNatural Language Processing Group, Stanford UniversityAbstractWe propose a model of natural language inference which identifiesvalid inferences by their lexical and syntactic features, without full se-mantic interpretation.
We extend past work in natural logic, which hasfocused on semantic containment and monotonicity, by incorporatingboth semantic exclusion and implicativity.
Our model decomposes aninference problem into a sequence of atomic edits linking premise to hy-pothesis; predicts a lexical semantic relation for each edit; propagatesthese relations upward through a semantic composition tree accordingto properties of intermediate nodes; and joins the resulting semanticrelations across the edit sequence.
A computational implementationof the model achieves 70% accuracy and 89% precision on the FraCaStest suite.
Moreover, including this model as a component in an ex-isting system yields significant performance gains on the RecognizingTextual Entailment challenge.1 IntroductionNatural language inference (NLI) is the problem of determining whethera natural language hypothesis h can reasonably be inferred from a givenpremise p. For example:(1) p: Every firm polled saw costs grow more than expected, even after adjusting for inflation.h: Every big company in the poll reported cost increases.A capacity for open-domain NLI is clearly necessary for full naturallanguage understanding, and NLI can also enable more immediate applica-tions, such as semantic search and question answering.
Consequently, NLIhas been the focus of intense research effort in recent years, centered aroundthe annual Recognizing Textual Entailment (RTE) competition (6).For a semanticist, the most obvious approach to NLI relies on full se-mantic interpretation: first, translate p and h into some formal meaningrepresentation, such as first-order logic (FOL), and then apply automated140reasoning tools to determine inferential validity.
While the formal approachcan succeed in restricted domains, it struggles with open-domain NLI taskssuch as RTE.
For example, the FOL-based system of (1) was able to finda proof for less than 4% of the problems in the RTE1 test set.
The dif-ficulty is plain: truly natural language is fiendishly complex.
The formalapproach faces countless thorny problems: idioms, ellipsis, paraphrase, am-biguity, vagueness, lexical semantics, the impact of pragmatics, and so on.Consider for a moment the difficulty of fully and accurately translating (1)to a formal meaning representation.Yet (1) also demonstrates that full semantic interpretation is often notnecessary to determining inferential validity.
To date, the most successfulNLI systems have relied on surface representations and approximate mea-sures of lexical and syntactic similarity to ascertain whether p subsumes h(9, 13, 10).
However, these approaches face a different problem: they lackthe precision needed to properly handle such commonplace phenomena asnegation, antonymy, downward-monotone quantifiers, non-factive contexts,and the like.
For example, if every were replaced by some or most through-out (1), the lexical and syntactic similarity of h to p would be unaffected,yet the inference would be rendered invalid.In this paper, we explore a middle way, by developing a model of what(11) called natural logic, which characterizes valid patterns of inference interms of syntactic forms which are as close as possible to surface forms.
Forexample, the natural logic approach might sanction (1) by observing that: inordinary upward monotone contexts, deleting modifiers preserves truth; indownward monotone contexts, inserting modifiers preserves truth; and everyis downward monotone in its restrictor NP.
Natural logic thus achieves thesemantic precision needed to handle inferences like (1), while sidesteppingthe difficulties of full semantic interpretation.The natural logic approach has a very long history,1originating in thesyllogisms of Aristotle and continuing through the medieval scholastics andthe work of Leibniz.
It was revived in recent times by (19, 20) and (17),whose monotonicity calculus explains inferences involving semantic contain-ment and inversions of monotonicity, even when nested, as in Nobody canenter without a valid passport |= Nobody can enter without a passport.
How-ever, because the monotonicity calculus lacks any representation of semanticexclusion, it fails to license many simple inferences, such as Stimpy is a cat|= Stimpy is not a poodle.1For a useful overview of the history of natural logic, see (21).
For recent work ontheoretical aspects of natural logic, see (7, 18, 23).141Another model which arguably belongs to the natural logic tradition(though not presented as such) was developed by (15) to explain inferencesinvolving implicatives and factives, even when negated or nested, as in Ed didnot forget to force Dave to leave |= Dave left.
While the model bears someresemblance to the monotonicity calculus, it does not incorporate semanticcontainment or explain interactions between implicatives and monotonicity,and thus fails to license inferences such as John refused to dance |= Johndidn?t tango.In this paper, we propose a new model of natural logic which extends themonotonicity calculus to incorporate semantic exclusion, and partly unifiesit with Nairn et al?s account of implicatives.
We first define an inventory ofbasic semantic relations which includes representations of both containmentand exclusion (section 2).
We then describe a general method for establish-ing the semantic relation between a premise p and a hypothesis h. Given asequence of atomic edits which transforms p into h, we determine the lexicalsemantic relation generated by each edit (section 4); project each lexicalsemantic relation into an atomic semantic relation, according to propertiesof the context in which the edit occurs (section 5); and join atomic semanticrelations across the edit sequence (section 3).
We have previously presentedan implemented system based on this model (14); here we offer a detailedaccount of its theoretical foundations.2 An inventory of semantic relationsThe simplest formulation of the NLI task is as a binary decision prob-lem: the relation between p and h is to be classified as either entailment(p |= h) or non-entailment (p 6|= h).
The three-way formulation refines thisby dividing non-entailment into contradiction (p |= ?h) and compatibility(p 6|= h ?
p 6|= ?h).2The monotonicity calculus carves things up differently:it interprets entailment as a semantic containment relation ?
analogous tothe set containment relation ?, and thus permits us to distinguish forwardentailment (p ?
h) from reverse entailment (p ?
h).
Moreover, it defines ?for expressions of every semantic type, including not only complete sentencesbut also individual words and phrases.
Unlike the three-way formulation,however, it lacks any way to represent contradiction (semantic exclusion).For our model, we want the best of both worlds: a comprehensive inven-tory of semantic relations that includes representations of both semantic2The first three RTE competitions used the binary formulation, while the three-wayformulation was adopted for RTE4.
The three-way formulation was also employed in theFraCaS test suite (5) and has been investigated in depth by (4).142containment and semantic exclusion.Following Sa?nchez Valencia, we proceed by analogy with set relations.In a universe U , the set of ordered pairs ?x, y?
of subsets of U can be parti-tioned into 16 equivalence classes, according to whether each of the four setsx ?
y, x ?
y, x ?
y, and x ?
y is empty or non-empty.3Of these 16 classes,nine represent degenerate cases in which either x or y is either empty oruniversal.
Since expressions having empty denotations (e.g., round squarecupola) or universal denotations (e.g., exists) fail to divide the world intomeaningful categories, they can be regarded as semantically vacuous.
Con-tradictions and tautologies may be common in logic textbooks, but theyare rare in everyday speech.
Thus, in a practical model of informal naturallanguage inference, we will rarely go wrong by assuming the non-vacuity ofthe expressions we encounter.4We therefore focus on the remaining sevenclasses, which we designate as the set B of basic semantic relations.symbol5name example set theoretic definition6x ?
y equivalence couch ?
sofa x = yx ?
y forward entailment crow ?
bird x ?
yx ?
y reverse entailment European ?
French x ?
yx?y negation human?nonhuman x ?
y = ?
?
x ?
y = Ux | y alternation cat | dog x ?
y = ?
?
x ?
y 6= Ux ` y cover animal ` nonhuman x ?
y 6= ?
?
x ?
y = Ux # y independence hungry # hippo (all other cases)First, the semantic containment relations (?
and ?)
of the monotonicitycalculus are preserved, but are factored into three mutually exclusive rela-3We use x to denote the complement of set x in universe U ; thus x ?
x = ?
andx ?
x = U .4Our model can easily be revised to accommodate vacuous expressions and relationsbetween them, but then becomes somewhat unwieldy.
The assumption of non-vacuity isclosely related to the assumption of existential import in traditional logic.
For a defenseof existential import in natural language semantics, see (2).5Selecting an appropriate symbol to represent each relation is a vexed problem.
Wesought symbols which (a) are easily approximated by a single ASCII character, (b) aregraphically symmetric iff the relations they represent are symmetric, and (c) do not exces-sively abuse accepted conventions.
The?symbol was chosen to evoke the logically similarbitwise XOR operator of the C programming language family; regrettably, it may also evokethe Boolean AND function.
The | symbol was chosen to evoke the Sheffer stroke commonlyused to represent the logically similar Boolean NAND function; regrettably, it may alsoevoke the Boolean OR function.
The ?
and ?
symbols were obviously chosen to resembletheir set-theoretic analogs, but a potential confusion arises because some logicians use thehorseshoe ?
(with the opposite orientation) to represent material implication.6Each relation in B obeys the additional constraints that ?
?
x ?
U and ?
?
y ?
U(i.e., x and y are non-vacuous).143tions: equivalence (?
), (strict) forward entailment (?
), and (strict) reverseentailment (?).
Next, we have two relations expressing semantic exclusion:negation (?
), or exhaustive exclusion, which is analogous to set complement;and alternation (|), or non-exhaustive exclusion.
The next relation is cover(`), or non-exclusive exhaustion.
Though its utility is not immediately obvi-ous, it is the dual under negation of the alternation relation.7Finally, the in-dependence relation (#) covers all other cases: it expresses non-equivalence,non-containment, non-exclusion, and non-exhaustion.
Note that # is theleast informative relation, in that it places the fewest constraints on its ar-guments.8Following Sa?nchez Valencia, we define the relations in B for all seman-tic types.
For semantic types which can be interpreted as characteristicfunctions of sets,9the set-theoretic definitions can be applied directly.
Thedefinitions can then be extended to other types by interpreting each type asif it were a type of set.
For example, propositions can be understood (perMontague) as denoting sets of possible worlds.
Thus two propositions standin the | relation iff there is no world where both hold (but there is someworld where neither holds).
Likewise, names can be interpreted as denotingsingleton sets, with the result that two names stand in the ?
relation iffthey refer to the same entity, or the | relation otherwise.By design, the relations in B are mutually exclusive, so that we candefine a function ?
(x, y) which maps every ordered pair of expressions10tothe unique relation in B to which it belongs.3 Joining semantic relationsIf we know that semantic relation R holds between x and y, and that se-mantic relation S holds between y and z, then what is the semantic relationbetween x and z?
The join of semantic relations R and S, which we denote7We describe relations R and S as duals under negation iff ?x, y : ?x, y?
?
R ?
?x, y?
?S.
Thus ?
and ?
are dual; | and ` are dual; and ?,?, and # are self-dual.
Thesignificance of this duality will become apparent in section 5.8Two sets selected uniformly at random from 2Uare overwhelmingly likely to belongto # (for large |U |).9That is, all functional types whose final output is a truth value.
If we assume a typesystem whose basic types are e (entities) and t (truth values), then this includes most of thefunctional types encountered in semantic analysis: e t (common nouns, adjectives, andintransitive verbs), ee t (transitive verbs), (e t)(e t) (adverbs), (e t)(e t) t(binary generalized quantifiers), and so on.10Assuming the expressions are non-vacuous, and belong to the same semantic type.144R ??
S,11is defined by:R ??
Sdef= {?x, z?
: ?y (?x, y?
?
R ?
?y, z?
?
S)}Some joins are quite intuitive.
For example, it is immediately clear that?
??
?
= ?, ?
??
?
= ?,???
?= ?, and for any R, (R ??
?)
= (?
?
?R) = R. Other joins are less obvious, but still accessible to intuition.
Forexample, | ??
?= ?.
This can be seen with the aid of Venn diagrams, or byconsidering simple examples: fish | human and human?nonhuman, thusfish ?
nonhuman.But we soon stumble upon an inconvenient truth: not every join yieldsa relation in B.
For example, if x | y and y | z, the relation between x andz is not determined.
They could be equivalent, or one might contain theother.
They might be independent or alternative.
All we can say for sureis that they are not exhaustive (since both are disjoint from y).
Thus, theresult of joining | and | is not a relation in B, but a union of such relations,specifically?
{?,?,?, |,#}.12We will refer to (non-trivial) unions of relations inB as union relations.13Of the 49 possible joins of relations in B, 32 yield a relation in B, while 17yield a union relation, with larger unions conveying less information.
Unionrelations can be further joined, and we can establish that the smallest setof relations which contains B and is closed under joining contains just 16relations.14One of these is the total relation, which contains all pairs of(non-vacuous) expressions.
This relation, which we denote ?, is the blackhole of semantic relations, in the sense that (a) it conveys zero informationabout pairs of expressions which belong to it, and (b) joining a chain ofsemantic relations will, if it contains any noise and is of sufficient length,11In Tarskian relation algebra, this operation is known as relation composition, and isoften represented by a semi-colon: R ; S. To avoid confusion with semantic composition(section 5), we prefer to use the term join for this operation, by analogy to the databaseJOIN operation (also commonly represented by ??
).12We use this notation as shorthand for the union ?
?
?
?
?
?
| ?
#.
To be precise,the result of this join is not identical with this union, but is a subset of it, since the unioncontains some pairs of sets (e.g.
?U \ a, U \ a?, for any |a| = 1) which cannot participatein the | relation.
However, the approximation makes little practical difference.13Some union relations hold intrinsic interest.
For example, in the three-way formulationof the NLI task described in section 2, the three classes can be identified asS{?,?
},S{?, |}, andS{?,`,#}.14That is, the relations inB plus 9 union relations.
Note that this closure fails to includemost of the 120 possible union relations.
Perhaps surprisingly, the unionsS{?,?}
andS{?, |} mentioned in footnote 13 do not appear.145lead inescapably to ?.15This tendency of joining to devolve toward less-informative semantic relations places an important limitation on the powerof the inference method described in section 7.A complete join table for relations in B is shown below:16??
?
?
?
?| ` #?
?
?
?
?| ` #?
?
?
??
?|# | | ?
?|`# ?|#?
?
??
?`# ?
` ?
?|`# ` ?`#?
?` | ?
?
?
#| | ?
?|`# | ?
??
?|# ?
?|#` ` ` ?
?|`# ?
?
??
?`# ?`## # ?`# ?|# # ?|# ?`# ?In an implemented model, the complexity introduced by union relationsis easily tamed.
Every union relation which results from joining relationsin B contains #, and thus can safely be approximated by #.
After all, #is already the least informative relation in B?loosely speaking, it indicatesignorance of the relationship between two expressions?and further joiningwill never serve to strengthen it.
Our implemented model therefore has noneed to represent union relations.4 Lexical semantic relationsSuppose x is a compound linguistic expression, and let e(x) be the resultof applying an atomic edit e (the deletion, insertion, or substitution of asubexpression) to x.
The semantic relation ?
(x, e(x)) will depend on (1) thelexical semantic relation generated by e, which we label ?
(e), and (2) otherproperties of the context x in which e is applied (to be discussed in section 5).For example, suppose x is red car.
If e is sub(car, convertible), then ?
(e)is ?
(because convertible is a hyponym of car).
On the other hand, if e isdel(red), then ?
(e) is ?
(because red is an intersective modifier).
Crucially,?
(e) depends solely on the lexical items involved in e, independent of context.How are lexical semantic relations determined?
Ultimately, this is theprovince of lexical semantics, which lies outside the scope of this work.
How-ever, the answers are fairly intuitive in most cases, and we can make anumber of useful observations.15In fact, computer experiments show that if relations are selected uniformly at randomfrom B, it requires on average just five joins to reach ?.16For compactness, we omit the union notation here; thus ?|# stands forS{?, |,#}.146Substitutions.
The semantic relation generated by a substitution edit issimply the relation between the substituted terms: ?
(sub(x, y)) = ?
(x, y).For open-class terms such as nouns, adjectives, and verbs, we can oftendetermine the appropriate relation by consulting a lexical resource such asWordNet.
Synonyms belong to the ?
relation (sofa ?
couch, forbid ?
pro-hibit); hyponym-hypernym pairs belong to the?
relation (crow ?
bird, frigid?
cold, soar ?
rise); and antonyms and coordinate terms generally belongto the | relation (hot | cold, cat | dog).17Proper nouns, which denote indi-vidual entities or events, will stand in the ?
relation if they denote the sameentity (USA ?
United States), or the | relation otherwise (JFK | FDR).Pairs which cannot reliably be assigned to another semantic relation will beassigned to the # relation (hungry # hippo).
Of course, there are many dif-ficult cases, where the most appropriate relation will depend on subjectivejudgments about word sense, topical context, and so on?consider, for ex-ample, the pair system and approach.
And some judgments may depend onworld knowledge not readily available to an automatic system.
For example,plausibly skiing | sleeping, but skiing # talking.Closed-class terms may require special handling.
Substitutions involvinggeneralized quantifiers generate a rich variety of semantic relations: all ?every, every ?
some, some?no, no | every, at least four ` at most six, andmost # ten or more.18Two pronouns, or a pronoun and a noun, shouldideally be assigned to the ?
relation if it can determined from context thatthey refer to the same entity, though this may be difficult for an automaticsystem to establish reliably.
Prepositions are somewhat problematic.
Somepairs of prepositions can be interpreted as antonyms, and thus assigned tothe | relation (above | below), but many prepositions are used so flexibly innatural language that they are best assigned to the ?
relation (on [a plane]?
in [a plane] ?
by [plane]).Generic deletions and insertions.
For deletion edits, the default be-havior is to generate the ?
relation (thus red car ?
car).
Insertion edits aresymmetric: by default, they generate the ?
relation (sing ?
sing off-key).This heuristic can safely be applied whenever the affected phrase is an in-tersective modifier, and can usefully be applied to phrases much longer thana single word (car which has been parked outside since last week ?
car).Indeed, this principle underlies most current approaches the RTE task, in17Note that most antonym pairs do not belong to the?relation, since they typically donot exclude the middle.18Some of these assertions assume the non-vacuity (section 2) of the predicates to whichthe quantifiers are applied.147which the premise p often contains much extraneous content not found inthe hypothesis h. Most RTE systems try to determine whether p subsumesh: they penalize new content inserted into h, but do not penalize contentdeleted from p.Special deletions and insertions.
However, some lexical items exhibitspecial behavior upon deletion or insertion.
The most obvious example isnegation, which generates the?relation (didn?t sleep?did sleep).
Implica-tives and factives (such as refuse to and admit that) constitute anotherimportant class of exceptions, but we postpone discussion of them to sec-tion 6.
Then there are non-intersective adjectives such as former and alleged.These have various behavior: deleting former seems to generate the | rela-tion (former student | student), while deleting alleged seems to generate the# relation (alleged spy # spy).
We lack a complete typology of such cases,but consider this an interesting problem for lexical semantics.
Finally, forpragmatic reasons, we typically assume that auxiliary verbs and punctua-tion marks are semantically vacuous, and thus generate the ?
relation upondeletion or insertion.
When combined with the assumption that morphologymatters little in inference,19this allows us to establish, e.g., that is sleeping?
sleeps and did sleep ?
slept.5 Semantic relations and semantic compositionHow are semantic relations affected by semantic composition?
In otherwords, how do the semantic relations between compound expressions dependon the semantic relations between their parts?
Say we have established thevalue of ?
(x, y), and let f be an expression which can take x or y as anargument.
What is the value of ?
(f(x), f(y)), and how does it depend onthe properties of f?The monotonicity calculus of Sa?nchez Valencia provides a partial an-swer.
It explains the impact of semantic composition on semantic relations?, ?, ?, and # by assigning semantic functions to one of three monotonic-ity classes: up, down, and non.
If f has monotonicity up (the default),then the semantic relation between x and y is projected through f withoutchange: ?
(f(x), f(y)) = ?
(x, y).
Thus some parrots talk ?
some birds talk.If f has monotonicity down, then ?
and ?
are swapped.
Thus no carptalk ?
no fish talk.
Finally, if f has monotonicity non, then ?
and ?
areprojected as #.
Thus most humans talk # most animals talk.19Indeed, the official definition of the RTE task explicitly specifies that tense be ignored.148The monotonicity calculus also provides an algorithm for computing theeffect on semantic relations of multiple levels of semantic composition.
Al-though Sa?nchez Valencia?s presentation of this algorithm uses a complexscheme for annotating nodes in a categorial grammar parse, the central ideacan be recast in simple terms: propagate a lexical semantic relation upwardthrough a semantic composition tree, from leaf to root, while respectingthe monotonicity properties of each node along the path.
Consider the sen-tence Nobody can enter without pants.
A plausible semantic compositiontree for this sentence could be rendered as (nobody (can ((without pants)enter))).
Now consider replacing pants with clothes.
We begin with thelexical semantic relation: pants ?
clothes.
The semantic function withouthas monotonicity down, so without pants ?
without clothes.
Continuing upthe semantic composition tree, can has monotonicity up, but nobody hasmonotonicity down, so we get another reversal, and find that nobody canenter without pants ?
nobody can enter without clothes.While the monotonicity calculus elegantly explains the impact of seman-tic composition on the containment relations (chiefly, ?
and ?
), it lacksany account of the exclusion relations (?and |, and, indirectly, `).
Toremedy this lack, we propose to generalize the concept of monotonicity toa concept of projectivity.
We categorize semantic functions into a numberof projectivity signatures, which can be seen as generalizations of both thethree monotonicity classes of Sa?nchez Valencia and the nine implication sig-natures of Nairn et al (see section 6).
Each projectivity signature is definedby a map B 7?
B which specifies how each semantic relation is projectedby the function.
(Binary functions can have different signatures for eachargument.)
In principle, there are up to 77possible signatures; in practice,probably no more than a handful are realized by natural language expres-sions.
Though we lack a complete inventory of projectivity signatures, wecan describe a few important cases.Negation.
We begin with simple negation (not).
Like most functions, itprojects ?
and # without change (not happy ?
not glad and isn?t swimming# isn?t hungry).
As a downward monotone function, it swaps ?
and ?
(didn?t kiss ?
didn?t touch).
But we can also establish that it projects?without change (not human?not nonhuman) and swaps | and ` (not French` not German and not more than 4 | not less than 6 ).
Its projectivitysignature is therefore {?:?,?:?,?:?,?
:?, | :`,`: |,#:#}.149Intersective modification.
Intersective modification has monotonicityup, but projects both?and | as | (living human | living nonhuman andFrench wine | Spanish wine), and projects ` as # (metallic pipe # nonfer-rous pipe).
It therefore has signature {?:?,?:?,?:?,?
: |, | : |,`:#,#:#}.20Quantifiers.
While semanticists are well acquainted with the monotonic-ity properties of common quantifiers, how they project the exclusion rela-tions may be less familiar.
The following table summarizes the projectivitysignatures of the most common binary generalized quantifiers for each ar-gument position:projectivity for 1stargument projectivity for 2ndargumentquantifier ?
?
?
?| ` # ?
?
?
?| ` #some ?
?
?
`?# `?# ?
?
?
`?# `?#no ?
?
?
|?# |?# ?
?
?
|?# |?#every ?
?
?
|?# |?# ?
?
?
|?|?# #not every ?
?
?
`?# `?# ?
?
?
`?`?# #A few observations:?
All quantifiers (like most other semantic functions) project ?
and #without change.?
The table confirms well-known monotonicity properties: no is downward-monotone in both arguments, every in its first argument, and not everyin its second argument.?
Relation | is frequently ?blocked?
by quantifiers (i.e., projected as #).Thus no fish talk # no birds talk and someone was early # someonewas late.
A notable exception is every in its second argument, where| is preserved: everyone was early | everyone was late.
(Note thesimilarity to intersective modification.)?
Because no is the negation of some, its projectivity signature can befound by projecting the signature of some through the signature ofnot.
Likewise for not every and every.?
Some results depend on assuming the non-vacuity of the other argu-ment to the quantifier: those marked with?assume it to be non-empty,while those marked with?assume it to be non-universal.
Withoutthese assumptions, # is projected.20At least for practical purposes.
The projection of?and | as | depends on the assump-tion of non-vacuity, and ` is actually projected asS{?,?,?, |,#}, which we approximateby #, as described in section 3.150Verbs.
Verbs (and verb-like constructions) exhibit diverse behavior.
Mostverbs are upward-monotone (though not all?see section 6), and many verbsproject?, |, and ` as # (eats humans # eats nonhumans, eats cats # eatsdogs, and eats mammals # eats nonhumans).
However, verbs which encodefunctional relations seem to exhibit the same projectivity as intersectivemodifiers, projecting?and | as |, and` as #.21Categorizing verbs accordingto projectivity is an interesting problem for lexical semantics, which mayinvolve codifying some amount of world knowledge.6 Implicatives and factives(15) offer an elegant account of inferences involving implicatives and fac-tives22such as manage to, refuse to, and admit that.
Their model clas-sifies such operators into nine implication signatures, according to theirimplications?positive (+), negative (?
), or null (?
)?in both positive andnegative contexts.
Thus refuse to has implication signature ?/?, becauseit carries a negative implication in a positive context (refused to dance im-plies didn?t dance), and no implication in a negative context (didn?t refuseto dance implies neither danced nor didn?t dance).Most of the phenomena observed by Nairn et al can be explained withinour framework by specifying, for each implication signature, the relationgenerated when an operator of that signature is deleted from (or insertedinto) a compound expression, as shown in the following table:signature ?(del(?))
?(ins(?))
exampleimplicatives +/?
?
?
he managed to escape ?
he escaped(up) +/?
?
?
he was forced to sell ?
he sold?/?
?
?
he was permitted to live ?
he livedimplicatives ?/+?
?he forgot to pay?he paid(down) ?/?
| | he refused to fight | he fought?/+ ` ` he hesitated to ask ` he askedfactives +/+ ?
?
he admitted that he knew ?
he knew(non) ?/?
| | he pretended he was sick | he was sick?/?
# # he wanted to fly # he flewThis table invites several observations.
First, as the examples make clear,there is room for variation regarding the appearance of infinitive arguments,21Consider the verbal construct is married to: is married to a German | is married to anon-German, is married to a German | is married to an Italian, is married to a European# is married to a non-German.
The AuContraire system (16) includes an intriguingapproach to identifying such functional phrases automatically.22We use ?factives?
as an umbrella term embracing counterfactives and nonfactivesalong with factives proper.151complementizers, passivization, and morphology.
An implemented modelmust tolerate such diversity.Second, some of the examples may seem more intuitive when one consid-ers their negations.
For example, deleting signature ?/?
generates ?
; undernegation, this is projected as ?
(he wasn?t permitted to live ?
he didn?tlive).
Likewise, deleting signature ?/+ generates `; under negation, this isprojected as | (he didn?t hesitate to ask | he didn?t ask).Third, a fully satisfactory treatment of the factives (signatures +/+, ?/?, and ?/?)
would require an extension to our present theory.
For example,deleting signature +/+ generates ?
; yet under negation, this is projectednot as ?, but as | (he didn?t admit that he knew | he didn?t know).
Theproblem arises because the implication carried by a factive is not an entail-ment, but a presupposition.23As is well known, the projection behavior ofpresuppositions differs from that of entailments (22).
It seems likely thatour model could be elaborated to account for projection of presuppositionsas well as entailments, but we leave this for future work.We can further cement implicatives and factives within our model byspecifying the monotonicity class for each implication signature: signatures+/?, +/?, and ?/?
have monotonicity up (force to tango ?
force to dance);signatures ?/+, ?/?, and ?/+ have monotonicity down (refuse to tango ?refuse to dance); and signatures +/+, ?/?, and ?/?
(the propositional at-titudes) have monotonicity non (think tangoing is fun # think dancing isfun).
We are not yet able to specify the complete projectivity signature cor-responding to each implication signature, but we can describe a few specificcases.
For example, implication signature ?/?
seems to project?as | (refuseto stay | refuse to go) and both | and ` as # (refuse to tango # refuse towaltz ).7 Putting it all togetherWe now have the building blocks of a general method to establish the se-mantic relation between a premise p and a hypothesis h. The steps are asfollows:1.
Find a sequence of atomic edits ?e1, .
.
.
, en?
which transforms p intoh: thus h = (en?
.
.
.
?
e1)(p).
For convenience, let us define x0= p,xn= h, and xi= ei(xi?1) for i ?
[1, n].2.
For each atomic edit ei:23Of course, the implicatives may carry presuppositions as well (he managed to escape it was hard to escape), but these implications are not activated by a simple deletion, aswith the factives.152(a) Determine the lexical semantic relation ?
(ei), as in section 4.
(b) Project ?
(ei) upward through the semantic composition tree ofexpression xi?1to find an atomic semantic relation ?
(xi?1, ei) =?
(xi?1, xi), as in section 5.3.
Join atomic semantic relations across the sequence of edits, as in sec-tion 3:?
(p, h) = ?
(x0, xn) = ?
(x0, e1) ??
.
.
.
??
?
(xi?1, ei) ??
.
.
.
??
?
(xn?1, en)However, this inference method has several important limitations, in-cluding the need to find an appropriate edit sequence connecting p and h;24the tendency of the join operation toward less informative semantic rela-tions, as described in section 3; and the lack of a general mechanism forcombining information from multiple premises.25Consequently, the methodhas less deductive power than first-order logic, and fails to sanction somefairly simple inferences, including de Morgan?s laws for quantifiers.
But themethod neatly explains many inferences not handled by the monotonicitycalculus, including this example from section 1:i xiei?
(ei) ?
(xi?1, ei) ?
(x0, xi)0 Stimpy is a cat1 Stimpy is a dog sub(cat, dog) | | |2 Stimpy is not a dog ins(not)?
?
?3 Stimpy is not a poodle sub(dog, poodle) ?
?
?Here, x0is transformed into x3by a sequence of three edits.
First, replacingcat with its coordinate term dog generates |.
Next, inserting not generates?, and | joined with?yields ?.
Finally, replacing dog with its hyponympoodle generates ?.
Because of the downward-monotone context created bynot, this is projected as ?, and ?
joined with ?
yields ?.
Therefore, x0entails x3.For an example involving an implicative, consider:24The order of edits can be significant, if one edit affects the projectivity properties ofthe context for another edit.
In practice, we typically find that different edit orders lead tothe same final result (albeit via different intermediate steps), or at worst to a result whichis compatible with, though less informative than, the desired result.
But in principle, editsequences involving lexical items with unusual properties?not exhibited, so far as we areaware, by any natural language expressions?could lead to incompatible results.
Thus welack any formal guarantee of soundness.25However, some inferences can be enabled by auxiliary premises encoded as lexicalsemantic relations.
For example, men ?
mortal can enable the classic syllogism Socratesis a man ?
Socrates is mortal.153i xiei?
(ei) ?
(xi?1, ei) ?
(x0, xi)0 We were not permitted to smoke1 We did not smoke del(permitted to) ?
?
?2 We smoked del(not)?
?|3 We smoked Cuban cigars ins(Cuban cigars) ?
?
|Again, x0is transformed into x3by a sequence of three edits.26First,deleting permitted to generates ?, according to its implication signature; butbecause not is downward-monotone, this is projected as ?.
Next, deletingnot generates?, and ?
joined with?yields |.
Finally, inserting Cuban cigarsrestricts the meaning of smoked, generating ?, and | joined with ?
yields |.So x3contradicts x0.A more complex example is presented in (14).8 Implementation and evaluationThe model of natural logic described here has been implemented in softwareas the NatLog system.
In previous work (14), we have presented a descrip-tion and evaluation of NatLog; this section summarizes the main results.Natlog faces three primary challenges:1.
Finding an appropriate sequence of atomic edits connecting premiseand hypothesis.
NatLog does not address this problem directly, butrelies instead on edit sequences from other sources.
We have investi-gated this problem separately in (12).2.
Determining the lexical semantic relation for each edit.
NatLog learnsto predict lexical semantic relations by using machine learning tech-niques and exploiting a variety of manually and automatically con-structed sources of information on lexical relations.3.
Computing the projection of each lexical semantic relation.
NatLogidentifies expressions with non-default projectivity and computes thelikely extent of their arguments in a syntactic parse using hand-craftedtree patterns.We have evaluated NatLog on two different test suites.
The first is theFraCaS test suite (5), which contains 346 NLI problems, divided into ninesections, each focused on a specific category of semantic phenomena.
Thegoal is three-way entailment classification, as described in section 2.
On26We neglect edits involving auxiliaries and morphology, which simply yield the ?
rela-tion.154this task, NatLog achieves an average accuracy of 70%.27In the sectionconcerning quantifiers, which is both the largest and the most amenableto natural logic, the system answers all problems but one correctly.
Un-surprisingly, performance is mediocre in four sections concerning semanticphenomena (e.g., ellipsis) not relevant to natural logic and not modeled bythe system.
But in the other five sections (representing about 60% of theproblems), NatLog achieves accuracy of 87%.
What?s more, precision is uni-formly high, averaging 89% over all sections.
Thus, even outside its areas ofexpertise, the system rarely predicts entailment when none exists.The RTE3 test suite (8) differs from FraCaS in several important ways:the goal is binary entailment classification; the problems have much longerpremises and are more ?natural?
; and the problems employ a diversity oftypes of inference?including paraphrase, temporal reasoning, and relationextraction?which NatLog is not designed to address.
Consequently, theNatLog system by itself achieves mediocre accuracy (59%) on RTE3 prob-lems.
However, its precision is comparatively high, which suggests a strategyof hybridizing with a broad-coverage RTE system.
We were able to showthat adding NatLog as a component in the Stanford RTE system (3) led toaccuracy gains of 4%.9 ConclusionThe model of natural logic presented here is by no means a universal solutionto the problem of natural language inference.
Many NLI problems hinge ontypes of inference not addressed by natural logic, and the inference methodwe describe faces a number of limitations on its deductive power (discussedin section 7).
Moreover, there is further work to be done in fleshing out ouraccount, particularly in establishing the proper projectivity signatures fora broader range of quantifiers, verbal constructs, implicatives and factives,logical connectives, and other semantic functions.Nevertheless, we believe our model of natural logic fills an importantniche.
While approximate methods based on lexical and syntactic similaritycan handle many NLI problems, they are easily confounded by inferences in-volving negation, antonymy, quantifiers, implicatives, and many other phe-nomena.
Our model achieves the logical precision needed to handle suchinferences without resorting to full semantic interpretation, which is in anycase rarely possible.
The practical value of the model is demonstrated byits success in evaluations on the FraCaS and RTE3 test suites.27Our evaluation excluded multi-premise problems, which constitute about 44% of thetest suite.155References[1] J. Bos and K. Markert.
Recognising textual entailment with logical inference.
In Proceedings ofEMNLP-05, 2005.
[2] M. Bo?ttner.
A note on existential import.
Studia Logica, 47(1):35?40, 1988.
[3] N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kiddon, B. MacCartney, M. C. de Marneffe,D.
Ramage, E. Yeh, and C. D. Manning.
Learning Alignments and Leveraging NaturalLogic.
In Proceedings of the ACL-07 Workshop on Textual Entailment and Paraphrasing,2007.
[4] C. Condoravdi, D. Crouch, V. de Paiva, R. Stolle, and D.G.
Bobrow.
Entailment, Intensional-ity and Text Understanding.
In Proceedings of the HLT-NAACL 2003 Workshop on TextMeaning, Morristown, NJ, USA, 2003.
[5] Robin Cooper et al Using the framework.
Technical Report LRE 62-051 D-16, The FraCaSConsortium, 1996.
[6] I. Dagan, O. Glickman, and B. Magnini.
The PASCAL Recognising Textual Entailment Challenge.In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,2005.
[7] Y. Fyodorov, Y.
Winter, and N. Francez.
A Natural Logic Inference System.
In Proceedings ofthe 2nd Workshop on Inference in Computational Semantics (ICoS-2), 2000.
[8] D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan.
The Third PASCAL Recognizing TextualEntailment Challenge.
In Proceedings of the ACL-07 Workshop on Textual Entailment andParaphrasing, 2007.
[9] O. Glickman, I. Dagan, and M. Koppel.
Web based probabilistic textual entailment.
In Proceedingsof the PASCAL Challenges Workshop on Recognizing Textual Entailment, 2005.
[10] A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink, and Y. Shi.
Recognizing Textual Entail-ment with LCC?s GROUNDHOG System.
In Proceedings of the Second PASCAL ChallengesWorkshop on Recognizing Textual Entailment, 2006.
[11] G. Lakoff.
Linguistics and natural logic.
Synthese, 22:151?271, 1970.
[12] B. MacCartney, M. Galley, and C. D. Manning.
A phrase-based alignment model for naturallanguage inference.
In Proceedings of EMNLP-08, Honolulu, HI, 2008.
[13] B. MacCartney, T. Grenager, M. C. de Marneffe, D. Cer, and C. D. Manning.
Learning toRecognize Features of Valid Textual Entailments.
In Proceedings of NAACL-06, New York,2006.
[14] B. MacCartney and C. D. Manning.
Modeling semantic containment and exclusion in naturallanguage inference.
In Proceedings of Coling-08, Manchester, UK, 2008.
[15] R. Nairn, C. Condoravdi, and L. Karttunen.
Computing relative polarity for textual inference.
InProceedings of ICoS-5, Buxton, UK, 2006.
[16] A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
It?s a Contradiction?No, it?s Not: A CaseStudy using Functional Relations.
In Proceedings of EMNLP-08, 2008.
[17] V. Sa?nchez Valencia.
Studies on Natural Logic and Categorial Grammar.
PhD thesis, Univ.Amsterdam, 1991.
[18] J. Sukkarieh.
Quasi-NL Knowledge Representation for Structurally-Based Inferences.
In Proceed-ings of the 3rd Workshop on Inference in Computational Semantics (ICoS-3), 2001.
[19] J. van Benthem.
The semantics of variety in categorial grammars.
In W. Buszkowski, W. Mar-ciszewski, and J. van Benthem, editors, Categorial grammar, pages 33?55.
John Benjamins,Amsterdam, 1988.
[20] J. van Benthem.
Language in Action: categories, lambdas and dynamic logic, volume 130 ofStudies in Logic.
North-Holland, Amsterdam, 1991.
[21] J. van Benthem.
A brief history of natural logic.
Technical Report PP-2008-05, Institute for Logic,Language & Computation, 2008.
[22] R. A. van der Sandt.
Presupposition projection as anaphora resolution.
Journal of Semantics,9(4), 1992.
[23] J. van Eijck.
Natural logic for natural language.
http://homepages.cwi.nl/\texttt{\~}jve/papers/05/nlnl/NLNL.pdf, 2005.156
