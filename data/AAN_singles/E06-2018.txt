Exploring the Sense Distributions of HomographsReinhard RappUniversity of Mainz, FASK76711 Germersheim, Germanyrrapp@uni-mainz.deAbstractThis paper quantitatively investigates inhow far local context is useful to disam-biguate the senses of an ambiguous word.This is done by comparing the co-occur-rence frequencies of particular contextwords.
First, one context word repre-senting a certain sense is chosen, and thenthe co-occurrence frequencies with twoother context words, one of the same andone of another sense, are compared.
Asexpected, it turns out that context wordsbelonging to the same sense have consid-erably higher co-occurrence frequenciesthan words belonging to different senses.In our study, the sense inventory is takenfrom the University of South Floridahomograph norms, and the co-occurrencecounts are based on the British NationalCorpus.1 IntroductionWord sense induction and disambiguation is ofimportance for many tasks in speech and lan-guage processing, such as speech recognition,machine translation, natural language under-standing, question answering, and information re-trieval.
As evidenced by several SENSEVALsense disambiguation competitions (Kilgarriff &Palmer, 2000), statistical methods are dominantin this field.
However, none of the published al-gorithms comes close to human performance inword sense disambiguation, and it is thereforeunclear in how far the statistical regularities thatare exploited in these algorithms are a solid basisto eventually solve the problem.Although this is a difficult question, in thisstudy we try to give at least a partial answer.
Ourstarting point is the observation that ambiguouswords can usually be disambiguated by their con-text, and that certain context words can be seenas indicators of certain senses.
For example, con-text words such as finger and arm are typical ofthe hand meaning of palm, whereas coconut andoil are typical of its tree meaning.
The essencebehind many algorithms for word sense disam-biguation is to implicitly or explicitly classify allpossible context words into groups relating toone or another sense.
This can be done in a su-pervised (Yarowsky, 1994), a semi-supervised(Yarowsky, 1995) or a fully unsupervised way(Pantel & Lin, 2002).However, the classification can only work ifthe statistical clues are clear enough and if thereare not too many exceptions.
In terms of wordco-occurrence statistics, we can say that withinthe local contexts of an ambiguous word, contextwords typical of the same sense should have highco-occurrence counts, whereas context words as-sociated with different senses should have co-occurrence counts that are considerably lower.Although the relative success of previous disam-biguation systems (e.g.
Yarowsky, 1995) sug-gests that this should be the case, the effect hasusually not been quantified as the emphasis wason a task-based evaluation.
Also, in most casesthe amount of context to be used has not beensystematically examined.2 MethodologyOur starting point is a list of 288 ambiguouswords (homographs) where each comes togetherwith two associated words that are typical of onesense and a third associated word that is typicalof another sense.
Table 1 shows the first ten en-tries in the list.
It has been derived from the Uni-versity of South Florida homograph norms (Nel-son et al, 1980) and is based on a combination ofnative speakers?
intuition and the expertise ofspecialists.The University of South Florida homographnorms comprise 320 words which were all se-lected from Roget?s International Thesaurus(1962).
Each word has at least two distinct mean-ings that were judged as likely to be understoodby everyone.
As described in detail in Nelson etal.
(1980), the compilation of the norms was con-ducted as follows: 46 subjects wrote down thefirst word that came to mind for each of the 320homographs.
In the next step, for each homo-graph semantic categories were chosen to reflect155its meanings.
All associative responses given bythe subjects were assigned to one of these catego-ries.
This was first done by four judges individu-ally, and then, before final categorization, eachresponse was discussed until a consensus wasachieved.The data used in our study (first ten itemsshown in Table 1) was extracted from thesenorms by selecting for each homograph the firsttwo words relating to its first meaning and thefirst word relating to its second meaning.Thereby we had to abandon those homographswhere all of the subjects?
responses had been as-signed to a single category, so that only one cate-gory appeared in the homograph norms.
This wasthe case for 32 words, which is the reason thatour list comprises only 288 instead of 320 items.Another resource that we use is the British Na-tional Corpus (BNC), which is a balanced sampleof written and spoken English that comprisesabout 100 million words (Burnard & Aston,1998).
This corpus was used without special pre-processing, i.e.
stop words were not removed andno stemming was conducted.
From the corpus weextracted concordances comprising text windowsof a certain width (e.g.
plus and minus 20 wordsaround the given word) for each of the 288homographs.
For each concordance we computedtwo counts: The first is the number of con-cordance lines where the two words associatedwith sense 1 occur together.
The second is thenumber of concordance lines where the first wordassociated with sense 1 and the word associatedwith sense 2 co-occur.
The expectation is that thefirst count should be higher as words associatedto the same sense should co-occur more oftenthan words associated to different senses.sense 1 sense 2homo-graph first asso-ciation (w1)second asso-ciation (w2)first asso-ciation (w3)arm leg hand warball game base dancebar drink beer crowbark dog loud treebase ball line bottombass fish trout drumbat ball boy flybay Tampa water houndbear animal woods weightbeam wood ceiling lightTable 1.
First ten of 288 homographs and someassociations to their first and second senses.However, as absolute word frequencies canvary over several orders of magnitude and as thiseffect could influence our co-occurrence countsin an undesired way, we decided to take this intoaccount by dividing the co-occurrence counts bythe concordance frequency of the second wordsin our pairs.
We did not normalize for the fre-quency of the first word as it is identical for bothpairs and therefore represents a constant factor.Note that we normalized for the observed fre-quency within the concordance and not withinthe entire corpus.If we denote the first word associated tosense 1 with w1, the second word associated withsense 1 with w2, and the word associated withsense 2 with w3, the two scores s1 and s2 that wecompute can be described as follows:In cases where the denominator was zero we as-signed a score of zero to the whole expression.For all 288 homographs we compared s1 to s2.
Ifit turns out that in the vast majority of cases s1 ishigher than s2, then this result would be an indi-cator that it is promising to use such co-occur-rence statistics for the assignment of contextwords to senses.
On the other hand, should thisnot be the case, the conclusion would be that thisapproach does not have the potential to work andshould be discarded.As in statistics the results are often not as clearcut as would be desirable, for comparison weconducted another experiment to help us with theinterpretation.
This time the question waswhether our results were caused by properties ofthe homographs or if we had only measuredproperties of the context words w1, w2 and w3.The idea was to conduct the same experimentagain, but this time not based on concordancesbut on the entire corpus.
However, consideringthe entire corpus would make it necessary to usea different kind of text window for counting theco-occurrences as there would be no given wordto center the text window around, which couldlead to artefacts and make the comparison prob-lematic.
We therefore decided to use concor-dances again, but this time not the concordancesof the homographs (first column in Table 1) butthe concordances of all 288 instances of w1 (sec-ond column in Table 1).
This way we had exactlynumber of lines where w1 and w2 co-occurs1 =occurrence count of w2 within concordancenumber of lines where w1 and w3 co-occurs2 =occurrence count of w3 within concordance156the same window type as in the first experiment,but this time the entire corpus was taken into ac-count as all co-occurrences of w2 or w3 with w1must necessarily appear within the concordanceof w1.We name the scores resulting from this ex-periment s3 and s4, where s3 corresponds to s1and s4 corresponds to s2, with the only differencebeing that the concordances of the homographsare replaced by the concordances of the instancesof w1.
Regarding the interpretation of the results,if the ratio between s3 and s4 should turn out tobe similar to the ratio between s1 and s2, then theinfluence of the homographs would be margin-ally or non existent.
If there should be a majordifference, then this would give evidence that, asdesired, a property of the homograph has beenmeasured.3 Results and discussionFollowing the procedure described in the previ-ous section, Table 2 gives some quantitative re-sults.
It shows the overall results for the homo-graph-based concordance and for the w1-basedconcordance for different concordance widths.
Ineach case not only the number of cases is givenwhere the results correspond to expectations(s1 > s2 and s3 > s4), but also the number ofcases where the outcome is undecided (s1 = s2and s3 = s4).
Although this adds some redun-dancy, for convenience also the number of caseswith an unexpected outcome is listed.
All threenumbers sum up to 288 which is the total numberof homographs considered.If we look at the left half of Table 2 whichshows the results for the concordances based onthe homographs, we can see that the number ofcorrect cases steadily increases with increasingwidth of the concordance until a width of ?300 isreached.
At the same time, the number of unde-cided cases rapidly goes down.
At a concordancewidth of ?300, the number of correct cases (201)outnumbers the number of incorrect cases (63) bya factor of 3.2.
Note that the increase of incorrectcases is probably mostly an artefact of the sparse-data-problem as the number of undecided casesdecreases faster than the number of correct casesincreases.On the right half of Table 2 the results for theconcordances based on w1 are given.
Here thenumber of correct cases starts at a far higher levelfor small concordance widths, increases up to aconcordance width of ?10 where it reaches itsmaximum, and then decreases slowly.
At theconcordance width of ?10 the ratio between cor-rect and incorrect cases is 2.6.How can we now interpret these results?
Whatwe can say for sure when we look at the numberof undecided cases is that the problem of datasparseness is much more severe if we considerthe concordances of the homographs rather thanthe concordances of w1.
This outcome can be ex-pected as in the first case we only take a (usuallysmall) fraction of the full corpus into account,whereas the second case is equivalent to consid-ering the full corpus.
What we can also say is thatthe optimal concordance width depends on datasparseness.
If data is more sparse, we need awider concordance width to obtain best results.concordance of homograph concordance of w1concordancewidth s1 > s2corrects1 = s2undecideds1 < s2incorrects3 > s4corrects3 = s4undecideds3 < s4incorrect?1 1 287 0 107 135 46?2 15 273 0 158 69 61?3 32 249 7 179 40 69?5 54 222 12 194 21 73?10 81 181 26 199 13 76?20 126 127 35 196 7 85?30 129 105 44 192 5 91?50 165 69 54 192 2 94?100 182 44 62 185 1 102?200 198 29 61 177 1 110?300 201 24 63 177 1 110?500 199 19 70 171 1 116Table 2.
Results for homograph-based concordance (left) and for w1-based concordance (right).157In case of the full corpus the optimal width isaround ?10 which is similar to average sentencelength.
Larger windows seem to reduce saliencyand therefore affect the results adversely.
Incomparison, if we look at the concordances ofthe homographs, the negative effect on saliencywith increasing concordance width seems to bemore than outweighed by the decrease in sparse-ness, as the results at a very large width of ?300are better than the best results for the full corpus.However, if we used a much larger corpus thanthe BNC, it can be expected that best resultswould be achieved at a smaller width, and thatthese are likely to be better than the onesachieved using the BNC.4 Conclusions and future workOur experiments showed that associations be-longing to the same sense of a homograph havefar higher co-occurrence counts than associationsbelonging to different senses.
This is especiallytrue when we look at the concordances of thehomographs, but ?
to a somewhat lesser extend ?also when we look at the full corpus.
The dis-crepancy between the two approaches can proba-bly be enlarged by increasing the size of the cor-pus.
However, further investigations are neces-sary to verify this claim.With the approach based on the concordancesof the homographs best results were achievedwith concordance widths that are about an orderof magnitude larger than average sentencelength.
However, human performance shows thatthe context within a sentence usually suffices todisambiguate a word.
A much larger corpuscould possibly solve this problem as it should al-low to reduce concordance width without loosingaccuracy.
However, since human language ac-quisition seems to be based on the reception ofonly in the order of 100 million words (Lan-dauer & Dumais, 1997, p. 222), and because theBNC already is of that size, there also must beanother solution to this problem.Our suggestion is to not look at the co-occur-rence frequencies of single word pairs, but at theaverage co-occurrence frequencies between sev-eral pairs derived from larger groups of words.Let us illustrate this by coming back to our ex-ample in the introduction, where we stated thatcontext words such as finger and arm are typicalof the hand meaning of palm, whereas coconutand oil are typical of its tree meaning.
Thesparse-data-problem may possibly prevent ourexpectation come true, namely that finger andarm co-occur more often than finger and coco-nut.
But if we add other words that are typical ofthe hand meaning, e.g.
hold or wrist, then an in-cidental lack of observed co-occurrences be-tween a particular pair can be compensated byco-occurrences between other pairs.
Since thenumber of possible pairs increases quadraticallywith the number of words that are considered,this should have a significant positive effect onthe sparse-data-problem, which is to be exam-ined in future work.AcknowledgmentsI would like to thank the three anonymous re-viewers for their detailed and helpful comments.ReferencesBurnard, Lou.
; Aston, Guy (1998).
The BNCHandbook: Exploring the British NationalCorpus with Sara.
Edinburgh UniversityPress.Kilgarriff, Adam; Palmer, Martha (eds.)
(2000).International Journal of Computers and theHumanities.
Special Issue on SENSEVAL,34(1-2), 2000.Landauer, Thomas K.; Dumais, Susan S. (1997).A solution to Plato?s problem: the latent se-mantic analysis theory of acquisition, induc-tion and representation of knowledge.
Psy-chological Review 104(2), 211-240.Nelson, Douglas L.; McEvoy, Cathy L.; Walling,John R.; Wheeler, Joseph W. (1980).
TheUniversity of South Florida homographnorms.
Behavior Research Methods & Instru-mentation 12(1), 16-37.Pantel, Patrick; Lin, Dekang (2002).
Discoveringword senses from text.
In: Proceedings ofACM SIGKDD, Edmonton, 613-619.Roget?s International Thesaurus (3rd ed., 1962).New York: Crowell.Yarowsky, David (1994).
Decision lists for lexi-cal ambiguity resolution: application to accentrestoration in Spanish and French.
Proceed-ings of the 32nd Meeting of the ACL, Las Cru-ces, NM, 88-95.Yarowsky, David (1995).
Unsupervised wordsense disambiguation rivaling supervised me-thods.
Proceedings of the 33rd Meeting of theACL, Cambridge, MA, 189-196.158
