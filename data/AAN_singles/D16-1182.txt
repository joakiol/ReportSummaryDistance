Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1765?1774,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAn Evaluation of Parser Robustness for Ungrammatical SentencesHoma B. HashemiIntelligent Systems ProgramUniversity of Pittsburghhashemi@cs.pitt.eduRebecca HwaComputer Science DepartmentUniversity of Pittsburghhwa@cs.pitt.eduAbstractFor many NLP applications that require aparser, the sentences of interest may not bewell-formed.
If the parser can overlook prob-lems such as grammar mistakes and producea parse tree that closely resembles the correctanalysis for the intended sentence, we say thatthe parser is robust.
This paper compares theperformances of eight state-of-the-art depen-dency parsers on two domains of ungrammat-ical sentences: learner English and machinetranslation outputs.
We have developed anevaluation metric and conducted a suite of ex-periments.
Our analyses may help practition-ers to choose an appropriate parser for theirtasks, and help developers to improve parserrobustness against ungrammatical sentences.1 IntroductionPrevious works have shown that, in general, parserperformances degrade when applied to out-of-domain sentences (Gildea, 2001; McClosky et al,2010; Foster, 2010; Petrov et al, 2010; Foster et al,2011).
If a parser performs reasonably well for awide range of out-of-domain sentences, it is said tobe robust (Bigert et al, 2005; Kakkonen, 2007; Fos-ter, 2007).Sentences that are ungrammatical, awkward, ortoo casual/colloquial can all be seen as special kindsof out-of-domain sentences.
These types of sen-tences are commonplace for NLP applications, fromproduct reviews and social media analysis to intel-ligent language tutors and multilingual processing.Since parsing is an essential component for manyapplications, it is natural to ask: Are some parsersmore robust than others against sentences that arenot well-formed?
Previous works on parser evalu-ation that focused on accuracy and speed (Choi etal., 2015; Kummerfeld et al, 2012; McDonald andNivre, 2011; Kong and Smith, 2014) have not takenungrammatical sentences into consideration.In this paper, we report a set of empirical analy-ses of eight leading dependency parsers on two do-mains of ungrammatical text: English-as-a-SecondLanguage (ESL) learner text and machine transla-tion (MT) outputs.
We also vary the types of train-ing sources; the parsers are trained with the PennTreebank (to be comparable with other studies) andTweebank, a treebank on tweets (to be a bit morelike the test domain) (Kong et al, 2014).The main contributions of the paper are:?
a metric and methodology for evaluating un-grammatical sentences without referring to agold standard corpus;?
a quantitative comparison of parser accuracy ofleading dependency parsers on ungrammaticalsentences; this may help practitioners to selectan appropriate parser for their applications; and?
a suite of robustness analyses for the parsers onspecific kinds of problems in the ungrammati-cal sentences; this may help developers to im-prove parser robustness in the future.2 Evaluation of Parser RobustnessParser evaluation for ungrammatical texts presentssome domain-specific challenges.
The typical ap-proach to evaluate parsers is to compare parser out-1765puts against manually annotated gold standards.
Al-though there are a few small semi-manually con-structed treebanks on learner texts (Geertzen et al,2013; Ott and Ziai, 2010) or tweets (Daiber andvan der Goot, 2016), their sizes make them unsuit-able for the evaluation of parser robustness.
More-over, some researchers have raised valid questionsabout the merit of creating a treebank for ungram-matical sentences or adapting the annotation schema(Cahill, 2015; Ragheb and Dickinson, 2012).A ?gold-standard free?
alternative is to comparethe parser output for each problematic sentence withthe parse tree of the corresponding correct sentence.Foster (2004) used this approach over a small set ofungrammatical sentences and showed that parser?saccuracy is different for different types of errors.A limitation of this approach is that the compari-son works best when the differences between theproblematic sentence and the correct sentence aresmall.
This is not the case for some ungrammaticalsentences (especially from MT systems).
Anotherclosely-related approach is to semi-automaticallycreate treebanks from artificial errors.
For exam-ple, Foster generated artificial errors to the sentencesfrom the Penn Treebank for evaluating the effect oferror types on parsers (Foster, 2007).
In anotherwork, Bigert et al (2005) proposed an unsupervisedevaluation of parser robustness based on the intro-duction of artificial spelling errors in error-free sen-tences.
Kakkonen (2007) adapted a similar methodto compare the robustness of four parsers over sen-tences with misspelled words.Our proposed evaluation methodology is similarto the ?gold-standard free?
approach; we comparethe parser output for an ungrammatical sentencewith the automatically generated parse tree of thecorresponding correct sentence.
In the next section,we discuss our evaluation metric to address the con-cerns that some ungrammatical sentences may bevery different from their corrected versions.
This al-lows us to evaluate parsers with more realistic datathat exhibit a diverse set of naturally occurring er-rors, instead of artificially generated errors or lim-ited error types.3 Proposed Evaluation MethodologyFor the purpose of robustness evaluation, we take theautomatically produced parse tree of a well-formedsentence as ?gold-standard?
and compare the parseroutput for the corresponding problematic sentenceagainst it.
Even if the ?gold-standard?
is not per-fectly correct in absolute terms, it represents thenorm from which parse trees of problematic sen-tences diverge: if a parser were robust against un-grammatical sentences, its output for these sentencesshould be similar to its output for the well-formedones.Determining the evaluation metric for compar-ing these trees, however, presents another chal-lenge.
Since the words of the ungrammatical sen-tence and its grammatical counterpart do not neces-sarily match (an example is given in Figure 1), wecannot use standard metrics such as Parseval (Blacket al, 1991).
We also cannot use adapted metricsfor comparing parse trees of unmatched sentences(e.g., Sparseval (Roark et al, 2006)), because thesemetrics consider all the words regardless of the mis-matches (extra or missing words) between two sen-tences.
This is a problem for comparing ungrammat-ical sentences to grammatical ones because a parseris unfairly penalized when it assigns relations to ex-tra words and when it does not assign relations tomissing words.
Since a parser cannot modify thesentence, we do not want to penalize these extra-neous or missing relations; on the other hand, wedo want to identify cascading effects on the parsetree due to a grammar error.
For the purpose ofevaluating parser robustness against ungrammaticalsentences, we propose a modified metric in whichthe dependencies connected to unmatched (extra ormissing) error words are ignored.
A more formaldefinition is as follows:?
Shared dependency is a mutual dependency be-tween two trees;?
Error-related dependency is a dependency con-nected to an extra word1 in the sentence;?
Precision is (# of shared dependencies) / (# ofdependencies of the ungrammatical sentence -1The extra word in the ungrammatical sentences is an un-necessary word error, and the extra word in the grammaticalsentence is a missing word error.1766I appreciate all about thisI appreciate all thisROOTROOTUngrammaticalGrammaticalFigure 1: Example of evaluating robustness of anungrammatical sentence (top) dependency parse treewith its corresponding grammatical sentence (bot-tom).# of error-related dependencies of the ungram-matical sentence);?
Recall is (# of shared dependencies) / (# of de-pendencies of the grammatical sentence - # oferror-related dependencies of the grammaticalsentence); and?
Robustness F1 is the harmonic mean of preci-sion and recall.Figure 1 shows an example in which the un-grammatical sentence has an unnecessary word,?about?, so the three dependencies connected to itare counted as error-related dependencies.
The twoshared dependencies between the trees result in aprecision of 2/(5?3) = 1, recall of 2/(4?0) = 0.5,and Robustness F1 of 66%.4 Experimental SetupOur experiments are conducted over a wide range ofdependency parsers that are trained on two differenttreebanks: Penn Treebank (PTB) and Tweebank.
Weevaluate the robustness of parsers over two datasetsthat contain ungrammatical sentences: writings ofEnglish-as-a-Second language learners and machinetranslation outputs.
We choose datasets for whichthe corresponding correct sentences are available (oreasily reconstructed).4.1 ParsersOur evaluation is over eight state-of-the-art depen-dency parsers representing a wide range of ap-proaches.
We use the publicly available versions ofeach parser with the standard parameter settings.Malt Parser (Nivre et al, 2007)2 A greedytransition-based dependency parser.
We use LI-BLINEAR setting in the learning phase.Mate Parser v3.6.1 (Bohnet, 2010)3 A graph-baseddependency parser that uses second-order maxi-mum spanning tree.MST Parser (McDonald and Pereira, 2006)4 A first-order graph-based parser that searches for maxi-mum spanning trees.Stanford Neural Network Parser (SNN) (Chen andManning, 2014)5 A transition-based parser thatuses word embeddings.
We use pre-trainedword embeddings from Collobert et al (2011) asrecommended by the authors.SyntaxNet (Andor et al, 2016)6 A transition-based neu-ral network parser.
We use the globally normalizedtraining of the parser with default parameters.Turbo Parser v2.3 (Martins et al, 2013)7 A graph-based dependency parser that uses dual decompo-sition algorithm with third-order features.Tweebo Parser (Kong et al, 2014)8 An extension of theTurbo Parser specialized to parse tweets.
TweeboParser adds a new constraint to the Turbo Parser?sinteger linear programming to ignore some Twittertokens from parsing, but also simultaneously usesthese tokens as parsing features.Yara Parser (Rasooli and Tetreault, 2015)9 Atransition-based parser that uses beam searchtraining and dynamic oracle.4.2 DataWe train all the parsers using two treebanks and testtheir robustness over two ungrammatical datasets.4.2.1 Parser Training DataPenn Treebank (PTB) We follow the standardsplits of Penn Treebank, using section 2-21 for train-ing, section 22 for development, and section 23 for2www.maltparser.org3code.google.com/p/mate-tools4seas.upenn.edu/?strctlrn/MSTParser/MSTParser.html5nlp.stanford.edu/software/nndep.shtml6github.com/tensorflow/models/tree/master/syntaxnet7www.cs.cmu.edu/?ark/TurboParser8github.com/ikekonglp/TweeboParser9github.com/yahoo/YaraParser1767testing.
We transform bracketed sentences fromPTB into dependency formats using Stanford Ba-sic Dependency representation (De Marneffe et al,2006) from Stanford parser v3.6.
We assign POStags to the training data using Stanford POS tagger(Toutanova et al, 2003) with ten-way jackknifing(with 97.3% accuracy).Tweebank Tweebank is a Twitter dependency cor-pus annotated by non-experts containing 929 tweets(Kong et al, 2014).
Kong et al (2014) used 717of tweets for training and 201 for test10.
We fol-low the same split in our experiments.
We use pre-trained POS tagging model of Kong et al (2014)(with 92.8% accuracy) over the tweets.The elements in tweets that have no syntacticfunction (such as hashtags, URLs and emoticons)are annotated as unselected tokens (no tokens as theheads).
In order to be able to use Tweebank in otherparsers, we link the unselected tokens to the wallsymbol (i.e.
root as the heads).
This assumption willgenerate more arcs from the root, but since we usethe same evaluation setting for all the parsers, theresults are comparable.
We evaluate the accuracy ofthe trained parser on Tweebank with the unlabeledattachment F1 score (same procedure as Kong et al(2014)).4.2.2 Robustness Test DataTo test the robustness of parsers, we choose twodatasets of ungrammatical sentences for which theircorresponding correct sentences are available.
For afair comparison, we automatically assign POS tagsto the test data.
When parsers are trained on PTB,we use the Stanford POS tagger (Toutanova et al,2003).
When parsers are trained on Tweebank, wecoarsen POS tags to be compatible with the TwitterPOS tags using the mappings specified by Gimpel etal.
(2011).English-as-a-Second Language corpus (ESL)For the ungrammatical sentences, we use the FirstCertificate in English (FCE) dataset (Yannakoudakiset al, 2011) that contains the writings of English asa second language learners and their correspondingerror corrections.
Given the errors and their correc-tions, we can easily reconstruct the corrected version10github.com/ikekonglp/TweeboParser/tree/master/Tweebankof each ungrammatical ESL sentence.
From this cor-pus, we randomly select 10,000 sentences with atleast one error; there are 4954 with one error; 2709with two errors; 1290 with three; 577 with four; 259with five; 111 with six; and 100 with 7+ errors.Machine Translation corpus (MT) Machinetranslation outputs are another domain of ungram-matical sentences.
We use the LIG (Potet et al,2012) which contains 10,881 and LISMI?s TRACEcorpus11 which contains 6,693 French-to-Englishmachine translation outputs and their human post-editions.
From these corpora, we randomly se-lect 10,000 sentences with at least edit distance one(upon words) with their human-edited sentence.
Thedistribution of the number of sentences with theiredit distances from 1 to 10+ is as follows (begin-ning with 1 edit distance and ending with 10+): 674;967; 1019; 951; 891; 802; 742; 650; 547; and 2752.4.3 Evaluation MetricIn the robustness evaluation metric (Section 3),shared dependencies and error-related dependenciesare detected based on alignments between words inthe ungrammatical and grammatical sentences.
Wefind the alignments in the FCE and MT data in aslightly different way.
In the FCE dataset, in whichthe error words are annotated, the grammatical andungrammatical sentences can easily be aligned.
Inthe MT dataset, we use the TER (Translation ErrorRate) tool (default settings)12 to find alignments.In our experiments, we present unlabeled robust-ness F1 micro-averaged across the test sentences.We consider punctuations when parsers are trainedwith the PTB data, because punctuations can bea source of ungrammaticality.
However, we ig-nore punctuations when parsers are trained with theTweebank data, because punctuations are not anno-tated in the tweets with their dependencies.5 ExperimentsThe experiments aim to address the following ques-tions given separate training and test data:1.
How do parsers perform on erroneous sen-tences?
(Section 5.1)11anrtrace.limsi.fr/trace_postedit.tar.bz212www.cs.umd.edu/?snover/tercom1768ParserTrain on PTB ?1-21 Train on TweebanktrainUAS Robustness F1 UAF1 Robustness F1PTB ?23 ESL MT Tweebanktest ESL MTMalt 89.58 93.05 76.26 77.48 94.36 80.66Mate 93.16 93.24 77.07 76.26 91.83 75.74MST 91.17 92.80 76.51 73.99 92.37 77.71SNN 90.70 93.15 74.18 53.4 88.90 71.54SyntaxNet 93.04 93.24 76.39 75.75 88.78 81.87Turbo 92.84 93.72 77.79 79.42 93.28 78.26Tweebo - - - 80.91 93.39 79.47Yara 93.09 93.52 73.15 78.06 93.04 75.83Table 1: Parsers?
performance in terms of accuracy and robustness.
The best result in each column is givenin bold, and the worst result is in italics.2.
To what extent is each parser negatively im-pacted by the increase in the number of errorsin sentences?
(Section 5.2)3.
To what extent is each parser negatively im-pacted by the interactions between multiple er-rors?
(Section 5.3)4.
What types of errors are more problematic forparsers?
(Section 5.4)5.1 Overall Accuracy and RobustnessThe overall performances of all parsers are shown inTable 1.
Note that the Tweebo Parser?s performanceis not trained on the PTB because it is a specializa-tion of the Turbo Parser, designed to parse tweets.Table 1 shows that, for both training conditions, theparser that has the best robustness score in the ESLdomain has also high robustness for the MT domain.This suggests that it might be possible to build robustparsers for multiple ungrammatical domains.
Thetraining conditions do matter ?
Malt performs betterwhen trained from Tweebank than from the PTB.
Incontrast, Tweebank is not a good fit with the neu-ral network parsers due to its small size.
Moreover,SNN uses pre-trained word embeddings, and 60% ofTweebank tokens are missing.Next, let us compare parsers within each train/testconfiguration for their relative robustness.
Whentrained on the PTB, all parsers are comparably ro-bust on ESL data, while they exhibit more differ-ences on the MT data, and, as expected, everyone?sperformance is much lower because MT errors aremore diverse than ESL errors.
We expected that bytraining on Tweebank, parsers will perform better onESL data (and maybe even MT data), since Twee-bank is arguably more similar to the test domainsthan the PTB; we also expected Tweebo to outper-form others.
The results are somewhat surprising.On the one hand, the highest parser score increasedfrom 93.72% (Turbo trained on PTB) to 94.36%(Malt trained on Tweebank), but the two neural net-work parsers performed significantly worse, mostlikely due to the small training size of Tweebank.
In-terestingly, although SyntaxNet has the lowest scoreon ESL, it has the highest score on MT, showingpromise in its robustness.5.2 Parser Robustness by Number of ErrorsTo better understand the overall results, we furtherbreakdown the test sentences by the number of er-rors each contains.
Our objectives are: (1) to observethe speed with which the parsers lose their robust-ness as the sentences become more error-prone; (2)to determine whether some parsers are more robustthan others when handling noisier data.Figure 2 presents four graphs, plotting robust-ness F1 scores against the number of errors for allparsers under each train/test configuration.
In termsof the parsers?
general degradation of robustness, weobserve that: 1) parsing robustness degrades fasterwith the increase of errors for the MT data than theESL data; 2) training on the PTB led to a more simi-lar behavior between the parsers than when trainingon Tweebank; 3) training on Tweebank does helpsome parsers to be more robust against many errors.In terms of relative robustness between parsers,1769Figure 2: Variation in parser robustness as the number of errors in the test sentences increases.we observe that Malt, Turbo, and Tweebo parsersare more robust than others given noisier inputs.The SNN parser is a notable outlier when trained onTweebank due to insufficient training examples.5.3 Impact of Error DistancesThis experiment explores the impact of the interac-tivity of errors.
We assume that errors have moreinteraction if they are closer to each other, and lessinteraction if they are scattered throughout the sen-tence.
We define ?near?
to be when there is at most1 word between errors and ?far?
to be when thereare at least 6 words between errors.
We expect allparsers to have more difficulty on parsing sentenceswhen their errors have more interaction, but how dothe parsers compare against each other?
We conductthis experiment using a subset of sentences that haveexactly three errors; we compare parser robustnesswhen these three errors are near each other with therobustness when the errors are far apart.Table 2 presents the results as a collection ofshaded bars.
This aims to give an at-a-glance vi-sualization of the outcomes.
In this representation,all parsers with the same train data and test domain(including both the near and far sets) are treated asone group.
The top row specifies the lowest score ofall parsers on both test sets; the bottom row speci-fies the highest score.
The shaded area of each barindicates the relative robustness of each parser withrespect to the lowest and highest scores of the group.An empty bar indicates that it is the least robust (cor-responding to the lowest score in the top row); afully shaded bar means it is the most robust (cor-responding to the highest score in the bottom row).Consider the left-most box, in which parsers trainedon PTB and tested on ESL are compared.
In thisgroup13, Yara (near) is the least robust parser with ascore of F1 = 87.3%, while SNN (far) is the mostrobust parser with a score of F1 = 93.4%; as ex-pected, all parsers are less robust when tested onsentences with near errors than far errors, but theydo exhibit relative differences: Turbo parser seemsmost robust in this setting.
Turbo parser?s lead inhandling error interactivity holds for most of theother train/test configurations as well; the only ex-ception is for Tweebank/MT, where SyntaxNet andMalt are better.
Compared to ESL data, near er-rors in MT data are more challenging for all parsers;when trained on PTB, most are equally poor, exceptfor Yara, which has the worst score (79.1%), eventhough it has the highest score when the errors arefar apart (91.5%).
Error interactivity has the mosteffect on Yara parser in all but one train/test config-uration (Tweebank/ESL).5.4 Impact of Error TypesIn the following experiments, we examine the im-pact of different error types.
To remove the impactdue to interactivity between multiple errors, thesestudies use a subset of sentences that have only oneerror.
Although all parsers are fairly robust for sen-tences containing one error, our focus here is on therelative performances of parsers over different errortypes: We want to see whether some error types aremore problematic for some parsers than others.5.4.1 Impact of grammatical error typesThe three main grammar error types are replace-ment (a word need replacing), missing (a word miss-ing), and unnecessary (a word is redundant).
Our13As previously explained, Tweebo is not trained on PTB, soit has no bars associated with it.1770Train on PTB ?1-21 Train on TweebanktrainParser ESL MT ESL MTNear Far Near Far Near Far Near Farmin 87.3 (Yara) 79.1 (Yara) 82.4 (SyntaxNet) 80.6 (SNN)MaltMateMSTSNNSyntaxNetTurboTweeboYaramax 93.4 (SNN) 91.5 (Yara) 94.5 (Malt) 94.4 (Malt)Table 2: Parser performance on test sentences with three near and three far errors.
Each box represents onetrain/test configuration for all parsers and error types.
The bars within indicate the level of robustness scaledto the lowest score (empty bar) and highest score (filled bar) of the group.Train on PTB ?1-21 Train on TweebanktrainParser ESL MT ESL MTRepl.
Miss.
Unnec.
Repl.
Miss.
Unnec.
Repl.
Miss.
Unnec.
Repl.
Miss.
Unnec.min 93.7 (MST) 92.8 (Yara) 89.4 (SyntaxNet) 87.8 (SNN)MaltMateMSTSNNSyntaxNetTurboTweeboYaramax 96.9 (Turbo) 97.2 (SNN) 97.8 (Malt) 97.6 (Malt)Table 3: Parser robustness on sentences with one grammatical error, each can be categorized as a replace-ment error, a missing word error or an unnecessary word error.goal is to see whether different error types have dif-ferent effect on parsers.
If yes, is there a parser thatis more robust than others?As shown in Table 3, replacement errors are theleast problematic error type for all the parsers; onthe other hand, missing errors are the most difficulterror type for parsers.
This finding suggests thata preprocessing module for correcting missing andunnecessary word errors may be helpful in the pars-ing pipeline.5.4.2 Impact of error word categoryAnother factor that might affect parser perfor-mances is the class of errors; for example, we mightexpect an error on a preposition to have a higher im-pact (since it is structural) than an error on an adjec-tive.
We separate the sentences into two groups: er-ror occurring on an open- or closed-class word.
Weexpect closed-class errors to have a stronger negativeimpact on the parsers because they contain functionwords such as determiners, pronouns, conjunctionsand prepositions.Table 4 shows results.
As expected, closed-classerrors are generally more difficult for parsers.
Butwhen parsers are trained on PTB and tested on MT,there are some exceptions: Turbo, Mate, MST andYara parsers tend to be more robust on closed-classerrors.
This result corroborates the importance ofbuilding grammar error correction systems to handleclosed-class errors such as preposition errors.5.4.3 Impact of error semantic roleAn error can be either in a verb role, an argumentrole, or no semantic role.
We extract semantic roleof the error by running Illinoise semantic role labeler(Punyakanok et al, 2008) on corrected version of the1771Train on PTB ?1-21 Train on TweebanktrainParser ESL MT ESL MTOpen class Closed class Open class Closed class Open class Closed class Open class Closed classmin 95.1 (SNN) 94.5 (Yara) 89.6 (SyntaxNet) 91.5 (SNN)MaltMateMSTSNNSyntaxNetTurboTweeboYaramax 96.8 (Malt) 96.1 (SNN) 97.6 (Malt) 97.0 (Malt)Table 4: Parser robustness on sentences with one error, where the error either occurs on an open-class(lexical) word or a closed-class (functional) word.Train on PTB ?1-21 Train on TweebanktrainParser ESL MT ESL MTVerb Argument No role Verb Argument No role Verb Argument No role Verb Argument No rolemin 94.1 (SyntaxNet) 91.8 (Malt) 91.8 (SNN) 92.2 (SNN)MaltMateMSTSNNSyntaxNetTurboTweeboYaramax 96.7 (Turbo) 96.7 (SyntaxNet) 96.9 (Malt) 96.9 (Malt)Table 5: Parser robustness on sentences with one error where the error occurs on a word taking on a verbrole, an argument role, or a word with no semantic role.sentences.
We then obtain the role of the errors usingalignments between ungrammatical sentence and itscorrected counterpart.Table 5 shows the average robustness of parserswhen parsing sentences that have one error.
Forparsers trained on the PTB data, handling sentenceswith argument errors seem somewhat easier thanthose with other errors.
For parsers trained on theTweebank, the variation in the semantic roles of theerrors does not seem to impact parser performance;each parser performs equally well or poorly acrossall roles; comparing across parsers, Malt seems par-ticularly robust to error variations due to semanticroles.6 Conclusions and RecommendationsIn this paper, we have presented a set of empiricalanalyses on the robustness of processing ungram-matical text for several leading dependency parsers,using an evaluation metric designed for this purpose.We find that parsers indeed have different responsesto ungrammatical sentences of various types.
Werecommend practitioners to examine the range ofungrammaticality in their input data (whether it ismore like tweets or has grammatical errors like ESLwritings).
If the input data contains text more simi-lar to tweets (e.g.
containing URLs and emoticons),Malt or Turbo parser may be good choices.
If theinput data is more similar to the machine translationoutputs; SyntaxNet, Malt, Tweebo and Turbo parserare good choices.Our results also suggest that some preprocess-ing steps may be necessary for ungrammatical sen-tences, such as handling redundant and missingword errors.
While there are some previous workson fixing the unnecessary words in the literature(Xue and Hwa, 2014), it is worthy to develop betterNLP methods for catching and mitigating the miss-ing word errors prior to parsing.
Finally, this workcorroborate the importance of building grammar er-ror correction systems for handling closed-class er-1772rors such as preposition errors.AcknowledgmentsThis work was supported in part by the National Sci-ence Foundation Award #1550635.
We would liketo thank the anonymous reviewers and the Pitt NLPgroup for their helpful comments.ReferencesDaniel Andor, Chris Alberti, David Weiss, AliakseiSeveryn, Alessandro Presta, Kuzman Ganchev, SlavPetrov, and Michael Collins.
2016.
Globally normal-ized transition-based neural networks.
arXiv preprintarXiv:1603.06042.Johnny Bigert, Jonas Sjo?bergh, Ola Knutsson, and Mag-nus Sahlgren.
2005.
Unsupervised evaluation ofparser robustness.
In Computational Linguistics andIntelligent Text Processing, pages 142?154.E.
Black, S. Abney, S. Flickenger, C. Gdaniec, C. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverageof English grammars.
In Proceedings of the DARPASpeech and Natural Language Workshop, pages 306?311.Bernd Bohnet.
2010.
Very high accuracy and fast depen-dency parsing is not a contradiction.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics, pages 89?97.Aoife Cahill.
2015.
Parsing learner text: to shoehorn ornot to shoehorn.
In Proceedings of LAW IX - The 9thLinguistic Annotation Workshop, page 144.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 740?750.Jinho D Choi, Joel Tetreault, and Amanda Stent.
2015.It depends: Dependency parser comparison using aweb-based evaluation tool.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics, pages 26?31.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Joachim Daiber and Rob van der Goot.
2016.
The de-noised web treebank: Evaluating dependency parsingunder noisy input conditions.
In LREC.Marie-Catherine De Marneffe, Bill MacCartney, Christo-pher D Manning, et al 2006.
Generating typed depen-dency parses from phrase structure parses.
In LREC,number 2006, pages 449?454.Jennifer Foster, O?zlem C?etinoglu, Joachim Wagner,Joseph Le Roux, Stephen Hogan, Joakim Nivre,Deirdre Hogan, Josef Van Genabith, et al 2011.
#hardtoparse: POS tagging and parsing the twitterverse.In proceedings of the Workshop On Analyzing Micro-text (AAAI 2011), pages 20?25.Jennifer Foster.
2004.
Parsing ungrammatical input: anevaluation procedure.
In LREC.Jennifer Foster.
2007.
Treebanks gone bad.
Interna-tional Journal of Document Analysis and Recognition,10(3-4):129?145.Jennifer Foster.
2010.
?cba to check the spelling?
in-vestigating parser performance on discussion forumposts.
In The Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 381?384.Jeroen Geertzen, Theodora Alexopoulou, and Anna Ko-rhonen.
2013.
Automatic linguistic annotation oflarge scale l2 databases: the EF-Cambridge open lan-guage database (EFCamDat).
In Proceedings of the31st Second Language Research Forum.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages167?202.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A Smith.
2011.
Part-of-speech tagging for Twit-ter: Annotation, features, and experiments.
In ACL-HLT, pages 42?47.Tuomo Kakkonen.
2007.
Robustness evaluation of twoCCG, a PCFG and a link grammar parsers.
Proceed-ings of the 3rd Language & Technology Conference:Human Language Technologies as a Challenge forComputer Science and Linguistics.Lingpeng Kong and Noah A Smith.
2014.
An empiricalcomparison of parsing methods for stanford dependen-cies.
arXiv preprint arXiv:1404.4314.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer, andNoah A Smith.
2014.
A dependency parser fortweets.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.Jonathan K Kummerfeld, David Hall, James R Curran,and Dan Klein.
2012.
Parser showdown at the wallstreet corral: An empirical investigation of error typesin parser output.
In Proceedings of the 2012 Joint1773Conference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 1048?1059.Andre?
FT Martins, Miguel Almeida, and Noah A Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics, pages 617?622.David McClosky, Eugene Charniak, and Mark Johnson.2010.
Automatic domain adaptation for parsing.
InThe Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 28?36.Ryan McDonald and Joakim Nivre.
2011.
Analyzingand integrating dependency parsers.
ComputationalLinguistics, 37(1):197?230.Ryan T McDonald and Fernando CN Pereira.
2006.
On-line learning of approximate dependency parsing algo-rithms.
In EACL.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(02):95?135.Niels Ott and Ramon Ziai.
2010.
Evaluating depen-dency parsing performance on german learner lan-guage.
Proceedings of the Ninth Workshop on Tree-banks and Linguistic Theories (TLT-9), 9:175?186.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, andHiyan Alshawi.
2010.
Uptraining for accurate deter-ministic question parsing.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 705?713.Marion Potet, Emmanuelle Esperanc?a-Rodier, LaurentBesacier, and Herve?
Blanchon.
2012.
Collection ofa large database of French-English SMT output cor-rections.
In LREC, pages 4043?4048.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287.Marwa Ragheb and Markus Dickinson.
2012.
Definingsyntax for learner language annotation.
In COLING(Posters), pages 965?974.Mohammad Sadegh Rasooli and Joel Tetreault.
2015.Yara parser: A fast and accurate dependency parser.arXiv preprint arXiv:1503.06733.Brian Roark, Mary Harper, Eugene Charniak, BonnieDorr, Mark Johnson, Jeremy G Kahn, Yang Liu, MariOstendorf, John Hale, Anna Krasnyanskaya, et al2006.
Sparseval: Evaluation metrics for parsingspeech.
In LREC.Kristina Toutanova, Dan Klein, Christopher D Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In NAACL,pages 173?180.Huichao Xue and Rebecca Hwa.
2014.
Redundancy de-tection in esl writings.
In EACL, pages 683?691.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics, pages 180?189.1774
