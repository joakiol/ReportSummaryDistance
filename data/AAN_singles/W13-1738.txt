Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 293?299,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsImproving interpretation robustness in a tutorial dialogue systemMyroslava O. Dzikovska and Elaine Farrow and Johanna D. MooreSchool of Informatics, University of EdinburghEdinburgh, EH8 9AB, United Kingdom{m.dzikovska,elaine.farrow,j.moore}@ed.ac.ukAbstractWe present an experiment aimed at improv-ing interpretation robustness of a tutorial dia-logue system that relies on detailed semanticinterpretation and dynamic natural languagefeedback generation.
We show that we canimprove overall interpretation quality by com-bining the output of a semantic interpreterwith that of a statistical classifier trained onthe subset of student utterances where seman-tic interpretation fails.
This improves on a pre-vious result which used a similar approach buttrained the classifier on a substantially largerdata set containing all student utterances.
Fi-nally, we discuss how the labels from the sta-tistical classifier can be integrated effectivelywith the dialogue system?s existing error re-covery policies.1 IntroductionGiving students formative feedback as they inter-act with educational applications, such as simu-lated training environments, problem-solving tutors,serious games, and exploratory learning environ-ments, is known to be important for effective learn-ing (Shute, 2008).
Suitable feedback can includecontext-appropriate confirmations, hints, and sug-gestions to help students refine their answers andincrease their understanding of the subject.
Pro-viding this type of feedback automatically, in nat-ural language, is the goal of tutorial dialogue sys-tems (Aleven et al 2002; Dzikovska et al 2010b;Graesser et al 1999; Jordan et al 2006; Litman andSilliman, 2004; Khuwaja et al 1994; Pon-Barry etal., 2004; VanLehn et al 2007).Much work in NLP for educational applicationshas focused on automated answer grading (Leacockand Chodorow, 2003; Pulman and Sukkarieh, 2005;Mohler et al 2011).
Automated answer assess-ment systems are commonly trained on large textcorpora.
They compare the text of a student answerwith the text of one or more reference answers sup-plied by human instructors and calculate a score re-flecting the quality of the match.
Automated grad-ing methods are integrated into intelligent tutoringsystems (ITS) by having system developers antic-ipate both correct and incorrect responses to eachquestion, with the system choosing the best match(Graesser et al 1999; Jordan et al 2006; Litmanand Silliman, 2004; VanLehn et al 2007).
Suchsystems have wide domain coverage and are robustto ill-formed input.
However, as matching relies onshallow features and does not provide semantic rep-resentations of student answers, this approach is lesssuitable for dynamically generating adaptive naturallanguage feedback (Dzikovska et al 2013).Real-time simulations and serious games arecommonly used in STEM learning environmentsto increase student engagement and support ex-ploratory learning (Rutten et al 2012; Mayo, 2007).Natural language dialogue can help improve learn-ing in such systems by asking students to explaintheir reasoning, either directly during interaction, orduring post-problem reflection (Aleven et al 2002;Pon-Barry et al 2004; Dzikovska et al 2010b).Interpretation of student answers in such systemsneeds to be grounded in the current state of a dynam-ically changing environment, and feedback may alsobe generated dynamically to reflect the changingsystem state.
This is typically achieved by employ-ing hand-crafted parsers and semantic interpreters toproduce structured semantic representations of stu-dent input, which are then used to instantiate ab-293stract tutorial strategies with the help of a naturallanguage generation system (Freedman, 2000; Clarket al 2005; Dzikovska et al 2010b).Rule-based semantic interpreters are known tosuffer from robustness and coverage problems, fail-ing to interpret out-of-grammar student utterances.In the event of an interpretation failure, most sys-tems have little information on which to base a feed-back decision and typically respond by asking thestudent to rephrase, or simply give away the answer(though more sophisticated strategies are sometimespossible, see Section 4).
While statistical scoring ap-proaches are more robust, they may still suffer fromcoverage issues when system designers fail to antic-ipate the full range of expected student answers.
Inone study of a statistical system, a human judge la-beled 33% of student utterances as not matching anyof the anticipated responses, meaning that the sys-tem had no information to use as a basis for choos-ing the next action and fell back on a single strategy,giving away the answer (Jordan et al 2009).Recently, Dzikovska et al(2012b) developed anannotated corpus of student responses (henceforth,the SRA corpus) with the goal of facilitating dy-namic generation of tutorial feedback.1 Student re-sponses are assigned to one of 5 domain- and task-independent classes that correspond to typical flawsfound in student answers.
These classes can be usedto help a system choose a feedback strategy basedonly on the student answer and a single referenceanswer.
Dzikovska et al(2013) showed that a sta-tistical classifier trained on this data set can be usedin combination with a semantic interpreter to sig-nificantly improve the overall quality of natural lan-guage interpretation in a dialogue-based ITS.
Thebest results were obtained by using the classifierto label the utterances that the semantic interpreterfailed to process.In this paper we further extend this result byshowing that we can obtain similar results by train-ing the classifier directly on the subset of utterancesthat cannot be processed by the interpreter.
Thedistribution of labels across the classes is differ-ent in this subset compared to the rest of the cor-pus.
Therefore we can train a subset-specific classi-1http://www.cs.york.ac.uk/semeval-2013/task7/index.php?id=datafier, reducing the amount of annotated training dataneeded without compromising performance of thecombined system.The rest of the paper is organized as follows.
InSection 2 we describe an architecture for combiningsemantic interpretation and classification in a sys-tem with dynamic natural language feedback gener-ation.
In Section 3 we describe an experiment to im-prove combined system performance using a classi-fier trained only on non-interpretable utterances.
Wediscuss future improvements in Section 4.2 BackgroundThe SRA corpus is made up of two subsets: (1)the SciEntsBank subset, consisting of written re-sponses to assessment questions (Nielsen et al2008b), and (2) the Beetle subset consisting of ut-terances collected from student interactions with theBEETLE II tutorial dialogue system (Dzikovska etal., 2010b).
The SRA corpus annotation schemedefines 5 classes of student answers (?correct?,?partially-correct-incomplete?, ?contradictory?, ?ir-relevant?
and ?non-domain?).
Each utterance is as-signed to one of the 5 classes based on pre-existingmanual annotations (Dzikovska et al 2012b).We focus on the Beetle subset because the Beetledata comes from an implemented system, meaningthat we also have access to the semantic interpreta-tions of student utterances produced by the BEETLEII interpretation component.
The system uses fine-grained semantic analysis to produce detailed diag-noses of student answers in terms of correct, incor-rect, missing and irrelevant parts.
We developed aset of rules to map these diagnoses onto the SRAcorpus 5-class annotation scheme to support systemevaluation (Dzikovska et al 2012a).In our previous work (Dzikovska et al 2013), weused this mapping as the basis for combining theoutput of the BEETLE II semantic interpreter withthe output of a statistical classifier, using a rule-based policy to determine which label to use foreach instance.
If the label from the semantic in-terpreter is chosen, then the full range of detailedfeedback strategies can be used, based on the corre-sponding semantic representation.
If the classifier?slabel is chosen, then the system can fall back to us-ing content-free prompts, choosing an appropriate294prompt based on the SRA corpus label.We evaluated 3 rule-based combination policies,chosen to reduce the effects of the errors that thesemantic interpreter makes, and taking into accounttutoring goals such as reducing student frustration.The best performing policy takes the classifier?s out-put if and only if the semantic interpreter is unableto process the utterance.2 This allows the system tochoose from a wider set of content-free prompts in-stead of always telling the student that the utterancewas not understood.As discussed earlier, non-interpretable utterancespresent a problem for both rule-based and statisticalapproaches.
Therefore, we carried out an additionalset of experiments, focusing on the performance ofsystem combinations that use policies designed toaddress non-interpretable utterances.
We discuss ourresults and future directions in the rest of the paper.3 Improving Interpretation Robustness3.1 Experimental SetupThe Beetle portion of the SRA corpus contains 3941unique student answers to 47 different explanationquestions.
Each question is associated with one ormore reference answers provided by expert tutors,and each student answer is manually annotated withthe label assigned by the BEETLE II interpreter anda gold-standard correctness label.In our experiments, we follow the procedure de-scribed in (Dzikovska et al 2013), using 10-foldcross-validation to evaluate the performance of thevarious stand-alone and combined systems.
We re-port the per-class F1 scores as evaluation metrics,using the macro-averaged F1 score as the primaryevaluation metric.Dzikovska et al(2013) used a statistical classi-fier based on lexical overlap, taken from (Dzikovskaet al 2012a), and evaluated 3 different rule-basedpolicies for combining its output with that of the se-mantic interpreter.
In two of those policies the inter-preter?s output is always used if it is available, andthe classifier?s label is used for a (subset of) non-interpretable utterances:1.
NoReject: the classifier?s label is used in allcases where semantic interpretation fails, thus2We will refer to such utterances as ?non-interpretable?
fol-lowing (Bohus and Rudnicky, 2005).creating a system that never rejects student in-put as non-interpretable2.
NoRejectCorrect: the classifier?s label isused for non-interpretable utterances which arelabeled as ?correct?
by the classifier.
This moreconservative policy aims to ensure that correctstudent answers are always accepted, but incor-rect answers may still be rejected with a requestto rephrase.We conducted a new experiment to evaluate thesetwo policies together with an enhanced classifier,discussed in the next section.3.2 ClassifierFor this paper, we extended the classifier from theprevious study (Dzikovska et al 2013), which wewill call Sim8, with additional features to improvehandling of lexical variability and negation.Sim8 uses the Weka 3.6.2 implementation ofC4.5 pruned decision trees, with default parameters.It uses 8 features based on lexical overlap similaritymetrics provided by Perl?s Text::Similaritypackage v.0.09: 4 metrics measuring overlap be-tween the student answer and the expected answer,and the same 4 metrics applied to the student?s an-swer and the question text.In our enhanced classifier, Sim20, we extendedthe baseline feature set with 12 additional features.8 of these are direct analogs of the baseline features,this time computed on the stemmed text to reducethe impact of syntactic variation, using the Porterstemmer from the Lingua::Stem package.3 Inaddition, 4 features were added to improve negationhandling and thus detection of contradictions.
Theseare:?
QuestionNeg, AnswerNeg: features in-dicating the presence of a negation markerin the question and the student?s answer re-spectively, detected using a regular expression.We distinguish three cases: a negation marker3We also experimented with features that involve removingstop words before computing similarity scores, and with usingSVMs for classification, but failed to obtain better performance.We continue to investigate different SVM kernels and alterna-tive classification algorithms such as random forests for our fu-ture work.295Standalone Sem.
Interp.
+ Sim20 Sem.
Interp.
+ Sim20NISem.
Interp.
Sim8 Sim20 no rej no rej corr no rej no rej corrcorrect 0.66 0.71 0.71 0.70 0.70 0.70 0.70pc inc 0.48 0.38 0.40 0.51 0.48 0.50 0.48contra 0.27 0.40 0.45 0.47 0.27 0.51 0.27irrlvnt 0.21 0.05 0.08 0.22 0.21 0.22 0.21nondom 0.65 0.73 0.78 0.83 0.65 0.83 0.65macro avg 0.45 0.45 0.48 0.55 0.46 0.55 0.46Table 1: F1 scores for three stand-alone systems, and for combination systems using the Sim20 and Sim20NIclassifiers together with the semantic interpreter.
Stand-alone performance for Sim20NI is not shown since it wastrained only on the non-interpretable data subset and is therefore not applicable for the complete data set.likely to be associated with domain content(e.g., ?not connected?
); a negation marker morelikely to be associated with general expressionsof confusion (such as ?don?t know?
); and nonegation marker present.?
BestOverlapNeg: true if the reference an-swer that has the highest F1 overlap with thestudent answer includes a negation marker.?
BestOverlapPolarityMatch: a flagcomputed from the values of AnswerNeg andBestOverlapNeg.
Again, we distinguishthree cases: they have the same polarity (boththe student answer and the reference answercontain negation markers, or both have nonegation markers); they have opposite polar-ity; or the student answer contains a negationmarker associated with an expression of confu-sion, as described above.3.3 EvaluationEvaluation results are shown in Table 1.
Unlessotherwise specified, all performance differences dis-cussed in the text are significant on an approximaterandomization significance test with 10,000 itera-tions (Yeh, 2000).Adding the new features to create the Sim20classifier resulted in a performance improvementcompared to the Sim8 classifier, raising macro-averaged F1 from 0.45 to 0.48, with an improvementin contradiction detection as intended.
But these im-provements did not translate into improvements inthe combined systems.
Combinations using Sim20performed exactly the same as the combinations us-ing Sim8 (not shown due to space limitations, see(Dzikovska et al 2013)).
Clearly, more sophisti-cated features are needed to obtain further perfor-mance gains in the combined systems.However, we noted that the subset of non-interpretable utterances in the corpus has a differ-ent distribution of labels compared to the full dataset.
In the complete data set, 1665 utterances (42%)are labeled as correct and 1049 (27%) as contradic-tory.
Among the 1416 utterances considered non-interpretable by the semantic interpreter, 371 (26%)belong to the ?correct?
class, and 598 (42%) to ?con-tradictory?
(other classes have similar distributionsin both subsets).
We therefore hypothesized that acombination system that uses the classifier outputonly if an utterance is non-interpretable, may ben-efit from employing a classifier trained specificallyon this subset rather than on the whole data set.If our hypothesis is true, it offers an interestingpossibility for combining rule-based and statisticalclassifiers in similar setups: if the classifier can betrained using only the examples that are problematicfor the rule-based system, it can provide improvedrobustness at a significantly lower annotation cost.We therefore trained another classifier,Sim20NI, using the same feature set as Sim20,but this time using only the instances rejectedas non-interpretable by the semantic interpreterin each cross-validation fold (1416 utterances,36% of all data instances).
We again used theNoReject and NoRejectCorrect policies tocombine the output of Sim20NI with that of thesemantic interpreter.
Evaluation results confirmedour hypothesis.
The system combinations thatuse Sim20 and Sim20NI perform identically on296macro-averaged F1, with NoReject being the bestcombination policy in both cases and significantlyoutperforming the semantic interpreter alone.
How-ever, the Sim20NI classifier has the advantage ofneeding significantly less annotated data to achievethis performance.4 Discussion and Future WorkOur research focuses on combining deep and shal-low processing by supplementing fine-grained se-mantic interpretations from a rule-based systemwith more coarse-grained classification labels.
Al-ternatively, we could try to learn structured se-mantic representations from annotated text (Zettle-moyer and Collins, 2005; Wong and Mooney, 2007;Kwiatkowski et al 2010), or to learn more fine-grained assessment labels (Nielsen et al 2008a).However, such approaches require substantiallylarger annotation effort.
Therefore, we believe it isworth exploring the use of the simpler 5-label anno-tation scheme from the SRA corpus.
We previouslyshowed that it is possible to improve system perfor-mance by combining the output of a symbolic inter-preter with that of a statistical classifier (Dzikovskaet al 2013).
The best combination policy used thestatistical classifier to label utterances rejected asnon-interpretable by the rule-based interpreter.In this paper, we showed that similar results canbe achieved by training the classifier only on non-interpretable utterances, rather than on the whole la-beled corpus.
The student answers that the inter-preter has difficulty with have a distinct distribution,which is effectively utilized by training a classifieronly on this subset.
This reduces the amount of an-notated training data needed, reducing the amount ofmanual labor required.In future, we will further investigate the best com-bination of parsing and statistical classification insystems that offer sophisticated error recovery poli-cies for non-understandings.
Our top-performingpolicy, NoReject, uses deep parsing and semanticinterpretation to produce a detailed semantic analy-sis for the majority of utterances, and falls back on ashallower statistical classifier for utterances that aredifficult for the interpreter.
This policy assumes thatit is always better to use a content-free prompt thanto reject a non-interpretable student utterance.
How-ever, interpretation problems can arise from incor-rect uses of terminology, and learning to speak inthe language of the domain has been positively cor-related with learning outcomes (Steinhauser et al2011).
Therefore, rejecting some non-interpretableanswers as incorrect could be a valid tutoring strat-egy (Sagae et al 2010; Dzikovska et al 2010a).The BEETLE II system offers several error re-covery strategies intended to help students phrasetheir answers in more acceptable ways by giving atargeted help message, e.g., ?I am sorry, I?m hav-ing trouble understanding.
Paths cannot be broken,only components can be broken?
(Dzikovska et al2010a).
Therefore, it may be worthwhile to con-sider other combination policies.
We evaluated theNoRejectCorrect policy, which uses the statis-tical classifier to identify correct answers rejectedby the semantic interpreter and asks for rephrasingsin other cases.
Using this policy resulted in only asmall improvement in system performance.
A dif-ferent classifier geared towards more accurate iden-tification of correct answers may help, and we areplanning to investigate this option in the future.Alternatively, we could consider a combinationpolicy which looks for rejected answers that theclassifier identifies as contradictory and changes thewording of the targeted help message to indicate thatthe student may have made a mistake, instead ofapologizing for the misunderstanding.
This has thepotential to help students learn correct terminologyrather than presenting the issue as strictly an inter-pretation failure.Ultimately, all combination policies must betested with users to ensure that improved robust-ness translates into improved system effectiveness.We have previously studied the effectiveness of ourtargeted help strategies with respect to improvinglearning outcomes (Dzikovska et al 2010a).
A sim-ilar study is required to evaluate our combinationstrategies.AcknowledgmentsWe thank Natalie Steinhauser, Gwendolyn Camp-bell, Charlie Scott, Simon Caine and Sarah Denhefor help with data collection and preparation.
Theresearch reported here was supported by the USONR award N000141010085.297ReferencesVincent Aleven, Octav Popescu, and Kenneth R.Koedinger.
2002.
Pilot-testing a tutorial dialogue sys-tem that supports self-explanation.
In Proc.
of ITS-02conference, pages 344?354.Dan Bohus and Alexander Rudnicky.
2005.
Sorry,I didn?t catch that!
- An investigation of non-understanding errors and recovery strategies.
In Pro-ceedings of SIGdial-2005, Lisbon, Portugal.Brady Clark, Oliver Lemon, Alexander Gruenstein, Eliz-abethOwen Bratt, John Fry, Stanley Peters, HeatherPon-Barry, Karl Schultz, Zack Thomsen-Gray, andPucktada Treeratpituk.
2005.
A general purpose ar-chitecture for intelligent tutoring systems.
In JanC.J.Kuppevelt, Laila Dybkjr, and NielsOle Bernsen, edi-tors, Advances in Natural Multimodal Dialogue Sys-tems, volume 30 of Text, Speech and Language Tech-nology, pages 287?305.
Springer Netherlands.Myroslava O. Dzikovska, Johanna D. Moore, NatalieSteinhauser, and Gwendolyn Campbell.
2010a.
Theimpact of interpretation problems on tutorial dialogue.In Proc.
of ACL 2010 Conference Short Papers, pages43?48.Myroslava O. Dzikovska, Johanna D. Moore, NatalieSteinhauser, Gwendolyn Campbell, Elaine Farrow,and Charles B. Callaway.
2010b.
Beetle II: a systemfor tutoring and computational linguistics experimen-tation.
In Proc.
of ACL 2010 System Demonstrations,pages 13?18.Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Jo-hanna D. Moore.
2012a.
Evaluating language under-standing accuracy with respect to objective outcomesin a dialogue system.
In Proc.
of EACL-12 Confer-ence, pages 471?481.Myroslava O. Dzikovska, Rodney D. Nielsen, and ChrisBrew.
2012b.
Towards effective tutorial feedback forexplanation questions: A dataset and baselines.
InProc.
of 2012 Conference of NAACL: Human Lan-guage Technologies, pages 200?210.Myroslava O. Dzikovska, Elaine Farrow, and Johanna D.Moore.
2013.
Combining semantic interpretation andstatistical classification for improved explanation pro-cessing in a tutorial dialogue system.
In In Proceed-ings of the The 16th International Conference on Ar-tificial Intelligence in Education (AIED 2013), Mem-phis, TN, USA, July.Reva Freedman.
2000.
Using a reactive planner as thebasis for a dialogue agent.
In Proceedings of the Thir-teenth Florida Artificial Intelligence Research Sympo-sium (FLAIRS 2000), pages 203?208.A.
C. Graesser, K. Wiemer-Hastings, P. Wiemer-Hastings, and R. Kreuz.
1999.
Autotutor: A simu-lation of a human tutor.
Cognitive Systems Research,1:35?51.Pamela Jordan, Maxim Makatchev, Umarani Pap-puswamy, Kurt VanLehn, and Patricia Albacete.2006.
A natural language tutorial dialogue system forphysics.
In Proc.
of 19th Intl.
FLAIRS conference,pages 521?527.Pamela Jordan, Diane Litman, Michael Lipschultz, andJoanna Drummond.
2009.
Evidence of misunder-standings in tutorial dialogue and their impact onlearning.
In Proc.
of 14th International Conference onArtificial Intelligence in Education, pages 125?132.Ramzan A. Khuwaja, Martha W. Evens, Joel A. Michael,and Allen A. Rovick.
1994.
Architecture ofCIRCSIM-tutor (v.3): A smart cardiovascular physi-ology tutor.
In Proc.
of 7th Annual IEEE Computer-Based Medical Systems Symposium.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilisticCCG grammars from logical form with higher-orderunification.
In Proc.
of EMNLP-2010 Conference,pages 1223?1233.Claudia Leacock and Martin Chodorow.
2003.
C-rater:Automated scoring of short-answer questions.
Com-puters and the Humanities, 37(4):389?405.Diane J. Litman and Scott Silliman.
2004.
ITSPOKE:an intelligent tutoring spoken dialogue system.
InDemonstration Papers at HLT-NAACL 2004, pages 5?8, Boston, Massachusetts.Merrilea J. Mayo.
2007.
Games for science and engi-neering education.
Commun.
ACM, 50(7):30?35, July.Michael Mohler, Razvan Bunescu, and Rada Mihalcea.2011.
Learning to grade short answer questions usingsemantic similarity measures and dependency graphalignments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 752?762, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Rodney D. Nielsen, Wayne Ward, and James H. Martin.2008a.
Learning to assess low-level conceptual under-standing.
In Proc.
of 21st Intl.
FLAIRS Conference,pages 427?432.Rodney D. Nielsen, Wayne Ward, James H. Martin, andMartha Palmer.
2008b.
Annotating students?
under-standing of science concepts.
In Proceedings of theSixth International Language Resources and Evalua-tion Conference, (LREC08), Marrakech, Morocco.Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-beth Owen Bratt, and Stanley Peters.
2004.
Advan-tages of spoken language interaction in dialogue-basedintelligent tutoring systems.
In Proc.
of ITS-2004 Con-ference, pages 390?400.298Stephen G Pulman and Jana Z Sukkarieh.
2005.
Au-tomatic short answer marking.
In Proceedings of theSecond Workshop on Building Educational Applica-tions Using NLP, pages 9?16, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Nico Rutten, Wouter R. van Joolingen, and Jan T. van derVeen.
2012.
The learning effects of computer simula-tions in science education.
Computers and Education,58(1):136 ?
153.Alicia Sagae, W. Lewis Johnson, and Stephen Bodnar.2010.
Validation of a dialog system for languagelearners.
In Proceedings of the 11th Annual Meeting ofthe Special Interest Group on Discourse and Dialogue,SIGDIAL ?10, pages 241?244, Stroudsburg, PA, USA.Association for Computational Linguistics.Valerie J Shute.
2008.
Focus on formative feedback.Review of educational research, 78(1):153?189.Natalie B. Steinhauser, Gwendolyn E. Campbell,Leanne S. Taylor, Simon Caine, Charlie Scott, My-roslava O. Dzikovska, and Johanna D. Moore.
2011.Talk like an electrician: Student dialogue mimickingbehavior in an intelligent tutoring system.
In Proc.
of15th international conference on Artificial Intelligencein Education, pages 361?368.Kurt VanLehn, Pamela Jordan, and Diane Litman.
2007.Developing pedagogically effective tutorial dialoguetactics: Experiments and a testbed.
In Proc.
of SLaTEWorkshop on Speech and Language Technology in Ed-ucation, Farmington, PA, October.Yuk Wah Wong and Raymond J. Mooney.
2007.
Learn-ing synchronous grammars for semantic parsing withlambda calculus.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguis-tics (ACL-2007), Prague, Czech Republic, June.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Proceed-ings of the 18th International Conference on Compu-tational linguistics (COLING 2000), pages 947?953,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Luke S. Zettlemoyer and Michael Collins.
2005.
Learn-ing to Map Sentences to Logical Form: StructuredClassification with Probabilistic Categorial Grammars.In Proceedings of the 21th Annual Conference onUncertainty in Artificial Intelligence (UAI-05), pages658?666, Arlington, Virginia.
AUAI Press.299
