Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341?344,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsWord Alignment withStochastic Bracketing Linear Inversion Transduction GrammarMarkus SAERS and Joakim NIVREComputational Linguistics GroupDept.
of Linguistics and PhilologyUppsala UniversitySwedenfirst.last@lingfil.uu.seDekai WUHuman Language Technology CenterDept.
of Computer Science and EngineeringHKUSTHong Kongdekai@cs.ust.hkAbstractThe class of Linear Inversion TransductionGrammars (LITGs) is introduced, and used toinduce a word alignment over a parallel cor-pus.
We show that alignment via Stochas-tic Bracketing LITGs is considerably fasterthan Stochastic Bracketing ITGs, while stillyielding alignments superior to the widely-used heuristic of intersecting bidirectionalIBM alignments.
Performance is measured asthe translation quality of a phrase-based ma-chine translation system built upon the wordalignments, and an improvement of 2.85 BLEUpoints over baseline is noted for French?English.1 IntroductionMachine translation relies heavily on word align-ments, which are usually produced by training IBM-models (Brown et al, 1993) in both directions andcombining the resulting alignments via some heuris-tic.
Automatically training an Inversion Transduc-tion Grammar (ITG) has been suggested as a viableway of producing superior alignments (Saers andWu, 2009).
The main problem of using Bracket-ing ITGs for alignment is that exhaustive biparsingruns in O(n6) time.
Several ways to lower the com-plexity of ITGs has been suggested, but in this paper,a different approach is taken.
Instead of using fullITGs, we explore the possibility of subjecting thegrammar to a linear constraint, making exhaustivebiparsing of a sentence pair in O(n4) time possible.This can be further improved by applying pruning.2 BackgroundA transduction is the bilingual version of a language.A language (Ll) can be formally viewed as a set ofsentences, sequences of tokens taken from a speci-fied vocabulary (Vl).
A transduction (Te,f ) betweentwo languages (Le and Lf ) is then a set of sentencepairs, sequences of bitokens from the cross produc-tion of the vocabularies of the two languages beingtransduced (Ve,f = Ve ?
Vf ).
This adds an extralayer of complexity to finding transductions fromraw bitexts, as an alignment has to be imposed.Simple (STG) and Syntax Directed (SDTG) Trans-duction Grammars (Aho and Ullman, 1972) canbe used to parse transductions between context-freelanguages.
Both work fine as long as a grammar isgiven and parsing is done as transduction, that is: asentence in one language is rewritten into the otherlanguage.
In NLP, interest has shifted away fromhand-crafted grammars, towards stochastic gram-mars induced from corpora.
To induce a stochas-tic grammar from a parallel corpus, expectations ofall possible parses over a sentence pair are typicallyneeded.
STGs can biparse sentence pairs in polyno-mial time, but are unable to account for the complex-ities typically found in natural languages.
SDTGs doaccount for the complexities in natural languages,but are intractable for biparsing.Inversion transductions (Wu, 1995; Wu, 1997) area special case of transductions that are not mono-tone, but where permutations are severely limited.By limiting the possible permutations, biparsing be-comes tractable.
This in turn means that ITGs can beinduced from parallel corpora in polynomial time,341as well as account for most of the reorderings foundbetween natural languages.An Inversion transduction is limited so that itmust be expressible as non-overlapping groups, in-ternally permuted either by the identity permuta-tion or the inversion permutation (hence the name).This requirement also means that the grammar is bi-narizable, yielding a two-normal form.
A produc-tion with the identity permutation is written insidesquare brackets, while productions with the inver-sion permutation is written inside angled brackets.This gives us a two-normal form that looks like this(where e/f is a biterminal):A ?
[B C]A ?
?B C?A ?
e/fThe time complexity for exhaustive ITG biparsing isO(Gn6), which is typically too large to be applica-ble to large grammars and long sentence.
The gram-mar constant G can be eliminated by limiting thegrammar to a bracketing ITG (BITG), which only hasone nonterminal symbol.
Saers & Wu (2009) showthat it is possible to apply exhaustive biparsing to alarge parallel corpus (?
100, 000 sentence pairs) ofshort sentences (?10 tokens in both language).
Theword alignments read off the Viterbi parse also in-creased translation quality when used instead of thealignments from bidirectional IBM alignments.The O(n6) time complexity is somewhat pro-hibitive for large corpora, so pruning in some form isneeded.
Saers, Nivre & Wu (2009) introduce a beampruning scheme, which reduces time complexity toO(bn3).
They also show that severe pruning is pos-sible without significant deterioration in alignmentquality.
Haghighi et.
al (2009) use a simpler aligneras guidance for pruning, which reduce the time com-plexity by two orders of magnitude, and also intro-duce block ITG, which gives many-to-one instead ofone-to-one alignments.
Zhang et.
al (2008) presenta method for evaluating spans in the sentence pair todetermine whether they should be excluded or not.The algorithm has a best case time complexity ofO(n3).In this paper we introduce Linear ITG (LITG), andapply it to a word-alignment task which is evaluatedby the phrase-based statistical machine translation(PBSMT) system that can be built from that.3 Stochastic Bracketing Linear InversionTransduction GrammarA Bracketing Linear Inversion Transduction Gram-mar (BLITG) is a BITG where rules may have at mostone nonterminal symbol in their production.
Thisgives us a normal form that is somewhat differentfrom the usual ITG:X ?
[Xe/f ]X ?
[e/fX]X ?
?Xe/f?X ?
?e/fX?X ?
?/?where one but not both of the tokens in the bitermi-nal may be the empty string ?, if a nonterminal isproduced.
By associating each rule with a probabil-ity, we get a Stochastic BLITG (SBLITG).3.1 Biparsing AlgorithmThe sentence pair to be biparsed consists of two vec-tors of tokens (e and f ).
An item is representedas a nonterminal (X), and one span in each of thelanguages (es..t and fu..v).
For notational conve-nience, an item will be written as the nonterminalwith the spans as subscripts (Xs,t,u,v).
The length ofan item is defined as the sum of the length of the twospans: |Xs,t,u,v| = t ?
s + v ?
u.
Items are gath-ered in buckets, Bn, according to their length so thatXs,t,u,v ?
B|Xs,t,u,v|.
The algorithm is initializedwith the item spanning the entire sentence pair:X0,|e|,0,|f | ?
B|X0,|e|,0,|f||Starting from this top bucket, buckets are processedin larger to smaller order: Bn, Bn?1, .
.
.
, B1.
Whileprocessing a bucket, only smaller items are added,meaning that B0 is fully constructed by the time B1has been processed.
Each item in B0 can have therule X ?
?/?
applied to it, eliminating the nonter-minal and halting processing.
If there are no itemsin B0, parsing has failed.To process a bucket, each item is extended by allapplicable rules, and the nonterminals in the produc-tions are added to their respective buckets.342System BLEU NIST PhrasesGIZA++ (intersect) 0.2629 6.7968 146,581,109GIZA++ (grow-diag-final) 0.2632 6.7410 1,298,566GIZA++ (grow-diag-final-and) 0.2742 6.9228 7,340,369SBLITG (b = 25) 0.3027 7.3664 13,551,915SBLITG (b = ?)
0.3008 7.3303 12,673,361Table 1: Results for French?English.Xs,t,u,v ?
[es,s+1/fu,u+1 Xs+1,t,u+1,v]| [Xs,t?1,u,v?1 et?1,t/fv?1,v ]| ?es,s+1/fv?1,v Xs+1,t,u,v?1?| ?Xs,t?1,u+1,v et?1,t/fu,u+1?| [es,s+1/?
Xs+1,t,u,v] | ?es,s+1/?
Xs+1,t,u,v?| [?/fu,u+1 Xs,t,u+1,v] | ?Xs,t,u+1,v ?/fu,u+1?| [Xs,t?1,u,v et?1,t/?]
| ?Xs,t?1,u,v et?1,t/?
?| [Xs,t,u,v?1 ?/fv?1,v] | ?
?/fv?1,v Xs,t,u,v?1?Note that there are two productions on each of thefour last rows.
These are distinct rules, but thesymbols in the productions are identical.
This phe-nomenon is due to the fact that the empty sym-bols can be ?read?
off either end of the span.
Inour experiments, such rules were merged into theirnon-inverting form, effectively eliminating the lastfour inverted rules (productions enclosed in angledbrackets) above.3.2 AnalysisLet n be the length of the longer sentence in thepair.
The number of buckets will be O(n), sincethe longest item will be at most 2n long.
Within abucket, there can be O(n2) starting points for items,but once the length of one of the spans is fixed, thelength of the other follows, adding a factor O(n),making the total number of items in a bucket O(n3).Each item in a bucket can be analyzed in 8 possibleways, requiring O(1) time.
In summary, we have:O(n)?O(n3)?O(1) = O(n4)The pruning scheme works by limiting the num-ber of items that are processed from each bucket, re-ducing the cost of processing a bucket from O(n3)to O(b), where b is the beam width.
This gives timecomplexity O(n) ?O(b) ?O(1) = O(bn).4 ExperimentsWe used the guidelines of the shared task ofWMT?081 to train our baseline system as well asour experimental system.
This includes induction ofword alignments with GIZA++ (Och and Ney, 2003),induction of a Phrase-based SMT system (Koehn etal., 2007), and tuning with minimum error rate train-ing (Och, 2003), as well as applying some utilityscripts provided for the workshop.
The translationmodel is combined with a 5-gram language model(Stolcke, 2002).Our experimental system uses alignments fromthe Viterbi parses, extracted during EM training of anSBLITG on the training corpus, instead of GIZA++.Since EM will converge fairly slowly, it was limitedto 10 iterations, after which it was halted.We used the French?English part of the WMT?08shared task, but limited the training set to sentencepairs where both sentences were of length 20 or less.This was necessary in order to carry out exhaustivesearch in the SBLITG algorithm.
In total, we had381,780 sentence pairs for training, and 2,000 sen-tence pairs each for tuning and testing.
The languagemodel was trained with the entire training set.To evaluate the systems we used BLEU (Papineniet al, 2002) and NIST (Doddington, 2002)Results are presented in Table 1.
It is interestingto note that there is no correlation between the num-ber of phrases extracted and translation quality.
Theonly explanation for the results we are seeing is thatthe SBLITGs find better phrases.
Since the only dif-ference is the word alignment strategy, this suggeststhat the word alignments from SBLITGs are bettersuited for phrase extraction than those from bidirec-tional IBM-models.
The fact that SBLITGs extractmore phrases than bidirectional IBM-models under1http://www.statmt.org/wmt08/343the grow-diag-x heuristics is significant, sincemore phrases means that more translation possibil-ities are extracted.
The fact that SBLITGs extractfewer phrases than bidirectional IBM-models underthe intersect heuristic is also significant, sinceit implies that simply adding more phrases is a badstrategy.
Combined, the two observations leads usto believe that there are some alignments missed bythe bidirectional IBM-models that are found by theSBLITG-models.
It is also interesting to see that thepruned version outperforms the exhaustive version.We believe this to be because the pruned version ap-proaches the correct grammar faster than the exhaus-tive.
That would mean that the exhaustive SBLITGwould be better in the limit, but the experiment waslimited to 10 iterations.5 ConclusionIn this paper we have focused on the benefits of ap-plying SBLITGs to the task of inducing word align-ments, which leads to a 2.85 BLEU points improve-ment compared to the standard model (heuristicallycombined bidirectional IBM-models).
In the future,we hope that LITGs will be a spring board towardsfull ITGs, with more interesting nonterminals thanthe BITGs seen in the literature so far.
With the pos-sibility of inducing full ITG from parallel corpora itbecomes viable to use ITG decoders directly as ma-chine translation systems.AcknowledgmentsThis work was funded by the Swedish National Gradu-ate School of Language Technology (GSLT), the DefenseAdvanced Research Projects Agency (DARPA) underGALE Contract No.
HR0011-06-C-0023, and the HongKong Research Grants Council (RGC) under researchgrants GRF621008, DAG03/04.EG09, RGC6256/00E,and RGC6083/99E.
Any opinions, findings and conclu-sions or recommendations expressed in this material arethose of the authors and do not necessarily reflect theviews of DARPA.The computations were performed onUPPMAX resources under project p2007020.ReferencesA.
V. Aho and J. D. Ullman.
1972.
The Theory of Pars-ing, Translation, and Compiling.
Prentice-Halll, En-glewood Cliffs, New Jersey.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proceedings of Human Language Technologyconference (HLT-2002), San Diego, California.A.
Haghighi, J. Blitzer, J. DeNero, and D. Klein.
2009.Better word alignments with supervised itg models.In Proceedings of ACL/IJCNLP 2009, pages 923?931,Singapore.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of theACL 2007 Demo and Poster Session, pages 177?180,Prague, Czech Republic.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.F.
J. Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proceedings of ACL 2003,pages 160?167, Sapporo, Japan.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL 2002, pages 311?318, Philadelphia, Pennsylvania.M.
Saers and D. Wu.
2009.
Improving phrase-basedtranslation via word alignments from Stochastic In-version Transduction Grammars.
In Proceedings ofSSST-3 at NAACL HLT 2009, pages 28?36, Boulder,Colorado.M.
Saers, J. Nivre, and D. Wu.
2009.
Learning stochas-tic bracketing inversion transduction grammars witha cubic time biparsing algorithm.
In Proceedings ofIWPT?09, pages 29?32, Paris, France.A.
Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In International Conference on SpokenLanguage Processing, Denver, Colorado.D.
Wu.
1995.
An algorithm for simultaneously bracket-ing parallel texts by aligning words.
In Proceedings ofWVLC-3, pages 69?82, Cambridge, Massachusetts.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?403.H.
Zhang, C. Quirk, R. C. Moore, and D. Gildea.
2008.Bayesian learning of non-compositional phrases withsynchronous parsing.
In Proceedings of ACL/HLT2008, pages 97?105, Columbus, Ohio.344
