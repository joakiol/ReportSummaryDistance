Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 844?853,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSyllable weight encodes mostly the same information for English wordsegmentation as dictionary stressJohn K Pate Mark JohnsonCentre for Language TechnologyMacquarie UniversitySydney, NSW, Australia{john.pate,mark.johnson}@mq.edu.auAbstractStress is a useful cue for English wordsegmentation.
A wide range of computa-tional models have found that stress cuesenable a 2-10% improvement in segmen-tation accuracy, depending on the kind ofmodel, by using input that has been anno-tated with stress using a pronouncing dic-tionary.
However, stress is neither invari-ably produced nor unambiguously iden-tifiable in real speech.
Heavy syllables,i.e.
those with long vowels or syllablecodas, attract stress in English.
We de-vise Adaptor Grammar word segmentationmodels that exploit either stress, or sylla-ble weight, or both, and evaluate the util-ity of syllable weight as a cue to wordboundaries.
Our results suggest that sylla-ble weight encodes largely the same infor-mation for word segmentation in Englishthat annotated dictionary stress does.1 IntroductionOne of the first skills a child must develop in thecourse of language acquisition is the ability to seg-ment speech into words.
Stress has long beenrecognized as a useful cue for English word seg-mentation, following the observation that wordsin English are predominantly stress-initial (Cutlerand Carter, 1987), together with the result that 9-month-old English-learning infants prefer stress-initial stimuli (Jusczyk et al., 1993).
A range ofstatistical (Doyle and Levy, 2013; Christiansen etal., 1998; B?orschinger and Johnson, 2014) andrule-based (Yang, 2004; Lignos and Yang, 2010)models have used stress information to improveword segmentation.
However, that work usesstress-marked input prepared by marking vowelsthat are listed as stressed in a pronouncing dic-tionary.
This pre-processing step glosses over thefact that stress identification itself involves a non-trival learning problem, since stress has many pos-sible phonetic reflexes and no known invariants(Campbell and Beckman, 1997; Fry, 1955; Fry,1958).
One known strong correlate of stress inEnglish is syllable weight: heavy syllables, whichend in a consonant or have a long vowel, at-tract stress in English.
We present experimentswith Bayesian Adaptor Grammars (Johnson et al.,2007) that suggest syllable weight encodes largelythe same information for word segmentation thatdictionary stress information does.Specifically, we modify the AdaptorGrammar word segmentation model ofB?orschinger and Johnson (2014) to comparethe utility of syllable weight and stress cues forfinding word boundaries, both individually and incombination.
We describe how a shortcoming ofAdaptor Grammars prevents us from comparingstress and weight cues in combination with the fullrange of phonotactic cues for word segmentation,and design two experiments to work around thislimitation.
The first experiment uses grammarsthat provide parallel analyses for syllable weightand stress, and learns initial/non-initial phonotac-tic distinctions.
In this first experiment, syllableweight cues are actually more useful than stresscues at larger input sizes.
The second experimentfocuses on incorporating phonotactic cues fortypical word-final consonant clusters (such asinflectional morphemes), at the expense of parallelstructures.
In this second experiment, weight cuesmerely match stress cues at larger input sizes,and the learning curve for the combined weight-and-stress grammar follows almost perfectly withthe stress-only grammar.
This second experimentsuggests that the advantage of weight over stressin the first experiment was purely due to poormodeling of word-final consonant clusters bythe stress-only grammar, not weight per se.
Alltogether, these results indicate that syllable weight844is highly redundant with dictionary-based stressfor the purposes of English word segmentation;in fact, in our experiments, there is no detectabledifference between relying on syllable weight andrelying on dictionary stress.2 BackgroundStress is the perception that some syllables aremore prominent than others, and reflects a com-plex, language-specific interaction between acous-tic cues (such as loudness and duration), andphonological patterns (such as syllable shapes).The details on how stress is assigned, produced,and perceived vary greatly across languages.Three aspects of the English stress system arerelevant for this paper.
First, although Englishstress can shift in different contexts (Liberman andPrince, 1977), such as from the first syllable of?fourteen?
in isolation to the second syllable whenfollowed by a stressed syllable, it is largely stableacross different tokens of a given word.
Second,most words in English end up being stress-initialon a type and token basis.
Third, heavy syllables(those with a long vowel or a consonant coda) at-tract stress in English.There is experimental evidence that English-learning infants prefer stress-initial words fromaround the age of seven months (Jusczyk et al.,1993; Juszcyk et al., 1999; Jusczyk et al., 1993;Thiessen and Saffran, 2003).
A variety of com-putational models have subsequently been devel-oped that take stress-annotated input and use thisregularity to improve segmentation accuracy.
Theearliest Simple Recurrent Network (SRN) mod-eling experiments of Christiansen et al.
(1998)and Christiansen and Curtin (1999) found thatstress improved word segmentation from about39% to 43% token f-score (see Evaluation).
Ryt-ting et al.
(2010) applied the SRN model to prob-ability distributions over phones obtained from aspeech recognition system, and found that the en-tropy of the probability distribution over phones,as a proxy to local hyperarticulation and hence astress cue, improved token f-score from about 16%to 23%.
In a deterministic approach using pre-syllabified input, Yang (2004), with follow-ups inLignos and Yang (2010) and Lignos (2011; 2012),showed that a ?Unique Stress Constraint?
(USC),or assuming each word has at most one stressedsyllable, leads to an improvement of about 2.5%boundary f-score.Among explicitly probabilistic models,Doyle and Levy (2013) incorporated stress intoGoldwater et al.
?s (2009) Bigram model.
Theydid this by modifying the base distribution overlexical forms to generate not simply phone stringsbut a sequence of syllables that may or maynot be stressed.
The resulting model can learnthat some sequences of syllables (in particular,sequences that start with a stressed syllable)are more likely than others.
However, observedstress improved token f-score by only 1%.B?orschinger and Johnson (2014) used AdaptorGrammars (Johnson et al., 2007), a generalizationof Goldwater et al.
?s (2009) Bigram model thatwill be described shortly, and found a clearer4-10% advantage in token f-score, depending onthe amount of training data.Together, the experimental and computationalresults suggest that infants in fact pay attentionto stress, and that stress carries useful informationfor segmenting words in running speech.
How-ever, stress identification is itself a non-trivialtask, as stress has many highly variable, context-sensitive, and optional phonetic reflexes.
How-ever, one strong phonological cue in English issyllable weight: heavy syllables attract stress.Heavy syllables, in turn, are syllables with acoda and/or a long vowel, which, in English,are tense vowels.
Turk et al.
(1995) replicatedthe Jusczyk et al.
(1993) finding that English-learning infants prefer stress-initial stimuli (usingnon-words), and then examined how stress inter-acted with syllable weight.
They found that sylla-ble weight was not a necessary condition to trig-ger the preference: infants preferred stress-initialstimuli even if the initial syllable was light.
How-ever, they also found that infants most stronglypreferred stimuli whose first syllable was bothstressed and heavy: infants preferred stress-initialand heavy-initial stimuli to stress-initial and light-initial stimuli.
This result suggests that infants aresensitive to syllable weight in determining typicalstress and rythmic patterns in their language.2.1 ModelsWe will adopt the Adaptor Grammar frameworkused by B?orschinger and Johnson (2014) to ex-plore the utility of syllable weight as a cueto word segmentation by way of its covariancewith stress.
Adaptor Grammars are Probabilis-tic Context Free Grammars (PCFGs) with a spe-845SyllOnsetkRhymeNucleus?Codats(a) Basic syllable.SyllIFOnsetIkRhymeIFNucleusI?CodaFts(b) Mono-syllable with initial Rhyme.SyllIFOnsetIkRhymeFNucleusF?CodaFts(c) Mono-syllable with final Rhyme.Figure 1: Different ways to incorporate phonotactics.
It is not possible to capture word-final codas andword initial rhymes in monosyllabic words with factors the size of a PCFG rule.cial set of adapted non-terminal nodes.
We un-derline adapted non-terminals (X) to distinguishthem from non-adapted non-terminals (Y).
Whilea vanilla PCFG can only directly model regular-ities that are expressed by a single re-write rule,an Adaptor Grammar model caches entire subtreesthat are rooted at adapted non-terminals.
AdaptorGrammars can thus learn the internal structure ofwords, such as syllables, syllable onsets, and syl-lable rhymes, while still learning entire words aswell.In Adaptor Grammars, parameters are associ-ated with PCFG rules.
While this has been a usefulfactorization in previous work, it makes it difficultto integrate syllable weight and syllable stress ina linguistically natural way.
A syllable is typicallyanalyzed as having an optional onset followed by arhyme, with the rhyme rewriting to a nucleus (thevowel) followed by an optional coda, as in Fig-ure 1a.
We expect stress and syllable weight to beuseful primarily because initial syllables tend to bedifferent from non-initial syllables.
However, dis-tinguishing final from non-final codas should beuseful as well, due to the frequency of suffixes inEnglish, and the importance of edge phenomena inphonology more generally (Brent and Cartwright,1996).
These principles come into conflict whenmodeling monosyllabic words.
If we say that amonosyllable is an Initial and Final SyllIF, andhas an initial Onset and an initial Rhyme, as inFigure 1b, then we can learn the initial/non-initialgeneralization about stressed or heavy rhymes atthe expense of the generalization about final andnon-final codas.
If we say that a monosyllable isan initial onset with a final rhyme, the reverse oc-curs: we can learn the final/non-final coda gen-eralization at the expense of the initial/non-initialregularities.
If we split the symbols further, we?dgeneralize even less: we?d essentially have to learnthe initial/non-initial patterns separately for mono-syllables and polysyllables.The most direct solution would introduce fac-tors that are ?smaller?
than a single PCFG rule.
Es-sentially, we would compute the score of a PCFGrule in terms of multiple features of its right-handside, rather than a single ?one-hot?
feature identi-fying the expansion.
We left this direction for fu-ture work and instead carried out two experimentsusing Adaptor Grammars that were designed towork around this limitation.Our first experiment focuses on modelingthe initial/non-initial distinction, leaving thefinal/non-final coda distinction unmodeled.
Themodels in this experiment assume parallel struc-tures for syllable weight and stress, and focus onproviding the most direct comparison between syl-lable weight and stress with a strictly initial/non-initial distinction.
This first experiment shows thatobserving dictionary stress is better early in learn-ing, but that modeling syllable weight is betterlater in learning.
However, it is possible that sylla-ble weight was more useful because modeling syl-lable weight involves modeling the characteristicsof codas; the advantage may not have been due toweight per se but due to having learned somethingabout the effects of suffixes on final codas.Our second experiment focuses on modelingsome aspects of final codas at the expense of main-taining a rigid parallelism in the structures for syl-lable weight and stress.
The models in this exper-iment split only those symbols that are necessaryto bring stress or weight patterns into the expres-sive power of the model, and focus on comparingricher models of syllable weight and stress thataccount for inital/internal/final distinctions.
Thissecond experiment shows that observing dictio-nary stress is better early in learning, and thatmodeling syllable weight merely catches up to846Sentence ?
Collocations3+(1)Collocations3 ?
Collocations2+(2)Collocations2 ?
Collocation+(3)Collocation ?
Word+(4)Figure 2: Three levels of collocation; symbols fol-lowed by+may occur one or more times.stress without surpassing it.
Moreover, a com-bined stress-and-weight model does no better thana stress model, suggesting that the weight gram-mar?s contribution is fully redundant, for the pur-poses of word segmentation, with the stress obser-vations.Together, these experiments suggest that sylla-ble weight eventually encodes everything aboutword segmentation that dictionary stress does, andthat any advantage that syllable weight has overobserving dictionary stress is entirely redundantwith knowledge of word-final codas.3 Experiments3.1 Adaptor GrammarsWe follow B?orschinger and Johnson (2014) in us-ing a 3-level collocation Adaptor Grammar, as in-troduced by Johnson and Goldwater (2009) andpresented in Figure 2, as the backbone for allmodels, including the baseline.
A 3-level collo-cation grammar assumes that words are groupedinto collocations of words that tend to appear witheach other, and that the collocations themselvesare grouped into larger collocations, up to threelevels of collocations.
This collocational struc-ture allows the model to capture strong word-to-word dependencies without having to groupfrequently-occuring word sequences into a single,incorrect, undersegmented ?word?
as the unigrammodel tends to do (Johnson and Goldwater, 2009)Word rewrites in different ways in Experiment Iand Experiment II, which will be explained in therelevant experiment section.3.2 Experimental Set-upWe applied the same experimental set-up used byB?orschinger and Johnson (2014), to their dataset,as described below.
To understand how differentmodeling assumptions interact with corpus size,we train on prefixes of each corpus with increas-ing input size: 100, 200, 500, 1,000, 2,000, 5,000,and 10,000 utterances.
Inference closely fol-lowed B?orschinger and Johnson (2014) and John-son and Goldwater (2009).
We set our hyperpa-rameters to encourage onset maximization.
Thehyperparameter for syllable nodes to rewrite toan onset followed by a rhyme was 10, and thehyperparameter for syllable nodes to rewrite to arhyme only was 1.
Similarly, the hyperparame-ter for rhyme nodes to include a coda was 1, andthe hyperparameter for rhyme nodes to excludethe coda was 10.
All other hyperparameters spec-ified vague priors.
We ran eight chains of eachmodel for 1,000 iterations, collecting 20 sampleswith a lag of 10 iterations between samples and aburn-in of 800 iterations.
We used the same batch-initialization and table-label resampling to encour-age the model to mix.After gathering the samples, we used them toperform a single minimum Bayes risk decodingof a separate, held-out test set.
This test set wasconstructed by taking the last 1,000 utterances ofeach corpus.
We use a common test-set insteadof just evaluating on the training data to ensurethat performance figures are comparable across in-put sizes; when we see learning curves slope up-ward, we can be confident that the increase is dueto learning rather than easier evaluation sets.We measured our models?
performance with theusual token f-score metric (Brent, 1999), the har-monic mean of how many proposed word tokensare correct (token precision) and how many of theactual word tokens are recovered (token recall).For example, a model may propose ?the in side?when the true segmentation is ?the inside.?
Thissegmentation would have a token precision of13,since one of three predicted words matches thetrue word token (even though the other predictedwords are valid word types), and a token recall of12, since it correctly recovered one of two words,yield a token f-score of 0.4.3.3 DatasetWe evaluated on a dataset drawn from the Alexportion of the Providence corpus (Demuth et al.,2006).
This dataset contains 17, 948 utteranceswith 72, 859 word tokens directed to one childfrom the age of 16 months to 41 months.
We useda version of this dataset that contained annota-tions of primary stress that B?orschinger and John-son (2014) added to this input using an extended847RhymeI ?
HeavyRhymeRhymeI ?
LightRhymeRhyme ?
HeavyRhymeRhyme ?
LightRhymeHeavyRhyme ?
LongVowelHeavyRhyme ?
Vowel CodaLightRhyme ?
ShortVowel(a) Weight-sensitive grammarRhymeI ?
RhymeSRhymeI ?
RhymeURhyme ?
RhymeSRhyme ?
RhymeURhymeS ?
Vowel Stress (Coda)RhymeU ?
Vowel (Coda)(b) Stress-sensitive grammarRhymeI ?
Vowel (Coda)Rhyme ?
Vowel (Coda)(c) Baseline grammarRhymeI ?
HeavyRhymeSRhymeI ?
HeavyRhymeURhymeI ?
LightRhymeSRhymeI ?
LightRhymeURhyme ?
HeavyRhymeSRhyme ?
HeavyRhymeURhyme ?
LightRhymeSRhyme ?
LightRhymeUHeavyRhymeS ?
LongVowel StressHeavyRhymeS ?
LongVowel Stress CodaHeavyRhymeU ?
LongVowelHeavyRhymeU ?
LongVowel CodaLightRhymeS ?
ShortVowel StressLightRhymeU ?
ShortVowel(d) Combined grammarFigure 3: Experiment I Grammarsversion of CMUDict (cmu, 2008).1The meannumber of syllables per word token was 1.2, andonly three word tokens had more than five sylla-bles.
Of the 40, 323 word tokens with a stressedsyllable, 27, 258 were monosyllabic.
Of the13, 065 polysyllabic word tokens with a stressedsyllable, 9, 931 were stress-initial.
Turning to the32, 536 word tokens with no stress (i.e., the func-tion words), all but 23 were monosyllabic (the 23were primarily contractions, such as ?couldn?t?
).3.4 Experiment I: Parallel StructuresThe goal of this first experiment is to provide themost direct comparison possible between gram-mars that attend to stress cues and grammars thatattend to syllable weight cues.
As these are bothhypothesized to be useful by way of an initial/non-initial distinction, we defined a word to be an ini-tial syllable SyllI followed by zero to three sylla-bles, and syllables to consist of an optional onset1This dataset and these Adap-tor Grammar models are available at:http://web.science.mq.edu.au/?jpate/stress/and a rhyme:Word ?
SyllI (Syll){0,3}(5)SyllI ?
(OnsetI) RhymeI (6)Syll ?
(Onset) Rhyme (7)In the baseline grammar, presented in Figure 3c,rhymes rewrite to a vowel followed by an optionalconsonant coda.
Rhymes then rewrite to be heavyor light in the weight grammar, as in Figure 3a, tobe stressed or unstressed in the stress grammar, asin Figure 3b.
In the combination grammar, rhymesrewrite to be heavy or light and stressed or un-stressed, as in Figure 3d.
LongVowel and Short-Vowel both re-write to all vowels.
An additionalgrammar that restricted them to rewrite to long andshort vowels, respectively, led to virtually identi-cal performance, suggesting that vowel quantitycan be learned for the purposes of word segmenta-tion from distributional cues.
We will also presentevidence that the model did manage to learn mostof the contrast.Figure 4 presents learning curves for the gram-mars in this parallel structured comparison.
Wesee that observing stress without modeling weight8480.60.70.80.9100 1000 10000Number of utterancesTokenF?Scorenoweight:nostressnoweight:stressweight:nostressweight:stressFigure 4: Learning curves on the Alex corpus for Experiment I grammars with parallel distinctionsbetween Stressed/Unstressed and Heavy/Light syllable rhymes.?????a?a??e??i?o???
?u?LongVowel ShortVowel VowelVowel1 10 100 1000 7000Vowel counts by quantityFigure 5: Heatmap of learned vowels in the Ex-periment I weight-only grammar.
Each cell cor-responds to the count of a particular vowel beinganalyzed as one of the three vowel types.
Diph-thongs are rarely ShortVowel.outperforms both the baseline and the weight-onlygrammar early in learning.
The weight-only gram-mar rapidly improves in performance at largertraining data sizes, increasing its advantage overthe baseline, while the advantage of the stress-only grammar slows and appears to disappear atthe largest training data size.
At 10,000 utterances,the improvement of the weight-only grammar overthe stress-only grammar is significant according toan independent samples t-test (t = 7.2, p < 0.001,14 degrees of freedom).
This pattern suggests thatannotated dictionary stress is easy to take advan-tage of at low data sizes, but that, with sufficientdata, syllable weight can provide even more in-formation about word boundaries.
The best over-all performance early in learning is obtained bythe combined grammar, suggesting that syllableweight and dictionary stress provide informationabout word segmentation that is not redundant.An examination of the final segmentation sug-gests that the weight grammar has learned thatinitial syllables tend to be heavy.
Specifically,across eight runs, 98.1% of RhymeI symbolsrewrote to HeavyRhyme, whereas only 54.5% ofRhyme symbols (i.e.
non-initial rhymes) rewroteto HeavyRhyme.849Model Mean TF Std.
Dev.noweight:nostress 0.830 0.005noweight:stress 0.831 0.008weight:nostress 0.861 0.008weight:stress 0.861 0.008Table 1: Segmentation Token F-score for Experi-ment I at 10,000 utterances across eight runs.We also examined the final segmentation to seewell the model learned the distinction betweenlong vowels and short vowels.
Figure 5 presents aheatmap, with colors on a log-scale, showing howmany times each vowel label rewrote to each pos-sible vowel in the (translated to IPA).
Although thequantity generalisations are not perfect, we do seea general trend where ShortVowel rarely rewritesto diphthongs.3.5 Experiment II: Word-final CodasExperiment I suggested that, under a ba-sic initial/non-initial distinction, syllable weighteventually encodes more information about wordboundaries than does dictionary stress.
This isa surprising result, since we initially investigatedsyllable weight as a noisy proxy for dictionarystress.
One possible source of the ?extra?
advan-tage that the syllable weight grammar exhibitedhas to do with the importance of word-final codas,which can encode word-final morphemes in En-glish (Brent and Cartwright, 1996).
Even thoughthe grammars did not explicitly model them, theweight grammar could implicitly capture a bias foror against having a coda in non-initial position,while the stress grammar could not.
This is be-cause most word tokens are one or two syllables,and only one of the two rhyme types of the weightgrammar included a coda.
Thus, the HeavyRhymesymbol could simultaneously capture the most im-portant aspects of both stress and coda constraints.To see if the extra advantage of the syllableweight grammar can be attributed to the influenceof word-final codas, we formulated a set of gram-mars that model word-final codas and also canlearn stress and/or syllable weight patterns.
Thesegrammars are more similar in structure to the onesthat B?orschinger and Johnson (2014) used.
For thebaseline and weight grammar, we again definedwords to consist of up to four syllables with an ini-tial SyllI syllable, but this time distinguished finalsyllables SyllF in polysyllabic words.
The non-stress grammars use the following rules for pro-ducing syllables:Word ?
SyllIF (8)Word ?
SyllI (Syll){0,2}SyllF (9)SyllIF ?
(OnsetI) RhymeI (10)SyllI ?
(OnsetI) RhymeI (11)Syll ?
(Onset) Rhyme (12)SyllF ?
(Onset) RhymeF (13)For the stress grammar, we followedB?orschinger and Johnson (2014) in distin-guishing stressed and unstressed syllables, ratherthan simply stressed rhymes as in Experiment I,to allow the model to learn likely stress patternsat the word level.
A word can consist of up tofour syllables, and any syllable and any numberof syllables may be stressed, as in Figure 6a.The baseline grammar is similar to the previousone, except it distinguishes word-final codas, asin Figure 6b.
The weight grammar, presented inFigure 6c, rewrites rhymes to a nucleus followedby an optional coda and distinguishes nuclei inopen syllables according to their position in theword.
The stress grammar, presented in Figure 6d,is the all-stress-patterns model (without the uniquestress constraint) B?orschinger and Johnson (2014).This grammar introduces additional distinctions atthe syllable level to learn likely stress patterns,and distinguishes final from non-final codas.
Thecombined model is identical to the stress model,except Vowel non-terminals in closed and word-internal syllables are replaced with Nucleus non-terminals, and Vowel non-terminals in word-inital(-final) open syllables are replaced with NucleusI(NucleusF) non-terminals.To summarize, the stress models distinguishstressed and unstressed syllables in initial, final,and internal position.
The weight models distin-guish the vowels of initial open syllables, the vow-els of final open syllables, and other vowels, al-lowing them to take advantage of an important cuefrom syllable weight for word segmentation: if aninitial vowel is open, it should usually be long.Figure 7 shows segmentation performance onthe Alex corpus with these more complete models.While the performance of the weight grammars isvirtually unchanged compared to Figure 4, the twogrammars that do not model syllable weight im-prove dramatically.
This result supports our pro-posal that much of the advantage of the weight850Word ?
{SyllUIF|SyllSIF}Word ?
{SyllUI|SyllSI} {SyllU|SyllS}{0,2}{SyllUF|SyllSF}(a) The all-patterns stress modelRhyme ?
Vowel (Coda)RhymeF ?
Vowel (CodaF)(b) Baseline grammarRhymeI ?
NucleusIRhymeI ?
Nucleus CodaRhyme ?
Nucleus (Coda)RhymeF ?
NucleusFRhymeF ?
Nucleus CodaF(c) Weight-sensitive grammarSyllSIF ?
OnsetI RhymeSFSyllUIF ?
OnsetI RhymeUFSyllSI ?
Onset RhymeSSyllUI ?
Onset RhymeUSyllSF ?
Onset RhymeSFSyllUF ?
Onset RhymeUFRhymeSI ?
Vowel Stress (Coda)RhymeUI ?
Vowel (Coda)RhymeS ?
Vowel Stress (Coda)RhymeU ?
Vowel (Coda)RhymeSF ?
Vowel Stress (CodaF)RhymeUF ?
Vowel (CodaF)(d) Stress-sensitive grammarFigure 6: Experiment II Grammars.0.60.70.80.9100 1000 10000Number of utterancesTokenF?Scorenoweight:nostressnoweight:stressweight:nostressweight:stressFigure 7: Learning curves on the Alex corpus for Experiment II grammars with word-final phonotacticsthat exploit Stress and Weight.851Model Mean TF Std.
Dev.noweight:nostress 0.846 0.007noweight:stress 0.880 0.005weight:nostress 0.865 0.011weight:stress 0.875 0.005Table 2: Segmentation Token F-score for Experi-ment II at 10,000 utterances across eight runs.grammars over stress in Experiment I was due tomodeling of word-final coda phonotactics.Table 2 presents token f-score at 10,000 train-ing utterances averaged across eight runs, alongwith the standard deviation in f-score.
We see thatthe noweight:nostress grammar is several standarddeviations than the grammars that model sylla-ble weight and/or stress, while the syllable weightand/or stress grammars exhibit a high degree ofoverlap.4 ConclusionWe have presented computational modeling exper-iments that suggest that syllable weight (eventu-ally) encodes nearly everything about word seg-mentation that dictionary stress does.
Indeed,our experiments did not find a persistent advan-tage to observing stress over modeling syllableweight.
While it is possible that a different mod-eling approach might find such a persistent advan-tage, this advantage could not provide more than13% absolute F-score.
This result suggests thatchildren may be able to learn and exploit impor-tant rhythm cues to word boundaries purely onthe basis of segmental input.
However, this resultalso suggests that annotating input with dictionarystress has missed important aspects of the roleof stress in word segmentation.
As mentioned,Turk et al.
(1995) found that infants preferred ini-tial light syllables to be stressed.
Such a prefer-ence obviously cannot be learned by attending tosyllable weight alone, so infants who have learnedweight distinctions must also be sensitive to non-segmental acoustic correlates to stress.
There wasno long-term advantage to observing stress in ad-dition to attending to syllable weight in our mod-els, however, suggesting that annotated dictionarystress does not capture the relevant non-segmentalphonetic detail.
More modeling is necessary to as-sess the non-segmental phonetic features that dis-tinguish stressed light syllables from unstressedlight syllables.This investigation also highlighted a weaknessof current Adaptor Grammar models: the ?small-est?
factors are the size of one PCFG rule.
Allow-ing further factorizations, perhaps using featurefunctions of a rule?s right-hand side, would allowmodels to capture finer-grained distinctions with-out fully splitting the symbols that are involved.ReferencesBenjamin B?orschinger and Mark Johnson.
2014.
Ex-ploring the role of stress in Bayesian word segmen-tation using Adaptor Grammars.
Transactions of theACL, 2:93?104.Michael R Brent and Timothy A Cartwright.
1996.Distributional regularity and phonotactic constraintsare useful for segmentation.
Cognition, 61:93?125.Michael Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word discov-ery.
Machine Learning, 34:71?105.Nick Campbell and Mary Beckman.
1997.
Stress,prominence, and spectral tilt.
In Proceedings of anESCA workshop, pages 67?70, Athens, Greece.Morten H. Christiansen and Suzanne L Curtin.
1999.The power of statistical learning: No need for alge-braic rules.
In Proceedings of the 21st annual con-ference of the Cognitive Science Society.Morten H. Christiansen, Joseph Allen, and Mark S.Seidenberg.
1998.
Learning to segment speechusing multiple cues: A connectionist model.
Lan-guage and Cognitive Processes, 13:221?268.2008.
The CMU pronouncing dictionary.http://www.speech.cs.cmu.edu/cgi-bin/cmudict.Anne Cutler and David M Carter.
1987.
The predom-inance of strong initial syllables in the English vo-cabulary.
Computer Speech & Language, 2(3):133?142.Katherine Demuth, Jennifer Culbertson, and JenniferAlter.
2006.
Word-minimality, epenthesis, and codalicensing in the acquisition of English.
Languageand Speech, 49:137?174.Gabriel Doyle and Roger Levy.
2013.
Combining mul-tiple information types in Bayesian word segmenta-tion.
In Proceedings of NAACL 2013, pages 117?126.
Association for Computational Linguistics.D B Fry.
1955.
Duration and intensity as physical cor-relates of linguistic stress.
J. Acoust.
Soc.
of Am.,27:765?768.D B Fry.
1958.
Experiments in the perception of stress.Language and Speech, 1:126?152.852Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A Bayesian framework for wordsegmentation: Exploring the effects of context.Cognition, 112(1):21?54.Mark Johnson and Sharon Goldwater.
2009.
Im-proving nonparametric Bayesian inference: exper-iments on unsupervised word segmentation withadaptor grammars.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 317?325.
As-sociation for Computational Linguistics.Mark Johnson, Thomas L Griffiths, and Sharon Gold-water.
2007.
Adaptor grammars: A framework forspecifying compositional nonparametric Bayesianmodels.
In B Schoelkopf, J Platt, and T Hoffmann,editors, Advances in Neural Information ProcessingSystems, volume 19.
The MIT Press.Peter W Jusczyk, Anne Cutler, and Nancy J Redanz.1993.
Infants?
preference for the predominant stresspatterns of English words.
Child Development,64(3):675?687.Peter W Juszcyk, Derek M Houston, and Mary New-some.
1999.
The beginnings of word segmentationin English-learning infants.
Cognitive Psychology,39(3?4):159?207.Mark Liberman and Alan Prince.
1977.
On stress andlinguistic rhythm.
Linguistic Inquiry, 8(2):249?336,Spring.Constantine Lignos and Charles Yang.
2010.
Reces-sion segmentation: simpler online word segmenta-tion using limited resources.
In Proceedings of ACL2010, pages 88?97.
Association for ComputationalLinguistics.Constantine Lignos.
2011.
Modeling infant word seg-mentation.
In Proceedings of the fifteenth confer-ence on computational natural language learning,pages 29?38.
Association for Computational Lin-guistics.Constantine Lignos.
2012.
Infant word segmentation:An incremental, integrated model.
In Proceedingsof the West Coast Conference on Formal Linguistics30.C Anton Rytting, Chris Brew, and Eric Fosler-Lussier.2010.
Segmenting words from natural speech: sub-segmental variation in segmental cues.
Journal ofChild Language, 37(3):513?543.Erik D Thiessen and Jenny R Saffran.
2003.
Whencues collide: use of stress and statistical cues to wordboundaries by 7-to-9-month-old infants.
Develop-mental Psychology, 39(4):706?716.Alice Turk, Peter W Jusczyk, and Louann Gerken.1995.
Do English-learning infants use syllableweight to determine stress?
Language and Speech,38(2):143?158.Charles Yang.
2004.
Universal grammar, statistics orboth?
Trends in Cognitive Science, 8(10):451?456.853
