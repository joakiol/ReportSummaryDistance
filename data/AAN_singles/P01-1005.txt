Scaling to Very Very Large Corpora forNatural Language DisambiguationMichele Banko and Eric BrillMicrosoft Research1 Microsoft WayRedmond, WA 98052 USA{mbanko,brill}@microsoft.comAbstractThe amount of readily available on-linetext has reached hundreds of billions ofwords and continues to grow.
Yet formost core natural language tasks,algorithms continue to be optimized,tested and compared after training oncorpora consisting of only one millionwords or less.
In this paper, weevaluate the performance of differentlearning methods on a prototypicalnatural language disambiguation task,confusion set disambiguation, whentrained on orders of magnitude morelabeled data than has previously beenused.
We are fortunate that for thisparticular application, correctly labeledtraining data is free.
Since this willoften not be the case, we examinemethods for effectively exploiting verylarge corpora when labeled data comesat a cost.1 IntroductionMachine learning techniques, whichautomatically learn  linguistic information fromonline text corpora, have been applied to anumber of natural language problemsthroughout the last decade.
A large percentageof papers published in this area involvecomparisons of different learning approachestrained and tested with commonly used corpora.While the amount of available online text hasbeen increasing at a dramatic rate, the size oftraining corpora typically used for learning hasnot.
In part, this is due to the standardization ofdata sets used within the field, as well as thepotentially large cost of annotating data forthose learning methods that rely on labeled text.The empirical NLP community has putsubstantial effort into evaluating performance ofa large number of machine learning methodsover fixed, and relatively small, data sets.
Yetsince we now have access to significantly moredata, one has to wonder what conclusions thathave been drawn on small data sets may carryover when these learning methods are trainedusing much larger corpora.In this paper, we present a study of theeffects of data size on machine learning fornatural language disambiguation.
In particular,we study the problem of selection amongconfusable words, using orders of magnitudemore training data than has ever been applied tothis problem.
First we show learning curves forfour different machine learning algorithms.Next, we consider the efficacy of voting, sampleselection and partially unsupervised learningwith large training corpora, in hopes of beingable to obtain the benefits that come fromsignificantly larger training corpora withoutincurring too large a cost.2 Confusion Set DisambiguationConfusion set disambiguation is the problem ofchoosing the correct use of a word, given a setof words with which it is commonly confused.Example confusion sets include: {principle ,principal}, {then, than}, {to,two,too}, and{weather,whether}.Numerous methods have been presentedfor confusable disambiguation.
The more recentset of techniques includes mult iplicative weight-update algorithms (Golding and Roth, 1998),latent semantic analysis (Jones and Martin,1997), transformation-based learning (Manguand Brill, 1997), differential grammars (Powers,1997), decision lists (Yarowsky, 1994), and avariety of Bayesian classifiers (Gale et al, 1993,Golding, 1995, Golding and Schabes, 1996).
Inall of these approaches, the problem isformulated as follows:  Given a specificconfusion set (e.g.
{to,two,too}), all occurrencesof confusion set members in the test set arereplaced by a marker;  everywhere the systemsees this marker, it must decide which memberof the confusion set to choose.Confusion set disambiguation is one of aclass of natural language problems involvingdisambiguation from a relatively small set ofalternatives based upon the string context inwhich the ambiguity site appears.
Other suchproblems include word sense disambiguation,part of speech tagging and some formulations ofphrasal chunking.
One advantageous aspect ofconfusion set disambiguation, which allows usto study the effects of large data sets onperformance, is that labeled training data isessentially free, since the correct answer issurface apparent in any collection of reasonablywell-edited text.3 Learning Curve Expe rimentsThis work was partially motivated by the desireto develop an improved grammar checker.Given a fixed amount of time, we consideredwhat would be the most effective way to focusour efforts in order to attain the greatestperformance improvement.
Some possibilitiesincluded modifying standard learningalgorithms, exploring new learning techniques,and using more sophisticated features.
Beforeexploring these somewhat expensive paths, wedecided to first see what happened if we simplytrained an existing method with much moredata.
This led to the exploration of learningcurves for various machine learning algorithms :winnow1, perceptron, na?ve Bayes, and a verysimple memory-based learner.
For the firstthree learners, we used the standard collection offeatures employed for this problem: the set ofwords within a window of the target word, andcollocations containing words and/or parts of1 Thanks to Dan Roth for making both Winnow andPerceptron available.speech.
The memory-based learner used onlythe word before and word after as features.0.700.750.800.850.900.951.000.1 1 10 100 1000Millions of WordsTestAccuracyMemory-BasedWinnowPerceptronNa?ve BayesFigure 1.
Learning Curves for Confusion SetDisambiguationWe collected a 1-billion-word trainingcorpus from a variety of English texts, includingnews articles, scientific abstracts, governmenttranscripts, literature and other varied forms ofprose.
This training corpus is three orders ofmagnitude greater than the largest trainingcorpus previously used for this problem.
Weused 1 million words of Wall Street Journal textas our test set, and no data from the Wall StreetJournal was used when constructing the trainingcorpus.
Each learner was trained at severalcutoff points in the training corpus, i.e.
the firstone million words, the first five million words,and so on, until all one billion words were usedfor training.
In order to avoid training biases thatmay result from merely concatenating thedifferent data sources to form a larger trainingcorpus, we constructed each consecutivetraining corpus by probabilistically samplingsentences from the different sources weightedby the size of each source.In Figure 1, we show learning curves foreach learner, up to one billion words of trainingdata.
Each point in the graph is the averageperformance over ten confusion sets for that sizetraining corpus.
Note that the curves appear tobe log-linear even out to one billion words.Of course for many problems, additionaltraining data has a non-zero cost.
However,these results suggest that we may want toreconsider the trade-off between spending timeand money on algorithm development versusspending it on corpus development.
At least forthe problem of confusable disambiguation, noneof the learners tested is close to asymptoting inperformance at the training corpus sizecommonly employed by the field.Such gains in accuracy, however, do notcome for free.
Figure 2 shows the size oflearned representations as a function of trainingdata size.
For some applications, this is notnecessarily a concern.
But for others, wherespace comes at a premium, obtaining the gainsthat come with a billion words of training datamay not be viable without an effort made tocompress information.
In such cases, one couldlook at numerous methods for compressing data(e.g.
Dagan and Engleson, 1995, Weng, et al1998).4 The Efficacy of VotingVoting has proven to be an effective techniquefor improving classifier accuracy for manyapplications, including part-of-speech tagging(van Halteren, et al 1998), parsing (Hendersonand Brill, 1999), and word sense disambiguation(Pederson, 2000).
By training a set of classifierson a single training corpus and then combiningtheir outputs in classification, it is often possibleto achieve a target accuracy with less labeledtraining data than would be needed if only oneclassifier  was being used.
Voting can beeffective in reducing both the bias of a particulartraining corpus and the bias of a specific learner.When a training corpus is very small, there ismuch more room for these biases to surface andtherefore for voting to be effective.
But doesvoting still offer performance gains whenclassifiers are trained on much larger corpora?The complementarity between twolearners was defined by Brill and Wu (1998) inorder to quantify the percentage of time whenone system is wrong, that another system iscorrect, and therefore providing an upper boundon combination accuracy.
As training sizeincreases significantly, we would expectcomplementarity between classifiers to decrease.This is due in part to the fact that a largertraining corpus will reduce the data set varianceand any bias arising from this.
Also, some ofthe differences between classifiers might be dueto how they handle a sparse training set.11010010001000010000010000001 10 100 1000Millions of WordsWinnowMemory-BasedFigure 2.
Representation Size vs. TrainingCorpus SizeAs a result of comparing a sample oftwo learners as a function of increasingly largetraining sets, we see in Table 1 thatcomplementarity does indeed decrease astraining size increases.Training Size (words) Complementarity(L1,L2)106 0.2612107 0.2410108 0.1759109 0.1612Table 1.
ComplementarityNext we tested whether this decrease incomplementarity meant that voting loses itseffectiveness as the training set increases.
Toexamine the impact of voting when using asignificantly larger training corpus, we ran 3 outof the 4 learners on our set of 10 confusablepairs, excluding the memory-based learner.Voting was done by combining the normalizedscore each learner assigned to a classificationchoice.
In Figure 3, we show the accuracyobtained from voting, along with the single bestlearner accuracy at each training set size.
Wesee that for very small corpora, voting isbeneficial, resulting in better performance thanany single classifier.
Beyond 1 million words,little is gained by voting, and indeed on thelargest training sets voting actually hurtsaccuracy.0.800.850.900.951.000.1 1 10 100 1000Millions of wordsTestAccuracyBestVotingFigure 3.
Voting Among Classifiers5 When Annotated Data Is Not FreeWhile the observation that learning curves arenot asymptoting even with orders of magnitudemore training data than is currently used is veryexciting, this result may have somewhat limitedramifications.
Very few problems exist forwhich annotated data of this size is available forfree.
Surely we cannot reasonably expect thatthe manual annotation of one billion wordsalong with corresponding parse trees will occurany time soon (but see (Banko and Brill 2001)for a discussion that this might not becompletely infeasible).
Despite this pitfall, thereare techniques one can use to try to obtain thebenefits of considerably larger training corporawithout incurring significant additional costs.
Inthe sections that follow, we study two suchsolutions: active learning and unsupervisedlearning.5.1    Active LearningActive learning involves intelligently selecting aportion of samples for annotation from a pool ofas-yet unannotated training samples.
Not allsamples in a training set are equally useful.
Byconcentrating human annotation efforts on thesamples of greatest utility to the machinelearning algorithm, it may be possible to attainbetter performance for a fixed annotation costthan if samples were chosen randomly forhuman annotation.Most active learning approaches workby first training a seed learner (or family oflearners) and then running the learner(s) over aset of unlabeled samples.
A sample ispresumed to be more useful for training themore uncertain its classification label is.Uncertainty can be judged by the relativeweights assigned to different labels by a singleclassifier (Lewis and Catlett, 1994).
Anotherapproach, committee-based sampling, firstcreates a committee of classifie rs and thenjudges classification uncertainty according tohow much the learners differ among labelassignments.
For example, Dagan and Engelson(1995) describe a committee-based samplingtechnique where a part of speech tagger istrained using an annotated seed corpus.
Afamily of taggers is then generated by randomlypermuting the tagger probabilities, and thedisparity among tags output by the committeemembers is used as a measure of classificationuncertainty.
Sentences for human annotationare drawn, biased to prefer those containing highuncertainty instances.While active learning has been shown towork for a number of tasks, the majority ofactive learning experiments in natural languageprocessing have been conducted using verysmall seed corpora and sets of unlabeledexamples.
Therefore, we wish to exploresituations where we have, or can afford, a non-negligible sized training corpus (such as forpart-of-speech tagging) and have access to verylarge amounts of unlabeled data.We can use bagging (Breiman, 1996), atechnique for generating a committee ofclassifiers, to assess the label uncertainty of apotential training instance.
With bagging, avariant of the original training set is constructedby randomly sampling sentences withreplacement from the source training set in orderto produce N new training sets of size equal tothe original.
After the N models have beentrained and run on the same test set, theirclassifications for each test sentence can becompared for classification agreement.
Thehigher the disagreement between classifiers, themore useful it would be to have an instance0%1%10%100%0.95 0.96 0.97 0.98 0.99 1.00Test AccuracyTrainingDataUsedSequentialSampling from 5MSampling from 10MSampling from 100MFigure 4.
Active Learning with Large Corporamanually labeled.We used the na?ve Bayes classifier,creating 10 classifiers each trained on bagsgenerated from an initial one million words oflabeled training data.
We present the activelearning algorithm we used below.Initialize: Training data consists of X wordscorrectly labeledIterate :1) Generate a committee of classifiers usingbagging on the training set2) Run the committee on unlabeled portion ofthe training set3) Choose M instances from the unlabeled setfor labeling - pick the M/2 with the greatestvote entropy and then pick another M/2randomly ?
and add to training setWe initially tried selecting the M mostuncertain examples, but this resulted in a sampletoo biased toward the difficult instances.Instead we pick half of our samples forannotation randomly and the other half fromthose whose labels we are most uncertain of, asjudged by the entropy of the votes assigned tothe instance by the committee.
This is, in effect,biasing our sample toward instances theclassifiers are most uncertain of.We show the results from sampleselection for confusion set disambiguation inFigure 4.
The line labeled "sequential" showstest set accuracy achieved for differentpercentages of the one billion word training set,where training instances are taken at random.We ran three active learning experiments,increasing the size of the total unlabeled trainingcorpus from which we can pick samples to beannotated.
In all three cases, sample selectionoutperforms sequential sampling.
At theendpoint of each training run in the graph, thesame number of samples has been annotated fortraining.
However, we see that the larger thepool of candidate instances for annotation is, thebetter the resulting accuracy.
By increasing thepool of unlabeled training instances for activelearning, we can improve accuracy with only afixed additional annotation cost.
Thus it ispossible to benefit from the availability ofextremely large corpora without incurring thefull costs of annotation, training time, andrepresentation size.5.2 Weakly Supervised LearningWhile the previous section shows that we canbenefit from substantially larger training corporawithout needing significant additional manualannotation, it would be ideal if we couldimprove classification accuracy using only ourseed annotated corpus and the large unlabeledcorpus, without requiring any additional handlabeling.
In this section we turn to unsupervisedlearning in an attempt to achieve this goal.Numerous approaches have been explored forexploiting situations where some amount ofannotated data is available  and a much largeramount of data exists unannotated, e.g.Marialdo's HMM part-of-speech tagger training(1994), Charniak's parser retraining experiment(1996), Yarowsky's seeds for word sensedisambiguation (1995) and Nigam et als (1998)topic classifier learned in part from unlabelleddocuments.
A nice discussion of this generalproblem can be found in Mitchell (1999).The question we want to answer iswhether there is something to be gained bycombining unsupervised and supervised learningwhen we scale up both the seed corpus and theunlabeled corpus significantly.
We can againuse a committee of bagged classifiers, this timefor unsupervised learning.
Whereas with activelearning we want to choose the most uncertaininstances for human annotation, withunsupervised learning we want to choose theinstances that have the highest probability ofbeing correct for automatic labeling andinclusion in our labeled training data.In Table 2, we show the test setaccuracy (averaged over the four mostfrequently occurring confusion pairs) as afunction of the number of classifiers that agreeupon the label of an instance.
For thisexperiment, we trained a collection of 10 na?veBayes classifiers, using bagging on a 1-million-word seed corpus.
As can be seen, the greaterthe classifier agreement, the more likely it is thata test sample has been correctly labeled.ClassifiersIn AgreementTestAccuracy10 0.87349 0.68928 0.62867 0.60276 0.54975 0.5000Table 2.
Committee Agreement vs. AccuracySince the instances in which all bags agree havethe highest probability of being correct, weattempted to automatically grow our labeledtraining set using the 1-million-word labeledseed corpus along with the collection of na?veBayes classifiers described above.
All instancesfrom the remainder of the corpus on which all10 classifiers agreed were selected, trusting theagreed-upon label.
The classif iers were thenretrained using the labeled seed corpus plus thenew training material collected automaticallyduring the previous step.In Table 3 we show the results fromthese unsupervised learning experiments for twoconfusion sets.
In both cases we gain fromunsupervised training compared to using onlythe seed corpus, but only up to a point.
At thispoint, test set accuracy begins to decline asadditional training instances are automaticallyharvested.
We are able to attain improvementsin accuracy for free using unsupervised learning,but unlike our learning curve experiments usingcorrectly labeled data, accuracy does notcontinue to improve with additional data.
{then, than} {among, between}TestAccuracy% TotalTraining DataTestAccuracy% TotalTraining Data106-wd labeled seed corpus 0.9624 0.1 0.8183 0.1seed+5x106 wds, unsupervised 0.9588 0.6 0.8313 0.5seed+107 wds, unsupervised 0.9620 1.2 0.8335 1.0seed+108 wds, unsupervised 0.9715 12.2 0.8270 9.2seed+5x108 wds, unsupervised 0.9588 61.1 0.8248 42.9109 wds, supervised 0.9878 100 0.9021 100Table 3.
Committee-Based Unsupervised LearningCharniak (1996) ran an experiment inwhich he trained a parser on one million wordsof parsed data, ran the parser over an additional30 million words, and used the resulting parsesto reestimate model probabilities.
Doing sogave a small improvement over just using themanually parsed data.
We repeated thisexperiment with our data, and show  theoutcome in Table 4.
Choosing only the labeledinstances most likely to be correct as judged bya committee of classifiers results in higheraccuracy than using all instances classified by amodel trained with the labeled seed corpus.Unsupervised:All LabelsUnsupervised:Most Certain Labels{then, than}107  words 0.9524 0.9620108  words 0.9588 0.97155x108 words 0.7604 0.9588{among, between}107  words 0.8259 0.8335108  words 0.8259 0.82705x108 words 0.5321 0.8248Table 4.
Comparison of Unsupervised LearningMethodsIn applying unsupervised learning toimprove upon a seed-trained method, weconsistently saw an improvement inperformance followed by a decline.
This islikely due to eventually having reached a pointwhere the gains from additional training data areoffset by the sample bias in mining theseinstances.
It may be possible to combine activelearning with unsupervised learning as a way toreduce this sample bias and gain the benefits ofboth approaches.6 ConclusionsIn this paper, we have looked into what happenswhen we begin to take advantage of the largeamounts of text that are now readily available.We have shown that for a prototypical naturallanguage classification task,  the performance oflearners can benefit significantly from muchlarger training sets.
We have also shown thatboth active learning and unsupervised learningcan be used to attain at least some of theadvantage that comes with additional trainingdata, while minimizing the cost of additionalhuman annotation.
We propose that a logicalnext step for the research community would beto direct efforts towards increasing the size ofannotated training collections, whiledeemphasizing the focus on comparing differentlearning techniques trained only on smalltraining corpora.
While it is encouraging thatthere is a vast amount of on-line text, muchwork remains to be done if we are to learn howbest to exploit this resource to improve naturallanguage processing.ReferencesBanko, M. and Brill, E. (2001).
Mitigating thePaucity of Data Problem.
Human LanguageTechnology.Breiman L., (1996).
Bagging Predictors, MachineLearning 24 123-140.Brill, E. and Wu, J.
(1998).
Classifier combinationfor improved lexical disambiguation.
InProceedings of the 17th International Conferenceon Computational Linguistics.Charniak, E. (1996).
Treebank Grammars ,Proceedings AAAI-96 , Menlo Park, Ca.Dagan, I. and Engelson, S. (1995).
Committee-basedsampling for training probabilistic classifiers.
InProc.
ML-95, the 12th Int.
Conf.
on MachineLearning.Gale, W. A., Church, K. W., and Yarowsky, D.(1993).
A method for disambiguating word sensesin a large corpus.
Computers and the Humanities,26:415--439.Golding, A. R. (1995).
A Bayesian hybrid method forcontext-sensitive spelling correction.
In Proc.
3rdWorkshop on Very Large Corpora, Boston, MA.Golding, A. R. and Roth, D.(1999),  A Winnow-Based Approach to Context-Sensitive SpellingCorrection.
Machine Learning, 34:107--130.Golding, A. R. and Schabes, Y.
(1996).
Combiningtrigram-based and feature-based methods forcontext-sensitive spelling correction.
In Proc.
34thAnnual Meeting of the Association forComputational Linguistics, Santa Cruz, CA.Henderson, J. C. and Brill, E. (1999).
Exploitingdiversity in natural language processing:combining parsers.
In 1999 Joint SigdatConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora.ACL, New Brunswick NJ.
187-194.Jones, M. P. and Martin, J. H. (1997).
Contextualspelling correction using latent semantic analysis.In Proc.
5th Conference on Applied NaturalLanguage Processing, Washington, DC.Lewis , D. D., & Catlett, J.
(1994).
Heterogeneousuncertainty sampling.
Proceedings of the EleventhInternational Conference on Machine Learning(pp.
148--156).
New Brunswick, NJ: MorganKaufmann.Mangu, L. and Brill, E. (1997).
Automatic ruleacquisition for spelling correction.
In Proc.
14thInternational Conference on Machine Learning.Morgan Kaufmann.Merialdo, B.
(1994).
Tagging English text with aprobabilistic model.
Computational Linguistics,20(2):155--172.Mitchell, T. M. (1999), The role of unlabeled data insupervised learning , in Proceedings of the SixthInternational Colloquium on Cognitive Science,San Sebastian, Spain.Nigam, N., McCallum, A., Thrun, S., and  Mitchell,T.
(1998).
Learning to classify text from labeledand unlabeled documents.
In Proceedings of theFifteenth National Conference on ArtificialIntelligence.
AAAI Press..Pedersen, T. (2000).
A simple approach to buildingensembles of naive bayesian classifiers for wordsense disambiguation.
In Proceedings of the FirstMeeting of the North American Chapter of theAssociation for Computational Linguistics May 1-3, 2000, Seattle, WAPowers, D. (1997).
Learning and application ofdifferential grammars.
In Proc.
Meeting of theACL Special Interest Group in Natural LanguageLearning, Madrid.van Halteren, H. Zavrel, J. and Daelemans, W.(1998).
Improving data driven wordclass taggingby system combination.
In COLING-ACL'98,pages 491497, Montreal, Canada.Weng, F., Stolcke, A, & Sankar, A (1998).
Efficientlattice representation and generation .
Proc.
Intl.Conf.
on Spoken Language Processing, vol.
6, pp.2531-2534.
Sydney, Australia.Yarowsky, D. (1994).
Decision lists for lexicalambiguity resolution: Application to accentrestoration in Spanish and French .
In Proc.
32ndAnnual Meeting of the Association forComputational Linguistics, Las Cruces, NM.Yarowsky, D. (1995) Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics.Cambridge, MA, pp.
189-196, 1995.
