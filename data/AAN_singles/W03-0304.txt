Statistical Translation Alignmentwith Compositionality ConstraintsMichel Simard and Philippe LanglaisLaboratoire de recherche applique?e en linguistique informatique (RALI)De?partement d?informatique et de recherche ope?rationnelleUniversite?
de Montre?alC.P.
6128, succursale Centre-ville, Local 2241Montre?al (Que?bec), Canada H3C 3J7{simardm,felipe}@iro.umontreal.caAbstractThis article presents a method for aligningwords between translations, that imposes acompositionality constraint on alignments pro-duced with statistical translation models.
Ex-periments conducted within the WPT-03 sharedtask on word alignment demonstrate the effec-tiveness of the proposed approach.1 IntroductionSince the pioneering work of the IBM machine trans-lation team almost 15 years ago (Brown et al, 1990),statistical methods have proven to be valuable tools inapproaching the automation of translation.
Word align-ments (WA) play a central role in the statistical modelingprocess, and reliable WA techniques are crucial in acquir-ing the parameters of the models (Och and Ney, 2000).Yet, the very nature of these alignments, as defined inthe IBM modeling approach (Brown et al, 1993), leadto descriptions of the correspondences between source-language (SL) and target-language (TL) words of a trans-lation that are often unsatisfactory, at least from a humanperspective.One notion that is typically evacuated in the statisti-cal modeling process is that of compositionality: a fun-damental assumption in statistical machine translation isthat, ultimately, all the words of a SL segment S con-tribute to produce all the words of its TL translation T , atleast to some degree.
While this makes perfect sense froma stochastic point of view, it contrasts with the hypothesisat the basis of most (if not all) other MT approaches, aswell as with our natural intuitions about translation: thatindividual portions of the SL text produce individual TLportions autonomously, and that the final translation T isobtained by somehow piecing together these TL portions.In what follows, we show how re-integrating compo-sitionality into the statistical translation word alignmentprocess leads to better alignments.
We first take a closerlook at the ?standard?
statistical WA techniques in section2, and then propose a way of imposing a compositional-ity constraint on these techniques in section 3.
In section4, we discuss various implementation issues, and finallypresent the experimental results of this approach on theWPT-03 shared task on WA in section 5.2 Statistical Word AlignmentBrown et al (1993) define a word alignment as a vec-tor a = a1...am that connects each word of a source-language text S = s1...sm to a target-language word inits translation T = t1...tn, with the interpretation thatword taj is the translation of word sj in S (aj = 0 isused to denote words of s that do not produce anything inT ).The Viterbi alignment between source and target sen-tences S and T is defined as the alignment a?
whose prob-ability is maximal under some translation model:a?
= argmaxa?APrM(a|S, T )where A is the set of all possible alignments between Sand T , and PrM(a|S, T ) is the estimate of a?s probabil-ity under model M, which we denote Pr(a|S, T ) fromhereon.
In general, the size of A grows exponentiallywith the sizes of S and T , and so there is no efficient wayof computing a?
efficiently.
However, under the indepen-dence hypotheses of IBM Model 2, the Viterbi alignmentcan be obtained by simply picking for each position i inS, the alignment that maximizes t(si|tj)a(j, i,m, n), theproduct of the model?s ?lexical?
and ?alignment?
proba-bility estimates.
This procedure can trivially be carriedout in O(mn) operations.
Because of this convenientproperty, we take the Viterbi-2 WA method (which welater refer to as the V method) as the basis for the rest ofthis work.3 CompositionalityIn IBM-style alignments, each SL token is connected to asingle (possibly null) TL token, typically the TL tokenwith which it has the most ?lexical affinities?, regard-less of other existing connections in the alignment and,more importantly, of the relationships it holds with otherSL tokens in its vicinity.
In practice, this means thatsome TL tokens can end up being connected to severalSL tokens, while other TL tokens are left unconnected.This contrasts with alternative alignment models such asthose of Melamed (1998) and Wu (1997), which impose a?one-to-one?
constraint on alignments.
Such a constraintevokes the notion of compositionality in translation: itsuggests that each SL token operates independently in theSL sentence to produce a single TL token in the TL sen-tence, which then depends on no other SL token.This view is, of course, extreme, and real-life transla-tions are full of examples that show how this composi-tionality principle breaks down as we approach the levelof word correspondences.
Yet, if we can find a way ofimposing compositionality constraints on WA?s, at leastto the level where it applies, then we should obtain moresensible results than with Viterbi alignments.For instance, consider a procedure that splits both theSL and TL sentences S and T into two independent parts,in such a way as to maximise the probability of the tworesulting Viterbi alignments:argmax?i,j,d???????
?d = 1 : Pr(a1|si1, tj1)?Pr(a2|smi+1, tnj+1)d = ?1 : Pr(a1|si1, tnj+1)?Pr(a2|smi+1, tj1)(1)In the triple ?i, j, d?
above, i represents a ?split point?in the SL sentence S, j is the analog for TL sentence T ,and d is the ?direction of correspondence?
: d = 1 denotesa ?parallel correspondence?, i.e.
s1...si corresponds tot1...tj and si+1...sm corresponds to tj+1...tn; d = ?1denotes a ?crossing correspondence?, i.e.
s1...si corre-sponds to tj+1...tn and si+1...sm corresponds to t1...tj .The triple ?I, J,D?
produced by this procedure refersto the most probable alignment between S and T , un-der the hypothesis that both sentences are made up oftwo independent parts (s1...sI and sI+1...sm on the onehand, t1...tJ and tJ+1...tn on the other), that correspondto each other two-by-two, following direction D. Suchan alignment suggests that translation T was obtainedby ?composing?
the translation of s1...sI with that ofsI+1...sm.In the above procedure, these ?composing parts?
ofS and T are further assumed to be contiguous sub-sequences of words.
Once again, real-life translations arefull of examples that contradict this (negations in Frenchand particle verbs in German are two examples that im-mediately spring to mind when aligning with English).Yet, this contiguity assumption turns out to be very con-venient, because examining pairings of non-contiguoussequences would quickly become intractable.
In con-trast, the procedure above can find the optimal partitionin polynomial time.The ?splitting?
process described above can be re-peated recursively on each pair of matching segments,down to the point where the SL segment contains a sin-gle token.
(TL segments can always be split, even whenempty, because IBM-style alignments allow connectingSL tokens to the ?null?
TL token, which is always avail-able.)
This recursive procedure actually produces twodifferent outputs:1.
A parallel partition of S and T into m pairs of seg-ments ?si, tkj ?, where each tkj is a (possibly null)contiguous sub-sequence of T ; this partition can ofcourse be viewed as an alignment on the words of Sand T .2. an IBM-style alignment, such that each SL and TLtoken is linked to at most one token in the other lan-guage: this alignment is actually the concatenationof individual Viterbi alignments on the ?si, tkj ?
pairs,which connects each si to (at most) one of the tokensin the corresponding tkj .In this procedure, which we call Compositional WA (orC for short), there are at least two problems.
First, eachSL token finds itself ?isolated?
in its own partition bin,which makes it impossible to account for multiple SL to-kens acting together to produce a TL sequence.
Second,the TL tokens that are not connected in the resulting IBM-style alignment do not play any role in the computationof the probability of the optimal alignment; therefore, thepair ?si, tkj ?
in which these ?superfluous?
tokens end upis more or less random.To compensate in part for these, we propose usingtwo IBM-2 models to compute the optimal partition: the?forward?
(SL?TL) model, and the ?reverse?
(TL?SL)model.
When examining a particular split ?i, j, d?
for Sand T , we compute both Viterbi alignments, forward andreverse, between all pairs of segments, and score eachpair with the product of the two alignments?
probabili-ties.In this variant, which we call Combined CompositionalWA (CC), we can no longer allow ?empty?
segments inthe TL, and so we stop the recursion as soon as either theSL or TL segment contains a single token.
The resultingpartition therefore consists in a series of 1-to-k or k-to-1alignments, with k ?
1.4 ImplementationThe C and CC WA methods of section 3 were imple-mented in a program called ralign (Recursive ?
or RALI?
alignment, as you wish).
As suggested above, this pro-gram takes as input a pair of sentence-aligned texts, andthe parameters of two IBM-2 models (forward and re-verse), and outputs WA?s for the given texts.
This pro-gram also implements plain Viterbi alignments, using theforward (V) or reverse (RV) models, as well as what wecall the Reverse compositional WA (or RC), which is justthe C method using the reverse IBM-2 model.The output format proposed for the WPT-03 sharedtask on WA allowed participants to distinguish between?sure?
(S) and ?probable?
(P) WA?s.
We figured that ouralignment procedure implicitly incorporated a way of dis-tinguishing between the two: within each produced pairof segments, we marked as ?sure?
all WA?s that were pre-dicted by both (forward and reverse) Viterbi alignments,and as ?probable?
all the others.The translation models for ralign were trained usingthe programs of the EGYPT statistical translation toolkit(Al-Onaizan et al, 1999).
This training was done usingthe data provided as part of the WPT-03 shared task onWA (table 1).
We thus produced two sets of models, onefor English and French (en-fr), and one for Romanianand English (ro-en).
All models were trained on both thetraining and test datasets1.
For en-fr, we considered allwords that appeared only once in the corpus to be ?un-known words?
(whittle option -f 2), so as to obtain de-fault values of ?real?
unknowns in the test corpus2.
In thecase of ro-en, there was too little training data for this tobe beneficial, and so we chose to use all words.English-Frenchcorpus tokens (SL/TL) sentence pairstraining 20M/24M 1Mtrial 772/832 37test 8K/9K 447Romanian-Englishcorpus tokens (SL/TL) sentence pairstraining 1M/1M 48Ktrial 513/547 17test 6K/6K 248Table 1: WPT-03 shared task resourcesWe trained and tested a number of translation mod-els before settling for this particular setup.
All of these1No cheating here: the test dataset did not contain referencealignments2This is necessary, even when training on the test corpus,because the EGYPT toolkit?s training program (GIZA) ignoresexcessively long sentences in the corpus.tests were performed using the trial data provided for theWPT-03 shared task.5 Experimental ResultsThe different word-alignment methods described in sec-tions 2 and 3 were run on the test corpora of the WPT-03 shared task on alignment.
Results were evaluated interms of alignment precision (P), recall (R), F-measureand alignment error rate (AER) (Och and Ney, 2000).
Asspecified in the shared task description, all of these met-rics were computed taking null-alignments into account(i.e.
tokens left unconnected in an alignment were actu-ally counted as aligned to virtual word token ?0?).
Theresults of our experiments are reproduced in table 2.We observe that imposing a ?contiguous composition-ality?
constraint (C and RC methods) allows for sub-stantial gains with regard to plain Viterbi alignments (Vand RV respectively), especially in terms of precisionand AER (a slight decline in recall can be observed be-tween the V and C methods on the ro-en corpus, but itis not clear whether this is significant).
These gains areeven more interesting when one considers that all pairs ofalignments (V and C, RV and RC) are obtained using ex-actly the same data.
This highlights both the deficienciesof IBM Model-2 and the importance of compositionality.Using both the forward and reverse models (CC) yieldsyet more gains with regard to all metrics.
This result isinteresting, because it shows the potential of the compo-sitional alignment method for integrating various sourcesof information.With regard to language pairs, it is interesting to notethat all alignment methods produce figures that are sub-stantially better in recall and worse in precision on the ro-en data, compared to en-fr.
Overall, ro-en alignments dis-play significantly higher F-measures.
This is surprising,considering that the provided en-fr corpus contained 20times more training material.
This phenomenon is likelydue to the fact that the en-fr test reference contains muchmore alignments per word (1.98 per target word) than thero-en (1.12).
All alignment methods described here pro-duce roughly between 1 and 1.25 alignments per targetwords.
This fact affects recall and F-measure figures pos-itively on the ro-en test, while precision and AER (whichcorrelates strongly with precision in practice) are affectedinversely.6 ConclusionIn this article, we showed how a compositionality con-straint could be imposed when computing word align-ments with IBM Models-2.
Our experiments on the WPT-03 shared task on WA demonstrated how this improvesthe quality of resulting alignments, when compared tostandard Viterbi alignments.
Our results also highlightEnglish-French Romanian-Englishmethod P R F AERV 0.6610 0.3387 0.4479 0.2700RV 0.6260 0.3212 0.4245 0.2944C 0.7248 0.3534 0.4751 0.2318RC 0.7422 0.3586 0.4835 0.2152CC 0.7756 0.3681 0.4992 0.1850method P R F AERV 0.5509 0.5442 0.5475 0.4524RV 0.5409 0.5375 0.5391 0.4608C 0.5818 0.5394 0.5597 0.4402RC 0.5865 0.5415 0.5630 0.4369CC 0.6361 0.5714 0.6020 0.3980Table 2: Alignment resultsthe benefit of using both forward and reverse translationmodels for this task.One of the weaknesses of the proposed method is theinability to produce many-to-many alignments.
To allowfor such alignments, it would be necessary to establish a?stopping condition?
on the recursion process, so as toprevent partitioning pairs of segments that display ?non-compositional?
phenomena in both SL and TL languages.We have begun experimenting with various such mecha-nisms.
One of these is to stop the recursion as soon as thepair of segments under consideration contains less thantwo ?sure?
alignments, i.e.
connections predicted by boththe forward and reverse models.
Another possibility is toestablish a threshold on the probability ?drop?
incurredby the optimal split on any given pair of segments.
Sofar, these experiments are inconclusive.Another problem is with ?null?
alignments, which theprogram is also unable to account for.
Currently, omis-sions and insertions in translation find themselves incor-porated into aligned segments.
A simple way to deal withthis problem would be to exclude from the final alignmentlinks that are not predicted by either the forward or re-verse Viterbi alignments.
But early experiments with thisapproach are unconvincing, and more elaborate filteringmechanisms will probably be necessary.Finally, IBM Model 2 is certainly not the state of theart in statistical translation modeling.
Thenagain, themethods proposed here are not dependent on the underly-ing translation model, and similar WA methods could bebased on more elaborate models, such as Models 3?5, orthe HMM-based models proposed by Och et al (1999)for example.
On the other hand, our compositional align-ment method could be used during the training processof higher-level models.
Whether this would lead to betterestimates of the models?
parameters remains to be seen,but it is certainly a direction worth exploring.References[Al-Onaizan et al1999] Yaser Al-Onaizan, Jan Curin,Michael Jahr, Kevin Knight, John Lafferty, DanMelamed, Franz-Josef Och, David Purdy, Noah H.Smith, and David Yarowsky.
1999.
Statistical Ma-chine Translation - Final Report, JHU Workshop 1999.Technical report, Johns Hopkins University.
[Brown et al1990] Peter F. Brown, John Cocke, StephenA.
Della Pietra, Vincent J. Della Pietra, Fredrick Je-linek, John D. Lafferty, Robert L. Mercer, and Paul S.Roossin.
1990.
A Statistical Approach to MachineTranslation.
Computational Linguistics, 16(2):79?85,June.
[Brown et al1993] Peter F. Brown, Stephen A. DellaPietra, Vincent J. Della Pietra, and Robert L. Mer-cer.
1993.
The Mathematics of Machine Transla-tion: Parameter Estimation.
Computational Linguis-tics, 19(2):263?311.
[Melamed1998] I. Dan Melamed.
1998.
Word-to-WordModels of Translational Equivalence.
Technical Re-port 98-08, Dept.
of Computer and Information Sci-ence, University of Pennsylvania, Philadelphia, USA.
[Och and Ney2000] Franz Josef Och and Hermann Ney.2000.
Improved statistical alignment models.
In Pro-ceedings of the 38th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 440?447,Hong-Kong, China, October.
[Och et al1999] Franz Josef Och, Christoph Tillmann,and Hermann Ney.
1999.
Improved Alignment Mod-els for Statistical Machine Translation.
In Proceedingsof the 4th Conference on Empirical Methods in Natu-ral Language Processing (EMNLP)and 7th ACL Work-shop on Very Large Corpora (WVLC), pages 20?28,College Park, USA.
[Wu1997] Dekai Wu.
1997.
Stochastic Inversion Trans-duction Grammars and Bilingual Parsing of ParallelCorpora.
Computational Linguistics, 23(3):377?404,September.
