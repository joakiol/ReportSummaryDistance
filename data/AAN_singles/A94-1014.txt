Acquir ing Knowledge From Encyc loped ic  Texts*Fernando GomezDept.
of Computer ScienceUniv.
of Central FloridaOrlando, FL 32816, USAgomez~cs ,  uc f .
eduRichard HullDept.
of Computer ScienceUniv.
of Central FloridaOrlando, FL 32816, USAhull@as, ucf.
eduCarlos SegamiMath.
and Comp.
Sci.
Dept.Barry UniversityMiami Shores, FL 33161, USAsegami~buvax, barry, eduAbstractA computational model for the acquisitionof knowledge from encyclopedic texts is de-scribed.
The model has been implementedin a program, called SNOWY, that readsunedited texts from The World Book En-cyclopedia, and acquires new concepts andconceptual relations about topics dealingwith the dietary habits of animals, theirclassifications and habitats.
The programis also able to answer an ample set of ques-tions about the knowledge that it has ac-quired.
This paper describes the essentialcomponents of this model, namely seman-tic interpretation, inferences and represen-tation, and ends with an evaluation of theperformance of the program, a sample ofthe questions that it is able to answer, andits relation to other programs of similar na-ture.1 IntroductionWe present an approach to the acquisition of knowl-edge from encyclopedic texts.
The goal of this re-search is to build a knowledge base about a giventopic by reading an encyclopedic article.
Expert sys-tems could use this database to tap in for piecesof knowledge, or a user could directly query thedatabase for specific answers.
Then, two possibleapplications could be derived from our research: (a)the automatic onstruction of databases from ency-clopedic texts for problem-solvers and (b) queryingan encyclopedia n natural anguage.
The idea is tobuild a database from an encyclopedic text on thefly.
Then, if a user asks the question, say, Whichbears eat seals?
the system would reply by sayingsomething like "I don't know.
But, wait a minute, Iam going to read this article and let you know."
Inthe process of reading the article, the system buildsa small knowledge base about bears and calls the*This research is being funded by NASA-KSC Con-tract NAG-10-0120question-answering system to answer any questionposed by an expert system or a human.
The long-term goal of our research is to read an entire ar-ticle on, say bears, and to build a knowledge baseabout bears.
Since this goal requires dealing withan extraordinary number of research issues, we haveconcentrated on a series of topics about animals, in-cluding diet, habitat, and classification.
A skimmerscans the article for sentences relevant to a giventopic and passes these sentences to the understand-ing system, called SNOWY, for complete parsing, in-terpretation, concept formation, concept recognitionand integration in long-term memory (LTM).
But,if, in the process of learning about the dietary habitsof, say beetles, the program is told that they culti-vate fungi, and the program is able to interpret hatsentence, that knowledge will also be integrated inLTM.
Consequently, our approach to understandingexpository texts is a bottom-up approach in whichfinal knowledge representation structures are builtfrom the logical form of the sentences, without inter-vening scriptal or frame knowledge about the topic.Hence, our system does not start with a frame con-taining the main slots to be filled for a topic, say"diet," as in recent MUC projects (Sundheim, 1992),but rather it will build everything relevant o dietfrom the output of the interpretation phase.
Then,when we are talking about the topic, it will not makeany difference if the sentences refer to what the an-imals eat, or what eats them.
Every aspect dealingwith the general idea of ingest can be analyzed andproperly integrated into memory.
Our corpora fortesting our ideas has been The World Book Ency-clopedia (World Book, 1987), which is one or twolevels less complex than the Collier's Encyclopedia,which, in turn, is less complex than the Encyclopae-dia Britannica.2 InterpretationIn order for the integration component to integratea concept in LTM, a successful parse and interpreta-tion needs to be produced for a sentence or at leastfor one of its clauses.
The input to the interpre-84(ACTION (INGEST(is-a (relathm)) (is-a (action))(subj (thing (actor))) (obj (thing (theme))) (subj (animate (acte0))(adverb (prep(time (at-time)) (with (Itm-ctgy(nesati~ (negation)) ( frequef:cy (frequeocy)) (uteri\[ (ingt~mem (s~ron~)))..... (human (accompany (strong)))(prep (physical-thing(in (Itm-ctgy (physical-thlng (at-loc (weak))) (c~ther~ (weak))))))(time-unit (at-time (strong))))) (obj (J~ysical -thing (theme))}(at ( ltm~gy (physical-thing (at-loc (strong)))(time-unit (at-time (strong))))) (DRINK(during (Itm-ctgy (physical-thing (at-time (strong))))) (is-a (inge.~t))(with (ltm-ctgy (animate.body-part (instrument (s rong)))) (obj (liquid (theme))))(state (state (strong)))))Figure 1: Organization of the Verbal Concepts(REPS ((VRS (G233)))SUBJ ((PARSER ((DFART THE) (VERB CROWNED) (NOUN EAGLE)))(REE (DEFINITE)) (PLURAL NIL)(INTERP (CROWNED-EAGLE (Q (ALL)))) (SEMANTIC-ROLE (ACTOR)))PREP ((PARSE (OF ((PN AFRICA)))) (INTERP (AFRICA (Q (CONSTANT))))(ATTACH-TO (CROWNED-EAGLE (LOCATION-R (AFRICA)))))VERB ((MAIN-VERB EAT EATS) (TENSE PRES)(NUM SING) (PRIM (IY4GEST)))OBJ ((PARSE ((NOUN MONKEYS))) (PLURAL T)) (INTERP (MONKEY (Q (?
))))(SEMANTIC-ROLE (THEME))))output for Structure G233:(SUBJ ((PARSE ((DFART THE) (VERB CROWNED) (NOUN EAGLE)))(REP (DEFINITE)) (PLURAL NIL)(INTERP (CROWNED-EAGLE (Q (ALL)))) (SEMANTIC-ROLE (ACTOR)))VERB ((MAIN-VERB LIVE LIVES) (TEMSE PRES) (NUM SING) (PRIM (I\]*4HABIT)))PREP ((PARSE (IN ((DFART THE) (NOUN RAIN) (NOUN FOREST))))(REF (DEFI~TITE)) (PLURAL NIL)(INTERP (RAIN-FOREST)) (SEMANTIC-ROLE (AT-LOC))(ATtACH-TO (VERB (STRONGLY))))tation phase is built by a top-down, lexical-drivenparser(Gomez, 1989), which parses the sentences di-rectly into syntactic cases.
Prepositional phrasesare left unattached inside the structure built by theparser.
It is up to the interpreter to attach them,identify their meaning and the thematic roles thatthey may stand for.
The parser is a deterministicmachine, containing mechanisms for minimizing theneed for backing up.
The average amount of timein parsing a sentence from the encyclopedia s aboutone second.
The parser has presently a lexicon ofabout 35,000 words.
The rate of success of produc-ing a full and correct parse of a sentence is, as ofthis writing, 76.8% for this Encyclopedia (see belowfor a detailed discussion of test results and machineused).
The parser begins the parsing of a sentenceon a syntactic basis until the meaning of the verbis recognized.
The rules that determine the mean-ing of the verbs, called VM rules, are classified assubj-rules, verb-rules, obj-rules, io-rules, pred-rules,prep-rules, and end-of-clause-rules.
These rules areactivated when the verb, a syntactic ase or a prepo-sitional phrase has been parsed, or when the end ofthe clause has been reached, respectively.
In mostcases, the antecedents of these rules contain selec-tional restrictions which determine whether the in-terpretation of the syntactic constituent is a sub-class of some concept in SNOWY's LTM ontology.If during an examination of LTM the selectional re-striction is passed, the consequent(s) of the VM ruleestablish the proper meaning or verbal concept forthe verb.
If no rules fire, the parser inserts the syn-tactic case or prepositional phrase in the structurebeing built and continues parsing.
These rules, ofwhich there are just over 300, incorporate a con-siderable degree of ambiguity procrastination (Rich,1987).
For instance, rather than writing an obj-rulefor determining the meaning of "take" saying: IfLTM(obj) is-a medicament then meaning-of take isingest, which will immediately jump to determinethe meaning of "take" in Peter took an aspirin whenthe obj is parsed, it is better to write that rule as anend-of-clause rule.
This will avoid making the wrongassumption about the meaning of the verb in Petertook an aspirin to Mary.Figure 2: Output of the parser and interpreter forthe sentence The crowned eagle of Africa lives in therain forests and eats monkeys.When the verbal concept has been identified, thesyntactic cases and prepositional phrases alreadyidentified by the parser and any subsequent con-stituents are interpreted by matching them againstthe representation f the verbal concept.
Verbal con-cepts are not reduced to a small set of primitives; onthe contrary they are organized into a classificationhierarchy containing the most general actions nearthe root node and the most specific at the bottom.Figure 1 contains simplified examples of the rootnode action, the subconcept ingest, and one sub-concept of ingest, drink.
The entry subj in the nodeingest means that the actor of ingest is an animate,and that this is indicated syntactically by the casesubj.
The entries for the preposition "with" meanthat if the object of the PP is a utensil, then thecase expressed by the preposition is the instrumentcase; if the object of the preposition is a human,then the case is the accompany case, etc.
The en-tries strong and weak indicate whether the verbalconcept claims that preposition strongly or weakly,respectively.
This information is used by the inter-preter to attach PPs.
The drink node has only oneentry for the case theme.
The others are inheritedfrom the nodes, ingest and action.
If the contextto be understood requires a more detailed under-standing of how animals drink, distinct from how hu-mans drink, then the concept drink may be split intothe concepts human-drink and animal-drink.
Con-sequently, the specification of the hierarchy dependson the domain knowledge to be acquired.
The algo-rithm that matches yntactic eases or prepositionalphrases to the representation of the verbal conceptsearches the hierarchy in a bottom-up fashion.
Thesearch ends with success or failure, if the LTM entryin the verbal concept is true or false, respectively.Hence, entries in subconcepts override the same en-tries in superconcepts.
The node action provides anexcellent way to handle situations not contemplatedin subconcepts by defaulting them to those in the85action root node.Figure 2 depicts the output of the interpreter.The slot subject contains the output of the parsermarked by the slot PARSE, the interpretation ofthe noun phrase marked by INTERP, and the the-matic role marked by SEMANTIC-ROLE.
The entryQ within the INTERP slot indicates the quantifierfor that case.
The entries for the slot PREP, contain,in addition to the PARSE slot, the slot ATTACH-TO indicating which concept in the parse structureto attach that constituent to, and the meaning ofthe preposition indicated, in this case, by the sub-slot LOCATION-R.
The quantifier for "monkey" isa question mark, because its value is not indicatedin the text.
In (Gomez et al, 1992), the reader mayfind a detailed iscussion of the interpretation issuespresented in this session.3 InferencesIf the knowledge about dietary habits of animalswere indicated in the texts by using "eat" and itscognates, the task of acquiring this knowledge wouldbe rather simple.
But, the fact is that an encyclo-pedia article may refer to the dietary habits in avariety of manners.
Figure 3 contains a hierarchyabout the diet topic.
The verbs that trigger theseverbal concepts are indicated by writing \[verb\].
Theverb "dig out" triggers the action dig-r. Those ver-bal concepts from which an ingest relation is inferredare indicated by writing an asterisk by its side.
Therepresentation f dig-r is:(dlg-r ( i s -a  (act ion) )(subj (animal (actor)) (machine (actor)))(obj  (phys ica l - th ing  ( theme)) )(addition-roles ( f i re -one((and (if% is-a actor an imal )(ife'/o is-a theme anin~at?
))(add (((actor (actot)) (pr ( ingest))( theme ( theme)) ) ) ) ) ) ) )The representation f dig-r is similar to the previ-ous verbal concepts, except for the entry that saysaddition-rule.
This is an inference rule saying that ifthe actor of this action is an animal and the themeis an animate, then add to LTM the relation say-ing that the actor ingests the theme.
This rule per-mits the acquisition of the fact that bears eat squir-rels and mice from the sentence A grizzly has long,curved claws that it uses chiefly to dig out groundsquirrels and mice.
Similar inference rules are storedin the other actions marked with an asterisk.
Then,if the system reads the sentence Bears are fond ofhoney, it will infer that they eat honey.
The infer-ence rules are also inherited from the nodes repre-senting the actions in the hierarchy.
Hence, if theverbal concept animal-fish, shown in Figure 3, wassuggested for the sentence Owls have been knownto fish in shallow creeks, we would inherit an addi-tion rule from the verbal concept animal-hunt whichwould then infer an ingest relation.- action*transport *animal-hum huraan-htmt harm *dig-r ingest\[~k~p\] A \[,l~k\] ~oot\] ~ / ~  \ ]~g~\]  \[,wallow\[/ \ \[search / ~.
*cause- *destroy *damage \]dig-up\[ \[consume\]to-ule \[devour\] / \ !",~'\].
/ \'D~lll tenasel / x, \[nibble\]body-part \]chew\]\]gulp-down\]\[skin\] {gobble-up\]animal-steal animal-captm'e-aulmal {gotgc\]\[steal\] \[trap\] I \[catch\] \[feed-on\]I \]live-on\[ \]take.away\] \[seize\] \]capture\] \]em\] \[take\] \[ingest\] l~h\ ]  *expedence-af fection- foranimal-fish \]ate fond-of\[ \[take-in\]\[fish\] \]like\] \]keep-down\[\]love\[\[prefer\]- -  Solid Lines - 1S-A linksFigure 3: Conceptual Verbs Organizing the TopicThe hierarchy is also helpful in avoiding incorrectinferences.
Sentences discussing humans hunting an-imals do not automatically imply ingest relations,especiMly when an explicit purpose is given which isnot ingest-related.
For example in People hunt somekinds of seals for their soft fur, it is unlikely thatthe people mentioned will eat those seals.
There-fore, we have separate verbal concepts for hunt re-lations where humans are the actors, whose infer-ence rules do not suggest ingest relations, except incases like Eskimos hunt polar bears for food.
Fur-thermore, addition rules typically have constraintswithin their antecedents to prevent inappropriate in-ferences, i.e., we would not want to infer an ingestrelation when processing the sentence Tigers searchfor warm places to sleep during the day, and a con-straint on the theme of "search for" to be at leastanimate rejects the inference.4 Interpreting Noun Phrases andRestrictive ModifiersDetecting classification relations in the text becomesa must, not only if questions of the type Which owlseat fish?, or Which eagles eat hyraxes?
are to beanswered, but also for the acquisition of the knowl-edge in sentences like The prey of polar bears con-sists of seals, or The diet of bears consists of nuts,berries and small rodents.
In order to achieve this,complex noun groups and restrictive modifiers arerepresented by the noun group interpreter as clas-sification hierarchies.
One of the senses of "diet" isrepresented as: Xl  (cf (is-a (food) R1)), where R1 isthe relation ingest with actor = animal, and theme= food.
Then, "the diet of bears" is represented asthe concept X2 (cf (is-a (food) R2)), where R2 isthe relation ingest with actor = bear, and theme =food.
The slot cf contains the necessary and suffi-cient conditions that define the concept X2.
Then,the meaning of X2 is:V(z)(X2(z) ~ Food(z) A R2(z))86The same interpretation is given to restrictive rel-ative clauses.
The phrase "eagles that live in the rainforests" is represented as X3 (cf (is-a (eagle) R3)),where R3 is the relation "live in the rain forests.
"Once these structures are built by the interpreter, aclassifier that is a component of the integration al-gorithm integrates these concepts in the proper po-sition in LTM.
The interpretation of complex nounsproceeds by attempting to determine the meaningof pairs of items in the complex noun, utilizing ascheme that combines the items in the complex nounfrom left to right.
For example, in the interpretationof "big red wine bottle" an attempt is made to finda meaning for the terms "big red, .... big wine," "bigbottle," "red wine," "red bottle" and "wine bottle.
"If one item in the complex noun can be paired (i.e.,a meaning can be found) with more than one otheritem in the complex noun, then the algorithm re-turns more than one interpretation for the complexnoun, and disambiguation routines are activated.
Inour example, a meaning is found for "big bottle,""red wine," "red bottle," and "wine bottle," fromwhich the algorithm returns the two possible inter-pretations:(bottle (size (big)) (color (red)) (pertain-to (wine)))(bottle (size (big)) (pertain-to (wine (color (red)))))Finding the meaning of terms of the form "itemlitem2" reduces to finding a relation that connectsthe concepts corresponding to "iteml" and "item2"in LTM.
Consequently, this algorithm as well as thealgorithm that finds the meaning of PPs and syn-tactic cases depends on a a priori set of conceptsthat constitutes the basic ontology of SNOWY.
AsSNOWY reads, it adds new concepts to this ontol-ogy as explained below.
Its initial ontology consistsof 1243 concepts.In order to find a semantic relation between twopair of items in a NP, the items or any of their su-perconcepts must belong to the a priori ontology.
Ifthe noun group interpreter does not find a seman-tic relation between two items, the algorithm willhyphenate them.
This has been the case for "rainforest," and "crowned eagle" (see Figure 2).
"Rain-forest" is constructed in LTM as a subconcept of"forest."
But, no semantic relation will be builtbetween "rain" and "forest."
However, if the pairof items is "sea mammal," the algorithm builds X3(cf(is-a(mammal) live-in(sea))), because "mammal"and "sea" are categorized in LTM as subconcepts of"animate" and "habitat," respectively.
The algo-rithm will produce the same representation for "seamammal" and "mammals that live in the sea," ex-cept for the names of the concepts, which are dummynames with no meaning.
The recognizer algorithmis able to tell that the two concepts are the sameconcept by examining the content of the cfslot, andactivating a classifier that analyzes the subsumptionrelations between a pair of concepts.
In (Gomez andSegami, 1989), the reader may find a detailed dis-cussion of the recognizer algorithm.
Note that thealgorithm will produce the concept X4 (cf(is-a(lion)live-in(sea))) if the noun group "sea lion" is not inquotation marks or capitalized.5 Final Knowledge Representat ionStructuresThe input of the interpreter is passed to the for-mation phase that builds the final knowledge rep-resentation structures.
These are in turn integratedinto LTM upon activating a recognizer algorithm andan integration algorithm both of which make exten-sive use of a classifier similar to the one reportedin (Schmolze and Lipkis, 1983).
The construction ofthe final knowledge representation structures i doneas follows.
The interpretation phase, if successful,has built a relation, and a set of thematic roles foreach sentence.
Let us call the thematic roles of therelation the entities for that relation.
All the n en-tities of a n-ary relation are represented as objectsin our language, and links are created pointing tothe representation of the relation, which is repre-sented as a separate structure, called an a-structure.Figure 4 depicts the representation produced fromthe interpretation of the sentence The crowned ea-gle of Africa lives in the rain forests and eats mon-keys.
Five objects have been created; CROWNED-EAGLE, AFRICA, RAIN-FOREST, MONKEY and@X235, which stands for the concept "crowned ea-gle of Africa."
The relation @A237 represents theingest relation between the object @X235 and theobject MONKEY.
The object MONKEY points tothis structure by the entry under MONKEY thatsays ingest~oby (@x235 (Smote (@a237)))), and theobject @X235 also points to this structure by theslot (ingest (monkey ($more (@a237)))).
The scopeof the quantifiers is from left to right.
Then, themeaning of structure @A239 (assuming that thequestion mark in the quantifier slot of MONKEYstands for "some" as the question-answering as-sumes) is V(x)(@X235(x) ~ 3y(MONKEY(y)  AINGEST(x,y))).
An a-structure can be linked toother a-structures by slots expressing causality, time,etc., as becomes necessary in the representation ofBirds migrate south when it freezes.6 ResultsTable 1 below provides tatistics revealing how wellthe system performed during testing.
The sys-tem was initially trained on ten articles: bears,beavers, beetles, elephants, frogs, penguins, rac-coons, seals, snakes, and tigers.
Then, two morearticles about sharks and eagles were analyzed to as-sess our progress.
Test texts were chosen randomlyby a student selecting a letter of the alphabet andthen finding texts about animals within those vol-87CROWNED*EAGLE(is-a (eagle))RAIN-FOREST(is-a (feces0)(related-to (@a239))@X235(cf (is-a (crowned-caBle)) (@a235))(location-r (afi'ie a ($rncte (@a237))))(ingest (monkey ($mofe (@a237))))(inhabit ($null ($mcfe (@a239))))@A235(instance-of (description))(args (@x235) (africa))(pr (location-r))(descr-subj (@r,235 (q (all))))(descr-obj (africa (q (constant))))AFRICA(location-of (@x235 (Smote (@a236))))MONKEY(ingest%by (@x235 (Smote (@a237))))@A237(args (@x235) (monkey))(pr (ingest))(actor (@x235 (q (zll))))(theme (monkey (q (?
))D(instance-of (action))@A239(args (@x235) (raln-fc~est))(1~ (inhabit))(actor (@x235 (q (all))))(at-lot (rain-forest (q (7))))(instance-of (action))Figure 4: Formation Structuresumes of the encyclopedia.
The letter "B" and theletter "M" were chosen.
The texts were then se-lected from those volumes.
In December of 1993, anarticle about birds was chosen.
No component of thesystem (lexicon, parser, interpreter, etc.)
was pre-prepared with information about this article.
Thelexicon of the parser consisted of 10,000 words.
Thistext was the largest article that the system had ana-lyzed, contMning approximately 1330 sentences and16,000 words.
None of the designers of the systemread this article prior to the test.
And even if theyhad read it, it would have been of very little use be-cause the system has reached such complexity thatit is not easy to assess how it is going to perform inan article of 1330 sentences.The first row of Table 1 indicates that 145sentences were selected from the bird text by akeyword/pattern-based kimmer.
Of these 145 sen-tences, 91 were relevant o the dietary habits do-main.
A total of 23 relevant sentences were missed,primarily due to keywords or patterns not contem-plated.
An example of sentence that is relevantbut was not selected is Robins and sparrows, forexample, are highly effective against cabbageworms,tomato worms, and leaf beetles.
The parser was ableto produce a correct parse for 58 of the 91 relevantsentences (64%), even in cases where the sentencecontained unknown words.
Among the sentencessuccessfully parsed, the parser encountered some 40unknown words of which approximately 70% werenames of birds, such as "grosbeaks", "flycatchers","titmice", "thrashers," etc., and 30% were commonwords.
Of those 58 parsed sentences, 32 (55%) wereinterpreted correctly.
The output for these 32 sen-tences was then passed to the formation, recogni-tion, and integration phases to be inserted into LTM.Interpretation failures can be attributed to missingVM rules, comparatives, and problems of anaphoricreference.Later, in April of 1994, three more texts were ran-domly chosen for testing.
A summary of the resultsfor those three tests is also given in Table 1.
InTable 1: Statistics for the Sentences of the Bird, Bat,Monkey, and Mouse TextsSel Re l  Irrel Miss Parse Interp Concepts T ime* DateBi rd  145 91 54  23 64% 55% 325 441 sees 12/93Bat 27 26 1 9 93% 60% 118 79 sees 4 /94Monkey 22 8 14 4 73% V5q~ 72  79  sees 4 /94Mouse  29 21 8 3 79% 50% 123 93 sees 4 /94* th is  t ime inc ludes  sk imming ,  pars ing ,  in terpret ing ,  forming, and integratingon a SPARC Classic machine (mic roSPARC 50MHz CPU) .B i rd  Bat  Monkey  Mouse CombinedSe lected  Sentences  145 27 22  29  223Average Words/Sentence 17.1 11.6 16.8 13,7 16.0Longest Sentence 35 21 27 26  35this test, the parser ran with a lexicon consisting ofabout 35,000 words.
The improvement of the inter-preter was mainly due to new interpretation rules,additions to the hierarchy of verbal concepts, andto the hierarchy of concepts that organize the infer-ences about the topic.All of the tests were run on a SPAP~C Classic Ma-chine executing Allegro Common Lisp.
The averagetime to completely process a selected sentence onthis platform was 3.1 seconds.
This is a conserva-tive figure because it includes the processing time ofthe skimmer, i.e., some amount of overhead is nec-essary for file handling and for determining when asentence is irrelevant.
Therefore, the average timefor processing a sentence is actually less than -3.
'1seconds.The following is a list of natural language ques-tions posed to the system after reading the bird,bat, and monkey articles and the contents of thesystem-generated answers.
Note that the systemoutput has been Mtered and condensed for the sakeof brevity.
In answering the questions, the sys-tem uses classification-based reasoning, not theoremproving.
Many complex chains of inferences can beobtained by keeping memory organized in a princi-pled manner.
In those cases in which a question asksabout a concept hat does not exist in LTM, the clas-sifier is activated to place that concept in LTM andobtain an answer.
See (Gomez and Segami, 1991)for a detailed discussion of all these issues and thetheorems proving the soundness of the inference al-gorithms.What do birds eat?
sapsucker ingest tree-sap; hum-mingbird ingest nectar; duck ingest plant-matter,grass, seaweed; louisiana-water-thrush ingest water-insect; young-bird ingest earthworm, insect, smallanimal; ........Which birds eat nectar?
hummingbird ingest nectarWhat kinds of insect eaters are there?
chickadee,creeper, flycatcher, kinglet, swallow, swift, thrasher,titmice, vireo, warbler, woodpecker, owlWhat is gravel?
I don't know, but I know that: birdingest gravel < related-to > bird *assist* grinding-88processDo most cactus dwellers eat insects?
yesWhat kills birds?
eagle is-a bird, and hunters andtrappers kill eagles; osprey is-a bird, and huntersand trappers kill ospreyWhen do most birds search for food?
at-time dayDo birds help people?
yes, bird help farmerHow do birds help farmers?
bird ingest <insectwhich ingest crop>; bird ingest weed-seedDo bats eat blood?
yes, some bat eat blood becausevampire-bat is-a bat and vampire-bat ingest bloodHow much blood do vampire bats eat?
vampire-batingest blood quantity 1 tablespoon *frequency* dayDo vampire bats attack human beings?
yes, vampirebat harm human *frequency* sometimesDo monkeys have enemies?
yes, some monkey has-enemy cheetah hyena jackal leopard lion because<monkeys inhabit at-loc ground> has-enemy chee-tah hyena jackal leopard lion7 Re la ted  WorkIn (Sondheimer et al, 1984), frame-like structures,KL-ONE structures in fact, are also used to guidesemantic interpretation i  an application domain.However, the overall approach to the interpreta-tion task presented here differs from that work.
Inapproaching the problem of unrestricted texts, weagree with those researchers (Hobbs, 1991; Grish-man et al, 1991) who think that it is possible tobuild correct parses and interpretations for real-world texts.
In fact, it is hard for us to see howstatistical methods (de Marcken, 1990; Church etal., 1991) could be used for building knowledge-baseswith sufficient expressive power to correctly answerquestions posed by expert systems or human users.We think that the same critique applies to skim-mers (Lehnert et al, 1991), but for very differentreasons.
In order to guarantee the correctness ofthe knowledge-base built, every element in the sen-tence needs to be interpreted.
For instance, if theadverb "mostly" is not interpreted in the sentenceThese owls eat mostly rodents, the integrity of theknowledge-base built is not going to suffer greatly.But, if we are talking about the adverb "rarely" inthe sentence These owls rarely eat rodents, the situ-ation becomes much more serious, as we found out.This work has advanced a new approach to se-mantic interpretation that occupies a middle groundbetween those approaches that rely heavily on theparser for building structures and attaching PPs,subordinate clauses, etc.
(Grishman et al, 1991;Hobbs, 1978; Tomita, 1985) and semantic-centeredapproaches (Riesbeck and Martin, 1986; Cardie andLehnert, 1991; Slator and Wilks, 1991).
Our parserdelegates all the burden of dealing with structuralambiguity (attachment of PPs, relative clauses, sub-ordinate clauses, etc.)
to the interpreter.
That is oneof the reasons why it is so fast.
The interpreter has avery sophisticated algorithm that uses the informa-tion built in the verbal concepts in order to attachPPs.
Yet, if the parser does not build a parse, albeita shallow one, the interpreter will not know whatto do.
Moreover, the interpreter does not questionthe parser when it says this constituent is an obj, orsubj, or a time-np, etc.
This is a situation that weare not happy about, because the parser identifiessome constituents incorrectly, especially the time-np.
We are studying mechanisms under which theinterpreter will override the parser and will get itout of trouble in processing very complex sentences(Krupka et al, 1991; Jacobs et al, 1991).8 Conc lus ionsWe have presented a method for the acquisition ofknowledge from encyclopedic texts.
The method de-pends on understanding what is being read, whichin turn depends on: (1) providing a successful parseand interpretation for a sentence, (2) building finalknowledge representation structures from the log-ical form of the sentence, which involves creatingnew concepts and relations as the system is reading,and (3) integrating in LTM those concepts and rela-tions that the recognizer algorithm fails to recognize,which in many cases involves the reorganization ofconcepts in LTM.The results that are reported in this paper are veryencouraging, because a high percentage of the fail-ures are due to some incomplete implementations ofsome aspects of the system.
For instance, in dealingwith anaphora we have incorporated in our systemsome of the ideas reported in (Grosz, 1983; Hobbs,1978), but our work is clearly insufficient in thatregard.
A major hole in the interpreter, as of thiswriting, is that it does not interpret comparatives,except very simple ones like quantifiers, e.g., "morethan 2."
The interpreter needs to have mechanismsin place to recover the elliptical elements in compara-tives, which in many cases require solving extrasen-tential reference, e.g., The golden eagle defends aterritory of about 20 to 60 square miles.
The baldeagle holds a smaller territory.
The skimmer usesvery rudimentary techniques, and there is a lot ofroom for improvement here.
In any case, this hasnot been a major concern of this research.
Moreover,because the system is so incredibly fast, if the skim-mer overgenerates, it is not much of a problem.
Anaspect related to the skimmer that we have no spaceto discussis that it became necessary to build analgorithm to recognize subclasses of the class of ani-mals being searched for every question.
For instance,if the question is Do sharks eat plankton, this algo-rithm analyzes every sentence in the encyclopedic ar-ticle, before being passed to the skimmer, searchingfor NPs denoting subclasses of sharks.
This is neces-sary because the author may introduce the concept"Mako sharks" in a context unrelated to the relation89ingest, and consequently, the skimmer is not goingto select this sentence.
Then, if the author lateron says Makos feed on other fish, including herring,mackerel, and swordfish, the system has no way torelate "Mako" to sharks, missing the fact that sharksfeed on herring, mackerel, and swordfish.
In June,we tested the parser on 2900 sentences from 25 ar-ticles, including non-animal articles such as black-holes, cancer, computer chips, napoleon, greenhouseeffect, etc., and the rate of success was 76.8%.
How-ever, some sentences are still not parsed becausesome subcategorizations of verbs are wrong or in-complete, or the phrasal exicon is incomplete.
Weare confident that the parser may reach a plateau at85% or 90% for this Encyclopedia.
The remaining10% or 15% may require considerable help from theinterpreter to be parsed.ReferencesCardie, C. and Lehnert, W. (1991).
A cognitivelyplausible approach to understanding complex syn-tax.
In Proc.
AAAI-91, pp.
117-124, Anaheim,CA, July 1991.Church, K., Gale, W., Hanks, P., and Hindle, D.(1991).
Parsing, Word Associations, and Typi-cal Predicate-Argument Relations.
In Current Is-sues in Parsing Technology, Tomita, Masaru, ed.,Kluwer Academic Publishers, Boston, MA, pp.103-112.Gomez, F. (1989).
WUP: A parser Based on WordUsage.
UCF-Tech-89-2, Dept.
of Computer Sci-ence, University of Central Florida, Orlando, F132816, (Under Revision).Gomez, F., and Segami, C. (1989).
The Recogni-tion and Integration of Concepts in Understand-ing Scientific Texts.
Journal of Experimental ndTheoretical Artificial Intelligence.
1, pp.
51-77.Gomez, F., Segami, C., and Hull, R. (1993).
Prepo-sitional Attachment, Prepositional Meaning andDetermination of Primitives and Thematic Roles.UCF Technical Report.Gomez, F., and Segami, C. (1991) Classification-Based Reasoning.
IEEE Trans.
on Systems, Manand Cybernetics, 21, no.
3, pp.
644-659.Grishman, R., Sterling, J., and Macleod, C. (1991).New York University: Description of the PRO-TEUS System as Used for MUC-3.
Proc.
3rd Mes-sage Understanding Conference (MUC-3), May,1991, Morgan Kaufmann, pp.
3-16.Grosz, B. J., Providing a Unified Account of DefiniteNoun Phrase in Discourse, Proc.
ACL, 1983, pp.44-5O.Hobbs, a. R. (1991).
SRI International: Descrip-tion of the TACITUS System as Used for MUC-3.Proc.
3rd Message Understanding Conf.
(MUC-3), May, 1991, Morgan Kaufmann, pp.
3-16.Hobbs, J. R. (1978).
Resolving pronoun refer-ences.
In Readings in Natural Language Process-ing, Grosz, B., Jones, K. S., Webber, B., eds., pp339-352, Morgan Kaufmann Publishers, Los Al-tos, CA.Jacobs, P. S., Krupka, G. R., Rau, L. F. (1991).Lexico-semantic pattern matching as a companionto parsing in text understanding.
In Proc.
DARPASpeech and Natural Language Workshop, pp.
337-341, Asilomar, CA.Krupka, G., Jacobs, P., Rau, L., and Iwanska, L.(1991).
GE: Description of the NLToolset Sys-tem as Used for MUC-3.
Proc.
3rd Message Un-derstanding Conf.
(MUC-3), May, 1991, MorganKaufmann, pp.
3-16.Lehnert, W., Cardie, C., Fisher, D., Riloff, E., andWilliams, R. (1991).
University of Massachusetts:Description of the CIRCUS System as Used forMUC-3.
Proc.
3rd Message Understanding Conf.
(MUC-3), May, 1991, Morgan Kaufmann, pp.
3-16.de Marcken, C. G. (1990).
Parsing the LOB cor-pus.
In Proc.
ACL, pp.
243-251, Helsinki, Finland,Aug.
1990.Rich, E., Barnett, J., Wittenburg, K., and Wrob-lewski, D. (1987).
Ambiguity Procrastination.AAAI-87, pp.
571-574.Riesbeck, C. K., and Martin, C. E. (1986).
DirectMemory Access Parsing.
Experience, Memory andReasoning, ed.
Kolodner, J. and Riesbeck, C. K.,Lawrence Erlbaum Associates, Hillsdale, N.J.Slator, Brian M., and Wilks, Yorick.
(1991).PREMO: Parsing by conspicuous lexical con-sumption.
In Current Issues in Parsing Technol-ogy, Tomita, Masaru, ed., Kluwer Academic Pub-lishers, Boston, MA, pp.
85-102.Schmolze, J., and Lipkis, T. (1983).
Classification ithe KL-ONE Knowledge Representation System,in Proc.
IJCAI-83, Karlsruhe: IJCAI, pp.
330-332, 1983.Sondheimer, N. K., Weischedel, R. M., and Bobrow,R.
J.
(1984).
Semantic Interpretation Using KL-ONE.
COLING-84, pp.
101-107.Sundheim, Beth M. (1992).
Overview of the FourthMessage Understanding Evaluation and Confer-ence.
Proc.
4th Message Understanding Conf.
(MUC-4), June, 1992, Morgan Kaufmann, pp.
3-21.Tomita, M. (1985).
Efficient parsing for natural an-guage.
Kluwer Academic Publishers, Boston, MA.The World Book Encyclopedia.
(1987).
World Book,Inc., Chicago.90
