A Deterministic Word Dependency AnalyzerEnhanced With Preference LearningHideki Isozaki and Hideto Kazawa and Tsutomu HiraoNTT Communication Science LaboratoriesNTT Corporation2-4 Hikaridai, Seikacho, Sourakugun, Kyoto, 619-0237 Japan{isozaki,kazawa,hirao}@cslab.kecl.ntt.co.jpAbstractWord dependency is important in parsing tech-nology.
Some applications such as Informa-tion Extraction from biological documents ben-efit from word dependency analysis even with-out phrase labels.
Therefore, we expect an ac-curate dependency analyzer trainable withoutusing phrase labels is useful.
Although suchan English word dependency analyzer was pro-posed by Yamada and Matsumoto, its accu-racy is lower than state-of-the-art phrase struc-ture parsers because of the lack of top-down in-formation given by phrase labels.
This papershows that the dependency analyzer can be im-proved by introducing a Root-Node Finder anda Prepositional-Phrase Attachment Resolver.Experimental results show that these modulesbased on Preference Learning give better scoresthan Collins?
Model 3 parser for these subprob-lems.
We expect this method is also applicableto phrase structure parsers.1 Introduction1.1 Dependency AnalysisWord dependency is important in parsing technol-ogy.
Figure 1 shows a word dependency tree.
Eis-ner (1996) proposed probabilistic models of depen-dency parsing.
Collins (1999) used dependencyanalysis for phrase structure parsing.
It is also stud-ied by other researchers (Sleator and Temperley,1991; Hockenmaier and Steedman, 2002).
How-ever, statistical dependency analysis of English sen-tences without phrase labels is not studied verymuch while phrase structure parsing is intensivelystudied.
Recent studies show that Information Ex-traction (IE) and Question Answering (QA) benefitfrom word dependency analysis without phrase la-bels.
(Suzuki et al, 2003; Sudo et al, 2003)Recently, Yamada and Matsumoto (2003) pro-posed a trainable English word dependency ana-lyzer based on Support Vector Machines (SVM).They did not use phrase labels by considering an-notation of documents in expert domains.
SVM(Vapnik, 1995) has shown good performance in dif-HeagirlatelescopewithsawHe saw a girl with a telescope.Figure 1: A word dependency treeferent tasks of Natural Language Processing (Kudoand Matsumoto, 2001; Isozaki and Kazawa, 2002).Most machine learning methods do not work wellwhen the number of given features (dimensionality)is large, but SVM is relatively robust.
In NaturalLanguage Processing, we use tens of thousands ofwords as features.
Therefore, SVM often gives goodperformance.However, the accuracy of Yamada?s analyzer islower than state-of-the-art phrase structure parserssuch as Charniak?s Maximum-Entropy-InspiredParser (MEIP) (Charniak, 2000) and Collins?
Model3 parser.
One reason is the lack of top-down infor-mation that is available in phrase structure parsers.In this paper, we show that the accuracy of theword dependency parser can be improved by addinga base-NP chunker, a Root-Node Finder, and aPrepositional Phrase (PP) Attachment Resolver.
Weintroduce the base-NP chunker because base NPsare important components of a sentence and can beeasily annotated.
Since most words are containedin a base NP or are adjacent to a base NP, we ex-pect that the introduction of base NPs will improveaccuracy.We introduce the Root-Node Finder because Ya-mada?s root accuracy is not very good.
Each sen-tence has a root node (word) that does not modifyany other words and is modified by all other wordsdirectly or indirectly.
Here, the root accuracy is de-fined as follows.Root Accuracy (RA) =#correct root nodes / #sentences (= 2,416)We think that the root node is also useful for depen-dency analysis because it gives global informationto each word in the sentence.Root node finding can be solved by various ma-chine learning methods.
If we use classifiers, how-ever, two or more words in a sentence can be classi-fied as root nodes, and sometimes none of the wordsin a sentence is classified as a root node.
Practically,this problem is solved by getting a kind of confi-dence measure from the classifier.
As for SVM,f(x) defined below is used as a confidence measure.However, f(x) is not necessarily a good confidencemeasure.Therefore, we use Preference Learning proposedby Herbrich et al (1998) and extended by Joachims(2002).
In this framework, a learning system istrained with samples such as ?A is preferable toB?
and ?C is preferable to D.?
Then, the systemgeneralizes the preference relation, and determineswhether ?X is preferable to Y?
for unseen X andY.
This framework seems better than SVM to selectbest things.On the other hand, it is well known that attach-ment ambiguity of PP is a major problem in parsing.Therefore, we introduce a PP-Attachment Resolver.The next sentence has two interpretations.He saw a girl with a telescope.1) The preposition ?with?
modifies ?saw.?
That is, hehas the telescope.
2) ?With?
modifies ?girl.?
That is,she has the telescope.Suppose 1) is the correct interpretation.
Then,?with modifies saw?
is preferred to ?with mod-ifies girl.?
Therefore, we can use PreferenceLearning again.Theoretically, it is possible to build a new De-pendency Analyzer by fully exploiting PreferenceLearning, but we do not because its training takestoo long.1.2 SVM and Preference LearningPreference Learning is a simple modification ofSVM.
Each training example for SVM is a pair(yi, xi), where xi is a vector, yi = +1 means thatxi is a positive example, and yi = ?1 means that xiis a negative example.
SVM classifies a given testvector x by using a decision functionf(x) = wf ?
?
(x) + b =?`iyi?iK(x, xi) + b,where {?i} and b are constants and ` is the numberof training examples.
K(xi, xj) = ?
(xi) ?
?
(xj) isa predefined kernel function.
?
(x) is a function thatmaps a vector x into a higher dimensional space.Training of SVM corresponds to the follow-ing quadratic maximization (Cristianini and Shawe-Taylor, 2000)W (?)
=?`i=1?i ?12?`i,j=1?i?jyiyjK(xi, xj),where 0 ?
?i ?
C and?`i=1 ?iyi = 0.
C is a softmargin parameter that penalizes misclassification.On the other hand, each training examplefor Preference Learning is given by a triplet(yi, xi.1, xi.2), where xi.1 and xi.2 are vectors.
Weuse xi.?
to represent the pair (xi.1, xi.2).
yi = +1means that xi.1 is preferable to xi.2.
We can regardtheir difference ?
(xi.1) ?
?
(xi.2) as a positive ex-ample and ?
(xi.2) ?
?
(xi.1) as a negative example.Symmetrically, yi = ?1 means that xi.2 is prefer-able to xi.1.Preference of a vector x is given byg(x) = wg??
(x) =?`iyi?i(K(xi.1, x)?K(xi.2, x)).If g(x) > g(x?)
holds, x is preferable to x?.
SincePreference Learning uses the difference ?
(xi.1) ??
(xi.2) instead of SVM?s ?
(xi), it corresponds tothe following maximization.W (?)
=?`i=1?i ?12?`i,j=1?i?jyiyjK(xi.
?, xj.?
)where 0 ?
?i ?
C and K(xi.
?, xj.?)
=K(xi.1, xj.1) ?
K(xi.1, xj.2) ?
K(xi.2, xj.1) +K(xi.2, xj.2).
The above linear constraint?`i=1 ?iyi = 0 for SVM is not applied toPreference Learning because SVM requires thisconstraint for the optimal b, but there is no b in g(x).Although SVMlight (Joachims, 1999) provides animplementation of Preference Learning, we use ourown implementation because the current SVMlightimplementation does not support non-linear kernelsand our implementation is more efficient.Herbrich?s Support Vector Ordinal Regression(Herbrich et al, 2000) is based on Preference Learn-ing, but it solves an ordered multiclass problem.Preference Learning does not assume any classes.2 MethodologyInstead of building a word dependency corpus fromscratch, we use the standard data set for comparison.Dependency Analyzer?
PP-Attachment Resolver?Root-Node Finder?Base NP Chunker?
(POS Tagger)?= SVM, ?
= Preference LearningFigure 2: Module layers in the systemThat is, we use Penn Treebank?s Wall Street Journaldata (Marcus et al, 1993).
Sections 02 through 21are used as training data (about 40,000 sentences)and section 23 is used as test data (2,416 sentences).We converted them to word dependency data by us-ing Collins?
head rules (Collins, 1999).The proposed method uses the following proce-dures.?
A base NP chunker: We implemented anSVM-based base NP chunker, which is a sim-plified version of Kudo?s method (Kudo andMatsumoto, 2001).
We use the ?one vs. allothers?
backward parsing method based on an?IOB2?
chunking scheme.
By the chunking,each word is tagged as?
B: Beginning of a base NP,?
I: Other elements of a base NP.?
O: Otherwise.Please see Kudo?s paper for more details.?
A Root-Node Finder (RNF): We will describethis later.?
A Dependency Analyzer: It works just like Ya-mada?s Dependency Analyzer.?
A PP-Attatchment Resolver (PPAR): This re-solver improves the dependency accuracy ofprepositions whose part-of-speech tags are INor TO.The above procedures require a part-of-speechtagger.
Here, we extract part-of-speech tags fromthe Collins parser?s output (Collins, 1997) for sec-tion 23 instead of reinventing a tagger.
Accordingto the document, it is the output of Ratnaparkhi?stagger (Ratnaparkhi, 1996).
Figure 2 shows the ar-chitecture of the system.
PPAR?s output is used torewrite the output of the Dependency Analyzer.2.1 Finding root nodesWhen we use SVM, we regard root-node finding asa classification task: Root nodes are positive exam-ples and other words are negative examples.For this classification, each word wi in a taggedsentence T = (w1/p1, .
.
.
, wi/pi, .
.
.
, wN/pN ) ischaracterized by a set of features.
Since the givenPOS tags are sometimes too specific, we introducea rough part-of-speech qi defined as follows.?
q = N if p = NN, NNP, NNS,NNPS, PRP, PRP$, POS.?
q = V if p = VBD, VB, VBZ, VBP,VBN.?
q = J if p = JJ, JJR, JJS.Then, each word is characterized by the followingfeatures, and is encoded by a set of boolean vari-ables.?
The word itself wi, its POS tags pi andqi, and its base NP tag bi = B, I,O.We introduce boolean variables such ascurrent word is John and cur-rent rough POS is J for each of thesefeatures.?
Previous word wi?1 and its tags, pi?1, qi?1,and bi?1.?
Next word wi+1 and its tags, pi+1, qi+1, andbi+1.?
The set of left words {w0, .
.
.
, wi?1}, andtheir tags, {p0, .
.
.
, pi?1}, {q0, .
.
.
, qi?1}, and{b0, .
.
.
, bi?1}.
We use boolean variables suchas one of the left words is Mary.?
The set of right words {wi+1, .
.
.
, wN},and their POS tags, {pi+1, .
.
.
, pN} and{qi+1, .
.
.
, qN}.?
Whether the word is the first word or not.We also add the following boolean features to getmore contextual information.?
Existence of verbs or auxiliary verbs (MD) inthe sentence.?
The number of words between wiand the nearest left comma.
Weuse boolean variables such as near-est left comma is two words away.?
The number of words between wi and the near-est right comma.Now, we can encode training data by using theseboolean features.
Each sentence is converted to theset of pairs {(yi, xi)} where yi is +1 when xi cor-responds to the root node and yi is ?1 otherwise.For Preference Learning, we make the set of triplets{(yi, xi.1, xi.2)}, where yi is always +1, xi.1 corre-sponds to the root node, and xi.2 corresponds to anon-root word in the same sentence.
Such a tripletmeans that xi.1 is preferable to xi.2 as a root node.2.2 Dependency analysisOur Dependency Analyzer is similar to Ya-mada?s analyzer (Yamada and Matsumoto,2003).
While scanning a tagged sentenceT = (w1/p1, .
.
.
, wn/pn) backward from theend of the sentence, each word wi is classified intothree categories: Left, Right, and Shift.1?
Right: Right means that wi directly modifiesthe right word wi+1 and that no word in Tmodifies wi.
If wi is classified as Right, theanalyzer removes wi from T and wi is regis-tered as a left child of wi+1.?
Left: Left means that wi directly modifies theleft word wi?1 and that no word in T modifieswi.
If wi is classified as Left, the analyzer re-moves wi from T and wi is registered as a rightchild of wi?1.?
Shift: Shift means that wi is not next to itsmodificand or is modified by another word inT .
If wi is classified as Shift, the analyzerdoes nothing for wi and moves to the left wordwi?1.This process is repeated until T is reduced to asingle word (= root node).
Since this is a three-classproblem, we use ?one vs. rest?
method.
First, wetrain an SVM classifier for each class.
Then, foreach word in T , we compare their values: fLeft(x),fRight(x), and fShift(x).
If fLeft(x) is the largest,the word is classified as Left.However, Yamada?s algorithm stops when allwords in T are classified as Shift, even when T hastwo or more words.
In such cases, the analyzer can-not generate complete dependency trees.Here, we resolve this problem by reclassifying aword in T as Left or Right.
This word is selected interms of the differences between SVM outputs:?
?Left(x) = fShift(x) ?
fLeft(x),?
?Right(x) = fShift(x) ?
fRight(x).These values are non-negative because fShift(x)was selected.
For instance, ?Left(x) ' 0 means thatfLeft(x) is almost equal to fShift(x).
If ?Left(xk)gives the smallest value of these differences, theword corresponding to xk is reclassified as Left.
If1Yamada used a two-word window, but we use a one-wordwindow for simplicity.
?Right(xk) gives the smallest value, the word cor-responding to xk is reclassified as Right.
Then, wecan resume the analysis.We use the following basic features for each wordin a sentence.?
The word itself wi and its tags pi, qi, and bi,?
Whether wi is on the left of the root node or onthe right (or at the root node).
The root node isdetermined by the Root-Node Finder.?
Whether wi is inside a quotation.?
Whether wi is inside a pair of parentheses.?
wi?s left children {wi1, .
.
.
, wik}, whichwere removed by the Dependency Analyzerbeforehand because they were classified as?Right.?
We use boolean variables such asone of the left child is Mary.Symmetrically, wi?s right children{wi1, .
.
.
, wik} are also used.However, the above features cover only near-sighted information.
If wi is next to a very longbase NP or a sequence of base NPs, wi cannot getinformation beyond the NPs.
Therefore, we add thefollowing features.?
Li, Ri: Li is available when wi immediatelyfollows a base NP sequence.
Li is the word be-fore the sequence.
That is, the sentence lookslike:.
.
.
Li ?
a base NP ?
wi .
.
.Ri is defined symmetrically.The following features of neigbors are also usedas wi?s features.?
Left words wi?3, .
.
.
, wi?1 and their basic fea-tures.?
Right words wi+1, .
.
.
, wi+3 and their basicfeatures.?
The analyzer?s outputs (Left/Right/Shift) forwi+1, .
.
.
, wi+3.
(This analyzer runs backwardfrom the end of T .
)If we train SVM by using the whole data at once,training will take too long.
Therefore, we splitthe data into six groups: nouns, verbs, adjectives,prepositions, punctuations, and others.2.3 PP attachmentSince we do not have phrase labels, we use allprepositions (except root nodes) as training data.We use the following features for resolving PP at-tachment.?
The preposition itself: wi.?
Candidate modificand wj and its POS tag.?
Left words (wi?2, wi?1) and their POS tags.?
Right words (wi+1, wi+2) and their POS tags.?
Previous preposition.?
Ending word of the following base NP and itsPOS tag (if any).?
i ?
j, i.e., Number of the words between wiand wj .?
Number of commas between wi and wj .?
Number of verbs between wi and wj .?
Number of prepositions between wi and wj .?
Number of base NPs between wi and wj .?
Number of conjunctions (CCs) between wi andwj .?
Difference of quotation depths between wi andwj .
If wi is not inside of a quotation, its quo-tation depth is zero.
If wj is in a quotation, itsquotation depth is one.
Hence, their differenceis one.?
Difference of parenthesis depths between wiand wj .For each preposition, we make the set of triplets{(yi, xi,1, xi,2)}, where yi is always +1, xi,1 corre-sponds to the correct word that is modified by thepreposition, and xi,2 corresponds to other words inthe sentence.3 Results3.1 Root-Node FinderFor the Root-Node Finder, we used a quadratic ker-nel K(xi, xj) = (xi ?
xj + 1)2 because it was betterthan the linear kernel in preliminary experiments.When we used the ?correct?
POS tags given in thePenn Treebank, and the ?correct?
base NP tags givenby a tool provided by CoNLL 2000 shared task2,RNF?s accuracy was 96.5% for section 23.
Whenwe used Collins?
POS tags and base NP tags basedon the POS tags, the accuracy slightly degraded to95.7%.
According to Yamada?s paper (Yamada and2http://cnts.uia.ac.be/conll200/chunking/Matsumoto, 2003), this root accuracy is better thanCharniak?s MEIP and Collins?
Model 3 parser.We also conducted an experiment to judge the ef-fectiveness of the base NP chunker.
Here, we usedonly the first 10,000 sentences (about 1/4) of thetraining data.
When we used all features describedabove and the POS tags given in Penn Treebank,the root accuracy was 95.4%.
When we removedthe base NP information (bi, Li, Ri), it droppedto 94.9%.
Therefore, the base NP information im-proves RNF?s performance.Figure 3 compares SVM and Preference Learn-ing in terms of the root accuracy.
We used thefirst 10,000 sentences for training again.
Accord-ing to this graph, Preference Learning is better thanSVM, but the difference is small.
(They are bet-ter than Maximum Entropy Modeling3 that yieldedRA=91.5% for the same data.)
C does not affect thescores very much unless C is too small.
In this ex-periment, we used Penn?s ?correct?
POS tags.
Whenwe used Collins?
POS tags, the scores dropped byabout one point.3.2 Dependency Analyzer and PPARAs for the dependency learning, we used the samequadratic kernel again because the quadratic kernelgives the best results according to Yamada?s experi-ments.
The soft margin parameter C is 1 followingYamada?s experiment.
We conducted an experimentto judge the effectiveness of the Root-Node Finder.We follow Yamada?s definition of accuracy that ex-cludes punctuation marks.Dependency Accuracy (DA) =#correct parents / #words (= 49,892)Complete Rate (CR) =#completely parsed sentences / #sentencesAccording to Table 1, DA is only slightly improved,but CR is more improved.3http://www2.crl.go.jp/jt/a132/members/mutiyama/software.htmlSVMPreference LearningAccuracy (%)C0.10.030.010.0030.0010.00030.000190919293949596 ?????????????
?Figure 3: Comparison of SVM and PreferenceLearning in terms of Root Accuracy (Trained with10,000 sentences)DA RA CRwithout RNF 89.4% 91.9% 34.7%with RNF 89.6% 95.7% 35.7%TheDependency Analyzer was trained with 10,000sentences.
RNF was trained with all of the training data.DA: Dependency Accuracy, RA: Root Acc., CR:Complete RateTable 1: Effectiveness of the Root-Node FinderAccuracy (%)CPreference LearningSVM0.10.030.010.0030.0010.00030.000170727476788082????????????????????????
?Figure 4: Comparison of SVM and PreferenceLearning in terms of Dependency Accuracy ofprepositions (Trained with 5,000 sentences)Figure 4 compares SVM and Preference Learningin terms of the Dependency Accuracy of preposi-tions.
SVM?s performance is unstable for this task,and Preference Learning outperforms SVM.
(Wecould not get scores of Maximum Entropy Model-ing because of memory shortage.
)Table 2 shows the improvement given by PPAR.Since training of PPAR takes a very long time, weused only the first 35,000 sentences of the train-ing data.
We also calculated the Dependency Accu-racy of Collins?
Model 3 parser?s output for section23.
According to this table, PPAR is better than theModel 3 parser.Now, we use PPAR?s output for each prepositioninstead of the dependency parser?s output unless themodification makes the dependency tree into a non-tree graph.
Table 3 compares the proposed methodwith other methods in terms of accuracy.
This dataexcept ?Proposed?
was cited from Yamada?s paper.IN TO averageCollins Model 3 84.6% 87.3% 85.1%Dependency Analyzer 83.4% 86.1% 83.8%PPAR 85.3% 87.7% 85.7%PPAR was trained with 35,000 sentences.
The numberof IN words is 5,950 and that of TO is 1,240.Table 2: PP-Attachment ResolverDA RA CRwith MEIP 92.1% 95.2% 45.2%phrase info.
Collins Model3 91.5% 95.2% 43.3%without Yamada 90.3% 91.6% 38.4%phrase info.
Proposed 91.2% 95.7% 40.7%Table 3: Comparison with related workAccording to this table, the proposed method isclose to the phrase structure parsers except Com-plete Rate.
Without PPAR, DA dropped to 90.9%and CR dropped to 39.7%.4 DiscussionWe used Preference Learning to improve the SVM-based Dependency Analyzer for root-node findingand PP-attachment resolution.
Preference Learn-ing gave better scores than Collins?
Model 3 parserfor these subproblems.
Therefore, we expect thatour method is also applicable to phrase structureparsers.
It seems that root-node finding is relativelyeasy and SVM worked well.
However, PP attach-ment is more difficult and SVM?s behavior was un-stable whereas Preference Learning was more ro-bust.
We want to fully exploit Preference Learn-ing for dependency analysis and parsing, but train-ing takes too long.
(Empirically, it takes O(`2) ormore.)
Further study is needed to reduce the compu-tational complexity.
(Since we used Isozaki?s meth-ods (Isozaki and Kazawa, 2002), the run-time com-plexity is not a problem.
)Kudo and Matsumoto (2002) proposed an SVM-based Dependency Analyzer for Japanese sen-tences.
Japanese word dependency is simpler be-cause no word modifies a left word.
Collins andDuffy (2002) improved Collins?
Model 2 parserby reranking possible parse trees.
Shen and Joshi(2003) also used the preference kernel K(xi.
?, xj.?
)for reranking.
They compare parse trees, but oursystem compares words.5 ConclusionsDependency analysis is useful and annotation ofword dependency seems easier than annotation ofphrase labels.
However, lack of phrase labels makesdependency analysis more difficult than phrasestructure parsing.
In this paper, we improved a de-terministic dependency analyzer by adding a Root-Node Finder and a PP-Attachment Resolver.
Pref-erence Learning gave better scores than Collins?Model 3 parser for these subproblems, and the per-formance of the improved system is close to state-of-the-art phrase structure parsers.
It turned outthat SVM was unstable for PP attachment resolu-tion whereas Preference Learning was not.
We ex-pect this method is also applicable to phrase struc-ture parsers.ReferencesEugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 132?139.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernelsover discrete structures, and the voted percep-tron.
In Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics(ACL), pages 263?270.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Proceed-ings of the Annual Meeting of the Association forComputational Linguistics, pages 16?23.Michael Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, Univ.
of Pennsylvania.Nello Cristianini and John Shawe-Taylor.
2000.
AnIntroduction to Support Vector Machines.
Cam-bridge University Press.Jason M. Eisner.
1996.
Three new probabilisticmodels for dependency parsing: An exploration.In Proceedings of the International Conferenceon Computational Linguistics, pages 340?345.Ralf Herbrich, Thore Graepel, Peter Bollmann-Sdorra, and Klaus Obermayer.
1998.
Learningpreference relations for information retrieval.
InProceedings of ICML-98 Workshop on Text Cate-gorization and Machine Learning, pages 80?84.Ralf Herbrich, Thore Graepel, and Klaus Ober-mayer, 2000.
Large Margin Rank Boundaries forOrdinal Regression, chapter 7, pages 115?132.MIT Press.Julia Hockenmaier and Mark Steedman.
2002.Generative models for statistical parsing withcombinatory categorial grammar.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics, pages 335?342.Hideki Isozaki and Hideto Kazawa.
2002.
Efficientsupport vector classifiers for named entity recog-nition.
In Proceedings of COLING-2002, pages390?396.Thorsten Joachims.
1999.
Making large-scalesupport vector machine learning practical.
InB.
Scho?lkopf, C. J. C. Burges, and A. J. Smola,editors, Advances in Kernel Methods, chapter 16,pages 170?184.
MIT Press.Thorsten Joachims.
2002.
Optimizing search en-gines using clickthrough data.
In Proceedings ofthe ACM Conference on Knowledge Discoveryand Data Mining.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In Proceedings ofNAACL-2001, pages 192?199.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.In Proceedings of CoNLL, pages 63?69.Mitchell P. Marcus, Beatrice Santorini, and Mary A.Marcinkiewicz.
1993.
Building a large annotatedcorpus of english: the penn treebank.
Computa-tional Linguistics, 19(2):313?330.Adwait Ratnaparkhi.
1996.
A maximum entropypart-of-speech tagger.
In Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing.Libin Shen and Aravind K. Joshi.
2003.
An SVMbased voting algorithm with application to parsereranking.
In Proceedings of the Seventh Confer-ence on Natural Language Learning, pages 9?16.Daniel Sleator and Davy Temperley.
1991.
ParsingEnglish with a Link grammar.
Technical ReportCMU-CS-91-196, Carnegie Mellon University.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern represen-tation model for automatic IE pattern acquisi-tion.
In Proceedings of the Annual Meeting of theAssociation for Cimputational Linguistics, pages224?231.Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, andEisaku Maeda.
2003.
Hierarchical direct acyclicgraph kernel: Methods for structured natural lan-guage data.
In Proceedings of ACL-2003, pages32?39.Vladimir N. Vapnik.
1995.
The Nature of Statisti-cal Learning Theory.
Springer.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Sta-tistical dependency analysis.
In Proceedings ofthe International Workshop on Parsing Technolo-gies, pages 195?206.
