Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1338?1347,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPA Study on the Semantic Relatedness of Query and Document Terms inInformation RetrievalChristof M?uller and Iryna GurevychUbiquitous Knowledge Processing (UKP) LabComputer Science DepartmentTechnische Universit?at Darmstadt, Hochschulstra?e 10D-64289 Darmstadt, Germanyhttp://www.ukp.tu-darmstadt.de/AbstractThe use of lexical semantic knowledge ininformation retrieval has been a field of ac-tive study for a long time.
Collaborativeknowledge bases like Wikipedia and Wik-tionary, which have been applied in com-putational methods only recently, offernew possibilities to enhance informationretrieval.
In order to find the most bene-ficial way to employ these resources, weanalyze the lexical semantic relations thathold among query and document termsand compare how these relations are repre-sented by a measure for semantic related-ness.
We explore the potential of differentindicators of document relevance that arebased on semantic relatedness and com-pare the characteristics and performanceof the knowledge bases Wikipedia, Wik-tionary and WordNet.1 IntroductionToday we face a rapidly growing number of elec-tronic documents in all areas of life.
This demandsfor more effective and efficient ways of searchingthese documents for information.
Especially user-generated content on the web is a growing sourceof huge amounts of data that poses special diffi-culties to IR.
The precise wording is often difficultto predict and current information retrieval (IR)systems are mainly based on the assumption thatthe meaning of a document can be inferred fromthe occurrence or absence of terms in it.
In or-der to yield a good retrieval performance, i.e., re-trieving all relevant documents without retrievingnon-relevant documents, the query has to be for-mulated by the user in an appropriate way.
Blairand Maron (1985) showed that with larger grow-ing document collections, it gets impossible forthe user to anticipate the terms that occur in allrelevant documents, but not in non-relevant ones.The use of semantic knowledge for improvingIR by compensating non-optimal queries has beena field of study for a long time.
First experi-ments on query expansion by Voorhees (1994) us-ing lexical-semantic relations extracted from a lin-guistic knowledge base (LKB), namely WordNet(Fellbaum, 1998), showed sginificant improve-ments in performance only for manually selectedexpansion terms.
The combination of Word-Net with thesauri built from the underlying doc-ument collections by Mandala et al (1998) im-proved the performance on several test collec-tions.
Mandala et al (1998) identified missingrelations, especially cross part of speech relationsand insufficient lexical coverage as reasons for thelow performance improvement when using onlyWordNet.In recent work, collaborative knowledge bases(CKB) like Wikipedia have been used in IR forjudging the document relevance by computing thesemantic relatedness (SR) of queries and docu-ments (Gurevych et al, 2007; Egozi et al, 2008;M?uller and Gurevych, 2008) and have shownpromising results.
These resources have a highcoverage of general and domain-specific terms.They are employed in several SR measures suchas Explicit Semantic Analysis (ESA) (Gabrilovichand Markovitch, 2007) that allow the cross part ofspeech computation of SR and are not restricted tostandard lexical semantic relations.The goal of this paper is to shed light on therole of lexical semantics in IR and the way itcan improve the performance of retrieval systems.There exist different kinds of resources for lexi-cal semantic knowledge and different ways to em-bed this knowledge into IR.
Wikipedia and Wik-tionary, which have been applied in computationalmethods only recently, offer new possibilities toenhance IR.
They have already shown an excel-lent performance in computing the SR of wordpairs (Strube and Ponzetto, 2006; Gabrilovich and1338Markovitch, 2007; Zesch et al, 2008).
However,it is not yet clearly understood, what the most ben-eficial method is to employ SR using these re-sources in IR.
We therefore perform a compara-tive study on an IR benchmark.
We particularlyanalyze the contribution of SR of query and docu-ment terms to this task.
To motivate those exper-iments we first prove that there exists a vocabu-lary gap in the test collection between queries anddocuments and show that the gap can be reducedby using lexical semantic knowledge.
As the vo-cabulary coverage of knowledge bases is a crucialfactor for being effective in IR, we compare thecoverage of Wikipedia, Wiktionary and WordNet.We then analyze the lexical semantic relations thathold among query and document terms and howthey are represented by the values of a SR mea-sure.
Finally, we explore the potential of differentSR-based indicators of document relevance.The remainder of this paper is structured as fol-lows: In Section 2 we give a short overview of theLKBs and CKBs and the measure of SR we em-ploy in this paper.
The test collection we use inour experiments is described in Section 3.
In Sec-tion 4 we analyze the vocabulary of the test collec-tion and determine the coverage of the knowledgebases.
This is followed by the examination of lex-ical semantic relations and the analysis of the SRof query terms in relevant and non-relevant docu-ments in Section 5.2 Knowledge Sources and SemanticRelatedness Measure2.1 Linguistic Knowledge BasesLKBs are mainly created by trained linguists fol-lowing clearly defined guidelines.
Therefore, theircontent is typically of high quality.
This labor andcost intensive approach, however, yields a numberof disadvantages for LKBs:?
their coverage and size are limited;?
they lack domain-specific vocabulary;?
continuous maintenance is often not feasible;?
the content can quickly be out-dated;?
only major languages are typically supported.The most common types of LKBs are (i) dic-tionaries, which alphabetically list words and theirsenses of a certain language along with their def-initions and possibly some additional informationand (ii) thesauri, which group words with similarmeaning together and define further semantic rela-tions between the words, e.g., antonymy.
The mostwidely used LKB is WordNet, which is a com-bination of dictionary and thesaurus.
Since thehypernym and hyponym relations between noungroups form an is-a hierarchy, WordNet can alsobe seen as an ontology.
The current version 3.0 ofWordNet, which we use in our experiments, con-tains over 155,000 English words organized intoalmost 118,000 so called synsets, i.e., groups ofsynonymous words.
WordNet covers mainly gen-eral vocabulary terms and its strongest part is thenoun hierarchy.2.2 Collaborative Knowledge BasesEnabled by the development of Web 2.0 technol-ogy and created by communities of volunteers,CKBs have emerged as a new source of lexicalsemantic knowledge in recent years.
In contrastto LKBs, they are created by persons with di-verse personal backgrounds and fields of exper-tise.
CKBs have the advantage of being freelyavailable unlike many LKBs.
However, the con-tent of CKBs is mainly semi- or unstructured textwhich initially requires the extraction of explicitknowledge that can then be used in computationalmethods.One of the CKBs we use in this paper isWikipedia, a freely available encyclopedia.
It cur-rently contains more than 12 million articles in265 languages.
Besides articles, Wikipedia alsooffers other forms of knowledge that can be usedin computational methods.
This includes the hi-erarchy of article categories (Strube and Ponzetto,2006; Zesch et al, 2007) and links between ar-ticles in the same language (Milne and Witten,2008) and across languages (Sch?onhofen et al,2007; Potthast et al, 2008; Sorg and Cimiano,2008; M?uller and Gurevych, 2008).
Due to itsencyclopedic character, Wikipedia contains manynamed entities and domain-specific terms whichare not found in WordNet.
In our experiments weused the Wikipedia dump of February 6th, 2007.The second CKB we use is Wiktionary which isa multilingual dictionary and an affiliated projectof Wikipedia.
It resembles WordNet by containingsynonym and hyponym information.
It also con-tains information usually not found in LKBs likeabbreviations, compounds, contractions, and theetymology of words.
The 171 language-specific1339editions of Wiktionary contain more than 5 mil-lion entries.
Note that each language-specific edi-tion contains not only entries for words of thatparticular language, but also for words of for-eign languages.
Wiktionary has been used inIR (M?uller and Gurevych, 2008; Bernhard andGurevych, 2009) and other tasks like sentimentanalysis (Chesley et al, 2006) or ontology learn-ing (Weber and Buitelaar, 2006).
In our experi-ments we used the Wiktionary dump of Oct 16,2007.2.3 Semantic Relatedness MeasureA wide range of methods for measuring the SR ofterm pairs are discussed in the literature.
In ourexperiments, we employ ESA as it can be usedwith all three knowledge bases in our experimentsand has shown an excellent performance in re-lated work.
ESA was introduced by Gabrilovichand Markovitch (2007) employing Wikipedia asa knowledge base.
Zesch et al (2008) exploredits performance using Wiktionary and WordNet asknowledge bases.The idea of ESA is to express a term?s mean-ing by computing its relation to Wikipedia articles.Each article title in Wikipedia is referred to as aconcept and the article?s text as the textual repre-sentation of this concept.
A term is representedas a high dimensional concept vector where eachvalue corresponds to the term?s frequency in therespective Wikipedia article.
The SR of two termsis then measured by computing the cosine betweenthe respective concept vectors.
When applyingESA to Wiktionary and WordNet, each word andsynset entry, respectively, is referred to as a dis-tinct concept, and the entry?s information1is usedas the textual representation of the concept.In our experiments, we apply pruning meth-ods as proposed by Gabrilovich and Markovitch(2007) with the goal of reducing noise and com-putational costs.
Wikipedia concepts are not takeninto account where the respective Wikipedia arti-cles have less than 100 words or fewer than 5 in- oroutlinks.
For all three knowledge bases, conceptsare removed from a term?s concept vector if theirnormalized values are below a predefined thresh-old (empirically set to 0.01).1For WordNet, the glosses and example sentences of thesynsets are used.
Wiktionary does not contain glosses for allentries due to instance incompleteness.
Therefore, a concate-nation of selected information from each entry is used.
SeeZesch et al (2008) for details.DocumentsNumber of documents 319115Number of unique terms 400194Ave.
document length 256.23QueriesNumber of queries 50Number of unique terms 117Ave.
query length 2.44Table 1: Statistics of the test data (after prepro-cessing).3 DataFor our study we use parts of the data fromthe HARD track at the TREC 2003 conference2.The document collection consists of newswire textdata in English from the year 1999, drawn fromthe Xinhua News Service (People?s Republic ofChina), the New York Times News Service, andthe Associated Press Worldstream News Service.3As we did not have access to the other documentcollections in the track, we restrict our experi-ments to the newswire text data.From the 50 available topics of that track, weuse only the title field, which consists of a fewkeywords describing the information need of auser.
Table 1 shows some descriptive statistics ofthe documents and topics.
The topics cover gen-eral themes like animal protection, Y2K crisis orAcademy Awards ceremony.
For the preprocess-ing of topics and documents we use tokenization,stopword removal and lemmatization employingthe TreeTagger (Schmid, 1994).
In our study, werely on the relevance assessments performed atTREC to distinguish between relevant and non-relevant documents for each topic.4 Vocabulary MismatchTo confirm the intuition that there exists a vocab-ulary mismatch between queries and relevant doc-uments, we computed the overlap of the terms inqueries and relevant documents.
The results areshown in the column String-based in Table 2.
Av-eraged over all 50 topics, 35.5% of the relevantdocuments do contain all terms of the query, and86.5% contain at least one of the query terms.However, this means that 13.5% of the relevantdocuments do not contain any query term and2http://trec.nist.gov/3AQUAINT Corpus, Linguistic Data Consortium (LDC)catalog number LDC2002T311340Measure String-based SR-Wikipedia SR-Wiktionary SR-WordNetThreshold 0.0 0.05 0.0 0.05 0.0 0.05Ave.
number of documents where all 35.5 91.2 72.2 82.6 65.2 74.8 50.8query terms are matched (in %)Ave. number of documents where at 86.5 100.0 99.1 97.7 97.2 94.9 92.9least one query term is matched (in %)Ave. number of query terms 55.8 95.6 84.0 87.0 76.8 79.2 65.7matched per document (in%)Table 2: Statistics about the matching of the terms of queries and relevant documents.cannot be retrieved by simple string-matching re-trieval methods.
In average, each relevant docu-ment matches 55.8% of the query terms.
With anaverage query length of 2.44 (see Table 1), thismeans that in general, only one of two query termsoccurs in the relevant documents which signifi-cantly lowers the probability of these documentsto have a high ranking in the retrieval result.In a second experiment, we proved the effec-tiveness of the SR measure and knowledge basesin reducing the vocabulary gap by counting thenumber of query terms that match the terms inthe relevant documents as string or are seman-tically related to them.
The results are shownin Table 2 for the different knowledge bases inthe columns SR-Wikipedia, SR-Wiktionary and SR-WordNet.
In order to analyse the performance ofthe SR measure when excluding very low SR val-ues that might be caused by noise, we additionallyapplied a threshold of 0.05, i.e.
only values abovethis threshold were taken into account.
The SRvalues range between 0 and 1.
However, the ma-jority of SR values lie between 0 and 0.1.Without threshold, using Wikipedia as knowl-edge base, in 91.2% of the relevant documents allquery terms were matched.
For Wiktionary with82.6% and WordNet with 74.8% the number islower, but still more than twice as high as for thestring-based matching.
Wikipedia matches in allrelevant documents at least one query term.
Theaverage number of query terms matched per doc-ument is also increased for all three knowledgebases.
Applying a threshold of 0.05, the values de-crease, but are still above the ones for string-basedmatching.The sufficient coverage of query and documentterms is crucial for the effectiveness of knowledgebases in IR.
It was found that LKBs do not nec-essarily provide a sufficient coverage (Mandala etal., 1998).
Table 3 shows the amount of termsin queries and documents that are contained inWikipedia, Wiktionary and WordNet.
WikipediaSR- SR- SR-Wikipedia Wiktionary WordNetQueriesPercentage of queries where 98.0 78.0 62.0all terms are coveredPercentage of 99.2 89.3 80.3covered termsPercentage of covered 99.1 88.9 80.3unique termsAve.
percentage of covered 99.6 89.2 80.1terms per queryAve.
percentage of covered 99.6 89.2 80.1unique terms per queryDocumentsPercentage of documents where 7.9 0.3 0.2all terms are coveredPercentage of 96.5 88.5 84.3covered termsPercentage of covered 34.5 12.9 10.0unique termsAve.
Percentage of terms 97.4 91.8 88.8covered per documentAve.
percentage of covered 96.3 88.0 83.6unique terms per documentTable 3: Statistics about the coverage of theknowledge bases.contains almost all query terms and also shows thebest coverage for the document terms, followedby Wiktionary and WordNet.
The values for allthree knowledge bases are all higher than 80% ex-cept for the percentage of queries or documentswhere all terms are covered and the number ofcovered unique terms.
The low percentage of cov-ered unique document terms for even Wikipedia ismostly due to named entities, misspellings, identi-fication codes and compounds.Judging from the number of covered queryand document terms alone, one would expectWikipedia to yield a better performance when ap-plied in IR than Wiktionary and especially Word-Net.
The higher coverage of Wikipedia is due to itsnature of being an encyclopedia featuring arbitrar-ily long articles whereas entries in WordNet, andalso Wiktionary, have a rather short length follow-ing specific guidelines.
The high coverage aloneis however not the only important factor for the ef-fectiveness of a resource.
It was shown by Zesch etal.
(2008) that Wiktionary outperforms Wikipedia1341in the task of ranking word pairs by their seman-tic relatedness when taking into account only wordpairs that are covered by both resources.5 Comparison of Semantic Relatednessin Relevant and Non-RelevantDocumentsWe have shown in Section 4 that a mismatch be-tween the vocabulary of queries and relevant doc-uments exists and that the SR measure and knowl-edge bases can be used to address this gap.
In or-der to further study the SR of query and documentterms with the goal to find SR-based indicators fordocument relevance, we created sets of relevantand non-relevant documents and compared theircharacteristic values concerning SR.5.1 Document SelectionFor analysing the impact of SR in the retrieval pro-cess, we compare relevant and non-relevant docu-ments that were assigned similar relevance scoresby a standard IR system.
For the document selec-tion we followed a method employed by Vechto-mova et al (2005).
We created two sets of docu-ments for each topic: one for relevant and one fornon-relevant documents.
We first retrieved up to1000 documents for the topic using the BM25 IRmodel4(Sp?arck Jones et al, 2000) as implementedby Terrier5.
The relevant retrieved documents con-stituted the first set.
For the second set we se-lected for each relevant retrieved document a non-relevant document which had the closest score tothe relevant document.
After selecting an equalnumber of relevant and non-relevant documents,we computed the mean average and the standarddeviation for the scores of each set.
If there was asubstantial difference between the values of morethan 20%, the sets were rearranged by exchang-ing non-relevant documents or excluding pairs ofrelevant and non-relevant documents.
If this wasnot possible, we excluded the corresponding topicfrom the experiments.Table 4 shows the statistics for the final sets.From the original 50 topics, 13 were excluded forthe above stated reasons or because no relevantdocuments were retrieved.
The average length ofabout 345 terms for relevant documents is almost40% larger than the length of non-relevant docu-4We used the default values for the constants of the model(k1= 1.2, b = 0.75).5http://ir.dcs.gla.ac.uk/terrier/Rel.
Nonrel.
Diff.
(%)Number of queries 37 37 0Number of documents 1771 1771 0Mean BM25 6.388 6.239 2.39document scoreStdev BM25 1.442 1.288 12.00document scoreAve.
query length 2.32 2.32 0Ave.
document length 345.22 248.89 38.70Ave.
query term in- 6.93 4.64 49.35stances in documentsTable 4: Data characteristics that are independentof the chosen knowledge base and threshold.ments.
Also the average number of query term in-stances is 6.93 in cotrast to 4.64 for non-relevantdocuments.
The large difference of average doc-ument length and query term instances suggests alarger difference of the average relevance scoresthan 20%.
However, in the BM25 model the rele-vance score is decreased with increasing documentlength and additional occurrences of a query termhave little impact after three or four occurrences.5.2 Types of Lexical Semantic RelationsThe most common classical lexical semantic re-lations between words are synonymy, hyponymyand a couple of others.
In order to analyze theimportance of these relations in the retrieval pro-cess, we automatically annotated the relations thathold between query and document terms usingWordNet.
Table 5 shows the percentage of lex-ical semantic relations between query and docu-ment terms (normalized by the number of queryand document terms).
The table also shows thecoverage of the relations by the SR measure, i.e.the percentage of annotated relations for whichthe SR measure computed a value above 0 or thethreshold 0.05, respectively.
The percentage of re-lation types in general is higher for relevant doc-uments.
Cohyponymy and synonymy are by farthe most frequently occurring relation types withup to almost 6%.
Hypernyms and hyponyms haveboth a percentage of less than 1%.
Holonymy andmeronymy do almost not occur.When applying no threshold, the SR measurecovers up to 21% of the synonyms and cohy-ponyms and up to 12% of the hyper- and hy-ponyms in relevant documents.
Using Wiktionaryas knowledge base, the SR measure shows a bet-ter coverage than with Wikipedia.
This is con-sistent with the findings in Zesch et al (2008).1342SR-Wikipedia SR-Wiktionary SR-WordNetRelation Type Percentage 0.0 0.05 0.0 0.05 0.0 0.05Relevant Documentssynonymy 3.61 17.81 13.13 18.33 13.78 15.28 12.18hypernymy 0.86 8.57 2.30 12.18 3.02 11.69 2.26hyponymy 0.88 5.72 1.28 6.33 1.67 6.54 1.02cohyponymy 5.64 19.49 10.49 21.04 10.05 16.85 8.14holonymy 0.02 0.61 0.17 0.74 0.17 0.53 0.00meronymy 0.07 1.94 0.78 2.23 0.74 1.88 0.76non-classical ?
58.80 6.62 23.22 3.13 12.77 2.56Non-Relevant Documentssynonymy 3.41 15.84 12.41 16.46 12.90 14.19 11.44hypernymy 0.56 6.10 1.95 9.43 2.10 8.93 1.57hyponymy 0.74 4.77 1.00 6.35 1.40 5.90 0.78cohyponymy 5.42 17.42 9.91 19.23 9.71 15.38 7.66holonymy 0.02 0.39 0.09 0.49 0.09 0.32 0.00meronymy 0.10 1.88 0.55 1.84 0.65 1.57 0.57non-classical ?
57.33 5.54 21.92 2.59 11.77 2.15Table 5: Percentage of lexical semantic relations between query and document terms and their coverageby SR scores above threshold 0.00 and 0.05 in percent.The reason for this is the method for construct-ing the textual representation of the concepts inthe SR measure, where synonyms and other re-lated words are concatenated.
Also SR-WordNetoutperforms Wikipedia for hypernymy and hy-ponymy.
In contrast to Wiktionary, no direct in-formation about related words is used to constructthe textual representation of concepts.
However,the very short and specific representations are builtfrom glosses and examples which often containhypernym-hyponym pairs.
As WordNet is used forboth, the automatic annotation of lexical seman-tic relations and the computation of SR values, itslower term coverage in general has not much im-pact on this experiment, as only the relations be-tween terms contained in WordNet are annotated.More than half of the SR values usingWikipedia are computed for term pairs which werenot annotated with a classical relation.
This is de-picted in Table 5 as non-classical relation.
Thesenon-classical relations can be for example func-tional relations (pencil and paper) (Budanitskyand Hirst, 2006).
However, as WordNet coversonly a small part of the terms in the test collec-tion, some of the SR values refered to as non-classical relations might actually be classical re-lations.
For Wiktionary and WordNet, the num-ber of non-classical relations is much lower, dueto their smaller size and the way the textual rep-resentations of concepts are constructed.
In gen-eral, the average number of SR scores for classicaland non-classical relations are almost consistentlyhigher for relevant documents which suggests thatthe comparison of SR scores could be beneficial inSR-Wikipedia SR-Wiktionary SR-WordNetRelation Type 0.0 0.05 0.0 0.05 0.0 0.05Relevant Documentssynonymy 0.362 0.371 0.372 0.374 0.366 0.368hypernymy 0.021 0.021 0.021 0.019 0.016 0.019hyponymy 0.017 0.021 0.008 0.012 0.007 0.015cohyponymy 0.270 0.334 0.315 0.353 0.312 0.363holonymy 0.001 0.000 0.001 0.000 0.000 0.000meronymy 0.004 0.003 0.003 0.002 0.003 0.002non-classical 0.045 0.356 0.098 0.491 0.205 0.599Non-Relevant Documentssynonymy 0.344 0.348 0.349 0.350 0.343 0.344hypernymy 0.027 0.030 0.025 0.029 0.022 0.028hyponymy 0.019 0.029 0.012 0.023 0.009 0.025cohyponymy 0.250 0.295 0.277 0.312 0.295 0.334holonymy 0.001 0.000 0.000 0.000 0.000 0.000meronymy 0.003 0.002 0.002 0.002 0.003 0.002non-classical 0.041 0.374 0.103 0.538 0.222 0.643Table 6: Average values of SR scores correspond-ing to lexical semantic relations between queryand document terms above threshold 0.00 and 0.05in percent.the IR process.When applying a threshold of 0.05, the mostvisible effect is that the percentage of non-classical relations is decreasing much strongerthan the percentage of classical relations.
Thecomparison of the average SR values for each re-lation type in Table 6 confirms that this is due tothe fact that the SR measure assigns on averagehigher values to the classical relations than to thenon-classical relations.
After applying a thresh-old of 0.05 the average SR values correspondingto non-classical relations increase and are equal toor higher than the values for classical relations.The values for classical relations are in generalhigher for relevant documents, whereas the valuesfor non-classical relations are lower.13435.3 SR-based Indicators for DocumentRelevanceFor each topic and document in one of the sets wecomputed the SR between the query and documentterms.
We then computed the arithmetic mean ofthe following characteristic values for each set: thesum of SR scores, the number of SR scores, thenumber of terms which are semantically related toa query term and the average SR score.
In orderto eliminate the difference in document length andaverage number of query term instances betweenthe relevant and non-relevant sets, we normalizedall values, except for the average SR score, by thedocument length and excluded the SR scores ofquery term instances.Figure 1 shows the average difference of thesevalues between relevant and non-relevant docu-ment sets for SR-thresholds from 0 to 0.6 (step-size=0.01).
As the majority of the SR scores havea low value, there is not much change for thresh-olds above 0.5.Except for the average SR score, the differenceshave a peak at thresholds between 0.01 and 0.09and decrease afterwards to a constant value.
TheSR scores computed using Wikipedia show thehighest differences.
Wiktionary and WordNet per-form almost equally, but show lower differencesthan Wikipedia, especially for the sum of scores.All three knowledge bases show higher differencesfor the number of scores and number of relatedterms than for the sum of scores.
The differencesat the peaks are statistically significant6, except forthe differences of the sum of scores for Wiktionaryand WordNet.For the average SR score, the differences aremostly negative at low thresholds and increase to alow positive value for higher thresholds.
A highernumber of very low SR values is computed for therelevant documents, which causes the average SRscore to be lower than for the non-relevant docu-ments at low thresholds.Additionally, Figure 2 shows the percentage oftopics where the mean value of the relevant docu-ment set is higher than the one of the non-relevantdocument set.
Wikipedia shows the highest per-centage with about 86% for the number of scoresand the number of related terms.
Wiktionary andWordNet have a low percentage for the sum ofscores, but reach up to 75% for the number ofscores and the number of related terms.6We used the Wilcoxon test at a significance level of 0.05.The analysis of the SR of query and documentterms shows that there are significant differencesfor relevant and non-relevant documents that canbe measured by computing SR scores with any ofthe three knowledge bases.
Especially when us-ing Wiktionary and WordNet, the number of SRscores and the number of related terms might bebetter indicators for the document relevance thanthe sum of SR scores.6 ConclusionsThe vocabulary mismatch of queries and docu-ments is a common problem in IR, which becomeseven more serious the larger the document collec-tion grows.
CKBs like Wikipedia and Wiktionary,which have been applied in computational meth-ods only recently, offer new possibilities to tacklethis problem.
In order to find the most beneficialway to employ these resources, we studied the se-mantic relatedness of query and document termsof an IR benchmark and compared the character-istics and performance of the CKBs Wikipedia andWiktionary to the LKB WordNet.We first proved that there exists a vocabularygap in the test collection between queries and doc-uments and that it can be reduced by employing aconcept vector based measure for SR with any ofthe three knowledge bases.
Using WordNet to au-tomatically annotate the lexical semantic relationsof query and document terms, we found that cohy-ponymy and synonymy are the most frequent clas-sical relation types.
Although the percentage ofannotated relations for which also the SR measurecomputed values above a predefined threshold wasat best 21%, the average number of SR scores forclassical and non-classical relations were almostconsistently higher for relevant documents.Comparing the number and the value of SRscores of query and document terms, a significantdifference between relevant and non-relevant doc-uments was observed by using any of the threeknowledge bases.
Although Wikipedia had thebest coverage of collection terms and showed thebest perfomance in our experiments, Wiktionaryand Wikipedia also seem to have a sufficientsize for being beneficial in IR.
In comparison toour previous work where the sum of SR scoreswas used as an indicator for document relevance(M?uller and Gurevych, 2008), the results suggestthat the number of SR scores and the number ofrelated terms might show a better performance, es-1344Figure 1: Differences between mean values of relevant and non-relevant document sets.Figure 2: Percentage of topics where the mean value of the relevant document sets is higher than the oneof the non-relevant document sets.1345pecially for Wiktionary and WordNet.In our future work, we plan to extend our analy-sis to other test collections and to query expansionmethods in order to generalize our conclusions.As the problem of language ambiguity has a highimpact on the use of SR measures, we will alsoconsider word sense disambiguation in our futureexperiments.AcknowledgmentsThis work was supported by the VolkswagenFoundation as part of the Lichtenberg-Professor-ship Program under grant No.
I/82806 and by theGerman Research Foundation under grant No.
GU798/1-3.
We would like to thank Aljoscha Bur-chardt for his helpful comments and the anony-mous reviewers for valuable feedback on this pa-per.ReferencesD.
Bernhard and I. Gurevych.
2009.
Combininglexical semantic resources with question & answerarchives for translation-based answer finding.
InProceedings of the Joint conference of the 47th An-nual Meeting of the Association for ComputationalLinguistics and the 4th International Joint Confer-ence on Natural Language Processing of the AsianFederation of Natural Language Processing, Singa-pore, Aug.D.
C. Blair and M. E. Maron.
1985.
An evaluationof retrieval effectiveness for a full-text document-retrieval system.
Commun.
ACM, 28(3):289?299.A.
Budanitsky and G. Hirst.
2006.
EvaluatingWordNet-based Measures of Semantic Distance.Computational Linguistics, 32(1):13?47.P.
Chesley, B. Vincent, L. Xu, and R. Srihari.
2006.Using Verbs and Adjectives to Automatically Clas-sify Blog Sentiment.
In Proceedings of AAAI-CAAW-06.O.
Egozi, E. Gabrilovich, and S. Markovitch.
2008.Concept-Based Feature Generation and Selectionfor Information Retrieval.
In Proceedings of theTwenty-Third AAAI Conference on Artificial Intelli-gence, Chicago, IL.C.
Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.E.
Gabrilovich and S. Markovitch.
2007.
ComputingSemantic Relatedness using Wikipedia-based Ex-plicit Semantic Analysis.
In Proceedings of TheTwentieth International Joint Conference for Artifi-cial Intelligence, pages 1606?1611, Hyderabad, In-dia.I.
Gurevych, C. M?uller, and T. Zesch.
2007.
What tobe?
- Electronic Career Guidance Based on Seman-tic Relatedness.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Lin-guistics, pages 1032?1039, Prague, Czech Republic,June.R.
Mandala, T. Tokunaga, and H. Tanaka.
1998.
TheUse of WordNet in Information Retrieval.
In SandaHarabagiu, editor, Proceedings of the COLING-ACLworkshop on Usage of WordNet in Natural Lan-guage Processing, pages 31?37.
Association forComputational Linguistics, Somerset, New Jersey.D.
Milne and I. Witten.
2008.
An effective, low-cost measure of semantic relatedness obtained fromwikipedia links.
In Wikipedia and AI workshop atthe AAAI-08 Conference (WikiAI08), Chicago, USA.C.
M?uller and I. Gurevych.
2008.
Using Wikipediaand Wiktionary in Domain-Specific Information Re-trieval.
In F. Borri, A. Nardi, and C. Peters, edi-tors, Working Notes for the CLEF 2008 Workshop,Aarhus, Denmark, Sep.M.
Potthast, B. Stein, and M. Anderka.
2008.
AWikipedia-Based Multilingual Retrieval Model.
InC. Macdonald, I. Ounis, V. Plachouras, I. Ruthven,and R. W. White, editors, 30th European Conferenceon IR Research, ECIR 2008, Glasgow, volume 4956of LNCS, pages 522?530.
Springer.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of Conferenceon New Methods in Language Processing.P.
Sch?onhofen, I. Biro, A.
A. Benczur, and K. Csalo-gany.
2007.
Performing Cross Language Retrievalwith Wikipedia.
In Working Notes for the CLEF2007 Workshop.P.
Sorg and P. Cimiano.
2008.
Cross-lingual Informa-tion Retrieval with Explicit Semantic Analysis.
InF.
Borri, A. Nardi, and C. Peters, editors, WorkingNotes for the CLEF 2008 Workshop, Aarhus, Den-mark, Sep.K.
Sp?arck Jones, S. Walker, and S. E. Robertson.
2000.A probabilistic model of information retrieval: de-velopment and comparative experiments.
Informa-tion Processing and Management, 36(6):779?808(Part 1); 809?840 (Part 2).M.
Strube and S. P. Ponzetto.
2006.
WikiRelate!
Com-puting Semantic Relatedness Using Wikipedia.
InProceedings of AAAI, pages 1419?1424.O.
Vechtomova, M. Karamuftuoglu, and S. E. Robert-son.
2005.
A Study of Document Relevance andLexical Cohesion between Query Terms.
In Pro-ceedings of the Workshop on Methodologies andEvaluation of Lexical Cohesion Techniques in Real-World Applications (ELECTRA 2005), the 28th An-nual International ACM SIGIR Conference, pages18?25, Salvador, Brazil, August.1346E.
M. Voorhees.
1994.
Query expansion using lexical-semantic relations.
In SIGIR ?94: Proceedingsof the 17th annual international ACM SIGIR con-ference on Research and development in informa-tion retrieval, pages 61?69, New York, NY, USA.Springer-Verlag New York, Inc.N.
Weber and P. Buitelaar.
2006.
Web-based OntologyLearning with ISOLDE.
In Proc.
of the Workshopon Web Content Mining with Human Language atthe International Semantic Web Conference, AthensGA, USA, 11.T.
Zesch, I. Gurevych, and M. M?uhlh?auser.
2007.Comparing Wikipedia and German Wordnet byEvaluating Semantic Relatedness on MultipleDatasets.
In Proceedings of HLT-NAACL, pages205?208.T.
Zesch, C. M?uller, and I. Gurevych.
2008.
Us-ing Wiktionary for Computing Semantic Related-ness.
In Proceedings of the Twenty-Third AAAI Con-ference on Artificial Intelligence, AAAI 2008, pages(861?867), Chicago, Illinois, USA.1347
