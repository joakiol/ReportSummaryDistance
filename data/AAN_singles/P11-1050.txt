Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 491?499,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsDiscovery of Topically Coherent Sentences for Extractive SummarizationAsli CelikyilmazMicrosoft Speech LabsMountain View, CA, 94041asli@ieee.orgDilek Hakkani-Tu?rMicrosoft Speech Labs |Microsoft ResearchMountain View, CA, 94041dilek@ieee.orgAbstractExtractive methods for multi-document sum-marization are mainly governed by informa-tion overlap, coherence, and content con-straints.
We present an unsupervised proba-bilistic approach to model the hidden abstractconcepts across documents as well as the cor-relation between these concepts, to generatetopically coherent and non-redundant sum-maries.
Based on human evaluations our mod-els generate summaries with higher linguisticquality in terms of coherence, readability, andredundancy compared to benchmark systems.Although our system is unsupervised and opti-mized for topical coherence, we achieve a 44.1ROUGE on the DUC-07 test set, roughly in therange of state-of-the-art supervised models.1 IntroductionA query-focused multi-document summarizationmodel produces a short-summary text of a set ofdocuments, which are retrieved based on a user?squery.
An ideal generated summary text should con-tain the shared relevant content among set of doc-uments only once, plus other unique informationfrom individual documents that are directly relatedto the user?s query addressing different levels of de-tail.
Recent approaches to the summarization taskhas somewhat focused on the redundancy and co-herence issues.
In this paper, we introduce a seriesof new generative models for multiple-documents,based on a discovery of hierarchical topics and theircorrelations to extract topically coherent sentences.Prior research has demonstrated the usefulnessof sentence extraction for generating summary texttaking advantage of surface level features such asword repetition, position in text, cue phrases, etc,(Radev, 2004; Nenkova and Vanderwende, 2005a;Wan and Yang, 2006; Nenkova et al, 2006).
Be-cause documents have pre-defined structures (e.g.,sections, paragraphs, sentences) for different levelsof concepts in a hierarchy, most recent summariza-tion work has focused on structured probabilisticmodels to represent the corpus concepts (Barzilayet al, 1999; Daume?-III and Marcu, 2006; Eisensteinand Barzilay, 2008; Tang et al, 2009; Chen et al,2000; Wang et al, 2009).
In particular (Haghighiand Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010) build hierarchical topic models to iden-tify salient sentences that contain abstract conceptsrather than specific concepts.
Nonetheless, all thesesystems crucially rely on extracting various levels ofgenerality from documents, focusing little on redun-dancy and coherence issues in model building.
Amodel than can focus on both issues is deemed to bemore beneficial for a summarization task.Topical coherence in text involves identifying keyconcepts, the relationships between these concepts,and linking these relationships into a hierarchy.
Inthis paper, we present a novel, fully generativeBayesian model of document corpus, which can dis-cover topically coherent sentences that contain keyshared information with as little detail and redun-dancy as possible.
Our model can discover hierar-chical latent structure of multi-documents, in whichsome words are governed by low-level topics (T)and others by high-level topics (H).
The main con-tributions of this work are:?
construction of a novel bayesian framework to491capture higher level topics (concepts) related to sum-mary text discussed in ?3,?
representation of a linguistic system as a sequenceof increasingly enriched models, which use posteriortopic correlation probabilities in sentences to designa novel sentence ranking method in ?4 and 5,?
application of the new hierarchical learningmethod for generation of less redundant summariesdiscussed in ?6.
Our models achieve compara-ble qualitative results on summarization of multiplenewswire documents.
Human evaluations of gener-ated summaries confirm that our model can generatenon-redundant and topically coherent summaries.2 Multi-Document Summarization ModelsPrior research has demonstrated the usefulness ofsentence extraction for summarization based on lex-ical, semantic, and discourse constraints.
Suchmodels often rely on different approaches includ-ing: identifying important keywords (Nenkova et al,2006); topic signatures based on user queries (Linand Hovy, 2002; Conroy et al, 2006; Harabagiuet al, 2007); high frequency content word featurebased learning (Nenkova and Vanderwende, 2005a;Nenkova and Vanderwende, 2005b), to name a few.Recent research focusing on the extraction of la-tent concepts from document clusters are close inspirit to our work (Barzilay and Lee, 2004; Daume?-III and Marcu, 2006; Eisenstein and Barzilay, 2008;Tang et al, 2009; Wang et al, 2009).
Some of thesework (Haghighi and Vanderwende, 2009; Celikyil-maz and Hakkani-Tur, 2010) focus on the discov-ery of hierarchical concepts from documents (fromabstract to specific) using extensions of hierarchaltopic models (Blei et al, 2004) and reflect this hier-archy on the sentences.
Hierarchical concept learn-ing models help to discover, for instance, that ?base-ball?
and ?football?
are both contained in a generalclass ?sports?, so that the summaries reference termsrelated to more abstract concepts like ?sports?.Although successful, the issue with concept learn-ing methods for summarization is that the extractedsentences usually contain correlated concepts.
Weneed a model that can identify salient sentences re-ferring to general concepts of documents and thereshould be minimum correlation between them.Our approach differs from the early work, in that,we utilize the advantages of previous topic modelsand build an unsupervised generative model that canassociate each word in each document with threerandom variables: a sentence S, a higher-level topicH, and a lower-level topic T, in an analogical wayto PAM models (Li and McCallum, 2006), i.e., a di-rected acyclic graph (DAG) representing mixtures ofhierarchical structure, where super-topics are multi-nomials over sub-topics at lower levels in the DAG.We define a tiered-topic clustering in which the up-per nodes in the DAG are higher-level topics H, rep-resenting common co-occurence patterns (correla-tions) between lower-level topics T in documents.This has not been the focus in prior work on genera-tive approaches for summarization task.
Mainly, ourmodel can discover correlated topics to eliminate re-dundant sentences in summary text.Rather than representing sentences as a layer inhierarchical models, e.g., (Haghighi and Vander-wende, 2009; Celikyilmaz and Hakkani-Tur, 2010),we model sentences as meta-variables.
This is sim-ilar to author-topic models (Rosen-Zvi et al, 2004),in which words are generated by first selecting anauthor uniformly from an observed author list andthen selecting a topic from a distribution over topicsthat is specific to that author.
In our model, wordsare generated from different topics of documents byfirst selecting a sentence containing the word andthen topics that are specific to that sentence.
Thisway we can directly extract from documents thesummary related sentences that contain high-leveltopics.
In addition in (Celikyilmaz and Hakkani-Tur,2010), the sentences can only share topics if the sen-tences are represented on the same path of capturedtopic hierarchy, restricting topic sharing across sen-tences on different paths.
Our DAG identifies tieredtopics distributed over document clusters that can beshared by each sentence.3 Topic Coherence for SummarizationIn this section we discuss the main contribution,our two hierarchical mixture models, which improvesummary generation performance through the use oftiered topic models.
Our models can identify lower-level topics T (concepts) defined as distributionsover words or higher-level topics H, which representcorrelations between these lower level topics given492sentences.
We present our synthetic experiment formodel development to evaluate extracted summarieson redundancy measure.
In ?6, we demonstrate theperformance of our models on coherence and infor-mativeness of generated summaries by qualitativeand intrinsic evaluations.For model development we use the DUC 2005dataset1, which consists of 45 document clusters,each of which include 1-4 set of human gener-ated summaries (10-15 sentences each).
Each doc-ument cluster consists ?
25 documents (25-30 sen-tences/document) retrieved based on a user query.We consider each document cluster as a corpus andbuild 45 separate models.For the synthetic experiments, we include the pro-vided human generated summaries of each corpusas additional documents.
The sentences in humansummaries include general concepts mentioned inthe corpus, the salient sentences of documents.
Con-trary to usual qualitative evaluations of summariza-tion tasks, our aim during development is to measurethe percentage of sentences in a human summarythat our model can identify as salient among all otherdocument cluster sentences.
Because human pro-duced summaries generally contain non-redundantsentences, we use total number of top-ranked hu-man summary sentences as a qualitative redundancymeasure in our synthetic experiments.In each model, a document d is a vector of Ndwords wd, where each wid is chosen from a vocabu-lary of size V , and a vector of sentences S, represent-ing all sentences in a corpus of size SD.
We identifysentences as meta-variables of document clusters,which the generative process models both sentencesand documents using tiered topics.
A sentence?s re-latedness to summary text is tied to the documentcluster?s user query.
The idea is that a lexical wordpresent or related to a query should increase its sen-tence?s probability of relatedness.4 Two-Tiered Topic Model - TTMOur base model, the two-tiered topic model (TTM),is inspired by the hierarchical topic model, PAM,proposed by Li and McCallum (2006).
PAM struc-tures documents to represent and learn arbitrary,nested, and possibly sparse topic correlations using1www-nlpir.nist.gov/projects/duc/data.html(Background)SpecificContentParameterswSSentencesxTLower-LevelTopics?Summary RelatedWord IndicatorSDK2?HSummaryContentIndicatorParameters?
?TLower-LevelTopicParametersHigher-LevelTopicParametersK1?K2K1?Documents in a Document ClusterNdDocumentSentenceselectoryHigher-LevelTopicsHFigure 1: Graphical model depiction of two-tiered topic model(TTM) described in section ?4.
S are sentences si=1..SD in doc-ument clusters.
The high-level topics (Hk1=1...K1 ), represent-ing topic correlations, are modeled as distributions over low-level-topics (Tk2=1...K2 ).
Shaded nodes indicate observed vari-ables.
Hyper-parameters for ?, ?H , ?T , ?
are omitted.a directed acyclic graph.
Our goals are not so dif-ferent: we aim to discover concepts from documentsthat would attribute for the general topics related to auser query, however, we want to relate this informa-tion to sentences.
We represent sentences S by dis-covery of general (more general) to specific topics(Fig.1).
Similarly, we represent summary unrelated(document specific) sentences as corpus specific dis-tributions ?
over background words wB, (functionalwords like prepositions, etc.
).Our two-tiered topic model for salient sentencediscovery can be generated for each word in the doc-ument (Algorithm 1) as follows: For a word wid indocument d, a random variable xid is drawn, whichdetermines if wid is query related, i.e., wid either ex-ists in the query or is related to the query2.
Oth-erwise, wid is unrelated to the user query.
Thensentence si is chosen uniformly at random (ysi?Uniform(si)) from sentences in the document con-taining wid (deterministic if there is only one sen-tence containing wid).
We assume that if a word isrelated to a query, it is likely to be summary-related2We measure relatedness to a query if a word exists in thequery or it is synonymous based on information extracted fromWordNet (Miller, 1995).493H1H2H3T1T2T3TTTTwB...WWW...H4T4TWSentencesDocumentSpecificWords?SHTTWK1K2T3 :?network?
?retail?C4H1starbucks,coffee, schultz,tazo, pasqua,states, subsidiaryacquire,  bought,purchase,disclose, joint-venture, johnsonstarbucks, coffee,retailer,frappaccinofrancisco, pepsi,area, profit,network, internet,Francisco-basedH2H3T2 :?coffee?T4 :?retail?T1 :?acquisition?High-Level TopicsLow-LevelTopicsFigure 2: Depiction of TTM given the query ?D0718D: Star-bucks Coffee : How has Starbucks Coffee attempted to ex-pand and diversify through joint ventures, acquisitions, orsubsidiaries??.
If a word is query/summary related sentenceS, first a sentence then a high-level (H) and a low-level (T )topic is sampled.
(Crepresents that a random variable is aparent of all C random variables.)
The bolded links fromH?Trepresent correlated low-level topics.
(so as the sampled sentence si).
We keep track ofthe frequency of si?s in a vector, DS ?
ZSD .
Ev-ery time an si is sampled for a query related wid, weincrement its count, a degree of sentence saliency.Given that wid is related to a query, it is as-sociated with two-tiered multinomial distributions:high-level H topics and low-level T topics.
A high-level topic Hki is chosen first from a distributionover low-level topics T specific to that si and onelow-level topic Tkj is chosen from a distributionover words, and wid is generated from the sampledlow-level topic.
If wid is not query-related, it is gen-erated as a background word wB .The resulting tiered model is shown as a graphand plate diagrams in Fig.1 & 2.
A sentence sampledfrom a query related word is associated with a dis-tribution over K1 number of high-level topics Hki ,each of which are also associated with K2 numberof low-level topics Tkj , a multinomial over lexicalwords of a corpus.
In Fig.2 the most confident wordsof four low-level topics is shown.
The bolded linksbetween Hki and Tkj represent the strength of cor-Algorithm 1 Two-Tiered Topic Model Generation1: Sample: si = 1..SD: ?
?
Beta(?
),2: k1 = 1...K1: ?H ?
Dirichlet(?H),3: k2 = 1...K1 ?K2: ?T ?
Dirichlet(?T ),4: and k = 1..K2: ?
?
Dirichlet(?
).5: for documents d?
1, ..., D do6: for words wid, i?
1, ..., Nd do7: - Draw a discrete x ?
Binomial(?wid)?8: - If x = 1, wid is summary related;9: ?
conditioned on S draw a sentence10: ysi ?
Uniform(si) containing wi,11: ?
sample a high-level topicHk1 ?
?Hk1(?H),12: and a low-level topic Tk2 ?
?Tk2(?T ),13: ?
sample a word wik1k2 ?
?Hk1Tk2 (?
),14: - If x = 0, the word is unrelated ?
?15: sample a word wB ?
?(?
),16: corpus specific distribution.17: end for18: end for?
if wid exists or related to the the query then x = 1 deterministic,otherwise it is stochastically assigned x ?
Bin(?).??
wid is a background word.relation between Tkj ?s, e.g., the topic ?acquisition?is found to be more correlated with ?retail?
than the?network?
topic given H1.
This information is usedto rank sentences based on the correlated topics.4.1 Learning and Inference for TTMOur learning procedure involves finding parame-ters, which likely integrates out model?s posteriordistribution P (H,T|Wd,S), d?D.
EM algorithmsmight face problems with local maxima in topicmodels (Blei et al, 2003) suggesting implementa-tion of approximate methods in which some of theparameters, e.g., ?H , ?T , ?, and ?, can be integratedout, resulting in standard Dirichlet-multinomial aswell as binomial distributions.
We use Gibbs sam-pling which allows a combination of estimates fromseveral local maxima of the posterior distribution.For each word, xid is sampled from a sentencespecific binomial ?
which in turn has a smooth-ing prior ?
to determine if the sampled word wid is(query) summary-related or document-specific.
De-pending on xid, we either sample a sentence alongwith a high/low-level topic pair or just sample back-ground words wB .
The probability distribution oversentence assignments, P (ysi = s|S) si ?
S, is as-sumed to be uniform over the elements of S, and de-terministic if there is only one sentence in the docu-494ment containing the corresponding word.
The opti-mum hyper-parameters are set based on the trainingdataset model performance via cross-validation 3.For each word we sample a high-level Hki anda low-level Tkj topic if the word is query related(xid = 1).
The sampling distribution for TTMfor a word given the remaining topics and hyper-parameters ?H , ?T , ?, ?, ?
is:pTTM(Hk1 , Tk2 , x = 1|w,H?k1 ,T?k2) ?
?H + nk1d?H?
?H?
+ nd?
?T + nk1k2d?T ?
?T ?
+ ndH??
+ nk1k2x2?
+ nk1k2?
?w + nwk1k2x?w?
?w?
+ nk1k2xand when x = 0 (a corpus specific word),pTTM(x = 0|w, zH?k, zt?k) ??
+ nxk1k22?
+ nk1k2?
?w + nw?w?
?w?
+ nThe nk1d is the number of occurrences of high-leveltopic k1 in document d, and nk1k2d is the number oftimes the low-level topic k2 is sampled together withhigh-level topic k1 in d, nwk1k2x is the number of oc-currences of word w sampled from path H-T giventhat the word is query related.
Note that the numberof tiered topics in the model is fixed to K1 and K2,which is optimized with validation experiments.
Itis also possible to construct extended models of TTMusing non-parametric priors, e.g., hierarchal Dirich-let processes (Li et al, 2007) (left for future work).4.2 Summary Generation with TTMWe can observe the frequency of draws of every sen-tence in a document cluster S, given it?s words arerelated, through DS ?
ZSD .
We obtain DS duringGibbs sampling (in ?4.1), which indicates a saliencyscore of each sentence sj ?
S, j = 1..SD:scoreTTM(sj) ?
# [wid ?
sj , xid = 1] /nwj (1)where wid indicates a word in a document d that ex-ists in sj and is sampled as summary related basedon random indicator variable xid.
nwj is the num-ber of words in sj and normalizes the score favoring3An alternative way would be to use Dirichlet priors (Blei etal., 2003) which we opted for due to computational reasons butwill be investigated as future research.sentences with many related words.
We rank sen-tences based on (1).
We compare TTM results onsynthetic experiments against PAM (Li and McCal-lum, 2006) a similar topic model that clusters topicsin a hierarchical structure, where super-topics aredistributions over sub-topics.
We obtain sentencescores for PAM models by calculating the sub-topicsignificance (TS) based on super-topic correlations,and discover topic correlations over the entire docu-ment space (corpus wide).
Hence; we calculate theTS of a given sub-topic, k = 1, ..,K2 by:TS(zk) =1D?d?D1K1K1?k1p(zksub|zk1sup) (2)where zksub is a sub-topic k = 1..K2 and zk1sup is asuper-topic k1.
The conditional probability of a sub-topic k given a super-topic k1, p(zksub|zk1sup), explainsthe variation of that sub-topic in relation to othersub-topics.
The higher the variation over the entirecorpus, the better it represents the general theme ofthe documents.
So, sentences including such topicswill have higher saliency scores, which we quantifyby imposing topic?s significance on vocabulary:scorePAM(si) =1K2K2?k?w?sip(w|zksub) ?
TS(zk)(3)Fig.
4 illustrates the average salience sentence se-lection performance of TTM and PAM models (for45 models).
The x-axis represents the percentage ofsentences selected by the model among all sentencesin the DUC2005 corpus.
100% means all sentencesin the corpus included in the summary text.
They-axis is the % of selected human sentences overall sentences.
The higher the human summary sen-tences are ranked, the better the model is in select-ing the salient sentences.
Hence, the system whichpeaks sooner indicates a better model.In Fig.4 TTM is significantly better in identifyinghuman sentences as salient in comparison to PAM.The statistical significance is measured based on thearea under the curve averaged over 45 models.5 Enriched Two-Tiered Topic ModelOur model can discover words that are related tosummary text using posteriors P?
(?H) and P?
(?T ),495?acquisition??coffee?
?network?H2T,WH?retail?seattle,acquire, sales,billion...coffee,starbucks...purchase,disclose,joint-venture,johnsonschultz, tazo,pasqua,states,subsidiarypepsi, area,profit,networkfranciscofrappaccino,retailer,mocca,organicT2T,WHHigh-Level TopicsH1WLT1T3T4Low-Level TopicsLow-Level TopicsL=2L=2L=2L=2L=1L=1?LIndicatorWordLevel(Background)SpecificContentParameterswSSentencesxTHLower-LevelTopicsHigher-LevelTopicsSummary RelatedWord IndicatorSD?HSummaryContentIndicatorParameters?
?TLower-LevelTopicParametersHigher-LevelTopicParametersSentenceselectorK1?K2K1y?Documents in a Document ClusterNdDocument?K1+K2WLWLWLFigure 3: Graphical model depiction of sentence level enriched two-tiered model (ETTM) described in section ?5.
Each pathdefined byH/T pair k1k2, has a multinomial ?
over which level of the path outputs a given word.
L indicates which level, i.e, highor low, the word is sampled from.
On the right is the high-level topic-word and low-level topic-word distributions characterized byETTM.
Each Hk1 also represented as distributions over general words WH as well as indicates the degree of correlation betweenlow-level topics denoted by boldness of the arrows.as well as words wB specific to documents (viaP?
(?))
(Fig.1).
TTM can discover topic correlations,but cannot differentiate if a word in a sentence ismore general or specific given a query.
Sentenceswith general words would be more suitable to in-clude in summary text compared to sentences con-taining specific words.
For instance for a given sen-tence: ?Starbucks Coffee has attempted to expandand diversify through joint ventures, and acquisi-tions.
?, ?starbucks?
and ?coffee?
are more gen-eral words given the document clusters comparedto ?joint?
and ?ventures?
(see Fig.2), because theyappear more frequently in document clusters.
How-ever, TTM has no way of knowing that ?starbucks?and ?coffee?
are common terms given the context.We would like to associate general words with high-level topics, and context specific words with low-level topics.
Sentence containing words that aresampled from high-level topics would be a bet-ter candidate for summary text.
Thus; we presentenriched TTM (ETTM) generative process (Fig.3),which samples words not only from low-level top-ics but also from high-level topics as well.ETTM discovers three separate distributions overwords: (i) high-level topics H as distributions overcorpus general words WH, (ii) low-level topics Tas distributions over corpus specific words WL, andLevel Generation for Enriched TTMFetch ?k ?
Beta(?
); k = 1...K1 ?K2.For wid, i = 1, ..., Nd, d = 1, ..D:If x = 1, sentence si is summary related;- sample Hk1 and Tk2- sample a level L from Bin(?k1k2)- If L = 1 (general word); wid ?
?Hki- else if L = 2 (context specific); wid ?
?Hk1Tk2else if x = 0, do Step 14-16 in Alg.
1.
(iii) background word distributions, i.e,.
documentspecific WB (less confidence for summary text).Similar to TTM?s generative process, if wid is re-lated to a given query, then x = 1 is determin-istic, otherwise x ?
{0, 1} is stochastically deter-mined if wid should be sampled as a backgroundword (wB) or through hierarchical path, i.e., H-Tpairs.
We first sample a sentence si for wid uni-formly at random from the sentences containing theword ysi?Uniform(si)).
At this stage we sample alevel Lwid ?
{1, 2} for wid to determine if it is ahigh-level word, e.g., more general to context like?starbucks?
or ?coffee?
or more specific to relatedcontext such as ?subsidiary?, ?frappucino?.
Eachpath through the DAG, defined by a H-T pair (totalof K1K2 pairs), has a binomial ?K1K2 over which49610.90.80.70.60.50.40.30.20.10%ofhumangeneratedsentencesusedinthegeneratedsummary0          10          20         30         40         50         60          70         80         90        100% of sentences added to the generated summary text.0.70.60.50.40.30.20.100           2            4           6            8           10ETIMTIMPAMhPAMETIMTIMhPAMPAMTIMETIMPAMHPAMFigure 4: Average saliency performance of four systems over45 different DUC models.
The area under each curve is shownin legend.
Inseam is the magnified view of top-ranked 10% ofsentences in corpus.level of the path outputs sampled word.
If the wordis a specific type, x = 0, then it is sampled from thebackground word distribution ?, a document specificmultinomial.
Once the level and conditional path isdrawn (see level generation for ETTM above) the restof the generative model is same as TTM.5.1 Learning and Inference for ETTMFor each word, x is sampled from a sentence spe-cific binomial ?, just like TTM.
If the word is relatedto the query x = 1, we sample a high and low-leveltopic pair H ?
T as well as an additional level L issampled to determine which level of topics the wordshould be sampled from.
L is a corpus specific bi-nomial one for all H ?
T pairs.
If L = 1, the wordis one of corpus general words and sampled fromthe high-level topic, otherwise (L = 2) the wordis corpus specific and sampled from a the low-leveltopic.
The optimum hyper-parameters are set basedon training performance via cross validation.The conditional probabilities are similar to TTM,but with additional random variables, which deter-mine the level of generality of words as follows:pETTM(Tk1 , Tk2 , L|w,T?k1 ,T?k2 , L) ?pTTM(Tk1 , Tk2 , x = 1|.)
?
?+NLk1k22?+nk1k25.2 Summary Generation with ETTMFor ETTM models, we extend the TTM sentencescore to be able to include the effect of the generalwords in sentences (as word sequences in languagemodels) using probabilities of K1 high-level topicdistributions, ?wHk=1..K1, as:scoreETTM(si) ?
# [wid ?
sj , xid = 1] /nwj ?1K1?k=1..K1?w?sip(w|Tk)where p(w|Tk) is the probability of a word in sibeing generated from high-level topic Hk.
Usingthis score, we re-rank the sentences in documentsof the synthetic experiment.
We compare the re-sults of ETTM to a structurally similar probabilis-tic model, entitled hierarchical PAM (Mimno et al,2007), which is designed to capture topics on a hi-erarchy of two layers, i.e., super topics and sub-topics, where super-topics are distributions over ab-stract words.
In Fig.
4 out of 45 models ETTM hasthe best performance in ranking the human gener-ated sentences at the top, better than the TTM model.Thus; ETTM is capable of capturing focused sen-tences with general words related to the main con-cepts of the documents and much less redundantsentences containing concepts specific to user query.6 Final ExperimentsIn this section, we qualitatively compare our modelsagainst state-of-the art models and later apply an in-trinsic evaluation of generated summaries on topicalcoherence and informativeness.For a qualitative comparison with the previousstate-of-the models, we use the standard summariza-tion datasets on this task.
We train our models on thedatasets provided by DUC2005 task and validate theresults on DUC 2006 task, which consist of a totalof 100 document clusters.
We evaluate the perfor-mance of our models on DUC2007 datasets, whichcomprise of 45 document clusters, each containing25 news articles.
The task is to create max.
250word long summary for each document cluster.6.1.
ROUGE Evaluations: We train each docu-ment cluster as a separate corpus to find the optimumparameters of each model and evaluate on test docu-ment clusters.
ROUGE is a commonly used measure,a standard DUC evaluation metric, which computesrecall over various n-grams statistics from a modelgenerated summary against a set of human generatedsummaries.
We report results in R-1 (recall againstunigrams), R-2 (recall against bigrams), and R-SU4497ROUGE w/o stop words w/ stop wordsR-1 R-2 R-4 R-1 R-2 R-4PYTHY 35.7 8.9 12.1 42.6 11.9 16.8HIERSUM 33.8 9.3 11.6 42.4 11.8 16.7HybHSum 35.1 8.3 11.8 45.6 11.4 17.2PAM 32.1 7.1 11.0 41.7 9.1 15.3hPAM 31.9 7.0 11.1 41.2 8.9 15.2TTM?
34.0 8.7 11.5 44.7 10.7 16.5ETTM?
32.4 8.3 11.2 44.1 10.4 16.4Table 1: ROUGE results of the best systems on DUC2007dataset (best results are bolded.)
?
indicate our models.
(recall against skip-4 bigrams) ROUGE scores w/ andw/o stop words included.For our models, we ran Gibbs samplers for 2000iterations for each configuration throwing out first500 samples as burn-in.
We iterated different valuesfor hyperparameters and measured the performanceon validation dataset to capture the optimum values.The following models are used as benchmark:(i) PYTHY (Toutanova et al, 2007): Utilizes hu-man generated summaries to train a sentence rank-ing system using a classifier model; (ii) HIERSUM(Haghighi and Vanderwende, 2009): Based on hier-archical topic models.
Using an approximation forinference, sentences are greedily added to a sum-mary so long as they decrease KL-divergence of thegenerated summary concept distributions from doc-ument word-frequency distributions.
(iii) HybHSum(Celikyilmaz and Hakkani-Tur, 2010): A semi-supervised model, which builds a hierarchial LDA toprobabilistically score sentences in training datasetas summary or non-summary sentences.
Using theseprobabilities as output variables, it learns a discrim-inative classifier model to infer the scores of newsentences in testing dataset.
(iv) PAM (Li and Mc-Callum, 2006) and hPAM (Mimno et al, 2007): Twohierarchical topic models to discover high and low-level concepts from documents, baselines for syn-thetic experiments in ?4 & ?5.Results of our experiments are illustrated in Table6.
Our unsupervised TTM and ETTM systems yield a44.1 R-1 (w/ stop-words) outperforming the rest ofthe models, except HybHSum.
Because HybHSumuses the human generated summaries as supervisionduring model development and our systems do not,our performance is quite promising considering thegeneration is completely unsupervised without see-ing any human generated summaries during train-ing.
However, the R-2 evaluation (as well as R-4) w/stop-words does not outperform other models.
Thisis because R-2 is a measure of bi-gram recall andneither of our models represent bi-grams whereas,for instance, PHTHY includes several bi-gram andhigher order n-gram statistics.
For topic models bi-grams tend to degenerate due to generating inconsis-tent bag of bi-grams (Wallach, 2006).6.2.
Manual Evaluations: A common DUCtask is to manually evaluate models on the qual-ity of generated summaries.
We compare our bestmodel ETTM to the results of PAM, our benchmarkmodel in synthetic experiments, as well as hybridhierarchical summarization model, hLDA (Celiky-ilmaz and Hakkani-Tur, 2010).
Human annotatorsare given two sets of summary text for each docu-ment set, generated from either one of the two ap-proaches: best ETTM and PAM or best ETTM andHybHSum models.
The annotators are asked tomark the better summary according to five criteria:non-redundancy (which summary is less redundant),coherence (which summary is more coherent), fo-cus and readability (content and no unnecessary de-tails), responsiveness and overall performance.We asked 3 annotators to rate DUC2007 predictedsummaries (45 summary pairs per annotator).
A to-tal of 42 pairs are judged for ETTM vs. PAM mod-els and 49 pairs for ETTM vs. HybHSum models.The evaluation results in frequencies are shown inTable 6.
The participants rated ETTM generatedsummaries more coherent and focused compared toPAM, where the results are statistically significant(based on t-test on 95% confidence level) indicat-ing that ETTM summaries are rated significantly bet-ter.
The results of ETTM are slightly better thanHybHSum.
We consider our results promising be-cause, being unsupervised, ETTM does not utilizehuman summaries for model development.7 ConclusionWe introduce two new models for extracting topi-cally coherent sentences from documents, an impor-tant property in extractive multi-document summa-rization systems.
Our models combine approachesfrom the hierarchical topic models.
We empha-498PAM ETTM TieHybHSumETTM TieNon-Redundancy 13 26 3 12 18 19Coherence 13 26 3 15 18 16Focus 14 24 4 14 17 18Responsiveness 15 24 3 19 12 18Overall 15 25 2 17 22 10Table 2: Frequency results of manual evaluations.
Tie in-dicates evaluations where two summaries are rated equal.size capturing correlated semantic concepts in docu-ments as well as characterizing general and specificwords, in order to identify topically coherent sen-tences in documents.
We showed empirically that afully unsupervised model for extracting general sen-tences performs well at summarization task usingdatasets that were originally used in building auto-matic summarization system challenges.
The suc-cess of our model can be traced to its capabilityof directly capturing coherent topics in documents,which makes it able to identify salient sentences.AcknowledgmentsThe authors would like to thank Dr. Zhaleh Feizol-lahi for her useful comments and suggestions.ReferencesR.
Barzilay and L. Lee.
2004.
Catching the drift: Proba-bilistic content models with applications to generationand summarization.
In Proc.
HLT-NAACL?04.R.
Barzilay, K.R.
McKeown, and M. Elhadad.
1999.Information fusion in the context of multi-documentsummarization.
Proc.
37th ACL, pages 550?557.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research.D.
Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.2004.
Hierarchical topic models and the nested chi-nese restaurant process.
In Neural Information Pro-cessing Systems [NIPS].A.
Celikyilmaz and D. Hakkani-Tur.
2010.
A hybrid hi-erarchical model for multi-document summarization.Proc.
48th ACL 2010.D.
Chen, J. Tang, L. Yao, J. Li, and L. Zhou.
2000.Query-focused summarization by combining topicmodel and affinity propagation.
LNCS?
Advances inData and Web Development.J.
Conroy, H. Schlesinger, and D. OLeary.
2006.
Topic-focused multi-document summarization using an ap-proximate oracle score.
Proc.
ACL.H.
Daume?-III and D. Marcu.
2006.
Bayesian query fo-cused summarization.
Proc.
ACL-06.J.
Eisenstein and R. Barzilay.
2008.
Bayesian unsuper-vised topic segmentation.
Proc.
EMNLP-SIGDAT.A.
Haghighi and L. Vanderwende.
2009.
Exploringcontent models for multi-document summarization.NAACL HLT-09.S.
Harabagiu, A. Hickl, and F. Lacatusu.
2007.
Sat-isfying information needs with multi-document sum-maries.
Information Processing and Management.W.
Li and A. McCallum.
2006.
Pachinko allocation:Dag-structure mixture models of topic correlations.Proc.
ICML.W.
Li, D. Blei, and A. McCallum.
2007.
Nonparametricbayes pachinko allocation.
The 23rd Conference onUncertainty in Artificial Intelligence.C.Y.
Lin and E. Hovy.
2002.
The automated acquisi-tion of topic signatures fro text summarization.
Proc.CoLing.G.
A. Miller.
1995.
Wordnet: A lexical database forenglish.
ACM, Vol.
38, No.
11: 39-41.D.
Mimno, W. Li, and A. McCallum.
2007.
Mixturesof hierarchical topics with pachinko allocation.
Proc.ICML.A.
Nenkova and L. Vanderwende.
2005a.
Documentsummarization using conditional random fields.
Tech-nical report, Microsoft Research.A.
Nenkova and L. Vanderwende.
2005b.
The impactof frequency on summarization.
Technical report, Mi-crosoft Research.A.
Nenkova, L. Vanderwende, and K. McKowen.
2006.A composition context sensitive multi-document sum-marizer.
Prof. SIGIR.D.
R. Radev.
2004.
Lexrank: graph-based centrality assalience in text summarization.
Jrnl.
Artificial Intelli-gence Research.M.
Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.2004.
The author-topic model for authors and docu-ments.
UAI.J.
Tang, L. Yao, and D. Chens.
2009.
Multi-topic basedquery-oriented summarization.
SIAM InternationalConference Data Mining.K.
Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,H.
Suzuki, and L. Vanderwende.
2007.
The ph-thy summarization system: Microsoft research at duc2007.
In Proc.
DUC.H.
Wallach.
2006.
Topic modeling: Beyond bag-of-words.
Proc.
ICML 2006.X.
Wan and J. Yang.
2006.
Improved affinity graphbased multi-document summarization.
HLT-NAACL.D.
Wang, S. Zhu, T. Li, and Y. Gong.
2009.
Multi-document summarization using sentence-based topicmodels.
Proc.
ACL 2009.499
