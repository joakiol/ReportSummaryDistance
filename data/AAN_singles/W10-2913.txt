Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 98?106,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsComputing Optimal Alignments for the IBM-3 Translation ModelThomas SchoenemannCentre for Mathematical SciencesLund University, SwedenAbstractPrior work on training the IBM-3 transla-tion model is based on suboptimal meth-ods for computing Viterbi alignments.
Inthis paper, we present the first methodguaranteed to produce globally optimalalignments.
This not only results in im-proved alignments, it also gives us the op-portunity to evaluate the quality of stan-dard hillclimbing methods.
Indeed, hill-climbing works reasonably well in prac-tice but still fails to find the global opti-mum for between 2% and 12% of all sen-tence pairs and the probabilities can beseveral tens of orders of magnitude awayfrom the Viterbi alignment.By reformulating the alignment problemas an Integer Linear Program, we canuse standard machinery from global opti-mization theory to compute the solutions.We use the well-known branch-and-cutmethod, but also show how it can be cus-tomized to the specific problem discussedin this paper.
In fact, a large number ofalignments can be excluded from the startwithout losing global optimality.1 IntroductionBrown et al (1993) proposed to approach theproblem of automatic natural language translationfrom a statistical viewpoint and introduced fiveprobability models, known as IBM 1-5.
Theirmodels were single word based, where eachsource word could produce at most one targetword.State-of-the-art statistical translation systemsfollow the phrase based approach, e.g.
(Och andNey, 2000; Marcu and Wong, 2002; Koehn, 2004;Chiang, 2007; Hoang et al, 2007), and hence al-low more general alignments.
Yet, single wordbased models (Brown et al, 1993; Brown et al,1995; Vogel et al, 1996) are still highly relevant:many phrase based systems extract phrases fromthe alignments found by training the single wordbased models, and those that train phrases directlyusually underperform these systems (DeNero etal., 2006).Single word based models can be divided intotwo classes.
On the one hand, models like IBM-1,IBM-2 and the HMM are computationally easy tohandle: both marginals and Viterbi alignments canbe computed by dynamic programming or evensimpler techniques.On the other hand there are fertility based mod-els, including IBM 3-5 and Model 6.
These mod-els have been shown to be of higher practical rel-evance than the members of the first class (Ochand Ney, 2003) since they usually produce betteralignments.
At the same time, computing Viterbialignments for these methods has been shown tobe NP-hard (Udupa and Maji, 2006), and comput-ing marginals is no easier.The standard way to handle these models ?
asimplemented in GIZA++ (Al-Onaizan et al, 1999;Och and Ney, 2003) ?
is to use a hillclimbing al-gorithm.
Recently Udupa and Maji (2005) pro-posed an interesting approximation based on solv-ing sequences of exponentially large subproblemsby means of dynamic programming and also ad-dressed the decoding problem.
In both cases thereis no way to tell how far away the result is fromthe Viterbi alignment.In this paper we solve the problem of find-ing IBM-3 Viterbi alignments by means of Inte-ger Linear Programming (Schrijver, 1986).
Whilethere is no polynomial run-time guarantee, in prac-tice the applied branch-and-cut framework is fastenough to find optimal solutions even for the largeCanadian Hansards task (restricted to sentenceswith at most 75 words), with a training time of 6hours on a 2.4 GHz Core 2 Duo (single threaded).98Integer Linear Programming in the context ofmachine translation first appeared in the work ofGermann et al (2004), who addressed the trans-lation problem (often called decoding) in terms ofa travelings-salesman like formulation.
Recently,DeNero and Klein (2008) addressed the trainingproblem for phrase-based models by means ofinteger linear programming, and proved that theproblem is NP-hard.
The main difference to ourwork is that they allow only consecutive words inthe phrases.
In their formulation, allowing arbi-trary phrases would require an exponential numberof variables.
In contrast, our approach handles theclassical single word based model where any kindof ?phrases?
in the source sentence are aligned toone-word phrases in the target sentence.Lacoste-Julien et al (2006) propose an inte-ger linear program for a symmetrized word-levelalignment model.
Their approach also allows totake the alignments of neighboring words into ac-count.
In contrast to our work, they only have avery crude fertility model and they are consider-ing a substantially different model.
It should benoted, however, that a subclass of their problemscan be solved in polynomial time - the problem isclosely related to bipartite graph matching.
Lessgeneral approaches based on matching have beenproposed in (Matusov et al, 2004) and (Taskar etal., 2005).Recently Bodrumlu et al (2009) proposed avery innovative cost function for jointly optimiz-ing dictionary entries and alignments, which theyminimize using integer linear programming.
Theyalso include a mechanism to derive N-best lists.However, they mention rather long computationtimes for rather small corpora.
It is not clear if thelarge Hansards tasks could be addressed by theirmethod.An overview of integer linear programming ap-proaches for natural language processing can befound on http://ilpnlp.wikidot.com/.To facilitate further research in this area, thesource code will be made publicly available.Contribution The key contribution of our workis a method to handle exact fertility models as aris-ing in the IBM-3 model in a global optimizationframework.
This is done by a linear number oflinear consistency constraints.
Unlike all previ-ous works on integer linear programming for ma-chine translation, we do not solely use binary co-efficients in the constraint matrix, hence showingthat the full potential of the method has so far notbeen explored.At the same time, our method allows us to give adetailed analysis of the quality of hillclimbing ap-proaches.
Moreover, we give a more detailed de-scription of how to obtain a fast problem-tailoredinteger solver than in previous publications, andinclude a mechanism to a priori exclude some vari-ables without losing optimality.2 The IBM-3 Translation ModelGiven a source sentence fJ1 , the statistical ap-proach to machine translation is to assign eachpossible target sentence eI1 a probability to be anaccurate translation.
For convenience in the trans-lation process, this probability is usually rewrittenasP (eI1|fJ1 ) =1p(fJ1 )?
p(eI1) ?
p(fJ1 |eI1) ,and the training problem is to derive suitable pa-rameters for the latter term from a bilingual cor-pus.
Here, the probability is expressed by sum-ming over hidden variables called alignments.
Thecommon assumption in single word based modelsis that each source position j produces a single tar-get position aj ?
{0, .
.
.
, I}, where an artificial 0-position has been introduced to mark words with-out a correspondence in the target sentence.
Thealignment of a source sentence is then a vector aJ1 ,and the probability can now be written asp(fJ1 |eI1) =?aJ1p(fJ1 , aJ1 |eI1) .We will focus on training the IBM-3 model whichis based on the concept of fertilities: given analignment aJ1 , the fertility ?i(aJ1 ) =?j:aj=i 1of target word i expresses the number of sourcewords aligned to it.
Omitting the dependence onaJ1 (and defining p(j|0) = 1), the probability isexpressed asp(fJ1 , aJ1 |eI1) = p(?0|J) ?I?i=1[?i!
p(?i|ei)]?
?j[p(fj|eaj ) ?
p(j|aj)].
(1)For the probability p(?0|J) of the fertility of theempty word, we use the modification introduced in(Och and Ney, 2003), see there for details.
In sum-mary, the model comprises a single word based99translation model, an inverted zero-order align-ment model and a fertility model.
We now discusshow to find the optimal alignment for given prob-abilities, i.e.
to solve the problemargmaxaJ1p(fJ1 , aJ1 |eI1) (2)for each bilingual sentence pair in the trainingset.
This is a desirable step in the approximateEM-algorithm that is commonly used to train themodel.3 Finding IBM-3 Viterbi Alignments viaInteger Linear ProgrammingInstead of solving (2) directly we consider theequivalent task of minimizing the negative loga-rithm of the probability function.
A significantpart of the arising cost function is already linearin terms of the alignment variables, a first step forthe integer linear program (ILP) we will derive.To model the problem as an ILP, we introducetwo sets of variables.
Firstly, for any source po-sition j ?
{1, .
.
.
, J} and any target positioni ?
{0, .
.
.
, I} we introduce an integer variablexij ?
{0, 1} which we want to be 1 exactly ifaj = i and 0 otherwise.
Since each source posi-tion must be aligned to exactly one target position,we arrive at the set of linear constraints?ixij = 1 , j = 1, .
.
.
, J .
(3)The negative logarithm of the bottom row of (1) isnow easily written as a linear function in terms ofthe variables xij :?i,jcxij ?
xij ,cxij = ?
log[p(fj|ei) ?
p(j|i)].For the part of the cost depending on the fertilities,we introduce another set of integer variables yif ?
{0, 1}.
Here i ?
{0, .
.
.
, I} and f ranges from 0 tosome pre-specified limit on the maximal fertility,which we set to max(15, J/2) in our experiments(fertilities > J need not be considered).
We wantyif to be 1 if the fertility of i is f , 0 otherwise.Hence, again these variables must sum to 1:?fyif = 1 , i = 0, .
.
.
, I .
(4)The associated part of the cost function is writtenas?i,fcyif ?
yif ,cyif = ?
log[f !
p(f |ei)], i = 1, .
.
.
, Icy0f = ?
log[p(?0 = f |J)].It remains to ensure that the variables yif express-ing the fertilities are consistent with the fertilitiesinduced by the alignment variables xij .
This isdone via the following set of linear constraints:?jxij =?ff ?
yif , i = 0, .
.
.
, I .
(5)Problem (2) is now reduced to solving the integerlinear programarg min{xij},{yif}?i,jcxij xij +?i,fcyif yifsubject to (3), (4), (5)xij ?
{0, 1}, yif ?
{0, 1} , (6)with roughly 2 I J variables and roughly J + 2Iconstraints.4 Solving the Integer Linear ProgramTo solve the arising integer linear programmingproblem, we first relax the integrality constraintson the variables to continuous ones:xij ?
[0, 1], yif ?
[0, 1] ,and obtain a lower bound on the problems by solv-ing the arising linear programming relaxation viathe dual simplex method.While in practice this can be done in a matter ofmilli-seconds even for sentences with I, J > 50,the result is frequently a fractional solution.
Herethe alignment variables are usually integral but thefertility variables are not.In case the LP-relaxation does not produce aninteger solution, the found solution is used as theinitialization of a branch-and-cut framework.
Hereone first tries to strengthen the LP-relaxation byderiving additional inequalities that must be validfor all integral solutions see e.g.
(Schrijver, 1986;Wolter, 2006) and www.coin-or.org.
Theseinequalities are commonly called cuts.
Then oneapplies a branch-and-bound scheme on the inte-ger variables.
In each step of this scheme, addi-tional inequalities are derived.
The process is fur-ther sped-up by introducing a heuristic to derive an100upper bound on the cost function.
Such bounds aregenerally given by feasible integral solutions.
Weuse our own heuristic as a plug-in to the solver.It generates solutions by thresholding the align-ment variables (winner-take-all) and deriving theinduced fertility variables.
An initial upper boundis furthermore given by the alignment found byhillclimbing.We suspect that further speed-ups are possibleby using so-called follow-up nodes: e.g.
if in thebranch-and-bound an alignment variable xij is setto 1, one can conclude that the fertility variableyi0 must be 0.
Also, sets of binary variables thatmust sum to 1 as in (3) and (4) are known as spe-cial ordered sets of type I and there are variantsof branch-and-cut that can exploit these proper-ties.
However, in our context they did not resultin speed-ups.Our code is currently based on the open sourceCOIN-OR project1 and involves the linear pro-gramming solver CLP, the integer programmingsolver CBC, and the cut generator library CGL.We have also tested two commercial solvers.
Forthe problem described in this paper, CBC per-formed best.
Tests on other integer programmingtasks showed however that the Gurobi solver out-performs CBC on quite a number of problems.5 Speed-ups by Deriving BoundsIt turns out that, depending on the cost function,some variables may a priori be excluded from theoptimization problem without losing global opti-mality.
That is, they can be excluded even beforethe first LP-relaxation is solved.The affected variables have relatively high costcoefficients and they are identified by consideringlower bounds and an upper bound on the cost func-tion.
Starting from the lower bounds, one can thenidentify variables that when included in a solutionwould raise the cost beyond the upper bound.An upper bound u on the problem is given byany alignment.
We use the one found by hillclimb-ing.
If during the branch-and-cut process tighterupper bounds become available, the process couldbe reapplied (as a so-called column cut generator).For the lower bounds we use different ones toexclude alignment variables and to exclude fertil-ity variables.1www.coin-or.org5.1 Excluding Alignment VariablesTo derive a lower bound for the alignment vari-ables, we first observe that the cost cxij for thealignment variables are all positive, whereas thecost cyif for the fertilities are frequently negative,due to the factorial of f .
A rather tight lowerbound on the fertility cost can be derived by solv-ing the problemlF,1 = min{?i}I?i=0cyi?is.t.
?i ?i = J , (7)which is easily solved by dynamic programmingproceeding along i.
A lower bound on the align-ment cost is given bylA =?j lA,j ,where lA,j = mini=0,...,Icxij .The lower bound is then given by l1 = lF,1 + lA,and we can be certain that source word j will notbe aligned to target word i ifcxij > lA,j + (u ?
l1) .5.2 Excluding Fertility VariablesExcluding fertility variables is more difficult ascost can be negative and we have used a constraintto derive lF,1 above.At present we are using a two ways to gener-ate a lower bound and apply the exclusion processwith each of them sequentially.
Both bounds arelooser than l1, but they immensely help to get thecomputation times to an acceptable level.The first bound builds upon l1 as derived above,but using a looser bound lF,2 for the fertility cost:lF,2 =?imin?icyi?i .This results in a bound l2 = lF,2 + lA, and fertilityvariables can now be excluded in a similar manneras above.Our second bound is usually much tighter andpurely based on the fertility variables:l3 =?imin?i[cyi?i + minJ?
{1,...,J} : |J=?i|cxi (J )],with cxi (J ) =?j?Jcxij ,101and where the cost of the empty set is defined as 0.Although this expression looks rather involved, itis actually quite easy to compute by simply sortingthe respective cost entries.
A fertility variable yifcan now be excluded if the difference between cyifand the contribution of i to l3 exceeds u ?
l3.We consider it likely that more variables can beexcluded by deriving bounds in the spirit of (7),but with the additional constraint that ?i = f forsome i and f .
We leave this for future work.6 ExperimentsWe have tested our method on three different tasksinvolving a total of three different languages andeach in both directions.
The first task is the well-known Canadian Hansards2 task (senate debates)for French and English.
Because of the largedataset we are currently only considering sentencepairs where both sentences have at most 75 words.Longer sentences are usually not useful to derivemodel parameters.The other two datasets are released by the Eu-ropean Corpus Initiative3.
We choose the UnionBank of Switzerland (UBS) corpus for English andGerman and the Avalanche Bulletins, originallyreleased by SFISAR, for French and German.
Forthe latter task we have annotated alignments for150 of the training sentences, where one annota-tor specified sure and possible alignments.
For de-tails, also on the alignment error rate, see (Och andNey, 2003).All corpora have been preprocessed withlanguage-specific rules; their statistics are given inTable 1.
We have integrated our method into thestandard toolkit GIZA++4 and are using the train-ing scheme 15H53545 for all tasks.
While we fo-cus on the IBM-3 stage, we also discuss the qualityof the resulting IBM-4 parameters and alignments.Experiments were run on a 2.4 GHz Core 2Duo with 4 GB memory.
For most sentence pairs,the memory consumption of our method is onlymarginally more than in standard GIZA++ (600MB).
In the first iteration on the large Hansardstask, however, there are a few very difficult sen-tence pairs where the solver needs up to 90 min-utes and 1.5 GB .
We observed this in both trans-lation directions.2www.isi.edu/natural-language/download/hansard/3The entire CD with many more corpora is available forcurrently 50 Euros.4available at code.google.com/p/giza-pp/ .Avalanche BulletinFrench German# sentences 2989max.
sentence length 88 57total words 64825 45629vocabulary size 1707 2113UBSEnglish German# sentences 2689max.
sentence length 92 91total words 62617 53417vocabulary size 5785 9127Canadian Hansards (max.
75)French English# sentences 180706max.
sentence length 75 75total words 3730570 3329022vocabulary size 48065 37633Table 1: Corpus statistics for all employed (train-ing) corpora, after preprocessing.6.1 Evaluating HillclimbingIn our first set of experiments, we compute Viterbialignments merely to evaluate the quality of thestandard training process.
That is, the modelparameters are updated based on the alignmentsfound by hillclimbing.
Table 2 reveals that, asexpected, hillclimbing does not always find theglobal optimum: depending on the task and it-eration number, between 2 and 12 percent of allhillclimbing alignments are suboptimal.
For shortsentences (i.e.
I, J ?
20) hillclimbing usuallyfinds the global optimum.Somewhat more surprisingly, even when a goodand hence quite focused initialization of the IBM-3 model parameters is given (by training HMMsfirst), the probability of the Viterbi alignment canbe up to a factor of 1037 away from the optimum.This factor occurred on the Hansards task for asentence pair with 46 source and 46 target wordsand the fertility of the empty word changed from9 (for hillclimbing) to 5.6.2 Hillclimbing vs. Viterbi AlignmentsWe now turn to a training scheme where theViterbi alignments are used to actually update themodel parameters, and compare it to the standardtraining scheme (based on hillclimbing).102Candian Hansards (max 75)French ?
EnglishIteration # 1 2 3 4 5# suboptimal alignments in hillclimbing 10.7% 10.7% 10.8% 11.1% 11.4%Maximal factor to Viterbi alignment 1.9 ?
1037 9.1 ?
1017 7.3 ?
1014 3.3 ?
1012 8.1 ?
1014English ?
FrenchIteration # 1 2 3 4 5# suboptimal alignments in hillclimbing 7.3% 7.5% 7.4% 7.4% 7.5%Maximal factor to Viterbi alignment 5.6 ?
1038 6.6 ?
1020 7.6 ?
1011 4.3 ?
1010 8.3 ?
1011Avalanche BulletinsFrench ?
GermanIteration # 1 2 3 4 5# suboptimal alignments in hillclimbing 7.5% 5.6% 4.9% 4.9% 4.4%Maximal factor to Viterbi alignment 6.1 ?
105 877 368 2.5 ?
104 429German ?
FrenchIteration # 1 2 3 4 5# suboptimal alignments in hillclimbing 4.2% 2.7% 2.5% 2.3% 2.1%Maximal factor to Viterbi alignment 40 302 44 3.3 ?
104 9.2 ?
104Union Bank of Switzerland (UBS)English ?
GermanIteration # 1 2 3 4 5# suboptimal alignments in hillclimbing 5.0% 4.0% 3.5% 3.3% 3.2%Maximal factor to Viterbi alignment 677 22 53 40 32German ?
EnglishIteration # 1 2 3 4 5# suboptimal alignments in hillclimbing 5.5% 3.3% 2.5% 2.2% 2.3%Maximal factor to Viterbi alignment 1.4 ?
107 808 33 33 1.8 ?
104Table 2: Analysis of Hillclimbing on all considered tasks.
All numbers are for the IBM-3 translationmodel.
Iteration 1 is the first iteration after the transfer from HMM, the final iteration is the transfer toIBM4.
The factors are w.r.t.
the original formulation, not the negative logarithm of it and are defined asthe maximal ratio between the Viterbi probability and the hillclimbing probability.103une baisse de la tempe?rature a en ge?ne?ral stabilise?
la couverture neigeuse .ein Temperaturru?ckgang hat die Schneedecke im allgemeinen stabilisiert .Standard training (hillclimbing).une baisse de la tempe?rature a en ge?ne?ral stabilise?
la couverture neigeuse .ein Temperaturru?ckgang hat die Schneedecke im allgemeinen stabilisiert .Proposed training (Viterbi alignments).Figure 1: Comparison of training schemes.
Shown are the alignments of the final IBM-3 iteration.Indeed Table 3 demonstrates that with the newtraining scheme, the perplexities of the final IBM-3 iteration are consistently lower.
Yet, this effectdoes not carry over to IBM-4 training, where theperplexities are consistently higher.
Either this isdue to overfitting or it is better to use the samemethod for alignment computation for both IBM-3 and IBM-4.
After all, both start from the HMMViterbi alignments.Interestingly, the maximal factor between thehillclimbing alignment and the Viterbi alignmentis now consistently higher on all tasks and in alliterations.
The extreme cases are a factor of 1076for the Canadian Hansards English ?
French taskand 1030 for the Bulletin French ?
German task.Table 4 demonstrates that the alignment errorrates of both schemes are comparable.
Indeed, amanual evaluation of the alignments showed thatmost of the changes affect words like articles orprepositions that are generally hard to translate.In many cases neither the heuristic nor the Viterbialignment could be considered correct.
An inter-esting case where the proposed scheme producedthe better alignment is shown in Figure 1.In summary, our results give a thorough justi-fication for the commonly used heuristics.
A testwith the original non-deficient empty word modelof the IBM-3 furthermore confirmed the impres-sion of (Och and Ney, 2003) that overly manywords are aligned to the empty word: the tendencyis even stronger in the Viterbi alignments.6.3 Optimizing Running TimeThe possibilities to influence the run-times of thebranch-and-cut framework are vast: there are nu-Union Bank (UBS) E ?
GFinal IBM-3 Final IBM-4Standard train.
49.21 35.73Proposed train.
49.00 35.76Union Bank (UBS) G ?
EFinal IBM-3 Final IBM-4Standard train.
62.38 47.39Proposed train.
62.08 47.43Avalanche F ?
GFinal IBM-3 Final IBM-4Standard train.
35.44 21.99Proposed train.
35.23 22.04Avalanche G ?
FFinal IBM-3 Final IBM-4Standard train.
34.60 22.78Proposed train.
34.48 22.76Canadian Hansards F ?
EFinal IBM-3 Final IBM-4Standard train.
105.28 55.22Proposed train.
92.09 55.35Canadian Hansards E ?
FFinal IBM-3 Final IBM-4Standard train.
70.58 37.64Proposed train.
70.03 37.73Table 3: Analysis of the perplexities in training.104French ?
GermanFinal IBM-3 Final IBM-4Standard train.
24.31% 23.01%Proposed train.
24.31% 23.24%German ?
FrenchFinal IBM-3 Final IBM-4Standard train.
33.03% 33.44%Proposed train.
33.00% 33.27%Table 4: Alignment error rates on the Avalanchebulletin task.merous ways to generate cuts and several of themcan be used simultaneously.
The CBC-packagealso allows to specify how many rounds of cutsto derive at each node.
Then there is the questionof whether to use the bounds derived in Section5 to a priori exclude variables.
Finally, branch-and-cut need not be done on all variables: sincesolving LP-relaxations typically results in integralalignments, it suffices to do branch-and-cut on thefertility variables and only add the alignment vari-ables in case non-integral values arise (this neverhappened in our experiments5).We could not possibly test all combinationsof the listed possibilities, and our primary focuswas to achieve acceptable run-times for the largeHansards task.
Still, in the end we have a quiteuniform picture: the lowest run-times are achievedby using Gomory Cuts only.
Moreover, includingall variables for branching was between 1.5 and 2times faster than only including fertility variables.Only by exploiting the bounds derived in Section5 the run-times for the Hansards task in directionfrom English to French became acceptable.
Webelieve that further speed-ups are possible by de-riving tighter bounds, and are planning to investi-gate this in the future.We end up with roughly 6 hours for theHansards task, roughly 3 minutes for the UBStask, and about 2.5 minutes for the Avalanche task.In all cases the run-times are much higher thanin the standard GIZA++ training.
However, weare now getting optimality guarantees where pre-viously one could not even tell how far away one isfrom the optimum.
And the Viterbi alignments ofseveral sentence pairs can of course be computedin parallel.Lastly, we mention the possibility of setting a5In fact, when fixing the fertility variables, the problemreduces to the polynomial time solvable assignment problem.limit on the branch-and-cut process, either on therunning time or on the number of nodes.
There isthen no longer a guarantee for global optimality,but at least one is getting a bound on the gap to theoptimum and one can be certain that the trainingtime will be sufficiently low.7 ConclusionWe present the first method to compute IBM-3Viterbi alignments with a guarantee of optimal-ity.
In contrast to other works on integer linearprogramming for machine translation, our formu-lation is able to include a precise and very gen-eral fertility model.
The resulting integer linearprogram can be solved sufficiently fast in prac-tice, and we have given many comments on howproblem-specific knowledge can be incorporatedinto standard solvers.The proposed method allows for the first timeto analyze the quality of hillclimbing approachesfor IBM-3 training.
It was shown that they can bevery far from the optimum.
At the same time, thisseems to happen mostly for difficult sentences thatare not suitable to derive good model parameters.In future work we want to derive tighter boundsto a priori exclude variables, combine the methodwith the N-best list generation of (Bodrumlu etal., 2009) and evaluate on a larger set of corpora.Finally we are planning to test other integer pro-gramming solvers.Acknowledgments We thank Fredrik Kahl forhelpful comments and an anonymous reviewer forpointing out freely available software packages.This research was funded by the European Re-search Council (GlobalVision grant no.
209480).ReferencesY.
Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.Smith, and D. Yarowsky.
1999.
Statistical ma-chine translation, Final report, JHU workshop.http://www.clsp.jhu.edu/ws99/.Tugba Bodrumlu, Kevin Knight, and Sujith Ravi.2009.
A new objective function for word align-ment.
In Proceedings of the Workshop on Inte-ger Linear Programming for Natural Langauge Pro-cessing (ILP), Boulder, Colorado, June.P.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra, andR.L.
Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311, June.105P.F.
Brown, J. Cocke, S.A. Della Pietra, V.J.
DellaPietra, F. Jelinek, J. Lai, and R.L.
Mercer.
1995.Method and system for natural language translation.U.S.
patent #5.477.451.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.J.
DeNero and D. Klein.
2008.
The complexityof phrase alignment problems.
In Annual Meet-ing of the Association for Computational Linguistics(ACL), Columbus, Ohio, June.John DeNero, Dan Gillick, James Zhang, and DanKlein.
2006.
Why generative phrase models under-perform surface heuristics.
In StatMT ?06: Proceed-ings of the Workshop on Statistical Machine Trans-lation, pages 31?38, Morristown, NJ, USA, June.U.
Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-mada.
2004.
Fast decoding and optimal decod-ing for machine translation.
Artificial Intelligence,154(1?2), April.H.
Hoang, A. Birch, C. Callison-Burch, R. Zens,A.
Constantin, M. Federico, N. Bertoldi, C. Dyer,B.
Cowan, W. Shen, C. Moran, and O. Bojar.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 177?180, Prague, Czech Republic, June.P.
Koehn.
2004.
Pharaoh: A beam search decoder forphrase-based statistical machine translation mod-els.
In Conference of the Association for MachineTranslation in the Americas (AMTA), pages 115?124, Washington, D.C., October.S.
Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.2006.
Word alignment via quadratic assignment.In Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, New York, New York, June.D.
Marcu and W. Wong.
2002.
A phrase-based,joint probability model for statistical machine trans-lation.
In Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), Philadelphia,Pennsylvania, July.E.
Matusov, R. Zens, and H. Ney.
2004.
Symmetricword alignments for statistical machine translation.In International Conference on Computational Lin-guistics (COLING), Geneva, Switzerland, August.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Annual Meetingof the Association for Computational Linguistics(ACL), pages 440?447, Hongkong, China, October.F.J.
Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.A.
Schrijver.
1986.
Theory of Linear and Integer Pro-gramming.
Wiley-Interscience Series in DiscreteMathematics and Optimization.
John Wiley & Sons.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005.A discriminative matching approach to word align-ment.
In Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), Vancouver,Canada, October.R.
Udupa and H.K.
Maji.
2005.
Theory of align-ment generators and applications to statistical ma-chine translation.
In The International Joint Con-ferences on Artificial Intelligence, Edinburgh, Scot-land, August.R.
Udupa and H.K.
Maji.
2006.
Computational com-plexity of statistical machine translation.
In Con-ference of the European Chapter of the Associationfor Computational Linguistics (EACL), Trento, Italy,April.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Inter-national Conference on Computational Linguistics(COLING), pages 836?841, Copenhagen, Denmark,August.K.
Wolter.
2006.
Implementation of Cutting PlaneSeparators for Mixed Integer Programs.
Master?sthesis, Technische Universita?t Berlin, Germany.106
