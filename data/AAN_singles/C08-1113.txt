Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 897?904Manchester, August 2008Training Conditional Random Fields Using Incomplete AnnotationsYuta Tsuboi, Hisashi KashimaTokyo Research Laboratory,IBM Research, IBM Japan, LtdYamato, Kanagawa 242-8502, Japan{yutat,hkashima}@jp.ibm.comShinsuke MoriAcademic Center for Computing andMedia Studies, Kyoto UniversitySakyo-ku, Kyoto 606-8501, Japanforest@i.kyoto-u.ac.jpHiroki OdaShinagawa, Tokyo, Japanoda@fw.ipsj.or.jpYuji MatsumotoGraduate School of Information Science,Nara Institute of Science and TechnologyTakayama, Ikoma, Nara 630-0101, Japanmatsu@is.naist.jpAbstractWe address corpus building situations,where complete annotations to the wholecorpus is time consuming and unrealistic.Thus, annotation is done only on crucialpart of sentences, or contains unresolvedlabel ambiguities.
We propose a parame-ter estimation method for Conditional Ran-dom Fields (CRFs), which enables us touse such incomplete annotations.
We showpromising results of our method as appliedto two types of NLP tasks: a domain adap-tation task of a Japanese word segmenta-tion using partial annotations, and a part-of-speech tagging task using ambiguoustags in the Penn treebank corpus.1 IntroductionAnnotated linguistic corpora are essential forbuilding statistical NLP systems.
Most of thecorpora that are well-known in NLP communi-ties are completely-annotated in general.
Howeverit is quite common that the available annotationsare partial or ambiguous in practical applications.For example, in domain adaptation situations, it istime-consuming to annotate all of the elements in asentence.
Rather, it is efficient to annotate certainparts of sentences which include domain-specificexpressions.
In Section 2.1, as an example of suchefficient annotation, we will describe the effective-ness of partial annotations in the domain adapta-tion task for Japanese word segmentation (JWS).In addition, if the annotators are domain expertsc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.rather than linguists, they are unlikely to be confi-dent about the annotation policies and may preferto be allowed to defer some linguistically complexdecisions.
For many NLP tasks, it is sometimesdifficult to decide which label is appropriate in aparticular context.
In Section 2.2, we show thatsuch ambiguous annotations exist even in a widelyused corpus, the Penn treebank (PTB) corpus.This motivated us to seek to incorporate suchincomplete annotations into a state of the art ma-chine learning technique.
One of the recent ad-vances in statistical NLP is Conditional RandomFields (CRFs) (Lafferty et al, 2001) that evaluatethe global consistency of the complete structuresfor both parameter estimation and structure infer-ence, instead of optimizing the local configurationsindependently.
This feature is suited to many NLPtasks that include correlations between elementsin the output structure, such as the interrelation ofpart-of-speech (POS) tags in a sentence.
However,conventional CRF algorithms require fully anno-tated sentences.
To incorporate incomplete anno-tations into CRFs, we extend the structured out-put problem in Section 3.
We focus on partial an-notations or ambiguous annotations in this paper.We also propose a parameter estimation methodfor CRFs using incompletely annotated corpora inSection 4.
The proposed method marginalizes outthe unknown labels so as to optimize the likelihoodof a set of possible label structures which are con-sistent with given incomplete annotations.We conducted two types of experiments and ob-served promising results in both of them.
One wasa domain adaptation task for JWS to assess theproposed method for partially annotated data.
Theother was a POS tagging task using ambiguous an-notations that are contained in the PTB corpus.
Wesummarize related work in Section 6, and conclude897      cutincised woundcut injuryabrasionorfile (or rasp)infl.
injuryinfl.infl.pickpocketFigure 1: An example of word boundary ambigui-ties: infl.
stands for an inflectional suffix of a verb.in Section 7.2 Incomplete Annotations2.1 Partial AnnotationsIn this section, we describe an example of an effi-cient annotation which assigns partial word bound-aries for the JWS task.It is not trivial to detect word boundaries fornon-segmented languages such as Japanese or Chi-nese.
For example, the correct segmentation ofthe Japanese phrase ?????????
(incisedwound or abrasion) is shown by the lowest boxessegmented by the solid lines in Figure 1.
How-ever, there are several overlapping segmentationcandidates, which are shown by the other boxes,and possible segmentation by the dashed lines.Thus, the decisions on the word segmentation re-quire considering the context, so simple dictionarylookup approach is not appropriate.
Therefore sta-tistical methods have been successfully used forJWS tasks.
Previous work (Kudo et al, 2004)showed CRFs outperform generative Markov mod-els and discriminative history-based methods inJWS.
In practice, a statistical word segment an-alyzer tends to perform worse for text from dif-ferent domains, so that additional annotations foreach target domain are required.
A major cause oferrors is the occurrence of unknown words.
For ex-ample, if ?????
(abrasion) is an unknown word,the system may accept the word sequence of ?????????
as ?????
(incised wound), ?????
(file), and ???
(injury) by mistake.On one hand, lists of new terms in the targetdomain are often available in the forms of techni-cal term dictionaries, product name lists, or othersources.
To utilize those domain word lists, Mori(2006) proposed a KWIC (KeyWord In Context)style annotation user interface (UI) with which auser can delimit a word in a context with a singleuser action.
In Figure 2, an annotator marks the oc-currences of ????
?, a word in the domain word???????
???
???????
???????
???
???????
???????
???
?????
?Figure 2: An example of KWIC style annotation:marked lines are identified as a correct segmenta-tion.list, if they are used as a real word in their con-text.
The ?????
in the first row is a part of an-other word ??????
(scratch), and the annotatormarks the last two rows as correctly segmented ex-amples.
This UI simplifies annotation operationsfor segmentation to yes/no decisions, and this sim-plification can also be effective for the reductionof the annotation effort for other NLP tasks.
Forexample, the annotation operations for unlabeleddependency parsing can be simplified into a seriesof yes/no decisions as to whether or not given twowords have syntactic dependency.
Compared withsentence-wise annotation, the partial annotation isnot only effective in terms of control operations,but also reduces annotation errors because it doesnot require annotating the word boundaries that anannotator is unsure of.
This feature is crucial forannotations made by domain experts who are notlinguists.1We believe partial annotation is effec-tive in creating corpora for many other structuredannotations in the context of the domain adapta-tions.2.2 Ambiguous AnnotationsAmbiguous annotations in this paper refer to a setof candidate labels annotated for a part of a struc-tured instance.
For example, the following sen-tence from the PTB corpus includes an ambiguousannotation for the POS tag of ?pending?
:That/DT suit/NN is/VBZ pending/VBG|JJ ./.
,where words are paired with their part-of-speechtag by a forward slash (?/?
).2Uncertainty concern-ing the proper POS tag of ?pending?
is representedby the disjunctive POS tag (?VBG and JJ?)
as in-dicated by a vertical bar.The existence of the ambiguous annotations isdue to the task definition itself, the procedure man-1The boundary policies of some words are different evenamong linguists.
In addition, the boundary agreement is evenlower in Chinese (Luo, 2003).2These POS tags used here are DT:determiner,NN:common noun, VBZ:present tense 3rd person singularverb, VBG:gerund or present participle verb, JJ:adjective,NNS:plural noun, RBR:comparative adverb, IN:prepositionor subordinating conjunction, and RB:adverb.898frequency word POS tags15 data NN|NNS10 more JJR|RBR7 pending JJ|VBG4 than IN|RBTable 1: Words in the PTB with ambiguous POSs.ual for the annotators, or the inadequate knowl-edge of the annotators.
Ideally, the annotationsshould be disambiguated by a skilled annotator forthe training data.
However, even the PTB cor-pus, whose annotation procedure is relatively well-defined, includes more than 100 sentences contain-ing POS ambiguities such as those listed in Ta-ble 1.
Although the number of ambiguous an-notations is not considerably large in PTB cor-pus, corpora could include more ambiguous anno-tations when we try to build wider coverage cor-pora.
Also, ambiguous annotations are more com-mon in the tasks that deal with semantics, such asinformation extraction tasks so that learning algo-rithms must deal with ambiguous annotations.3 Problem DefinitionIn this section, we give a formal definition of thesupervised structured output problem that uses par-tial annotations or ambiguous annotations in thetraining phase.
Note that we assume the input andoutput structures are sequences for the purpose ofexplanation, though the following discussion is ap-plicable to other structures, such as trees.Let x=(x1, x2, ?
?
?
, xT) be a sequence of ob-served variables xt?
X and y=(y1, y2, ?
?
?
, yT)be a sequence of label variables yt?
Y .
Then thesupervised structured output problem can be de-fined as learning a map X ?
Y .
In the Japaneseword segmentation task, x can represent a givensequence of character boundaries and y is a se-quence of the corresponding labels, which spec-ify whether the current position is a word bound-ary.3In the POS tagging task, x represents a wordsequence and y is a corresponding POS tag se-quence.
An incomplete annotation, then, is definedas a sequence of subset of the label set instead of asequence of labels.
Let L=(L1, L2, ?
?
?
, LT) be asequence of label subsets for an observed sequence3Peng et al (2004) defined the word segmentation prob-lem as labeling each character as whether or not the previouscharacter boundary of the current character is a word bound-ary.
However, we employ our problem formulation since itis redundant to assign the first character of a sentence as theword boundary in their formulation.x, where Lt?
2Y?
{?}.
The partial annotationat position s is where Lsis a singleton and the restLt6=sis Y .
For example, if a sentence with 6 char-acter boundaries (7 characters) is partially anno-tated using the KWIC UI described in Section 2.1,a word annotation where its boundary begins witht = 2 and ends with t = 5 will be represented as:L = ({?,?
}, {?
}, {?
}, {?
}, {?}?
??
?partial annotation, {?,?
}),where ?
and ?
denote the word boundary la-bel and the non-word boundary label, respectively.The ambiguous annotation is represented as a setwhich contains candidate labels.
The example sen-tence including the ambiguous POS tag in Sec-tion 2.2 can be represented as:L = ({DT}, {NN}, {VBZ}, {VBG, JJ}?
??
?ambiguous annotation, {.
}).Note that, if all the elements of a given sequenceare annotated, it is the special case such that thesize of all elements is one, i.e.
|Lt| = 1 for allt = 1, ?
?
?
, T .
The goal in this paper is traininga statistical model from partially or ambiguouslyannotated data, D = {(x(n), L(n))}Nn=1.4 Marginalized Likelihood for CRFsIn this section, we propose a parameter estimationprocedure for the CRFs (Lafferty et al, 2001) in-corporating partial or ambiguous annotations.
Let?
(x, y) : X ?Y ?
<ddenote a map from a pairof an observed sequence x and a label sequence yto an arbitrary feature vector of d dimensions, and?
?
<ddenotes the vector of the model parame-ters.
CRFs model the conditional probability of alabel sequence y given an observed sequence x as:P?
(y|x) =e???(x,y)Z?,x,Y?
(1)where ?
denotes the inner product of the vectors,and the denominator is the normalization term thatguarantees the model to be a probability:Z?,x,S =?y?Se???
(x,y).Then once ?
has been estimated, the la-bel sequence can be predicted by?y =argmaxy?Y P?(y|x).
Since the original CRFlearning algorithm requires a completely labeledsequence y, the incompletely annotated data(x, L) is not directly applicable to it.899Let YL denote all of the possible label sequenceconsistent with L. We propose to use the condi-tional probability of the subset YL given x:P?
(YL|x) =?y?YLP?
(y|x), (2)which marginalizes the unknown ys out.
Thenthe maximum likelihood estimator for this modelcan be obtained by maximizing the log likelihoodfunction:LL(?)
=N?n=1lnP?
(YL(n) |x(n)) (3)=N?n=1(lnZ?,x(n),YL(n)?
lnZ?,x(n),Y).This modeling naturally embraces label ambigui-ties in the incomplete annotation.4Unfortunately, equation (3) is not a concavefunction5so that there are local maxima in theobjective function.
Although this non-concavityprevents efficient global maximization of equation(3), it still allows us to incorporate incomplete an-notations using gradient ascent iterations (Sha andPereira, 2003).
Gradient ascent methods requirethe partial derivative of equation (3):?
LL(?)??=N?n=1???y?YL(n)P?
(y|YL(n) , x(n))?
(x(n), y)??y?YP?(y|x(n))?
(x(n), y)?
?, (4)whereP?
(y|YL, x) =e???
(x,y)Z?,x,YL(5)is a conditional probability that is normalized overYL.Equations (3) and (4) include the summationsof all of the label sequences in Y or YL.
It is notpractical to enumerate and evaluate all of the labelconfigurations explicitly, since the number of all ofthe possible label sequences is exponential on thenumber of positions t with |Lt| > 1.
However, un-der the Markov assumption, a modification of the4It is common to introduce a prior distribution over the pa-rameters to avoid over-fitting in CRF learning.
In the experi-ments in Section 5, we used a Gaussian prior with the mean 0and the variance ?2so that ?||?||22?2is added to equation (3).5Since its second order derivative can be positive.domain #sentences #words(A) conversation 11,700 145,925(B) conversation 1,300 16,348(C) medical manual 1,000 29,216Table 2: Data statistics.Types TemplateCharacters c?1, c+1,Character types c?2c?1, c?1c+1, c+1c+2,Term in dic.
c?2c?1c+1, c?1c+1c+2Term in dic.
starts atc?1, c+1Term in dic.
ends atTable 3: Feature templates: Each subscript standsfor the relative distance from a character boundary.Forward-Backward algorithm guarantees polyno-mial time computation for the equations (3) and(4).
We explain this algorithm in Appendix A.5 ExperimentsWe conducted two types of experiments, assessingthe proposed method in 1) a Japanese word seg-mentation task using partial annotations and 2) aPOS tagging task using ambiguous annotations.5.1 Japanese Word Segmentation TaskIn this section, we show the results of domainadaptation experiments for the JWS task to assessthe proposed method.
We assume that only par-tial annotations are available for the target domain.In this experiment, the corpus for the source do-main is composed of example sentences in a dic-tionary of daily conversation (Keene et al, 1992).The text data for the target domain is composedof sentences in a medical reference manual (Beers,2004) .
The sentences of all of the source domaincorpora (A), (B) and a part of the target domaintext (C) were manually segmented into words (seeTable 2).The performance measure in the experiments isthe standard F measure score, F = 2RP/(R + P )whereR =# of correct words# of words in test data?
100P =# of correct words# of words in system output?
100.In this experiment, the performance was evaluatedusing 2-fold cross-validation that averages the re-sults over two partitions of the data (C) into the9009191.59292.59393.59494.5950 100 200 300 400 500 600 700 800 900 1000Number of word annotationsFProposed methodArgmax as training dataPoint-wise classifierFigure 3: Average performances varying the num-ber of word annotations over 2 trials.data for annotation and training (C1) versus thedata for testing (C2).We implemented first order Markov CRFs.
Asthe features for the observed variables, we use thecharacters and character type n-gram (n=1, 2, 3)around the current character boundary.
Thecharacter types are categorized into Hiragana,Katakana, Kanji, English alphabet, Arabic numer-als, and symbols.
We also used lexical featuresconsulting a dictionary: one is to check if anyof the above defined character n-grams appear ina dictionary (Peng et al, 2004), and the other isto check if there are any words in the dictionarythat start or end at the current character boundary.We used the unidic6(281K distinct words) as thegeneral purpose dictionary, and the Japanese Stan-dard Disease Code Master (JSDCM)7(23K dis-tinct words) as the medical domain dictionary.
Thetemplates for the features we used are summarizedin Table 3.
To reduce the number of parameters,we selected only frequent features in the source do-main data (A) or in about 50K of the unsegmentedsentences of the target domain.8The total numberof distinct features was about 300K.A CRF that was trained using only the sourcedomain corpus (A), CRFS, achieved F=96.84 inthe source domain validation data (B).
However,it showed the need for the domain adaptation thatthis CRFSsuffered severe performance degrada-tion (F=92.3) on the target domain data.
Thisexperiment was designed for the case in which auser selects the occurrences of words in the wordlist using the KWIC interface described in Sec-tion 2.1.
We employed JSDCM as a word listin which 224 distinct terms appeared on averageover 2 test sets (C1).
The number of word an-6Ver.
1.3.5; http://www.tokuteicorpus.jp/dist/7Ver.
2.63; http://www2.medis.or.jp/stdcd/byomei/8The data (B) and (C), which were used for validation andtest, were excluded from this feature selection process.notations varied from 100 to 1000 in this exper-iment.
We prioritized the occurrences of eachword in the list using a selective sampling tech-nique.
We used label entropy (Anderson et al,2006), H(yst) =?yst?Y stP??
(yst|x) lnP??
(yst|x), as importance metric of each word occurrence,where??
is the model parameter of CRFS, and yst=(yt, yt+1, ?
?
?
, ys) ?
Ystis a subsequence startingat t and ending at s in y.
Intuitively, this metricrepresents the prediction confidence of CRFS.9Astraining data, we mixed the complete annotations(A) and these partial annotations on data (C1) be-cause that performance was better than using onlythe partial annotations.We used conjugate gradient method to find thelocal maximum value with the initial value beingset to be the parameter vector of CRFS.
Since theamount of annotated data for the target domain waslimited, the hyper-parameter ?
was selected usingthe corpus (B).For the comparison with the proposed method,the CRFs were trained using the most probablelabel sequences consistent with L (denoted asargmax).
The most probable label sequences werepredicted by the CRFS.
Also, we used a point-wiseclassifier, which independently learns/classifieseach character boundary and just ignores the unan-notated positions in the learning phase.
As thepoint-wise classifier, we implemented a maximumentropy classifier which uses the same features andoptimizer as CRFs.Figure 3 shows the performance comparisonsvarying the number of word annotations.
Thecombination of both the proposed method and theselective sampling method showed that a smallnumber of word annotations effectively improvedthe word segmentation performance.
In addi-tion, the proposed method significantly outper-formed argmax and point-wise classifier based onthe Wilcoxon signed rank test at the significancelevel of 5%.
This result suggests that the pro-posed method maintains CRFs?
advantage over thepoint-wise classifier and properly incorporates par-tial annotations.5.2 Part-of-speech Tagging TaskIn this section, we show the results of the POS tag-ging experiments to assess the proposed methodusing ambiguous annotations.9We selected word occurrences in a batch mode since eachtraining of the CRFs takes too much time for interactive use.901Ex.1 Ex.2ambiguous sentences (training) 118unique sentences (training) 1,480 2,960unique sentences (test) 11,840Table 4: Training and test data for POS tagging.As mentioned in Section 2.2, there are wordswhich have two or more candidate POS tags in thePTB corpus (Marcus et al, 1993).
In this experi-ment, we used 118 sentences in which some words(82 distinct words) are annotated with ambiguousPOS tags, and these sentences are called the POSambiguous sentences.
On the other hand, we callsentences in which the POS tags of these terms areuniquely annotated as the POS unique sentences.The goal of this experiment is to effectively im-prove the tagging performance using both thesePOS ambiguous sentences and the POS uniquesentences as the training data.
We assume that theamount of training data is not sufficient to ignorethe POS ambiguous sentences, or that the POS am-biguous sentences make up a substantial portion ofthe total training data.
Therefore we used a smallpart (1/10 or 1/5) of the POS unique sentences fortraining the CRFs and evaluated their performanceusing other (4/5) POS unique sentences.
We con-ducted two experiments in which different num-bers of unique sentences were used in the trainingphases, and these settings are summarized in Ta-ble 4.The feature sets for each word are the case-insensitive spelling, the orthographic features ofthe current word, and the sentence?s last word.
Theorthographic features are whether a spelling beginswith a number or an upper case letter; whetherit begins with an upper case letter and contains aperiod (?.?
); whether it is all upper case letters orall lower case letters; whether it contains a punc-tuation mark or a hyphen; and the last one, two,and three letters of the word.
Also, the sentence?slast word corresponds to a punctuation mark (e.g.?.
?, ??
?, ?!?).
We employed only features that ap-peared more than once.
The total number of re-sulting distinct features was about 14K.
Althoughsome symbols are treated as distinct tags in thePTB tag definitions, we aggregated these symbolsinto a symbol tag (SYM) since it is easy to restoreoriginal symbol tags from the SYM tag.
Then, thenumber of the resulting tags was 36.For the comparison with the proposed method(mrg), we used three heuristic rules that disam-biguated the annotated candidate POS tags in thePOS ambiguous sentences.
These rules selected aPOS tag 1) at random, 2) as the first one in thedescription order10, 3) as the most frequent tagin the corpus.
In addition, we evaluated the casewhen the POS ambiguous sentences are 4) dis-carded from the training data.For evaluation, we employed the Precision(P) and Average Precision for Ambiguous words(APA):P=# of correctly tagged word# of all word occurrences?100?APA=1|A|?w?A# of the correctly tagged w# of all occurrences of w?100?where A is a word set and is composed of the wordfor which at least one of its occurrences is ambigu-ously annotated.
Here, we employed APA to eval-uate each ambiguous words equally, and |A| was82 in this experiment.
Again, we used the conju-gate gradient method to find the local maximumvalue with the initial value being set to be the pa-rameters obtained in the CRF learning for the dis-carded setting.Table 5 shows the average performance of POStagging over 5 different POS unique data.
Sincethe POS ambiguous sentences are only a fractionof all of the training data, the overall performance(P) was slightly improved by the proposed method.However, according to the performance for am-biguously annotated words (APA), the proposedmethod outperformed other heuristics for POS dis-ambiguation.
The P and APA scores betweenthe proposed method and the comparable methodsare significantly different based on the Wilcoxonsigned rank test at the 5% significance level.
Al-though the performance improvement in this POStagging task was moderate, we believe the pro-posed method will be more effective to the NLPtasks whose corpus has a considerable number ofambiguous annotations.6 Related WorkPereira and Schabes (1992) proposed a grammaracquisition method for partially bracketed corpus.Their work can be considered a generative modelfor the tree structure output problem using partialannotations.
Our discriminative model can be ex-tended to such parsing tasks.10Although the order in which the candidate tags appearhas not been standardized in the PTB corpus, we assume thatannotators might order the candidate tags with their confi-dence.902mrg random first frequent discardedEx.1P 94.39 94.27 94.26 94.27 94.19APA 73.10 71.58 72.65 71.68 71.91Ex.2P 95.08 94.98 94.97 94.97 94.98APA 76.70 74.27 75.28 74.32 75.16Table 5: The average POS tagging performance over 5 trials.Our model is interpreted as one of the CRFswith hidden variables (Quattoni et al, 2004).There are previous work which handles hiddenvariables in discriminative parsers (Clark and Cur-ran, 2006; Petrov and Klein, 2008).
In their meth-ods, the objective functions are also formulated assame as equation (3).For interactive annotation, Culotta et al (2006)proposed corrective feedback that effectively re-duces user operations utilizing partial annotations.Although they assume that the users correct en-tire label structures so that the CRFs are trained asusual, our proposed method extends their systemwhen the users cannot annotate all of the labels ina sentence.7 Conclusions and Future WorkWe are proposing a parameter estimation methodfor CRFs incorporating partial or ambiguous an-notations of structured data.
The empirical resultssuggest that the proposed method reduces the do-main adaptation costs, and improves the predictionperformance for the linguistic phenomena that aresometimes difficult for people to label.The proposed method is applicable to otherstructured output tasks in NLP, such as syntacticparsing, information extraction, and so on.
How-ever, there are some NLP tasks, such as the wordalignment task (Taskar et al, 2005), in which it isnot possible to efficiently calculate the sum scoreof all of the possible label configurations.
Re-cently, Verbeek and Triggs (2008) independentlyproposed a parameter estimation method for CRFsusing partially labeled images.
Although the ob-jective function in their formulation is equivalentto equation (3), they used Loopy Belief Propaga-tion to approximate the sum score for their ap-plication (scene segmentation).
Their results im-ply these approximation methods can be used forsuch applications that cannot use dynamic pro-gramming techniques.AcknowledgmentsWe would like to thank the anonymous reviewersfor their comments.
We also thank Noah Smith,Ryu Iida, Masayuki Asahara, and the membersof the T-PRIMAL group for many helpful discus-sions.ReferencesAnderson, Brigham, Sajid Siddiqi, and Andrew Moore.2006.
Sequence selection for active learning.
Tech-nical Report CMU-IR-TR-06-16, Carnegie MellonUniversity.Beers, Mark H. 2004.
The Merck Manual of MedicalInformation (in Japanese).
Nikkei Business Publi-cations, Inc, Home edition.Clark, Stephen and James R. Curran.
2006.
Par-tial training for a lexicalized-grammar parser.
InProceedings of the Annual Meeting of the NorthAmerican Association for Computational Linguis-tics, pages 144?151.Culotta, Aron, Trausti Kristjansson, Andrew McCal-lum, and Paul Viola.
2006.
Corrective feedback andpersistent learning for information extraction.
Artifi-cial Intelligence Journal, 170:1101?1122.Keene, Donald, Hiroyoshi Hatori, Haruko Yamada, andShouko Irabu, editors.
1992.
Japanese-English Sen-tence Equivalents (in Japanese).
Asahi Press, Elec-tronic book edition.Kudo, Taku, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields toJapanese morphological analysis.
In Proceedings ofEmpirical Methods in Natural Language Processing.Lafferty, John, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the 18th International Con-ference on Machine Learning.Luo, Xiaoquan.
2003.
A maximum entropy chinesecharacter-based parser.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 192?199.Marcus, Mitchell P., Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large annotatedcorpus of English: The Penn treebank.
Computa-tional Linguistics, 19(2).Mori, Shinsuke.
2006.
Language model adaptationwith a word list and a raw corpus.
In Proceedingsof the 9th International Conference on Spoken Lan-guage Processing.903Peng, Fuchun, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detectionusing conditional random fields.
In Proceedings ofthe International Conference on Computational Lin-guistics.Pereira, Fernando C. N. and Yves Schabes.
1992.Inside-outside reestimation from partially bracketedcorpora.
In Proceedings of Annual Meeting Associ-ation of Computational Linguistics, pages 128?135.Petrov, Slav and Dan Klein.
2008.
Discriminativelog-linear grammars with latent variables.
In Ad-vances in Neural Information Processing Systems,pages 1153?1160, Cambridge, MA.
MIT Press.Quattoni, Ariadna, Michael Collins, and Trevor Darrell.2004.
Conditional random fields for object recogni-tion.
In Advances in Neural Information ProcessingSystems.Sha, Fei and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceedingsof Human Language Technology-NAACL, Edmon-ton, Canada.Taskar, Ben, Simon Lacoste-Julien, and Dan Klein.2005.
A discriminative matching approach to wordalignment.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.Verbeek, Jakob and Bill Triggs.
2008.
Scene segmen-tation with CRFs learned from partially labeled im-ages.
In Advances in Neural Information ProcessingSystems, pages 1553?1560, Cambridge, MA.
MITPress.Appendix A Computation of Objectiveand Derivative functionsHere we explain the effective computation proce-dure for equation (3) and (4) using dynamic pro-gramming techniques.Under the first-order Markov assumption11, twotypes of features are usually used: one is pairs ofan observed variable and a label variable (denotedas f(xt, yt) : X ?
Y ), the other is pairs of twolabel variables (denoted as g(yt?1, yt) : Y ?
Y )at time t. Then the feature vector can be de-composed as ?
(x, y) =?T+1t=1?
(xt, yt?1, yt)where ?
(xt, yt?1, yt) = f(xt, yt) + g(yt?1, yt).In addition, let S and E be special label vari-ables to encode the beginning and ending of a se-quence, respectively.
We define ?
(xt, yt?1, yt) tobe ?
(xt, S, yt) at the head t = 1 and g(yt?1, E) atthe tail where t = T + 1.
The technique of the ef-fective calculation of the normalization value is the11Note that, although the rest of the explanation based onthe first-order Markov models for purposes of illustration, thefollowing arguments are easily extended to the higher orderMarkov CRFs and semi-Markov CRFs.precomputation of the ?
?,x,L[t, j], and?
?,x,L[t, j]matrices with given ?,x, and L. The matrices ?and ?
are defined as follows, and should be cal-culated in the order of t = 1, ?
?
?
, T , and t =T + 1, ?
?
?
, 1, respectively?
?,x,L[t, j]=????????
?0 if j /?
Lt?
?
?
(xt, S, j) else if t = 1ln?i?Lt?1e?[t?1,i]+??ffi(xt,i,j)else?
?,x,L[t, j]=????????
?0 if j /?
Lt?
?
g(j, E) else if t = T + 1ln?k?Lt+1e??ffi(xt,j,k)+?
[t+1,k]elseNote that L = (Y, ?
?
?
, Y ) is used to calculate allthe entries in Y .
In the rest of this section, we omitthe subscripts ?, x, and L of ?, ?, Z unless mis-understandings could occur.
The time complexityof the ?
[t, j] or ?
[t, j] computation is O(T |Y |2).Finally, equations (3) and (4) are efficiently cal-culated using ?, ?.
The logarithm of Z in equation(3) is calculated as:lnZ?,YL= ln?j?LTe??,L[T,j]+?
?g(j,E).Similarly, the first and second terms of equation(4) can be computed as:?y?YLP?,L(y|x)?
(x, y) =?i?LT?L(T, i, E)g(i, E)+T?t=1?j?Lt??
?L(t, j)f(xt, j) +?i?Lt?1?L(t, i, j)g(i, j)?
?where ?, x are omitted in this equation, and ?
?,x,Land ?
?,x,L are the marginal probabilities:?
?,x,L(t, j) = P?,L(yt = j|x)= e?[t,j]+?
[t,j]?ln ZYL, and?
?,x,L(t, i, j) = P?,L(yt?1 = i, yt = j|x)= e?[t?1,i]+??ffi(xt,i,j)+?
[t,j]?ln ZYL.Note that YL is replaced with Y and L =(Y, ?
?
?
, Y ) to compute the second term.904
