Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1301?1309,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsDiscriminating Gender on TwitterJohn D. Burger and John Henderson and George Kim and Guido ZarrellaThe MITRE Corporation202 Burlington RoadBedford, Massachusetts, USA 01730{john,jhndrsn,gkim,jzarrella}@mitre.orgAbstractAccurate prediction of demographic attributes fromsocial media and other informal online content isvaluable for marketing, personalization, and legal in-vestigation.
This paper describes the construction ofa large, multilingual dataset labeled with gender, andinvestigates statistical models for determining thegender of uncharacterized Twitter users.
We exploreseveral different classifier types on this dataset.
Weshow the degree to which classifier accuracy variesbased on tweet volumes as well as when variouskinds of profile metadata are included in the models.We also perform a large-scale human assessment us-ing Amazon Mechanical Turk.
Our methods signifi-cantly out-perform both baseline models and almostall humans on the same task.1 IntroductionThe rapid growth of social media in recent years, exem-plified by Facebook and Twitter, has led to a massivevolume of user-generated informal text.
This in turn hassparked a great deal of research interest in aspects of so-cial media, including automatically identifying latent de-mographic features of online users.
Many latent featureshave been explored, but gender and age have generatedgreat interest (Schler et al, 2006; Burger and Henderson,2006; Argamon et al, 2007; Mukherjee and Liu, 2010;Rao et al, 2010).
Accurate prediction of these featureswould be useful for marketing and personalization con-cerns, as well as for legal investigation.In this work, we investigate the development of high-performance classifiers for identifying the gender ofTwitter users.
We cast gender identification as the ob-vious binary classification problem, and explore the useof a number of text-based features.
In Section 2, we de-scribe our Twitter corpus, and our methods for labelinga large subset of this data for gender.
In Section 3 wediscuss the features that are used in our classifiers.
Wedescribe our Experiments in Section 4, including our ex-ploration of several different classifier types.
In Section 5we present and analyze performance results, and discusssome directions for acquiring additional data by simpleself-training techniques.
Finally in Section 6 we summa-rize our findings, and describe extensions to the work thatwe are currently exploring.2 DataTwitter is a social networking and micro-blogging plat-form whose users publish short messages or tweets.
Inlate 2010, it was estimated that Twitter had 175 millionregistered users worldwide, producing 65 million tweetsper day (Miller, 2010).
Twitter is an attractive venuefor research into social media because of its large vol-ume, diverse and multilingual population, and the gener-ous nature of its Terms of Service.
This has led many re-searchers to build corpora of Twitter data (Petrovic et al,2010; Eisenstein et al, 2010).
In April 2009, we begansampling data from Twitter using their API at a rate ofapproximately 400,000 tweets per day.
This representedapproximately 2% of Twitter?s daily volume at the time,but this fraction has steadily decreased to less than 1% by2011.
This decrease is because we sample roughly thesame number of tweets every day while Twitter?s overallvolume has increased markedly.
Our corpus thus far con-tains approximately 213 million tweets from 18.5 millionusers, in many different languages.In addition to the tweets that they produce, each Twitteruser has a profile with the following free-text fields:?
Screen name (e.g., jsmith92, kingofpittsburgh)?
Full name (e.g., John Smith, King of Pittsburgh)?
Location (e.g., Earth, Paris)?
URL (e.g., the user?s web site, Facebook page, etc.)?
Description (e.g., Retired accountant and grandfa-ther)All of these except screen name are completely op-tional, and all may be changed at any time.
Note that none1301Users TweetsTraining 146,925 3,280,532Development 18,380 403,830Test 18,424 418,072Figure 1: Dataset Sizesof the demographic attributes we might be interested inare present, such as gender or age.
Thus, the existingprofile elements are not directly useful when we wish toapply supervised learning approaches to classify tweetsfor these target attributes.
Other researchers have solvedthis problem by using labor-intensive methods.
For ex-ample, Rao et al (2010) use a focused search methodol-ogy followed by manual annotation to produce a datasetof 500 English users labeled with gender.
It is infeasibleto build a large multilingual dataset in this way, however.Previous research into gender variation in online dis-course (Herring et al, 2004; Huffaker, 2004) has foundit convenient to examine blogs, in part because blog sitesoften have rich profile pages, with explicit entries for gen-der and other attributes of interest.
Many Twitter usersuse the URL field in their profile to link to another facetof their online presence.
A significant number of userslink to blogging websites, and many of these have well-structured profile pages indicating our target attributes.
Inmany cases, these are not free text fields.
Users on thesesites must select gender and other attributes from drop-down menus in order to populate their profile informa-tion.
Accordingly, we automatically followed the TwitterURL links to several of the most represented blog sitesin our dataset, and sampled the corresponding profiles.By attributing this blogger profile information to the as-sociated Twitter account, we created a corpus of approx-imately 184,000 Twitter users labeled with gender.We partitioned our dataset by user into three distinctsubsets, training, development, and test, with sizes as in-dicated in Figure 1.
That is, all the tweets from each userare in a single one of the three subsets.
This is the corpuswe use in the remainder of this paper.This method of gleaning supervised labels for ourTwitter data is only useful if the blog profiles are in turnaccurate.
We conducted a small-scale quality assurancestudy of these labels.
We randomly selected 1000 Twitterusers from our training set and manually examined thedescription field for obvious indicators of gender, e.g.,mother to 3 boys or just a dude.
Only 150 descriptions(15% of the sample) had such an explicit gender cue.
136of these also had a blog profile with the gender selected,and in all of these the gender cue from the user?s Twit-ter description agreed with the corresponding blog pro-file.
This may only indicate that people who misrepresenttheir gender are simply consistent across different aspectsof their online presence.
However, the effort involved inmaintaining this deception in two different places sug-gests that the blog labels on the Twitter data are largelyreliable.Initial analysis using the blog-derived labels showedthat our corpus is composed of 55% females and 45%males.
This is consistent with the results of an earlierstudy which used name/gender correlations to estimatethat Twitter is 55% female (Heil and Piskorski, 2009).Figure 2 shows several statistics broken down by gender,including the Twitter users who did not indicate their gen-der on their blog profile.
In our dataset females tweet at ahigher rate than males and in general users who providetheir gender on their blog profile produce more tweetsthan users who do not.
Additionally, of the 150 userswho provided a gender cue in their Twitter user descrip-tion, 105 were female (70%).
Thus, females appear morelikely to provide explicit indicators about their gender inour corpus.The average number of tweets per user is 22 and isfairly consistent across our traing/dev/test splits.
Thereis wide variance, however, with some users representedby only a single tweet, while the most prolific user in oursample has nearly 4000 tweets.It is worth noting that many Twitter users do not tweetin English.
Table 3 presents an estimated breakdown oflanguage use in our dataset.
We ran automatic languageID on the concatenated tweet texts of each user in thetraining set.
The strong preponderance of English in ourdataset departs somewhat from recent studies of Twitterlanguage use (Wauters, 2010).
This is likely due in part tosampling methodology differences between the two stud-ies.
The subset of Twitter users who also use a blog sitemay be different from the Twitter population as a whole,and may also be different from the users tweeting duringthe three days of Wauters?s study.
There are also possiblelongitudinal differences: English was the dominant lan-guage on Twitter when the online service began in 2006,and this was still the case when we began sampling tweetsin 2009, but the proportion of English tweets had steadilydropped to about 50% in late 2010.
Note that we do notuse any explicit encoding of language information in anyof the experiments described below.Our Twitter-blog dataset may not be entirely represen-tative of the Twitter population at general, but this hasat least one advantage.
As with any part of the Inter-net, spam is endemic to Twitter.
However by samplingonly Twitter users with blogs we have largely filtered outspammers from our dataset.
Informal inspection of a fewthousand tweets revealed a negligible number of commer-cial tweets.3 FeaturesTweets are tagged with many sources of potentially dis-criminative metadata, including timestamps, user color1302Users Tweets Mean tweetsCount Percentage Count Percentage per userFemale 100,654 42.3% 2,429,621 47.7% 24.1Male 83,075 35.0 1,672,813 32.8 20.1Not provided 53,817 22.7 993,671 19.5 18.5Figure 2: Gender distribution in our blog-Twitter datasetLanguage Users PercentageEnglish 98,004 66.7%Portuguese 21,103 14.4Spanish 8,784 6.0Indonesian 6,490 4.4Malay 1,401 1.0German 1,220 0.8Chinese 985 0.7Japanese 962 0.7French 878 0.6Dutch 761 0.5Swedish 686 0.5Filipino 643 0.4Italian 631 0.4Other 4,377 3.0Figure 3: Language ID statistics from training setpreferences, icons, and images.
We have restricted ourexperiments to a subset of the textual sources of featuresas listed in Figure 4.We use the content of the tweet text as well as threefields from the Twitter user profile described in Section 2:full name, screen name, and description.
For each user inour dataset, a field is in general a set of text strings.
Thisis obviously true for tweet texts but is also the case forthe profile-based fields since a Twitter user may changeany part of their profile at any time.
Because our sam-ple spans points in time where users have changed theirscreen name, full name or description, we include all ofthe different values for those fields as a set.
In addition,a user may leave their description and full name blank,which corresponds to the empty set.In general, our features are quite simple.
Both word-and character-level ngrams from each of the four fieldsare included, with and without case-folding.
Our fea-ture functions do not count multiple occurrences of thesame ngram.
Initial experiments with count-valued fea-ture functions showed no appreciable difference in per-formance.
Each feature is a simple Boolean indicatorrepresenting presence or absence of the word or characterngram in the set of text strings associated with the partic-ular field.
The extracted set of such features representsthe item to the classifier.For word ngrams, we perform a simple tokenizationFeature extractionCharngramsWordngramsDistinctfeaturesScreen name 1?5 none 432,606Full name 1?5 1 432,820Description 1?5 1?2 1,299,556Tweets 1?5 1?2 13,407,571Total 15,572,522Figure 4: Feature types and countsthat separates words at transitions between alphanumericcharacters and non-alphanumeric.1 We make no attemptto tokenize unsegmented languages such as Chinese, nordo we perform morphological analysis on language suchas Korean; we do no language-specific processing at all.We expect the character-level ngrams to extract useful in-formation in the case of such languages.Figure 4 indicates the details and feature counts for thefields from our training data.
We ignore all features ex-hibited by fewer than three users.4 ExperimentsWe formulate gender labeling as the obvious binary clas-sification problem.
The sheer volume of data presentsa challenge for many of the available machine learningtoolkits, e.g.
WEKA (Hall et al, 2009) orMALLET (Mc-Callum, 2002).
Our 4.1 million tweet training corpuscontains 15.6 million distinct features, with feature vec-tors for some experiments requiring over 20 gigabytesof storage.
To speed experimentation and reduce thememory footprint, we perform a one-time feature genera-tion preprocessing step in which we convert each featurepattern (such as ?caseful screen name character trigram:Joh?)
to an integer codeword.
The learning algorithmsdo not access the codebook at any time and instead dealsolely with vectors of integers.
We compress the data fur-ther by concatenating all of a user?s features into a singlevector that represents the union of every tweet producedby that user.
This condenses the dataset to about 180,000vectors occupying 11 gigabytes of storage.We performed initial feasibility experiments using awide variety of different classifier types, including Sup-port Vector Machines, Naive Bayes, and Balanced Win-1We use the standard regular expression pattern \b.1303now2 (Littlestone, 1988).
These initial experiments werebased only on caseful word unigram features from tweettexts, which represent less than 3% of the total featurespace but still include large numbers of irrelevant fea-tures.
Performance as measured on the development setranged from Naive Bayes at 67.0% accuracy to BalancedWinnow2 at 74.0% accuracy.
A LIBSVM (Chang andLin, 2001) implementation of SVM with a linear ker-nel achieved 71.8% accuracy, but required over fifteenhours of training time while Winnow needed less thanseven minutes.
No classifier that we evaluated was ableto match Winnow?s combination of accuracy, speed, androbustness to increasing amounts of irrelevant features.We built our own implementation of the BalancedWin-now2 algorithm which allowed us to iterate repeatedlyover the training data on disk rather than caching the en-tire dataset in memory.
This reduced our memory re-quirements to the point that we were able to train on theentire dataset using a single machine with 8 gigabytes ofRAM.We performed a grid search to select learning parame-ters by measuring their affect on Winnow?s performanceon the development set.
We found that two sets of pa-rameters were required: a low learning rate (0.03) waseffective when using only one type of input feature (suchas only screen name features, or only tweet text features),and a higher learning rate (0.20) was required when mix-ing multiple types of features in one classifier.
In bothcases we used a relatively large margin (35%) and cooledthe learning rate by 50% after each iteration.These learning parameters were used during all of theexperiments that follow.
All gender prediction modelswere trained using data from the training set and evalu-ated on data from the development set.
The test set washeld out entirely until we finalized our best performingmodels.4.1 Field combinationsWe performed a number of experiments with the Winnowalgorithm described above.
We trained it on the train-ing set and evaluated on the development set for each ofthe four user fields in isolation, as well as various com-binations, in order to simulate different use cases for sys-tems that perform gender prediction from social mediasources.
In some cases we may have all of the metadatafields available above, while in other cases we may onlyhave a sample of a user?s tweet content or perhaps justone tweet.
We simulated the latter condition by randomlyselecting a single tweet for each dev and test user; thistweet was used for all evaluations of that user under thesingle-tweet condition.
Note, however, that for trainingthe single tweet classifier, we do not concatenate all of auser?s tweets as described above.
Instead, we pair eachuser in the training set with each of their tweets in turn,in order to take advantage of all the training data.
Thisamounted to over 3 million training instances for the sin-gle tweet condition.We paid special attention to three conditions: singletweet, all fields, and all tweets.
For these conditions, weevaluated the learned models on the training data, the de-velopment set, and the test set, to study over-training andgeneralization.
Note that for all experiments, the evalua-tion includes some users who have left their full name ordescription fields blank in their profile.In all cases, we compare results to a maximum likeli-hood baseline that simply labels all users female.4.2 Human performanceWe wished to compare our classifier?s efficacy to humanperformance on the same task.
A number of researchershave recently experimented with the use of Amazon Me-chanical Turk (AMT) to create and evaluate human lan-guage data (Callison-Burch and Dredze, 2010).
AMTand other crowd-sourcing platforms allow simple tasks tobe posted online for large numbers of anonymous work-ers to complete.We used AMT to measure human performance on gen-der determination for the all tweets condition.
Each AMTworker was presented with all of the tweet texts froma single Twitter user in our development set and askedwhether the author was male or female.
We redundantlyassigned five workers to each Twitter user, for a total of91,900 responses from 794 different workers.
We experi-mented with a number of ways to combine the five humanlabels for each item, including a simple majority vote anda more sophisticated scheme using an expectation maxi-mization algorithm.4.3 Self-trainingOur final experiments were focused on exploring the useof unlabeled data, of which we have a great deal.
Weperformed some initial experiments on a self-training ap-proach to labeling more data.
We trained the all-fieldsclassifier on half of our training data, and applied it to theother half.
We trained a new classifier on this full train-ing set, which now included label errors introduced by thelimitations of the first classifier.
This provided a simula-tion of a self-training setup using half the training data.Any robust gains due to self-training should be revealedby this setup.5 Results5.1 Field combinationsFigure 5 shows development set performance on variouscombinations of the user fields, all of which outperformthe maximum likelihood baseline that classifies all usersas female.
The single most informative field with respect1304Baseline (F) 54.9%One tweet text 67.8Description 71.2All tweet texts 75.5Screen name (e.g.
jsmith92) 77.1Full name (e.g.
John Smith) 89.1Tweet texts + screen name 81.4Tweet texts + screen name + description 84.3All four fields 92.0Figure 5: Development set accuracy using various fieldsCondition Train Dev TestBaseline (F) 54.8% 54.9 54.3One tweet text 77.8 67.8 66.5Tweet texts 77.9 75.5 74.5All fields 98.6 92.0 91.8Figure 6: Accuracy on the training, development and test setsto gender is the user?s full name, which provides an accu-racy of 89.1%.
Screen name is often a derivative of fullname, and it too is informative (77.1%), as is the user?sself-assigned description (71.2).Using only tweet texts performs better than using onlythe user description (75.5% vs. 71.2).
Tweet texts aresufficient to decrease the error by nearly half over theall-female prior.
It appears that the tweet texts con-vey more about a Twitter user?s gender than their ownself-descriptions.
Even a single (randomly selected)tweet text contains some gender-indicative information(67.2%).
These results are similar to previous work.
Raoet al (2010) report results of 68.7% accuracy on genderfrom tweet texts alone using an ngram-only model, ris-ing to 72.3 with hand-crafted ?sociolinguistic-based?
fea-tures.
Test set differences aside, this is comparable withthe ?All tweet texts?
line in Figure 5, where we achievean accuracy of 75.5%.Performance of models built from various aggregatesof the four basic fields are shown in Figure 5 as well.
Thecombination of tweet texts and a screen name representsa use case common to many different social media sites,such as chat rooms and news article comment streams.The performance of this combination (81.4%) is signif-icantly higher than either of the individual components.As we have observed, full name is the single most infor-mative field.
It out-performs the combination of the otherthree fields, which perform at 84.3%.
Finally, the classi-fier that has access to features from all four fields is ableto achieve an accuracy of 92.0%.The final test set accuracy is shown in Figure 6.
Thistest set was held out entirely during development and hasbeen evaluated only with the four final models reportedRank MI Feature f P (Female|f)1 0.0170 !
0.6012 0.0164 : 0.6563 0.0163 lov 0.6874 0.0162 love 0.6805 0.0161 lov 0.6766 0.0160 love 0.6897 0.0160 !
0.6188 0.0149 :) 0.6979 0.0148 y!
0.68710 0.0145 my 0.63711 0.0143 love 0.69112 0.0143 haha 0.70513 0.0141 my 0.63414 0.0140 my 0.63715 0.0140 :) 0.69716 0.0139 my 0.63417 0.0138 !
i 0.71118 0.0138 hah 0.69819 0.0137 hah 0.71420 0.0135 so 0.66121 0.0134 haha 0.71422 0.0132 so 0.66123 0.0128 i 0.61824 0.0127 ooo 0.70825 0.0126 !
i 0.74326 0.0123 i lov 0.72827 0.0120 ove 0.67128 0.0117 ay!
0.71829 0.0116 aha 0.67830 0.0116 <3 0.85631 0.0115 cute 0.82632 0.0114 i lo 0.70433 0.0114 :)$ 0.70134 0.0110 :( 0.73135 0.0109 :)$ 0.70136 0.0109 !$ 0.61437 0.0107 ahah 0.71638 0.0106 <3 0.857464 0.0051 ht | 0.506465 0.0051 hank 0.641466 0.0051 too 0.659467 0.0051 yay!
0.818468 0.0051 http | 0.506469 0.0051 htt | 0.506624 0.0047 Googl | 0.317625 0.0047 ing!
0.718626 0.0047 hair 0.749627 0.0047 b 0.573628 0.0047 y : 0.725629 0.0046 Goog | 0.318Figure 7: A selection of tweet text features, ranked by mutualinformation.
Character ngrams in Courier, words in bold.Underscores are spaces, $ matches the end of the tweet text.| marks ?male?
features.1305in this figure.
The difference between the scores on thetrain and development sets show how well the model canfit the data.
There are features in the user name and userscreen name fields that make the data trivially separable.The tweet texts, however, present more ambiguity for thelearners.
The difference between the development andtest set scores suggest that only minimal hill-climbing oc-curred during our development.We have performed experiments to better understandhow performance scales with training data size.
Figure 8shows how performance increases for both the all-fieldsand tweet-texts-only classifiers as we train on more users,with little indication of leveling off.As discussed in Section 2, there is wide variance inthe number of tweets available from different users.
InFigure 9 we show how the tweet text classifier?s accu-racy increases as the number of tweets from the user in-creases.
Each point is the average classifier accuracy forthe user cohort with exactly that many tweets in our devset.
Performance increases given more tweets, althoughthe averages get noisy for the larger tweet sets, due tosuccessively smaller cohort sizes.Some of the most informative features from tweet textsare shown in Figure 7, ordered by mutual informationwith gender.
There are far more of these strong featuresfor the female category than the male: only five of the top1000 features are associated more strongly with males,i.e.
they have lower P (Female|feature) than the prior,P (Female) = 0.55.Some of these features are content-based (hair, andseveral fragments of love), while others are stylistic (ooo,several emoticons).
The presence of http as a strongmale feature might be taken to indicate that men includelinks in their tweet texts far more often than women,but a cursory examination seems to show instead thatwomen are simply more likely to include ?bare?
links,e.g., emnlp.org vs. http://emnlp.org.5.2 Human performanceFigure 10 shows the results of the human performancebenchmarks using Amazon Mechanical Turk.
The rawper-response performance is 60.4%, only moderately bet-ter than the all-female baseline.
When averaged acrossworkers, however, this improves substantially, to 68.7.This would seem to indicate that there were a few poorworkers who did many annotations, and in fact when welimit the performance average to those workers who pro-duced 100 or more responses, we do see a degradation to62.2.The problem of poor quality workers is endemic toanonymous crowd sourcing platforms like MechanicalTurk.
A common way to combat this is to use redun-dancy, with a simple majority vote to choose among mul-tiple responses for each item.
This allows us to treat theBaseline 54.9Average response 60.4Average worker 68.7Average worker (100 or more responses) 62.2Worker ensemble, majority vote 65.7Worker ensemble, EM-adjusted vote 67.3Winnow all-tweet-texts classifier 75.5Figure 10: Comparing with humans on the all tweet texts taskfive workers who responded to each item as an ensem-ble.
As Figure 10 indicates, this provides some improve-ment over the raw result (65.7% vs. 60.4).
A differentapproach, first proposed by Dawid and Skene (1979), isto use an expectation maximization algorithm to estimatethe quality of each source of labels, as well as estimate theposterior for each item.
In this case, the first is an AMTworker?s capability and the second is the distribution ofgender labels for each Twitter user.The Dawid and Skene approach has previously beenapplied to Mechanical Turk responses (Ipeirotis et al,2010).
We used their implementation on our AMT re-sults but with only moderate improvement over the sim-ple majority ensemble (67.3% vs. 65.7).
All of the aggre-gate human results are substantially below the all-tweet-texts classifier score, suggesting that this is a difficulttask for people to perform.
As Figure 11 indicates, mostworkers perform below 80% accuracy, and less than 5%of the prolific workers out-perform the automatic classi-fier.
These high-scoring workers may indeed be good atthe task, or they may have simply been assigned a less-difficult subset of the data.
Figure 12 illustrates this byshowing aligned worker performance and classifier per-formance on the precise set of items that each workerperformed on.
Here we see that, with few exceptions,the automatic classifier performs as well or better thanthe AMT workers on their subset.5.3 Self-trainingFinally, as described in Section 4.3, we performed someinitial experiments on a self-training approach to label-ing more data.
As described above the all-fields classi-fier achieves an accuracy of 92% on the development setwhen trained on the full training set.
Training on half ofthe training data results in a drop to 91.1%.
The sec-ond classifier trained on the full training set, but withsome label errors introduced by the first, had further de-graded performance of 90.9%.
Apparently the errorful la-bels introduced by the simplistic self-training procedureoverwhelmed any new information that might have beengained from the additional data.
We are continuing to ex-plore ways to use the large amounts of unsupervised datain our corpus.1306Figure 8: Performance increases when training with more usersFigure 9: Performance increases with more tweets from target user1307Figure 11: Human accuracy in rank order (100 responses or more), with classifier performance (line)Figure 12: Classifier vs. human accuracy on the same subsets (100 responses or more)13086 ConclusionIn this paper, we have presented several configurations ofa language-independent classifier for predicting the gen-der of Twitter users.
The large dataset used for construc-tion and evaluation of these classifiers was drawn fromTwitter users who also completed blog profile pages.These classifiers were tested on the largest set ofgender-tagged tweets to date that we are aware of.
Thebest classifier performed at 92% accuracy, and the clas-sifier relying only on tweet texts performed at 76% ac-curacy.
Human performance was assessed on this lattercondition, and only 5% of 130 humans performed 100 ormore classifications with higher accuracy than this ma-chine.In future work, we will explore how well such modelscarry over to gender identification in other informal on-line genres such as chat and forum comments.
Further-more, we have been able to assign demographic featuresbeside gender, including age and location, to our Twit-ter dataset.
We have begun to build classifiers for thesefeatures as well.AcknowledgementsThe authors would like to thank the anonymous review-ers.
This work was funded under the MITRE InnovationProgram.ReferencesShlomo Argamon, Moshe Koppel, James W. Pennebaker, andJonathan Schler.
2007.
Mining the blogosphere: Age,gender, and the varieties of self-expression.
First Monday,12(9), September.John D. Burger and John C. Henderson.
2006.
An explorationof observable features related to blogger age.
In Computa-tional Approaches to Analyzing Weblogs: Papers from the2006 AAAI Spring Symposium.
AAAI Press.Chris Callison-Burch and Mark Dredze.
2010.
Creating speechand language data with Amazon?s Mechanical Turk.
In Pro-ceedings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechanical Turk,CSLDAMT ?10.
Association for Computational Linguistics.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: a libraryfor support vector machines.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.A.P.
Dawid and A.M. Skene.
1979.
Maximum likelihood esti-mation of observer error-rates using the EM algorithm.
Jour-nal of the Royal Statistical Society.
Series C (Applied Statis-tics), 28(1).Jacob Eisenstein, Brendan O?Connor, Noah A. Smith, andEric P. Xing.
2010.
A latent variable model for geographiclexical variation.
In Conference on Empirical Methods onNatural Language Processing.Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer,Peter Reutemann, and Ian H. Witten.
2009.
The WEKA datamining software: An update.
SIGKDD Explorations, 11(1).Bill Heil and Mikolaj Jan Piskorski.
2009.
New Twitter re-search: Men follow men and nobody tweets.
Harvard Busi-ness Review, June 1.Susan C. Herring, Inna Kouper, Lois Ann Scheidt, and Eli-jah L. Wright.
2004.
Women and children last: The discur-sive construction of weblogs.
In L. Gurak, S. Antonijevic,L.
Johnson, C. Ratliff, and J. Reyman, editors, Into the Bl-ogosphere: Rhetoric, Community, and Culture of Weblogs.http://blog.lib.umn.edu/blogosphere/.David Huffaker.
2004.
Gender similarities and differences inonline identity and language use among teenage bloggers.Master?s thesis, Georgetown University.
http://cct.georgetown.edu/thesis/DavidHuffaker.pdf.Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010.Quality management on Amazon Mechanical Turk.
InProceedings of the Second Human Computation Workshop(KDD-HCOMP 2010).Nick Littlestone.
1988.
Learning quickly when irrelevant at-tributes abound: A new linear-threshold algorithm.
MachineLearning, 2, April.Andrew Kachites McCallum.
2002.
MALLET: A machinelearning for language toolkit.
http://mallet.cs.umass.edu.Claire Cain Miller.
2010.
Why Twitter?s C.E.O.demoted himself.
New York Times, October 30.http://www.nytimes.com/2010/10/31/technology/31ev.html.Arjun Mukherjee and Bing Liu.
2010.
Improving gender clas-sification of blog authors.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural Language Process-ing, Cambridge, MA, October.
Association for Computa-tional Linguistics.Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010.
TheEdinburgh Twitter corpus.
In Computational Linguistics in aWorld of Social Media.
AAAI Press.
Workshop at NAACL.Delip Rao, David Yarowsky, Abhishek Shreevats, and ManaswiGupta.
2010.
Classifying latent user attributes in Twitter.In 2nd International Workshop on Search and Mining User-Generated Content.
ACM.Jonathan Schler, Moshe Koppel, Shlomo Argamon, and JamesPennebaker.
2006.
Effects of age and gender on blogging.In Computational Approaches to Analyzing Weblogs: Papersfrom the 2006 AAAI Spring Symposium.
AAAI Press, March.Robin Wauters.
2010.
Only 50% of Twitter mes-sages are in English, study says.
TechCrunch, Febru-ary 1. http://techcrunch.com/2010/02/24/twitter-languages/.1309
