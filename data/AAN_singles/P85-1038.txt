DICTIONARIES OF THE HINDGeorge A. MillerDepartment of PsychologyPrinceton UniversityPrinceton, NJ 08544, USAABSTRACTHow lexical information should beformulated, and how it is organized incomputer memory for rapid retrieval, arecentral questions for computationallinguists who want to create systems forlanguage understanding.
How lexicalknowledge is acquired, and how it isorganized in human memory for rapidretrieval during language use, are alsocentral questions for cognitive psycholo-gists.
Some examples of psycholinguisticresearch on the lexical component oflanguage are reviewed with special atten-tion to their implications for the compu-tational problem.INTRODUCTIONI would like to describe some recentpsychological research on the nature andorganization of lexical knowledge, yet tointroduce it that way, as research on thenature and organization of lexicalknowledge, usually leaves the impressionthat it is abstract and not verypractical.
But that impression is pre-cisely wrong; the work is very practicaland not at all abstract.
So I shall takea different tack.Computer scientists -- those in ar-tificial intelligence especlally -- some-times introduce their work by emphasizingits potential contribution to an under-standing of the human mind.
I propose toadopt that strategy in reverse: to intro-duce work in psychology by emphasizingIts potential contribution to the devel-opment of information processing andcommunication systems.
We may both bewrong, of course, but at least thisstrategy indicates a spirit of coopera-tion.Let me sketch a general picture ofthe future.
You may not share my expec-tations, but once you see where I thinkevents are leading, you will understandwhy I believe that research on the natureand organization of lezical knowledge isworth doing.
You may disagree, but atleast you will understand.Some Technological AssumptionsI assume that computers are going tobe directly linked by communication net-works.
Even now, in local area networks,a workstation can access information onany disk connected anywhere in the net.Soon such networks will not be locallyrestricted.
The model that is emergingis of a very large computer whose partsare geographically distributed; largecorporations, government agencies, uni-versity consortia, groups of scientists,and others who can afford it will beworking together in shared informationenvironments.
For example, someday theAssociation foe Computational Linguisticswill maintain and update an exhaustiveknowledge base immediately accessible toall computational linguists.Our present conception of computersas distinct objects will not fade away --the local workstation seems destined togrow smaller and more powerful every year-- but developments in networking willallow users to think of their own work-stations not merely as computers, but aswindows into a vast information spacethat they can use however they desire.Most of the parts needed for such asystem already exist, and fiber optictechnology will soon transmit broadbandsignals over long distances at affordablecosts.
Putting the parts together intolarge, non-local networks is no trivialtask, but it will happen.Computer scientists probably havetheir own versions of this story, but nospecial expertise is required to see thatrapid progress lies ahead.
Moreover,this development will have implicationsfor cognitive psychology.
However thetechnological implementation works out,at least one aspect raises questions ofconsiderable psychological interest: inparticular, how will people use it?
Whatkind of man-machine interface will therebe?305What might lie "beyond the key-board," as one futurist has put it (Bolt,1984), has been a subject for much crea-tive speculation, since the possibilitiesare numerous and diverse.
Although nosingle interface will be optimal forevery use, many users will surely want tointeract with the system in somethingreasonably close to a natural language.Indeed, if the development of informationnetworks is to be financed by those whouse them, the interface will have to beas natural as possible -- which meansthat natural language processing will bea part of the interface.Natural  Language InterfacesNatural language interfaces to largeknowledge bases are going to become gen-erally available.
The only question iswhen.
How long will it take?
Systemsalready exist that converse and answerquestions on restricted topics.
How muchremains to be done?Before these systems will be gener-ally useful, three difficult requirementswill have to be met.
An interface must:(1) have access to a large, general-pur-pose knowledge base; (2) be able to dealwith an enormous vocabulary~ (3) be ableto reason in ways that human users findfamiliar.
Other features would be highlydesirable (e.g., automatic speech recog-nition, digital processing of images,spatially distributed displays of infor-mation), but the three listed above seemcritical.Requirement (I) will be met by thecreation of the network.
How a user'sspecial interests will shape the organ-ization of his knowledge base and hislocally resident programs poses fascin-ating problems, but I do not understandthem well enough to comment.
I simplyassume that eventually every user canhave at his disposal, either locally orremotely, whatever data bases and expertsystems he desires.Requirement (3), the ability to drawinferences as people do, is probably themost difficult.
It is not likely to be"solved" by any single insight, but arobust system for revising belief struc-tures will be an essential component ofany satisfactory interface.
I believethat psychologists and other cognitivescientists have much to contribute to thesolution of this problem, but the mostpromising work to date has been done bycomputer scientists.
Since I have littleto say about the problem other than howdifficult it is, I will turn instead torequirement (2), which seems more trac-table.THE VOCABULARY PROBLEMGiving a system a large vocabularyposes no difficulty in principle.
Andeveryone who has tried to develop systemsto process natural language recognizesthe importance of a large vocabulary.Thus, the vocabulary problem looks like agood place to start.
The dimensions ofthe problem are larger than might beexpected, however, so there has been somedisagreement about the best strategy.If, in addition to understanding auser's queries, the system is expected tounderstand all the words in the vastknowledge base to which it will haveaccess, then it should probably have onthe order of 250,000 lexical entries: at1,000 bytes/entry (a modest estimate),that is 250 megabytes.
Since standarddictionaries do not contain many of thewords that are printed in newspapers(Walker & Amsler, 1984), another 250,000megabytes would probably be required forproper nouns.
Since I am imagining thefuture, however, I will assume that suchlarge memories will be available inex-pensively at every user's workstation.It is not memory size per se that posesthe problem.The problem is how to get al thatinformation into a computer.
Even if youknew how the information should be repre-sented, a good lexical entry would take along time to write.
Writing 250,000 ofthem is a daunting task.No doubt there are many excitingprojects that I don't happen to knowabout, but on the basis of my perusal ofthe easily accessible literature thereseem to he two approaches to the vocabu-lary problem.
One uses a machine-read-able version of some traditional diction-ary and tries to adapt it to the needs ofa language processing system.
Call thisthe "book" approach.
The other writesiexical entries for some fragment of theEnglish lexicon, hut formulates those en-tries in a notation that is convenientfor computational manipulation.
Callthis the "demo" approach.The book approach has the advantageof including a large number of words, butthe information with each word is diffi-cult to use.
The demo approach has theadvantage that the information about eachword is easy to use, but there are usual-ly not many words.
The real problem,therefore, is how to combine these twoapproaches: how to attain the coverage ofa traditional dictionary in a computa-tionally convenient form.306QThe Book ApproachIf you adopt the book approach, whatyou want to do is translate traditionaldict ionary entries into a notation thatmakes evident to the machine the morpho-logical, syntactic, semantic, and prag-matic properties that are needed in orderto construct interpretations for senten-ces.
Since there are many entries to betranslated, the natural solution is towrite a program that will do it automa-tically.
But that is not an easy task.One reason the translations are dif-ficult is that synonyms are hard to findin a conventional dictionary.
Alpha-betical ordering is the only way that alexicographer who works by hand can keeptrack of his data, but an alphabeticalorder puts together words with similarspell ings and scatters haphazardly wordswith similar meanings.
Consequently,similar senses of different words may bewritten very differently; they may bewritten at different times and even bydifferent people.
(For example, comparethe entries for the modal verbs 'can,''must,' and 'will' in the Oxford EnglishDictionary.)
Only a very smart programcould appreciate which definit ions shouldbe paraphrases of one another.Another reason that the translationsare diff icult is that lexicographers arefond of polysemy.
It is a mark of care-ful scholarship that all the senses of aword should be distinguished; the morecareful the scholarship, the greater thenumber of distinctions.When dictionary entries are takenl iterally the results for sentence inter-pretation are ridiculous.
Consider anexample.
Suppose the language processoris asked to provide an interpretation forsome simple sentence, say:"The boy loves his mother.
"And imagine it has available the text ofMerriam-Webster's Ninth New Col leo ia teD ~ .
Ignoring sub-senses:"the" has 4 senses,"boy" has 3,"love" has 9 as a noun and 4 as averb,"his" has  2 entries, and"mother" has 4 as a noun, 3 as an ad-jective, 2 as a verb.Such numbers invite calculation.
If weassume the system has a parser able to dono more than recognize that "love" is averb and "mother" is a noun, then, on thebasis of the literal information in thisdictionary, there are 4x3x4x2x4 - 384candidate interpretations.
This calcula-tion assumes minimal parsing and maximalreliance on the dictionary.
Of course,no self-respecting parser would tolerateso many parallel interpretations of asentence, but the i l l us t ra t ion  gives afeeling for how much work a good parserdoes.
A-d all of it is done in order to"disambiguate" a sentence that nobody whoknows English would consider to be theleast ambiguous.
: Synonymy and polysemy pose seriousproblems, even before we raise the ques-tion of how to translate conventionaldefinit ions into computational ly usefulnotations.
Any system will have to copewith synonymy and polysemy, of course,but the book approach to the vocabularyproblem seems to raise them in acuteforms, while providing little of the in-formation required to resolve them.
Withsufficient patience this approach willsurely lead to a satisfactory solution,but no one should think it wil l be easy.The Vocabulary MatrixAs presented so far, synonymy andpolysemy appear to be two distinct prob-lems.
From another point of view, theyare merely two different ways of lookingat the same problem.In essence, a conventional dict ion-ary is simply a mapping of senses ontowords, and a mapping can be convenientlyrepresented as a matrix: call it a vocab-ulary matrix.
Imagine a huge matrix withall the words in a language across thetop of the matrix, and all the differentsenses that those words can express downthe the side.
If a particular sense canbe expressed by a word, then the cell inthat row and column contains an entry;otherwise it contains nothing.
The entryitself can provide syntactic information,or examples of usage, or even a picture-- whatever the lexicographer deems im-portant enough to include.
Table 1 showsa fragment of a vocabulary matrix.Table i. Fragment of a Vocabulary MatrixColumns represent modal verbs; rowsrepresent modal senses; 'E' in a cellmeans the word in that column can expressthe sense in that row.WORDSSENSES can may _ m u ~ ~ _ M i l  1be able to E .
.
.be permitted to E E .
.
.be possible E E .
.be obliged to .
.
E .certain to be .
.
Ebe necessary .
.
Eexpected  to be  .
.
E E307Several comments should be made about thevocabulary matrix.First, it should be apparent thatany conventional dictionary can be repre-sented as a vocabulary matrix: simply adda column to the matrix for every word,and add a row to the matrix for everysense of every word that is given in theprinted dictionary.
(A lexical matrixcan be viewed as an impractical w~y ofprinting a dictionary on a single, verylarge sheet of paper.
)Second, entering such a matrix con-sists of searching down some column oracross some row.
So a vocabulary matrixcan be entered either with a word or witha sense.
Thus, one difference betweenconventional dicticnaries, which can beentered only with a word, and the dic-tionary in out mind, which can be enteredwith either words or senses, disappearswhen dictionaries are represented in thismore abstract form.Third, if you enter the matrix witha sense and search along a row, you findall the words that express that sense.When different words express the samesense, we say they are g~iQ~ym~USo Onthe other hand, if you enter the matrixwith a word and look down that column,you find all the different senses thatthat word can express.
When one word canexpress two or more senses, we say thatit is ambiguous, or ~ixsemglL~.
Thus,the two great complications of lexicalknowledge, synonymy and polysemy, areseen as complementary aspects of a singleabstract structure=Finally, since the vocabulary matrixserves only to represent the mappingbetween the two domains, it is free toexpand as new words, or new senses forfamiliar words, are added.
Of course,the number of columns is relatively fixedby the size of the vocabulary, so themajor degrees of freedom are in decidingwhat the senses are and how to representthem.The Demo ApproachWhen the question is raised of whata computationally useful lexical entryshould look like, it is time to shiftfrom the book approach to the demo ap-proach, where serious attempts have beenmade to establish a conceptual notationin which semantic interpretations can beexpressed for computational use.By "the demo approach" I mean thestrategy of building a system to processlanguage that is confined to some welldefined content area.
Since languageprocessing is a large and difficultenterprise, it is sensible to begin bytrying out one's ideas in a small way tosee whether they work.
If the ideasdon't work in a limited domain, theycertainly won't work in the unlimiteddomain of general discourse.
The resultof this approach has been a series ofprogressively more ambitious demonstra-tion programs.Among those who take this approach,two extremes can be distinguished.
Onthe one hand are those who feel thatsyntactic analysis is essential andshould be carried, if not to completion,then as far as possible before resortingto semantic information.
On the otherhand are those who prefer semantics-basedprocessing and consider syntactic cri-teria only when they get in trouble.The difference is largely one ofemphasis, since neither extreme seemswilling to rely totally on one or theother kind of information, and mostworkers would probably locate themselvessomewhere in the middle.
Since I amconcerned here with the lexical aspectsof language comprehension, however, Ishall look primarily at semantics-basedprocessing.Vocabulary SizeMost of these demos have small vo-cabularies.
It is surprising how muchyou can do with 1,500 well chosen words;a demo with more than 5,000 words wouldbe evidence of manic energy on the partof its creator.
A few thousand lexicalentries have been all that was requiredin order to test the ideas that the de-signer was interested in.The problem, of course, is thatwriting dictionary definitions is hardwork, and writing them in LISP doesn'tmake it any easier.
If you are satisfiedwith definitions that take five lines ofcode, then, obviously, you can build amuch larger dictionary than if you try tocram into an entry all the differentsenses that are found in conventionaldictionaries.
But even with shortdefinitions, a great many have to bewritten.If you want the language processorto have as large a vocabulary as theaverage user, you will have to give it atleast i00,000 words.
One way to get  afeeling for how many words that is is totranslate it into a rate of acquisition.Several years ago I looked at MildredTemplin's (1953) data that way.
Templinmeasured the vocabulary size of childrenof average intelligence at 6, 7, and 8years of age.
In two years they acquired28,300 - 13,000 = 15,300 words, which308averages out to about 21 words per day(Miller, 1977).Most people, when they hear thatresult, confess that they had no ideathat children are learning new words atsuch a rapid rate.
But the arithmeticholds just as well for computers as forchildren.
If you want the language pro-cessor to have a vocabulary of 100,000words, and if you are willing to spendten years putting definitions into it,then you will have to put in more than 27new definitions every day.How far from this goal are today'sdemos?
The answer should be simple, butit's not.
It is hard to tell exactly howmany words these systems can handle.Definitions are usually written in termsof a relatively small set of semanticprimitives, and the inheritance ofproperties is assumed wherever possible.The goal, of course, is to create anunambiguous semantic representation thatcan be used as input to an inferencingsystem, so the form of these representa-tions is much more important than theirvariety, at least in the initial experi-ments.
In the hands of a clever program-mer, a few hundred semantic primitivescan really do an enormous amount of work.Although it is often assumed thatthe fewer semantic primitives a systemrequires, the better it is, in fact thereseems to be little advantage to keepingthe number small.
When the number ofprimitives is small, definitions becomelong permutations of that small number ofdifferent atoms (Miller, 1978).
When theset of primitives gets too small, defini-tions become like machine code: the com-puter loves them, but people find themhard to read or write.C~In lng  Book and DemoHow large a set of semantic primi-tives do we need?
It is claimed thatBasic English can express any idea withonly 850 words, but that really cuts thevocabulary to the bone.
TheDictionary of Contemporary Enalish~ whichis very popular with people learningEnglish as a second language, uses aconstrained vocabulary of about 2,000words (plus some specialized terms) towrite its definitions.Using the L ~  as a guide, RichardCullingford and I tried to estimate howmuch effort would be involved in creat-ing a computationally useful lexicon.Our initial thought was to write LISPprograms for 2,000 basic terms, then useCullingford's language processor(Cullingford, 1985) to translate all ofthe definitions into LISP.
We quicklyrealized, however, that the 2,000 wordsare polysemous; different senses are usedin different definitions.
As a roughestimate, we thought 12,000 basicconcepts might suffice.An examination of the ~ defi-nitions also indicated that a great dealof information might have to be added tothe translated definitions.
Many of thesimpler conceptual dependencies (informa-tion required for disambiguation, as wellas for drawing inferences; Schank, 1975)have to be included in the definitions.Each translated definition would have tobe checked to see that all senserelations, predicate-argument structures,and selectional restrictions wereexplicit and correct, and a wide varietyof pragmatic facts (e.g., that "anyhow"in initial position signals a change oftopic) would probably have to be added.We have not undertaken this task.Not only would writing 12,000 defini-tions (and checking out and supple-menting 50,000 more) require a majorcommitment of time and energy, but we donot have Longman's permission to usetheir dictionary this way.
I report it,not as a project currently under way, butsimply as one way to think about themagnitude of the vocabulary problem.So the situation is roughly this: Inorder to have natural language interfacesto the marvellous information sourcesthat will soon be available, one thing wemust do is beef up the vocabularies thatnatural language processors can handle.That will not be an easy thing toaccomplish.
Although there is noprincipled reason why natural languageprocessors should not have vocabularieslarge enough to deal with a any domain oftopics, we are presently far from havingsuch vocabularies on llne.THE SEARCH PROBLEMAs we look ahead to having largevocabularies, we must begin to think morecarefully about the search problem.In general, the larger a data baseis, the longer it takes to locate some-thing in it.
How a large vocabulary canbe organized in human memory to permitretrieval of word meanings at conversa-tional rates is a fascinating question,especially since retrieval from thesubjective lexicon does not seem to getslower as a person's vocabulary getslarger.
The technical issues involved inachieving such performance with silicon309memories raise questions I understandonly well enough to recognize that thereare many possibilities and no easy an-swers.
Instead of speculating about thecomputer, therefore, I will take a momentto marvel at how well people manage theirlarge vocabularies.In the past fifteen years or so anumber of cognitive psychologists havebeen sufficiently impressed by people'slexical skills to design experiments thatthey hoped would reveal how people do it.This is not the time to review all thatresearch (see Simpson, 1984), but some ofthe questions that have been raised meritattention.Psychologists have considered twokinds of theories of lexical access,known as search theories and thresholdtheories.Search theories assume that a pas-sive trace is stored in the mental lexi-con and that lexical access consists ofmatching the stimulus to its memory rep-resentation.
Preliminary analysis of thestimulus is said to generate a set ofcandidates, which is searched seriallyuntil a match is found.Threshold theories claim that eachsense of every word ks an independentdetector waiting for its features tooccur.
When the feature count for anysense gets above some threshold, thatsense becomes conscious.Both kinds of theories can accountfor most of the experimental data, butnot all of it -- which is unfortunate,since a clear decision in favor of one orthe other might help to resolve the ques-tion of whether lexical access involves aserial processor with search and retrie-val, or a parallel processor with simpleactivation.
Since the brain apparentlyuses slow and noisy components, somethingsearching in parallel seems plausible,but such devices are not yet well under-s tood .Accesslnq Ambiquous WordsSome of the most interesting psycho-logical research on lexical access con-cerns how people get at the meanings ofpolysemous words.
These studies exploita phenomenon called priming: when a wordin a given lexical domain occurs, otherwords in that domain become more acces-sible.For example, a person is asked tosay, as quickly as possible, whether asequence of letters spells an Englishword.
If the word DOCTOR has just beenpresented, then NURSE will be recognizedmore rapidly than if the preceding wordhad been unrelated~ like BUTTER (Meyer &Schvaneveldt, 1971; Becket, 1980).
Therecognition of DOCTOR is said to primethe recognition of NURSE.This lexlcal decision task can beused to study polysemy if the primingword is ambiguous, and if it ks followedby probe words appropriate to its dif-ferent senses.For example, the ambiguous primePALM might be followed on some occasionsby BAND and on other occasions by TREE.The question ks whether all senses of apolysemous word are activated simultan-eously, or whether context can facili-tate one meaning and inhibit all others.Three explanations of the results ofthese experiments are presently in compe-tition.Context dependent access--Only thesense that is appropriate to the contextis retrieved or activated.Ordered access--Search starts withthe most frequent sense and continuesserially until a sense ks found that sat-isfies the context.Exhaustive access--Everything isactivated in parallel at the same time,then context selects the most appropriatesense.At present, exhaustive access seemsto be the favorite.
According to thattheory, disambiguation is a post-accessprocess; the access process itself ks acognitive "module," automatic and insul-ated from contextual influence.
My ownsuspicion is that none of these theoriesis exactly right, and that Simpson (1984)is probably closer to the truth when hesuggests that multiple meanings are ac-cessed, but that dominant meanings appearfirst and subordinate meanings come inmore slowly and then disappear.Psychological research on lexicalaccess is continuing; the complete storyis not yet ready to be told.
One aspectof the work is so obvious, however, thatits importance tends to be overlooked.Semantic FieldsThe priming phenomenon presupposesan organization of lexical knowledge intopatterns of conceptually related words,patterns that some linguists have calledsemantic fields.
Apparently a semanticfield can fluctuate in accessibility as awhole.310I have generally taken the existenceof semantic fields as evidence in favorof theories of semantic decomposition(Miller & Johnson-Laird, 1976).
The ideais that all the words in a semantic fleldshare some primitive semantic concept,and it is the activation or suppressionof that shared concept that affects theaccessibillty of the words sharing it.I will illustrate the problem by de-Scribing some research we have been doingon vocabulary growth in school children.The results indicate that we need betterways to teach new words~ with that needin mind I will return to the question ofwhat we can reasonably expect from natu-ral language interfaces.Nominal semantic fields are fre-quently organized hierarchically and soare relatively simple to appreciate.Verbal semantic fields, however, tend tobe more complex.
For example, all themotion verbs -- "move," "come," "go,""bring," "rise," "fall," "walk," "run,"=turn," and so on -- share a semanticprimitive that might be glossed as"change location as a function of time.
"In a similar manner, verbs of possession-- "possess," "have," "own," "borrow,""buy," "sell," "find," and so on -- sharea semantic primitive that has to do withEights of ownership.Not all semantic primes nucleatesemanti?
fields, however.
There is acausative primitive that differentiates"rise" and "raise," "fall" and "fell,""die" and "kill," and so on, yet thecausative verbs "raise," "fell," "kill"do not form a causative semantic field.Johnson-Laird and I distinguished twoclasses of semantic primitives: those(like motion) around which a semanticfield can form, and those (like causa-tion) used to differentiate conceptswithin a given field.Although the nature of semanticprimitives is a matter of considerableinterest to anyone who proposes a sem-antic notation for writing the defini-tions that a language processing systemwill use, they have received relativelylittle attention from psychologists.Experimental psychologlsts have a strongtendency to concentrate on questions offunction and process at the expense ofquestions of content.
Perhaps theirattempts to understand the processes ofdisambiguation will stimulate greaterinterest in these structural questions.THE PROBLEM OF CONTEXTThe reason that lexical polysemycauses so little actual ambiguity isthat, in actual use, context providesinformation that can be used to selectthe intended sense.
Although contextualdisambiguation is simple enough whenpeople do it, it is not easy for a compu-ter to do, even when the text is seman-tically well-formed.
With semanticallyill-formed input the problem is muchworse.Chi ld ren 's  Use o f  D ic t ionar iesWe have been looking at what happenswhen teachers send children to the dic-tionary to "look up a word and write asentence using it."
The results can beamusing: for example, Deese (1967) hasreported on a 7th-grade teacher who toldher class to look up "chaste" and use itin a sentence.
Their sentences included:"The milk was chaste," "The plates werestill chaste after much use," and "Theamoeba is a chaste animal.
"In order to understand what theywere doing, you have to see the diction-ary entry for "chaste':CHASTE: i. innocent of unlawful sexualintercourse.
2. celibate.
3. pure inthought and act, modest.
4. severelysimple in design or execution, austere.As Deese noted, each of the children'ssentences is compatible with informationprovided by the dictionary that they hadbeen told to consult.You might think that Deese's obser-vation was merely an amusing reflectionof some quirk in the dictionary entry foe"chaste," but that assumption would bequite wrong.
Patti Gildea and I (Miller& Gildea, 1985) have confirmed Deese'sobservation many times over.
We asked5th and 6th grade children to look wordsup and to write sentences using them.
Asof this writing, our i0- and 11-year oldfriends have written a few thousand sen-tences for us, and we are still collect-ingthem.Our goal is to discover which kindsof mistakes are most frequent.
In orderto do this, we evaluate each sentence aswe enter it into a data management systemand, if something is wrong, we describethe mistake.
By collecting our descrip-tions, we have made a first, tentativeclassification.This project is still going on, so Ican give only a preliminary report basedon about 20% of our data.
So far we haveanalyzed 457 sentences incorporating 22target words: 12 are relatively commonwords that most of the children knew, andi0 are relatively rare words with whichthey were unfamiliar.
The common words311were selected from the core vocabulary ofwords introduced by authors of 4th-gradebasal readers; the rare words were selec-ted from those introduced in 12th-gradereaders (Taylor, Frackenpohl, & White,1979).
It is convenient to refer to themas the 4th-grade words and the 12th-gradewords, respectively.Errors were relatively frequent.
Ofthe sentences classified so far, only 21%of those using 4th-grade words were suf-ficiently odd or unacceptable to indicatethat the author did not have a good graspon the meaning and use of the word, but63t of the sentences using 12th-gradewords were judged to be odd= Thus, themajority of the errors occurred with the12th-grade words.Table 2 shows our current classifi-cation.
Note that the categories are notmutually exclusive: some ingenious young-sters are able to make two oz even threemistakes in a single sentence.Table 2Classification of SentencesTYPe of.
Sentence 4th-arade 12th~azadeNo mistake 197(249) 76(208)Selectional error i0 58Wrong part of speech 4 41Wrong preposition 4 24Inappropriate topic 0 24Used rhyming word 0 14Inappropriate object 5 9Wrong entry 4 9Word not used 9 1Object missing 5 3Two senses confounded 4 3No response 0 4Not a word ?
3Unacceptable idiom 3 0Sentence not complete 3 0Most of the descriptive phrases in Table2 should be self-explanatory, but someexamples may help.
Skip the selectionalerrors; I shall say more about them in amoment.Cons ider "Wrong part of speech":a student wrote "my hobby is 1 isteningto Ouran Duran records, I have obtainedan ACCRUE for it', thus using a verb as anoun.
As an example of "Wrong prepo-sition," consider the student who wrote:aBe very METICULOUS on your work."
Anexample of "Inappropriate topic" is: "Thetrain was TRANSITORY."
An example of"Inappropriate ob ject"  is: " I  was METIC-ULOUS about falling off the cliff."
Ex-amples of "Used rhyming word" are =Did itever ACCRUE to you that Maria T. alwaysmarks with a special pencil on my face?
',"Did you evict that old TENET?
", and "Theman had a knee REPARATION o"Other categories were even less fre-quent, so return now to the most commontype of mistake, the one labelled "Selec-tional error="Vlolatlons of Seleetlonal PreferencesThe sentences that Deese reportedillustrate selectional errors.
Furtherexamples can be taken from our data= "Wehad a branch ACCRUE on our plant," "1bought a battery that was TRANSITORY,""The rocket REPUDIATE off into the sky,""John is always so TENET to me="It is unfair to call these sentences"errors" and to laugh at the children'smistakes= The students were doing theirbest to use the dictionary.
If there wasany mistake, it was made by adults whomisunderstood the nature of the task thatthey had assigned.Take the "accrue" sentence, for ex-ample= The definition that the studentssaw was:ACCRUE= come as a growth or result= "In-terest will accrue to you every yearfrom money left in a savings bank.Ability to think will accrue to youfrom good habits of study.
"We assume that the student read this def-inition looking for something she under-stood and found "come as a growth."
Shecomposed a sentence around this phrase:"We had a branch COME AS A GROWTH on ourplant', then substituted "accrue" for it.This strategy seems to account forthe other examples.
A familiar word isfound in the definition, a sentence iscomposed around it, then the unfamiliarword is substituted for the familiarword.
Some further evidence supports theclaim that something like this strategyis being used.
One intriguing clue isthat sometimes the final substitution isnot made= the written sentence containsthe word selected from the definition butnot the word that it defined.
And, sincesubstitution is not a simple mental oper-ation for children, sometimes the selec-ted word or phrase from the definition isactually written in the margin of thepaper, alongside the requested sentence.These are called selectional errorsbecause they violate selectional pref-erences.
For example, the girl who dis-covered that "stimulate" means "stir up"and so wrote, "Mrs. Jones stimulated thecake," violated the selectional prefer-ence that =stimulate" should take an ani-mate ob ject .312One reason these errors are so fre-quent is that dict ionaries do not pro-vide much information about selectionalpreferences.
We think we know how toremedy that deficiency, but that is notwhat I want to discuss here.
For themoment it suffices if you recognize thatwe have a plentiful supply ~f  sentencescontaining violations of selectionalpreferences, and that the sentences areof some educational significance.Intelligent Tutoring?Now let me pose the following ques-tion.
Could we use these sentences as a"bug catalog" in an intell igent tutoringsystem?At the moment, intell igent tutoringsystems (Sleeman & Brown, 1982) use manymenus to obtain the student's answers toquestions, and some people feel that thisis actually an advantage.
But I suspectthat if we had a good language interface,one that understood natural language re-sponses, it would soon replace the menus.In any case, imagine an intell igenttutoring system that can handle naturallanguage input.
Imagine that the tutorasked children to write sentences con-taining words that they had just seendefined, recognized when a selectionaler ror  had occurred, then undertook to ex-plain the mistake.What would the intell igent tutorhave to know in order to detect and cor-rect a selectional error?
Otherwisesaid, what more would it have to knowthan any language comprehender has toknow?The question is not rhetorical~ Iask it because I would really like toknow the answer.
In my view, it posessomething of a dilemma.
The problem, asYorick Wilks (1978) has pointed out, isthat any simple rules of co-occurrencethat we are likely to propose will, inreal discourse, be violated as often asthey are observed.
(Not only do peopleoften say one thing and mean another, butthe prevalence of figurative and idioma-tic language is consistently underesti-mated by theorists.)
If we give theintell igent tutor strict rules in orderto detect selectional errors like "Ourcar depletes gasoline," will it not alsotreat "Our car drinks gasoline" as anerror?
On the other hand, if the tutoraccepted the latter, would it not alsoaccept the former?An even simpler dilemma, one oftennoted, is that a system that blocks suchphrases as "colorless green ideas" willalso block such sentences as "There areno colorless green ideas."
If our tutorteaches chi ldren to avoid "stimulate thecake," wil l  it also teach them to avoid=you can't st imulate a cake'?When subtle semantic dist inct ionsare at issue, it is customary to remarkthat a satisfactory language understand-ing system will have to know a great dealmore that the l inguistic values of words.It wil l  have to know a great deal aboutthe world, and about things that peoplepresuppose without reflection.
Suchremarks are probably true, but they offerl itt le guidance in getting the job done.Since I have no better answer, Iwil l  simply agree that the lexical infor-mation available to any satisfactory lan-guage understanding system will have tobe closely coordinated with the system'sgeneral information about the world.
Topursue that idea would, of course, gobeyond the lexical l imits I have imposedhere, but it does suggest that we willhave to write our dict ionary not once,but many times -- until we get it right.So, while there is no principledobstacle to having large vocabularies inour natural language interfaces, thereare still many problems to be solved.There is work here for everyone -- lin-guists, philosophers, and psychologists,as well as computer scientists -- and itis not abstract or impractical work.
Theanswers we provide will shape importantaspects of the information systems of thefuture.ReferencesAmsler, R. A.
(1984) Machine-readabledictionaries.
Annual  Review QfInformation Science and TeGhnolouv,19, 161-209.Becket, C. A.
(1980) Semantic contexteffects in visual word recognition: Ananalysis of semantic strategies.Memory  & Cooni~ion, 8, 493-512.Bol t ,  R.A. (1984) The Human Interface:Where People and Computers meet.Belmont, Ca\]if.
: Lifetime Learning.Cullingford, R. E. (1985) Natural Lan-guage Processing: A Knowledge Engine-ering Approach.
(Manuscript).Deese, J.meaning.641-651.
(1967) Meaning and change ofAmerican Psvcholooist, 22,313Meyer, D. E., & Schvaneveldt, R. W.(1971) Faciliation in recognizingpairs of words: Evidence of a depen-dence between retrieval operations.Journal ofLExDerimental_Psvcholoav,90, 227-234.Miller, G. A.
(1977)ADDrentices?
Children and Lanauaue.New York: Seabury Press.Miller, G. A.
(1978) Semantic relationsamong words.
In M. Halle, J. Bresnan,& G. A. Miller (eds.
), L i ~Theor~ and Psvcholoaical RealitY?C~mhridge, Mass.
: MIT Press.Miller, G. A .
,  & G i ldea ,  P. M. (1985)How to misread a dictionary.
AILABulletin (in press).Miller, G. A., & Johnson-Laird, P. N.(1976) Lanuuaue and Perception.Cambridge, Mass.
: Harvard UniversityPress.Procter, P.
(ed.)
(1978) Z d ~tionarv of Contemporary Enulish.Harlow, Essex: Longman.chank, R. C. (1975)marion Processing.North-Holland.Conceotual Infor-Amsterdam:Simpson, G. B.
(1984) Lexical ambiguityand its role in models of word recog-nition?
Psvcholoaical Bulletin, 96,316-340.Sleeman, D.,  & Brown,  J.  S. (eds .
)(1982) Intelliaent Tutorina Systems.New York: Academic Press.Taylor, S. E., Frackenpohl, H., & White,C.
E. (1979) A revised core vocab-ulary.
In EDL Core Vocabularies in~Eadinu.
Mathematics.
Science.
and? "
.
New York:McGraw-Hill.Templin, M. C. (1957) Certain LanuuaaeSkills in Children= Their DeveloomenEand Interrelationships.
Minneapolis:University of Minnesota Press.Walker, D. E., & Amsler, R. A.
(1984)The use of machine ~eadable diction-aries in subianguage analysis.
In R.I. Kittredge (ed.
), Workshop on Sub~lanuuage Analv~iSo (Available fromthe authors at Bell Communications Re-search, 435 South Street,Mocristown, NJ 07960.
)Wilks, Y.
A.
(1978) Making preferencesmore active.
Artif icial Intslliaence,11, 197-223.314
