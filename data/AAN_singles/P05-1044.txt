Proceedings of the 43rd Annual Meeting of the ACL, pages 354?362,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsContrastive Estimation: Training Log-Linear Models on Unlabeled Data?Noah A. Smith and Jason EisnerDepartment of Computer Science / Center for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21218 USA{nasmith,jason}@cs.jhu.eduAbstractConditional random fields (Lafferty et al, 2001) arequite effective at sequence labeling tasks like shal-low parsing (Sha and Pereira, 2003) and named-entity extraction (McCallum and Li, 2003).
CRFsare log-linear, allowing the incorporation of arbi-trary features into the model.
To train on unlabeleddata, we require unsupervised estimation methodsfor log-linear models; few exist.
We describe a novelapproach, contrastive estimation.
We show that thenew technique can be intuitively understood as ex-ploiting implicit negative evidence and is computa-tionally efficient.
Applied to a sequence labelingproblem?POS tagging given a tagging dictionaryand unlabeled text?contrastive estimation outper-forms EM (with the same feature set), is more robustto degradations of the dictionary, and can largely re-cover by modeling additional features.1 IntroductionFinding linguistic structure in raw text is not easy.The classical forward-backward and inside-outsidealgorithms try to guide probabilistic models to dis-cover structure in text, but they tend to get stuck inlocal maxima (Charniak, 1993).
Even when theyavoid local maxima (e.g., through clever initializa-tion) they typically deviate from human ideas ofwhat the ?right?
structure is (Merialdo, 1994).One strategy is to incorporate domain knowledgeinto the model?s structure.
Instead of blind HMMsor PCFGs, one could use models whose features?This work was supported by a Fannie and John HertzFoundation fellowship to the first author and NSF ITR grant IIS-0313193 to the second author.
The views expressed are not nec-essarily endorsed by the sponsors.
The authors also thank threeanonymous ACL reviewers for helpful comments, colleaguesat JHU CLSP (especially David Smith and Roy Tromble) andMiles Osborne for insightful feedback, and Eric Goldlust andMarkus Dreyer for Dyna language support.are crafted to pay attention to a range of domain-specific linguistic cues.
Log-linear models can be socrafted and have already achieved excellent perfor-mance when trained on annotated data, where theyare known as ?maximum entropy?
models (Ratna-parkhi et al, 1994; Rosenfeld, 1994).Our goal is to learn log-linear models fromunannotated data.
Since the forward-backwardand inside-outside algorithms are instances ofExpectation-Maximization (EM) (Dempster et al,1977), a natural approach is to construct EM algo-rithms that handle log-linear models.
Riezler (1999)did so, then resorted to an approximation becausethe true objective function was hard to normalize.Stepping back from EM, we may generally en-vision parameter estimation for probabilistic mod-eling as pushing probability mass toward the train-ing examples.
We must consider not only wherethe learner pushes the mass, but also from where themass is taken.
In this paper, we describe an alterna-tive to EM: contrastive estimation (CE), which (un-like EM) explicitly states the source of the probabil-ity mass that is to be given to an example.1One reason is to make normalization efficient.
In-deed, CE generalizes EM and other practical tech-niques used to train log-linear models, includingconditional estimation (for the supervised case) andRiezler?s approximation (for the unsupervised case).The other reason to use CE is to improve accu-racy.
CE offers an additional way to inject domainknowledge into unsupervised learning (Smith andEisner, 2005).
CE hypothesizes that each positiveexample in training implies a domain-specific setof examples which are (for the most part) degraded(?2).
This class of implicit negative evidence pro-vides the source of probability mass for the observedexample.
We discuss the application of CE to log-linear models in ?3.1Not to be confused with contrastive divergence minimiza-tion (Hinton, 2003), a technique for training products of experts.354We are particularly interested in log-linear modelsover sequences, like the conditional random fields(CRFs) of Lafferty et al (2001) and weighted CFGs(Miyao and Tsujii, 2002).
For a given sequence, im-plicit negative evidence can be represented as a lat-tice derived by finite-state operations (?4).
Effec-tiveness of the approach on POS tagging using un-labeled data is demonstrated (?5).
We discuss futurework (?6) and conclude (?7).2 Implicit Negative EvidenceNatural language is a delicate thing.
For any plausi-ble sentence, there are many slight perturbations ofit that will make it implausible.
Consider, for ex-ample, the first sentence of this section.
Supposewe choose one of its six words at random and re-move it; on this example, odds are two to one thatthe resulting sentence will be ungrammatical.
Or,we could randomly choose two adjacent words andtranspose them; none of the results are valid conver-sational English.
The learner we describe here takesinto account not only the observed positive exam-ple, but also a set of similar but deprecated negativeexamples.2.1 Learning settingLet ~x = ?x1, x2, ...?, be our observed example sen-tences, where each xi ?
X, and let y?i ?
Y be theunobserved correct hidden structure for xi (e.g., aPOS sequence).
We seek a model, parameterized by~?, such that the (unknown) correct analysis y?i is thebest analysis for xi (under the model).
If y?i were ob-served, a variety of training criteria would be avail-able (see Tab.
1), but y?i is unknown, so none apply.Typically one turns to the EM algorithm (Dempsteret al, 1977), which locally maximizes?ip(X = xi | ~?
)=?i?y?Yp(X = xi, Y = y | ~?
)(1)where X is a random variable over sentences andY a random variable over analyses (notation is of-ten abbreviated, eliminating the random variables).An often-used alternative to EM is a class of so-called Viterbi approximations, which iteratively findthe probabilistically-best y?
and then, on each itera-tion, solve a supervised problem (see Tab.
1).joint likelihood (JL) ?ip(xi, y?i | ~?
)conditionallikelihood (CL)?ip(y?i | xi, ~?
)classificationaccuracy (Juangand Katagiri, 1992)?i?
(y?i , y?
(xi))expectedclassificationaccuracy (Klein andManning, 2002)?ip(y?i | xi, ~?
)negated boostingloss (Collins, 2000) ?
?ip(y?i | xi, ~?
)?1margin (Crammerand Singer, 2001) ?
s.t.
?~??
?
1;?i,?y 6= y?i ,~?
?
(~f(xi, y?i ) ?
~f(xi, y)) ?
?expected localaccuracy (Altun etal., 2003)?i?jp(`j(Y ) = `j(y?i ) | xi, ~?
)Table 1: Various supervised training criteria.
All functions arewritten so as to be maximized.
None of these criteria are avail-able for unsupervised estimation because they all depend on thecorrect label, y?.2.2 A new approach: contrastive estimationOur approach instead maximizes?ip(Xi = xi | Xi ?
N(xi), ~?
)(2)where the ?neighborhood?
N(xi) ?
X is a set ofimplicit negative examples plus the example xi it-self.
As in EM, p(xi | ..., ~?)
is found by marginal-izing over hidden variables (Eq.
1).
Note that thex?
?
N(xi) are not treated as hard negative exam-ples; we merely seek to move probability mass fromthem to the observed x.The neighborhood of x, N(x), contains examplesthat are perturbations of x.
We refer to the mappingN : X ?
2X as the neighborhood function, and theoptimization of Eq.
2 as contrastive estimation (CE).CE seeks to move probability mass from theneighborhood of an observed xi to xi itself.
Thelearner hypothesizes that good models are thosewhich discriminate an observed example from itsneighborhood.
Put another way, the learner assumesnot only that xi is good, but that xi is locally op-timal in example space (X), and that alternative,similar examples (from the neighborhood) are infe-rior.
Rather than explain all of the data, the modelmust only explain (using hidden variables) why the355observed sentence is better than its neighbors.
Ofcourse, the validity of this hypothesis will dependon the form of the neighborhood function.Consider, as a concrete example, learning nat-ural language syntax.
In Smith and Eisner (2005),we define a sentence?s neighborhood to be a set ofslightly-altered sentences that use the same lexemes,as suggested at the start of this section.
While theirsyntax is degraded, the inferred meaning of any ofthese altered sentences is typically close to the in-tended meaning, yet the speaker chose x and notone of the other x?
?
N(x).
Why?
Deletionsare likely to violate subcategorization requirements,and transpositions are likely to violate word orderrequirements?both of which have something to dowith syntax.
x was the most grammatical option thatconveyed the speaker?s meaning, hence (we hope)roughly the most grammatical option in the neigh-borhood N(x), and the syntactic model should makeit so.3 Log-Linear ModelsWe have not yet specified the form of our probabilis-tic model, only that it is parameterized by ~?
?
Rn.Log-linear models, which we will show are a naturalfit for CE, assign probability to an (example, label)pair (x, y) according top(x, y | ~?)def=1Z(~?
)u(x, y | ~?
)(3)where the ?unnormalized score?
u(x, y | ~?)
isu(x, y | ~?
)def= exp(~?
?
~f(x, y))(4)The notation above is defined as follows.
~f : X ?Y ?
Rn?0 is a nonnegative vector feature function,and ~?
?
Rn are the corresponding feature weights(the model?s parameters).
Because the features cantake any form and need not be orthogonal, log-linearmodels can capture arbitrary dependencies in thedata and cleanly incorporate them into a model.Z(~?)
(the partition function) is chosen so that?
(x,y) p(x, y |~?)
= 1; i.e., Z(~?)
=?
(x,y) u(x, y |~?).
u is typically easy to compute for a given (x, y),but Z may be much harder to compute.
All the ob-jective functions in this paper take the form?i?
(x,y)?Aip(x, y | ~?)?
(x,y)?Bip(x, y | ~?)
(5)likelihood criterion Ai Bijoint {(xi, y?i )} X?
Yconditional {(xi, y?i )} {xi} ?
Ymarginal (a la` EM) {xi} ?
Y X?
Ycontrastive {xi} ?
Y N(xi) ?
YTable 2: Supervised (upper box) and unsupervised (lower box)estimation with log-linear models in terms of Eq.
5.where Ai ?
Bi (for each i).
For log-linear modelsthis is simply?i?
(x,y)?Aiu(x, y | ~?)?
(x,y)?Biu(x, y | ~?)
(6)So there is no need to compute Z(~?
), but we do needto compute sums over A and B. Tab.
2 summarizessome concrete examples; see also ?3.1?3.2.We would prefer to choose an objective functionsuch that these sums are easy.
CE focuses on choos-ing appropriate small contrast sets Bi, both for effi-ciency and to guide the learner.
The natural choicefor Ai (which is usually easier to sum over) is the setof (x, y) that are consistent with what was observed(partially or completely) about the ith training ex-ample, i.e., the numerator?
(x,y)?Ai p(x, y |~?)
isdesigned to find p(observation i | ~?).
The idea is tofocus the probability mass within Bi on the subsetAi where the i the training example is known to be.It is possible to build log-linear models whereeach xi is a sequence.2 In this paper, each modelis a weighted finite-state automaton (WFSA) wherestates correspond to POS tags.
The parameter vector~?
?
Rn specifies a weight for each of the n transi-tions in the automaton.
y is a hidden path throughthe automaton (determining a POS sequence), and xis the string it emits.
u(x, y | ~?)
is defined by ap-plying exp to the total weight of all transitions in y.This is an example of Eqs.
4 and 6 where fj(x, y) isthe number of times the path y takes the jth transi-tion.The partition function Z(~?)
of the WFSA is foundby adding up the u-scores of all paths through theWFSA.
For a k-state WFSA, this equates to solvinga linear system of k equations in k variables (Tarjan,1981).
But if the WFSA contains cycles this infi-nite sum may diverge.
Alternatives to exact com-2These are exemplified by CRFs (Lafferty et al, 2001),which can be viewed alternately as undirected dynamic graph-ical models with a chain topology, as log-linear models overentire sequences with local features, or as WFSAs.
Because?CRF?
implies CL estimation, we use the term ?WFSA.
?356putation, like random sampling (see, e.g., Abney,1997), will not help to avoid this difficulty; in addi-tion, convergence rates are in general unknown andbounds difficult to prove.
We would prefer to sumover finitely many paths in Bi.3.1 Parameter estimation (supervised)For log-linear models, both CL and JL estimation(Tab.
1) are available.
In terms of Eq.
5, bothset Ai = {(xi, y?i )}.
The difference is in B: forJL, Bi = X ?
Y, so summing over Bi is equiva-lent to computing the partition function Z(~?).
Be-cause that sum is typically difficult, CL is preferred;Bi = {xi} ?
Y for xi, which is often tractable.For sequence models like WFSAs it is computed us-ing a dynamic programming algorithm (the forwardalgorithm for WFSAs).
Klein and Manning (2002)argue for CL on grounds of accuracy, but see alsoJohnson (2001).
See Tab.
2; other contrast sets Biare also possible.When Bi contains only xi paired with the currentbest competitor (y?)
to y?i , we have a technique thatresembles maximum margin training (Crammer andSinger, 2001).
Note that y?
will then change acrosstraining iterations, making Bi dynamic.3.2 Parameter estimation (unsupervised)The difference between supervised and unsuper-vised learning is that in the latter case, Ai is forcedto sum over label sequences y because they weren?tobserved.
In the unsupervised case, CE maximizesLN(~?
)= log?i?y?Yu(xi, y | ~?)?
(x,y)?N(xi)?Yu(x, y | ~?)
(7)In terms of Eq.
5, A = {xi}?Y and B = N(xi)?Y.EM?s objective function (Eq.
1) is a special casewhere N(xi) = X, for all i, and the denomina-tor becomes Z(~?).
An alternative is to restrict theneighborhood to the set of observed training exam-ples rather than all possible examples (Riezler, 1999;Johnson et al, 1999; Riezler et al, 2000):?i[u(xi | ~?
)/?ju(xj | ~?
)](8)Viewed as a CE method, this approach (though ef-fective when there are few hypotheses) seems mis-guided; the objective says to move mass to each ex-ample at the expense of all other training examples.Another variant is conditional EM.
Let xi be apair (xi,1, xi,2) and define the neighborhood to beN(xi) = {x?
= (x?1, xi,2)}.
This approach hasbeen applied to conditional densities (Jebara andPentland, 1998) and conditional training of acousticmodels with hidden variables (Valtchev et al, 1997).Generally speaking, CE is equivalent to somekind of EM when N(?)
is an equivalence relationon examples, so that the neighborhoods partition X.Then if q is any fixed (untrained) distribution overneighborhoods, CE equates to running EM on themodel defined byp?
(x, y | ~?
)def= q (N(x)) ?
p(x, y | N(x), ~?
)(9)CE may also be viewed as an importance sam-pling approximation to EM, where the sample spaceX is replaced by N(xi).
We will demonstrate ex-perimentally that CE is not just an approximation toEM; it makes sense from a modeling perspective.In ?4, we will describe neighborhoods of se-quences that can be represented as acyclic latticesbuilt directly from an observed sequence.
The sumover Bi is then the total u-score in our model of allpaths in the neighborhood lattice.
To compute this,intersect the WFSA and the lattice, obtaining a newacyclic WFSA, and sum the u-scores of all its paths(Eisner, 2002) using a simple dynamic programmingalgorithm akin to the forward algorithm.
The sumover Ai may be computed similarly.CE with lattice neighborhoods is not confined tothe WFSAs of this paper; when estimating weightedCFGs, the key algorithm is the inside algorithm forlattice parsing (Smith and Eisner, 2005).3.3 Numerical optimizationTo maximize the neighborhood likelihood (Eq.
7),we apply a standard numerical optimization method(L-BFGS) that iteratively climbs the function usingknowledge of its value and gradient (Liu and No-cedal, 1989).
The partial derivative of LN with re-spect to the jth feature weight ?j is?LN??j=?iE~?
[fj | xi] ?E~?
[fj | N(xi)] (10)This looks similar to the gradient of log-linear like-lihood functions on complete data, though the ex-pectation on the left is in those cases replaced by anobserved feature value fj(xi, y?i ).
In this paper, the357natural language is a delicate thinga.
DEL1WORD:natural language is a delicate thinglanguage is a delicate thingis adelicatething ?:??
?b.
TRANS1:natural language a delicate thingisdelicateisisanaturalais a delicate thinglanguagelanguagedelicatething: xx2 1x2x1 ::x x2 3 :x x3 2:x xm m?1 xm?1:xm?
?...
(Each bigram xi+1i in the sentence has anarc pair (xi : xi+1, xi+1 : xi).)c.
DEL1SUBSEQ:natural language is a delicate thinglanguageisisaaa delicate thing?:??:??:??????
?Figure 1: A sentence and three lattices representing some of its neighborhoods.
The transducer used to generate each neighborhoodlattice (via composition with the sentence, followed by determinization and minimization) is shown to its right.expectations in Eq.
10 are computed by the forward-backward algorithm generalized to lattices.We emphasize that the function LN is not glob-ally concave; our search will lead only to a local op-timum.3 Therefore, as with all unsupervised statisti-cal learning, the bias in the initialization of ~?
will af-fect the quality of the estimate and the performanceof the method.
In future we might wish to applytechniques for avoiding local optima, such as deter-ministic annealing (Smith and Eisner, 2004).4 Lattice NeighborhoodsWe next consider some non-classical neighborhoodfunctions for sequences.
When X = ?+ for somesymbol alphabet ?, certain kinds of neighborhoodshave natural, compact representations.
Given an in-put string x = ?x1, x2, ..., xm?, we write xji forthe substring ?xi, xi+1, ..., xj?
and xm1 for the wholestring.
Consider first the neighborhood consisting ofall sequences generated by deleting a single symbolfrom the m-length sequence xm1 :DEL1WORD(xm1 ) ={x`?11 xm`+1 | 1 ?
` ?
m}?
{xm1 }This set consists of m + 1 strings and can be com-pactly represented as a lattice (see Fig.
1a).
Another3Without any hidden variables, LN is globally concave.neighborhood involves transposing any pair of adja-cent words:TRANS1(xm1 ) ={x`?11 x`+1x`xm`+2 | 1 ?
` < m}?
{xm1 }This set can also be compactly represented as a lat-tice (Fig.
1b).
We can combine DEL1WORD andTRANS1 by taking their union; this gives a largerneighborhood, DELORTRANS1.4The DEL1SUBSEQ neighborhood allows the dele-tion of any contiguous subsequence of words that isstrictly smaller than the whole sequence.
This latticeis similar to that of DEL1WORD, but adds some arcs(Fig.
1c); the size of this neighborhood is O(m2).A final neighborhood we will consider isLENGTH, which consists of ?m.
CE with theLENGTH neighborhood is very similar to EM; it isequivalent to using EM to estimate the parametersof a model defined by Eq.
9 where q is any fixed(untrained) distribution over lengths.When the vocabulary ?
is the set of words in anatural language, it is never fully known; approx-imations for defining LENGTH = ?m include us-ing observed ?
from the training set (as we do) oradding a special OOV symbol.4In general, the lattices are obtained by composing the ob-served sequence with a small FST and determinizing and mini-mizing the result; the relevant transducers are shown in Fig.
1.358304050607080901000.1  1  10% correct tagssmoothing parameter0 812K 24K 48K 96Ksel.
oracle sel.
oracle sel.
oracle sel.
oracleCRF (supervised) 100.0 99.8 99.8 99.5HMM (supervised) 99.3 98.5 97.9 97.2LENGTH 74.9 77.4 78.7 81.5 78.3 81.3 78.9 79.3DELORTR1 70.8 70.8 78.6 78.6 78.3 79.1 75.2 78.8TRANS1 72.7 72.7 77.2 77.2 78.1 79.4 74.7 79.0EM 49.5 52.9 55.5 58.0 59.4 60.9 60.9 62.1DEL1WORD 55.4 55.6 58.6 60.3 59.9 60.2 59.9 60.4DEL1SSQ 53.0 53.3 55.0 56.7 55.3 55.4 57.3 58.7random expected 35.2 35.1 35.1 35.1ambiguous words 6,244 12,923 25,879 51,521Figure 2: Percent ambiguous words tagged correctly in the 96K dataset, as the smoothing parameter (?
in the case of EM, ?2 in theCE cases) varies.
The model selected from each criterion using unlabeled development data is circled in the plot.
Dataset size isvaried in the table at right, which shows models selected using unlabeled development data (?sel.?)
and using an oracle (?oracle,?the highest point on a curve).
Across conditions, some neighborhood roughly splits the difference between supervised models andEM.5 ExperimentsWe compare CE (using neighborhoods from ?4)with EM on POS tagging using unlabeled data.5.1 Comparison with EMOur experiments are inspired by those inMerialdo (1994); we train a trigram tagger usingonly unlabeled data, assuming complete knowledgeof the tagging dictionary.5 In our experiments,we varied the amount of data available (12K?96Kwords of WSJ), the heaviness of smoothing, and theestimation criterion.
In all cases, training stoppedwhen the relative change in the criterion fell below10?4 between steps (typically ?
100 steps).
For thiscorpus and tag set, on average, a tagger must decidebetween 2.3 tags for a given token.The generative model trained by EM was identicalto Merialdo?s: a second-order HMM.
We smoothedusing a flat Dirichlet prior with single parameter ?for all distributions (?-values from 0 to 10 weretested).6 The model was initialized uniformly.The log-linear models trained by CE used thesame feature set, though the feature weights are nolonger log-probabilities and there are no sum-to-oneconstraints.
In addition to an unsmoothed trial, wetried diagonal Gaussian priors (quadratic penalty)with ?2 ranging from 0.1 to 10.
The models wereinitialized with all ?j = 0.Unsupervised model selection.
For each (crite-5Without a tagging dictionary, tag names are interchange-able and cannot be evaluated on gold-standard accuracy.
Weaddress the tagging dictionary assumption in ?5.2.6This is equivalent to add-?
smoothing within every M step.rion, dataset) pair, we selected the smoothing trialthat gave the highest estimation criterion score on a5K-word development set (also unlabeled).Results.
The plot in Fig.
2 shows the Viterbi ac-curacy of each criterion trained on the 96K-worddataset as smoothing was varied; the table shows,for each (criterion, dataset) pair the performance ofthe selected ?
or ?2 and the one chosen by an oracle.LENGTH, TRANS1, and DELORTRANS1 are con-sistently the best, far out-stripping EM.
These gainsdwarf the performance of EM on over 1.1M words(66.6% as reported by Smith and Eisner (2004)),even when the latter uses improved search (70.0%).DEL1WORD and DEL1SUBSEQ, on the other hand,are poor, even worse than EM on larger datasets.An important result is that neighborhoods do notsucceed by virtue of approximating log-linear EM;if that were so, we would expect larger neighbor-hoods (like DEL1SUBSEQ) to out-perform smallerones (like TRANS1)?this is not so.
DEL1SUBSEQand DEL1WORD are poor because they do not givehelpful classes of negative evidence: deleting a wordor a short subsequence often does very little dam-age.
Put another way, models that do a good job ofexplaining why no word or subsequence should bedeleted do not do so using the familiar POS cate-gories.The LENGTH neighborhood is as close to log-linear EM as it is practical to get.
The inconsis-tencies in the LENGTH curve (Fig.
2) are notableand also appeared at the other training set sizes.Believing this might be indicative of brittleness inViterbi label selection, we computed the expected359DELORTRANS1 TRANS1 LENGTH EMwords in trigram trigram+ spelling trigramtrigram+ spelling trigramtrigram+ spelling trigramtagging dict.
sel.
oracle sel.
oracle sel.
oracle sel.
oracle sel.
oracle sel.
oracle sel.
oracle randomexpectedambiguous wordsave.tags/tokenall train & dev.
78.3 90.1 80.9 91.1 90.4 90.4 88.7 90.9 87.8 90.4 87.1 91.9 78.0 84.4 69.5 13,150 2.31st 500 sents.
72.3 84.8 80.2 90.8 80.8 82.9 88.1 90.1 68.1 78.3 76.9 83.2 77.2 80.5 60.5 13,841 3.7count ?
2 69.5 81.3 79.5 90.3 77.0 78.6 78.7 90.1 65.3 75.2 73.3 73.8 70.1 70.9 56.6 14,780 4.4count ?
3 65.0 77.2 78.3 89.8 71.7 73.4 78.4 89.5 62.8 72.3 73.2 73.6 66.5 66.5 51.0 15,996 5.5Table 3: Percent of all words correctly tagged in the 24K dataset, as the tagging dictionary is diluted.
Unsupervised model selection(?sel.?)
and oracle model selection (?oracle?)
across smoothing parameters are shown.
Note that we evaluated on all words (unlikeFig.
3) and used 17 coarse tags, giving higher scores than in Fig.
2.accuracy of the LENGTH models; the same ?dips?were present.
This could indicate that the learnerwas trapped in a local maximum, suggesting that,since other criteria did not exhibit this behavior,LENGTH might be a bumpier objective surface.
Itwould be interesting to measure the bumpiness (sen-sitivity to initial conditions) of different contrastiveobjectives.75.2 Removing knowledge, adding featuresThe assumption that the tagging dictionary is com-pletely known is difficult to justify.
While a POSlexicon might be available for a new language, cer-tainly it will not give exhaustive information aboutall word types in a corpus.
We experimented withremoving knowledge from the tagging dictionary,thereby increasing the difficulty of the task, to seehow well various objective functions could recover.One means to recovery is the addition of features tothe model?this is easy with log-linear models butnot with classical generative models.We compared the performance of the bestneighborhoods (LENGTH, DELORTRANS1, andTRANS1) from the first experiment, plus EM, us-ing three diluted dictionaries and the original one,on the 24K dataset.
A diluted dictionary adds (tag,word) entries so that rare words are allowed withany tag, simulating zero prior knowledge about theword.
?Rare?
might be defined in different ways;we used three definitions: words unseen in the first500 sentences (about half of the 24K training cor-pus); singletons (words with count ?
1); and wordswith count ?
2.
To allow more trials, we projectedthe original 45 tags onto a coarser set of 17 (e.g.,7A reviewer suggested including a table comparing differentcriterion values for each learned model (i.e., each neighborhoodevaluated on each other neighborhood).
This table contained nobig surprises; we note only that most models were the best ontheir own criterion, and among unsupervised models, LENGTHperformed best on the CL criterion.RB?
?ADV).To take better advantage of the power of log-linear models?specifically, their ability to incorpo-rate novel features?we also ran trials augmentingthe model with spelling features, allowing exploita-tion of correlations between parts of the word and apossible tag.
Our spelling features included all ob-served 1-, 2-, and 3-character suffixes, initial capital-ization, containing a hyphen, and containing a digit.Results.
Fig.
3 plots tagging accuracy (on am-biguous words) for each dictionary on the 24Kdataset.
The x-axis is the smoothing parameter (?for EM, ?2 for CE).
Note that the different plots arenot comparable, because their y-axes are based ondifferent sets of ambiguous words.So that models under different dilution conditionscould be compared, we computed accuracy on allwords; these are shown in Tab.
3.
The reader willnotice that there is often a large gap between unsu-pervised and oracle model selection; this draws at-tention to a need for better unsupervised regulariza-tion and model selection techniques.Without spelling features, all models performworse as knowledge is removed.
But LENGTH suf-fers most substantially, relative to its initial perfor-mance.
Why is this?
LENGTH (like EM) requiresthe model to explain why a given sentence was seeninstead of some other sentence of the same length.One way to make this explanation is to manipulateemission weights (i.e., for (tag, word) features): thelearner can construct a good class-based unigrammodel of the text (where classes are tags).
This isgood for the LENGTH objective, but not for learninggood POS tag sequences.In contrast, DELORTRANS1 and TRANS1 do notallow the learner to manipulate emission weights forwords not in the sentence.
The sentence?s good-ness must be explained in a way other than by thewords it contains: namely through the POS tags.
To360check this intuition, we built local normalized mod-els p(word | tag) from the parameters learned byTRANS1 and LENGTH.
For each tag, these werecompared by KL divergence to the empirical lexicaldistributions (from labeled data).
For the ten tagsaccounting for 95.6% of the data, LENGTH moreclosely matched the empirical lexical distributions.LENGTH is learning a correct distribution, but thatdistribution is not helpful for the task.The improvement from adding spelling featuresis striking: DELORTRANS1 and TRANS1 recovernearly completely (modulo the model selectionproblem) from the diluted dictionaries.
LENGTHsees far less recovery.
Hence even our improved fea-ture sets cannot compensate for the choice of neigh-borhood.
This highlights our argument that a neigh-borhood is not an approximation to log-linear EM;LENGTH tries very hard to approximate log-linearEM but requires a good dictionary to be on par withthe other criteria.
Good neighborhoods, rather, per-form well in their own right.6 Future WorkForemost for future work is the ?minimally super-vised?
paradigm in which a small amount of la-beled data is available (see, e.g., Clark et al (2003)).Unlike well-known ?bootstrapping?
approaches(Yarowsky, 1995), EM and CE have the possible ad-vantage of maintaining posteriors over hidden labels(or structure) throughout learning; bootstrapping ei-ther chooses, for each example, a single label, orremains completely agnostic.
One can envision amixed objective function that tries to fit the labeledexamples while discriminating unlabeled examplesfrom their neighborhoods.8Regardless of how much (if any) data are labeled,the question of good smoothing techniques requiresmore attention.
Here we used a single zero-mean,constant-variance Gaussian prior for all parameters.Better performance might be achieved by allowingdifferent variances for different feature types.
This8Zhu and Ghahramani (2002) explored the semi-supervisedclassification problem for spatially-distributed data, wheresome data are labeled, using a Boltzmann machine to modelthe dataset.
For them, the Markov random field is over label-ing configurations for all examples, not, as in our case, com-plex structured labels for a particular example.
Hence their B(Eq.
5), though very large, was finite and could be sampled.All train & development words are in the tagging dictionary:404550 55606570758085Tagging dictionary taken from the first 500 sentences:404550 55606570758085Tagging dictionary contains words with count ?
2:404550 55606570758085Tagging dictionary contains words with count ?
3:404550 5560657075808540 4550 5560 6570 580 850.1  1  10smoothing parameter0 850DELORTRANS1  TRANS1  LENGTH 4 5EM trigrammodel?
trigram+spellingFigure 3: Percent ambiguous words tagged correctly (withcoarse tags) on the 24K dataset, as the dictionary is diluted andwith spelling features.
Each graph corresponds to a differentlevel of dilution.
Models selected using unlabeled developmentdata are circled.
These plots (unlike Tab.
3) are not compara-ble to each other because each is measured on a different set ofambiguous words.361leads to a need for more efficient tuning of the priorparameters on development data.The effectiveness of CE (and different neighbor-hoods) for dependency grammar induction is ex-plored in Smith and Eisner (2005) with considerablesuccess.
We introduce there the notion of design-ing neighborhoods to guide learning for particulartasks.
Instead of guiding an unsupervised learner tomatch linguists?
annotations, the choice of neighbor-hood might be made to direct the learner toward hid-den structure that is helpful for error-correction taskslike spelling correction and punctuation restorationthat may benefit from a grammatical model.Wang et al (2002) discuss the latent maximumentropy principle.
They advocate running EM manytimes and selecting the local maximum that maxi-mizes entropy.
One might do the same for the localmaxima of any CE objective, though theoretical andexperimental support for this idea remain for futurework.7 ConclusionWe have presented contrastive estimation, a newprobabilistic estimation criterion that forces a modelto explain why the given training data were betterthan bad data implied by the positive examples.
Wehave shown that for unsupervised sequence model-ing, this technique is efficient and drastically out-performs EM; for POS tagging, the gain in accu-racy over EM is twice what we would get from tentimes as much data and improved search, stickingwith EM?s criterion (Smith and Eisner, 2004).
Onthis task, with certain neighborhoods, contrastiveestimation suffers less than EM does from dimin-ished prior knowledge and is able to exploit newfeatures?that EM can?t?to largely recover fromthe loss of knowledge.ReferencesS.
P. Abney.
1997.
Stochastic attribute-value grammars.
Com-putational Linguistics, 23(4):597?617.Y.
Altun, M. Johnson, and T. Hofmann.
2003.
Investigatingloss functions and optimization methods for discriminativelearning of label sequences.
In Proc.
of EMNLP.E.
Charniak.
1993.
Statistical Language Learning.
MIT Press.S.
Clark, J. R. Curran, and M. Osborne.
2003.
BootstrappingPOS taggers using unlabelled data.
In Proc.
of CoNLL.M.
Collins.
2000.
Discriminative reranking for natural lan-guage parsing.
In Proc.
of ICML.K.
Crammer and Y.
Singer.
2001.
On the algorithmic imple-mentation of multiclass kernel-based vector machines.
Jour-nal of Machine Learning Research, 2(5):265?92.A.
Dempster, N. Laird, and D. Rubin.
1977.
Maximum likeli-hood estimation from incomplete data via the EM algorithm.Journal of the Royal Statistical Society B, 39:1?38.J.
Eisner.
2002.
Parameter estimation for probabilistic finite-state transducers.
In Proc.
of ACL.G.
E. Hinton.
2003.
Training products of experts by mini-mizing contrastive divergence.
Technical Report GCNU TR2000-004, University College London.T.
Jebara and A. Pentland.
1998.
Maximum conditional like-lihood via bound maximization and the CEM algorithm.
InProc.
of NIPS.M.
Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999.Estimators for stochastic ?unification-based?
grammars.
InProc.
of ACL.M.
Johnson.
2001.
Joint and conditional estimation of taggingand parsing models.
In Proc.
of ACL.B.-H. Juang and S. Katagiri.
1992.
Discriminative learning forminimum error classification.
IEEE Trans.
Signal Process-ing, 40:3043?54.D.
Klein and C. D. Manning.
2002.
Conditional structure vs.conditional estimation in NLP models.
In Proc.
of EMNLP.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In Proc.
of ICML.D.
C. Liu and J. Nocedal.
1989.
On the limited memory methodfor large scale optimization.
Mathematical Programming B,45(3):503?28.A.
McCallum and W. Li.
2003.
Early results for named-entity extraction with conditional random fields.
In Proc.of CoNLL.B.
Merialdo.
1994.
Tagging English text with a probabilisticmodel.
Computational Linguistics, 20(2):155?72.Y.
Miyao and J. Tsujii.
2002.
Maximum entropy estimation forfeature forests.
In Proc.
of HLT.A.
Ratnaparkhi, S. Roukos, and R. T. Ward.
1994.
A maximumentropy model for parsing.
In Proc.
of ICSLP.S.
Riezler, D. Prescher, J. Kuhn, and M. Johnson.
2000.
Lex-icalized stochastic modeling of constraint-based grammarsusing log-linear measures and EM training.
In Proc.
of ACL.S.
Riezler.
1999.
Probabilistic Constraint Logic Programming.Ph.D.
thesis, Universita?t Tu?bingen.R.
Rosenfeld.
1994.
Adaptive Statistical Language Modeling:A Maximum Entropy Approach.
Ph.D. thesis, CMU.F.
Sha and F. Pereira.
2003.
Shallow parsing with conditionalrandom fields.
In Proc.
of HLT-NAACL.N.
A. Smith and J. Eisner.
2004.
Annealing techniques forunsupervised statistical language learning.
In Proc.
of ACL.N.
A. Smith and J. Eisner.
2005.
Guiding unsupervised gram-mar induction using contrastive estimation.
In Proc.
of IJ-CAI Workshop on Grammatical Inference Applications.R.
E. Tarjan.
1981.
A unified approach to path problems.
Jour-nal of the ACM, 28(3):577?93.V.
Valtchev, J. J. Odell, P. C. Woodland, and S. J.
Young.
1997.MMIE training of large vocabulary speech recognition sys-tems.
Speech Communication, 22(4):303?14.S.
Wang, R. Rosenfeld, Y. Zhao, and D. Schuurmans.
2002.The latent maximum entropy principle.
In Proc.
of ISIT.D.
Yarowsky.
1995.
Unsupervised word sense disambiguationrivaling supervised methods.
In Proc.
of ACL.X.
Zhu and Z. Ghahramani.
2002.
Towards semi-supervisedclassification with Markov random fields.
Technical ReportCMU-CALD-02-106, Carnegie Mellon University.362
