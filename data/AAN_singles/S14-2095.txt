Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 546?550,Dublin, Ireland, August 23-24, 2014.Senti.ue: Tweet Overall Sentiment ClassificationApproach for SemEval-2014 Task 9Jos?e SaiasDI - ECT - Universidade de?EvoraRua Rom?ao Ramalho, 597000-671?Evora, Portugaljsaias@uevora.ptAbstractThis document describes the senti.uesystem and how it was used for partici-pation in SemEval-2014 Task 9 challenge.Our system is an evolution of our priorwork, also used in last year?s edition ofSentiment Analysis in Twitter.
This sys-tem maintains a supervised machine learn-ing approach to classify the tweet overallsentiment, but with a change in the usedfeatures and the algorithm.
We use a re-stricted set of 47 features in subtask B and31 features in subtask A.In the constrained mode, and for the fivedata sources, senti.ue achieved a scorebetween 78,72 and 84,05 in subtask A, anda score between 55,31 and 71,39 in sub-task B.
For the unconstrained mode, ourscore was slightly below, except for onecase in subtask A.1 IntroductionThis paper describes the approach taken by ateam of Universidade de?Evora?s Computer Sci-ence Department in SemEval-2014 Task 9: Senti-ment Analysis in Twitter (Rosenthal et al., 2014).SemEval-2014 Task 9 has an expression-level(subtask A) and a message-level (subtask B) polar-ity classification challenges.
The first subtask aimsto determine whether a word (or phrase) is posi-tive, negative or neutral, within the textual contextin which it appears.
The second subtask concernsthe classification of the overall text polarity, whichcorresponds to automatically detecting the senti-ment expressed in a Twitter message.
In both sub-tasks, systems can operate in constrained or un-constrained mode.
Constrained means that learn-This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/ing is based only on provided training texts, withthe possible aid of static resources such as lexi-cons.
Extra tweets or additional annotated doc-uments for training are permitted only in uncon-strained mode.The system we used to respond to this challengeis called senti.ue, and follows on from ourprevious work on Natural Language Processing(NLP) and Sentiment Analysis (SA).
We devel-oped work in automatic reputation assessment, us-ing a Machine Learning (ML) based classifier forcomments with impact on a particular target entity(Saias, 2013).
We also participated in the previ-ous edition of SemEval SA task, where we haveimplemented the basis for the current system.
Inlast year?s solution (Saias and Fernandes, 2013),we treated both subtasks using the same method(except the training set).
We have updated themethod for subtask A, now considering also thetext around the area to classify, by dedicating newfeatures to those preceding and following tweetparts.
Text overall sentiment classification is thecore objective of our system, and is performed, asbefore, with a supervised machine learning tech-nique.
For subtask B, we fixed some implemen-tation issues in the previous version, and we wentfrom 22 to 53 features, explained in Section 3.2 Related WorkThe popularity of social networks and microblog-ging facilitated the sharing of opinions.
To knowwhether people are satisfied or not with a particu-lar brand or product is of great interest to market-ing companies.
Much work has appeared in SA,trying to capture valuable information in expres-sions of contentment or discontentment.Important international scientific events, NLP re-lated, include SA challenges and workshops.
Thiswas the case in SemEval-2013, whose task 2 (Wil-son et al., 2013) required sentiment analysis ofTwitter and SMS text messages.
Being the pre-546decessor task of the challenge for which this workwas developed, it is similar to this year?s Task 9.The participating systems achieved better resultsin contextual polarity subtask (A) than those ob-tained for the overall message polarity subtask (B).In that edition, the best results were obtained bysystems in constrained mode.
The most commonmethod was supervised ML with features that canbe related to text words, syntactic function, dis-course elements relation, internet slang and sym-bols, or clues from sentiment lexicons.
In thattask, the NRC-Canada system (Mohammad etal., 2013) obtained the best performance, achiev-ing an F1 of 88.9% in subtask A and 69% in sub-task B.
That system used one SVM classifier foreach subtask, together with text surface based fea-tures, features associated with manually createdand automatically generated sentiment lexicons,and n-gram features.
Other systems with good re-sults in that task were GU-MLT-LT (G?unther andFurrer, 2013) and AVAYA (Becker et al., 2013).The first was implemented in the Python lan-guage.
It includes features for: text tokens af-ter normalization, stems, word clusters, and twovalues for the accumulated positive and accumu-lated negative SentiWordNet (Baccianella et al.,2010) scores, considering negation.
Its machinelearning classifier is based on linear models withstochastic gradient descent.
The approach takenin the AVAYA system centers on training high-dimensional, linear classifiers with a combinationof lexical and syntactic features.
This system usesBag-of-Words features, with negation representedin word suffix, and including not only the rawword forms but also combinations with lemmasand PoS tags.
Then, word polarity features areadded, using the MPQA lexicon (Wiebe et al.,2005), as well as syntactic dependency and PoStag features.
Other features consider emoticons,capitalization, character repetition, and emphasischaracters, such as asterisks and dashes.
The re-sulting model was trained with the LIBLINEAR(Fan et al., 2008) classification library.Another NLP task very close to SA is polarityclassification on the reputation of an entity.
Here,instead the sentiment in the perspective of theopinion holder, the goal is to detect the impact ofthis particular opinion on some entity?s reputation.The diue system (Saias, 2013) uses a supervisedML approach for reputation polarity classification,including Bag-of-Words and a limited set of fea-tures based on sentiment lexicons and superficialtext analysis.3 MethodThis work follows on from our previous partici-pation in SemEval-2013 SA task, where we havedevoted greater effort to subtask B.
We start by ex-plaining our current approach for this subtask, andthen we describe how such classifier is also usedin subtask A.3.1 Message Polarity ClassificationThe senti.ue system maintains a supervisedmachine learning approach to perform the over-all sentiment classification.
As before, Python andthe Natural Language Toolkit (NLTK1) are usedfor text processing and ML feature extraction.The first step was to obtain the tweet content andforming the instances of the training set.
Dur-ing the download phase, several tweets were notfound.
In constrained mode, we got only 7352 in-stances available for training.Tweet preprocessing includes tokenization, whichis punctuation and white space based, negation de-tection, and lemmatization, through NLTK classWordNetLemmatizer.
After that, the systemruns the ML component.
Instead of the solu-tion we used in 2013, with two differently con-figured classifiers in a pipeline, we chose to usea single classifier, which this year is based onSciKit-Learn2, and to increase the numberof features that are extracted to represent eachinstance.
The classification algorithm was Sup-port Vector Machines (SVM), using SVC3class,with a linear kernel and 10?5tolerance for stop-ping criterion.
SVC class implementation is basedon libsvm (Chang and Lin, 2011), and usesone-against-one approach for multi-class classifi-cation.
From each instance, the system extracts the47 features in Figure 1.
The first two features rep-resent the index of the first polarized token.
Thefollowing represent the repeated occurrence of aquestion mark, and the existence of a token withnegation (not, never).
Then there are two fea-tures that indicate whether there is negation beforepositive or negative words.
The following 8 fea-1Python based platform with resources and programminglibraries suitable for linguistic processing (Bird, 2006).2Open source tool for Machine Learning in Python -http://scikit-learn.org/3http://scikit-learn.org/stable/modules/svm.html#svm-classification547tures indicate whether there are positive or nega-tive terms, just after, or near, a question mark oran exclamation mark.
We build a table with wordsor phrases marked as positive or negative in sub-task A data.
Using this resource, 4 features test thepresence and the count of word n-grams markedas positive or negative.
Then the TA.alike featuresrepresent the same, but after lemmatization andsynonym verification.
To find the synonyms ofa term, we used the WordNet (Princeton Univer-sity, 2010) resource.
The probability of each wordbelonging to a class was calculated.
There are 3features avgProbWordOn, one per class, that rep-resent the average of this probability for each in-stance words.
Next 3 features represent the same,but focusing only on the last 5 words of each text.Then we have 6 ProbLog2Prob features, repre-senting the average of P ?
log2(P ), for all words,or only the latest 5 words, for all classes.
P isthe probability of the word belonging to one class.One feature cumulates the token polarity values,according to SentiWordNet.
The final 12 featuresare based on sentiment lexicons: AFINN (Nielsen,2011), Bing Liu (Liu et al., 2005), MPQA, anda custom polarity table with some manually en-tered entries.
For each resource, we count the in-stance tokens with negative and positive polarity,and create a feature direction, having the value 1if countTokens.pos>countTokens.neg, -1 if count-Tokens.pos<countTokens.neg, or 0.For the unconstrained mode, the only differenceis the use of more instances for the training set,with 3296 short texts obtained from SemEval-2014 Task 4 data4, about laptops and restaurants.3.2 Contextual Polarity DisambiguationIn this subtask, the download phase fetched only6506 tweets.
These instances have boundariesmarking the substring to classify.
Our systemstarts by splitting the document into text segments:fullText, leftText, rightText, sentenceText, chosenText.
Thefirst corresponds to the entire tweet.
The follow-ing represent the text before and the text after thechosen text.
Then we have the sentence wherethe chosen text is, and finally the text segmentthat systems must classify.
The preprocessing de-scribed before is then applied to each of these textsegments.
For each instance, the system gener-ates the 31 features listed in Figure 2.
First 27features represent 9 values for each chosenText , sen-4http://alt.qcri.org/semeval2014/task4/firstIndexOf.
{pos,neg}, hasDoubleQuestionMark,hasNegation, hasNegationBefore.{pos,neg},{pos,neg}.{After,Near}.Exclamation,{pos,neg}.{After,Near}.Question,hasTA.
{pos,neg}.NGrams, countTA.{pos,neg}.NGrams,hasTA.alike.{pos,neg}.NGrams,countTA.alike.{pos,neg}.NGrams,avgProbWordOn.{pos,neg,neutral},last5AvgProbWordOn.{pos,neg,neutral},avgW.ProbLog2Prob.{pos,neg,neutral},last5AvgW.ProbLog2Prob.{pos,neg,neutral},SentiWordNetAccumulatedValue,{AFINN,Liu,MPQA,custom}.countTokens.
{pos,neg},{AFINN,Liu,MPQA,custom}.directionFigure 1: features for message polarity{AFINN,Liu,custom}.countTokens.{pos,neg},{AFINN,Liu,custom}.direction,{AFINN,Liu,custom}.sentence.countTok.{pos,neg},{AFINN,Liu,custom}.sentence.direction,{AFINN,Liu,custom}.left.countTokens.{pos,neg},{AFINN,Liu,custom}.left.direction,b.sentClass.
{left,right,sentence,chosenText}Figure 2: features for contextual polaritytenceText and leftText instance segments.
These val-ues represent the count of polarized tokens, andthe direction (1, 0, or -1, as before), according to3 sentiment lexicons.
The final 4 features have theoverall sentiment classification, using the subtaskB classifier, for each text segment: leftText , right-Text , sentenceText , and chosenText .
In unconstrainedmode the instances used for subtask A training arethe same.
The difference with respect to the con-strained mode is the overall sentiment classifierused for the last 4 features, which corresponds tothe unconstrained classifier of subtask B.This subtask has specific features, different fromthose used in the previous subtask, and after sometests with SciKit-Learn classifiers, we foundthat, in this case, our best results were not ob-tained with SVM.
For subtask A, we chose Gradi-ent Boosting classifier5, an ensemble method thatcombines the predictions of several models, con-figured with deviance loss function, 0.1 for learn-ing rate, and 100 regression estimators with indi-vidual maximum depth of 4.5http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html548run LJ?14 SMS?13 T?13 T?14 T?14sA const.
81,90 78,72 84,05 80,54 82,75A unc.
79,70 82,93 83,80 77,07 80,02B const.
71,39 59,34 67,34 63,81 55,31B unc.
68,08 56,16 65,21 61,47 54,09Table 1: senti.ue scoreLJ?14 SMS?13 T?13 T?14 T?14sA avg 77,08 77,37 79,94 76,84 68,33A best 85,61 89,31 90,14 86,63 82,75B avg 63,52 55,63 59,78 60,41 45,44B best 74,84 70,28 72,12 70,96 58,16Table 2: all systems: higher and average score4 ResultsWe submitted four runs, with the system outputfor each subtask, and both constrained and uncon-strained modes.
Test set documents come fromfive sources: LiveJournal blogs (LJ?14), SMS test(SMS?13) and Twitter test (T?13) data from lastyear, a new Twitter collection (T?14), and 100tweets whose text includes sarcasm (T?14s).
Theprimary metric to evaluate the results is the aver-age F-measure for positive and negative classes.Table 1 shows the score obtained by our system.In the constrained mode, and for the five datasources, senti.ue achieved a score between78,72 and 84,05 in subtask A, and a score between55,31 and 71,39 in subtask B.
Comparing theevaluation between constrained and unconstrainedmodes, the latter was always a little below, ex-cept for one case in subtask A and SMS2013 data,where the extra training data led to a 4% score im-provement.
In this SA challenge there were a totalof 27 submissions in subtask A and 50 submis-sions in subtask B.
Among these, the best scoreand the average score for each subtask are shownin Table 2.
In both subtaks, our system result isabove the participating systems average score.
Insubtask A and the Twitter Sarcasm 2014 collection(T?14s), senti.ue achieved the highest score,with 82,75% in constrained mode.For each data set, tables 3 and 4 show the preci-sion and recall of our system result on the high-est scored mode, per class.
In subtask A preci-sion is between 64 and 99% for positive and nega-tive classes, taking the value of zero in the neutralclass.
For the overall sentiment subtask, precisionis similar among the 3 classes, having the mini-mum value in the negative class of sarcasm tweets.The best recall value was obtained in the positivetask, mode, data Positive Negative NeutralA, C, LJ?14 87,27 86,69 0,00A, U, SMS?13 85,06 85,87 1,89A, C, T?13 91,11 79,10 0,00A, C, T?14 90,37 74,74 1,14A, C, T?14s 98,78 64,86 0,00B, C, LJ?14 65,11 80,59 67,64B, C, SMS?13 48,98 55,08 88,73B, C, T?13 65,65 65,39 77,99B, C, T?14 65,89 62,87 71,00B, C, T?14s 78,79 32,50 61,54Table 3: senti.ue precision in best modetask, mode, data Positive Negative NeutralA, C, LJ?14 80,11 74,70 0,00A, U, SMS?13 80,62 80,48 11,54A, C, T?13 85,05 81,16 0,00A, C, T?14 89,09 68,25 14,29A, C, T?14s 83,51 88,89 0,00B, C, LJ?14 77,65 64,99 68,30B, C, SMS?13 83,68 58,81 74,58B, C, T?13 78,72 60,93 68,87B, C, T?14 80,07 49,42 60,28B, C, T?14s 55,32 76,47 36,36Table 4: senti.ue recall in best modeclass of the 2014 tweet collection.5 ConclusionsContinuing last year experience, we participatedin SemEval-2014 Task 9 to test our approach for areal-time SA system for the English used nowa-days in social media content.
We changed themethod for subtask A, now considering also thetext around the area to classify, by dedicating newfeatures to it, which led to good results.
Ourmethod for overall sentiment is ML based, usinga restricted set of features that are dedicated tosuperficial text properties, negation presence, andsentiment lexicons.
Without a deep linguistic anal-ysis, our system achieved a reasonable result insubtask B.
The evaluation of our solution, in bothsubtasks, shows an appreciable improvement, by10% or more, when compared to our results in2013.
We believe that the additional training in-stances used in unconstrained mode and subtaskB, about laptops and restaurants, have a writingstyle different from most of the test set documents.And perhaps this is the cause for lower score inthe unconstrained mode, something that happenedalso with many systems in the past edition (Wilsonet al., 2013).This time, we implemented the contextual polaritysolution based on the subtask B classifier.
Giventhe results, we intend to do, in the near future, a549new iteration of our system where the overall clas-sifier will depend on (or receive features from) thecurrent subtask A classifier.It seems to us that senti.ue feature engineeringcan be improved, maintaining this line of develop-ment.
Once stabilized, the introduction of namedentity recognition and a richer linguistic analysiswill help to identify the sentiment target entities,as the ultimate goal for this system.ReferencesStefano Baccianella, Andrea Esuli and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An Enhanced LexicalResource for Sentiment Analysis and Opinion Min-ing.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation -LREC?10.
European Language Resources Associa-tion.
Malta.Lee Becker, George Erhart, David Skiba and Valen-tine Matula.
2013.
AVAYA: Sentiment Analysis onTwitter with Self-Training and Polarity Lexicon Ex-pansion.
In Second Joint Conference on Lexicaland Computational Semantics (*SEM), Volume 2:Proceedings of the Seventh International Workshopon Semantic Evaluation (SemEval 2013).
Atlanta,Georgia, USA.Steven Bird.
2006.
NLTK: the natural language toolkit.In Proceedings of the COLING?06/ACL on Interac-tive presentation sessions.
Australia.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM : a library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A Library for Large Linear Classication.
Journal ofMachine Learning Research, 9:1871?1874.Tobias G?unther and Lenz Furrer.
2013.
GU-MLT-LT:Sentiment Analysis of Short Messages using Lin-guistic Features and Stochastic Gradient Descent.
InSecond Joint Conference on Lexical and Computa-tional Semantics (*SEM), Volume 2: Proceedingsof the Seventh International Workshop on SemanticEvaluation (SemEval 2013).
Atlanta, Georgia, USA.Bing Liu, Minqing Hu and Junsheng Cheng.
2005.Opinion Observer: Analyzing and Comparing Opin-ions on the Web.
In Proceedings of the 14th Interna-tional World Wide Web conference (WWW-2005).Chiba, Japan.Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets.
In Pro-ceedings of the seventh international workshop onSemantic Evaluation Exercises (SemEval-2013).
At-lanta, Georgia, USA.Finn?Arup Nielsen.
2011.
A New ANEW: Evaluation ofa Word List for Sentiment Analysis in Microblogs.In Proceedings, 1st Workshop on Making Sense ofMicroposts (#MSM2011): Big things come in smallpackages.
pp: 93-98.
Greece.Princeton University.
2010.
?About WordNet.?
Word-Net.
http://wordnet.princeton.eduSara Rosenthal, Alan Ritter, Veselin Stoyanov, andPreslav Nakov.
2014.
SemEval-2014 Task 9: Sen-timent Analysis in Twitter.
In Proceedings of theEighth International Workshop on Semantic Evalu-ation (SemEval?14).
August 23-24, 2014, Dublin,Ireland.Jos?e Saias.
2013.
In search of reputation assess-ment: Experiences with polarity classification inreplab 2013.
In Pamela Forner, Roberto Nav-igli, and Dan Tufis, editors, CLEF 2013 EvaluationLabs and Workshop Online Working Notes - OnlineReputation Management (RepLab), Valencia, Spain,September 2013.
ISBN 978-88-904810-5-5.Jos?e Saias and Hil?ario Fernandes.
2013. senti.ue-en:an approach for informally written short texts insemeval-2013 sentiment analysis task.
In SecondJoint Conference on Lexical and Computational Se-mantics (*SEM), Volume 2: Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 508?512, Atlanta, Geor-gia, USA.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalua-tion, 39:165?210.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,Alan Ritter, Sara Rosenthal and Veselin Stoyanov.2013.
SemEval-2013 task 2: Sentiment Analysisin Twitter.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation.
ACL.550
