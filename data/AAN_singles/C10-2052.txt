Coling 2010: Poster Volume, pages 454?462,Beijing, August 2010What?s in a Preposition?Dimensions of Sense Disambiguation for an Interesting Word ClassDirk Hovy, Stephen Tratz, and Eduard HovyInformation Sciences InstituteUniversity of Southern California{dirkh, stratz, hovy}@isi.eduAbstractChoosing the right parameters for a wordsense disambiguation task is critical tothe success of the experiments.
We ex-plore this idea for prepositions, an of-ten overlooked word class.
We examinethe parameters that must be considered inpreposition disambiguation, namely con-text, features, and granularity.
Doingso delivers an increased performance thatsignificantly improves over two state-of-the-art systems, and shows potential forimproving other word sense disambigua-tion tasks.
We report accuracies of 91.8%and 84.8% for coarse and fine-grainedpreposition sense disambiguation, respec-tively.1 IntroductionAmbiguity is one of the central topics in NLP.
Asubstantial amount of work has been devoted todisambiguating prepositional attachment, words,and names.
Prepositions, as with most other wordtypes, are ambiguous.
For example, the word incan assume both temporal (?in May?)
and spatial(?in the US?)
meanings, as well as others, lesseasily classifiable (?in that vein?).
Prepositionstypically have more senses than nouns or verbs(Litkowski and Hargraves, 2005), making themdifficult to disambiguate.Preposition sense disambiguation (PSD) hasmany potential uses.
For example, due to therelational nature of prepositions, disambiguatingtheir senses can help with all-word sense disam-biguation.
In machine translation, different sensesof the same English preposition often correspondto different translations in the foreign language.Thus, disambiguating prepositions correctly mayhelp improve translation quality.1 Coarse-grainedPSD can also be valuable for information extrac-tion, where the sense acts as a label.
In a recentstudy, Hwang et al (2010) identified prepositionrelated features, among them the coarse-grainedPP labels used here, as the most informative fea-ture in identifying caused-motion constructions.Understanding the constraints that hold for prepo-sitional constructions could help improve PP at-tachment in parsing, one of the most frequentsources of parse errors.Several papers have successfully addressedPSD with a variety of different approaches (Rudz-icz and Mokhov, 2003; O?Hara and Wiebe, 2003;Ye and Baldwin, 2007; O?Hara and Wiebe, 2009;Tratz and Hovy, 2009).
However, while it is oftenpossible to increase accuracy by using a differ-ent classifier and/or more features, adding morefeatures creates two problems: a) it can lead tooverfitting, and b) while possibly improving ac-curacy, it is not always clear where this improve-ment comes from and which features are actuallyinformative.
While parameter studies exist forgeneral word sense disambiguation (WSD) tasks(Yarowsky and Florian, 2002), and PSD accuracyhas been steadily increasing, there has been noexploration of the parameters of prepositions toguide engineering decisions.We go beyond simply improving accuracy toanalyze various parameters in order to determinewhich ones are actually informative.
We explorethe different options for context and feature se-1See (Chan et al, 2007) for the relevance of word sensedisambiguation and (Chiang et al, 2009) for the role ofprepositions in MT.454lection, the influence of different preprocessingmethods, and different levels of sense granular-ity.
Using the resulting parameters in a MaximumEntropy classifier, we are able to improve signif-icantly over existing results.
The general outlinewe present can potentially be extended to otherword classes and improve WSD in general.2 Related WorkRudzicz and Mokhov (2003) use syntactic andlexical features from the governor and the preposi-tion itself in coarse-grained PP classification withdecision heuristics.
They reach an average F-measure of 89% for four classes.
This shows thatusing a very small context can be effective.
How-ever, they did not include the object of the prepo-sition and used only lexical features for classifi-cation.
Their results vary widely for the differentclasses.O?Hara and Wiebe (2003) made use of a win-dow size of five words and features from thePenn Treebank (PTB) (Marcus et al, 1993) andFrameNet (Baker et al, 1998) to classify prepo-sitions.
They show that using high level fea-tures, such as semantic roles, significantly aid dis-ambiguation.
They caution that using colloca-tions and neighboring words indiscriminately mayyield high accuracy, but has the risk of overfit-ting.
O?Hara and Wiebe (2009) show compar-isons of various semantic repositories as labels forPSD approaches.
They also provide some resultsfor PTB-based coarse-grained senses, using a five-word window for lexical and hypernym features ina decision tree classifier.SemEval 2007 (Litkowski and Hargraves,2007) included a task for fine-grained PSD (morethan 290 senses).
The best participating system,that of Ye and Baldwin (2007), extracted part-of-speech and WordNet (Fellbaum, 1998) featuresusing a word window of seven words in a Max-imum Entropy classifier.
Tratz and Hovy (2009)present a higher-performing system using a set of20 positions that are syntactically related to thepreposition instead of a fixed window size.Though using a variety of different extractionmethods, contexts, and feature words, none ofthese approaches explores the optimal configura-tions for PSD.3 Theoretical BackgroundThe following parameters are applicable to otherword classes as well.
We will demonstrate theireffectiveness for prepositions.Analyzing the syntactic elements of preposi-tional phrases, one discovers three recurring ele-ments that exhibit syntactic dependencies and de-fine a prepositional phrase.
The first one is thegoverning word (usually a noun, verb, or adjec-tive)2, the preposition itself, and the object of thepreposition.Prepositional phrases can be fronted (?In May,prices dropped by 5%?
), so that the governor (inthis case the verb ?drop?)
occurs later in the sen-tence.
Similarly, the object can be fronted (con-sider ?a dessert to die for?
).In the simplest version, we can do classificationbased only on the preposition and the governor orobject alone.3 Furthermore, directly neighboringwords can influence the preposition, mostly two-word prepositions such as ?out of?
or ?becauseof?.To extract the words discussed above, one caneither employ a fixed window size, (which hasto be large enough to capture the words), or se-lect them based on heuristics or parsing informa-tion.
The governor and object can be hard to ex-tract if they are fronted, since they do not occur intheir unusual positions relative to the preposition.While syntactically related words improve overfixed-window-size approaches (Tratz and Hovy,2009), it is not clear which words contribute most.There should be an optimal context, i.e., the small-est set of words that achieves the best accuracy.
Ithas to be large enough to capture all relevant infor-mation, but small enough to avoid noise words.4We surmise that earlier approaches were not uti-lizing that optimal context, but rather include a lotof noise.Depending on the task, different levels of sensegranularity may be used.
Fewer senses increasethe likelihood of correct classification, but may in-2We will refer to the governing word, irrespective ofclass, as governor.3Basing classification on the preposition alone is not fea-sible, because of the very polysemy we try to resolve.4It is not obvious how much information a sister-PP canprovide, or the subject of the superordinate clause.455correctly conflate prepositions.
A finer granular-ity can help distinguish nuances and better fit thedifferent contexts.
However, it might suffer fromsparse data.4 Experimental SetupWe explore the different context types (fixed win-dow size vs. selective), the influence of the wordsin that context, and the preprocessing method(heuristics vs. parsing) on both coarse and fine-grained disambiguation.
We use a most-frequent-sense baseline.
In addition, we compare to thestate-of-the-art systems for both types of granu-larity (O?Hara and Wiebe, 2009; Tratz and Hovy,2009).
Their results show what has been achievedso far in terms of accuracy, and serve as a secondmeasure for comparison beyond the baseline.4.1 ModelWe use the MALLET implementation (McCal-lum, 2002) of a Maximum Entropy classifier(Berger et al, 1996) to construct our models.
Thisclassifier was also used by two state-of-the-artsystems (Ye and Baldwin, 2007; Tratz and Hovy,2009).
For fine-grained PSD, we train a separatemodel for each preposition due to the high num-ber of possible classes for each individual prepo-sition.
For coarse-grained PSD, we use a singlemodel for all prepositions, because they all sharethe same classes.4.2 DataWe use two different data sets from existing re-sources for coarse and fine-grained PSD to makeour results as comparable to previous work as pos-sible.For the coarse-grained disambiguation, we usedata from the POS tagged version of the WallStreet Journal (WSJ) section of the Penn Tree-Bank.
A subset of the prepositional phrases inthis corpus is labelled with a set of seven classes:beneficial (BNF), direction (DIR), extent (EXT),location (LOC), manner (MNR), purpose (PRP),and temporal (TMP).
We extract only those prepo-sitions that head a PP labelled with such a class(N = 35, 917).
The distribution of classes ishighly skewed (cf.
Figure 1).
We compare thePTB class distribPage 1LOC TMP DIR MNR PRP EXT BNF020004000600080001000012000140001600018000 169951033254141781 1071 280 44classesfrequencyFigure 1: Distribution of Class Labels in the WSJSection of the Penn TreeBank.results of this task to the findings of O?Hara andWiebe (2009).For the fine-grained task, we use data fromthe SemEval 2007 workshop (Litkowski and Har-graves, 2007), separate XML files for the 34 mostfrequent English prepositions, comprising 16, 557training and 8096 test sentences, each instancecontaining one example of the respective prepo-sition.
Each preposition has between two and 25senses (9.76 on average) as defined by The Prepo-sition Project (Litkowski and Hargraves, 2005).We compare our results directly to the findingsfrom Tratz and Hovy (2009).
As in the originalworkshop task, we train and test on separate sets.5 ResultsIn this section we show experimental results forthe influence of word extraction method (parsingvs.
POS-based heuristics), context, and feature se-lection on accuracy.
Each section compares theresults for both coarse and fine-grained granular-ity.
Accuracy for the coarse-grained task is in allexperiments higher than for the fine-grained one.5.1 Word ExtractionIn order to analyze the impact of the extractionmethod, we compare parsing versus POS-basedheuristics for word extraction.Both O?Hara and Wiebe (2009) and Tratz andHovy (2009) use constituency parsers to prepro-cess the data.
However, parsing accuracy varies,456and the problem of PP attachment ambiguity in-creases the likelihood of wrong extractions.
Thisis especially troublesome in the present case,where we focus on prepositions.5 We use theMALT parser (Nivre et al, 2007), a state-of-the-art dependency parser, to extract the governor andobject.The alternative is a POS-based heuristics ap-proach.
The only preprocessing step needed isPOS tagging of the data, for which we used thesystem of Shen et al (2007).
We then use simpleheuristics to locate the prepositions and their re-lated words.
In order to determine the governorin the absence of constituent phrases, we considerthe possible governing noun, verb, and adjective.The object of the preposition is extracted as firstnoun phrase head to the right.
This approach isfaster than parsing, but has problems with long-range dependencies and fronting of the PP (e.g.,the PP appearing earlier in the sentence than itsgovernor).
word selectionPage 1MALT 84.4 94.084.8 90.984.8 91.8extraction method fine coarseHeuristicsMALT + HeuristicsTable 1: Accuracies (%) for Word-Extraction Us-ing MALT Parser or Heuristics.Interestingly, the extraction method does notsignificantly affect the final score for fine-grainedPSD (see Table 1).
The high score achieved whenusing the MALT parse for coarse-grained PSDcan be explained by the fact that the parser wasoriginally trained on that data set.
The good re-sults we see when using heuristics-based extrac-tion only, however, means we can achieve high-accuracy PSD even without parsing.5.2 ContextWe compare the effects of fixed window size ver-sus syntactically related words as context.
Table 2shows the results for the different types and sizesof contexts.65Rudzicz and Mokhov (2003) actually motivate theirwork as a means to achieve better PP attachment resolution.6See also (Yarowsky and Florian, 2002) for experimentson the effect of varying window size for WSD.contextPage 191.6 80.492.0 81.491.6 79.891.0 78.780.7 78.994.2 56.994.0 84.8Context coarse fine2-word window3-word window4-word window5-word windowGovernor, prepPrep, objectGovernor, prep, objectTable 2: Accuracies (%) for Different ContextTypes and SizesThe results show that the approach using bothgovernor and object is the most accurate one.
Ofthe fixed-window-size approaches, three words toeither side works best.
This does not necessarilyreflect a general property of that window size, butcan be explained by the fact that most governorsand objects occur within this window size.7 Thisdista ce can vary from corpus to corpus, so win-dow size would have to be determined individu-ally for each task.
The difference between usinggovernor and preposition versus preposition andobject between coarse and fine-grained classifica-tion might reflect the annotation process: whileLitkowski and Hargraves (2007) selected exam-ples based on a search for governors8, most anno-tators in the PTB may have based their decisionof the PP label on the object that occurs in it.
Weconclude that syntactically related words present abetter context for classification than fixed windowsizes.5.3 FeaturesHaving established the context we want to use, wenow turn to the details of extracting the featurewords from that context.9 Using higher-level fea-tures instead of lexical ones helps accounting forsparse training data (given an infinite amount ofdata, we would not need to take any higher-level7Based on such statistics, O?Hara and Wiebe (2003) ac-tually set their window size to 5.8Personal communication.9As one reviewer pointed out, these two dimensions arehighly interrelated and influence each other.
To examine theeffects, we keep one dimension constant while varying theother.457features into account, since every case would becovered).
Compare O?Hara and Wiebe (2009).Following the prepocessing, we use a set ofrules to select the feature words, and then gen-erate feature values from them using a varietyof feature-generating functions.10 The word-selection rules are listed below.Word-Selection Rules?
Governor from the MALT parse?
Object from the MALT parse?
Heuristically determined object of the prepo-sition?
First verb to the left of the preposition?
First verb/noun/adjective to the left of thepreposition?
Union of (First verb to the left, Firstverb/noun/adjective to the left)?
First word to the leftThe feature-generating functions, many ofwhich utilize WordNet (Fellbaum, 1998), arelisted below.
To conserve space, curly braces areused to represent multiple functions in a singleline.
The name of each feature is the combinationof the word-selection rule and the output from thefeature-generating function.WordNet-based Features?
{Hypernyms, Synonyms} for {1st, all}sense(s) of the word?
All terms in the definitions (?glosses?)
of theword?
Lexicographer file names for the word?
Lists of all link types (e.g., meronym links)associated with the word?
Part-of-speech indicators for the existence ofNN/VB/JJ/RB entries for the word?
All sentence frames for the word?
All {part, member, substance}-of holonymsfor the word?
All sentence frames for the wordOther Features?
Indicator that the word-finding rule found aword10Some words may be selected by multiple word-selectionrules.
For example, the governor of the preposition maybe identified by the Governor from MALT parse rule, firstnoun/verb/adjective to left, and the first word to the left rule.?
Capitalization indicator?
{Lemma, surface form} of the word?
Part-of-speech tag for the word?
General POS tag for the word (e.g.
NNS?NN, VBZ?
VB)?
The {first, last} {two, three} letters of eachword?
Indicators for suffix types (e.g., de-adjectival, de-nominal [non]agentive,de-verbal [non]agentive)?
Indicators for a wide variety of other affixesincluding those related to degree, number, or-der, etc.
(e.g., ultra-, poly-, post-)?
Roget?s Thesaurus divisions for the wordTo establish the impact of each feature word onthe outcome, we use leave-one-out and only-oneevaluation.11 The results can be found in Table 3.A word that does not perform well as the only at-tribute may still be important in conjunction withothers.
Conversely, leaving out a word may nothurt performance, despite being a good single at-tribute.
word selectionPage 1Word LOO LOO92.1 80.1 84.3 78.993.4 94.2 84.9 56.392.0 77.9 85.0 62.192.1 78.7 84.3 78.592.1 78.4 84.5 81.092.0 78.8 84.4 77.291.9 93.0 84.9 56.891.8 ?
84.8 ?coarse fineOnly OnlyMALT governorMALT objectHeuristics VB to leftHeur.
NN/VB/ADJ to leftHeur.
Governor UnionHeuristics word to leftHeuristics objectnoneTable 3: Accuracies (%) for Leave-One-Out (LOO) and Only-One Word-Extraction-RuleEvaluation.
none includes all words and serves forcomparison.
Important words reduce accuracy forLOO, but rank high when used as only rule.Independent of the extraction method (MALTparser or POS-based heuristics), the governor isthe most informative word.
Combining severalheuristics to locate the governor is the best sin-gle feature for fine-grained classification.
The rulelooking only for a governing verb fails to account11Since the feature words are not independent of one an-other, neither of the two measures is decisive on its own.458full bothPage 1Total Total Total Total?
?
6 100.0 125 90.4 53 47.2364 94.0 5 80.0 ?
?
74 93.223 69.6 78 65.4 ?
?
1 0.0151 96.7 87 79.3 ?
?
7 71.453 79.2 841 92.5 of 1478 87.9 71 64.892 92.4 16 43.8 76 84.2 28 75.0173 96.0 45 71.1 441 81.4 2287 90.8?
?
5 80.0 58 91.4 15 53.3?
?
58 70.7 out ?
?
90 68.950 80.0 358 93.9 ?
?
62 90.3?
?
1 0.0 98 79.6 417 89.4155 69.0 107 86.0 ?
?
6 83.384 100.0 232 84.5 per ?
?
3 100.0?
?
2 50.0 82 65.9 ?
?367 86.4 3078 92.0 ?
?
449 94.4?
?
5 100.0 ?
?
2 0.0?
?
420 91.7 208 48.1 364 69.020 90.0 384 83.3 ?
?
62 93.568 77.9 65 87.7 ?
?
3 100.0?
?
94 71.3 to 572 89.7 3166 97.528 78.6 11 72.7 ?
?
55 65.529 100.0 4 100.0 102 97.1 2 100.0?
?
1 0.0 ?
?
604 91.4102 94.1 98 84.7 ?
?
2 50.0?
?
45 64.4 ?
?
208 94.2248 88.3 1341 87.5 up ?
?
20 75.0down 153 81.7 16 56.2 ?
?
23 73.939 87.2 547 92.1 via ?
?
22 40.9?
?
1 0.0 ?
?
1 100.0478 82.4 1455 84.5 ?
?
3 33.3578 85.5 1712 90.5 578 84.4 272 69.5in 688 77.0 15706 95.0 ?
?
213 96.238 73.7 24 91.7 ?
?
69 63.8297 86.2 415 80.0Overall 8096 84.8 35917 91.8fine coarse fine coarsePrep Acc Acc Prep Acc Accaboard likeabout nearabove nearestacross nextafteragainst offalong onalongside ontoamidamong outsideamongst overaround pastasastride roundat sinceatop thanbecause throughbefore throughoutbehind tillbelowbeneath towardbeside towardsbesides underbetween underneathbeyond untilbyuponduringexcept whetherfor whilefrom withwithininside withoutintoTable 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics.
Sorted bypreposition.for noun governors, which consequently leads toa slight improvement when left out.Curiously, the word directly to the left is a bet-ter single feature than the object (for fine-grainedclassification).
Leaving either of them out in-creases accuracy, which implies that their infor-mation can be covered by other words.459coarse both 2009Page 1Most Frequent Sensef1 f1 f1LOC 71.8 97.4 82.6 90.8 93.2 92.0 94.7 96.4 95.6TMP 77.5 39.4 52.3 84.5 85.2 84.8 94.6 94.6 94.6DIR 91.6 94.2 92.8 95.6 96.5 96.1 94.6 94.5 94.5MNR 69.9 43.2 53.4 82.6 55.8 66.1 83.3 75.0 78.9PRP 78.2 48.8 60.1 79.3 70.1 74.4 90.6 83.8 87.1EXT 0.0 0.0 0.0 81.7 84.6 82.9 87.5 82.1 84.7BNF 0.0 0.0 0.0 ?
?
?
75.0 34.1 46.9O'Hara/Wiebe 2009 10-fold CVClass prec rec prec rec prec recTable 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification.
Comparison to O?Haraand Wiebe (2009).
Classes ordered by frequency5.4 Comparison with Related WorkTo situate our experimental results within thebody of work on PSD, we compare them to botha most-frequent-sense baseline and existing workfor both granularities (see Table 6).
The resultsuse a syntactically selective context of preposi-tion, governor, object, and word to the left asdetermined by combined extraction information(POS tagging and parsing).accuraciesPage 175.8 39.689.3* 78.3**93.9 84.8coarse fineBaselineRelated WorkOur systemTable 6: Accuracies (%) for Different Classifi-cations.
Comparison with O?Hara and Wiebe(2009)*, and Tratz and Hovy (2009)**.Our system easily exceeds the baseline for bothcoarse and fine-grained PSD (see Table 6).
Com-parison with related work shows that we achievean improvement of 6.5% over Tratz and Hovy(2009), which is significant at p < .0001, andof 4.5% over O?Hara and Wiebe (2009), which issignificant at p < .0001.A detailed overview over all prepositions forfrequencies and accuracies of both coarse andfine-grained PSD can be found in Table 4.In addition to overall accuracy, O?Hara andWiebe (2009) also measure precision, recall andF-measure for the different classes.
They omittedBNF because it is so infrequent.
Due to differenttraining data and models, the two systems are notstrictly comparable, yet they provide a sense ofthe general task difficulty.
See Table 5.
We notethat both systems perform better than the most-frequent-sense baseline.
DIR is reliably classifiedusing the baseline, while EXT and BNF are neverselected for any preposition.
Our method addsconsiderably to the scores for most classes.
Thelow score for BNF is mainly due to the low num-ber of instances in the data, which is why it wasexcluded by O?Hara and Wiebe (2009).6 ConclusionTo get maximal accuracy in disambiguatingprepositions?and also other word classes?oneneeds to consider context, features, and granular-ity.
We presented an evaluation of these parame-ters for preposition sense disambiguation (PSD).We find that selective context is better thanfixed window size.
Within the context for prepo-sitions, the governor (head of the NP or VP gov-erning the preposition), the object of the prepo-sition (i.e., head of the NP to the right), and theword directly to the left of the preposition havethe highest influence.12 This corroborates the lin-guistic intuition that close mutual constraints holdbetween the elements of the PP.
Each word syn-tactically and semantically restricts the choice ofthe other elements.
Combining different extrac-tion methods (POS-based heuristics and depen-dency parsing) works better than either one in iso-lation, though high accuracy can be achieved justusing heuristics.
The impact of context and fea-tures varies somewhat for different granularities.12These will likely differ for other word classes.460Not surprisingly, we see higher scores for coarsergranularity than for the more fine-grained one.We measured success in accuracy, precision, re-call, and F-measure, and compared our results toa most-frequent-sense baseline and existing work.We were able to improve over state-of-the-art sys-tems in both coarse and fine-grained PSD, achiev-ing accuracies of 91.8% and 84.8% respectively.AcknowledgementsThe authors would like to thank Steve DeNeefe,Victoria Fossum, and Zornitsa Kozareva for com-ments and suggestions.
StephenTratz is supportedby a National Defense Science and Engineeringfellowship.ReferencesBaker, C.F., C.J.
Fillmore, and J.B. Lowe.
1998.The Berkeley FrameNet Project.
In Proceedings ofthe 17th international conference on Computationallinguistics-Volume 1, pages 86?90.
Association forComputational Linguistics Morristown, NJ, USA.Berger, A.L., V.J.
Della Pietra, and S.A. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39?71.Chan, Y.S., H.T.
Ng, and D. Chiang.
2007.
Word sensedisambiguation improves statistical machine trans-lation.
In Annual Meeting ?
Association For Com-putational Linguistics, volume 45, pages 33?40.Chiang, D., K. Knight, and W. Wang.
2009.
11,001new features for statistical machine translation.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 218?226, Boulder, Colorado, June.Association for Computational Linguistics.Fellbaum, C. 1998.
WordNet: an electronic lexicaldatabase.
MIT Press USA.Hwang, J. D., R. D. Nielsen, and M. Palmer.
2010.Towards a domain independent semantics: Enhanc-ing semantic representation with construction gram-mar.
In Proceedings of the NAACL HLT Workshopon Extracting and Using Constructions in Computa-tional Linguistics, pages 1?8, Los Angeles, Califor-nia, June.
Association for Computational Linguis-tics.Litkowski, K. and O. Hargraves.
2005.
The preposi-tion project.
ACL-SIGSEM Workshop on ?The Lin-guistic Dimensions of Prepositions and Their Use inComputational Linguistic Formalisms and Applica-tions?, pages 171?179.Litkowski, K. and O. Hargraves.
2007.
SemEval-2007Task 06: Word-Sense Disambiguation of Preposi-tions.
In Proceedings of the 4th International Work-shop on Semantic Evaluations (SemEval-2007),Prague, Czech Republic.Marcus, M.P., M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: the Penn TreeBank.
Computational Linguis-tics, 19(2):313?330.McCallum, A.K.
2002.
MALLET: A Machine Learn-ing for Language Toolkit.
2002. http://mallet.
cs.umass.
edu.Nivre, J., J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13(02):95?135.O?Hara, T. and J. Wiebe.
2003.
Preposition semanticclassification via Penn Treebank and FrameNet.
InProceedings of CoNLL, pages 79?86.O?Hara, T. and J. Wiebe.
2009.
Exploiting seman-tic role resources for preposition disambiguation.Computational Linguistics, 35(2):151?184.Rudzicz, F. and S. A. Mokhov.
2003.
To-wards a heuristic categorization of prepo-sitional phrases in english with word-net.
Technical report, Cornell University,arxiv1.library.cornell.edu/abs/1002.1095-?context=cs.Shen, L., G. Satta, and A. Joshi.
2007.
Guided learn-ing for bidirectional sequence classification.
In Pro-ceedings of the 45th Annual Meeting of the Associa-tion of Computational Linguistics, volume 45, pages760?767.Tratz, S. and D. Hovy.
2009.
Disambiguation ofpreposition sense using linguistically motivated fea-tures.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Companion Volume: Student Re-search Workshop and Doctoral Consortium, pages96?100, Boulder, Colorado, June.
Association forComputational Linguistics.Yarowsky, D. and R. Florian.
2002.
Evaluating sensedisambiguation across diverse parameter spaces.Natural Language Engineering, 8(4):293?310.461Ye, P. and T. Baldwin.
2007.
MELB-YB: PrepositionSense Disambiguation Using Rich Semantic Fea-tures.
In Proceedings of the 4th International Work-shop on Semantic Evaluations (SemEval-2007),Prague, Czech Republic.462
