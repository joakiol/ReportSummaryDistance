Proceedings of ACL-08: HLT, pages 816?824,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsGenerating Impact-Based Summaries for Scientific LiteratureQiaozhu MeiUniversity of Illinois at Urbana-Champaignqmei2@uiuc.eduChengXiang ZhaiUniversity of Illinois at Urbana-Champaignczhai@cs.uiuc.eduAbstractIn this paper, we present a study of a novelsummarization problem, i.e., summarizing theimpact of a scientific publication.
Given a pa-per and its citation context, we study how toextract sentences that can represent the mostinfluential content of the paper.
We proposelanguage modeling methods for solving thisproblem, and study how to incorporate fea-tures such as authority and proximity to ac-curately estimate the impact language model.Experiment results on a SIGIR publicationcollection show that the proposed methodsare effective for generating impact-based sum-maries.1 IntroductionThe volume of scientific literature has been growingrapidly.
From recent statistics, each year 400,000new citations are added to MEDLINE, the majorbiomedical literature database 1.
This fast growthof literature makes it difficult for researchers, espe-cially beginning researchers, to keep track of the re-search trends and find high impact papers on unfa-miliar topics.Impact factors (Kaplan and Nelson, 2000) areuseful, but they are just numerical values, so theycannot tell researchers which aspects of a paper areinfluential.
On the other hand, a regular content-based summary (e.g., the abstract or conclusion sec-tion of a paper or an automatically generated topicalsummary (Giles et al, 1998)) can help a user know1http://www.nlm.nih.gov/bsd/history/tsld024.htmabout the main content of a paper, but not necessar-ily the most influential content of the paper.
Indeed,the abstract of a paper mostly reflects the expectedimpact of the paper as perceived by the author(s),which could significantly deviate from the actual im-pact of the paper in the research community.
More-over, the impact of a paper changes over time due tothe evolution and progress of research in a field.
Forexample, an algorithm published a decade ago maybe no longer the state of the art, but the problem def-inition in the same paper can be still well accepted.Although much work has been done on text sum-marization (See Section 6 for a detailed survey), tothe best of our knowledge, the problem of impactsummarization has not been studied before.
In thispaper, we study this novel summarization problemand propose language modeling-based approachesto solving the problem.
By definition, the impactof a paper has to be judged based on the consent ofresearch community, especially by people who citedit.
Thus in order to generate an impact-based sum-mary, we must use not only the original content, butalso the descriptions of that paper provided in paperswhich cited it, making it a challenging task and dif-ferent from a regular summarization setup such asnews summarization.
Indeed, unlike a regular sum-marization system which identifies and interprets thetopic of a document, an impact summarization sys-tem should identify and interpret the impact of a pa-per.We define the impact summarization problem inthe framework of extraction-based text summariza-tion (Luhn, 1958; McKeown and Radev, 1995), andcast the problem as an impact sentence retrieval816problem.
We propose language models to exploitboth the citation context and original content of apaper to generate an impact-based summary.
Westudy how to incorporate features such as author-ity and proximity into the estimation of languagemodels.
We propose and evaluate several differentstrategies for estimating the impact language model,which is key to impact summarization.
No exist-ing test collection is available for evaluating impactsummarization.
We construct a test collection us-ing 28 years of ACM SIGIR papers (1978 - 2005)to evaluate the proposed methods.
Experiment re-sults on this collection show that the proposed ap-proaches are effective for generating impact-basedsummaries.
The results also show that using both theoriginal document content and the citation contextsis important and incorporating citation authority andproximity is beneficial.An impact-based summary is not only useful forfacilitating the exploration of literature, but alsohelpful for suggesting query terms for literatureretrieval, understanding the evolution of researchtrends, and identifying the interactions of differentresearch fields.
The proposed methods are also ap-plicable to summarizing the impact of documents inother domains where citation context exists, such asemails and weblogs.The rest of the paper is organized as follows.
InSection 2 and 3, we define the impact-based summa-rization problem and propose the general languagemodeling approach.
In Section 4, we present differ-ent strategies and features for estimating an impactlanguage model, a key challenge in impact summa-rization.
We discuss our experiments and results inSection 5.
Finally, the related work and conclusionsare discussed in Section 6 and Section 7.2 Impact SummarizationFollowing the existing work on topical summariza-tion of scientific literature (Paice, 1981; Paice andJones, 1993), we define an impact-based summaryof a paper as a set of sentences extracted froma paper that can reflect the impact of the paper,where ?impact?
is roughly defined as the influenceof the paper on research of similar or related top-ics as reflected in the citations of the paper.
Suchan extraction-based definition of summarization hasalso been quite common in most existing generalsummarization work (Radev et al, 2002).By definition, in order to generate an impact sum-mary of a paper, we must look at how other paperscite the paper, use this information to infer the im-pact of the paper, and select sentences from the orig-inal paper that can reflect the inferred impact.
Notethat we do not directly use the sentences from the ci-tation context to form a summary.
This is because incitations, the discussion of the paper cited is usuallymixed with the content of the paper citing it, andsometimes also with discussion about other paperscited (Siddharthan and Teufel, 2007).Formally, let d = (s0, s1, ..., sn) be a paper tobe summarized, where si is a sentence.
We referto a sentence (in another paper) in which there isan explicit citation of d as a citing sentence of d.When a paper is cited, it is often discussed consec-utively in more than one sentence near the citation,thus intuitively we would like to consider a windowof sentences centered at a citing sentence; the win-dow size would be a parameter to set.
We call sucha window of sentences a citation context, and use Cto denote the union of all the citation contexts of din a collection of research papers.
Thus C itself isa set (more precisely bag) of sentences.
The taskof impact-based summarization is thus to 1) con-struct a representation of the impact of d, I , basedon d and C; 2) design a scoring function Score(.
)to rank sentences in d based on how well a sentencereflects I .
A user-defined number of top-ranked sen-tences can then be selected as the impact summaryfor d.The formulation above immediately suggests thatwe can cast the impact summarization problem asa retrieval problem where each candidate sentencein d is regarded as a ?document,?
the impact of thepaper (i.e., I) as a ?query,?
and our goal is to ?re-trieve?
sentences that can reflect the impact of thepaper as indicated by the citation context.
Lookingat the problem in this way, we see that there are twomain challenges in impact summarization: first, wemust be able to infer the impact based on both thecitation contexts and the original document; second,we should measure how well a sentence reflects thisinferred impact.
To solve these challenges, in thenext section, we propose to model impact with un-igram language models and score sentences using817Kullback-Leibler divergence.
We further proposemethods for estimating the impact language modelbased on several features including the authority ofcitations, and the citation proximity.3 Language Models for ImpactSummarization3.1 Impact language modelsFrom the retrieval perspective, our collection is thepaper to be summarized, and each sentence is a?document?
to be retrieved.
However, unlike in thecase of ad hoc retrieval, we do not really have aquery describing the impact of the paper; instead,we have a lot of citation contexts that can be usedto infer information about the query.
Thus the mainchallenge in impact summarization is to effectivelyconstruct a ?virtual impact query?
based on the cita-tion contexts.What should such a virtual impact query looklike?
Intuitively, it should model the impact-reflecting content of the paper.
We thus propose torepresent such a virtual impact query with a unigramlanguage model.
Such a model is expected to assignhigh probabilities to those words that can describethe impact of paper d, just as we expect a querylanguage model in ad hoc retrieval to assign highprobabilities to words that tend to occur in relevantdocuments (Ponte and Croft, 1998).
We call such alanguage model the impact language model of paperd (denoted as ?I ); it can be estimated based on bothd and its citation context C as will be discussed inSection 4.3.2 KL-divergence scoringWith the impact language model in place, wecan then adopt many existing probabilistic retrievalmodels such as the classical probabilistic retrievalmodels (Robertson and Sparck Jones, 1976) and theKullback-Leibler (KL) divergence retrieval model(Lafferty and Zhai, 2001; Zhai and Lafferty, 2001a),to solve the problem of impact summarization byscoring sentences based on the estimated impact lan-guage model.
In our study, we choose to use the KL-divergence scoring method to score sentences as thismethod has performed well for regular ad hoc re-trieval tasks (Zhai and Lafferty, 2001a) and has aninformation theoretic interpretation.To apply the KL-divergence scoring method, weassume that a candidate sentence s is generated froma sentence language model ?s.
Given s in d and thecitation context C , we would first estimate ?s basedon s and estimate ?I based on C , and then score swith the negative KL divergence of ?s and ?I .
Thatis,Score(s) = ?D(?I ||?s)=?w?Vp(w|?I) log p(w|?s)?
?w?Vp(w|?I) log p(w|?I)where V is the set of words in our vocabulary and wdenotes a word.From the information theoretic perspective, theKL-divergence of ?s and ?I can be interpretedas measuring the average number of bits wastedin compressing messages generated according to?I (i.e., impact descriptions) with coding non-optimally designed based on ?s.
If ?s and ?I arevery close, the KL-divergence would be small andScore(s) would be high, which intuitively makessense.
Note that the second term (entropy of ?I ) isindependent of s, so it can be ignored for ranking s.We see that according to the KL-divergence scor-ing method, our main tasks are to estimate ?s and?I .
Since s can be regarded as a short document, wecan use any standard method to estimate ?s.
In thiswork, we use Dirichlet prior smoothing (Zhai andLafferty, 2001b) to estimate ?s as follows:p(w|?s) =c(w, s) + ?s ?
P (w|D)|s| + ?s(1)where |s| is the length of s, c(w, s) is the count ofword w in s, p(w|D) is a background model esti-mated using c(w,D)Pw?
?V c(w?,D)(D can be the set of allthe papers available to us) and ?s is a smoothing pa-rameter to be empirically set.
Note that as the lengthof a sentence is very short, smoothing is critical foraddressing the data sparseness problem.The remaining challenge is to estimate ?I accu-rately based on d and its citation contexts.4 Estimation of Impact Language ModelsIntuitively, the impact of a paper is mostly reflectedin the citation context.
Thus the estimation of theimpact language model should be primarily basedon the citation context C .
However, we would like818our impact model to be able to help us select impact-reflecting sentences from d, thus it is important forthe impact model to explain well the paper contentin general.
To achieve this balance, we treat the ci-tation context C as prior information and the currentdocument d as the observed data, and use Bayesianestimation to estimate the impact language model.Specifically, let p(w|C) be a citation context lan-guage model estimated based on the citation con-text C .
We define Dirichlet prior with parameters{?Cp(w|C)}w?V for the impact model, where ?Cencodes our confidence on this prior and effectivelyserves as a weighting parameter for balancing thecontribution of C and d for estimating the impactmodel.
Given the observed document d, the poste-rior mean estimate of the impact model would be(MacKay and Peto, 1995; Zhai and Lafferty, 2001b)P (w|?I) =c(w, d) + ?cp(w|C)|d| + ?c(2)?c can be interpreted as the equivalent sample size ofour prior.
Thus setting ?c = |d| means that we putequal weights on the citation context and the doc-ument itself.
?c = 0 yields p(w|?I) = p(w|d),which is to say that the impact is entirely capturedby the paper itself, and our impact summarizationproblem would then become the standard single doc-ument (topical) summarization.
Intuitively though,we would want to set ?c to a relatively large num-ber to exploit the citation context in our estimation,which is confirmed in our experiments.An alternative way is to simply interpolate p(w|d)and p(w|C) with a constant coefficient:p(w|?I) = (1 ?
?
)p(w|d) + ?p(w|C) (3)We will compare the two strategies in Section 5.How do we estimate p(w|C)?
Intuitively, wordsoccurring in C frequently should have high proba-bilities.
A simple way is to pool together all the sen-tences in C and use the maximum likelihood estima-tor,p(w|C) =?s?C c(w, s)?w??V?s?
?C c(w?, s?
)(4)where c(w, s) is the count of w in s.One deficiency of this simple estimate is that wetreat all the (extended) citation sentences equally.However, there are at least two reasons why we wantto assign unequal weights to different citation sen-tences: (1) A sentence closer to the citation labelshould contribute more than one far away.
(2) A sen-tence occurring in a highly authorative paper shouldcontribute more than that in a less authorative paper.To capture these two heuristics, we define a weightcoefficient ?s for a sentence s in C as follows:?s = pg(s)pr(s)where pg(s) is an authority score of the paper con-taining s and pr(s) is a proximity score that rewardsa sentence close to the citation label.For example, pg(s) can be the PageRank value(Brin and Page, 1998) of the document with s, whichmeasures the authority of the document based on acitation graph, and is computed as follows: We con-struct a directed graph from the collection of scien-tific literature with each paper as a vertex and eachcitation as a directed edge pointing from the citingpaper to the cited paper.
We can then use the stan-dard PageRank algorithm (Brin and Page, 1998) tocompute a PageRank value for each document.
Weused this approach in our experiments.We define pr(s) as pr(s) = 1?k , where k is thedistance (counted in terms of the number of sen-tences) between sentence s and the center sentenceof the window containing s; by ?center sentence?,we mean the citing sentence containing the citationlabel.
Thus the sentence with the citation label willhave a proximity of 1 (because k = 0), while thesentences away from the citation label will have adecaying weight controlled by parameter ?.With ?s, we can then use the following?weighted?
maximum likelihood estimate for theimpact language model:p(w|C) =?s?C ?sc(w, s)?w??V?s?
?C ?s?c(w?, s?
)(5)As we will show in Section 5, this weightedmaximum likelihood estimate performs better thanthe simple maximum likelihood estimate, and bothpg(s) and pr(s) are useful.8195 Experiments and Results5.1 Experiment Design5.1.1 Test set constructionBecause no existing test set is available for evalu-ating impact summarization, we opt to create a testset based on 28 years of ACM SIGIR papers (1978- 2005) available through the ACM Digital Library2and the SIGIR membership.
Leveraging the explicitcitation information provided by ACM Digital Li-brary, for each of the 1303 papers, we recorded allother papers that cited the paper and extracted thecitation context from these citing papers.
Each ci-tation context contains 5 sentences with 2 sentencesbefore and after the citing sentence.Since a low-impact paper would not be useful forevaluating impact summarization, we took all the14 papers from the SIGIR collection that have noless than 20 citations by papers in the same col-lection as candidate papers for evaluation.
An ex-pert in Information Retrieval field read each paperand its citation context, and manually created animpact-based summary by selecting all the ?impact-capturing?
sentences from the paper.
Specifically,the expert first attempted to understand the most in-fluential content of a paper by reading the citationcontexts.
The expert then read each sentence ofthe paper and made a decision whether the sentencecovers some ?influential content?
as indicated in thecitation contexts.
The sentences that were decidedas covering some influential content were then col-lected as the gold standard impact summary for thepaper.We assume that the title of a paper will alwaysbe included in the summary, so we excluded the ti-tle both when constructing the gold standard andwhen generating a summary.
The gold standardsummaries have a minimum length of 5 sentencesand a maximum length of 18 sentences; the me-dian length is 9 sentences.
These 14 impact-basedsummaries are used as gold standards for our exper-iments, based on which all summaries generated bythe system are evaluated.
This data set is available athttp://timan.cs.uiuc.edu/data/impact.html.
We mustadmit that using only 14 papers and only one expertfor evaluation is a limitation of our work.
However,2http://www.acm.org/dlgoing beyond the 14 papers would risk reducing thereliability of impact judgment due to the sparsenessof citations.
How to develop a better test collectionis an important future direction.5.1.2 Evaluation MetricsFollowing the current practice in evaluating sum-marization, particularly DUC3, we use the ROUGEevaluation package (Lin and Hovy, 2003).
AmongROUGE metrics, ROUGE-N (models n-gram co-occurrence, N = 1, 2) and ROUGE-L (modelslongest common sequence) generally perform wellin evaluating both single-document summarizationand multi-document summarization (Lin and Hovy,2003).
Since they are general evaluation measuresfor summarization, they are also applicable to eval-uating the MEAD-Doc+Cite baseline method to bedescribed below.
Thus although we evaluated ourmethods with all the metrics provided by ROUGE,we only report ROUGE-1 and ROUGE-L in this pa-per (other metrics give very similar results).5.1.3 Baseline methodsSince impact summarization has not been previ-ously studied, there is no natural baseline method tocompare with.
We thus adapt some state-of-the-artconventional summarization methods implementedin the MEAD toolkit (Radev et al, 2003)4 to obtainthree baseline methods: (1) LEAD: It simply ex-tracts sentences from the beginning of a paper, i.e.,sentences in the abstract or beginning of the intro-duction section; we include LEAD to see if such?leading sentences?
reflect the impact of a paper asauthors presumably would expect to summarize apaper?s contributions in the abstract.
(2) MEAD-Doc: It uses the single-document summarizer inMEAD to generate a summary based solely on theoriginal paper; comparison with this baseline cantell us how much better we can do than a conven-tional topic-based summarizer that does not considerthe citation context.
(3) MEAD-Doc+Cite: Herewe concatenate all the citation contexts in a paper toform a ?citation document?
and then use the MEADmultidocument summarizer to generate a summaryfrom the original paper plus all its citation docu-ments; this baseline represents a reasonable way3http://duc.nist.gov/4?http://www.summarization.com/mead/?820Sum.
Length Metric Random LEAD MEAD-Doc MEAD-Doc+Cite KL-Divergence3 ROUGE-1 0.163 0.167 0.301* 0.248 0.3233 ROUGE-L 0.144 0.158 0.265 0.217 0.2995 ROUGE-1 0.230 0.301 0.401 0.333 0.4675 ROUGE-L 0.214 0.292 0.362 0.298 0.44410 ROUGE-1 0.430 0.514 0.575 0.472 0.64910 ROUGE-L 0.396 0.494 0.535 0.428 0.62215 ROUGE-1 0.538 0.610 0.685 0.552 0.73015 ROUGE-L 0.499 0.586 0.650 0.503 0.705Table 1: Performance Comparison of Summarizersof applying an existing summarization method togenerate an impact-based summary.
Note that thismethod may extract sentences in the citation con-texts but not in the original paper.5.2 Basic ResultsWe first show some basic results of impact sum-marization in Table 1.
They are generated us-ing constant coefficient interpolation for the impactlanguage model (i.e., Equation 3) with ?
= 0.8,weighted maximum likelihood estimate for the ci-tation context model (i.e., Equation 5) with ?
= 3,and ?s = 1, 000 for candidate sentence smoothing(Equation 1).
These results are not necessarily opti-mal as will be seen when we examine parameter andmethod variations.From Table 1, we see clearly that our methodconsistently outperforms all the baselines.
Amongthe baselines, MEAD-Doc is consistently better thanboth LEAD and MEAD-Doc+Cite.
While MEAD-Doc?s outperforming LEAD is not surprising, it isa bit surprising that MEAD-Doc also outperformsMEAD-Doc+Cite as the latter uses both the cita-tion context and the original document.
One possi-ble explanation may be that MEAD is not designedfor impact summarization and it has been trappedby the distracting content in the citation context 5.Indeed, this can also explain why MEAD-Doc+Citetends to perform worse than LEAD by ROUGE-Lsince if MEAD-Doc+Cite picks up sentences fromthe citation context rather than the original papers,it would not match as well with the gold standardas LEAD which selects sentences from the origi-5One anonymous reviewer suggested an interesting im-provement to the MEAD-Doc+Cite baseline, in which wewould first extract sentences from the citation context and thenfor each extracted sentence find a similar one in the original pa-per.
Unfortunately, we did not have time to test this approachbefore the deadline for the camera-ready version of this paper.nal papers.
These results thus show that conven-tional summarization techniques are inadequate forimpact summarization, and the proposed languagemodeling methods are more effective for generatingimpact-based summaries.In Table 2, we show a sample impact-based sum-mary and the corresponding MEAD-Doc regularsummary.
We see that the regular summary tendsto have general sentences about the problem, back-ground and techniques, not very informative in con-veying specific contributions of the paper.
None ofthese sentences was selected by the human expert.
Incontrast, the sentences in the impact summary coverseveral details of the impact of the paper (i.e., spe-cific smoothing methods especially Dirichlet prior,sensitivity of performance to smoothing, and dualrole of smoothing), and sentences 4 and 6 are alsoamong the 8 sentences picked by the human expert.Interestingly, neither sentence is in the abstract ofthe original paper, suggesting a deviation of the ac-tual impact of a paper and that perceived by the au-thor(s).5.3 Component analysisWe now turn to examine the effectiveness of eachcomponent in the proposed methods and differentstrategies for estimating ?I .Effectiveness of interpolation: We hypothesizedthat we need to use both the original document andthe citation context to estimate ?I .
To test this hy-pothesis, we compare the results of using only d,only the citation context, and interpolation of themin Table 3.
We show two different strategies of inter-polation (i.e., constant coefficient with ?
= 0.8 andDirichlet with ?c = 20, 000) as described in Sec-tion 4.From Table 3, we see that both strategies of in-terpolation indeed outperform using either the origi-821Impact-based summary:1.
Figure 5: Interpolation versus backoff for Jelinek-Mercer (top), Dirichlet smoothing (middle), and absolute discounting (bottom).2.
Second, one can de-couple the two different roles of smoothing by adopting a two stage smoothing strategy in which Dirichlet smoothing isfirst applied to implement the estimation role and Jelinek-Mercer smoothing is then applied to implement the role of query modeling3.
We find that the backoff performance is more sensitive to the smoothing parameter than that of interpolation, especially in Jelinek-Mercerand Dirichlet prior.4.
We then examined three popular interpolation-based smoothing methods (Jelinek-Mercer method, Dirichlet priors, and absolute discounting),as well as their backoff versions, and evaluated them using several large and small TREC retrieval testing collections.summary 5.
By rewriting the query-likelihood retrieval model using a smoothed document language model, we derived a general retrievalformula where the smoothing of the document language model can be interpreted in terms of several heuristics used intraditional models,including TF-IDF weighting and document length normalization.6.
We find that the retrieval performance is generally sensitive to the smoothing parameters, suggesting that an understanding and appropriatesetting of smoothing parameters is very important in the language modeling approach.Regular summary (generated using MEAD-Doc):1.
Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with thatof language model estimation, which has been studied extensively in other application areas such as speech recognition.2.
The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of thequery according to the estimated language model.3.
On the one hand, theoretical studies of an underlying model have been developed; this direction is, for example, represented by the variouskinds of logic models and probabilistic models (e.g., [14, 3, 15, 22]).4.
After applying the Bayes?
formula and dropping a document-independent constant (since we are only interested in ranking documents), wehave p(d|q) ?
(q|d)p(d).5.
As discussed in [1], the righthand side of the above equation has an interesting interpretation, where, p(d) is our prior belief that d is relevantto any query and p(q|d) is the query likelihood given the document, which captures how well the document ?fits?
the particular query q.6.
The probability of an unseen word is typically taken as being proportional to the general frequency of the word, e.g., as computed using thedocument collection.Table 2: Impact-based summary vs. regular summary for the paper ?A study of smoothing methods for languagemodels applied to ad hoc information retrieval?.nal document model (p(w|d)) or the citation contextmodel (p(w|C)) alone, which confirms that both theoriginal paper and the citation context are importantfor estimating ?I .
We also see that using the citationcontext alone is better than using the original paperalone, which is expected.
Between the two strate-gies, Dirichlet dynamic coefficient is slightly betterthan constant coefficient (CC), after optimizing theinterpolation parameter for both strategy.InterpolationMeasure P (w|d) P (w|C) ConstCoef DirichletROUGE-1 0.529 0.635 0.643 0.647ROUGE-L 0.501 0.607 0.619 0.623Table 3: Effectiveness of interpolationCitation authority and proximity: These heuris-tics are very interesting to study as they are uniqueto impact summarization and not well studied in theexisting summarization work.pg(s) pr(s)=1/?kpr(s) off ?
= 2 ?
= 3 ?
= 4Off 0.685 0.711 0.714 0.700On 0.708 0.712 0.706 0.703Table 4: Authority (pg(s)) and proximity (pr(s))In Table 4, we show the ROUGE-L values for var-ious combinations of these two heuristics (summarylength is 15).
We turn off either pg(s) or pr(s) bysetting it to a constant; when both are turned off, wehave the unweighted MLE of p(w|C) (Equation 4).Clearly, using weighted MLE with any of the twoheuristics is better than the unweighted MLE, indi-cating that both heuristics are effective.
However,combining the two heuristics does not always im-prove over using a single one.
Since intuitively thesetwo heuristics are orthogonal, this may suggest thatour way of combining the two scores (i.e., taking aproduct of them) may not be optimal; further studyis needed to better understand this.
The ROUGE-1results are similar.Tuning of other parameters: There are three otherparameters which need to be tuned: (1) ?s for can-didate sentence smoothing (Equation 1); (2) ?c inDirichlet interpolation for impact model estimation(Equation 2); and (3) ?
in constant coefficient inter-polation (Equation 3).
We have examined the sen-sitivity of performance to these parameters.
In gen-eral, for a wide range of values of these parameters,the performance is relatively stable and near opti-mal.
Specifically, the performance is near optimal as822long as ?s and ?c are sufficiently large (?s ?
1000,?c ?
20, 000), and the interpolation parameter ?
isbetween 0.4 and 0.9.6 Related WorkGeneral text summarization, including single docu-ment summarization (Luhn, 1958; Goldstein et al,1999) and multi-document summarization (Kraaij etal., 2001; Radev et al, 2003) has been well stud-ied; our work is under the framework of extractivesummarization (Luhn, 1958; McKeown and Radev,1995; Goldstein et al, 1999; Kraaij et al, 2001),but our problem formulation differs from any exist-ing formulation of the summarization problem.
Itdiffers from regular single-document summarizationbecause we utilize extra information (i.e.
citationcontexts) to summarize the impact of a paper.
It alsodiffers from regular multi-document summarizationbecause the roles of original documents and cita-tion contexts are not equivalent.
Specifically, cita-tion contexts serve as an indicator of the impact ofthe paper, but the summary is generated by extract-ing the sentences from the original paper.Technical paper summarization has also beenstudied (Paice, 1981; Paice and Jones, 1993; Sag-gion and Lapalme, 2002; Teufel and Moens, 2002),but the previous work did not explore citation con-text to emphasize the impact of papers.Citation context has been explored in severalstudies (Nakov et al, 2004; Ritchie et al, 2006;Schwartz et al, 2007; Siddharthan and Teufel,2007).
However, none of the previous studies hasused citation context in the same way as we did,though the potential of directly using citation sen-tences (called citances) to summarize a paper waspointed out in (Nakov et al, 2004).Recently, people have explored various types ofauxiliary knowledge such as hyperlinks (Delort etal., 2003) and clickthrough data (Sun et al, 2005), tosummarize a webpage; such work is related to oursas anchor text is similar to citation context, but it isbased on a standard formulation of multi-documentsummarization and would contain only sentencesfrom anchor text.Our work is also related to work on using lan-guage models for retrieval (Ponte and Croft, 1998;Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001)and summarization (Kraaij et al, 2001).
However,we do not have an explicit query and constructingthe impact model is a novel exploration.
We alsoproposed new language models to capture the im-pact.7 ConclusionsWe have defined and studied the novel problem ofsummarizing the impact of a research paper.
We castthe problem as an impact sentence retrieval problem,and proposed new language models to model the im-pact of a paper based on both the original contentof the paper and its citation contexts in a literaturecollection with consideration of citation autority andproximity.To evaluate impact summarization, we created atest set based on ACM SIGIR papers.
Experimentresults on this test set show that the proposed im-pact summarization methods are effective and out-perform several baselines that represent the existingsummarization methods.An important future work is to construct largertest sets (e.g., of biomedical literature) to facilitateevaluation of impact summarization.
Our formula-tion of the impact summarization problem can befurther improved by going beyond sentence retrievaland considering factors such as redundancy and co-herency to better organize an impact summary.
Fi-nally, automatically generating impact-based sum-maries can not only help users access and digestinfluential research publications, but also facilitateother literature mining tasks such as milestone min-ing and research trend monitoring.
It would be in-teresting to explore all these applications.AcknowledgmentsWe are grateful to the anonymous reviewers for theirconstructive comments.
This work is in part sup-ported by a Yahoo!
Graduate Fellowship and NSFgrants under award numbers 0713571, 0347933, and0428472.ReferencesSergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual web search engine.
InProceedings of the Seventh International Conferenceon World Wide Web, pages 107?117.823J.-Y.
Delort, B. Bouchon-Meunier, and M. Rifqi.
2003.Enhanced web document summarization using hyper-links.
In Proceedings of the Fourteenth ACM Confer-ence on Hypertext and Hypermedia, pages 208?215.C.
Lee Giles, Kurt D. Bollacker, and Steve Lawrence.1998.
Citeseer: an automatic citation indexing sys-tem.
In Proceedings of the Third ACM Conference onDigital Libraries, pages 89?98.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, andJaime Carbonell.
1999.
Summarizing text documents:sentence selection and evaluation metrics.
In Proceed-ings of ACM SIGIR 99, pages 121?128.Nancy R. Kaplan and Michael L. Nelson.
2000.
Deter-mining the publication impact of a digital library.
J.Am.
Soc.
Inf.
Sci., 51(4):324?339.W.
Kraaij, M. Spitters, and M. van der Heijden.
2001.Combining a mixture language model and naive bayesfor multi-document summarisation.
In Proceedings ofthe DUC2001 workshop.John Lafferty and Chengxiang Zhai.
2001.
Documentlanguage models, query models, and risk minimiza-tion for information retrieval.
In Proceedings of ACMSIGIR 2001, pages 111?119.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy, pages 71?78.H.
P. Luhn.
1958.
The automatic creation of literatureabstracts.
IBM Journal of Research and Development,2(2):159?165.D.
MacKay and L. Peto.
1995.
A hierarchical Dirich-let language model.
Natural Language Engineering,1(3):289?307.Kathleen McKeown and Dragomir R. Radev.
1995.
Gen-erating summaries of multiple news articles.
In Pro-ceedings of the 18th Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 74?82.P.
Nakov, A. Schwartz, and M. Hearst.
2004.
Citances:Citation sentences for semantic analysis of biosciencetext.
In Proceedings of ACM SIGIR?04 Workshop onSearch and Discovery in Bioinformatics.Chris D. Paice and Paul A. Jones.
1993.
The identifi-cation of important concepts in highly structured tech-nical papers.
In Proceedings of the 16th Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 69?78.C.
D. Paice.
1981.
The automatic generation of literatureabstracts: an approach based on the identification ofself-indicating phrases.
In Proceedings of the 3rd An-nual ACM Conference on Research and Developmentin Information Retrieval, pages 172?191.Jay M. Ponte and W. Bruce Croft.
1998.
A languagemodeling approach to information retrieval.
In Pro-ceedings of the 21st Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 275?281.Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-own.
2002.
Introduction to the special issue on sum-marization.
Comput.
Linguist., 28(4):399?408.Dragomir R. Radev, Simone Teufel, Horacio Saggion,Wai Lam, John Blitzer, Hong Qi, Arda Celebi, DanyuLiu, and Elliott Drabek.
2003.
Evaluation challengesin large-scale document summarization: the meadproject.
In Proceedings of the 41st Annual Meetingon Association for Computational Linguistics, pages375?382.A.
Ritchie, S. Teufel, and S. Robertson.
2006.
Creatinga test collection for citation-based ir experiments.
InProceedings of the HLT-NAACL 2006, pages 391?398.S.
Robertson and K. Sparck Jones.
1976.
Relevanceweighting of search terms.
Journal of the AmericanSociety for Information Science, 27:129?146.Hpracop Saggion and Guy Lapalme.
2002.
Generatingindicative-informative summaries with sumUM.
Com-putational Linguistics, 28(4):497?526.A.
S. Schwartz, A. Divoli, and M. A. Hearst.
2007.
Mul-tiple alignment of citation sentences with conditionalrandom fields and posterior decoding.
In Proceedingsof the 2007 EMNLP-CoNLL, pages 847?857.A.
Siddharthan and S. Teufel.
2007.
Whose idea wasthis, and why does it matter?
attributing scientificwork to citations.
In Proceedings of NAACL/HLT-07,pages 316?323.Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,Yuchang Lu, and Zheng Chen.
2005.
Web-page sum-marization using clickthrough data.
In Proceedingsof the 28th Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, pages 194?201.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles: experiments with relevance andrhetorical status.
Comput.
Linguist., 28(4):409?445.ChengXiang Zhai and John Lafferty.
2001a.
Model-based feedback in the language modeling approachto information retrieval.
In Proceedings of the TenthInternational Conference on Information and Knowl-edge Management (CIKM 2001), pages 403?410.Chengxiang Zhai and John Lafferty.
2001b.
A studyof smoothing methods for language models applied toad hoc information retrieval.
In Proceedings of the24th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 334?342.824
