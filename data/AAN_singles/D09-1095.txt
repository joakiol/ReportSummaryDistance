Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 910?918,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPCombining Collocations, Lexical and Encyclopedic Knowledgefor Metonymy ResolutionVivi Nastase and Michael StrubeEML Research gGmbHHeidelberg, Germanyhttp://www.eml-research.de/nlpAbstractThis paper presents a supervised methodfor resolving metonymies.
We enhancea commonly used feature set with fea-tures extracted based on collocation in-formation from corpora, generalized us-ing lexical and encyclopedic knowledgeto determine the preferred sense of thepotentially metonymic word using meth-ods from unsupervised word sense disam-biguation.
The methodology developedaddresses one issue related to metonymyresolution ?
the influence of local context.The method developed is applied to themetonymy resolution task from SemEval2007.
The results obtained, higher for thecountries subtask, on a par for the compa-nies subtask ?
compared to participatingsystems ?
confirm that lexical, encyclo-pedic and collocation information can besuccessfully combined for metonymy res-olution.1 IntroductionMetonymies are a pervasive phenomenon inlanguage.
They occur because in communicating,we use words as pointers to a larger body ofknowledge, that encompasses various facets of theconcept evoked by a given word.A listener need not understand the cello tobe moved by its playing, just as it is unnecessaryfor a rider to understand technical jargon; allthat matters is sensation, and here the Kawasakiexcels.
The cockpit is sensibly designed, with anarrow front seat portion ...Kawasaki is a company, it has an organization, fa-cilities, employees, it makes specific products.
Inthe context above, the company name stands infor its products ?
motorcycles.
Motorcycles haveparts, the cockpit and front seat are some of them,and this provides the discourse links between thetwo sentences.
Constraints on the interpretationof a word w in context comes both from the localand global context, and are applied to the infor-mation/knowledge evoked by w. The local con-straints come from the words with which w is(grammatically) related to.
The global constraintscome from the domain/topic of the text, discourserelations that span across sentences.Metonymic words have a rather small num-ber of possible interpretations (also called read-ings) which occur frequently (Markert and Nissim,2002).
Idiosyncratic interpretations are also pos-sible, but very rare.
One can view the possibleinterpretations of a potentially metonymic word(PMW) as corresponding to the word?s possiblesenses (Nissim and Markert, 2003), bringing thetask close to word sense disambiguation.The approach to metonymy resolution pre-sented here is supervised, with unsupervised fea-ture enrichment.
We apply techniques inspired byunsupervised word sense disambiguation, whichallow us to go beyond the annotated data providedin training, and quantify the restrictions imposedon the interpretation of a PMW by its grammat-ically related neighbours through collocation in-formation extracted from corpora.
The only anno-tation required for the corpora are automaticallyinduced part-of-speech tags from which we ob-tain grammatical relations through regular expres-sion matching over sequences of parts-of-speech.Collocation information is combined with lexicalresources ?
WordNet ?
and encyclopedic knowl-edge extracted from Wikipedia to help us gener-alize the collocations found to determine higherlevel constraints on a word?s grammatical collo-cates.
In the example above, Kawasaki is gram-matically related to the verb excel ?
it is its sub-ject.
To determine the most likely interpretationof Kawasaki given that it is in the subject relationwith excel we look at all the nouns in the corpora910that appear as this verb?s subjects, and estimatefrom this the preferences excel has for its subjects.Let us say the corpus contains the following col-locations in subject position (with frequency in-formation in parentheses): player (4), musician(50), car (30), computer (12), camera (40), driver(55), bike (20) ....
The knowledge resources ?WordNet, isa relations extracted from Wikipedia?
will help generalize these collocations: player,musician, driver to person and car, computer,camera, bike to artifact.
This together withfrequency of occurrence are used to estimate theprobability that the verb excel takes a person orartifact-type subject.
These are excel?s se-lectional preferences towards certain collocates,and will help determine which possible interpre-tation for the PMW Kawasaki is appropriate inthis context ?
organization-for-peopleor organization-for-product.The paper continues with related work in Sec-tion 2 and the description of the data in Section 3.The representation used is introduced in Section4.
The results and the discussion are presented inSection 5.
The paper wraps up with conclusionsand future work.2 Related WorkAnalysis of metonymies as a linguistic phe-nomenon dates back at least to the 1930s (Stern,1931), and are increasingly recognized as an im-portant phenomenon to tackle in the interest ofhigher level language processing tasks, such asanaphora resolution (Harabagiu, 1998; Markertand Hahn, 2002), question answering (Stallard,1993) or machine translation (Kamei and Wakao,1992).Until the early 90s, the main view aboutmetonymies was that they violate semantic con-straints in their immediate context.
To resolvemetonymies then amounts to detecting violatedconstraints, usually from those imposed by theverbs on their arguments (Pustejovsky, 1991;Hobbs et al, 1993; Fass, 1991).
Markert andHahn (2002) showed that this approach missesmetonymies which do not violate selectional re-strictions.
In this case referential cohesion re-lations may indicate that the literal reading isnot appropriate and give clues about the intendedmetonymic interpretation.Markert and Nissim (2003) have combinedobservations from the linguistic analysis ofmetonymies with results of corpus studies.
Lin-guistic research has postulated that (i) conven-tional metonymic readings are very systematic;(ii) unconventional metonymies can be created onthe fly and their interpretation is context depen-dent; (iii) metonymies are frequent.
The factthat most metonymic interpretations are system-atic and correspond to a small set of possible read-ings allow the metonymy resolution to be mod-elled as a classifier learning task.
Markert and Nis-sim (2002) and Nissim and Markert (2003) haveshown that conventional metonymies can be effec-tively resolved using a supervised machine learn-ing approach.
Moreover, grammatically relatedwords are crucial in determining the interpretationof a PMW.
The shortcoming is that manually an-notated data is in short supply, and the approachsuffers from data sparseness.
To address this prob-lem, Nissim and Markert (2003) proposed a wordsimilarity-based method.
They use Lin?s thesaurus(Lin, 1998) to determine how close two lexicalheads are, and use this instead of the more re-strictive identity constraint when comparing twoinstances.
This technique is complex, requiringsmoothing, multiple iterations over the thesaurusand hybrid methods to allow a back-off to gram-matical roles.The supervised approach to resolvingmetonymies was encouraged by the metonymyresolution task at the semantic evaluation exerciseSemEval 2007 (Markert and Nissim, 2007).
Theparticipating systems in this task were varied.Most of them (four out of five) have used super-vised machine learning techniques.
The systemsthat beat the baseline used either the grammaticalannotations provided by the organizers (Farkaset al, 2007; Nicolae et al, 2007), or a robustand deep (not freely available) parser (Brun etal., 2007).
These systems represented instancesin a manner similar to (Nissim and Markert,2005).
They used additional manually builtresources ?
WordNet, FrameNet, Levin?s verbclasses, manually built lists of ?trigger?
words?
to generalize the existing features.
Brun etal.
(2007) also used the British National Corpus(BNC) for computing the distance between wordsbased on their syntactic distribution.While lexical resources and corpora are usedto estimate word similarity, all these systems relyexclusively on the data provided by the organiz-ers ?
instance representation captures only infor-mation that can be derived from or between thedata points provided.
The approach presented heregoes beyond the given data, and induces from cor-pora measures that allow the system to determine911what are the preferences of the words surround-ing a PMW towards each of PMW?s possible read-ings.
The technique employed is adapted fromunsupervised word sense disambiguation (WSD).In short, we use the local grammatical contextas it is commonly used in WSD approaches, toguide the system in choosing the reading that fitsbest.
The benefits of using grammatical informa-tion for automatic WSD were first explored byYarowsky (1995) and Resnik (1996) in unsuper-vised approaches to disambiguating single wordsin context.
The method described here uses au-tomatically induced selectional preferences, com-puted from sense-untagged data, similar to Nas-tase (2008).3 DataWe work with the data from the metonymy reso-lution task at SemEval 2007 (Markert and Nissim,2007), generated based on a scheme developed byMarkert and Nissim (2003).The metonymy resolution task at SemEval 2007consisted of two subtasks ?
one for resolvingcountry names, the other for companies.
For eachsubtask there is a training and a test portion.
Fig-ure 1 shows the text fragment for one sample,and Table 1 the data statistics.
The reading col-umn shows the possible interpretations of a PMWfor countries and companies respectively.
For ex-ample, org-for-product would be the inter-pretation of the PMW Kawasaki in the exampleshown in the introduction.Occurrences of country and company nameswere annotated with a small number of possi-ble readings, as shown in Table 1.
This reflectsprevious analyses of the metonymy phenomenon,which showed that there is a rather small numberof possible interpretations that appear more fre-quently (Markert and Nissim, 2002).
Special in-terpretations are very rarely encountered.Within the framework of the SemEval task,metonymy resolution is evaluated on thegiven test data, on three levels of granular-ity: coarse ?
distinguish between literal andnon-literal readings; medium ?
distinguishbetween literal, mixed and non-literalreadings; fine ?
identify the specific reading of thetarget word/words (potentially metonymic word -PMW).4 RepresentationThe method presented in this paper is a supervisedlearning method, along the same general lines asreading train testlocations 925 908literal 737 721mixed 15 20othermet 9 11obj-for-name 0 4obj-for-representation 0 0place-for-people 161 141place-for-event 3 10place-for-product 0 1organizations 1090 842literal 690 520mixed 59 60othermet 14 8obj-for-name 8 6obj-for-representation 1 0org-for-members 220 161org-for-event 2 1org-for-product 74 67org-for-facility 15 16org-for-index 7 3Table 1: Reading distributionsthe systems which participated in the SemEvalcompetition.
As such, it represents each PMW inthe data through features that describe its contextand some semantic characteristics.
The minimumset of necessary features is taken to be that pre-sented by Nissim and Markert (2005), and provedto be effective in solving metonymies.
Theseare the M&N features (Markert and Nissim fea-tures).
We expand on these features and estimatepreferences from words in a PMW?s context to-wards specific PMW interpretations.
These con-stitute the selectional preference features.
Finally,Wikipedia is a source of facts which can be usedto derive information that can bias the decision to-wards certain interpretations for a PMW.
Each ofthese features are described in more detail in thefollowing subsections.4.1 M&N featuresThe features used by Nissim and Markert (2005)are:?
grammatical role of PMW (subj, obj, ...);?
lemmatized head/modifier of PMW (an-nounce, say, ...);?
determiner of PMW (def, indef, bare,demonst, other, ...);912XML tagged text<sample id=?samp114?><bnc:title> Computergram international</bnc:title><par>LITTLE FEAR OF MICHELANGELOThe computer industry equivalent of ?Smallearthquake in Chile?
...The Michelangelo computer virus that receivedworldwide attention last year is expected to causeeven fewer problems this Saturday than it didwhen it struck last year, a team of <annot><orgreading=?literal?> IBM </org></annot> re-searchers said.</par></sample>Grammatical annotationsSampleID|Lemma|PMW|GrRole|Readingsamp114|researcher|IBM|premod|literalsamp4|be|Williams Holdings|subj|literalsamp5|parent|Fujitsu Ltd|app|mixedsamp5|have|Fujitsu Ltd|subj|mixedsamp5|keep|Fujitsu Ltd|subj|mixedsamp8|against|IBM|pp|literalPOS tags<bnc:s id=?samp114-bncCNJ-s341?> ...<bnc:w id=?samp114-bncCNJ-s343-w29?bnc:type=?NN0?> team </bnc:w> <bnc:wid=?samp114-bncCNJ-s343-w30?bnc:type=?PRF?> of </bnc:w> <annot> <orgreading=?literal?> <bnc:w possmeto=?yes?id=?samp114-bncCNJ-s343-w31?bnc:type=?NP0?> IBM </bnc:w> </org></annot> <bnc:wid=?samp114-bncCNJ-s343-w32?bnc:type=?NN2?> researchers </bnc:w> ...Figure 1: Sample annotation?
grammatical number of PMW (sg, pl);?
number of grammatical roles in which thePMW appears in its current context;?
number of words in PMW;All these features can be extracted from thegrammatically annotated and POS tagged dataprovided by the organizers.4.2 Selectional preference featuresThe grammatical relations and the connectedwords are important to describe the local contextof the target PMW.
Because of the limited amountof annotated data (a few thousand instances), lem-mas of PMW?s grammatically related words willmake for very sparse data that a machine learn-ing system would not be able to generalize over.Nissim and Markert (2003) and the teams partici-pating in the metonymy resolution task have thensupplemented their systems with Lin?s thesaurus,WordNet, Beth Levin?s verb groups, FrameNet in-formation, or manually designed lists of words togeneralize the grammatically related words andthus find shared characteristics across instances ofmetonymies in text.The notion of selectional restrictions used inmetonymy resolution ?
meaning the restrictionsimposed on the interpretation of a PMW by itscontext ?
is similar to the notion of selectionalpreferences from word sense disambiguation ?meaning the preferences of a word for the sensesof the words in its context.
We import this no-tion, and compute selectional preferences for thewords in a PMW?s (grammatical) neighbourhood,and allow them to influence the chosen reading forthe PMW.
Applying methods from unsupervisedWSD allow us to estimate such preferences from(sense/metonymy) untagged corpora.A potentially metonymic word (or phrase) hasa small number of possible readings.
These canbe viewed as possible senses, and the task is tochoose the one that fits best in the given context.The preference for each possible sense can bedetermined based on the PMW?s grammaticallyrelated words.
To estimate these sense preferenceswe use grammatical collocations extracted fromthe British National Corpus (BNC), detectedusing regular expression matching over sequencesof POS using the Word Sketch Engine (Kilgarriffet al, 2004).
The scores are computed followinga technique similar to Nastase (2008), which isillustrated using the following example:The Kawasaki drives well, steers brilliantlyboth under power and in tight corners ...The PMW Kawasaki is involved in the follow-ing grammatical relations in the previous sentence:(drive,subject,Kawasaki)(steer,subject,Kawasaki)913SampleID Lemma PMW GrRole Reading act animal artifact ... person ...samp190 say Sun subj org-for-members 0.00056 0.01171 0.01958 ... 0.61422 ...samp190 claim Sun subj org-for-members 0.00198 0.00099 0.00893 ... 0.50211 ...Table 2: Grammatical annotation file enhanced with selectional preference estimatesThe BNC provides the collocations(drive,subject,X) and (steer,subject,Y), to de-termine what kind of subject drive and steerprefer, in ?word-POS:frequency?
format:drive subject chauffeur-n:12, engine-n:30, car-n:62, taxi-n:13,motorist-n:10, disk-n:15,truck-n:11, man-n:75, ...steer subject power-n:6, car-n:3, sport-n:2, firm-n:2, boy-n:2,government-n:2, man-n:2,people-n:2 ...The target whose interpretation must be deter-mined is Kawasaki.
If for a potentially metonymicword representing a company name, there are thefollowing possible interpretations: company,member/person, product/artifact,facility, name, we compute the preferencefor each of these interpretations based on theextracted collocations.
For the verb drive forexample, the collocations engine, car, taxi, truckare all artifacts (according to WordNet), andthus vote for the product/artifact reading,while chauffeur, motorist, man are all person,and vote for the member/person reading.Preferences from different grammatical relationfor the same PMW are summed.Formally, we choose the PMWs?
?senses?
?a set of words which are close to the possiblereadings of metonymic words in the data.
Inthis work, these senses are the WordNet 3.0supersenses:S = { act, animal, artifact,attribute, body, cognition,communication, event, feeling,food, group, location, motive,object, person, phenomenon,plant, possession, process,quantity, relation, shape, state,substance, time }.Because none of these can be seen as a sensefor ?company?, the list is supplemented withcompany and organization.
Granted, thereis no 1:1 mapping from these supersenses to PMWreadings, but find such a strict correspondence isnot necessary because the context preferences foreach of these senses are used as features, and themapping to PMW readings is found through a su-pervised learned model.To compute the preference of a word w inthe grammatical context of a PMW t (the target)towards each of t?s possible senses, we considereach relation (w,R, t), where R is the grammati-cal relation.
The set C of word collocations areextracted from the BNCC = {(w,R,wj: fj)|(w,R,wj) ?
BNC,fjis the frequency of occurrence}and used to compute a preference score Psifor each sense si?
S:Psi=?(w,R,wi,j:fi,j)?Csifi,j?
(w,R,wj:fj)?CfjwhereCsi= {(w,R,wj: fj)|(w,R,wj: fj) ?
C;supersense(wj, si) ?
isa(wj, si)}.supersense(wj, si) is true if siis a super-sense of one of wj?s senses;isa(wj, si) is true if siis a hypernym of oneof wj?s senses in WordNet, or is a fact extractedfrom Wikipedia.To determine the supersense and isa relation weuse WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia1 (Nastase and Strube,2008).
The collocations extracted from BNC con-tain numerous named entities, most of which arenot part of WordNet.
If an isa relation be-tween a collocate from the corpus wjand a pos-sible sense of a PMW sicannot be established us-ing supersense information (for the supersenses)or through transitive closure in the hypernym-hyponym hierarchy in WordNet (for company1http://www/eml-research.de/nlp/download/wikirelations.php914and organization) for any sense of wj, it istried against the Wikipedia-based links.This process transforms the grammatical anno-tation file and enhances it with the collocation es-timates, as shown in Table 2 (compare this with asample of the original file presented in Figure 1).4.3 Product and event featuresFarkas et al (2007) observed that using the PMWsthemselves as features leads to improvement ondetermining the reading for organization names,and postulate that this is because some companynames are more likely to be used in a metonymicway.
This is often the case with companies thatmake products which are commonly used (cars,for example).Brun et al (2007) note that certain locations,such as Vietnam, are more likely to be used withan event reading than others locations.
Generally,locations strongly associated with events tend tobe used to refer to the event, and more often havea place-for-event interpretation rather thana literal one.These two observations have lead us to mine forthese pieces of information in the Wikipedia rela-tions, and to add two more features for a targetPMW:has-product will take a value of 1 if any of thePMW?s hypernyms (according to the isa re-lations extracted from Wikipedia) containsthe string manufacturer, will have the value0 otherwise;has-event will have the value 1 if any of thePMW?s hypernyms refers to an event (move-ments/operations/riots), and value 0 other-wise.4.4 Data representationAs mentioned before, the representation built canbe seen as consisting of roughly three subsets offeatures:?
the M&N features proposed by Nissim andMarkert (2005).
To combine the grammati-cal information from all relations, we trans-form the grammatical relations into features(as opposed to values).
For a relation subjectfor example, we generate a binary subjectfeature that indicates whether for a giventarget this grammatical relation is filled ornot, and a subject lemma feature , whosevalue is the lemma of the grammatically re-lated word.?
the selectional preference scores.
Each ofthese features corresponds to one of the ele-ments of S, presented above.
These featurescombine the selectional preferences of all thegrammatical relations for one target PMW.?
product and event information fromWikipedia ?
has-product and has-event.The grammatical annotation file consists of oneentry for each grammatical relation in which aPMW appears.
For the final representation, in-formation about all relations of a given PMW iscompressed into one instance.
Because the ba-sic features were binarized, and instead of havingone grammatical role feature now each possiblegrammatical relation has its own feature, combin-ing several entries for one PMW is easy, as it onlyimplies setting the correct value for the grammati-cal relations that are valid in the PMWs context.The final representation consists of 63 features+ class feature for the subset for company PMWs,59 features + class feature for the subset contain-ing countries PMWs.
The sample ID and thePMW itself were not part of this representation.5 ResultsThe models for determining a PMW?s correct in-terpretation are learned on the training data pro-vided, and evaluated on the test portion, usingthe answer keys and evaluation script providedwith the data.
For learning the models we useWeka (Witten and Frank, 2005), and select thefinal learning algorithms based on 10-fold cross-validation on the training data.
We have settled onsupport vector machines (SMO in Weka), and weuse the learner?s default settings.Tables 3 and Table 4 show the results obtained,and the baseline and the best results from the Sem-Eval task for comparison (Markert and Nissim,2007).
The baseline in Table 3 corresponds toclassifying everything as the most frequent class?
literal interpretation.
The M&N feat.
andM&N feat.bin.
correspond to datasets that con-tain only the M&N features and the binarizedversions of these features, respectively.
SemEvalbest gives the best results obtained on each taskin the SemEval 2007 task (Markert and Nissim,2007).
SMOwikiare the results obtained with thecomplete feature set described in Section 4, andSMOSPare the results obtained when only thenew features are used ?
only selectional prefer-ence, has-product and has-event features (none of915task ?
method ?
baseline SemEval best SMOwikiSMOSPM&Nfeat.
M&Nfeat.bin.LOCATION-COARSE 79.4 85.2 86.1 82.8 79.4 83.4LOCATION-MEDIUM 79.4 84.8 85.9 82.6 79.4 82.3LOCATION-FINE 79.4 84.1 85.0 82.0 79.4 81.3ORGANIZATION-COARSE 61.8 76.7 74.9 66.6 73.8 74.0ORGANIZATION-MEDIUM 61.8 73.3 72.4 65.0 69.8 69.4ORGANIZATION-FINE 61.8 72.8 71.0 64.7 68.4 68.5Table 3: Accuracy scorestask ?
method ?
base max SMOwikiSMOLOCATION-COARSEliteral 79.4 91.2 91.6 91.6non-literal 20.6 57.6 59.1 58.8LOCATION-MEDIUMliteral 79.4 91.2 91.6 91.6metonymic 18.4 58.0 61.5 61.5mixed 2.2 8.3 16 8.7LOCATION-FINEliteral 79.4 91.2 91.6 91.6place-for-people 15.5 58.9 61.7 61.7place-for-event 1.1 16.7 0 0place-for-product 1.1 0 0 0obj-for-name 0.4 66.7 0 0obj-for-rep 0 0 0 0othermet 1.2 0 0 0mixed 2.2 8.3 16 8.7ORGANIZATION-COARSEliteral 61.8 82.5 81.4 81.2non-literal 38.2 65.2 61.6 60.7ORGANIZATION-MEDIUMliteral 61.8 82.5 81.4 81.2metonymic 31.0 60.4 58.7 58.1mixed 7.2 30.8 26.8 28.9ORGANIZATION-FINEliteral 61.8 82.6 81.4 81.2org-for-members 19.1 63.0 59.7 59.2org-for-event 0.1 0 0 0org-for-product 8.0 50.0 44.4 44org-for-facility 2.0 22.2 36.3 38.1org-for-index 0.3 0 0 0org-for-name 0.7 80.0 58.8 58.8org-for-rep 0 0 0 0othermet 1.0 0 0 0mixed 7.2 34.3 27.1 29.3Table 4: Detailed F-scoresthe M&N features).
The baseline for detailed read-ing results in Table 4 reflects the distribution ofthe classes in the test file.
The max column showsthe best performance for each task in the SemEval2007 competition (Markert and Nissim, 2007).The SMO column shows the results of learningwhen Wikipedia information is not used to com-pute the values of the collocation, has-product andhas-event features.Nissim and Markert (2003) have shown thatgrammatical roles are very strong features.
Exper-iments on the data represented exclusively throughgrammatical role features confirm this observa-tion, as the results obtained using only the syn-tactic features (no lexical head information) givethe same results as the M&N feat.bin.
which doesinclude lexical information.On the location metonymies, the current ap-proach performs better on all evaluation types(coarse, medium, fine) by 0.9, 1.1 and 0.9% pointsrespectively.
The improvement comes from rec-ognizing better the metonymic readings, as it isapparent from the detailed F-score results in Ta-ble 4.
For the coarse readings, the F-score forthe non-literal reading is 1.5% points higherthan the best performance at SemEval, and 2.5%and 7.7% points respectively for the metonymicand mixed readings for the medium and finecoarseness.
It is interesting that the learning isquite successful even when only selectional pref-erence and Wikipedia-based has-product and has-event features are used ?
the SMOSPcolumn inTable 3.
The grammatical role and the relatedlemma were used to derive these collocation fea-tures, but they do not appear as such in the repre-sentation used for this batch of experiments.For company metonymies the current approachdoes not perform better than the state-of-the-art.For these metonymies the syntactic information isnot as useful.
This is evidenced by the lower per-formance of the classifier that uses only syntacticinformation (column M&N feat.bin.
in Table 3),despite the fact that the training dataset for com-916panies is larger than the one for countries.
Thisobservation is further supported by the low resultswhen using only selectional preference features.It indicates that for company metonymies the lo-cal context does not provide as strong clues as itdoes for locations.
For such PMWs we shouldexplore the larger context.
We have made a startwith the Wikipedia-based features built followingthe observation about companies and their prod-ucts made by Farkas et al (2007) and Brun etal.
(2007).
In future work we plan to analysethis matter further, and find a method to derivemore such features, and without manually pro-vided clues (such as manufacturer or riots).Wikipedia derived information does not con-tribute very much, but as expected it is helpfulto identify other classes than the literal one.It is helpful to detect the mixed class ?
16%F-score when using Wikipedia information com-pared to 8.7% for the countries data when we esti-mate preferences using only WordNet.
It also in-creases the performance on the non-literal,metonymic and org-for-members classesin coarse, medium and fine classification re-spectively for both countries and companies.There is a small improvement for recognizing theorg-for-product reading for organizationswhen using Wikipedia-based features.
It is an in-dication that the has-product feature is useful.
Wecannot draw conclusions about the has-event fea-ture, as there are only 3 training instances for theplace-for-event reading.
The results are en-couraging, as we have just scraped the surface ofthe information that Wikipedia can provide.The corpus derived selectional preferences per-form very well, especially for determining thereading of locations.
Analysis of the data andthe features gives some indication as to why thishappens: in the grammatical annotations provided,when the PMW is a prepositional complement orhas a prepositional complement, the grammati-cally related word is a preposition.
We extract onlygrammatical collocations for open-class words, re-stricted by the grammatical relation of interest,so we do not extract collocations for preposi-tions.
Location prepositions (in, at, from) areless ambiguous than others (e.g.
for), which aremore common for the organization data.
We haveattempted to bypass this problem by generatingparses using the dependency output of the Stan-ford Parser (de Marneffe et al, 2006), and bypass-ing the preposition ?
incorporate it in the gram-matical role (pp in, for example), and using aslemma the head of the prepositional complementor the constituent which dominates the preposi-tional phrase, depending on the position of thePMW.
Now we can use the grammatical relationand the associated open-class word to look for col-locations.
This approach did not lead to good re-sults, because the quality of the automatic parsesis far from the manually provided information.6 ConclusionsWe have explored the use of selectional preferencescores derived from a sense untagged corpus as lo-cal constrains for determining the interpretation ofpotentially metonymic words.
Such methods werepreviously successfully used for word sense dis-ambiguation, and transfer nicely to the metonymyresolution task.
Adding encyclopedic knowledgeto the mix improved the results further, by fillingin gaps for WordNet, and extracting informationparticular to PMW.
We plan to expand on this, andfind methods to extract more such features auto-matically, without manually provided clues.For a more comprehensive treatment ofmetonymies one must take into consideration notonly local context but also discourse relations.A possible avenue of research is to build uponcoreference resolution systems, and use thementions they detect and link to each other in amanner similar to using grammatical informationand grammatically related words to determineconstraints from a larger context.
Determiningthe link between two mentions in a text can takeadvantage of encyclopedic knowledge, and thesystem?s ability to infer the connection betweenthe mentions.There is much work on unsupervised wordsense disambiguation.
Working with untaggeddata gives a system access to a much larger in-formation base.
Since selectional preferences ac-quired from sense-untagged corpora have workedwell for the metonymy resolution task, we plan topush further towards unsupervised metonymy res-olution, putting to use the lessons learned from un-supervised WSD.AcknowledgementsWe thank the Klaus Tschira Foundation, Heidel-berg, for financial support, and the organizers ofthe SemEval-2007 Task #8 Metonymy Resolution?
Katja Markert and Malvina Nissim ?
for makingthe annotated data freely available.917ReferencesCaroline Brun, Maud Ehrmann, and GuillaumeJacquet.
2007.
XRCE-M: A hybrid system fornamed entity metonymy resolution.
In Proceedingsof the 4th International Workshop on Semantic Eval-uations (SemEval-1), Prague, Czech Republic, 23?24 June 2007, pages 488?491.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation, Genoa, Italy,22?28 May 2006, pages 449?454.Richard Farkas, Eszter Simon, Gyorgy Szarvas, andDaniel Varga.
2007.
GYDER: Maxent metonymyresolution.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations (SemEval-1),Prague, Czech Republic, 23?24 June 2007, pages161?164.Dan C. Fass.
1991.
met?
: A method for discriminatingmetonomy and metaphor by computer.
Computa-tional Linguistics, 17(1):49?90.Sanda M. Harabagiu.
1998.
Deriving metonymic coer-cions from WordNet.
In In Workshop on the Usageof WordNet in Natural Language Processing Sys-tems, Montreal, Canada, August 16, 1998, pages142?148.Jerry Hobbs, Mark Stickel, Douglas Appelt, and PaulMartin.
1993.
Interpretation as abduction.
ArtificialIntelligence, 63(1-2):69?142.Shin-ichiro Kamei and Takahiro Wakao.
1992.Metonymy: Reassessment, survey of acceptability,and its treatment in a machine translation system.In Proceedings of the 30th Annual Meeting of theAssociation for Computational Linguistics, Newark,Del., 28 June ?
2 July 1992, pages 309?311.Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and DavidTugwell.
2004.
The Sketch Engine.
In Proceedingsof the 11th International Congress of the EuropeanAssociation for Lexicography, Lorient, France, 6?10July 2004, pages 105?116.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th Inter-national Conference on Machine Learning, Madi-son, Wisc., 24?27 July 1998, pages 296?304.Katja Markert and Udo Hahn.
2002.
Metonymies indiscourse.
Artificial Intelligence, 135(1/2):145?198.Katja Markert and Malvina Nissim.
2002.
Metonymyresolution as classification task.
In Proceedings ofthe 2002 Conference on Empirical Methods in Nat-ural Language Processing, Philadelphia, Penn., 6?7July 2002, pages 204?213.Katja Markert and Malvina Nissim.
2003.
Corpus-based metonymy analysis.
Metaphor and Symbol,18(3):175?188.Katja Markert and Malvina Nissim.
2007.
SemEval-2007 Task 08: Metonymy Resolution at SemEval-2007.
In Proceedings of the 4th International Work-shop on Semantic Evaluations (SemEval-1), Prague,Czech Republic, 23?24 June 2007, pages 36?41.Vivi Nastase and Michael Strube.
2008.
DecodingWikipedia category names for knowledge acquisi-tion.
In Proceedings of the 23rd Conference on theAdvancement of Artificial Intelligence, Chicago, Ill.,13?17 July 2008, pages 1219?1224.Vivi Nastase.
2008.
Unsupervised all-words wordsense disambiguation with grammatical dependen-cies.
In Proceedings of the 3rd International JointConference on Natural Language Processing, Hy-derabad, India, 7?12 January 2008, pages 757?762.Cristina Nicolae, Gabriel Nicolae, and SandaHarabagiu.
2007.
UTD-HLT-CG: Semanticarchitecture for metonymy resolution and classifica-tion of nominal relations.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations(SemEval-1), Prague, Czech Republic, 23?24 June2007, pages 454?459.Malvina Nissim and Katja Markert.
2003.
Syn-tactic features and word similarity for supervisedmetonymy resolution.
In Proceedings of the 41stAnnual Meeting of the Association for Computa-tional Linguistics, Sapporo, Japan, 7?12 July 2003,pages 56?63.Malvina Nissim and Katja Markert.
2005.
Learningto buy a Renault and talk to BMW: A supervisedapproach to conventional metonymy.
In Proceed-ings of the 6th International Workshop on Computa-tional Semantics, Tilburg, Netherlands, January 12-14, 2005.James Pustejovsky.
1991.
The generative lexicon.Computational Linguistics, 17(4):209?241.Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computationalrealization.
Cognition, (61):127?159.David Stallard.
1993.
Two kinds of metonymy.
InProceedings of the 31st Annual Meeting of the As-sociation for Computational Linguistics, Columbus,Ohio, 22?26 June 1993, pages 87?94.Gustaf Stern.
1931.
Meaning and Changes of Mean-ing.
Indiana University Press, Bloomington, Indi-ana.
(1968; first published in Sweden 1931).Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann, San Francisco, Cal., 2nd edi-tion.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivalling supervised methods.
In Pro-ceedings of the 33rd Annual Meeting of the Asso-ciation for Computational Linguistics, Cambridge,Mass., 26?30 June 1995, pages 189?196.918
