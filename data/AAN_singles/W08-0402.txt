Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 10?18,ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Scalable Decoder for Parsing-based Machine Translationwith Equivalent Language Model State MaintenanceZhifei Li and Sanjeev KhudanpurDepartment of Computer Science and Center for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21218, USAzhifei.work@gmail.com and khudanpur@jhu.eduAbstractWe describe a scalable decoder for parsing-based machine translation.
The decoder iswritten in JAVA and implements all the es-sential algorithms described in Chiang (2007):chart-parsing, m-gram language model inte-gration, beam- and cube-pruning, and uniquek-best extraction.
Additionally, paralleland distributed computing techniques are ex-ploited to make it scalable.
We also proposean algorithm to maintain equivalent languagemodel states that exploits the back-off prop-erty of m-gram language models: instead ofmaintaining a separate state for each distin-guished sequence of ?state?
words, we mergemultiple states that can be made equivalent forlanguage model probability calculations dueto back-off.
We demonstrate experimentallythat our decoder is more than 30 times fasterthan a baseline decoder written in PYTHON.We propose to release our decoder as an open-source toolkit.1 IntroductionLarge-scale parsing-based statistical machine trans-lation (MT) has made remarkable progress in thelast few years.
The systems being developed differin whether they use source- or target-language syn-tax.
For instance, the hierarchical translation sys-tem of Chiang (2007) extracts a synchronous gram-mar from pairs of strings, Quirk et al (2005), Liu etal.
(2006) and Huang et al (2006) perform syntac-tic analyses in the source-language, and Galley et al(2006) use target-language syntax.A critical component in parsing-based MT sys-tems is the decoder, which is complex to imple-ment and scale up.
Most of the systems describedabove employ tailor-made, dedicated decoders thatare not open-source, which results in a high barrierto entry for other researchers in the field.
How-ever, with the algorithms proposed in (Huang andChiang, 2005; Chiang, 2007; Huang and Chiang,2007), it is possible to develop a general-purpose de-coder that can be used by all the parsing-based sys-tems.
In this paper, we describe an important first-step towards an extensible, general-purpose, scal-able, and open-source parsing-based MT decoder.Our decoder is written in JAVA and implements allthe essential algorithms described in Chiang (2007):chart-parsing, m-gram language model integration,beam- and cube-pruning, and unique k-best extrac-tion.
Additionally, parallel and distributed comput-ing techniques are exploited to make it scalable.Straightforward integration of an m-gram lan-guage model (LM) into a parsing-based decodersubstantially increases its computational complex-ity.
Therefore, it is important to develop efficientmethods for LM integration.
We propose an algo-rithm to maintain equivalent LM states by exploit-ing the back-off property of m-gram LMs.
Specifi-cally, instead of maintaining a separate state for eachdistinguished sequence of ?state?
words, we mergemultiple states that can be made equivalent for LMcalculations by anticipating such back-off.We demonstrate experimentally that our decoderis 38 times faster than a previous decoder written inPYTHON.
Furthermore, the distributed computingpermits improving translation quality via large-scaleLMs.
We have successfully use our decoder to trans-late about a million sentences in a parallel corpus forlarge-scale discriminative training experiments.102 Parsing-based MT DecoderIn this section, we discuss the core algorithms imple-mented in our decoder.
These algorithms have beendiscussed by Chiang (2007) in detail, and we reca-pitulate the essential parts here for completeness.2.1 Grammar FormalismOur decoder assumes a probabilistic synchronouscontext-free grammar (SCFG).
Following the nota-tion in Venugopal et al (2007), a probabilistic SCFGcomprises a set of source-language terminal sym-bols TS , a set of target-language terminal symbolsTT , a shared set of nonterminal symbols N , and aset of rules of the formX ?
?
?, ?,?, w?
, (1)where X ?
N , ?
?
[N?TS ]?
is a (mixed) sequenceof nonterminals and source terminals, ?
?
[N?TT ]?is a sequence of nonterminals and target terminals,?
is a one-to-one correspondence or alignment be-tween the nonterminal elements of ?
and ?, andw ?
0 is a weight assigned to the rule.
An illus-trative rule for Chinese-to-English translation isNP ?
?NP0{ NP1 , NP1 of NP0 ?
,where the Chinese word { (pronounced de or di)means of, and the alignment, encoded via subscriptson the nonterminals, causes the two noun phrasesaround { to be reordered around of in the transla-tion.
The rule weight is omitted in this example.A bilingual SCFG derivation is analogous to amonolingual CFG derivation.
It begins with a pairof aligned start symbols.
At each step, an alignedpair of nonterminals is rewritten as the two corre-sponding components of a single rule.
In this sense,the derivations are generated synchronously.Our decoder presently handles SCFGs of the kindextracted by Heiro (Chiang, 2007), but is easily ex-tensible to more general SCFGs and closely relatedformalisms such as synchronous tree substitutiongrammars (Eisner, 2003; Chiang, 2006).2.2 MT Decoding as Chart ParsingGiven a source-language sentence f?, the decodermust find the target-language yield e(D) of the bestderivation D among all derivations with source-language yield f(D) = f?, i.e.e?
= e(arg maxD : f(D)=f?w(D)), (2)where w(D) is the composite weight of D.The parser may be treated as a deductive proofsystem (Shieber et al, 1995).
Formally (cf.
(Chiang,2007)), a parser defines a space of weighted items,with some items designated as axioms and some asgoals, and a set of inference rules of the formI1 : w1 ?
?
?
Ik : wkI : w ?
,which states that if all the antecedent items Ii areprovable, respectively with weight wi, then the con-sequent item I is provable with weight w, providedthe side condition ?
holds.
For a grammar with amaximum of two (pairs of) nonterminals per rule1,Figure 1 illustrates the resulting chart parsing proce-dure, including the integration of an m-gram LM.The actual decoding algorithm maintains a chart,which contains an array of cells.
Each cell in turnmaintains a list of proved items.
The parsing processstarts with the axioms, and proceeds by applying theinference rules to prove more and more items untila goal item is proved.
Whenever the parser proves anew item, it adds the item to the appropriate chartcell.
It also maintains backpointers to antecedentitems, which are used for k-best extraction, as dis-cussed in Section 2.4 below.In a SCFG-based decoder, an item is identi-fied by its source-language span, left-side non-terminal label, and left- and right-context for thetarget-language m-gram LM.
Therefore, in a givencell, the maximum possible number of items isO(|N ||TT |2(m?1)), and the worst case decodingcomplexity isO(|N |K |TT |2K(m?1)n3), (3)where K is the maximum number of nonterminalpairs per rule and n is the source-language sentencelength (Venugopal et al, 2007).1For more general grammars with K ?
2 pairs of non-terminals per rule, see Venugopal et al (2007).11X???,??
:w (X ?
?
?, ?,w?)
?
GX?
?fji+1, ??
: w[X, i, j; q(?)]
: wp(?)Z?
?f i1i+1Xfjj1+1, ??
: w [X,i1,j1;e1] : w1[Z, i, j; q(?? )]
: ww1p(?? )
??
= ?[e1/X]Z?
?f i1i+1X1fi2j1+1Y2fjj2+1, ??
: w [X,i1,j1;e1] : w1 [Y,i2,j2;e2] : w2[Z, i, j; q(?? )]
: ww1w2p(?? )
??
= ?
[e1/X1, e2/Y2]Goal item: [S, 0, n; ?s?m?1 ?
e?/s?
]Figure 1: Inference rules from Chiang (2007) for a parser with an m-gram LM.
G denotes the translation grammar.w[x/X] denotes substitution of the string x for the symbol X in the string w. The function p(?)
provides the LMprobability for all complete m-grams in a string, while the function q(?)
elides symbols whose m-grams have beenaccounted for by p(?).
Details about the functions p(?)
and q(?)
are provided in Section 4.2.3 Pruning in a DecoderSevere pruning is needed in order to make the decod-ing computationally feasible for SCFGs with largevocabularies TT and detailed nonterminal sets.
Inour decoder, we incorporate two pruning techniquesdescribed by (Chiang, 2007; Huang and Chiang,2007).
For beam pruning, in each cell, we discardall items whose weight is worse, by a relative thresh-old ?, than the weight of the best item in the samecell.
If too many items pass the threshold, a cell onlyretains the top-b items by weight.
When combiningsmaller items to obtain a larger item by applying aninference rule, we use cube-pruning to simulate k-best extraction in each destination cell, and discardcombinations that lead to an item whose weight isworse than the best item in that cell by a margin of?.2.4 k-best Extraction Over Hyper-graphsFor each source-language sentence f?, the outputof the chart-parsing algorithm may be treated as ahyper-graph representing a set of likely hypothesesD in (2).
Briefly, a hyper-graph is a set of verticesand hyper-edges, with each hyper-edge connectinga set of antecedent vertices to a consequent vertex,and a special vertex designated as the target vertex.In parsing parlance, a vertex corresponds to an itemin the chart, a hyper-edge corresponds to a SCFGrule with the nonterminals on the right-side replacedby back-pointers to antecedent items, and the targetvertex corresponds to the goal item2.Given a hyper-graph for a source-language sen-tence f?, we use the k-best extraction algorithm ofHuang and Chiang (2005) to extract its k most likelytranslations.
Moreover, since many different deriva-tions D in (2) may lead to the same target-languageyield e(D), we adopt the modification described inHuang et al (2006) to efficiently generate the uniquek best translations of f?.3 Parallel and Distributed ComputingMany applications of parsing-based MT entail theuse of SCFGs extracted from millions of bilin-gual sentence pairs and LMs extracted from bil-lions of words of target-language text.
This requiresthe decoder to make use of distributed computingto spread the memory required to load large-scaleSCFGs and LMs onto multiple processors.
Further-more, techniques such as iterative minimum error-rate training (Och et al, 2003) as well as web-basedMT services require the decoder to translate a largenumber of source-language sentences per unit time.This requires the decoder to make use of parallelcomputing to utilize each individual multi-core pro-cessor more effectively.
We have incorporated twosuch performance enhancements in our decoder.2In a decoder integrating an m-gram LM, there may be mul-tiple goal items due to different LM contexts.
However, one canimage a single goal item identified by the span [0, n] and thegoal nonterminal S, but not by the LM contexts.123.1 Parallel DecodingWe have enhanced our decoder to translate multiplesource-language sentences in parallel by exploitingthe ability of a multi-core processor to concurrentlyrun several threads that share memory.
Specifi-cally, given one (or more) document(s) containingmultiple source-language sentences, the decoder au-tomatically splits the set of sentences into severalsubsets, and initiates concurrent decoding threads;once all the threads finish, the main thread mergesback the translations.
Since all the threads naturallyshare memory, the decoder needs to load the (large)SCFG and LM into memory only once.
This multi-threading provides a very significant speed-up.3.2 Distributed Language ModelsIt is not possible in some cases to load a very largeLM into memory on a single machine, particularlyif the SCFG is also very large.
In other cases, load-ing the LM each time the decoder runs may be tootime-consuming relative to the time required for de-coding itself, such as in iterative decoding with up-dated combination weights during minimum error-rate training.
It is therefore desirable to have dedi-cated servers to load parts of the LM3 ?
an idea thathas been exploited by (Zhang et al, 2006; Emami etal., 2007; Brants et al, 2007).Our implementation can load a (partitioned) LMon different servers before initiating decoding.
Thedecoder remotely calls the servers to obtain individ-ual LM probabilities, and linearly interpolates themon the fly using a given set of interpolation weights.With this architecture, one can deal with a very largetarget-language text corpus by splitting it into manyparts and training separate LMs from each.
The run-time interpolation capability may also be used forLM adaptation, e.g.
for building document-specificlanguage models.To mitigate potential network communication de-lays inherent to a distributed LM, we implement asimple cache mechanism in the decoder.
The cachesaves the outcomes of the most recent LM calls,including interpolated LM probabilities; the cacheis reset whenever its size exceeds a threshold.
Wecould have maintained a cache at each LM serveras well; however, the resultant saving is not signif-3Similarly, distributing the SCFG is also possible.icant because the trie data-structures used to imple-ment m-gram LMs are quite fast relative to the cachelookup overhead.4 Equivalent LM-state MaintenanceIt is clear from the complexity (3) of the inferencerules (Figure 1) that a straightforward integrationof an m-gram LM adds a multiplicative factor of|TT |2K(m?1) to the computational complexity of thedecoder, where TT is the set of target-language ter-minal symbols.
We illustrate in this section how thispotentially very large multiplier can be dramaticallyreduced by exploiting the structure of the LM.4.1 Applying an m-gram LM in the DecoderIntegrating an LM into chart parsing requires twofunctions p(?)
and q(?)
(see Figure 1) that oper-ate on strings over TT ?
{?
}, where ?
is a special?placeholder?
symbol for an elided part of a target-language string.The function p(e) calculates the LM probabilityof the complete m-grams in e ?
e1 .
.
.
el, i.e.p(e1 .
.
.
el) =?m?
i?
l & ?
6?
eii?
(m?1)PLM(ei |hi) , (4)where hi = ei?
(m?1) .
.
.
ei?1 is the m?1-word?LM history?
of the target-language word ei.Since the p-probability of e does not include theLM probability for the partial m-grams (i.e., the first(m?
1) words) of e, the exact weights of two items[X, i, j; e] and [X, i, j; e?]
in the chart are not avail-able during the bottom-up pruning of Section 2.3.Therefore, as an approximation, we also computep?
(e) =min{m?1, |e|}?k=1PLM(ek | e1 .
.
.
ek?1), (5)an estimate of the LM probability of the m?1-gramprefix of e. This estimated probability is taken intoaccount for pruning purposes (only).The function q(e1 .
.
.
el) determines the left andright LM states that must be maintained for futurecomputation of the exact LM probability, respec-tively, of e1 .
.
.
em?1 and el+1 .
.
.
el+m?1.q(e1 .
.
.
el) (6)={e1 .
.
.
el if l < m?
1,e1 .
.
.
em?1 ?
el?
(m?2) .
.
.
el otherwise.134.2 Back-off Parameterization of m-gram LMsWhile many different methods are popular for esti-mating m-gram LMs, most store the estimated LMparameters in the ARPA back-off file format; usingthe notation eji to denote a target-language word se-quence ei ei+1 .
.
.
ej , the LM probability calcula-tion is carried out asPBO(em | em?11 ) (7)={pi(em1 ) if em1 ?
LM?
(em?11 )?
PBO(em | em?12 ) otherwise,where the lower order probability PBO(em | em?12 )is recursively defined in the same way, and ?
(em?11 )is the back-off weight of the history.
The LM filecontains the parameter pi(?)
for each listed m-gram,and the parameters pi(?)
and ?(?)
for each listed m?-gram, 1 ?
m?
< m; for unlisted m?-grams, ?(?)
= 1by definition.Observe from (7) that if em1 is not listed in the LM,the back-off weight ?(?)
is the same for all wordsem, and the backed-off probability PBO(em | ?)
is thesame for all words e1.
Furthermore, as m grows, thefraction of possible m-grams actually observed in atraining corpus diminishes rapidly.4.3 The Equivalent LM State of an ItemThe maximum possible number of items in a cell in-creases exponentially with the LM order m, as dis-cussed in Section 2.2.
With pruning (cf.
Section2.3), we restrict the maximum number of items ineach cell to some threshold b.
Intuitively, therefore,if we increase the LM order m, we should also in-crease the beam size b to reduce search errors.
Thiscould slow down the decoder significantly.Recall from the previous subsection, however,that when m increases, the fraction of m-gramsthat will need to back-off also increases.
Moreover,even for modest values of m, the decoder consid-ers many ?unseen?
m-grams (due to reordering andtranslation combinations) that do not appear in natu-ral texts, leading to frequent back-off during the LMprobability calculation (7).
In this subsection, wepropose a method to collapse equivalent LM statesso that the decoder effectively considers many moreitems in each cell without increasing beam size.We merge multiple LM states (6) that alreadyhave?or back-off to?the same ?LM history?
inthe calculation (7) of LM probabilities, e.g.
dueto different unlisted m-grams that back-off to thesame m?1-gram.
For simplicity, we only considerLM state merging by the function q(?)
of (6) whenl ?
m?1.Though the equivalent LM state maintenancetechnique is discussed here in the context of aparsing-based MT decoder, it is also applicable tostandard left-to-right phrase-based decoders.
In par-ticular, the right-side equivalent LM state mainte-nance proposed in Section 4.3.1 may be used.4.3.1 Obtaining the Equivalent Right LM StateRecall that the right LM state ell?
(m?2) of el1serves as the ?LM history?
for calculating the ex-act LM probabilities of the yet-to-be-determinedword el+1.
Recall further the computation (7) ofPBO(el+1 | ell?(m?2)).?
If the m-gram el+1l?
(m?2) is not listed in the LMfor any word el+1, then the LM will back-off toPBO(el+1 | ell?
(m?3)), which does not dependon the word el?(m?2).?
If the m?1-gram ell?
(m?2) also is not listed inthe LM, then ?(ell?
(m?2)) = 1.If these two conditions hold true, q(?)
may safelyelide the word el?
(m?2) in (6) no matter what wordsfollow el1.
The right LM state is thus reduced fromm?
1 words to m?
2 words.The argument above can be applied recursivelyto the resulting right LM state ell?
(m?2)+i, wherei ?
[0,m ?
2], leading to the equivalent right statecomputation procedure of Figure 2.
The procedureIS-A-PREFIX(em?1 ) checks if its argument em?1 is a pre-fix of any k-gram listed in the LM, k ?
[m?,m].4.3.2 Obtaining the Equivalent Left LM StateRecall that the left LM state em?11 of el1 isthe prefix whose exact LM probability is unknownduring bottom-up parsing, and is replaced by theestimated probability p?
(em?11 ) of (5) for pruningpurposes.
Recall further the computation (7) ofPBO(em?1 | em?20 ).?
If the m-gram em?10 is not listed in the LMfor any word e0, then it will back-off to14EQ-R-STATE (ell?
(m?2))1 ers ?
ell?
(m?2)2 for i ?
0 to m?
2 ?
left to right3 if IS-A-PREFIX (ell?
(m?2)+i)4 break ?
stop reducing ers5 else6 ers ?
ell?
(m?2)+i+1 ?
reduce state7 return ersFigure 2: Equivalent Right LM State Computation.PBO(em?1 | em?21 ), which can be computedright away based on em?11 without waiting forthe unknown e0.
Moreover, the back-off weight?
(em?20 ) does not depend on the word em?1.Therefore, q(?)
may safely elide the word em?1, andreduce the left LM state in (6) from em?11 to em?21 .Also, p(?)
should also co-opt PBO(em?1 | em?21 ) intothe complete m-gram probability of (4) and p?(?
)should exclude em?1 in (5).The argument above can again be applied recur-sively to the resulting left LM state ei1, i ?
[1,m?1],leading to the equivalent left state procedure of Fig-ure 3.
The procedure IS-A-SUFFIX(em?1 ) checks ifem?1 is a suffix of any listed k-gram in the LM, k ?[m?,m].
In Figure 3, fin refers to the probabilitythat can be computed right away based on the stateitself, for co-opting into the complete m-gram prob-ability of (4) as mentioned above.4.3.3 Modified Cost Functions for ParsingWhen carrying out the reduction of the left andright LM states to their shortest equivalents, the for-mula (4) for calculating the probability of the com-plete m-grams in an item [X, i, j; e], where e = el1,is modified asp(el1)= EQ-L-STATE(em?11 ).fin??m?
i?
l & ?
6?
eii?
(m?1)PLM(ei |hi)with the further qualification that some care must betaken later to incorporate the back-off weights of the?LM histories?
of the suffix of em?11 that went miss-ing due to left LM state reduction.EQ-L-STATE (em?11 )1 els ?
em?112 fin ?
1 ?
update to final probability p3 for i ?
m?
1 to 1 ?
right to left4 if IS-A-SUFFIX(ei1)5 break ?
stop reducing els6 else7 fin ?
PBO(ei | ei?11 )?
fin8 els ?
ei?11 ?
reduce state9 return els, finFigure 3: Equivalent Left LM State Computation.The estimated probability of the left LM state ismodified asp?
(e) ={p?
(e) if |e| < m?
1p?
(EQ-L-STATE(em?11 ).els) otherwise,with p?
as defined in (5).Finally, the LM state function isq(el1)=??????
?e1 .
.
.
el if l < m?
1EQ-L-STATE(em?11 ).els ?EQ-R-STATE(ell?
(m?2)).ers otherwise.4.3.4 Suffix and Prefix Look-UpAs done in the SRILM toolkit (Stolcke, 2002), aback-off m-gram LM is stored using a reverse triedata structure.
We store the suffix and prefix infor-mation in the same data structure without incurringmuch additional memory cost.
Specifically, the pre-fix information is stored at the back-off state, whilethe suffix information is stored as one bit alongsidethe regular m-gram probability.5 Experimental ResultsIn this section, we evaluate the performance of ourdecoder on a Chinese to English translation task.5.1 System TrainingWe use various parallel text corpora distributed bythe Linguistic Data Consortium (LDC) for the NISTMT evaluation.
The parallel data we select containsabout 570K Chinese-English sentence pairs, adding15up to about 19M words on each side.
To train theEnglish language models, we use the English sideof the parallel text and a subset of the English Giga-word corpus, for a total of about 130M words.We use the GIZA toolkit (Och and Ney, 2000),a suffix-array architecture (Lopez, 2007), theSRILM toolkit (Stolcke, 2002), and minimum er-ror rate training (Och et al, 2003) to obtain word-alignments, a translation model, language models,and the optimal weights for combining these mod-els, respectively.5.2 Improvements in Decoding SpeedWe use a PYTHON implementation of a state-of-the-art decoder as our baseline4 for decoder compar-isons.
For a direct comparison, we use exactly thesame models and pruning parameters.
The SCFGcontains about 3M rules, the 5-gram LM explicitlylists about 49M k-grams, k = 1, 2, .
.
.
, 5, and thepruning uses ?
= 10, b = 30 and ?
= 0.1.Decoder Speed BLEU-4(sec/sent) MT ?03 MT ?05Python 26.5 34.4% 32.7%Java 1.2 34.5% 32.9%Java (parallel) 0.7Table 1: Decoder Comparison: Translation speed andquality on the 2003 and 2005 NIST MT benchmark tests.As shown in Table 1, the JAVA decoder (withoutexplicit parallelization) is 22 times faster than thePYTHON decoder, while achieving slightly bettertranslation quality as measured by BLEU-4 (Pap-ineni et al, 2002).
The parallelization further speedsit up by a factor of 1.7, making the parallel JAVA de-coder is 38 times faster than the PYTHON decoder.We have used the decoder to successfully decodeabout one million sentences for a large-scale dis-criminative training experiment.5.3 Impact of a Distributed Language ModelWe use the SRILM toolkit to build eight 7-gram lan-guage models, and load and call the LMs using a4We are extremely thankful to Philip Resnik at University ofMaryland for allowing us the use of their PYTHON decoder asthe baseline.
Thanks also go to David Chiang who originallyimplement the decoder.distributed LM architecture5 as discussed in Section3.2.
As shown in Table 2, the 7-gram distributed lan-guage model (DLM) significantly improves trans-lation performance over the 5-gram LM.
However,decoding is significantly slower (12.2 sec/sent whenusing the non-parallel decoder) due to the added net-work communication overhead.LM type # k-grams MT ?03 MT ?055-gram LM 49 M 34.5% 32.9%7-gram DLM 310 M 35.5% 33.9%Table 2: Distributed language model: the 7-gram LMcannot be loaded alongside the SCFG on a single ma-chine; via distributed computing, it yields significant im-provement in BLEU-4 over a 5-gram.5.4 Utility of Equivalent LM StatesTo reduce the number of search errors, one may ei-ther increase the beam size, or employ techniquessuch as the equivalent LM state maintenance de-scribed in Section 4.
In this subsection, we comparethe tradeoff between the search effort (measured bydecoding time per sentence) and the search qual-ity (measured by the average model cost of the besttranslation found).Intuitively, collapsing equivalent LM states is use-ful only when the language model is very sparse, i.e.,most of the evaluated m-grams will need to back-off.
A sparse LM is obtained in practice by usinga large order m relative to the amount of trainingdata.
To test this intuition, we train a 7-gram LMusing only the English side of the parallel text (?19M words).
Figure 4 compares maintenance ofthe full LM state v/s the equivalent LM state.
Thebeam size b for decoding with equivalent LM statesis fixed at 30; it is increased considerably?30, 50,70, 90, 120, and 150?with the full LM state inan effort to reduce search errors.
It is clear fromthe figure that collapsing items that differ due onlyto equivalent LM states improves the search qualityconsiderably while actually reducing search effort.This shows the effectiveness of equivalent LM statemaintenance.5Since our distributed LM architecture dynamically interpo-lates multiple LM scores, it cannot yet exploit the equivalentLM state maintenance of Section 4, for different LMs will havedifferent reduced LM states.
We will address this in the future.16Search Effort vs Search Quality19.9519.9719.9920.0120.0320.0520.070 2 4 6 8 10Number of Seconds per SentenceAvg ModelCostforone-bestsBaselineEquivLMbeam size = 30Figure 4: Search quality with equivalent 7-gram LM statemaintenance (EquivLM) and without it (Baseline) as afunction of search effort as controlled by the beam size.We also train a 3-gram LM using an English cor-pus of about 130M words, and repeat the above ex-periments.
In this case, maintaining equivalent LMstates costs more decoding time than using the fullLM state to achieve the same search quality.
Thisis due partly to our inefficient implementation of theprefix- and suffix-lookup required to determine theequivalent LM state, and partly to the fact that with130M words, a 3-gram LM backs off less frequently.6 ConclusionsWe have described a scalable decoder for parsing-based machine translation.
It is written in JAVA andimplements all the essential algorithms describedin Chiang (2007): chart-parsing, m-gram languagemodel integration, beam- and cube-pruning, andunique k-best extraction.
Additionally, parallel anddistributed computing techniques are exploited tomake it scalable.
We demonstrate that our decoderis 38 times faster than a baseline decoder written inPYTHON, and that the distributed language modelis very useful to improve translation quality in alarge-scale task.
We also describe an algorithm thatexploits the back-off property of an m-gram modelto maintain equivalent LM states, and show that bet-ter search quality is obtained with less search effortwhen the search space is organized to exploit thisequivalence.
We plan to incorporate some additionalsyntax-based components into the decoder and re-lease it as an open-source toolkit.AcknowledgmentsWe thank Philip Resnik, Chris Dyer, SmarandaMuresan and Adam Lopez for very helpful discus-sions, and the anonymous reviewers for their con-structive comments.
This research was partially sup-ported by the Defense Advanced Research ProjectsAgency?s GALE program via Contract No?
HR0011-06-2-0001.ReferencesThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2006.
Large Language Models inMachine Translation.
In Proceedings of EMNLP 2007.David Chiang.
2006.
An Introduction toSynchronous Grammars.
Available athttp://www.isi.edu/?chiang/papers/synchtut.pdf.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201-228.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proceedings of ACL2003.Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.2007.
Large-scale distributed language modeling.
InProceedings of ICASSP 2007.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING/ACL 2006.Liang Huang and David Chiang.
2005.
Better k-best pars-ing.
In Proceedings of IWPT 2005.Liang Huang and David Chiang.
2007.
Forest Rescoring:Faster Decoding with Integrated Language Models.
InProceedings of the ACL 2007.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA 2006.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of COLING-ACL 2006.Adam Lopez.
2007.
Hierarchical Phrase-Based Transla-tion with Suffix Arrays.
In Proceedings of EMNLP2007.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL2003.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of ACL2000.17Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL2002.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency Treelet Translation: Syntactically InformedPhrasal SMT.
In Proceedings of ACL 2005.Stuart Shieber, Yves Schabes, and Fernando Pereira.1995.
Principles and implementation of deductiveparsing.
Journal of Logic Programming, 24:3-15.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing, volume2, pages 901-904.Ashish Venugopal, Andreas Zollmann, Stephan Vo-gel.
2007.
An Efficient Two-Pass Approach toSynchronous-CFG Driven Statistical MT.
In Proceed-ings of NAACL 2007.Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.2006.
Distributed language modeling for n-best list re-ranking.
In Proceedings of EMNLP 2006.18
