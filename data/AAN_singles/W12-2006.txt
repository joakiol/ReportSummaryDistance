The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 54?62,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsHOO 2012:A Report on the Preposition and Determiner Error Correction Shared TaskRobert Dale, Ilya Anisimoff and George NarrowayCentre for Language TechnologyMacquarie UniversitySydney NSW 2109Australiardale@acm.org, anisimoff@gmail.com, george.narroway@me.comAbstractIncorrect usage of prepositions and determin-ers constitute the most common types of er-rors made by non-native speakers of English.It is not surprising, then, that there has beena significant amount of work directed towardsthe automated detection and correction of sucherrors.
However, to date, the use of differ-ent data sets and different task definitions hasmade it difficult to compare work on the topic.This paper reports on the HOO 2012 sharedtask on error detection and correction in theuse of prepositions and determiners, wheresystems developed by 14 teams from aroundthe world were evaluated on the same previ-ously unseen errorful text.1 IntroductionIt is widely recognized that the correct usage ofdeterminers and prepositions in English is a ma-jor problem area for non-native speakers of the lan-guage.1 The issues here have been explored anddiscussed extensively in the literature; an excellentand up-to-date summary is available in (Leacock etal., 2010).
However, the various teams that have at-tempted to tackle these problems so far have tendedto use slightly different task specifications, and dif-ferent data sets for evaluation; this makes it very dif-1We use the broad term ?non-native speaker?, abbreviated?NNS?, in this paper; other work makes a distinction betweenESL (English as a Second Language) speakers (who live andspeak in a primarily English-speaking environment) or EFL(English as a Foreign Language) speakers (who are learning En-glish in a non-English-speaking country.ficult to compare the results achieved using differentapproaches.To address this problem, the aim of the HOO 2012Shared Task was to provide a forum for the compar-ative evaluation of different approaches to the cor-rection of these errors.2 The shared task provides acommon training dataset, a shared evaluation frame-work, and a set of previously unseen test data.These proceedings contain detailed reports by all14 teams who participated in HOO 2012.
Thepresent paper provides a summary of the task and itsevaluation, and a report on the results of that evalu-ation.Section 2 provides an overview of the task and thetimeline across which it was carried out; Section 3provides details of the participating teams; Section 4describes the training and test data in more detail;Section 5 presents the results of the evaluation; andSection 6 provides some concluding remarks anddiscussion, reflecting on lessons learned.2 The TaskNon-native speakers who are learning English findprepositions and determiners particularly problem-atic.
The selection of the appropriate preposition ina given context often appears to be a matter of id-iom or convention rather than being governed by aconsistent set of rules; and selecting a determiner2HOO stands for ?Helping Our Own?, a reflection of the his-torical origins of the exercise as an attempt to develop tools tohelp researchers in natural language processing to write betterpapers: see (Dale and Kilgarriff, 2010) for the background tothis enterprise and (Dale and Kilgarriff, 2011) for a report onthe pilot round of the task held in 2011.54Team ID Group or Institution Subtasks RunsCU Computer Laboratory, University of Cambridge, UK DRC 8ET Educational Testing Service, New Jersey, USA DR 3JU Jadavpur University, Kolkata, India DRC 1KU Natural Language Processing Lab, Korea University, Seoul, Korea DRC 10LE KU Leuven, Belgium DRC 2NA NAIST, Japan DRC 8NU National University of Singapore, Singapore DRC 1TC Department of Computer Science and Statistics, Trinity College Dublin, Ireland DRC 10TH NLPLAB, National Tsing Hua University, Hsinchu, Taiwan DRC 4UD UKP Lab, Technische Universita?t Darmstadt, Germany DRC 3UI Cognitive Computation Group, University of Illinois, USA DRC 10UT Theoretical Computational Linguistics Group, University of Tu?bingen, Germany DRC 10VA Valkuil.net, The Netherlands DRC 6VT VTEX, Vilnius, Lithuania DRC 9Table 1: Participating teamsdepends on a complex of contextual factors whichis particularly challenging for those whose nativelanguage does not make use of determiners.
Theliterature suggests that mistakes in the use of thedeterminers and prepositions account for 20?50%of grammar and usage errors; the extent to whicha learner has problems with determiners varies de-pending on their native language, while the degree ofdifficulty experienced with prepositions is less var-ied (see Chapter 3 in (Leacock et al, 2010)).For the shared task, we made use of data drawnfrom the CLC FCE Dataset, a set of 1,244 examscripts written by candidates sitting the CambridgeESOL First Certificate in English (FCE) examina-tion in 2000 and 2001, and made available by Cam-bridge Universiy Press; see (Yannakoudakis et al,2011).
This data is described in more detail below.The version of the data we provided to teams astraining data consisted of the original text as writtenby the examination subjects, so it contains many er-rors besides the preposition and determiner errors; itthus provides a quite realistic challenge, as opposedto artificial data sets where the only errors presentare the particular errors of interest.
The training datawe provided consisted of the raw, errorful texts, andfor each text file a set of gold-standard standoff an-notations indicating the locations of the prepositionand determiner errors and their corrections, whichwe extracted from the CUP data annotations.The task consisted in attempting to generate setsof standoff annotations that matched those in thegold standard.
Teams were to be evaluated on threesubtasks: detection, recognition and correction.
Thefirst of these is a measure of a system?s success indetermining that something is wrong in a text andthat it requires fixing; the second requires also thatthe precise extent of the error be identified, and thecorrect type assigned; and the third requires that acorrection matching that in the gold standard be of-fered.
Scores on each of these subtasks were com-puted for preposition and determiner errors com-bined, and for preposition and determiner errors sep-arately; thus, each participating system run couldgenerate up to nine distinct scores.
In addition, wealso provided teams with detection, recognition andcorrection scores for each of the six base error types(see Table 2); some teams report on these statisticsin their individual reports.The training data and evaluation tools were madeavailable on 27th January 2012; test data was re-leased on April 6th 2012, with submissions of resultsfrom teams due on April 13th 2012.
Teams thereforehad 10 weeks to develop a system that could handlethe training data, and one week to provide results onthe test data.3 The ParticipantsAt the time we released the training data, 26 teamsregistered interest in the task.
The test data, re-ceipt of which required a signed agreement withCambridge University Press, was requested by 15teams; one of these teams subsequently withdrew55Type Tag Original CorrectionReplacement Preposition RT I could only travel on July I could only travel in JulyMissing Preposition MT I am looking forward your reply I am looking forward to your replyUnwanted Preposition UT I have booked a flight to home I have booked a flight homeReplacement Determiner RD wich was situeded on a seaside wich was situeded on the seasideMissing Determiner MD I will give you all information I will give you all the informationUnwanted Determiner UD One of my hobbies is the photography One of my hobbies is photographyTable 2: Examples of the six base error typesfrom the competition.
The 14 teams who completedthe shared task are listed in Table 1.34 The Data4.1 Basic StatisticsThe training data consisted of 1000 files drawn fromthe publicly available FCE dataset.
These were con-verted from the native FCE format into the HOOdata format, which was slightly revised from the ver-sion used in HOO 2011 (see (Dale and Kilgarriff,2011)).
The original data was marked up with all theerrors found by the CUP annotator, but we discardedannotations of errors other than the six base types wewere interested in, and converted the remaining er-rors into standoff annotations.
The six types, withexamples of each, are shown in Table 2;4 Figure 1shows a fragment of an FCE data file, and Figure 2shows a standoff annotation example extracted fromthis file in the HOO format.Elements of some of these files were removed todispose of nested edits and other phenomena thatcaused difficulties in the preprocessing of the data.5The resulting set of training data comprised a totalof 374680 words, for an average of 375 words perfile.The test data consisted of a further 100 previouslyunseen files provided to us for this shared task byCUP.
These were processed in the same manner asthe training data.
The test data comprised 18013words, for an average of 180 words per file.
Counts3The ?Subtasks?
column indicates which subtasks the teamtook part in: detection (D), recognition (R) and correction (C).The ?Runs?
column is explained later.4For the present exercise we used the preposition and deter-miner error tags as provided in the CLE tagset.
The full CLEtagset is described in (Nicholls, 2003).5This preprocessing step was not perfect, and we subse-quently discovered it had introduced some noise into the data.<p>First I must say that most <#UT>of</#UT>people don?t see any problems with<#RV>growing|increasing</#RV><#RD>a|the</#RD> list of<#UP>car?s|car</#UP> owners.Some of them think that it shows how<#SX>reach|rich</#SX> our country is.</p>Figure 1: A fragment of an FCE data file<edit type="RD" index="0005"file="0006" part="1"start="427" end="428"><original>a</original><corrections><correction>the</correction></corrections></edit>Figure 2: A standoff error annotationof the different error types in the training and testdata are provided in Table 3, demonstrating that theerror rate remained fairly constant across trainingand test data.
However, whereas the training dataincluded information on author?s first language (L1)and age range, the L1 information was not presentin the test data, thus removing a potentially usefulfeature that some teams may have hoped to exploit.4.2 Revisions to the Gold-Standard DataNote that Table 3 shows counts for two versions ofthe gold-standard test data: the original version asderived from the CUP-provided data set (?Test A?
),and a revised version (?Test B?)
which incorporatescorrections to errors found in the annotations.The evaluation process quickly revealed that thereappeared to be cases of annotation error in the orig-inal test data.
This concerned us because it meantthat system performance was being under-reported:56Type # Training # Test A # Test BUT 822 43 39MT 1105 57 56RT 2618 136 148Prep 4545 236 243UD 1048 53 62MD 2230 125 131RD 609 39 37Det 3887 217 230Total 8432 453 473Words/Error 44.18 39.77 38.08Table 3: Data statisticsin particular, systems were identifying real errors inthe source texts which had not been annotated inthe gold standard, and were consequently being pe-nalised for finding spurious errors which were not infact spurious.To address this problem, once teams had submit-ted their results, we allowed a brief period whereteams could review the gold-standard data to iden-tify possible corrections to that data.
Table 4 showsthe number of revisions requested by each team, andthe number of these revisions that were accepted.Note that there were a significant number of revi-sions (99) that were requested by more than oneteam, so the total count of revision requests is largerthan the actual number of revisions considered.
Ofthe total 357 requests, 205 were acted on, in somecases not in the manner requested by the team; 152requests led to no changes being made to the anno-tations.Note that the teams?
original sets of submitted ed-its were compared against this revised gold standard,so there was no sense in which a system?s behaviourcould be tuned to the test data.
However, clearlyany given team might stand to benefit from iden-tifying particular errors their system had identifiedthat were not in the gold standard, effectively tuningthe test data to system behaviour.
Consequently, weprovide results below for both the original and therevised data sets, and briefly discuss the impact ofthese corrections.5 ResultsEach team was allowed to submit up to 10 sepa-rate ?runs?
over the test data, thus allowing them toTeam Requested Acted OnCU 51 30ET 22 18LE 5 5NU 83 59UI 151 54UT 45 39Totals 357 205Table 4: Requests for corrections to the gold-standard datahave different configurations of their systems evalu-ated; the number of runs submitted by each team isshown in Table 1.
We report here only on the best-performing runs from each team.Teams were asked to indicate whether they hadused only publicly-available data to train their sys-tems, or whether they had made use of privately-helddata: only the ET and CU teams used privately-helddata, and in the latter case only for a subset of theirruns.
In the tabulated results provided here, reportedruns that involve privately-held data are marked withan asterisk.The results of the evaluation are provided here insix tables.
Tables 5 and 6 provide results for prepo-sition and determiner errors combined; Tables 7 and8 provide results for preposition errors only; and Ta-bles 9 and 10 provide results for determiner errorsonly.
In each pair, the first table shows results beforethe revisions described in Section 4.2 were carriedout, and the second table shows the results using therevised gold-standard data.
Each table shows preci-sion, recall and F-score (computed as the harmonicmean) for each of detection, recognition and correc-tion; for each of these, the best score is shown inbold.6 Note that team ET did not participate in thecorrection subtask.The scores for all teams improve as a consequenceof the revisions being made to the data.
The result ofa paired t-test on the ?before?
and ?after?
combinedpreposition and determiner scores across teams wasstatistically significant (t = ?3.17, df(12), p < .01);6The precise definitions of these measures as imple-mented in the evaluation tool, and further details onthe evaluation process, are provided in (Dale and Nar-roway, 2012) and elaborated on at the HOO website atwww.correcttext.org/hoo2012.57F-scores improved by a mean value of 2.32.
Thesame analyses for preposition scores also resultedin significant improvement (t = ?3.29, df(12),p < .01), with a mean improvement in F-scoresof 2.6.
A smaller (but still statistically significant)improvement in determiner scores was also present(t = ?2.86, df(12), p < .05), with a mean improve-ment in F-scores of 1.99.There are also positive correlations between therankings before and after revisions.
Pearson corre-lation coefficients for the ?before?
and ?after?
scoresfor prepositions and determiners combined, prepo-sitions only, and determiners only (respectively) are.993, .985 and .996.
All correlation coefficients aresignificant at p < .001, n = 13 (teams).However, some systems improve more than oth-ers.
An obvious question to ask, then, is whether thebenefit that a team achieves is positively correlatedwith the number of accepted corrections they pro-posed; a calculation of Pearson?s correlation coeffi-cient suggests that this is indeed the case (r = 0.821,p = 0.044 (one-tailed)).7 This suggests, then, thatthe ?before?
results may be a more reliable indicatorof comparative performance.6 Discussion and ConclusionsIn this section, we make some observations onlessons learned with regard to various aspects of theshared task.6.1 Data AcquisitionData annotated with non-native speaker errors hassignificant commercial value, and so is not easyto find in the public domain.
We were fortunateto be able to take advantage of the recently-made-available FCE dataset as training data, but this leftus with the problem of acquiring previously unseentest data.
To address this, we entered into negotia-tion with Cambridge University Press with the aimof acquiring some additional previously unreleaseddata.
We started this process in December 2011, butit quickly became apparent that some of the legalaspects would necessarily make this a slow process.7Computed here on the combined preposition and deter-miner scores, and taking account only of the five teams thatproposed corrections, these being UI, NU, LE, UT and CU.
ETwas not included in this calculation since they did not submit tothe corrections subtask.As a back-up plan, we informed teams that we mighthave to fall back on some of the already-availableFCE data as test data; to this end, we asked teamsonly to use versions and subsets of the FCE data thatwe made directly available.
We thus selected 1000files from the 1200 that make up the public FCE dataset as training data, and reserved the remainder as asource of possible test data.This is clearly not an ideal situation; fortunately,we finally signed agreements for the use of a newset of FCE data in the week before the test data wasdue to be released, but this was leaving things rathertight.
The moral here is that one needs to be confi-dent of one?s data sources early on in the process.6.2 Data QualityAs discussed above, it became apparent that therewere what appeared to be annotation errors in ourdata.
This is perhaps inevitable given the nature ofthe source data, which was annotated by only oneannotator (subsequent to some prior automatic pro-cessing).
The issue of reliability of annotation in thisarea has been noted by others (see, for example, thediscussion in Chapter 5 in (Leacock et al, 2010)).Assuming that we agree an error is present?and thisis not always in itself straightforward?there is oftenmore than one way to correct that error; however,the FCE annotation scheme does not permit multiplepossible corrections, so in the source data we used,there was only ever one correction per error.
Ourrevision process identified a number of cases wherealternative corrections were equally acceptable, andfortunately the HOO annotation scheme allowed usto incorporate multiple possible corrections; but it?squite clear that we did not identify all cases wheremultiple corrections were valid.8This is a significant issue.
If we cannot entirelytrust our gold-standard data, then we cannot placetoo much trust in the results of evaluations carriedout using that data.
Of course, annotation qualityis a problem in any task, but it may be more se-vere in cases like the present one because the judge-ments here are often less clear cut: whereas thereis rarely dispute as to whether a given string consti-tutes a named entity, it is not always so clear that8The HOO scheme also allows optional edits, but we did notmake use of these here since it complicates the scoring process;see (Dale and Kilgarriff, 2011) for discussion.58Detection Recognition CorrectionTeam Run P R F Run P R F Run P R FCU 2 13.12 34.88 19.07 7 8.13 41.5 13.6 0 70.0 4.64 8.7ET 1 33.59* 37.97* 35.65* 1 30.27* 34.22* 32.12* ?
?
?
?JU 1 6.93 7.28 7.1 1 6.3 6.62 6.46 1 2.52 2.65 2.58KU 0 4.61 49.23 8.43 0 2.67 28.48 4.88 0 1.45 15.45 2.65LE 0 37.38 26.49 31.01 0 33.33 23.62 27.65 0 31.15 22.08 25.84NA 3 40.19 28.04 33.03 3 36.39 25.39 29.91 3 29.43 20.53 24.19NU 0 57.42 26.49 36.25 0 55.98 25.83 35.35 0 45.45 20.97 28.7TC 9 5.33 25.61 8.82 9 4.18 20.09 6.92 9 2.66 12.8 4.41TH 1 17.74 48.12 25.92 1 15.38 41.72 22.47 1 9.44 25.61 13.79UD 2 8.94 31.13 13.88 2 5.51 19.21 8.57 2 1.2 4.19 1.87UI 8 37.22 43.71 40.2 1 34.23 36.64 35.39 1 26.39 28.26 27.29UT 6 37.46 25.39 30.26 7 32.01 23.18 26.89 7 21.95 15.89 18.44VA 3 12.5 15.23 13.73 3 10.87 13.25 11.94 3 6.16 7.51 6.77VT 5 10.6 5.08 6.87 5 10.14 4.86 6.57 5 8.76 4.19 5.67Table 5: Results before revisions, all errorsDetection Recognition CorrectionTeam Run P R F Run P R F Run P R FCU 2 14.04 35.73 20.16 7 8.69 42.49 14.43 6 5.73 28.54 9.54ET 1 38.09* 41.23* 39.59* 1 35.55* 38.48* 36.95* ?
?
?
?JU 1 8.19 8.25 8.22 1 7.56 7.61 7.59 1 3.15 3.17 3.16KU 0 5.01 51.16 9.12 0 3.04 31.08 5.54 0 1.86 19.03 3.39LE 0 41.12 27.91 33.25 0 36.45 24.74 29.47 0 34.27 23.26 27.71NA 3 45.25 30.23 36.25 3 40.82 27.27 32.7 3 33.86 22.62 27.12NU 0 70.33 31.08 43.11 0 69.38 30.66 42.52 0 61.72 27.27 37.83TC 8 6.56 26.0 10.48 8 4.91 19.45 7.84 8 3.09 12.26 4.94TH 1 19.2 49.89 27.73 1 17.33 45.03 25.03 1 10.82 28.12 15.63UD 2 9.95 33.19 15.31 2 5.77 19.24 8.87 2 1.33 4.44 2.05UI 2 43.56 42.92 43.24 1 38.97 39.96 39.46 1 32.58 33.4 32.99UT 7 39.94 27.7 32.71 7 35.67 24.74 29.21 5 31.58 17.76 22.73VA 3 13.22 15.43 14.24 3 11.59 13.53 12.49 3 7.25 8.46 7.8VT 5 11.52 5.29 7.25 5 11.06 5.07 6.96 5 9.68 4.44 6.09Table 6: Results after revisions, all errorssomething is an error, or where that error should belocated.
The incorporation of optional and multiplecorrections in the HOO framework was intended toaddress this kind of problem, but the value of thesefeatures is only delivered if the scheme is used dur-ing annotation, rather than being applied after anno-tation has already been carried out.6.3 The Annotation Scheme and EvaluationToolsGiven real non-native speaker data that contains awide range of errors other than those that we wereparticularly concerned with in this shared task, wewere faced with three alternatives in how we pre-pared the data for use in the task.1.
We could provide the data with all original er-rors in place.2.
We could provide the data with all but thepreposition and determiner errors corrected.3.
We could provide the data with selected errorscorrected or replaced.The problem with the first of these options, ofcourse, is that other errors that appear in the context59Detection Recognition CorrectionTeam Run P R F Run P R F Run P R FCU 2 14.88 59.32 23.79 2 9.99 39.83 15.97 0 61.11 4.66 8.66ET 1 31.95* 42.37* 36.43* 1 27.16* 36.02* 30.97* ?
?
?
?JU 1 6.1 7.63 6.78 1 5.42 6.78 6.03 1 3.05 3.81 3.39KU 0 3.39 66.95 6.46 0 2.51 49.58 4.79 0 1.27 25.0 2.41LE 0 32.81 17.8 23.08 0 27.34 14.83 19.23 0 25.78 13.98 18.13NA 6 41.13 24.58 30.77 3 36.43 19.92 25.75 3 30.23 16.53 21.37NU 0 56.99 22.46 32.22 0 53.76 21.19 30.4 0 41.94 16.53 23.71TC 9 6.49 29.66 10.65 9 5.19 23.73 8.52 9 3.06 13.98 5.02TH 1 17.39 59.32 26.9 1 14.16 48.31 21.9 1 9.19 31.36 14.22UD 2 11.84 36.86 17.92 2 9.66 30.08 14.62 1 7.63 4.24 5.45UI 1 38.21 45.34 41.47 5 31.05 40.25 35.06 1 20.36 24.15 22.09UT 2 39.35 25.85 31.2 7 35.76 22.88 27.91 0 25.45 11.86 16.18VA 0 13.44 14.41 13.91 0 11.46 12.29 11.86 0 7.51 8.05 7.77VT 7 12.24 2.54 4.21 7 12.24 2.54 4.21 7 12.24 2.54 4.21Table 7: Results before revisions, preposition errors onlyDetection Recognition CorrectionTeam Run P R F Run P R F Run P R FCU 2 15.41 59.43 24.47 2 10.63 40.98 16.88 0 66.67 4.92 9.16ET 1 35.14* 45.08* 39.5* 1 32.27* 41.39* 36.27* ?
?
?
?JU 1 7.12 8.61 7.79 1 6.44 7.79 7.05 1 3.73 4.51 4.08KU 0 3.67 70.08 6.98 0 2.9 55.33 5.51 0 1.7 32.38 3.23LE 0 35.16 18.44 24.19 0 29.69 15.57 20.43 0 28.13 14.75 19.35NA 6 48.23 27.87 35.32 6 41.84 24.18 30.65 6 33.33 19.26 24.42NU 0 72.04 27.46 39.76 0 70.97 27.05 39.17 0 60.22 22.95 33.23TC 8 7.72 29.92 12.27 8 5.92 22.95 9.41 9 3.34 14.75 5.45TH 1 18.76 61.89 28.79 1 16.27 53.69 24.98 1 10.68 35.25 16.4UD 2 12.65 38.11 19.0 2 10.2 30.74 15.32 1 9.16 4.92 6.4UI 1 41.43 47.54 44.27 1 37.14 42.62 39.69 1 26.79 30.74 28.63UT 2 41.94 26.64 32.58 2 39.35 25.0 30.58 0 35.45 15.98 22.03VA 0 14.23 14.75 14.49 0 12.65 13.11 12.88 0 8.7 9.02 8.85VT 7 16.33 3.28 5.46 7 16.33 3.28 5.46 7 16.33 3.28 5.46Table 8: Results after revisions, preposition errors onlyof a preposition or determiner error could confuse asystem focussed only on preposition or determinererrors; if the surrounding context contains errors,then it cannot be relied upon to deliver the kindsof features that one would expect to find in well-formed text.
To partially address this, many teamsran a spelling correction process on the texts priorto applying their techniques; but this only catches asmall proportion of the potential problems.However, the second option has the oppositeproblem: by removing all the other errors from thetext, we would be providing a very artificial datasetwhere one assumes some other process has fixed allthe other errors before the errors of interest hereare addressed.
While there are some types of er-rors that might sensibly be addressed before othersin a pipeline, in general this is not a very plausiblemodel; any real system is going to have to addressnoisy data containing many different kinds of errorssimultaneously.A third alternative, that of selectively removingor correcting errors, is something of a middle road,and has been used in other work using the CLC data:in particular, Gamon (2010) removes from the datasentences where some other error appears immedi-ately next to a preposition or determiner error.60Detection Recognition CorrectionTeam Run P R F Run P R F Run P R FCU 6 7.8 49.31 13.48 6 6.86 43.32 11.84 6 5.25 33.18 9.07ET 0 51.67* 28.57* 36.8* 0 50.83* 28.11* 36.2* ?
?
?
?JU 1 7.73 6.45 7.04 1 7.73 6.45 7.04 1 1.66 1.38 1.51KU 0 12.85 10.6 11.62 0 6.7 5.53 6.06 0 6.15 5.07 5.56LE 0 40.41 35.94 38.05 0 37.31 33.18 35.12 0 34.72 30.88 32.68NA 1 37.43 32.26 34.65 1 36.36 31.34 33.66 1 28.88 24.88 26.73NU 0 57.76 30.88 40.24 0 57.76 30.88 40.24 0 48.28 25.81 33.63TC 3 8.68 8.76 8.72 3 7.76 7.83 7.8 3 4.11 4.15 4.13TH 1 17.69 34.56 23.4 1 17.69 34.56 23.4 1 9.91 19.35 13.1UD 2 6.41 24.88 10.19 1 1.98 6.45 3.03 0 0.0 0.0 0.0UI 0 40.0 37.79 38.86 0 38.05 35.94 36.97 0 35.61 33.64 34.6UT 5 34.38 25.35 29.18 5 31.87 23.5 27.06 6 25.75 19.82 22.4VA 3 11.04 15.21 12.79 3 10.37 14.29 12.02 3 5.02 6.91 5.81VT 5 9.82 7.37 8.42 5 9.82 7.37 8.42 5 7.98 5.99 6.84Table 9: Results before revisions, determiner errors onlyDetection Recognition CorrectionTeam Run P R F Run P R F Run P R FCU 6 8.53 51.09 14.63 6 7.37 44.1 12.63 6 5.91 35.37 10.13ET 0 57.5* 30.13* 39.54* 0 56.67* 29.69* 38.97* ?
?
?
?JU 1 9.39 7.42 8.29 1 9.39 7.42 8.29 1 2.21 1.75 1.95KU 0 14.53 11.35 12.75 0 6.7 5.24 5.88 0 6.15 4.8 5.39LE 0 44.56 37.55 40.76 0 40.93 34.5 37.44 0 38.34 32.31 35.07NA 1 41.18 33.62 37.02 1 39.57 32.31 35.58 1 33.16 27.07 29.81NU 0 68.1 34.5 45.8 0 68.1 34.5 45.8 0 62.93 31.88 42.32TC 8 5.17 20.96 8.3 3 7.31 6.99 7.14 8 2.8 11.35 4.49TH 1 19.34 35.81 25.11 1 19.34 35.81 25.11 1 11.08 20.52 14.4UD 1 8.07 24.89 12.19 1 1.98 6.11 2.99 0 0.0 0.0 0.0UI 0 43.9 39.3 41.47 2 45.98 34.93 39.7 0 41.46 37.12 39.17UT 5 39.38 27.51 32.39 5 35.63 24.89 29.31 6 30.54 22.27 25.76VA 3 11.71 15.28 13.26 3 10.7 13.97 12.12 3 6.02 7.86 6.82VT 5 9.82 6.99 8.16 5 9.82 6.99 8.16 5 7.98 5.68 6.63Table 10: Results after revisions, determiner errors onlyIn the end, we opted for the first alternative here,on the grounds that this is the best approximation tothe real task of non-native speaker error correction.The third alternative would also have been possible,but we were concerned about the impact on the sizeof our test data set that would result from carryingout this process across the board.
However, in the re-vision step described in Section 4.2, we did removeinstances of a particular error type, where a preposi-tion error was immediately followed by a verb error;consider the following sentence and its correction.
(1) a.
What do you do for trying to save the wildlife?b.
What do you do to try to save the wild life?The compound nature of these errors meant thatteams were unlikely to correct them; and it might beargued that they are not preposition errors in the con-ventional sense.
However, we did not remove theseinstances uniformly, so some still remain in the testdata.An orthogonal issue with regard to the HOO an-notation scheme is that we require precise identifi-cation of error locations and accurate specificationof these locations at a character-offset level in our61standoff edit notation.
It is often inaccuracies atthis level that contribute to the differences between ateam?s detection score and the corresponding recog-nition score.
While precise character offset infor-mation is important for some error correction tasks(for example, one would not want an automated cor-rector to insert corrections misplaced by one charac-ter), arguably it is too strict in the present circum-stances.
Dahlmeier and Ng (2012) propose an al-ternative evaluation scheme which, along with otherproperties, overcomes this by operating in terms oftokens rather than character offsets.6.4 SummaryOverall, we were immensely pleased with the levelof interest in this shared task.
The HOO 2012 train-ing data and evaluation tools are publicly available,so interested parties who did not take part in theshared task can still try their hand retrospectively;unfortunately, our contract with CUP means that thetest data used in this round is not publicly available.Our future plans include packaging a subset of theinitially held-out public FCE data set as a new testset, with the aim of establishing a standardised train-ing and testing setup in the same way as Section 23of the Wall Street Journal corpus is conventionallyused as a test set.
We have strongly encouraged theuse of publicly available data sets, and have askedteams to be as detailed as possible in their reports inthe interests of replicability; we hope this will makeit possible for new entrants to the area to get up tospeed quickly.Of course, the FCE data also supports work onmany other kinds of errors.
We expect to addresssubsets of these in future HOO rounds.AcknowledgementsWe?d like to acknowledge the kind assistance ofvarious people in making this shared task possi-ble: Ted Briscoe for seeding the enterprise by work-ing to make the FCE data publicly available; Di-ane Nicholls and Adam Kilgarriff for encourage-ment and advice along the way; Ann Fiddes at Cam-bridge University Press for providing the previouslyunseen test data; Richard Cox for statistics; andJoel Tetreault, Claudia Leacock and Jill Burstein foragreeing to host the shared task at the Building Ed-ucational Applications Workshop.
Macquarie Uni-versity provided financial support via a ResearchDevelopment Grant.
Finally, of course, we?d liketo thank all the teams for participating.ReferencesDaniel Dahlmeier and Hwee Tou Ng.
2012.
Better evalu-ation for grammatical error correction.
In Proceedingsof the 2012 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, Montre?al, Canada, 3rd?8th June 2012.Robert Dale and Adam Kilgarriff.
2010.
Helping OurOwn: Text massaging for computational linguisticsas a new shared task.
In Proceedings of the 6th In-ternational Natural Language Generation Conference,pages 261?266, Dublin, Ireland, 7th?9th July 2010.Robert Dale and Adam Kilgarriff.
2011.
Helping ourown: The HOO 2011 pilot shared task.
In Proceed-ings of the 13th European Workshop on Natural Lan-guage Generation, Nancy, France, 28th?30th Septem-ber 2011.Robert Dale and George Narroway.
2012.
A frame-work for evaluating text correction.
In Proceed-ings of the Eighth International Conference on Lan-guage Resources and Evaluation (LREC2012), Istan-bul, Turkey, 21st?27th May 2012.Michael Gamon.
2010.
Using mostly native data to cor-rect errors in learners writing.
In Proceedings of theEleventh Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics (NAACL), pages 163?171, Los Angeles, USA.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated Grammatical Er-ror Detection for Language Learners.
Synthesis Lec-tures on Human Language Technologies.
Morgan andClaypool.Diane Nicholls.
2003.
The Cambridge LearnerCorpus?error coding and analysis for lexicographyand ELT.
In D Archer, P Rayson, A Wilson, andT McEnery, editors, Proceedings of the Corpus Lin-guistics 2003 Conference, pages 572?581, Lancaster,UK, 29th March?2nd April 2001.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading esol texts.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, Portland, Ore-gon, USA, 19th?24th June 2011.62
