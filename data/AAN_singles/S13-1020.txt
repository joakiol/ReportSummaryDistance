Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 143?147, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUPC-CORE: What Can Machine Translation Evaluation Metrics andWikipedia Do for Estimating Semantic Textual Similarity?Alberto Barro?n-Ceden?o1,2 Llu?
?s Ma`rquez1 Maria Fuentes1 Horacio Rodr?
?guez1 Jordi Turmo11 TALP Research Center, Universitat Polite`cnica de CatalunyaJordi Girona Salgado 1?3, 08034, Barcelona, Spain2 Facultad de Informa?tica, Universidad Polite?cnica de MadridBoadilla del Monte, 28660 Madrid, Spainalbarron, lluism, mfuentes, horacio, turmo @lsi.upc.eduAbstractIn this paper we discuss our participation tothe 2013 Semeval Semantic Textual Similaritytask.
Our core features include (i) a set of met-rics borrowed from automatic machine trans-lation, originally intended to evaluate auto-matic against reference translations and (ii) aninstance of explicit semantic analysis, builtupon opening paragraphs of Wikipedia 2010articles.
Our similarity estimator relies on asupport vector regressor with RBF kernel.
Ourbest approach required 13 machine transla-tion metrics + explicit semantic analysis andranked 65 in the competition.
Our post-competition analysis shows that the featureshave a good expression level, but overfittingand ?mainly?
normalization issues causedour correlation values to decrease.1 IntroductionOur participation to the 2013 Semantic Textual Sim-ilarity task (STS) (Agirre et al 2013)1 was focusedon the CORE problem: GIVEN TWO SENTENCES,s1 AND s2, QUANTIFIABLY INFORM ON HOW SIMI-LAR s1 AND s2 ARE.
We considered real-valued fea-tures from four different sources: (i) a set of linguis-tic measures computed with the Asiya Toolkit forAutomatic MT Evaluation (Gime?nez and Ma`rquez,2010b), (ii) an instance of explicit semantic analy-sis (Gabrilovich and Markovitch, 2007), built on topof Wikipedia articles, (iii) a dataset predictor, and(iv) a subset of the features available in Takelab?sSemantic Text Similarity system (?Saric?
et al 2012).1http://ixa2.si.ehu.es/sts/Our approaches obtained an overall modest resultcompared to other participants (best position: 65 outof 89).
Nevertheless, our post-competition analysisshows that the low correlation was caused mainly bya deficient data normalization strategy.The paper distribution is as follows.
Section 2 of-fers a brief overview of the task.
Section 3 describesour approach.
Section 4 discuss our experiments andobtained results.
Section 5 provides conclusions.2 Task OverviewDetecting two similar text fragments is a difficulttask in cases where the similarity occurs at seman-tic level, independently of the implied lexicon (e.gin cases of dense paraphrasing).
As a result, simi-larity estimation models must involve features otherthan surface aspects.
The STS task is proposed asa challenge focused in short English texts of dif-ferent nature: from automatic machine translationalternatives to human descriptions of short videos.The test partition also included texts extracted fromnews headlines and FrameNet?Wordnet pairs.The range of similarity was defined between 0(no relation) up to 5 (semantic equivalence).
Thegold standard values were averaged from differenthuman-made annotations.
The expected system?soutput was composed of a real similarity value, to-gether with an optional confidence level (our confi-dence level was set constant).Table 1 gives an overview of the development(2012 training and test) and test datasets.
Notethat both collections extracted from SMT data arehighly biased towards the maximum similarity val-ues (more than 75% of the instances have a similar-143Table 1: Overview of sub-collections in the development and test datasets, including number of instances and distri-bution of similarity values (in percentage) as well as mean, minimum, and maximum lengths.similarity distribution lengthdataset instances [0, 1) [1, 2) [2, 3) [3, 4) [4, 5] mean min maxdev-[train + test]MSRpar 1,500 1.20 8.13 17.13 48.73 24.80 17.84 5 30MSRvid 1,500 31.00 14.13 15.47 20.87 18.53 6.66 2 24SMTEuroparl 1,193 0.67 0.42 1.17 12.32 85.4 21.13 1 72OnWN 750 2.13 2.67 10.40 25.47 59.33 7.57 1 34SMTnews 399 1.00 0.75 5.51 13.03 79.70 11.72 2 28testheadlines 750 15.47 22.00 16.27 24.67 21.60 7.21 3 22OnWN 561 36.54 9.80 7.49 17.11 29.05 7.17 5 22FNWN 189 34.39 29.63 28.57 6.88 0.53 19.90 3 71SMT 750 0.00 0.27 3.47 20.40 75.87 26.40 1 96ity higher than 4) and include the longest instances.On the other hand, the FNWN instances are shiftedtowards low similarity levels (more than 60% have asimilarity lower than 2).3 ApproachOur similarity assessment model relies uponSVMlight?s support vector regressor, with RBF ker-nel (Joachims, 1999).2 Our model estimation pro-cedure consisted of two steps: parameter defini-tion and backward elimination-based feature selec-tion.
The considered features belong to four fami-lies, briefly described in the following subsections.3.1 Machine Translation Evaluation MetricsWe consider a set of linguistic measures originallyintended to evaluate the quality of automatic trans-lation systems.
These measures compute the qualityof a translation by comparing it against one or sev-eral reference translations, considered as gold stan-dard.
A straightforward application of these mea-sures to the problem at hand is to consider s1 as thereference and s2 as the automatic translation, or viceversa.
Some of the metrics are not symmetric so wecompute similarity between s1 and s2 in both direc-tions and average the resulting scores.The measures are computed with the AsiyaToolkit for Automatic MT Evaluation (Gime?nez andMa`rquez, 2010b).
The only pre-processing carriedout was tokenization (Asiya performs additional in-box pre-processing operations, though).
We consid-2We also tried with linear kernels, but RBF always obtainedbetter results.ered a sample from three similarity families, whichwas proposed in (Gime?nez and Ma`rquez, 2010a) asa varied and robust metric set, showing good corre-lation with human assessments.3Lexical Similarity Two metrics of TranslationError Rate (Snover et al 2006) (i.e.
the esti-mated human effort to convert s1 into s2): -TERand -TERpA.
Two measures of lexical precision:BLEU (Papineni et al 2002) and NIST (Dod-dington, 2002).
One measure of lexical recall:ROUGEW (Lin and Och, 2004).
Finally, four vari-ants of METEOR (Banerjee and Lavie, 2005) (exact,stemming, synonyms, and paraphrasing), a lexicalmetric accounting for F -Measure.Syntactic Similarity Three metrics that estimatethe similarity of the sentences over dependencyparse trees (Liu and Gildea, 2005): DP-HWCMic-4for grammatical categories chains, DP-HWCMir-4over grammatical relations, and DP-Or(?)
overwords ruled by non-terminal nodes.
Also, one mea-sure that estimates the similarity over constituentparse trees: CP-STM4 (Liu and Gildea, 2005).Semantic Similarity Three measures that esti-mate the similarities over semantic roles (i.e.
ar-guments and adjuncts): SR-Or, SR-Mr(?
), andSR-Or(?).
Additionally, two metrics that es-timate similarities over discourse representations:DR-Or(?)
and DR-Orp(?
).3Asiya is available at http://asiya.lsi.upc.edu.Full descriptions of the metrics are available in the Asiya Tech-nical Manual v2.0, pp.
15?21.1443.2 Explicit Semantic AnalysisWe built an instance of Explicit Semantic Analy-sis (ESA) (Gabrilovich and Markovitch, 2007) withthe first paragraph of 100k Wikipedia articles (dumpfrom 2010).Pre-processing consisted of tokenizationand lemmatization.3.3 Dataset PredictionGiven the similarity shifts in the different datasets(cf.
Table 1), we tried to predict what dataset an in-stance belonged to on the basis of its vocabulary.
Webuilt binary maxent classifiers for each dataset in thedevelopment set, resulting in five dataset likelihoodfeatures: dMSRpar, dSMTeuroparl, dMSRvid,dOnWN, and dSMTnews.4 Pre-processing consistedof tokenization and lemmatization.3.4 BaselineWe considered the features included in the TakelabSemantic Text Similarity system (?Saric?
et al 2012),one of the top-systems in last year competition.
Thissystem is used as a black box.
The resulting featuresare named tklab n, where n = [1, 21].Our runs departed from three increasing subsetsof features: AE machine translation evaluation met-rics and explicit semantic analysis, AED the pre-vious set plus dataset prediction, and AED T theprevious set plus Takelab?s baseline features (cf.
Ta-ble 3).
We performed a feature normalization, whichrelied on the different feature?s distribution over theentire dataset.
Firstly, features were bounded in therange ??3?
?2 in order to reduce the potentially neg-ative impact of outliers.
Secondly, we normalizedaccording to the z-score (Nardo et al 2008, pp.
28,84); i.e.
x = (x ?
?)/?.
As a result, each real-valued feature distribution in the dataset has ?
= 0and ?
= 1.
During the model tuning stage we triedwith other numerous normalization options: normal-izing each dataset independently, together with thetraining set, and without normalization at all.
Nor-malizing according to the entire dev-test dataset ledto the best results4We used the Stanford classifier; http://nlp.stanford.edu/software/classifier.shtmlTable 2: Tuning process: parameter definition and featureselection.
Number of features at the beginning and endof the feature selection step included.run parameter def.
feature sel.c ?
?
corr b e corrAE 3.7 0.06 0.3 0.8257 19 14 0.8299AED 3.8 0.03 0.2 0.8413 24 19 0.8425AED T 2.9 0.02 0.3 0.8761 45 33 0.88034 Experiments and ResultsSection 4.1 describes our model tuning strategy.Sections 4.2 and 4.3 discuss the official and post-competition results.4.1 Model TuningWe used only the dev-train partition (2012 training)for tuning.
By means of a 10-fold cross validationprocess, we defined the trade-off (c), gamma (?
),and tube width (?)
parameters for the regressor andperformed a backward-elimination feature selectionprocess (Witten and Frank, 2005, p. 294), indepen-dently for the three experiments.The results for the cross-validation process aresummarized in Table 2.
The three runs allow for cor-relations higher than 0.8.
On the one hand, the bestregressor parameters obtain better results as morefeatures are considered, still with very small differ-ences.
On the other hand, the low correlation in-crease after the feature selection step shows that afew features are indeed irrelevant.A summary of the features considered in each ex-periment (also after feature selection) is displayed inTable 3.
The correlation obtained over the dev-testpartition are corrAE = 0.7269, corrAED = 0.7638,and corrAEDT = 0.8044 ?it would have appearedin the top-10 ranking of the 2012 competition.4.2 Official ResultsWe trained three new regressors with the featuresconsidered relevant by the tuning process, but usingthe entire development dataset.
The test 2013 parti-tion was normalized again by means of z-score, con-sidering the means and standard deviations of the en-tire test dataset.
Table 4 displays the official results.Our best approach ?AE?, was positioned in rank65.
The worst results of run AED can be explainedby the difference in the nature of the test respect to145Table 3: Features considered at the beginning of each run, represented as empty squares ().
Filled squares ()represent features considered relevant after feature selection.Feature AE AED AED T Feature AE AED AED T Feature AED TDP-HWCM c-4    METEOR-pa    tklab 7 DP-HWCM r-4    METEOR-st    tklab 8 DP-Or(*)    METEOR-sy    tklab 9 CP-STM-4    ESA    tklab 10 SR-Or(*)    dMSRpar   tklab 11 SR-Mr(*)    dSMTeuroparl   tklab 12 SR-Or    dMSRvid   tklab 13 DR-Or(*)    dOnWN   tklab 14 DR-Orp(*)    dSMTnews   tklab 15 BLEU    tklab 1  tklab 16 NIST    tklab 2  tklab 17 -TER    tklab 3  tklab 18 -TERp-A    tklab 4  tklab 19 ROUGE-W    tklab 5  tklab 20 METEOR-ex    tklab 6  tklab 21 Table 4: Official results for the three runs (rank included).run headlines OnWN FNWN SMT meanAE (65) 0.6092 0.5679 -0.1268 0.2090 0.4037AED (83) 0.4136 0.4770 -0.0852 0.1662 0.3050AED T (72) 0.5119 0.6386 -0.0464 0.1235 0.3671the development dataset.
AED T obtains worst re-sults than AE on the headlines and SMT datasets.The reason behind this behavior can be in the dif-ference of vocabularies respect to that stored in theTakelab system (it includes only the vocabulary ofthe development partition).
This could be the samereason behind the drop in performance with respectto the results previously obtained on the dev-test par-tition (cf.
Section 4.1).4.3 Post-Competition ResultsOur analysis of the official results showed the mainissue was normalization.
Thus, we performed amanifold of new experiments, using the same con-figuration as in run AE, but applying other normal-ization strategies: (a) z-score normalization, but ig-noring the FNWN dataset (given its shift throughlow values); (b) z-score normalization, but consid-ering independent means and standard deviations foreach test dataset; and (c) without normalizing any ofdataset (including the regressor one).Table 5 includes the results.
(a) makes evidentthat the instances in FNWN represent ?anomalies?that harm the normalized values of the rest of sub-sets.
Run (b) shows that normalizing the test setsTable 5: Post-competition experiments resultsrun headlines OnWN FNWN SMT meanAE (a) 0.6210 0.5905 -0.0987 0.2990 0.4456AE (b) 0.6072 0.4767 -0.0113 0.3236 0.4282AE (c) 0.6590 0.6973 0.1547 0.3429 0.5208independently is not a good option, as the regressoris trained considering overall normalizations, whichexplains the correlation decrease.
Run (c) is com-pletely different: not normalizing any dataset ?both in development and test?
reduces the influ-ence of the datasets to each other and allows for thebest results.
Indeed, this configuration would haveadvanced practically forty positions at competitiontime, locating us in rank 27.Estimating the adequate similarities over FNWNseems particularly difficult for our systems.
We ob-serve two main factors.
(i) FNWN presents an im-portant similarity shift respect to the other datasets:nearly 90% of the instances similarity is lower than2.5 and (ii) the average lengths of s1 and s2 are verydifferent: 30 vs 9 words.
These characteristics madeit difficult for our MT evaluation metrics to estimateproper similarity values (be normalized or not).We performed two more experiments overFNWN: training regressors with ESA as the onlyfeature, before and after normalization.
The correla-tion was 0.16017 and 0.3113, respectively.
That is,the normalization mainly affects the MT features.1465 ConclusionsIn this paper we discussed on our participation to the2013 Semeval Semantic Textual Similarity task.
Ourapproach relied mainly upon a combination of au-tomatic machine translation evaluation metrics andexplicit semantic analysis.
Building an RBF supportvector regressor with these features allowed us for amodest result in the competition (our best run wasranked 65 out of 89).AcknowledgmentsWe would like to thank the organizers of this chal-lenging task for their efforts.This research work was partially carried out dur-ing the tenure of an ERCIM ?Alain Bensoussan?Fellowship.
The research leading to these results re-ceived funding from the EU FP7 Programme 2007-2013 (grants 246016 and 247762).
Our researchwork is partially supported by the Spanish researchprojects OpenMT-2 and SKATER (TIN2009-14675-C03, TIN2012-38584-C06-01).ReferencesEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 SharedTask: Semantic Textual Similarity, including a Pilot onTyped-Similarity.
In *SEM 2013: The Second JointConference on Lexical and Computational Semantics.Association for Computational Linguistics.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Gold-stein et al(Goldstein et al 2005), pages 65?72.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-Gram Co-occurrence Statistics.
In Proceedings of the SecondInternational Conference on Human Language Tech-nology Research, pages 138?145, San Francisco, CA.Morgan Kaufmann Publishers Inc.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting Semantic Relatedness Using Wikipedia-basedExplicit Semantic Analysis.
In Proceedings of the20th International Joint Conference on Artificial Intel-ligence, pages 1606?1611, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010a.
Asiya:An Open Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathemat-ical Linguistics, (94).Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010b.
LinguisticMeasures for Automatic Machine Translation Evalua-tion.
Machine Translation, 24(3?4):209?240.Jade Goldstein, Alon Lavie, Chin-Yew Lin, and ClareVoss, editors.
2005.
Proceedings of the ACL Work-shop on Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation and/or Summarization.
Asso-ciation for Computational Linguistics.Thorsten Joachims, 1999.
Advances in Kernel Methods ?Support Vector Learning, chapter Making large-ScaleSVM Learning Practical.
MIT Press.Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic Evaluation of Machine Translation Quality Us-ing Longest Common Subsequence and Skip-BigramStatistics.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics(ACL 2002), Stroudsburg, PA. Association for Com-putational Linguistics.Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
In Goldsteinet al(Goldstein et al 2005), pages 25?32.Michela Nardo, Michaela Saisana, Andrea Saltelli, Ste-fano Tarantola, Anders Hoffmann, and Enrico Giovan-nini.
2008.
Handbook on Constructing Composite In-dicators: Methodology and User Guide.
OECD Pub-lishing.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics (ACL 2002), pages 311?318,Philadelphia, PA. Association for Computational Lin-guistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of Association for MachineTranslation in the Americas, pages 223?231.Frane ?Saric?, Goran Glavas?, Mladen Karan, Jan ?Snajder,and Bojana Dalbelo Bas?ic?.
2012.
TakeLab: Sys-tems for Measuring Semantic Text.
In First JointConference on Lexical and Computational Semantics(*SEM), pages 441?448, Montre?al, Canada.
Associa-tion for Computational Linguistics.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann, San Francisco, CA, 2 edition.147
