Proceedings of NAACL HLT 2007, Companion Volume, pages 161?164,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsSimultaneous Identification of Biomedical Named-Entity andFunctional Relations Using Statistical Parsing Techniques ?Zhongmin Shi and Anoop Sarkar and Fred PopowichSchool of Computing ScienceSimon Fraser University{zshi1,anoop,popowich}@cs.sfu.caAbstractIn this paper we propose a statistical pars-ing technique that simultaneously iden-tifies biomedical named-entities (NEs)and extracts subcellular localization re-lations for bacterial proteins from thetext in MEDLINE articles.
We builda parser that derives both syntactic anddomain-dependent semantic informationand achieves an F-score of 48.4% for therelation extraction task.
We then proposea semi-supervised approach that incor-porates noisy automatically labeled datato improve the F-score of our parser to83.2%.
Our key contributions are: learn-ing from noisy data, and building an an-notated corpus that can benefit relation ex-traction research.1 IntroductionRelation extraction from text is a step beyondNamed-Entity Recognition (NER) and generally de-mands adequate domain knowledge to build rela-tions among domain-specific concepts.
A Biomedi-cal Functional Relation (relation for short) states in-teractions among biomedical substances.
In this pa-per we focus on one such relation: Bacterial ProteinLocalization (BPL), and introduce our approach foridentifying BPLs from MEDLINE1 articles.BPL is a key functional characteristic of pro-teins.
It is essential to the understanding of the func-tion of different proteins and the discovery of suit-able drugs, vaccines and diagnostic targets.
We arecollaborating with researchers in molecular biologywith the goal of automatically extracting BPLs from?This research was partially supported by NSERC, Canada.1MEDLINE is a bibliographic database of biomedicalscientific articles at National Library of Medcine (NLM,http://www.nlm.nih.gov/).text with BioNLP techniques, to expand their pro-tein localization database, namely PSORTdb2(Reyet al, 2005).
Specifically, the task is to produce asoutput the relation tuple BPL(BACTERIUM, PRO-TEIN, LOCATION) along with source sentence anddocument references.
The task is new to BioNLPin terms of the specific biomedical relation beingsought.
Therefore, we have to build annotated cor-pus from scratch and we are unable to use existingBioNLP shared task resources in our experiments.In this paper we extract from the text of biomedicalarticles a relation among: a LOCATION (one of thepossible locations shown in Figure 1 for Gram+ andGram- bacteria); a particular BACTERIUM, e.g.
E.Coli, and a PROTEIN name, e.g.
OprF.
(Nair and Rost, 2002) used the text taken fromSwiss-Prot annotations of proteins, and trained asubcellular classifier on this data.
(Hoglund et al,2006) predicted subcellular localizations using anSVM trained on both text and protein sequence data,by assigning each protein name a vector based onterms co-occurring with the localization name foreach organism.
(Lu and Hunter, 2005) applied a hi-erarchical architecture of SVMs to predict subcel-lular localization by incorporating a semantic hier-archy of localization classes modeled with biolog-ical processing pathways.
These approaches eitherignore the actual location information in their pre-dicted localization relations, or only focus on a smallportion of eukaryotic proteins.
The performance ofthese approaches are not comparable due to differenttasks and datasets.2 System OutlineDuring our system?s preprocessing phase, sentencesare automatically annotated with both syntactic in-formation and domain-specific semantic informa-tion.
Syntactic annotations are provided by a statis-tical parser (Charniak and Johnson, 2005).
Domain-2http://db.psort.org.161cytoplasm cytoplasmGram+ Gram-cytoplasmicmembranecell wallperiplasmoutermembrane secretedinnermembraneFigure 1: Illustration of possible locations of pro-teins with respect to the bacterial cell structure.specific semantic information includes annotationson PROTEIN, BACTERIUM and LOCATION NEsby dictionary lookups from UMLS3, NCBI Taxon-omy4 and SwissProt5, and two automatic Bio-NErecognizers: MMTx6 and Lingpipe7.We propose the use of a parser that simultane-ously identifies NEs and extracts the BPL relationsfrom each sentence.
We define NEs to be Relevantto each other only if they are arguments of a BPL re-lation, otherwise they are defined to be Irrelevant.A sentence may contain multiple PROTEIN (LO-CATION or ORGANISM) NEs, e.g., there are twoPROTEIN NEs in the sentence below but only one,OmpA, is relevant.
Our system aims to identify thecorrect BPL relation among all possible BPL tuples(candidate relations) in the sentence by only recog-nizing relevant NEs.
Each input sentence is assumedto have at least one BPL relation.Nine of 10 monoclonal antibodies mapped within the carboxy-terminal region of [PROTEIN OprF] that is homologous tothe [ORGANISM Escherichia coli] [LOCATION outer membrane]protein [PROTEIN OmpA].3 Statistical Syntactic and Semantic ParserSimilar to the approach in (Miller et al, 2000) and(Kulick et al, 2004), our parser integrates both syn-tactic and semantic annotations into a single annota-tion as shown in Figure 2.
A lexicalized statisticalparser (Bikel, 2004) is applied to the parsing task.The parse tree is decorated with two types of seman-3http://www.nlm.nih.gov/research/umls/4http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=Taxonomy5http://www.ebi.ac.uk/swissprot/6MetaMap Transfer, http://mmtx.nlm.nih.gov/7http://www.alias-i.com/PO_LNK/NPPO_PTR/PP PO_PTR/PPDT PO_PTR/NP PRN NNThe PROTEIN_R/NPPROTEIN_R/JJphospholipasePROTEIN_R/NNPC-LRB--LRB-NPNNPPLCgeneINofPO_PTR/NPORGANISM_R/NPORGANISM_R/NNP ORGANISM_R/NNPPseudomonas aeruginosa-RRB--RRB-Figure 2: An example of parsing resultstic annotations:1) Annotations on relevant PROTEIN, BAC-TERIUM and LOCATION NEs.
Tags are PRO-TEIN R, BACTERIUM R and LOCATION R respec-tively.2) Annotations on paths between relevant NEs.
Thelower-most node that spans both NEs is tagged asLNK and all nodes along the path to the NEs aretagged as PTR.Binary relations are apparently much easier torepresent on the parse tree, therefore we split theBPL ternary relation into two binary relations: BP(BACTERIUM and PROTEIN) and PL (PROTEINand LOCATION).
After capturing BP and PL rela-tions, we will predict BPL as a fusion of BP and PL,see ?4.1.
In contrast to the global inference done us-ing our generative model, heavily pipelined discrim-inative approaches usually have problems with errorpropagation.
A more serious problem in a pipelinedsystem when using syntactic parses for relation ex-traction is the alignment between the named enti-ties produced by a separate system and the syntac-tic parses produced by the statistical parser.
Thisalignment issue is non-trivial and we could not pro-duce a pipelined system that dealt with this issuesatisfactorily for our dataset.
As a result, we didnot directly compare our generative approach to apipelined strategy.4 Experiment Settings and EvaluationsThe training and test sets are derived from a smallexpert-curated corpus.
Table 1 lists numbers of sen-tences and relevant NEs in each BP/PL/BPL set.Since the parsing results include both NE and pathtags (note that we do not use any external NER sys-tem), there are two metrics to produce and evalu-ate PL or BP relations: Name-only and Name-pathmetrics.
The name-only metric only measures Rel-162PL BP BPLTraining set 289 / 605 258 / 595 352 / 852Test set 44 / 134 28 / 127 62 / 182Table 1: Sizes of training and test sets (number ofsentences / number of relevant NEs)evant PROTEIN, BACTERIUM and LOCATIONNEs (see Section 2).
It does not take path annota-tions into account.
The name-only metric is mea-sured in terms of Precision, Recall and F-score, inwhich True Positive (TP ) is the number of correctlyidentified NEs, False Positive (FP ) is the number ofincorrectly identified NEs and False Negative (FN )is the number of correct NEs that are not identified.The name-path measures nodes being annotatedas LNK, PTR or R along the path between NEson the parse tree, therefore it represents confidenceof NEs being arguments of the relation.
The name-path metric is a macro-average measure, which isthe average performance of all sentences in data set.In measurement of the name-path metric, TP is thenumber of correctly annotated nodes on the path be-tween relevant NEs.
FP is the number of incor-rectly annotated nodes on the path and FN is thenumber of correct nodes that are not identified.4.1 Fusion of BP and PLThe BPL relation can be predicted by a fusion ofBP and PL once they are extracted.
Specifically, aBP and a PL that are extracted from the same sen-tence are merged into a BPL.
The predicted BPLrelations are then evaluated by the same name-onlyand name-path metrics as for binary relations.
In thename-path metric, nodes on both PL and BP pathsare counted.
Note that we do not need a commonprotein NER to merge the BP and PL relations.
E.g.,for name-only evaluation, assume true BPL(B1, P1,L1): if we predict BP(B1, ) and PL(P1, L2), thenTP=2 due to B1, P1; FP=1 due to L2; and FN=1due to P1.5 NER and BPL ExtractionBaseline: An intuitive method for relation extrac-tion would assume that any sentence containingPROTEIN, ORGANISM and LOCATION NEs hasthe relation.
We employ this method as a baselinesystem, in which NEs are identified by the auto-matic NE recognizers and dictionary lookups as in-troduced in ?2.
The system is evaluated against thetest set in Table 1.
Results in Table 2 show low pre-cision for PROTEIN NER and the name-path metric.Extraction using Supervised Parsing: We first ex-periment a fully supervised approach by training theparser on the BP/PL training set and evaluate on thetest set (see Table 1).
The name-only and name-pathevaluation results in Table 2 show poor syntacticparsing annotation quality and low recall on PRO-TEIN NER.
The major reason of these problems isthe lack of training data.Extraction using Semi-supervised Parsing: Ex-periments with purely supervised learning show thatour generative model requires a large curated setto minimize the sparse data problem, but domain-specific annotated corpora are always rare and ex-pensive.
However, there is a huge source of unla-beled MEDLINE articles available that may meetour needs, by assuming that any sentence contain-ing BACTERIUM, PROTEIN and LOCATION NEshas the BPL relation.
We then choose such sentencesfrom a subset of the MEDLINE database as thetraining data.
These sentences, after being parsedand BPL relations inserted, are in fact the very noisydata when used to train the parser, since the assumedrelations do not necessarily exist.
The reason thisnoisy data works at all is probably because we canlearn a preference for structural relations betweenentities that are close to each other in the sentence,and thus distinguish between competing relations inthe same sentence.
In future work, we hope to ex-plore explicit bootstrapping from the labeled data toimprove the quality of the noisy data.Two experiments were carried out correspondingto choices of the training set: 1) noisy data only, 2)noisy data and curated training data.
Evaluation re-sults given in Table 2.Evaluation results on the name-only metric showthat, compared to supervised parsing, our semi-supervised method dramatically improves recall forNER.
For instance, recall for PROTEIN NER in-creases from 25.0% to 81.3%; recall on BAC-TERIUM and LOCATION NERs increases about30%.
As for the name-path metric, the over-all F-score is much higher than our fully super-vised method increasing from 39.9% to 74.5%.
Itshows that the inclusion of curated data in the semi-163Name-only Evaluation (%) Name-Path Evaluation (%)Method Measure PL BP BPL PL BP BPLPROT LOC PROT BACTP 42.3 78.6 41.9 81.3 40.7 27.1 38.9 31.0Baseline R 92.5 97.3 87.8 97.4 90.9 56.5 69.0 60.7F 58.0 87.0 56.7 88.6 56.2 36.6 49.8 41.0Supervised P 66.7 87.5 66.7 72.7 76.9 45.9 41.2 43.9(training data R 25.0 56.0 10.5 47.1 35.3 36.7 36.3 36.5only) F 36.4 68.3 18.2 57.1 48.4 40.8 38.6 39.9Semi-supervised P 66.7 95.5 70.6 94.1 80.8 76.2 83.5 79.3(noisy data R 84.2 80.8 80.0 84.2 81.8 67.8 72.4 67.0only) F 74.4 87.5 75.0 88.9 81.3 71.7 77.5 74.2Semi-supervised P 73.9 95.5 76.5 94.1 84.8 77.0 81.1 78.7(noisy data + R 81.0 80.8 81.3 84.2 81.7 68.5 73.7 70.7training data) F 77.3 87.5 78.8 88.9 83.2 72.5 77.2 74.5Table 2: Name-only and name-path evaluation results.
PROTEIN, LOCATION and BACTERIUM arePROT, LOC and BACT for short.
The training data is the subset of curated data in Table 1.supervised method does not improve performancemuch.
Precision of PROTEIN NER increases 6.5%on average, while F-score of overall BPL extractionincreases only slightly.
We experimented with train-ing the semi-supervised method using noisy dataalone, and testing on the entire curated set, i.e., 333and 286 sentences for BP and PL extractions respec-tively.
Note that we do not directly train from thetraining set in this method, so it is still ?unseen?
datafor this model.
The F-scores of path-only and path-name metrics are 75.5% and 67.1% respectively.6 Discussion and Future WorkIn this paper we introduced a statistical parsing-based method to extract biomedical relations fromMEDLINE articles.
We made use of a large un-labeled data set to train our relation extractionmodel.
Experiments show that the semi-supervisedmethod significantly outperforms the fully super-vised method with F-score increasing from 48.4%to 83.2%.
We have implemented a discriminativemodel (Liu et al, 2007) which takes as input the ex-amples with gold named entities and identifies BPLrelations on them.
In future work, we plan to let thediscriminative model take the output of our parserand refine our current results further.
We also planto train a graphical model based on all extracted BP,PL and BPL relations to infer relations from multi-ple sentences and documents.ReferencesD.
Bikel.
2004.
A distributional analysis of a lexicalized statis-tical parsing model.
In Proc.
of EMNLP ?04, pages 182?189.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best pars-ing and maxent discriminative reranking.
In Proc.
of ACL?05, pages 173?180.A.
Hoglund, T. Blum, S. Brady, P. Donnes, J. Miguel,M.
Rocheford, O. Kohlbacher, and H. Shatkay.
2006.
Sig-nificantly improved prediction of subcellular localization byintegrating text and protein sequence data.
In Proc.
of PSB?06, volume 11, pages 16?27.S.
Kulick, A. Bies, M. Libeman, M. Mandel, R. McDonald,M.
Palmer, A. Schein, and L. Ungar.
2004.
Integrated an-notation for biomedical information extraction.
In Proc.
ofHLT/NAACL ?04, pages 61?68, Boston, May.Y.
Liu, Z. Shi, and A. Sarkar.
2007.
Exploiting rich syntacticinformation for relation extraction from biomedical articles.In NAACL-HLT ?07, poster track, Rochester, NY, April.Z.
Lu and L. Hunter.
2005.
Go molecular function terms arepredictive of subcellular localization.
In Proc.
of PSB ?05,volume 10, pages 151?161.S.
Miller, H. Fox, L. Ramshaw, and R. Weischedel.
2000.
Anovel use of statistical parsing to extract information fromtext.
In Proc.
of NAACL ?06, pages 226?233.R.
Nair and B. Rost.
2002.
Inferring subcellular localizationthrough automated lexical analysis.
In Bioinformatics, vol-ume 18, pages 78?86.S.
Rey, M. Acab, J. Gardy, M. Laird, K. deFays, C. Lam-bert, and F. Brinkman.
2005.
Psortdb: A database of sub-cellular localizations for bacteria.
Nucleic Acids Research,33(D):164?168.164
