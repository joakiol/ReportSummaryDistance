Proceedings of NAACL HLT 2007, pages 180?187,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsLexicalized Markov Grammars for Sentence Compression?Michel Galley and Kathleen R. McKeownColumbia UniversityDepartment of Computer ScienceNew York, NY 10027, USA{galley,kathy}@cs.columbia.eduAbstractWe present a sentence compression system basedon synchronous context-free grammars (SCFG),following the successful noisy-channel approachof (Knight and Marcu, 2000).
We define a head-driven Markovization formulation of SCFG dele-tion rules, which allows us to lexicalize probabili-ties of constituent deletions.
We also use a robustapproach for tree-to-tree alignment between arbi-trary document-abstract parallel corpora, which letsus train lexicalized models with much more datathan previous approaches relying exclusively onscarcely available document-compression corpora.Finally, we evaluate different Markovized models,and find that our selected best model is one that ex-ploits head-modifier bilexicalization to accuratelydistinguish adjuncts from complements, and thatproduces sentences that were judged more gram-matical than those generated by previous work.1 IntroductionSentence compression addresses the problem of re-moving words or phrases that are not necessaryin the generated output of, for instance, summa-rization and question answering systems.
Giventhe need to ensure grammatical sentences, a num-ber of researchers have used syntax-directed ap-proaches that perform transformations on the out-put of syntactic parsers (Jing, 2000; Dorr et al,2003).
Some of them (Knight and Marcu, 2000;Turner and Charniak, 2005) take an empirical ap-proach, relying on formalisms equivalent to proba-bilistic synchronous context-free grammars (SCFG)?This material is based on research supported in partby the U.S. National Science Foundation (NSF) under GrantNo.
IIS-05-34871 and the Defense Advanced Research ProjectsAgency (DARPA) under Contract No.
HR0011-06-C-0023.Any opinions, findings and conclusions or recommendationsexpressed in this material are those of the authors and do notnecessarily reflect the views of the NSF or DARPA.
(Lewis and Stearns, 1968; Aho and Ullman, 1969) toextract compression rules from aligned Penn Tree-bank (PTB) trees.
While their approach proved suc-cessful, their reliance on standard maximum like-lihood estimators for SCFG productions results inconsiderable sparseness issues, especially given therelative flat structure of PTB trees; in practice, manySCFG productions are seen only once.
This problemis exacerbated for the compression task, which hasonly scarce training material available.In this paper, we present a head-drivenMarkovization of SCFG compression rules, anapproach that was successfully used in syntacticparsing (Collins, 1999; Klein and Manning, 2003)to alleviate issues intrinsic to relative frequencyestimation of treebank productions.
Markovizationfor sentence compression provides several benefits,including the ability to condition deletions ona flexible amount of syntactic context, to treathead-modifier dependencies independently, and tolexicalize SCFG productions.Another part of our effort focuses on better align-ment models for extracting SCFG compression rulesfrom parallel data, and to improve upon (Knightand Marcu, 2000), who could only exploit 1.75% ofthe Ziff-Davis corpus because of stringent assump-tions about human abstractive behavior.
To alleviatetheir restrictions, we rely on a robust approach foraligning trees of arbitrary document-abstract sen-tence pairs.
After accounting for sentence pairs withboth substitutions and deletions, we reached a reten-tion of more than 25% of the Ziff-Davis data, whichgreatly benefited the lexical probabilities incorpo-rated into our Markovized SCFGs.Our work provides three main contributions:180(1) Our lexicalized head-driven Markovizationyields more robust probability estimates, and ourcompressions outperform (Knight and Marcu, 2000)according to automatic and human evaluation.
(2) We provide a comprehensive analysis of the im-pact of different Markov orders for sentence com-pression, similarly to a study done for PCFGs (Kleinand Manning, 2003).
(3) We provide a frameworkfor exploiting document-abstract sentence pairs thatare not purely compressive, and augment the avail-able training resources for syntax-directed sentencecompression systems.2 Synchronous Grammars for SentenceCompressionOne successful syntax-driven approach (Knight andMarcu, 2000, henceforth K&M) relies on syn-chronous context-free grammars (SCFG) (Lewisand Stearns, 1968; Aho and Ullman, 1969).
SCFGscan be informally defined as context-free grammars(CFGs) whose productions have two right-hand sidestrings instead of one, namely source and targetright-hand side.
In the case of sentence compres-sion, we restrict the target side to be a sub-sequenceof the source side (possibly identical), and we willcall this restricted grammar a deletion SCFG.
For in-stance, a deletion SCFG rule that removes an adver-bial phrase (ADVP) between an noun phrase (NP)and a verb phrase (VP) may be written as follows:S ?
?NP ADVP VP, NP VP?In a sentence compression framework similar tothe one presented by K&M, we build SCFGs thatare fully trainable from a corpus of document andreduced sentences.
Such an approach comprisestwo subproblems: (1) transform tree pairs into syn-chronous grammar derivations; (2) based on thesederivations, assign probabilities to deletion SCFGproductions, and more generally, to compressionsproduced by such grammars.
Since the main point ofour paper lies in the exploration of better probabilityestimates through Markovization and lexicalizationof SCFGs, we first address the latter problem, anddiscuss the task of building synchronous derivationsonly later in Section 4.2.1 Stochastic Synchronous GrammarsThe overall goal of a sentence compression system isto transform a given input sentence f into a conciseand grammatical sentence c ?
C, which is a sub-sequence of f .
Similarly to K&M and many suc-cessful syntactic parsers (Collins, 1999; Klein andManning, 2003), our sentence compression systemis generative, and attempts to find the optimal com-pression c?
by estimating the following function:1c?
= argmaxc?C{p(c|f)}= argmaxc?C{p(f , c)}(1)If ?
(f , c) is the set of all tree pairs that yield (f , c)according to some underlying SCFG, we can esti-mate the probability of the sentence pair using:p(f , c) =?
(pif ,pic)??
(f ,c)P (pif , pic) (2)We note that, in practice (and as in K&M), Equa-tion 2 is often approximated by restricting ?
(f , c)to a unique full tree p?if , the best hypothesis of anoff-the-shelf syntactic parser.
This implies that eachpossible compression c is the target-side yield of atmost one SCFG derivation.As in standard PCFG history-based models, theprobability of the entire structure (Equation 2) is fac-tored into probabilities of grammar productions.
If?
is a derivation ?
= r1 ?
?
?
?
?
rj ?
?
?
?
rJ , whererj denotes the SCFG rule lj ?
?
?jf , ?jc?, we get:p(pif , pic) =J?j=1p(?jf , ?jc|lj) (3)The question we will now address is how to esti-mate the probability p(?jf , ?jc|lj) of each SCFG pro-duction.2.2 Lexicalized Head-Driven Markovization ofSynchronous GrammarsA main issue in our enterprise is to reliably estimateproductions of deletion SCFGs.
In a sentence com-pression framework as the one presented by K&M,we use aligned trees of the form of the Penn Tree-bank (PTB) (Marcus et al, 1994) to acquire andscore SCFG productions.
However, the use of thePTB structure faces many challenges also encoun-tered in probabilistic parsing.1In their noisy-channel approach, K&M further break downp(c, f) into p(f |c) ?
p(c), which we refrain from doing for rea-sons that will become obvious later.181Firstly, PTB tree structures are relatively flat, par-ticularly within noun phrases.
For instance, adjec-tive phrases (ADJP)?which are generally good can-didates for deletions?appear in 90 different NP-rooted SCFG productions in Ziff-Davis,2 61 ofwhich appear only once, e.g., NP ?
?DT ADJP JJNN NN, DT JJ NN NN?.
While it may seem ad-vantageous to maintain many constituents within thesame domain of locality of an SCFG production, aswe may hope to exploit its large syntactic context tocondition deletions more accurately, the sparsity ofsuch productions make them poor candidates for rel-ative frequency estimation, especially in a task withlimited quantities of training material.
Indeed, ourbase training corpus described in Section 4 containsonly 951 SCFG productions, 593 appearing once.Secondly, syntactic categories in the PTB are par-ticularly coarse grained, and lead to many incorrectcontext-free assumptions.
Some important distinc-tions, such as between arguments and adjuncts, arebeyond the scope of the PTB annotation, and it isoften difficult to determine out of context whether agiven constituent can safely be deleted from a right-hand side.One first type of annotation that can effectively beadded to each syntactic category is its lexical headand head part-of-speech (POS), following work insyntactic parsing (Collins, 1999).
This type of an-notation is particular beneficial in the case of, e.g.,prepositional phrases (PP), which may be eithercomplement or adjunct.
As in the case of Figure 1(in which adjuncts appear in italic), knowing that thePP headed by ?from?
appears in a VP headed by?fell?
helps us to determine that the PP is a com-plement to the verb ?fell?, and that it should pre-sumably not be deleted.
Conversely, the PP headedby ?because?
modifying the same verb is an adjunct,and can safely be deleted if unimportant.3 Also, asdiscussed in (Klein and Manning, 2003), POS an-notation can be useful as a means of backing offto more frequently occurring head-modifier POS oc-currences (e.g., VBD-IN) when specific bilexical co-2Details about the SCFG extraction procedure are given inSection 4.
In short, we refer here to a grammar generated from823 sentence pairs.3The PP headed by ?from?
is an optional argument, and thusmay still be deleted.
Our point is that lexical information in gen-eral should help give lower scores to deletions of constituentsthat are grammatically more prominent.NNEarningNPRBalsoADVPVBDfell INfrom DTtheJJyear-agoNNperiodNPPPINbecauseINof VBGslowingNNmicrochipNNdemandNPPPVP ..SFigure 1: Penn Treebank tree with adjuncts in italic.occurrences are sparsely seen (e.g., ?fell?-?from?
).At a lower level, lexicalization is clearly desirablefor pre-terminals.
Indeed, current SCFG modelssuch as K&M have no direct way of preventinghighly improbable single word removals, such asdeletions of adverbs ?never?
or ?nowhere?, whichmay turn a negative statement into a positive one.4A second type of annotation that can be added tosyntactic categories is the so-called parent annota-tion (Johnson, 1998), which was effectively used insyntactic parsing to break unreasonable context-freeassumptions.
For instance, a PP with a VP parentis marked as PP?VP.
It is reasonable to assume that,e.g., that constituents deep inside a PP have morechances to be removed than otherwise expected, andone may seek to increase the amount of verticalcontext that is available for conditioning each con-stituent deletion.To achieve the above desiderata for better SCFGprobability estimates?i.e., reduce the amount ofsister annotation within each SCFG production, byconditioning deletions on a context smaller than anentire right-hand side, and at the same time in-crease the amount of ancestor and descendent an-notation through parent (or ancestor) annotation andlexicalization?we follow the approach of (Collins,1999; Klein and Manning, 2003), i.e., factor-ize n-ary grammar productions into products of nright-hand side probabilities, a technique sometimescalled Markovization.Markovization is generally head-driven, i.e., re-flects a decomposition centered around the head ofeach CFG production:l ?
?Lm ?
?
?L1HR1 ?
?
?Rn?
(4)4K&M incorporate lexical probabilities through n-grammodels, but such language models are obviously not good forpreventing such unreasonable deletions.182where H is the head, L1, .
.
.
, Lm the left modi-fiers, R1, .
.
.
, Rn are right modifiers, and ?
termi-nation symbols needed for accurate probability es-timations (e.g., to capture the fact that certain con-stituents are more likely than others to be the right-most constituent); for simplicity, we will ignore ?in later discussions.
For a given SCFG productionl ?
?
?f , ?c?, we ask, given the source RHS ?fthat is assumed given (e.g., provided by a syntacticparser), which of its RHS elements are also presentin ?c.
That is, we write:p(?c|?f , l) = (5)p(kml , ?
?
?
, k1l , kh, k1r , ?
?
?
, knr |?f , l)where kh, kil , kjr (?k?
for keep) are binary variablesthat are true if and only if constituents H,Li, Rj (re-spectively) of the source RHS ?f are present in thetarget side ?c.
Note that the conditional probabil-ity in Equation 5 enables us to estimate Equation 3,since p(?f , ?c|l) = p(?c|?f , l) ?
p(?f |l).
We canrely on a state-of-the-art probabilistic parser to ef-fectively compute either p(?f |l) or the probabilityof the entire tree pif , and need not worry about esti-mating this term.
In the case of sentence compres-sion from the one-best hypothesis of the parser, wecan ignore p(?f |l) altogether, since pif is the samefor all compressions.We can rewrite Equation 5 exactly using a head-driven infinite-horizon Markovization:p(?c|?f , l) = p(kh|?f , l) (6)?
?i=1...mp(kil |k1l , ?
?
?
, ki?1l , kh, ?f , l)?
?i=1...np(kir|k1r , ?
?
?
, ki?1r , kh,?, ?f , l)where ?
= (k1l , ?
?
?
, kml ) is a term needed by thechain rule.
One key issue is to make linguisticallyplausible assumptions to determine which condi-tioning variables in the terms should be deleted.
Fol-lowing our discussion in the first part of this section,we may start by making an order-s Markov approx-imation centered around the head, i.e., we condi-tion each binary variable (e.g., kir) on a context ofup to s sister constituents between the current con-stituent and the head (e.g., (Ri?s, .
.
.
, Ri)).
In or-der to incorporate bilexical dependencies betweenthe head and each modifier, we also condition allmodifier probabilities on head variables H (and kh).These assumptions are overall quite similar to theones made in Markovized parsing models.
If we as-sume that all other conditioning variables in Equa-tion 6 are irrelevant, we write:p(?c|?f , l) = ph(kh|H, l) (7)?
?i=1...mpl(kil |Li?s, ..., Li, ki?sl , ..., ki?1l ,H, kh, l)?
?i=1...npr(kir|Ri?s, ..., Ri, ki?sr , ..., ki?1r ,H, kh, l)Note that it is important to condition deletions onboth constituent histories (Ri?s, .
.
.
, Ri) and non-deletion histories (ki?sr , .
.
.
, ki?1r ); otherwise wewould be unable to perform deletions that must op-erate jointly, as in production S?
?ADVP COMMANP VP, NP VP?
(in which the ADVP should not bedeleted without the comma).
Without binary his-tories, we often observed superfluous punctuationsymbols and dangling coordinate conjunctions ap-pearing in our outputs.Finally, we label l with an order-v ancestor anno-tation, e.g., for the VP in Figure 1, l = ?
for v = 0,l =VP?S for v = 2, and so on.
We also replace Hand modifiers Li and Ri by lexicalized entries, e.g.,H =(VP,VBD,fell) and Ri =(PP,IN,from).
Notethat to estimate pl(kil | ?
?
?
), we only lexicalize LiandH , and none of the other conditioning modifiers,since this would, of course, introduce too many con-ditioning variables (the same goes for pr(kir| ?
?
?
)).The question of how much sister and vertical (s andv) context is needed for effective sentence compres-sion, and whether to use lexical or POS annotation,will be evaluated in detail in Section 5.3 The DataTo acquire SCFG productions, we used Ziff-Davis,a corpus of technical articles and human abstractivesummaries.
Articles and summaries are paired bydocument, so the first step was to perform sentencealignment.
In the particular case of sentence com-pression, a simple approach is to just consider com-pression pairs (f,c), where c is a substring of f. K&Midentified only 1,087 such paired sentences in the en-tire corpus, which represents a recall of 1.75%.For our empirical evaluations, we split the data asfollows: among the 1,055 sentences that were taken183to train systems described in K&M, we selected thefirst 32 sentence pairs to be an auxiliary test corpus(for future work), the next 200 sentences to be ourdevelopment corpus, and the remaining 823 to beour base training corpus (ZD-0), which will be aug-mented with additional data as explained in the nextsection.
We feel it is important to use a relativelylarge development corpus, since we will provide inSection 5 detailed analyses of model selection onthe development set (e.g., by evaluating differentMarkov structures), and we want these findings tobe as significant as possible.
Finally, we used thesame test data as K&M for human evaluation pur-poses (32 sentence pairs).4 Tree Alignment and Synchronous Gram-mar InferenceWe now describe methods to train SCFG modelsfrom sentence pairs.
Given a tree pair (f , c), whoserespective parses (pif , pic) were generated by theparser described in (Charniak and Johnson, 2005),the goal is to transform the tree pair into SCFGderivations, in order to build relative frequency es-timates for our Markovized models from observedSCFG productions.
Clearly, the two trees maysometimes be structurally quite different (e.g., agiven PP may attach to an NP in pif , while attach-ing to VP in pic), and it is not always possible tobuild an SCFG derivation given the constraints in(pif , pic).
The approach taken by K&M is to analyzeboth trees and count an SCFG rule whenever twonodes are ?deemed to correspond?, i.e., roots are thesame, and ?c is a sub-sequence of ?f .
This leadsto a quite restricted number of different productionson our base training set (ZD-0): 823 different pro-ductions were extracted, 593 of which appear onlyonce.
This first approach has serious limitations;the assumption that sentence compression appropri-ately models human abstractive data is particularlyproblematic.
This considerably limits the amountof training data that can be exploited in Ziff-Davis(which contains overall more than 4,000 documents-abstract pairs), and this makes it very difficult totrain lexicalized models.An approach to slightly loosen this assumptionis to consider document-abstract sentence pairs inwhich the condensed version contains one or moresubstitutions or insertions.
Consider for exampleDT[3]The[4]JJ[5]second[6]NN[7]computer[8]NP[2]VBDstarted RPupPRTVP CCand VBD[10]ran[11] IN[13]without[14] NN[16]incident[17]NP[15]PP[12]VP[9]VP .[18].
[19]S[1]DT[3]The[4]JJ[5]second[6]NN[7]unit[8]NP[2]VBD[10]ran[11] IN[13]without[14] NN[16]incident[17]NP[15]PP[12]VP[9] .[18].
[19]S[1]Figure 2: Full sentence and its revision.
While the latter is not acompression of the former, it could still be used to gather statis-tics to train a sentence compression system, e.g., to learn thereduction of a VP coordination.the tree pair in Figure 2: the two sentences are syn-tactically very close, but the substitution of ?com-puter?
with ?unit?
makes this sentence pair unus-able in the framework presented in K&M.
Arguably,there should be ways to exploit abstract sentencesthat are slightly reworded in addition to being com-pressed.
To use sentence pairs with insertions andsubstitutions, we must find a way to align tree pairsin order to identify SCFG productions.
More specif-ically, we must define a constituent alignment be-tween the paired abstract and document sentences,which determine how the two trees are synchronizedin a derivation.
Obtaining this alignment is no triv-ial matter as the number of non-deleting edits in-creases.
To address this, we synchronized tree pairsby finding the constituent alignment that minimizesthe edit distance between the two trees, i.e., mini-mize the number of terminals and non-terminals in-sertions, substitutions and deletions.5 While criteria5The minimization problem is known to be NP hard, so weused an approximation algorithm (Zhang and Shasha, 1989) that184other than minimum tree edit distance may be effec-tive, we found?after manual inspections of align-ments between sentences with less than five non-deleting edits?that this method generally producesgood alignments.
A sample alignment is provided inFigure 2.
Once a constituent alignment is available,it is then trivial to extract all deletion SCFG rulesavailable in a tree pair, e.g., NP ?
?DT JJ NN, DTJJ NN?
in the figure.We also exploited more general tree productionsknown as synchronous tree substitution grammar(STSG) rules, in an approach quite similar to (Turnerand Charniak, 2005).
For instance, the STSG rulerooted at S can be decomposed into two SCFG pro-ductions if we allow unary rules such as VP?VP tobe freely added to the compressed tree.
More specif-ically, we decompose any STSG rule that has in itstarget (compressed) RHS a single context free pro-duction, and that contains in its source (full) RHSa single context free production adjoined with anynumber of tree adjoining grammar (TAG) auxiliarytrees (Joshi et al, 1975).
In the figure, the initial treeis S ?
NP VP, and the adjoined (auxiliary) tree isVP ?
VP CC VP.6 We found this approach quitehelpful, since most useful compressions that mimicTAG adjoining operations are missed by the extrac-tion procedure of K&M.Since we found that exploiting sentence pairs con-taining insertions had adverse consequences in termsof compression accuracies, we only report experi-ments with sentence pairs containing no insertions.We gathered sentence pairs with up to six substi-tutions using minimum edit distance matching (wewill refer to these sets as ZD-0 to ZD-6).
With alimit of up to six substitutions (ZD-6), we were ableto train our models on 16,787 sentences, which rep-resents about 25% of the total number of summarysentences of the Ziff-Davis corpus.5 ExperimentsAll experiments presented in this section are per-formed on the Ziff-Davis corpus.
We note first thatall probability estimates of our Markovized gram-runs in polynomial time.6To determine whether a given one-level tree is an auxiliary,we simply check the following properties: all its leaves but one(the ?foot node?)
must be nodes attached to deleted subtrees(e.g., VP and CC in the figure), and the foot node (VP[9]) musthave the same syntactic category as the root node.mars are smoothed.
Indeed, incorporating lexicaldependencies within models trained on data sets assmall as 16,000 sentence pairs would be quite fu-tile without incorporating robust smoothing tech-niques.
Different smoothing techniques were eval-uated with our models, and we found that interpo-lated Witten-Bell discounting was the method thatperformed best.
We used relative frequency es-timates for each of the models presented in Sec-tion 2.2 (i.e., ph, pl, pr), and trained pl separatelyfrom pr.
We interpolated our most specific models(lexical heads, POS tags, ancestor and sister annota-tion) with lower-order models.7Automatic evaluation on development sets is per-formed using word-level classification accuracy, i.e.,the number of words correctly classified as beingeither deleted or not deleted, divided by the to-tal number of words.
In our first evaluation, weexperimented with different horizontal and verticalMarkovizations (Table 1).
First, it appears that ver-tical annotation is moderately helpful.
It providesgains in accuracy ranging from .5% to .9% for v = 1over a simpler models (v = 0), but higher orders(v > 1) have a tendency to decrease performance.On the other hand, sister annotation of order 1 ismuch more critical, and provides 4.1% improvementover a simpler model (s = 0, v = 0).
Manual exami-nations of compression outputs confirmed this anal-ysis: without sister annotation, deletion of punctu-ation and function words (determiners, coordinateconjunctions, etc.)
is often inaccurate, and compres-sions clearly lack fluency.
This annotation is alsohelpful for phrasal deletions; for instance, we foundthat PPs are deleted in 31.4% of cases in Ziff-Davisif they do not immediately follow the head con-stituent, but this percentage drops to 11.1% for PPsthat immediately follow the head.
It seems, how-ever, that increasing sister annotation beyond s > 1only provide limited improvements.In our second evaluation reported in Table 2, we7We relied on the SRI language modeling (SRILM) toolkitlibrary for all smoothing experiments.
We used the followingorder in our deleted interpolation of ph: lexical head, head POS,ancestor annotation, and head category.
For pl and pr , we re-moved first: lexical head, lexical head of the modifier, headPOS, head POS of the modifier, sister annotation (Li deletedbefore kil ), kh, category of the head, category of the modifier.We experimented with different deletion interpolation order-ings, and this ordering appears to work quite well in practice,and was used in all experiments reported in this paper.185assessed the usefulness of lexical and POS anno-tation (setting s and v to 0).
In the table, we useM to denote any of the modifiers Li or Ri, andc, t, w respectively represent syntactic constituent,POS, and lexical conditioning.
While POS annota-tion is clearly advantageous compared to using onlysyntactic categories, adding lexical variables to themodel also helps.
As is shown in the table, it is es-pecially important to know the lexical head of themodifier we are attempting to delete.
The addition ofwm to conditioning variables provides an improve-ment of 1.3% (from 66.5% to 67.8%) on our op-timal Ziff-Davis training corpus (ZD-6).
Further-more, bilexical head-modifier dependencies providea relatively small improvement of .5% (from 69.8%to 70.3%) over the best model that does not incor-porate the lexical head wh.
Note that lexical con-ditioning also helps in the case where the trainingdata is relatively small (ZD-0), though differencesare less significant, and bilexical dependencies actu-ally hurt performance.
In subsequent experiments,we experimented with different Markovizations andlexical dependency combination, and finally settledwith a model (s = 1 and v = 1) incorporating allconditioning variables listed in the last line of Ta-ble 2.
This final tuning was combined with humaninspection of generated outputs, since certain modi-fications that positively impacted output quality sel-dom changed accuracies.We finally took the best configuration selectedabove, and evaluated our model against the noisy-channel model of K&M on the 32 test sentences se-lected by them.
We performed both automatic andhuman evaluation against the output produced byKnight and Marcu?s original implementation of theirnoisy channel model (Table 3).
In the former case,we also provide Simple String Accuracies (SSA).8For human evaluation, we hired six native-speakerjudges who scored grammaticality and content (im-portance) with scores from 1 to 5, using instructionsas described in K&M.
Both types of evaluations fa-vored our Markovized model against the noisy chan-nel model.Table 4 shows several outputs of our system8SSA is defined as: SSA = 1 ?
(I + D + S)/R.
Thenumerator terms are respectively the number of inserts, deletes,and substitutions, and R is the length of the reference compres-sion.Vertical Horizontal OrderOrder s = 0 s = 1 s = 2 s = 3v = 0 63 67.1 67.2 67.2v = 1 63.9 67.6 67.7 67.7v = 2 65.7 66.6 66.9 66.9v = 3 65.2 66.8 67.1 67Table 1: Markovizations accuracies on Ziff-Davis devel set.Conditioning Variables ZD-0 ZD-3 ZD-6M=cm H=ch 62.2 62.4 64.4M=(cm, tm) H=ch 63.0 63.4 66.5M=(cm, wm) H=ch 64.2 65.2 66.7M=(cm, tm, wm) H=ch 63.8 65.8 67.8M=(cm, tm, wm) H=(ch, th) 66.7 68.6 69.8M=(cm, tm, wm) H=(ch, wh) 66.9 68.9 70.3M=(cm, tm, wm) H=(ch, th, wh) 66.3 69.1 69.8Table 2: Accuracies on Ziff-Davis devel set with different head-modifier annotations.Models Acc SSA Grammar Content Len(%)NoisyC 61.3 14.6 4.37 ?
0.5 3.87 ?
1.2 70.4Markov 67.9 31.7 4.68 ?
0.4 4.22 ?
0.4 62.7Human - - 4.95 ?
0.1 4.43 ?
0.3 53.3Table 3: Accuracies on Ziff-Davis test set.
(Markov) that significantly differed from the outputof the noisy channel model (NoisyC), which con-firms our finding that Markovized models can pro-duce quite grammatical output.
Our compression forthe first sentence underlines one of the advantages ofconstituent-based classifiers, which have the abilityof deleting a very long phrase (here, a PP) at once.The three next sentences display some advantagesof our approach over the K&M model: here, the lat-ter model performs deletion with too little lexico-syntactic information, and accidentally removes cer-tain modifiers that are sometimes, but not always,good candidates for deletions (e.g., ADJP in Sen-tence 2, PP in sentences 3 and 4).
On the other hand,our model keeps these constituent intact.
Finally, thefifth and last example is one of the only three cases(among the 32 sentences) where our model produceda sentence we judged clearly ungrammatical.
Afterinspection, we found that our parser assigned par-ticularly errorful trees to those inputs, which maypartially explain these ungrammatical outputs.6 Related WorkA relatively large body of work addressed the prob-lem of sentence compression.
One successful recentapproach (McDonald, 2006) combines a discrimi-native framework with a set of features that cap-ture information similar to the K&M model.
Mc-186Input Many debugging features, including user-defined break pointsand variable-watching and message-watching windows, havebeen added.NoisyC Many debugging features, including user-defined points andvariable-watching and message-watching windows, have beenadded.Markov Many debugging features have been added.Human Many debugging features have been added.Input The chemical etching process used for glare protection is effec-tive and will help if your office has the fluorescent-light overkillthat ?s typical in offices.NoisyC The process used for glare protection is and will help if youroffice has the overkillMarkov The chemical etching process used for glare protection is ef-fective.Human Glare protection is effective.Input The utilities will be bundled with Quickdex II in a $90 pack-age called super quickdex, which is expected to ship in latesummer.NoisyC The utilities will be bundledMarkov The utilities will be bundled with Quickdex II.Human The utilities will be bundled with Quickdex II.Input The discounted package for the SparcServer 470 is priced at$89,900, down from the regular $107,795.NoisyC The package for the 470 is pricedMarkov The discounted package for the SparcServer 470 is at $89,900.Human The SparcServer 470 is priced at $89,900, down from the reg-ular $107,795.Input Prices range from $5,000 for a microvax 2000 to $179,000 forthe vax 8000 or higher series.NoisyC Prices range from $5,000 for a 2000 to $179,000 for the vax8000 or higher series.Markov Prices range from $5,000 for a microvax for the vax.Human Prices range from $5,000 to $179,000.Table 4: Compressions of sample test sentences.Donald?s features include compression bigrams, aswell as soft syntactic evidence extracted from parsetrees and dependency trees.
The strength of McDon-ald?s approach partially stems from its robustnessagainst redundant and noisy features, since each fea-ture is weighted proportionally to its discriminativepower, and his approach is thus hardly penalizedby uninformative features.
In contrast, our workputs much more emphasis on feature analysis thanon efficient optimization, and relies on a statisti-cal framework (maximum-likelihood estimates) thatstrives for careful feature selection and combination.It also describes and evaluates models incorporatingsyntactic evidence that is new to the sentence com-pression literature, such as head-modifier bilexicaldependencies, and nth-order sister and vertical an-notation.
We think this work leads to a better un-derstanding of what type of syntactic and lexical ev-idence makes sentence compression work.
Further-more, our work leaves the door open to uses of ourfactored model in a constituent-based or word-baseddiscriminative framework, in which each elemen-tary lexico-syntactic structure of this paper can bediscriminatively weighted to directly optimize com-pression quality.
Since McDonald?s approach doesnot incorporate SCFG deletion rules, and conditionsdeletions on less lexico-syntactic context, we believethis will lead to levels of performance superior toboth papers.7 ConclusionsWe presented a sentence compression system basedon SCFG deletion rules, for which we defineda head-driven Markovization formulation.
ThisMarkovization enabled us to incorporate lexical con-ditioning variables into our models.
We empiricallyevaluated different Markov structures, and obtaineda best system that generates particularly grammati-cal sentences according to a human evaluation.
Oursentence compression system is freely available forresearch and educational purposes.AcknowledgmentsWe would like to thank Owen Rambow, MichaelCollins, Julia Hirschberg, and Daniel Ellis for theirhelpful comments and suggestions.ReferencesA.
Aho and J. Ullman.
1969.
Syntax directed translations andthe pushdown assembler.
3:37?56.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best pars-ing and maxent discriminative reranking.
In Proc.
of ACL.M.
Collins.
1999.
Head-driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, U. of Pennsylvania.B.
Dorr, D. Zajic, and R. Schwartz.
2003.
Hedge: A parse-and-trim approach to headline generation.
In Proc.
of DUC.H.
Jing.
2000.
Sentence reduction for automatic text summa-rization.
In Proc.
of NAACL, pages 310?315.M.
Johnson.
1998.
PCFG models of linguistic tree representa-tions.
Computational Linguistics, 24(4):613?632.A.
Joshi, L. Levy, and M. Takahashi.
1975.
Tree adjunct gram-mar.
Journal of Computer and System Science, 21(2).D.
Klein and C. Manning.
2003.
Accurate unlexicalized pars-ing.
In Proc.
of ACL.K.
Knight and D. Marcu.
2000.
Statistics-based summarization?
step one: Sentence compression.
In Proc.
of AAAI.P.
Lewis and R. Stearns.
1968.
Syntax-directed transduction.In Journal of the Association for Computing Machinery, vol-ume 15, pages 465?488.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1994.
Build-ing a large annotated corpus of english: The penn treebank.Computational Linguistics, 19(2):313?330.R.
McDonald.
2006.
Discriminative sentence compressionwith soft syntactic constraints.
In Proc.
of EACL.J.
Turner and E. Charniak.
2005.
Supervised and unsupervisedlearning for sentence compression.
In Proc.
of ACL.K.
Zhang and D. Shasha.
1989.
Simple fast algorithms for theediting distance between trees and related problems.
SIAMJ.
Comput., 18(6):1245?1262.187
