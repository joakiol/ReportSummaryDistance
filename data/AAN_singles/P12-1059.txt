Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 563?571,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsMining Entity Types from Query Logs via User Intent ModelingPatrick PantelMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAppantel@microsoft.comThomas LinComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195, USAtlin@cs.washington.eduMichael GamonMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAmgamon@microsoft.comAbstractWe predict entity type distributions in Websearch queries via probabilistic inference ingraphical models that capture how entity-bearing queries are generated.
We jointlymodel the interplay between latent user in-tents that govern queries and unobserved en-tity types, leveraging observed signals fromquery formulations and document clicks.
Weapply the models to resolve entity types in newqueries and to assign prior type distributionsover an existing knowledge base.
Our mod-els are efficiently trained using maximum like-lihood estimation over millions of real-worldWeb search queries.
We show that modelinguser intent significantly improves entity typeresolution for head queries over the state of theart, on several metrics, without degradation intail query performance.1 IntroductionCommercial search engines are providing ever-richer experiences around entities.
Querying for adish on Google yields recipe filters such as cooktime, calories, and ingredients.
Querying for amovie on Yahoo triggers user ratings, cast, tweetsand showtimes.
Bing further allows the movie tobe directly added to the user?s Netflix queue.
En-tity repositories such as Freebase, IMDB, FacebookPages, Factual, Pricegrabber, and Wikipedia are in-creasingly leveraged to enable such experiences.There are, however, inherent problems in the en-tity repositories: (a) coverage: although coverage ofhead entity types is often reliable, the tail can besparse; (b) noise: created by spammers, extractionerrors or errors in crowdsourced content; (c) am-biguity: multiple types or entity identifiers are of-ten associated with the same surface string; and (d)over-expression: many entities have types that arenever used in the context of Web search.There is an opportunity to automatically tailorknowledge repositories to the Web search scenario.Desirable capabilities of such a system include: (a)determining the prior type distribution in Web searchfor each entity in the repository; (b) assigning a typedistribution to new entities; (c) inferring the correctsense of an entity in a particular query context; and(d) adapting to a search engine and time period.In this paper, we build such a system by lever-aging Web search usage logs with large numbers ofuser sessions seeking or transacting on entities.
Wecast the task as performing probabilistic inferencein a graphical model that captures how queries aregenerated, and then apply the model to contextuallyrecognize entity types in new queries.
We motivateand design several generative models based on thetheory that search users?
(unobserved) intents gov-ern the types of entities, the query formulations, andthe ultimate clicks on Web documents.
We show thatjointly modeling user intent and entity type signifi-cantly outperforms the current state of the art on thetask of entity type resolution in queries.
The majorcontributions of our research are:?
We introduce the idea that latent user intentscan be an important factor in modeling type dis-tributions over entities in Web search.?
We propose generative models and inferenceprocedures using signals from query context,click, entity, entity type, and user intent.563?
We propose an efficient learning technique anda robust implementation of our models, usingreal-world query data, and a realistic large setof entity types.?
We empirically show that our models outper-form the state of the art and that modeling latentintent contributes significantly to these results.2 Related Work2.1 Finding Semantic ClassesA closely related problem is that of finding the se-mantic classes of entities.
Automatic techniques forfinding semantic classes include unsupervised clus-tering (Schu?tze, 1998; Pantel and Lin, 2002), hy-ponym patterns (Hearst, 1992; Pantel et al, 2004;Kozareva et al, 2008), extraction patterns (Etzioniet al, 2005), hidden Markov models (Ritter et al,2009), classification (Rahman and Ng, 2010) andmany others.
These techniques typically lever-age large corpora, while projects such as WordNet(Miller et al, 1990) and Freebase (Bollacker et al,2008) have employed editors to manually enumeratewords and entities with their semantic classes.The aforementioned methods do not use querylogs or explicitly determine the relative probabilitiesof different entity senses.
A method might learn thatthere is independently a high chance of eBay being awebsite and an employer, but does not specify whichusage is more common.
This is especially problem-atic, for example, if one wishes to leverage Freebasebut only needs the most commonly used senses (e.g.,Al Gore is a US Vice President), rather thanall possible obscure senses (Freebase contains 30+senses, including ones such as ImpersonatedCelebrity and Quotation Subject).
Inscenarios such as this, our proposed method can in-crease the usability of systems that find semanticclasses.
We also expand upon text corpora meth-ods in that the type priors can adapt to Web searchsignals.2.2 Query Log MiningQuery logs have traditionally been mined to improvesearch (Baeza-Yates et al, 2004; Zhang and Nas-raoui, 2006), but they can also be used in place of(or in addition to) text corpora for learning seman-tic classes.
Query logs can contain billions of en-tries, they provide an independent signal from textcorpora, their timestamps allow the learning of typepriors at specific points in time, and they can containinformation such as clickthroughs that are not foundin text corpora.
Sekine and Suzuki (2007) used fre-quency features on context words in query logs tolearn semantic classes of entities.
Pas?ca (2007) usedextraction techniques to mine instances of semanticclasses from query logs.
Ru?d et al (2011) foundthat cross-domain generalizations learned from Websearch results are applicable to NLP tasks such asNER.
Alfonseca et al (2010) mined query logs tofind attributes of entity instances.
However, theseprojects did not learn relative probabilities of differ-ent senses.2.3 User Intents in SearchLearning from query logs also allows us to lever-age the concept of user intents.
When users sub-mit search queries, they often have specific intents inmind.
Broder (2002) introduced 3 top level intents:Informational (e.g., wanting to learn), Navigational(wanting to visit a site), and Transactional (e.g.,wanting to buy/sell).
Rose and Levinson (2004) fur-ther divided these into finer-grained subcategories,and Yin and Shah (2010) built hierarchical tax-onomies of search intents.
Jansen et al (2007), Huet al (2009), and Radlinski et al (2010) examinedhow to infer the intent of queries.
We are not awareof any other work that has leveraged user intents tolearn type distributions.2.4 Topic Modeling on Query LogsThe closest work to ours is Guo et al?s (2009) re-search on Named Entity Recognition in Queries.Given an entity-bearing query, they attempt to iden-tify the entity and determine the type posteriors.
Ourwork significantly scales up the type posteriors com-ponent of their work.
While they only have fourpotential types (Movie, Game, Book, Music) foreach entity, we employ over 70 popular types, allow-ing much greater coverage of real entities and theirtypes.
Because they only had four types, they wereable to hand label their training data.
In contrast,our system self-labels training examples by search-ing query logs for high-likelihood entities, and musthandle any errors introduced by this process.
Ourmodels also expand upon theirs by jointly modeling564entity type with latent user intents, and by incorpo-rating click signals.Other projects have also demonstrated the util-ity of topic modeling on query logs.
Carman etal.
(2010) modeled users and clicked documents topersonalize search results and Gao et al (2011) ap-plied topic models to query logs in order to improvedocument ranking for search.3 Joint Model of Types and User IntentsWe turn our attention now to the task of mining thetype distributions of entities and of resolving thetype of an entity in a particular query context.
Ourapproach is to probabilistically describe how entity-bearing queries are generated in Web search.
Wetheorize that search queries are governed by a latentuser intent, which in turn influences the entity types,the choice of query words, and the clicked hosts.
Wedevelop inference procedures to infer the prior typedistributions of entities in Web search as well as toresolve the type of an entity in a query, by maximiz-ing the probability of observing a large collection ofreal-world queries and their clicked hosts.We represent a query q by a triple {n1, e, n2},where e represents the entity mentioned in the query,n1 and n2 are respectively the pre- and post-entitycontexts (possibly empty), referred to as refiners.Details on how we obtain our corpus are presentedin Section 4.2.3.1 Intent-based Model (IM)In this section we describe our main model, IM, il-lustrated in Figure 1.
We derive a learning algorithmfor the model in Section 3.2 and an inference proce-dure in Section 3.3.Recall our discussion of intents from Section 2.3.The unobserved semantic type of an entity e in aquery is strongly correlated with the unobserveduser intent.
For example, if a user queries for?song?, then she is likely looking to ?listen to it?,?download it?, ?buy it?, or ?find lyrics?
for it.
Ourmodel incorporates this user intent as a latent vari-able.The choice of the query refiner words, n1 and n2,is also clearly influenced by the user intent.
Forexample, refiners such as ?lyrics?
and ?words?
aremore likely to be used in queries where the intent isFor each query/click pair {q, c}type t ?Multinomial(?
)intent i ?Multinomial(?t)entity e ?Multinomial(?t)switch s1 ?
Bernoulli(?i)switch s2 ?
Bernoulli(?i)if (s1) l-context n1 ?Multinomial(?i)if (s2) r-context n2 ?Multinomial(?i)click c ?Multinomial(?i)Table 1: Model IM: Generative process for entity-bearing queries.to ?find lyrics?
than in queries where the intent is to?listen?.
The same is true for clicked hosts: clicks on?lyrics.com?
and ?songlyrics.com?
are more likelyto occur when the intent is to ?find lyrics?, whereasclicks on ?pandora.com?
and ?last.fm?
are morelikely for a ?listen?
intent.Model IM leverages each of these signals: latentintent, query refiners, and clicked hosts.
It generatesentity-bearing queries by first generating an entitytype, from which the user intent and entity is gen-erated.
In turn, the user intent is then used to gen-erate the query refiners and the clicked host.
In ourdata analysis, we observed that over 90% of entity-bearing queries did not contain any refiner words n1and n2.
In order to distribute more probability massto non-empty context words, we explicitly representthe empty context using a switch variable that deter-mines whether a context will be empty.The generative process for IM is described in Ta-ble 1.
Consider the query ?ymca lyrics?.
Our modelfirst generates the type song, then given the typeit generates the entity ?ymca?
and the intent ?findlyrics?.
The intent is then used to generate the pre-and post-context words ?
and ?lyrics?, respectively,and a click on a host such as ?lyrics.com?.For mathematical convenience, we assume thatthe user intent is generated independently of theentity itself.
Without this assumption, we wouldrequire learning a parameter for each intent-type-entity configuration, exploding the number of pa-rameters.
Instead, we choose to include these depen-dencies at the time of inference, as described later.Recall that q = {n1, e, n2} and let s = {s1, s2},where s1 = 1 if n1 is not empty and s2 = 1 if n2 isnot empty, 0 otherwise.
The joint probability of themodel is the product of the conditional distributions,as given by:565yQttn2ef fTttEGuo?09yQttn2essf fTtt ETModel M0yQttn2ew Tcssf fTtt ETModel M1 Model IMttQttn2qqTiiew Kcssf fKyTKFigure 1: Graphical models for generating entity-bearing queries.
Guo?09 represents the current state of the art (Guoet al, 2009).
Models M0 and M1 add an empty context switch and click information, respectively.
Model IM furtherconstrains the query by the latent user intent.P (t, i, q, c | ?,?,?, ?,?,?)
=P (t | ?
)P (i | t,?
)P (e | t,?
)P (c | i,?
)2?j=1P (nj | i,?
)I[sj=1]P (sj |i, ?
)We now define each of the terms in the joint dis-tribution.
Let T be the number of entity types.
Theprobability of generating a type t is governed by amultinomial with probability vector ?
:P (t=t?)
=T?j=1?
I[j=t?
]j , s.t.T?j=1?j = 1where I is an indicator function set to 1 if its condi-tion holds, and 0 otherwise.Let K be the number of latent user intents thatgovern our query log, where K is fixed in advance.Then, the probability of intents i is defined as amultinomial distribution with probability vector ?tsuch that ?
= [?1, ?2, ..., ?T ] captures the matrix ofparameters across all T types:P (i=i?
| t=t?)
=K?j=1?I[j=i?
]t?,j, s.t.
?tK?j=1?t,j = 1LetE be the number of known entities.
The prob-ability of generating an entity e is similarly governedby a parameter ?
across all T types:P (e=e?
| t=t?)
=E?j=1?I[j=e?
]t?,j, s.t.
?tE?j=1?t,j = 1The probability of generating an empty or non-empty context s given intent i is given by a Bernoulliwith parameter ?i:P (s | i=i?)
= ?I[s=1]i?(1?
?i?
)I[s=0]Let V be the shared vocabulary size of all queryrefiner words n1 and n2.
Given an intent, i, theprobability of generating a refiner n is given by amultinomial distribution with probability vector ?isuch that ?
= [?1, ?2, ..., ?K ] represents parame-ters across intents:P (n=n?
| i=i?)
=V?v=1?I[v=n?
]i?,v, s.t.
?iV?v=1?i,v = 1Finally, we assume there areH possible click val-ues, corresponding to H Web hosts.
A click on ahost is similarly determined by an intent i and is gov-erned by parameter ?
across all K intents:P (c=c?
| i=i?)
=H?h=1?I[h=c?
]i?,h, s.t.
?iH?h=1?i,h = 13.2 LearningGiven a query corpus Q consisting of N inde-pendently and identically distributed queries qj ={nj1, ej , nj2} and their corresponding clicked hostscj , we estimate the parameters ?
, ?, ?, ?, ?, and?
by maximizing the (log) probability of observingQ.
The logP (Q) can be written as:logP (Q) =N?j=1?t,iP j(t, i | q, c) logP j(q, c, t, i)In the above equation, P j(t, i | q, c) is the poste-rior distribution over types and user intents for thejth query.
We use the Expectation-Maximization(EM) algorithm to estimate the parameters.
Theparameter updates are obtained by computing thederivative of logP (Q) with respect to each parame-ter, and setting the resultant to 0.The update for ?
is given by the average of theposterior distributions over the types:566?t?
=?Nj=1?i Pj(t=t?, i | q, c)?Nj=1?t,i Pj(t, i | q, c)For a fixed type t, the update for ?t is given bythe weighted average of the latent intents, where theweights are the posterior distributions over the types,for each query:?t?,?i =?Nj=1 Pj(t=t?, i=i?
| q, c)?Nj=1?i Pj(t=t?, i | q, c)Similarly, we can update ?, the parameters thatgovern the distribution over entities for each type:?t?,e?
=?Nj=1?i Pj(t=t?, i | q, c)I[ej=e?
]?Nj=1?i Pj(t=t?, i | q, c)Now, for a fixed user intent i, the update for?i is given by the weighted average of the clickedhosts, where the weights are the posterior distribu-tions over the intents, for each query:?i?,c?
=?Nj=1?t Pj(t, i=i?
| q, c)I[cj=c?
]?Nj=1?t Pj(t, i=i?
| q, c)Similarly, we can update ?
and ?, the parametersthat govern the distribution over query refiners andempty contexts for each intent, as:?i?,n?=?Nj=1?t Pj(t,i=i?|q,c)[I[nj1=n?]I[sj1=1]+I[nj2=n?
]I[sj2=1]]?Nj=1?t Pj(t,i=i?|q,c)[I[sj1=1]+I[sj2=1]]and?i?
=?Nj=1?t Pj(t, i=i?
| q, c)[I[s1=1] + I[s2=1]]2?Nj=1?t Pj(t, i=i?
| q, c)3.3 DecodingGiven a query/click pair {q, c}, and the learned IMmodel, we can apply Bayes?
rule to find the poste-rior distribution, P (t, i | q, c), over the types andintents, as it is proportional to P (t, i, q, c).
We com-pute this quantity exactly by evaluating the joint foreach combination of t and i, and the observed valuesof q and c.It is important to note that at runtime when a newquery is issued, we have to resolve the entity in theabsence of any observed click.
However, we do haveaccess to historical click probabilities, P (c | q).We use this information to compute P (t | q) bymarginalizing over i as follows:P (t | q) =?iH?j=1P (t, i | q, cj)P (cj | q) (1)3.4 Comparative ModelsFigure 1 also illustrates the current state-of-the-artmodel Guo?09 (Guo et al, 2009), described in Sec-tion 2.4, which utilizes only query refinement wordsto infer entity type distributions.
Two extensions tothis model that we further study in this paper are alsoshown: Model M0 adds the empty context switchparameter and Model M1 further adds click infor-mation.
In the interest of space, we omit the updateequations for these models, however they are triv-ial to adapt from the derivations of Model IM pre-sented in Sections 3.1 and 3.2.3.5 DiscussionFull Bayesian Treatment: In the above mod-els, we learn point estimates for the parameters(?,?,?, ?,?,?).
One can take a Bayesian ap-proach and treat these parameters as variables (forinstance, with Dirichlet and Beta prior distribu-tions), and perform Bayesian inference.
However,exact inference will become intractable and wewould need to resort to methods such as variationalinference or sampling.
We found this extension un-necessary, as we had a sufficient amount of trainingdata to estimate all parameters reliably.
In addition,our approach enabled us to learn (and perform infer-ence in) the model with large amounts of data withreasonable computing time.Fitting to an existing Knowledge Base: Al-though in general our model decodes type distribu-tions for arbitrary entities, in many practical casesit is beneficial to constrain the types to those ad-missible in a fixed knowledge base (such as Free-base).
As an example, if the entity is ?ymca?,admissible types may include song, place, andeducational institution.
When resolvingtypes, during inference, one can restrict the searchspace to only these admissible types.
A desirableside effect of this strategy is that only valid ambigu-ities are captured in the posterior distribution.5674 Evaluation MethodologyWe refer to QL as a set of English Web searchqueries issued to a commercial search engine overa period of several months.4.1 Entity InventoryAlthough our models generalize to any entity reposi-tory, we experiment in this paper with entities cover-ing a wide range of web search queries, coming from73 types in Freebase.
We arrived at these types bygrepping for all entities in Freebase within QL, fol-lowing the procedure described in Section 4.2, andthen choosing the top most frequent types such that50% of the queries are covered by an entity of oneof these types1.4.2 Training Data ConstructionIn order to learn type distributions by jointly mod-eling user intents and a large number of types, werequire a large set of training examples containingtagged entities and their potential types.
Unlike inGuo et al (2009), we need a method to automaticallylabel QL to produce these training cases since man-ual annotation is impossible for the range of entitiesand types that we consider.
Reliably recognizing en-tities in queries is not a solved problem.
However,for training we do not require high coverage of en-tities in QL, so high precision on a sizeable set ofquery instances can be a proper proxy.To this end, we collect candidate entities inQL via simple string matching on Freebase entitystrings within our preselected 73 types.
To achievehigh precision from this initial (high-recall, low-precision) candidate set we use a number of heuris-tics to only retain highly likely entities.
The heuris-tics include retaining only matches on entities thatappear capitalized more than 50% in their occur-rences in Wikipedia.
Also, a standalone score fil-ter (Jain and Pennacchiotti, 2011) of 0.9 is used,which is based on the ratio of string occurrence as1In this process, we omitted any non-core Freebase type(e.g., /user/* and /base/*), types used for representation(e.g., /common/* and /type/*), and too general types (e.g.,/people/person and /location/location) identi-fied by if a type contains multiple other prominent subtypes.Finally, we conflated seven of the types that overlapped witheach other into four types (such as /book/written workand /book/book).an exact match in queries to how often it occurs as apartial match.The resulting queries are further filtered by keep-ing only those where the pre- and post-entity con-texts (n1 and n2) were empty or a single word (ac-counting for a very large fraction of the queries).
Wealso eliminate entries with clicked hosts that havebeen clicked fewer than 100 times over the entireQL.
Finally, for training we filter out any query withan entity that has more than two potential types2.This step is performed to reduce recognition er-rors by limiting the number of potential ambiguousmatches.
We experimented with various thresholdson allowable types and settled on the value two.The resulting training data consists of several mil-lion queries, 73 different entity types, and approx-imately 135K different entities, 100K different re-finer words, and 40K clicked hosts.4.3 Test Set AnnotationWe sampled two datasets, HEAD and TAIL, eachconsisting of 500 queries containing an entity be-longing to one of the 73 types in our inventory, froma frequency-weighted random sample and a uniformrandom sample of QL, respectively.We conducted a user study to establish a goldstandard of the correct entity types in each query.A total of seven different independent and paid pro-fessional annotators participated in the study.
Foreach query in our test sets, we displayed the query,associated clicked host, and entity to the annotator,along with a list of permissible types from our typeinventory.
The annotator is tasked with identifyingall applicable types from that list, or marking the testcase as faulty because of an error in entity identifi-cation, bad click host (e.g.
dead link) or bad query(e.g.
non-English).
This resulted in 2,092 test cases({query, entity, type}-tuples).
Each test case wasannotated by two annotators.
Inter-annotator agree-ment as measured by Fleiss?
?
was 0.445 (0.498on HEAD and 0.386 on TAIL), considered moderateagreement.From HEAD and TAIL, we eliminated three cat-egories of queries that did not offer any interestingtype disambiguation opportunities:?
queries that contained entities with only one2For testing we did not omit any entity or type.568HEAD TAILnDCG MAP MAPW Prec@1 nDCG MAP MAPW Prec@1BFB 0.71 0.60 0.45 0.30 0.73 0.64 0.49 0.35Guo?09 0.79?
0.71?
0.62?
0.51?
0.80?
0.73?
0.66?
0.52?M0 0.79?
0.72?
0.65?
0.52?
0.82?
0.75?
0.67?
0.57?M1 0.83?
0.76?
0.72?
0.61?
0.81?
0.74?
0.67?
0.55?IM 0.87?
0.82?
0.77?
0.73?
0.80?
0.72?
0.66?
0.52?Table 2: Model analysis on HEAD and TAIL.
?
indicates statistical significance over BFB, and ?
over both BFB andGuo?09.
Bold indicates statistical significance over all non-bold models in the column.
Significance is measuredusing the Student?s t-test at 95% confidence.potential type from our inventory;?
queries where the annotators rated all potentialtypes as good; and?
queries where judges rated none of the potentialtypes as goodThe final test sets consist of 105 head queries with359 judged entity types and 98 tail queries with 343judged entity types.4.4 MetricsOur task is a ranking task and therefore the classicIR metrics nDCG (normalized discounted cumula-tive gain) and MAP (mean average precision) areapplicable (Manning et al, 2008).Both nDCG and MAP are sensitive to the rankposition, but not the score (probability of a type) as-sociated with each rank, S(r).
We therefore alsoevaluate a weighted mean average precision scoreMAPW, which replaces the precision componentof MAP, P (r), for the rth ranked type by:P (r) =?rr?=1 I(r?)S(r?
)?rr?=1 S(r?
)(2)where I(r) indicates if the type at rank r is judgedcorrect.Our fourth metric is Prec@1, i.e.
the precision ofonly the top-ranked type of each query.
This is espe-cially suitable for applications where a single sensemust be determined.4.5 Model SettingsWe trained all models in Figure 1 using the trainingdata from Section 4.2 over 100 EM iterations, withtwo folds per model.
For Model IM, we varied thenumber of user intents (K) in intervals from 100 to400 (see Figure 3), under the assumption that multi-ple intents would exist per entity type.We compare our results against two baselines.The first baseline is an assignment of Freebase typesaccording to their frequency in our query set BFB,and the second is Model Guo?09 (Guo et al, 2009)illustrated in Figure 1.5 Experimental ResultsTable 2 lists the performance of each model on theHEAD and TAIL sets over each metric defined inSection 4.4.
On head queries, the addition of theempty context parameter ?
and click signal ?
to-gether (Model M1) significantly outperforms boththe baseline and the state-of-the-art model Guo?09.Further modeling the user intent in Model IM re-sults in significantly better performance over allmodels and across all metrics.
Model IM showsits biggest gains in the first position of its ranking asevidenced by the Prec@1 metric.We observe a different behavior on tail querieswhere all models significantly outperform the base-line BFB, but are not significantly different fromeach other.
In short, the strength of our proposedmodel is in improving performance on the head atno noticeable cost in the tail.We separately tested the effect of adding theempty context parameter ?.
Figure 2 illustrates theresult on the HEAD data.
Across all metrics, ?
im-proved performance over all models3.
The moreexpressive models benefitted more than the less ex-pressive ones.Table 2 reports results for Model IM using K =200 user intents.
This was determined by varyingK and selecting the top-performing value.
Figure 3illustrates the performance of Model IM with dif-ferent values of K on the HEAD.3Note that model M0 is just the addition of the ?
parameterover Guo?09.56900.020.040.060.080.10.120.140.16M0 M1 IMRelativegain of switchvs.noswitchEffect of Empty Switch Parameter (s) on HEADNo switchnDCGMAPMAPWPrec@1Figure 2: The switch parameter ?
improves performanceof every model and metric.00.10.20.30.40.50.60.70.80.91 Varying K (latent intents) -  TAIL0.60.650.70.750.80.850.90.951100 150 200 300 400KModel IM -  Varying K (latent intents)nDCGMAPMAPWPrec@1Figure 3: Model performance vs. the number of latentintents (K).Our models can also assign a prior type distribu-tion to each entity by further marginalizing Eq.
1over query contexts n1 and n2.
We measured thequality of our learned type priors using the subsetof queries in our HEAD test set that consisted ofonly an entity without any refiners.
The results forModel IM were: nDCG = 0.86, MAP = 0.80,MAPW = 0.75, and Prec@1 = 0.70.
All met-rics are statistically significantly better than BFB,Guo?09 and M0, with 95% confidence.
Comparedto Model M1, Model IM is statistically signifi-cantly better on Prec@1 and not significantly dif-ferent on the other metrics.Discussion and Error Analysis: Contrary toour results, we had expected improvements forboth HEAD and TAIL.
Inspection of the TAILqueries revealed that entities were greatly skewedtowards people (e.g., actor, author, andpolitician).
Analysis of the latent user in-tent parameter ?
in Model IM showed that mostpeople types had most of their probability massassigned to the same three generic and common in-tents for people types: ?see pictures of?, ?find bio-graphical information about?, and ?see video of?.
Inother words, latent intents in Model IM are over-expressive and they do not help in differentiatingpeople types.The largest class of errors came from queriesbearing an entity with semantically very similartypes where our highest ranked type was not judgedcorrect by the annotators.
For example, for thequery ?philippine daily inquirer?
our system rankednewspaper ahead of periodical but a judgerejected the former and approved the latter.
For?ikea catalogue?, our system ranked magazineahead of periodical, but again a judge rejectedmagazine in favor of periodical.An interesting success case in the TAIL is high-lighted by two queries involving the entity ?ymca?,which in our data can either be a song, place,or educational institution.
Our systemlearns the following priors: 0.63, 0.29, and 0.08,respectively.
For the query ?jamestown ymca ny?,IM correctly classified ?ymca?
as a place and forthe query ?ymca palomar?
it correctly classified itas an educational institution.
We furtherissued the query ?ymca lyrics?
and the type songwas then highest ranked.Our method is generalizable to any entity collec-tion.
Since our evaluation focused on the Freebasecollection, it remains an open question how noiselevel, coverage, and breadth in a collection will af-fect our model performance.
Finally, although wedo not formally evaluate it, it is clear that trainingour model on different time spans of queries shouldlead to type distributions adapted to that time period.6 ConclusionJointly modeling the interplay between the under-lying user intents and entity types in web searchqueries shows significant improvements over thecurrent state of the art on the task of resolving entitytypes in head queries.
At the same time, no degrada-tion in tail queries is observed.
Our proposed modelscan be efficiently trained using an EM algorithm andcan be further used to assign prior type distributionsto entities in an existing knowledge base and to in-sert new entities into it.Although this paper leverages latent intents insearch queries, it stops short of understanding thenature of the intents.
It remains an open problemto characterize and enumerate intents and to iden-tify the types of queries that benefit most from intentmodels.570ReferencesEnrique Alfonseca, Marius Pasca, and Enrique Robledo-Arnuncio.
2010.
Acquisition of instance attributesvia labeled and related instances.
In Proceedings ofSIGIR-10, pages 58?65, New York, NY, USA.Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Men-doza.
2004.
Query recommendation using query logsin search engines.
In EDBT Workshops, Lecture Notesin Computer Science, pages 588?596.
Springer.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a collabo-ratively created graph database for structuring humanknowledge.
In Proceedings of SIGMOD ?08, pages1247?1250, New York, NY, USA.Andrei Broder.
2002.
A taxonomy of web search.
SIGIRForum, 36:3?10.Mark James Carman, Fabio Crestani, Morgan Harvey,and Mark Baillie.
2010.
Towards query log based per-sonalization using topic models.
In CIKM?10, pages1849?1852.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsu-pervised named-entity extraction from the web: Anexperimental study.
volume 165, pages 91?134.Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.2011.
Clickthrough-based latent semantic models forweb search.
In Proceedings of SIGIR ?11, pages 675?684, New York, NY, USA.
ACM.Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li.
2009.Named entity recognition in query.
In Proceedingsof SIGIR-09, pages 267?274, New York, NY, USA.ACM.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th International Conference on ComputationalLinguistics, pages 539?545.Jian Hu, Gang Wang, Frederick H. Lochovsky, Jian taoSun, and Zheng Chen.
2009.
Understanding user?squery intent with wikipedia.
In WWW, pages 471?480.Alpa Jain and Marco Pennacchiotti.
2011.
Domain-independent entity extraction from web search querylogs.
In Proceedings of WWW ?11, pages 63?64, NewYork, NY, USA.
ACM.Bernard J. Jansen, Danielle L. Booth, and Amanda Spink.2007.
Determining the user intent of web search en-gine queries.
In Proceedings of WWW ?07, pages1149?1150, New York, NY, USA.
ACM.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008.Semantic class learning from the web with hyponympattern linkage graphs.
In Proceedings of ACL.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1990.Wordnet: An on-line lexical database.
volume 3,pages 235?244.Marius Pas?ca.
2007.
Weakly-supervised discovery ofnamed entities using web search queries.
In Proceed-ings of the sixteenth ACM conference on Conferenceon information and knowledge management, CIKM?07, pages 683?690, New York, NY, USA.
ACM.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In SIGKDD, pages 613?619, Ed-monton, Canada.Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.2004.
Towards terascale knowledge acquisition.
InCOLING, pages 771?777.Filip Radlinski, Martin Szummer, and Nick Craswell.2010.
Inferring query intent from reformulations andclicks.
In Proceedings of the 19th international con-ference on World wide web, WWW ?10, pages 1171?1172, New York, NY, USA.
ACM.Altaf Rahman and Vincent Ng.
2010.
Inducing fine-grained semantic classes via hierarchical and collec-tive classification.
In Proceedings of COLING, pages931?939.Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009.What is this, anyway: Automatic hypernym discov-ery.
In Proceedings of AAAI-09 Spring Symposium onLearning by Reading and Learning to Read, pages 88?93.Daniel E. Rose and Danny Levinson.
2004.
Under-standing user goals in web search.
In Proceedings ofthe 13th international conference on World Wide Web,WWW ?04, pages 13?19, New York, NY, USA.
ACM.Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, andHinrich Schu?tze.
2011.
Piggyback: Using searchengines for robust cross-domain named entity recog-nition.
In Proceedings of ACL ?11, pages 965?975,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24:97?123,March.Satoshi Sekine and Hisami Suzuki.
2007.
Acquiring on-tological knowledge from query logs.
In Proceedingsof the 16th international conference on World WideWeb, WWW ?07, pages 1223?1224, New York, NY,USA.
ACM.Xiaoxin Yin and Sarthak Shah.
2010.
Building taxon-omy of web search intents for name entity queries.
InWWW, pages 1001?1010.Z.
Zhang and O. Nasraoui.
2006.
Mining search en-gine query logs for query recommendation.
In WWW,pages 1039?1040.571
