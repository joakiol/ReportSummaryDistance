BBN BYBLOS and HARCFebruary 1992 ATIS Benchmark ResultsFrancis Kubala, Chris Barry, Madeleine Bates, Robert Bobrow, Pascale Fung,Robert Ingria, John Makhoul, Long Nguyen, Richard Schwartz, David StallardBBN Systems and  Techno log iesCambr idge  MA 02138ABSTRACTWe present results from the February '92 evaluation on the ATIStravel planning domain for HARC, the BBN spoken language sys-tem (SLS).
In addition, we discuss in detail the individual perfor- 2.mance of BYBLOS, the speech recognition (SPREC) component.In the official scoring, conducted by NIST, BBN's HARC system 3.produced aweighted SLS score of 43.7 on all 687 evaluable utter-ances in the test set.
This was the lowest error achieved by any ofthe 7 systems evaluated.4.For the SPREC evaluation BBN's BYBLOS system achieved aword error rate of 6.2% on the same 687 utterances and 9.4% onthe entire test set of 971 utterances.
These results were significantlybetter than any other speech system evaluated.1.
OVERVIEWThe BBN HARC spoken language system consists of BY-BLOS, the speech recognition component, and DELPHI,the natural anguage processing component.
In this paper,we concentrate on BYBLOS and its interaction with DEL-PHI through the N-best interface.
Results are presented forspeech recognition alone and for the overall spoken languagesystem.
A detailed discussion of DELPHI is presented in\[2,3\] elsewhere in these proceedings.2.
BYBLOS - SPEECH RECOGNIT IONThe BYBLOS speech recognition system produces an or-dered list of the N top-scoring hypotheses which is thenreordered by several detailed knowledge sources.
The N-best strategy \[4,8\] permits the use of computationally pro-hibitive models by greatly reducing the search space to afew dozen word sequences.
It has enabled us to use cross-word-boundary triphone models and trigram language mod-els with ease.
The N-best list is also a robust interfacebetween speech and natural anguage that provides a way torecover from speech errors in the top choice word sequence.The overall system architecture for this evaluation is similarto that used in the February '91 tests \[6\].
Specifically, weuse a 4-pass approach to produce the N-best lists for naturallanguage processing.1.
A forward pass with a bigram grammar and discreteHMM phonetic models saves the top word-endingscores and times.A backward pass with a bigram produces an inital N-best list.Rescofing each of the N sentence hypotheses withcross-word-boundary triphones and semi-continuousdensity HMMs reorders the N-best list.Resconng with a trigram grammar reorders the N-bestlist again.Each utterance is quantized and decoded twice, once witheach gender-dependent codebook and model.
For each utter-ance, the N-best list with the higher top-1 hypothesis coreis chosen.
Then they are passed to DELPHI for further e-ordering and interpretation while the top choices in the listsconstitute the SPREC results reported here.2.1 Training and Development Test DataWe used speech data from the ATIS2 subcorpus exclusivelyto train the parameters of the acoustic model.
This subcorpusconsists of 10411 spontaneous tterances from 286 subjects.The data originated from 5 collection sites using a varietyof strategies for eliciting and capturing spontaneous queriesfrom the subjects \[7\].
The training data was not balancedacross the five sites, however.
MIT was represented by3-4 times as much data as any other site.
Overall, MITdata accounted for nearly half of the ATIS2 subcorpus (4600utterances).The evaluation test data was drawn from this same pool ofdata so we decided to ignore the earlier batches of ATISdata that were collected under still different circumstances(most of it was read speech) and would not be representedin the new test (dialects were predominantly southern in theATIS0 subcorpus).We filtered the training data for quality in several ways.All utterances that were marked as truncated in the SRO(speech recognition output) transcription were ignored.
Sim-ilarly, we omitted from the training all utterances that con-tained a word fragment, We also ignored any utterances72that contained rare nonspeech events.
Finally, our forward-backward training program rejected any input that failed toalign properly.
These steps removed about 1200 utterancesfrom consideration.Another 600 utterances were removed ue to name conflictsbetween a number of subjects from AT&T and MIT thatwere given identical speaker codes, thus making it difficultto match the speech utterances with the transcriptions.We held another 890 utterances out of the training as a devel-opment est set.
We included 2 male and 2 female subjectsfrom each of the 5 collection sites in this set.
Each speakerhad roughly 40 utterances.This left a total of 7670 utterances from 237 speakers fortraining the HMMs.
Since we train gender-dependent mod-els, the training was further divided into 3317 utterances forthe female speakers and 4349 for the males.For statistical language model training we used all availablesentence texts from ATIS0, ATIS1, and ATIS2.
During thedevelopment phase, we excluded the 890 sentences fromthe held-out est set.
For the final evaluation this data wasincluded, resulting in a total of 14500 sentences for trainingthe language models.2.2 Recognition Lexicon and GrammarsThe lexicon used for recognition was initialized by includ-ing all words observed in the complete grammar ~ainingtexts.
This had the side-effect of including the entire de-velopment test set within the vocabulary.
Common closed-classes of words such as days of the week, months, numbers,plane types, etc.
were completed by hand.
Similarly, we in-cluded derivations (mostly plurals and possessives) of manyopen-class words in the domain.
We also added about 400concatenated word tokens for the commonly occurring se-quences uch as WASHINGTON_D_C, SAN_FRANCISCO,or D_C_TEN.
The final size of the lexicon was 1881 words.For the February '92 evaluation test set only 35 words, oc-curring 42 times, were out-of-vocabulary (OOV) for thislexicon.
This is only a 0.4% OOV word occurrence rateover the whole test set.We estimated the parameters of our statistical bigram andtrigram grammars using a backing-off procedure similar toKatz \[5\].
The N-grams were computed on word classes inorder to share the very sparse training.
A total of 1054semantic lasses were defined (most words remained sin-gletons in their class).
The perplexity of these grammarsas measured on the evaluation test set (ignoring out-of-vocabulary words) is summarized in Table 1.
The perplex-ities have been measured separately on each of the threesentence classes in the test.
The trigram language modelconsistently, but rather modestly, reduced perplexity acrossSentence Bigram TrigramClass Perplexity PerplexityA+DA+D+XADX17201520351215101428Table 1: N-gram perplexities on the February '92 test set.all three classes.
We did observe that recognition perfor-mance consistently improved with the trigrarn model.More striking are the differences between the perplexities ofthe three sentence classes and the large values for the Class Xsentences (those which are unevaluable with respect o thedatabase).
We observed that our recognition performancewas well correlated with these measured perplexities.2.3 Automatic EndpointingWe estimated that 35% of the entire corpus is ambient noise.Part of the high ambient-to-speech ratio is due to the variousways in which the data was collected.Several different strategies were employed by the collect-ing sites for endpointing the waveforms, including subject-initiated push-to-talk and push-and-hold, as well as auto-matic endpointing, and wizard-initiated manual endpointing.This led to highly variable and often very long leaders ofambient noise on both ends of the waveforms.
In addition,these segments frequently contained a variety of nonspeechevents.We employed the speech-detector that we use in our real-time recognizer front-end to remove most of the long du-ration intervals of ambient noise from the input.
This stepmakes all subsequent processing faster and avoids many spu-rious word insertions in the results.
The parameters of thedetector were set on a small sample of ATIS data from 4 col-lection sites in October '91.
These same parameters wereused for all data processed thereafter, including all of thedata from AT&T, none of which was included in the pa-rameter tuning sample.
Although we have carefully verifiedthat the detector is working properly on only very smallsamples of data, we believe it is quite accurate since we donot observe many errors at the ends of utterances.2.4 Automatic Location of SilenceAnother eason for the prevalence of ambient noise is due tothe subjects having difficulty satisfying the simulated travelplanning problem given them to elicit spontaneous utter-73ances.
Hence, the ATIS2 corpus is marked with a greatnumber of hesitations and pauses in the speech - many ofwhich are quite long in duration.We observed that the marking of such pauses in the SROtranscriptions was highly inconsistent.
Some transcriberssimply neglected to make detailed markings altogether,while others marked only the extremely long pauses.
Sincewe do not allow silence to appear optionally between wordsin our training algorithm, we believed that the large num-ber of unmarked pauses could degrade our phonetic models.Therefore, we devised a procedure to automatically locatethe remaining pauses in the training data and thereby correctthe corresponding transcriptions.1.
Train an initial model as usual, ignoring unmarkedpauses.2.
Run the recognizer constrained to the correct answeron the training data, but allow optional silence betweenevery word.3.
Retrain using the recognized output from (2) as thecorrected transcriptions.We found that a large positive bias was needed in step 2above to induce the recognizer to hypothesize inter-wordsilences.
Since the initial model was trained disallowingmany real pauses, the corrupted phonetic models easily ab-sorb many frames of silence.
We adjusted the bias by com-paring the recognizer's output on a sample of speech withknown pause locations.
Although the hypothesized pauselocations never matched the truth exactly, we did observe a15% reduction in word error rate on a very early develop-ment condition.
Specifically, it improved the performance ofthe cross-word-boundary rescoring stage, whereas the non-cross-word N-best stage did not change.
This is entirelyconsistent with the correction we made; only the cross-word-boundary models were corrupted by unlabeled inter-wordpauses.2.5 Nonspeech EventsThis corpus is also notable for its large number and varietyof nonspeech events audible in the waveforms.
The phe-nomena run the gamut from common filled pauses (um'sand uh's), throat clearings, coughing, and laughter, to un-intelligible mutterings of 20 seconds duration.
There wereover 175 different markings for nonspeech events in the SROtranscriptions.
While these events are typical of casual con-versational speech between people, their high frequency andseverity in this corpus are likely a consequence of the factthat nearly all subjects were completely new to speech andnatural language technologies and had little or no training inhow to speak or specific feedback about heir speech qualityfrom the system.To handle the nonspeech events, we first identified those thatappeared to have enough training samples to make a robustestimate of the HMM paraameters.
We then mapped a widevariety of marked events into a set of generic nonspeech to-kens: \[AMB_NOISE\], \[BREATH_NOISE\], \[MIC_NOISE\],and \[MOUTH_NOISE\].
In all, we attempted to model only11 unique nonspeech events.
All nonspeech tokens wereassigned to the same class in the grammar.We tried 3 different ways to use nonspeech models in thesystem:1.
Treat nonspeech as normal words, including estimatingN-gram probabilities for them.2.
Treat nonspeech as normal words in acoustic train-ing, but do not include them in the grammar training,thereby making them very unlikely.3.
Treat nonspeech like silence, allowing them optionallybetween any words with fixed grammar transition prob-abilities.Although method 3 was intuitively more appealing andknown to work rather well for silence, it was the least effec-tive of the three approaches for nonspeech.
As was the caselast year \[6\], when we try to recognize nonspeech eventsaccurately the false alarm rate is high enough to offset anypotential gain.
The most successful was method 2 whicheffectively disallowed nonspeech in the decoder output.Modeling nonspeech events carefully may not be importantfor another eason - there are not enough errors due to non-speech events.
There are only about 120 actual nonspeechevents in the test SROs.
There are 184 marked, but 66 ofthem are \[POP\]s at the beginnings of utterances that aren'treally in the dam!
Apparently the transcriber was markingpops caused by the D/A system during playback.If these markings do not greatly underestimate he true fre-quency of occurrence, then there is relatively little to begained by modeling nonspeech accurately.
Moreover, of theremaining 118 nonspeech events, half are breath noise orambient noise at levels that do not interfere with our recog-nition.We have indeed noticed that we make errors around mostof the long or loud filled-pauses.
But there are only 55filled pauses (as indicated by the SRO) in the 971 utteranceevaluation test set.
These are primarily from 2-3 speakers.Given that we have nearly 1000 word errors across the entiretest set, modeling filled-pauses well will have a very smallimpact on overall performance at this point.742.6 February '92 Evaluation ConditionsThe February '92 evaluation test set has data from 37 speak-ers.
20 subjects were female and 17 were male.
The numberof utterances per speaker varied from 5 to 64, but the num-ber of utterances from each of the 5 data-collection sites wascarefully balanced.
All results given are for the Sennheiserchannel (same as the training data).
The recognition modewas speaker-independent - the test speakers were not in thetraining set.By committee decision there was no common baseline con-trol condition for the February '92 ATIS tests.
The onlyconstraint was that the single common test set must be used.Under these circumstances, there is a strong temptation totry to train on as much material as one can.
We have resistedthis temptation for two reasons.
First, we feel that the sim-ple addition of training data for incremental improvementsis scientifically uninteresting.
Secondly, in our experiencewith the current definition of ATIS, we have seen very littleimprovement for increasing the training data beyond about4000 utterances.Last year, we attempted to improve on the February '91common baseline by augmenting the 3700 common acous-tic training utterances with 9000 more collected from 15speakers at BBN.
The resulting performance improvementwas statistically insignificant.
Since the test subjects werecollected at TI and had predominantly southern dialects, ourconclusion was that the additional training we collected atBBN did not match the test data sufficiently to be of muchuse.
When the training data does match the test, we normallyexpect a quadrupling of the training data to yield a halvingof the error rate.
This result made it clear to us that simplyincreasing the amount of training has limited scientific andpractical value.Recently we had another demonstration f the rather weakcontribution of additional training data in the ATIS domain.As the ATIS2 data became available, we moved from a pi-lot development training set of 4100 utterances to our finalset of 7700 utts.
On a common development test set, weobserved no significant gain for nearly doubling the trainingdata, even though the additional data matched the test con-ditions\[ Moreover, we observed that data originating from aparticular site primarily improved performance only on testdata from the same site.2.7 Speech Recognition ResultsOfficial results for BYBLOS on this evaluation are givenin Table 2.
The performance is shown for two compositeresults and as a function of utterance class type.
The 6.2%word error rate on class A+D sentences and the combinedA+D+X error rate of 9.4% were significantly better than anyother speech system reporting on this data.
The individualSentence # Word % WordClass # Sentences Errors ErrorsA+DA+D+XADX68797140228528450110153051965146.29.45.87.017.2Table 2: BYBLOS Official SPREC results on the February'92 test set.speaker esults varied widely from 0.0% word error to 30%error with the median at about 7.5% The female speakersgot significantly better esults than the male speakers.Performance on the class X utterances i markedly worsethan either class A or D utterances.
In fact, more than halfof the speech errors occur on these utterances.
The ratio ofthe error rate for class X utterances to other utterances ihigher than we have ever seen.
Since these utterances arenot evaluable by the natural language component, i  does notseem profitable to try to improve the speech performance onthese utterances for a spoken language system.In Table 3 we observe a large variation in overall perfor-mance on the class A + D utterances for each segment ofthe test data originating at a given collection site, as shown inthe righlmost column.
We believe that most of this variationcan be explained by two easily measured factors - amountof training data from the matching site, and the number oferrors due to all spontaneous speech effects.
The actual# UttsSite TrainingMIT 3700BBN 1400CMU 1000SRI i 800Ar ri 800% Word Error Due To:ModelingDeficiency2.74.55.35.76.4SpontaneousEffects0.50.80.52.04.0Overall% WordError3.25.35.87.710.4Table 3: BYBLOS performance on February '92 test as afunction of originating site (class A + D).number of training utterances that we used from each site isshown in Table 3.
The next column shows the word errorrate that we attribute to general modeling deficiencies afterremoving those errors that we judged were due to sponta-neous speech effects.
The variation due to modeling seemswell correlated to the amount of training data available from75each site.
The numbers how the expected halving of theerror :rate for a quadrupling of the training data.
In partic-ular, we feel that the higher performance on the M1T datacan be explained entirely by the increased amount of datafrom that particular site.The errors due to spontaneous speech effects in Table 3were counted by matching the output of BYBLOS againstthe SRO transcriptions.
The SROs contain specific markingsfor many spontaneous speech effects including: nonspeechevents, word fragments, mispronunciations, emphatic stress,lengthening, and verbal deletions.
Any error that occurred inthe immediate vicinity of such a marking was counted as anerror due to spontaneous speech.
The table shows that thenoticably worse performance on data from SRI and AT&Tcan be explained by the larger proportion of errors due tospontaneous speech effects.
It also shows that errors due tospontaneous speech effects account for only about 22% ofthe total.In order to calibrate our recent improvements, we retestedon the October '91 dry-run test set.
The current systemgives a word error rate of 7.8% whereas our unofficial resultin October was 14.4% word error.
(Note that we did notuse the ATIS2 speech to train the system for the October'91 dry-run test.)
We attribute our improvement to severalimportant factors:1.
More appropriate training material - ATIS2 multi-sitespontaneous data instead of read ATIS0 data from TIand BBN,2.
A trigram language model - versus a bigram in October,3.
Automatic location of silences in the training data.Note that the quantity of ATIS2 training data used (7700utts) is only half the amount used to estimate the modelused for the October '91 dry-run (about 13,500 utts).
Clearlythe quality of the training material is an important factor inperformance.2.
HARC - SPOKEN LANGUAGEUNDERSTANDINGHARC, BBN's spoken language system, utilizes BYBLOSas its speech recognition component, and DELPHI as itsnatural anguage understanding component.
DELPHI uses adefinite clause grammar formalism, augmented by the use ofconstraint nodes \[9\] and a labelled argument formalism \[3\].The parsing algorithm uses a statistically trained agenda toproduce the single best parse for an input utterance \[1\].We experimented with several conditions to optimize theconnection of BYBLOS with DELPHI.
The basic interfacebetween speech and natural language in HARC is the N-best list.
Previously, we had allowed the natural anguagecomponent to search arbitrarily far down the N-best list untilit either found a hypothesis that produced a database retrievalor reached the end of the N-best list.
For this evaluation, weexplored the nature of this connection in more detail.
'theparameters we varied were:?
the depth of the search that NL performed on the N-bestoutput of speech?
the processing strategy used by NL on the speech outputIn our earlier work in integrating speech and natural an-guage, we had noticed that while it was beneficial for NLto look beyond the first hypothesis in an N-best list, the an-swers obtained by NL from speech output ended to degradethe further down in the N-best list they were obtained.
Dur-ing this last period, we performed a number of experimentsto determine the break-even point for NL search.
We usedan N of 1, 5, 10, and 20 in our experiments.During our recent development work, we utilized a numberof fall-hack strategies for NL text processing \[2\].
In ap-plying these fall-hack strategies to speech output, we exam-ined the trade-off between processing speech output with amore restrictive scheme, and thereby potentially discardingmeangingful utterances vs. processing speech output witha more forgiving strategy, and thereby potentially allowingin meaningless or misleading utterances.
We experimentedwith three processing strategies:fallback processing turned offfallback processing turned ona combined strategy, in which an initial pass with madewith fallback processing turned off.
I f  no hypothesisproduced a database retrieval, a second pass was made,with the fallback strategy engaged.We show the results of one such experiment, utilizing theOctober '91 dry-run corpus as development test in Table4.
The results of our experiments indicated that an Nof 5 was optimal, and that the two-pass processing strategywas slightly better than either of the others.
This was theconfiguration we used on the February '92 evaluation data.In Table 5 we show our Weighted Error on the February '92evaluation data for Combined Class A+D, and Classes Aand D separately, as calculated by NIST.
During the test run,we had neglected to include the date information providedfor individual scenarios.
We include the results of a re-runwith the same system as ran the February '92 test set, with76Condition N WEText (1) 47.9FaUback on 1 64.6" 5 58.0" 20 60.1Fallback off 1 64.2" 5 56.9" 20 59.0Two Pass 5 56.6Table 4: SLS weighted error (WE) on the October '91 dry-run test set with varying N-best list length (N).the only change being the inclusion of the date information.Interestingly, the lack of date information only affected 3utterances, which were given False answers without he date,and True answers with it.Corpus Official WE WE with dateA+D 43.7 42.8A 35.8 34.8D 54.7 54.0Table 5: SLS weighted error (WE) on the February '92 testset.4.
SUMMARYWe have shown superior speech recognition performancewith only a modest amount of training speech by aggres-sively handling the idiosyncrasies of this corpus.
All utter-ances that are degraded ue to severe disfluencies or prob-lems with data-capture are eliminated from the training set.The excessively long and numerous egments of ambientnoise in the data are removed from consideration by a goodspeech detector in the front-end.
The very numerous hesita-tion phenomena are automatically ocated and then explic-itly modeled where they occur in the training.
Nonspeechevents, such as filled-pauses, are made very unlikely in thegrammar to clamp the false alarm rate.In addition, the trigram language model on word classessignificantly improved recognition performance compared toa bigram model.With these improvements, the official BYBLOS speechrecognition results for the February '92 DARPA evaluationwere 6.2% word error for the Class A+D subset of the testand 9.4% overall.
Both of these results were significantlybetter than any other speech system tested.Finally, we have shown how the N-best interface betweenthe speech and natural components reduces the error ratecompared to considering the top choice only.
This wasshown to be true whether a robust fragment processor wasused as a fall-back or not.The official SLS result for HARC was a weighted error of43.7.
This was the best overall result for a spoken languagesystem in the February '92 DARPA evaluation.ACKNOWLEDGEMENTThis work was supported by the Defense Advanced ResearchProjects Agency and monitored by the Office of Naval Re-search under Contract No.
N00014-89-C-0008.REFERENCES1.
Bobrow, R. "Statistical Agenda Parsing", in Speech and Nat-ural Language: Proceedings of a Workshop Held at PacOfcGrove, California, February 19-22, 1991, Morgan KaufmannPublishers, Inc., San Marco, California, pp.
222-224.2.
Bobrow R., D. Stallard, "Fragment Processing in the DELPHISystem", elsewhere in these proceedings.3.
Bobrow, R., R. Ingria, and D. Stallard "Syntactic/SemanticCoupling in the DELPHI System", elsewhere in these pro-ceedings.4.
Chow, Y-L. and R.M.
Schwartz, "The N-Best Algorithm:An Efftieient Procedure for Finding Top N Sentence Hypothe-ses", Proceedings of the DARPA Speech and Natural Lan-guage Workshop, Morgan Kanfmann Publishers, Inc., Oct.1989.5.
Katz, S., "Estimation of Probabiliities from Sparse Data forthe Language Model Component of a Speech Recognizer",IEEE Transactions on Acoustics, Speech, and Signal Process-ing, Mar.
1987, Vol.
35, No.
3.6.
Kubala, F., S. Austin, C. Barry, J. Makhoul, P. Placeway,R.
Schwartz, "BYBLOS Speech Recognition Benchmark Re-suits", Proceedings of the DARPA Speech and Natural Lan-guage Workshop, Morgan Kaufmann Publishers, Inc., Feb.1991.7.
MADCOW, "Multi-Site Data Collection for a Spoken Lan-guage Corpus", elsewhere in these proceedings.8.
Schwartz, R.M., and S.A. Austin, "Efficient, High-Performance Algorithms for N-Best Search", Proceedings ofthe~ DARPA Speech and Natural Language Workshop, MorganKaufmann Publishers, Inc., Jun.
1990.9.
StaUard, D. "Unification-Based Semantic Interpretation i  theBBN Spoken Language System", in Speech and Natural Lan-guage: Proceedings of a Workshop Held at Cape Cod, Mas-sachusetts, October 15-18, 1989, Morgan Kaufmann, Pub-lishers, Inc., San Mateo, California, pp.
39-46.77
