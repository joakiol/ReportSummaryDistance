Proceedings of the ACL-HLT 2011 Student Session, pages 122?126,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsK-means Clustering with Feature HashingHajime SenumaDepartment of Computer ScienceUniversity of Tokyo7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japanhajime.senuma@gmail.comAbstractOne of the major problems of K-means isthat one must use dense vectors for its cen-troids, and therefore it is infeasible to storesuch huge vectors in memory when the featurespace is high-dimensional.
We address this is-sue by using feature hashing (Weinberger etal., 2009), a dimension-reduction technique,which can reduce the size of dense vectorswhile retaining sparsity of sparse vectors.
Ouranalysis gives theoretical motivation and jus-tification for applying feature hashing to K-means, by showing how much will the objec-tive ofK-means be (additively) distorted.
Fur-thermore, to empirically verify our method,we experimented on a document clusteringtask.1 IntroductionIn natural language processing (NLP) and text min-ing, clustering methods are crucial for various taskssuch as document clustering.
Among them, K-means (MacQueen, 1967; Lloyd, 1982) is ?the mostimportant flat clustering algorithm?
(Manning et al,2008) both for its simplicity and performance.One of the major problems of K-means is that ithas K centroids which are dense vectors where Kis the number of clusters.
Thus, it is infeasible tostore them in memory and slow to compute if the di-mension of inputs is huge, as is often the case withNLP and text mining tasks.
A well-known heuris-tic is truncating after the most significant features(Manning et al, 2008), but it is difficult to analyzeits effect and to determine which features are signif-icant.Recently, Weinberger et al (2009) introduced fea-ture hashing, a simple yet effective and analyzabledimension-reduction technique for large-scale mul-titask learning.
The idea is to combine featureswhich have the same hash value.
For example, givena hash function h and a vector x, if h(1012) =h(41234) = 42, we make a new vector y by set-ting y42 = x1012 + x41234 (or equally possiblyx1012?x41234,?x1012+x41234, or?x1012?x41234).This trick greatly reduces the size of dense vec-tors, since the maximum index value becomesequivalent to the maximum hash value of h. Further-more, unlike random projection (Achlioptas, 2003;Boutsidis et al, 2010), feature hashing retains spar-sity of sparse input vectors.
An additional usefultrait for NLP tasks is that it can save much memoryby eliminating an alphabet storage (see the prelim-inaries for detail).
The authors also justified theirmethod by showing that with feature hashing, dot-product is unbiased, and the length of each vectoris well-preserved with high probability under someconditions.Plausibly this technique is useful also for clus-tering methods such as K-means.
In this paper, tomotivate applying feature hashing to K-means, weshow the residual sum of squares, the objective ofK-means, is well-preserved under feature hashing.We also demonstrate an experiment on documentclustering and see the feature size can be shrunk into3.5% of the original in this case.1222 Preliminaries2.1 NotationIn this paper, || ?
|| denotes the Euclidean norm, and?
?, ??
does the dot product.
?i,j is the Kronecker?sdelta, that is, ?i,j = 1 if i = j and 0 otherwise.2.2 K-meansAlthough we do not describe the famous algorithmof K-means (MacQueen, 1967; Lloyd, 1982) here,we remind the reader of its overall objective forlater analysis.
If we want to group input vec-tors into K clusters, K-means can surely outputclusters ?1, ...?K and their corresponding vectors?1, ...,?K such that they locally minimize the resid-ual sum of squares (RSS) which is defined asK?k=1?x??k||x?
?k||2.In the algorithm, ?k is made into the mean of thevectors in a cluster ?k.
Hence comes the name K-means.Note that RSS can be regarded as a metric sincethe sum of each metric (in this case, squared Eu-clidean distance) becomes also a metric by con-structing a 1-norm product metric.2.3 Additive distortionSuppose one wants to embed a metric space (X, d)into another one (X ?, d?)
by a mapping ?.
Its ad-ditive distortion is the infimum of  which, for anyobserved x, y ?
X , satisfies the following condition:d(x, y)?
 ?
d?(?
(x), ?
(y)) ?
d(x, y) + .2.4 Hashing tricksAccording to an account by John Langford 1, aco-author of papers on feature hashing (Shi et al,2009; Weinberger et al, 2009), hashing tricks fordimension-reduction were implemented in variousmachine learning libraries including Vowpal Wab-bit, which he realesed in 2007.Ganchev and Dredze (2008) named their hashingtrick random feature mixing and empirically sup-ported it by experimenting on NLP tasks.
It is simi-lar to feature hashing except lacking of a binary hash1http://hunch.net/?jl/projects/hash_reps/index.htmlfunction.
The paper also showed that hashing tricksare useful to eliminate alphabet storage.Shi et al (2009) suggested hash kernel, that is,dot product on a hashed space.
They conducted thor-ough research both theoretically and experimentally,extending this technique to classification of graphsand multi-class classification.
Although they testedK-means in an experiment, it was used for classifi-cation but not for clustering.Weinberger et al (2009) 2 introduced a techniquefeature hashing (a function itself is called the hashedfeature map), which incorporates a binary hash func-tion into hashing tricks in order to guarantee the hashkernel is unbiased.
They also showed applicationsto various real-world applications such as multitasklearning and collaborative filtering.
Though theirproof for exponential tail bounds in the original pa-per was refuted later, they reproved it under someextra conditions in the latest version.
Below is thedefinition.Definition 2.1.
Let S be a set of hashable features,h be a hash function h : S ?
{1, ...,m}, and ?
be?
: S ?
{?1}.
The hashed feature map ?(h,?)
:R|S| ?
Rm is a function such that the i-th elementof ?(h,?
)(x) is given by?(h,?
)i (x) =?j:h(j)=i?
(j)xj .If h and ?
are clear from the context, we simplywrite ?(h,?)
as ?.As well, a kernel function is defined on a hashedfeature map.Definition 2.2.
The hash kernel ?
?, ???
is defined as?x,x???
= ??
(x), ?(x?
)?.They also proved the following theorem, whichwe use in our analysis.Theorem 2.3.
The hash kernel is unbiased, that is,E?[?x,x???]
= ?x,x?
?.The variance isV ar?[?x,x???]
=1m??
?i 6=jx2ix?2j + xix?ixjx?j??
.2The latest version of this paper is at arXiv http://arxiv.org/abs/0902.2206, with correction to Theorem3 in the original paper included in the Proceeding of ICML ?09.1232.4.1 Eliminating alphabet storageIn this kind of hashing tricks, an index of inputsdo not have to be an integer but can be any hash-able value, including a string.
Ganchev and Dredze(2008) argued this property is useful particularly forimplementing NLP applications, since we do notanymore need an alphabet, a dictionary which mapsfeatures to parameters.Let us explain in detail.
In NLP, features can beoften expediently expressed with strings.
For in-stance, a feature ?the current word ends with -ing?can be expressed as a string cur:end:ing (herewe suppose : is a control character).
Since indicesof dense vectors (which may be implemented witharrays) must be integers, traditionally we need a dic-tionary to map these strings to integers, which maywaste much memory.
Feature hashing removes thismemory waste by converting strings to integers withon-the-fly computation.3 MethodFor dimension-reduction to K-means, we proposea new method hashed K-means.
Suppose you haveN input vectors x1, ...,xN .
Given a hashed fea-ture map ?, hashed K-means runs K-means on?
(x1), ..., ?
(xN ) instead of the original ones.4 AnalysisIn this section, we show clusters obtained by thehashed K-means are also good clusters in the orig-inal space with high probability.
While Weinbergeret al (2009) proved a theorem on (multiplicative)distortion for Euclidean distance under some tightconditions, we illustrate (additive) distortion forRSS.
Since K-means is a process which monoton-ically decreases RSS in each step, if RSS is not dis-torted so much by feature hashing, we can expectresults to be reliable to some extent.Let us define the difference of the residual sum ofsquares (DRSS).Definition 4.1.
Let ?1, ...?K be clusters, ?1, ...,?Kbe their corresponding centroids in the originalspace, ?
be a hashed feature map, and ?
?1 , ...,?
?K betheir corresponding centroids in the hashed space.Then, DRSS is defined as follows:DRSS = |K?k=1?x??k||?(x)?
?
?k ||2?K?k=1?x??k||x?
?k||2|.Before analysis, we define a notation for the (Eu-clidean) length under a hashed space:Definition 4.2.
The hash length || ?
||?
is defined as||x||?
= ||?(x)||=???
(x), ?(x)?
=??x,x?
?.Note that it is clear from Theorem 2.3 thatE?[||x||2?]
= ||x||2, and equivalently E?[||x||2?
?||x||2] = 0.In order to show distortion, we want to use Cheby-shev?s inequality.
To this end, it is vital to know theexpectation and variance of the sum of squared hashlengths.
Because the variance of the sum of ran-dom variables derives from each covariance betweenpairs of variables, first we show the covariance be-tween the squared hash length of two vectors.Lemma 4.3.
The covariance between the squaredhash length of two vectors x,y ?
Rn isCov?
(||x||2?, ||y||2?)
=?(x,y)m,where?
(x,y) = 2?i 6=jxixjyiyj .This lemma can be proven by the same techniquedescribed in the Appendix A of Weinberger et al(2009).Now we see the following lemma.Lemma 4.4.
Suppose we have N vectorsx1, ...,xN .
Let us define X =?i ||xi||2?
?
?i ||xi||2 =?i(||xi||2?
?
||xi||2).
Then, for any > 0,P?
?|X| ??m????N?i=1N?j=1?(xi,xj)??
?12.124Proof.
This is an application of Chebyshev?s in-equality.
Namely, for any  > 0,P(|X ?
E?
[X]| ?
?V ar?
[X])?12.Since the expectation of a sum is the sum of ex-pectations we readily know the zero expectation:E?
[X] = 0.Since adding constants to the inputs of covariancedoes not change its result, from Lemma 4.3, for anyx,y ?
Rn,Cov?(||x||2?
?
||x||2, ||y||2?
?
||y||2) =?
(x,y)m.Because the variance of the sum of random vari-ables is the sum of the covariances between everypair of them,V ar?
[X] =1mN?i=1N?j=1?
(xi,xj).Finally, we see the following theorem for additivedistortion.Theorem 4.5.
Let ?
be the sum of ?
(x,y) for anyobserved pair of x,y, each of which expresses thedifference between an example and its correspond-ing centroid.
Then, for any ,P (|DRSS| ?
) ?
?2m.Thus, if m ?
?
?1??2 where 0 < ?
<= 1, withprobability at least 1?
?, RSS is additively distortedby .Proof.
Note that a hashed feature map ?(h,?)
is lin-ear, since ?
(x) = Mx with a matrix M suchthat Mi,j = ?
(i)?h(i),j .
By this liearlity, ?
?k =|?k|?1?x??k?
(x) = ?(|?k|?1?x?
?kx) =?(?k).
Reapplying linearlity to this result, we have||?(x)??
?k ||2 = ||x??k||2?.
Lemma 4.4 completesthe proof.The existence of ?
in the theorem suggests that touse feature hashing, we should remove useless fea-tures which have high values from data in advance.For example, if frequencies of words are used as0.20.250.30.350.40.450.50.550.60.650.710  100  1000  10000  100000  1e+06F5measurehash size mhashed k-meansFigure 1: The change of F5-measure along with the hashsizefeatures, function words should be ignored not onlybecause they give no information for clustering butalso because their high frequencies magnify distor-tion.5 ExperimentsTo empirically verify our method, from 20 News-groups, a dataset for document classification or clus-tering 3, we chose 6 classes and randomly drew 100documents for each class.We used unigrams and bigrams as features and ranour method for various hash sizes m (Figure 1).
Thenumber of unigrams is 33,017 and bigrams 109,395,so the feature size in the original space is 142,412.To measure performance, we used the F5 mea-sure (Manning et al, 2008).
The scheme countscorrectness pairwisely.
For example, if a docu-ment pair in an output cluster is actually in thesame class, it is counted as true positive.
In con-trast, if it is actually in the different class, it iscounted as false positive.
Following this man-ner, a contingency table can be made as follows:Same cluster Diff.
clustersSame class TP FNDiff.
classes FP TNNow, F?
measure can be defined asF?
=(?2 + 1)PR?2P +Rwhere the precision P = TP/(TP + FP ) and therecall R = TP/(TP + FN).3http://people.csail.mit.edu/jrennie/20Newsgroups/125In short, F5 measure strongly favors precision torecall.
Manning et al (2008) stated that in somecases separating similar documents is more unfavor-able than putting dissimilar documents together, andin such cases the F?
measure (where ?
> 1) is agood evaluation criterion.At the first look, it seems odd that performancecan be higher than the original where m is low.
Apossible hypothesis is that since K-means only lo-cally minimizes RSS but in general there are manylocal minima which are far from the global optimalpoint, therefore distortion can be sometimes usefulto escape from a bad local minimum and reach abetter one.
As a rule, however, large distortion killsclustering performance as shown in the figure.Although clustering is heavily case-dependent, inthis experiment, the resulting clusters are still reli-able where the hash size is 3.5% of the original fea-ture space size (around 5,000).6 Future WorkArthur and Vassilvitskii (2007) proposed K-means++, an improved version of K-means whichguarantees its RSS is upper-bounded.
Combiningtheir method and the feature hashing as shown in ourpaper will produce a new efficient method (possiblyit can be named hashed K-means++).
We will ana-lyze and experiment with this method in the future.7 ConclusionIn this paper, we argued that applying feature hash-ing to K-means is beneficial for memory-efficiency.Our analysis theoretically motivated this combina-tion.
We supported our argument and analysis byan experiment on document clustering, showing wecould safely shrink memory-usage into 3.5% of theoriginal in our case.
In the future, we will analyzethe technique on other learning methods such as K-means++ and experiment on various real-data NLPtasks.AcknowledgementsWe are indebted to our supervisors, Jun?ichi Tsujiiand Takuya Matsuzaki.
We are also grateful to theanonymous reviewers for their helpful and thought-ful comments.ReferencesDimitris Achlioptas.
2003.
Database-friendly randomprojections: Johnson-Lindenstrauss with binary coins.Journal of Computer and System Sciences, 66(4):671?687, June.David Arthur and Sergei Vassilvitskii.
2007. k-means++: The Advantages of Careful Seeding.
In Proceedingsof the Eighteenth Annual ACM-SIAM Symposium onDiscrete Algorithms, pages 1027?1035.Christos Boutsidis, Anastasios Zouzias, and PetrosDrineas.
2010.
Random Projections for k-meansClustering.
In Advances in Neural Information Pro-cessing Systems 23, number iii, pages 298?306.Kuzman Ganchev and Mark Dredze.
2008.
Small Statis-tical Models by Random Feature Mixing.
In Proceed-ings of the ACL08 HLT Workshop on Mobile LanguageProcessing, pages 19?20.Stuart P. Lloyd.
1982.
Least Squares Quantization inPCM.
IEEE Transactions on Information Theory,28(2):129?137.J MacQueen.
1967.
Some Methods for Classificationand Analysis of Multivariate Observations.
In Pro-ceedings of 5th Berkeley Symposium on MathematicalStatistics and Probability, pages 281?297.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.Qinfeng Shi, James Petterson, Gideon Dror, John Lang-ford, Alex Smola, and S.V.N.
Vishwanathan.
2009.Hash Kernels for Structured Data.
Journal of MachineLearning Research, 10:2615?2637.Kilian Weinberger, Anirban Dasgupta, John Langford,Alex Smola, and Josh Attenberg.
2009.
Feature Hash-ing for Large Scale Multitask Learning.
In Proceed-ings of the 26th International Conference on MachineLearning.126
