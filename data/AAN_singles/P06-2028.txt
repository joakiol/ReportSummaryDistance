Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215?222,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing Lexical Dependency and Ontological Knowledge to Improve aDetailed Syntactic and Semantic Tagger of EnglishAndrew FinchNiCT?-ATR?Kyoto, Japanandrew.finch@atr.jpEzra BlackEpimenides Corp.New York, USAezra.black@epimenides.comYoung-Sook HwangETRISeoul, Koreayshwang7@etri.re.krEiichiro SumitaNiCT-ATRKyoto, Japaneiichiro.sumita@atr.jpAbstractThis paper presents a detailed study ofthe integration of knowledge from bothdependency parses and hierarchical wordontologies into a maximum-entropy-basedtagging model that simultaneously labelswords with both syntax and semantics.Our findings show that information fromboth these sources can lead to strong im-provements in overall system accuracy:dependency knowledge improved perfor-mance over all classes of word, and knowl-edge of the position of a word in an on-tological hierarchy increased accuracy forwords not seen in the training data.
Theresulting tagger offers the highest reportedtagging accuracy on this tagset to date.1 IntroductionPart-of-speech (POS) tagging has been one of thefundamental areas of research in natural languageprocessing for many years.
Most of the prior re-search has focussed on the task of labeling textwith tags that reflect the words?
syntactic role inthe sentence.
In parallel to this, the task of wordsense disambiguation (WSD), the process of de-ciding in which semantic sense the word is beingused, has been actively researched.
This paper ad-dresses a combination of these two fields, that is:labeling running words with tags that comprise, inaddition to their syntactic function, a broad seman-tic class that signifies the semantics of the word inthe context of the sentence, but does not neces-sarily provide information that is sufficiently fine-grained as to disambiguate its sense.
This differs?National Institute of Information and CommunicationsTechnology?ATR Spoken Language Communication Research Labsfrom what is commonly meant by WSD in that al-though each word may have many ?senses?
(bysenses here, we mean the set of semantic labelsthe word may take), these senses are not specificto the word itself but are drawn from a vocabularyapplicable to the subset of all types in the corpusthat may have the same semantics.In order to perform this task, we draw on re-search from several related fields, and exploit pub-licly available linguistic resources, namely theWordNet database (Fellbaum, 1998).
Our aim isto simultaneously disambiguate the semantics ofthe words being tagged while tagging their POSsyntax.
We treat the task as fundamentally a POStagging task, with a larger, more ambiguous tagset.
However, as we will show later, the ?n-gram?feature set traditionally employed to perform POStagging, while basically competent, is not up tothis challenge, and needs to be augmented by fea-tures specifically targeted at semantic disambigua-tion.2 Related WorkOur work is a synthesis of POS tagging and WSD,and as such, research from both these fields is di-rectly relevant here.The basic engine used to perform the taggingin these experiments is a direct descendent of themaximum entropy (ME) tagger of (Ratnaparkhi,1996) which in turn is related to the taggers of(Kupiec, 1992) and (Merialdo, 1994).
The MEapproach is well-suited to this kind of labeling be-cause it allows the use of a wide variety of featureswithout the necessity to explicitly model the inter-actions between them.The literature on WSD is extensive.
For a goodoverview we direct the reader to (Nancy and Jean,1998).
Typically, the local context around the215word to be sense-tagged is used to disambiguatethe sense (Yarowsky, 1993), and it is common forlinguistic resources such as WordNet (Li et al,1995; Mihalcea and Moldovan, 1998; Ramakrish-nan and Prithviraj, 2004), or bilingual data (Li andLi, 2002) to be employed as well as more long-range context.
An ME-system for WSD that op-erates on similar principles to our system (Suarez,2002) was based on an array of local features thatincluded the words/POS tags/lemmas occurring ina window of +/-3 words of the word being dis-ambiguated.
(Lamjiri et al, 2004) also developedan ME-based system that used a very simple setof features: the article before; the POS beforeand after; the preposition before and after, and thesyntactic category before and after the word be-ing labeled.
The features used in both of theseapproaches resemble those present in the featureset of a standard n-gram tagger, such as the oneused as the baseline for the experiments in this pa-per.
The semantic tags we use can be seen as aform of semantic categorization acting in a similarmanner to the semantic class of a word in the sys-tem of (Lamjiri et al, 2004).
The major differenceis that with a left-to-right beam-search tagger, la-beled context to the right of the word being labeledis not available for use in the feature set.Although POS tag information has been utilizedin WSD techniques (e.g.
(Suarez, 2002)), therehas been relatively little work addressing the prob-lem of assigning a part-of-speech tag to a wordtogether with its semantics, despite the fact thatthe tasks involve a similar process of label disam-biguation for a word in running text.3 Experimental DataThe primary corpus used for the experiments pre-sented in this paper is the ATR General EnglishTreebank.
This consists of 518,080 words (ap-proximately 20 words per sentence, on average) oftext annotated with a detailed semantic and syntac-tic tagset.To understand the nature of the task involvedin the experiments presented in this paper, oneneeds some familiarity with the ATR GeneralEnglish Tagset.
For detailed presentations,see (Black et al, 1996b; Black et al, 1996a;Black and Finch, 2001).
An apercu can begained, however, from Figure 1, which showstwo sample sentences from the ATR Treebank(and originally from a Chinese take?out foodflier), tagged with respect to the ATR GeneralEnglish Tagset.
Each verb, noun, adjective andadverb in the ATR tagset includes a semanticlabel, chosen from 42 noun/adjective/adverbcategories and 29 verb/verbal categories, someoverlap existing between these category sets.Proper nouns, plus certain adjectives andcertain numerical expressions, are further cat-egorized via an additional 35 ?proper?noun?categories.
These semantic categories are in-tended for any ?Standard?American?English?text, in any domain.
Sample categories include:?physical.attribute?
(nouns/adjectives/adverbs),?alter?
(verbs/verbals), ?interpersonal.act?(nouns/adjectives/adverbs/verbs/verbals),?orgname?
(proper nouns), and ?zipcode?(numericals).
They were developed by the ATRgrammarian and then proven and refined viaday?in?day?out tagging for six months at ATR bytwo human ?treebankers?, then via four months oftagset?testing?only work at Lancaster University(UK) by five treebankers, with daily interactionsamong treebankers, and between the treebankersand the ATR grammarian.
The semantic catego-rization is, of course, in addition to an extensivesyntactic classification, involving some 165 basicsyntactic tags.The test corpus has been designed specificallyto cope with the ambiguity of the tagset.
It is pos-sible to correctly assign any one of a number of?allowable?
tags to a word in context.
For exam-ple, the tag of the word battle in the phrase ?alegal battle?
could be either NN1PROBLEM orNN1INTER-ACT, indicating that the semantics iseither a problem, or an inter-personal action.
Thetest corpus consists of 53,367 words sampled fromthe same domains as, and in approximately thesame proportions as the training data, and labeledwith a set of up to 6 allowable tags for each word.During testing, only if the predicted tag fails tomatch any of the allowed tags is it considered anerror.4 Tagging Model4.1 ME ModelOur tagging framework is based on a maximumentropy model of the following form:p(t, c) = ?K?k=0?fk(c,t)k p0 (1)where:216(_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DD1 coupon_NN1DOCUMENTwhen_CSWHEN ordering_VVGINTER-ACTOR_CCOR ONE_MC1WORD FREE_JJMONEY FANTAIL_NN1ANIMAL SHRIMPS_NN1FOODFigure 1: Two ATR Treebank Sentences from a Take?Out Food Flier- t is tag being predicted;- c is the context of t;- ?
is a normalization coefficient that ensures:?Lt=0?
?Kk=0 ?fk(c,t)k p0 = 1;- K is the number of features in the model;- L is the number of tags in our tag set;- ?k is the weight of feature fk;- fk are feature functions and fk{0, 1};- p0 is the default tagging model (in our case,the uniform distribution, since all of the in-formation in the model is specified using MEconstraints).Our baseline model contains the following fea-ture predecate set:w0 t?1 pos0 pref1(w0)w?1 t?2 pos?1 pref2(w0)w?2 pos?2 pref3(w0)w+1 pos+1 suff1(w0)w+2 pos+2 suff2(w0)suff3(w0)where:- wn is the word at offset n relative to the wordwhose tag is being predicted;- tn is the tag at offset n;- posn is the syntax-only tag at offset n as-signed by a syntax-only tagger;- prefn(w0) is the first n characters of w0;- suffn(w0) is the last n characters of w0;This feature set contains a typical selection ofn-gram and basic morphological features.
Whenthe tagger is trained in tested on the UPENN tree-bank (Marcus et al, 1994), its accuracy (excludingthe posn features) is over 96%, close to the state ofthe art on this task.
(Black et al, 1996b) adopteda two-stage approach to prediction, first predictingsyntax, then semantics given the syntax, whereasin (Black et al, 1998) both syntax and semanticswere predicted together in one step.
In using syn-tactic tags as features, we take a softer approachto the two-stage process.
The tagger has accessto accurate syntactic information; however, it isnot necessarily constrained to accept this choiceof syntax.
Rather, it is able to decide both syn-tax and semantics while taking semantic contextinto account.
In order to find the most probablesequence of tags, we tag in a left-to-right mannerusing a beam-search algorithm.4.2 Feature selectionFor reasons of practicability, it is not always pos-sible to use the full set of features in a model: of-ten it is necessary to control the number of fea-tures to reduce resource requirements during train-ing.
We use mutual information (MI) to selectthe most useful feature predicates (for more de-tails, see (Rosenfeld, 1996)).
It can be viewed asa means of determining how much information agiven predicate provides when used to predict anoutcome.That is, we use the following formula to gaugea feature?s usefulness to the model:I(f ;T ) =?f?
{0,1}?t?Tp(f, t)logp(f, t)p(f)p(t)(2)where:- t ?
T is a tag in the tagset;- f ?
{0, 1} is the value of any kind of predi-cate feature.Using mutual information is not without itsshortcomings.
It does not take into account anyof the interactions between features.
It is possi-ble for a feature to be pronounced useful by thisprocedure, whereas in fact it is merely giving thesame information as another feature but in differ-ent form.
Nonetheless this technique is invaluablein practice.
It is possible to eliminate features217which provide little or no benefit to the model,thus speeding up the training.
In some cases iteven allows a model to be trained where it wouldnot otherwise be possible to train one.
For the pur-poses of our experiments, we use the top 50,000predicates for each model to form the feature set.5 External Knowledge Sources5.1 Lexical DependenciesFeatures derived from n-grams of words and tagsin the immediate vicinity of the word being taggedhave underpinned the world of POS tagging formany years (Kupiec, 1992; Merialdo, 1994; Rat-naparkhi, 1996), and have proven to be useful fea-tures in WSD (Yarowsky, 1993).
Lower-ordern-grams which are closer to word being taggedoffer the greatest predictive power (Black et al,1998).
However, in the field of WSD, relationalinformation extracted from grammatical analysisof the sentence has been employed to good effect,and in particular, subject-object relationships be-tween verbs and nouns have been shown be effec-tive in disambiguating semantics (Nancy and Jean,1998).
We take the broader view that dependencyrelationships in general between any classes ofwords may help, and use the ME training processto weed out the irrelevant relationships.
The prin-ciple is exactly the same as when using a word inthe local context as a feature, except that the wordin this case has a grammatical relationship with theword being tagged, and can be outside the localneighborhood of the word being tagged.
For bothtypes of dependency, we encoded the model con-straints fstl(d) as boolean functions of the form:fstl(d) ={1 if d.s = s ?
d.t = t ?
d.l = l0 otherwise(3)where:- d is a lexical dependency, consisting of asource word (the word being tagged) d.s, atarget word d.t and a label d.l- s and t (words), and l (link label) are specificto the featureWe generated two distinct features for each de-pendency.
The source and target were exchangedto create these features.
This was to allow themodels to capture the bidirectional nature of thedependencies.
For example, when tagging a verb,the model should be aware of the dependent ob-ject, and conversely when tagging that object, themodel should have a feature imposing a constraintarising from the identity of the dependent verb.5.1.1 Dependencies from the CMU LinkGrammarWe parsed our corpus using the parser detailedin (Grinberg et al, 1995).
The dependencies out-put by this parser are labeled with the type of de-pendency (connector) involved.
For example, sub-jects (connector type S) and direct objects of verbs(O) are explicitly marked by the process (a full listof connectors is provided in the paper).
We usedall of the dependencies output by the parser as fea-tures in the models.5.1.2 Dependencies from Phrasal StructureIt is possible to extract lexical dependenciesfrom a phrase-structure parse.
The procedure isexplained in detail in (Collins, 1996).
In essence,each non-terminal node in the parse tree is as-signed a head word, which is the head of one ofits children denoted the ?head child?.
Dependen-cies are established between this headword andthe heads of each of the children (except for thehead child).
In these experiments we used theMXPOST tagger (Ratnaparkhi, 1996) combinedwith Collins?
parser (Collins, 1996) to assign parsetrees to the corpus.
The parser had a 98.9% cover-age of the sentences in our corpora.
Again, all ofthe dependencies output by the parser were usedas features in the models.5.2 Hierarchical Word OntologiesIn this section we consider the effect of featuresderived from hierarchical sets of words.
The pri-mary advantage is that we are able to constructthese hierarchies using knowledge from outsidethe training corpus of the tagger itself, and therebyglean knowledge about rare words.
In these exper-iments we use the human annotated word taxon-omy of hypernyms (IS-A relations) in the Word-Net database, and an automatically acquired on-tology made by clustering words in a large corpusof unannotated text.We have chosen to use hierarchical schemes forboth the automatic and manually acquired ontolo-gies because this offers the opportunity to com-bat data-sparseness issues by allowing features de-rived from all levels of the hierarchy to be used.The process of training the model is able to de-218Top-level categoryappleedible fruit apple treefruitreproductivestructure fruit treeplant organplant partnatural objectobject angiospermoustreetreewoody plantvascular plantplantpeargrape crab applewild appleHierarchyfor sense 1 Hierarchy for sense 2Figure 2: The WordNet taxonomy for both (WordNet) senses of the word applecide the levels of granularity that are most usefulfor disambiguation.
For the purposes of generat-ing features for the ME tagger we treat both typesof hierarchy in the same fashion.
One of these fea-tures is illustrated in Figure 5.3.
Each predicateis effectively a question which asks whether theword (or word being used in a particular sense inthe case of the WordNet hierarchy) is a descendentof the node to which the predicate applies.
Thesepredicates become more and more general as onemoves up the hierarchy.
For example in the hierar-chy shown in Figure 5.2, looking at the nodes onthe right hand branch, the lowest node representsthe class of apple trees whereas the top node rep-resents the class of all plants.We expect these hierarchies to be particularlyuseful when tagging out of vocabulary words(OOV?s).
The identity of the word being taggedis by far the most important feature in our baselinemodel.
When tagging an OOV this information isnot available to the tagger.
The automatic cluster-ing has been trained on 100 times as much dataas our tagger, and therefore will have informationabout words that tagger has not seen during train-ing.
To illustrate this point, suppose that we aretagging the OOV pomegranate.
This word is in theWordNet database, and is in the same synset as the?fruit?
sense of the word apple.
It is reasonable toassume that the model will have learned (from themany examples of all fruit words) that the predi-cate representing membership of this fruit synsetshould, if true, favor the selection of the correct tagfor fruit words: NN1FOOD.
The predicate will betrue for the word pomegranate which will therebybenefit from the model?s knowledge of how to tagthe other words in its class.
Even if this is not soat this level in the hierarchy, it is likely to be so atsome level of granularity.
Precisely which levelsof detail are useful will be learned by the modelduring training.5.2.1 Automatic Clustering of TextWe used the automatic agglomerative mutual-information-based clustering method of (Ushioda,1996) to form hierarchical clusters from approx-imately 50 million words of tokenized, unanno-tated text drawn from similar domains as the tree-bank used to train the tagger.
Figure 5.2 showsthe position of the word apple within the hierar-chy of clusters.
This example highlights both thestrengths and weaknesses of this approach.
Onestrength is that the process of clustering proceedsin a purely objective fashion and associations be-tween words that may not have been consideredby a human annotator are present.
Moreover, theclustering process considers all types that actuallyoccur in the corpus, and not just those words thatmight appear in a dictionary (we will return to thislater).
A major problem with this approach is that219eggapplecoca PREDICATE:Is the word in thesubtree below thisnode?coffee chicken diamond tin newsstandwellhead calf after-market palm-oilwinter-wheat meat milk timber ?Figure 3: The dendrogram for the automatically acquired ontology, showing the word applethe clusters tend to contain a lot of noise.
Rarewords can easily find themselves members of clus-ters to which they do not seem to belong, by virtueof the fact that there are too few examples of theword to allow the clustering to work well for thesewords.
This problem can be mitigated somewhatby simply increasing the size of the text that isclustered.
However the clustering process is com-putationally expensive.
Another problem is that aword may only be a member of a single cluster;thus typically the cluster set assigned to a wordwill only be appropriate for that word when usedin its most common sense.Approximately 93% of running words in the testcorpus, and 95% in the training corpus were cov-ered by the words in the clusters (when restrictedto verbs, nouns, adjectives and adverbs, these fig-ures were 94.5% and 95.2% respectively).
Ap-proximately 81% of the words in the vocabularyfrom the test corpus were covered, and 71% of thetraining corpus vocabulary was covered.5.2.2 WordNet TaxonomyFor this class of features, we used the hypernymtaxonomy of WordNet (Fellbaum, 1998).
Fig-ure 5.2 shows the WordNet hypernym taxonomyfor the two senses of the word apple that are inthe database.
The set of predicates query member-ship of all levels of the taxonomy for all WordNetsenses of the word being tagged.
An example ofone such predicate is shown in the figure.Only 63% of running words in both the train-ing and the test corpus were covered by the wordsin the clusters.
Although this figure appears low,it can be explained by the fact that WordNet onlycontains entries for words that have senses in cer-tain parts of speech.
Some very frequent classes ofwords, for example determiners, are not in Word-Net.
The coverage of only nouns, verbs, adjectivesand adverbs in running text is 94.5% for both train-ing and test sets.
Moreover, approximately 84%of the words in the vocabulary from the test cor-pus were covered, and 79% on the training cor-pus.
Thus, the effective coverage of WordNet onthe important classes of words is similar to that ofthe automatic clustering method.6 Experimental ResultsThe results of our experiments are shown in Ta-ble 1.
The task of assigning semantic and syntac-tic tags is considerably more difficult than simplyassigning syntactic tags due to the inherent ambi-guity of the tagset.
To gauge the level of humanperformance on this task, experiments were con-ducted to determine inter-annotator consistency;in addition, annotator accuracy was measured on5,000 words of data.
Both the agreement and ac-curacy were found to be approximately 97%, withall of the inconsistencies and tagging errors aris-ing from the semantic component of the tags.
97%accuracy is therefore an approximate upper boundfor the performance one would expect from an au-tomatic tagger.
As a point of reference for a lowerbound, the overall accuracy of a tagger which usesonly a single feature representing the identity ofthe word being tagged is approximately 73%.The overall baseline accuracy was 82.58% withonly 30.58% of OOV?s being tagged correctly.Of the two lexical dependency-based approaches,220the features derived from Collins?
parser were themost effective, improving accuracy by 0.8% over-all.
To put the magnitude of this gain into perspec-tive, dropping the features for the identity of theprevious word from the baseline model, only de-graded performance by 0.2%.
The features fromthe link grammar parser were handicapped due tothe fact that only 31% of the sentences were ableto be parsed.
When the model (Model 3 in Ta-ble 1) was evaluated on only the parsable portionon the test set, the accuracy obtained was roughlycomparable to that using the dependencies fromCollins?
parses.
To control for the differences be-tween these parseable sentences and the full testset, Model 4 was tested on the same 31% of sen-tence that parsed.
Its accuracy was within 0.2% ofthe accuracy on the whole test set in all cases.
Nei-ther of the lexical dependency-based approacheshad a particularly strong effect on the performanceon OOV?s.
This is in line with our intuition, sincethese features rely on the identity of the word be-ing tagged, and the performance gain we see isdue to the improvement in labeling accuracy of thecontext around the OOV.In contrast to this, for the word-ontology-basedfeature sets, one would hope to see a marked im-provement on OOV?s, since these features weredesigned specifically to address this issue.
We dosee a strong response to these features in the ac-curacy of the models.
The overall accuracy whenusing the automatically acquired ontology is only0.1% higher than the accuracy using dependenciesfrom Collins?
parser.
However the accuracy onOOV?s jumps 3.5% to 35.08% compared to just0.7% for Model 4.
Performance for both cluster-ing techniques was quite similar, with the Word-Net taxonomical features being slightly more use-ful, especially for OOV?s.
One possible explana-tion for this is that overall, the coverage of bothtechniques is similar, but for rarer words, the MIclustering can be inconsistent due to lack of data(for an example, see Figure 5.2: the word news-stand is a member of a cluster of words that appearto be commodities), whereas the WordNet clus-tering remains consistent even for rare words.
Itseems reasonable to expect, however, that the au-tomatic method would do better if trained on moredata.
Furthermore, all uses of words can be cov-ered by automatic clustering, whereas for exam-ple, the common use of the word apple as a com-pany name is beyond the scope of WordNet.In Model 7 we combined the best lexical depen-dency feature set (Model 4) with the best cluster-ing feature set (Model 6) to investigate the amountof information overlap existing between the fea-ture sets.
Models 4 and 6 improved the base-line performance by 0.8% and 1.3% respectively.In combination, accuracy was increased by 2.3%,0.2% more than the sum of the component mod-els?
gains.
This is very encouraging and indicatesthat these models provide independent informa-tion, with virtually all of the benefit from bothmodels manifesting itself in the combined model.7 ConclusionWe have described a method for simultaneouslylabeling the syntax and semantics of words in run-ning text.
We develop this method starting froma state-of-the-art maximum entropy POS taggerwhich itself outperforms previous attempts to tagthis data (Black et al, 1996b).
We augment thistagging model with two distinct types of knowl-edge: the identity of dependent words in the sen-tence, and word class membership information ofthe word being tagged.
We define the features insuch a manner that the useful lexical dependen-cies are selected by the model, as is the granu-larity of the word classes used.
Our experimentalresults show that large gains in performance areobtained using each of the techniques.
The de-pendent words boosted overall performance, es-pecially when tagging verbs.
The hierarchicalontology-based approaches also increased over-all performance, but with particular emphasis onOOV?s, the intended target for this feature set.Moreover, when features from both knowledgesources were applied in combination, the gainswere cumulative, indicating little overlap.Visual inspection the output of the tagger onheld-out data suggests there are many remainingerrors arising from special cases that might be bet-ter handled by models separate from the main tag-ging model.
In particular, numerical expressionsand named entities cause OOV errors that the tech-niques presented in this paper are unable to handle.In future work we would like to address these is-sues, and also evaluate our system when used as acomponent of a WSD system, and when integratedwithin a machine translation system.221# Model Accuracy (?
c.i.)
OOV?s Nouns Verbs Adj/Adv1 Baseline 82.58?
0.32 30.58 68.47 74.32 70.992 + Dependencies (link grammar) 82.74?
0.32 30.92 68.18 74.96 73.023 As above (only parsed sentences) 83.59?
0.53 30.92 69.16 77.21 73.524 + Dependencies (Collins?
parser) 83.37?
0.31 31.24 69.36 75.78 72.625 + Automatically acquired ontology 83.71?
0.31 35.08 71.89 75.83 75.346 + WordNet ontology 83.90?
0.31 36.18 72.28 76.29 74.477 + Model 4 + Model 6 84.90?
0.31 37.02 72.80 78.36 76.16Table 1: Tagging accuracy (%), ?+?
being shorthand for ?Baseline +?, ?c.i.?
denotes the confidenceinterval of the mean at a 95% significance level, calculated using bootstrap resampling.ReferencesE.
Black and A. Finch.
2001.
Developing and prov-ing effective broad-coverage semantic-and-syntactictagsets for natural language: The atr approach.
InProceedings of ICCPOL-2001.E.
Black, S. Eubank, H. Kashioka, R. Garside,G.
Leech, and D. Magerman.
1996a.
Beyondskeleton parsing: producing a comprehensive large?scale general?english treebank with full grammati-cal analysis.
In Proceedings of the 16th Annual Con-ference on Computational Linguistics, pages 107?112, Copenhagen.E.
Black, S. Eubank, H. Kashioka, and J. Saia.
1996b.Reinventing part-of-speech tagging.
Journal of Nat-ural Language Processing (Japan), 5:1.Ezra Black, Andrew Finch, and Hideki Kashioka.1998.
Trigger-pair predictors in parsing and tag-ging.
In Proceedings, 36th Annual Meeting ofthe Association for Computational Linguistics, 17thAnnual Conference on Computational Linguistics,Montreal, Canada.Michael John Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In ArivindJoshi and Martha Palmer, editors, Proceedings ofthe Thirty-Fourth Annual Meeting of the Associationfor Computational Linguistics, pages 184?191, SanFrancisco.
Morgan Kaufmann Publishers.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Dennis Grinberg, John Lafferty, and Daniel Sleator.1995.
A robust parsing algorithm for LINKgrammars.
Technical Report CMU-CS-TR-95-125,CMU, Pittsburgh, PA.J.
Kupiec.
1992.
Robust part-of-speech tagging usinga hidden markov model.
Computer Speech and Lan-guage, 6:225?242.A.
K. Lamjiri, O. El Demerdash, and L.Kosseim.
2004.Simple features for statistical word sense disam-biguation.
In Proc.
ACL 2004 ?
Third Interna-tional Workshop on the Evaluation of Systems for theSemantic Analysis of Text (Senseval-3), Barcelona,Spain, July.
ACL-2004.C.
Li and H. Li.
2002.
Word translation disambigua-tion using bilingual bootstrapping.Xiaobin Li, Stan Szpakowicz, and Stan Matwin.
1995.A wordnet-based algorithm for word sense disam-biguation.
In IJCAI, pages 1368?1374.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313?330.B.
Merialdo.
1994.
Tagging english text with aprobabilistic model.
Computational Linguistics,20(2):155?172.Rada Mihalcea and Dan I. Moldovan.
1998.
Wordsense disambiguation based on semantic density.
InSanda Harabagiu, editor, Use of WordNet in NaturalLanguage Processing Systems: Proceedings of theConference, pages 16?22.
Association for Compu-tational Linguistics, Somerset, New Jersey.I.
Nancy and V. Jean.
1998.
Word sense disambigua-tion: The state of the art.
Computational Linguis-tics, 24:1:1?40.G.
Ramakrishnan and B. Prithviraj.
2004.
Soft wordsense disambiguation.
In International Conferenceon Global Wordnet (GWC 04), Brno, Czeck Repub-lic.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the EmpiricalMethods in Natural Language Processing Confer-ence.R.
Rosenfeld.
1996.
A maximum entropy approach toadaptive statistical language modelling.
ComputerSpeech and Language, 10:187?228.A.
Suarez.
2002.
A maximum entropy-based wordsense disambiguation system.
In Proc.
InternationalConference on Computational Linguistics.A.
Ushioda.
1996.
Hierarchical clustering of words.In In Proceedings of COLING 96, pages 1159?1162.D.
Yarowsky.
1993.
One sense per collocation.
InIn the Proceedings of ARPA Human Language Tech-nology Workshop.222
