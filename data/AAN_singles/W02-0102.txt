An Interactive Spreadsheet for Teaching the Forward-Backward AlgorithmJason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD, USA 21218-2691jason@cs.jhu.eduAbstractThis paper offers a detailed lesson plan on the forward-backward algorithm.
The lesson is taught from a live, com-mented spreadsheet that implements the algorithm and graphsits behavior on a whimsical toy example.
By experimentingwith different inputs, one can help students develop intuitionsabout HMMs in particular and Expectation Maximization ingeneral.
The spreadsheet and a coordinated follow-up assign-ment are available.1 Why Teach from a Spreadsheet?Algorithm animations are a wonderful teaching tool.They are concrete, visual, playful, sometimes inter-active, and remain available to students after the lec-ture ends.
Unfortunately, they have mainly been lim-ited to algorithms that manipulate easy-to-draw datastructures.Numerical algorithms can be ?animated?
byspreadsheets.
Although current spreadsheets do notprovide video, they can show ?all at once?
how acomputation unfolds over time, displaying interme-diate results in successive rows of a table and ongraphs.
Like the best algorithm animations, theylet the user manipulate the input data to see whatchanges.
The user can instantly and graphically seethe effect on the whole course of the computation.Spreadsheets are also transparent.
In Figure 1, theuser has double-clicked on a cell to reveal its un-derlying formula.
The other cells that it depends onare automatically highlighted, with colors keyed tothe references in the formula.
There is no program-ming language to learn: spreadsheet programs areaimed at the mass market, with an intuitive designand plenty of online help, and today?s undergrad-uates already understand their basic operation.
Anadventurous student can even experiment with mod-ifying the formulas, or can instrument the spread-sheet with additional graphs.Finally, modern spreadsheet programs such asMicrosoft Excel support visually attractive layoutswith integrated comments, color, fonts, shading, andFigure 1: User has double-clicked on cell D29.drawings.
This makes them effective for both class-room presentation and individual study.This paper describes a lesson plan that was cen-tered around a live spreadsheet, as well as a subse-quent programming assignment in which the spread-sheet served as a debugging aid.
The materials areavailable for use by others.Students were especially engaged in class, appar-ently for the following reasons:?
Striking results (?It learned it!?)
that could be im-mediately apprehended from the graphs.?
Live interactive demo.
The students were eagerto guess what the algorithm would do on partic-ular inputs and test their guesses.?
A whimsical toy example.?
The departure from the usual class routine.?
Novel use of spreadsheets.
Several students whothought of them as mere bookkeeping tools wereawed by this, with one calling it ?the coolest-assspreadsheet ever.
?2 How to Teach from a Spreadsheet?It is possible to teach from a live spreadsheet by us-ing an RGB projector.
The spreadsheet?s zoom fea-ture can compensate for small type, although under-graduate eyes prove sharp enough that it may be un-necessary.
(Invite the students to sit near the front.
)Of course, interesting spreadsheets are much toobig to fit on the screen, even with a ?View / FullScreen?
command.
But scrolling is easy to followif it is not too fast and if the class has previouslyJuly 2002, pp.
10-18.
Association for Computational Linguistics.Natural Language Processing and Computational Linguistics, Philadelphia,Proceedings of the Workshop on Effective Tools and Methodologies for Teachingbeen given a tour of the overall spreadsheet layout(by scrolling and/or zooming out).
Split-screen fea-tures such as hide rows/columns, split panes, andfreeze panes can be moderately helpful; so can com-mands to jump around the spreadsheet, or switch be-tween two windows that display different areas.
Itis a good idea to memorize key sequences for suchcommands rather than struggle with mouse menusor dialog boxes during class.3 The Subject MatterAmong topics in natural language processing,the forward-backward or Baum-Welch algorithm(Baum, 1972) is particularly difficult to teach.The algorithm estimates the parameters of aHidden Markov Model (HMM) by Expectation-Maximization (EM), using dynamic programmingto carry out the expectation steps efficiently.HMMs have long been central in speech recog-nition (Rabiner, 1989).
Their application to part-of-speech tagging (Church, 1988; DeRose, 1988)kicked off the era of statistical NLP, and they havefound additional NLP applications to phrase chunk-ing, text segmentation, word-sense disambiguation,and information extraction.The algorithm is also important to teach for ped-agogical reasons, as the entry point to a family ofEM algorithms for unsupervised parameter estima-tion.
Indeed, it is an instructive special case of (1)the inside-outside algorithm for estimation of prob-abilistic context-free grammars; (2) belief propa-gation for training singly-connected Bayesian net-works and junction trees (Pearl, 1988; Lauritzen,1995); (3) algorithms for learning alignment mod-els such as weighted edit distance; (4) general finite-state parameter estimation (Eisner, 2002).Before studying the algorithm, students shouldfirst have worked with some if not all of the keyideas in simpler settings.
Markov models can beintroduced through n-gram models or probabilisticfinite-state automata.
EM can be introduced throughsimpler tasks such as soft clustering.
Global opti-mization through dynamic programming can be in-troduced in other contexts such as probabilistic CKYparsing or edit distance.
Finally, the students shouldunderstand supervised training and Viterbi decod-ing of HMMs, for example in the context of part-of-speech tagging.Even with such preparation, however, theforward-backward algorithm can be difficult for be-ginning students to apprehend.
It requires them tothink about all of the above ideas at once, in com-bination, and to relate them to the nitty-gritty of thealgorithm, namely?
the two-pass computation of mysterious ?
and ?probabilities?
the conversion of these prior path probabilities toposterior expectations of transition and emissioncountsJust as important, students must develop an under-standing of the algorithm?s qualitative properties,which it shares with other EM algorithms:?
performs unsupervised learning (what is this andwhy is it possible?)?
alternates expectation and maximization steps?
maximizes p(observed training data) (i.e., totalprobability of all hidden paths that generate thosedata)?
finds only a local maximum, so is sensitive toinitial conditions?
cannot escape zeroes or symmetries, so theyshould be avoided in initial conditions?
uses the states as it sees fit, ignoring the sugges-tive names that we may give them (e.g., part ofspeech tags)?
may overfit the training data unless smoothing isusedThe spreadsheet lesson was deployed in two 50-minute lectures at Johns Hopkins University, in anintroductory NLP course aimed at upper-level un-dergraduates and first-year graduate students.
A sin-gle lecture might have sufficed for a less interactivepresentation.The lesson appeared in week 10 of 13, by whichtime the students had already been exposed to mostof the preparatory topics mentioned above, includ-ing Viterbi decoding of a part-of-speech trigram tag-ging model.
However, the present lesson was theirfirst exposure to EM or indeed to any kind of unsu-pervised learning.Figure 2: Initial guesses of parameters.Figure 3: Diary data and reconstructed weather.4 The Ice Cream Climatology Data[While the spreadsheet could be used in many ways,the next several sections offer one detailed lessonplan.
Questions for the class are included; subse-quent points often depend on the answers, which areconcealed here in footnotes.
Some fragments of thefull spreadsheet are shown in the figures.
]The situation: You are climatologists in the year2799, studying the history of global warming.
Youcan?t find any records of Baltimore weather, but youdo find my diary, in which I assiduously recordedhow much ice cream I ate each day (see Figure 3).What can you figure out from this about the weatherthat summer?Let?s simplify and suppose there are only twokinds of days: C (cold) and H (hot).
And let?s sup-pose you have guessed some probabilities as shownon the spreadsheet (Figure 2).Thus, you guess that on cold days, I usually ateonly 1 ice cream cone: my probabilities of 1, 2,or 3 cones were 70%, 20% and 10%.
That addsup to 100%.
On hot days, the probabilities werereversed?I usually ate 3 ice creams.
So other thingsequal, if you know I ate 3 ice creams, the odds are7 to 1 that it was a hot day, but if I ate 2 ice creams,the odds are 1 to 1 (no information).You also guess (still Figure 2) that if today is cold,tomorrow is probably cold, and if today is hot, to-morrow is probably hot.
(Q: How does this setupresemble part-of-speech tagging?1)We also have some boundary conditions.
I onlykept this diary for a while.
If I was more likely tostart or stop the diary on a hot day, then that is use-ful information and it should go in the table.
(Q: Isthere an analogy in part-of-speech tagging?2) Forsimplicity, let?s guess that I was equally likely tostart or stop on a hot or cold day.
So the first day Istarted writing was equally likely (50%) to be hot orcold, and any given day had the same chance (10%)of being the last recorded day, e.g., because on anyday I wrote (regardless of temperature), I had a 10%chance of losing my diary.5 The Trellis and ??
Decoding[The notation pi(H) in this paper stands for the prob-ability of H on day i, given all the observed ice creamdata.
On the spreadsheet itself the subscript i isclear from context and is dropped; thus in Figure 3,p(H) denotes the conditional probability pi(H), not aprior.
The spreadsheet likewise omits subscripts on?i(H) and ?i(H).
]Scroll down the spreadsheet and look at the lowerline of Figure 3, which shows a weather reconstruc-tion under the above assumptions.
It estimates therelative hot and cold probabilities for each day.
Ap-parently, the summer was mostly hot with a coldspell in the middle; we are unsure about the weatheron a few transitional days.We will now see how the reconstruction was ac-complished.
Look at the trellis diagram on thespreadsheet (Figure 4).
Consistent with visual intu-ition, arcs (lines) represent days and states (points)represent the intervening midnights.
A cold day isrepresented by an arc that ends in a C state.3 (So1A: This is a bigram tag generation model with tags C and H.Each tag independently generates a word (1, 2, or 3); the wordchoice is conditioned on the tag.2A: A tagger should know that sentences tend to start withdeterminers and end with periods.
A tagging that ends with adeterminer should be penalized because p(Stop | Det) ?
0.3These conventions are a compromise between a traditionalview of HMMs and a finite-state view used elsewhere in thecourse.
(The two views correspond to Moore vs. Mealy ma-chines.)
In the traditional view, states would represent days andFigure 4: The ?-?
trellis.each arc effectively inherits the C or H label of itsterminal state.
)Q: According to the trellis, what is the a prioriprobability that the first three days of summer areH,H,C and I eat 2,3,3 cones respectively (as I did)?4Q: Of the 8 ways to account for the 2,3,3 cones,which is most probable?5 Q: Why do all 8 pathshave low probabilities?6Recall that the Viterbi algorithm computes, ateach state of the trellis, the maximum probabilityof any path from Start.
Similarly, define ?
at astate to be the total probability of all paths to thatstate from Start.
Q: How would you compute itby dynamic programming?7 Q: Symmetrically, howwould you compute ?
at a state, which is defined tobe the total probability of all paths to Stop?The ?
and ?
values are computed on the spread-sheet (Figure 1).
Q: Are there any patterns in thevalues?8Now for some important questions.
Q: What isthe total probability of all paths from Start towould bear emission probabilities such as p(3 | H).
In Figure 4,as in finite-state machines, this role is played by the arcs (whichalso carry transition probabilities such as p(H | C)); this allows?
and ?
to be described more simply as sums of path probabili-ties.
But we persist in a traditional labeling of the states as H orC so that the ??
notation can refer to them.4A: Consult the path Start ?
H ?
H ?
C, which hasprobability (0.5?0.2)?(0.8?0.7)?
(0.1?0.1) = 0.1?0.56?0.01 =0.00056.
Note that the trellis is specialized to these data.5A: H,H,H gives probability 0.1 ?
0.56 ?
0.56 = 0.03136.
(Starting with C would be as cheap as starting with H, but thengetting from C to H would be expensive.
)6A: It was a priori unlikely that I?d eat exactly this sequenceof ice creams.
(A priori there were many more than 8 possiblepaths, but this trellis only shows the paths generating the actualdata 2,3,3.)
We?ll be interested in the relative probabilities ofthese 8 paths.7A: In terms of ?
at the predecessor states: just replace?max?
with ?+?
in the Viterbi algorithm.8A: ?
probabilities decrease going down the column, and ?probabilities decrease going up, as they become responsible forgenerating more and more ice cream data.Figure 5: Computing expected counts and their totals.Stop in which day 3 is hot?9 It is shown in col-umn H of Figure 1.
Q: Why is column I of Figure 1constant at 9.13e-19 across rows?10 Q: What doesthat column tell us about ice cream or weather?11Now the class may be able to see how to completethe reconstruction:p(day 3 hot | 2, 3, 3, .
.
.)
= p(day 3 hot,2,3,3,...)p(2,3,3,...)= ?3(H)??3(H)?3(C)??3(C)+?3(H)?
?3(H) =9.0e-199.8e-21+9.0e-19which is 0.989, as shown in cell K29 of Figure 5.Figure 3 simply graphs column K.6 Understanding the ReconstructionNotice that the lower line in Figure 3 has the samegeneral shape as the upper line (the original data),but is smoother.
For example, some 2-ice-creamdays were tagged as probably cold and some asprobably hot.
Q: Why?12 Q: Since the first day has2 ice creams and doesn?t follow a hot day, why wasit tagged as hot?13 Q: Why was day 11, which hasonly 1 ice cream, tagged as hot?14We can experiment with the spreadsheet (usingthe Undo command after each experiment).
Q: Whatdo you predict will happen to Figure 3 if we weaken9A: By the distributive law, ?3(H) ?
?3(H).10A: It is the total probability of all paths that go through ei-ther C or H on a given day.
But all paths do that, so this is simplythe total probability of all paths!
The choice of day doesn?t mat-ter.11A: It is the probability of my actual ice cream consumption:p(2, 3, 3, .
.
.)
=?~w p(~w, 2, 3, 3, .
.
.)
= 9.13e-19, where ~wranges over all 233 possible weather state sequences such asH,H,C,.
.
.
.
Each summand is the probability of a trellis path.12A: Figure 2 assumed a kind of ?weather inertia?
in whicha hot day tends to be followed by another hot day, and likewisefor cold days.13Because an apparently hot day follows it.
(See footnote 5.
)It is the ?
factors that consider this information from the future,and make ?1(H) ?
?1(H) ?1(C) ?
?1(C).14A: Switching from hot to cold and back (HCH) has proba-bility 0.01, whereas staying hot (HHH) has probability 0.64.
Soalthough the fact that I ate only one ice cream on day 11 favorsC by 7 to 1, the inferred ?fact?
that days 10 and 12 are hot favorsH by 64 to 1.
(a)(b)Figure 6: With (a) no inertia, and (b) anti-inertia.or remove the ?weather inertia?
in Figure 2?15 Q:What happens if we try ?anti-inertia?
?16Even though the number of ice creams is not de-cisive (consider day 11), it is influential.
Q: Whatdo you predict will happen if the distribution of icecreams is the same on hot and cold days?17 Q: Whatif we also change p(H | Start) from 0.5 to 0?187 Reestimating Emission ProbabilitiesWe originally assumed (Figure 2) that I had a 20%chance of eating 2 cones on either a hot or a coldday.
But if our reconstruction is right (Figure 3), Iactually ate 2 cones on 20% of cold days but 40+%of hot days.15A: Changing p(C | C) = p(H | C) = p(C | H) = p(H |H) = 0.45 cancels the smoothing effect (Figure 6a).
The lowerline now tracks the upper line exactly.16A: Setting p(C | H) = p(H | C) = 0.8 and p(C | C) =p(H | H) = 0.1, rather than vice-versa, yields Figure 6b.17A: The ice cream data now gives us no information aboutthe weather, so pi(H) = pi(C) = 0.5 on every day i.18A: p1(H) = 0, but pi(H) increases toward an asymptoteof 0.5 (the ?limit distribution?).
The weather is more likely toswitch to hot than to cold if it was more likely cold to beginwith; so pi(H) increases if it is < 0.5.Figure 7: Parameters of Figure 2 updated by reestimation.So now that we ?know?
which days are hot andwhich days are cold, we should really update ourprobabilities to 0.2 and 0.4, not 0.2 and 0.2.
Afterall, our initial probabilities were just guesses.Q: Where does the learning come from?isn?t thiscircular?
Since our reconstruction was based on theguessed probabilities 0.2 and 0.2, why didn?t the re-construction perfectly reflect those guesses?19Scrolling rightward on the spreadsheet, we finda table giving the updated probabilities (Figure 7).This table feeds into a second copy of the forward-backward calculation and graph.
Q: The secondgraph of pi(H) (not shown here) closely resemblesthe first; why is it different on days 11 and 27?20The updated probability table was computed bythe spreadsheet.
Q: When it calculated how often Iate 2 cones on a reconstructed hot day, do you thinkit counted day 27 as a hot day or a cold day?218 Reestimating Transition ProbabilitiesNotice that Figure 7 also updated the transition prob-abilities.
This involved counting the 4 kinds of daysdistinguished by Figure 8:22 e.g., what fraction of H19A: The reconstruction of the weather underlying the ob-served data was a compromise between the guessed probabili-ties (Figure 2) and the demands of the actual data.
The modelin Figure 2 disagreed with the data: it would not have predictedthat 2-cone days actually accounted for more than 20% of alldays, or that they were disproportionately likely to fall between3-cone days.20A: These days fall between hot and cold days, so smoothinghas little effect: their temperature is mainly reconstructed fromthe number of ice creams.
1 ice cream is now better evidence ofa cold day, and 2 ice creams of a hot day.
Interestingly, days 11and 14 can now conspire to ?cool down?
the intervening 3-ice-cream days.21A: Half of each, since p27(H) ?
0.5!
The actual compu-tation is performed in Figure 5 and should be discussed at thispoint.22Notice how p(H ?
C) and p(C ?
H) spike when theweather changes, on day 14 and either day 27 or 28.Figure 8: Second-order weather reconstruction.days were followed by H?
Again, fractional countswere used to handle uncertainty.Q: Does Figure 3 (first-order reconstruction)contain enough information to construct Figure 8(second-order reconstruction)?23Continuing with the probabilities from the end offootnote 23, suppose we increase p(H | Start) to0.7.
Q: What will happen to the first-order graph?24Q: What if we switch from anti-inertia back to iner-tia (Figure 9)?25Q: In this last case, what do you predict will hap-pen when we reestimate the probabilities?26This reestimation (Figure 10) slightly improvedthe reconstruction.
[Defer discussion of what ?im-proved?
means: the class still assumes that good re-constructions look like Figure 3.]
Q: Now what?A: Perhaps we should do it again.
And again, andagain.
.
.
Scrolling rightward past 10 successive rees-timations, we see that this arrives at the intuitively23A: No.
A dramatic way to see this is to make the dis-tribution of ice cream distribution the same on hot and colddays.
This makes the first-order graph constant at 0.5 as in foot-note 17.
But we can still get a range of behaviors in the second-order graph; e.g., if we switch from inertia to anti-inertia as infootnote 16, then we switch from thinking the weather is un-known but constant to thinking it is unknown but oscillating.24A: pi(H) alternates and converges to 0.5 from both sides.25A: pi(H) converges to 0.5 from above (cf.
footnote 18), asshown in Figure 9.26A: The first-order graph suggests that the early days ofsummer were slightly more likely to be hot than cold.
Sincewe ate more ice cream on those days, the reestimated probabili-ties (unlike the initial ones) slightly favor eating more ice creamon hot days.
So the new reconstruction based on these proba-bilities has a very shallow ?U?
shape (bottom of Figure 10), inwhich the low-ice-cream middle of the summer is slightly lesslikely to be hot.Figure 9: An initial poor reconstruction that will be improvedby reestimation.correct answer (Figure 11)!Thus, starting from an uninformed probability ta-ble, the spreadsheet learned sensible probabilities(Figure 11) that enabled it to reconstruct the weather.The 3-D graph shows how the reconstruction im-proved over time.The only remaining detail is how the transitionprobabilities in Figure 8 were computed.
Recall thatto get Figure 3, we asked what fraction of pathspassed through each state.
This time we must askwhat fraction of paths traversed each arc.
(Q: Howto compute this?27) Just as there were two possiblestates each day, there are four possible arcs each day,and the graph reflects their relative probabilities.9 Reestimation ExperimentsWe can check whether the algorithm still learns fromother initial guesses.
The following examples appearon the spreadsheet and can be copied over the tableof initial probabilities.
(Except for the pathologi-cally symmetric third case, they all learn the samestructure.)1.
No weather inertia, but more ice cream on hotdays.
The model initially behaves as in foot-27A: The total probability of all paths traversing q ?
r is?
(q) ?
p(q ?
r) ?
?
(r).Figure 10: The effect of reestimation on Figure 9.note 15, but over time it learns that weather doeshave inertia.2.
Inertia, but only a very slight preference for moreice cream on hot days.
The pi(H) graph is ini-tially almost as flat as in footnote 17.
But overseveral passes the model learns that I eat a lotmore ice cream on hot days.3.
A completely symmetric initial state: no inertia,and no preference at all for more ice cream on hotdays.
Q: What do you expect to happen underreestimation?284.
Like the previous case, but break the symmetryby giving cold days a slight preference to eatmore ice cream (Figure 12).
This initial state isalmost perfectly symmetric.
Q: Why doesn?t thiscase appear to learn the same structure as the pre-vious ones?29The final case does not converge to quite the sameresult as the others: C and H are reversed.
(It is28A: Nothing changes, since the situation is too symmetric.As H and C behave identically, there is nothing to differentiatethem and allow them to specialize.29A: Actually it does; it merely requires more iterations toconverge.
(The spreadsheet is only wide enough to hold 10 iter-ations; to run for 10 more, just copy the final probabilities backover the initial ones.
Repeat as necessary.)
It learns both inertiaand a preference for more ice cream on cold days.Figure 11: Nine more passes of forward-backward reestimationon Figures 9?10.
Note that the final graph is even smoother thanFigure 3.Figure 12: Breaking symmetry toward the opposite solution.Figure 13: If T is the sequence of 34 training observations(33 days plus Stop), then p(T ) increases rapidly during rees-timation.
To compress the range of the graph, we don?t plotp(T ) but rather perplexity per observation = 1/ 34?p(T ) =2?
(log2 p(T ))/34.now H that is used for the low-ice-cream midsummerdays.)
Should you care about this difference?
Asclimatologists, you might very well be upset that thespreadsheet reversed cold and hot days.
But since Cand H are ultimately just arbitrary labels, then per-haps the outcome is equally good in some sense.What does it mean for the outcome of this unsuper-vised learning procedure to be ?good??
The datasetis just the ice cream diary, which makes no referenceto weather.
Without knowing the true weather, howcan we tell whether we did a good job learning it?10 Local Maximization of LikelihoodThe answer: A good model is one that predicts thedataset as accurately as possible.
The dataset actu-ally has temporal structure, since I tended to havelong periods of high and low ice cream consump-tion.
That structure is what the algorithm discov-ered, regardless of whether weather was the cause.The state C or H distinguishes between the two kindsof periods and tends to persist over time.So did this learned model predict the dataset well?It was not always sure about the state sequence,but Figure 13 shows that the likelihood of the ob-served dataset (summed over all possible state se-quences) increased on every iteration.
(Q: How isthis found?30)That behavior is actually guaranteed: repeated30It is the total probability of paths that explain the data, i.e.,all paths in Figure 4, as given by column I of Figure 1; seefootnote 10.forward-backward reestimation converges to a localmaximum of likelihood.
We have already discov-ered two symmetric local maxima, both with per-plexity of 2.827 per day: the model might use C torepresent cold and H to represent hot, or vice versa.Q: How much better is 2.827 than a model with notemporal structure?31Remember that maximizing the likelihood of thetraining data can lead to overfitting.
Q: Do you seeany evidence of this in the final probability table?32Q: Is there a remedy?3311 A Trick EndingWe get very different results if we slightly mod-ify Figure 12 by putting p(1 | H) = 0.3 withp(2 | H) = 0.4.
The structure of the solution isvery different (Figure 14).
In fact, the final param-eters now show anti-inertia, giving a reconstructionsimilar to Figure 6b.
Q: What went wrong?34In the two previous local maxima, H meant ?lowice-cream day?
or ?high ice-cream day.?
Q: Accord-ing to Figure 14, what does Hmean here?35 Q: Whatdoes the low value of p(H | H) mean?36So we see that there are actually two kinds ofstructure coexisting in this dataset: days with a lot(little) ice cream tend to repeat, and days with 2 icecreams tend not to repeat.
The first kind of structuredid a better job of lowering the perplexity, but both31A: A model with no temporal structure is a unigram model.A good guess is that it will have perplexity 3, since it will becompletely undecided between the 3 kinds of observations.
(Itso happens that they were equally frequent in the dataset.)
How-ever, if we prevent the learning of temporal structure (by settingthe initial conditions so that the model is always in state C, or isalways equally likely to be in states C and H), we find that theperplexity is 3.314, reflecting the four-way unigram distributionp(1) = p(2) = p(3) = 11/34, p(Stop)=1/34.32A: p(H | Start) ?
1 because we become increasinglysure that the training diary started on a hot day.
But this singletraining observation, no matter how justifiably certain we are ofit, might not generalize to next summer?s diary.33A: Smoothing the fractional counts.
Note: If a prior is usedfor smoothing, the algorithm is guaranteed to locally maximizethe posterior (in place of the likelihood).34A: This is a third local maximum of likelihood, unrelatedto the others, with worse perplexity (3.059).
Getting stuck inpoor local maxima is an occupational hazard.35A: H usually emits 2 ice creams, whereas C never does.
SoH stands for a 2-ice-cream day.36A: That 2 ice creams are rarely followed by 2 ice creams.Looking at the dataset, this is true.
So even this local maximumsuccessfully discovered some structure: it discovered (to mysurprise) that when I make up data, I tend not to repeat 2?s!Figure 14: A suboptimal local maximum.are useful.
Q: How could we get our model to dis-cover both kinds of structure (thereby lowering theperplexity further)?37Q: We have now seen three locally optimal mod-els in which the H state was used for 3 differentthings?even though we named it H for ?Hot.?
Whatdoes this mean for the application of this algorithmto part-of-speech tagging?3812 Follow-Up AssignmentIn a follow-up assignment, students applied Viterbidecoding and forward-backward reestimation topart-of-speech tagging.39In the assignment, students were asked to testtheir code first on the ice cream data (provided asa small tagged corpus) before switching to real data.This cemented the analogy between the ice creamand tagging tasks, helping students connect the classto the assignment.37A: Use more states.
Four states would suffice to distinguishhot/2, cold/2, hot/not2, and cold/not2 days.38A: There is no guarantee that N and V will continue to dis-tinguish nouns and verbs after reestimation.
They will evolve tomake whatever distinctions help to predict the word sequence.39Advanced students might also want to read about a mod-ern supervised trigram tagger (Brants, 2000), or the mixed re-sults when one actually trains trigram taggers by EM (Merialdo,1994).Furthermore, students could check their ice creamoutput against the spreadsheet, and track down basicbugs by comparing their intermediate results to thespreadsheet?s.
They reported this to be very useful.Presumably it helps learning when students actuallyfind their bugs before handing in the assignment, andwhen they are able to isolate their misconceptionson their own.
It also made office hours and gradingmuch easier for the teaching assistant.13 AvailabilityThe spreadsheet (in Microsoft Excel) and assign-ment are available at http://www.cs.jhu.edu/?jason/papers/#tnlp02.Also available is a second version of the spread-sheet, which uses the Viterbi approximation for de-coding and reestimation.
The Viterbi algorithm isimplemented in an unconventional way so that thetwo spreadsheets are almost identical; there is noneed to follow backpointers.
The probability of thebest path through state H on day 3 is ?3(H) ?
?3(H),where ?
and ?
are computed like ?
and ?
but maxi-mizing rather than summing path probabilities.
TheViterbi approximation treats p3(H) as 1 or 0 accord-ing to whether ?3(H) ?
?3(H) equals max(?3(C) ?
?3(C), ?3(H) ?
?3(H)).Have fun!
Comments are most welcome.ReferencesL.
E. Baum.
1972.
An inequality and associated max-imization technique in statistical estimation of proba-bilistic functions of a Markov process.
Inequalities, 3.Thorsten Brants.
2000.
TnT: A statistical part-of-speechtagger.
In Proc.
of ANLP, Seattle.K.
W. Church.
1988.
A stochastic parts program and nounphrase parser for unrestricted text.
Proc.
of ANLP.Steven J. DeRose.
1988.
Grammatical category disam-biguation by statistical optimization.
ComputationalLinguistics, 14(1):31?39.Jason Eisner.
2002.
Parameter estimation for probabilis-tic finite-state transducers.
In Proc.
of ACL.Steffen L. Lauritzen.
1995.
The EM algorithm for graph-ical association models with missing data.
Computa-tional Statistics and Data Analysis, 19:191?201.Bernard Merialdo.
1994.
Tagging English text with aprobabilistic model.
Comp.
Ling., 20(2):155?172.Judea Pearl.
1988.
Probabilistic Reasoning in Intelli-gent Systems: Networks of Plausible Inference.
Mor-gan Kaufmann, San Mateo, California.L.
R. Rabiner.
1989.
A tutorial on hidden Markov mod-els and selected applications in speech recognition.Proceedings of IEEE, 77(2):257?285, February.
