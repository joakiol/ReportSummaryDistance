Evaluating Message UnderstandingSystems: An Analysis of the ThirdMessage Understanding Conference(MUG-3)Nancy Chinchor*Science Applications International Corp.David D. LewWUniversity of ChicagoLynette Hirschman tMassachusetts Institute of TechnologyThis paper describes and analyzes the results of the Third Message Understanding Conference(MUC-3).
It reviews the purpose, history, and methodology of the conference, summarizes theparticipating systems, discusses i sues of measuring system effectiveness, describes the linguisticphenomena tests, and provides a critical look at the evaluation in terms of the lessons learned.One of the common problems with evaluations i that the statistical significance of the results isunknown.
In the discussion of system performance, the statistical significance of the evaluationresults is reported and the use of approximate randomization tocalculate the statistical significanceof the results of MUC-3 is described.1.
MUC-3 Purpose, History, and Methodology1.1 IntroductionThe Third Message Understanding Conference (MUC-3) represented a significant stepfor the message processing community and for computational linguistics as a whole.The conference brought ogether 15 systems that were tested on a naturally occurringcorpus of newswire reports on terrorism in Latin America.
The systems were evaluatedon their ability to extract significant information from the newswire reports in theform of "templates" summarizing who did what to whom.
We can enumerate thesuccesses of the conference in many dimensions: the number of participating systems(15 systems, up from 8 at the previous Message Understanding Conference, MUCK-II), the scale of the application (100 times more text than the previous conference),the rigorous definition of the evaluation method (including automated and interactivescoring procedures), and the cooperative framework that enabled the participants odevelop both the training data and the evaluation procedures.
These are significantaccomplishments in a field that has only recently begun to address ystem evaluationissues.The MUC-3 conference has already been described in a conference proceedings(Proceedings ofthe Third Message Understanding Conference (MUC-3), 1991) and in an AI* 10260 Campus Point Drive M/S A2-F, San Diego CA 92121The MITRE Corporation, M/S K329, 202 Burlington Road, Bedford, MA 01730.AT&T Bell Laboratories, Murray Hill NJ 07974(~) 1993 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 3Magazine article (Lehnert and Sundheim 1991).
However, both of these reports wereproduced simultaneously with the system-building and evaluation activities of MUC-3.
It became clear that more time and additional analysis were needed to digest thefindings of MUC-3 and to make them accessible to the broader computational lin-guistics community.
This paper addresses these issues, providing an overview of theconference, the participating system,.~, the evaluation methodology, and system perfor-mance.
We also present information on the statistical significance of the reported testresults and an analysis of what we have learned, both about message understandingand about system evaluation.The authors participated extensively in the conference asconsultants, but were not associated with a specific system undergoing formal evalu-ation.1.2 BackgroundThe Third Message Understanding Conference evolved out of the two earlier messageunderstanding conferences held in 1987 and 1989.
Prior to 1987, formal evaluationof message processing (or text understanding) systems was virtually nonexistent.
Thedominant research paradigm was to build a natural language system and then to debugit on some set of examples.
Sometimes the examples were made up for the purpose ofdebugging.
In other cases, the system was debugged on a corpus of naturally occurringsentences.
However, there was no notion of a "blind test" using previously unseen testdata.The first Message Understanding Conference (MUCK) was held at the Naval Com-mand, Control, and Ocean Surveillance Center RDT&E Division (NRaD) 1 in May 1987.Organized by Beth Sundheim, its goal was a qualitative valuation of the state of theart in message understanding.
Six sites agreed to demonstrate running systems atthe conference.
A small set of training messages (ten tactical naval operations reportson ship sightings and engagements'.)
was distributed, plus several hundred additionalmessages of various types, provided for general background.
At the conference, achactively participating roup demonstrated its system running on one of the twelvetraining messages.
In addition, two previously unseen messages were distributed atthe conference and all active participants were asked to get their systems running onthese two new messages.
The sites reported on what they needed to do to get theirsystems to handle the test messages.
However there was no "task" associated withthe messages and no quantitative evaluation procedure.A second conference, MUCK-II, was held in May 1989 at NRaD.
Based on theirexperience at the first MUCK, participants requested that a specific task be provided,along with an evaluation procedure to determine whether a system's answers wereright or wrong.
The domain chosen for MUCK-II was another type of tactical navaloperations message, also about ship sightings and engagements.
The task consistedof filling in templates for incident:s mentioned in each report.
The template typesincluded "DETECT, ....
ATTACK," and so on.
For each template there was a set of slotsdescribing the actors involved in the incident, as well as the time, date, outcome, andother related information.
Eight sites submitted systems to be evaluated.
There was alarger amount of training data provided (105 messages) and two rounds of test data(20 messages, run first "blind" and then with system fixes, followed, just before theconference, by an additional 5 messages for a second blind test).
In addition to thetest data, NRaD furnished a list of specialized naval terminology, a hierarchy of termsfor the relevant portion of the naw~l domain, and extensive documentation filling1 Formerly the Naval Ocean Systems Center (NOSC).410Nancy Chinchor et al Evaluating Message Understanding SystemsAbbreviation Participating Site LocationADS Advanced Decision Systems Mountain View, CABBN BBN Systems and Technologies Cambridge, MAGE General Electric Research and Development Center Schenectady, NYGTE GTE Government Systems Mountain View, CAHughes Hughes Research Laboratories Malibu, CA\[TP Intelligent Text Processing, Inc. Santa Monica, CALSI Language Systems, Inc. Woodland Hills, CAMDESC McDonnell Douglas Electronics Systems Company Santa Ana, CANYU New York University New York, NYPRC PRC, Inc. McLean, VASRI SRI International Menlo Park, CASynch/UMD Synchronetics, Inc. and University of Maryland Baltimore, MDU Mass University of Massachusetts Amherst, MAUNL/USL University of Nebraska-Lincoln Lincoln, NEand University of Southwestern Louisiana and Lafayette, LAUnisys Unisys Center for Advanced Information Technology Paoli, PAFigure 1MUC-3 participants.templates.
One portion of the evaluation that was not completely determined beforethe conference was a scoring procedure.
Scoring guidelines were provided but sitesscored their own runs by hand.
This led to substantial variation in how answers werescored among sites, and resulted in a consensus that a more rigorous and objectivescoring procedure was needed.Out of these experiences at the earlier message understanding conferences cameMUC-3, held in May 1991.
The major changes with MUC-3 were:1.
A new, more general domain, namely Latin American terrorist activity,with text originating from foreign news sources.2.
A ten-fold increase in the number  of training documents (1,300), andmore than a hundred-fold increase in number  of words of text(approximately 400,000 words of text, with a vocabulary of 18,000words).3.
An automated, interactive scoring program and a rigorous definition ofthe scoring procedure, to promote bias-free, reproducible scores and toallow off-site testing.4.
A broadening of the task to require distinguishing relevant fromirrelevant messages.1.3 Part ic ipat ing SystemsFifteen sites participated in MUC-3, as shown in Figure 1.
The official results of the siteson a test of 100 previously unseen messages are summarized in the plots in Figures 2and 3, showing recall versus precision and recall versus overgeneration.
These metricsare defined and discussed in Section 2.3.
Note that perfect performance would be at100% recall and 100% precision in Figure 2, whereas, in Figure 3, it would be at 100%recall and 0% overgeneration.
The systems are listed below and described briefly inSection 3.2.
The reader should consult the conference proceedings for detailed system411Computational Linguistics Volume 19, Number 3Figure 2Recall vs. precision.100908O7060PRECISION 504030-20-10-?
SRI GE?
UUUMassA~N:S mznP INYU;YN ?
UN~'Y S I~M~C IBBN13 ?GTE ?mHUGHESmm UNIJUSL0 i i i u l m m m I0 10 20 30 40 50 60 70 80 90 100RECALLOVERGENERATION1009080706050 mUNLAJSL40 ?ADS ?
HUGHESGTE ?30 SY~ UNISYS?20 LSI ?
IIMDC .... BBNmpR c rwT~if =UMassITP ?
GE10 mSRIu i !
!
n n n u |0 10 20 30 40 50 60 70 80 90 100RECALLFigure 3Recall vs. overgeneration.412Nancy Chinchor et al Evaluating Message Understanding Systemsdescriptions and references.
For descriptive purposes, we can group the systems intothree broad classes:Pattern-Matching SystemsThese systems were characterized by fairly direct mappings from text to fillers, with-out the construction of elaborate intermediate structures.
The mapping methods usedvaried widely.
Some treated a text as an unordered set of words, as in traditional textcategorization techniques from information retrieval (Lewis 1991).
Slot fillers were de-fined in terms of the presence of words, Boolean combinations of words, or weightedcombinations of words.
Other methods canned text for patterns pecified in someextension of the language of regular expressions and produced corresponding fillers.Some systems also used hierarchies of word-based concepts or patterns defined interms of other patterns.While many groups used pattern-matching as an aid to producing more structuredanalyses, five groups used pattern-matching exclusively.
For two groups (Hughes andUNL/USL), use of pattern-matching techniques was the focus of research.
Three othergroups (ADS, MDESC, and Unisys) implemented a pattern-matching front end withthe intention of integrating it with a linguistically based component.
However, forreasons of time, the linguistic component was not included in the MUC-3 evaluation.Syntax-Driven SystemsA second group of systems processed the messages by first obtaining a syntactic rep-resentation of sentences from the text, which was used as input to semantics andsubsequent processing.
This group included BBN, ITP, LSI, NYU, PRC, and SRI.
Thesesystems varied in their completeness of parsing (from partial parsing to full parsing),their use of feedback from semantic and pragmatic processing to influence syntacticparsing, and in their use of pattern-matching to supplement other forms of analysis.Semantics-Driven SystemsThe third group of systems were guided by semantic predictions and evolving se-mantic structures.
These mechanisms drove varying amounts of syntactic analysis,ranging from limited phrase parsing to syntactically rich partial parsing, in support ofthe semantic predictions.
The systems also used pattern-matching techniques heavily,but with a closer coupling to a domain model than the systems classified as pattern-matching systems.
We put the systems from GE, GTE, Synch/UMD, and U Mass inthis group.1.4 Evaluation Methodology1.4.1 Introduction to the MUC-3 Task.
The task for the MUC-3 evaluation was toextract data about terrorist incidents from newswire articles.
The extracted ata wereput into simulated atabase records defined by a template.
The template database slotsdescribed important aspects of those incidents, such as the type of incident, the targets,perpetrators, date, location, and effects.
An automated system that could perform thistask would be useful in an environment where many incoming messages make it tooexpensive and time-consuming for personnel to do the data extraction and qualitycontrol.The specific task domain for MUC-3 was restricted to terrorist acts involving nineLatin American countries.
Terrorist acts were defined as violent acts perpetrated withpolitical aims and a motive of intimidation.
These acts could be perpetrated by aterrorist or guerrilla group, the government or the military, or by unknown individuals.The targets excluded terrorist or guerrilla groups, the military, and police; attacks413Computational Linguistics Volume 19, Number 3TST2-MUC3-O069BOGOTA, 7 SEP 89 (INRAVISION TELEVISION CADENA 1) -- \[REPORT\] [MARIBELOSORIO\] \[TEXT\] MEDELLIN CONTINUES TO LIVE THROUGH A WAVE OF TERROR.FOLLOWING LAST NIGHT'S ATTACK ON A BANK, WHICH CAUSED A LOT OF DAMAGE,A LOAD OF DYNAMITE WAS HURLED AGAINST A POLICE STATION.
FORTUNATELY NOONE WAS HURT.
HOWEVER, AT APPROXIMATELY 1700 TODAY A BOMB EXPLODEDINSIDE A FAST-FOOD RESTAURANT.A MEDIUM-SIZED BOMB EXPLODED SHORTLY BEFORE 1700 AT THE PRESTOINSTALLATIONS LOCATED ON \[WORDS INDISTINCT\] AND PLAYA AVENUE.APPROXIMATELY 35 PEOPLE WER~!
INSIDE THE RESTAURANT AT THE TIME.
A WORKERNOTICED A SUSPICIOUS PACKAGE UNDER A TABLE WHERE MINUTES BEFORE TWOMEN HAD BEEN SEATED.
AFTER AN INITIAL MINOR EXPLOSION, THE PACKAGEEXPLODED.
THE 35 PEOPLE HAD ALREADY BEEN EVACUATED FROM THE BUILDING,AND ONLY 1 POLICEMAN WAS SLIGHTLY INJURED; HE WAS THROWN TO THEGROUND BY THE SHOCK WAVE.
THE AREA WAS IMMEDIATELY CORDONED OFF BYTHE AUTHORITIES WHILE THE OTHER BUSINESSES CLOSED THEIR DOORS.
IT IS NOTKNOWN HOW MUCH DAMAGE WAS CAUSED; HOWEVER, MOST OF THE DAMAGE WASOCCURRED INSIDE THE RESTAURANT.
THE MEN WHO LEFT THE BOMB FLED ANDTHERE ARE NO CLUES AS TO THEIR WHEREABOUTS.Figure 4Example of a MUC-3 message.on these organizations or their members were considered acts of guerrilla warfare.Figure 4 shows an example of a MUC-3 message as excerpted in the Foreign BroadcastInformation Service (FBIS) Daily Reports.
The original source of the example is Inra-vision Television Cadena 1 as shown in the dateline.
FBIS was the secondary sourcefor all of the articles used in MUC-3.The message in Figure 4 describes three violent acts.
The hurling of dynamiteat the police station was not considered a relevant errorist act because the policewere the target and no civilians were involved.
The attack on the bank is referred tobriefly, while the bombing of the restaurant is quite detailed.
The erroneous phrase"was occurred" appeared in the original message text.
The templates in Figure 5 arethe answer key templates for the attack on the bank and the bombing of the fast-foodrestaurant.
21.4.2 Gathering and Preparing Texts for the Training and Test Corpora.
The texts inthe MUC-3 corpus were gathered using a keyword query on an electronic databasecontaining articles in message format from open sources worldwide.
Most of the arti-cles in the MUC-3 corpus were translated from Spanish sources by FBIS.The keyword query for choosing texts for the MUC-3 corpus was a two-partquery using country or nationality :name and inflected forms of common words asso-ciated with terrorist acts.
The nine countries of interest were Argentina, Bolivia, Chile,Colombia, Ecuador, E1 Salvador, Guatemala, Honduras, and Peru.
The word list usedfor selecting messages having to do with terrorism was based on the verbal and nom-inal forms abduct, abduction, ambush, arson, assassinate, assassination, assault, blow \[up\],bomb, bombing, explode, explosion, hijack, hijacking, kidnap, kidnapping, kill, killing, murder,rob, shoot, shooting, steal, and terrorist.Messages fulfilling both parts of the query were downloaded from the mixed caseelectronic source into uppercase ASCII format.
3 The messages were prepared for use2 The answer key contains all possible cor~'ect ways of filling in the template.
Alternative fillers areseparated by slashes, and optional fillers are preceded by question marks.3 The change was a consequence of the downloading.
It would be preferable to preserve the case of theoriginal text.
However, the capabil ity to 'process both mixed case and uppercase only is required for414Nancy Chinchor et al Evaluating Message Understanding Systems0.
MESSAGE ID1.
TEMPLATE ID2.
DATE OF INCIDENT3.
TYPE OF INCIDENT4.
CATEGORY OF INCIDENT5.
PERPETRATOR: ID OF INDIV(S)6.
PERPETRATOR: ID OF ORG(S)7.
PERPETRATOR: CONFIDENCE8.
PHYSICAL TARGET: ID(S)9.
PHYSICAL TARGET: TOTAL NUM10.
PHYSICAL TARGET: TYPE(S)11.
HUMAN TARGET: ID(S)12.
HUMAN TARGET: TOTAL NUM13.
HUMAN TARGET: TYPE(S)14.
TARGET: FOREIGN NATION(S)15.
INSTRUMENT: TYPES(S)16.
LOCATION OF INCIDENT17.
EFFECT ON PHYSICAL TARGET(S)18.
EFFECT ON HUMAN TARGET(S)0.
MESSAGE ID1.
TEMPLATE ID2.
DATE OF INCIDENT3.
TYPE OF INCIDENT4.
CATEGORY OF INCIDENT5.
PERPETRATOR: ID OF INDIV(S)6.
PERPETRATOR: ID OF ORG(S)7.
PERPETRATOR: CONFIDENCE8.
PHYSICAL TARGET: ID(S)9.
PHYSICAL TARGET: TOTAL NUM10.
PHYSICAL TARGET: TYPE(S)11.
HUMAN TARGET: ID(S)12.
HUMAN TARGET: TOTAL NUM13.
HUMAN TARGET: TYPE(S)14.
TARGET: FOREIGN NATION(S)15.
INSTRUMENT: TYPES(S)16.
LOCATION OF INCIDENT17.
EFFECT ON PHYSICAL TARGET(S)18.
EFFECT ON HUMAN TARGET(S)TST2-MUC3-00691(06 SEP 89) / (06 SEP 89 - 07 SEP 89)ATTACK?
TERRORIST ACT"BANK"1FINANCIAL: "BANK"COLOMBIA: MEDELLIN (CITY)SOME DAMAGE: "BANK"TST2-MUC3-0069207 SEP 89BOMBINGTERRORIST ACT"TWO MEN" / "MEN""FAST-FOOD RESTAURANT" /"PRESTO INSTALLATIONS" / "RESTAURANT"1COMMERCIAL: "FAST-FOOD RESTAURANT" /"PRESTO INSTALLATIONS" / "RESTAURANT""PEOPLE""POLICEMAN"36CIVILIAN: "PEOPLE"LAW ENFORCEMENT: "POLICEMAN".COLOMBIA: MEDELLIN (CITY)SOME DAMAGE: "FAST-FOOD RESTAURANT" /"PRESTO INSTALLATIONS" / "RESTAURANT"INJURY: "POLICEMAN"NO INJURY: "PEOPLE"F igure  5Filled templates for the example MUC-3 message.by creat ing or augment ing  the datel ine and text type in format ion at the front of thearticle, and then remov ing  the or iginal  headers and rout ing information.
Exact du-pl icate messages were removed.
Other  minor  preparat ions inc luded mod i fy ing  thereal-world applications.415Computational Linguistics Volume 19, Number 3articles to compensate for control clharacters and punctuation (brackets and exclama-tion marks) lost during the download, removing idiosyncratic features of the texts(including parentheses added by the transcribers to mark uncertainties), and improv-ing readability by double-spacing between paragraphs.The downloaded corpus of over 1,600 messages was divided into one developmentcorpus containing 1,300 messages and 3 test corpora each containing 100 messages.Neither the chronological order of messages nor the fact that the same real-worldevent was described from different perspectives in different messages was consideredwhen separating the texts into development and test corpora.
The choice of messagesfor the test sets was based solely on the frequency with which incidents concerninga given country were represented.
Unique identifiers were assigned to each messagein the MUC-3 corpus; for example, DEV-MUC3-0001 indicates the first developmentmessage for MUC-3, and TST2-MUC3-0050 indicates the 50th message in the official(phase two) test set.
The first and second 100-message t st sets were used for the twophases of MUC-3, the dry run and the official test, respectively.
The third test set ofover 100 messages was saved for use in MUC-4.1.4.3 Nature of the Original Textual Data.
The original textual data consisted of news-paper stories, radio and television broadcasts, peeches, interviews, news conferencetranscripts, and communiqu6s.
The average message length was 12 sentences and theaverage sentence length was 27 words) The texts varied widely in complexity, style,and richness of information.
Newspaper stories were written in normal reporting stylecontaining much factual information and frequent long, complex sentences.
The storiescontained numerous quotes as well\[ as excerpts from all of the other types of textualdata.Radio and television broadcasts consisted of transcribed utterances that containedcommentary and/or excerpts from reports by correspondents and other sources.Speeches were in transcribed form and contained rhetorical and colorful languagewith heavy use of figures of speech.
Speeches were of interest because they containedmetaphorical uses of some "perpetration" verbs, for example, "philosophers... havelaunched a new attack" and "... destroy the economic infrastructure."
Some excerptsof speeches appeared embedded i~L news reports.
Interviews with single or multipleinterviewers appeared as entire messages or as excerpts embedded in news reports.
In-frequently, messages contained transcripts of news conferences with moderator, guests,and reporters peaking.
Rebel communiqu6s often contained lists of demands, allega-tions, and intentions.
Communiqu6s sometimes contained lines identifying date andplace of origin and signatures of their originators.
Some of these communiqu6s wereembedded in news reports complete with commentary, date, location, and signatures.The discourse styles and sentence structures varied considerably across all of thesetypes of textual data.The texts of all types contained numerous anomalies.
There were words and pas-sages marked as "indistinct" in transcripts (e.g., first sentence in second paragraphof Figure 4) and passages marked as "omitted."
Articles were sometimes plit intomessage-length segments of approximately two pages each and were annotated toindicate continuation.
The texts contained nonstandard constructions resulting fromthe translation of Spanish to Engli:~h and also contained unflagged errors, includingtypographical errors, misspellings, omitted words, grammatical errors, and punctua-tion errors.
In addition, the texts posed other challenges including the frequent use of4 Additional statistics oncerning the MUC-3 corpus are presented lsewhere (Hirschman 1991b).416Nancy Chinchor et al Evaluating Message Understanding SystemsSpanish names, untranslated words, and Spanish acronyms followed or preceded bythe corresponding full phrase in English or Spanish.1.4.4 Nature of the Template Fills.
The MUC-3 template fill task was complex notonly because of the richness of the textual data but also because of the variety ofinformation required to fill template slots and because of the interdependencies amongthe slot fillers.
The templates contained 18 slots.
The message-id and template-id slotsidentified the template.
If the message did not contain information on terrorist events,it was considered irrelevant.
In this case, only an empty template was generated withjust the message-id slot filled.
If the message was relevant, here could be one or moretemplates generated for the message.
The possibility that a message contained morethan one relevant incident made the MUC-3 task considerably more difficult than atask requiring only one template per message.To determine the relevance of an incident in a message, a system needed to de-termine the dates of any possible terrorist acts, discriminate between old and newinformation, discriminate between vague and specific descriptions, determine whatcountry the perpetrators and/or targets belonged to, discriminate between terrorismand either general criminal activity or guerrilla warfare, and determine whether theincident was actual, attempted, or threatened as opposed to planned, denied, or hy-pothetical.To fill the slots in the template, the systems needed to isolate the incidents in eachmessage and to extract information about he associated perpetrators and targets.
As-pects of the terrorist action such as the date, location, instruments u ed, and the effectson the targets also had to be extracted.
In order to extract he data and place it in thetemplate correctly, systems had to resolve ambiguous information, infer informationfrom vague references, categorize some of the information i to predetermined classesof entities, put data into canonical form, and show links among certain data items.The template slots fell into formal categories depending on the nature of correctfillers.
A slot filler could be a string directly extracted from the text (e.g., "FAST-FOOD RESTAURANT"), a member of a finite set of fills (e.g., "TERRORIST ACT"),a numerical response (e.g., "36"), a specially formatted response such as a date orlocation (e.g., "07 SEP 89"), or a null value (e.g., "-" or "*") that indicated that noinformation was available or required for the incident.The slots that took fillers from a finite set could either take one filler for eachtemplate (message-id, incident-type, and incident-category slots), several fillers for eachtemplate with all of the fillers being different (instrument-type slot), or several fillers foreach template with multiple instances of the same filler allowed (perpetrator-confidence,physical-target-type, human-target-type, foreign-nation, physical-target-effect, and human-target-effect slots).
The latter slots required that their filler(s) be cross-referenced to thefiller(s) of another slot to capture interdependencies.
For example, in template 2 of Fig-ure 5, the human-target-type "LAW ENFORCEMENT" is cross-referenced to the human-target-id "POLICEMAN."
There could be multiple instances of "LAW ENFORCE-MENT" in the human-target-type slot that refer to different human-target-ids.For each type of incident, there was a different set of slots that were allowedto be filled.
For example, physical-target slots were not filled for MURDER incidents,since physical damage accompanying a murder was defined to be part of a separateATTACK.
The effect-on-human-target slo  was required to be blank as well, since thisinformation was redundant for the MURDER incident ype.The template design for MUC-3 structured the template in such a way that thefilling of the slots was a well-defined but complex problem.
In addition to the featuresof template design mentioned above, optional templates and optional and alternative417Computational Linguistics Volume 19, Number 3slot fillers were specified in the answer keys to capture borderline cases.
The tem-plate design provided several interesting problems for the development of scoringalgorithms, coring guidelines, and answer keys.1.4.5 Generating Answer Keys.
To evaluate system performance on the test sets, thesystems' responses to the template fill task were compared with the responses inanswer keys using a semi-automated scoring system developed for MUC-3.
NRaDgenerated the answer keys for the MUC-3 test sets.
However, the development mes-sages also required answer keys for use as training material.
Early in the evaluationschedule, NRaD completed the answer key for the first 100 development messages, andthe participants completed the answer keys for the other 1,200 development messages.Participants were assigned blocks of 75 messages that overlapped partially with blocksfrom other sites.
Sites had to agree on templates for those overlapping messages.
Thepurpose of the overlapping blocks was to encourage uniformity of the answer keys forthe development messages.
In actuality, template design, the development of guide-lines for filling templates, and the generation of development set answer keys wereall interdependent activities.
The answer keys for the development corpus had to beupdated whenever there were changes in template design or in the guidelines forfilling templates.During the generation of the official answer key, an attempt was made to measurethe consistency of human template filling.
Two evaluators at NRaD prepared answerkeys for the official test set in the following manner.
Each evaluator ead the testmessages and determined which templates were to be generated for the messages.The evaluators discussed the template-level decisions concerning relevancy of the vi-olent incidents mentioned in the messages, came to an agreement as to how manytemplates were to be generated for each message, and then independently generatedtemplate fills for the relevant incidents.
One of the evaluators made a final decisionas to the contents of the official answer key based on the two independently pro-duced sets of templates.The other evaluator's key was then scored against he officialanswer key and received scores of 87% recall and 91% precision as shown in theMATCHED/MISSING row of Figure 8.
5 These scores reflect he degree to which well-trained evaluators agree.
They also give some idea of what generous upper boundson system performance might be.
It should be noted that the evaluators took severaldays to complete the template-filling task for 100 test messages.2.
Measures of Effectiveness for Data ExtractionGiven the official answer key, it wa:s necessary to come up with measures of how welleach system's output agreed with the key.
The MUCK-II evaluation used a simple,ad hoc scoring scheme for this purpose.
For MUC-3, the organizers and participantsfelt a more rigorous approach was :needed.
Effectiveness measures have long been thesubject of research in information retrieval (IR), and the decision was made to adaptand extend IR measures of effectiveness for MUC-3.
In this section we first describe theformal model underlying the most widely used IR measures of effectiveness.
We thendiscuss the changes made in order to adapt hese measures to the data extraction task.Finally, we describe the procedures and software used for computing these measures,given system output and the official answer key.5 Recall, precision, and MATCHED/MISS!\[NG are defined in Section 2.3; briefly, recall measurescompleteness, precision measures accuracy, and MATCHED/MISSING is a manner of scoring thatpenalizes for missing information but not spurious information.418Nancy Chinchor et al Evaluating Message Understanding SystemsYes is Correct No is CorrectDecides Yes a b a+bDecides No  c d c+da+c b+d a+b+c+d=nFigure 6Contingency table for a set of binary decisions.2.1 Effectiveness Measures in Information RetrievalA primary focus of IR research is text classification.
If we consider a single category,then a text classification system must make n binary decisions to categorize n units oftext.
The result of n such decisions can be summarized in a contingency table, as shownin Figure 6.
Each entry in the table specifies the number of decisions with the indicatedresult.
For instance, a is the number of times the system decided Yes (assigned the textto the category), and Yes was in fact the correct answer.Given the contingency table, three important measures of the system's effective-ness are:1. recall = a/(a+c)2. precision = a/(a+b)3. fallout = b/(b+d)Recall (true positive rate) and fallout (false positive rate) originated in signal detectiontheory (Swets 1964).
They measure the ability of a system to detect a signal in thepresence of noise, and the system's relative willingness to make errors of commissionversus errors of omission.
Recall is widely used in IR, but fallout is often replaced byprecision, a measure that is more intuitive though sometimes less informative (Swets1969).
For MUC-3, a fourth measure, overgeneration, was defined.
It is discussed inSection 2.3 and does not have a direct interpretation i  terms of the contingency table.A system can achieve perfect recall by never deciding No; or perfect precision,fallout, and overgeneration by never deciding Yes.
Therefore, at least recall, plus oneof the other three measures, is necessary for a nontrivial evaluation of a system's effec-tiveness under the contingency table model.
Note that all of these measures assume auniform cost for errors across texts and across type of error (cell b versus cell c).
Treat-ing some fillers as being more important han others may be desirable in evaluatingdata extraction systems for particular applications.2.2 Adapting the Contingency Table Model to MUC-3The contingency table measures eemed a natural approach for measuring the abil-ity of data extraction systems to minimize uncertainty about correct fillers, and formeasuring the systems' trade-off between generating incorrect fillers and missing cor-rect fillers.
However, the MUC-3 task differs from text classification and other signal419Computational Linguistics Volume 19, Number 3detection tasks in many ways, and these had to be taken into account in producingeffectiveness measures for MUC-3.Consider a simplified version o:f the MUC-3 task: a template consists of m slots,with slot i having ni possible fillers.
Slot i can be filled with between 0 and ni of thosefillers, and no filler can be repeated.
The contingency model applies straightforwardlyto this simplified case, since a filled--out emplate can be viewed as the result of nl +n2 + " ' "  +nm decisions to assign or not assign each of the legal slot fillers.The actual MUC-3 task departs :in several ways from this simplified model.
Mostset fill slots do allow the same fill to occur several times.
String fills, dates, and loca-tions are difficult to view as the result of binary decisions.
The optional and alternativeversions of correct answers in the MUC-3 answer keys call into question a dichotomybetween correct and incorrect decisions, as does the allowing of partial credit foranswers.
Our (imperfect) approach to all these difficulties was to model systems asmaking a binary decision between generating a correct filler and generating an incor-rect filler.
Partial matches were scored as one-half a correct decision.
This obviously isa considerable simplification of the t:ask that actually faced systems.The number, as well as the character, of decisions in MUC-3 also departed fromthis simple model.
Several slots allowed an unbounded number of fillers, and thus apotentially unbounded number of decisions, whether or not decisions were viewedas binary.
Our solution to this was t:o assume that the number of Yes decisions madeby the system was the number of fillers generated by the system, while the numberof decisions for which Yes was correct was the number of fillers in the key, withoptional fillers being counted only if attempted by a system.
This gave us values forcontingency table cells a, b, and c, but not d. Recall and precision, but not fallout,could be computed for slots with an unbounded number of fillers and/or  fillers froman unbounded set of alternatives.
A slightly different method was used for set fillslots, and this method did allow a variant on fallout to be computed.Other issues arose because the decisions made in template filling are not inde-pendent of each other.
Complex rules for assigning partial credit in cases of cross-referenced slots had to be defined.
In addition, real-world relationships made somefillers completely dependent on others.
For instance, the filler for EFFECT ON HU-MAN TARGET would always have been DEATH if the INCIDENT TYPE was MUR-DER.
To avoid treating such cases as two decisions, we forbade slots to be filled incertain cases.
This decision was made for evaluation purposes.
In viewing the tem-plates as database ntries, however, it may be desirable to make redundant informationexplicit.At the template level, a system might generate more or fewer templates than werepresent in the answer key.
The appropriate treatment of unmatched templates wasnot at all clear, so the scoring program was designed to generate overall scores un-der three different assumptions (MATCHED ONLY, MATCHED/MISSING, and ALLTEMPLATES), which are described in the next section.2.3 Scoring ProcedureEach template fill generated by a MUC-3 system was compared with the correspondingfills in the answer key.
The fill was counted as falling into one of six categories, asshown in Figure 7.
The counts COR .
(correct), SPU (spurious), MIS (missing), and NON(noncommittal) are roughly analogous to the values a, b, c, and d, respectively, in thecontingency table of Figure 6.
PAR (partial) indicates the number of decisions that arecounted as adding only 0.5 to cell a.
A decision in INC (incorrect) would cor respondto two incorrect decisions in the contingency table model, one counted in cell b (for420Nancy Chinchor et al Evaluating Message Understanding SystemsCategory Criterion ColumnCorrect response =key CORPartial response ~key PARIncorrect response ~key INCSpurious key is blank and response isnot SPUMissing response isblank and key is not MISNoncommittal key and response are both blank NONFigure 7Scoring criteria.entering a spurious answer) and one counted in cell c (for failing to get the correctanswer).The comparison between system response and answer key was done using a semi-automated scoring program, which produced a summary score report like that shownin Figure 8.
The summary score report showed the total over all templates for eachof the six response categories.
The user of the scoring program could override itsdecision and interactively score an answer as partially or completely correct evenwhen it did not match the key.
The ICR (interactive correct) and IPA (interactivepartially correct) columns indicated the number of fills scored interactively as correctand partial, respectively.
The POS column contained the number possible, which wasthe sum of the number correct, partial, incorrect, and missing.
The number possiblewas computed from the system responses in comparison with the answer key ratherthan from the answer key alone because of the optional templates and slot fillersappearing in the key.
The ACT column contained the number of actual fills, whichwas the sum of the number correct, partial, incorrect, and spurious.The evaluation metrics of recall, precision, overgeneration, and fallout (shown inthe last four columns of Figure 8) were calculated using the totals in these columnsboth for the individual slots and for all of the slots combined 6 as follows:?
Recall was the degree of completeness of attempted fills.Recall = (COR + (0.5 ?
PAR))/POS?
Precision was the degree of the accuracy of attempted fills.Precision = (COR + (0.5 * PAR))/ACT?
Overgeneration was the degree of spurious generation.Overgeneration = SPU/ACT?
Fallout was the degree of producing incorrect fills relative to the numberof possible incorrect fills for set-fill slots.
7Fallout = (INC + SPU)/(NUMBER_POSSIBLE_INCORRECT)6 A more detai led mathematical description of the evaluation metrics complete with examples can befound in Chinchor (1991a).7 The number of possible incorrect fills is the cardinality of the set of al lowable fills minus the number offills in the key.421Computational Linguistics Volume 19, Number 3SLOT POS ACTtemplate-id 117 115incident-date 113 110incident-type 117 114category 90 109indiv-perps 104 61org-perps 69 68perp-confidence 69 68phys-target-ids 59 57phys-target-num 41 41phys-target-types 59 57human-target-ids 144 133human-target-num 93 88human-target-types 144 133target-nationality 19 19instrument-types 24 22incident-location 117 113phys-effects 41 44human-effects 56 55MATCHED ONLY 1442 1407MATCHED/MISSING 1476 1407ALL TEMPLATES 1476 1425SET FILLS ONLY 619 621COR E~R IN(114 0 090 10 10112 1 188 0 059 0 258 0 156 1 254 3 039 0 252 4 1129 2 079 6 2126 2 314 2 016 1 088 24 137 3 043 2 21254 61 271254 61 271254 61 27544 16 9ICR IPA0 031 100 10 010 015 012 114 30 011 433 20 623 24 20 00 18 310 2171 37171 37171 3768 15SPU MIS NON1 3 390 3 40 3 021 2 60 43 499 10 479 10 470 2 760 0 760 2 762 13 231 6 232 13 233 3 1035 7 880 4 04 1 888 9 8065 100 82765 134 84883 134 85252 50 511REC PRE OVG FAL97 99 184 86 096 99 0 098 81 19 1557 97 084 85 1382 83 13 294 97 095 95 092 95 0 090 98 288 93 188 95 2 079 79 16 069 75 23 085 88 094 88 9 078 80 14 189 91 587 91 587 90 689 89 8Figure 8Sample summary score report showing one evaluator's scores for the answer key.As discussed in the previous section, these measures are considerably modi f ied fromthose of the same name in in format ion retrieval, and  should not necessari ly be inter-preted in the same way.At the bot tom of the summary  score report were four different summar ies  ofsystem performance for the full complement  of slots.
The first three of the summaryscore rows corresponded to giv ing different importance to over- or under -popu la t ingthe database.
For a strict score, slot fills in a spur ious template could all be scored asspur ious  or, for a lenient score, just tlhe template could be scored as spurious.
Similarly,slot fills in a miss ing template could be scored as ind iv idua l ly  miss ing or just thetemplate could be scored as missing.
The first summary  score row was the MATCHEDONLY row, which indicated the scores result ing from scoring miss ing and spur ious422Nancy Chinchor et al Evaluating Message Understanding Systemstemplates only in the template-id slot.
The MATCHED/MISSING row contained theofficial MUC-3 test results.
The missing template slots were scored as missing, whereasthe spurious templates were scored only in the template-id slot.
The totals in this rowwere the totals of the tallies in the columns as shown.
The metrics were calculatedbased on the summary totals.
The ALL TEMPLATES row had missing templates scoredas missing for all missing slot fills and spurious templates scored as spurious for allspurious lot fills.
8 This row was the strictest score for the systems.
Another way ofdescribing the differences between these measures i that ALL TEMPLATES roughlycorresponded to taking a message-level view of performance, MATCHED ONLY atemplate-level view, and MATCHED/MISSING something in between.The fourth summary score row gave the scores for SET FILLS ONLY.
The totalshere were for slots with finite set fills only.
A global fallout score as well as recall,precision, and overgeneration could be calculated for these slots and was given in thisrow.
Note that the denominator f the fallout score is determined by the number ofpossible incorrect fills for each slot instance and, thus, depends on the cardinality ofthe set of possible fills and how many of those fills appear in the answer key.2.4 Scoring SoftwareThe scoring for MUC-3 was carried out using an interactive scoring program espe-cially developed for the evaluation.
The program simultaneously displayed the systemresponse and corresponding answer key template in different windows.
It automati-cally scored the displayed templates, requiring user interaction only for instances ofmismatching slot fills.
The user would determine whether the mismatch should bescored as a full match, a partial match, or a mismatch, according to the official scoringguidelines.
The scoring program kept a history of user interactions.
It also produceda detailed score report of the template-by-template scores and a summary score re-port.
In addition to being used for the official scoring, the program was used by theparticipating sites during development asa tool for determining progress and for re-gression testing.
Additional features allowed the program to be adapted whenever thetemplate design changed, allowed the scoring of subsets of slots and templates usefulin linguistic phenomena testing (Chinchor 1991b), and allowed the merging of partialcredit decisions across sites for more uniform scoring.
During interactive scoring, theprogram kept track of what partial matches were allowed.
By pooling these recordsacross sites, the systems could be rescored, giving all systems the benefit of a partialmatch allowed for one site.
Each site scored its answers individually for presentationat the conference, but the official scores were produced by volunteers from two sitesworking together.3.
Participant Methodologies3.1 Technical RequirementsThe nature of the task in MUC-3 imposed a number of requirements on the partici-pating systems.
Despite considerable divergence in technical approach, all the systemshad to cope with issues such as large vocabulary, English and Spanish proper names,generation of well-formed templates, and discrimination of relevant from irrelevantmessages.
In Figure 9 we show a generic architecture for the MUC-3 systems.
9 It con-sists of a preprocessing module, a lexical processing module, a linguistic component (for8 The ALL TEMPLATES measure was adopted as the official measure for MUC-4.9 This is intended tobe a generic architecture.
Each system put together its own subset of thesecapabilities governed by a specific ontrol strategy (not represented at all in the figure).423Computational Linguistics Volume 19, Number 3m_  pREp.ocEss,.o".....
~!!
!iii~iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiLEXICALPROCESSING  iiiiiiiiiiiiiiiii!i   :  iiiii iiiiiiiiii:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::SYNTAXSEMANTICSiiiiiiiiiiii i!
i  iiiiiiiiiiiiiiiDISCOURSEi !
i i i : : i i i i i i ; : , ;R~i i !
i i i i i i i i l l~ JTEMPLATEGENERATIONTEMPLATEFigure 9A generic message understanding system architecture.the linguistically based systems), and a template generation module.
It is interesting tonote that two of the modules (preprocessing and template generation) are not really partof standard computational linguistitcs.
This reflects the demands of a "realistic" ap-plication, where a substantial amount of effort is devoted to system engineering anddata transformation issues.3.1.1 Preprocessing.
Almost all systems included a module that was referred to as a"preprocessor."
The function of the module varied from system to system, but oftenincluded:?
Parsing of the message into fixed field information and free text.?
Identification of interesting :~egments of text (where the segment couldrange from the word level all the way up to the message level).
Textsegments or even entire messages that were categorized as uninterestingwere often filtered out from further processing.?
Segmentation f the character stream into tokens.?
Regularization of the message text, including regularization of certainkinds of forms (dates, numbers), bracketing into phrases, identificationof certain discourse transition markers, etc.
These operations oftenoverlapped with those in lexical processing.424Nancy Chinchor et al Evaluating Message Understanding Systems3.1.2 Lexical Processing.
The lexical processing stage was responsible for providinginformation on words in the text stream.
This could include:?
Morphological processing: Decomposition of words into parts, usingexplicit morphological rules or string-matching methods.?
Assignment of syntactic lasses: Methods used included lexicon lookup(of exact form or root), inference from affixes (detected by morphologicalanalysis or string matching) and statistical tagging.
Some systems used asmall set of atomic classes, others specified more elaborate complementstructures or sets of features.?
Semantic analysis: Tagging words as to semantic types, instantiatingknowledge base concepts, or activating semantic predictions.
This variedwidely between systems.?
Special form processing: There were a number of productive xpressiontypes (e.g., times, dates) that needed special processing.
In addition,many systems had modules for handling names (especially Spanishnames).?
Spelling correction.?
New words: Many systems had a mechanism for categorizing unknownwords; it often interacted heavily with morphological nalysis, syntacticclass assignment, and spelling correction.3.1.3 Linguistic Processing.
Both the syntax-driven and the semantics-driven systemsprovided a stage of linguistic processing.
This processing could include:?
Syntactic processing to identify basic phrases and/or sentences; thisoften included robust or partial parsing.?
Semantic processing, which could include application of selectionalrestraints, reduction of information to a canonical semanticrepresentation, most often in the form of a logical expression or aninstantiated frame, and identification of individual events.?
Discourse processing, which handled resolution of referring expressions(including pronouns and definite reference), temporal analysis, andreconstruction of implicit information, as well as determination of otherdiscourse relations.3.1.4 Template Generation.
The final stage, template generation, was present in all ofthe systems, since templates were the output o be evaluated in MUC-3.
However, therewas often no one-to-one correspondence b tween event representations and templates,leading systems to develop various strategies for deciding how to map events foundin a single message into zero or more templates.
Thus issues for template generationincluded:?
Whether to generate a template at all, since there were complex rulesconcerning what constituted a terrorist incident;?
Whether to generate multiple templates for a message because a singlemessage, or even a single sentence, could report more than one incident;425Computational Linguistics Volume 19, Number 3?
Whether to merge multiple event representations i to a single template;and?
How to fill in the slots of the templates.3.2 System SummariesThe following subsections provide brief descriptions of the participating systems; allinformation is taken from the MUC-3 proceedings, unless otherwise noted.
The readeris referred to these proceedings for detailed system descriptions and references and toFigures 2 and 3 for system results.3.2.1 Pattern-Matching Systems.ADSThe ADS system combined a pattern-matching text categorization system and a naturallanguage system.
The text categorization system was designed to serve as a loosefilter in the preprocessing stage, anti as a possible back-up in case of failure to parse.Because of difficulties in scaling the lexicon to the full MUC-3 training set, the naturallanguage system was not run, and only the results from the text categorization systemare reported for MUC-3.
Heuristically weighted rules for this system were handcraftedfor each of the significant event ypes.
A template with only the incident-type slot filledwas created for each sentence containing incident-type information.HughesThe Hughes system made use of machine learning methods to correlate input textwith filled templates.
Processing of a new message began by using a semantic gram-mar to decompose each sentence into phrases.
Phrasal rules and content words wereassociated with nodes in a concept hierarchy.
A sentence was represented asa list ofbinary features corresponding to concept nodes.
This list of feature values was usedto retrieve the k most similar cases from a case memory (k = 12 for MUC-3).
Each casewas a pairing of a previously analyzed sentence (i.e., a list of feature values) from thetraining corpus with its corresponding template.
A tentative set of slot fills, includingstring fills, for each sentence was produced by combining fillers from retrieved cases.Competitive statistical filtering was used to form groups of adjacent sentences andassign them definite incident ypes.
The tentative fills for the sentences in each groupwere combined to form a single template, though dates and locations were handledby special purpose code.MDESCThe MDESC system was planned as a multicomponent system providing tools fordeveloping customized text understanding systems, but only the first module, theskimmer, was used in MUC-3.
TILe skimmer identified fixed (multiword) phrases,including proper names, replacing them with the corresponding concepts.
Keywordswere also used to tag segments of text.
These segments were then grouped into actorsand objects and used to fill templates.
The templates were checked by a semantictrimming and rejection process.UnisysThe Unisys system was also envisioned with a pattern-matching component as a pre-processor, followed by a linguistic' component for fuller analysis of interesting seg-ments.
However, only the pattern-matching component was used in MUC-3.
Thepattern-matching component combined a statistical keyword-based retrieval of rel-426Nancy Chinchor et al Evaluating Message Understanding Systemsevant text portions plus a knowledge-based component, which supported constraintsstated in terms of various string relations, such as contiguity, same text, same para-graph, same sentence.
These rules were used to infer facts, and the template-generationsystem then operated on these facts to produce filled templates.UNL/USLThe UNL/USL system was primarily oriented toward filling set-valued slots.
Subsetsof the training corpus were used in training linear classifiers by the perceptron algo-rithm.
A separate classifier was used to find an activation level for each set fill, basedon the presence of words and multiword phrases in the text.
An additional classifierwas trained to find an activation level for overall relevance, that is, whether a mes-sage should produce any templates.
Templates were generated for a message if themessage scored high enough on overall relevance, or if handcrafted rules indicatedthat sufficient individual fillers had been highly activated.
If the decision was madeto generate templates, exactly one template was created for each incident type witha sufficient activation level.
Then each sufficiently activated filler was assigned to thesingle incident type with the most similar pattern of activations across paragraphs.In addition, the string fill slots perpetrator-id and incident-location slots were filled if astring was found in the text that exactly matched a known filler of those slots on thetraining corpus.
As with set fills, each string fill was assigned to a single template bycomparison of paragraph level activation vectors.3.2.2 Syntax-Driven Systems.BBNThe BBN system used automated part-of-speech tagging and a robust parser to ob-tain partial syntactic parses.
A set of partial parses spanning the input string wasthen "glued together" for semantic (case frame) analysis.
Discourse analysis createdevent structures based on the semantic ase frames and also performed limited refer-ence resolution.
To partially automate the knowledge acquisition process, the domain-dependent case frames for semantics were acquired using a tool to hypothesize caseframes based on predicate occurrences in training data from the tagged TREEBANKcorpus (Santorini 1990) validated by a developer.ITPA domain-independent naive semantics lexicon was at the heart of the ITP system.The syntactic analysis was done using a borrowed parser because a broad-coveragegovernment-binding parser was still under development.
The parser produced a sin-gle parse (indicating ambiguous attachments), which the system post-processed toresolve attachment ambiguities and to perform word sense disambiguation based onthe naive semantics lexicon.
Discourse analysis identified actors and events and in-cluded a treatment of modals, negation, coordination, temporal processing, locativeprocessing, and reference resolution.LSIThe LSI system eliminated uninformative messages by using Boolean combinations ofkeywords and other heuristics.
For linguistic processing, the system used a govern-ment binding-based parser that was still under development.
An "Unexpected Inputs"component shadowed the message-processing components and provided modules forhandling unexpected words, parsing failures, and template generation problems.
Thesystem used a single handcrafted lexicon for syntactic and semantic knowledge with a427Computational Linguistics Volume 19, Number 3tool that allowed system developers to link semantic information to the frame database(knowledge base) and lexicon.NYUThe NYU system used a lexicon derived from the Oxford Advanced Learner's Diction-ary, supplemented with hand-built entries for selected words, as well as for organi-zations, locations, and proper names found in training messages.
The domain-specificsemantic hierarchy and lexicosemantic models were developed using keyword-in-context indices derived from the training data.
The system filtered out sentences ifthey did not contain any words corresponding to an interesting semantic oncept.
Thebroad coverage linguistic string grammar and underlying chart parsing mechanismwere augmented with parse heuristics that enabled the system to process well over90% of the input deemed relevant.
Semantics was expressed in terms of frame-likeentity and event structures and was followed by reference resolution.
The templategeneration module suppressed generation of templates according to a set of heuris-tic rules.
The system was run in several configurations, changing the heuristics thatcontrolled when to generate templates.PRCThe PRC system used its existing core modules for the lexicon, morphology, syntax,and semantics.
These were extended and adapted to the MUC-3 domain using various?
interactive and batch tools.
The system used a semantic oncept analyzer for rejectionof hypothesized parses.
The parser also used a time-out mechanism: for sentencestaking more than a threshold time (approximately 50% of the corpus), the parsersimply returned the longest parsed substring or a run-on sentence analysis.
Featuresnew for MUC-3 were the addition of heuristics for guessing unknown words and adiscourse module that collected conceptual frames for use in template generation.SRIFor linguistic processing, the SRI system used its broad-coverage grammar with ahandcrafted 12,000-word lexicon.
A relevance filter based on n-gram statistics andhand-specified keywords was used to screen out sentences unlikely to contain usefulinformation.
Implementations of various parse heuristics were key to robustness inparsing the remaining sentences.
The syntactic processing stage produced a parse anda logical form, which was passed to the pragmatics component.
Pragmatics was doneby abductive theorem proving, which attempted to build the lowest cost explanationfor the observed set of semantic relations.
The process of building this explanation i -volved resolving reference (by minimizing the number of entities), filling in contextualinformation, and inferring relationships among events, all driven by a domain-specificcollection of axioms.
Templates were generated from the output of the pragmatics com-ponent using heuristics about the number and kind of templates to generate from an"interesting act.
"3.2.3 Semantics-Driven Systems.GEThe GE system tool set was used l:o adapt and customize xisting modules for theMUC-3 application.
Preprocessing was quite extensive in the MUC-3 domain andincluded initial segmentation and bracketing into discrete units, as well as activationof relevant emplates based on the presence of keywords.
At the heart of the systemwas a domain-independent 10,000 word-root lexicon and a 1,000-concept semantichierarchy.
Processing used "relation-driven" control, which combined syntactic and428Nancy Chinchor et al Evaluating Message Understanding Systemssemantic preference scores.
This permitted semantic preferences to influence parsepriorities, avoiding a combinatorial explosion of possible parses.
The grammar, like thelexicon and the concept hierarchy, required only minor adaptation to the MUC-3 task.Discourse processing used text segmentation based on definite and indefinite referenceand various cue phrases to identify and segment events.
Subsequent processing splitand merged events, extracted temporal relations, and filled in missing arguments togenerate templates.GTEThe GTE system used a phrase parser to identify phrases.
These phrases served asinput to a semantic ase frame analyzer that combined expectation-driven a alysiswith a bottom-up analysis of the information contained in the phrases.
There wasa special module in semantics for conjunction processing.
Reference resolution washandled by merging "similar" concepts at the frame level and by combining eventsinto a single template.Synch/UMDThe Synch/UMD system was developed by researchers across several sites using apipelined architecture to facilitate multisite development.
The first stage was a phraseparser, which segmented the text into phrases.
This was followed by a semantic phraseinterpreter, which mapped the phrases into a semantic net representation via a spread-ing activation process, to make the system robust in the face of missing knowledge.The syntactic omponent then performed some limited kinds of syntactic regulariza-tion, as needed.
The template generator worked from the semantic net representation,determining whether and how to generate templates given the semantic oncepts.U MassThe U Mass system used a semantic ase frame approach to drive the processing.There was some initial preprocessing of phrases, dates, and names, followed by lex-ical look-up, which associated words with concepts.
A text that did not trigger anyconcept nodes was classified as irrelevant.
Once a concept node was activated by as-sociation with a word, it triggered certain semantic predictions for slot fillers; thesemight also require some limited syntactic phrase parsing.
This stage generated a set ofinstantiated case frames, which went through a process of "consolidation" to mergeframes into template-level vents, using a set of domain-specific rules.
An optionalcase-based reasoning component associated a set of concept nodes with the slot fillsderived from those nodes.
These were grouped into classes by incident type.
For anew message, the associated concepts were compared with the case base, which wasgenerated automatically from the training corpus, to find matching configurations ofconcepts.
These were used to hypothesize possible templates and slot fillers.4.
Signif icance Tests on Measures of Effect ivenessRecall and precision measure the absolute ability of systems to perform the data ex-traction task.
Measuring the relative ability of different systems requires not only thatwe can measure ffectiveness but also that we can distinguish meaningful differencesin effectiveness from inconsequential ones.
Defining a meaningful difference may de-pend on the particular data extraction application.
For instance, suppose two systemsbehave similarly in producing slot fills corresponding to proper names in the origi-nal text, except hat system A always includes a few extraneous words along with aproper name, and system B does not.
System A would get a substantially ower score429Computational Linguistics Volume 19, Number 3according to MUC-3 criteria.
However, the two systems might be equally effectiveoperationally if the extracted names were used to index messages for text retrieval,and users always searched on, say, the last names of individuals.
On the other hand,system A would in fact be inferior if extracted proper names had to be matched iden-tically against other names in order t:o join two records.Differences in effectiveness are also inconsequential when there is a significantprobability that the difference resulted from chance.
Determining whether differencesare uninteresting in this way is the province of statistical hypothesis testing.
In thissection we review the main concepts in statistical significance testing and describe ourapproach to significance testing for the MUC-3 data.4.1 Review of Statistical Significance TestingA statistical significance test measures the extent o which data disagree with a par-ticular hypothesis.
1?The test begins with the posing of the null hypothesis, that is, thehypothesis that a relationship of interest is not present.
In order to pose a null hy-pothesis we must determine a relationship of interest hat can be tested and state thatit is not present.
One thing of interest about two systems in MUC-3 is whether onesystem is better than the other on the data extraction task.
To know this would requireknowing whether one system is better than the other in both recall and precision.Recall and precision are examples of test statistics.
A test statistic is a functionthat can be applied to a set of sample data to produce a single numerical value.
Aset of sample data consists of observations--instances of values of a set of randomvariables.
In MUC-3, examples of random variables are the number correct, partiallycorrect, possible, and actual.
The test statistic of recall is a function of the numbercorrect, partially correct, and possible.
The test statistic of precision is a function ofthe number correct, partially correct, and actual.A null hypothesis i stated in terms of a single statistic.
The relationship of interestis whether one system is better than another.
However, in order to state the nullhypothesis in terms of a single test statistic, we must consider recall and precisionseparately.
Also, for testing purposes we can simplify the relationship to just look atwhether one system differs from another since we know the direction of the differencefrom the scores themselves.The form of the null hypotheses we will test for MUC-3 is:System X and system Y do not differ in recall.Corresponding null hypotheses will be tested for precision as well.
Notice that thesingle test statistic for the null hypothesis is the difference in recall between the twosystems.
The null hypotheses are more formally stated as:The absolute value of the differe~ce between system X's overall recall (precision)score for the data extraction task and system Y' s overall recall (precision) scorefor the data extraction task is approximately equal to zero.To test a null hypothesis, the test statistic is applied to the observations of therandom variables and the value of the test statistic is used to determine a p-value, orsignificance level--the probability that a test statistic as extreme or more extreme thanthe actual value could have arisen by chance, given the null hypothesis.
If the value10 In this section we draw on descriptions ofhypothesis testing appearing in Noreen (1989) and Chatfield(1988).430Nancy Chinchor et al Evaluating Message Understanding Systemsof the test statistic is unlikely to have arisen by chance, then we have evidence againstthe null hypothesis.4.2 A Computer-Intensive Approach to Significance TestingIn significance t sting, we need to compute the probability that a test statistic value asextreme as the test statistic for the actual data could have arisen randomly, given thenull hypothesis.
If observations are drawn from a known probability distribution andthe test statistic is of the proper form, then this probability can be found analytically.However, the significance tests that arise from this conventional, analytic approachare inapplicable to the MUC-3 data, given our lack of knowledge of the distributionof the random variables.The availability of cheap computing power has resulted in an increased interest incomputer-intensive m thods in statistics (Efron and Tibshirani 1991).
These methodsavoid some of the more restrictive assumptions of conventional statistical methodsby explicitly simulating large numbers of random trials, thus eliminating the needto know the distribution of the random variables.
Our analysis of the MUC-3 resultsuses one of these methods, a significance-testing technique known as approximaterandomization (Noreen 1989).An exact randomization test simulates all logically possible sets of observations,applying the test statistic formula to produce a pseudostatistic for each observation.In other words, an exact randomization test would simulate all the logically possibleresults and calculate the significance l vel of the actual results by comparison with thesimulated results.
For a problem like ours, there are too many logically possible resultsfor it to be practical to run an exact randomization test.
Approximate randomization testsare similar to exact randomization tests, but use a nonexhaustive random sample ofthe possible sets of observations.
An approximate randomization test for the MUC-3results can be run in less than a day.
It is also possible to provide the confidence l vel ofthe test that measures how indicative the approximate randomization is of the exactrandomization.Both exact and approximate randomization require the production of a large num-ber, ns (number of shuffles, as defined in Section 4.3), of pseudostatistics (random valuesof the test statistic).
These random values form an empirically generated distributioncurve that can be used to calculate the significance l vel of the actual test statistic.
Eachrandom value is compared with the actual value of the test statistic on the originaldata.
The number of times, nge, that the pseudostatistic is greater than or equal tothe true statistic is recorded.
The significance l vel (p-value) of the test is the ratio of(nge + 1)/(ns + 1).
In general, the lower the p-value, the less probable it is that the nullhypothesis holds; that is, in our case, the lower the p-value, the more likely it is thatthe two systems are significantly different.
The confidence level for the significancelevel can be calculated or looked up in published tables (Noreen 1989) if approxi-mate randomization is used.
The higher the confidence level, the more probable itis that the approximate randomization test gave the significance l vel that an exactrandomization test would have given.4.3 Application of Approximate Randomization to the MUC-3 ResultsWe use the approximate randomization technique set forth by Noreen (1989) withstratified shuffling to control for categorical variables that are not of primary interestin the hypothesis test.
For more details on this method see Chinchor (1992).The test statistic we have chosen is the absolute value of the difference in recall orprecision.
The data consist of the four-tuples of number possible, actual, correct, andpartially correct for each message for each system.
The actual test statistic is calculated431Computational Linguistics Volume 19, Number 3for each pair of systems.
The desired number of shuffles is set to 9,999 because it wasdetermined that 9,999 shuffles produced slightly higher confidence l vels than 999 andwere worth the 16-fold increase in computing time.In the algorithm, once the desired number of shuffles is set, the counters for thenumber of shuffles, ns, and the number of times the pseudostatistic is greater thanor equal to the actual statistic, nge, are set to 0.
A loop then increments ns until ithas exceeded the desired number of shuffles.
The first step in this loop is to shufflethe data.
Data is shuffled by exchange of the systems' message scores depending onthe outcome of a computer-simulated coin flip.
After 100 coin flips, one per messagein the MUC-3 test, the absolute value of the difference in the test statistics of theresulting pseudosystems can be compared with the corresponding absolute value ofthe difference in the test statistics of tlhe actual systems.
The value of nge is incrementedevery time a randomized pair of pseudosystems satisfies the inequality:\]statpseudoA--l~tatpseudoBI ~ IstatA--statB\[where stat is the test statistic.
The significance l vel is estimated by (nge + 1)/(ns + 1),where the ones are added to ensure that the test is valid.
The corresponding confidencelevel is then found by table lookup.According to Noreen,Randomization is used to test the generic null hypothesis that onevariable (or group of variables) is unrelated to another variable (orgroup of variables).
Significance is assessed by shuffling one variable(or set of variables) relative to another variable (or set of variables).Shuffling ensures that there :is in fact no relationship between the vari-ables.
If the variables are related, then the value of the test statistic forthe original unshuffied ata should be unusual relative to the valuesof the test statistic that are obtained after shuffling.
(Noreen 1989, p. 9)In our case, the four-tuple of data associated with each message for each system isthe dependent set of variables that is shuffled.
The explanatory variable is the system.Shuffling ensures that there is no relationship between the differences in the scores andthe systems that produced them, that is, that the differences were achieved by chance.If they were not achieved by chance, then the value of the actual test statistic will befar enough out on the tail of the elnpirically generated istribution to be significant(as indicated by the significance l vel).Informally speaking, shuffling two systems with a large recall score difference islikely to produce more homogeneous p eudosystems whose difference in recall willbe less than or equal to the obserw~d ifference most of the time.
This will lead to asmall nge value and a low p-value.
Conversely, shuffling two systems whose behavioris quite similar may lead to a larger number of times that the recall score differencebetween the pseudosystems is greater than or equal to that of the real systems; thiscreates a larger p-value and a lower likelihood that the differences are significant.4.4 Choice of ObservationsIn MUC-3 there are several ways of breaking down the output of systems into ob-servations on random variables.
The number of correct and partially correct answersgenerated by a system are recorded at the slot level and then aggregated atthe templateand message levels before recall is computed.
Recall could be viewed as a summarymeasure computed from the 2,771 or more (system-dependent) observations of system432Nancy Chinchor et al Evaluating Message Understanding Systemsbehavior on slots in key templates, from the 163 or more (system-dependent) observa-tions of system behavior on key templates, or from exactly 100 (system-independent)observations of system behavior on messages.
The system-dependent counts are a re-sult of optional slots and templates in the key that are counted only when a systemhas chosen to fill those slots or templates.
The number of messages i independent ofthe system's fills.Whether data extraction systems ucceed or fail, and thus should be observed atthe message l vel, template level, slot level, or at some completely orthogonal level ofgranularity is an open question.
From the standpoint of significance testing, however,the most reasonable approach available to us was to consider ecall and precisionas test statistics computed on observations at the message level, since this assumesthe smallest and only exact number of observations.
The two corresponding messagefour-tuples of possible, actual, correct, and partial scores for the two systems wererandomly exchanged.
The pseudostatistic was arrived at by summing the possible,actual, correct, and partial for the two pseudosystems constructed by the randomexchange and computing based on those sums.Two hypothetical examples illustrate the method applied at the message level.Suppose we have two systems reporting precision results for a set of 100 messages.System A has a score of 15/20 on each of 50 relevant messages and no spurioustemplates, for a total of 750/1000 = 75% precision.
System B has the identical score oneach of the relevant messages except one, for which it has a score of 0/20.
System B hasa precision of 735/1000 = 73.5%.
In the random shuffle, either system A or system Bwill have the "0 precision" template.
The pseudostatistic will always be equal to themeasured absolute difference between system A and system B, that is, 1.5%.
Since adifference in precision of at least 1.5% has a probability of 1.0 of arising when thesystems are randomly shuffled, an observed ifference of 1.5% has a p-value of 1.0and, therefore, is not statistically significant.Now suppose that we have a third system, system C, which always gets 18/20 onthe same set of 50 relevant messages with no spurious templates.
Any random shuffleof systems A and C is likely to produce a smaller difference than the absolute value ofthe difference between A and C. The p-value will be extremely close to zero indicatingwith virtual certainty that the two systems are significantly different.These examples correspond with our intuition that two systems that show consis-tent differences across a range of messages will more likely be statistically significantlydifferent in their scores than two systems that differ on just a few messages.5.
Significance ResultsThis section presents ignificance results on the differences between recall and preci-sion for each of the 105 pairs of MUC-3 systems.
We draw some tentative conclusionsabout he behavior of MUC-3 systems with respect to the recall and precision measuresand provide some cautions with respect to interpreting our results.5.1 Results of the Approximate Randomization TestThe results of the approximate randomization test for 9,999 shuffles are presentedin Figure 10.
Recall and precision scores, as computed for each system using theMATCHED/MISSING method (Section 2.3), are listed along the top and left side (recallfirst, then precision).
For purposes of the statistical testing, more exact values of recalland precision than those listed were calculated from the raw data.
The top of each cellin the table presents the significance l vel (p-value) for the difference in recall between433Computational Linguistics Volume 19, Number 3~o i~i~ o ~ ~ iiiiiiiiiiiiiiiiiiiiiiii ~o 0 ~o?p6 ?
?6  6 ?
p6  o p , ?
,?
?
o 6 ?
6 ??
?
i!iiii - -  ~.
?
?
?
?
?o l  ?
o oO d oo  Oo ooI?
?
~i~:i:~i:.!:!!!:!
:~:i i ?
1' ::::i~ 3 '3~ ~ G ; ........... o o ~ ~-  " 3 '  3 '33 '  :::::x :~:~: o o .
.
.
.
.
.
.
.
.
.
.
?
o o ooo  ?~ o o  o o o O li~iii!iiii!~t o o o o......................... ?
,o 6 ?
,o o 6 p ?6  ,o ~ ?
p 6 6 ?434Nancy Chinchor et al Evaluating Message Understanding Systemsthe corresponding systems, while the bottom of the cell presents the correspondingfigure for the difference in precision.There are two cells for each pair of systems, ince a system appears in both a rowand column.
We present the p-value only at the intersection of the row for the higherscoring system and the column for the lower scoring system.
A row containing manyp-values indicates that the system for that row produced relatively high scores.
Theactual values show the significance of the differences in effectiveness.
Systems withskewed effectiveness (high recall and low precision, or vice versa) show up as rowswith many values, but with most of those values in either the top or the bottom of cells.It is common in many scientific fields to reject he null hypothesis (i.e., reject hehypothesis that there are no significant differences) in exactly those cases where thep-value is less than a prespecified threshold.
Some standard rejection levels are 0.01,0.05, and 0.10.
Although we prefer the most conservative 0.01 cutoff level for this data,we present he raw p-values to allow readers to draw their own conclusions.The p-value computed by approximate randomization is an approximation of thevalue that would be computed by the computationally intractable exact randomizationprocedure.
Noreen (1989) gives tables for computing the confidence l vel for a givenrejection level, that is, what the probability is that the hypothesis would be rejectedby exact randomization given that approximate randomization produces a particularestimate of the p-value.
Rather than presenting a table of confidence l vels, we put anasterisk in front of p-values in Figure 10 that have a confidence l vel of 0.99 or betterfor a rejection level of 0.01.
The values with asterisks are those for which we can rejectthe null hypothesis at the most conservative cutoff with high confidence.
Overall theconfidence l vels for the approximate randomization test indicate that these pairwisetests are adequate for our purposes compared with the exact randomization test runfor the same data.5.2 AnalysisThe results of the approximate randomization test are intuitive in that the systemswhose scores are numerically very different also have a significance l vel less than0.01.
Likewise, systems whose scores are numerically very close have a significancelevel higher than 0.10.
However, it should be noted that the same difference in scoresmay not equate to the same significance l vel when different systems are involved.The test measures the difference from what would have occurred randomly, and eachpair of systems provides a different set of conditions for the shuffle.
Thus, we cannotconclude from the approximate randomization tests that the evaluation scores aregenerally significant within a certain range of percentage points.The approximate randomization tests with 9,999 shuffles how that 59% (62 outof 105) of the pairs differ with significance l vels less than 0.10 for both recall andprecision.
Similarly, 39% (41 out of 105) differ in exactly one score.
Of those, 27 differin recall (26% of the total) and 14 differ in precision (13% of the total).
Only 2% (2 outof 105) of the pairs do not differ in either score.
At the most conservative significancelevel of 0.01 with confidence of 1.000 and 9,999 shuffles, 60% (126 out of 210) differ inat least one of their scores, and only 31% (33 out of 105) differ in both of their scores.Under these conservative conditions, 11% of the pairs do not differ in either score(12 out of 105).
Although it was impossible to directly test a hypothesis about recalland precision simultaneously, 24% (25 out of 105 pairs of systems) were such that onesystem had higher values of both recall and precision, and the null hypotheses forboth measures could be rejected at the 0.01 level with 0.99 confidence.The results of the approximate randomization tests can be represented accordingto the groupings of systems that do not differ significantly at any of the three cutoff435Computational Linguistics Volume 19, Number 3RECALL706050403020100 l I I I l I I l I I I I I I g0 I 2 3 4 5 6 7 8 9 101112131415SYN GTEADSUNLLS~ UNI rrP SRI MDCPRC HU GENYU BBN UMaSYSTEMFigure 11The systems howing no significant difference in their recall scores at the 0.10 level aregrouped together.levels with respect o either recall or precision.
In Figures 11 and 12, the systems areordered with respect o their recall and precision scores, respectively, and groupedaccording to significant differences.
Systems are contained within the same group ifthey all are not significantly different from each other at the 0.10 level and all havesignificantly different effectiveness from all systems outside the group at the 0.10 level.If a system is in a group by itself, then it is significantly different from all other systemsfor that score.
If groups overlap, the systems in the intersection are not significantlydifferent from the systems in either group.In summary, then, the use of the approximate randomization test method nowallows us to report he significant differences in scores between systems participatingin MUC-3.
The approximate randomization test does not tell us anything about therepresentativeness of the sample of messages used in the MUC-3 evaluation.
How-ever, for the sample of test messages used, the approximate randomization methodindicates that the results of MUC-3 are statistically different enough to distinguish theperformance of most of the participating systems.5.3 Independence and Representativeness of ObservationsAny significance test assumes that observations are independent; what appear to bemultiple observations ofa set of variables are not in fact copies of the same observation.In one sense, the 100 MUC-3 test :messages were independent observations, ince, inan operational setting, each would be a separate document coming over the newswireand confronting a data extraction system.On the other hand, there were strong dependencies among the 100 messages.
Somemessages described the same real world events, occasionally using similar language.There were 65 test messages that had one or more templates in the answer key, for a436Nancy Chinchor et al Evaluating Message Understanding SystemsPRECISION706050403020100ss ?
53 ?0 1 3 4 6 7 8 9 101112131415UNLHU GTE LSI MDCBBN PRC UNI SYN ADS NYU ITP UMa GE SRISYSTEMFigure 12The systems howing no significant difference in their precision scores at the 0.10 level aregrouped together.total of 128 templates describing events.
(Another 35 empty templates correspondedto messages with no reportable vents.)
Among these 128 filled templates, we foundat least 30 that were based on the same real world event as some other template, andanother 15 or so unclear cases.Whether these duplications had an impact on our significance results dependson where data extraction systems ucceeded or failed.
If success or failure dependedon the particulars of the language used to describe events, then, except for exactlyrepeated text passages, duplication did not have a significant impact on results.
Ifsuccess or failure is based on the nature of the event, then the presence of multiplemessages about the same event resulted in the effective number of observations beingreduced, with the result that insignificant differences in systems may have appearedstatistically significant.In addition to the duplication among test stories, there was duplication of eventsbetween training and test stories.
This meant that system builders were able to con-sciously or unconsciously take advantage of prior knowledge of some of the eventsthat would be present in the test set.
For instance, on the MUC-3 corpus the propernoun Jesuits was statistically a very good predictor of the set fill MURDER, due tonumerous descriptions of one particular attack (Lewis 1991).
This effect means thatthe recall and precision scores we observed may be higher than those that would beobserved in an operational setting.Duplication between training and test stories is one instance of the broader ques-tion of representativeness of observations.
One system might be significantly betterthan another on the MUC-3 test set, but the second might be better on some other textstream, or even on a different chronological period from the same text stream.
(Thelatter issue is discussed in Section 7.5 with reference to the MUC-4 results.)
Experimen-437Computational Linguistics Volume 19, Number 3tation with more and larger data sets will be needed before we can have confidencein the robustness of results uch as those reported here.6.
Linguistic Phenomena Test ExperimentThe scores discussed above provide a black-box measure of system effectiveness becauseonly input/output pairs were examined (Palmer and Finin 1990).
The subsystem ech-anisms used to create those outputs, were not considered as they would have been ina glass-box test because these subsystems differed widely from system to system.
Inan attempt to gain some insight into the strengths and weaknesses of subsystems, weexamined the effect of particular linguistic onstructions on the ability of systems toextract data correctly.
The general method of testing linguistic phenomena was to findall instances of the chosen phenomenon i the test messages, to determine the slotsthat could only be filled correctly if a system were able to handle the phenomenon,and to configure the scoring program to score just those slot instances.Three such linguistic phenomena tests were attempted during a MUC-3 dry runin February 1991.
The phenomena examined were negation, conjunction, and activeversus passive verb forms.
However, most systems were poor enough at extractingdata from any linguistic onstruction during this test that differences caused by partic-ular linguistic onstructions were unnoticeable.
Those differences that were observedmirrored the differences in overall ..~cores.The linguistic phenomena tests carried out as an experiment during the final MUC-3 run in May 1991 were more successful and showed that differences in handlingof linguistic structures could be discerned through black-box testing.
This was notonly because the preliminary tests, taught us to use a relatively frequent linguisticphenomenon but also because many sites had improved system performance in theinterim, allowing meaningful scores on subsets of the data to be measured.6.1 HypothesesThe phenomenon f apposition was chosen for the experiment because of its frequency,its importance for slot fills, and the variety of appositive structures in the texts.
Atypical example of apposition from the test messages was "MSGR GREGORIO ROSACHAVEZ, AUXILIARY BISHOP OF SAN SALVADOR,...." To determine whether thephenomenon of apposition was being isolated, the phenomenon was tested to see ifperformance on affected slots was different han the overall performance.The instances of the phenomenon i the texts were also divided into subsets tosee whether the performance on the subsets would behave in a predictable way.
Textswere first divided according to the complexity of the appositive construction.
Forexample, "VENEZUELAN PRESIDENT CARLOS ANDRES PEREZ" was a simple case,whereas "UNIVERSITY STUDENT'S HUGO MARTINEZ AND RAUL RAMIREZ" wascomplex because of the conjunction i  the head noun phrase.
Any complexity withinthe apposition, even as slight as a missing comma, put the example in the complexcategory.
Higher scores were expected for the simpler appositive constructions.The texts were also divided according to whether the appositive was postposed orpreposed.
In the examples given above, "AUXILIARY BISHOP OF SAN SALVADOR"is postposed, while "VENEZUELAN PRESIDENT" and "UNIVERSITY STUDENTS"are both preposed.
It was expected that although the systems would score differentlyon these types of apposition, there would be no clear trend seen.
Postposed appos-itives are more commonly thought of as appositives and are marked by some formof punctuation such as commas or dashes.
However, preposed appositives could beprocessed as adjectival phrases.438Nancy Chinchor et al Evaluating Message Understanding SystemsA final approach to measuring the effect of apposition involved altering the orig-inal text.
Systems were scored on modified messages where the relevant informationwas conveyed by a simple sentence rather than by an appositive construction.
Anexample would be to substitute the sentence "CARLOS ANDRES PEREZ IS THEVENEZUELAN PRESIDENT" in the message and to remove the appositive, "VENE-ZUELAN PRESIDENT" from the original sentence.
It was expected that the systemswould score higher on the messages with the appositions removed.In summary, the following hypotheses were tested:1.
The systems hould score differently on the appositive constructionsthan they did on the overall testing.2.
The systems hould score higher on the simpler appositive constructions.3.
The systems hould score differently on postposed and preposedappositives.4.
The systems hould score higher on their responses to messages wheresimple sentences were substituted for appositive constructions.6.2 Experimental MethodAll sentences from the test messages containing appositions were extracted and ana-lyzed according to the appositions' effect on slot fills, their position (appositive pre-posed or postposed), and their complexity (simple or complex).
Configuration fileswere composed that specified which slots in which templates were to be scored forappositioned noun phrases, simple appositive constructions, complex appositive con-structions, preposed appositives, and postposed appositives.
A set of test messageswith simple sentences substituted for the appositive constructions was generated fromthe official test messages.
The configuration files and modified messages were sent tothe sites as part of the MUC-3 test package.
The sites were required to automaticallyrescore their templates using their official history file with each of the configurationfiles.
It took two weeks to develop the test package for the linguistic phenomena ex-periment and it took the sites on the order of two hours to automatically rescore theirsystems.
Sites voluntarily chose to participate in the test that compared their perfor-mance on the 'minimal pairs' of appositive constructions and simple sentences.
Thistest required them to run their systems on the modified test messages and to rescoreusing the apposition configuration file for the new templates.
This part of the exper-iment was voluntary because the process was more time consuming than those teststhat only required automatic rescoring.6.3 Experimental Results nThe recall and precision scores on the apposition test are shown in the scatter plotin Figure 13.
These results are quite different from the overall results for MUC-3 inFigure 2.
These results indicate that the appositive constructions could be isolatedby the test procedure at current levels of performance, although the systems' overallperformance on the subset of involved slots may have played a role in these results.Figure 14 shows the combined results of recall multiplied by precision for the sim-ple and complex appositive constructions.
The systems cored higher on the simpler11 The detailed results of the linguistic phenomena test experiment appear in the MUC-3 proceedings andcontain the raw scores and plots of recall and precision (see Chinchor 1991b).439Computational Linguistics Volume 19, Number 3100PRECIS ION9080706OmFFP50PRC40 ?
?
uNl30 ?
G1ENUN201() ?
LSIADSSYN0o 1'o??
?
NYUNEEMDC=m HU.
.
.
.
.
7~ ~ ' 20 30 40 50 60 8 90 100RECALLFigure 13The scatterplot f the recall versus preci.sion scores for appositioned noun phrases hows thatthe systems cored differently on the appositive constructions than on the overall test.RXP600050004000300020001000 "0"~2 3 4 5 6 7 8 9 10 11 12S ITE1!14 15i= SIMPLERXP II COMPLEX RXPFigure 14The plot of the product of recall and precision scores for the simple and the complexappositive constructions shows that the systems cored higher for the simpler appositiveconstructions a predicted.appositive constructions than they did on the more complex appositive constructions.The results indicate strongly that the tests were isolating the linguistic phenomenon.440Nancy Chinchor et al Evaluating Message Understanding Systems40003000R X P 2000100002 3 4 5 6 7 8 9 10 11 12 13 14 15?
POST R X P \[n - - -  PRE RXPSITEFigure 15The plot of the product of recall and precision scores for the postposed and preposedappositives show that there is no clear trend in the scores.Recall Recall Precision PrecisionSite No Appositives Appositives No Appo~fives AppositivesX 28 32 53 62Y 38 43 68 77Figure 16The table of recall and precision scores for the 'minimal pair' type test shows that thephenomenon of apposition was being isolated by the scoring of slots because both sets ofscores went down considerably as a result of the substitution of simple sentences for theappositive constructions.The systems cored differently for the postposed and the preposed appositives, butdid not score consistently higher on either.
The results in Figure 15 were predicted be-cause although the postposed appositives were more typical and usually indicated bycommas or dashes, the systems could interpret the preposed appositives as adjectival.The results for the 'minimal pair' type test were unexpectedly higher for the mes-sages containing appositive constructions than they were for the messages containingsimple substituted sentences.
Two systems volunteered to run the test and, for both,the results in Figure 16 showed that the exact opposite of the hypothesis was true.The explanation was that both systems filtered out sentences containing the copulabecause this sentence type did not contain information relevant o the task most ofthe time.
However, the drop of about 5% in the recall scores and 9% in the precisionscores indicates that we were isolating the phenomenon of apposition even though441Computational Linguistics Volume 19, Number 3the change is in the opposite direction of what was originally expected.
The scoresdid not drop more when the appositive constructions were transformed because partof the information still remained in the original sentence, such as the full name of theperpetrator r target.6.4 Conclusions Concerning the Linguistic Phenomena Test ExperimentThe linguistic phenomena test experiment gives several strong indications that linguis-tic phenomena can be isolated by scoring the affected slots in the system responses.The development of a range of linguistic phenomena tests spanning the levels of lin-guistic structure from morphology to discourse would focus attention on the generalityof the underlying linguistic mechanisms.7.
Lessons LearnedWhat lessons have we learned from studying the systems and the MUC-3 application?There are several questions to consider:?
What have we learned about the state of the art in messageunderstanding??
Can we identify specific system techniques that provided a significantperformance gain??
What have we learned about testing isolated linguistic phenomena??
What have we learned about evaluation itself?7.1 State of the Art in Message UnderstandingMUC-3 focused on assessing the progress of the research in message understandingin the context of a realistic application domain.
The fact that there were a number ofsystems that were able to handle a task of this size and complexity is encouraging.Overall, the cluster of top-performing systems reported 40-50% recall, 55-65% preci-sion, and 10-20% overgeneration.
12 This performance is certainly a milestone markingsignificant progress in message understanding technology.This level of progress raises the question of how soon such systems may be readyfor use in real applications.
This question cannot be answered without concrete appli-cation requirements--for example, what levels of recall, precision, and overgenerationare required for a specific application?
How redundant are the messages?
Does thesystem have to match human performance?
Will humans post-edit he system out-put before use?
We cannot answer these questions; however, looking at the MUC-3results, we can make a few observations.
First, the current systems do not allow anysignificant trade off between precision and recall.
Several sites (BBN, NYU, U Mass)reported alternative systems that produced small changes in the precision-recall trade-off.
However, most systems, particularly the linguistically based systems, did not haveadjustable parameters that could be tightened or relaxed to produce different rade-offs.
Also, recall appeared to be more of a limiting factor than precision for the MUC-3systems.
No system achieved more than 51% recall, and every system had higher pre-cision than recall.
Nonetheless, it is possible to imagine applications where this levelof recall would prove sufficient.12 The figures quoted here all refer to the "official" measure ofMATCHED/MISSING scores; eeSection 2.3 for an explanation f the various coring measures.442Nancy Chinchor et al Evaluating Message Understanding SystemsPrecision, recall, and overgeneration are not the only useful measures of systemperformance.
For real applications, a critical issue is the length of time it takes to makethe application run in a new task domain.
The median preparation time reported forthe systems participating in MUC-3 was 10-11 person months.
Of course, these figuresincluded basic system development and tool development, which can be looked at asa one-time cost, shortening development times for future applications.
Nonetheless,reduction of cost to port to a new application is an area that needs considerably morework.Although a number of systems reported effort in tool-building, few system de-velopers pent time on automated knowledge acquisition or learning techniques.
Thisis of some concern, because many systems reported that missing domain knowledgewas a significant source of error.
However, there were some exceptions.
The Hughessystem (3 person months of effort) drew heavily on automated learning and patternclassification techniques; however, its performance was substantially below that of thebest systems (especially in precision).
The BBN system also used some automated andsemi-automated learning in several places.
It is hard to quantify how important hiswas, but their precision and recall scores were competitive and their level of effort(6 person months) was lower than the other comparably performing systems.
Finally,some systems did address the knowledge acquisition bottleneck for lexical knowl-edge, but not by learning techniques.
NYU, for example, used a machine-readabledictionary; both GE and ITP reported using sizable domain-independent lexicons thatrequired only minimal adaptation for new applications.The issue of processing speed, a problem for some systems during MUCK-II, ap-peared to be resolved for MUC-3.
The time to process 100 messages ranged from over300 minutes to only around 30 minutes.
The median processing time for all the sys-tems was about one minute per message.
Some of this speed-up was obtained by usingfaster hardware.
Some was obtained by faster algorithms, and some was obtained bythe use of preprocessing techniques, including relevance filtering.
In general, it is clearthat processing speed is not a major obstacle for current message understanding sys-tems, especially as the cost of machine cycles continues to drop.In conclusion, the progress of the field is encouraging.
Systems were able to handlea large volume of text in a realistic domain and achieve reasonable precision withfairly low overgeneration.
For certain applications, it may be necessary to achievehigher recall, even at the expense of precision.
However, the major obstacle for messageunderstanding systems is the cost of porting the system to new applications.
This, morethan limited recall, may hold back the deployment of message processing technologyin "real" applications.7.2 What We Learned about Specific TechniquesTo build better systems, we would like to know which techniques worked well andwhich ones did not.
A black-box evaluation makes it difficult to tease apart the con-tributing factors that made a system perform well or poorly.
However, several thingsemerge quite clearly.
First, there is a healthy diversity of approaches among the top-performing systems.
These included a semantics-driven case frame system, severalsyntax-driven systems, and a system that used a hybrid control approach drawingheavily on semantic information.For the syntax-driven systems, only those systems that provided a robust pars-ing capability performed well.
Among the top-performing systems, there were twosuccessful approaches to robust parsing: use of partial parsing and use of heuris-tics added to a standard full-sentence parser that allowed recovery of parsed sub-strings.443Computational Linguistics Volume 19, Number 3As always, acquisition of domain knowledge (lexical semantics and an adequatedomain model) proved a major task.
For many systems, the incompleteness of thisdomain knowledge was a significant source of error; extensions to the domain knowl-edge were likely to produce significant performance improvement.
There was a rangeof approaches to the lexicon development and lexical semantics.
At one end was thehighly domain-specific knowledge ngineering approach of the U Mass system.
In themiddle were systems like NYU, which used a machine-readable dictionary for syntac-tic information augmented by hand-coded semantics, or the BBN system, which didautomatic part-of-speech tagging but required hand-coding of semantic rules (withsome computer aids).
An approach that seemed to work well was the use of a hand-crafted but domain-independent, semantically rich lexicon (GE, ITP).
The other endof the lexicon/semantic knowledge spectrum was represented by the Hughes system,which "learned" its lexicon and semantics from the training corpus.
This is appealingbecause it makes the approach ighly portable.
However, the approach was not amongthe higher-scoring systems.The proper treatment of discourse turned out to be important for the MUC-3 task,and many system developers identified this as an area needing substantially morework.
Most systems had trouble distinguishing elaboration of previously mentionedevents from the introduction of new events.
Many systems lacked a systematic ap-proach to the problem and relied on some simple heuristics ("collapse two events ifthey are of the same type, with the same target, and occur on the same day"), some-times coupled with the use of keywords ("meanwhile," in sum") to help detect achange of topic.Preprocessing was a key feature in many of the MUC-3 systems.
Relevance filter-ing, either statistically trained or knowledge-based, was successfully used by severalsystems to reduce processing load.
The use of text categorization techniques from in-formation retrieval was more successful in this role than when used as the sole methodfor filling templates.
Systems also used other kinds of preprocessing, including specialhandling of Spanish names (BBN, SRI), regularization of special forms such as dates,organization ames, and place names (many sites), reduction of syntactic omplexityby bracketing out certain structures (GE), and flagging of discourse segment markers(GE, ITP).
It is clear that, in an application involving large volumes of text, such pre-processing techniques provide an effective way to trade off a requirement for speedwith a requirement for accuracy.7.3 Testing Linguistic PhenomenaThe linguistic phenomena testing was an attempt o gain insight into the inner work-ings of the systems by performing tests that had some of the characteristics of glass-boxtests but were still fundamentally black-box in nature.
The phenomena tests were de-signed to be system-independent a d to give an indication of how well systems weresolving various aspects of the text processing problem.
Although the phenomena testexperiment reported here indicates that it was possible to isolate linguistic phenomenausing the scoring mechanisms of MUC-3, there were a number of confounding factorsthat degraded the capability to determine xact performance on linguistic phenomena.The use of alternate and optional slot fillers in the answer key affected the clarity of theresults.
Some of the scoring guidelines for giving partial credit also affected the clarityof the results.
However, the largest noticeable ffects were due to system-dependentoperations.
Some systems did not attempt o fill certain slots at all, due to limitationson development time.
Their scores on the phenomena tests really do not indicate theircapability with respect o those phenomena tested.
The more robust he systems werein terms of the task, the more indicative the phenomena tests were of their capabilities444Nancy Chinchor et al Evaluating Message Understanding Systemsin certain linguistic areas of text processing.
However, these tests must be designedand analyzed with awareness of the strategies used by the task designers and the sys-tem designers before solid conclusions can be drawn as to performance on particularlinguistic phenomena.7.4 What We Learned about EvaluationPerhaps the thing that we learned the most about was evaluation itself: the process,the costs, the payoffs, and the pitfalls.
On the whole, the participants--the systemdevelopers, the people responsible for the evaluation process, and the observers--allfelt strongly that this evaluation was successful and that it was worth the (very large)amount of time and effort.
The format of the conference required that participantsdescribe their system in detail and provide an analysis of what did and did not work.This enabled all participants to learn from each other's experiences, which greatlyincreased the value of the exercise.
The fact that the participants were involved inthe planning of the conference and defining the methods used for evaluation alsocontributed to its success.
Overall, our conclusions are that evaluation is costly; itrequires the investment of substantial resources, both to create the evaluation infras-tructure and to port the systems to the chosen application.
This kind of evaluation canonly be successful if system developers feel that they benefit from their participationby gaining new insights into their own systems in relation to alternatives representedby other systems.Another lesson we learned is that black-box evaluation is good for getting a snap-shot of the field, but it is not necessarily a good predictor of future system perfor-mance.
Several MUC-3 developers showed interesting statistics indicating that theirperformance was increasing steadily for each week of continued evelopment, withno significant fall-off as MUC-3 approached.
This is strong evidence that these systemswill show improved performance if given more time for development.
Other systemssuffered from slow start-up, limited resources, or missing components hat preventedthem from achieving peak performance.Successive black-box evaluations may be required to show whether a system has"topped out" or whether outine bug fixes, new modules, and additional knowledgeproduce further performance improvements.
This observation has led to a new pro-posal to measure system "convergence" (Hirschman 1991a).
The proposal is to performan initial evaluation using two test sets, $1 and $2.
Following this, each site wouldbe allowed to use test set $1 for development for a short period of time (perhaps afew days), but would not be allowed to look at test set $2.
At the end of this period,the system would again be scored on both test sets.
The improvement o  test set $2relative to the improvement o  test set $1 would provide some measure of how wellthe system was convergingmwhether changes made to fix problems in one test setactually helped in another test set.Another lesson was that black-box evaluation is not effective for determiningwhich techniques are responsible for good performance across systems.
Performancetrade-offs are very system-specific, and insights depend on a careful analysis of howthe particular system failed.
SRI provided such an analysis of errors for a subset ofthe test messages, and this gave some interesting insights into that particular system.However, it is difficult o draw any cross-system comparisons.
If we wish to have moreconsistent insights into the strengths and weaknesses ofcomponents within individualsystems, we will have to incorporate glass-box measures or rely on more sophisticatedtests such as the linguistic phenomena tests described in Section 6.
In addition, weneed effectiveness measures that go beyond recall and precision to take into accountthe structured nature of the information being extracted.
Such measures might require445Computational Linguistics Volume 19, Number 3the use of a structured atabase to evaluate the effectiveness of the retrieval of certainkinds of information.7.5 MUC-4Although this paper is about MUC-3, MUC-4 has already taken place, so it seemsappropriate to discuss it briefly here.
For more on the results and system details,the reader is referred to the conference proceedings, Proceedings ofthe Fourth MessageUnderstanding Conference (MUC-4).
MUC-4 evaluated the performance of 17 systemson the same domain as MUC-3.
Six of the participants were new entrants and elevenwere veterans of MUC-3.
There were some good first-time systems among the newentrants, and some of the veteran systems were dramatically revised.The major changes to the evaluation consisted of altering the template to betterreflect ext processing capabilities and altering the scoring to be more consistent andmore demanding.
The scoring changes included:?
Greater automation of the scoring;?
Enforcement of mapping templates to the answer key based on contentof the templates instead of allowing mapping purely by optimization ofthe scores;?
A focus on the stricter ALL TEMPLATES row as the official score insteadof the MATCHED/MISSING row;?
The calculation of a single score known as the F-measure (van Rijsbergen1979) combining recall and precision with variable weights;?
The calculation of a region of performance on the precision-recall graph foreach system using the MATCHED/MISSING, MATCHED ONLY,MATCHED/SPURIOUS, and ALL TEMPLATES scores as the fourcorners; and?
The measurement of text-filtering capabilities, to give an indication ofhow well systems judged the relevance of messages.In addition, results of three adjunct ests were reported at MUC-4.
The first was ananalysis of the text-filtering capabilities of the MUC-3 and MUC-4 systems (Lewis andTong 1992).
The second test was an analysis of the effect of discourse complexity onthe performance of the MUC-4 systems (Hirschman 1992).
This test looked at mes-sage subsets based on how many relevant sentences occurred in the message and howmany templates were generated.
The third adjunct est showed that the "flat" tem-plate design of MUC-4 with cross references closely approximates an object-orientedtemplate design, but that the object-oriented design allows the collection of additionalperformance data for diagnosis (Krupka and Rau 1992).There were two blind test sets in MUC-4 labeled TST3 and TST4.
TST3 was a setof 100 messages taken from the same chronological segment of the FBIS corpus as thetest and the development sets of MUC-3.
TST4, on the other hand, consisted of 100messages that had been produced up to two years earlier than TST3 and the MUC-3data sets.
Testing of systems on TST4 was intended to detect whether systems hadbeen tuned too heavily to texts produced uring a particular period of time.
Systemscores for TST4 were largely in agreement with those for TST3, suggesting that systemswere not in fact overtuned to the events that occurred uring a particular period.The progress of the veteran participants between MUC-3 and MUC-4 was mea-sured by forward-converting their MUC-3 TST2 templates to the MUC-4 format and446Nancy Chinchor et al Evaluating Message Understanding Systemsscoring TST2 and TST3 using the MUC-4 scoring methods.
The results for TST2 andTST3 could then be meaningfully compared for the systems.
All but one system scoredhigher on TST3.
A typical improvement in the ALL TEMPLATES F-measure for sys-tems was ten points with two systems increasing at least twice that much.The TST3 and TST4 tests and the TST2/TST3 progress test show that the MUC-4 results are an improvement over MUC-3.
However, we are cautious about statingexactly how much improvement there has been in the state of the art of data extractionsince MUC-3.
The degree of improvement is hard to quantify, since we lack good dataon the kinds of variability found in FBIS and other message streams and how thesefactors affect data extraction systems.
MUC-5, which will feature new domains andsources of text, will increase our understanding of these issues.
However, the data setsdeveloped for MUC-3 and MUC-4 will continue to be valuable resources for futureexperimentation.8.
ConclusionIn this paper, we have sketched the evaluation techniques that were applied to 15 textprocessing systems during MUC-3.
In addition to the raw results, we have introduceda method of computing significance for the results across systems using approximaterandomization.
The results showed that the systems fell into a number of distinctclusters for both precision and recall.
In many cases, the systems performing well inprecision also performed well in recall.
We can conclude that the evaluation method-ology used in MUC-3 can indeed discriminate among systems and that there weresignificant differences in effectiveness among the systems fielded for MUC-3.We have also been able to draw several other conclusions.
The first is that allsystems performed worse in recall than in precision; furthermore, none of the linguis-tically based systems had adjustable parameters to increase recall at the expense ofprecision (although several systems could be run in several configurations to produceslight changes in overall precision and recall).
Achievement of high recall scores (over60%) is a problem for the current ext-understanding systems.
However, several sys-tems have shown steady improvement with time, and their performance may showfurther improvement with continued evelopment.
Even some of the high-performingsystems may not yet have reached peak performance.This raises the issue of portability--most ystems pent approximately one personyear preparing their systems to run for MUC-3.
If porting takes 12 months of time forhighly trained system developers, portability will be a serious tumbling block both tobuilding real systems and to changing the evaluation paradigm.
At the end of MUC-3,the participating system developers did not want to spend another year porting theirsystem to yet another evaluation application.
This underscores the need for seriousresearch on portable systems.Generality of linguistic coverage is an important part both of system portabilityand overall system performance.
In order to evaluate the linguistic coverage, we de-vised a successful method for isolating specific linguistic phenomena nd measuringsystem performance on them within the black-box framework, even though specificsof the performance of systems varied and made these tests more difficult to interpret.Development of a suite of such tests with adequate linguistic coverage would pro-vide insight into how the handling of certain common linguistic phenomena relates tooverall system performance.
This insight would be similar to that obtainable throughglass-box testing, but the method of testing is still technically a black-box methodbecause it looks only at the output given certain input.447Computational Linguistics Volume 19, Number 3We had hoped to draw conclusions concerning the relative effectiveness of thevarious language processing techniques used by the participating systems.
However,the results of MUC-3, even with their statistical significance now known, do not sup-port the recommendation f one approach over another.
We did notice, however, thatpattern-matching and information retrieval techniques proved successful only whencombined with linguistic techniques.
Used in isolation, these techniques were not pow-erful enough to extract he wide range of information eeded in the MUC-3 task.
Wealso noted that robust processing techniques (filtering, skimming, robust or partialparsing) were important for good performance.Overall, we believe that the MUC conferences have made a major contribution toevaluation methodology.
They have both benefited from and inspired other work onevaluation, as evidenced by the growing body of research on evaluation techniquesfor natural anguage understanding including two workshops on the subject: Palmerand Finin, 1990 and Neal and Walter, 1991.
In glass-box evaluation, researchers havedeveloped a parse evaluation methodology (Black et al 1991) that is now in use.
Thereis active research in the evaluation of spoken language understanding: Hirschmanet al, 1992 and Price et al, 1992.
In addition, there is a new effort in evaluationof machine translation systems based on quality assessment measures and use of amultiple choice reading comprehension test.
This work is beginning to provide theevaluation techniques that will enable the natural language community to assess itsprogress, to understand its results, and to focus future research towards robust, high-performance systems capable of handling real-world applications.AcknowledgmentsWe wish to acknowledge the contributions of many individuals without whom wewould not have results to discuss here.
First, there are the individual system devel-opers listed by sites in Figure 1; second, there is the Program Committee, which wasresponsible for the general framework of the conference and many of the detaileddecisions about evaluation.
Next, the government attendees at MUC-3 provided in-sight into possible requirements and applications of such systems; their enthusiasmwas contagious and made us feel that years of research in natural anguage processingwas ready to bear fruit.
And finally, we wish to acknowledge the key people respon-sible for making the conference happen: Beth Sundheim of NRaD, who, as organizerof all of the Message Understanding Conferences, has made a major contribution tothe state of evaluation for natural anguage systems; and Charles Wayne and ThomasCrystal of the Advanced Research Projects Agency/Software and Intelligent SystemsTechnology Office, whose financial and moral support for MUC-3 and MUC-4 wereinvaluable.ReferencesBlack, E.; Abney, S.; Flickinger, D.; Gdaniec,C.
; Grishman, R.; Harrison, P.; Hindle, D.;Ingria, R.; Jelinek, E; Klavans, J.;Liberman, M.; Marcus, M.; Roukos, S.;Santorini, B.; and Strzalkowski, T.
(1991).
"A procedure for quantitativelycomparing the syntactic overage ofEnglish grammars."
In Proceedings, Speechand Natural Language Workshop.
PacificGrove, CA.
February 1991, 306-311.Chatfield, C. (1988).
Problem Solving: AStatistician's Guide.
Chapman and Hall.Chinchor, N. (1991a).
"Evaluation metrics.
"In Proceedings, Third Message UnderstandingConference (MUC-3).
Morgan Kaufmann.San Mateo, CA.Chinchor, N. (1991b).
"Linguisticphenomena test experiment."
InProceedings, Third Message UnderstandingConference (MUC-3).
Morgan Kaufmann.San Mateo, CA.Chinchor, N. (1992).
"The statisticalsignificance of the MUC-4 results."
InProceedings, Fourth Message UnderstandingConference (MUC-4).
Morgan Kaufmann.San Mateo, CA.448Nancy Chinchor et al Evaluating Message Understanding SystemsCohen, P., ed.
(1991).
Proceedings, Workshopon AI Methodology.
Amherst, MA, June.Efron, B., and Tibshirani, R.
(1991).
"Statistical data analysis in the computerage."
Science, 253, 390-395.Hirschman, L. (1991a).
"Evaluation forlanguage understanding systems."
InProceedings, Workshop on AI Methodology,edited by P. Cohen, Amherst, MA, June1991.Hirschman, L. (1991b).
"ComparingMUCK-II and MUC-3: Assessing thedifficulty of different tasks."
InProceedings, Third Message UnderstandingConference (MUC-3).
Morgan Kaufmann.San Mateo, CA.Hirschman, L. (1992).
"An adjunct est fordiscourse processing in MUC-4."
InProceedings, Fourth Message UnderstandingConference (MUC-4).
Morgan Kaufmann.San Mateo, CA.Hirschman, L.; Bates, M.; Dahl, D.; Fisher,W.
; Garofolo, J.; Hunicke-Smith, K.;Pallett, D.; Pao, C.; Price, P.; andRudnicky, A.
(1992).
"Multi-site datacollection for a spoken language corpus.
"In Proceedings, International Conference onSpoken Language Processing (ICSLP-92).Banff, Canada.Krupka, G., and Rau, L. (1992).
"GE adjuncttest report: Object-oriented design andscoring for MUC-4."
In Proceedings, FourthMessage Understanding Conference (MUC-4).Morgan Kaufmann.
San Mateo, CA.Lehnert, W., and Sundheim, B.
(1991).
"Anevaluation of text analysis technologies.
"AI Magazine, 12(3), 81-94.Lewis, D. D. (1991).
"Data extraction as textcategorization: An experiment with theMUC-3 corpus."
In Proceedings, ThirdMessage Understanding Conference (MUC-3).Morgan Kaufmann.
San Mateo, CA.Lewis, D. D., and Tong, R. M. (1992).
"Textfiltering in MUC-3 and MUC-4."
InProceedings, Fourth Message UnderstandingConference (MUC-4).
Morgan Kaufmann.San Mateo, CA.Neal, J. G., and Walter, S. M., eds.
(1991).Natural Language Processing SystemsEvaluation Workshop.
Berkeley, CA.
June1991.Noreen, E. W. (1989).
Computer IntensiveMethods for Testing Hypotheses: AnIntroduction.
John Wiley & Sons.Palmer, M., and Finin, T. (1990).
"Workshopon the Evaluation of Natural LanguageProcessing Systems."
ComputationalLinguistics, 16(3), 175-181.Price, P.; Hirschman, L.; Shriberg, E.; andWade, E. (1992).
"Subject-basedevaluation measures for interactivespoken language systems."
In Proceedings,Speech and Natural Language Workshop.Arden House, NY, February 1992.Proceedings, Third Message UnderstandingConference (MUC-3).
(1991).
MorganKaufmann.
San Mateo, CA.Proceedings, Fourth Message UnderstandingConference (MUC-4).
(1992).
MorganKaufmann.
San Mateo, CA.van Rijsbergen, C. J.
(1979).
InformationRetrieval.
Butterworth.Santorini, B.
(1990).
"Part-of-speech taggingguidelines for the Penn TREEBANKproject (3rd revision)."
MS-CIS-90-47.
CISDepartment, University of Pennsylvania.Philadelphia, PA.Swets, J.
A., ed.
(1964).
Signal Detection andRecognition by Human Observers.
JohnWiley & Sons.Swets, J.
A.
(1969).
"Effectiveness ofinformation retrieval methods."
AmericanDocumentation, 72-89.449
