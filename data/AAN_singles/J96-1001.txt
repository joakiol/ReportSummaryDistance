Translating Collocations for BilingualLexicons: A Statistical ApproachFrank  Smadja*NetPatrol ConsultingVasi le ios Hatz ivass i log lou  tColumbia UniversityKath leen  R. McKeown tColumbia UniversityCollocations are notoriously difficult for non-native speakers to translate, primarily because theyare opaque and cannot be translated on a word-by-word basis.
We describe a program namedChampollion which, given a pair of parallel corpora in two different languages and a list ofcollocations in one of them, automatically produces their translations.
Our goal is to provide a toolfor compiling bilingual exical information above the word level in multiple languages,for differentdomains.
The algorithm we use is based on statistical methods and produces p-word translations ofn-word collocations in which n and p need not be the same.
For example, Champollion translatesmake ... decision, employment equity, and stock market into prendre ... d6cision, 6quit6en mati6re d'emploi, and bourse respectively.
Testing Champollion on three years' worth ofthe Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at73% accuracy on average.
In this paper, we describe the statistical measures used, the algorithm,and the implementation f Champollion, presenting our results and evaluation.1.
IntroductionHieroglyphics remained undeciphered for centuries until the discovery of the Rosettastone in the beginning of the 19th century in Rosetta, Egypt.
The Rosetta stone is atablet of black basalt containing parallel inscriptions in three different scripts: Greekand two forms of ancient Egyptian writings (demotic and hieroglyphics).
Jean-FrancoisChampollion, a linguist and Egyptologist, made the assumption that these inscriptionswere parallel and managed after several years of research to decipher the hieroglyphicinscriptions.
He used his work on the Rosetta stone as a basis from which to producethe first comprehensive hieroglyphics dictionary (Budge 1989).In this paper, we describe a modern version of a similar approach: given a largecorpus in two languages, our system produces translations of common word pairsand phrases that can form the basis of a bilingual exicon.
Our focus is on the use ofstatistical methods for the translation of multiword expressions, uch as collocationswhich are often idiomatic in nature.
Published translations of such collocations are notreadily available, even for languages uch as French and English, despite the fact thatcollocations have been recognized as one of the main obstacles to second languageacquisition (Leed and Nakhimovsky 1979).
* The work reported in this paper was done while the author was at Columbia University.
His currentaddress is NetPatrol Consulting, Tel Maneh 6, Haifa 34363, Israel.
E-maih smadj a?netvision, et.
?1.t Department of Computer Science, 450 Computer Science Building, Columbia University, New York, NY10027, USA.
E-mail: kathy@cs, columbia, edu, vh@cs, columbia, edu.
(D 1996 Association for Computational LinguisticsComputational Linguistics Volume 22, Number 1We have developed a program named Champollion', which, given a sentence-aligned parallel bilingual corpus, translates collocations (or individual words) in thesource language into collocations (or individual words) in the target language.
Thealigned corpus is used as a reference, or database corpus, and represents Champol-lion's knowledge of both languages.
Champollion uses statistical methods to incremen-tally construct the collocation translation, adding one word at a time.
As a correlationmeasure, Champollion uses the Dice coefficient (Dice 1945; S6rensen 1948) commonlyused in information retrieval (Salton and McGill 1983; Frakes and Baeza-Yates 1992).For a given source language collocation, Champollion identifies individual words in thetarget language that are highly correlated with the source collocation, thus producinga set of words in the target language.
These words are then combined in a systematic,iterative manner to produce a translation of the source language collocation.
Cham-pollion considers all pairs of these words and identifies any that are highly correlatedwith the source collocation.
Next, triplets are produced by adding a highly correlatedword to a highly correlated pair, and the triplets that are highly correlated with thesource language collocation are passed to the next stage.
This process is repeated untilno more highly correlated combinations of words can be found.
Champollion selectsthe group of words with the highest cardinality and correlation factor as the targetcollocation.
Finally, it produces the correct word ordering of the target collocation byexamining samples in the corpus.
If word order is variable in the target collocation,Champollion labels it flexible (for example, to take steps to can appear as took immediatesteps to, steps were taken to, etc.
); otherwise, the correct word order is reported and thecollocation is labeled rigid.To evaluate Champollion, we used a collocation compiler, XTRACT (Smadja 1993),to automatically produce several lists of source (English) collocations.
These sourcecollocations contain both flexible word pairs, which can be separated by an arbitrarynumber of words, and fixed constituents~ uch as compound noun phrases.
UsingXTRACT on three parts of the English data in the Hansards corpus, each representingone year's worth of data, we extracted three sets of collocations, each consisting of300 randomly selected collocations occurring with medium frequency.
We then ranChampollion on each of these sets, using three separate database corpora of varyingsize, also taken from the Hansards corpus.
We asked several people fluent in bothFrench and English to judge the results, and the accuracy of Champollion was found torange from 65% to 78%.
In our discussion of results, we show how problems for thelower score can be alleviated by increasing the size of the database corpus.In the following sections, we first present a review of related work in statisticalnatural anguage processing dealing with bilingual data.
Our algorithm depends onusing a measure of correlation to find words that are highly correlated across lan-guages.
We describe the measure that we use and then provide a detailed escriptionof the algorithm, following this with a theoretical nalysis of the performance ofour al-gorithm.
Next, we turn to a description of the results and evaluation.
Finally, we showhow the results can be used for a variety of applications, closing with a discussion ofthe limitations of our approach and of future work.1 None of the authors is affiliated with Boitet's research center on machine translation i  Grenoble,France, which is also named "Champollion'.Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexicons2.
Related WorkThe recent availability of large amounts of bilingual data has attracted interest inseveral areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, andMercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993), wordalignment (Gale and Church 1991a; Brown et al 1993; Dagan, Church, and Gale 1993;Fung and McKeown 1994; Fung 1995b), alignment of groups of words (Smadja 1992;Kupiec 1993; van der Eijk 1993), and statistical translation (Brown et al 1993).
Of these,aligning groups of words is most similar to the work reported here, although, as weshall show, we consider a greater variety of groups than is typical in other research.In this section, we describe work on sentence and word alignment and statisticaltranslation, showing how these goals differ from our own, and then describe workon aligning groups of words.
Note that there is additional research using statisticalapproaches tobilingual problems, but it is less related to ours, addressing, for example,word sense disambiguation i the source language by statistically examining context(e.g., collocations) in the source language, thus allowing appropriate word selectionin the target language.
(Brown et al 1991; Dagan, Itai, and Schwall 1991; Dagan andItai 1994).Our use of bilingual corpora ssumes a prealigned corpus.
Thus, we draw on workdone at AT&T Bell Laboratories by Gale and Church (1991a, 1991b, 1993) and at IBMby Brown, Lai, and Mercer (1991) on bilingual sentence alignment.
Sentence alignmentprograms take a paired bilingual corpus as input and determine which sentences inthe target language translate which sentences in the source language.
Both the AT&Tand the IBM groups use purely statistical techniques based on sentence length toidentify sentence pairing in corpora such as the Hansards.
The AT&T group (Gale andChurch 1993) defines sentence length by the number of characters in the sentences,while the IBM group (Brown, Lai, and Mercer 1991) defines sentence length by thenumber of words in the sentence.
Both approaches achieve similar results and havebeen influential in much of the research on statistical natural language processing,including ours.
It has been noted in more recent work that length-based alignmentprograms uch as these are problematic for many cases of real world parallel data, suchas OCR (Optical Character Recognition) input, in which periods may not be noticeable(Church 1993), or languages where insertions or deletions are common (Shemtov 1993;Fung and McKeown 1994).
These algorithms were adequate for our purposes, butcould be replaced by algorithms more appropriate for noisy input corpora, if necessary.Sentence alignment echniques are generally used as a preprocessing stage, beforethe main processing component that proposes actual translations, whether of words,phrases, or full text, and they are used this way in our work as well.Translation can be approached using statistical techniques alone.
Brown et al (1990,1993) use a stochastic language model based on techniques used in speech recognition,combined with translation probabilities compiled on the aligned corpus, to do sentencetranslation.
Their system, Candide, uses little linguistic and no semantic informationand currently produces good quality translations for short sentences containing highfrequency vocabulary, as measured by individual human evaluators (see Berger et al\[1994\] for information on recent results).
While they also align groups of words acrosslanguages in the process of translation, they are careful to point out that such groupsmay or may not occur at constituent breaks in the sentence.
In contrast, our work aimsat identifying syntactically and semantically meaningful units, which may be eitherconstituents or flexible word pairs separated by intervening words, and provides thetranslation of these units for use in a variety of bilingual applications.
Thus, the goalsof our research are somewhat different.3Computational Linguistics Volume 22, Number 1Kupiec (1993) describes a technique for finding noun phrase correspondences inbilingual corpora using several stages.
First, as for Champollion, the bilingual corpusmust be aligned by sentences.
Then, each corpus is separately run through a part-of-speech tagger and noun phrase recognizer.
Finally, noun phrases are mapped toeach other using an iterative re-estimation algorithm.
Evaluation was done on the 100highest-ranking correspondences produced by the program, yielding 90% accuracy.Evaluation has not been completed for the remaining correspondences--4900 distinctEnglish noun phrases.
The author indicates that the technique has several limitations,due in part to the compounded error rates of the taggers and noun phrase recognizers.Van der Eijk (1993) uses a similar approach for translating terms.
His work is basedon the assumption that terms are noun phrases and thus, like Kupiec, uses sentencealignment, tagging, and a noun phrase recognizer.
His work differs in the correlationmeasure he uses: he compares local frequency of the term (i.e., frequency in sentencescontaining the term) to global frequency (i.e., frequency in the full corpus), decreasingthe resulting score by a weight representing the distance between the actual position ofthe target erm and its expected position in the corpus; this weight is small if the targetterm is exactly aligned with the source term and larger as the distance increases.
Hisevaluation shows 68% precision and 64% recall.
We suspect hat the lower precisionis due in part to the fact that van der Eijk evaluated all translations produced by theprogram while Kupiec only evaluated the top 2%.
Note that the greatest differencebetween these two approaches and ours is that van der Eijk and Kupiec only handlenoun phrases whereas collocations have been shown to include parts of noun phrases,categories other than noun phrases (e.g., verb phrases), as well as flexible phrases thatinvolve words separated by an arbitrary number of other words (e.g., to take ... steps,to demonstrate ... support).
In this work, as in earlier work (Smadja 1992), we addressthe full range of collocations including both flexible and rigid collocations for a varietyof syntactic ategories.Another approach, begun more recently than our work, is taken by Dagan andChurch (1994), who use statistical methods to translate technical terminology.
Likevan der Eijk and Kupiec, they preprocess their corpora by tagging and by identifyingnoun phrases.
However, they use a word alignment program as opposed to sentencealignment and they include single words as candidates for technical terms.
One of themajor differences between their work and ours is that, like van der Eijk and Kupiec,they only handle translation of uninterrupted sequences of words; they do not handlethe broader class of flexible collocations.
Their system, Termight, first extracts candidatetechnical terms, presenting them to a terminologist for filtering.
Then, Termight iden-tifies candidate translations for each occurrence of a source term by using the wordalignment to find the first and last target positions aligned with any words of the sourceterms.
All candidate translations for a given source term are sorted by frequency andpresented to the user, along with a concordance.
Because Termight does not use ad-ditional correlation statistics, relying instead only on the word alignment, it will findtranslations for infrequent erms; none of the other approaches, including Champol-lion, can make this claim.
Accuracy, however, is considerably ower; the most frequenttranslation for a term is correct only 40% of the time (compare with Champollion's73% accuracy).
Since Termight is fully integrated within a translator's editor (anotherunique feature) and is used as an aid for human translators, it gets around the problemof accuracy by presenting the sorted list of translations to the translator for a choice.In all cases, the correct ranslation was found in this list and translators were able tospeed up both the task of identifying technical terminology and translating terms.Other recent related work aims at using statistical techniques to produce trans-lations of single words (Fung and McKeown 1994; Wu and Xia 1994; Fung 1995b)Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexiconsas opposed to collocations or phrases.
Wu and Xia (1994) employed an estimation-maximization technique to find the optimal word alignment from previously sentence-aligned clean parallel corpora 2, with additional significance filtering.
The work by Fungand McKeown (1994) and Fung (1995b) is notable for its use of techniques suitable toAsian/Romance language pairs as well as Romance language pairs.
Given that Asianlanguages differ considerably in structure from Romance languages, tatistical meth-ods that were previously proposed for pairs of European languages do not work wellfor these pairs.
Fung and McKeown's work also focuses on word alignment from noisyparallel corpora, where there are no clear sentence boundaries or perfect ranslations.Work on the translation of single words into multiword sequences that integratestechniques for machine-readable dictionaries with statistical corpus analysis (Klavansand Tzoukermann 1990; Klavans and Tzoukermann i press) is also relevant.
Whilethis work focuses on a smaller set of words for translation (movement verbs), it pro-vides a sophisticated approach using multiple knowledge sources to address bothone-to-many word translations and the problem of sense disambiguation.
Given onlyone word in the source, their system, BICORD, uses the corpus to extend dictionarydefinitions and provide translations that are appropriate for a given sense but do notoccur in the dictionary, producing a bilingual exicon of movement verbs as output.3.
Collocations and Machine TranslationCollocations, commonly occurring word pairs and phrases, are a notorious ource ofdifficulty for non-native speakers of a language (Leed and Nakhimovsky 1979; Benson1985; Benson, Benson, and Ilson 1986).
This is because they cannot be translated on aword-by-word basis.
Instead, a speaker must be aware of the meaning of the phraseas a whole in the source language and know the common phrase typically used inthe target language.
While collocations are not predictable on the basis of syntactic orsemantic rules, they can be observed in language and thus must be learned throughrepeated usage.
For example, in American English one says set the table while in BritishEnglish the phrase lay the table is used.
These are expressions that have evolved overtime.
It is not the meaning of the words lay and set that determines the use of one orthe other in the full phrase.
Here, the verb functions as a support verb; it derives itsmeaning in good part from the object in this context and not from its own semanticfeatures.
In addition, such collocations are flexible.
The constraint is between the verband its object and any number of words may occur between these two elements (e.g.,You will be setting a gorgeously decorated and lavishly appointed table designed for a king).Collocations also include rigid groups of words that do not change from one contextto another, such as compounds, as in Canadian Charter of Rights and Freedoms.To understand the difficulties that collocations pose for translation, consider sen-tences (le) and (lf) in Figure 1.
Although these sentences are relatively simple, au-tomatically translating (le) as (lf) involves everal problems.
Inability to translate ona word-by-word basis is due in part to the presence of collocations.
For example,the English collocation to demonstrate support is translated as prouver son adhdsion.
Thistranslation uses words that do not correspond to individual words in the source; theEnglish translation of prouver is prove and son adhdsion translates as one's adhesion.
As aphrase, however, prouver son adhdsion carries the same meaning as the source phrase.Other groups of words in (le) cause similar problems, including to take steps to, provi-2 These corpora had little noise.
Most sentences neatly corresponded to translations i  the paired corpus,with few extraneous sentences.5Computational Linguistics Volume 22, Number 1(le) "Mr. Speaker, our Government has demonstrated its support forthese important principles by taking steps to enforce the provi-sions of the Charter more vigorously.
"(lf) "Monsieur le Pr6sident, notre gouvernement a prouv6 son adh6sionces importants principes en prenant des mesures pour appliquerplus syst6matiquement les pr6ceptes de la Charte.
"Figure 1Example pair of matched sentences from the Hansards corpus.sions of the Charter, and to enforce provisions.
These groups are identified as collocationsfor a variety of reasons.
For example, to take steps is a collocation because to take isused here as a support verb for the noun steps.
The agent our government doesn't actu-ally physically take anything; rather, it has begun the process of enforcement throughsmall, concrete actions.
While the French translation en prenant des mesures does usethe French for take, the object is the translation of a word that does not appear in thesource, measures.
These are flexible collocations exhibiting variations in word order.On the other hand, the compound provisions of the Charter is very commonly used as awhole in a much more rigid way.This example also illustrates that collocations are domain dependent, often form-ing part of a sublanguage.
For example, Mr. Speaker is the proper way to refer tothe Speaker of the House in the Canadian Parliament when speaking English.
TheFrench equivalent, Monsieur le Prdsident, is not the literal translation but instead usesthe translation of the term President.
While this is an appropriate translation for theCanadian Parliament, in different contexts another translation would be better.
Notethat these problems are quite similar to the difficulties in translating technical termi-nology, which also is usually part of a particular technical sublanguage (Dagan andChurch 1994).
The ability to automatically acquire collocation translations i thus adefinite advantage for sublanguage translation.
When moving to a new domain andsublanguage, translations that are appropriate can be acquired by running Champollionon a new corpus from that domain.Since in some instances parts of a sentence can be translated on a word-by-wordbasis, a translator must know when a full phrase or pair of words must be consid-ered for translation and when a word-by-word technique will suffice.
Two tasks musttherefore be considered:..Identify collocations, or phrases which cannot be translated on aword-by-word basis, in the source language.Provide adequate translation for these collocations.For both tasks, general knowledge of the two languages i not sufficient.
It is alsonecessary to know the expressions used in the sublanguage, since we have seen thatidiomatic phrases often have different translations in a restricted sublanguage than ingeneral usage.
In order to produce a fluent ranslation of a full sentence, it is necessaryto know the specific translation for each of the source collocations.We use XTRACT (Smadja and McKeown 1990; Smadja 1991a; Smadja 1993), aSmadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexiconstool we developed previously, to identify collocations in the source language (task1).
XTRACT works in three stages.
In the first stage, word pairs that co-occur withsignificant frequency are identified.
These words can be separated by up to four inter-vening words and thus constitute flexible collocations.
In the second stage, XTRACTidentifies combinations of word pairs from stage one with other words and phrases,producing compounds and idiomatic templates (i.e., phrases with one or more holesto be filled by specific syntactic types).
In the final stage, XTRACT filters any pairs thatdo not consistently occur in the same syntactic relation, using a parsed version of thecorpus.
This tool has been used in several projects at Columbia University and hasbeen distributed to a number of research and commercial sites worldwide.XTRACT has been developed and tested on English-only input.
For optimal per-formance, XTRACT itself relies on other tools, such as a part-of-speech tagger and arobust parser.
Although such tools are becoming more widely available in many lan-guages, they are still hard to find.
We have thus assumed in Champollion that thesetools were only available in one of the two languages; namely, English, termed thesource language throughout the paper.4.
The Similarity MeasureTo rank the proposed translations so that the best one is selected, Champollion uses aquantitative measure of correlation between the source collocation and its completeor partial translations.
This measure is also used to reduce the search space to amanageable size, by filtering out partial translations that are not highly correlatedwith the source collocation.
In this section, we discuss the properties of similaritymeasures that are appropriate for our application.
We explain why the Dice coefficientmeets these criteria and why this measure ismore appropriate than another frequentlyused measure--mutual information.Our approach is based on the assumption that each collocation is unambiguous inthe source language and has a unique translation i  the target language (at least in aclear majority of the cases).
In this way, we can ignore the context of the collocationsand their translations, and base our decisions only on the patterns of co-occurrence ofeach collocation and its candidate translations across the entire corpus.
This approachis quite different from those adopted for the translation of single words (Klavansand Tzoukermann 1990; Dorr 1992; Klavans and Tzoukermann 1996), since for singlewords polysemy cannot be ignored; indeed, the problem of sense disambiguation hasbeen linked to the problem of translating ambiguous words (Brown et al 1991; Dagan,Itai, and Schwall 1991; Dagan and Itai 1994).
The assumption of a single meaning percollocation was based on our previous experience with English collocations (Smadja1993), is supported for less opaque collocations by the fact that their constituent wordstend to have a single sense when they appear in the collocation (Yarowsky 1993), andwas verified during our evaluation of Champollion (Section 7).We construct a mathematical model of the events we want to correlate, namely, theappearance of any word or group of words in the sentences of our corpus, as follows:To each group of words G, in either the source or the target language, we map a binaryrandom variable Xc that takes the value "1" if G appears in a particular sentence and"0" if not.
Then, the corpus of paired sentences comprising our database representsa collection of samples for the various random variables X for the various groups ofwords.
Each new sentence in the corpus provides a new independent sample for everyvariable XG.
For example, if G is unemployment rate and the words unemployment rateappear only in the fifth and fifty-fifth sentences of our corpus (not necessarily in thatorder and perhaps with other words intervening), then in our sample collection, XcSmadja, McKeown', and Hatzivassiloglou Translating Collocations for Bilingual Lexiconsmutual  information represents the log-likelihood ratio of the joint probabil ity of see-ing a "1" in both variables over the probabil ity that such an event would have if thetwo variables were independent, and thus provides a measure of the departure fromindependence.The Dice coefficient, on the other hand, combines the conditional probabilitiesp(X= 1 I Y= 1) and p(Y= 1 I X= 1) with equal weights in a single number.
This canbe shown by replacing p(X= 1, Y= 1) on the right side of equation (1): 3Dice(X, Y) = 2.p(X=I,Y=I) p(X=l)+p(Y=l)2p(X=l )  p (Y=l )  +p(X=I,Y=I) p(X=I,Y=I)2p(X=l )  p (Y=l )  +p(W=l IX=l )p (X=l )  p (X=I IY=I )p (Y=I )21 1 +p(Y=l  IX=l )  p (X=l  I Y=I )As is evident from the above equation, the Dice coefficient depends only on theconditional probabilities of seeing a "1" for one of the variables after seeing a "1" forthe other variable, and not on the marginal probabilities of " l ' s  for the two variables.In contrast, both the average and the specific mutual  information depend on both theconditional and the marginal probabilities.
For SI(X, Y) in particular, we havesi(x, Y)log p (X=l )= logP(Y=l  I X=I )p(Y= 1) (2)p(X=I,Y=I)= logp(x=l )p (Y=l )p (X=l  \] Y=I)p(Y=I)= log p(X=l)p(Y=l)p(X=I IY=I )To select among the three measures, we first observe that for our application,1-1 matches (paired samples where both X and Y are 1) are significant while 0-0matches (samples where both X and Y are 0) are not.
These two types of matchescorrespond to the cases where either both word groups of interest appear in a pair ofaligned sentences or neither word group does.
Seeing the two word groups in aligned3 In the remainder of this discussion, we assume that p(X= 1, Y= 1) is not zero.
This is a justifiedassumption for our model, since we cannot say that two words or word groups will not occur in thesame sentence or in a sentence and its translation; such an event may well happen by chance, orbecause the words or word groups are parts of different syntactic onstituents, even for unrelatedwords and word groups.
The above assumption guarantees that all three measures are alwayswell-defined; in particular, it guarantees that the marginal probabilities p(X= 1) and p(Y= 1) and theconditional probabilities p(X = 1 I Y = 1) and p(Y = 1 I X = 1 ) are all nonzero.Computational Linguistics Volume 22, Number 1sentences (a 1-1 match) certainly contributes to their association and increases ourbelief that one is the translation of the other.
Similarly, seeing only one of them (a 1-0or 0-1 mismatch) decreases our belief in their association.
But, given the many possiblegroups of words that can appear in each sentence, the fact that neither of two groupsof words appears in a pair of aligned sentences does not offer any information abouttheir similarity.
Even when the word groups have been observed relatively few times(together or separately), seeing additional sentences containing none of the groups ofwords we are interested in should not affect our estimate of their similarity.In other words, in our case, X and Y are highly asymmetric; a "1" value (and a 1-1match) is much more informative than a "0" value (or 0-0 match).
Therefore, we shouldselect a similarity measure that is based only on 1-1 matches and mismatches.
0-0matches hould be completely ignored; otherwise, they would dominate the similaritymeasure, given the overall relatively low frequency of any particular word or wordgroup in our corpus.The Dice coefficient satisfies the above requirement of asymmetry: adding 0-0matches does not change any of the absolute frequencies fxY, fx, and fy, and so doesnot affect Dice(X, Y).
On the other hand, average mutual information depends onlyon the distribution of X and Y and not on the actual values of the random variables.In fact, I(X, Y) is a completely symmetric measure.
If the variables X and Y are trans-formed so that every "1" is replaced with a "0" and vice versa, the average mutualinformation between X and Y remains the same.
This is appropriate in the contextof communications for which mutual information was originally developed (Shannon1948), where the ones and zeros encode two different states with no special preferencefor either of them.
But in the context of translation, exchanging the "l"s and "0"s isequivalent to considering a word or word group to be present when it was absentand vice versa, thus converting all 1-1 matches to 0-0 matches and all 0-0 matches to1-1 matches.
As explained above, such a change should not be considered similaritypreserving, since 1-1 matches are much more significant than 0-0 ones.As a concrete xample, consider a corpus of 100 matched sentences, where each ofthe word groups associated with X and Y appears five times.
Furthermore, supposethat the two groups appear twice in a pair of aligned sentences and each word groupalso appears three times by itself.
This situation is depicted in the column labeled"Original Variables" in Table 1.
Since each word group appears two times with theother group and three times by itself, we would normally consider the source andtarget groups somewhat similar but not strongly related.
And indeed, the value of the{2x2 ~--_ 0.4)  intuitively corresponds to that assessment of similarity.
4 Dice coefficient ,Y4-5Now, suppose that the "0"s and "l"s in X and Y are exchanged, so that the situation isnow described by the last column of Table 1.
The transformed variables now indicatethat out of 100 sentences, the two word groups appear together 92 times, while eachappears by itself three times and there are two sentences that contain none of thegroups.
We would consider such evidence to strongly indicate very high similaritybetween the two groups, and indeed the Dice coefficient of the transformed variables2x92 0.9684.
However, the average mutual information of the variables i s  now 95+95 -would remain the same.Specific mutual information falls somewhere in between the Dice coefficient andaverage mutual information: it is not completely symmetric but neither does it ig-nore 0-0 matches.
This measure is very sensitive to the marginal probabilities (relativefrequencies) of the "l"s in the two variables, tending to give higher values as these4 Recall that the Dice coefficient is always between 0 and 1.10Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual LexiconsTable 1Example values of Dice(X, Y), I(X, Y), and SI(X, Y) after interchanging O's and l's.Original Variables Transformed Variables1-1 matches 2 920-0 matches 92 21-0 and 0-1 mismatches 6 6Total 100 100Dice coefficient 0.4000 0.9684Average mutual information (bits) 0.0457 0.0457Specific mutual information (bits) 3.0000 0.0277probabilities decrease.
Adding 0-0 matches lowers the relative frequencies of " l"s,and therefore always increases the estimate of SI(X, Y).
Furthermore, as the marginalprobabilities of the two word groups become very small, SI(X, Y) tends to infinity,independently of the distribution of matches (including 1-1 and 0-0 ones) and mis-matches, as long as the joint probability of 1-1 matches is not zero.
By taking the limitof SI(X,Y) for p (X=l )  --* 0 or p (Y=l )  ~ 0 in equation (2) we can easily verify thatthis happens even if the conditional probabilities p(X= 1 I Y= 1) and p(Y= 1 I X= 1)remain constant, a fact that should indicate a constant degree of relatedness betweenthe two variables.
Neither of these problems occurs with the Dice coefficient, exactlybecause that measure combines the conditional probabilities of " l"s  in both directionswithout looking at the marginal distributions of the two variables.
In fact, in casessuch as the examples of Table 1, where p(X = 1 I Y = 1) = p(Y = 1 t X = 1), the Dicecoefficient becomes equal to these conditional probabilities.The dependence of SI(X, Y) on the marginal probabilities of " l"s  shows that usingit would make rare word groups look more similar than they really are.
For ourexample in Table 1, the specific mutual information is SI(X, Y) = log 0.02 log 8 =0.05  x0 .05  - -3 bits for the original variables, but SI(X', Y') = log 0.92 log 1.019391 = 0.0277070.95  x0 .95  - -bits for the transformed variables.
Note, however, that the change is in the oppositedirection from the appropriate one; that is, the new variables are deemed far lesssimilar than the old ones.
This can be attributed to the fact that the number of " l "s  inthe original variables is far smaller.SI(X,Y) also suffers disproportionately from estimation errors when the observedcounts of " l"s  are very small.
While all similarity measures will be inaccurate whenthe data is sparse, the results produced by specific mutual information can be moremisleading than the results of other measures, because S!
is not bounded.
This is nota problem for our application, as Champollion applies absolute frequency thresholds toavoid considering very rare words and word groups; but it indicates another potentialproblem with the use of SI to measure similarity.Finally, another criterion for selecting a similarity measure is its suitability fortesting for a particular outcome, where outcome is determined by the application.
Inour case, we need a clear-cut est to decide when two events are correlated.
Both formutual information and the Dice coefficient, his involves comparison with an exper-imentally determined threshold.
Although the two measures are similar in that theycompare the joint probability p(X= 1, Y = 1) with the marginal probabilities, they havedifferent asymptotic behaviors.
This was demonstrated in the previous paragraphs forthe cases of small and decreasing relative frequencies.
Here we examine two more11Computational Linguistics Volume 22, Number 1cases associated with specific tests.
We consider the two extreme cases, whereThe two events are perfectly independent.
In this case,p(X= x, Y=y) = p(X=x)p(Y=y).The two events are perfectly correlated in the positive direction: eachword group appears every time (and only when) the other appears inthe corresponding sentence.
Then0 i fxCyp(X=x, Y=y) = p(X=x) = p(Y=y) if x = yIn the first case, both average and specific mutual information are equal to 0 sincelog p(X=x,Y-y) = log I = 0 for all x and y, and are thus easily testable, whereas the p(X--x)p(Y--y)Dice coefficient is equal to 2x (p(X=t)xp(Y=l)) and is thus a function of the individual fre- p(X=I)+p(Y=I)quencies of the two word groups.
In this case, the test is easier to decide using mutualinformation.
In the second case, the results are reversed; specific mutual informationis equal to log p(X=l) = - log(p(X=l)) ,  and it can be shown that the average mutualinformation becomes equal to the entropy H(X) of X (or Y).
Both of these measuresdepend on the individual probabilities (or relative frequencies) of the word groups,2xp(X-1) 1.
In this case, the test is easier whereas the Dice coefficient is equal to p(X-1)+p(x-1) -to decide using the Dice coefficient.
Since we are looking for a way to identify posi-tively correlated events we must be able to easily test the second case, while testingthe first case is not relevant.
Specific mutual information is a good measure of inde-pendence (which it was designed to measure), but good measures of independenceare not necessarily good measures of similarity.The above arguments all support he use of the Dice coefficient over either averageor specific mutual information.
We have confirmed the theoretically expected behaviorof the similarity measures through testing.
In our early work on Champollion (Smadja1992), we used specific mutual information (S/) as a correlation metric.
After carefullystudying the errors produced, we suspected that the Dice measure would producebetter esults for our task, according to the arguments given above.Consider the example given in Table 2.
In the table, the second column representscandidate French word pairs for translating the single word today.
The third columngives the frequency of the word today in a subset of the Hansards containing 182,584sentences.
The fourth column gives the frequency of each French word pair in theFrench counterpart of the same corpus, and the fifth column gives the frequency ofappearance of today and each French word pair in matched sentences.
Finally, thesixth and seventh columns give the similarity scores for today and each French wordpair computed according to the Dice measure or specific mutual information (in bits)respectively.
Of the four candidates, aujourd hui (shown in bold) is the only correcttranslation.
5 We see from the table that the specific mutual information scores fail toidentify aujourd hui as the best candidate--it is only ranked fourth.
Furthermore, thefour SI scores are very similar, thus not clearly differentiating the results.
In contrast,5 Note that the correct translation is really a single word in contemporary French.
Aujourd'hui hasevolved from a collocation (au jour d'hui) which has become so rigid that it is now considered a singleword.
Hui can still appear on its own, but aujourd is not a French word, so Champollion's Frenchtokenizer erroneously considered the apostrophe character as a word separator in this case.
Champollionwill correct this error by putt ing aujourd and hui back together and identifying them as a rigidcollocation.12Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual LexiconsTable 2Dice versus pecific mutual information scores for the English word today.
The correcttranslation is shown in bold.English (X) French (Y) fx fY fxY Dice(X, Y) SI(X, Y)d6bat aujourd 3121 143 130 0.08 5.73d6bat hui 3121 143 130 0.08 5.73today s6nat hui 3121 52 46 0.03 5.69aujourd hui 3121 2874 2408 0.80 5.62the Dice coefficient clearly identifies aujourd hui as the group of words most similar totoday, which is what we want.After implementing Champollion, we attempted to generalize these results and con-firm our theoretical argumentation by performing an experiment to compare SI andthe Dice coefficient in the context of Champollion.
We selected a set of 45 collocationswith mid-range frequency identified by XTRACT and we ran Champollion on themusing sample training corpora (databases).
For each run of Champollion, and for eachinput collocation, we took the final set of candidate translations of different lengthsproduced by Champollion (with the intermediate stages driven by the Dice coefficient)and compared the results obtained using both the Dice coefficient and SI at the laststage for selecting the proposed translation.
The 45 collocations were randomly se-lected from a larger set of 300 collocations o that the Dice coefficient's performanceon them is representative (i.e., approximately 70% of them are translated correctly byChampollion when the Dice measure is used), and the correct ranslation is always in-cluded in the final set of candidate translations.
In this way, the number of erroneousdecisions made when SI is used at the final pass is a lower bound on the number oferrors that would have been made if SI had also been used in the intermediate stages.We compared the results and found that out of the 45 source collocations,?
2 were not frequent enough in the database to produce any candidatetranslations.?
Using the Dice coefficient, 36 were correctly translated and 7 wereincorrectly translated.?
Using SI, 26 were correctly translated and 17 incorrectly.
6Table 3 summarizes these results and shows the breakdown across categories.
Inthe table, the numbers of collocations correctly and incorrectly translated when theDice coefficient is used are shown in the second and third rows respectively.
Forboth cases, the second column indicates the number of collocations that were correctlytranslated with SI and the third column indicates the number of these collocations thatwere incorrectly translated with SI.
The last column and the last row show the totalnumber of collocations correctly and incorrectly translated when the Dice coefficientor SI is used respectively.
From the table we see that every time SI produced good6 In this section, incorrect translations are those judged as incorrect by the authors.
We did notdistinguish between errors due to XTRACT (identifying an invalid English collocation) orChampoUion(providing awrong translation for a valid collocation).13Computational Linguistics Volume 22, Number 1Table 3Comparison of Dice and SI scores on a small set ofexamples.SI Correct SI Incorrect TotalDice Correct 26 10 36Dice Incorrect 0 7 7Total 26 17 43Table 4Dice versus specific mutual information scores on two example English collocations.
Thecorrect ranslation for each source collocation is shown in bold.English (X) French (Y) fx fY fXY Dice(X, Y) SI(X, Y)cartes 69 89 54 0.68 2.68cartes cr4dit 69 57 52 0.83 2.86 credit cards cartes cr6dit taux 69 23 22 0.48 2.88cartes cr6dit taux paient 69 2 2 0.06 2.90positive 116 89 73 0.71 2.59affirmative action positive action 116 75 73 0.76 2.66positive action sociale 116 2 2 0.03 2.68results, the Dice coefficient also produced good results; there were no cases for whichSI produced a correct result while the Dice coefficient produced an incorrect one.
Inaddition, we see that out of the 17 incorrect results produced by SI, the Dice coefficientcorrected 10.
Although based on only a few cases, this experiment confirms that theDice coefficient outperforms SI in the context of ChampolUon.Table 4 gives concrete xamples from this experiment in which the Dice coefficientoutperforms pecific mutual  information.
The table has a format similar to that ofTable 2.
X represents an English collocation (credit card or affirmative action), and Yrepresents candidate translations in French (for the credit cards example: cartes, cartescredit, cartes credit taux, and cartes crddit taux paient).
The correct translations are againshown in bold.
The third and fourth columns give the independent frequencies ofeach word group, while the fifth column gives the number  of times that both groupsappear in matched sentences.
The two subsequent columns give the similarity valuescomputed according to the Dice coefficient and specific mutual  information (in bits).The corpus used for these examples contained 54,944 sentences in each language.
Wesee from Table 4 that, as for the today example in Table 2, the SI scores are very closeto each other and fail to select the correct candidate whereas the Dice scores cover awider range and clearly peak for the correct translation.In conclusion, both theoretical arguments and experimental results support  thechoice of the Dice coefficient over average or specific mutual  information for our14Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexiconsapplication.
7 Consequently, we have used the Dice coefficient as the similarity measurein Champollion.5.
Champollion: The Algorithm and the ImplementationChampollion translates ingle words or collocations in one language into collocations(including single word translations) in a second language using the aligned corpus as areference database.
Before running Champollion there are two steps that must be carriedout: source and target language sentences of the database corpus must be aligned anda list of collocations to be translated must be provided in the source language.
For ourexperiments, we used corpora that had been aligned by Gale and Church's sentenceal ignment program (Gale and Church 1991b) as our input data.
8 Since our intent inthis paper is to evaluate Champollion, we tried not to introduce errors into the trainingdata; for this purpose, we kept only the 1-1 alignments.
Indeed, more complex sentencealignments tend to have a much higher al ignment error rate (Gale and Church 1991b).By doing so, we lost an estimated 10% of the text (Brown, Lai, and Mercer 1991), whichwas not problematic since we had enough data.
In the future, we plan to design moreflexible techniques that would work from a loosely aligned corpus (see Section 9).To compile collocations, we used XTRACT on the English version of the Hansards.Some of the collocations retrieved are shown in Table 5.
Collocations labeled "fixed,"such as International Human Rights Covenants, are rigid compounds.
Collocations labeled"flexible" are pairs of words that can be separated by intervening words or occur inreverse order, possibly with different inflected forms.Given a source English collocation, Champollion first identifies in the databasecorpus all the sentences containing the source collocation.
It then attempts to find allwords that can be part of the translation of the collocation, producing all words thatare highly correlated with the source collocation as a whole.
Once this set of words isidentified, Champollion iteratively combines these words in groups, so that each groupis in turn highly correlated with the source collocation.
Finally, Champollion producesas the translation the largest group of words having a high correlation with the sourcecollocation.More precisely, for a given source collocation, Champollion i itially identifies a setS of k words that are highly correlated with the source collocation.
This operation isdescribed in detail in Section 5.1 below.
Champollion assumes that the target colloca-tion is a combination of some subset of these words.
Its search space at this pointthus consists of the powerset ~(S) of S containing 2 k elements.
Instead of computinga correlation factor for each of the 2 k elements with the source collocation, Champollionsearches a part of this space in an iterative manner.
Champollion first forms all pairsof words in S, evaluates the correlation between each pair and the source collocationusing the Dice coefficient, and keeps only those pairs that score above some thresh-old.
Subsequently, it constructs the three-word elements of ~P(S) containing one of7 The choice of the Dice coefficient isnot crucial; for example, using the Jaccard coefficient or any othersimilarity measure that is monotonically related to the Dice coefficient would be equivalent.
What isimportant is that the selected measure satisfy the conditions of asymmetry, insensitivity to marginalword probabilities, and convenience in testing for correlation.
There are many other possible measuresof association, and the general points made in this section may apply to them insofar as they alsoexhibit he properties we discussed.
For example, the normalized chi-square measure (?2) used in Galeand Church (1991a) shares ome of the important properties of average mutual information (forexample, it is completely symmetric with respect to 1-1 and 0-0 matches).8 We are thankful to Ken Church and the AT&T Bell Laboratories for providing us with a prealignedHansards corpus.Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual LexiconsSOURCE COLLOCATION:official, 492languages, 266The numbers indicate the frequencies of the input words in the English corpus.NUMBER OF SENTENCES IN COMMON: 167The words appear together in 167 English sentences.Champollion ow gives all the candidate final translations; that is, the best translations at eachstage of the iteration process.
The best single word translation is thus (officielles), the best pair(officielles, langues), the best translation with 8 words (suivantes, doug, ddposer, lewis, pdtitions,honneur, officielles, langues ).The word groups are treated as sets, with no ordering.
The numbers arethe associated similarity score (using the Dice coefficient)for the best ranslation at each iteration andthe number of candidate translations that passed the threshold among the word groups consideredat that iteration.
There are thus 11 single words that pass the thresholds at the first iteration, 35pairs of words, and so on.CANDIDATE TRANSLATIONS:officielles, 0.94 out of 11officielles langues, 0.95 out of 35honneur officielles langues, 0.45 out of 61d6poser honneur officielles langues, 0.36 out of 71d6poser p6titions honneur officielles langues, 0.34 out of 56d6poser lewis p6titions honneur officielles langues, 0.32 out of 28doug d6poser lewis p6titions honneur officielles langues, 0.32 out of 8suivantes doug d4poser lewis p6titions honneur officielles langues, 0.20 out of 1Champollion then selects the optimal translation, which is the translation with the highest simi-larity score.
In this case the result is correct.SELECTED TRANSLATION:officielles langues 0.951070An example sentence inFrench where the selected translation is used is also shown.EXAMPLE SENTENCE:Le d6put6 n' ignore pas que le gouvernement compte pr6senter, avant la fin de 1'ann6e, un projet de r6vision de la Loi sur les langues officielles.Finally, additional information concerning word order is computed and presented.
For a rigidcollocation such as this one, Champollion will print for all words in the selected translation exceptthe first one their distance from the first word.
In our example, the second word (langues) appearsin most cases one word before officielles, to form the compound langues officielles.
Note thatthis information is added during postprocessing after the translation has been selected, and takesvery little time to compute because of the indexing.
In this case, it took a few seconds to computethis information.WORD ORDER:officielleslangues: selected position: -1Figure 2Sample output of Champollion.guage that satisfy the following two conditions:1.
The value of the Dice coefficient between the word and the sourcecollocation W is at least Ta, where T~ is an empirically chosen threshold,and2.
The word appears in the target language opposite the source collocationat least Tf times, where Tf is another empirically chosen threshold.17Computational Linguistics Volume 22, Number 1Words that pass these tests are collected in a set S, from which the final translationwill eventually be produced.
When given official anguages as input (see Figure 2),this step produces a set S with the following eleven words: suivantes, doug, d~poser,supr~matie, l wis, p~titions, honneur, programme, mixte, officielles, and langues.The Dice threshold Ta (currently set at 0.10) is the major criterion that Champollionuses to decide which words or partial collocations should be kept as candidates for thefinal translation of the source collocation.
In Section 6 we explain why this incrementalfiltering process is necessary and we show that it does not significantly degrade thequality of Champollion's output.
To our surprise, we found that the filtering processmay even increase the quality of the proposed translation.The absolute frequency threshold Tf (currently set at 5) also helps limit the sizeof S, by rejecting words that appear too few times opposite the source collocation.Its most important function, however, is to remove from consideration words thatappear too few times for our statistical methods to be meaningful.
Applying the Dicemeasure (or any other statistical similarity measure) to very sparse data can producemisleading results, so we use Tf as a guide for the applicability of our method to lowfrequency words.It is possible to modify the thresholds Td and Tf according to properties of thedatabase corpus and the collocations that are translated.
Such an approach would uselower values of the thresholds, especially of Tf, for smaller corpora or less frequentcollocations.
In that case, a separate stimation phase is needed to automatically de-termine the values of the thresholds.
The alternative we currently support is to allowthe user to replace the default hresholds during the execution of Champollion withvalues that are more appropriate for the corpus at hand.After all words have been collected in S, the initial set of possible translations Pis set equal to S, and Champollion proceeds with the next stage.Stage 2--Step 2: Scoring of possible translations.
In this step, Champollion examines allmembers of the set P of possible translations.
For each member x of P, Champollioncomputes the Dice coefficient between the source language collocation W and x.
If theDice coefficient is below the threshold Td, x is discarded from further consideration;otherwise, x is saved in a set P'.When given official languages as input, the first iteration of Step 2 simply sets P~to P, the second iteration selects 35 word pairs out of the possible 110 candidates,the third iteration selects 61 word triplets, and so on until the final (ninth) iterationwhen none of the three elements of P passes the threshold Ta and thus P~ has noelements.Stage 2--Step 3: Identifying the locally best ranslation.
Once the set of surviving transla-tions P~ has been computed, Champollion checks if it is empty.
If it is, there cannot beany more translations to be considered, so Champollion proceeds to Step 5.
If P' is notempty, Champollion locates the translation that looks locally the best; that is, amongall members of P~ analyzed at this iteration, the translation that has the highest Dicecoefficient value with the source collocation.
This translation is saved in a table C ofcandidate final translations, along with its length in words and its similarity score.Champollion then continues with the next step.The first iteration of Step 3 on our example collocation would select the wordofficielles (among the 11 words in S) as the first candidate translation, with a score of0.94.
On the second iteration, the word pair (officielles, langues) is selected (out of 35pairs that pass the threshold) with a score of 0.95.
On the third run, the word triplet(honneur, officieUes, langues), is selected (out of 61 triplets) with a score of 0.45.
On the18Computational Linguistics Volume 22, Number 15.1 Computational and Implementation FeaturesConsidering the size of the corpora that must be handled by Champollion, special carehas been taken to minimize the number of disk accesses made during processing.
Wehave experimented on up to two full years of the Hansards corpus, amounting tosome 640,000 sentences in each language or about 220 megabytes of uncompressedtext.
With corpora of this magnitude, Champollion takes between one and two minutesto translate a collocation, thus enabling its practical use as a bilingual exicographytool.To achieve fficient processing of the corpus database, Champollion is implementedin two phases: the preparation phase and the actual translation phase.
The preparationphase reads in the database corpus and indexes it for fast future access using a com-mercial B-tree package (Informix 1990).
Each word in the original corpus is associatedwith a set of pointers to all the sentences containing it and to the positions of the wordin each of these sentences.
The frequency of each word (in sentences) is also computedat this stage.
Thus, all the necessary information is collected from the corpus databaseat this preprocessing phase with only one pass over the corpus file.
At the translationphase, only the indices are accessed.For the translation phase, we developed an algorithm that avoids computing theDice coefficient for French words when the result must necessarily fall below thethreshold.
Using the index file on the English part of the corpus, we collect all Frenchsentences that match the source collocation, and produce a list of all words that appearin these sentences, together with their frequency (in sentences) in this subset of theFrench corpus.
This operation takes only a few seconds to perform, and yields a listof a few thousand French words.
The list also contains the local frequency of thesewords (i.e., frequency within this subset of the French corpus), and is sorted by thisfrequency in decreasing order.
We start from the top of this list and work our waydownwards until we find a word that fails either of the following tests:.2.The word's local frequency is lower than the threshold Tf.The word's local frequency is so low that we know it would beimpossible for the Dice coefficient between it and the source collocationto be higher than the threshold Td.Once a word fails one of the above tests, we are guaranteed that all subsequentwords in the list (with lower local frequencies) will also fail the same test.
By applyingthese two tests and removing all closed-class words from the list, we greatly reducethe number of words that must be considered.
In practice, about 90-98% of the wordsin the list fail to meet he two tests above, so we dramatically reduce our search spacewithout having to perform any relatively expensive operations.
For the remainingwords in the list, we need to compute their Dice coefficient value so as to select hebest-ranking one-word translation of the source collocation.The first of the above tests is rather obviously valid and easy to apply.
For thesecond test, we compute an upper bound for the Dice coefficient between the wordunder consideration and the source collocation.
Let X and Y stand for the sourcecollocation and the French word under consideration, respectively, atsome step of theloop through the word list.
At this point, we know the global frequency of the sourcecollocation (fx) and the local frequency of the candidate translation word (fxY), butnot the global frequency of the candidate word (fy).
We need all these three quantitiesto compute the Dice coefficient, but while fx is computed once for all Y, and it isvery efficient o compute fxY at the same time as the set of sentences matching X is20Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexiconsidentified, it is more costly to find fy even if a special access tructure ismaintained.
So,we first check whether there is any possibility that this word correlates with the sourcecollocation highly enough to pass the Dice threshold by assuming temporarily that theword does not appear at all outside the sentences matching the source collocation.
Bysetting fY=fxY, we can efficiently compute the Dice coefficient between X and Y underthis assumption:Dicea (X, Y) = 2" fxY 2. fxYfx + fY = fx + fxYOf course, this assumption most likely won't be true.
But since we know thatfxY < fY, it follows that Dicea(X,Y) is never less than the true value of the Dice co-efficient between X and y10 Comparing Dicea(X,Y) with the Dice threshold Ta willonly filter out words that are guaranteed not to have a high enough Dice coefficientvalue independently of their overall frequency f ; thus, this is the most efficient pro-cess for this task that also guarantees correctness, n Another possible implementationinvolves representing the words as integers using hashing.
Then it would be possibleto compute fr and the Dice coefficient in linear time.
Our method, in comparison,takes O(n log n) time to sort n candidates by their local frequency fxY, but it retrievesthe frequency f  and computes the Dice coefficient for a much smaller percentage ofthem.6.
Analysis of Champollion's Heuristic Filtering StageIn this section, we analyze the generative capacity of our algorithm.
In particular, wecompare it to the obvious method of exhaustively generating and testing all possiblegroups of k words, with k varying from 1 to some maximum length of the translation m.Our concern is whether our algorithm will actually generate all valid translations--those with final Dice coefficient above the threshold--while it is clear that the exhaus-tive algorithm would.
12 Does the filtering process we use sometimes cause our algo-rithm to omit a valid translation?
In other words, is there a possibility that a groupof words has high similarity with the source collocation (above the threshold) and atthe same time one or more of its subgroups have similarity below the threshold?
Inthe worst case, as we show below, the answer to this question is affirmative.
How-ever, if only very few translations are missed in practice, the algorithm is indeed agood choice.
In this section, we first show why the filtering we use is necessary andhow it can miss valid translations, and then present he results of Monte Carlo sim-ulation experiments (Rubenstein 1981) showing that with appropriate selection of thethreshold, the algorithm misses very few translations, that this rate of failure can bereduced even more by using different hresholds at each level, and that the missedtranslations are in general the less interesting ones, so that the rejection of some of thevalid (according to the Dice coefficient) ranslations most likely leads to an increase ofChampollion" s performance.10 And actually is a fight upper  bound, realized when fx=o,y=l = O.11 Heuristic filtering of words with low local frequency may be more or less efficient, depending on theword, but a higher percentage of discarded words will come at the cost of inadvertently throwing outsome valid words.12 In this section we refer to missed valid translations or failures, using these terms to describecandidate translations that are above the Dice threshold but are nevertheless rejected due to thenon-exhaustive algorithm we use.
These candidate translations are not necessarily correct translationsfrom a performance perspective.21Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexiconsand with a similar derivation, for the upper bound (i ~ 3),?
Pi~_ rj .2 (Q_ i ) !The sums of the bounds on the values Pi for i = 3 to m, plus the value P1 + P2 =Q + (Q), give upper and lower bounds on the total number of candidate translationsgenerated and examined by Champollion.
When the ri's are high, the actual number ofcandidate translations will be close to the lower bound.
On the other hand, low val-ues for the ri's (i.e., a low threshold Td) will result in the actual number of candidatetranslations being close to the upper bound.
To estimate the average number of candi-date translations examined, we make the simplifying assumption that the decisions toreject each candidate translation with i words are made independently with constantprobability ri.
Under these assumptions, the probability 7i of generating a particularcandidate translation with i words is the same for all translations with length i; thesame applies to the probability ;~i that a translation with i words is included in theset of translations of length i that will generate the candidate translations of lengthi + 1.
Clearly, 1 = 71 = 72 = 1 and ,~i = ri'Yi for i > 2.
For a particular translation withi _> 3 words to be generated, at least one of its i subsets with i - 1 words must havesurvived the threshold.
With our assumptions, we have7i = 1 - (1 - )ii_l) iFrom this recurrence quation and the boundary conditions given above we cancompute the values of 7/ and /~i for all i.
Then the expected (average) number ofcandidate translations with i ___ 3 words examined by Champollion will beand the sum of these terms for i = 3 to m, plus the terms Q and (2Q), gives the totalcomplexity of our algorithm.
In Table 6 we show the number of candidate transla-tions examined by the exhaustive algorithm and the corresponding best-, worst-, andaverage-case behavior of Champollion for several values of Q and m, using empiricalestimates of the ri's.6.2 Effects of the Filtering ProcessWe showed above that filtering is necessary to bring the number of proposed transla-tions down to manageable vels.
For any corpus of reasonable size, we can find caseswhere a valid translation is missed because a part of it does not pass the threshold.
LetN be the size of the corpus in terms of matched sentences.
Separate the N sentencesinto eight categories, depending on whether each of the source collocation (X) and thepartial translations (i.e., A and B) appear in it.
Let the counts of these sentences benABX, nABY:, nAgX, ?
?., n~2,  where a bar indicates that the corresponding term is absent.We can then find values of the n...'s that cause the algorithm to miss a valid translationas long as the corpus contains a modest number of sentences.
This happens when oneor more of the parts of the final translation appear frequently in the corpus but nottogether with the other parts or the source collocation.
This phenomenon occurs evenif we are allowed to vary the Dice thresholds at each stage of the algorithm.
With ourcurrent constant Dice threshold Td = 0.1, we may miss a valid translation as long asthe corpus contains at least 20 sentences.23Computational Linguistics Volume 22, Number 1Table 6Candidate translations examined by the exact and approximate algorithms for representativeword set sizes and translation lengths.Maximum Exhaustive Champollion's algorithmWords translationlength algorithm Best Worst Average5 2.37- 10 6 2,884 14,302 13,5585O10 1.34.10 l?
2,888 15,870 15,0325 1.85.107 9,696 75,331 71,129 7510 9.74.1011 9,748 96,346 90,8805 7.94.107 24,820 259,873 244,95010010 1.94.1013 25,127 391,895 369,0705 6.12.10 s 104,331 1 ,589 ,228 1,496,04115010 1.26- 1015 108,057 3 ,391 ,110 3,190,075While our algorithm will necessarily miss some valid translations, this is a worstcase scenario.
To study the average-case behavior of our algorithm, we simulatedits performance with randomly selected points with integer non-negative coordinates(nABX, nABy?, naf~x, nA~;~, n,~x, nABS, nA~x) from the hyperplane defined by the equationnABX + nAB R -b nAF~X + nA~ R q- nAB X q- nAuy ?+ nA~ X = Nowhere No is the number of "interesting" sentences in the corpus for the translationunder consideration, that is, the number of sentences that contain at least one of X,A, or  B.
13 Sampling from this six-dimensional polytope in seven-dimensional space isnot easy.
We accomplish it by constructing a mapping from the uniform distributionto each allowed value for the n...'s, using combinatorial methods.
For example, forNo = 50, there are 3,478,761 different points with nABX = 0 but only one with nABX = 50.Using the above method, we sampled 20,000 points for each of several values forNo (No = 50, 100, 500, and 1000).
The results of the simulation were very similar for thedifferent values of No, with no apparent pattern emerging as No increased.
Therefore,in the following we give averages over the values of No tried.We first measured the percentage of missed valid translations when either A or B,or both, do not pass the threshold but AB should, for different values of the thresholdparameter (solid line in Figure 3).
We observed that for low values of the threshold,less than 1% of the valid translations are missed; for example, for the threshold valueof 0.10 we currently use, the error rate is 0.74%.
However, as the threshold increases,the rate of failure can become unacceptable.A higher value for the threshold has two advantages: First, it offers higher se-lectivity, allowing fewer false positives (proposed translations that are not considered13 Note that the number of sentences that do not contain any of X, A, or B does not enter any of the Dicecoefficients computed by Champollion and consequently does not affect he algorithm's decisions.
Asdiscussed inSection 4, this gives a definite advantage tothe Dice method over other measures ofsimilarity.24Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexicons0013-00?
a= l. .
.
.
.
.
.
.
-..-..-..-.
27.----,----'- .................... = ~ ~, _~_.._.~ _.=..~-...-.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
-'a~a2/__3~20.bs 0.
0 0.
0 0.bsFinal thresholdFigure 3Failure rate of the translation algorithm with constant and increasing thresholds.
The casec~ = 1 (solid line) represents he basic algorithm with no threshold changes.accurate by the human judges).
Second, it speeds up the execution of the algorithm, asall fractions ri's decrease and the overall number of candidate translations i reduced.However, as Figure 3 shows, high values of the threshold parameter cause the algo-rithm to miss a significant percentage of valid translations.
Intuitively, we expect hisproblem to be alleviated if a higher threshold value is used for the final admittanceof a translation, but a lower threshold is used internally when the subparts of thetranslation are considered.
Our second simulation experiment tested this expectationfor various values of the final threshold using a lower initial threshold equal to aconstant ~ < 1 times the final threshold.
The results are represented by the remainingcurves of Figure 3.
Surprisingly, we found that with moderate values of c~ (close to1) this method gives a very low failure rate even for high final threshold values, andis preferable to using a constant but lower threshold just to reduce the failure rate.For example, running the algorithm at an initial threshold of 0.3 and a final thresholdof 0.6 gives a failure rate of 0.45%, much less than the failure rate of 6.59% whichcorresponds to a constant hreshold of 0.3 for both stages.
TMThe above analyses how that the algorithm fails quite rarely when the thresholdis low, and its performance can be improved with a sequence of increasing thresholds.We also studied cases where the algorithm does fail.
For this purpose, we stratified14 The curves in Figure 3 become noticeably less smooth for values of the final threshold that are greaterthan 0.8.
This happens for all settings of c~ in Figure 3.
This apparently different behavior for highthreshold values can be traced to sampling issues.
Since few of the 20,000 points in each sample meetthe criterion of having Dice(AB, X) greater or equal to the threshold for high final threshold values, theestimate of the percentage of failures is more susceptible to random variation in such cases.Furthermore, since the same sample (for a given No) is used for all values of c~, any such randomvariation due to small sample size will be replicated in all curves of Figure 3.25Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual LexiconsTable 7Failure rate of several variants of the translation algorithm for representative thresholds.Final c~ = 1 c~ = 3/4 c~ = 1/2 Low Dice(A,B) High Dice(A,B)threshold (c~ = 1) (c~ = 1)0.05 0.39% 0.05% 0.02% 1.80% 0.02%0.10 0.89% 0.21% 0.04% 4.99% 0.11%0.20 2.88% 0.70% 0.13% 25.26% 0.27%0.40 12.42% 2.29% 0.26% 96.33% 2.08%0.80 67.11% 10.79% 1.17% 100.00% 31.83%Table 8Some translations produced by Champollion.English Collocation French Translation Found by Champollionadditional costsaffirmative actionapartheid ... South Africacollective agreementdemonstrate supportemployment equityfree tradefreer tradehead officehealth insurancemake ... decisiontake.
.
,  stepscot/ts suppldmentairesaction positiveapartheid ... afrique sudconvention collectiveprouver ... adh6sion6quit6 ... mati6re.., emploilibre-4changelibdralisation ... 6changessi6ge socialassurance-maladieprendre ... d6cisionprendre.. ,  mesuresand year, taken from the aligned Hansards.
Table 8 illustrates the range of translationswhich Champollion produces.
Flexible collocations are shown with ellipsis points ( .
.
.
)indicating where additional, variable words could appear.
These examples how caseswhere a two word collocation is translated as one word (e.g., health insurance), a twoword collocation is translated as three words (e.g., employment equity), and how wordscan be inverted in the translation (e.g., additional costs).
In this section, we discuss thedesign of the separate tests and our evaluation methodology, and present he resultsof our evaluation.7.1 Experimental SetupWe carried out three tests with Champollion using two database corpora and threesets of source collocations.
The first database corpus (DB1) consists of 8 months ofHansards aligned data taken from 1986 (16 megabytes, 3.5 million words) and thesecond database corpus (DB2) consists of all of the 1986 and 1987 transcripts of theCanadian Parl iament (a total of approximately 45 megabytes and 8.5 million words).For the first corpus (DB1), we ran XTRACT and obtained a set of approximately 3,000collocations from which we randomly selected a subset of 300 for manual  evaluationpurposes.
The 300 collocations were selected from among the collocations of mid-rangefrequency--col locations appearing more than 10 times in the corpus.
We call this firstset of source collocations C1.
The second set (C2) is a set of 300 collocations imilarlyselected from the set of approximately 5,000 collocations identified by XTaACT on alldata from 1987.
The third set of collocations (C3) consists of 300 collocations elected27Computational Linguistics Volume 22, Number 18.
ApplicationsA bilingual exicon of collocations has a variety of potential uses.
The most obviousare machine translation and machine-assisted human translation, but other multilin-gual applications, including information retrieval, summarization, and computationallexicography, also require access to bilingual exicons.While some researchers are attempting machine translation through purely sta-tistical techniques, the more common approach is to use some hybrid of interlingualand transfer techniques.
These symbolic machine translation systems must have ac-cess to a bilingual exicon and the ability to construct one semi-automatically wouldease the development of such systems.
Champollion is particularly promising for thispurpose for two reasons.
First, it constructs translations for multiword collocations.Collocations are known to be opaque; that is, their meaning often derives from thecombination of the words and not from the meaning of the individual words them-selves.
As a result, translation of collocations cannot be done on a word-by-word basis,and some representation f collocations in both languages i needed if the system is totranslate fluently.
Second, collocations are domain dependent.
Particularly in techni-cal domains, the collocations differ from those in general use.
Accordingly, the abilityto automatically discover collocations for a given domain by using a new corpus asinput to Champollion would ease the work required to transfer an MT system to a newdomain.Multilingual systems are now being developed in addition to pure machine trans-lation systems.
These systems also need access to bilingual phrases.
We are currentlydeveloping a multilingual summarization system, in which we will use the resultsfrom Champollion.
An early version of this system (McKeown and Radev 1995) pro-duces short summaries of multiple news articles covering the same event using asinput the templates produced by information extraction systems developed under theARPA message understanding program.
Since some information extraction systems,such as General Electric's NLToolset (Jacobs and Rau 1990), already produce similarrepresentations for Japanese and English news articles, the addition of an Englishsummary generator will automatically allow for English summarization f Japanese.In addition, we are planning to add a second language for the summaries.
While theoutput is not a direct translation of input articles, collocations that appear frequentlyin the news articles will also appear in summaries.
Thus, a list of bilingual collocationswould be useful for the summarization process.Information retrieval is another prospective application.
As shown in Maarek andSmadja (1989) and more recently in Broglio et al (1995), the precision of informationretrieval systems can be improved through the use of collocations in addition to themore traditional single word indexing units.
A collocation gives the context in whicha given word was used, whicl~ will help retrieve documents using the word withthe same sense and thus improve precision.
The well-known New Mexico example ininformation retrieval describes an oft-encountered problem when single word searchesare employed: searching for new and Mexico independently will retrieve a multitude ofdocuments that do not relate to New Mexico.
Automatically identifying and explicitlyusing collocations such as New Mexico at search or indexing time can help solve thisproblem.
We have licensed XTRACT to several sites that are using it to improve theaccuracy of their retrieval or text categorization systems.A bilingual list of collocations could be used for the development of a multilingualinformation retrieval system.
In cases where the database of texts includes documentswritten in multiple languages, the search query need only be expressed in one lan-guage.
The bilingual collocations could be used to translate the query (particularly30Computational Linguistics Volume 22, Number 1Table 10Some translations with closed class words produced by Champollion.English Collocation French Translation Found by Champollionamount of moneycapital gainsconsumer protectiondispute settlement mechanismdrug abuseemployment equityenvironmental protectionfederal sales taxsomme d' argentgains en capitalla protection des consommateursm6canisme de r6glement des diff6rends1' abus des drogues6quitd en mati~re d'emploiprotection de 1' environnementtaxe de vente f~deraleTools for the target language.
Tools in French, such as a morphological nalyzer, a tagger,a list of acronyms, a robust parser, and various lists of tagged words, would be mosthelpful and would allow us to improve our results.
For example, a tagger for Frenchwould allow us to run XTRACT on the French part of the corpus, and thus to translatefrom either French or English as input.
In addition, running XTRACT on the French partof the corpus would allow for independent confirmation of the proposed translations,which should be French collocations.
Similarly, a morphological nalyzer would allowus to produce richer results, since several forms of the same word would be conflated,increasing both the expected and the actual frequencies of the co-occurrence events;this has been found empirically to have a positive effect in overall performance inother problems (Hatzivassiloglou in press).
Note that ignoring inflectional distinctionscan sometimes have a detrimental effect if only particular forms of a word participatein a given collocation.
Consequently, it might be beneficial to take into account boththe distribution of the base form and the differences between the distributions of thevarious inflected forms.In the current implementation f Champollion, we were restricted to using tools foronly one of the two languages, ince at the time of implementation tools for Frenchwere not readily available.
However, from the above discussion it is clear that certaintools would improve the system's performance.Separating corpus-dependent translations from general ones.
Champollion identifies trans-lations for the source collocations using the aligned corpora database as its entireknowledge of the two languages.
Consequently, sometimes the results are specific tothe domain and seem peculiar when viewed in a more general context.
For example,we have already mentioned that Mr. Speaker was translated as Monsieur le Prdsident,which is obviously only valid for this domain.
Canadian family is another example; itis often translated as famille (the Canadian qualifier is dropped in the French version).This is an important feature of the system, since in this way the sublanguage of thedomain is employed for the translation.
However, many of the collocations that Cham-poUion identifies are general, domain-independent o es.
ChampoUion cannot make anydistinction between domain-specific and general collocations.
What is clearly neededis a way to determine the generality of each produced translation, as many transla-tions found by ChampoUion are of general use and could be directly applied to otherdomains.
This may be possible by intersecting the output of Champollion on corporafrom many different domains.32Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual LexiconsHandling low frequency collocations.
The statistics we used do not produce good resultswhen the frequencies are low.
This shows up clearly when our evaluation results on thefirst two experiments are compared.
Running the collocation set C2 over the databaseDB1 produced our worst results, and this can be attributed to the low frequencyin DB1 of many collocations in C2.
Recall that C2 was extracted from a different(and larger) corpus from DB1.
This problem is due not only to the frequencies ofthe source collocations or of the words involved but also to the frequencies of their"official" translations.
Indeed, while most collocations exhibit unique senses in a givendomain, sometimes a source collocation appearing multiple times in the corpus is notconsistently translated into the same target collocation in the database.
This samplingproblem, which generally affects all statistical approaches, was not addressed in thepaper.
We reduced the effects of low frequencies by purposefully limiting ourselvesto source collocations of frequencies higher than 10, containing individual words withfrequencies higher than 15.Analysis of the effects of our thresholds.
Various thresholds are used in Champollion's algo-rithm to reduce the search space.
A threshold too low would significantly slow downthe search as, according to Zipf's law (Zipf 1949), the number of terms occurring ntimes in a general English corpus is a decreasing function of n 2.
Unfortunately, some-times this filtering step causes Champollion to miss a valid translation.
For example,one of the incorrect translations made by Champollion is that important factor was trans-lated into facteur (factor) alone instead of the proper translation facteur important.
Theerror is due to the fact that the French word important did not pass the first step ofthe algorithm as its Dice coefficient with important factor was too low.
Important occursa total of 858 times in the French part of the corpus and only 8 times in the rightcontext, whereas a minimum of 10 appearances is required to pass this step.Although the theoretical nalysis and simulation experiments of Section 6.2 showthat such cases of missing the correct ranslation are rare, more work needs to bedone in quantifying this phenomenon.
In particular, experiments with actual corpusdata should supplement the theoretical results (based on uniform distributions).
Fur-thermore, more experimentation with the values of the thresholds needs to be done,to locate the optimum trade-off point between efficiency and accuracy.
An additionaldirection for future experiments is to vary the thresholds (and especially the frequencythreshold Tf) according to the size of the database corpus and the frequency of thecollocation being translated.Incorporating the length of the translation i to the score.
Currently our scoring method onlyuses the lengths of candidate translations to break a tie in the similarity measure.
Itseems, however, that longer translations should get a "bonus."
For example, using ourscoring technique the correlation of the collocation official languages with the Frenchword officielles i equal to 0.94 and the correlation with the French collocation languesofficielles i  0.95.
Our scoring only uses the relative frequencies of the events withouttaking into account that some of these events are composed of multiple single events.We plan to refine our scoring method so that the length (number of words involved)of the events is taken into account.Using nonparallel corpora.
Champollio n requires an aligned bilingual corpus as input.However, finding bilingual corpora can be problematic in some domains.
Althoughorganizations such as the United Nations, the European Community, and governmentsof countries with several official languages are big producers, such corpora are stilldifficult o obtain for research purposes.
While aligned bilingual corpora will become33Computational Linguistics Volume 22, Number 1more available in the future, it would be helpful if we could relax the constraintfor aligned data.
Bilingual corpora in the same domain, which are not necessarilytranslations of each other, are more easily available.
For example, news agencies uchas the Associated Press and Reuters publish in several languages.
News stories oftenrelate similar facts but they are not direct translations of one another.
Even thoughthe stories probably use equivalent erminology, totally different techniques wouldbe necessary to be able to use such "nonalignable" corpora as databases.
Ultimately,such techniques would be more useful than those currently used, because they wouldbe able to extract knowledge from noisy data.
While this is definitely a large researchproblem, our research team at Columbia University has begun work in this area (Fungand McKeown 1994) that shows promise for noisy parallel corpora (in which thetarget corpus may contain either additional or deleted paragraphs and where thelanguages themselves do not involve neat sentence-by-sentence translations).
Bilingualword correspondences xtracted from nonparallel corpora with techniques uch asthose proposed by Fung (1995a) also look promising.10.
ConclusionWe have presented a method for translating collocations, implemented in Champollion.The ability to provide translations for collocations i important for three main reasons.First, because they are opaque constructions, they cannot be translated on a word-by-word basis.
Instead, translations must be provided for the phrase as a whole.
Second,collocations are domain dependent.
Each domain includes a variety of phrases thathave specific meanings and translations that apply only in the given domain.
Finally,a quick look at a bilingual dictionary, even for two widely studied languages uchas English and French, shows that correspondences between collocations in two lan-guages are largely unexplored.
Thus, the ability to compile a set of translations for anew domain automatically will ultimately increase the portability of machine transla-tion systems.
By applying Champollion to a corpus in a new domain, translations forthe domain-specific collocations can be automatically compiled and inaccurate resultsfiltered by a native speaker of the target language.The output of our system is a bilingual list of collocations that can be used ina variety of multilingual applications.
It is directly applicable to machine translationsystems that use a transfer approach, since such systems rely on correspondences be-tween words and phrases of the source and target languages.
For interlingua systems,identification of collocations and their translations provide a means of augmentingthe interlingua.
Since such phrases cannot be translated compositionally, they indi-cate where concepts representing such phrases must be added to the interlingua.
Suchbilingual phrases are also useful for other multilingual tasks, including informationretrieval of multilingual documents given a phrase in one language, summarizationin one language of texts in another, and multilingual generation.Finally, we have carried out three evaluations of the system on three separate yearsof the Hansards corpus.
These evaluations indicate that Champollion has a high rate ofaccuracy: in the best case, 78% of the French translations of valid English collocationswere judged to be good.
This is a good score in comparison with evaluations carriedout on full machine translation systems.
We conjecture that by using statistical tech-niques to translate a particular type of construction, known to be easily observable inlanguage, we can achieve better results than by applying the same technique to allconstructions uniformly.Our work is part of a paradigm of research that focuses on the development of toolsusing statistical analysis of text corpora.
This line of research aims at producing tools34Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexiconsthat satisfactorily handle relatively simple tasks.
These tools can then be used by othersystems to address more complex tasks.
For example, previous work has addressedlow-level tasks such as tagging a free-style corpus with part-of-speech information(Church 1988), aligning a bilingual corpus (Gale and Church 1991b; Brown, Lai, andMercer 1991), and producing a list of collocations (Smadja 1993).
While each of thesetools is based on simple statistics and tackles elementary tasks, we have demonstratedwith our work on Champollion that by combining them, one can reach new levels ofcomplexity in the automatic treatment of natural anguages.AcknowledgmentsThis work was supported jointly by theAdvanced Research Projects Agency andthe Office of Naval Research under grantN00014-89-J-1782, by the Office of NavalResearch under grant N00014-95-1-0745, bythe National Science Foundation undergrant GER-90-24069, and by the New YorkState Science and Technology Foundationunder grants NYSSTF-CAT(91)-053 andNYSSTF-CAT(94)-013.
We wish to thankPascale Fung and Dragomir Radev forserving as evaluators, Thanasis Tsantilas fordiscussions relating to the average-casecomplexity of Champollion, and theanonymous reviewers for providing usefulcomments on an earlier version of thepaper.
We also thank Ofer Wainberg for hisexcellent work on improving the efficiencyof Champollion and for adding thepreposition extension, and Ken Church andAT&T Bell Laboratories for providing uswith a prealigned Hansards corpus.ReferencesBahl, Lalit R.; Brown, Peter E; de Souza,Peter V.; and Mercer, Robert L. (1986).Maximum Mutual Information of HiddenMarkov Model Parameters for SpeechRecognition.
In Proceedings, InternationalConference on Acoustics, Speech, and SignalProcessing (ICASSP-86), Tokyo, Japan, 1:49-52, IEEE Acoustics, Speech and SignalProcessing Society, Institute of Electronicsand Communication Engineers of Japan,and Acoustical Society of Japan.Benson, Morton (1985).
"Collocations andIdioms."
In Dictionaries, Lexicography, andLanguage Learning, edited by Robert Ilson.Pergamon Institute of English, Oxford,England, 61-68.Benson, Morton; Benson, Evelyn; and Ilson,Robert.
(1986).
The BBI CombinatoryDictionary of English: A Guide to WordCombinations.
John Benjamins, Amsterdamand Philadelphia.Berger, Adam L.; Brown, Peter F.; DellaPietra, Stephen A.; Della Pietra, Vincent J.;Gillet, John R.; Lafferty, John D.; Mercer,Robert L.; Printz, Harry; and Ureg, Lubog.(1994).
The Candide System for MachineTranslation.
In Proceedings, ARPA Workshopon Human Language Technology, Plainsboro,New Jersey, 157-162.
ARPA Software andIntelligent Systems Technology Office,Morgan Kaufmann, San Francisco,California.Broglio, John; Callan, James P.; Croft,W.
Bruce; and Nachbar, Daniel W. (1995).Document Retrieval and Routing Usingthe INQUERY System.
In Proceedings,Third Text Retrieval Conference (TREC-3),Gaithersburg, Maryland, 29-39.
NationalInstitute of Standards and Technology(NIST).Brown, Peter E; Cocke, John; Della Pietra,Stephen A.; Della Pietra, Vincent J.;Jelinek, Fredrick; Lafferty, John D.;Mercer, Robert L.; and Roosin, Paul S.(1990).
A Statistical Approach to MachineTranslation.
Computational Linguistics,16(2): 79-85.Brown, Peter F.; Lai, Jennifer C.; and Mercer,Robert L. (1991).
Aligning Sentences inParallel Corpora.
In Proceedings, 29thAnnual Meeting of the ACL, Berkeley,California, 169-184.
Association forComputational Linguistics.Brown, Peter E; Della Pietra, Stephen A.;Della Pietra, Vincent J.; and Mercer,Robert L. (1991).
Word-SenseDisambiguation Using StatisticalMethods.
In Proceedings, 29th AnnualMeeting of the ACL, Berkeley, California,264-270.
Association for ComputationalLinguistics.Brown, Peter E; Della Pietra, Stephen A.;Della Pietra, Vincent J.; and Mercer,Robert L. (1993).
The Mathematics ofStatistical Machine Translation: ParameterEstimation.
Computational Linguistics,19(2): 263--311.Budge, E. A. Wallis.
(1989).
The Rosetta Stone.Dover Publications, New York.
(Originally published as The Rosetta Stonein the British Museum, Religious TractSociety, London, 1929.
)Chen, Stanley F. (1993).
Aligning Sentencesin Bilingual Corpora Using Lexical35Computational Linguistics Volume 22, Number 1Information.
In Proceedings, 31st AnnualMeeting of the ACL, Columbus, Ohio, 9-16.Association for ComputationalLinguistics.Church, Kenneth W. (1988).
A StochasticParts Program and Noun Phrase Parserfor Unrestricted Text.
In Proceedings,Second Conference on Applied NaturalLanguage Processing (ANLP-88), Austin,Texas, 136-143.
Association forComputational Linguistics.Church, Kenneth W. (1993).
Char_align: AProgram for Aligning Parallel Texts at theCharacter Level.
In Proceedings, 31stAnnual Meeting of the ACL, Columbus,Ohio, 1-8.
Association for ComputationalLinguistics.Church, Kenneth W.; Gale, William A.;Hanks, Patrick; and Hindle, Donald.(1991).
Using Statistics in LexicalAnalysis.
In Lexical Acquisition: UsingOn-line Resources to Build a Lexicon, editedby Uri Zernik.
Lawrence Erlbaum,Hillsdale, New Jersey, 115-165.Church, Kenneth W. and Hanks, Patrick.(1990).
Word Association Norms, MutualInformation, and Lexicography.Computational Linguistics, 16(1): 22-29.Cover, Thomas M. and Thomas, Joy A.(1991).
Elements of Information Theory.Wiley, New York.Dagan, Ido and Church, Kenneth W. (1994).Termight: Identifying and Translating'Technical Terminology.
In Proceedings,Fourth Conference on Applied NaturalLanguage Processing (ANLP-94), Stuttgart,Germany, 34-40.
Association forComputational Linguistics.Dagan, Ido; Church, Kenneth W.; and Gale,William A.
(1993).
Robust Bilingual WordAlignment for Machine-AidedTranslation.
In Proceedings, Workshop onVery Large Corpora: Academic and IndustrialPerspectives, Columbus, Ohio, 1-8.Association for ComputationalLinguistics.Dagan, Ido and Itai, Alon.
(1994).
WordSense Disambiguation Using a SecondLanguage Monolingual Corpus.Computational Linguistics, 20(4): 563-596.Dagan, Ido; Itai, Alon; and Schwall, Ulrike.(1991).
Two Languages Are MoreInformative Than One.
In Proceedings, 29thAnnual Meeting of the ACL, Berkeley,California, 130-137.
Association forComputational Linguistics.Dagan, Ido; Marcus, Shaul; and Markovitch,Shaul.
(1993).
Contextual Word Similarityand Estimation from Sparse Data.
InProceedings, 31st Annual Meeting of the ACL,Columbus, Ohio, 164-171.
Association forComputational Linguistics.Dice, Lee R. (1945).
Measures of theAmount of Ecologic Association betweenSpecies.
Journal of Ecology, 26: 297-302.Dorr, Bonnie J.
(1992).
The Use of LexicalSemantics in Interlingual MachineTranslation.
Machine Translation, 7(3):135-193.van der Eijk, Pim.
(1993).
Automating theAcquisition of Bilingual Terminology.
InProceedings, Sixth Conference ofthe EuropeanChapter of the Association for ComputationalLinguistics, Utrecht, The Netherlands,113-119.
Association for ComputationalLinguistics.Frakes, William B. and Baeza-Yates, Ricardo,eds.
(1992).
Information Retrieval: DataStructures and Algorithms.
Prentice Hall,Englewood Cliffs, New Jersey.Fung, Pascale.
(1995a).
Compiling BilingualLexicon Entries from a Non-ParallelEnglish-Chinese Corpus.
In Proceedings,Third Annual Workshop on Very LargeCorpora, Boston, Massachusetts, 173-183.Fung, Pascale.
(1995b).
A Pattern MatchingMethod for Finding Noun and ProperNoun Translations from Noisy ParallelCorpora.
In Proceedings, 33rd AnnualMeeting of the ACL, Boston, Massachusetts,236-243.
Association for ComputationalLinguistics.Fung, Pascale and McKeown, Kathleen R.(1994).
Aligning Noisy Parallel CorporaAcross Language Groups: Word PairFeature Matching by Dynamic TimeWarping.
In Proceedings, First Conference ofthe Association for Machine Translation in theAmericas (AMTA), Columbia, Maryland,81-88.Gale, William A. and Church, Kenneth W.(1991a).
Identifying WordCorrespondences in Parallel Texts.
InProceedings, DARPA Speech and NaturalLanguage Workshop, Pacific Grove,California, 152-157.
Morgan Kaufmann,San Mateo, California.Gale, William A. and Church, Kenneth W.(1991b).
A Program for AligningSentences in Bilingual Corpora.
InProceedings, 29th Annual Meeting of the ACL,Berkeley, California, 177-184.
Associationfor Computational Linguistics.Gale, William A. and Church, Kenneth W.(1993).
A Program for Aligning Sentencesin Bilingual Corpora.
ComputationalLinguistics, 19(1): 75-102.Hatzivassiloglou, Vasileios.
(in press).
"DoWe Need Linguistics When We HaveStatistics?
A Comparative Analysis of theContributions of Linguistic Cues to aStatistical Word Grouping System."
In The36Computational Linguistics Volume 22, Number 1Approach to Automatic CompoundExtraction.
In Proceedings, 32nd AnnualMeeting of the ACL, Las Cruces, NewMexico, 242-247.
Association forComputational Linguistics.Wu, Dekai and Xia, Xuanyuin.
(1994).Learning an English-Chinese L xiconfrom a Parallel Corpus.
In Proceedings,First Conference ofthe Association forMachine Translation i  the Americas (AMTA),Columbia, Maryland, 206-213.Yarowsky, David.
(1993).
One Sense PerCollocation.
In Proceedings, ARPAWorkshop on Human Language Technology,Plainsboro, New Jersey, 266-271.
ARPASoftware and Intelligent SystemsTechnology Office, Morgan Kaufmann,San Francisco, California.Zipf, George K. (1949).
Human Behavior andthe Principle of Least Effort: An Introductionto Human Ecology.
Addison-Wesley,Reading, Massachusetts.38
