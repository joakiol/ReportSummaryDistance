Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 285?288,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUtility Evaluation of Cross-document Information ExtractionHeng Jia, Zheng Chena, Jonathan Feldmana, Antonio Gonzaleza, Ralph Grishmanb, Vivek UpadhyayaaComputer Science Department, Queens College and the Graduate Center, City University of New YorkNew York, NY 11367, USAbComputer Science Department, New York University, New York, NY 10003, USAhengji@cs.qc.cuny.edu, zchen1@gc.cuny.edu, agonzalez117@qc.cuny.edu, grishman@cs.nyu.edu,vivekqc@gmail.comAbstractWe describe a utility evaluation to determinewhether cross-document information extrac-tion (IE) techniques measurably improve userperformance in news summary writing.
Twogroups of subjects were asked to perform thesame time-restricted summary writing tasks,reading news under different conditions: withno IE results at all, with traditional single-document IE results, and with cross-documentIE results.
Our results show that, in compari-son to using source documents only, the qual-ity of summary reports assembled using IEresults, especially from cross-document IE,was significantly better and user satisfactionwas higher.
We also compare the impact ofdifferent user groups on the results.1 IntroductionInformation Extraction (IE) is a task of identifying??facts??
(entities, relations and events) within un-structured documents, and converting them intostructured representations (e.g., databases).
IEtechniques have been effectively applied to differ-ent domains (e.g.
daily news, Wikipedia, biomedi-cal reports, financial analysis and legaldocumentations) and different languages.
Recentlywe described a new cross-document IE task (Ji etal., 2009) to extract events across-documents andtrack them on a time line.
Compared to traditionalsingle-document IE, this new task can extract moresalient, accurate and concise event information.However, a significant question remains: willthe events extracted by IE, especially this newcross-document IE task, actually help end-users tomake better use of the large volumes of news?
Inorder to investigate whether we have reached thisgoal, we performed an extrinsic utility (i.e., use-fulness) and usability evaluation on IE results.Two groups of subjects were asked to perform thesame time-restricted summary writing tasks, read-ing news under different conditions: with no IEresults at all, with traditional single-document IEresults, and with cross-document IE results.
Ourresults show that, in comparison to using sourcedocuments only, the quality of summary reportsassembled using IE techniques, especially fromcross-document IE, was significantly better.
Also,as extraction quality increases from no IE at all tosingle-document IE and then to cross-document IE,user satisfaction increases.
We also compare theimpact of different user groups on the results.
Tothe best of our knowledge, this is the first system-atic evaluation of cross-document IE from a us-ability perspective.2 Overview of IE SystemsWe applied the English single-document IE system(Ji and Grishman, 2008) and cross-document IEsystem presented in (Ji et al, 2009).
Both systemswere developed for the ACE program1.The single-document IE system can extractevents from individual documents.
The core stagesinclude entity extraction, time expression extrac-tion and normalization, relation extraction andevent extraction.
Events include the 33 distincttypes defined in ACE05.
The extraction results arepresented in tabular form.The cross-document IE system can identify im-portant person entities which are frequently in-1 http://www.itl.nist.gov/iad/mig/tests/ace/2005/285volved in events as ?
?centroid entities??
; and then foreach centroid entity, link and order the events cen-tered around it on a time line and associate them toa geographical map.
The event chains are pre-sented in a user-friendly graphical interface (Ji andChen, 2009).
Both systems link the events back totheir context documents.3 Evaluation Methods3.1 Study ExecutionOur measurement challenge is to assess how IEtechniques affect users??
abilities to perform real-world tasks.
We followed the summary writingtask described in the Integrated Feasibility Ex-periment of the DARPA TIDES program (Colbathand Kubala, 2003) and the daily task conducted byintelligence analysts (Bodnar, 2003).
Each task inour evaluation is based on writing a summary ofACE-type events involving a specific centroid en-tity, using one of three levels of support:?
Level (I): Read the news articles, with assistanceof keyword based sentence search;?
Level (II): (I) + with assistance from single-document IE results;?
Level (III): (I) + with assistance from cross-document IE results.The summary writing task for each entity usingany level should be finished in 10 minutes.
Theusers can choose to trust the IE results to createnew sentences or select relevant sentences fromthe source documents.
The IE systems were ap-plied to a corpus of 106 articles from ACE 2005training data.3.2 Summary ScoringWe measure user responses in three aspects:?
Observer-based Quantity -- How many sen-tences are extracted in each summary?
Howmany of them are uniquely correct??
Observer-based Quality-- How fluent and coher-ent are the sentences in each summary??
User-based Usability -- How does the user feelabout the system?3.3 User Group SelectionWe selected user groups based on the principlesthat we should run as many tests as we can afford(Nielsen, 1994), and at least 5 to insure that wedetect any major usability problems (Faulkner,2003).
Two different groups of users were asked toconduct the evaluation:(1) Hallway EvaluationWe chose the first group of users with a ??HallwayTesting??
user-study method described in (Nielsen,1994).
We randomly asked 11 PhD students in thefield of natural language processing to conduct theevaluation.
In order to evaluate these three levelsindependently, each student was asked to write atmost one summary, using one of the three levels,for any single centroid entity.
To avoid the impactof diverse text comprehension abilities, each stu-dent was involved in all of these three levels fordifferent centroid entities.
(2) Remote EvaluationAn effective utility evaluation will require userswith a diversity of prior knowledge and computerexperience.
Therefore we asked the second groupof 11 users in a remote usability testing mode(Hammontree et al, 1994).
We sent out the requestto university-wide undergraduate student mailinglists and found 11 users to work on the evaluation.The evaluation procedure follows the HallwayTesting method, except that the tests are carriedout in the user?
?s own environment (rather than labs)helping further simulate real-life scenario testing.Also the users didn?
?t meet with the observers andthus they were not aware of any expectations forresults.4 Evaluation ResultsIn this section we will focus on reporting the re-sults from Hallway Evaluation, while providingcomparisons with Remote Evaluation.4.1 Observer-based QuantityThe summaries were judged by two annotators andthe judgements reconciled.
A summary sentence isjudged as uniquely correct if it: (1) includes rele-vant events involving the centroid entity; and (2)the same information was not included in previoussentences in the current summary.
This metric canbe considered as an approximate com bination ofthe ?
?content responsiveness?
?, ??non-redundancy?
?and ??focus??
criteria in the NIST TACsummarization track2.
Table 1 presents the2http://www.nist.gov/tac/2009/Summarization/update.summ.09.guidelines.html286Cen-troid(I) (II) (III) Cen-troid(I) (II) (III) Cen-troid(I) (II) (III)Bush 3/1/0 5/1/2 6/0/0 Al-douri 4/3/3 4/2/0 6/0/1 Ba?
?asyir 3/1/0 3/0/0 5/0/0Ibrahim 4/0/1 5/0/0 8/0/0 Giuliani 2/0/0 3/2/0 5/0/0 Erdogan 1/0/1 4/0/0 4/0/0Toefting 0/0/0 7/1/0 4/0/0 Blair 2/0/1 3/0/0 5/0/0 Diller 3/0/0 4/1/0 3/0/0Putin 2/1/0 4/3/2 7/1/1 Pasko 3/0/0 3/0/0 2/0/0 Overall 27/6/6 45/10/5 55/1/2Table 1.
# (uniquely correct sentences)/ #(redundant correct sentences)/#(spurious sentences) in a summary in Hallway Evaluationquantified Hallway Testing results for each cen-troid separately and the overall score.
It shows thatoverall Level (II) contained 18 more correct sen-tences than the baseline (I), while (III) achieved 11further correct sentences.
(I) obtained significantlyfewer sentences without assistance from IE tools.We conducted the Wilcoxon Matched-PairsSigned-Ranks Test on a query entity basis for ac-curacy - number of (uniquely correct sen-tences)/number of (total extracted sentences in asummary).
The results show that (III) is signifi-cantly better than (I) at a 99.2% confidence level,and better than (II) at a 96.9% confidence level.
(II)is not significantly better than (I).We can also see that for some centroid entitiessuch as ??Putin?
?, ??Al-douri??
and ??Giuliani?
?, (II)generated more sentences but also introduced moreredundant information.
The user feedback has in-dicated that they did not have enough time to re-move redundancy.
In contrast, (III) yielded muchless redundant information.
In fact, the averagetime the users spent using (III) was only about 7.2minutes.
Therefore we can conclude that cross-document IE can produce more informative sum-maries in a more efficient way.Error analysis showed that the major error typespropagated from IE to summaries are as follows.1.
Event time errors.
For example, the summarysentence ?
?Toefting was convicted in September2001 of assaulting a pair of restaurant workers inthe capital??
was judged as incorrect because thetime argument should be ?
?October 2002??.2.
Pronoun resolution errors.
When a pronoun ismistakenly linked to an entity, incorrect event ar-guments will be included in the summaries.3.
Event type errors.
When an event is mis-classified, the users tend to use incorrect templatesand thus generate wrong summaries.4.
Negative events.
Sometimes the event attrib-ute classifier makes mistakes and the users includenegative events in the summaries.4.2 Impact of User GroupsIn the Remote Testing, the accuracy results fromthe three levels are as follows: 21/37, 28/37 and31/36.
Thus both user groups benefited from usingIE techniques, but the enhancements vary a lot.
Inthe Hallway Testing, the users were better trainedand more familiar with IE tools (including thegraphical interface of cross-document IE); and thusthey can benefit more from the IE techniques.
Incontrast, in the Remote Evaluation, the users hadquite diverse knowledge backgrounds.
For exam-ple, one remote user was only able to find 1-2 sen-tences using any of the three levels; while another,more skilled remote user found more than 5 sen-tences with any level.
However the RemoteEvaluation is important to gather the feedback ofthe more subjective usability evaluation in section4.4.
Because the users in Hallway Testing may beaware of the observations that the observer is hop-ing to achieve, they may provide potentially biasedfeedback.4.3 Observer-based QualityThe evaluation also showed that (III) producedsummaries with better quality.
We asked the ob-servers to give a score between [1, 10] to eachsummary according to the following TAC summa-rization quality criteria: Readability/Fluency, Ref-erential Clarity and Structure/Coherence.
Table 2shows the evaluation results for the three differentmethods.Criteria (I) (II) (III)Readability/Fluency 9.4 8.5 8.2Referential Clarity 6.1 8.3 8.7Structure/Coherence 7.1 7.6 8.5Table 2.
Observer-based Average QualityIn their detailed feedback, the users indicatedthat (III) has the following advantages: (1) Better287Cen-nt-r eoindt (n-Ir )Bur sneor hn3Cdo or /-1r /hht0e/ or  o3Cne/dr ne1oer 5oh/tior )222ur6/-r eohnAoer t-0l-n4-r  (3or /eat3o- ir ti(-ar henii01nht3o- r(-?
?oeo-hoyr)bur6/-rao-oe/ or/5i e/h (Aorit33/e(oiyrmner  8or 5(nae/C8(h/dr oAo- ir )oyayr o3CdnG3o- uErin3or tioeir 4oeor /5dor  nr tior iCoh(??
(hr  o3Cd/ oirith8r/irgTf7r4/ir8(eo1r5GrD7Pr/ rk2sf.r nr4e( orit33/e(oiyr mner o#/3CdoEr /r io- o-hor gqti8r /-1rqd/(er3o r/ r6/3Crc/A(1r/-1r 8orpHr 8eoor (3oir(-rs/eh8r Bwwb.r 4/ir 1oe(Ao1r ?
?en3r  8eoor 1(???
?oeo- rg6n- /h 0soo (-a.r oAo- ir (-r  8oroAo- r h8/(-iyr )vur6/-r hn--oh r eod/ o1r oAo- ir (- nr 3neor hn-h(iorit33/e(oiyrmnero#/3CdoErioAoe/droAo- ir4oeorhn-0-oh o1r nrao-oe/ or  8or?
?nddn4(-ario- o-hoirgT/ilnr4/ir /CCo/do1r?
?ner  eo/in-rhe(3orn-rLCe(dr ,WErBwwbrand thenreodo/io1rn-rxt-or,MErBwwb.yrk8oreo/1/5(d0( Gr ihneoir (-rk/5dorBr /dinr (-1(h/ or  8/ r /r3neoro??0?
?oh (Aor  o3Cd/ or ao-oe/ (n-r 3o 8n1r i8ntd1r 5or1oAodnCo1r nrCen1thor3neor?
?dto- rit33/e(oir5/io1rn-r2freoitd iyr4.4 User-based Usabilityk8or tioer ?
?oo15/hlr ?
?en3r 5n 8r oA/dt/ (n-ir /dinri8n4o1r  8/ r )22ur /-1r )222ur eoitd ir 4oeor  eti o1r /d03ni roSt/ddGEr/-1r)222ur4/irhd/(3o1r nrCenA(1or 8or3ni r tio?
?tdr ?
?t-h (n-iyr k8or Cni( (Aor hn33o- ir/5nt r)222ur(-hdt1orgko3Cne/drR(-l(-ar/ddn4irdna(0h/dreo/in-(-ar/-1rao-oe/d(9/ (n-.Erg6o- en(1rio/eh8r8odCir  nr ?
?nhtir (33o1(/ odG.Er g%C/ (/drR(-l(-ar /d0dn4ir  nr 5en4ior /ddr  8orCd/hoir 48(h8r /rCoein-r 8/irA(i( o1.Er g?
?/3or 1(i/35(at/ (n-r 8odCir  nr ??
(d oer (e0eodoA/- r (-?
?ne3/ (n-.Er g6/-r ??
(-1r loGr (-?
?ne3/ (n-r?
?en3r oAo- r h8/(-i.Er gk(3od(-or 8odCir hneeod/ oroAo- i.Ir /-1r  8or -oa/ (Aor hn33o- ir (-hdt1org%n3o (3oir 2fr oeeneir 3(ido/1r dnh/ (-ar  8or io-0o-hoi.Erg?
?nritCCne rn??r-/3orC/(erio/eh8r?
?ner3oo 0(-ar oAo- i.Erg?
?nrhndnero3C8/i(ir n?
?r oAo- ir n-r  8orne(a(-/dr 1nht3o- i.r /-1r g?
?nr itaaoi (n-ir n?
?r  o30Cd/ oir nrhn3Cniorit33/eGrio- o-hoi.yrr5 Conclusion and Future Workk8enta8r /r t (d( Gr oA/dt/ (n-r n-r it33/eGr 4e( (-ar4or 8/Aor CenAo1r  8/ r 2fr  oh8-(StoiEr oiCoh(/ddGrhenii01nht3o- r2fErh/-r/(1r-o4ir5en4i(-aErio/eh8r/-1r/-/dGi(iyr2-rC/e (htd/eEr o3Cne/droAo- r e/hl(-ar/heniir 1nht3o- ir 8odCir tioeir Coe?
?ne3r 5o  oer / r?
?/h 0a/ 8oe(-ar  8/-r  8oGr1nr4( 8nt r2fyrpioeir /dinrCen1tho1r3neor(-?
?ne3/ (Aorit33/e(oir4( 8rhenii01nht3o- r2fr 8/-r4( 8r e/1( (n-/dri(-ado01nht3o- r2fyr?
?or/dinrhn3C/eo1r/-1r/-/dG9o1r 8or1(???
?oeo-hoir5o 4oo-r  4nr tioer aentCiyr %th8r 3o/iteoir n?
?r  8or5o-o??
( ir  nr  8or oAo- t/dr o-1r tioeir /dinr CenA(1o1r?
?oo15/hlrn-r48/ r4nelir4oddr /-1r (1o- (??
(o1r /11(0(n-/dr eoio/eh8r Cen5do3iEr ith8r /ir  nr o#C/-1r  8orho- en(1r  nr /rC/(ern?
?ro- ( (oir /-1r  nrCenA(1orhn-??
(01o-hor3o e(hir(-r 8or(- oe?
?/hoyr2-r 8or?
?t teor4or/(3rnrio rtCr/-rn-d(-or-o4ir/e (hdor/-/dGi(iriGi o3r/-1rCoe?
?ne3rd/eaoer/-1reoatd/ert (d( GroA/dt/ (n-iyrrAcknowledgementk8(ir 4nelr 4/ir itCCne o1r 5Gr  8or py%yr ?
?%mr6L7ff7r L4/e1r t-1oer Pe/- r 22%0wjMb,vjEr  8orpy%yr Le3Gr 7oio/eh8r R/5ne/ neGr t-1oer 6nnCoe/0(Aor Laeoo3o- r ?
?t35oer ??j,,?
?m0wj0B0wwMbErPnnadoEr 2-hyEr 6p?
?Fr 7oio/eh8r f-8/-ho3o- r Ten0ae/3Erm/htd GrTt5d(h/ (n-rTenae/3r/-1rP7k2rTen0ae/3yrk8orA(o4ir/-1rhn-hdti(n-irhn- /(-o1r(-r 8(ir1nht3o- r /eor  8niorn?
?r  8or /t 8neir /-1r i8ntd1r -n r5or(- oeCeo o1r/ireoCeoio- (-ar  8orn????
(h(/drCnd(h(oiEro( 8oero#Ceoiio1rner(3Cd(o1Ern?
?r 8orLe3Gr7oio/eh8rR/5ne/ neGr ner  8or py%yr PnAoe-3o- yr k8or py%yrPnAoe-3o- r (ir /t 8ne(9o1r  nr eoCen1thor /-1r 1(i0e(5t oreoCe(- ir ?
?nerPnAoe-3o- rCteCnioir -n 4( 80i /-1(-ar/-GrhnCGe(a8 r-n / (n-r8oeorn-yrReferencesxn8-r ?
?yr qn1-/eyr Bwwbyr ?
?/e-(-ar L-/dGi(ir ?
?ner  8or 2-0?
?ne3/ (n-rLaoOr7o 8(-l(-ar  8or 2- odd(ao-horTenhoiiyrCenter for Strategic Intelligence Research, JointMilitary Intelligence CollegeEr?
?/i8(-a n-Ercy6yr%o/-r 6nd5/ 8r /-1rme/-h(ir Ht5/d/yrBwwbyr kLT0NROrL-rLt n3/ o1r L-/dGi :ir Lii(i /- yr Proc.
HLT-NAACL2003 (demonstrations)yrR/te/rm/tdl-oeyrBwwbyrqoGn-1r 8or??
(Ao0tioer/iit3C (n-Orqo-o??
( irn?
?r(-heo/io1ri/3Cdori(9oir(-rti/5(d( Gr oi (-ayrBehavior Research Methods Instruments and Com-putersrbM)buErb;j0b[byrsn- Gr ]/33n- eooEr T/tdr ?
?o(doer /-1r ?
?/-1(-(r ?
?/G/lyr,jjvyr 7o3n or pi/5(d( Gr koi (-ayr Interactionsyr znd0t3or,Er2iitorbyrT/aoiOrB,0BMyr]o-arx(r /-1rQ8o-ar68o-yrBwwjyr6enii01nht3o- rko30Cne/dr /-1r %C/ (/dr Toein-r ke/hl(-ar %Gi o3r co3n-0i e/ (n-yrProc.
HLT-NAACL 2009yr]o-ar x(Er 7/dC8r Pe(i83/-Er Q8o-ar 68o-r /-1r Te/i8/- rPtC /yr Bwwjyr 6enii01nht3o- r fAo- r f# e/h (n-Er7/-l(-ar /-1r ke/hl(-ayr Proc.
Recent Advances inNatural Language Processing 2009.rx/ln5r ??
(odio-yr ,jjvyr pi/5(d( Gr f-a(-ooe(-ayr snea/-rH/t?
?3/--rTt5d(i8oeiyr288
