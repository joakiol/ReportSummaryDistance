A Uniform Architecture for Parsing and GenerationStuart M. SHIEBERArtificial Intelligence CenterSRI InternationalMenlo Park, California, USA*Abst rac tThe use of a single grammar for both parsing and generationis an idea with a certain elegance, the desirability of whichseveral researchers have noted.
In this paper, we discuss amore radical possibility: not only can a single grammar beused by different processes engaged in various "directions" ofprocessing, but one and the same language-processing archi-tecture can be used for processing the grammar in the variousmodes.
In particular, parsing and generation can be viewedas two processes engaged in by a single parameterized theo-rem pr6ver for the logical interpretation f the formalism.
Wediscuss our current implementation f such an architecture,which is parameterized in such a way that it can be used foreither purpose with grammars written in the PATR formal-ism.
Furthermore, the architecture allows fine tuning to re-flect different processing strategies, including parsing modelsintended to mimic psycholinguistic phenomena.
This tuningallows the parsing system to operate within the same realmof efficiency as previous architectures for parsing alone, butwith much greater flexibility for engaging in other processingregimes.1 Introductionthe use of a single grammar for both parsing and generation is an idea~ith a certain elegance, the desirability of which several researchersnave noted.
Of course, judging the correctness of such a system re-quires a characterization f the meaning of grammars that is indepen-dent of their use by a particular processing, mechanism--that is, thebrmalism in which the grammars are expressed must have an abstract~emantics.
As a paradigm example of such a formalism, we might take~ny of the various logic- or unification-based grammar formalisms.As described by Pereira and Warren \[1983\], the parsing of strings~ccording to the specifications ofa grammar with an independent log-cal semantics can be thought of as the constructive proving of the;tring's grammaticality: parsing can he viewed as logical deduction.-3ut, given a deductive framework that can represent the semanticsff the formalism abstractly enough to be independent of processing,he generation of strings matching some criteria can equally well behought of as a deductive process, namely, a process of constructive~roof of the existence of a string that matches the criteria.
The dif-erence rests in which information is given as premises and what the~oal is to be proved.
This observation opens up the following possi-bility: not only can a single grammar be used by different processes~ngaged in various "directions" of processing, but one and the sameanguage-processing architecture can be employed for processing thegrammar in the various modes.
In particular, parsing and generatioa:an be viewed as two processes engaged in by a single parameterized;heorem prover for the logical interpretation of the formalism.We will discuss our current implementation f such an architecture,~hich is parameterized in such a way that it can be used either for~arsing or generation with respect o grammars written in a particular~rammar formalism which has a logical semantics, the PATR formal-sm.
Furthermore, the architecture allows fine tuning to reflect differ-mt l:':ocessing strategies, including parsing models intended to mimic)s~'cholinguistiC phenomena.
This tuning allows the parsing system to)perate within the same realm of efficiency as previous architecturesor parsing alone, but with much greater flexibility for engaging in,ther processing regimes.
*This research was sponsored by the Nippon Telegraph and Telephone Corpo-ation under a contract with SRI International.2 Language Processing as DeductionViewed intuitively, natural-language-utterance gen ration is a nonde-terministic top-down process of building a phrase that conforms tocertain given criteria, e.g., that the phrase be a sentence and thatit convey a particular meaning.
Parsing, on the other hand, is usu-ally thought of as proceeding bottom-up in an effort to determine whatproperties hold of a given expression.
As we have mentioned, however,both of these processes can be seen as variants of a single method forextracting certain goal theorems from the deductive closure of somegiven premises under the rules or constraints of the grammar.
Thevarious processes differ as to what the premises are and which goaltheorems are of interest.
In generation~ for instance, the premises arethe lexical items of the language and goal theorems are of the form"expression a is a sentence with meaning M" for some given M. Inparsing, the premises are the words a of the sentence to be parsedand goal theorems are of the form "expression a is a sentence (withproperties P)".
In this case, a is given a priori.This deductive view of language processing clearly presupposes anaxiomatic approach to language description.
Fortunately , most cur-rent linguistic theory approaches the problem of linguistic descriptionaxiomatically, and many current formalisms in use in natural-languageprocessing, especially the logic grammar and unification-based for-malisms follow this approach as well.
Consequently, the results pre-sented here will, for the most part, be applicable to any of theseformalisms.
We will, however, describe the system schematically--without relying on any of the particular formalisms, but using notationthat schematizes an augmented context-free formalism like definite-clause grammars or PATR.
We merely assume that grammars classifyphrases under a possibly infinite set of structured objects, as is com-mon in the unification-based formalisms.
These structures--terms indefinite-clause grammars, directed graphs in PATt~, and so forth--willbe referred to generically as nonterminais, ince they play the role inthe augmented context-free formalisms that the atomic nonterminalsymbols fulfill in standard context-free grammars.
We will assumethat the notion of a unifier of such objects and most general unifier(mgu) are well defined; the symbol 0 will be used for unifiers.Following Pereira and Warren, the lemmas we will be proving from agrammar and a set of premises will include the same kind of conditionalinformation encoded by the items of Earley's parsing algorithm.
InEarley's algorithm, the existence of an item (or dotted rule) of theformin state set j > i makes a claim that, for some string position k > j ,the  substring between i and k can be classified as an N if the sub-str!ng between j and k can be decomposed into a sequence of stringsclassified, respectively, under Vm,..., V~.
We will use a notation rem-iniscent of Pereira and Warren's t to emphasize the conditional natureof the claim and its independence from V1,.. .
,  V,n-1, namely,\[i,N *-- V,~.. 'Vn,j\]2.1 Termino logyWe digress here to introduce some terminology.
If n = 0, then we willleave off the arrow; \[i, N,j\] then expresses the fact that a constituentadmitted as a nonterminal N occurs between positions i and j .
Suchitems will be referred to as nonconditional items; if n > 0, the itemwill be considered conditional.
In the grammars we are interested in~rules will include either all nonterminals on the right-hand side or a~Iterminals.
We can think of the former as grammar rules proper, theXLater, in the sections containing examples of the architecture's operation, wewill reintroduce V1,...,  Vm-1 and the dot marker to aid readability.d14latter as lexical entries.
Nonconditional i tems formed by immediateinference from a lexieal entry will be called lexical items.
For instance,if there is a grammar ule NP ---, sonny, then the item \[0, NP,  1\] isa lexical item.
A prediction item (or, simply, a prediction) is an itemwith identical start and end positions.2.2 Ru les  of  In fe renceThe two basic deduction steps or rules of inference we will use are--following Earley's terminology--prediction a d completion.
2The inference rule of prediction is as follows:\[ i ,A *- BC1.
.
'Cm, j \ ]  B'  -+ Dr ."
"Dn 0 = mgu(B ,B ' )\[j, B'O ~- DxO.. .
D,O, j\]This rule corresponds to the logically valid inference consisting of in-stantiating a rule of the grammar as a conditional statement.
3The inference rule of completion is as follows:\[i,A , -BC1.
.
"Cm,j\] Li, B' ,k\]  0 = mgu(B ,B ' )\[i, AO *-- C~O.. .C~O,k\]This rule corresponds to the logically valid inference consisting of lin-ear resolution of the conditional expression with respect o the non-conditional (unit) lemma.3 Parameter i z ing  a Theorem-ProvingArchitectureThis characterization f parsing as deduction should be familiar fromthe work of Pereira and Warren.
As they have demonstrated, sucha view of purging is applicable beyond the context-free grammars byregarding the variables in the inference rules as logical variables andusing unification of B and B t to solve for the most general unifier.Thus, this approach is applicable to most, if not all, of the logic gram-mar or unification-based formalisms.In particular, Pereira and Warren construct a parsing algorithm us-ing a deduction strategy which mimics Earley's algorithm.
We wouldlike to generalize the approach, so that the deduction strategy (or atleast portions of it) are parameters of the deduction system.
The pa-rameterization should have sufficient generality that parsers and gen-erators with w~rious control strategies, including Pereira and Warren'sBarley deduction parser, are instances of the general architecture.We start the development of such an architecture by consideringthe unrestricted use of these two basic inference rules to form thedeductive closure of the premises and the goals.
The exhaustive useof prediction and completion as basic inference rules does provide acomplete algorithm for proving lemmas of the sort described.
However,several problems immediately present hemselves.First, proofs using these inference rules can be redundant.
Variouscombinations of proof steps will lead to the same lemmas, and com-binatorial havoc may result.
The traditional solution to this problemis to store lemmas in a table, i.e., the well-formed-substring table orchart in tabular parsing algorithms.
In extending tabular parsing tonon-context-free formalisms, the use of subsumption rather than iden-tity in testing for redundancy of lemmas becomes necessary, and hasbeen described elsewhere \[Pereira nd Shieber, 1987\].Second, deduction is a nondeterministic process and the order ofsearching the various paths in the proof space is critical and differsamong processing tasks.
We therefore parameterize the theorem-proving process by a priority function that assigns to each lemmaa priority.
Lemmas are then added to the table in order of their pri-ority.
As they are added, furtlmr lemmas that are consequences of the2Pereira nd Warren use the terms insfantiation and reduction for their analogsto these rules.3As Jted previously \[Shieber, 1985\], this rule of inference can lead to arbitrarynumbers of cousequents through repeated application when used with a grammarformalism with an infinite \[structured\] nonterminal domain.
The solution proposedin that paper is to restrict the information passed from the predicting to the pre-dicted item, corresponding to the rule\[i,A 4- BC~ ?..Cry,j\] B' .--* Da .?.
Dn 0 = mgu(B~,  B')\[j, B'O ~ DIO'" DuO,j\]where B~ is a aonterminal with a bounded subset of the information of B. Thisinference rule is the one actually used in the implemented system.
The reader isdirected to the earlier paper for further discussion.new lemma and existing ones in the table may be deduced?
These arethemselves assigned priorities, and so forth.
The technique chosen forimplementing this facet of the process is the use of an agenda struc-tured as a priority queue to store the lemmas that have not yet beenadded to the table.Finally, depending on the kind of language processing we are inter-ested in, the premises of the problem and the types of goal lemmaswe are searching for will be quite different.
Therefore, we parameter-ize the theorem prover by an initial set of axioms to be added to theagenda nd by a predicate on lemmas that determines which are tobe regarded as satisfying the goal conditions on lemmas.The structure of the architecture, then, is as follows.
The processoris an agenda-based tabular theorem prover over lemmas of the sortdefined above.
It is parameterized bya The initial conditions,u A priority function on temmas, and?
A predicate xpressing the concept of a successful prooflBy varying these parameters, the processor can be used to implementlanguage parsers and generators embodying a wide variety of controlstrategies.4 Instances of the ArchitectureWe now define some examples of the use of the architecture to processwith grammars.4.1 Parser  Ins tancesConsider a processor to parse a given string built by using this archi-tecture under the following parameterization:?
The initialization of the agenda includes axioms for each wordin the string (e.g., \[O, sonny, 1\] and \[1,1eft,2\] for the sentence'Sonny left') and an initial prediction for each rule whose left-hand side matches the start symbol of the grammar (e.g., \[0, S ~-NP VP, 0\]).
4?
The priority function orders lemmas inversely by.
their end posi-tion, and for lemmas with the same end position, in accordancewith their addition to the agenda in a first-in-first-out manner.?
The success criterion is that the lemma be nonconditional, that itsstart and end positions be thefirst and last positions in the string,respectively, and that the nonterminal be the start nonterminal, sUnder this parameterization, the architecture mimics Earley's algo-rithm parsing the sentence in question, and considers uccessful thoselemmas that represent proofs of the string's grammaticality with re-spect to the grammar, sAlternatively, by changing the priority function, we can engenderdifferent parsing behavior.
For instance, if we just order lemmas ina last-in-first-out manner (treating the agenda s a stack) we have a"greedy" parsing algorithm, which pursues parsing possibilities depth-first and backtracks when dead-ends occur.An interesting possibility involves ordering lemmas as follows:?
1.
Highest priority are prediction items, then lexical items, thenother conditional items, then other nonconditional items.2.
If (1) does not order items, items ending farther to the right havehigher priority?3.
If (1) and (2) do not order items, items constructed from theinstantiation of longer rules have higher priority.This complex ordering implements a quite simple parsing strategy.The first condition guarantees that no nonconditional items will beadded until conditional items have been computed?
Thus, items cor-responding to the closure (in the sense of LI~ parsing) of the non-conditional items are always added to the table.
Unlike LI~ parsing,4For formalisms with complex structured nonterminals, the start "symbol" needonly be unifiable with the left-haud-side nonterminal.
That is, if S is the startnonterminal nd S' ~ C1 .~.C, is a rule and 0 = mgu(S,S'), then \[0, S'0 *-C18... C,8, 0\] is an initial prediction.5Again, for formalisms with complex structured nontermiuals, the staxt symbolneed only subsume the item's nontermiual.SAssuming that the prediction inference rule uses the restriction mechanism, thearchitecture actually mimics the variant of Eariey's algorithm previously describedin \[Shieber, 1985\].615however, the closure here is computed at run time rather than beingpreeomptled.
The last two Conditions correspond to disambiguationof shift/reduce and reduce/reduce onflicts in LR parsing respectively.The former equires that shifts be preferred to reductions, the latterthat longer reductions receive preference.In sum, this ordering strategy implements a sentence-disambigua-tio n parsing method that has previously been argued \[Shieber, 1983\]to model certain psycholinguistic phenomena--for instance, right asso-ciation and minimal attachment \[Fra~zier and Fodor, 1978\].
However,unlike the earlier characterization in terms of LlZ disambiguation, thismechanism can be used for arbitrary logic or unification-based gram-mars, not just context-free grammars.
Furthermore, the architectureallows for fine tuning of the disambiguation strategy beyond that de-scribed in earlier work.
Finally, the strategy is complete, allowing"backtracking" if earlier proof paths lead to a dead eudf4.2 A Pars ing  ExampleAs a demonstration f the architecture used as a parser, we considerthe Earley and backtracking-LR instances in parsing the ambiguoussentence:Castillo said Sonny was shot yesterday.Since the operation of the architecture as a parser is quite similar tothat of previous parsers for unification-based formalisms, we will onlyhighlight a few crucial steps in the process.The Earley parser assigns higher priority to items ending earlier inthe sentence.
The highest-priority initialization items are added first, s\[O,S-~ ,NP  VP,  O\] "\[0, N P .--+ castillo ?, 1\] 'Castillo'By Completion, the item\[0, S ---* NP  ?
VP,  1\] 'Castillo'is generated, which in turn predicts\[1, VP -4 ?
VP  XI 1\] "\[1, vp - -+ .
v, 1\] "\[1, VP --* o VP  AdvP, 1\] "The highest-priority item remMniug on the agenda is the initial item\[1, V -+ sa id .
,  2\] 'said'Processing progresses in this manner, performing all operations ata string position before moving on to the next position until the finalposition is reached, at which point the final initial item correspondingto the word 'yesterday' is added.
The following flurry of items isgenerated by completion.
9\[5, AdvP .--+ yesterday.
,  6\] 'yesterday'(2) \[I, VP ---+ VP  AdvP .
,  6\] 'said Sonny was shotyesterday'(3) \[3, VP ---* VP  AdvP .
,  6\] 'was shot yesterday'\[4, V P ---* V P AdvP  .
, 6\] 'shot yesterday'\[1, VP --* VP .AdvP ,  6\] 'said Sonny was shotyesterday'(4) \[0, S ~ NP  VP .
,  6\] 'Castillo said Sonny wasshot yesterday'\[3, V P --* V P ?
Adv P, 6\] 'was shot yesterday'(5) \[2, S --* NP  VP?
,6 \ ]  'Sonny was shot yesterday'\[4, V P ~ V P ?
AdvP, 6\] 'shot yesterday'(6) \[1, VP ---+ VP  S ?, 6\] 'said Sonny was shotyesterday'\[1, VP ---* VP  ?
AdvP, 6\] ~said Sonny was shotyesterday'(7) \[0, S --* NP  VP .
,  6\] 'Castillo said Sonny wasshot yesterday'' 7Modeling uf an incomplete version of the shift-reduce t chnique is also possible.The simplest method, however, involves eliminating the chart completing, andmimicking closure, shift, and reduction operations as operations on LR states (setsof items) directly.
Though this method is not a straightforward instantiation ofthearchitecture of Section 3 (since the chart is replaced by separate state sets), we haveimplemented a parser using much of the same technology described here and havesuccessfully modeled the garden path phenomena that rely on the incompletenessof the shift-reduce t chnique.SThe format used in displaying these items reverts to one similar to Earley'salgorithm, with a dot marking the position in the rule covered by the string gener-ated so far, so as to describe more clearly the portion of each grammar rule used.In addition, the string actually parsed or generated is given in single quotes afterthe item for convenience.SThe four instances of 'said Sonny was shot yesterday' arise because of lexicalambiguity in the verb 'said' and adverbial-attachment ambiguity.
Only the finiteversion of 'said' is used in forming the final sentence.616Note that the first full parse found (4) is derived from the high attach-meat of the word 'yesterday' (2) (which is composed from (i) directly),the second (7) from the low attachment (6) (derived from (5), whichis derived in turn from (3)).By corhparison, the shift-reduce parser generates exactly the sameitems as the Earley parser, but in a different order.
The crucial order-ing difference occurs in the following generated items:(1) \[5,AdvP --+ yesterday ?, 6\] 'yesterday'(3) \[3, VP ~ VP  AdvP .
,  6\] 'was shot yesterday'\[3, VP ---* V P ?
Adv P, 6\] 'was shot yesterday'(5) \[2, S --~ NP  VP  ?, 6\] 'Sonny was shot yesterday'(6) \[1, VP-+ VP  S ?, 6\] 'said Sonny was shotyesterday'\[1, VP --~ VP  ?
AdvP, 6\] 'said Sonny was shotyesterday'(7) \[0, S ~ NP  VP  ?, 6\] 'Castillo said Sonny wasshot yesterday'(8) \[2, S---* NP  VP  ?, 5\] 'Sonny was shot'\[1, VP ---+ VP  S ,,  5\] 'said Sonny was shot'\[1, VP --+ VP  ?
AdvP, 5\] 'said Sonny was shot'(2) \[1, VP ~ VP  AdvP .
,  6\] 'said Sonny was shotyesterday'\[1, VP ---* VP  ?
AdvP, 6\] 'said Sonny was shotyesterday'(4) \[0, S ~ NP  VP?
,  6\] 'Castillo said Sonny wasshot yesterday'Note that the reading of the sentence (7) with the low attachmentof the adverb--the so-called "right association" reading--is generatedbefore the reading with the higher attachment (4), in accordance withcertain psycholinguistic results \[Frazier and Fodor, 1978\].
This is be-cause item (3) has higher priority than item (8), since (3) correspondsto the shifting of the word 'yesterday' and (8) to the reduction Ofan NP and VP to S. The second clause of the priority definition or-ders such shifts before reductions.
In summary, this instance of thearchitecture develops parses in right-association/minlmal-attachmentpreference order.4.3 Generator  Ins tancesAs a final example of the use of this architecture, we consider'using itfor generation by changing the initialization condition as follows:* The init ial ization of the agenda includes axioms for each wordin the lexicon at each position (e.g., \[O, sonny ,  1\] and \[0, left, 1\]and /1, left, 2/, and so on) and an initial prediction for eachrule whose left-hand side is the start symbol of the grammar(e.g., \[0, S +- NP VP,0\]).
In the case of a grammar formalismwith more complex information structures as nonterminals, e.g.,definite-clause grammars, the "start symbol" might include infor-mation about, say, the meaning of the sentence to be generated,We will refer to this as the goal meaning.u The success criterion is that the nonterminal be subsumed by thestart nonterminal (and therefore have the appropriate meaning).Under this parameterization, the architecture serves as a generatorfor the grammar, generating sentences with the intended meaning.By changing the priority function, the order in which possibilities arepursued in generation can be controlled, thereby modeling depth-firststrategies, breadth-first strategies, and so forth.Of course, as described, this approach to generation is sorely inade-quate for several reasons.
First, it requires that we initially insert theentire lexicon into the agenda t arbitrary numbers of string positions.Not only is it infeasible to insert the lexicon so many times (indeed,even once is too much) but it also leads to massive redundancy ingeneration.
The same phrase may be generated starting at many dif-ferent positions.
For parsing, keeping track of which positions phrasesoccur at is advantageous; for generation, once a phrase is generated,we want to be able to use it in a variety of places.A simple solution to this problem is to ignore the string positionsin the generation process.
This can be done by positioning all lemmasat a single position.
Thus we need insert the lexicon only once, eachword being inserted at the single position, e.g., \[0, sonny ,  0\].Although this simplifies the set of initial items, by eliminating index-ing based on string position we remove the feature of tabular parsingmethods such as Earley's algorithm that makes parsing reasonably effi-cient.
The generation behavior exhibited is therefore not goal-directed;once the lexicon is inserted many phrases might be built that couldnot contribute in any way to a sentence with the appropriate mean-ing.
In order to direct the behavior of the generator towards a goalmeaning, we can modify the priority function so that it is partial; notevery item will be assigned a priority and those that are not will neverbe added to the table (or agenda) at all.
The filter we have been usingassigns priorities only to items that might contribute semantically tothe goal meaning.
In particular, the meaning associated with the i temmust  subsume some port ion o f  the goal mean ing} ?
This technique, asort of indexing on meaning, replaces the indexing on string positionthat is more appropriate for parsing than generation.As a rule, filtering the items by making the priority function par-tial can lead to incompleteness of the parsing or generation process, nHowever, the subsumption filter described here for use in generationdoes not yield incompleteness of the generation algorithm under oneassumption about the grammar, which we might call semant ic  mono-tonicity.
A grammar is semantically monotonic if, for every phraseadmitted by tim grammar, the semantic structure of each immediatesubphrase subsumes some portion of the semantic structure of the en-tire phrase.
Under this condition, items which do not subsume partof the goal meaning can be safely ignored, since any phrase built fromthem will also not subsume part of the goal meaning and thus willfail to satisfy the success criterion.
Thus the question of complete-ness of the algorithm depends on an easily detectable property of thegrammar.
Semantic monotonicity is, by intention, a property of theparticular grammar we have been using.
?4.4 A Generat ion  ExampleAs an example of the generation process, we consider the generationof a sentence with a goal logical formpassionately( love(sonny, kait))The example was run using a toy grammar that placed subcate-gorization information in the lexicon, as in the style of analysis ofhead-driven phrase-structure grammar (HPSG).
The grammar ignoredtense and aspect information, so that, for instance, auxiliary verbsmerely identified their own semantics with that of their postverbalcomplement .nThe initial items included the following:(1) \[0, NP.
-~ sonny , ,  O\] 'Sonny'(2) \[0, NP.+ kait ,,13\] 'Knit'\[0, V -?
to .
,  O\] 'to'\[0, V -* was ?, O\] 'was'\[% v - ,  were .
,  O\] 'were'\[0, V -+ loves *, 0\] 'loves'\[0, V -+ love , ,  0\] 'love'\[0, V -* loved , ,  0\] 'loved'\[0, AdvP .--* passionately ,, O\] 'passionately'(3) \[0, S ~ ?
NP  VP,  0\] "Note that auxiliary verbs were included, as the semantic structureof an auxiliary is merely a variable (coiindexed with the semanticstructure of its postverbal complement), which subsumes some part (infact, every part) of the goal logical form./3 Similarly, the noun phrases'Sonny' and ~Kait ~ (with semantics sonny  and ka iL  respectively) areadded, as these logical forms each subsume the respective innermostarguments of the goal logical form.
Several forms of the verb 'love'are considered, again because the semantics in this grammar makesno tense/aspect distinctions.
But no other proper nouns or verbs are*?Since the success 'criterion requires that a successful item be subsumed by thestart nonterminal and the priority filter requires that a successful item's emanticssubsume the start ~tonterminai% semantics, it follows that successful items matchthe start symbol exactly in semantic nformation; overgeneration in this sense isnot a problem,11 Indeed, we might want such incompleteness for certain cases of psycholinguis-tically motivated psrsing models such as the simulated Lit model described above.nFor reference, the grammar issimilar in spirit to the third sample grammar in\[Shieber, 1986\].asIt holds in general that closed-class lexical items---case-m~rking prepositions,function verbs, etc.~-are uniformly considered initial items for purposes of genera-tion because of their vestigial semantics.
This is as desired, and follows from theoperation of semantic filtering, rather than from any ad hoc techniques.considered (although the lexicon that was used contained them) asthey do not pass the semantic filter.The noun phrase 'Sonny' can be used as the subject of the sentenceby combining items (1) and (3) yielding(4) \[0, S --~ NP  ?
VP,  0\] 'Sonny'(The corresponding item with the subject 'Knit' will be generatedlater.)
Prediction yields the following chain of items.\[0, VP .-+ ?
VP  AdvP, 0\] "\[0, w -~.
v, 0\] "The various verbs, including the forms of 'love', can complete thislatter item.\[0, VP ~ V .,  O\] 'to'\[0, V P --.
v .
, 0\] 'is'\[O, VP  -* V.,0\] 'was'\[0, VP -~ V. ,  O\] 'were'(5) \[0, vP  -~ v .
,  0\] 'loves'\[0, VP -~ V. ,  O\] 'love'\[0, VP ~ V .
,  0\] 'love'\[0, VP ---* V .
,  0\] 'loved'The passive form of the verb 'loved' might be combined with the ad-verb.\[0, VP .-~ VP  ?
AdvP, 0\] 'loved'\[0, VP --, V P AdvP  .
,  0\] 'loved passionately'The latter item might be used in a sentence 'Knit was loved passion-ately.'
This sentence will eventually be generated but will fail thesuccess criterion because its semantics i insufficiently instantiated.Prediction from item (4) also yields the rule for adding complementsto a verb phrase.\[0, vP  - - , .
VP  X,O\] "Eventually, this item is completed with items (5) and (2).\[0, VP ---, V P ?
N P, 0\] 'loves'\[0, VP --~ VP  NP  ,,  0\] 'loves Knit'The remaining items generated are\[0, VP ---* VP  , AdvP, 0\] 'loves Knit'\[0, V P ---, V P Adv P o, 0\] 'loves Knit passionately'\[0, S ---* NP  VP .
,  0\] 'Sonny loves Knitpassionately'This final item matchesthe success criterion, and is the only such item.Therefore, the sentence 'Sonny loves Kait passionately' is generatedfor the logical form pass ionate ly ( love(sonny ,  knit ) ) .Looking over the generation process, the set of phrases actively ex-plored by the generator included 'Kate is loved', 'Kate is loved pas-sionately', 'were loved passionately' and similar passive constructions,'Sonny loves Kalt', and various subphrases of these.
However, otherphrases composed of the same words, such as 'Knit loves Knit', 'Sonnyis loved', and so forth, are eliminated by the semantics filter.
Thus,the the generation process is, on the whole, quite goal-directed; thesubphrases considered in the generation process are "reasonable".5 The  Imp lementat ionThe architecture described above has been implemented for the PATRgrammar formalism in a manner eminiscent of object-oriented pro-gramming.
Instances of the architecture are built as follows.
Ageneral-purpose processor-building fimction, taking a priority func-tion and success criterion fnnction as arguments, returns an objectthat corresponds to the architecture instance.
The object can be sentinitialization items as arbitrary lemmas of the usual form.
Whenevera successful lemma is constructed (according to the success criterion)it is returned, along with a continuation function that can be called iffurther sohttions are needed.
No processing is done after a successflfllemma has been pro?ed unless further solutions are requested.Using this implementation, wehave built instances of the architec-ture for Barley parsing and the other parsing variants described in thispaper, including the shift/reduce simulator.
In addition, a generatorwas built that is complete for semantically monotonic grammars.
It isinteresting to note that the generator is more than an order of magni-tude faster than our original PATR generator, which worked purely by617top-down depth-first backtracking search, that is, following the Prologsearch strategy.The implementation is in Common Lisp and runs on Symbolics 3600,Sun, and Macintosh computers.
It is used (in conjunction with a moreextensive .grammar) ~s the generation component of the GENESYSsystem for utterance planning and production.6 PrecursorsPerhaps the clearest espousal of the notion of grammar eversabilitywas made by Kay \[1975\], whose research into functional grammar hasbeen motivated by the desire to "make it possible to generate andanalyze sentences with the same grammar."
Other researchers havealso put such ideas into effect.
Jacobs's PHRED system \[Jacobs, 1985\]"operates from a declarative knowledge base of linguistic knowledge,common to that used by PHRAN", an analyzer for so-called phrasalgrammars.
Jacobs notes that other systems ~ have shared at least partof the linguistic information for parsing and generation; for instance,the HAM-ANS \[Wahlster t al., 1983\]'a;nd VII~-LANG \[Steinacker andBuchberger, 1983\] systems utilize the same lexical information for bothtasks.
Kasper has used a system for parsing rammars in a unification-based formalism (SItI's Z-PATR system) to parse sentences with re-spect to the large ISI NIGEL grammar, which had been previouslyused for generation alone.Nonetheless, all of these systems rely on often radically differentarchitectures for the two processes.
Precedent for using a single ar-chitecture for both tasks is much more difficult to find.
The germ ofthe idea can be found in the General Syntactic Processor (GSP) de-signed for the MIND system at Rand.
Kaplan and K~y proposed useof the GSP for parsing with respect o augmented transition etworksand generation by traiisformational grammars \[Kaplan, 1973\].
How-ever, detailed implementation was apparently never carried out.
Inany case, although the PrOposal involved using the same arehitecture~different formalisms (and hence grammars) were presupposed for thetwo tasks, ttunning a definite-clanse grammar (DCG) "backwards"has been proposed previously, although the normal Prolog executionmechanism renders uch a technique unusable in practice.
However,=.- alternative xecution models might make the practice feasible.
Asmentioned above, the technique described here is just such an exe:cution model, and is directly related to the Earley deduction modelof Pereira and Warren \[1983\].. Hasida and Isizaki \[1987\] present an-other method for generating and analyzing using a DCG-like formal-ism, which they call dependency propagation.
The technique seemsto entail using dataflow dependencies implicit in the grammar to con-trol processing in a coroutining manner.
The implementation statusof their method and its practical utility are as yet unclear.The use of an agenda nd scheduling schemes to allow varying thecontrol structure of a parser also finds precedent in the work of Kaplan\[1973\] and Kay \[1967\].
Kay's "powerful parser" and the GSP both em-ployed an agenda mechanism to control additions to the chart.
How-ever, the "tasks" placed on the agenda were at the same time morepowerful (corresponding to unconstrained rewrite rules) and more pro-cedural (allowing register operations and other procedural constructs).This work merely applies the notion in the context of the simple declar-ative formalisms presupposed, and provides it with a logical founda-tion on which a proof of correctness can be developed.
TM Because theformalisms are simpler, the agenda need only keep track of one typeof task: addition of a chart item.7 Fur ther  KesearchPerhaps the most immediate problem raised by the methodology forgeneration i troduced in this paper is the strong requirement of se-mantic monotonicity, which serves as yet another instance of the strait-jacket of compositionality, The semantic-monotonicity onstraint al-lows the goal logical form to be systematically decomposed so that a.dynamic-programming generation process can be indexed by the partsof the decomposition (the subformulas), just as the constraint of stringconcatenation i  context-free grammars allows a goal sentence to besystematically decomposed so that a dynamic-programming parsingprocess can be indexed by the subparts of that decomposition (the14Such a proof is currently in preparation.canonical intentionallylogical equivalentforms LFsNL expressiongrammardefinesLF la  /LF 1 ~- - - -  LF lbLF l cLF 2a /LF2  ~- -  LF2bLF 2c?
LF  3a  /LF 3 ~ LF 3bLF 3e , II .
.
.
.
Iintentional equivalencedefinesFigure 1: Canonical Logical Formssubstrings).
And just az the concatenation restriction of context-freegrammars can be problematic, so can the restriction of semantic mono-tonicity.
Finding a weaker constraint on grammars that still allowsefficient processing is thus an important research objective.Even with the semantic-monotonicity onstraint, he process of in-dexing by the highly structured logical forms is not nearly so efficientas indexing by simple integer string positions.
Partial match retrievalor similar techniques from the Prolog literature might be useful here.Nothing has been said al~out he importartt problem of guaranteeingthat the syntactic and semantic goal properties will actually be real-ized in the sentence generated.
The success criterion for generationdescribed here would require that the logical form for the sentencegenerated be identical to the goal logical form.
However, there is noguarantee that the other properties of the sentence include those ofthe goal; only compatibility is guaranteed.
Researchers at the Univer-sity of Stuttgart have proposed solutions to this problem based on thetype of existential constraint found in lexieal-functional grammar.
Weexpect hat their methods might be applicable within th~ presentedarchitecture.Finally, on a more pessimistic note, we turn to a widespread problemin all systems for automatic generation of natural language, which Ap-pelt \[1987\] has discussed under the rubric "the problem of logical-formequivalence".
The mapping from logical forms to natural-languageexpressions i  in general many-to-one.
For instance, the logical formsred(x) h ball(x) and ball(x) h red(x) might both be realized as thenominal 'red ball'.
However, most systems for describing the string-LF relation declaratively do so by assigning a minimal set of logicalforms to each string, with each logical form standing proxy for all itslogical equivalents.
The situation is represented graphically as Figure1.The problem is complicated further in that, strictly speaking, theclass of equivalent logical forms from the standpoint of generation isnot really closed under logical equivalence.
Instead, what is actu-ally required is a finer-grained notion of intentional equivalence, underwhich, for instance, p and p A (q Y -~q) would not necessarily be inten..tionally equivalent; hey might correspond to different uttera~aces, oneabout p only, the other about both p and q.In such a system, merely using the grammar per se to drive gener-ation (as we do here) allows for the generation of strings from only asubset of the logical-form expressions that have natural-language re-lata, that is, LF1, LF2, and LF3 in the figure.
We might call these thecanonical ogical forms.
Even if the grammar is reversible, the prob-lem remains, because logical equivalence is in general not computable.And even in restricted cases in which it is computable, for instance alogic with a confluence property under which all logically equivalent61Bexpressions do have a canonical form, the problem is not solved unlessthe notion of canonical form implicit in the logic corresponds exactlyto that of the natural-language grammar.It should be noted that this kind of problem is quite deep.
Any,sys-tem that :represents meanings in some way (not necessarily with logicalexpressions) must face a correlate of this problem.
Furthermore, al-though the issue impinges on syntax because it arises in the realm ofgrammar, it is primarily a semantic problem, as we will shortly see.There are two apparent possible approaches to a resolution of thisproblem.
We might use a logic in vchich logical equivalence classes ofexpressions are all trivial, that is, any two distinct expressions meansomething diiferent.
In such a logic, there are no artifactual syn-tactic remnants in the syntax of the logical anguage.
Furthermore,expressions ofthe logic must be relatable to expressions of the naturallanguage with a reversible grammar.
Alternatively, we could use alogic for which canonical forms, corresponding exactly to the naturallanguage graramar's logical forms, do exist.The difference between the two approaches is only an apparent one,for in the latter case the equivalence classes of logical forms can beidentified as h)gical forms of a new logical anguage with no artifactualdistinctlons.
Thus, the second case reduces to the first.
The centralproblem in either case, then~ is discovery of a logical language whichexactly and uniquely represents all the meaning distinctions ofnaturallanguage utterances and no others.
This holy grail, of course, is justthe goal of knowledge representation for natural language in general;we are unlikely to be able to rely on a full solution soon.However, by looking at approximations of this goal, suitablyadapted to the particular problems of generation, we can hope toachieve some progress.
In the case of approximations, it does nothold that the two methodologies reduce one to another; in this case,we feel that the second approach--designing a logical language thatapproximates in its canonical forms those needed for grammaticalapplications-qs more likely to yield good incremental results.l'?eferences\[Appelt, 1987\] Douglas E. Appelt.
Bidirectional grammars and thedesign of natural anguage ~eneration systems.
In TheoreticalIssues in Natural Language Pracessing--3, pages 185-191, NewMexico State University, Las Cruces, New Mexico, 7-9 January1987.\[Frazier and Fodor, 1978\] Lyn Frazier and Janet Dean Fodor.
Thesausage machine: a new two-stage parsing model.
Cognition,6:291-325, 1978.\[Hasida nd Isizaki, 1987\] KSiti Hasida and Syun Isizaki.
Depen-dency propagation: a unified theory of sentence comprehensionand generatimu In Proceedings ofAAAI-87~ pages 664-670, Seat-tle, Washington, 13-17 July 1987.\[Jacobs, 1985\] Paul S. Jaeobs.
PHRED: a generator for naturallanguage interfaces.
Computational Linguistics, 11(4):219-242,October-December 1985.\[Kaplan, 1973\] Ronald M. Kapian.
A general syntactic processor.
InRandall t~ustin, editor, Natural Language Processing, pages 193-241, Algorithmics Press, New York, I973.\[Kay, 1967\] Martin Kay.
Experiments with a powerful parser.
InProceedings Of the Second International Conference on Compu-tational Linguistics, August 1967.\[Kay, 1975\] Martin Kay.
Syntactic processing and functional sen.tence perspective.
In Theoretical Issues in Natural LanguagePracessing--Supplement to the Proceedings, pages 12-15, Cam-bridge, Massachusetts, 10-13 June 1975~\[Pereira and Warren, 1983\] Fernando C. N. Pereira and David tL D.Warren.
Parsing as deduction.
In Proceedings of the 21st An-nual Meeting of the Association for Computational Linguistics,pages 137-144, Massachusetts Institute of Technology, iCam-bridge, Massachusetts, 15-17 June 1983.\[Pereira and Shieber, 1987\] FernandoC.
N. Pereira and Stuart M.Shieber.
Proloy and Natural-Language Analysis.
Volume 10 ofCSLILecture Notes, Center for the Study of Language and Infor-mation, Stanford, California, 1987.\[Shieber, 1983\] Stuart M. Shieber.
Sentence disambiguation by ashift-reduce parsing technique.
In Proceedings of the 21st An-nual Meeting of the Association for Computational Linguistics,pages 113-118, Massachusetts Institute of Technology, Cam-bridge, Massachusetts, 15-17 June 1983.\[Shieber, 1985\] Stuart M. Shieber.
Using restriction to extend parsingalgorithms for complex-feature-based formalisms.
In Proceedingsof the 23rd Annual Meeting of the Association for ComputationalLinguistics, pages 145-152, University of Chicago, Chicago, Illi-nois, 8-12 July 1985.\[Shieber, 1986\] Stuart M. Shieber.
An Introduction to Unification.Based Approaches to Grammar.
Volume 4 of CSLI Lecture Notes,Center for the Study of Language and Information, Stanford, Cal-ifornia, 1986.\[Steinackerand Buchberger, 1983\] Ingeborg Steinacker and ErnstBuchberger.
l~elating syntax and semantics: the syntactico-semantic lexicon of the system VIE-LANG.
In Proceedings of theFirst Conference of the European Chapter of the Association forComputational Lir~guistics, pages 96-100, Piss, Italy, 1-2 Septem-ber 1983.\[Wahlster tal., 1983\] Wolfgang Wahlster, Heinz Marburger, An-thony Jameson, and Stephan Busemann.
Overanswering yes-noquestions: extended responses in a natural language interface toa vision system.
In Proceedings of the Eighth Mternational JointConference on Artificial Intelligence, pages 643-646, Karlsruhe,West Germany, 8-12 August \]983.619
