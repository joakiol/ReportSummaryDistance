TREQ-AL: A word alignment system with limited language resourcesDan Tufi?, Ana-Maria Barbu, Radu IonRomanian Academy Institute for Artificial Intelligence13, ?13 Septembrie?, 74311, Bucharest 5, Romania{tufis,abarbu,radu}@racai.roAbstractWe provide a rather informal presentation of aprototype system for word alignment based onour previous translation equivalence approach,discuss the problems encountered in theshared-task on word-aligning of a parallelRomanian-English text, present the preliminaryevaluation results and suggest further ways ofimproving the alignment accuracy.1 IntroductionIn (Tufi?
and Barbu, 2002; Tufi?, 2002) we largelydescribed our extractor of translation equivalents, calledTREQ.
It was aimed at building translation dictionariesfrom parallel corpora.
We described in (Ide et al 2002)how this program is used in word clustering and inchecking out the validity of the cross-lingual linksbetween the monolingual wordnets of the multilingualBalkanet lexical ontology (Stamatou et al 2002).
In thispaper we describe the TREQ-AL system, which buildson TREQ and aims at generating a word-alignment mapfor a parallel text (a bitext).
TREQ-AL was built in lessthan two weeks for the Shared Task proposed by theorganizers of the workshop on ?Building and UsingParallel Texts:Data Driven Machine Translation andBeyond?
at the HLT-NAACL 20031 conference.
It canbe improved in several ways that became conspicuouswhen we analyzed the evaluation results.
TREQ-AL hasno need for an a priori bilingual dictionary, as this willbe automatically extracted by TREQ.
However, if sucha dictionary is available, both TREQ and TREQ-ALknow to make best use of it.
This ability allows bothsystems to work in a bootstrapping mode and to producelarger dictionaries and better alignments as they areused.The word alignment, as it was defined in the sharedtask is different and harder than the problem oftranslation equivalence as previously addressed.
In adictionary extraction task one translation pair isconsidered correct, if there is at least one context inwhich it has been rightly observed.
A multiplyoccurring pair would count only once for the final1 http://www.cs.unt.edu/~rada/wpt/index.html#shareddictionary.
This is in sharp contrast with the alignmenttask where each occurrence of the same pair equallycounts.Another differentiating feature between the twotasks is the status of functional word links.
In extractingtranslation equivalents one is usually interested only inthe major categories (open classes).
In our case (becauseof the WordNet centered approach of our currentprojects) we were especially interested in POS-preserving translation equivalents.
However, since inEuroWordNet and Balkanet one can define cross-POSlinks, the different POS translation equivalents becameof interest (provided these categories are major ones).The word alignment task requires each word(irrespective of its POS) or punctuation mark in bothparts of the bitext be assigned a translation in the otherpart (or the null translation if the case).Finally, the evaluations of the two tasks, even ifboth use the same measures as precision or recall, haveto be differently judged.
The null alignments in adictionary extraction task have no significance, while ina word alignment task they play an important role (inthe Romanian-English gold standard data the nullalignments represent 13,35% of the total number oflinks).2 The preliminary data processingThe TREQ system requires sentence aligned paralleltext, tokenized, tagged and lemmatized.
The firstproblem we had with the training and test data wasrelated to the tokenization.
In the training data therewere several occurrences of glued words (probably dueto a problem in text export of the initial data files) plusan unprintable character (hexadecimal code A0) thatgenerated several tagging errors due to guesserimperfect performance (about 70% accurate).To remedy these inconveniences we wrote a scriptthat automatically split the glued words and eliminatedthe unprintable characters occurring in the training data.The set of splitting rules, learnt from the trainingdata was posted on the site of the shared task.
The set ofrules is likely to be incomplete (some glued wordsmight have survived in the training data) and also mightproduce wrong splitting in some cases (e.g.
turnoverbeing split always in turn over).The text tokenization, as considered by theevaluation protocol, was the simplest possible one, withwhite spaces and punctuation marks taken as separators.The hyphen (?-?)
was always considered a separator andconsequently taken to be always a token by itself.However, in Romanian, the hyphen is more frequentlyused as an elision marker (as in ?intr-o?= ?intru o?/in a),a clitics separator (as in ?da-mi-l?=?da ?mi ?l?=?da mieel?/give to me it/him) or as a compound marker (as in?terchea-berchea?
/(approx.)
loafer) than as a separator.In such cases the hyphen cannot be considered a token.A similar problem appeared in English with respect tothe special quote character, which was dealt with inthree different ways: it was sometimes split as a distincttoken (we?ll = we + ?
+ ll), sometimes was adjoined tothe string (a contracted positive form or a genitival)immediately following it (I?m = I + ?m, you?ve =you+?ve,  man?s = man + ?s etc.)
and systematically leftuntouched in the negative contracted forms (couldn?t,wasn?t, etc).Since our processing tools (especially the tokeniser)were built with a different segmentation strategy inmind, we generated the alignments based on our owntokenization and, at the end, we ?re-tokenised?
the textaccording to the test data model (and consequently re-index) all the linking pairs.For tagging the Romanian side of the training bitextwe used the tiered-tagging approach (Tufi?, 1999) butwe had to construct a new language model since ourstandard model was created from texts containingdiacritics.
As the Romanian training data did not containdiacritical characters, this was by no means a trivial taskin the short period of time at our disposal (actually ittook most of the training time).
The lack of diacritics inthe training data and the test data induced spuriousambiguities that degraded the tagging accuracy with atleast 1%.
This is to say that we estimate that on anormal Romanian text (containing the diacriticalcharacters) the performance of our system would havebeen better.
The English training data was tagged byEric Gaussier, warmly acknowledged here.
As thetagsets used for the two languages in the paralleltraining corpus were quite different, we defined a tagsetmapping and translated the tagging of the English partinto a tagging closer to the Romanian one.
Thismapping introduced some ambiguities that were solvedby hand.
Based on the training data (both Romanian andEnglish texts), tagged with similar tagsets, we built thelanguage models used for the test data alignment.POS-preserving translation equivalence is a toorestrictive condition for the present task and we defineda meta-tagset, common for both languages thatconsidered frequent POS alternations.
For instance, theverb, noun and adjective tags, in both languages wereprefixed with a common symbol, given that verb-adjective, noun-verb, noun-adjective and the othercombinations are typical for Romanian-Englishtranslation equivalents that do not preserve the POS.With these prefixes, the initial algorithm for extractingPOS-preserving translation equivalents could be usedwithout any further modifications.
Using the tag-prefixes seems to be a good idea not only for legitimatePOS-alternating translations, but also for overcomingsome typical tagging errors, such as participles versusadjectives.
In both languages, this is by far the mostfrequent tagging error made by our tagger.The last preprocessing phase is encoding the corpusin a XCES-Align-ana format as used in the MULTEXT-EAST corpus (see http://nl.ijs.si/ME/V2/) which is thestandard input for the TREQ translation equivalentsextraction program.
Since the description of TREQ isextensively given elsewhere, we will not go into furtherdetails, except of saying that the resulted translationdictionary extracted from the training data contains49283 entries (lemma-form).
The filtering of thetranslation equivalents candidates (Tufi?
and Barbu,2002) was based on the log-likelihood and the cognatescores with a threshold value set to 15 and 0,43respectively.
We roughly estimated the accuracy of thisdictionary based on the aligned gold standard: precisionis about 85% and recall is about 78% (remember, thedictionary is evaluated in terms of lemma entries, andthe non-matching meta-category links are excluded).3 The TREQ-AL linking programThis program takes as input the dictionary created byTREQ and the parallel text to be word-aligned.
Thealignment procedure is a greedy one and considers thealigned translation units independent of the othertranslation units in the parallel corpus.
It has 4 steps:1. left-to-right pre-alignment2.
right-to-left adjustment of the pre-alignment3.
determining alignment zones and filtering them out4.
the word-alignment inside the  alignment zones3.1 The left-to-right pre-alignmentFor each sentence-alignment unit, this step scans thewords from the first to the last in the source-languagepart (Romanian).
The considered word is initially linkedto all the words in the target-language part (English) ofthe current sentence-alignment unit, which are found inthe translation dictionary as potential translations.
If forthe source word no translations are identified in thetarget part of the translation unit, the control advances tothe next source word.
The cognate score and the relativedistance are decision criteria to choose among thepossible links.
When consecutive words in the sourcepart are associated with consecutive or close to eachother words in the target part, these are taken as formingan ?alignment chain?
and, out of the possible links, areconsidered those that correspond to the densestgrouping of words in each language.
High cognatescores in an alignment chain reinforce the alignment.One should note that at the end of this step it is possibleto have 1-to-many association links if multipletranslations of one or more source words are found inthe target part of the current translation unit (and,obviously, they satisfy the selection criteria).3.2 The right-to-left adjustment of the pre-alignmentThis step tries to correct the pre-alignment errors (whenpossible) and makes a 1-1 choice in case of the 1-mlinks generated before.
The alignment chains (found inthe previous step) are given the highest priority inalignment disambiguation.
That is, if for one word inthe source language there are several alignmentpossibilities, the one that belongs to an alignment chainis always selected.
Then, if among the competingalignments one has a cognate score higher than theothers then this is the preferred one (this heuristics isparticularly useful in case of several proper namesoccurring in the same translation unit).
Finally, therelative position of words in the competing links istaken into account to minimize the distance between thesurrounding already aligned words.The first two phases result in a 1-1 word mapping.The next two steps use general linguistic knowledgetrying to align the words that remain unaligned (eitherdue to no translation equivalents or because of failure tomeet the alignment criteria) after the previous steps.This could result in n-m word alignments, but also inunlinking two previously linked words since a wrongtranslation pair existing in the extracted dictionarymight license a wrong link.3.3 Alignment zones and filtering suspicious links outAn alignment zone (in our approach) is a piece of textthat begins with a conjunction, a preposition, or apunctuation mark and ends with the token preceding thenext conjunction, preposition, punctuation or end ofsentence.
A source-language alignment zone is mappedto one or more target-language alignment zones via thelinks assigned in the previous steps (based on thetranslation equivalents).
One has to note that themapping of the alignment zones is not symmetric.
Analignment zone that contains no link is called a virginzone.In most of the cases the words in the sourcealignment zone (starting zone) are linked to words in thetarget algnment zone/s (ending zone/s).
The links witheither side outside the alignment zones are suspiciousand they are deleted.
This filtering proved to be almost100% correct in case the outlier resides in a zone non-adjacent to the starting or ending zones.
The failures ofthis filtering were in the majority of cases due to awrong use of punctuation in one or the other part of thetranslation unit (such as omitted comma, a commabetween the subject and predicate).3.4 The word-alignment inside the alignment zonesFor each un-linked word in the starting zone thealgorithm looks for a word in the ending zone/s of thesame category (not meta-category).
If such a mappingwas not possible, the algorithm tries to link the sourceword to a target word of the same meta-category, thusresulting in a cross-POS alignment.
The possible meta-category mappings are specified by the user in anexternal mapping file.
Any word in the source or targetlanguages that is not assigned a link after the fourprocessing steps described above is automaticallyassigned a null link.4 Post-processingAs said in the second section, our tokenization wasdifferent from the tokenization in the training and testdata.
To comply with the evaluation protocol, we had tore-tokenize the aligned text and re-compute the indexesof the links.
Re-tokenizing the text meant splittingcompounds and contracted future forms and gluingtogether the previously split negative contracted forms(do+n?t=don?t).
Although the re-tokenization was apost-processing phase, transparent for the task itself, itwas a source of missing some links for the negativecontracted forms.
In our linking the English ?n?t?
wasalways linked to the Romanian negation and the Englishauxiliary/modal plus the main verb were linked to theRomanian translation equivalent found for the mainverb.
Some multi-word expressions recognized by thetokenizer as one token, such as dates (25 Ianuarie,2001), compound prepositions (de la, pina la),conjunctions (pentru ca, de cind, pina cind) or adverbs(de jur imprejur, in fata) as well as the hyphenseparated nominal compounds (mass-media, prim-ministru) were split, their positions were re-indexed andthe initial one link of a split compound was replacedwith the set obtained by adding one link for eachconstituent of the compound to the target English word.If the English word was also a compound the number oflinks generated for one aligned multiword expressionwas equal to the N*M, where N represented the numberof words in the source compound and M the number ofwords in the target compound.5 EvaluationThe results of the evaluation of TREQ-AL performanceare shown in the Table 1.
In our submission file thesentence no.
221 was left out by (our) mistake.
We usedthe official evaluation program to re-evaluate oursubmission with the omitted sentence included and theprecision improved with 0,09%, recall with 0,45%, F-measure and AER with 0,33%.).
The figures in the firstand second columns of the Table 1 are those consideredby the official evaluation.
The last column contains theevaluation of the result that was our main target.
SinceTREQ-AL produces only ?sure?
links, AER (alignmenterror rate - see the Shared Task web-page for furtherdetails) reduces to 1 - F-measure.TREQ-AL uses no external bilingual-resources.
Amachine-readable bilingual dictionary would certainlyimprove the overall performance.
The present versionof the system (which is far from being finalized) seemsto work pretty well on the non-null assignments and thisis not surprising, because these links are supposed to berelevant for a translation dictionary extraction systemand this was the very reason we developed TREQ.Moreover if we consider only the content words (maincategories: noun, verbs, adjectives and general adverbs),which are the most relevant with respect to ourimmediate goals (multilingual wordnets interlinking andword sense disambiguation), we think TREQ-ALperforms reasonably well and is worth furtherimproving it.Non-nulllinks onlyNull linksincludedDictionaryentriesPrecision 81,38% 60,43% 84,42%Recall 60,71% 62,80% 77,72%F-measure 69,54% 61,59% 80,93%AER 30,46% 38,41%Table 1.
Evaluation results6 Conclusions and further workTREQ-AL was developed in a short period of time andis not completely tested and debugged.
At the time ofwriting we already noticed two errors that wereresponsible for several wrong or missed links.
There arealso some conceptual limitations which, when removed,are likely to further improve the performance.
Forinstance all the words in virgin alignment zones areautomatically given null links but the algorithm couldbe modified to assign all the links in the Cartesianproduct of the words in the corresponding virgin zones.The typical example for such a case is represented bythe idiomatic expressions (tanda pe manda = the listthat sum up).
A bilingual dictionary of idioms as anexternal resource certainly would significantly improvethe results.
Also, with an additional preprocessingphase, for collocation recognition, many missing linkscould be recovered.
At present only those collocationsthat represent 1-2 or 2-1 alignments are recovered.A major improvement will be to make thealgorithm symmetric.
There are many cases whenreversing the source and target languages new links canbe established.
This can be explained by differentpolysemy degrees of the translation equivalent wordsand the way we associate alignment zones.The word order in Romanian and English to someextent is similar, but in the present version of TREQ-ALthis is not explicitly used.
One obvious and easyimprovement of TREQ-AL performance would be totake advantage of the similarity in word order and mapthe virgin zones and afterwards, the words in the virginzones.Finally, we noticed in the gold standard somewrong alignments.
One example is the following:??
a XI ?
a ??
= ??
eleventh?
?Our program aligned all the 4 tokens in Romanian (a,XI, ?, a) to the English token (eleventh), while the goldstandard assigned only ?XI?
to ?eleventh?
and the otherthree Romanian tokens were given a null link.
We alsonoticed some very hard to achieve alignments(anaphoric links).7 ReferencesTufi?, D. Barbu, A.M.: ?Revealing translatorsknowledge: statistical methods in constructingpractical translation lexicons for language and speechprocessing?, in International Journal of SpeechTechnology.
Kluwer Academic Publishers, no.5,pp.199-209, 2002.Tufi?, D. ?A cheap and fast way to build usefultranslation lexicons?
in Proceedings of the 19thInternational Conference on ComputationalLinguistics, COLING2002,  Taipei, 25-30 August,2002, pp.
1030-1036p.Ide, N., Erjavec, T., Tufis, D.: ?Sense Discriminationwith Parallel Corpora?
in Proceedings of the SIGLEXWorkshop on Word Sense Disambiguation: RecentSuccesses and Future Directions.
ACL2002, JulyPhiladelphia 2002, pp.
56-60.Stamou, S., Oflazer K., Pala  K., Christoudoulakis D.,Cristea D., Tufis D., Koeva  S., Totkov G., DutoitD., Grigoriadou M.. ?BALKANET A MultilingualSemantic Network for the Balkan Languages?, inProceedings of the International Wordnet Conference,Mysore, India, 21-25 January 2002.Tufi?, D. ?Tiered Tagging and CombinedClassifiers?
In F. Jelinek, E. N?th (eds) Text,Speech and Dialogue, Lecture Notes in ArtificialIntelligence 1692, Springer, 1999, pp.
28-33.
