UMASS/HUGHES: DESCRIPTION OF THE CIRCUS SYSTEM USEDFOR TIPSTER TEXTW.
Lehnert, J. McCarthy, S. Soderland, E. Riloff, C. Cardie, J. Peterson, F. FengUniversity of MassachusettsDeparUnent ofComputer ScienceBox 34610Amherst, MA 01003-4610lehnen@cs.umass.eduC.
Dolan, S. GoldmanHughes Research Laboratories3011 Malibu Canyon Road M/S RL96Malibu, CA 90265cpd@aic.hrl.hac.comINTRODUCTIONThe primary goal of our effort is the development ofrobust and portable language processing capabilitiesfor information extraction appfications.
The systemunder evaluation here is based on language processingcomponents that have demonstrated strongperformance capabilities in previous evaluations\[Lehnert et al 1992a\].
Having demonstrated thegeneral viability of these techniques, we are nowconcentrating on the practicality of our technology bycreating trainable system components oreplace hand-coded ~t~ and manually-engineered software.Our general strategy is to automate the constructionof domain-specific dictionaries and other language-related resources so that information extraction can becustomized for specific application s with a minimalamount of human assistance.
We employ a hybridsystem architecture that combines elective conceptextraction \[Lehnert 1991\] technologies developed atUMass with trainable classifier technologiesdeveloped at Hughes \[Dolan et al 1991\].
Our Tipstersystem incorporates even trainable languagecomponents tohandle (1) lexical recognition and part-of-speech tagging, (2) knowledge ofsemantic/syntactic interactions, (3) semantic featuretagging, (4) noun phrase analysis, (5) limitedcoreference r solution, (6) domain object recognition,and (7) relational link recognition.
Our trainablecomponents have been developed so domain expertswho have no background in natural anguage ormachine learning can train individual systemcomponents inthe space of a few hours.Many critical aspects of a complete informationextraction are not appropriate for customization rtrainable knowledge acquisition.
For example, oursystem uses low-level text specialists designed torecognize dates, locations, revenue objects, and othercommon constructions that involve knowledge ofconventional language.
Resources of this type areportable across domains (although not all domainsrequire all specialists) and should be developed assharable language resources.
The UMass/I-Iughesfocus has been on other aspects of informationextraction that can benefit from corpus-basedknowledge acquisition.
For example, in any giveninformation extraction application, some sentencesare more important than others, and within a singlesentence some phrases are more important thanothers.
When a dictionary is customized for a specificapplication, vocabulary coverage can be sensitive tothe fact that a lot of words contribute little or noinformation to the final extraction task: fulldictionary coverage is not needed for informationextraction applications.In this paper we will overview our hybrid architectureand trainable system components.
We will look atexamples taken from our official test runs, discuss thetest results obtained in our official and optional testruns, and identify promising opportunities for~cldjtional research.TRAINABLE LANGUAGE PROCESSINGOur Tipster system relies on two major tools thatsupport automated dictionary construction: (1) OTB, atrainable part-of-speech tagger, and (2) AutoSlog, adictionary construction tool that operates inconjunction with the CIRCUS sentence analyzer.
Wetrained OTB for EJV on a subset of EJV texts andthen again for EME using only EME texts.
OTB isnotable for the high hit rates it obtains on the basisof relatively little training.
We found that OTBattained overall hit rates of 97% after training on only1009 sentences for EJV.
OTB crossed the 97%threshold in EME after only 621 training sentences.241Incremental OTB training requires human interactionwith a point-and-click interface.
Our EJV training wascompleted after 16 hours with the interface; our EMEtraining required 10 hours.AutoSlog is a dictionary construction tool thatanalyzes ource texts in conjunction with associatedkey templates (or text annotations) in order topropose concept node (CN) definitions for CIRCUS\[Riloff & Lehnert 1993; Riloff 1993\].
A specialinterface is then used for a manual review of theAutoSlog definitions in order to separate the goodones from the bad ones.
Of 3167 AutoSlog CNdefinitions proposed in response to 1100 EJV keytemplates, 944 (30%) were retained after manualinspection.
For EME, AutoSlog proposed 2952 CNdefinitions in response to 1000 key templates and2275 (77%) of these were retained after manualinspection.
After generalizing the original definitionswi th  active/passive transformations, verb tensegeneralizations, and singular/plural generalizations,our final EJV dictionary contained 3017 CNdefinitions and our final EME dictionary contained4220 CN definitions.
It took 20 hours to manuallyinspect and filter the full EJV dictionary; the fullEME dictionary was completed in 17 hours.
TheCIRCUS dictionary used in our official run was basedexclusively on AutoSlog CN definitions.
No hand-coded or manually altered efinitions were added to theCN dictionary.When CIRCUS processes a sentence it can invoke asemantic feature tagger (MayTag) that dynamicallyassigns features to nouns and noun modifiers.MayTag uses a feature taxonomy based on thesemantics of our target emplates, and it dynamicallyassigns context-sensitive tags using a corpus-drivencase-based reasoning algorithm \[Cardie 93\].
MayTagoperates as an optional enhancement to CIRCUSsentence analysis.
We ran CIRCUS with MayTag forEJV, but did not use it for EME (we'll return to adiscussion of this and other domain differences later).MayTag was trained on 174 EJV sentences containing5591 words (3060 open class words and 2531 closedclass words).
Our tests indicate that MayTag achievesa 74% hit rate on general semantic features (covering14 possible tags) and a 75% hit rate on specificsemantic features (covenng 42 additional tags).Interactive training for MayTag took 14 hours using atext editor.An important aspect of the Tipster task concernsinformation extraction at the level of noun phrases.Important set fill information is often found inmodifiers, such as adjectives and prepositionalphrases.
Part-of-speech tags help us identify basicLnoun phrase components, but higher-level processesare needed to determine if a prepositional phraseshould be attached, how a conjunction should bescoped, or ff a comma should be crossed.
Noun phraserecognition is a non-trivial problem at this higherlevel.
To address the more complicated aspects ofnoun phrase recognition, we use a trainable classifierthat attempts to find the best termination point for arelevant noun phrase.
This component was trainedexclusively on the EJV corpus and then used withoutalteration for both EJV and EME.
Experimentsindicate that he noun phrase classifier terminates EJVnoun phrases perfectly 87% of the time.
7% of itsnoun phrases pick up spurious text (they are extendedtoo far), and 6% are truncated (they are not extended orextended far enough).
Similar hit rates are found withEME test data: 86% for exact NP recognition, with6% picking up spurious text and 8% being truncated.The noun phrase classifier was trained on 1350 EJVnoun phrases examined in context.
It took 14 hoursto manually mark these 1350 instances using a texteditor.Before we can go from CIRCUS output o templateinstantiations, we create intermediate structures calledmemory tokens.
Memory tokens incorporatecoreference decisions and structure relevantinformation to facilitate template generation.
Memorytokens record source strings from the original inputtext, OTB tags, MayTag features, and pointers toconcept nodes that exa'acted individual noun phrases.Discourse analysis contributes to critical decisionsassociated with memory tokens.
Here we find thegreatest challenges to trainable language systems.Thus far, we have implemented one trainablecomponent that contributes to coreference r solutionin limited contexts.
We isolate compound nounphrases that are syntactically consistent withappositive constructions and pass these NP pairs onto a coreference lassifier.
Since adjacent NPs may beseparated by a comma if they occur in a list or at aclause boundary, it is easy to confuse legitimateappositives with pairings of unrelated (but adjacent)NPs.
Appositive recognition is therefore treated as abinary classification problem that can be handled withcorpus-driven training.
For our official Tipster runswe trained aclassifier to handle appositive recognitionusing EJV development texts and then used theresulting classifier for both EJV and EME.
Our besttest results with this classifier showed an 87% hit rateon EJV appositives.
It took 10 hours to manuallyclassify 2276 training instances for the appositiveclassifier using a training interface.242I,text filePREPROCESSOR!SPECIALISTSITRAINABLE P-O-STAGGERpreprocessed t xt fileCIRCUSSENTENCEANALYZr~RIIRAINABLE APPOSITIVERECOGNITIONIAUTOMATED CNDICTIONARY!TRAINABLE SEMANTICTAGGER!TRAINABLE NPANALY~Rvl,'~....... ~d key temp~_~.....~,Jmemory tokens4,LEXICALCOREFERENCEII I TRAINABLE TEMPLATEGENERATORl4,final template instanfiation~ve lopment  tex~~ I~--- ,,...._ ~o, tom~,__.j'Figure 1: System Architecture243Our final tool, "ITG, is responsible for the creation oftemplate generators that map CIRCUS output intofinal template instantiations.
TTG templategenerators are responsible for the recognition andcreation of domain objects as well as the insertion ofrelational inks between domain objects.
'VFG iscorpus-driven and requires no human interventionduring training.
Application-specific a cess methods(pathing functions) must be hand-coded for a newdomain but these can be added to TTG in a few daysby a knowledgeable technician working with adequatedomain documentation.
Once these adjustments are inplace, TTG uses memory tokens and key templates totrain classifiers for template generation.
No furtherhuman intervention is required to create the templategenerators, although additional testing, tuning andadjustments are needed for optimal performance.Our hybrid architecture demonstrates how machinelearning capabilities can be utilized to acquire manydifferent kinds of knowledge from a corpus.
Thesesame acquisition techniques also make it easy toexploit the resulting knowledge without additionalknowledge ngineering or sophisticated reasoning.The knowledge we can readily acquire from a corpusof representative t xts is limited with respect oreusability, but nevertheless cost-effective in a systemdevelopment scenario predicated on customizedsoftware.
The trainable components used for bothEJV and EME were completed after 101 hours ofinteractive work by a human-in-the-loop.
Moreover,most of our training interfaces can be effectivelyoperated by domain experts: programming knowledgeor familiarity with computational linguistics isgenerally not required.
(Although one technicalbackground isneeded to train OTB.).
Near the end ofthis paper will report the results of a systemdevelopment experiment that supports this claim.There will always be a need for some amount ofmanual programming during the system developmentcycle for a new information extraction application.Even so, significant amounts of system developmentthat used to rely on experienced programmers havebeen shifted over to trainable language components.The ability to automate knowledge acquisition on thebasis of key templates represents a significantredistribution of labor away from skilled knowledgeengineers, who need access to domain knowledge,directly to the domain experts themselves.
By puttingdomain experts into the role of the human-in-the-loopwe can reduce dependence on software technicians.When significant amounts of system developmentwork is being handled by automated knowledgeacquisition and expert-assisted knowledge acquisition,it will become increasingly cost-effective tocustomize and maintain a variety of informationextraction applications.
We have only just begun toexplore the range of possibilities associated withtrainable language processing systems.The hybrid architecture underlying our official Tipstersystems was less than six months old at the time ofthe evaluation, and most of the trainable languagecomponents hat we utilized were less than a year old.Less than 24 person/months were expended for bothof the EJV and EME systems, although this estimateis confounded by the fact that trainable componentsand their associated interfaces were being designed,implemented, and tested by the same peopleresponsible for our Tipster system development.
Thecreation of a trainable system component represents aone-time system development investment that can beapplied to subsequent systems at much less overhead.Figure 1 outlines the basic flow-of-control throughthe major components of the UMass/Hughes Tipstersystem.
Note that most of the trainable componentsdepend only on the texts from the developmentcorpus.
The concept node dictionary and the trainabletemplate generator also rely on answer keys duringtraining.
In the case of the concept node dictionary,we have been able to drive our dictionary constructionprocess on the basis of annotated texts created byusing a point-and-click text marking interface.
So thesubstantial overhead associated with creating a largecollection of key templates i not needed to supportautomated dictionary construction.
However, we donot see how to support rainable template generationwithout a set of key templates, o this one trainablecomponent requires a significant investment withrespect to labor.SYSTEM EVOLUTION AND PROJECTGOALSOur preparation for Tipster was somewhat limitedrelative to other Tipster extraction sites.
We beganour effort one year later than other sites and we werenot funded to work with Japanese.
Our funding (andsystem development) began October 1, 1992 and the24-month evaluation took place in July 1993.
Duringthat period we redesigned the overall systemarchitecture initially described in our proposal, anddesigned a number of wainable system componentsfrom scratch.
OTB, AutoSlog, and MayTag werealready available, but needed to be trained and appliedto the new domains.
Two project personnel attendedthe 12-month Tipster meeting in September 1992,and that was our first introduction to the domainguidelines for EJV and EME.Our initial system design was based on a simplisticmodel of how the UMass and Hughes technologiesdemonstrated at MUC-3 and MUC4 might be brought244together into a hybrid information extraction system.The original idea was to pass CIRCUS output o"ITTG (the Hughes Trainable Template Generator)without further alteration to either CIRCUS or 'FIG.This approach was put into place for the 18-monthTipster evaluation in February 1993 and found to beinadequate for a number of reasons.
First, the outputof CIRCUS was highly fragmented and completedunstructured at the discourse level.
CIRCUS wasextracting but not organizing related information evenwhen that information all came from the samesentence.
So TFG was being asked to consolidateinformation that required reorganization at manylevels.
This problem was aggravated by the fact thatCIRCUS tended to create a lot of output for a giventext.
Some of this output was irrelevant or off-target,and some of it was legitimate but redundant.
TTGwas expected to sort out the good from the bad andnot get tangled up in the redundancies.
In fact, TFGwas designed to handle noise in the ~aining data, butthe amount of data being generated by CIRCUScreated massive training runs for "FIG.
TTG had neverbeen pushed this hard before, so we found ourselvescontending with memory limitation problems andlong runtimes.
Since TFG is not just one decisiontree but a collection of about 30 decision trees, theseadditional complications produced significantstumbling blocks.We barely had a working system in place for the 18-month evaluation in February and there was noopportunity to adjust this initial implementationbefore the evaluation.
As painful as it was to subjecta first-pass system to a formal evaluation, theexercise left no doubt about he problems inherent inour system design.
The hand-coded consolidationheuristics developed at UMass for MUC-3 and MUC-4 could not be functionally duplicated by TFG alone.Additional processing needed to be inserted betweenCIRCUS and TTG.
We were also losing a lot ofinformation that should have been recognized duringpreprocessing by low-level specialists designed topick up dates and locations.
These specialists werenot particularly challenging, but they did requite a lotof programming time and we had not made anadequate investment inour preprocessing specialists.At the same time, we were pleased with the trainablecomponents hat allowed us to customize CIRCUSfor two new domains.
The OTB part-of-speech taggerneeded some adjustments but seemed quite promising.The AutoSlog dictionary construction tool hadworked very well for EJV and reasonably well forEME.
We came to understand that EME was heavilydependent on keyword recognition for its technicalvocabulary.
AutoSlog had not been designed tocollect or organize xtensive synonym lists, but thiswas not an inherently difficult ask.Six months prior to the final Tipster evaluation, wehad to make some major decisions.
One of ouroptions was to hand-code new consofidation heuristicsfor FEIV and EME.
We knew how to do this based onsimilar efforts for MUC-3 and MUC-4, and weprobably would have been able to improve theperformance of our system dramatically in the sixmonths remaining to us by following this route.
Butwe had made a research commitment to investigatetrainable technologies for information extractionsystem development: we were interested in findingalternatives tohand-coded heuristics.
So we began tolook at the gap between CIRCUS and 'FIG with aneye for the problems that might be managed bytrainable decision trees.We identified three phenomena that seemed to becausing significant difficulties for us at the 18-monthevaluation: (1) noun phrase termination, (2)appositive recognition, and (3) coreference r solution.Noun phrase termination refers to the problem ofknowing when an NP can be extended across potentialboundary markers like commas, conjunctions andprepositional phrases.
Appositive recognition is asubset of the general coreference r solution problem,but we found it useful to separate out appositivesbecause appositive candidates can be reliablyrecognized on the basis of a syntactic pattern, whichmakes appositives somewhat easier to tackle than thegeneral coreference problem.We worked for a few weeks on the design of featurevectors for the appositive problem and the nounphrase termination problem.
Training instances werethen collected and our first decision trees for theseproblems were running in March.
Baselinecomparisons with hand-coded heuristics for nounphrase termination showed that a hand-codedcomponent was able to make noun phrase terminationdecisions correctly about 65% of the time.
ID3decision trees were showing us hit rates in the 80-85% range.
We found a similar level of success forour ID3 appositive d-trees.
These trees were generallyable to categorize potential appositive candidatescorrectly about 85% of the time.
Subsequentexperimentation with these modules during March andApril failed to improve our initial performance l vels.During this same period we were also first coming toappreciate the difficulties inherent in a system basedon many trainable components.
With changes tilloccurring to critical upstream components like theprocessing specialists and OTB, we knew that oldertraining data for AutoSlog and our ID3-basedcomponents was slowly falling out of sync with therest of the system.
It wasn't agood idea to wain a treeon one set of data and then test the tree on a245sul)stantially different data set.
If a training setincorporated a certain amount of noise (e.g.
false hitswith respect to locations), and the test data containedno noise or a much reduced noise level, an appositivetree or noun phrase termination tree might not operateas effectively as it would if the noise levels wereconsistent across both data sets.
But it wasn'tpractical for us to continually update the training setsin order to keep everything in phase.
So we wereconstantly waking with components hat were eitheroutdated or out of phase, and it was difficult o knowhow significant that complication would prove to be.We were also dealing with a lot of softwareengineering complexity in trying to manage all thedata sets, decision trees, and various configurations ofthe system under inspection.
Our Tipster systemdevelopment effort was proving to be much morecomplicated than our MUC-3 and MUC-4 efforts hadbeen.In all system development efforts, downstreamdevelopment depends on upstream stability, and thisis especially true when a number of trainablecomponents are involved.
The introduction of majornew components six months before the final Tipsterevaluation was far from ideal.
In particular, we wereup against he fact that we could not retrain TFGuntil the new components were producing output.
Weknew that TTG would benefit from a lot ofexperimentation, but we couldn't do anything aboutthat until we had new capabilities in place to handlenoun phrases, appositives, and coreference.
Nounphrase termination was very important for AutoSlog,so we held off on our final dictionary constructionuntil May in order to benefit from our progress onnoun phrase termination.In retrospect, we can also see that we were too slowto get started on the coreference module.
We knewthat coreference would benefit from noun phrasetermination and appositives, o it made sense to delaythe coreference work until we had made at least someprogress on these other components.
But we did notappreciate how much more complicated datacollection for coreference would prove to be, and wedid not allow enough time to design feature vectorsfor coreference.
Coreferenee data collection was nottackled before May, and progress on coreference wentmuch more slowly than expected.
We did eventuallytrain some decision trees for general coreferenceresolution, but our preliminary test results were notstrong, so we were forced to abandon trainablecoreference in the eleventh hour.
Manually-codedheuristics were then created in an effort o manage atleast some coreference decisions prior to templategeneration.The difficulties associated with coreference and thelast minute drive to pull together manual coreferenceheuristics prevented us from experimenting with "FIGas much as we would have liked.
Someimprovements were made to make TI'G more efficientand less memory intensive, but nothing could be donethat depended on memory token input.
In the end, wewere not able to begin TTG training andexperimentation u til July, at which time we werestruggling to produce a working system in time forthe final evaluation.The lack of complete system throughput until Julymeant that we couldn't run the scoring program inorder to obtain internal benchmarks during the sixmonths prior to the 24-month evaluation.
We ran thescoring program on EME output for the first time onJuly 21, and on F.JV output for the first time on July25.
The official test runs were executed on July 31.Our previous experience with MUC-3 and MUC-4taught us that significant improvements to scorereports can be made by studying high-frequency slotsand looking for simple adjustments that improveperformance for these slots.
There was no time toexperiment with any such adjustments in EJValthough some effort was made to optimize the EMEsystem in response to some internal testing based onthe 18-month test set.
Unfortunately, these EMEadjustments may have backfired for reasons that wewill describe in the next section.Despite our abbreviated development schedule anddifficulties establishing upstream system stability, welearned a lot about he integration of machine learningtechniques into natural language processing systems.Although Tipster was originally conceived to obtain acomparative valuation of mature text processingtechnologies, we always viewed our Tipster effort asbeing somewhat more exploratory in nature.
No onehad previously attempted tointegrate NLP techniqueswith machine learning techniques at any of theprevious MUC conferences.
The UMass/Hughessystem represented an ambitious undertaking thatwould have benefited greatly from another year ofcollaboration.THE OFF IC IAL  TEST  RUNSOur official test runs were conducted on threeDECstations running Allegro Common Lisp.
Theruns went smoothly in EME but we encountered onefatal error in one of the EJV test sets.
Portions of ourofficial EJV and EME score reports are shown infigures 2 and 3.246AO = all objectsMO = matched objectsTF = text filteringFM = F-measuresFM I 48 I 17F-measures?
J I~ IB  BI~IBB~Ez t i  i rm i i r r ln i, l i  i~ l i  ilrlrti mEP&R 2P&R P&2RFigure 2: Official EJV Score Report Summaries~ I I il~l~i BEgin i l t ' tn nixP&R 2P&R P&2RiF-measures \[ 34 .84 \ [  37 .42 i  32.591Figure 3: Official EME Score Report Summaries~ i l k l l l~ l  E E IEE .~!
E ~1~ EL l  i l l  i~ l l f t l  E~J\[F-measurqP&R 2P&R P&2ROVG37Figure 4: Unofficial Tips2 EME Score ReportsBased on the minimal amount of internal testingconducted prior to the official test runs, we weresurprised to see that our EME scores were about thesame as our EJV scores.
We had expected to do muchbetter on the EME test set.
In the week prior to thefinal evaluation, we made adjustments o the EMEsystem based on internal testing with the Tips2 testset from the 18-month evaluation.
Subsequent testingon Tips2 suggested that we should have done muchbetter on the official test runs.We had restricted our feedback loop to this one testset because we knew that answer keys compiled forofficial test sets tend to be more reliable than answerkeys in the general development corpus.
We alsoexpected the test sets from the 18-month evaluationto be predictive of the test sets used for the 24-monthevaluation.
Unfortunately, this turned out not to bethe case with the Tips2 EME test set.
After the finalevaluation, we discovered that Tips2 contained adisproportionate number of short texts (97% of theTips2 texts were less than 2200 bytes long).
We hadinadvertently tuned our EME system for short textswithout realizing it.
The EME test set used for the24-month evaluation contained a large number oftexts that were significantly longer than 2200 bytes.Tips3/lTips3/2Tips3/3Tips2the complete test setE \]g:'lrl IIF Ii791271391321 7713313713sl 7S13S1401381671441521471the short texts onlyIE IR IP IFI Jm l l~ lE lEE ImEgiE  i lmEE InE = Error rate R = Recall P = Precision F = P&R F-score(all objects cores only)Figure 5: EME Tuning Effects for Short TextsWe can see the effects of this disparity betweentraining materials and test materials ff we compare theoverall EME score reports with score reports based ononly a subset of the shorter EME test texts.
Figure 5shows how our system would have performed if theEME test set had been restricted to shorter texts thatwere consistent with the Tips2 test set.
When we testthe system on only the shorter texts from the Tips3test set, we see significantly higher test scores.
TheTips3 subset scores are a tittle weaker than the Tips2scores, but that difference is probably due to the factthat we were tuning the system in response to Tips2and we would therefore xpect o see our strongestpossible performance on those same texts.247THE OPTIONAL TEST RUNSWe ran optional tests to see what sort ofrecall/precision trade-offs were available from thesystem.
Since the template generator is a set ofclassifiers, and each classifier outputs a certaintyassociated with a hypothesized template fragment, wehave many parameters that can be manipulated.Raising the threshold on the certainty for ahypothesis will, in most cases, increase precision andreduce recall.
In the experiments reported here, wehave varied the parameters over broad classes ofdiscrimination trees.
There are three importantclasses of decision tree: (1) trees that filter thecreation of objects based on string fills, (2) trees thatfalter the Creation of objects based on set fills, and (3)trees that hypothesize r lations among objects.
Anexample of the first class is the tree that filters theCIRCUS output for entity names in the FJV domain.An example of the second class is the tree that filterspossible lithography objects based on evidence of thetype of lithography process.
The trees thathypothesize TIE_UP_RELATIONSHIP's andMI~CAPABILITY's are examples of the third class.For these experiments we have varied the certaintythresholds for all trees of a given class.
Figure 6shows the trade-off achieved for EME.c:a==?
),.807060504030201000I1 ?=1I I !
I30 40 10 20Reca l lFigure 6: Trade-off curve for EMEC _oW80706050403020100?
?11I I I10 20Reca l l30I40Figure 7: Trade-off curve for EJV248This trade-off curve was achieved by varying, inconcert the thresholds on all three classes ofdiscrimination tree from 0.0 to 0.9.
Figure 7 showsthe trade-off curve achieved in EJV.
The differencebetween the two curves highlights difference betweenthe two domains and between the systemconfigurations used for the two domains.
The EMEcurve shows a much more dramatic trade-off.
TheEJV curve shows that only modest varying of recalland precision is achievable.
Part of this is areflection of the two domains.
In EJV, mostrelationships were found via two noun phrases thatshared acommon CN trigger.
This method proved tobe effective at detecting relationships.
Therefore theonly real difference in the trade-off comes fromvarying the thresholds for the string-fall and set-filltrees, which generate the objects that are thencomposed into relationships.
In EME, there notnearly as many shared triggers and so the templategenerator must attempt intelligent guesses forrelations.
The probabilistic guesses made in EME aremuch more amenable to threshold manipulation thanthe more structured information used in EJV.
Also,in EJV the system ran with a slot masseur thatembodied some domain knowledge.
In EJV, TFGwas configured to only hypothesize objects ff the slotmasseur had found a reasonable slot-fill or set-fill.This use of domain knowledge further limited theefficacy of changing certainty thresholds.TRAINABLE INFORMATIONEXTRACTION IN ACT IONBefore CIRCUS can tackle an input sentence, wehave to pass the source text through a preprocessorthat locates sentence boundaries and reworks thesource text into a list structure.
The preprocessorreplaces punctuation marks with special symbols andappfies text processing specialists to pick up dates,locations, and other objects of interest o the targetdomain.
We use the same preprocessing specialistsfor both EJV and EME: many specialists will applyto multiple domains.
A subset of the Gazetteer wasused to support he location specialist, but no otherMRDs are used by the preprocessing specialists.
Wedo not have a specialist hat attempts to recognizecompany names.
Figure 8 shows sample outputfrom the preprocessor along with subsequentprocessing.Note that the date specialist had to consult thedateline of the source text in ~der to determine thatSource  Text  :BRIDGESTONE SPORTS CO. SA ID  FR IDAY IT HAS SET  UP A JO INT  VENTURE IN TA IWAN WITH A LOCALCONCERN AND A JAPANESE TRADING HOUSE TO PRODUCE GOLF  CLUBS TO BE SHIPPED TO JAPAN.Preprooessed  Text  :(*START* SRIDGESTONE SPORTS CO= SAID **ON 241189 W HAS SET UP A JOINT VENTURE IN @@ Taiwan WITI~I.A LOCALCONCERN AND A JAPANESE TRADING HOUSE T(.
)PRODUCE GOLF CLUBS TO BE SHIPPED TO ~-_Japan *END )OTB Tags:(*START* BRIDGESTONE SPORTS CO= SAID **ON_241189 1T HAS SET UP A JOINTst r t  nm nm noun verb  $date$  noun aug  pasp  ptc l  a r t  nmVENTURE IN @@_Taiwan WITH A LOCAL CONCERN AND A JAPANESE TRADINGnoun prep  $1ocat ion$  prep  ar t  nm noun con J  a r t  nm nmHOUSE TO PRODUCE GOLF CLUBS TO BE SHIPPED 1D @@japan *END*)noun in f  verb  nm noun in f  aux  pasp  prep  $1ocat ion$  s topexwacted NPs rejected CN features: accepted CN features:BRIDGESTONE SPORTS CO= iv-personIT iv-personA JOINT VENTUREGOLF CLUBSjv-enlib,, jv-p~'ent, jvjr-entity, companyjv-procLserv, produclionFigure 8: Preprocessing and CIRCUS analysis249"Friday" must refer to November 24, 1989.
Once thepreprocessor has completed its analysis, the OTBpart-of-speech tagger identifies parts of speech: OTBtagged 97.1% of the words in EJV 0592 correctly.One error associated with "... A COMPANY ACTIVE INTRADING WITH TAIWAN ..." led to a truncated nounphrase when "active" was tagged as a head nouninstead of a nominative predicate.With part-of-speech tags in place, CIRCUS can beginselective concept extraction.
On this first sentencefrom FjV 0592, CIRCUS triggers 18 CN definitionstriggered by the words "said" (3 CNs), "set" (3 CNs),"venture" (9 CNs), "produce" (1 CN), and "shipped"(2 CNs).
These CNs extract a number of key nounphrases, and assign semantic features to these nounphrases based on soft constraints in the CNdefinition.
Some of these features were recognized tobe inconsistent with the slot fill and others weredeemed acceptable.
For example, in Figure 8 we seethat different CNs picked up "8RIDGESTONE SPORTSCO."
with incompatible semantic features (it wasassociated with both a joint venture and a jo intventure parent feature).As we can see from this sentence, CN feature typesare not always reliable, and CIRCUS does not alwaysrecognize the violation of a soft feature constraint.
Anindependent set of semantic features are obtained fromMayTag.
In the first sentence of EJV 0592, MayTagonly missed marking "golf clubs" as a product/service.
An independent set of semantic features areobtained from MayTag as shown in Figure 9.In addition to extracting some noun phrases andassigning semantic features to those noun phrases, wealso call the noun phrase classifier to see if any of thesimple NPs picked up by .the CN definitions houldbe extended to longer NPs.
For this sentence, thenoun phrase classifier extended only one NP: itdecided that "A JOINT VENTURE" should be extended topick up "A JOINT VENTURE IN TAIWAN WITH A LOCALCONCERN AND A JAPANESE TRADING HOUSE'.
Thesecond prepositional phrase should not have beenincluded - this is an NP expansion that wasoverextended.Each noun phrase extracted by a CIRCUS conceptnode will eventually be preserved in a memory tokenthat records the CN features, MayTag features, anyNP extensions, and other information associated withCN definitions.
But before we look at the memorytokens, let's briefly review the other NPs that areextracted from the remainder of the text.
For eachpreprocessed sentence produced in response to EJV0592, we will put the noun phrases extracted byCIRCUS into boldface and use underlines to indicatehow the noun phrase classifier extends ome of theseNPs.words*START*BR IDGESTONESPORTSCO=SAIDITHASSETUPAJOINTVENTUREINJV -LOCATIONWITHALOCALCONCERNANDAJAPANESETRADINGHOUSETOPRODUCEGOLFCLUBSTOBESHIPPEDTOJV -LOCATIONMavTa~semanficfeamres((WS-JV-ENTITY) (WS-COMPANY-NAME))((WS-JV-ENTITY) (WS-COMPANY-NAME))((WS-JV-ENTITY) (WS-GENERIC-COMPANY))((WS-ENTITY) NIL)((WS-JV-ENTITY) NIL)((WS-JV-ENTITY) NIL)((WS-LOCATION) NIL)((WS-ENTITY) NIL)((WS-JV-ENTITY) NIL)( (WS-NATIONALITY) NIL)( (WS-PRODUCT-SERVICE)((WS-JV-ENTITY) NIL)((WS-ENTITY) NIL)((WS-ENTITY) NIL)(WS-SALES) )((WS-LOCATION) NIL)hits & misses; cor rect; cor rect; cor rect; cor rect; cor rect; cor rect; cor rect; cor rect; cor rect; cor rect; cor rect; cor rect; i ncor rect; i ncor rect; correctFigure 9: MayTag semantic feature tags250*START.
~IDGESTONE SPORTS CO=- SAID **0N_241189 ~ HAS SET UP A doiNT VENTURE IN ~ Talwan WITH A LEX~ALQONCERN ,a.ND A JAPANESE TRADING HOUSE TO PRODUCE .~.QJ,,E.bd=.U.~.a TO BE SHIPPED TO @@_Japan *END*) (*START"THE JOINT VENTURE $COMMA$ BRIDGESTONE SPORTS TAIWAN CO= $COMMA$ CAPITAUZED AT$COMMA$ WILL START PRODUCTION **DURING 0190 WITH PRODUCTION OF &&20000 IRON AND METAL WOOD CLUBS AMONTH *END*) (*START" THE MONTHLY OUTPUT WILL BE LATER RAISED TO &&50000 UNITS $COMMA$SAID *END*) (*START" THE NEW COMPANY $COMMA$ BASED IN KAOHSIUNG $COMMAS SOUTHERNTAIWAN $COMMA$ IS OWNED %%75 BY BRIDGESTONE SPORTS $COMMA$ %%15 BY UNION PRECISION CASTING CO=.
OF(~b Taiwall AND THE REMAINDER BY TAGA CO= SCOMMA$ A COMPANY ACTIVE IN TRADING WITH TAIWAN $COMMA$THE OFFICIALS SAID *END*) (*START* BRIDGESTONE SPORTS HAS S(~FAR BEEN ENTRUSTING PRODUCTION OFCLUB PARTS WITH UNION PRECISION CASTING AND OTHER TAIWAN COMPANIES *END*) (*START* WITH THEESTABLISHMENT OF THE TAIWAN UNIT $COMMA$ THE JAPANESE SPORTS G(X)DS MAKER PLANS TO INCREASEPRODUCTION OF LUXURY CLUBS IN ~ Jaoan *END*)Figure 10: Noun phrase analysisAs far as our CN dictionary coverage isconcerned, wewere able to identify all of the relevant noun phrasesneeded with the exception of'A LOCAL CONCERN ANDA JAPANESE TRADING HOUSE" which should have beenpicked up by a JV parent CN.
In fact, our AutoSlogdictionary had two such definitions in place forexactly this type of construction, but neitherdefinition was able to complete its instantiationbecause of a previously unknown problem with timestamps inside CIRCUS.
This was a processingfailure--not a dictionary failure.Trainable noun phrase analysis processes 13 of the 17NP instances hown in Figure 10 correctly.
Three ofthe NPs were expanded too fur, and one was expandedbut not quite far enough due to a tagging error byOTB ("a company active ...").
An inspection of the13 correct instances reveals that 7 of these would havebeen correctly terminated by simple heuristics basedon part-of-speech tags.
It is important to note that hetrainable NP analyzer had to deduce these more"obvious" heuristics in the same way that it deducesdecisions for more complicated ecisions.
It isencouraging to see that straightforwurd heuristics canbe acquired automatically by trainable classifiers.When our analyzer makes a mistake, it generallyhappens with the more complicated noun phrases(which is where hand-coded heuristics tend to breakdown as well).After the noun phrase classifier has attempted tofindthe best termination points for the relevant NPs, wethen call the coreference lassifier to consider pairs ofadjacent NPs separated by a comma.
In this text wefind three such appositive candidates (the second ofwhich contains an extended NP that was not properlyterminated):THE JOINT VENTURE, BRIDGESTONE SPORTS TAIWAN CO.TAGA CO., A COMPANY ACTIVETHE NEW COMPANY, BASED IN KAOHSlUNG, SOUTHERN TAIWANIn the third case, the location specialist failed torecognize ither Kaohsiung or Southern Taiwan asnames of locations.
On the other hand, the fragment"based in Kaohsiung" was recognized as a locationdescription and therefore reformatted it as "THE NEWCOMPANY (%BASED-IN% KAOHSIUNG), SOUTHERNTAIWAN" which set up the entire construct as anappositive candidate.
The coreference classifier thenwent on to accept each of these three instances asvalid appositive constructions.
This was the rightdecision in the first two cases, but wrong in the third.I f  ful l  location recognition had been working, thislast instance would have never been handed to thecoreference classifier in the first place.The coreference lassifier tells us when adjacent nounphrases hould be merged into a single memorytoken.
We also invoke some hand-coded heuristics forcoreference decisions that can be handled on the basisof lexical features alone.
These heuristics determinethat Bridgestone Sports Co. is coreferent withBridgestone Sports, and that "THE JOINT VENTURE,,BRIDGESTONE SPORTS TAIWAN CO." is coreferent with"A JOINT VENTURE IN TAIWAN ..." Our lexicalcoreference heuristics are nevertheless veryconservative, so they fail to merge our four productservice instances in spite of the fact that "clubs"appears in three of these string falls.
In effect, we passthe following memory token output o TTG:2515 recognized companies (#4 and #5 should have been merged):I 1) "TAGA CO=" aka "A COMPANY ACTIVE" 2) "UNION PRECISION CASTING CO= OF @@_TAIWAN (3) "BRIDGESTONE SPORTS CO=" aka "BRIDGESTONE SPORTS"(4) "THE NEW COMPANY ~%BASED-IN% KAOHSIUNG)" aka "SOUTHERN TAIWAN(5) "THE JOINT VENTURE aka "BRIDGESTONE SPORTS TAIWAN CO=" aka"A JOINT VENTURE IN @@_TAIWAN WITH A LEX~AL CONCERN AND A JAPANESE TRADING HOUSE"4 product service strings (all of these should have been merged):!
"GOLF CLU BS""&&20000 IRON AND METAL WCOD CLUBS""GOLF CLUB PARTS WITH UNION PRECISION CASTING AND OTHER TAIWAN COMPANIES""LUXURY CLUBS IN @@_JAPAN"1 ownership and 2 percent objects :(10) ~X)0000~rWD"(11) "%%15"(12) "%%75"We failed to extract "the remainder by ..." for the third ownership objectbecause out percentage specialist was not watching for non-numeric referentsin a perceaatage context - this could be fixed with an adjustment tothespecialist.Figure 11: Extracted Text StringsWhen TTG receives memory tokens as input, theobject existence classifiers try to filter out spuriousinformation picked up by overzealous CN definitions.Unfortunately, in the case of 0592, TTG filtered outtwo good memory tokens: (#1 describing the parentTago Co.), and (#5 describing the joint venture).
Itwas particularly damaging to throw away #5 becausethat memory token contained the correct companyname (Bndgestone Sports Taiwan Co.).
Of the 3remaining memory tokens describing companies,TTG correctly identified the two parent companies onthe basis of semantic features, but then it was forcedto pick up #4 as the child company.
Our pathingfunction was smart enough to know that -THE NEWCOMPANY" was probably not a good company name,but that left us with "SOUTHERN TAIWAN" for thecompany name.
So a failure that started with locationrecognition led to a mistake in trainable appositiverecognition, which then combined with a failure inlexical coreference r cognition and a filtering error byTTG in order to give us a joint venture named"SOUTHERN TAIWAN" instead of "BRIDGESTONESPORTS TAIWAN."
Overly aggressive Efltering by TTGresulted in the loss of our 4 product service memorytokens.Our CN instantiations do not explicitly representrelational information, but CNs that share a commontrigger word can be counted on to link two CNinstantiations in some kind of a relationship.
Triggerfamilies can reliably tell us when two entities are?
related, but they can't ell us what that relationship s.We relied on TTG to deduce specific relationships onthe basis of its training.
In cases like "75% BYBRIDGESTONE SPORTS', TTG had no trouble linkingextracted percentage objects with companies.
But ourtrainable link recognition ran into more difficultieswhen trigger families contained multiple companiesAmong the features that TTG had available fordiscrimination where closed class features, such asmemory token types, semantic features, and CNpatterns, and open class features (i.e.
trigger words).However, although there exist heuristics fordiscriminating relationships based on particularwords, the combination of the algorithms used (ID3)and the amount of data (600 stories) failed to inducethese heuristics.
There may be other algorithms,however, that can use the same or less data andexternal knowledge to derive such heuristics from thetraining data.The processing for EME proceeds very similarly toEJV, with the exception that MayTag is not used inour EME configuration, and in the EME system weused our standard CN mechanism and an additionalkeyword CN mechanism (KCN).
The KCNmechanism was used to recognize specific types ofprocessing, equipment, and devices that have one oronly a few possible manifestations.
In Figure 12 wesee the OTB tags for the first sentence, all of whichare correct.
In fact, for EME text 2789568, OTB had100% hit rate.The memory token structure in Figure 12 illustratesthe processing of the text prior to TTG.
Two NPsare identified as the same entity, "Nikon" and "NikonCorp."
The two NPs are merged into one memorytoken based on name merging heuristics.
The secondNP demonstrates how multiple recognitionmechanisms can add robustness to the processing.252*start* **DURING2Q91 $COMMA$ Nikon Corp= $LPAREN$ &&7731strt Sdate$ punc nm noun punc noun$RPAREN$ plans to market the NSR-1755EX8A $COMMA$ a new stepperpunc verb inf verb art noun punt art run nounintended for use in the production of &&64- Mbit DRAMs *end*pasp prep noun prep art noun prep nm nm noun stop( TOKEN(TYPE (ME-ENTITY))(SUBTYPE NIL)(RELATION NIL)(SLOT-FILLS(TYPE COMPANY)(NAME ( : SYM-L IST  N IKON CORP=) ) )(NPS(NP 2 1 (NIKON)(CNS %ME-ENT ITY -NAME-SUBJECT-VERB-AND-DO-STEPPER%)  )(NP 0 3 (NIKON CORP =)(CNS %ME-ENT I TY-NAME-  SUBJECT-VERB-AND- INF IN I  T IVE-P LANS-TO-MARKET % )(KCNS % KEYWORD-ME-ENTI  TY-CORP=%%LEAD-NP%) ) ) )Figure 12: EME processingFEATURE RELATION CERTAINTY AFTER FEATURE0.360.23 The process is not X-RAYThe entity is not tri~ered off "developed"The process is not CVDThe process is not LITHOGRAPHY of UKN typeThe process is not ETCHINGThe entity is not triggered off "from"0.140.030.040.060.12Figure 13: ME-CAPABILITY developer featuresFEATURE RELATION CERTAINTY AFTER FEATURE0.380.47 The process is not packagingThe entity is not in a PPA CN marked the entity as an entityThe process is not layering type s0uttering0.580.550.40Figure 14: ME-CAPABILITY distributor features"Nikon Corp." is picked up by both a CN triggeredoff of "plans to market" and by two KCNs, one thatlooks for "Corp." and another that looks for the leadNP in the story.Unfortunately, our system did not get any lithographyobjects for this story.
On our list of things to get toif time permitted was creating a lithography object foran otherwise orphaned stepper.
We would have onlygotten one lithography object since we merged allmentions of "stepper" into one memory token.We created a synthetic version of the system thatinserted a lithography memory token corresponding toeach stepper.
One was discard by 'rrG and anotherwas created because there were two different253equipment objects attached to the remaininglithography object.
The features that 'FIG used tohypothesize a new ME_CAPABILITY are illustrativeof one of the weaknesses of this particular method.TTG used the features in Figure 13 to decide not togenerate an MI~_CAPABILITY developer.All of the features are negative, and the absence ofeach feature reduces the certainty that the relationholds, because each feature's presence, broadlyspeaking, is positive evidence of a relation.Therefore, the node of the decision tree that is foundis a grouping of cases that have no particular positiveevidence to support he relation, but also no negativeevidence.
With the relation threshold set at 0.3, thisyields a negative identification of a relation.However, there are strong indications of a relationhere.
For example, the trigger "plans to market" isgood evidence of a relation, however, the naturedecision tree algorithms (recursively splitting thetraining data) causes us to lose that feature (in favorof other, better features).
Figure 14 shows whatfeatures "I'FG used to generate an MIX_CAPABILITYdistributor.Again, we do not see here the features that we wouldexpect, given the text.
A human generating ruleswould say that "plans to market" is a good indicationof a MECAPABILITY distributor.WHAT WORKS AND WHAT NEEDSWORKWhen we look at individual texts and work up a walkthrough analysis of what is and is not working, wefind that many of our trainable language componentsare working very well.
The dictionary coverageprovided by AutoSlog appears to be quite adequate.OTB is operating reliably enough for subsequentsentence analysis.
When we run into difficulties withour trainable components, we often fred that many ofthese difficulties tem from a mismatch of trainingdata with test data.
For example, when we trained thecoreference interface for appositive recognition, weeliminated from the training data all candidate pairsinvolving locations because the location specialistshould be identifying locations for us.
If thecoreference classifier were operating in an idealenvironment, it would never encounter unrecognizedlocations.
Unfortunately, aswe saw with EJV 0592,the location specialist does not trap all the locations,and this led to a bad coreference decision.
In an earlierversion of the coreference lassifier we had trained iton imperfect data containing unrecognized locations,but as the location specialist improved, we felt thatthe training for the coreference lassifier was fallingincreasingly out of sync with the rest of the systemso we updated it by eliminating all the locationinstances.
Then when the coreference lassifier wasconfronted with an unrecognized location, it failed toclassify it correctly.
When upstream systemcomponents are continually evolving (as they wereduring our TIPSTER development cycle), it isdifficult o synchronize downstream dependencies ntraining data.
A better system development cyclewould stabilize upstream components before trainingdownstream components in order to maintain the bestpossible synchronization across trainablecomponents.The importance of reliable representative trainingmaterials was demonstrated to us with even moreimpact after the final 24-month EME evaluationwhen we discovered that he EME test materials usedfor the 18-month evaluation were not representativeof the test materials used for the 24-monthevaluation.
As explained earlier, our EME systemwas tuned for shorter texts than we encountered in theofficial test sets.
With a trainable system, it is crucialto use representative texts for all parameter tuning.TTG was easy to tune by straightforwardexperimentation with different parameter settings, butwe failed to demonstrate its full utility in EMEbecause of the differences between Tips2 and Tips3.
'VFG nevertheless enhanced the output of CIRCUSand other discourse processing modules.
In module-specific testing TTG typically added 6-12% ofaccuracy in identifying domain objects andrelationships.
That added value is measured againstpicking that most likely class (yes or no) for aparticular domain object (e.g.
JV-ENTITY or ME-LITHOGRAPHY) or relationship (e.g.
JV-TIE-UP orME-MICROELECTRONICS -CAPABILITY).
How-ever, TTG fell far below our expectations for correctlyfiltering and connecting the parser's output.
We findtwo reasons for this short fall.
First, some amountof the deficit can be attributed to the systemdevelopment cycle since TFG sits at the end of thecycle of training and testing various modules.The second, and by far the dominant effect comesfrom the combination ofthe training algorithm (ID3)and the amount of data.
As mentioned previously,there are two types of features used by TI'G: (1)closed class (e.g.
token type, semantic features, andCN patterns) and (2) open class features (i.e.
CNtrigger words).
Using open class features can bedifficult, because most algorithms cannot detectreliable discriminating features if there are too manyfeatures---reliable features cannot be separated fromnoise.
Using trigger words in conjunction relationsbetween memory token results in 3,000-5,000 binaryfeatures.
With no noise suppression added to thealgorithm and given a large number of features, ID3254will create very deep decision trees that classifystories in the training set based on noise.We ran two sets of decision trees in deciding how toconfigure our system for the final test run.
MIN-TREES using only dosed class features and no noisesuppression and MAX-TREE using dosed class andopen class features and a noise suppression rule.
Thenoise suppression was a termination condition on therecursion of the ID3 algorithm.
Recursion wasterminated when all features resulted in creating anode that classified examples from few than 10different source texts.
Using closed class featuresrarely resulted in a terminal node that classifiedexamples from fewer than 10 stories.
In all tests theMAX-trees performed better.
However, as a result ofthe noise suppression, o decision tree contained verymany discriminations on a trigger.
The performanceof the MAX-trees indicated that individual words aregood discriminators, however their scarcity in thedecision trees indicates that we are not using theappropriate algorithm.
We believe that data-leanalgorithms (such as explanation-based l arning) inconcert with shared knowledge bases might beeffective.In attributing performance to various components, wemeasured 25 random texts in EME.
At the memorytoken stage we found that CIRCUS had extractedstring-fills and set-fills with a recall/precision of68/54.
However our score output for those slots was32/45 (measured only on the slots we attempted).Even when the thresholds for TrG were lowered to0.0, so that all output came through, the recall wasnot anywhere near 68.
Therefore it would appear thatthe difficult part of the template task is not findinggood things to put in the template, but figuring howto split and merge objects.
We do not (yet) have atrainable component that handles splitting andmerging decisions in general.The EJV and EME systems that we tested in ourofficial evaluation were in many ways incompletesystems.
Although our upstream components wereoperating reasonably well, additional feedback cycleswere badly needed for other components operatingdownstream.
In particular, trainable coreference andtrainable template generation did not received the limeand attention they deserve.
We are generallyencouraged by the success of our trainablecomponents for part-of-speech tagging, dictionarygeneration, oun phrase analysis, semantic featuretagging, and coreference based on appositiverecognition.
But we encountered substantialdifficulties with general coreference prior to templategeneration.
This appears to be the greatest challengeremaining for trainable components supportinginformation extraction.
We know from our earlierwork in the domain of terrorism that coreferenceresolution can be reasonably well-managed on thebasis of hand-coded heuristics \[Lehnert et al 1992b\].But this type of solution does not port acrossdomains and therefore represents a significant systemdevelopment bottleneck.
True portability will only beachieved with trainable coxeference apabilities.We believe that trainable discourse analysis was themajor stumbling block standing between our Tipstersystem and the performance l vels attained bysystems incorporating hand-coded discourse analysis.We remain optimistic that state-of-the-art performancewill be obtained by corpus-driven machine learningtechniques but it is clear that more research is neededto meet his very important challenge.
To facilitateresearch in this area by other sites, UMass will makeconcept extraction training d~!~ (CIRCUS output) forthe full EJV and EME corpora vailable to researchlaboratories with internet access.
When paired withTipster key templates available from the LinguisticData Consortium, this data will allow a wide range ofresearchers who may not be experts in naturallanguage to tackle the challenge of trainablecoreference and template generation as problems inmachine learning.
We believe it is important for theNLP community to encourage and support theinvolvement of a wider research community in ourquest for practical information extractiontechnologies.SYSTEM REQUIREMENTSThe final 24-month evaluation was conducted on oneDECstation 5000/240 and two DECstation5000/133s.
Each machine was configured with 64MBRAM and a 300 MB internal disk.
Two of thesemachines also had 1.35 GB external disks.
The5000/240 was running at 40 MHz and the 5000/133sran at 33 MHz.
All of our code was written inAllegro Common Lisp 4.1 and ran under the Ultrix4.2 operating system.Our source code occupies about 3.6 MB of disk space.The data used in the evaluation (including responsetemplates and trace files for both EJV and E/vIE) takesup 26.9 MB.
It took 27 hours of run time (distributedacross the three machines) to complete the EJV testsets.
It took 12 hours to complete the EME test sets.The difference in these run times is due to the factthat we used the MayTag semantic feature tagger onthe EJV texts but not the E/VIE texts.
No attempt wasmade to optimize runtimes in either system.Additional system development was also conducted ona number of Mac IIs configured with MacintoshCommon Lisp 2.0.
Both of our Tipster systems can255be run on a Mac (a minimum of 8 MB RAM isrecommended).
The AutoSlog training interface, OTBtraining interface, and appositive training interfacewere all implemented and run on MACs using theMacintosh Common Lisp Interface Toolkit, as wasthe system demo presented atthe 24-month meeting.ACKNOWLEDGMENTThis work was supported by the Department ofDefense under Contract No.
MDA904-92-C-2390.The views expressed in this paper are those of theauthors and should not be interpreted to representopinions or policies of the United StatesGovernment.BIBLIOGRAPHYCardie, C. (1993).
A Case-Based Approach toKnowledge Acquisition for Domain-Specific SentenceAnalysis.
Eleventh National Conference on ArtificialIntelligence (AAAI-93).
Washington, D.C. pp.
798-803.Dolan, C. P., Goldman, S. R., Cuda, T. V., &Nakamura, A. M. (1991).
Hughes Trainable TextSkimmer: Description of the 'ITS System as Used forMUC-3.
In B. Sundheim (Ed.
), Third MessageUnderstanding Conference (MUC-3).
Naval OceanSystems Center, San Diego California: MorganKaufmann.
pp.
155-162.Lehnert, W. (1991).
Symbolic/Subsymbolic SentenceAnalysis: Exploiting the Best of Two Worlds.Advances in Connectionist and Neural ComputationTheory.
Vol.
L (ed: J. Pollack and J. Barnden) AblexPublishing, Norwoed, New Jersey.
pp.
135-164.Lehnert, W., Cardie, C., Fisher, D., McCarthy, J.,Riloff, E., Soderland, S. (1992a).
University ofMassachusetts: MUC-4 Text Results and AnalysisProceedings of the Fourth Message UnderstandingConference (MUC-4).
Morgan Kaufmann.
SanMateo, CA.
pp.
151-158.Lehnert, W., Cardie, C., Fisher, D., McCarthy, J.,Riloff, E., Sederland, S. (1992b).
Description of theCIRCUS system as Used for MUC-4.
Proceedings ofthe Fourth Message Understanding Conference(MUC-4).
Morgan Kaufmann.
San Marco, CA.
pp.282-288.Quinlan, J. R. (1983).
Learning EfficientClassification Procedures and Their Appfication toChess End Games.
In R. S. Michalski, J. G.Carbonell, & T. M. Mitchell (Eds.
), MachineLearning: An Artificial Intelligence Approach.Morgan Kaufmann.
pp.
463-482.Riloff, E. (1993).
Automatically Constructing aDictionary for Information Extraction Tasks.Eleventh National Conference on ArtificialIntelligence (AAAI-93).
Washington, D.C. pp.
811-816.Riloff E., and Lehnert, W. (1993).
AutomatedDictionary Construction for Information Extractionfrom Text.
Proceedings of the Ninth IEEE Conferenceon Artificial Intelligence for Applications.
IEEEComputer S~iety Press.
pp.
93-99.256
