Adapting an NER-System for German to the Biomedical DomainMarc R?SSLERComputational LinguisticsUniversity Duisburg-EssenDuisburg ?
Germanymarc.roessler@uni-duisburg.deAbstractIn this paper, we report the adaptation of anamed entity recognition (NER) system to thebiomedical domain in order to participate inthe ?Shared Task Bio-Entity Recognition?.The system is originally developed for Ger-man NER that shares characteristics with thebiomedical task.
To facilitate adaptability, thesystem is knowledge-poor and utilizes unla-beled data.
Investigating the adaptability of thesingle components and the enhancements nec-essary, we get insights into the task of bio-entity recognition.1 IntroductionNER describes the detection and classification ofproper names into predefined categories.
Besidethe distinction between rule-based and automati-cally trained systems, the approaches can be classi-fied according to the amount of domain- and/orlinguistic knowledge they incorporate.In order to build an efficient and easy to adaptsystem, we developed a knowledge-poor approachthat is successful for German person names(R?ssler, 2004).
German NER shares some char-acteristics with bio-entity recognition such as theunreliable capitalization of names, the resultingdifficulties of boundary detection and the entailedtreatment of homonymic and polysemic items.
Webelieve that the process of adaptation is able tosketch out some interesting aspects of the biomedi-cal domain.In Section 2 we introduce the design guidelinesand the underlying model of our knowledge-poorapproach to NER.
In Section 3 we describe theadaptation of the system and the modifications andenhancements involved.
Section 4 introduces athree-level model to observe word forms that al-lows further improvements based on discourseunits and the utilization of unlabeled data.
Thesetechniques were successfully applied to Germanperson names, i.e.
they led to more than 10 pointsincrease in f-score, thus exhibiting state of the artperformance.
However, they completely failed onthe bio-entity task.
We will discuss what the failureof this technique reveals about the bio-entity task.Section 5 presents and discusses the final evalua-tion, while Section 6 contains some concludingremarks.2 A knowledge-poor approach to NERThe optimal practice in NER yields efficient andhighly reliable results based only on cheaply avail-able resources like an annotated corpus of reason-able size and non-annotated data.
Approaches richin handcrafted knowledge or dependent on otherlanguage technology tools suffer from severallimitations: They are laborious to maintain and toadapt to new domains, especially with respect tothe creation and evaluation of the domain-sensitivelists of NEs.
Furthermore, the application of addi-tional tools like part-of-speech tagger, syntacticchunker etc.
increases processing time, and it is notclear at the moment whether such tools facilitatethe task without additional adaptations to the newdomain.
In order to build an efficient and easy toadapt system, we developed a knowledge-poorapproach.
We refrain from?
any additional linguistic tools like morpho-logical analyser, part of speech tagger orsyntactic chunker;?
any handcrafted linguistic resources likedictionaries;?
any handcrafted knowledge providing listslike gazetteers, lists of NEs or lists of triggerwords.From a linguistic point of view, NEs are phe-nomena located at the phrase-level.
Nevertheless,for the sake of straightforwardness, we restrict ourmodel to single words.
To overcome the knowl-edge sparseness, the so-called three-level model ofword form observance was developed and success-fully applied to German person names (R?ssler,2004).
In Section 4 we discuss our attempts toapply this model to the biomedical domain.The approach is based on linear SVM classifiers.SVM (Vapnik, 1995) is a powerful machinelearning algorithm for binary classification able tohandle large numbers of parameters efficiently.
Itis common within the NLP community to useSVMs with non-linear kernels.
Takeuchi and Col-92lier (2003) successfully applied a polynomial ker-nel function for biomedical NER.
Beside the goodclassifier capabilities of non-linear kernels, theyare very expensive in terms of processing time fortraining and applying.
Therefore, we favor linearSVMs1 not suffering from these limitations.Instead of using surface words in combinationwith morphological analyses and/or handcraftedsuffix and prefix lists, we represent words with aset of positional character n-grams.
Using thetraining data, this set is compiled by extracting thelast uni- and bigram, three trigrams from the end,and three trigrams from the beginning of everyword.
All the entries occurring less than four timesare removed.
Table 1 contains an example of thisfeature set f3.
The representation is capable ofcapturing simple morphological regularities of NEsand the context words surrounding them.
Addi-tionally, we use deterministic word-surface fea-tures (feature set f1) commonly used in NER (seeBykel et al, 1997), indexing, for instance, whethera word form is capitalized, consists of numbers,contains capitals, etc.
We also consider wordlength and map it to one dimension (feature set f2).To capture the context of the word to classify, weset a six-word window, consisting of the threepreceding, the current, and the two succeedingwords.
All the features mentioned in Table 1 areextracted for all words of the defined window.f1 Word-surface feature like e.g.
?4-digit number?,?ATCG-sequence?, ?Uppercase only?
etc.f2 Character-based word lengthf3 Sub-word form representation with positionalcharacter n-grams.
?Hammer?
is represented as:?r?, ?er?, ?mer?
at the end, ?ham?
at first, ?amm?at second, ?mme?
at next to last position.f4 Probabilites of all classes if higher than zero,calculated by the second-order Markov Model.Table 1: The table shows the feature sets f1-f4extracted for all words of a 6-word window.
Fea-ture set f4 is described in Section 3.3 Adapting the SystemAfter adding ATCG sequence (see Shen et al2003) and GreekLetter (see Collier et al 2000) asdomain-specific deterministic word-surface fea-tures, we ran first experiments on the GENIA(2003) corpus.
While inspecting the results wenoticed that special attention was necessary toaddress the correct boundary detection of the enti-1 All experiments were conducted with the SVMlightsoftware package, freely available at:http://svmlight.joachims.org.ties and the transformation of the output of theSVM-classifiers to the IOB-notation.A first step to improve the boundary detection isbased on the output of a second-order MarkovModel in order to support the SVMs that are notoptimised to tag linear sequences.
We trained TnT(Brants, 1998), a Markov Model implemented forPOS-tagging on the surface words, and used theprobabilities for all classes as features for theSVMs (feature set f4 on Table 1).The second step was implemented within thepost-processing component designed to transformthe output of the SVM-classifiers to the IOB-notation.
In order to facilitate the multi-class out-put, we set up a total of seven classifiers: Five ofthem specific to the five NE-classes and two addi-tional classifiers assigning a general begin-tag anda general outside-tag.
Although a dynamic pro-gramming approach to resolve the multi-class issuefor SVMs is an important desideratum, we imple-mented a simple heuristic as a first step.To transform the output of the seven classifiersinto the IOB-output, we first applied a simple one-vs-rest method based on the decision values of theSVMs.
The general begin tag was used to supportthe correct detection of the B-tags.In a second post-processing step, we improvedthe results based on a definition of the revisabilityof a label assigned with respect to a competinglabel.
According to this, a label is revisable if thecompeting label is among the three best labels andhas a decision value higher than 0.2, or if the valueof the outside-classifier is lower than 0.2, i.e.
thelabel OUTSIDE is not that confident.
A label isconsidered to be competing to the current label if itwas assigned to the word before or the word after.4 Attempting to utilize the three-level modelto the biomedical domainThe three-level model described in R?ssler(2004) is motivated by the fact that lexical re-sources in the form of named entity lists deal withsurface words, i.e.
word forms, thus ignoring theproblems of homonymy and polysemy.To address this issue, we distinguish three dif-ferent levels to observe word forms and the se-mantic labels assigned to them and show how theyare related to and support the NER:?
The local level describes a single occurrenceof a word form.
The correct labelling ofthese occurrences is the actual task of NER.?
The discourse level describes all occurrencesof a word form within a text unit and thesemantic labels assigned to them.
Address-ing word sense disambiguation, Gale et al(1992) introduced the idea of a word senselocated at the discourse-level and observed a93strong one-sense-per-discourse tendency, i.e.several occurrences of a polysemous wordform have a tendency to belong to the samesemantic class within one discourse.
It iscommon practice in NER to utilize the dis-course level to disambiguate items in non-predictive contexts (see e.g.
Mikheev et al,1999).?
The corpus level describes all occurrences ofa word form within all texts available for theapplication.
The larger the corpus, the morelikely a particular word form is seen asmember of two or more semantic classes.In order to utilize the discourse level, all wordstagged as entity within one MEDLINE abstract arestored in a dynamic lexicon.
Then, the processeddiscourse unit is matched against the dynamic lexi-con in order to detect entities in non-predictivecontexts.
To find the correct boundaries the unit ispost-processed as described in Section 3.To reflect the issues concerning polysemy andhomonymy of lexical resources, we propose so-phisticated word-form based NE lists, representinghow likely a particular entry will be tagged with aparticular label.
These values are specific for acorpus, i.e.
they are located at the corpus level.To create such resources, we propose a form oflexical bootstrapping.
We assume that the prob-abilities calculated on the basis of a weak classifierapplied to a large unlabeled corpus are sufficientfor our task.
Therefore, we trained classifiers forall classes and applied them to a 30-million wordcorpus extracted from MEDLINE (1999), using thesearch term [?blood cell?
or ?transcription factor?
].This automatically annotated corpus was used tocreate a corpus specific lexicon containing about95,000 word forms.
For all these entries, we ex-tracted the total frequency of being tagged with aparticular label and the relative frequency of beingtagged with a discretized decision value by theSVM classifiers, i.e.
we set five thresholds andcounted how often an item was labelled with adecision value fulfilling a particular threshold.Both techniques completely failed: Neither theutilization of the discourse-level, nor the lexicalbootstrapping had a positive impact when appliedto the biomedical domain.
This raises the questionon the specifics of the biomedical domain.The utilization of the discourse-level is provedof value in most NE-tasks, thus the failure withinthe biomedical domain is surprising.
The one-sense-per-discourse tendency is obviously weakerin the biomedical domain, since genes and proteinscan share the same name and be mentioned in thesame abstract.
Additionally, the NEs occurringwithin the GENIA corpus consist in average ofmore than two words and seem to be diverse intheir appearance, even within one document.
Foralmost every word form, even brackets and stop-words can be a part of an NE, it is a great deal ofwork to develop heuristics improving recall with-out lowering precision dramatically.
Moreover, themethod is highly sensitive to precision errors, as itspreads out elements tagged incorrectly.
Further-more, it is questionable if abstracts ?
due to theirenormous density and shortness ?
are appropriatetext units for this method.The failure of the lexical bootstrapping is moredifficult to interpret since this technique is not thatwell-tested.
In our experiments, it was successfullyapplied to German person names and also hadsome positive impact on German organization andlocation names.
One source of problems can beseen in the low precision of the classifier used tocreate the annotated corpus.
We assume that ahigh-precision and low-recall classifier will pro-duce better lexical resources.
Another source canbe seen in the complexity and the length of bio-logical names.
The restriction to single words isprobably not appropriate for the bootstrappingprocess.
For future research, we will investigatethe bootstrapping of external evidence, i.e.
we willnot focus on the learning of names, but rather onthe units that indicate the beginning or the end of aname-class.5 EvaluationAll the evaluation was conducted on the corpusmade available for the shared task Bio-Entity Rec-ognition.
All configurations were trained on the2000 abstracts provided, i.e.
500,000 words to trainand we finally evaluated them on the 100,000words evaluation data.
Table 2 shows the scoresfor the different classifiers and components in thefirst rows, and the performance of the bestconfiguration evaluated for each NE-class.On the basis of the scores in Table 2 it is possi-ble to discuss the impact and values of the differentcomponents of the system.Using the surface words instead of f3, the sub-word-form representation with positional charactern-grams leads to a decrease of more than 2 pointsin terms of recall and precision.The f-score of the Markov Model, trained on theword forms, is almost comparable to the basicSVM-configuration f1-f3, but the precision of theSVM is higher.The post-processing component cannot be ap-plied to the output of the Markov Model, as thedefinition of the revisability is specifically de-signed for the output of the seven SVM-classifiers.The post-processing component shows very goodresults and leads to an increase of 4 points almost94equal for precision and recall, i.e.
the component isable to address the boundary detection problem bymeans of the definition of the revisability of a tagwith regard to a competing tag.f1 f2 f3 f4 postProcR P FMarkov Model only 62.6 54.1 58.0x x  2 57.9 54.4 56.1x x x 61.0 56.2 58.5x x x x 65.4 59.9 62.6x x x x 66.3 60.1 63.1Overall scorex x x x x 67.4 60.1 64.0protein x x x x x 72.9 62.0 67.0cell_line x x x x x 55.2 42.9 48.3DNA x x x x x 57.9 52.6 55.1cell_type x x x x x 62.7 70.6 66.5RNA x x x x x 44.1 49.5 46.7Table 2: Overall scores and scores for each NEclass.
See Table 1 for the feature sets f1-f4; post-Proc refers to the second post-processing compo-nent described in Section 3.Combining the basic SVM-configuration f1-f3with f4, the probabilities calculated by the MarkovModel, leads to a slight increase compared to thepost-processing component.
We are convinced thatboth the post processing and the Markov Modelcover similar phenomena by supporting the SVMto detect the correct boundaries.The combination of all feature sets f1-f4 with thepost-processing leads to a further increase of 1point, demonstrating the ability of the SVM tooptimize its predictions on heterogeneous knowl-edge sources.6 ConclusionWe have demonstrated the adaptation of an NEtagger originally developed for German to thebiomedical domain.
We believe that the process ofadaptation is able to sketch out some interestingaspects of the new domain.The names of the biomedical domain have mor-phological features that can be covered by the sub-word-form representation with positional charactern-grams.The failure of the techniques based on the three-level model indicate that the polysemic andhomonymic items and the complexity of biologicalnames hamper or even inhibit a furtheroptimization of models based on simple n-grams ofwords.
We believe that the consideration of more2 Instead of the positional character n-grams the sys-tem is trained on surface words.complex units and longer distant phenomena willlead to further progress in NE-tagging.
For thebiomedical domain, the work of Takeuchi andCollier (2003) demonstrates the successfulincorporation of shallow parsing.For future research, we plan to address these is-sues by focusing on learning external evidence, i.e.triggers and longer-distant phenomena from unla-beled texts.ReferencesD.
Bikel, S. Miller, R. Schwartz, and R. Weischedel.1997.
Nymble: a High-Performance LearningName-finder.
Proceedings of the Fifth Conferenceon Applied Natural Language Processing.
Wash-ington, DC.T.
Brants.
1998.
TnT - A Statistical Part-of-SpeechTagger.
Saarland University, ComputationalLinguistics.
Saarbruecken.
Available at:http://www.coli.uni-sb.de/~thorsten/tnt/.N.
Collier, C. Nobata, and J. Tsujii.
2000.
Ex-tracting the Names of Genes and Gene Productswith a Hidden Markov Model.
Proceedings ofCOLING'2000.
Saarbruecken.W.A.
Gale, K.W.
Church, and D. Yarowsky.
1992.One sense per discourse.
Proceedings of DARPAspeech and Natural Language Workshop.
Harri-man, NY.GENIA Corpus 2003.
Available at:http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/.MEDLINE.
1999.
The PubMed database is avail-able at: http://www.ncbi.nlm.nih.gov/PubMed/.A.
Mikheev, M. Moens, and C.Grover, C. 1999.Named Entity recognition without gazetteers.
Pro-ceedings of EACL'99.
Bergen.M.
R?ssler.
2004.
Corpus-based Learning of Lexi-cal Resources for German Named Entity Recog-nition.
Proceedings of LREC 2004.
Lisboa.D.
Shen, J. Zhang, G. Zhou, J. Su, and C. Tan.2003.
Effective Adaptation of Hidden MarkovModel-based Named Entity Recognizer for Bio-medical Domain.
Proceedings of the ACL 2003Workshop on Natural Language Processing inBiomedicine.
Sapporo.K.
Takeuchi and N. Collier.
2003.
Bio-MedicalEntity Extraction using Support Vector Ma-chines.
Proceedings of the ACL 2003 Workshopon Natural Language Processing in Biomedicine.ACL 2003.
Sapporo.V.
Vapnik.
1995.
Statistical Learning Theory.Springer.
New York.95
