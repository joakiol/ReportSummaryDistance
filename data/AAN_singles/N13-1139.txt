Proceedings of NAACL-HLT 2013, pages 1196?1205,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsOptimal Data Set Selection: An Application to Grapheme-to-PhonemeConversionYoung-Bum Kim and Benjamin SnyderUniversity of Wisconsin-Madison{ybkim,bsnyder}@cs.wisc.eduAbstractIn this paper we introduce the task of unla-beled, optimal, data set selection.
Given alarge pool of unlabeled examples, our goal isto select a small subset to label, which willyield a high performance supervised modelover the entire data set.
Our first proposedmethod, based on the rank-revealing QR ma-trix factorization, selects a subset of wordswhich span the entire word-space effectively.For our second method, we develop the con-cept of feature coverage which we optimizewith a greedy algorithm.
We apply thesemethods to the task of grapheme-to-phonemeprediction.
Experiments over a data-set of 8languages show that in all scenarios, our selec-tion methods are effective at yielding a small,but optimal set of labelled examples.
Whenfed into a state-of-the-art supervised model forgrapheme-to-phoneme prediction, our meth-ods yield average error reductions of 20% overrandomly selected examples.1 IntroductionOver the last 15 years, supervised statistical learninghas become the dominant paradigm for building nat-ural language technologies.
While the accuracy ofsupervised models can be high, expertly annotateddata sets exist for a small fraction of possible tasks,genres, and languages.
The would-be tool builderis thus often faced with the prospect of annotatingdata, using crowd-sourcing or domain experts.
Withlimited time and budget, the amount of data to be an-notated might be small, especially in the prototypingstage, when the exact specification of the predictiontask may still be in flux, and rapid prototypes aredesired.In this paper, we propose the problem of unsuper-vised, optimal data set selection.
Formally, givena large set X of n unlabeled examples, we mustselect a subset S ?
X of size k  n to label.Our goal is to select such a subset which, whenlabeled, will yield a high performance supervisedmodel over the entire data set X .
This task can bethought of as a zero-stage version of active learn-ing: we must choose a single batch of examples tolabel, without the benefit of any prior labelled datapoints.
This problem definition avoids the practicalcomplexity of the active learning set-up (many it-erations of learning and labeling), and ensures thatthe labeled examples are not tied to one particularmodel class or task, a well-known danger of activelearning (Settles, 2010).
Alternatively, our methodsmay be used to create the initial seed set for the ac-tive learner.Our initial testbed for optimal data set selec-tion is the task of grapheme-to-phoneme conver-sion.
In this task, we are given an out-of-vocabularyword, with the goal of predicting a sequence ofphonemes corresponding to its pronunciation.
Astraining data, we are given a pronunciation dic-tionary listing words alongside corresponding se-quences of phones, representing canonical pronun-ciations of those words.
Such dictionaries are usedas the final bridge between written and spoken lan-guage for technologies that span this divide, such asspeech recognition, text-to-speech generation, andspeech-to-speech language translation.
These dic-tionaries are necessary: the pronunciation of words1196continues to evolve after their written form has beenfixed, leading to a large number of rules and ir-regularities.
While large pronunciation dictionariesof over 100,000 words exist for several major lan-guages, these resources are entirely lacking for themajority of the world?s languages.
Our goal is toautomatically select a small but optimal subset ofwords to be annotated with pronunciation data.The main intuition behind our approach is thatthe subset of selected data points should efficientlycover the range of phenomena most commonly ob-served across the pool of unlabeled examples.
Weconsider two methods.
The first comes from a lineof research initiated by the numerical linear algebracommunity (Golub, 1965) and taken up by computerscience theoreticians (Boutsidis et al 2009), withthe name COLUMN SUBSET SELECTION PROBLEM(CSSP).
Given a matrixA, the goal of CSSP is to se-lect a subset of k columns whose span most closelycaptures the range of the full matrix.
In particu-lar, the matrix A?
formed by orthogonally project-ing A onto the k-dimensional space spanned by theselected columns should be a good approximationto A.
By defining AT to be our data matrix, whoserows correspond to words and whose columns corre-spond to features (character 4-grams), we can applythe CSSP randomized algorithm of (Boutsidis et al2009) on A to obtain a subset of k words which bestspan the entire space of words.Our second approach is based on a notion of fea-ture coverage.
We assume that the benefit of seeinga feature f in a selected word bears some positiverelationship to the frequency of f in the unlabeledpool.
However, we further assume that the lion?sshare of benefit accrues the first few times that welabel a word with feature f , with the marginal util-ity quickly tapering off as more such examples havebeen labeled.
We formalize this notion and providean exact greedy algorithm for selecting the k datapoints with maximal feature coverage.To assess the benefit of these methods, we ap-ply them to a suite of 8 languages with pronunci-ation dictionaries.
We consider ranges from 500to 2000 selected words and train a start-of-the-artgrapheme-to-phoneme prediction model (Bisani andNey, 2008).
Our experiments show that both meth-ods produce significant improvements in predictionquality over randomly selected words, with our fea-ture coverage method consistently outperformingthe randomized CSSP algorithm.
Over the 8 lan-guages, our method produces average reductions inerror of 20%.2 BackgroundGrapheme-to-phoneme Prediction The task ofgrapheme-to-phoneme conversion has been consid-ered in a variety of frameworks, including neuralnetworks (Sejnowski and Rosenberg, 1987), rule-based FSA?s (Kaplan and Kay, 1994), and pronun-ciation by analogy (Marchand and Damper, 2000).Our goal here is not to compare these methods, sowe focus on the probabilistic joint-sequence modelof Bisani and Ney (2008).
This model defines ajoint distribution over a grapheme sequence g ?
G?and a phoneme sequence ?
?
?
?, by way of anunobserved co-segmentation sequence q.
Each co-segmentation unit qi is called a graphone and con-sists of an aligned pair of zero or one graphemes andzero or one phonemes: qi ?
G?{}???
{}.1 Theprobability of a joint grapheme-phoneme sequenceis then obtained by summing over all possible co-segmentations:P (g,?)
=?q?S(g,?
)P (q)where S(g,?)
denotes the set of all graphone se-quences which yield g and ?.
The probability of agraphone sequence of length K is defined using anh-order Markov model with multinomial transitions:P (q) =k+1?i=1P (qi|qi?h, .
.
.
, qi?1)where special start and end symbols are assumed forqj<1 and qk+1, respectively.To deal with the unobserved co-segmentation se-quences, the authors develop an EM training regimethat avoids overfitting using a variety of smoothingand initialization techniques.
Their model producesstate-of-the-art or comparable accuracies across a1The model generalizes easily to graphones consisting ofmore than one grapheme or phoneme, but in both (Bisani andNey, 2008) and our initial experiments we found that the 01-to-01 model always performed best.1197wide range of languages and data sets.2 We use thepublicly available code provided by the authors.3 Inall our experiments we set h = 4 (i.e.
a 5-grammodel), as we found that accuracy tended to be flatfor h > 4.Active Learning for G2P Perhaps most closelyrelated to our work are the papers of Kominek andBlack (2006) and Dwyer and Kondrak (2009), bothof which use active learning to efficiently bootstrappronunciation dictionaries.
In the former, the au-thors develop an active learning word selection strat-egy for inducing pronunciation rules.
In fact, theirgreedy n-gram selection strategy shares some ofthe some intuition as our second data set selectionmethod, but they were unable to achieve any accu-racy gains over randomly selected words without ac-tive learning.Dwyer and Kondrak use a Query-by-Baggingactive learning strategy over decision tree learn-ers.
They find that their active learning strategyproduces higher accuracy across 5 of the 6 lan-guages that they explored (English being the ex-ception).
They extract further performance gainsthrough various refinements to their model.
Evenso, we found that the Bisani and Ney grapheme-to-phoneme (G2P) model (Bisani and Ney, 2008) al-ways achieved higher accuracy, even when trainedon random words.
Furthermore, the relative gainsthat we observe using our optimal data set selectionstrategies (without any active learning) are muchlarger than the relative gains of active learning foundin their study.Data Set Selection and Active LearningEck et al2005) developed a method for train-ing compact Machine Translation systems byselecting a subset of sentences with high n-gramcoverage.
Their selection criterion essentially cor-responds to our feature coverage selection methodusing coverage function cov2 (see Section 3.2).
Asour results will show, the use of a geometric featurediscount (cov3) provided better results in our task.Otherwise, we are not aware of previous work2We note that the discriminative model of Jiampojamarn andKondrak (2010) outperforms the Bisani and Ney model by anaverage of about 0.75 percentage points across five data sets.3http://www-i6.informatik.rwth-aachen.de/web/Software/g2p.htmlproposing optimal data set selection as a general re-search problem.
Of course, active learning strategiescan be employed for this task by starting with a smallrandom seed of examples and incrementally addingsmall batches.
Unfortunately, this can lead to data-sets that are biased to work well for one particularclass of models and task, but may otherwise performworse than a random set of examples (Settles, 2010,Section 6.6).
Furthermore the active learning set-up can be prohibitively tedious and slow.
To illus-trate, Dwyer and Kondrak (2009) used 190 iterationsof active learning to arrive at 2,000 words.
Eachiteration involves bootstrapping 10 different sam-ples, and training 10 corresponding learners.
Thus,in total, the underlying prediction model is trained1,900 times.
In contrast, our selection methods arefast, can select any number of data points in a sin-gle step, and are not tied to a particular predictiontask or model.
Furthermore, these methods can becombined with active learning in selecting the initialseed set.Unsupervised Feature Selection Finally, we notethat CSSP and related spectral methods have beenapplied to the problem of unsupervised feature se-lection (Stoppiglia et al 2003; Mao, 2005; Wolf andShashua, 2005; Zhao and Liu, 2007; Boutsidis et al2008).
These methods are related to dimensionalityreduction techniques such as Principal ComponentsAnalysis (PCA), but instead of truncating features inthe eigenbasis representation (where each feature isa linear combination of all the original features), thegoal is to remove dimensions in the standard basis,leading to a compact set of interpretable features.
Aslong as the discarded features can be well approxi-mated by a (linear) function of the selected features,the loss of information will be minimal.Our first method for optimal data-set creation ap-plies a randomized CSSP approach to the transposeof the data matrix, AT .
Equivalently, it selects theoptimal k rows ofA for embedding the full set of un-labeled examples.
We use a recently developed ran-domized algorithm (Boutsidis et al 2009), and anunderlying rank-revealing QR factorization (Golub,1965).1198(a) (b) (c)Figure 1: Various versions of the feature coverage function.
Panel (a) shows cov1 (Equation 5).
Panel (b) shows cov2(Equation 6).
Panel (c) shows cov3 (Equation 7) with discount factor ?
= 1.2.3 Two Methods for Optimal Data SetSelectionIn this section we detail our two proposed methodsfor optimal data set selection.
The key intuition isthat we would like to pick a subset of data pointswhich broadly and efficiently cover the features ofthe full range of data points.
We assume a large poolX of n unlabeled examples, and our goal is to se-lect a subset S ?
X of size k  n for labeling.We assume that each data point x ?
X is a vec-tor of m feature values.
Our first method applies toany real or complex feature space, while our secondmethod is specialized for binary features.
We willuse the (n ?
m) matrix A to denote our unlabeleddata: each row is a data point and each column isa feature.
In all our experiments, we used the pres-ence (1) or absence (0) of each character 4-gram asour set of features.3.1 Method 1: Row Subset SelectionTo motivate this method, first consider the task offinding a rank k approximation to the data matrix A.The SVD decomposition yields:A = U?V T?
U is (n ?
n) orthogonal and its columns formthe eigenvectors of AAT?
V is (m?m) orthogonal and its columns formthe eigenvectors of ATA?
?
is (n?m) diagonal, and its diagonal entriesare the singular values ofA (the square roots ofthe eigenvalues of both AAT and ATA).To obtain a rank k approximation to A, we start byrewriting the SVD decomposition as a sum:A =?
?i=1?iuivTi (1)where ?
= min(m,n), ?i is the ith diagonal entry of?, ui is the ith column of U , and vi is the ith columnof V .
To obtain a rank k approximation to A, wesimply truncate the sum in equation 1 to its first kterms, yielding Ak.
To evaluate the quality of thisapproximation, we can measure the Frobenius normof the residual matrix ||A ?
Ak||F .4 The Eckart-Young theorem (Eckart and Young, 1936) states thatAk is optimal in the following sense:Ak = argminA?
s.t.
rank(A?)=k||A?
A?||F (2)In other words, truncated SVD gives the best rankk approximation to A in terms of minimizing theFrobenius norm of the residual matrix.
In CSSP,the goal is similar, with the added constraint that theapproximation to A must be obtained by projectingonto the subspace spanned by a k-subset of the orig-inal rows of A.5 Formally, the goal is to produce a(k ?m) matrix S formed from rows of A, such that||A?AS+S||F (3)4The Frobenius norm ||M ||F is defined as the entry-wise L2norm:?
?i,j m2ij5Though usually framed in terms of column selection, weswitch to row selection here as our goal is to select data pointsrather than features.1199is minimized over all(nk)possible choices for S.Here S+ is the (m ?
k) Moore-Penrose pseudo-inverse of S, and S+S gives the orthogonal projec-tor onto the rowspace of S. In other words, our goalis to select k data points which serve as a good ap-proximate basis for all the data points.
Since AS+Scan be at most rank k, the constraint considered hereis stricter than that of Equation 1, so the truncatedSVD Ak gives a lower bound on the residual.Boutsidis et al2009) develop a randomized algo-rithm that produces a submatrix S (consisting of krows of A) which, with high probability, achieves aresidual bound of:||A?AS+S||F ?
O(k?log k)||A?Ak||F (4)in running time O(min{mn2,m2n}).
The algo-rithm proceeds in three steps: first by computing theSVD of A, then by randomly sampling O(k log k)rows of A with importance weights carefully com-puted from the SVD, and then applying a determin-istic rank-revealing QR factorization (Golub, 1965)to select k of the sampled rows.
To give some in-tuition, we now provide some background on rankrevealing factorizations.Rank revealing QR / LQ (RRQR) Every real(n?m) matrix can be factored asA = LQ, whereQis (m?m) orthogonal and L is (n?m) lower trian-gular.6 It is important to notice that in this triangularfactorization, each successive row of A introducesexactly one new basis vector from Q.
We can thusrepresent row i as a linear combination of the firsti?
1 rows along with the ith row of Q.A rank-revealing factorization is one which dis-plays the numerical rank of the matrix ?
defined tobe the singular value index r such that?r  ?r+1 = O()for machine precision .
In the case of the LQfactorization, our goal is to order the rows of Asuch that each successive row has decreasing rep-resentational importance as a basis for the futurerows.
More formally, If there exists a row permu-tation ?
such that ?A has a triangular factorization6We replace the standard upper triangular QR factorizationwith an equivalent lower triangular factorization LQ to focusintuition on the rowspace of A.Language Training Test TotalDutch 11,622 104,589 116,211English 11209 100891 112100French 2,748 24,721 27,469Frisian 6,198 55,778 61,976German 4,942 44,460 49,402Italian 7,529 79,133 86,662Norwegian 4,172 37,541 41,713Spanish 3,150 28,341 31,491Table 1: Pronunciation dictionary size for each of the lan-guages.
?A = LQ with L =[L11 0L21 L22], where the small-est singular value of L11 is much greater than thespectral norm of L22, which is itself almost zero:?min(L11) ||L22||2 = O()then we say that ?A = LQ is a rank-revealing LQfactorization.
Both L11 and L22 will be lower tri-angular matrices and if L11 is (r ?
r) then A hasnumerical rank r (Hong and Pan, 1992).Implementation In our implementation of theCSSP algorithm, we first prune away 4-gram fea-tures that appear in fewer than 3 words, then com-pute the SVD of the pruned data matrix usingthe PROPACK package,7 which efficiently handlessparse matrixes.
After sampling k log k words fromA (with sampling weights calculated from the top-ksingular vectors), we form a submatrix B consist-ing of the sampled words.
We then use the RRQRimplementation from ACM Algorithm 782 (Bischofand Quintana-Ort?
?, 1998) (routine DGEQPX) tocompute ?B = LQ.
We finally select the first krows of ?B as our optimal data set.
Even for ourlargest data sets (English and Dutch), this entire pro-cedure runs in less than an hour on a 3.4Ghz quad-core i7 desktop with 32 GB of RAM.3.2 Method 2: Feature Coverage MaximizationIn our previous approach, we adopted a generalmethod for approximating a matrix with a subset ofrows (or columns).
Here we develop a novel objec-tive function with the specific aim of optimal data setselection.
Our key assumption is that the benefit of7http://soi.stanford.edu/?rmunk/PROPACK/1200seeing a new feature f in a selected data point bearsa positive relationship to the frequency of f in theunlabeled pool of words.
However, we further as-sume that the lion?s share of benefit accrues quickly,with the marginal utility quickly tapering off as welabel more and more examples with feature f .
Notethat for this method, we assume a boolean featurespace.To formalize this intuition, we will define the cov-erage of a selected (k ?m) submatrix S consistingof rows ofA, with respect to a feature index j.
For il-lustration purposes, we will list three alternative def-initions:cov1(S; j) = ||sj ||1 (5)cov2(S; j) = ||aj ||1 I(||sj ||1 > 0)(6)cov3(S; j) = ||aj ||1 ?||aj ||1?||sj ||1I(||sj ||1 < ||aj ||1)(7)In all cases, sj refers the jth column of S, aj refersthe jth column of A, I(?)
is a 0-1 indicator function,and ?
is a scalar discount factor.8Figure 1 provides an intuitive explanation of thesefunctions: cov1 simply counts the number of se-lected data points with boolean feature j.
Thus, fullcoverage (||aj ||: the entire number of data pointswith the feature) is only achieved when all datapoints with the feature are selected.
cov2 lies at theopposite extreme.
Even a single selected data pointwith feature j triggers coverage of the entire feature.Finally, cov3 is designed so that the coverage scalesmonotonically as additional data points with featurej are selected.
The first selected data point will cap-ture all but 1?
of the total coverage, and each furtherselected data point will capture all but 1?
of what-ever coverage remains.
Essentially, the coverage fora feature scales as a geometric series in the numberof selected examples having that feature.To ensure that the total coverage (?|aj ||1) isachieved when all the data points are selected, weadd an indicator function for the case of ||cj ||1 =||aj ||1 .98Chosen to be 5 in all our experiments.
We experimentedwith several values between 2 and 10, without significant dif-ferences in results.9Otherwise, the geometric coverage function would con-verge to ||aj || only as ||cj || ?
?.500 Words 2000 WordsRAND CSSP FEAT RAND CSSP FEATDut 48.2 50.8 59.3 69.8 75.0 77.8Eng 25.4 26.5 29.5 40.3 40.1 42.8Fra 66.9 69.2 72.1 81.2 82.0 84.8Fri 42.7 48.0 53.6 62.2 65.3 68.5Ger 55.2 58.6 65.0 74.2 78.6 80.8Ita 80.6 82.8 82.8 85.3 86.1 86.8Nor 48.1 49.5 55.0 66.1 69.9 71.6Spa 90.7 96.8 95.0 98.1 98.4 99.0avg 57.2 60.3 64.0 72.2 74.4 76.5Table 2: Test word accuracy across the 8 languages forrandomly selected words (RAND), CSSP matrix subsetselection (CSSP), and Feature Coverage Maximization(FEAT).
We show results for 500 and 2000 word train-ing sets.Setting our feature coverage function to cov3, wecan now define the overall feature coverage of theselected points as:coverage(S) =1||A||1?jcov3(S; j) (8)where ||A||1 is the L1 entrywise matrix norm,?i,j |Aij |, which ensures that 0 ?
coverage(S) ?1 with equality only achieved when S = A, i.e.when all data points have been selected.We provide a brief sketch of our optimization al-gorithm: To pick the subset S of k words whichoptimizes Equation 8, we incrementally build opti-mal subsets S?
?
S of size k?
< k. At each stage,we keep track of the unclaimed coverage associatedwith each feature j:unclaimed(j) = ||aj ||1 ?
cov3(S?
; j)To add a new word, we scan through the pool of re-maining words, and calculate the additional cover-age that selecting word w would achieve:?
(w) =?feature j in wunclaimed(j)(?
?
1?
)We greedily select the word which adds the mostcoverage, remove it from the pool, and update theunclaimed feature coverages.
It is easy to show thatthis greedy algorithm is globally optimal.1201500 1000 1500 200025303540English500 1000 1500 200070758085 French500 1000 1500 20009092949698Spanish500 1000 1500 20008081828384858687 Italian500 1000 1500 2000556065707580German500 1000 1500 20005055606570Norwegian500 1000 1500 200050556065707580 Dutch500 1000 1500 2000455055606570 FrisianFeat Coverage RRQR RandomFigure 2: Test word accuracy across the 8 languages for (1) feature coverage, (2) CSSP matrix subset selection, (3)and randomly selected words.4 Experiments and AnalysisTo test the effectiveness of the two proposed dataset selection methods, we conduct grapheme-to-phoneme prediction experiments across a test suiteof 8 languages: Dutch, English, French, Frisian,German, Italian, Norwegian, and Spanish.
The datawas obtained from the PASCAL Letter-to-PhonemeConversion Challenge,10 and was processed tomatch the setup of Dwyer and Kondrak (2009).The data comes from a range of sources, includ-ing CELEX for Dutch and German (Baayen et al1995), BRULEX for French (Mousty et al 1990),CMUDict for English,11 the Italian Festival Dictio-nary (Cosi et al 2000), as well as pronunciation dic-tionaries for Spanish, Norwegian, and Frisian (orig-inal provenance not clear).As Table 1 shows, the size of the dictionariesranges from 31,491 words (Spanish) up to 116,211words (Dutch).
We follow the PASCAL challengetraining and test folds, treating the training set as ourpool of words to be selected for labeling.Results We consider training subsets of sizes 500,1000, 1500, and 2000.
For our baseline, we train the10http://pascallin.ecs.soton.ac.uk/Challenges/PRONALSYL/11http://www.speech.cs.cmu.edu/cgi-bin/cmudictG2P model (Bisani and Ney, 2008) on randomly se-lected words of each size, and average the resultsover 10 runs.
We follow the same procedure forour two data set selection methods.
Figure 2 plotsthe word prediction accuracy for all three meth-ods across the eight languages with varying trainingsizes, while Table 2 provides corresponding numer-ical results.
We see that in all scenarios the two dataset selection strategies fare better than random sub-sets of words.In all but one case, the feature coverage methodyields the best performance (with the exception ofSpanish trained with 500 words, where the CSSPyields the best results).
Feature coverage achievesaverage error reduction of 20% over the randomlyselected training words across the different lan-guages and training set sizes.Coverage variants We also experimented withthe other versions of the feature coverage functiondiscussed in Section 3.2 (see Figure 1).
While cov1tended to perform quite poorly (usually worse thanrandom), cov2 ?
which gives full credit for eachfeature the first time it is seen ?
yields results justslightly worse than the CSSP matrix method on av-erage, and always better than random.
In the 2000word scenario, for example, cov2 achieves averageaccuracy of 74.0, just a bit below the 74.4 accuracyof the CSSP method.
It is also possible that more1202RAND CSSP FEAT SVDFra 0.66 0.62 0.65 0.51Fry 0.75 0.72 0.75 0.6Ger 0.71 0.67 0.71 0.55Ita 0.64 0.61 0.67 0.49Nor 0.7 0.61 0.64 0.5Spa 0.65 0.67 0.68 0.53avg 0.69 0.65 0.68 0.53Table 3: Residual matrix norm across 6 languages forrandomly selected words (RAND), CSSP matrix subsetselection (CSSP), feature coverage maximization (FEAT),and the rank k SVD (SVD).
Lower is better.RAND CSSP FEATDut 0.66 0.72 0.81Eng 0.52 0.58 0.69Fra 0.68 0.74 0.81Fry 0.7 0.79 0.84Ger 0.68 0.74 0.81Ita 0.79 0.84 0.9Nor 0.7 0.79 0.84Spa 0.67 0.75 0.8avg 0.68 0.74 0.81Table 4: Feature coverage across the 8 languages for ran-domly selected words (RAND), CSSP matrix subset selec-tion (CSSP), and feature coverage maximization (FEAT).Higher is better.careful tuning of the discount factor ?
of cov3 wouldyield further gains.Optimization Analysis Both the CSSP and fea-ture coverage methods have clearly defined objec-tive functions ?
formulated in Equations 3 and 8,respectively.
We can therefore ask how well eachmethods fares in optimizing either one of the twoobjectives.First we consider the objective of the CSSP al-gorithm: to find k data points which can accuratelyembed the entire data matrix.
Once the data pointsare selected, we compute the orthogonal projectionof the data matrix onto the submatrix, obtaining anapproximation matrix A?.
We can then measure theresidual norm as a fraction of the original matrixnorm:||A?
A?||F||A||F(9)As noted in Section 3.1, truncated SVD minimizesthe residual over all rank k matrices, so we can com-CSSP FEAT FEAT-SLSfettered internationalization ratingexceptionally underestimating oversgellert schellinger nationdaughtry barristers schermanblowed constellations olingerharmonium complementing andersoncassini bergerman interrupees characteristically statedtewksbury heatherington pressley overstated connerTable 5: Top 10 words selected by CSSP, feature cov-erage (FEAT), and feature coverage with stratified lengthsampling (FEAT-SLS)pare our three methods ?
random selections, CSSP,and feature coverage ?
all of which select k exam-ples as a basis, against the lower bound given bySVD.
Table 3 shows the result of this analysis fork = 2000 (Note that we were unable to computethe projection matrices for English and Dutch dueto the size of the data and memory limitations).
Asexpected, SVD fares the best, with CSSP as a some-what distant second.
On average, feature coverageseems to do a bit better than random.A similar analysis for the feature coverage objec-tive function is shown in Table 4.
Unsurprisingly,this objective is best optimized by the feature cov-erage method.
Interestingly though, CSSP seemsto perform about halfway between random and thefeature coverage method.
This makes some sense,as good basis data points will tend to have frequentfeatures, while at the same time being maximallyspread out from one another.
We also note thatthe poor coverage result for English in Table 4 mir-rors its overall poor performance in the G2P predic-tion task ?
not only are the phoneme labels unpre-dictable, but the input data itself is wild and hard tocompress.Stratified length sampling As Table 5 shows,the top 10 words selected by the feature coveragemethod are mostly long and unusual, averaging 13.3characters in length.
In light of the potential an-notation burden, we developed a stratified samplingstrategy to ensure typical word lengths.
Before se-lecting each new word, we first sample a word lengthaccording to the empirical word length distribution.We then choose among words of the sampled length1203according to the feature coverage criterion.
This re-sults in more typical words of average length, withonly a very small drop in performance.5 Conclusion and Future WorkIn this paper we proposed the task of optimal dataset selection in the unsupervised setting.
In contrastto active learning, our methods do not require re-peated training of multiple models and iterative an-notations.
Since the methods are unsupervised, theyalso avoid tying the selected data set to a particularmodel class (or even task).We proposed two methods for optimally select-ing a small subset of examples for labeling.
Thefirst uses techniques developed by the numerical lin-ear algebra and theory communities for approximat-ing matrices with subsets of columns or rows.
Forour second method, we developed a novel notionof feature coverage.
Experiments on the task ofgrapheme-to-phoneme prediction across eight lan-guages show that our method yields performanceimprovements in all scenarios, averaging 20% re-duction in error.
For future work, we intend to applythe data set selection strategies to other NLP tasks,such as the optimal selection of sentences for tag-ging and parsing.AcknowledgmentsThe authors thank the reviewers and acknowledgesupport by the NSF (grant IIS-1116676) and a re-search gift from Google.
Any opinions, findings, orconclusions are those of the authors, and do not nec-essarily reflect the views of the NSF.ReferencesRH Baayen, R. Piepenbrock, and L. Gulikers.
1995.The celex lexical database (version release 2)[cd-rom].Philadelphia, PA: Linguistic Data Consortium, Uni-versity of Pennsylvania.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5):434?451, 5.C.H.
Bischof and G.
Quintana-Ort??.
1998.
Algorithm782: codes for rank-revealing qr factorizations ofdense matrices.
ACM Transactions on MathematicalSoftware (TOMS), 24(2):254?257.C.
Boutsidis, M.W.
Mahoney, and P. Drineas.
2008.
Un-supervised feature selection for principal componentsanalysis.
In Proceeding of the 14th ACM SIGKDDinternational conference on Knowledge discovery anddata mining, pages 61?69.C.
Boutsidis, M. W. Mahoney, and P. Drineas.
2009.An improved approximation algorithm for the columnsubset selection problem.
In Proceedings of the twen-tieth Annual ACM-SIAM Symposium on Discrete Al-gorithms, pages 968?977.
Society for Industrial andApplied Mathematics.P.
Cosi, R. Gretter, and F. Tesser.
2000.
Festivalparla italiano.
Proceedings of GFS2000, Giornate delGruppo di Fonetica Sperimentale, Padova.K.
Dwyer and G. Kondrak.
2009.
Reducing the anno-tation effort for letter-to-phoneme conversion.
In Pro-ceedings of the ACL, pages 127?135.
Association forComputational Linguistics.Matthias Eck, Stephan Vogel, and Alex Waibel.
2005.Low cost portability for statistical machine translationbased on n-gram coverage.
In Proceedings of the Ma-chine Translation Summit X.C.
Eckart and G. Young.
1936.
The approximation ofone matrix by another of lower rank.
Psychometrika,1(3):211?218.G.
Golub.
1965.
Numerical methods for solving lin-ear least squares problems.
Numerische Mathematik,7(3):206?216.Yoo Pyo Hong and C-T Pan.
1992.
Rank-revealingfactorizations and the singular value decomposition.Mathematics of Computation, 58(197):213?232.S.
Jiampojamarn and G. Kondrak.
2010.
Letter-phonemealignment: An exploration.
In Proceedings of theACL, pages 780?788.
Association for ComputationalLinguistics.R.M.
Kaplan and M. Kay.
1994.
Regular models ofphonological rule systems.
Computational linguistics,20(3):331?378.J.
Kominek and A. W. Black.
2006.
Learning pronunci-ation dictionaries: language complexity and word se-lection strategies.
In Proceedings of the NAACL, pages232?239.
Association for Computational Linguistics.K.Z.
Mao.
2005.
Identifying critical variables of prin-cipal components for unsupervised feature selection.Systems, Man, and Cybernetics, Part B: Cybernetics,IEEE Transactions on, 35(2):339?344.Y.
Marchand and R.I. Damper.
2000.
A multistrategy ap-proach to improving pronunciation by analogy.
Com-putational Linguistics, 26(2):195?219.P.
Mousty, M. Radeau, et al1990.
Brulex.
une base dedonne?es lexicales informatise?e pour le franc?ais e?crit etparle?.
L?anne?e psychologique, 90(4):551?566.T.J.
Sejnowski and C.R.
Rosenberg.
1987.
Parallel net-works that learn to pronounce english text.
Complexsystems, 1(1):145?168.1204Burr Settles.
2010.
Active learning literature survey.Technical Report TR1648, Department of ComputerSciences, University of Wisconsin-Madison.H.
Stoppiglia, G. Dreyfus, R. Dubois, and Y. Oussar.2003.
Ranking a random feature for variable and fea-ture selection.
The Journal of Machine Learning Re-search, 3:1399?1414.L.
Wolf and A. Shashua.
2005.
Feature selection for un-supervised and supervised inference: The emergenceof sparsity in a weight-based approach.
The Journalof Machine Learning Research, 6:1855?1887.Z.
Zhao and H. Liu.
2007.
Spectral feature selection forsupervised and unsupervised learning.
In Proceedingsof the ICML, pages 1151?1157.1205
