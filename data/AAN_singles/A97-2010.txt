A Broad-Coverage Word Sense TaggerDekang L inDepartment  of Computer  ScienceUniversity of ManitobaWinnipeg, Manitoba, Canada R3T 2N2l indek@cs.umanitoba.caPrevious corpus-based Word Sense Disambigua-tion (WSD) algorithms (Hearst, 1991; Bruce andWiebe, 1994; Leacock et al, 1996; Ng and Lee,1996; Yarowsky, 1992; Yarowsky, 1995) determinethe meanings of polysemous words by exploitingtheir local contexts .
A basic intuition that un-derlies those algorithms is the following:(1) Two occurrences of the same word haveidentical meanings if they have similar localcontexts.In other words, previous corpus-based WSD algo-rithms learn to disambiguate a polysemous wordfrom previous usages of the same word.
This hasseveral undesirable consequences.
Firstly, a wordmust occur thousands of times before a good clas-sifter can be trained.
There are thousands of poly-semous words, e.g., 11,562 polysemous nouns inWordNet (Miller, 1990).
For every polysemousword to occur thousands of times each, the corpusmust contain billions of words.
Secondly, learningto disambiguate a word from the previous usages ofthe same word means that whatever was learnedfor one word is not used on other words, whichobviously missed generality in natural languages.Thirdly, these algorithms cannot deal with wordsfor which classifiers have not been trained, whichexplains why most previous WSD algorithms onlydeal with a dozen of polysemous words.We demonstrate a new WSD algorithm that re-lies on a different intuition:(2) Two different words are likely to have similarmeanings if they occur in identical ocalcontexts.The local context of a word is defined in our algo-rithm as a syntactic dependency relationship thatthe word participates in.
To disambiguate a pol-ysemous word, we search a local context databaseto retrieve the list of words (called selectors) thatappeared in the same local context as the polyse-mous word in the training corpus.
The meaning ofthe polysemous word is determined by maximizingits similarity to the selectors.For example, consider the sentence:(3) The new facility will employ 500 of theexisting 600 employeesThe word "facility" has 5 possible meanings inWordNet 1.5:1. installation2.
proficiency/technique3.
adeptness4.
readiness5.
toilet/bathroomSince the word "facility" is the subject of "em-ploy" and is modified by "new" in (3), we retrieveother words that appeared in the same contextsand obtain the following two groups of selectors(the log A column shows the likelihood ratios (Dun-ning, 1993) of these words in the local contexts):?
Subjects of "employ" with top-20 highest likeli-hood ratios:word freq , Iog,k word freqORG" 64 50.4plant 14 31.0company 27 28.6operation 8 23.0industry 9 14.6firm 8 13.5pirate 2 12.1unit 9 9.32shift 3 8.48postal service 2 7.73machine 3 6.56corporat ion 3 6.47manufacturer  3 6.21insurance company 2 6.06aerospace 2 5.81memory  device 1 5.79depar tment  3 5.55foreign office 1 5.41enterprise 2 5.39pilot 2 537*ORG includes all proper names recognized as organizations18?
Modifiees of "new" with top-20 highest likeli-hood ratios:word freq log ,kpost 432 952.9issue 805 902.8product  675 888.6rule 459 875.8law 356 541.5technology 237 382.7generat ion 150 323.2model 207 319.3job 260 269.2system 318 251.8word freq log )~bonds  223 245.4cap i ta l  178 241.8order  228 236.5version 158 223.7posit ion 236 207.3high 152 201.2cont ract  279 198.1bill 208 194.9venture 123 193.7program 283 183.8Since the similarity between Sense 1 of "facility"and the selectors is greater than that of othersenses, the word "facility" in (3) is tagged "SenseThe key innovation of our algorithm is that apolysemous word is disambiguated with past us-ages of other words.
Whether or not it appears inthe training corpus is irrelevant.Compared with previous corpus-based algo-rithms, our approach offers several advantages:?
The same knowledge sources are used for allwords, as opposed to using a separate classifierfor each individual word.
For example, the sameset of selectors can also be used to disambiguate"school" in "the new school employed 100 peo-ple".?
It requires a much smaller training corpus thatneeds not be sense-tagged.?
It is able to deal with words that are infrequentor do not even appear in the training corpus.?
The same mechanism can also be used to inferthe semantic ategories of unknown words.In the demonstrated system, the local contextdatabase is constructed with 8,665,362 dependencyrelationships that are extracted from a 25-million-word Wall Street Journal corpus.
The corpusis parsed with a broad-coverage parser, PRINCI-PAR, in 126 hours on a SPARC-Ultra 1/140 with96MB of memory.
The nouns in the input text aretagged with their senses in WordNet 1.5.
Propernouns that do not contain simple markers (e.g.,Mr., Inc.) to indicate their categories are treatedas 3-way ambiguous and are tagged as "group","person", or "location" by the system.ReferencesRebecca Bruce and Janyce Wiebe.
1994.
Word-sense disambiguation using decomposable mod-els.
In Proceedings of the 32nd Annual Meetingo,f the Associations/or Computational Linguis-tics, pages 139-145, Las Cruces, New Mexico.Ted Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Computa-tional Linguistics, 19(1):61-74, March.Marti Hearst.
1991. noun homograph disambigua-tion using local context in large text corpora.
InConference on Research and Development in In-/ormation Retrieval ACM/SIGIR, pages 36-47,Pittsburgh, PA.Claudia Leacock, Goeffrey Towwell, and Ellen M.Voorhees.
1996.
Towards building contextualrepresentations of word senses using statisticalmodels.
In Corpus Processing for Lexical Acqui-sition, chapter 6, pages 97-113.
The MIT Press.George A. Miller.
1990.
WordNet: An on-linelexical database.
International Journal of Lexi-cography, 3(4):235-312.Hwee Tow Ng and Hian Beng Lee.
1996.
Integrat-ing multiple knowledge sources to disambiguateword sense: An examplar-based approach.
InProceedings of 34th Annual Meeting of the As-sociation for Computational Linguistics, pages40-47, Santa Cruz, California.David Yarowsky.
1992.
Word-sense disambigua-tion using statistical models of Roger's cate-gories trained on large corpora.
In Proceedingsof COLING-92, Nantes, France.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of 33rd Annual Meeting o/the As-sociation /or Computational Linguistics, pages189-196, Cambridge, Massachusetts, June.19
