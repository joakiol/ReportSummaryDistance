Grammatical  analysis in the OVIS spoken-dialogue systemMark- Jan  NederhofGosse  BoumaRob Koe l ingGer t jan  van  NoordFaculty of Arts, Humani t ies  Comput ing  & BCNUniversity of GroningenP.O.
Box 716, NL-9700 AS Groningen, The Nether lands{markj an,gosse,koel ing,vannoord} @let.rug.nlAbst rac tWe argue that grammatical processing is aviable alternative to concept spotting forprocessing spoken input in a practical dia-logue system.
We discuss the structure ofthe grammar, the properties of the parser,and a method for achieving robustness.
Wediscuss test results suggesting that gram-matical processing allows fast and accurateprocessing of spoken input.1 IntroductionThe NWO Priority Programme Language andSpeech Technology is a research programme aim-ing at the development of spoken language informa-tion systems.
Its immediate goal is to develop ademonstrator f a public transport information sys-tem, which operates over ordinary telephone lines.This demonstrator is called OVIS, Openbaar Ver-voer Informatie Systeem (Public Transport Informa-tion Systern).
The language of the system is Dutch.At present, a prototype is in operation, which is aversion of a German system developed by PhilipsDialogue Systems in Aachen (Aust et al, 1995),adapted to Dutch.This German system processes poken input us-ing "concept spotting", which means that the small-est information-carrying units in the input are ex-tracted, such as names of train stations and expres-sions of time, and these are translated more or lessindividually into updates of the internal databaserepresenting the dialogue state.
The words betweenthe concepts thus perceived are ignored.The use of concept spotting is common in spoken-language information systems (Ward, 1989; Jacksonet al, 1991; Aust et al, 1995; Allen et al, 1996).Arguments in favour of this kind of shallow parsingis that it is relatively easy to develop the NLP com-ponent, since larger sentence constructs do not haveto be taken into account, and that the robustnessof the parser is enhanced, since sources of ungram-maticality occurring between concepts are skippedand therefore do not hinder the translation of theutterance to updates.The prototype presently under construction de-parts from the use of concept spotting.
The gram-mar for OVIS describes grarnrnat'ical user utterances,i.e.
whole sentences are described.
Yet, as part ofthis it also describes phrases uch as expressions oftime and prepositional phrases involving e.g.
trainstations, in other words, the former concepts.
Byan appropriate parsing algorithm one thus combinesthe robustness that can be achieved using conceptspotting with the flexibility of a sophisticated lan-guage model.The main objective of this paper is to show thatour grammatical pproach is feasible in terms of ac-curacy and computational resources, and thus is aviable alternative to pure concept spotting.Although the added benefit of grammatical nal-ysis over concept spotting is not clear for our rela-tively simple application, the grammatical pproachmay become ssential as soon as the application isextended insuch a way that mor~ complicated gram-matical constructions eed to be recognized.
In thatcase, simple concept spotting may not be able tocorrectly process all constructions, whereas the ca-pabilities of the grammatical pproach extend muchfurther.Whereas ome (e.g.
(Moore et al, 1989)) arguethat grammatical nalysis may improve recognitionaccuracy, our current experiments have as yet notbeen able to reveal a clear advantage in this respect.As the basis for our implementation wehave cho-sen definite-clause grammars (DCGs) (Pereira andWarren, 1980), a flexible formalism which is relatedto various kinds of common linguistics description,and which allows application of various parsing algo-rithms.
DCGs can be translated directly into Prolog,66for which interpreters and compilers exist that arefast enough to handle real-time processing of spokeninput.
The grammar for OVIS is in turn written ina way to allow an easy translation to pure DCGs.
1The structure of this paper is as follows.
In Sec-tion 2 we describe the grammar for OVIS, and inSection 3 we describe the output of the NLP mod-ule.
The robust parsing algorithm is described inSection 4.
Section 5 reports test results, showingthat grammatical nalysis allows fast and accurateprocessing of spoken input.2 A computat iona l  g rammar  forDutchIn developing the OVIS grammar we have tried tocombine the short-term goal of developing a gram-mar which meets the requirements imposed by theapplication (i.e.
robust processing of the output ofthe speech recognizer, extensive coverage of locativephrases and temporal expressions, and the construc-tion of fine-grained semantic representations) withthe long-term goal of developing a general, compu-tational, grammar which covers all the major con-structions of Dutch.The grammar currently covers the majority ofverbal subcategorization types (intransitives, tran-sitives, verbs selecting a PP, and modal and aux-iliary verbs), NP-syntax (including pre- and post-nominal modification, with the exception of relativeclauses), PP-syntax, the distribution of vP-modifiers,various clausal types (declaratives, yes/no and WH-questions, and subordinate clauses), all temporalexpressions and locative phrases relevant to thedomain, and various typical spoken-language con-structs.
Due to restrictions imposed by the speechrecognizer, the lexicon is relatively small (2000 wordforms, most of which are names of stations andcities).From a linguistic perspective, the OVIS-grammarcan be characterized as a constraint-based gram-mar, which makes heavy use of (multiple) inheri-tance.
As the grammar assumes quite complex lexi-cal signs, inheritance is absolutely essential for orga-nizing the lexicon succinctly.
However, we not onlyuse inheritance at the level of the lexicon (which is awell-known approach to computational lexica), buthave also structured the rule-component using in-heritance.An important restriction imposed by the gram-mar-parser interface is that rules must specify thecategory of their mothers and daughters.
That is,a DCGs are called pure if they do not contain any callsto external Prolog predicates.each rule must specify the type of sign of its motherand daughters.
A consequence of this requirementis that general rule-schemata, as used in CategorialGrammar and HPSG cannot be used directly in theOVIS grammar.
A rule which specifies that a headdaughter may combine with a complement daugh-ter, if this complement unifies with the first elementon SUBCAT of the head (i.e.
a version of the cate-gorial rule for functor-argument application) cannotbe implemented directly, as it leaves the categoriesof the daughters and mother unspecified.
Neverthe-less, capturing eneralizations of this type does seemdesirable.We have therefore adopted an architecture forgrammar rules similar to that of HPSG (Pollard andSag, 1994), in which individual rules are classified invarious tructures, which are in turn defined in termsof general principles.
For instance; the grammarcurrently contains several head-complement rules(which allow a verb, preposition, or determiner tocombine with one or more complements).
Theserules need only specify category-information andtherelative order of head and complement(s).
All otherinformation associated with the rule (concerning thematching of head-features, the instantiation of fea-tures used to code long-distance dependencies, andthe semantic effect of the rule) follows from thefact that the rules are instances of the class head-complement s ructure.
This class itself is defined interms of general principles, such as the head-feature,valence, filler and semantics principle.
Other rulesare defined in terms of the classes head-adjunct andhead-filler structure, which in thrn inherit from (asubset of) the general principles mentioned above.Thus, even though the grammar contains a rela-tively large number of rules (compared to lexicalistframeworks such as HPSG and cG), the redundancyin these rules is minimal.The resulting rammar has the interesting prop-erty that it combines the strong tendency towardslexicalism and positing general combinatoric ruleschemata present in frameworks such as HPSG withrelatively specific grammar rules to facilitate fficientprocessing.3 In teract ion  w i th  the  d ia loguemanagerThe semantic component of the grammar pro-duces (simplified) Quasi-Logical Forms (Alshawi,1992).
These are linguistically motivated, domain-independent representations of the meaning of ut-terances.67QLFS allow considerable underspecification.
Thisis convenient in this application because most am-biguities that arise, such as ambiguities of scope,do not need to be resolved.
These QLFs are trans-lated into domain-specific "updates" to be passedon to the dialogue manager (DM) for further process-ing.
The DM keeps track of the information providedby the user by maintaining an information state orform.
This form is a hierarchical structure, withslots and values for the origin and destination of aconnection, for the time at which the user wantsto arrive or depart, etc.
The distinction betweenslots and values can be regarded as a special case ofground and focus distinction (Vallduvi, 1990).
Up-dates specify the ground and focus of the user ut-terances.
For example, the utterance "No, I don'twant to travel to Leiden but to Abcoude/" yields thefollowing update:us erwant s. t ray  el.
de st inat ion.
(\[# place.town.leiden\] ;\[ !
place, town.
abcoude\] )One important property of this representation isthat it allows encoding of speech-act information.The "#" in the update means that the informationbetween the square brackets (representing the focusof the user-utterance) must be retracted, while the"!"
denotes the corrected information.4 Robust parsingThe input to the NLP module consists of word-graphs produced by the speech recognizer.
A word-graph is a compact representation for all lists ofwords that the speech recognizer hypothesizes fora spoken utterance.
The nodes of the graph repre-sent points in time, and an edge between two nodesrepresents a word that may have been uttered be-tween the corresponding points in time.
Each edgeis associated with an acoustic score representing ameasure of confidence that the word perceived thereis the word that was actually uttered.
These scoresare negative logarithms of probabilities and there-fore require addition as opposed to multiplicationwhen two scores are combined.At an early stage, the word-graph is optimized toeliminate the epsilon transitions.
Such transitionsrepresent periods of time when the speech recognizerhypothesizes that no words are uttered.
After thisoptimization, the word-graph contains exactly onestart node and one or more final nodes, associatedwith a score, representing a measure of confidencethat the utterance nds at that point.In the ideal case, the parser will find one or morepaths in a given word-graph that can be assignedan analysis according to the grammar, such that thepaths cover the complete time span of the utterance,i.e.
the paths lead from the start node to a final node.Each analysis gives rise to an update of the dialoguestate.
From that set of updates, one is then passedon to the dialogue manager.However, often no such paths can be found in theword-graph, due to:?
errors made by the speech recognizer,?
linguistic onstructions not covered in the gram-mar, and?
irregularities in the spoken utterance.Our solution is to allow recognition of paths in theword-graph that do not necessarily span the com-plete utterance.
Each path should be an instanceof some major category from th~ grammar, such asS, NP, PP, etc.
In our application, this often comesdown to categories uch as "temporal expression"and "locative phrases".
Such paths will be calledmaximal projections.
A list of maximal projectionsthat do not pair-wise overlap and that lie on a singlepath from the start node to a final node in the word-graph represents a reading of the utterance.
Thetransitions between the maxima!
projections will becalled skips.The optimal such list is computed, according tocriteria to be discussed below.
The categories ofthe maximal projections in the list are then com-bined and the update for the complete utterance iscomputed.
This last phase, contains, among otherthings, some domain-specific linguistic knowledgedealing with expressions that may be ungrammati-cal in other domains; e.g.
the utterance "AmsterdamRotterdam" does not exemplify a general grammat-ical construction of Dutch, but in the particular do-main of OVIS such an utterance occurs frequently,with the meaning "departure from Amsterdam andarrival in Rotterdam".We will now describe the robust parsing modulein more detail.
The first phase that is needed isthe application of a parsing algorithm which is suchthat:1. grammaticality is investigated for all paths, notonly for the complete paths from the first to afinal node in the word-graph, and2.
grammaticality of those paths is investigated foreach category from a fixed set.Almost any parsing technique, ,such as left-cornerparsing, LR parsing, etc., can be adapted so that68the first constraint above is satisfied; the second con-straint is achieved by structuring the grammar suchthat the top category directly generates a number ofgrammatical categories.The second phase is the selection of the optimallist of maximal projections lying on a single pathfrom the start node to a final node.
At each nodewe visit, we compute a partial score consisting of atuple (S, P, A), where S is the number of transitionson the path not part of a maximal projection (theskips), P is the number of maximal projections, Ais the sum of the acoustic scores of all the transi-tions on the path, including those internal in maxi-mal projections.
We define the relation ~ on triplessuch that ($1, P1, A1) ~ ($2, P2, A2) if and only if:?
S1 < $2, or?
$1 = 5'2 and P1 < P2, or?
$1 = $2 and P1 = P2 and A1 < A2.In words, for determining which triple has mini-mal score (i.e.
is optimal), the number of skips hasstrictly the highest importance, then the number ofprojections, and then the acoustic scores.Our branch-and-bound algorithm maintains apriority queue, which contains pairs of the form(g, (S, P, A)), consisting of a node g and a triple(S, P,A) found at the node, or pairs of the form(N, (S, P, A)), with the same meaning except thatN is now a final node of which the acoustic scoreis incorporated into A. Popping an element fromthe queue yields a pair of which the second ele-ment is an optimal triple with regard to the rela-tion ~ fined above.
Initially, the queue contains just(No, (0, 0, 0)), where No is the start node, and pos-sibly (No, (0, 0, A)), if No is also a final state withacoustic score A.A node N is marked as seen when a triple hasbeen encountered at N that must be optimal withrespect o all paths leading to N from the start node.The following is repeated until a final node isfound with an optimal triple:1.
Pop an optimal element from the queue.2.
If it is of the form (N, (S, P,A)) then return thepath leading to that triple at that node, andhalt.3.
Otherwise, let that element be (N, (S, P, A)).4.
If N was already marked as seen then abort thisiteration and return to step 1.5.
Mark N as seen.6.
For each maximal projection from N to M withacoustic score A' ,  enqueue (M, (S, P + t, A +A' ) ) .
If M is a final node with acoustic scoreA", then furthermore enqueue (M, (S, P+i ,  A+A' + A")).7.
For each transition from N to M with acousticscore A', enqueue (U, (S + 1, P, A + A')).
If Uis a final node with acoustic score A ~, then fur-thermore nqueue (U,  (S + 1, P, A + A' + A")).Besides S, P, and A, other factors can be takeninto account as well, such as the semant ic  score,which is obtained by comparing the updates corre-sponding to maximal projections with the meaningof the question generated by the system prior to theuser utterance.We are also experimenting with the bigram score.Bigrams attach a measure of likelihood to the occur-rence of a word given a preceding word.Note that when bigrams are used, simply labellingnodes in the graph as seen is nc~t a valid method toprevent recomputation of subpaths.
The requiredadaptation to the basic branch-and-bound algorithmis not discussed here.Also, in the actual implementation the X bestreadings are produced, instead of a single best read-ing.
This requires a generalization of the above pro-cedure so that instead of using the label "seen", weattach labels "seen i ?
imes" to each node, where0<i<X.5 EvaluationThis section evaluates the NLP component with re-spect to efficiency and accuracy.5.1 Test  setWe present a number of results to indicate how wellthe NLP component currently performs.
We useda corpus of more than 20K word-graphs, output ofa preliminary version of the speech recognizer, andtypical of the intended application.
The first 3800word-graphs of this set are semantically annotated.This set is used in the experiments below.
Somecharacteristics of this test set are given in Table 1.As can be seen from this table, this test set is consid-erably easier than the rest of this set.
For this rea-son, we also present results (where applicable) for aset of 5000 arbitrarily selected word-graphs.
At thetime of the experiment, no further annotated corpusmaterial was available to us.5.2 Eff iciencyWe report on two different experiments.
In the firstexperiment, he parser is given the utterance as it69graphs transitions words t /w w/gtest 5000 54687 16020 3.4 3.2test 3800 36074 13312 2.7 3.5total 21288 242010 70872 3.4 3.3Table 1: This table lists the number of transitions,the number of words of the actual utterances, theaverage number of transitions per word, and the av-erage number of words per utterances.was actually spoken (to simulate a situation in whichspeech recognition is perfect).
In the second exper-iment, the parser takes the full word-graph as itsinput.
The results are then passed on to the ro-bustness component.
We report on a version of therobustness component which incorporates bigram-scores (other versions are substantially faster).All experiments were performed on a HP-UX9000/780 machine with more than enough corememory.
Timings measure CPU-time and should beindependent ofthe load on the machine.
The timingsinclude all phases of the NLP component (includinglexical lookup, syntactic and semantic analysis, ro-bustness, and the compilation of semantic represen-tations into updates).
The parser is a head-cornerparser implemented (in SICStus Prolog) with selec-tive memoization and goal-weakening asdescribed in(van Noord, 1997).
Table 2 summarizes the resultsof these two experiments.From the experiments we can conclude that al-most all input word-graphs can be treated fastenough for practical applications.
In fact, we havefound that the few word-graphs which cannot betreated efficiently almost exclusively represent caseswhere speech recognition completely fails and nouseful combinations of edges can be found in theword-graph.
As a result, ignoring these few casesdoes not seem to result in a degradation of practicalsystem performance.5.3 AccuracyIn order to evaluate the accuracy of the NLP compo-nent, we used the same test set of 3800 word-graphs.For each of these graphs we know the correspondingactual utterances and the update as assigned by theannotators.
We report on word and sentence accu-racy, which is an indication of how well we are ableto choose the best path from the given word-graph,and on concept accuracy, which indicates how oftenthe analyses are correct.The string comparison on which sentence accu-racy and word accuracy are based is defined by theminimal number of substitutions, deletions and in-sertions that is required to turn the first string intothe second (Levenshtein distance).
The string thatis being compared with the actual utterance is de-fined as the best path through the word-graph, giventhe best-first search procedure defined in the previ-ous section.
Word accuracy is defined as 1 -  ~ wheren is the length of the actual utterance and d is thedistance as defined above.In order to characterize the test sets somewhatfurther, Table 3 lists the word and sentence accu-racy both of the best path through the word-graph(using acoustic scores only), the best possible paththrough the word-graph, and a combination of theacoustic score and a bigram language model.
Thefirst two of these can be seen as natural upper andlower boundaries.5.4 Concept  AccuracyWord accuracy provides a measure for the extentto which linguistic processing contributes to speechrecognition.
However, since the main task of the lin-guistic component is to analyze utterances semanti-cally, an equally important measure is concept ac-curacy, i.e.
the extent to which semantic analysiscorresponds with the meaning of the utterance thatwas actually produced by the user.For determining concept accuracy, we have useda semantically annotated corpus of 3800 user re-sponses.
Each user response was annotated withan update representing the meaning of the utter-ance that was actually spoken.
The annotationswere made by our project partners in Amsterdam,in accordance with the guidelines given in (Veldhuij-zen van Zanten, 1996).Updates take the form described in Section 3.
Anupdate is a logical formula which can be evaluatedagainst an information state and which gives rise to anew, updated information state.
The most straight-forward method for evaluating concept accuracy inthis setting is to compare (the normal form of) theupdate produced by the grammar with (the normalform of) the annotated update.
A major obstacle forthis approach, however, is the fact that very fine-grained semantic distinctions can be made in theupdate-language.
While these distinctions are rel-evant semantically (i.e.
in certain cases they maylead to slightly different updates of an informationstate), they often can be ignored by a dialogue man-ager.
For instance, the update below is semanticallynot equivalent to the one given in Section 3, as theground-focus distinction is slightly different.us erwant s .travel .destination.place( \[# town.leiden\] ;\[ !
town.
abcoude\] )70mode total msec msec/sent max msec max kbytes3800 graphs: user utterance5000 graphs:125290word-graph 303550user utterance 152940word-graph 477920100 200 500 10003800 graphs: 80.6 92.4 98.2 99.55000 graphs: 81.3 91.2 96.9 I 98.7 99232 330 ' 8680 8910 146130 630 19295 109801 47862000 5000!99.9 99.9\]99.5 .91Table 2: In the first table we list respectively the total number of milliseconds CPU-time ~equired for all 3800word-graphs, the average number of milliseconds per word-graph, and the maximum number of millisecondsfor a word-graph.
The final column lists the maximum space requirements (per word-graph, in Kbytes).
Forword-graphs the average CPU-times are actually quite misleading because CPU-times vary enormously fordifferent word-graphs.
For this reason, we present in the second table the proportion of word-graphs thatcan be treated by the NLP component within a given amount of CPU-time (in milliseconds).method3800 graphs: AcousticPossibleAcoustic + Bigram5000 graphs: AcousticPossibleAcoustic + BigramWA SA78.9 60.692.6 82.786.3 74.372.7 57.689.8 81.782.3 74.0Table 3: Word accuracy and sentence accuracy based on acoustic score only (Acoustic); using the bestpossible path through the word-graph, based on acoustic scores only (Possible); a combination of acousticscore and bigram score (Acoustic + Bigram), as reported by the current version of the system.However, the dialogue manager will decide in bothcases that this is a correction of the destinationtown.Since semantic analysis is the input for the dia-logue manager, we have therefore measured conceptaccuracy in terms of a simplified version of the up-date language.
Following the proposal in (Boros andothers, 1996), we translate ach update into a set ofsemantic units, were a unit in our case is a triple(CommunicativeFunction, Slot, Value).
For in-stance, the example above, as well as the example inSection 3, translates as(denial, destination_town, leiden)( corrections destination_town, abcoude )Both the updates in the annotated corpus and theupdates produced by the system were translated intosemantic units of the form given above.Semantic accuracy is given in the following tablesaccording to four different definitions.
Firstly, welist the proportion of utterances for which the corre-sponding semantic units exactly match the semanticunits of the annotation (match).
Furthermore wecalculate precision (the number of correct semanticunits divided by the number of semantic units whichwere produced) and recall (the number of correctsemantic units divided by the number of semanticunits of the annotation).
Finally, following (Borosand others, 1996), we also present concept accuracyasCA = IO0 (1 -  SUs + SUi SUD ) ~where SU is the total number of semantic units inthe translated corpus annotation, and SUs, SUt,and SUp are the number of substitutions, insertions,and deletions that are necessary to make the trans-lated grammar update equivalent to the translationof the corpus update.We obtained the results given in Table 4.The following reservations should be made withrespect o the numbers given above.
* The test set is not fully representative of thetask, because the word-graphs are relativelysimple.?
The test set was also used during the designof the grammar.
Therefor~ the experiment is71Method3800 graphs: user utteranceword-graphsword-graphs (+bigram)5000 graphs: word-graphsword-graphs (+bigram)WA SA85.3 72.986.5 75.179.5 70.082.4 74.2Semantic accuracymatch precision recall CA97.9 99.2 98.5 98.581.0 84.7 86.6 84.481.8 85.5 8'7.4 85.2Table 4: Evaluation of the NLP component with respect o word accuracy, sentence accuracy and conceptaccuracy.
Semantic accuracy consists of the percentage of graphs which receive a fully correct analysis(match), percentages for precision and recall of semantic slots, and concept accuracy.
The first row presentsthe results if the parser is given the actual user utterance (obviously WA and SA are meaningless in thiscase).
The second and third rows present he results for word-graphs.
In the third row 'bigram informationis incorporated in the robustness component.methodologically unsound since no clear sepa-ration exists between training and test material.?
Errors in the annotated corpus were correctedby us.?
Irrelevant differences between annotation andanalysis were ignored (for example in the caseof the station names cuijk and cuyk).Even if we take into account hese reservations, itseems that we can conclude that the robustness com-ponent adequately extracts useful information evenin cases where no full parse is possible: concept accu-racy is (luckily) much higher than sentence accuracy.Conc lus ionWe have argued in this paper that sophisticatedgrammatical nalysis in combination with a robustparser can be applied successfully as an ingredientof a spoken dialogue system.
Grammatical nalysisis thereby shown to be a viable alternative to tech-niques uch as concept spotting.
We showed that fora state-of-the-art pplication (public transport infor-mation system) grammatical nalysis can be appliedefficiently and effectively.
It is expected that the useof sophisticated grammatical nalysis allows for eas-ier construction of linguistically more complex spo-ken dialogue systems.AcknowledgmentsThis research is being carried out within the frame-work of the Priority Programme Language andSpeech Technology (TST).
The TST-Programme issponsored by NWO (Dutch Organization for Scien-tific Research).Re ferencesJ.F.
Allen, B.W.
Miller, E.K.
Ringger, and T. Si-korski.
1996.
A robust system for natural spokendialogue.
In 34th Annual Meeting of the Associ-ation for Computational Linguistics, Proceedingsof the Conference, pages 62-70, Santa Cruz, Cal-ifornia, USA, June.H.
Alshawi, editor.
1992.
The Core Language En-gine.
MIT Press.H.
Aust, M. Oerder, F. Seide, and V. Steinbiss.1995.
The Philips automatic train timetable infor-mation system.
Speech Communication, 17:249-262.M.
Boros et al 1996.
Towards t~nderstanding spon-taneous peech: word accuracy vs. concept ac-curacy.
In Proceedings of the Fourth Interna-tional Conference on Spoken Language Processing(ICSLP 96), Philadelphia.E.
Jackson, D. Appelt, J.
Bear, R. Moore, andA.
Podlozny.
1991.
A template matcher for robustNL interpretation.
In Speech and Natural Lan-guage Workshop, pages 190-~194, Pacific Grove,California, February.R.
Moore, F. Pereira, and H. Murveit.
1989.
Inte-grating speech and natural-language processing.In Speech and Natural Language Workshop, ages243-247, Philadelphia, Pennsylvania, February.F.C.N.
Pereira and D.H.D.
Warren.
1980.
Def-inite clause grammars for language analysis--asurvey of the formalism and a comparison withthe augmented transition etworks.
Artificial In-telligence, 13:231-278.C.
Pollard and I.A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.E.
Vallduvi.
1990.
The Informational Component.Ph.D.
thesis, University of Pennsylvania.72G.
van Noord.
1997.
An efficient implementation fthe head-corner parser.
Computational Linguis-tics, 23.
To appear.G.
Veldhuijzen van Zanten.
1996.
Semantics of up-date expressions.
Document No.
24, NWO pro-gramme Language and Speech Technology.W.
Ward.
1989.
Understanding spontaneousspeech.
In Speech and Nalural Language Work-shop, pages 137-141, Philadelphia, Pennsylvania,February.73
