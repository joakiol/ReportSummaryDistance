Proceedings of NAACL-HLT 2013, pages 148?157,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsExperiments with Spectral Learning of Latent-Variable PCFGsShay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar31Dept.
of Computer Science, Columbia University2Dept.
of Statistics/3Dept.
of Computer and Information Science, University of Pennsylvania{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.eduAbstractLatent-variable PCFGs (L-PCFGs) are ahighly successful model for natural languageparsing.
Recent work (Cohen et al 2012)has introduced a spectral algorithm for param-eter estimation of L-PCFGs, which?unlikethe EM algorithm?is guaranteed to give con-sistent parameter estimates (it has PAC-styleguarantees of sample complexity).
This paperdescribes experiments using the spectral algo-rithm.
We show that the algorithm providesmodels with the same accuracy as EM, but isan order of magnitude more efficient.
We de-scribe a number of key steps used to obtainthis level of performance; these should be rel-evant to other work on the application of spec-tral learning algorithms.
We view our resultsas strong empirical evidence for the viabilityof spectral methods as an alternative to EM.1 IntroductionLatent-variable PCFGS (L-PCFGs) are a highly suc-cessful model for natural language parsing (Mat-suzaki et al 2005; Petrov et al 2006).
Recentwork (Cohen et al 2012) has introduced a spectrallearning algorithm for L-PCFGs.
A crucial prop-erty of the algorithm is that it is guaranteed to pro-vide consistent parameter estimates?in fact it hasPAC-style guarantees of sample complexity.1 Thisis in contrast to the EM algorithm, the usual methodfor parameter estimation in L-PCFGs, which has theweaker guarantee of reaching a local maximum ofthe likelihood function.
The spectral algorithm isrelatively simple and efficient, relying on a singularvalue decomposition of the training examples, fol-lowed by a single pass over the data where parame-ter values are calculated.Cohen et al(2012) describe the algorithm, andthe theory behind it, but as yet no experimental re-sults have been reported for the method.
This paper1under assumptions on certain singular values in the model;see section 2.3.1.describes experiments on natural language parsingusing the spectral algorithm for parameter estima-tion.
The algorithm provides models with slightlyhigher accuracy than EM (88.05% F-measure on testdata for the spectral algorithm, vs 87.76% for EM),but is an order of magnitude more efficient (9h52mfor training, compared to 187h12m, a speed-up of19 times).We describe a number of key steps in obtain-ing this level of performance.
A simple backed-offsmoothing method is used to estimate the large num-ber of parameters in the model.
The spectral algo-rithm requires functions mapping inside and outsidetrees to feature vectors?we make use of featurescorresponding to single level rules, and larger treefragments composed of two or three levels of rules.We show that it is important to scale features by theirinverse variance, in a manner that is closely relatedto methods used in canonical correlation analysis.Negative values can cause issues in spectral algo-rithms, but we describe a solution to these problems.In recent work there has been a series of results inspectral learning algorithms for latent-variable mod-els (Vempala and Wang, 2004; Hsu et al 2009;Bailly et al 2010; Siddiqi et al 2010; Parikh etal., 2011; Balle et al 2011; Arora et al 2012;Dhillon et al 2012; Anandkumar et al 2012).
Mostof these results are theoretical (although see Luqueet al(2012) for empirical results of spectral learn-ing for dependency parsing).
While the focus ofour experiments is on parsing, our findings shouldbe relevant to the application of spectral methods toother latent-variable models.
We view our results asstrong empirical evidence for the viability of spec-tral methods as an alternative to EM.2 BackgroundIn this section we first give basic definitions for L-PCFGs, and then describe the spectral learning algo-rithm of Cohen et al(2012).1482.1 L-PCFGs: Basic DefinitionsWe follow the definition in Cohen et al(2012)of L-PCFGs.
An L-PCFG is an 8-tuple(N , I,P,m, n, pi, t, q) where:?
N is the set of non-terminal symbols in thegrammar.
I ?
N is a finite set of in-terminals.P ?
N is a finite set of pre-terminals.
We as-sume thatN = I?P , and I?P = ?.
Hence wehave partitioned the set of non-terminals intotwo subsets.?
[m] is the set of possible hidden states.2?
[n] is the set of possible words.?
For all a ?
I, b, c ?
N , h1, h2, h3 ?
[m], we have a context-free rule a(h1) ?b(h2) c(h3).
The rule has an associated pa-rameter t(a?
b c, h2, h3|a, h1).?
For all a ?
P , h ?
[m], x ?
[n], we have acontext-free rule a(h) ?
x.
The rule has anassociated parameter q(a?
x|a, h).?
For all a ?
I, h ?
[m], pi(a, h) is a parameterspecifying the probability of a(h) being at theroot of a tree.A skeletal tree (s-tree) is a sequence of rulesr1 .
.
.
rN where each ri is either of the form a?
b cor a?
x.
The rule sequence forms a top-down, left-most derivation under a CFG with skeletal rules.A full tree consists of an s-tree r1 .
.
.
rN , togetherwith values h1 .
.
.
hN .
Each hi is the value forthe hidden variable for the left-hand-side of rule ri.Each hi can take any value in [m].For a given skeletal tree r1 .
.
.
rN , define ai to bethe non-terminal on the left-hand-side of rule ri.
Forany i ?
[N ] such that ri is of the form a?
b c, de-fine h(2)i and h(3)i as the hidden state value of the leftand right child respectively.
The model then definesa probability mass function (PMF) asp(r1 .
.
.
rN , h1 .
.
.
hN ) =pi(a1, h1)?i:ai?It(ri, h(2)i , h(3)i |ai, hi)?i:ai?Pq(ri|ai, hi)The PMF over skeletal trees is p(r1 .
.
.
rN ) =?h1...hNp(r1 .
.
.
rN , h1 .
.
.
hN ).2For any integer n, we use [n] to denote the set {1, 2, .
.
.
n}.The parsing problem is to take a sentence as in-put, and produce a skeletal tree as output.
A stan-dard method for parsing with L-PCFGs is as follows.First, for a given input sentence x1 .
.
.
xn, for anytriple (a, i, j) such that a ?
N and 1 ?
i ?
j ?
n,the marginal ?
(a, i, j) is defined as?
(a, i, j) =?t:(a,i,j)?tp(t) (1)where the sum is over all skeletal trees t forx1 .
.
.
xn that include non-terminal a spanningwords xi .
.
.
xj .
A variant of the inside-outsidealgorithm can be used to calculate marginals.Once marginals have been computed, Good-man?s algorithm (Goodman, 1996) is used to findarg maxt?
(a,i,j)?t ?
(a, i, j).32.2 The Spectral Learning AlgorithmWe now give a sketch of the spectral learning algo-rithm.
The training data for the algorithm is a setof skeletal trees.
The output from the algorithm is aset of parameter estimates for t, q and pi (more pre-cisely, the estimates are estimates of linearly trans-formed parameters; see Cohen et al(2012) and sec-tion 2.3.1 for more details).The algorithm takes two inputs in addition to theset of skeletal trees.
The first is an integer m, speci-fying the number of latent state values in the model.Typically m is a relatively small number; in our ex-periments we test values such as m = 8, 16 or 32.The second is a pair of functions ?
and ?, that re-spectively map inside and outside trees to featurevectors in Rd and Rd?, where d and d?
are integers.Each non-terminal in a skeletal tree has an associ-ated inside and outside tree.
The inside tree for anode contains the entire subtree below that node; theoutside tree contains everything in the tree excludingthe inside tree.
We will refer to the node above theinside tree that has been removed as the ?foot?
of theoutside tree.
See figure 1 for an example.Section 3.1 gives definitions of ?
(t) and ?
(o)used in our experiments.
The definitions of ?
(t) and3In fact, in our implementation we calculate marginals?(a?
b c, i, k, j) for a, b, c ?
N and 1 ?
i ?
k < j, and?
(a, i, i) for a ?
N , 1 ?
i ?
n, then apply the CKY algorithmto find the parse tree that maximizes the sum of the marginals.For simplicity of presentation we will refer to marginals of theform ?
(a, i, j) in the remainder of this paper.149VPVsawNPDtheNdogSNPDtheNcatVPFigure 1: The inside tree (shown left) and out-side tree (shown right) for the non-terminal VPin the parse tree [S [NP [D the ] [N cat]][VP [V saw] [NP [D the] [N dog]]]]?
(o) are typically high-dimensional, sparse featurevectors, similar to those in log-linear models.
Forexample ?
might track the rule immediately belowthe root of the inside tree, or larger tree fragments;?
might include similar features tracking rules orlarger rule fragments above the relevant node.The spectral learning algorithm proceeds in twosteps.
In step 1, we learn an m-dimensional rep-resentation of inside and outside trees, using thefunctions ?
and ?
in combination with a projectionstep defined through singular value decomposition(SVD).
In step 2, we derive parameter estimates di-rectly from training examples.2.2.1 Step 1: An SVD-Based ProjectionFor a given non-terminal a ?
N , each instance ofa in the training data has an associated outside tree,and an associated inside tree.
We define Oa to bethe set of pairs of inside/outside trees seen with a inthe training data: each member of Oa is a pair (o, t)where o is an outside tree, and t is an inside tree.Step 1 of the algorithm is then as follows:1.
For each a ?
N calculate ?
?a ?
Rd?d?as[?
?a]i,j =1|Oa|?(o,t)?Oa?i(t)?j(o)2.
Perform an SVD on ??a.
Define Ua ?
Rd?m(V a ?
Rd?
?m) to be a matrix containing them left (right) singular vectors correspondingto the m largest singular values; define ?a ?Rm?m to be the diagonal matrix with the mlargest singular values on its diagonal.3.
For each inside tree in the corpus with root la-bel a, defineY (t) = (Ua)>?
(t)For each outside tree with a foot node labeleda, defineZ(o) = (?a)?1(V a)>?
(o)Note that Y (t) and Z(o) are both m-dimensionalvectors; thus we have used SVD to project insideand outside trees to m-dimensional vectors.2.3 Step 2: Parameter EstimationWe now describe how the functions Y (t) and Z(o)are used in estimating parameters of the model.First, consider the t(a?
b c, h2, h3|a, h1) parame-ters.
Each instance of a given rule a?
b c in thetraining corpus has an outside tree o associated withthe parent labeled a, and inside trees t2 and t3 as-sociated with the children labeled b and c. For anyrule a?
b cwe defineQa?b c to be the set of triples(o, t(2), t(3)) occurring with that rule in the corpus.The parameter estimate is thenc?(a?
b c, j, k|a, i) =count(a?
b c)count(a)?
Ea?b ci,j,k(2)whereEa?b ci,j,k =?
(o,t(2),t(3))?Qa?b cZi(o)?
Yj(t(2))?
Yk(t(3))|Qa?b c|Here we use count(a?
b c) and count(a) to referto the count of the rule a?
b c and the non-terminala in the corpus.
Note that once the SVD step hasbeen used to compute representations Y (t) andZ(o)for each inside and outside tree in the corpus, calcu-lating the parameter value c?(a?
b c, j, k|a, i) is avery simple operation.Similarly, for any rule a ?
x, define Qa?x tobe the set of outside trees seen with that rule in thetraining corpus.
The parameter estimate is thenc?(a?
x|a, i) =count(a?
x)count(a)?
Ea?xi (3)where Ea?xi =?o?Qa?x Zi(o)/|Qa?x|.A similar method is used for estimating parame-ters c?
(a, i) that play the role of the pi parameters (de-tails omitted for brevity; see Cohen et al(2012)).2.3.1 Guarantees for the AlgorithmOnce the c?(a?
b c, j, k|a, i), c?(a?
x|a, i) andc?
(a, i) parameters have been estimated from the150training corpus, they can be used in place of the t,q and pi parameters in the inside-outside algorithmfor computing marginals (see Eq.
1).
Call the re-sulting marginals ??
(a, i, j).
The guarantees for theparameter estimation method are as follows:?
Define ?a = E[?
(T )(?
(O))>|A = a] whereA,O, T are random variables corresponding tothe non-terminal label at a node, the outsidetree, and the inside tree (see Cohen et al(2012)for a precise definition).
Note that ?
?a, as de-fined above, is an estimate of ?a.
Then if ?ahas rank m, the marginals ??
will converge tothe true values ?
as the number of training ex-amples goes to infinity, assuming that the train-ing samples are i.i.d.
samples from an L-PCFG.?
Define ?
to be the m?th largest singular valueof ?a.
Then the number of samples requiredfor ??
to be -close to ?
with probability at least1?
?
is polynomial in 1/, 1/?, and 1/?.Under the first assumption, (Cohen et al2012) show that the c?
parameters converge tovalues that are linear transforms of the orig-inal parameters in the L-PCFG.
For example,define c(a?
b c, j, k|a, i) to be the value thatc?(a?
b c, j, k|a, i) converges to in the limit of infi-nite data.
Then there exist invertible matrices Ga ?Rm?m for all a ?
N such that for any a?
b c, forany h1, h2, h3 ?
Rm,t(a?
b c, h2, h3|a, h1) =?i,j,k[Ga]i,h1 [(Gb)?1]j,h2 [(Gc)?1]k,h3c(a?
b c, j, k|a, i)The transforms defined by the Ga matrices are be-nign, in that they cancel in the inside-outside algo-rithm when marginals ?
(a, i, j) are calculated.
Sim-ilar relationships hold for the pi and q parameters.3 Implementation of the AlgorithmCohen et al(2012) introduced the spectral learningalgorithm, but did not perform experiments, leavingseveral choices open in how the algorithm is imple-mented in practice.
This section describes a numberof key choices made in our implementation of thealgorithm.
In brief, they are as follows:The choice of functions ?
and ?.
We will de-scribe basic features used in ?
and ?
(single-levelrules, larger tree fragments, etc.).
We will also de-scribe a method for scaling different features in ?and ?
by their variance, which turns out to be im-portant for empirical results.Estimation ofEa?b ci,j,k andEa?xi .
There are a verylarge number of parameters in the model, lead-ing to challenges in estimation.
The estimates inEqs.
2 and 3 are unsmoothed.
We describe a simplebacked-off smoothing method that leads to signifi-cant improvements in performance of the method.Handling positive and negative values.
As de-fined, the c?
parameters may be positive or negative;as a result, the ??
values may also be positive or neg-ative.
We find that negative values can be a signif-icant problem if not handled correctly; but with avery simple fix to the algorithm, it performs well.We now turn to these three issues in more detail.Section 4 will describe experiments measuring theimpact of the different choices.3.1 The Choice of Functions ?
and ?Cohen et al(2012) show that the choice of featuredefinitions ?
and ?
is crucial in two respects.
First,for all non-terminals a ?
N , the matrix ?a mustbe of rank m: otherwise the parameter-estimationalgorithm will not be consistent.
Second, the num-ber of samples required for learning is polynomialin 1/?, where ?
= mina?N ?m(?a), and ?m(?a)is the m?th smallest singular value of ?a.
(Note thatthe second condition is stronger than the first; ?
> 0implies that ?a is of rank m for all a.)
The choiceof ?
and ?
has a direct impact on the value for ?
:roughly speaking, the value for ?
can be thought ofas a measure of how informative the functions ?
and?
are about the hidden state values.With this in mind, our goal is to define a rel-atively simple set of features, which neverthelessprovide significant information about hidden-statevalues, and hence provide high accuracy under themodel.
The inside-tree feature function ?
(t) makesuse of the following indicator features (throughoutthese definitions assume that a?
b c is at the rootof the inside tree t):?
The pair of nonterminals (a, b).
E.g., for the in-side tree in figure 1 this would be the pair (VP, V).151?
The pair (a, c).
E.g., (VP, NP).?
The rule a?
b c.
E.g., VP ?
V NP.?
The rule a?
b c paired with the rule at theroot of t(i,2).
E.g., for the inside tree in fig-ure 1 this would correspond to the tree fragment(VP (V saw) NP).?
The rule a?
b c paired with the rule atthe root of t(i,3).
E.g., the tree fragment(VP V (NP D N)).?
The head part-of-speech of t(i,1) paired with a.4E.g., the pair (VP, V).?
The number of words dominated by t(i,1) pairedwith a (this is an integer valued feature).In the case of an inside tree consisting of a singlerule a?
x the feature vector simply indicates theidentity of that rule.To illustrate the function ?, it will be useful tomake use of the following example outside tree:SNPDtheNcatVPVsawNPD NdogNote that in this example the foot node of the out-side tree is labeled D. The features are as follows:?
The rule above the foot node.
We take careto mark which non-terminal is the foot, using a* symbol.
In the above example this feature isNP ?
D?
N.?
The two-level and three-level rule fragmentsabove the foot node.
In the above example these fea-tures would beVPV NPD?
NSNP VPV NPD?
N?
The label of the foot node, together with thelabel of its parent.
In the above example this is(D, NP).?
The label of the foot node, together with the la-bel of its parent and grandparent.
In the above ex-ample this is (D, NP, VP).?
The part of speech of the first head word alongthe path from the foot of the outside tree to the rootof the tree which is different from the head node of4We use the English head rules from the Stanford parser(Klein and Manning, 2003).the foot node.
In the above example this is N.?
The width of the span to the left of the foot node,paired with the label of the foot node.?
The width of the span to the right of the footnode, paired with the label of the foot node.Scaling of features.
The features defined aboveare almost all binary valued features.
We scale thefeatures in the following way.
For each feature ?i(t),define count(i) to be the number of times the featureis equal to 1, and M to be the number of trainingexamples.
The feature is then redefined to be?i(t)?
?Mcount(i) + ?where ?
is a smoothing term (the method is rela-tively insensitive to the choice of ?
; we set ?
= 5 inour experiments).
A similar process is applied to the?
features.
The method has the effect of decreasingthe importance of more frequent features in the SVDstep of the algorithm.The SVD-based step of the algorithm is veryclosely related to previous work on CCA (Hotelling,1936; Hardoon et al 2004; Kakade and Foster,2009); and the scaling step is derived from previ-ous work on CCA (Dhillon et al 2011).
In CCAthe ?
and ?
vectors are ?whitened?
in a preprocess-ing step, before an SVD is applied.
This whiten-ing process involves calculating covariance matricesCx = E[?
?>] and Cy = E[?
?>], and replacing ?by (Cx)?1/2?
and ?
by (Cy)?1/2?.
The exact cal-culation of (Cx)?1/2 and (Cy)?1/2 is challenging inhigh dimensions, however, as these matrices will notbe sparse; the transformation described above canbe considered an approximation where off-diagonalmembers of Cx and Cy are set to zero.
We will seethat empirically this scaling gives much improvedaccuracy.3.2 Estimation of Ea?b ci,j,k and Ea?xiThe number of Ea?b ci,j,k parameters is very large,and the estimation method described in Eqs.
2?3 isunsmoothed.
We have found significant improve-ments in performance using a relatively simple back-off smoothing method.
The intuition behind thismethod is as follows: given two random variablesXand Y , under the assumption that the random vari-ables are independent, E[XY ] = E[X] ?
E[Y ].
It152makes sense to define ?backed off?
estimates whichmake increasingly strong independence assumptionsof this form.Smoothing of binary rules For any rule a?
b cand indices i, j ?
[m] we can define a second-ordermoment as follows:Ea?b ci,j,?
=?
(o,t(2),t(3))?Qa?b cZi(o)?
Yj(t(2))|Qa?b c|The definitions ofEa?b ci,?,k andEa?b c?,j,k are analogous.We can define a first-order estimate as follows:Ea?b c?,?,k =?
(o,t(2),t(3))?Qa?b cYk(t(3))|Qa?b c|Again, we have analogous definitions of Ea?b ci,?,?
andEa?b c?,j,?
.
Different levels of smoothed estimate canbe derived from these different terms.
The first isE2,a?b ci,j,k =Ea?b ci,j,?
?
Ea?b c?,?,k + Ea?b ci,?,k ?
Ea?b c?,j,?
+ Ea?b c?,j,k ?
Ea?b ci,?,?3Note that we give an equal weight of 1/3 to each ofthe three backed-off estimates seen in the numerator.A second smoothed estimate isE3,a?b ci,j,k = Ea?b ci,?,?
?
Ea?b c?,j,?
?
Ea?b c?,?,kUsing the definition of Oa given in section 2.2.1, wealso defineF ai =?
(o,t)?Oa Yi(t)|Oa|Hai =?
(o,t)?Oa Zi(o)|Oa|and our next smoothed estimate asE4,a?b ci,j,k = Hai ?F bj ?
Fck .Our final estimate is?Ea?b ci,j,k + (1?
?
)(?E2,a?b ci,j,k + (1?
?
)Ka?b ci,j,k)where Ka?b ci,j,k = ?E3,a?b ci,j,k + (1?
?
)E4,a?b ci,j,k .Here ?
?
[0, 1] is a smoothing parameter, set to?|Qa?b c|/(C +?|Qa?b c|) in our experiments,where C is a parameter that is chosen by optimiza-tion of accuracy on a held-out set of data.Smoothing lexical rules We define a similarmethod for the Ea?xi parameters.
DefineEai =?x??o?Qa?x?
Zi(o)?x?
|Qa?x?
|hence Eai ignores the identity of x in making its es-timate.
The smoothed estimate is then defined as?Ea?xi +(1??
)Eai .
Here, ?
is a value in [0, 1] whichis tuned on a development set.
We only smooth lex-ical rules which appear in the data less than a fixednumber of times.
Unlike binary rules, for which theestimation depends on a high order moment (thirdmoment), the lexical rules use first-order moments,and therefore it is not required to smooth rules witha relatively high count.
The maximal count for thiskind of smoothing is set using a development set.3.3 Handling Positive and Negative ValuesAs described before, the parameter estimates maybe positive or negative, and as a result themarginals computed by the algorithm may in somecases themselves be negative.
In early exper-iments we found this to be a signficant prob-lem, with some parses having a very large num-ber of negatives, and being extremely poor in qual-ity.
Our fix is to define the output of the parserto be arg maxt?
(a,i,j)?t |?
(a, i, j)| rather thanarg maxt?
(a,i,j)?t ?
(a, i, j) as defined in Good-man?s algorithm.
Thus if a marginal value ?
(a, i, j)is negative, we simply replace it with its absolutevalue.
This step was derived after inspection of theparsing charts for bad parses, where we saw evi-dence that in these cases the entire set of marginalvalues had been negated (and hence decoding underEq.
1 actually leads to the lowest probability parsebeing output under the model).
We suspect that thisis because in some cases a dominant parameter hashad its sign flipped due to sampling error; more the-oretical and empirical work is required in fully un-derstanding this issue.4 ExperimentsIn this section we describe parsing experiments us-ing the L-PCFG estimation method.
We give com-parisons to the EM algorithm, considering bothspeed of training, and accuracy of the resultingmodel; we also give experiments investigating thevarious choices described in the previous section.153We use the Penn WSJ treebank (Marcus et al1993) for our experiments.
Sections 2?21 wereused as training data, and sections 0 and 22 wereused as development data.
Section 23 is used asthe final test set.
We binarize the trees in train-ing data using the same method as that described inPetrov et al(2006).
For example, the non-binaryrule VP ?
V NP PP SBAR would be convertedto the structure [VP [@VP [@VP V NP] PP]SBAR] where @VP is a new symbol in the grammar.Unary rules are removed by collapsing non-terminalchains: for example the unary rule S ?
VP wouldbe replaced by a single non-terminal S|VP.For the EM algorithm we use the initializationmethod described in Matsuzaki et al(2005).
For ef-ficiency, we use a coarse-to-fine algorithm for pars-ing with either the EM or spectral derived gram-mar: a PCFG without latent states is used to calcu-late marginals, and dynamic programming items areremoved if their marginal probability is lower thansome threshold (0.00005 in our experiments).For simplicity the parser takes part-of-speechtagged sentences as input.
We use automaticallytagged data from Turbo Tagger (Martins et al2010).
The tagger is used to tag both the devel-opment data and the test data.
The tagger was re-trained on sections 2?21.
We use the F1 measureaccording to the Parseval metric (Black et al 1991).For the spectral algorithm, we tuned the smoothingparameters using section 0 of the treebank.4.1 Comparison to EM: AccuracyWe compare models trained using EM and the spec-tral algorithm using values form in {8, 16, 24, 32}.5For EM, we found that it was important to use de-velopment data to choose the number of iterationsof training.
We train the models for 100 iterations,then test accuracy of the model on section 22 (devel-opment data) at different iteration numbers.
Table 1shows that a peak level of accuracy is reached for allvalues of m, other than m = 8, at iteration 20?30,with sometimes substantial overtraining beyond thatpoint.The performance of a regular PCFG model, esti-mated using maximum likelihood and with no latent5Lower values of m, such as 2 or 4, lead to substantiallylower performance for both models.section 22 section 23EM spectral EM spectralm = 8 86.87 85.60 ?
?m = 16 88.32 87.77 ?
?m = 24 88.35 88.53 ?
?m = 32 88.56 88.82 87.76 88.05Table 2: Results on the development data (section 22,with machine-generated POS tags) and test data (section23, with machine-generated POS tags).states, is 68.62%.Table 2 gives results for the EM-trained modelsand spectral-trained models.
The spectral modelsgive very similar accuracy to the EM-trained modelon the test set.
Results on the development set withvarying m show that the EM-based models performbetter for m = 8, but that the spectral algorithmquickly catches up as m increases.4.2 Comparison to EM: Training SpeedTable 3 gives training times for the EM algorithmand the spectral algorithm for m ?
{8, 16, 24, 32}.All timing experiments were done on a single IntelXeon 2.67GHz CPU.
The implementations for theEM algorithm and the spectral algorithm were writ-ten in Java.
The spectral algorithm also made useof Matlab for several matrix calculations such as theSVD calculation.For EM we show the time to train a single iter-ation, and also the time to train the optimal model(time for 30 iterations of training for m = 8, 16, 24,and time for 20 iterations for m = 32).
Note thatthis latter time is optimistic, as it assumes an oraclespecifying exactly when it is possible to terminateEM training with no loss in performance.
The spec-tral method is considerably faster than EM: for ex-ample, for m = 32 the time for training the spectralmodel is just under 10 hours, compared to 187 hoursfor EM, a factor of almost 19 times faster.6The reason for these speed ups is as follows.Step 1 of the spectral algorithm (feature calculation,transfer + scaling, and SVD) is not required by EM,but takes a relatively small amount of time (about1.2 hours for all values of m).
Once step 1 has beencompleted, step 2 of the spectral algorithm takes a6In practice, in order to overcome the speed issue with EMtraining, we parallelized the E-step on multiple cores.
The spec-tral algorithm can be similarly parallelized, computing statisticsand parameters for each nonterminal separately.15410 20 30 40 50 60 70 80 90 100m = 8 83.51 86.45 86.68 86.69 86.63 86.67 86.70 86.82 86.87 86.83m = 16 85.18 87.94 88.32 88.21 88.10 87.86 87.70 87.46 87.34 87.24m = 24 83.62 88.19 88.35 88.25 87.73 87.41 87.35 87.26 87.02 86.80m = 32 83.23 88.56 88.52 87.82 87.06 86.47 86.38 85.85 85.75 85.57Table 1: Results on section 22 for the EM algorithm, varying the number of iterations used.
Best results in each roware in boldface.single EM spectral algorithmEM iter.
best model total feature transfer + scaling SVD a?
b c a?
xm = 8 6m 3h 3h32m22m 49m36m 1h34m 10mm = 16 52m 26h6m 5h19m 34m 3h13m 19mm = 24 3h7m 93h36m 7h15m 36m 4h54m 28mm = 32 9h21m 187h12m 9h52m 35m 7h16m 41mTable 3: Running time for the EM algorithm and the various stages in the spectral algorithm.
For EM we show thetime for a single iteration, and the time to train the optimal model (time for 30 iterations of training for m = 8, 16, 24,time for 20 iterations of training for m = 32).
For the spectral method we show the following: ?total?
is the totaltraining time; ?feature?
is the time to compute the ?
and ?
vectors for all data points; ?transfer + scaling?
is timeto transfer the data from Java to Matlab, combined with the time for scaling of the features; ?SVD?
is the time forthe SVD computation; a?
b c is the time to compute the c?(a?
b c, h2, h3|a, h1) parameters; a?
x is the time tocompute the c?(a?
x, h|a, h) parameters.
Note that ?feature?
and ?transfer + scaling?
are the same step for all valuesof m, so we quote a single runtime for these steps.single pass over the data: in contrast, EM requiresa few tens of passes (certainly more than 10 passes,from the results in table 1).
The computations per-formed by the spectral algorithm in its single passare relatively cheap.
In contrast to EM, the inside-outside algorithm is not required; however variousoperations such as calculating smoothing terms inthe spectral method add some overhead.
The net re-sult is that form = 32 the time for training the spec-tral method takes a very similar amount of time to asingle pass of the EM algorithm.4.3 Smoothing, Features, and NegativesWe now describe experiments demonstrating the im-pact of various components described in section 3.The effect of smoothing (section 3.2) Withoutsmoothing, results on section 22 are 85.05% (m =8, ?1.82), 86.84% (m = 16, ?1.48), 86.47%(m = 24, ?1.88), 86.47% (m = 32, ?2.09) (ineach case we show the decrease in performance fromthe results in table 2).
Smoothing is clearly impor-tant.Scaling of features (section 3.1) Without scalingof features, the accuracy on section 22 with m = 32is 84.40%, a very significant drop from the 88.82%accuracy achieved with scaling.Handling negative values (section 3.3) Replac-ing marginal values ?
(a, i, j) with their absolutevalues is also important: without this step, accu-racy on section 22 decreases to 80.61% (m = 32).319 sentences out of 1700 examples have differentparses when this step is implemented, implying thatthe problem with negative values described in sec-tion 3.3 occurs on around 18% of all sentences.The effect of feature functions To test the effectof features on accuracy, we experimented with asimpler set of features than those described in sec-tion 3.1.
This simple set just includes an indicatorfor the rule below a nonterminal (for inside trees)and the rule above a nonterminal (for outside trees).Even this simpler set of features achieves relativelyhigh accuracy (m = 8: 86.44 , m = 16: 86.86,m = 24: 87.24 , m = 32: 88.07 ).This set of features is reminiscent of a PCFGmodel where the nonterminals are augmented theirparents (vertical Markovization of order 2) and bina-rization is done while retaining sibling information(horizontal Markovization of order 1).
See Kleinand Manning (2003) for more information.
The per-155formance of this Markovized PCFG model lags be-hind the spectral model: it is 82.59%.
This is prob-ably due to the complexity of the grammar whichcauses ovefitting.
Condensing the sibling and parentinformation using latent states as done in the spectralmodel leads to better generalization.It is important to note that the results for bothEM and the spectral algorithm are comparable tostate of the art, but there are other results previ-ously reported in the literature which are higher.For example, Hiroyuki et al(2012) report an ac-curacy of 92.4 F1 on section 23 of the Penn WSJtreebank using a Bayesian tree substitution gram-mar; Charniak and Johnson (2005) report accuracyof 91.4 using a discriminative reranking model; Car-reras et al(2008) report 91.1 F1 accuracy for a dis-criminative, perceptron-trained model; Petrov andKlein (2007) report an accuracy of 90.1 F1, usingL-PCFGs, but with a split-merge training procedure.Collins (2003) reports an accuracy of 88.2 F1, whichis comparable to the results in this paper.5 ConclusionThe spectral learning algorithm gives the same levelof accuracy as EM in our experiments, but has sig-nificantly faster training times.
There are several ar-eas for future work.
There are a large number of pa-rameters in the model, and we suspect that more so-phisticated regularization methods than the smooth-ing method we have described may improve perfor-mance.
Future work should also investigate otherchoices for the functions ?
and ?.
There are natu-ral ways to extend the approach to semi-supervisedlearning; for example the SVD step, where repre-sentations of outside and inside trees are learned,could be applied to unlabeled data parsed by a first-pass parser.
Finally, the methods we have describedshould be applicable to spectral learning for otherlatent variable models.AcknowledgementsColumbia University gratefully acknowledges thesupport of the Defense Advanced Research ProjectsAgency (DARPA) Machine Reading Program un-der Air Force Research Laboratory (AFRL) primecontract no.
FA8750-09-C-0181.
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the author(s)and do not necessarily reflect the view of DARPA,AFRL, or the US government.
Shay Cohen wassupported by the National Science Foundation un-der Grant #1136996 to the Computing Research As-sociation for the CIFellows Project.
Dean Fosterwas supported by National Science Foundation grant1106743.ReferencesA.
Anandkumar, R. Ge, D. Hsu, S. M. Kakade, andM.
Telgarsky.
2012.
Tensor decompositions for learn-ing latent-variable models.
arXiv:1210.7559.S.
Arora, R. Se, and A. Moitra.
2012.
Learning topicmodels - going beyond SVD.
In Proceedings ofFOCS.R.
Bailly, A. Habrar, and F. Denis.
2010.
A spectralapproach for probabilistic grammatical inference ontrees.
In Proceedings of ALT.B.
Balle, A. Quattoni, and X. Carreras.
2011.
A spec-tral learning algorithm for finite state transducers.
InProceedings of ECML.E.
Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverage ofEnglish grammars.
In Proceedings of DARPA Work-shop on Speech and Natural Language.X.
Carreras, M. Collins, and T. Koo.
2008.
TAG, Dy-namic Programming, and the Perceptron for Efficient,Feature-rich Parsing.
In Proceedings of CoNLL, pages9?16.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InProceedings of ACL.S.
B. Cohen, K. Stratos, M. Collins, D. F. Foster, andL.
Ungar.
2012.
Spectral learning of latent-variablePCFGs.
In Proceedings of ACL.M.
Collins.
2003.
Head-driven statistical models for nat-ural language processing.
Computational Linguistics,29:589?637.P.
Dhillon, D. P. Foster, and L. H. Ungar.
2011.
Multi-view learning of word embeddings via CCA.
In Pro-ceedings of NIPS.P.
Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H.Ungar.
2012.
Spectral dependency parsing with latentvariables.
In Proceedings of EMNLP.J.
Goodman.
1996.
Parsing algorithms and metrics.
InProceedings of ACL.156D.
Hardoon, S. Szedmak, and J. Shawe-Taylor.
2004.Canonical correlation analysis: An overview with ap-plication to learning methods.
Neural Computation,16(12):2639?2664.S.
Hiroyuki, M. Yusuke, F. Akinori, and N. Masaaki.2012.
Bayesian symbol-refined tree substitution gram-mars for syntactic parsing.
In Proceedings of ACL,pages 440?448.H.
Hotelling.
1936.
Relations between two sets of vari-ants.
Biometrika, 28:321?377.D.
Hsu, S. M. Kakade, and T. Zhang.
2009.
A spec-tral algorithm for learning hidden Markov models.
InProceedings of COLT.S.
M. Kakade and D. P. Foster.
2009.
Multi-view regres-sion via canonical correlation analysis.
In COLT.D.
Klein and C. D. Manning.
2003.
Accurate unlexical-ized parsing.
In Proc.
of ACL, pages 423?430.F.
M. Luque, A. Quattoni, B. Balle, and X. Carreras.2012.
Spectral learning for non-deterministic depen-dency parsing.
In Proceedings of EACL.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn treebank.
Computational Linguistics,19:313?330.A.
F. T. Martins, N. A. Smith, E. P. Xing, M. T.Figueiredo, and M. Q. Aguiar.
2010.
TurboParsers:Dependency parsing by approximate variational infer-ence.
In Proceedings of EMNLP.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Proba-bilistic CFG with latent annotations.
In Proceedingsof ACL.A.
Parikh, L. Song, and E. P. Xing.
2011.
A spectral al-gorithm for latent tree graphical models.
In Proceed-ings of The 28th International Conference on MachineLearningy (ICML 2011).S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proc.
of HLT-NAACL.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In Proceedings of COLING-ACL.S.
Siddiqi, B.
Boots, and G. Gordon.
2010.
Reduced-rank hidden markov models.
JMLR, 9:741?748.S.
Vempala and G. Wang.
2004.
A spectral algorithm forlearning mixtures of distributions.
Journal of Com-puter and System Sciences, 68(4):841?860.157
