A DYNAMICAL  SYSTEM APPROACHTO CONTINUOUS SPEECH RECOGNIT IONV.Digalaki~f J.R. Rohlieek~i M.Ostendor~t Boston University44 Cumin ington St.Boston, MA 02215ABSTRACTA dynamical system model is proposed for better epresent-ing the spectral dynamics of speech for recognition.
We assumethat the observed feature vectors of a phone segment are theoutput of a stochastic linear dynamical system and consider twoalternative assumptions regarding the relationship ofthe segmentlength and the evolution of the dynamics.
Training is equivalentto the identification f a stochastic linear system, and we followa nontraditional approach based on the Estlmate-Maximize algo-rithm.
We evaluate this model on a phoneme classification taskusing the TIMIT database.INTRODUCTIONA new direction in speech recognition via statistical meth-ods is to move from frame-based models, such as HiddenMarkov Models (HMMs), to segment-based models thatprovide a better framework for modeling the dynamics ofthe speech production mechanism.
The Stochastic SegmentModel (SSM) is a joint model for a sequence of observa-tions, allowing explicit modeling of time correlation.
Orig-inally in the SSM, a phoneme was modeled as a sequenceof feature vectors that obeyed a multivariate Gaussian dis-tribution.
The variable length of an observed phoneme washandled either by modeling a fixed-length transformation fthe observations \[6\] or by assuming the observation was apartially observed sample of a trajectory represented by afixed-length model \[7\].
In the first case, the maximum like-lihood estimates of the parameters can be obtained irectly,but the Estimate-Maximize algorithm \[2\] may be requiredin the second case.Unfortunately, the joint Gaussian model suffers from es-timation problems, given the number of acoustic featuresand the analysis-frame rate that modern continuous speechrecognizers use.
Therefore, a more constrained assumptionabout the correlation structure must be made.
In previ-ous work \[3\], we chose to constrain the model to a time-inhomogeneous Gauss-Markov process.
Under the Gauss-Markov assumption, we were able to model well the timecorrelation of the first few cepstral coefficients, but the per-formance decreased when a larger number of features wereBBN Inc.10 Moulton St.Cambridge, MA 02138used.
We attribute the performance decrease to insufficienttraining data and the noisy nature of the cepstral coeffi-dents.
In this work we deal with the problem of noisy ob-servations through a time-inhomogeneous dynamical systemformalism, including observation noise in our model.Under the assumption that we model speech as a Gaus-sian process at the frame-rate l vel, a linear state-space dy-namical system can be used to parameterize the densityof a segment of speech.
This a natural generalization ofour previous Gauss-Markov approach, with the addition ofmodeling error in the form of observation noise.We can make two different assumptions to address thetime-variability issue:1.
Trajectory invariance (A1): There are underlying un-observed trajectories in state-space that basic unitsof speech follow.
In the dynamical system formalism,this assumption translates to a fixed sequence of statetransition matrices for any occurrence of a speech seg-ment.
Then, the problem of variable segment lengthcan be solved by assuming that the observed featurevectors are not only a noisy version of the fixed un-derlying trajectory, but also an incomplete one withmissing observations.
Successive observed frames ofspeech have stronger correlation for longer observa-tions, since the underlying trajectory is sampled atshorter intervals (in feature space).2.
Correlation invariance (A2): The underlying trajec-tory in phase space is not invariant under time-warpingtransformations.
In this case, the sequence of statetransition matrices for a particular observation of aphoneme depends on the phoneme l ngth, and we havea complete (albeit noisy) observation of the state se-quence.
In this case, we assume that it is the corre-lation between successive frames that is invariant ovariations in the segment length.Under either assumption, the training problem with aknown segmentation is that of maximum likelihood identifi-cation of a dynamical system.
We use here an nontraditionnl253method based on the EM algorithm, that can be easily usedunder either correlation or trajectory invariance.
The modelis described in Section, and the identification algorithms arein Section .
In Section we shall briefly describe phonemeclassification and recognition algorithms for this model, andfinally in Section we present phone classification results onthe T IMIT  database \[5\].A DYNAMICAL  MODEL FORSPEECH SEGMENTSA segment of speech is represented by an L-long se-quence  of q-dimensional feature vector Z = \ [z l  z2 .
.
.
zL\] .The original stochastic segment model for Z had two compo-nents \[7\]: i )  a t ime transformation TL to model the variable-length observed segment in terms of a fixed-length unob-served sequence Z = YTL ,  where Y = \[yl y2 .
.
.
yM\],  andii) a probabilistic representation f the unobserved featuresequence Y.
We assumed in the past \[3\] that the densityof Y was that of an inhomogeneous Ganss-Markov process.We then showed how the EM algorithm can be used to esti-mate the parameters of the models under this assumption.In this work, we extend the modeling of the feature se-quence, to the more general Markovian representation foreach different phone model otxk+1 = Fk(a)zk + w~yk = Hk(0~)zk + v, (1)where wk,  vk are uncorrelated Gaussian vectors with covari-&r icesE{wkw~'la} = q,(a)Skzwhere 6m is the Kronecker delta.
We further assume thatthe initial state xo is Gaussian with mean and covariance/~o(o~), ~0(o 0.
In this work, we arbitrarily choose the dimen-sion of the state to be equal to that of the feature vectorand Hk(cr) = I ,  the identity matrix.
The sequence Y iseither fully or partially observed under the assumptions ofcorrelation and trajectory invariance respectively.
In orderto reduce the number of free parameters in our model, weassume that a phone segment is locally stationary over dif-ferent regions within the segment, where those regions aredefined by a fixed time warping that in this work we simplychoose as linear.
In essence, we are tying distributions, andthe way this is done under the correlation and trajectoryinvariance assumptions i shown in Figure 1.The likelihood of the observed sequence Z can be ob-tained by the Kalman predictor, asLlog p(z la)  = - {logk=l+ekT(.)\[~(ke)(.)\]--lek(.)}
+ constant (2)L.3 L-3Correlation \[nvarlence Traje(:tory InvarlancoFigure 1: Distribution tying for (a) Correlation and (b) Trajec-tory invariance.where (,e)(ot) is the prediction error variance given phonemodel a.
In the trajectory invariance case, innovations areonly computed at the points where the output of the systemis observed, and the predicted state estimate for these timescan be obtained by the/ -step ahead prediction form of theKalman filter, where I is the length of the last "black-out"interval - the number of missing observations y immediatelybefore the last observed frame z.TRAIN INGThe classical method to obtain maximum likelihood es-t imates involves the construction of a time-varying Kahnanpredictor 'and the expression of the likelihood function interms of the prediction error as in (2) \[1\].
The minimizationof the log-likelihood function is equivalent o a nonlinearprogramming problem, and iterativc optimization methodshave to be used that all require the first and perhaps the sec-ond derivatives of the log-likelihood function with respectto the system parameters.
The solution requires the inte-gration of adjoint equations, and the method becomes tooinvolved under the trajectory invariance assumption, wherewe have missing observations.We have developed a nontraditional iterative method formaximum likelihood identification of a stochastic dynami-cal system, based on tlle observation that tile computationof the estimates would be simple if tile state of the systemwere observable: using simple first and second order suffi-cient statistics of time state and observation vectors.
TheEstimate-Maximize algorithm provides an approach for es-timating parameters for processes having unobserved com-ponents, in this case the state vectors, and therefore ca,be used for maximum likelihood identification of dynamicalsystems.254If we denote the parameter vector of phone model a by8, then a t  the p t h  iteration of the EM algorithm the newestimate of the parameter vector is obtained by minimizing+log l&l] + constant I Z, dp)} (3)where we have suppressed the parameterization of the sys-tem parameters on phone model cw and the first summationis over all occurrences of a specific phone model in the train-ing data.Since the noise process is assumed to be Gaussian, theEM algorithm simply involves iteratively computing the ex-pected first and second order sufficient statistics given thecurrent parameter estimates.
I t  is known from Kalman fd-tering theory [I] that the conditional distribution of thestate X given the observations Z on an interval is Gaus-sian.
The sufficient statistics are thenzki;, if observed;E{ykx:I~, 8) =HkE{xkx~IZ),  if missing.where the quantities on the right, i k l L ,  CkIL,  Ck,k-lIL arethe fixed interval smoothed state estimate, its variance andthe one lag cross-covariance respectively.
The computationof these sufficient statistics can be done recursively.
UnderA2, since Y = Z, it reduces to the fixed-interval smoothingform of the Kalman filter, together with some additional re-cursions for the computation of the cross-covariance.
Theserecursions consist of a forward pass through the data, fol-lowed by a backward pass and are summarized in Table 1.Under Al ,  the recursions take the form of a fixed intervalsmoother with blackouts, and can be derived similarly tothe standard Kalman filter recursions.To summarize, assuming a known segmentation and there-fore a known sequence of system models, the EM algorithminvolves at  each iteration the computation of the sufficientstatistics described previously using the recursions of Ta-Forward recursionsYk - H k i k l k - 1c ~ + ~ ~ ~  = F c ~ ~ ~ F ~ T  + Q~Backward Recursionsble 1 and the old estimates of the model parameters (Esti-mate step).
The new estimates for the system parameterscan then be obtained from these statistics as simple multi-variate regression coefficients (Maximize step).
In addition,the structure of the system matrices can be constrained inorder to satisfy identifiability conditions.
When the seg-mentation is unknown, since the estimates obtained fromour known segmentation method are Maximum Likelihood-ones, training can be done in an iterative fashion, as de-scribed in [6].RECOGNITIONWhen the phonetic segmentation is known, under bothassumptions A1 and A2 the model sequence can be deter-mined from the segmentation and therefore the MAP rulecan be used for phone classification, where the likelihood ofthe observations is obtained from the Kalman predictor (2).For connected-phone recognition, with unknown segmen-tation, the MAP rule for detecting the most likely phoneticsequence involves computing the total probability of a cer-tain sequence by summing over all possible segmentations.Because of the computational complexity of this approach,one can jointly search for the most likely phone sequenceand segmentation given the observed sequence.
This can bedone with a Dynamic-Programming recursion.
In previouswork we have also introduced alternative fast algorithmsfor both phone classification and recognition [4] which yieldperformance similar to Dynamic-Programming with signif-icant computation savings.EXPERIMENTAL RESULTSWe have implemented a system based on our correla-tion invariance assumption and performed phone classifi-0.90.80.70.6| r vod ?Classif ication rate I~ 1  I I I I I I I II I I I0 2 4 6 8 10I terat ion12F igure  2:  C lass i f i ca t ion  per fo rmance  o f  tes t  data  vs.  numbero f  i te ra t ions  and  log - l i ke l lhood  ra t io  o f  each  i te ra t ion  re la t ive  tothe  convergent  va lue  fo r  the  t ra in ing  data .cation experiments on the T IMIT  database \[5\].
We usedMel-warped cepstra nd their derivatives together with thederivative of log power.The number of different distribu-tions (time-invariant regions) for each segment model was5.
We used 61 phonetic models, but in counting errorswe folded homophones together and effectively used the re-duced CMU/MIT  39 symbol set.
The measurement-noisevariance was common over all different phone-models andwas not reestimated after the first iteration.
In experimentswith class-dependent measurement oise, we observed a de-crease in performance, which we attribute to "over-training";a first order Gauss-Markov structure can adequately modelthe training data, because of the small length of the time-invariant regions in the model.
In addition, the observedfeature vectors were centered around a class-dependent mean.Duration probabilities as well as a priori class probabilitieswhere also used in these experiments.
The training set thatwe used consist of 317 speakers (2536 sentences), and eval-uation of our algorithms is done on a separate test set with12 speakers (96 sentences).The effectiveness of the training algorithm is shown inFigure 2, where we present the normalized log-likelihoodof the training data and classification rate of the test dataversus the number of iterations.
We used 10 cepstra forthis experiment, and the initial parameters for the modelswhere uniform across all classes, except he class-dependentmeans.
We can see the fast initial convergence of the EMalgorithm, and that the best performance is achieved afteronly 4 iterations.In Figure 3 we show the classification rates for no cor-relation modeling (independent frames), the Gauss-Markovmodel and the Dynamical system model for different num-bers of input features.
We also include in the same plot theclassification rates when the derivatives of the cepstra are7472706866646260585654I 1 I I I?
.
.
.
.?
.
?
X "  "?
.
.
?
.
??
.
.
?
??
.
?
?
.
- .
.
~ "  - .
.?
. "
7 ~ "  .
.
?
"X" ."
. '
.~ .
- ?
"O '"  .- .
?
Indep.
f rames  -~.." , ' Gauss -Markov  model  ?
+-?
-". '
" Dynam?System model  - O.." ."
lnd .
f r ,+der iv  - ?.~"1  I6 813 .
.
.
.
.+ .
.
.
.
.
.7D.S.
mode l+der iv .
"Z~- ?I I I I10 12 14 16 18Number  of  Cepstra l  CoefficientsF igure  3: C lass i f i ca t ion  ra tes  fo r  var ious  types  o f  Cor re la t ionmode l ing  and  numbers  o f  cepst ra l  coef f i c ientsincluded in the feature set, so that some form of correlationmodeling is included in the independent-frame odel.
Wecan see that the proposed model clearly outperforms theindependent-frame odel.
Furthermore, we should noticethe significance of incorporating observation oise in themodel, by comparing the performance of the new model tothe earlier, Gauss-Markov one.CONCLUSIONIn this paper, we have shown that segment model basedon a stochastic linear system model which incorporates amodeling/observation noise term is effective for speech recog-nition.
We have shown that classification performance us-ing this model is significantly better than is obtained usingeither an independent-frame or a Gauss-Markov assump-tion on the observed frames.
Finally, we have presented anovel approach to the system parameter estimation problembased on the EM algorithm.ACKNOWLEDGEMENTSThis work was supported jointly by NSF and DARPAunder NSF grant number IRI-8902124.
This paper also ap-pears in the Proceedings of the International Conference onAcoustics, Speech and Signal Processing.REFERENCES1.
P .E .Ca lnes ,  "L inear  S tochast i c  Sys tems" ,  Jo lm \Vi ley &Sons,  1988.2562.
A.P.Dempster, N.M.L~ird and D.B.Rubin, "Maximum Like-lihood Estimation from Incomplete Data," in Journal of theRoyal Statistical Society (B), Vol.
39, No.
1, pp.
1-38, 1977.3.
V. Digalakis, M. Ostendorf and J.
It.
Roldlcek, "Improve-ments in the Stochastic Segment Model for Phoneme Ftecog-nition," in Proceedings of the Second DARPA Workshop onSpeech and Natural Language, pp.
332-338, October 1989.4.
V. Digalakis, M. Ostendorf and J. R. Roldicek, "Fast SearchAlgorithms for Connected Phone Recognition Using theStochastic Segment Model," manuscript submitted to IEEETrans.
Acoustic Speech and Signal Processing (a shorter ver-sion appeared in Proceedings of the Third DARPA Work-shop on Speech and Natural Language, June 1990).5.
L.F. Lamel, R. H. Kassel and S. Seneff, "Speech DatabaseDevelopment: Design and Analysis of the Acoustic-PhoneticCorpus," in Proc.
DARPA Speech Recognition Workshop,Report No.
SAIC-86/1546, pp.
100-109, Feb. 1986.6.
M. Ostendorf and S. Roucos, "A Stochastic Segment Modelfor Phoneme-based Continuous Speech Recognition," inIEEE Trans.
Acoustic Speech and Signal Processing, VoLASSP-37(12), pp.
1857-1869, December 1989.7.
S. Roucos, M. Ostendorf, H. Gish, and A. Derr, "Stochas-tic Segment Modeling Using the Estimate-Maximize Algo-rithm," in IEEE Int.
Conf.
Acoust., Speech, Signal Process-ing, pp.
127-130, New York, New York, April 1988.257
