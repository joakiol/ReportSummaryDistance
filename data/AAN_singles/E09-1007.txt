Proceedings of the 12th Conference of the European Chapter of the ACL, pages 51?59,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsClique-Based Clustering for improving Named Entity Recognition systemsJulien Ah-PineXerox Research Centre Europe6, chemin de Maupertuis38240 Meylan, Francejulien.ah-pine@xrce.xerox.comGuillaume JacquetXerox Research Centre Europe6, chemin de Maupertuis38240 Meylan, Franceguillaume.jacquet@xrce.xerox.comAbstractWe propose a system which builds, in asemi-supervised manner, a resource thataims at helping a NER system to anno-tate corpus-specific named entities.
Thissystem is based on a distributional ap-proach which uses syntactic dependen-cies for measuring similarities betweennamed entities.
The specificity of thepresented method however, is to combinea clique-based approach and a clusteringtechnique that amounts to a soft clusteringmethod.
Our experiments show that theresource constructed by using this clique-based clustering system allows to improvedifferent NER systems.1 IntroductionIn Information Extraction domain, named entities(NEs) are one of the most important textual unitsas they express an important part of the meaningof a document.
Named entity recognition (NER)is not a new domain (see MUC1 and ACE2 confer-ences) but some new needs appeared concerningNEs processing.
For instance the NE Oxford illus-trates the different ambiguity types that are inter-esting to address:?
intra-annotation ambiguity: Wikipedia listsmore than 25 cities named Oxford in the world?
systematic inter-annotation ambiguity: thename of cities could be used to refer to the uni-versity of this city or the football club of thiscity.
This is the case for Oxford or Newcastle?
non-systematic inter-annotation ambiguity:Oxford is also a company unlike Newcastle.The main goal of our system is to act in a com-plementary way with an existing NER system, inorder to enhance its results.
We address two kinds1http://www-nlpir.nist.gov/related projects/muc/2http://www.nist.gov/speech/tests/aceof issues: first, we want to detect and correctlyannotate corpus-specific NEs3 that the NER sys-tem could have missed; second, we want to correctsome wrong annotations provided by the existingNER system due to ambiguity.
In section 3, wegive some examples of such corrections.The paper is organized as follows.
We present,in section 2, the global architecture of our systemand from ?2.1 to ?2.6, we give details about eachof its steps.
In section 3, we present the evalu-ation of our approach when it is combined withother classic NER systems.
We show that the re-sulting hybrid systems perform better with respectto F-measure.
In the best case, the latter increasedby 4.84 points.
Furthermore, we give examples ofsuccessful correction of NEs annotation thanks toour approach.
Then, in section 4, we discuss aboutrelated works.
Finally we sum up the main pointsof this paper in section 5.2 Description of the systemGiven a corpus, the main objectives of our systemare: to detect potential NEs; to compute the possi-ble annotations for each NE and then; to annotateeach occurrence of these NEs with the right anno-tation by analyzing its local context.We assume that this corpus dependent approachallows an easier NE annotation.
Indeed, even ifa NE such as Oxford can have many annotationtypes, it will certainly have less annotation possi-bilities in a specific corpus.Figure 1 presents the global architecture of oursystem.
The most important part concerns steps3 (?2.3) and 4 (?2.4).
The aim of these sub-processes is to group NEs which have the sameannotation with respect to a given context.
Onthe one hand, clique-based methods (see ?2.3 for3In our definition a corpus-specific NE is the one whichdoes not appear in a classic NEs lexicon.
Recent news articlesfor instance, are often constituted of NEs that are not in aclassic NEs lexicon.51Figure 1: General description of our systemdetails on cliques) are interesting as they allowthe same NE to be in different cliques.
In otherwords, cliques allow to represent the different pos-sible annotations of a NE.
The clique-based ap-proach drawback however, is the over productionof cliques which corresponds to an artificial overproduction of possible annotations for a NE.
Onthe other hand, clustering methods aim at struc-turing a data set and such techniques can be seenas data compression processes.
However, a sim-ple NEs hard clustering doesn?t allow a NE to bein several clusters and thus to express its differ-ent annotations.
Then, our proposal is to combineboth methods in a clique-based clustering frame-work.
This combination leads to a soft-clusteringapproach that we denote CBC system.
The fol-lowing paragraphs, from 2.1 to 2.6, describe therespective steps mentioned in Figure 1.2.1 Detection of potential Named EntitiesDifferent methods exist for detecting potentialNEs.
In our system, we used some lexico-syntactic constraints to extract expressions from acorpus because it allows to detect some corpus-specific NEs.
In our approach, a potential NE is anoun starting with an upper-case letter or a nounphrase which is (see (Ehrmann and Jacquet, 2007)for similar use):?
a governor argument of an attribute syntacticrelation with a noun as governee argument (e.g.presidentattribute?????
George Bush)?
a governee argument of a modifier syntactic re-lation with a noun as a governor argument (e.g.companymodifier?????
Coca-Cola).The list of potential NEs extracted from the cor-pus will be denoted NE and the number of NEs|NE|.2.2 Distributional space of NEsThe distributional approach aims at evaluating adistance between words based on their syntac-tic distribution.
This method assumes that wordswhich appear in the same contexts are semanti-cally similar (Harris, 1951).To construct the distributional space associatedto a corpus, we use a robust parser (in our ex-periments, we used XIP parser (A?
?t et al, 2002))to extract chunks (i.e.
nouns, noun phrases, .
.
.
)and syntactic dependencies between these chunks.Given this parser?s output, we identify triple in-stances.
Each triple has the form w1.R.w2 wherew1 and w2 are chunks and R is a syntactic relation(Lin, 1998), (Kilgarriff et al, 2004).One triple gives two contexts (1.w1.R and2.w2.R) and two chunks (w1 and w2).
Then, weonly select chunks w which belong to NE.
Eachpoint in the distributional space is a NE and eachdimension is a syntactic context.
CT denotes theset of all syntactic contexts and |CT| represents itscardinal.We illustrate this construction on the sentence?provide Albania with food aid?.
We obtain thethree following triples (note that aid and food aidare considered as two different chunks):provide VERB?I-OBJ?Albania NOUNprovide VERB?PREP WITH?aid NOUNprovide VERB?PREP WITH?food aid NPFrom these triples, we have the followingchunks and contexts4:Chunks: Contexts:provide VERB 1.provide VERB.I-OBJAlbania NOUN 1.provide VERB.PREP WITHaid NOUN 2.Albania NOUN.I-OBJfood aid NP 2.aid NOUN.PREP WITH2.food aid NP.PREP WITHAccording to the NEs detection method de-scribed previously, we only keep the chunks andcontexts which are in bold in the above table.4In the context 1.VERB:provide.I-OBJ, the figure 1means that the verb provide is the governor argument of theIndirect OBJect relation.52We also use an heuristic in order to reduce theover production of chunks and contexts: in our ex-periments for example, each NE and each contextshould appear more than 10 times in the corpus forbeing considered.D is the resulting (|NE| ?
|CT|) NE-Contextmatrix where ei : i = 1, .
.
.
, |NE| is a NE andcj : j = 1, .
.
.
, |CT| is a syntactic context.
Thenwe have:D(ei, cj) = Nb.
of occ.
of cj associated to ei (1)2.3 Cliques of NEs computationA clique in a graph is a set of pairwise adja-cent nodes which is equivalent to a complete sub-graph.
A maximal clique is a clique that is not asubset of any other clique.
Maximal cliques com-putation was already employed for semantic spacerepresentation (Ploux and Victorri, 1998).
In thiswork, cliques of lexical units are used to representa precise meaning.
Similarly, we compute cliquesof NEs in order to represent a precise annotation.For example, Oxford is an ambiguous NEbut a clique such as <Cambridge, Oxford, Ed-inburgh University, Edinburgh, Oxford Univer-sity> allows to focus on the specific annota-tion <organization> (see (Ehrmann and Jacquet,2007) for similar use).Given the distributional space described in theprevious paragraph, we use a probabilistic frame-work for computing similarities between NEs.The approach that we propose is inspired fromthe language modeling framework introduced inthe information retrieval field (see for example(Lavrenko and Croft, 2003)).
Then, we constructcliques of NEs based on these similarities.2.3.1 Similarity measures between NEsWe first compute the maximum likelihood esti-mation for a NE ei to be associated with a con-text cj : Pml(cj |ei) =D(ei,cj)|ei|, where |ei| =?|CT|j=1 D(ei, cj) is the total occurrences of the NEei in the corpus.This leads to sparse data which is not suitablefor measuring similarities.
In order to counterthis problem, we use the Jelinek-Mercer smooth-ing method: D?
(ei, cj) = ?Pml(cj |ei) + (1 ??
)Pml(cj |CORP) where CORP is the corpus andPml(cj |CORP) =Pi D(ei,cj)Pi,j D(ei,cj).
In our experi-ments we took ?
= 0.5.Given D?, we then use the cross-entropy as asimilarity measure between NEs.
Let us denote bys this similarity matrix, we have:s(ei, e?i) = ??cj?CTD?
(ei, cj) log(D?(ei?
, cj)) (2)2.3.2 From similarity matrix to adjacencymatrixNext, we convert s into an adjacency matrix de-noted s?.
In a first step, we binarize s as fol-lows.
Let us denote {ei1, .
.
.
, ei|NE|}, the list of NEsranked according to the descending order of theirsimilarity with ei.
Then, L(ei) is the list of NEswhich are considered as the nearest neighbors ofei according to the following definition:L(ei) = (3){ei1, ..., eip :?pi?=1 s(ei, eii?
)?|NE|i?=1 s(ei, ei?)?
a; p ?
b}where a ?
[0, 1] and b ?
{1, .
.
.
, |NE|}.
L(ei)gathers the most significant nearest neighbors of eiby choosing the ones which bring the a most rele-vant similarities providing that the neighborhood?ssize doesn?t exceed b.
This approach can be seenas a flexible k-nearest neighbor method.
In ourexperiments we chose a = 20% and b = 10.Finally, we symmetrize the similarity matrix asfollows and we obtain s?:s?
(ei, ei?)
={1 if ei?
?
L(ei) or ei ?
L(ei?
)0 otherwise(4)2.3.3 Cliques computationGiven s?, the adjacency matrix between NEs, wecompute the set of maximal cliques of NEs de-noted CLI.
Then, we construct the matrix T ofgeneral term:T (clik, ei) ={1 if ei ?
clik0 otherwise(5)where clik is an element of CLI.
T will be theinput matrix for the clustering method.In the following, we also use clikfor denoting the vector represented by(T (clik, e1), .
.
.
, T (clik, e|NE|)).Figure 2 shows some cliques which contain Ox-ford that we can obtain with this method.
This fig-ure also illustrates the over production of cliquessince at least cli8, cli10 and cli12 can be annotatedas <organization>.53Figure 2: Examples of cliques containing Oxford2.4 Cliques clusteringWe use a clustering technique in order to groupcliques of NEs which are mutually highly simi-lar.
The clusters of cliques which contain a NEallow to find the different possible annotations ofthis NE.This clustering technique must be able to con-struct ?pure?
clusters in order to have precise an-notations.
In that case, it is desirable to avoidfixing the number of clusters.
That?s the reasonwhy we propose to use the Relational Analysis ap-proach described below.2.4.1 The Relational Analysis approachWe propose to apply the Relational Analysis ap-proach (RA) which is a clustering model thatdoesn?t require to fix the number of clusters(Michaud and Marcotorchino, 1980), (Be?de?carraxand Warnesson, 1989).
This approach takes as in-put a similarity matrix.
In our context, since wewant to cluster cliques of NEs, the correspond-ing similarity matrix S between cliques is givenby the dot products matrix taken from T : S =T ?
T ?.
The general term of this similarity matrixis: S(clik, clik?)
= Skk?
= ?clik, clik??.
Then, wewant to maximize the following clustering func-tion:?
(S,X) = (6)|CLI|?k,k?=1(Skk?
??(k??,k???
)?S+ Sk??k???|S+|)?
??
?contkk?Xkk?where S+ = {(clik, clik?)
: Skk?
> 0}.In other words, clik and clik?
have more chancesto be in the same cluster providing that their sim-ilarity measure, Skk?
, is greater or equal to themean average of positive similarities.X is the solution we are looking for.
It is a bi-nary relational matrix with general term: Xkk?
=1, if clik is in the same cluster as clik?
; andXkk?
=0, otherwise.
X represents an equivalence rela-tion.
Thus, it must respect the following proper-ties:?
binarity: Xkk?
?
{0, 1};?k, k?,?
reflexivity: Xkk = 1;?k,?
symmetry: Xkk?
?Xk?k = 0;?k, k?,?
transitivity: Xkk?
+ Xk?k??
?
Xkk??
?1;?k, k?, k?
?.As the objective function is linear with respecttoX and as the constraints thatX must respect arelinear equations, we can solve the clustering prob-lem using an integer linear programming solver.However, this problem is NP-hard.
As a result, inpractice, we use heuristics for dealing with largedata sets.2.4.2 The Relational Analysis heuristicThe presented heuristic is quite similar to anotheralgorithm described in (Hartigan, 1975) known asthe ?leader?
algorithm.
But unlike this last ap-proach which is based upon euclidean distancesand inertial criteria, the RA heuristic aims at max-imizing the criterion given in (6).
A sketch of thisheuristic is given in Algorithm 1, (see (Marco-torchino and Michaud, 1981) for further details).Algorithm 1 RA heuristicRequire: nbitr = number of iterations; ?max = maximalnumber of clusters; S the similarity matrixm?P(k,k?
)?S+ Skk?|S+|Take the first clique clik as the first element of the firstcluster?
= 1 where ?
is the current number of clusterfor q = 1 to nbitr dofor k = 1 to |CLI| dofor l = 1 to ?
doCompute the contribution of clique clik with clus-ter clul: contl =Pclik??clul(Skk?
?m)end forclul?
is the cluster id which has the highest contribu-tion with clique clik and contl?
is the correspondingcontribution valueif (contl?
< (Skk ?m)) ?
(?
< ?max) thenCreate a new cluster where clique clik is the firstelement and ??
?+ 1elseAssign clique clik to cluster clul?if the cluster where was taken clik before its newassignment, is empty then??
??
1end ifend ifend forend forWe have to provide a number of iterations54or/and a delta threshold in order to have an approx-imate solution in a reasonable processing time.Besides, it is also required a maximum number ofclusters but since we don?t want to fix this param-eter, we put by default ?max = |CLI|.Basically, this heuristic has a O(nbitr?
?max?|CLI|) computation cost.
In general terms, we canassume that nbitr << |CLI|, but not ?max <<|CLI|.
Thus, in the worst case, the algorithm hasa O(?max ?
|CLI|) computation cost.Figure 3 gives some examples of clusters ofcliques5 obtained using the RA approach.Figure 3: Examples of clusters of cliques (only theNEs are represented) and their associated contexts2.5 NE resource construction using the CBCsystem?s outputsNow, we want to exploit the clusters of cliques inorder to annotate NE occurrences.
Then, we needto construct a NE resource where for each pair (NEx syntactic context) we have an annotation.
To thisend, we need first, to assign a cluster to each pair(NE x syntactic context) (?2.5.1) and second, toassign each cluster an annotation (?2.5.2).2.5.1 Cluster assignment to each pair (NE xsyntactic context)For each cluster clul we provide a scoreFc(cj , clul) for each context cj and a score5We only represent the NEs and their frequency in thecluster which corresponds to the number of cliques whichcontain the NEs.
Furthermore, we represent the most relevantcontexts for this cluster according to equation (7) introducedin the following.Fe(ei, clul) for each NE ei.
These scores6 aregiven by:Fc(cj , clul) = (7)?ei?clulD(ei, cj)?|NE|i=1 D(ei, cj)?ei?clul1{D(ei,cj) 6=0}where 1{P} equals 1 if P is true and 0 otherwise.Fe(ei, clul) = #(clul, ei) (8)Given a NE ei and a syntactic contextcj , we now introduce the contextual clus-ter assignment matrix Actxt(ei, cj) as fol-lows: Actxt(ei, cj) = clu?
where: clu?
=Argmax{clul:clul3ei;Fe(ei,clul)>1}Fc(cj , clul).In other words, clu?
is the cluster for which wefind more than one occurrence of ei and the high-est score related to the context cj .Furthermore, we compute a default cluster as-signment matrix Adef , which does not depend onthe local context: Adef (ei) = clu?
where: clu?
=Argmax{clul:clul3{clik:clik3ei}}|clik|.In other words, clu?
is the cluster containing thebiggest clique clik containing ei.2.5.2 Clusters annotationSo far, the different steps that we have introducedwere unsupervised.
In this paragraph, our aim is togive a correct annotation to each cluster (hence, toall NEs in this cluster).
To this end, we need someannotation seeds and we propose two differentsemi-supervised approaches (regarding the classi-fication given in (Nadeau and Sekine, 2007)).
Thefirst one is the manual annotation of some clusters.The second one proposes an automatic cluster an-notation and assumes that we have some NEs thatare already annotated.Manual annotation of clusters This method isfastidious but it is the best way to match the cor-pus data with a specific guidelines for annotatingNEs.
It also allows to identify new types of an-notation.
We used the ACE2007 guidelines formanually annotating each cluster.
However, ourCBC system leads to a high number of clusters ofcliques and we can?t annotate each of them.
For-tunately, it also leads to a distribution of the clus-ters?
size (number of cliques by cluster) which is6For data fusion tasks in information retrieval field, thescoring method in equation (7) is denoted CombMNZ (Foxand Shaw, 1994).
Other scoring approaches can be used seefor example (Cucchiarelli and Velardi, 2001).55similar to a Zipf distribution.
Consequently, in ourexperiments, if we annotate the 100 biggest clus-ters, we annotate around eighty percent of the de-tected NEs (see ?3).Automatic annotation of clusters We supposein this context that many NEs in NE are alreadyannotated.
Thus, under this assumption, we havein each cluster provided by the CBC system, bothannotated and non-annotated NEs.
Our goal is toexploit the available annotations for refining theannotation of a cluster by implicitly taking intoaccount the syntactic contexts and for propagatingthe available annotations to NEs which have noannotation.Given a cluster clul of cliques, #(clul, ei) is theweight of the NE ei in this cluster: it is the numberof cliques in clul that contain ei.
For all annota-tions ap in the set of all possible annotations AN,we compute its associated score in cluster clul: itis the sum of the weights of NEs in clul that isannotated ap.Then, if the maximal annotation score is greaterthan a simple majority (half) of the total votes7, weassign the corresponding annotation to the clus-ter.
We precise that the annotation <none>8 isprocessed in the same way as any other annota-tions.
Thus, a cluster can be globally annotated<none>.
The limit of this automatic approach isthat it doesn?t allow to annotate new NE types thanthe ones already available.In the following, we will denote by Aclu(clul)the annotation of the cluster clul.The cluster annotation matrix Aclu associatedto the contextual cluster assignment matrix Actxtand the default cluster assignment matrix Adef in-troduced previously will be called the CBC sys-tem?s NE resource (or shortly the NE resource).2.6 NEs annotation processes using the NEresourceIn this paragraph, we describe how, given the CBCsystem?s NE resource, we annotate occurrences ofNEs in the studied corpus with respect to its localcontext.
We precise that for an occurrence of a NEei its associated local context is the set of syntac-tical dependencies cj in which ei is involved.7The total votes number is given byPei?clul#(clul, ei).8The NEs which don?t have any annotation.2.6.1 NEs annotation process for the CBCsystemGiven a NE occurrence and its local context wecan use Actxt(ei, cj) and Adef (ei) in order to getthe default annotation Aclu(Adef (ei)) and the listof contextual annotations {Aclu(Actxt(ei, cj))}j .Then for annotating this NE occurrence usingour NE resource, we apply the following rules:?
if the list of contextual annotations{Aclu(Actxt(ei, cj))}j is conflictual, weannotate the NE occurrence as <none>,?
if the list of contextual annotations is non-conflictual, then we use the corresponding an-notation to annotate the NE occurrence?
if the list of contextual annotations is empty,we use the default annotation Aclu(Adef (ei)).The NE resource plus the annotation process de-scribed in this paragraph lead to a NER systembased on the CBC system.
This NER system willbe called CBC-NER system and it will be tested inour experiments both alone and as a complemen-tary resource.2.6.2 NEs annotation process for an hybridsystemWe place ourselves into an hybrid situation wherewe have two NER systems (NER 1 + NER 2)which provide two different lists of annotatedNEs.
We want to combine these two systems whenannotating NEs occurrences.Therefore, we resolve any conflicts by applyingthe following rules:?
If the same NE occurrence has two different an-notations from the two systems then there aretwo cases.
If one of the two system is CBC-NER system then we take its annotation; oth-erwise we take the annotation provided by theNER system which gave the best precision.?
If a NE occurrence is included in another onewe only keep the biggest one and its annota-tion.
For example, if Jacques Chirac is anno-tated <person> by one system and Chirac by<person> by the other system, then we onlykeep the first annotation.?
If two NE occurrences are contiguous and havethe same annotation, we merge the two NEs inone NE occurrence.3 ExperimentsThe system described in this paper rather targetcorpus-specific NE annotation.
Therefore, our ex-56periments will deal with a corpus of recent newsarticles (see (Shinyama and Sekine, 2004) formotivations regarding our corpus choice) ratherthan well-known annotated corpora.
Our corpusis constituted of news in English published onthe web during two weeks in June 2008.
Thiscorpus is constituted of around 300,000 words(10Mb) which doesn?t represent a very large cor-pus.
These texts were taken from various presssources and they involve different themes (sports,technology, .
.
.
).
We extracted randomly a sub-set of articles and manually annotated 916 NEs (inour experiments, we deal with three types of an-notation namely <person>, <organization> and<location>).
This subset constitutes our test set.In our experiments, first, we applied the XIPparser (A?
?t et al, 2002) to the whole corpus in or-der to construct the frequency matrix D given by(1).
Next, we computed the similarity matrix be-tween NEs according to (2) in order to obtain s?
de-fined by (4).
Using the latter, we computed cliquesof NEs that allow us to obtain the assignment ma-trix T given by (5).
Then we applied the clusteringheuristic described in Algorithm 1.
At this stage,we want to build the NE resource using the clus-ters of cliques.
Therefore, as described in ?2.5,we applied two kinds of clusters annotations: themanual and the automatic processes.
For the firstone, we manually annotated the 100 biggest clus-ters of cliques.
For the second one, we exploitedthe annotations provided by XIP NER (Brun andHage`ge, 2004) and we propagated these annota-tions to the different clusters (see ?2.5.2).The different materials that we obtained consti-tute the CBC system?s NE resource.
Our aim nowis to exploit this resource and to show that it allowsto improve the performances of different classicNER systems.The different NER systems that we tested arethe following ones:?
CBC-NER system M (in short CBC M) basedon the CBC system?s NE resource using themanual cluster annotation (line 1 in Table 1),?
CBC-NER system A (in short CBC A) basedon the CBC system?s NE resource using the au-tomatic cluster annotation (line 1 in Table 1),?
XIP NER or in short XIP (Brun and Hage`ge,2004) (line 2 in Table 1),?
Stanford NER (or in short Stanford) associ-ated to the following model provided by thetool and which was trained on different newsSystems Prec.
Rec.
F-me.1CBC-NER system M 71.67 23.47 35.36CBC-NER system A 70.66 32.86 44.862XIP NER 77.77 56.55 65.48XIP + CBC M 78.41 60.26 68.15XIP + CBC A 76.31 60.48 67.483Stanford NER 67.94 68.01 67.97Stanford + CBC M 69.40 71.07 70.23Stanford + CBC A 70.09 72.93 71.484GATE NER 63.30 56.88 59.92GATE + CBC M 66.43 61.79 64.03GATE + CBC A 66.51 63.10 64.765Stanford + XIP 72.85 75.87 74.33Stanford + XIP + CBC M 72.94 77.70 75.24Stanford + XIP + CBC A 73.55 78.93 76.156GATE + XIP 69.38 66.04 67.67GATE + XIP + CBC M 69.62 67.79 68.69GATE + XIP + CBC A 69.87 69.10 69.487GATE + Stanford 63.12 69.32 66.07GATE + Stanford + CBC M 65.09 72.05 68.39GATE + Stanford + CBC A 65.66 73.25 69.25Table 1: Results given by different hybrid NERsystems and coupled with the CBC-NER systemcorpora (CoNLL, MUC6, MUC7 and ACE):ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkelet al, 2005) (line 3 in Table 1),?
GATE NER or in short GATE (Cunningham etal., 2002) (line 4 in Table 1),?
and several hybrid systems which are given bythe combination of pairs taken among the setof the three last-mentioned NER systems (lines5 to 7 in Table 1).
Notice that these baselinehybrid systems use the annotation combinationprocess described in ?2.6.1.In Table 1 we first reported in each line, the re-sults given by each system when they are appliedalone (figures in italics).
These performances rep-resent our baselines.
Second, we tested for eachbaseline system, an extended hybrid system thatintegrates the CBC-NER systems (with respect tothe combination process detailed in ?2.6.2).The first two lines of Table 1 show that thetwo CBC-NER systems alone lead to rather poorresults.
However, our aim is to show that theCBC-NER system is, despite its low performancesalone, complementary to other basic NER sys-tems.
In other words, we want to show that theexploitation of the CBC system?s NE resource isbeneficial and non-redundant compared to otherbaseline NER systems.This is actually what we obtained in Table 1 asfor each line from 2 to 7, the extended hybrid sys-tems that integrate the CBC-NER systems (M or57A) always perform better than the baseline eitherin terms of precision9 or recall.
For each line, weput in bold the best performance according to theF-measure.These results allow us to show that the NE re-source built using the CBC system is complemen-tary to any baseline NER systems and that it al-lows to improve the results of the latter.In order to illustrate why the CBC-NER systemsare beneficial, we give below some examples takenfrom the test corpus for which the CBC system Ahad allowed to improve the performances by re-spectively disambiguating or correcting a wrongannotation or detecting corpus-specific NEs.First, in the sentence ?From the start, his par-ents, Lourdes and Hemery, were with him.
?, thebaseline hybrid system Stanford + XIP anno-tated the ambiguous NE ?Lourdes?
as <location>whereas Stanford + XIP + CBC A gave the correctannotation <person>.Second, in the sentence ?Got 3 percent chanceof survival, what ya gonna do??
The back read,?A) Fight Through, b) Stay Strong, c) OvercomeBecause I Am a Warrior.
?, the baseline hybridsystem Stanford + XIP annotated ?Warrior?
as<organization> whereas Stanford + XIP + CBCA corrected this annotation with <none>.Finally, in the sentence ?Matthew, also a fa-vorite to win in his fifth and final appearance,was stunningly eliminated during the semifinalround Friday when he misspelled ?secernent?.
?,the baseline hybrid system Stanford + XIP didn?tgive any annotation to ?Matthew?
whereas Stan-ford + XIP + CBC A allowed to give the annota-tion <person>.4 Related worksMany previous works exist in NEs recognition andclassification.
However, most of them do not builda NEs resource but exploit external gazetteers(Bunescu and Pasca, 2006), (Cucerzan, 2007).A recent overview of the field is given in(Nadeau and Sekine, 2007).
According to this pa-per, we can classify our method in the categoryof semi-supervised approaches.
Our proposal isclose to (Cucchiarelli and Velardi, 2001) as it usessyntactic relations (?2.2) and as it relies on exist-ing NER systems (?2.6.2).
However, the partic-ularity of our method concerns the clustering of9Except for XIP+CBC A in line 2 where the precision isslightly lower than XIP?s one.cliques of NEs that allows both to represent thedifferent annotations of the NEs and to group thelatter with respect to one precise annotation ac-cording to a local context.Regarding this aspect, (Lin and Pantel, 2001)and (Ngomo, 2008) also use a clique computa-tion step and a clique merging method.
However,they do not deal with ambiguity of lexical unitsnor with NEs.
This means that, in their system, alexical unit can be in only one merged clique.From a methodological point of view, our pro-posal is also close to (Ehrmann and Jacquet, 2007)as the latter proposes a system for NEs fine-grained annotation, which is also corpus depen-dent.
However, in the present paper we use allsyntactic relations for measuring the similarity be-tween NEs whereas in the previous mentionedwork, only specific syntactic relations were ex-ploited.
Moreover, we use clustering techniquesfor dealing with the issue related to over produc-tion of cliques.In this paper, we construct a NE resource fromthe corpus that we want to analyze.
In that con-text, (Pasca, 2004) presents a lightly supervisedmethod for acquiring NEs in arbitrary categoriesfrom unstructured text of Web documents.
How-ever, Pasca wants to improve web search whereaswe aim at annotating specific NEs of an ana-lyzed corpus.
Besides, as we want to focus oncorpus-specific NEs, our work is also related to(Shinyama and Sekine, 2004).
In this work, theauthors found a significant correlation between thesimilarity of the time series distribution of a wordand the likelihood of being a NE.
This result mo-tivated our choice to test our approach on recentnews articles rather than on well-known annotatedcorpora.5 ConclusionWe propose a system that allows to improve NErecognition.
The core of this system is a clique-based clustering method based upon a distribu-tional approach.
It allows to extract, analyze anddiscover highly relevant information for corpus-specific NEs annotation.
As we have shown in ourexperiments, this system combined with anotherone can lead to strong improvements.
Other appli-cations are currently addressed in our team usingthis approach.
For example, we intend to use theconcept of clique-based clustering as a soft clus-tering method for other issues.58ReferencesS.
A?
?t, J.P. Chanod, and C. Roux.
2002.
Robustnessbeyond shallowness: incremental dependency pars-ing.
NLE Journal.C.
Be?de?carrax and I. Warnesson.
1989.
Relationalanalysis and dictionnaries.
In Proceedings of AS-MDA 1988, pages 131?151.
Wiley, London, New-York.C.
Brun and C. Hage`ge.
2004.
Intertwining deepsyntactic processing and named entity detection.
InProceedings of ESTAL 2004, Alicante, Spain.R.
Bunescu and M. Pasca.
2006.
Using encyclope-dic knowledge for named entity disambiguation.
InProceedings of EACL 2006.A.
Cucchiarelli and P. Velardi.
2001.
UnsupervisedNamed Entity Recognition using syntactic and se-mantic contextual evidence.
Computational Lin-guistics, 27(1).S.
Cucerzan.
2007.
Large-scale named entity disam-biguation based on wikipedia data.
In Proceedingsof EMNLP/CoNLL 2007, Prague, Czech Republic.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A framework and graphicaldevelopment environment for robust NLP tools andapplications.
In Proceedings of ACL 2002, Philadel-phia.M.
Ehrmann and G. Jacquet.
2007.
Vers une dou-ble annotation des entite?s nomme?es.
Traitement Au-tomatique des Langues, 47(3).J.R.
Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by gibbs sampling.
In Proceed-ings of ACL 2005.E.A.
Fox and J.A.
Shaw.
1994.
Combination of multi-ple searches.
In Proceedings of the 3rd NIST TRECConference, pages 105?109.Z.
Harris.
1951.
Structural Linguistics.
University ofChicago Press.J.A.
Hartigan.
1975.
Clustering Algorithms.
John Wi-ley and Sons.A.
Kilgarriff, P. Rychly, P. Smr, and D. Tugwell.
2004.The sketch engine.
In In Proceedings of EURALEX2004.V.
Lavrenko and W.B.
Croft.
2003.
Relevance modelsin information retrieval.
In W.B.
Croft and J. Laf-ferty (Eds), editors, Language modeling in informa-tion retrieval.
Springer.D.
Lin and P. Pantel.
2001.
Induction of semanticclasses from natural language text.
In Proceedingsof ACM SIGKDD.D.
Lin.
1998.
Using collocation statistics in informa-tion extraction.
In Proceedings of MUC-7.J.F.
Marcotorchino and P. Michaud.
1981.
Heuris-tic approach of the similarity aggregation problem.Methods of operation research, 43:395?404.P.
Michaud and J.F.
Marcotorchino.
1980.
Optimisa-tion en analyse de donne?es relationnelles.
In DataAnalysis and informatics.
North Holland Amster-dam.D.
Nadeau and S. Sekine.
2007.
A survey of NamedEntity Recognition and Classification.
LingvisticaeInvestigationes, 30(1).A.
C. Ngonga Ngomo.
2008.
Signum a graph algo-rithm for terminology extraction.
In Proceedings ofCICLING 2008, Haifa, Israel.M.
Pasca.
2004.
Acquisition of categorized namedentities for web search.
In Proceedings of CIKM2004, New York, NY, USA.S.
Ploux and B. Victorri.
1998.
Construction d?espacesse?mantiques a` l?aide de dictionnaires de synonymes.TAL, 39(1).Y.
Shinyama and S. Sekine.
2004.
Named Entity Dis-covery using comparable news articles.
In Proceed-ings of COLING 2004, Geneva.59
