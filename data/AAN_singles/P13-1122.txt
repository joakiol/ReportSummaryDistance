Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1243?1253,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsHEADY: News headline abstraction through event pattern clusteringEnrique AlfonsecaGoogle Inc.ealfonseca@google.comDaniele PighinGoogle Inc.biondo@google.comGuillermo Garrido?NLP & IR Group at UNEDggarrido@lsi.uned.esAbstractThis paper presents HEADY: a novel, ab-stractive approach for headline generationfrom news collections.
From a web-scalecorpus of English news, we mine syntac-tic patterns that a Noisy-OR model gener-alizes into event descriptions.
At inferencetime, we query the model with the patternsobserved in an unseen news collection,identify the event that better captures thegist of the collection and retrieve the mostappropriate pattern to generate a head-line.
HEADY improves over a state-of-the-art open-domain title abstraction method,bridging half of the gap that separatesit from extractive methods using human-generated titles in manual evaluations, andperforms comparably to human-generatedheadlines as evaluated with ROUGE.1 IntroductionMotivation.
News events are rarely reportedonly in one way, from a single point of view.
Dif-ferent news agencies will interpret the event in dif-ferent ways; various countries or locations mayhighlight different aspects of it depending on howthey are affected; and opinions and in-depth anal-yses will be written after the fact.The variety of contents and styles is both an op-portunity and a challenge.
On the positive side, wehave the same events described in different ways;this redundancy is useful for summarization, asthe information content reported by the majorityof news sources most likely represents the centralpart of the event.
On the other hand, variabilityand subjectivity can be difficult to isolate.
Forsome applications it is important to understand,given a collection of related news articles and re-?Work done during an internship at Google Zurich.?
Carmelo and La La Party It Up with Kim and Ciara?
La La Vazquez and Carmelo Anthony: WeddingDay Bliss?
Carmelo Anthony, actress LaLa Vazquez wed inNYC?
Stylist to the Stars?
LaLa, Carmelo Set Off Celebrity Wedding Weekend?
Ciara rocks a sexy Versace Spring 2010 mini toLaLa Vasquez and Carmelo Anthony?s wedding(photos)?
Lala Vasquez on her wedding dress, cake, reality tvshow and fiance?, Carmelo Anthony (video)?
VAZQUEZ MARRIES SPORTS STAR AN-THONY?
Lebron Returns To NYC For Carmelo?s Wedding?
Carmelo Anthony?s stylist dishes on the wedding?
Paul pitching another Big Three with ?Melo inNYC??
Carmelo Anthony and La La Vazquez Get Marriedat Star-Studded Wedding CeremonyTable 1: Headlines observed for a news collectionreporting the same wedding event.ports, how to formulate in an objective way whathas happened.As a motivating example, Table 1 shows the dif-ferent headlines observed in news reporting thewedding between basketball player Carmelo An-thony and actress LaLa Vazquez.
As can be seen,there is a wide variety of ways to report the sameevent, including different points of view, high-lighted aspects, and opinionated statements on thepart of the reporter.
When presenting this event toa user in a news-based information retrieval or rec-ommendation system, different event descriptionsmay be more appropriate.
For example, a user mayonly be interested in objective, informative sum-maries without any interpretation on the part ofthe reporter.
In this case, Carmelo Anthony, ac-1243tress LaLa Vazquez wed in NYC would be a goodchoice.Goal.
Our final goal in this research is to build aheadline generation system that, given a news col-lection, is able to describe it with the most com-pact, objective and informative headline.
In par-ticular, we want the system to be able to:?
Generate headlines in an open-domain, unsu-pervised way, so that it does not need to relyon training data which is expensive to pro-duce.?
Generalize across synonymous expressionsthat refer to the same event.?
Do so in an abstractive fashion, to enforcenovelty, objectivity and generality.In order to advance towards this goal, this paperexplores the following questions:?
What is a good way of using syntactic pat-terns to represent events for generating head-lines??
Can we have satisfactory readability with anopen-domain abstractive approach, not rely-ing on training data nor on manually pre-defined generation templates??
How far can we get in terms of informative-ness, compared to the human-produced head-lines, i.e., extractive approaches?Contributions.
In this paper we presentHEADY, which is at the same time a novel systemfor abstractive headline generation, and a smoothclustering of patterns describing the same events.HEADY is fully open-domain and can scale toweb-sized data.
By learning to generalize eventsacross the boundaries of a single news story ornews collection, HEADY produces compact andeffective headlines that objectively convey therelevant information.When compared to a state-of-the-art open-domain headline abstraction system (Filippova,2010), the new headlines are statistically signifi-cantly better both in terms of readability and in-formativeness.
Also, automatic evaluations usingROUGE, having objective headlines for the newsas references, show that the abstractive headlinesare on par with human-produced headlines.2 Related workHeadline generation and summarization.Most headline generation work in the past hasfocused on the problem of single-document sum-marization: given the main passage of a singlenews article, generate a very short summary ofthe article.
From early in the field, it was pointedout that a purely extractive approach is not goodenough to generate headlines from the bodytext (Banko et al, 2000).
Sometimes the mostimportant information is distributed across severalsentences in the document.
More importantly,quite often, the single sentence selected as themost informative for the news collection is alreadylonger than the desired headline size.
For thisreason, most early headline generation work fo-cused on either extracting and reordering n-gramsfrom the document to be summarized (Banko etal., 2000), or extracting one or two informativesentences from the document and performinglinguistically-motivated transformations to themin order to reduce the summary length (Dorr etal., 2003).
The first approach is not guaranteedto produce grammatical headlines, whereas thesecond approach is tightly tied to the actualwording found in the document.
Single-documentheadline generation was also explored at theDocument Understanding Conferences between2002 and 20041.In later years, there has been more interest inproblems such as sentence compression (Galleyand McKeown, 2007; Clarke and Lapata, 2008;Cohn and Lapata, 2009; Napoles et al, 2011;Berg-Kirkpatrick et al, 2011), text simplification(Zhu et al, 2010; Coster and Kauchak, 2011;Woodsend and Lapata, 2011) and sentence fu-sion (Barzilay and McKeown, 2005; Wan et al,2007; Filippova and Strube, 2008; Elsner and San-thanam, 2011).
All of them have direct applica-tions for headline generation, as it can be con-strued as selecting one or a few sentences fromthe original document(s), and then reducing themto the target title size.
For example, Wan et al(2007) generate novel utterances by combiningPrim?s maximum-spanning-tree algorithm with ann-gram language model to enforce fluency.
Un-like HEADY, the method by Wan and colleaguesis an extractive method that can summarize singledocuments into a sentence, as opposed to generat-ing a sentence that can stand for a whole collec-1http://duc.nist.gov/1244tion of news.
Filippova (2010) reports a systemthat is very close to our settings: the input is acollection of related news articles, and the systemgenerates a headline that describes the main event.This system uses sentence compression techniquesand benefits from the redundancy in the collection.One difference with respect to HEADY is that itdoes not use any syntactic information aside frompart-of-speech tags, and it does not require a train-ing step.
We have used this approach as a baselinefor comparison.There are not many fully abstractive systems fornews summarization.
The few that exist, such asthe work by Genest and Lapalme (2012), rely onmanually written generation templates.
In con-trast, HEADY automatically learns the templatesor headline patterns automatically, which allows itto work in open-domain settings without relyingon supervision or manual annotations.Open-domain pattern learning.
Pattern learn-ing for relation extraction is an active area of re-search that is very related to our problem of eventpattern learning for headline generation.
TextRun-ner (Yates et al, 2007), ReVerb (Fader et al, 2011)and NELL (Carlson et al, 2010; Mohamed et al,2011) are some examples of open-domain systemsthat learn surface patterns that express relationsbetween pairs of entities.
PATTY (Nakashole etal., 2012) generalizes the patterns to also includesyntactic information and ontological (class mem-bership) constraints.
Our patterns are more similarto the ones used by PATTY, which also producesclusters of synonymous patterns.
The main differ-ences are that (a) HEADY is not limited to con-sider patterns expressing relations between pairsof entities; (b) we identify synonym patterns us-ing a probabilistic, Bayesian approach that takesadvantage of the multiplicity of news sources re-porting the same events.
Chambers and Jurafsky(2009) present an unsupervised method for learn-ing narrative schemas from news, i.e., coherentsets of events that involve specific entity types (se-mantic roles).
Similarly to them, we move fromthe assumptions that 1) utterances involving thesame entity types within the same document (inour case, a collection of related documents) arelikely describing aspects of the same event, and2) meaningful representations of the underlyingevents can be learned by clustering these utter-ances in a principled way.Noisy-OR networks.
Noisy-OR Bayesian net-works (Pearl, 1988) have been applied in thepast to a wide class of large-scale probabilis-tic inference problems, from the medical do-main (Middleton et al, 1991; Jaakkola and Jor-dan, 1999; Onisko et al, 2001), to syntheticimage-decomposition and co-citation data analy-sis (S?ingliar and Hauskrecht, 2006).
By assum-ing independence between the causes of the hid-den variables, noisy-OR models tend to be reli-able (Friedman and Goldszmidt, 1996) as they re-quire a relatively small number of parameters tobe estimated (linear with the size of the network).3 Headline generationIn this section, we describe the HEADY system fornews headline abstraction.
Our approach takes asinput, for training, a corpus of news articles or-ganized in news collections.
Once the model istrained, it can generate headlines for new collec-tions.
An outline of HEADY?s main componentsfollows (details of each component are providedin Sections 3.1, 3.2 and 3.3):Pattern extraction.
Identify, in each of the newscollections, syntactic patterns connecting k enti-ties, for k ?
1.
These will be the candidate pat-terns expressing events.Training.
Train a Noisy-OR Bayesian networkon the co-occurrence of syntactic patterns.
Eachpattern extracted in the previous step is added asan observed variable, and latent variables are usedto represent the hidden events that generate pat-terns.
An additional noise variable links to everyterminal node, allowing every terminal to be gen-erated by language background (noise) instead ofby an actual event.Inference.
Generate a headline from an unseennews collection.
First, patterns are extracted usingthe pattern extraction procedure mentioned above.Given the patterns, the posterior probability of thehidden event variables is estimated.
Then, fromthe activated hidden events, the likelihood of ev-ery pattern can be estimated, even if they do notappear in the collection.
The single pattern withthe maximum probability is selected to generate anew headline from it.
Being the product of extra-news collection generalization, the retrieved pat-tern is more likely to be objective and informativethan patterns directly observed in the news collec-tion.1245Algorithm 1 COLLECTIONTOPATTERNS?
(N ):N is a repository of news collections, ?
is a setof parameters controlling the extraction process.R ?
{}for all N ?
N doPREPROCESSDATA(N)E ?
GETRELEVANTENTITIES(N ?
)for all Ei ?
COMBINATIONS?
(E) dofor all n ?
N doP ?
EXTRACTPATTERNS?
(n, Ei)R{N,Ei} ?
R{N,Ei} ?
PreturnR3.1 Pattern extractionIn this section we detail the process for obtain-ing the event patterns that constitute the buildingblocks of learning and inference.Patterns are extracted from a large repositoryN of news collections N1, .
.
.
, N|N |.
Each newscollection N = {ni} is an unordered collec-tion of related news, each of which can be seenas an ordered sequence of sentences, i.e.
: n =[s0, .
.
.
s|n|].Algorithm 1 presents a high-level view of thepattern extraction process.
The different steps aredescribed below:PREPROCESSDATA: We start by preprocess-ing all the news in the news collections with astandard NLP pipeline: tokenization and sentenceboundary detection (Gillick, 2009), part-of-speechtagging, dependency parsing (Nivre, 2006), co-reference resolution (Haghighi and Klein, 2009)and entity linking based on Wikipedia and Free-base.
Using the Freebase dataset, each entity isannotated with all its Freebase types (class labels).In the end, for each entity mentioned in the docu-ment we have a unique identifier, a list with all itsmentions in the document and a list of class labelsfrom Freebase.As a result of this process, we obtain for eachsentence in the corpus a representation as exem-plified in Figure 1 (1).
In this example, the men-tions of three distinct entities have been identified,i.e., e1, .
.
.
, e3.
In the Freebase list of types (classlabels), e1 is a person and a celebrity, and e3 is astate and a location.GETRELEVANTENTITIES: For each news col-lection N we collect the set E of the entities men-tioned most often within the collection.
Next, wegenerate the set COMBINATIONS?
(E) consistingNNP CC NNP TO VB IN NNPPortia and Helen to marry in Californiae1 e2 e3person actress statecelebrity locationrootcc conjnsubjaux prep pobj1NNP NNPe1 e2person actresscelebrityconj 2NNP CC NNP TO VBe1 and e2 to marryperson actresscelebritycc conjnsubjaux3NNP CC NNP TO VBperson and actress to marrycc conjnsubjaux4NNP CC NNP TO VBcelebrity and actress to marrycc conjnsubjauxFigure 1: Pattern extraction process from an anno-tated dependency parse.
(1): an MST is extractedfrom the entity pair e1, e2 (2); nodes are heuristi-cally added to the MST to enforce grammaticality(3); entity types are recombined to generate the fi-nal patterns (4).of non-empty subsets of E, without repeated en-tities.
The number of entities to consider in eachcollection, and the maximum size for the subsetsof entities to consider are meta-parameters embed-ded in ?.2EXTRACTPATTERNS: For each subset of rel-evant entities Ei, event patterns are mined fromthe articles in the news collection.
The processby which patterns are extracted from a news isexplained in Algorithm 2 and exemplified graphi-cally in Figure 1 (2?4).GETMENTIONNODES: Using the dependencyparse T for a sentence s, we first identify the setof nodes Mi that mention the entities in Ei.
IfT does not contain exactly one mention of eachtarget entity in Ei, then the sentence is ignored.Otherwise, we obtain the minimum spanning treefor the nodeset Pi, i.e., the shortest path in the de-pendency tree connecting all the nodes inMi (Fig-ure 1, 2).
Pi is the set of nodes around which thepatterns will be constructed.APPLYHEURISTICS: With very high probabil-ity, the MST Pi that we obtain does not constitutea grammatical or useful extrapolation of the origi-nal sentence s. For example, the MST for the en-2As our objective is to generate very short titles (under10 words), we only consider combinations of up to three ele-ments of E.1246Algorithm 2 EXTRACTPATTERNS?
(n, Ei): n isthe list of sentences in a news article.
Sentencesare POS-tagged, dependency parsed and annotatedwith respect to a set of entities E ?
EiP ?
?for all s ?
n[0 : 2) doT ?
DEPPARSE(s)Mi ?
GETMENTIONNODES(t, Ei)if ?e ?
Ei, count(e,Mi) 6= 1 then continuePi ?
GETMINIMUMSPANNINGTREE?(Mi)APPLYHEURISTICS?
(Pi) or continueP ?
P ?
COMBINEENTITYTYPES?
(Pi)return Ptity pair ?e1, e2?
in the example does not provide agood description of the event as it is neither ade-quate nor fluent.
For this reason, we apply a set ofpost-processing heuristic transformations that aimat including a minimal set of meaningful nodes.These include making sure that both the root of theclause and its subject appear in the extracted pat-tern, and that conjunctions between entities shouldnot be dropped (Figure 1, 3).COMBINEENTITYTYPES: Finally, a distinctpattern is generated from each possible combina-tion of entity type assignments for the participat-ing entities.
(Figure 1, 4).It is important to note that both at training andtest time, for pattern extraction we only considerthe title and the first sentence of the article body.The reason is that we want to limit ourselves, ineach news collection, to the most relevant eventreported in the collection, which appears most ofthe times in these two sentences.
Unlike titles, firstsentences do not extensively use puns or rhetoricsas they tend to be grammatical and informativerather than catchy.The patterns mined from the same news collec-tion and for the same set of entities are groupedtogether, and constitute the building blocks of theclustering algorithm which is described below.3.2 TrainingThe extracted patterns are used to learn a Noisy-OR (Pearl, 1988) model by estimating the prob-ability that each (observed) pattern activates oneor many (hidden) events.
Figure 2 represents thetwo levels: the hidden event variables at the top,and the observed pattern variables at the bottom.An additional noise variable links to every termi-e1 ... en noisep3p2p1 ... pmFigure 2: Probabilistic model.
The associationsbetween latent event variables and observed pat-tern variables are modeled by noisy-OR gates.Events are assumed to be marginally independent,and patterns conditionally independent given theevents.nal node, allowing all terminals to be generated bylanguage background (noise) instead of by an ac-tual event.
The associations between latent eventsand observed patterns are modeled by noisy-ORgates.In this model, the conditional probability of ahidden event ei given a configuration of observedpatterns p ?
{0, 1}|P| is calculated as:P (ei = 0 | p) = (1?
qi0)?j?pij(1?
qij)pj= exp???
?i0 ??j?pii?ijpj??
,where pii is the set of active events (i.e., pii =?j{pj} | pj = 1), and qij = P (ei = 1 | pj = 1)is the estimated probability that the observed pat-tern pi can, in isolation, activate the event e. Theterm qi0 is the so-called ?noise?
term of the model,and it accounts for the fact that an observed eventei might be activated by some pattern that hasnever been observed (Jaakkola and Jordan, 1999).In Algorithm 1, at the end of the process wegroup in R[N,Ei] all the patterns extracted fromthe same news collection N and entity sub-set Ei.These groups represent rough clusters of patterns,that we can use to bootstrap the optimization ofthe model parameters ?ij = ?
log(1 ?
qij).
Weinitiate the training process by randomly selecting100,000 of these groups, and optimize the weightsof the model through 40 EM (Dempster et al,1977) iterations.3.3 Inference (generation of new headlines)Given an unseen news collection N , the inferencecomponent of HEADY generates a single headlinethat captures the main event reported by the newsin N .
In order to do so, we first need to select a1247single event-pattern p?
that is especially relevantfor N .
Having selected p?, in order to generate aheadline it is sufficient to replace the entity place-holders in p?
with the surface forms observed inN .To identify p?, we start from the assumption thatthe most descriptive event encoded by N must de-scribe an important situation in which some subsetof the relevant entities E in N are involved.The basic inference algorithm is a two-step random walk in the Bayesian network.Given a set of entities E and sentences n,EXTRACTPATTERNS?
(n, E) collects patterns in-volving those entities.
By normalizing the fre-quency of the extracted patterns, we get a prob-ability distribution over the observed variables inthe network.
A two-step random walk traversingto the latent event nodes and back to the patternnodes allows us to generalize across events.
Wecall this algorithm INFERENCE(n, E).In order to decide which is the most relevant setof events that should appear in the headline, weuse the following procedure:1.
Given the set of entities E mentioned in thenews collection, we consider each entity sub-set Ei ?
E including up to three entities3.For each Ei, we run INFERENCE(n, Ei),which computes a distribution wi over pat-terns involving the entities in Ei.2.
We invoke again INFERENCE, now using atthe same time all the patterns extracted forevery subset of Ei ?
E. This computes aprobability distribution w over all patterns in-volving any admissible subset of the entitiesmentioned in the collection.3.
Third, we select the entity-specific distribu-tion that approximates better the overall dis-tributionw?
= arg maxicos(w,wi)We assume that the corresponding set of en-tities Ei are the most central entities in thecollection and therefore any headline shouldmake sure to mention them all.3As we noted before, we impose this limitation to keep thegenerated headlines relatively short and to limit data sparsityissues.4.
Finally, we select the pattern with the highestweight in w?
as the pattern that better cap-tures the main event reported in the news col-lection:p?
= pj | wj = arg maxj w?jThe headline is then produced from p?, replac-ing placeholders with the entities in the documentfrom which the pattern was extracted.While in many cases information about entitytypes would be sufficient to decide about the or-der of the entities in the generated sentences (e.g.,?
[person] married in [location]?
for the entityset {ea = ?Mr.
Brown?, eb = ?Los Angeles?
}),in other cases class assignment can be ambigu-ous (e.g., ?
[person] killed [person]?
for {ea =?Mr.
A?, eb = ?Mr.
B?}).
To handle these cases,when extracting patterns for an entity set {ea, eb},we keep track of the alphabetical ordering ofthe entities, e.g., from a news collection about?Mr.
B?
killing ?Mr.
A?
we would producepatterns such as ?
[person:2] killed [person:1]?
or?
[person:1] was killed by [person:2]?
since ea =?Mr.
A?
< eb = ?Mr.
B?.
At inference time,when we query the model with such patterns wecan only activate events whose assignments arecompatible with the entities observed in the text,making the replacement straightforward and un-ambiguous.4 Experiment settingsIn our method we use patterns that are fully lex-icalized (with the exception of entity placehold-ers) and enriched with syntactic data.
Under thesecircumstances, the Noisy-OR can effectively gen-eralize and learn meaningful clusters only if pro-vided with large amounts of data.
To our bestknowledge, available data sets for headline gen-eration are not large enough to support this kindof inference.For this reason, we rely on a corpus of newscrawled from the web between 2008 and 2012which have been clustered based on closeness intime and cosine similarity, using the vector-spacemodel and tf.idf weights.
News collections withless than 5 documents are discarded4, and those4There is a very long tail of singleton articles, which donot offer useful examples of lexical or syntactic variation, andmany very small collections that tend to be especially noisy,hence the decision to consider only collections with at least 5documents.1248larger than 50 documents are capped, by randomlypicking 50 documents from the collection5.
Thetotal number of news collections after clusteringis 1.7 million.
From this set, we have set asidea few hundred collections that will remain unseenuntil the final evaluation.As we have no development set, we have doneno tuning of the parameters for pattern extractionnor for the Bayesian network training (100,000 la-tent variables to represent the different events, 40EM iterations, as mentioned in Section 3.2).
TheEM iterations on the noisy-OR were distributedacross 30 machines with 16 GB of memory each.4.1 Systems usedOne of the questions we wanted to answer inthis research was whether it was possible to ob-tain the same quality with automatically abstractedheadlines as with human-generated headlines.
Forevery news collection we have as many human-generated headlines as documents.
To decidewhich human-generated headline should be usedin this comparison, we used three different meth-ods that pick one of the collection headlines:?
Latest headline: selects the headline fromthe latest document in the collection.
Intu-itively this should be the most relevant onefor news about sport matches and competi-tions, where the earlier headlines offer pre-views and predictions, and the later headlinesreport who won and the final scores.?
Most frequent headline: some headlinesare repeated across the collection, and thismethod chooses the most frequent one.
Ifthere are several with the same frequency,one is taken at random6.?
TopicSum: we use TopicSum (Haghighi andVanderwende, 2009), a 3-layer hierarchicaltopic model, to infer the language model thatis most central for the collection.
The newstitle that has the smallest Kullback-Leibler5Even though we did not run any experiment to find anoptimal value for this parameter, 50 documents seems likea reasonable choice to avoid redundancy while allowing forconsiderable lexical and syntactic variation.6The most frequent headline only has a tie in 6 collectionsin the whole test set.
In 5 cases two headlines are tied at fre-quencies around 4, and in one case three headlines are tied atfrequency 2.
All six are large collections with 50 news arti-cles, so this baseline is significantly different from a randombaseline.R-1 R-2 R-SU4HEADY 0.3565 0.1903 0.1966Most frequent pattern 0.3560 0.1864 0.1959TopicSum 0.3594 0.1821 0.1935MSC 0.3470 0.1765 0.1855Most frequent headline 0.3177 0.1401 0.1668Latest headline 0.2814 0.1191 0.1425Table 2: Results from the automatic evaluation,sorted according to the ROUGE-2 and ROUGE-SU4 scores.divergence with respect the collection lan-guage model is the one chosen.A headline generation system that addressesthe same application as ours is (Filippova, 2010),which generates a graph from the collection sen-tences and selects the shortest path between thebegin and the end node traversing words in thesame order in which they were found in the orig-inal documents.
We have used this system, calledMulti-Sentence Compression (MSC), for compar-isons.Finally, in order to understand whether thenoisy-OR Bayesian network is useful for general-izing across patterns into latent events, we added abaseline that extracts all patterns from the test col-lection following the same COLLECTIONTOPAT-TERNS algorithm (including the application of thelinguistically motivated heuristics), and then pro-duces a headline straightaway from the most fre-quent pattern extracted.
In other words, the onlydifference with respect to HEADY is that in thiscase no generalization through the Noisy-OR net-work is carried out, and that headlines are gen-erated from patterns directly observed in the testnews collections.
We call this system Most fre-quent pattern.4.2 Annotation activitiesIn order to evaluate HEADY?s performance, wecarried out two annotation activities.First, from the set of collections that we hadset aside at the beginning, we randomly chose 50collections for which all the systems could gen-erate an output, and we asked raters to manuallywrite titles for them.
As this is not a simple taskto be crowdsourced, for this evaluation we reliedon eight trained raters.
We collected between fourand five reference titles for each of the fifty newscollections, to be used to compare the headline1249Readability InformativenessTopicSum 4.86 4.63Most freq.
headline ?
?4.61 ?
?34.43Latest headline ?
?4.55 ?
4.00HEADY ?
4.28 ?
3.75Most freq.
pattern ?
3.95 ?
3.82MSC 3.00 3.05Table 3: Results from the manual evaluation.
At95% confidence, TopicSum is significantly betterthan all others for readability, and only indistin-guishable from the most frequent pattern for in-formativeness.
For the rest, 3 means being signifi-cantly better than HEADY, ?
than the most frequentpattern, and ?
than MSC.generation methods using automatic summariza-tion metrics.Then, we took the output of the systems for the50 test collections and asked human raters to eval-uate the headlines:1.
Raters were shown one headline and asked torate it in terms of readability on a 5-pointLikert scale.
In the instructions, the raterswere provided with examples of ungrammat-ical and grammatical titles to guide them inthis annotation.2.
After the previous rating is done, raters wereshown a selection of five documents from thecollection, and they were asked to judge theinformativeness of the previous headline forthe news in the collection, again on a 5-pointLikert scale.This second annotation was carried out by inde-pendent raters in a crowd-sourcing setting.
Theraters did not have any involvement with the in-ception of the model or the writing of the pa-per.
They did not know that the headlines theywere rating were generated according to differ-ent methods.
We measured inter-judge agreementon the Likert-scale annotations using their Intra-Class Correlation (ICC) (Cicchetti, 1994).
TheICC for readability is 0.76 (0.95 confidence in-terval [0.71, 0.80]), and for informativeness it is0.67 (0.95 confidence interval [0.60, 0.73]).
Thismeans strong agreement for readability, and mod-erate agreement for informativeness.5 ResultsThe COLLECTIONTOPATTERNS algorithm wasrun on the training set, producing a 230 millionevent patterns.
Patterns that were obtained fromthe same collection and involving the same entitieswere grouped together, for a total of 1.7 millionpattern collections.
The pattern groups are used tobootstrap the Noisy-OR model training.
Trainingthe HEADY model that we used for the evaluationtook around six hours on 30 cores.Table 2 shows the results of the comparisonof the headline generation systems using ROUGE(R-1, R-2 and R-SU4) (Lin, 2004) with the col-lected references.
According to Owczarzak etal.
(2012), ROUGE is still a competitive met-ric that correlates well with human judgementsfor ranking summarizers.
The significance testsfor ROUGE are performed using bootstrap resam-pling and a graphical significance test (Minka,2002).
The human annotators that created thereferences for this evaluation were explicitly in-structed to write objective titles, which is the kindof headlines that the abstractive systems aim atgenerating.
It is common to see real headlinesthat are catchy, joking, or with a double mean-ing, and therefore they use a different vocabularythan objective titles that simply mention what hap-pened.
TopicSum sometimes selects objective ti-tles amongst the human-made titles and that iswhy it also scores very well with the ROUGEscores.
But the other two criteria for choosinghuman-made headlines select non-objective titlesmuch more often, and this lowers their perfor-mance when measured with ROUGE with respectto the objective references.Table 3 lists the results of the manual evaluationof readability and informativeness of the generatedheadlines.
The first result that we can see is thedifference in the rankings between the two evalu-ations.
Part of this difference might be due to thefact that ROUGE is not as good for discriminatingbetween human-made and automatic summaries.In fact, in the DUC competitions, the gap betweenhuman summaries and automatic summaries wasalso more apparent in the manual evaluations thanusing ROUGE.
Another part of the observed dif-ference may be due to the design of the evalua-tion.
The manual evaluation is asking raters tojudge whether real, human-written titles that wereactually used for those news are grammatical andinformative.
As could be expected, as these arepublished titles, the real titles score very good onthe manual evaluation.Some other interesting results are:1250Model Generated titleTopicSum Modern Family?s Eric Stonestreet laughs offCharlize Theron rumoursMSC Modern Family star Eric Stonestreet is datingCharlize Theron.Latest headline Eric laughs off Theron dating rumoursFrequent pattern Eric Stonestreet jokes about Charlize relationshipFrequent headline Charlize Theron dating Modern Family starHEADY Eric Stonestreet not dating Charlize TheronTopicSum McFadzean rescues point for Crawley TownMSC Crawley side challenging for a point against Old-ham Athletic.Latest headline Reds midfielder victim of racist tweetFrequent pattern Kyle McFadzean fired a equaliser Crawley weremadeFrequent headline Latics halt Crawley chargeHEADY Kyle McFadzean rescues point for Crawley TownF.C.TopicSum UCI to strip Lance Armstrong of his 7 Tour titlesMSC The international cycling union said today.Latest headline Letters: elderly drivers and Lance ArmstrongFrequent pattern Lance Armstrong stripped of Tour de France ti-tlesFrequent headline Today in the news: third debate is tonightHEADY Lance Armstrong was stripped of Tour de FrancetitlesTable 4: A comparison of the titles generated bythe different models for three news collections.?
Amongst the automatic systems, HEADY per-formed better than MSC, with statistical sig-nificance at 95% for all the metrics.
Head-lines based on the most frequent patternswere better than MSC for all metrics butROUGE-2.?
The most frequent pattern baseline andHEADY have comparable performance acrossall the metrics (not statistically significantlydifferent), although HEADY has slightly bet-ter scores for all metrics except for informa-tiveness.While we do not take any step to explicitlymodel stylistic variation, estimating the weightsof the Noisy-OR network turns out to be a veryeffective way of filtering out sensational wordingto the advantage of plainer, more objective style.This may not clearly emerge from the evaluation,as we did not explicitly ask the raters to annotatethe items based on their objectivity, but a manualinspection of the clusters suggests that the gener-alization is working in the right direction.Table 4 presents a selection of outputs producedby the six models for three different news collec-tions.
The first example shows a news collectioncontaining news about a rumour that was imme-diately denied.
In the second example, HEADYgeneralization improves over the most frequentpattern.
In the third case, HEADY generates agood title from a noisy collection (containing dif-ferent but related events).
The examples alsoshow that TopicSum is very effective in selectinga good human-generated headline for each collec-tion.
This opens the possibility of using TopicSumto automatically generate ROUGE references forfuture evaluations of abstractive methods.6 ConclusionsWe have presented HEADY, an abstractive head-line generation system based on the generaliza-tion of syntactic patterns by means of a Noisy-ORBayesian network.
We evaluated the model bothautomatically and through human annotations.HEADY performs significantly better than a state-of-the-art open domain abstractive model (Filip-pova, 2010) in all evaluations, and is in par withhuman-generated headlines in terms of ROUGEscores.
We have shown that it is possible toachieve high quality generation of news headlinesin an open-domain, unsupervised setting by suc-cessfully exploiting syntactic and ontological in-formation.
The system relies on a standard NLPpipeline, requires no manual data annotation andcan effectively scale to web-sized corpora.For feature work, we plan to improve all compo-nents of HEADY in order to fill in the gap with thehuman-generated titles in terms of readability andinformativeness.
One of the directions in whichwe plan to move is the removal of the syntac-tic heuristics that currently enforce pattern well-formedness and to automatically learn the neces-sary transformations from the data.Two other lines of work that we plan to exploreare the possibility of personalizing the headlinesto user interests (as stored in user profiles or ex-pressed as user queries), and to investigate furtherapplications of the Bayesian network of event pat-terns, such as its use for relation extraction andknowledge base population.AcknowledgmentsThe research leading to these results has receivedfunding from: the EU?s 7th Framework Pro-gramme (FP7/2007-2013) under grant agreementnumber 257790; the Spanish Ministry of Scienceand Innovation?s project Holopedia (TIN2010-21128-C02); and the Regional Government ofMadrid?s MA2VICMR (S2009/TIC1542).
Wewould like to thank Katja Filippova and the anony-mous reviewers for their insightful comments.1251ReferencesMichele Banko, Vibhu O. Mittal, and Michael J. Wit-brock.
2000.
Headline generation based on statis-tical translation.
In Proceedings of the 38th AnnualMeeting of the Association for Computational Lin-guistics, ACL ?00, pages 318?325.
Association forComputational Linguistics.Regina Barzilay and Kathleen R McKeown.
2005.Sentence fusion for multidocument news summa-rization.
Computational Linguistics, 31(3):297?328.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 481?490.
Association forComputational Linguistics.Andrew Carlson, Justin Betteridge, Bryan Kisiel,Burr Settles, Estevam R Hruschka Jr, and Tom MMitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of theTwenty-Fourth Conference on Artificial Intelligence(AAAI 2010), pages 3?3.Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised Learning of Narrative Schemas and TheirParticipants.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 2 - Volume2, pages 602?610.Domenic V Cicchetti.
1994.
Guidelines, criteria, andrules of thumb for evaluating normed and standard-ized assessment instruments in psychology.
Psycho-logical Assessment, 6(4):284.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression: An integer linearprogramming approach.
Journal of Artificial Intelli-gence Research, 31(1):399?429.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of ArtificialIntelligence Research, 34:637?674.William Coster and David Kauchak.
2011.
Learning tosimplify sentences using Wikipedia.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gen-eration, pages 1?9.
Association for ComputationalLinguistics.Arthur P. Dempster, Nan M. Laird, and Donald B.Rubi.
1977.
Maximum likelihood from incompletedata via the EM algorithm.
Journal of the Royal Sta-tistical Society, Series B, 39(1):1?38.Bonnie Dorr, David Zajic, and Richard Schwartz.2003.
Hedge trimmer: A parse-and-trim approachto headline generation.
In Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume5, pages 1?8.
Association for Computational Lin-guistics.Micha Elsner and Deepak Santhanam.
2011.
Learn-ing to fuse disparate sentences.
In Proceedings ofthe Workshop on Monolingual Text-To-Text Gener-ation, pages 54?63.
Association for ComputationalLinguistics.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 1535?1545.
Association for ComputationalLinguistics.Katja Filippova and Michael Strube.
2008.
Sentencefusion via dependency graph compression.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 177?185.
As-sociation for Computational Linguistics.Katja Filippova.
2010.
Multi-sentence compression:Finding shortest paths in word graphs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, pages 322?330.
Associationfor Computational Linguistics.Nir Friedman and Moises Goldszmidt.
1996.
LearningBayesian networks with local structure.
In Proceed-ings of the Twelfth Conference Annual Conferenceon Uncertainty in Artificial Intelligence (UAI-96),pages 252?262, San Francisco, CA.
Morgan Kauf-mann.Michel Galley and Kathleen McKeown.
2007.
Lex-icalized Markov grammars for sentence compres-sion.
Proceedings of the North American Chap-ter of the Association for Computational Linguistics,pages 180?187.Pierre-Etienne Genest and Guy Lapalme.
2012.
Fullyabstractive approach to guided summarization.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics, short papers.Association for Computational Linguistics.Dan Gillick.
2009.
Sentence boundary detection andthe problem with the us.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, Companion Vol-ume: Short Papers, pages 241?244.
Association forComputational Linguistics.Aria Haghighi and Dan Klein.
2009.
Simple coref-erence resolution with rich syntactic and semanticfeatures.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing: Volume 3-Volume 3, pages 1152?1161.
Asso-ciation for Computational Linguistics.Aria Haghighi and Lucy Vanderwende.
2009.
Ex-ploring content models for multi-document summa-rization.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 362?370.
Association forComputational Linguistics.1252Tommi S. Jaakkola and Michael I. Jordan.
1999.Variational probabilistic inference and the QMR-DT Network.
Journal of Artificial Intelligence Re-search, 10:291?322.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81.Blackford Middleton, Michael Shwe, David Hecker-man, Max Henrion, Eric Horvitz, Harold Lehmann,and Gregory Cooper.
1991.
Probabilistic diag-nosis using a reformulation of the INTERNIST-1/QMR knowledge base.
I.
The probabilistic modeland inference algorithms.
Methods of information inmedicine, 30(4):241?255, October.Tom Minka.
2002.
Judging Significance from ErrorBars.
CM U Tech R eport.Thahir P Mohamed, Estevam R Hruschka Jr, andTom M Mitchell.
2011.
Discovering relations be-tween noun categories.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1447?1455.
Association for Com-putational Linguistics.Ndapandula Nakashole, Gerhard Weikum, and FabianSuchanek.
2012.
Patty: A taxonomy of relationalpatterns with semantic types.
EMNLP12.Courtney Napoles, Chris Callison-Burch, Juri Ganitke-vitch, and Benjamin Van Durme.
2011.
Paraphras-tic sentence compression with a character-basedmetric: Tightening without deletion.
In Proceed-ings of the Workshop on Monolingual Text-To-TextGeneration, pages 84?90.
Association for Computa-tional Linguistics.Joakim Nivre.
2006.
Inductive Dependency Parsing,volume 34 of Text, Speech and Language Technol-ogy.
Springer.Agnieszka Onisko, Marek J. Druzdzel, and Hanna Wa-syluk.
2001.
Learning Bayesian network parame-ters from small data sets: application of Noisy-ORgates.
International Journal of Approximated Rea-soning, 27(2):165?182.Karolina Owczarzak, John M. Conroy, Hoa TrangDang, and Ani Nenkova.
2012.
An assessment ofthe accuracy of automatic evaluation in summariza-tion.
In Proceedings of the NAACL-HLT 2012 Work-shop on Evaluation Metrics and System Comparisonfor Automatic Summarization, pages 1?9.
Associa-tion for Computational Linguistics.Judea Pearl.
1988.
Probabilistic reasoning in intelli-gent systems: networks of plausible inference.
Mor-gan Kaufmann.Toma?s?
S?ingliar and Milos?
Hauskrecht.
2006.
Noisy-orcomponent analysis and its application to link analy-sis.
J. Mach.
Learn.
Res., 7:2189?2213, December.Stephen Wan, Robert Dale, Mark Dras, and Ce?cileParis.
2007.
Global Revision in Summarisation:Generating Novel Sentences with Prim?s Algorithm.In Proceedings of PACLING 2007 - 10th Conferenceof the Pacific Association for Computational Lin-guistics.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 409?420.
Associationfor Computational Linguistics.Alexander Yates, Michael Cafarella, Michele Banko,Oren Etzioni, Matthew Broadhead, and StephenSoderland.
2007.
TextRunner: Open informationextraction on the web.
In Proceedings of HumanLanguage Technologies: The Annual Conference ofthe North American Chapter of the Association forComputational Linguistics: Demonstrations, pages25?26.
Association for Computational Linguistics.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of The23rd International Conference on ComputationalLinguistics, pages 1353?1361.1253
