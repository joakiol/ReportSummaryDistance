Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 1?18,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPReport of NEWS 2009 Machine Transliteration Shared TaskHaizhou Li?, A Kumaran?, Vladimir Pervouchine?
and Min Zhang?
?Institute for Infocomm Research, A*STAR, Singapore 138632{hli,vpervouchine,mzhang}@i2r.a-star.edu.sg?Multilingual Systems Research, Microsoft Research IndiaA.Kumaran@microsoft.comAbstractThis report documents the details of theMachine Transliteration Shared Task con-ducted as a part of the Named Enti-ties Workshop (NEWS), an ACL-IJCNLP2009 workshop.
The shared task featuresmachine transliteration of proper namesfrom English to a set of languages.
Thisshared task has witnessed enthusiastic par-ticipation of 31 teams from all over theworld, with diversity of participation fora given system and wide coverage for agiven language pair (more than a dozenparticipants per language pair).
Diversetransliteration methodologies are repre-sented adequately in the shared task for agiven language pair, thus underscoring thefact that the workshop may truly indicatethe state of the art in machine transliter-ation in these language pairs.
We mea-sure and report 6 performance metrics onthe submitted results.
We believe that theshared task has successfully achieved thefollowing objectives: (i) bringing togetherthe community of researchers in the areaof Machine Transliteration to focus on var-ious research avenues, (ii) Calibrating sys-tems on common corpora, using commonmetrics, thus creating a reasonable base-line for the state-of-the-art of translitera-tion systems, and (iii) providing a quan-titative basis for meaningful comparisonand analysis between various algorithmicapproaches used in machine translitera-tion.
We believe that the results of thisshared task would uncover a host of inter-esting research problems, giving impetusto research in this significant research area.1 IntroductionNames play a significant role in many NaturalLanguage Processing (NLP) and Information Re-trieval (IR) systems.
They have a critical rolein Cross Language Information Retrieval (CLIR)and Machine Translation (MT) systems as the sys-tems?
performances are shown to positively cor-relate with the correct conversion of names be-tween the languages in several studies (Demner-Fushman and Oard, 2002; Mandl and Womser-Hacker, 2005; Hermjakob et al, 2008; Udupa etal., 2009).
The traditional source for name equiva-lence, the bilingual dictionaries ?
whether hand-crafted or statistical ?
offer only limited supportas they do not have sufficient coverage of names.New names are introduced to the vocabulary of alanguage every day.All of the above point to the critical need for ro-bust Machine Transliteration technology and sys-tems.
This has attracted attention from the re-search community.
Over the last decade scores ofpapers on Machine Transliteration have appearedin the top Computational Linguistics, InformationRetrieval and Data Management conferences, ex-ploring diverse algorithmic approaches in a widevariety of different languages (Knight and Graehl,1998; Li et al, 2004; Zelenko and Aone, 2006;Sproat et al, 2006; Sherif and Kondrak, 2007;Hermjakob et al, 2008; Goldwasser and Roth,2008; Goldberg and Elhadad, 2008; Klementievand Roth, 2006).
However, there has not beenany coordinated effort in calibrating the state-of-the-art technical capabilities of machine translit-eration: the studies explore different algorithmicapproaches in different language pairs and reporttheir performance in different metrics and testedon different corpora.The overarching objective of this shared taskis to drive the machine transliteration technologyforward, to measure and baseline the state-of-the-1art and to provide a meaningful comparison be-tween the most promising algorithmic approachesin order to stimulate the discussions among the re-searchers.
The NLP community in Asia is espe-cially interested in transliteration as several majorAsian languages do not use Latin script in their na-tive writing systems.
The Named Entity Workshop(NEWS 2009) in ACL-IJCNLP 2009 in Singaporeprovides an ideal platform for the shared task totake off.
This is precisely what we address in thisshared task on machine transliteration that is con-ducted as a part of the Named Entity Workshop(NEWS-2009), an ACL-IJCNLP 2009 workshop.The shared task aims at achieving the followingobjectives:?
Providing a forum to bring together the com-munity of researchers in the area of MachineTransliteration to focus on various researchavenues in this important research area.?
Calibrating systems on common hand-craftedcorpora, using common metrics, in many dif-ferent languages, thus creating a reasonablebaseline for the state-of-the-art of translitera-tion systems.?
Analysing the results so that a reason-able comparison of different algorithmicapproaches and their trade-offs (such as,transliteration quality vs. generality of ap-proach across languages vs. training datasize, etc.)
may be explored.We believe that a substantial part of what we haveset out to achieve has been accomplished, and wepresent this report as a record of the task pro-cess, system participation and results and our find-ings.
It is our hope that this reporting will generatelively discussions during the NEWS workshop andsubsequent research in this important area.This introduction outlines the purpose of thetransliteration shared task conducted as a part ofthe NEWS workshop.
Section 2 outlines the ma-chine transliteration task and the corpora used andSection 3 discusses the metrics chosen for evalua-tion, along with the rationale for choosing them.Section 4 sketches the participation.
Section 5presents the results of the shared task and the anal-ysis of the results.
Section 6, summarises thequeries and feedback we have received from theparticipants and Section 7 concludes, presentingsome lessons learnt from the current edition of theshared task, and some ideas we want to pursuein the future plan for the Machine Transliterationtasks.2 Transliteration Shared TaskIn this section, we outline the definition of the task,the process followed and the rationale for the de-cisions.2.1 ?Transliteration?
: A definitionThere exists several terms that are used inter-changeably in the contemporary research litera-ture for the conversion of names between twolanguages, such as, transliteration, transcription,and sometimes Romanisation, especially if Latinscripts are used for target strings (Halpern, 2007).Our aim is not only at capturing the name con-version process from a source to a target language,but also at its ultimate utility for downstream ap-plications, such as CLIR and MT.
We have nar-rowed down to three specific requirements for thetask, as follows: ?Transliteration is the conver-sion of a given name in the source language (atext string in the source writing system or orthog-raphy) to a name in the target language (anothertext string in the target writing system or orthog-raphy), such that the target language name is:(i) phonemically equivalent to the source name(ii) conforms to the phonology of the target lan-guage and (iii) matches the user intuition of theequivalent of the source language name in the tar-get language.
?Given that the phoneme set of languages maynot be exactly the same, the first requirement mustbe diluted to ?close to?, instead of ?equivalent?.The second requirement is needed to ensure thatthe target string is a valid string as per the targetlanguage phonology.
The third requirement is in-troduced to produce what a normal user would ex-pect (at least for the popular names), and in or-der to make it useful for downstream applicationslike MT or CLIR systems.
Though the third re-quirement make systems produce target languagestrings that marginally violate the first or secondrequirements, it ensures that such transliterationsystem is of value to downstream systems.
All theabove requirements are implicitly enforced by thechoice of name pairs used to define the trainingand test corpora in a given language pair.
In caseswhere multiple equivalent target language namesare possible for a source language name, we in-2clude all of them.After much debate, we have also retained thetask name as ?transliteration?, though our defi-nition may be closest to the ?popular transcrip-tion?
(Halpern, 2007), due to the popularity ofterm ?Machine Transliteration?
among the lan-guage technology researchers.2.2 Shared Task DescriptionThe shared task is specified as development of ma-chine transliteration systems in one or more of thespecified language pairs.
Each language pair ofthe shared task consists of a source and a targetlanguage, implicitly specifying the transliterationdirection.
Training and development data in eachof the language pairs have been made available toall registered participants for developing a translit-eration system for that specific language pair usingany approach that they find appropriate.At the evaluation time, a standard hand-craftedtest set consisting of between 1,000 and 3,000source names (approximately 10% of the train-ing data size) have been released, on which theparticipants are required to produce a ranked listof transliteration candidates in the target languagefor each source name.
The system output istested against a reference set (which may includemultiple correct transliterations for some sourcenames), and the performance of a system is cap-tured in multiple metrics (defined in Section 3),each designed to capture a specific performancedimension.For every language pair every participant is re-quired to submit one run (designated as a ?stan-dard?
run) that uses only the data provided by theNEWS workshop organisers in that language pair,and no other data or linguistic resources.
Thisstandard run ensures parity between systems andenables meaningful comparison of performanceof various algorithmic approaches in a given lan-guage pair.
Participants are allowed to submitmore runs (designated as ?non-standard?)
for ev-ery language pair using either data beyond thatprovided by the shared task organisers or linguis-tic resources in a specific language, or both.
Thisessentially may enable any participant to demon-strate the limits of performance of their system ina given language pair.The shared task timelines provide adequate timefor development, testing (approximately 2 monthsafter the release of the training data) and the finalresult submission (5 days after the release of thetest data).2.3 Shared Task CorporaWe have had two specific constraints in selectinglanguages for the shared task: language diversityand data availability.
To make the shared task in-teresting and to attract wider participation, it isimportant to ensure a reasonable variety amongthe languages in terms of linguistic diversity, or-thography and geography.
Clearly, the ability ofprocuring and distributing a reasonably large (ap-proximately 10K paired names for training andtesting together) hand-crafted corpora consistingprimarily of paired names is critical for this pro-cess.
At the end of the planning stage and afterdiscussion with the data providers, we have cho-sen the set of 7 languages shown in Table 1 for thetask (Li et al, 2004; Kumaran and Kellner, 2007;MSRI, 2009; CJKI, 2009).For all of the languages chosen, we have beenable to procure paired names data between En-glish and the respective languages and were ableto make them available to the participants.
In ad-dition, we have been able to procure a specificcorpus of about 40K Romanised Japanese namesand their Kanji counterparts, and the correspond-ing language pair (Japanese names from their Ro-manised form to Kanji) has been included as oneof the task language pair.It should be noted here that each corpus has adefinite skew in its characteristics: the names inthe Chinese, Japanese and Korean (CJK) languagecorpora are Western names; the Indic languages(Hindi, Kannada and Tamil) corpora consists of amix of Indian and Western names.
The Roman-ised Kanji to Kanji corpus consists only of nativeJapanese names.
While such characteristics mayhave provided us an opportunity to specificallymeasure the performance for forward translitera-tions (in CJK) and backward transliterations (inRomanised Kanji), we do not highlight such finedistinctions in this edition.Finally, it should be noted here that the corporaprocured and released for NEWS 2009 representperhaps the most diverse and largest corpora to beused for any common transliteration tasks today.3 Evaluation Metrics and RationaleThe participants have been asked to submit re-sults of one standard and up to four non-standard3Source language Target language Data Source Data Size (No.
source names) Task IDTraining Development TestingEnglish Hindi Microsoft Research India 9,975 974 1,000 EnHiEnglish Tamil Microsoft Research India 7,974 987 1,000 EnTaEnglish Kannada Microsoft Research India 7,990 968 1,000 EnKaEnglish Russian Microsoft Research India 5,977 943 1,000 EnRuEnglish Chinese Institute for Infocomm Research 31,961 2,896 2,896 EnChEnglish Korean Hangul CJK Institute 4,785 987 989 EnKoEnglish Japanese Katakana CJK Institute 23,225 1,492 1,489 EnJaJapanese name (in English) Japanese Kanji CJK Institute 6,785 1,500 1,500 JnJkTable 1: Source and target languages for the shared task on transliteration.runs.
Each run contains a ranked list of up to10 candidate transliterations for each source name.The submitted results are compared to the groundtruth (reference transliterations) using 6 evaluationmetrics capturing different aspects of translitera-tion performance.
Since a name may have mul-tiple correct transliterations, all these alternativesare treated equally in the evaluation, that is, anyof these alternatives is considered as a correcttransliteration, and all candidates matching any ofthe reference transliterations are accepted as cor-rect ones.The following notation is further assumed:N : Total number of names (sourcewords) in the test setni : Number of reference transliterationsfor i-th name in the test set (ni ?
1)ri,j : j-th reference transliteration for i-thname in the test setci,k : k-th candidate transliteration (systemoutput) for i-th name in the test set(1 ?
k ?
10)Ki : Number of candidate transliterationsproduced by a transliteration system3.1 Word Accuracy in Top-1 (ACC)Also known as Word Error Rate, it measures cor-rectness of the first transliteration candidate in thecandidate list produced by a transliteration system.ACC = 1 means that all top candidates are cor-rect transliterations i.e.
they match one of the ref-erences, and ACC = 0 means that none of the topcandidates are correct.ACC =1NN?i=1{1 if ?ri,j : ri,j = ci,1;0 otherwise}(1)3.2 Fuzziness in Top-1 (Mean F-score)The mean F-score measures how different, on av-erage, the top transliteration candidate is from itsclosest reference.
F-score for each source wordis a function of Precision and Recall and equals 1when the top candidate matches one of the refer-ences, and 0 when there are no common charactersbetween the candidate and any of the references.Precision and Recall are calculated based onthe length of the Longest Common Subsequence(LCS) between a candidate and a reference:LCS(c, r) =12(|c|+ |r| ?
ED(c, r)) (2)where ED is the edit distance and |x| is the lengthof x.
For example, the longest common subse-quence between ?abcd?
and ?afcde?
is ?acd?
andits length is 3.
The best matching reference, thatis, the reference for which the edit distance hasthe minimum, is taken for calculation.
If the bestmatching reference is given byri,m = argminj(ED(ci,1, ri,j)) (3)then Recall, Precision and F-score for i-th wordare calculated asRi =LCS(ci,1, ri,m)|ri,m|(4)Pi =LCS(ci,1, ri,m)|ci,1|(5)Fi = 2Ri ?
PiRi + Pi(6)?
The length is computed in distinct Unicodecharacters.?
No distinction is made on different charactertypes of a language (e.g., vowel vs. conso-nants vs. combining diereses?
etc.
)43.3 Mean Reciprocal Rank (MRR)Measures traditional MRR for any right answerproduced by the system, from among the candi-dates.
1/MRR tells approximately the averagerank of the correct transliteration.
MRR closer to 1implies that the correct answer is mostly producedclose to the top of the n-best lists.RRi ={minj 1j if ?ri,j , ci,k : ri,j = ci,k;0 otherwise}(7)MRR =1NN?i=1RRi (8)3.4 MAPrefMeasures tightly the precision in the n-best can-didates for i-th source name, for which referencetransliterations are available.
If all of the refer-ences are produced, then the MAP is 1.
Let?s de-note the number of correct candidates for the i-thsource word in k-best list as num(i, k).
MAPrefis then given byMAPref =1NN?i1ni(ni?k=1num(i, k))(9)3.5 MAP10MAP10 measures the precision in the 10-best can-didates for i-th source name provided by the can-didate system.
In general, the higher MAP10 is,the better is the quality of the transliteration sys-tem in capturing the multiple references.MAP10 =1NN?i=1110(10?k=1num(i, k))(10)3.6 MAPsysMAPsys measures the precision in the top Ki-bestcandidates produced by the system for i-th sourcename, for which ni reference transliterations areavailable.
This measure allows the systems to pro-duce variable number of transliterations, based ontheir confidence in identifying and producing cor-rect transliterations.MAPsys =1NN?i=11Ki(Ki?k=1num(i, k))(11)4 Participation in Shared TaskThere have been 31 systems from around theworld that participated in the shared task and sub-mitted the transliteration results for a common testdata, produced by their systems trained on thecommon training corpora.A few teams have participated in all or almostall tasks (that is, language pairs); most others par-ticipated in 3 tasks on average.
Each language pairhas attracted on average around 13 teams.
The par-ticipation details are shown in Table 3 and the de-mographics of the participating teams by countryis shown in Figure 1.?
?
?
?
?
?
?
?
?
?
??????????????????????????????????????????????
?0 1 2 3 4 5 6 7 8 9 10Figure 1: Participation by country.Teams are required to submit at least one stan-dard run for every task they participated in.
In total104 standard and 86 non-standard runs have beensubmitted.
Table 2 shows the number of standardand non-standard runs submitted for each task.
Itis clear that the most ?popular?
tasks are translit-eration from English to Hindi and from English toChinese, attempted by 21 and 18 participants re-spectively.
Overall, as can be noted from the re-sults, each task has received significant participa-tion.5 Task Results and Analysis5.1 Standard runsThe 8 individual plots in Figure 2 summarise (foreach task) the results of standard runs via 3 mea-sured metrics concerning output of at least onecorrect candidate per source word, namely, ac-curacy in top-1, F -score and Mean ReciprocalRank (MRR).
The plots in Figure 3 summarise (foreach task) the results for 3 metrics on ranked or-dered transliteration output of the systems, namelyMAPref , MAP10 and MAPsys metrics.
All theresults are presented numerically in Tables 8?11,for all evaluation metrics.
These are the official5Englishto HindiEnglishto TamilEnglishto Kan-nadaEnglishto Rus-sianEnglishto Chi-neseEnglishto Ko-reanEnglishtoJapaneseKatakanaJapanesetranslit-erated toJapaneseKanjiLanguage pair code EnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJkStandard runs 21 13 14 13 18 8 10 7Non-standard runs 18 5 5 16 20 9 5 8Table 2: Number of runs submitted for each task.
Number of participants coincides with the number ofstandard runs submitted.evaluation results published for this edition of thetransliteration shared task.
Note that two teamshave updated their results (after fixing bugs in theirsystems) after the deadline; their results are iden-tified specifically.We find that two approaches to transliterationare most popular in the shared task submissions.One of these approaches is Phrase-based statis-tical machine transliteration (Finch and Sumita,2008), an approach initially developed for ma-chine translation (Koehn et al, 2003).
Systemsthat adopted this approach are (Song, 2009; Haqueet al, 2009; Noeman, 2009; Rama and Gali, 2009;Chinnakotla and Damani, 2009).1 The other isConditional Random Fields(Lafferty et al, 2001)(CRF), adopted by (Aramaki and Abekawa, 2009;Shishtla et al, 2009).
With only a few exceptions,most implementations are based on approachesthat are language-independent.
Indeed, many ofthe participants fielded their systems on multiplelanguages, as can be seen from Table 3.We also note that combination of several differ-ent models via re-ranking of their outputs (CRF,Maximum Entropy Model, Margin Infused Re-laxed Algorithm) proves to be very successful (Ohet al, 2009); their system (reported as Team ID6) produced the best or second-best transliterationperformance consistently across all metrics, in alltasks, except Japanese back-transliteration.
Exam-ples of other model combinations are (Das et al,2009).At least two teams (reported as Team IDs 14and 27) incorporate language origin detection intheir system (Bose and Sarkar, 2009; Khapra andBhattacharyya, 2009).
The Indian language cor-pora contains names of both English and Indic ori-gin.
Khapra and Bhattacharyya (2009) demon-strate how much the transliteration performancecan be improved when language of origin detec-1To maintain anonymity, papers of the teams that submit-ted anonymous results are not cited in this report.tion is employed, followed by a language-specifictransliteration model for decoding.Some systems merit specific mention as theyadopt are rather unique approaches.
Jiampoja-marn et al (2009) propose DirectTL discrimina-tive sequence prediction model that is language-independent (reported as Team ID 7).
Theirtransliteration accuracy is among the highest inseveral tasks (EnCh, EnHi and EnRu).
Zelenko(2009) present an approach to the transliterationproblem based on Minimum Description Length(MDL) principle.
Freitag and Wang (2009) ap-proach the problem of transliteration with bidirec-tional perceptron edit models.Finally, in Figure 4 we present a plot whereeach point represents a standard run by a system,with different tasks marked with specific shapeand colour.
This plot gives a bird-eye-view ofthe system performances across two most uncorre-lated evaluation metrics, namely accuracy in top-1(ACC) and Mean F -score.
Not surprisingly, wenotice very high performance in terms of F -scorefor English to Russian transliteration task, likelybecause Russian orthography follows pronuncia-tion very closely, except for characters like softand hard signs that can hardly be recovered fromEnglish words.We also observe that Japanese back-transliteration has proven to be much harderthan other (forward-transliteration) tasks.
Ingeneral, we note that a well-performing translit-eration system performs well across all metrics.We are curious about the correlation betweendifferent metrics, and the results (specifically,the Spearman?s rank correlation coefficient) arepresented below:?
Accuracy in top-1 vs. F -score: 0.40?
Accuracy in top-1 vs. MRR: 0.97?
Accuracy in top-1 vs. MAPref : 0.9976?
Accuracy in top-1 vs. MAP10: 0.89?
Accuracy in top-1 vs. MAPsys: 0.80We find that F -score is the most uncorrelated met-ric: the Spearman?s rank correlation coefficientbetween F -score and accuracy in top-1 is 0.40 andbetween F -score and MRR it is 0.44.
This is likelybecause all metrics, except for F -score, are basedon word accuracy, while F -score is based on wordsimilarity allowing non-matching words to havescores well above 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Accuracy in top-100.10.20.30.40.50.60.70.80.91F-score English to ChineseEnglish to HindiEnglish to TamilEnglish to KannadaEnglish to RussianEnglish to KoreanEnglish to JapaneseJapanese transliterated to Japanese KanjiFigure 4: Accuracy in top-1 vs. F -score for dif-ferent tasks.5.2 Non-standard runsFor the non-standard runs there exist no restric-tions for the teams on the use of more data or otherlinguistic resources.
The purpose of non-standardruns is to see how accurate personal name translit-eration can be, for a given language pair.
The ap-proaches used in non-standard runs are typical andmay be summarised as follows:?
Dictionary lookup.?
Pronunciation dictionaries to convert wordsto their phonetic transcription.?
Additional corpora for training and dictio-nary lookup, such as LDC English-Chinesenamed entity list LDC2005T34 (LinguisticData Consortium, 2005).?
Web search, and in particular, Wikipediasearch.
First, transliteration candidates aregenerated.
Then a Web search is performedto see if any of the candidates appear in thesearch results.
Based on the results, the can-didates are re-ranked.The results are shown in Tables 16?19.
For En-glish to Chinese and English to Russian transliter-ation tasks the accuracy in top-1 can go as high as0.909 and 0.955 respectively when Web search isused to aid transliteration.5.3 Post-evaluationTwo participants have found a bug in their systemimplementation and re-evaluated the results afterthe deadline.
Their results are marked specificallyin Tables 4?8 and 16.6 Process Analysis and Fine-tuningIn this section we highlight some of the sugges-tions and feedback that we have received from theparticipants during the course of this shared task.While a few of them have been implemented in thecurrent edition, many of these may be consideredin the future editions of the shared task.More or different languages There is quite abit of interest in enhancing the list of languagepairs short-listed.
While we are constrained (inthis edition) due to the availability of manuallyverified data, certainly more languages will be in-cluded in the future editions, as some specific datahave already been promised for future editions.Bidirectional transliteration Many partic-ipants express interest in transliterations intoEnglish; and this reflexive task will be added inthe future editions.
We believe it will encouragemore participation as it will be easy to read andverify system output in English for those teamsnot familiar with the non-English side of thelanguage.Forward vs. backward transliteration Thereis quite a bit of interest expressed in specificallyseparating forward and backward transliterationtasks.
However, such separation requires specificcorpora with known origin for each name pair, andclearly we are constrained by the availability ofcorpora.
When corpora is available, the task maybe designated explicitly in future editions.Number of standard runs The number of stan-dard runs that may be submitted may be increasedin the future editions, as many participants wouldlike to submit many standard runs, trained withdifferent parameters.7Errors in training and development corporaWhile we have taken all precautions in acquiringand creating the corpora, some errors still remain.We thank those who have sent us the errata.
How-ever, since the affected part is less than 0.5% ofthe data, we believe that the effect on final resultsis minimal.
The errata will be made available toall participants.7 Conclusions and Future PlansWe are pleased to report a comprehensive cal-ibration and baselining of machine translitera-tion apporaches as most state-of-the-art machinetransliteration techniques are represented in theshared task.
The most popular techniques such asPhrase-Based Machine Transliteration (Koehn etal., 2003), and Conditional Random Fields (Laf-ferty et al, 2001) are inspired by recent progress inmachine translation.
As the standard runs are lim-ited by the use of corpus, most of the systems areimplemented under the direct orthographic map-ping (DOM) framework (Li et al, 2004).
Whilethe standard runs allow us to conduct meaning-ful comparison across different algorithms, werecognise that the non-standard runs open up moreopportunities for exploiting larger linguistic cor-pora.
It is also noted that several systems have re-ported improved performance over any previouslyreported results on similar corpora.NEWS 2009 Shared Task represents a suc-cessful debut of a community effort in drivingmachine transliteration techniques forward.
Theoverwhelming responses in the first shared taskalso warrant continuation of such an effort in fu-ture ACL or IJCNLP events.AcknowledgementsThe organisers of the NEWS 2009 Shared Taskwould like to thank the Institute for Infocomm Re-search (Singapore), Microsoft Research India andCJK Institute (Japan) for providing the corporaand technical support.
Without those, the SharedTask would not be possible.
We thank those par-ticipants who identified errors in the data and sentus the errata.
We want to thank Monojit Choud-hury for his contribution to metrics defined for theshared task.
We also want to thank the membersof programme committee for their invaluable com-ments that improve the quality of the shared taskpapers.
Finally, we wish to thank all the partici-pants for their active participation that have madethis first machine transliteration shared task a com-prehensive one.8ReferencesEiji Aramaki and Takeshi Abekawa.
2009.
Fast de-coding and easy implementation: Transliteration asa sequential labeling.
In Proc.
ACL/IJCNLP NamedEntities Workshop Shared Task.Dipankar Bose and Sudeshna Sarkar.
2009.
Learn-ing multi character alignment rules and classifica-tion of training data for transliteration.
In Proc.ACL/IJCNLP Named Entities Workshop SharedTask.Manoj Kumar Chinnakotla and Om P. Damani.
2009.Experiences with English-Hindi, English-Tamil andEnglish-Kannada transliteration tasks at NEWS2009.
In Proc.
ACL/IJCNLP Named Entities Work-shop Shared Task.CJKI.
2009.
CJK Institute.
http://www.cjk.org/.Amitava Das, Asif Ekbal, Tapabrata Mondal, andSivaji Bandyopadhyay.
2009.
English to Hindimachine transliteration system at NEWS 2009.In Proc.
ACL/IJCNLP Named Entities WorkshopShared Task.D.
Demner-Fushman and D. W. Oard.
2002.
The ef-fect of bilingual term list size on dictionary-basedcross-language information retrieval.
In Proc.
36-thHawaii Int?l.
Conf.
System Sciences, volume 4, page108.2.Andrew Finch and Eiichiro Sumita.
2008.
Phrase-based machine transliteration.
In Proc.
3rd Int?l.Joint Conf NLP, volume 1, Hyderabad, India, Jan-uary.Dayne Freitag and Zhiqiang Wang.
2009.
Nametransliteration with bidirectional perceptron editmodels.
In Proc.
ACL/IJCNLP Named EntitiesWorkshop Shared Task.Yoav Goldberg and Michael Elhadad.
2008.
Identifica-tion of transliterated foreign words in Hebrew script.In Proc.
CICLing, volume LNCS 4919, pages 466?477.Dan Goldwasser and Dan Roth.
2008.
Translitera-tion as constrained optimization.
In Proc.
EMNLP,pages 353?362.Jack Halpern.
2007.
The challenges and pitfallsof Arabic romanization and arabization.
In Proc.Workshop on Comp.
Approaches to Arabic Script-based Lang.Rejwanul Haque, Sandipan Dandapat, Ankit KumarSrivastava, Sudip Kumar Naskar, and Andy Way.2009.
English-Hindi transliteration using context-informed PB-SMT.
In Proc.
ACL/IJCNLP NamedEntities Workshop Shared Task.Ulf Hermjakob, Kevin Knight, and Hal Daume?.
2008.Name translation in statistical machine translation:Learning when to transliterate.
In Proc.
ACL,Columbus, OH, USA, June.Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,Kenneth Dwyer, and Grzegorz Kondrak.
2009.
Di-recTL: a language-independent approach to translit-eration.
In Proc.
ACL/IJCNLP Named EntitiesWorkshop Shared Task.Mitesh Khapra and Pushpak Bhattacharyya.
2009.
Im-proving transliteration accuracy using word-origindetection and lexicon lookup.
In Proc.
ACL/IJCNLPNamed Entities Workshop Shared Task.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discov-ery from multilingual comparable corpora.
In Proc.21st Int?l Conf Computational Linguistics and 44thAnnual Meeting of ACL, pages 817?824, Sydney,Australia, July.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4).P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
HLT-NAACL.A Kumaran and T. Kellner.
2007.
A generic frame-work for machine transliteration.
In Proc.
SIGIR,pages 721?722.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
Int?l.Conf.
Machine Learning, pages 282?289.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.In Proc.
42nd ACL Annual Meeting, pages 159?166,Barcelona, Spain.Linguistic Data Consortium.
2005.
LDC Chinese-English name entity lists LDC2005T34.T.
Mandl and C. Womser-Hacker.
2005.
The effect ofnamed entities on effectiveness in cross-language in-formation retrieval evaluation.
In Proc.
ACM Symp.Applied Comp., pages 1059?1064.MSRI.
2009.
Microsoft Research India.http://research.microsoft.com/india.Sara Noeman.
2009.
Language independent translit-eration system using phrase based SMT approachon substring.
In Proc.
ACL/IJCNLP Named EntitiesWorkshop Shared Task.Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-sawa.
2009.
Machine transliteration with target-language grapheme and phoneme: Multi-enginetransliteration approach.
In Proc.
ACL/IJCNLPNamed Entities Workshop Shared Task.Taraka Rama and Karthik Gali.
2009.
Modeling ma-chine transliteration as a phrase based statistical ma-chine translation problem.
In Proc.
ACL/IJCNLPNamed Entities Workshop Shared Task.9Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In Proc.
45th Annual Meetingof the ACL, pages 944?951, Prague, Czech Repub-lic, June.Praneeth Shishtla, V Surya Ganesh, S Sethurama-lingam, and Vasudeva Varma.
2009.
A language-independent transliteration schema using characteraligned models.
In Proc.
ACL/IJCNLP Named Enti-ties Workshop Shared Task.Yan Song.
2009.
Name entities transliteration viaimproved statistical translation on character-levelchunks.
In Proc.
ACL/IJCNLP Named EntitiesWorkshop Shared Task.Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006.Named entity transliteration with comparable cor-pora.
In Proc.
21st Int?l Conf Computational Lin-guistics and 44th Annual Meeting of ACL, pages 73?80, Sydney, Australia.Raghavendra Udupa, K. Saravanan, Anton Bakalov,and Abhijit Bhole.
2009.
?They are out there, ifyou know where to look?
: Mining transliterationsof OOV query terms for cross-language informa-tion retrieval.
In LNCS: Advances in InformationRetrieval, volume 5478, pages 437?448.
SpringerBerlin / Heidelberg.Dmitry Zelenko and Chinatsu Aone.
2006.
Discrimi-native methods for transliteration.
In Proc.
EMNLP,pages 612?617, Sydney, Australia, July.Dmitry Zelenko.
2009.
Combining MDL translitera-tion training with discriminative modeling.
In Proc.ACL/IJCNLP Named Entities Workshop SharedTask.10Team ID Organisation English toHindiEnglish toTamilEnglish toKannadaEnglish toRussianEnglish toChineseEnglishto Ko-reanEnglishtoJapaneseKatakanaJapanesetranslit-erated toJapaneseKanjiEnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJk1 IIT Bombay x x x2 Institution of ComputationalLinguistics Peking Univer-sityx3 University of Tokyo x x x x x x x4?
University of Illinois,Urbana-Champaignx x5 IIT Bombay x x6 NICT x x x x x x x x7 University of Alberta x x x x x x8 x x x x x x x x9 x x x x x x x x10 Johns Hopkins University x x x x x11 x x x12 x x13 Jadavpur University x14 IIIT Hyderabad x15 x x x16?
ARL-CACI x17 x x x x x x x x18 x19?
Chaoyang University ofTechnologyx20 Pondicherry University x x x21 Microsoft Research x x22 SRI International x x x x x23 IBM Cairo TDC x x24 SRA x x x x x x x x25 IIT Kharagpur x x x26 Institute of Software ChineseAcademy of Sciencesx27 x28 George Washington Univer-sityx29?
x30 Dublin City University x31 IIIT x x x x xTable 3: Participation of teams in different tasks.
?Participants without a system paper.11?????????????????????
?
??
??
?
?
??
??
??
??
?
??
??
??
??
??
?
?
??
??
??????????
?Site?ID(a) English to Hindi?????????????????????
??
??
?
??
??
?
?
??
?
??
?
??
??????????
?Site?ID(b) English to Kannada?????????????????????
??
??
??
?
??
??
?
?
??
?
??
??????????
?Site?ID(c) English to Tamil?????????
????????????
?
??
??
?
??
??
?
??
?
?
??
??????????
?Site?ID(d) English to Russian?????????
????????????
?
??
?
?
??
?
??
??
?
?
??
??
??
??
??
??
??????????
?Site?ID(e) English to Chinese???????????????????
?
??
??
?
?
?
?????????
?Site?ID(f) English to Korean?????????????????????
??
??
?
??
?
?
??
??
?????????
?Site?ID(g) English to Japanese Katakana?????????????????????
??
?
?
?
?
??????????
?Site?ID(h) Japanese transliterated to Japanese KanjiFigure 2: Accuracy in top-1, F -score and MRR for standard runs.12??????????????
?
??
??
?
?
??
??
??
??
?
??
??
??
??
??
?
?
??
??
???????????????
?Site?ID(a) English to Hindi??????????????
??
??
?
??
??
?
?
??
?
??
?
??
???????????????
?Site?ID(b) English to Kannada???????????????
??
??
??
?
??
??
?
?
??
?
??
???????????????
?Site?ID(c) English to Tamil??????????????
?
??
??
?
??
??
?
??
?
?
??
???????????????
?Site?ID(d) English to Russian??????????????????
?
??
?
?
??
?
??
??
?
?
??
??
??
??
??
??
???????????????
?Site?ID(e) English to Chinese???????????????
?
??
??
?
?
?
??????????????
?Site?ID(f) English to Korean??????????????
??
??
?
??
?
?
??
??
??????????????
?Site?ID(g) English to Japanese Katakana???????????????
??
?
?
?
?
???????????????
?Site?ID(h) Japanese transliterated to Japanese KanjiFigure 3: MAPref , MAP10 and MAPsys scores for standard runs.13Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation7 0.498 0.890 0.603 0.488 0.195 0.195 University of Alberta6 0.483 0.892 0.607 0.477 0.202 0.202 NICT13 0.471 0.861 0.519 0.463 0.162 0.383 Jadavpur University14 0.463 0.876 0.573 0.454 0.201 0.201 IIIT Hyderabad8 0.462 0.876 0.576 0.454 0.189 0.1891 0.423 0.863 0.544 0.417 0.179 0.202 IIT Bombay11 0.418 0.879 0.546 0.412 0.183 0.24021 0.418 0.864 0.522 0.409 0.170 0.170 Microsoft Research17 0.415 0.858 0.505 0.406 0.164 0.16824 0.409 0.864 0.527 0.402 0.174 0.176 SRA5 0.409 0.881 0.546 0.400 0.184 0.184 IIT Bombay31 0.407 0.877 0.544 0.402 0.195 0.195 IIIT16 0.406 0.863 0.514 0.397 0.170 0.280 ARL-CACI30 0.399 0.863 0.488 0.392 0.157 0.157 Dublin City University10 0.398 0.855 0.515 0.389 0.170 0.170 Johns Hopkins University25 0.366 0.854 0.493 0.360 0.164 0.164 IIT Kharagpur3 0.363 0.864 0.503 0.360 0.170 0.170 University of Tokyo9 0.349 0.829 0.455 0.341 0.151 0.15122 0.212 0.788 0.317 0.207 0.106 0.106 SRI International29 0.053 0.664 0.089 0.053 0.037 0.03720 0.004 0.012 0.004 0.004 0.001 0.004 Pondicherry University21 0.466 0.881 0.567 0.457 0.183 0.183 Microsoft Research (post-evaluation)22 0.465 0.886 0.567 0.458 0.185 0.185 SRI International (post-evaluation)Table 4: Standard runs for English to Hindi task.TeamID ACC F -score MRR MAPref MAP10 MAPsys Organisation6 0.474 0.910 0.608 0.465 0.204 0.204 NICT17 0.436 0.894 0.551 0.427 0.184 0.18911 0.435 0.902 0.572 0.430 0.195 0.26531 0.406 0.894 0.542 0.399 0.193 0.193 IIIT1 0.405 0.892 0.542 0.397 0.181 0.184 IIT Bombay25 0.404 0.883 0.539 0.398 0.182 0.182 IIT Kharagpur24 0.374 0.880 0.512 0.369 0.174 0.174 SRA3 0.365 0.884 0.504 0.360 0.172 0.172 University of Tokyo8 0.361 0.883 0.510 0.354 0.174 0.17410 0.327 0.870 0.458 0.317 0.156 0.156 Johns Hopkins University9 0.316 0.848 0.451 0.307 0.154 0.15422 0.141 0.760 0.256 0.139 0.090 0.090 SRI International20 0.061 0.131 0.068 0.059 0.021 0.056 Pondicherry University22 0.475 0.909 0.581 0.466 0.193 0.193 SRI International (post-evaluation)Table 5: Standard runs for English to Tamil task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation6 0.398 0.880 0.526 0.391 0.178 0.178 NICT17 0.370 0.867 0.499 0.362 0.170 0.17511 0.363 0.870 0.482 0.355 0.164 0.2181 0.360 0.861 0.479 0.351 0.161 0.164 IIT Bombay31 0.350 0.864 0.482 0.344 0.175 0.175 IIIT24 0.345 0.854 0.462 0.336 0.157 0.157 SRA8 0.343 0.855 0.458 0.334 0.155 0.1555 0.335 0.859 0.453 0.327 0.154 0.154 IIT Bombay25 0.335 0.856 0.457 0.328 0.154 0.154 IIT Kharagpur3 0.324 0.856 0.438 0.315 0.148 0.148 University of Tokyo10 0.235 0.817 0.353 0.229 0.121 0.121 Johns Hopkins University9 0.177 0.799 0.307 0.178 0.109 0.10922 0.091 0.735 0.180 0.090 0.064 0.064 SRI International20 0.004 0.009 0.004 0.004 0.001 0.004 Pondicherry University22 0.396 0.874 0.494 0.385 0.161 0.161 SRI International (post-evaluation)Table 6: Standard runs for English to Kannada task.14Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation7 0.613 0.928 0.696 0.613 0.212 0.212 University of Alberta6 0.605 0.926 0.701 0.605 0.215 0.215 NICT17 0.597 0.925 0.691 0.597 0.212 0.25524 0.566 0.919 0.662 0.566 0.203 0.216 SRA8 0.564 0.917 0.677 0.564 0.210 0.21031 0.548 0.916 0.640 0.548 0.210 0.210 IIIT23 0.545 0.917 0.596 0.545 0.286 0.299 IBM Cairo TDC3 0.531 0.912 0.635 0.531 0.219 0.219 University of Tokyo10 0.506 0.901 0.609 0.506 0.204 0.204 Johns Hopkins University4 0.504 0.909 0.618 0.504 0.193 0.193 University of Illinois, Urbana-Champaign9 0.500 0.906 0.613 0.500 0.192 0.19222 0.364 0.876 0.440 0.364 0.136 0.136 SRI International27 0.354 0.869 0.394 0.354 0.134 0.13422 0.609 0.928 0.686 0.609 0.209 0.209 SRI International (post-evaluation)Table 7: Standard runs for English to Russian task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation6 0.731 0.895 0.812 0.731 0.246 0.246 NICT7 0.717 0.890 0.785 0.717 0.237 0.237 University of Alberta15 0.713 0.883 0.794 0.713 0.241 0.2418 0.666 0.864 0.765 0.666 0.234 0.2342 0.652 0.858 0.755 0.652 0.232 0.232 Institution of Computational Linguistics PekingUniversity China17 0.646 0.867 0.747 0.646 0.229 0.2299 0.643 0.854 0.745 0.643 0.228 0.22918 0.621 0.852 0.718 0.621 0.220 0.22224 0.619 0.847 0.711 0.619 0.217 0.217 SRA4 0.607 0.840 0.695 0.607 0.213 0.213 University of Illinois, Urbana-Champaign3 0.580 0.826 0.653 0.580 0.199 0.199 University of Tokyo26 0.498 0.786 0.603 0.498 0.187 0.189 Institute of Software Chinese Academy of Sci-ences31 0.493 0.804 0.600 0.493 0.192 0.192 IIIT22 0.468 0.768 0.546 0.468 0.168 0.168 SRI International28 0.456 0.763 0.587 0.456 0.185 0.185 George Washington University10 0.450 0.755 0.514 0.450 0.157 0.166 Johns Hopkins University23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC19 0.199 0.606 0.229 0.199 0.070 0.070 Chaoyang University of Technology22 0.671 0.872 0.725 0.672 0.218 0.218 SRI International (post-evaluation)Table 8: Standard runs for English to Chinese task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation17 0.476 0.742 0.596 0.477 0.187 0.1996 0.473 0.740 0.584 0.473 0.182 0.182 NICT12 0.451 0.720 0.576 0.451 0.181 0.18124 0.413 0.702 0.524 0.412 0.165 0.165 SRA7 0.387 0.693 0.469 0.387 0.146 0.146 University of Alberta8 0.362 0.662 0.460 0.362 0.144 0.1449 0.332 0.648 0.425 0.331 0.134 0.1353 0.170 0.512 0.218 0.170 0.069 0.069 University of TokyoTable 9: Standard runs for English to Korean task.15Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation6 0.537 0.858 0.657 0.529 0.223 0.223 NICT15 0.510 0.838 0.624 0.498 0.209 0.20917 0.503 0.843 0.627 0.491 0.212 0.2127 0.500 0.847 0.604 0.487 0.199 0.199 University of Alberta21 0.465 0.827 0.559 0.454 0.183 0.183 Microsoft Research3 0.457 0.828 0.576 0.445 0.194 0.194 University of Tokyo8 0.449 0.816 0.571 0.436 0.192 0.19224 0.420 0.807 0.541 0.410 0.182 0.184 SRA12 0.408 0.808 0.537 0.398 0.182 0.1829 0.406 0.800 0.529 0.393 0.180 0.18021 0.469 0.834 0.567 0.454 0.186 0.186 Microsoft Research (post-evaluation)Table 10: Standard runs for English to Japanese Katakana task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation15 0.627 0.763 0.706 0.605 0.292 0.29217 0.606 0.749 0.695 0.586 0.287 0.2888 0.596 0.741 0.687 0.575 0.282 0.2827 0.560 0.730 0.644 0.525 0.244 0.244 University of Alberta9 0.555 0.708 0.653 0.538 0.261 0.2616 0.532 0.716 0.583 0.485 0.214 0.218 NICT24 0.509 0.675 0.600 0.491 0.226 0.226 SRATable 11: Standard runs for Japanese Transliterated to Japanese Kanji task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation7 0.509 0.893 0.610 0.498 0.198 0.198 University of Alberta1 0.487 0.873 0.594 0.481 0.195 0.229 IIT Bombay6 0.475 0.893 0.601 0.469 0.200 0.200 NICT6 0.469 0.884 0.581 0.464 0.192 0.193 NICT6 0.455 0.888 0.575 0.448 0.191 0.191 NICT5 0.448 0.885 0.570 0.439 0.190 0.190 IIT Bombay6 0.443 0.879 0.555 0.437 0.184 0.191 NICT17 0.424 0.862 0.513 0.415 0.166 0.17430 0.421 0.864 0.519 0.415 0.171 0.171 Dublin City University30 0.420 0.867 0.519 0.413 0.170 0.170 Dublin City University30 0.419 0.868 0.464 0.419 0.338 0.338 Dublin City University16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI30 0.407 0.856 0.507 0.399 0.168 0.168 Dublin City University16 0.400 0.864 0.516 0.391 0.171 0.212 ARL-CACI13 0.389 0.831 0.487 0.385 0.160 0.328 Jadavpur University13 0.384 0.828 0.485 0.380 0.160 0.325 Jadavpur University16 0.273 0.796 0.358 0.266 0.119 0.193 ARL-CACITable 12: Non-standard runs for English to Hindi task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation6 0.478 0.910 0.606 0.472 0.203 0.203 NICT6 0.459 0.906 0.583 0.453 0.195 0.196 NICT6 0.459 0.906 0.583 0.453 0.195 0.196 NICT6 0.453 0.907 0.584 0.446 0.196 0.196 NICT17 0.437 0.894 0.555 0.426 0.185 0.193Table 13: Non-standard runs for English to Tamil task.16Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation6 0.399 0.881 0.522 0.391 0.176 0.176 NICT6 0.386 0.877 0.503 0.379 0.169 0.169 NICT6 0.380 0.869 0.488 0.370 0.163 0.163 NICT17 0.374 0.868 0.502 0.366 0.170 0.1766 0.373 0.869 0.485 0.362 0.162 0.168 NICTTable 14: Non-standard runs for English to Kannada task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation17 0.955 0.989 0.966 0.955 0.284 0.50417 0.609 0.928 0.701 0.609 0.214 0.2637 0.608 0.927 0.694 0.608 0.212 0.212 University of Alberta7 0.607 0.927 0.690 0.607 0.211 0.211 University of Alberta6 0.600 0.927 0.634 0.600 0.189 0.189 NICT6 0.600 0.926 0.699 0.600 0.214 0.214 NICT7 0.591 0.928 0.679 0.591 0.208 0.208 University of Alberta6 0.561 0.918 0.595 0.561 0.178 0.182 NICT6 0.557 0.920 0.596 0.557 0.179 0.233 NICT23 0.545 0.917 0.618 0.545 0.188 0.206 IBM Cairo TDC23 0.524 0.913 0.602 0.524 0.184 0.203 IBM Cairo TDC23 0.524 0.913 0.579 0.524 0.277 0.291 IBM Cairo TDC4 0.496 0.908 0.613 0.496 0.191 0.191 University of Illinois, Urbana-Champaign27 0.338 0.872 0.408 0.338 0.128 0.12827 0.293 0.845 0.325 0.293 0.099 0.09927 0.162 0.849 0.298 0.162 0.188 0.188Table 15: Non-standard runs for English to Russian task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation17 0.909 0.960 0.933 0.909 0.276 0.2767 0.746 0.900 0.814 0.746 0.245 0.245 University of Alberta7 0.734 0.895 0.807 0.734 0.244 0.244 University of Alberta7 0.732 0.895 0.803 0.732 0.242 0.242 University of Alberta6 0.731 0.894 0.812 0.731 0.246 0.246 NICT6 0.715 0.890 0.741 0.715 0.220 0.231 NICT6 0.699 0.884 0.729 0.699 0.216 0.232 NICT6 0.684 0.873 0.711 0.684 0.211 0.211 NICT22 0.663 0.867 0.754 0.663 0.230 0.230 SRI International17 0.658 0.865 0.752 0.658 0.230 0.23018 0.587 0.834 0.665 0.587 0.203 0.33026 0.500 0.786 0.607 0.500 0.189 0.191 Institute of Software Chinese Academy of Sciences22 0.487 0.787 0.622 0.487 0.196 0.196 SRI International28 0.462 0.764 0.564 0.462 0.175 0.175 George Washington University28 0.458 0.763 0.602 0.458 0.191 0.191 George Washington University23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC19 0.279 0.668 0.351 0.279 0.110 0.110 Chaoyang University of Technology28 0.058 0.353 0.269 0.058 0.101 0.101 George Washington University28 0.050 0.359 0.260 0.050 0.098 0.098 George Washington University4 0.001 0.249 0.001 0.001 0.000 0.000 University of Illinois, Urbana-Champaign22 0.674 0.873 0.763 0.674 0.232 0.232 SRI International (post-evaluation)22 0.500 0.793 0.636 0.500 0.200 0.200 SRI International (post-evaluation)Table 16: Non-standard runs for English to Chinese task.17Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation17 0.794 0.894 0.836 0.793 0.249 0.32312 0.785 0.887 0.840 0.785 0.252 0.44112 0.784 0.889 0.840 0.784 0.252 0.48412 0.781 0.885 0.839 0.781 0.252 0.46012 0.740 0.868 0.806 0.740 0.243 0.2436 0.461 0.737 0.576 0.461 0.180 0.180 NICT6 0.457 0.734 0.506 0.457 0.153 0.153 NICT6 0.447 0.718 0.493 0.447 0.149 0.149 NICT6 0.369 0.679 0.406 0.369 0.123 0.123 NICTTable 17: Non-standard runs for English to Korean task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation6 0.535 0.858 0.656 0.526 0.222 0.222 NICT6 0.517 0.850 0.567 0.495 0.177 0.188 NICT6 0.513 0.854 0.567 0.495 0.178 0.178 NICT7 0.510 0.848 0.614 0.496 0.202 0.202 University of Alberta6 0.500 0.842 0.547 0.480 0.170 0.196 NICTTable 18: Non-standard runs for English to Japanese Katakana task.Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation17 0.717 0.818 0.784 0.691 0.319 0.31917 0.703 0.805 0.768 0.673 0.311 0.31117 0.698 0.805 0.774 0.676 0.317 0.31717 0.681 0.790 0.755 0.657 0.308 0.3096 0.525 0.713 0.607 0.503 0.248 0.249 NICT6 0.525 0.712 0.606 0.502 0.248 0.248 NICT6 0.523 0.712 0.572 0.479 0.211 0.213 NICT6 0.517 0.705 0.603 0.496 0.248 0.249 NICTTable 19: Non-standard runs for Japanese Transliterated to Japanese Kanji task.18
