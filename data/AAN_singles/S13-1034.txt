Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 234?240, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsCNGL-CORE: Referential Translation Machinesfor Measuring Semantic SimilarityErgun Bic?iciCentre for Next Generation Localisation,Dublin City University, Dublin, Ireland.ebicici@computing.dcu.ieJosef van GenabithCentre for Next Generation Localisation,Dublin City University, Dublin, Ireland.josef@computing.dcu.ieAbstractWe invent referential translation machines(RTMs), a computational model for identify-ing the translation acts between any two datasets with respect to a reference corpus selectedin the same domain, which can be used forjudging the semantic similarity between text.RTMs make quality and semantic similarityjudgments possible by using retrieved rele-vant training data as interpretants for reach-ing shared semantics.
An MTPP (machinetranslation performance predictor) model de-rives features measuring the closeness of thetest sentences to the training data, the diffi-culty of translating them, and the presence ofacts of translation involved.
We view seman-tic similarity as paraphrasing between any twogiven texts.
Each view is modeled by an RTMmodel, giving us a new perspective on the bi-nary relationship between the two.
Our pre-diction model is the 15th on some tasks and30th overall out of 89 submissions in total ac-cording to the official results of the SemanticTextual Similarity (STS 2013) challenge.1 Semantic Textual Similarity JudgmentsWe introduce a fully automated judge for semanticsimilarity that performs well in the semantic textualsimilarity (STS) task (Agirre et al 2013).
STS isa degree of semantic equivalence between two textsbased on the observations that ?vehicle?
and ?car?are more similar than ?wave?
and ?car?.
Accurateprediction of STS has a wide application area in-cluding: identifying whether two tweets are talk-ing about the same thing, whether an answer is cor-rect by comparing it with a reference answer, andwhether a given shorter text is a valid summary ofanother text.The translation quality estimation task (Callison-Burch et al 2012) aims to develop quality indicatorsfor translations at the sentence-level and predictorswithout access to a reference translation.
Bicici etal.
(2013) develop a top performing machine transla-tion performance predictor (MTPP), which uses ma-chine learning models over features measuring howwell the test set matches the training set relying onextrinsic and language independent features.The semantic textual similarity (STS) task (Agirreet al 2013) addresses the following problem.
Giventwo sentences S1 and S2 in the same language, quan-tify the degree of similarity with a similarity score,which is a number in the range [0, 5].
The semantictextual similarity prediction problem involves find-ing a function f approximating the semantic textualsimilarity score given two sentences, S1 and S2:f(S1, S2) ?
q(S1, S2).
(1)We approach f as a supervised learning problemwith (S1, S2, q(S1, S2)) tuples being the trainingdata and q(S1, S2) being the target similarity score.We model the problem as a translation task whereone possible interpretation is obtained by translat-ing S1 (the source to translate, S) to S2 (the targettranslation, T).
Since linguistic processing can re-veal deeper similarity relationships, we also look atthe translation task at different granularities of infor-mation: plain text (R for regular) , after lemmatiza-tion (L), after part-of-speech (POS) tagging (P), andafter removing 128 English stop-words (S) 1.
Thus,1http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/234we obtain 4 different perspectives on the binary re-lationship between S1 and S2.2 Referential Translation Machine (RTM)Referential translation machines (RTMs) we de-velop provide a computational model for quality andsemantic similarity judgments using retrieval of rel-evant training data (Bic?ici and Yuret, 2011a; Bic?ici,2011) as interpretants for reaching shared seman-tics (Bic?ici, 2008).
We show that RTM achieves verygood performance in judging the semantic similarityof sentences and we can also use RTM to automat-ically assess the correctness of student answers toobtain better results (Bic?ici and van Genabith, 2013)than the state-of-the-art (Dzikovska et al 2012).RTM is a computational model for identifyingthe acts of translation for translating between anygiven two data sets with respect to a reference cor-pus selected in the same domain.
RTM can be usedfor automatically judging the semantic similarity be-tween texts.
An RTM model is based on the selec-tion of common training data relevant and close toboth the training set and the test set where the se-lected relevant set of instances are called the inter-pretants.
Interpretants allow shared semantics to bepossible by behaving as a reference point for simi-larity judgments and providing the context.
In semi-otics, an interpretant I interprets the signs used torefer to the real objects (Bic?ici, 2008).
RTMs pro-vide a model for computational semantics using in-terpretants as a reference according to which seman-tic judgments with translation acts are made.
EachRTM model is a data translation model between theinstances in the training set and the test set.
We usethe FDA (Feature Decay Algorithms) instance se-lection model for selecting the interpretants (Bic?iciand Yuret, 2011a) from a given corpus, which canbe monolingual when modeling paraphrasing acts,in which case the MTPP model (Section 2.1) is builtusing the interpretants themselves as both the sourceand the target side of the parallel corpus.
RTMs mapthe training and test data to a space where translationacts can be identified.
We view that acts of transla-tion are ubiquitously used during communication:Every act of communication is an act oftranslation (Bliss, 2012).src/backend/snowball/stopwords/Translation need not be between different languagesand paraphrasing or communication also containacts of translation.
When creating sentences, we useour background knowledge and translate informa-tion content according to the current context.Given a training set train, a test set test, andsome monolingual corpus C, preferably in the samedomain as the training and test sets, the RTM stepsare:1.
T = train ?
test.2.
select(T, C)?
I3.
MTPP(I,train)?
Ftrain4.
MTPP(I,test)?
Ftest5.
learn(M,Ftrain)?M6.
predict(M,Ftest)?
q?Step 2 selects the interpretants, I, relevant to theinstances in the combined training and test data.Steps 3, 4 use I to map train and test to a newspace where similarities between translation acts canbe derived more easily.
Step 5 trains a learningmodel M over the training features, Ftrain, andStep 6 obtains the predictions.
RTM relies on therepresentativeness of I as a medium for buildingtranslation models for translating between trainand test.Our encouraging results in the STS task providesa greater understanding of the acts of translation weubiquitously use when communicating and how theycan be used to predict the performance of transla-tion, judging the semantic similarity between text,and evaluating the quality of student answers.
RTMand MTPP models are not data or language specificand their modeling power and good performance areapplicable across different domains and tasks.
RTMexpands the applicability of MTPP by making it fea-sible when making monolingual quality and simi-larity judgments and it enhances the computationalscalability by building models over smaller but morerelevant training data as interpretants.2.1 The Machine Translation PerformancePredictor (MTPP)In machine translation (MT), pairs of source and tar-get sentences are used for training statistical MT(SMT) models.
SMT system performance is af-fected by the amount of training data used as well235as the closeness of the test set to the training set.MTPP (Bic?ici et al 2013) is a top performing ma-chine translation performance predictor, which usesmachine learning models over features measuringhow well the test set matches the training set to pre-dict the quality of a translation without using a ref-erence translation.
MTPP measures the coverage ofindividual test sentence features and syntactic struc-tures found in the training set and derives featurefunctions measuring the closeness of test sentencesto the available training data, the difficulty of trans-lating the sentence, and the presence of acts of trans-lation for data transformation.2.2 MTPP Features for Translation ActsMTPP uses n-gram features defined over text orcommon cover link (CCL) (Seginer, 2007) struc-tures as the basic units of information over whichsimilarity calculations are made.
Unsupervisedparsing with CCL extracts links from base wordsto head words, which allow us to obtain structuresrepresenting the grammatical information instanti-ated in the training and test data.
Feature functionsuse statistics involving the training set and the testsentences to determine their closeness.
Since theyare language independent, MTPP allows quality es-timation to be performed extrinsically.
Categoriesfor the 289 features used are listed below and theirdetailed descriptions are presented in (Bic?ici et al2013) where the number of features are given in {#}.?
Coverage {110}: Measures the degree towhich the test features are found in the train-ing set for both S ({56}) and T ({54}).?
Synthetic Translation Performance {6}: Calcu-lates translation scores achievable according tothe n-gram coverage.?
Length {4}: Calculates the number of wordsand characters for S and T and their ratios.?
Feature Vector Similarity {16}: Calculates thesimilarities between vector representations.?
Perplexity {90}: Measures the fluency of thesentences according to language models (LM).We use both forward ({30}) and backward({15}) LM based features for S and T.?
Entropy {4}: Calculates the distributional sim-ilarity of test sentences to the training set.?
Retrieval Closeness {24}: Measures the de-gree to which sentences close to the test set arefound in the training set.?
Diversity {6}: Measures the diversity of co-occurring features in the training set.?
IBM1 Translation Probability {16}: Calculatesthe translation probability of test sentences us-ing the training set (Brown et al 1993).?
Minimum Bayes Retrieval Risk {4}: Calculatesthe translation probability for the translationhaving the minimum Bayes risk among the re-trieved training instances.?
Sentence Translation Performance {3}: Calcu-lates translation scores obtained according toq(T,R) using BLEU (Papineni et al 2002),NIST (Doddington, 2002), or F1 (Bic?ici andYuret, 2011b) for q.?
Character n-grams {4}: Calculates the cosinebetween the character n-grams (for n=2,3,4,5)obtained for S and T (Ba?r et al 2012).?
LIX {2}: Calculates the LIX readabilityscore (Wikipedia, 2013; Bjo?rnsson, 1968) forS and T. 23 ExperimentsSTS contains sentence pairs from news headlines(headlines), sense definitions from semantic lexicalresources (OnWN is from OntoNotes (Pradhan etal., 2007) and WordNet (Miller, 1995) and FNWN isfrom FrameNet (Baker et al 1998) and WordNet),and statistical machine translation (SMT) (Agirre etal., 2013).
STS challenge results are evaluated withthe Pearson?s correlation score (r).The test set contains 2250 (S1, S2) sentence pairswith 750, 561, 189, and 750 sentences from eachtype respectively.
The training set contains 5342sentence pairs with 1500 each from MSRpar andMSRvid (Microsoft Research paraphrase and videodescription corpus (Agirre et al 2012)), 1592 fromSMT, and 750 from OnWN.3.1 RTM ModelsWe obtain CNGL results for the STS task as fol-lows.
For each perspective described in Section 1,we build an RTM model.
Each RTM model viewsthe STS task from a different perspective using the289 features extracted dependent on the interpre-tants using MTPP.
We extract the features both on2LIX=AB + C100A , where A is the number of words, C iswords longer than 6 characters, B is words that start or end withany of ?.
?, ?
:?, ?!
?, ???
similar to (Hagstro?m, 2012).236r R P L S R+PR+LR+SL+PL+SL+STLR+P+LR+P+SL+P+SL+P+STLR+P+L+SR+P+L+STLS1 ?
S2RR .7904 .7502 .8200 .7788 .8074 .8232 .8101 .8247 .8218 .8509 .8266 .8172 .8304 .8530 .8323 .8499SVR .8311 .8060 .8443 .8330 .8404 .8517 .8498 .8501 .8593 .8556 .8496 .8422 .8586 .8579 .8527 .8564S2 ?
S1RR .7922 .7651 .8169 .7891 .8064 .8196 .8136 .8219 .8257 .8257 .8226 .8164 .8284 .8284 .8313 .8324SVR .8308 .8165 .8407 .8302 .8361 .8506 .8467 .8510 .8567 .8567 .8525 .8460 .8588 .8588 .8575 .8574S1  S2RR .8079 .787 .8279 .8101 .8216 .8333 .8275 .8346 .8375 .8409 .8361 .8312 .8412 .8434 .8432 .844SVR .8397 .8237 .8554 .841 .8432 .857 .851 .8557 .8605 .8626 .8505 .8505 .8591 .8622 .8602 .8588Table 1: CV performance on the training set with tuning.
Underlined are the settings we use in our submissions.
RTMmodels in directions S1 ?
S2, S2 ?
S1, and the bi-directional models S1  S2 are displayed.the training set and the test set.
The training cor-pus used is the English side of an out-of-domaincorpus on European parliamentary discussions, Eu-roparl (Callison-Burch et al 2012) 3.
In-domaincorpora are likely to improve the performance.
Weuse the Stanford POS tagger (Toutanova et al 2003)to obtain the perspectives P and L. We use the train-ing corpus to build a 5-gram target LM.We use ridge regression (RR) and support vec-tor regression (SVR) with RBF kernel (Smola andScho?lkopf, 2004).
Both of these models learn a re-gression function using the features to estimate a nu-merical target value.
The parameters that govern thebehavior of RR and SVR are the regularization ?for RR and the C, , and ?
parameters for SVR.
Attesting time, the predictions are bounded to obtainscores in the range [0, 5].
We perform tuning on asubset of the training set separately for each RTMmodel and optimize against the performance evalu-ated with R2, the coefficient of determination.We do not build a separate model for differenttypes of sentences and instead use all of the train-ing set for building a large prediction model.
Wealso use transductive learning since using only therelevant training data for training can improve theperformance (Bic?ici, 2011).
Transductive learningis performed at the sentence level where for each testinstance, we select 1250 relevant training instancesusing the cosine similarity metric over the featurevectors and build an individual model for the test in-stance and predict the similarity score.3We use WMT?13 corpora from www.statmt.org/wmt13/.3.2 Training ResultsTable 1 lists the 10-fold cross-validation (CV) re-sults on the training set for RR and SVR for differ-ent RTM systems using optimized parameters.
Aswe combine different perspectives, the performanceimproves and we use the L+S with SVR for run 1(LSSVR), L+P+S with SVR for run 2 (LPSSVR),and L+P+S with SVR using transductive learningfor run 3 (LPSSVRTL) all in the translation direc-tion S1 ?
S2.
Lemmatized RTM, L, performs thebest among the individual perspectives.
We alsobuild RTM models in the direction S2 ?
S1, whichgives similar results.
The last main row combinesthem to obtain the bi-directional results, S1  S2,which improves the performance.
Each additionalperspective adds another 289 features to the repre-sentation and the bi-directional results double thenumber of features.
Thus, S1  S2 L+P+S is us-ing 1734 features.3.3 STS Challenge ResultsTable 2 presents the STS challenge r and rankingresults containing our CNGL submissions, the bestsystem result, and the mean results over all submis-sions.
There were 89 submissions from 35 compet-ing systems (Agirre et al 2013).
The results areranked according to the mean r obtained.
We alsoinclude the mean result over all of the submissionsand its corresponding rank.According to the official results, CNGL-LSSVRis the 30th system from the top based on the mean robtained and CNGL-LPSSVR is 15th according tothe results on OnWN out of 89 submissions in total.237System head OnWN FNWN SMT mean rankCNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36UMBC-EB.-PW .7642 .7529 .5818 .3804 .6181 1mean .6071 .5089 .2906 .3004 .4538 57Table 2: STS challenge r and ranking results ranked ac-cording to the mean r obtained.
head is headlines andmean is the mean of all submissions.CNGL submissions perform unexpectedly low in theFNWN task and only slightly better than the averagein the SMT task.
The lower performance is likely tobe due to using an out-of-domain corpus for buildingthe RTM models and it may also be due to using andoptimizing a single model for all types of tasks.3.4 Bi-directional RTM ModelsThe STS task similarity score is directional invari-ant: q(S1, S2) = q(S2, S1).
We develop RTM mod-els in the reverse direction and obtain bi-directionalRTM models by combining both.
Table 3 lists thebi-directional results on the STS challenge test setafter tuning, which shows that slight improvement inthe scores are possible when compared with Table 2.Transductive learning improves the performance ingeneral.
We also compare with the performance ob-tained when combining uni-directional models withmean, min, or max functions.
Taking the minimumperforms better than other combination approachesand can achieve r = 0.5129 with TL.
One can alsotake the individual confidence scores obtained foreach score when combining scores.4 ConclusionReferential translation machines provide a cleanand intuitive computational model for automaticallymeasuring semantic similarity by measuring the actsof translation involved and achieve to be the 15th onsome tasks and 30th overall in the STS challenge outof 89 submissions in total.
RTMs make quality andsemantic similarity judgments possible based on theretrieval of relevant training data as interpretants forreaching shared semantics.System head OnWN FNWN SMT meanLSmean .6552 .6943 .2016 .3005 .5086mean TL .6397 .6808 .1776 .3147 .5028min .6512 .6947 .2003 .2984 .5066min TL .6416 .6853 .1903 .3143 .5055max .6669 .6680 .1867 .2737 .4958max TL .6493 .6805 .1846 .3127 .5059S1  S2 .6388 .6695 .1667 .2999 .4938S1  S2 TL .6285 .6686 .0918 .2931 .4816LPSmean .6510 .6971 .1179 .2861 .4961mean TL .6524 .6918 .1940 .3176 .5121min .6608 .6953 .1704 .2922 .5053min TL .6509 .6864 .1792 .3156 .5084max .6588 .6800 .1355 .2868 .4961max TL .6493 .6805 .1846 .3127 .5059S1  S2 .6251 .6843 .0677 .2994 .4845S1  S2 TL .6370 .6978 .0951 .2980 .4936RLPSmean .6517 .7136 .1002 .2880 .4996mean TL .6383 .6841 .2434 .3063 .5059min .6615 .7099 .1644 .2877 .5072min TL .6606 .6987 .1972 .3059 .5129max .6589 .7019 .0995 .2935 .5008max TL .6362 .6896 .2044 .3153 .5063S1  S2 .6300 .7011 .0817 .2798 .4850S1  S2 TL .6321 .6956 .1995 .3128 .5052Table 3: Bi-directional STS challenge r and ranking re-sults ranked according to the mean r obtained.
We com-bine the two directions by taking the mean, min, or themax or use the bi-directional RTM model S1  S2.AcknowledgmentsThis work is supported in part by SFI (07/CE/I1142)as part of the Centre for Next Generation Locali-sation (www.cngl.ie) at Dublin City University andin part by the European Commission through theQTLaunchPad FP7 project (No: 296347).
We alsothank the SFI/HEA Irish Centre for High-End Com-puting (ICHEC) for the provision of computationalfacilities and support.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In *SEM 2012:The First Joint Conference on Lexical and Computa-tional Semantics ?
Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 385?393,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.238Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In *SEM 2013: The Second JointConference on Lexical and Computational Semantics.Association for Computational Linguistics.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational Linguistics - Volume 1,ACL ?98, pages 86?90, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual simi-larity by combining multiple content similarity mea-sures.
In *SEM 2012: The First Joint Conferenceon Lexical and Computational Semantics ?
Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation (SemEval2012), pages 435?440, Montre?al, Canada, 7-8 June.Association for Computational Linguistics.Ergun Bic?ici and Josef van Genabith.
2013.
CNGL:Grading student answers by acts of translation.
In*SEM 2013: The First Joint Conference on Lexicaland Computational Semantics and Proceedings of theSeventh International Workshop on Semantic Evalua-tion (SemEval 2013), Atlanta, Georgia, USA, 14-15June.
Association for Computational Linguistics.Ergun Bic?ici and Deniz Yuret.
2011a.
Instance selec-tion for machine translation using feature decay al-gorithms.
In Proceedings of the Sixth Workshop onStatistical Machine Translation, pages 272?283, Edin-burgh, Scotland, July.
Association for ComputationalLinguistics.Ergun Bic?ici and Deniz Yuret.
2011b.
RegMT system formachine translation, system combination, and evalua-tion.
In Proceedings of the Sixth Workshop on Sta-tistical Machine Translation, pages 323?329, Edin-burgh, Scotland, July.
Association for ComputationalLinguistics.Ergun Bic?ici, Declan Groves, and Josef van Genabith.2013.
Predicting sentence translation quality using ex-trinsic and language independent features.
MachineTranslation.Ergun Bic?ici.
2011.
The Regression Model of MachineTranslation.
Ph.D. thesis, Koc?
University.
Supervisor:Deniz Yuret.Ergun Bic?ici.
2008.
Consensus ontologies in sociallyinteracting multiagent systems.
Journal of Multiagentand Grid Systems.Carl Hugo Bjo?rnsson.
1968.
La?sbarhet.
Liber.Chris Bliss.
2012.
Comedy is transla-tion, February.
http://www.ted.com/talks/chris bliss comedy is translation.html.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311,June.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical machinetranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 10?51, Montre?al, Canada, June.
Association for Compu-tational Linguistics.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, pages 138?145, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Myroslava O. Dzikovska, Rodney D. Nielsen, and ChrisBrew.
2012.
Towards effective tutorial feedbackfor explanation questions: A dataset and baselines.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 200?210, Montre?al, Canada, June.
Associationfor Computational Linguistics.Kenth Hagstro?m.
2012.
Swedish readability calcula-tor.
https://github.com/keha76/Swedish-Readability-Calculator.George A. Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41,November.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Mar-cus, Martha Palmer, Lance A. Ramshaw, and Ralph M.Weischedel.
2007.
Ontonotes: a unified relationalsemantic representation.
Int.
J. Semantic Computing,1(4):405?419.Yoav Seginer.
2007.
Learning Syntactic Structure.
Ph.D.thesis, Universiteit van Amsterdam.Alex J. Smola and Bernhard Scho?lkopf.
2004.
A tutorialon support vector regression.
Statistics and Comput-ing, 14(3):199?222, August.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech239tagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 173?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Wikipedia.
2013.
Lix.
http://en.wikipedia.org/wiki/LIX.240
