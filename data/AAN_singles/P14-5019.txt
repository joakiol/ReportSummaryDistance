Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 109?114,Baltimore, Maryland USA, June 23-24, 2014.c?2014 Association for Computational Linguisticslex4all: A language-independent tool for building and evaluatingpronunciation lexicons for small-vocabulary speech recognitionAnjana Vakil, Max Paulus, Alexis Palmer, and Michaela RegneriDepartment of Computational Linguistics, Saarland University{anjanav,mpaulus,apalmer,regneri}@coli.uni-saarland.deAbstractThis paper describes lex4all, an open-source PC application for the generationand evaluation of pronunciation lexiconsin any language.
With just a few minutesof recorded audio and no expert knowl-edge of linguistics or speech technology,individuals or organizations seeking tocreate speech-driven applications in low-resource languages can build lexicons en-abling the recognition of small vocabular-ies (up to 100 terms, roughly) in the targetlanguage using an existing recognition en-gine designed for a high-resource sourcelanguage (e.g.
English).
To build such lex-icons, we employ an existing method forcross-language phoneme-mapping.
Theapplication also offers a built-in audiorecorder that facilitates data collection, asignificantly faster implementation of thephoneme-mapping technique, and an eval-uation module that expedites research onsmall-vocabulary speech recognition forlow-resource languages.1 IntroductionIn recent years it has been demonstrated thatspeech recognition interfaces can be extremelybeneficial for applications in the developing world(Sherwani and Rosenfeld, 2008; Sherwani, 2009;Bali et al., 2013).
Typically, such applicationstarget low-resource languages (LRLs) for whichlarge collections of speech data are unavailable,preventing the training or adaptation of recogni-tion engines for these languages.
However, an ex-isting recognizer for a completely unrelated high-resource language (HRL), such as English, canbe used to perform small-vocabulary recognitiontasks in the LRL, given a pronunciation lexiconmapping each term in the target vocabulary to asequence of phonemes in the HRL, i.e.
phonemeswhich the recognizer can model.This is the motivation behind lex4all,1an open-source application that allows users to automati-cally create a mapped pronunciation lexicon forterms in any language, using a small number ofspeech recordings and an out-of-the-box recog-nition engine for a HRL.
The resulting lexiconcan then be used with the HRL recognizer to addsmall-vocabulary speech recognition functionalityto applications in the LRL, without the need forthe large amounts of data and expertise in speechtechnologies required to train a new recognizer.This paper describes the lex4all application andits utility for the rapid creation and evaluation ofpronunciation lexicons enabling small-vocabularyspeech recognition in any language.2 Background and related workSeveral commercial speech recognition systemsoffer high-level Application Programming Inter-faces (APIs) that make it extremely simple to addvoice interfaces to an application, requiring verylittle general technical expertise and virtually noknowledge of the inner workings of the recogni-tion engine.
If the target language is supported bythe system ?
the Microsoft Speech Platform,2forexample, supports over 20 languages ?
this makesit very easy to create speech-driven applications.If, however, the target language is one of themany thousands of LRLs for which high-qualityrecognition engines have not yet been devel-oped, alternative strategies for developing speech-recognition interfaces must be employed.
Thoughtools for quickly training recognizers for new lan-guages exist (e.g.
CMUSphinx3), they typicallyrequire many hours of training audio to produceeffective models, data which is by definition not1http://lex4all.github.io/lex4all/2http://msdn.microsoft.com/en-us/library/hh3615723http://www.cmusphinx.org109available for LRLs.
In efforts to overcome thisdata scarcity problem, recent years have seenthe development of techniques for rapidly adapt-ing multilingual or language-independent acousticand language models to new languages from rela-tively small amounts of data (Schultz and Waibel,2001; Kim and Khudanpur, 2003), methods forbuilding resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al.,2014), and even a web-based interface, the RapidLanguage Adaptation Toolkit4(RLAT), which al-lows non-expert users to exploit these techniquesto create speech recognition and synthesis toolsfor new languages (Vu et al., 2010).
While theygreatly reduce the amount of data needed to buildnew recognizers, these approaches still requirenon-trivial amounts of speech and text in the targetlanguage, which may be an obstacle for very low-or zero-resource languages.
Furthermore, evenhigh-level tools such as RLAT still demand someunderstanding of linguistics/language technology,and thus may not be accessible to all users.However, many useful applications (e.g.
for ac-cessing information or conducting basic transac-tions by telephone) only require small-vocabularyrecognition, i.e.
discrimination between a fewdozen terms (words or short phrases).
For ex-ample, VideoKheti (Bali et al., 2013), a text-free smartphone application that delivers agricul-tural information to low-literate farmers in In-dia, recognizes 79 Hindi terms.
For such small-vocabulary applications, an engine designed torecognize speech in a HRL can be used as-is toperform recognition of the LRL terms, given agrammar describing the allowable combinationsand sequences of terms to be recognized, and apronunciation lexicon mapping each target term toat least one pronunciation (sequence of phonemes)in the HRL (see Fig.
1 for an example).This is the thinking behind Speech-based Auto-mated Learning of Accent and Articulation Map-ping, or ?Salaam?
(Sherwani, 2009; Qiao et al.,2010; Chan and Rosenfeld, 2012), a method ofcross-language phoneme-mapping that discoversaccurate source-language pronunciations for termsin the target language.
The basic idea is to discoverthe best pronunciation (phoneme sequence) for atarget term by using the source-language recog-nition engine to perform phone decoding on oneor more utterances of the term.
As commercial4http://i19pc5.ira.uka.de/rlat-dev<lexicon version="1.0" xmlns="http://www.w3.org/2005/01/pronunciation-lexicon" xml:lang="en-US" alphabet="x-microsoft-ups"><lexeme><grapheme>beeni</grapheme><phoneme>B E NG I</phoneme><phoneme>B EI N I I</phoneme></lexeme></lexicon>Figure 1: Sample XML lexicon mapping theYoruba word beeni (?yes?)
to two possible se-quences of American English phonemes.recognizers such as Microsoft?s are designed forword-decoding, and their APIs do not usually al-low users access to the phone-decoding mode, theSalaam approach uses a specially designed ?super-wildcard?
recognition grammar to mimic phonedecoding and guide pronunciation discovery (Qiaoet al., 2010; Chan and Rosenfeld, 2012).
This al-lows the recognizer to identify the phoneme se-quence best matching a given term, without anyprior indication of how many phonemes that se-quence should contain.Given this grammar and one or more audiorecordings of the term, Qiao et al.
(2010) use an it-erative training algorithm to discover the best pro-nunciation(s) for that term, one phoneme at a time.Compared to pronunciations hand-written by a lin-guist, pronunciations generated automatically bythis algorithm yield substantially higher recog-nition accuracy: Qiao et al.
(2010) report wordrecognition accuracy rates in the range of 75-95%for vocabularies of 50 terms.
Chan and Rosen-feld (2012) improve accuracy on larger vocabu-laries (up to approximately 88% for 100 terms)by applying an iterative discriminative training al-gorithm, identifying and removing pronunciationsthat cause confusion between word types.The Salaam method is fully automatic, demand-ing expertise neither in speech technology norin linguistics, and requires only a few recordedutterances of each word.
At least two projectshave successfully used the Salaam method to addvoice interfaces to real applications: an Urdutelephone-based health information system (Sher-wani, 2009), and the VideoKheti application men-tioned above (Bali et al., 2013).
What has not ex-isted before now is an interface that makes this ap-proach accessible to any user.110Given the established success of the Salaammethod, our contribution is to create a more time-efficient implementation of the pronunciation-discovery algorithm and integrate it into an easy-to-use graphical application.
In the following sec-tions, we describe this application and our slightlymodified implementation of the Salaam method.3 System overviewWe have developed lex4all as a desktop applica-tion for Microsoft Windows,5since it relies on theMicrosoft Speech Platform (MSP) as explained inSection 4.1.
The application and its source codeare freely available via GitHub.6The application?s core feature is its lexicon-building tool, the architecture of which is illus-trated in Figure 2.
A simple graphical user in-terface (GUI) allows users to type in the writtenform of each term in the target vocabulary, andselect one or more audio recordings (.wav files)of that term.
Given this input, the program usesthe Salaam method to find the best pronuncia-tion(s) for each term.
This requires a pre-trainedrecognition engine for a HRL as well as a seriesof dynamically-created recognition grammars; theengine and grammars are constructed and man-aged using the MSP.
We note here that our imple-mentation of Salaam deviates slightly from that ofQiao et al.
(2010), improving the time-efficiencyand thus usability of the system (see Sec.
4).Once pronunciations for all terms in the vocab-ulary have been generated, the application outputsa pronunciation lexicon for the given terms as anXML file conforming to the Pronunciation Lexi-con Specification.7This lexicon can then be di-rectly included in a speech recognition applicationbuilt using the MSP API or a similar toolkit.4 Pronunciation mapping4.1 Recognition engineFor the HRL recognizer we use the US Englishrecognition engine of the MSP.
The engine is usedas-is, with no modifications to its underlying mod-els.
We choose the MSP for its robustness andease of use, as well as to maintain comparabilitywith the work of Qiao et al.
(2010) and Chan andRosenfeld (2012).
Following these authors, weuse an engine designed for server-side recognition5Windows 7 or 8 (64-bit).6http://github.com/lex4all/lex4all7http://www.w3.org/TR/pronunciation-lexicon/Figure 2: Overview of the core components of thelex4all lexicon-building application.of low-quality audio, since we aim to enable thecreation of useful applications for LRLs, includ-ing those spoken in developing-world communi-ties, and such applications should be able to copewith telephone-quality audio or similar (Sherwaniand Rosenfeld, 2008).4.2 Implementation of the Salaam methodPronunciations (sequences of source-languagephonemes) for each term in the target vocabu-lary are generated from the audio sample(s) ofthat term using the iterative Salaam algorithm(Sec.
2), which employs the source-language rec-ognizer and a special recognition grammar.
Inthe first pass, the algorithm finds the best candi-date(s) for the first phoneme of the sample(s), thenthe first two phonemes in the second pass, and soon until a stopping criterion is met.
In our im-plementation, we stop iterations if the top-scoringsequence for a term has not changed for three con-secutive iterations (Chan and Rosenfeld, 2012), orif the best sequence from a given pass has a lowerconfidence score than the best sequence from the111previous pass (Qiao et al., 2010).
In both cases, atleast three passes are required.After the iterative training has completed, the n-best pronunciation sequences (with n specified byusers ?
see Sec.
5.2) for each term are written tothe lexicon, each in a <phoneme> element corre-sponding to the <grapheme> element containingthe term?s orthographic form (see Fig.
1).4.3 Running timeA major challenge we faced in engineering a user-friendly application based on the Salaam algo-rithm was its long running time.
The algorithmdepends on a ?super-wildcard?
grammar that al-lows the recognizer to match each sample of agiven term to a ?phrase?
of 0-10 ?words?, eachword comprising any possible sequence of 1, 2, or3 source-language phonemes (Qiao et al., 2010).Given the 40 phonemes of US English, this givesover 65,000 possibilities for each word, resultingin a huge training grammar and thus a long pro-cessing time.
For a 25-term vocabulary with 5training samples per term, the process takes ap-proximately 1-2 hours on a standard modern lap-top.
For development and research, this long train-ing time is a serious disadvantage.To speed up training, we limit the length of each?word?
in the grammar to only one phoneme, in-stead of up to 3, giving e.g.
40 possibilities in-stead of tens of thousands.
The algorithm can stilldiscover pronunciation sequences of an arbitrarylength, since, in each iteration, the phonemes dis-covered so far are prepended to the super-wildcardgrammar, such that the phoneme sequence of thefirst ?word?
in the phrase grows longer with eachpass (Qiao et al., 2010).
However, the new imple-mentation is an order of magnitude faster: con-structing the same 25-term lexicon on the samehardware takes approximately 2-5 minutes, i.e.less than 10% of the previous training time.To ensure that the new implementation?s vastlyimproved running time does not come at the costof reduced recognition accuracy, we evaluate andcompare word recognition accuracy rates usinglexicons built with the old and new implementa-tions.
The data we use for this evaluation is asubset of the Yoruba data collected by Qiao et al.
(2010), comprising 25 Yoruba terms (words) ut-tered by 2 speakers (1 male, 1 female), with 5samples of each term per speaker.
To determinesame-speaker accuracy for each of the two speak-Old New pFemale average 72.8 73.6 0.75Male average 90.4 90.4 1.00Same-speakerOverall average 81.6 82 0.81Trained on male 70.4 66.4 ?Trained on female 76.8 77.6 ?Cross-speakerAverage 73.6 72 0.63Table 1: Word recognition accuracy for Yoruba us-ing old (slower) and new (faster) implementations,with p-values from t-tests for significance of dif-ference in means.
Bold indicates highest accuracy.ers, we perform a leave-one-out evaluation on thefive samples recorded per term per speaker.
Cross-speaker accuracy is evaluated by training the sys-tem on all five samples of each term recorded byone speaker, and testing on all five samples fromthe other speaker.
We perform paired two-tailed t-tests on the results to assess the significance of thedifferences in mean accuracy.The results of our evaluation, given in Table 1,indicate no statistically significant difference inaccuracy between the two implementations (all p-values are above 0.5 and thus clearly insignifi-cant).
As our new, modified implementation of theSalaam algorithm is much faster than the original,yet equally accurate, lex4all uses the new imple-mentation by default, although for research pur-poses we leave users the option of using the origi-nal (slower) implementation (see Section 5.2).4.4 Discriminative trainingChan and Rosenfeld (2012) achieve increased ac-curacy (gains of up to 5 percentage points) byapplying an iterative discriminative training al-gorithm.
This algorithm takes as input the setof mapped pronunciations generated using theSalaam algorithm; in each iteration, it simulatesrecognition of the training audio samples usingthese pronunciations, and outputs a ranked list ofthe pronunciations in the lexicon that best matcheach sample.
Pronunciations that cause ?confu-sion?
between words in the vocabulary, i.e.
pro-nunciations that the recognizer matches to sam-ples of the wrong word type, are thus identifiedand removed from the lexicon, and the process isrepeated in the next iteration.We implement this accuracy-boosting algorithmin lex4all, and apply it by default.
To enable fine-112tuning and experimentation, we leave users theoption to change the number of passes (4 by de-fault) or to disable discriminative training entirely,as mentioned in Section 5.2.5 User interfaceAs mentioned above, we aim to make the creationand evaluation of lexicons simple, fast, and aboveall accessible to users with no expertise in speechor language technologies.
Therefore, the applica-tion makes use of a simple GUI that allows usersto quickly and easily specify input and output filepaths, and to control the parameters of the lexicon-building algorithms.Figure 3 shows the main interface of the lex4alllexicon builder.
This window displays the termsthat have been specified and the number of audiosamples that have been selected for each word.Another form, accessed via the ?Add word?
or?Edit?
buttons, allows users to add to or edit thevocabulary by simply typing in the desired ortho-graphic form of the word and selecting the audiosample(s) to be used for pronunciation discovery(see Sec.
5.1 for more details on audio input).Once the target vocabulary and training audiohave been specified, and the additional optionshave been set if desired, users click the ?BuildLexicon?
button and specify the desired name andtarget directory of the lexicon file to be saved, andpronunciation discovery begins.
When all pronun-ciations have been generated, a success messagedisplaying the elapsed training time is displayed,and users may either proceed to the evaluationmodule to assess the newly created lexicon (seeSec.
6), or return to the main interface to build an-other lexicon.5.1 Audio input and recordingThe GUI allows users to easily browse their filesystem for pre-recorded audio samples (.wavfiles) to be used for lexicon training.
To simplifydata collection and enable the development of lexi-cons even for zero-resource languages, lex4all alsooffers a simple built-in audio recorder to recordnew speech samples.The recorder, built using the open-source libraryNAudio,8takes the default audio input device asits source and records one channel with a samplingrate of 8 kHz, as the recognition engine we employis designed for low-quality audio (see Section 4.1).8http://naudio.codeplex.com/Figure 3: Screenshot of the lexicon builder.5.2 Additional optionsAs seen in Figure 3, lex4all includes optional con-trols for quick and easy fine-tuning of the lexicon-building process (the default settings are pictured).First of all, users can specify the maxi-mum number of pronunciations (<phoneme> el-ements) per word that the lexicon may contain;allowing more pronunciations per word may im-prove recognition accuracy (Qiao et al., 2010;Chan and Rosenfeld, 2012).
Secondly, users maytrain the lexicon using our modified, faster imple-mentation of the Salaam algorithm or the origi-nal implementation.
Finally, users may choosewhether or not discriminative training is applied,and if so, how many passes are run (see Sec.
4.4).6 Evaluation module for researchIn addition to its primary utility as a lexicon-building tool, lex4all is also a valuable researchaide thanks to an evaluation module that allowsusers to quickly and easily evaluate the lexiconsthey have created.
The evaluation tool allows usersto browse their file system for an XML lexicon filethat they wish to evaluate; this may be a lexiconcreated using lex4all, or any other lexicon in thesame format.
As in the main interface, users thenselect one or more audio samples (.wav files)for each term they wish to evaluate.
The systemthen attempts to recognize each sample using thegiven lexicon, and reports the counts and percent-ages of correct, incorrect, and failed recognitions.113Users may optionally save this report, along witha confusion matrix of word types, as a comma-separated values (.csv) file.The evaluation module thus allows users toquickly and easily assess different configurationsof the lexicon-building tool, by simply changingthe settings using the GUI and evaluating the re-sulting lexicons.
Furthermore, as the applica-tion?s source code is freely available and modifi-able, researchers may even replace entire modulesof the system (e.g.
use a different pronunciation-discovery algorithm), and use the evaluation mod-ule to quickly assess the results.
Therefore, lex4allfacilitates not only application development butalso further research into small-vocabulary speechrecognition using mapped pronunciation lexicons.7 Conclusion and future workWe have presented lex4all, an open-source appli-cation that enables the rapid automatic creationof pronunciation lexicons in any (low-resource)language, using an out-of-the-box commercialrecognizer for a high-resource language and theSalaam method for cross-language pronunciationmapping (Qiao et al., 2010; Chan and Rosen-feld, 2012).
The application thus makes small-vocabulary speech recognition interfaces feasiblein any language, since only minutes of training au-dio are required; given the built-in audio recorder,lexicons can be constructed even for zero-resourcelanguages.
Furthermore, lex4all?s flexible andopen design and easy-to-use evaluation modulemake it a valuable tool for research in language-independent small-vocabulary recognition.In future work, we plan to expand the selectionof source-language recognizers; at the moment,lex4all only uses US English as the source lan-guage, but any of the 20+ other HRLs supportedby the MSP could be added.
This would enable in-vestigation of the target-language recognition ac-curacy obtained using different source languages,though our initial exploration of this issue sug-gests that phonetic similarity between the sourceand target languages might not significantly affectaccuracy (Vakil and Palmer, 2014).
Another futuregoal is to improve and extend the functionality ofthe audio-recording tool to make it more flexibleand user-friendly.
Finally, as a complement to theapplication, it would be beneficial to create a cen-tral online data repository where users can uploadthe lexicons they have built and the speech sam-ples they have recorded.
Over time, this could be-come a valuable collection of LRL data, enablingdevelopers and researchers to share and re-use dataamong languages or language families.AcknowledgementsThe first author was partially supported by aDeutschlandstipendium scholarship sponsored byIMC AG.
We thank Roni Rosenfeld, Hao YeeChan, and Mark Qiao for generously sharing theirspeech data and valuable advice, and DietrichKlakow, Florian Metze, and the three anonymousreviewers for their feedback.ReferencesKalika Bali, Sunayana Sitaram, Sebastien Cuendet,and Indrani Medhi.
2013.
A Hindi speech recog-nizer for an agricultural video search application.
InACM DEV ?13.Hao Yee Chan and Roni Rosenfeld.
2012.
Discrimi-native pronunciation learning for speech recognitionfor resource scarce languages.
In ACM DEV ?12.Woosung Kim and Sanjeev Khudanpur.
2003.
Lan-guage model adaptation using cross-lingual infor-mation.
In Eurospeech.Fang Qiao, Jahanzeb Sherwani, and Roni Rosenfeld.2010.
Small-vocabulary speech recognition forresource-scarce languages.
In ACM DEV ?10.Tim Schlippe, Sebastian Ochs, and Tanja Schultz.2014.
Web-based tools and methods for rapid pro-nunciation dictionary creation.
Speech Communica-tion, 56:101?118.Tanja Schultz and Alex Waibel.
2001.
Language-independent and language-adaptive acoustic model-ing for speech recognition.
Speech Communication,35(1-2):31 ?
51.Jahanzeb Sherwani and Roni Rosenfeld.
2008.
Thecase for speech technology for developing regions.In HCI for Community and International Develop-ment Workshop, Florence, Italy.Jahanzeb Sherwani.
2009.
Speech interfaces for in-formation access by low literate users.
Ph.D. thesis,Carnegie Mellon University.Anjana Vakil and Alexis Palmer.
2014.
Cross-language mapping for small-vocabulary ASR inunder-resourced languages: Investigating the impactof source language choice.
In SLTU ?14.Ngoc Thang Vu, Tim Schlippe, Franziska Kraus, andTanja Schultz.
2010.
Rapid bootstrapping of fiveEastern European languages using the Rapid Lan-guage Adaptation Toolkit.
In Interspeech.114
