Schank/Riesbeck vs. Norman/Rumelhart: What's the Difference?Marc EisenstadtThe Open UniversityMilton Keynes, ENGLANDThis paper explores the fundamental differences betweentwo sentence-parsers developed in the early 1970's:Riesbeck's parser for $chank's'conceptual dependency'theory (4, 5), and the 'LNR' parser for Norman andRumelhart's 'active :~emantic network' theory (3).
TheRiesbeck parser and the I,NR parser share a common goal -that of trsnsforming an input sentence into a canonicalform for later use by memory~inference~paraphraseprocesses, l,'or both parserz, this transformation i s  theact of 'comprehension', although they appear to go aboutit in very (Jifferent ways.
Are these differences realor apparent?Riesbeck's parser i~ implemented as n production system,in which input text can either ssti~{y the conditionside of any production rule within ~ packet ofcurrently-active rules, or else interrupt processing bydisabling the current packet of rules and enabling('triggering') a new packet of rules.
In operation, themain verb of each segment of text is located, and apointer to its lexical decomposition (canonical form) isestablished in memory.
The surrounding text, primerilynoun phrases, is then systematically mapped onto vacantcase frame slots within the memory representation of thedecomposed verb.
Case information is signposted by averb-triggered packet of production rules which expectscertain cldsses of entity (e.g.
animate recipient) to beencountered in the text.
Phrase boundaries are handledby keyword-triggered packets of rules which initiate andterminate the parsing of phrases.In contrast to this, the LNR parser is implemented as anaugmented transition network, in which input text caneither satisfy a current expectation or cause back-tracking to a point at which an alternative expectationcan be satisfied.
In operation, input text is mappedonto a surface case frame, which is an n-ary predicatecontaining a pointer to the appropriate code responsiblefor decomposing the predicate into canonical form.
Caseinformation is signposted by property-list indicatorsstored in the lexical entry for verbs.
These indicatorsact as signals or flags which are inspected by augmentedtests on PUSH NP and PUSH PP arcs in order to decidewhether such transitions are to be allowed.
Phraseboundaries are handled by the standard ATN PUSH and POPmechanisms, with provision for backtracking if aninitially-fulfilled expectation later turns out to havebeen incorrect.In order to determine which differences are due tonotational conventions, I have implemented versions ofboth parsers in Kaplan's General Syntactic Processor(GSP) formalism (2)~ a simple but elegant generalizationof ATNs.
In GSP terms, Riesbeck's active packets ofproduction rules are grammar states, and each rule isrepresented as a grammar arc.
Rule-packet triggering ishandled by storing in the lexicon the GSP code whichtransfers control to a new grammar state when aninterrupt is called for.
Each packet is in effect asub-grammar of the type handled normally by an ATN PUSHand POP.
The important difference is that the expensiveactions normally associated with PUSH and POP (e.g.saving registers, building structures) only occur afterit is safe to perform them.
That is, bottom-upinterrupts and very cheap 'lookahead' ensure that waste-ful backtracking is largely avoided.Riesbeck's verb-triggered packet of rules (i.e.
theentire sub-grammar which is entered after the verb isencountered) is isomorphic to the LNR-style use oflexical flags, which are in effect 'raised' and'lowered' ~olely for the benefit of augmented tests onverb-independent ~rcs.
Where Riesbeck depicts a'satisfied expectation' by deleting the relevantproduction rule from the currently-active packet, LNRachieves the same effect by using augmented tests onPUSH NP and PUSII PP arcz to determine whether aparticular case frame Slot has already been filled.Both approaches are handled with equal ease by GSP.In actual practice, Riesbeck's case frame expectationsare typically tests for simple selectional restrictions,whereas LNR's case frame expectations are typicallytests for the order in which noun phrases are encounter-ed.
Prepositions, naturally, are used by both parsersas important case frame clues: Riesbeck has a verb-triggered action alter the interrupt code associatedwith prepositions so that they 'behave' in preciselythe right way; this is isomorphic to LNR's flags whichare stored in the lexical entry for a verb and examinedby augmented tests on verb-independent prepositionalphrase arcs in the grammar.The behaviour of Riesbeck's verb-triggered packets(verb-dependent sub-grammars) is actually independent ofwhen a pointer to the lexical decomposition of the verbis established (i.e.
whether a pointer is added as soonas the verb is encountered or whether it is added afterthe end of the sentence has been reached).
Thus, anyclaims about the possible advantages of 'early' or'instantaneous' decomposition are moot.
SinceRiesbeck's cases are filled primarily on the basis offairly simple selectional restrictions, there is noobvious reason why his parser couldn't have built someother kind of internal representation, based on any oneof several linguistic theories of lexical decomposition.Although Riesbeck's decomposition could occur after theentire sentence has been parsed, LNR's decompositionmust occur at this point, because it uses a network-matching algorithm to find already-present structures inmemory, and relies upon the arguments of the main n-arypredicate of the sentence being as fully specified aspossible.Computationally, the major difference between the twoparsers is that Riesbeck's parser uses interrupts toinitiate 'safe' PUSHes and POPs to and from sub-gra,s,ars,whereas the L~R parser performs 'risky' PUSHes and POPslike any purely top-down parser.
Riesbeck's mechanismis potentially very powerful, and the performance of theLNR parser can be improved by allowing this mechanism tobe added automatically by the compiler which transformsan LNR augmented transition network into GSP ~ch inecode.
Each parser can thus be mapped fairly clesJnlyonto the other, with the only irreconcilable differencebetween them being the degree to which they rely onverb-dependent selectional restrictions to guide theprocess of filling in case frames.
This character-ization of the differences between them, based onimplementing them within a common GSP framework, issomewhat surprising, since (a) the differences havenothing to do with 'conceptual dependency' or 'activesept ic  networks' s~ud (b) the computational differencebetween them immediately suggests a way to auton~ticallyincorporate bottom-up processing into the LNR parser toimprove not only its efficiency, but also itspsychological plausibility.
A GSP implementation of a'hybrid' version of the two parsers is outlined in (I).15REFERENCES(1) Eisenstadt, M. Alternative parsers fur conceptualdependency: getting there is half the fun?Proceedings of the sixth international~oint conference on artifici%lintelli~ence, Tokyo, 1979.
(2) Kaplan, R.M.
A general syntactic processor.
InR.
Ruetin (Ed.)
Natural languageprocessing.
Englewood Cliffs, N.J.:Prentice-Hal1, 1973.
(3)(5)Norman, D.A., Rumelhart, D.E., and the LNRResearch Group.
Explorations incognition.
San Francisco: W.h.
Freeman1975.Riesbeck, C.K.
Computational understanding:analysis of sentences and context.Working paper 4, Istituto per gli StudiSemantici e Cognitivi, Castab~nola,Switzerland, 1974.Schank, R.C.
Conceptual dependency: a theory ofnatural language understanding.Co~rtitive Psychology, vol.
3, no.
4, 1972.16
