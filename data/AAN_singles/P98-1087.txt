A Connect ionist  Architecture for Learning to ParseJ ames  Henderson  and Peter  LaneDept  of Computer  Science, Univ of ExeterExeter  EX4 4PT ,  Un i ted  K ingdomj amie@dcs, ex.
ac.
uk, pc lane~dcs,  ex.
ac.
ukAbst ractWe present a connectionist architecture and demon-strate that it can learn syntactic parsing from a cor-pus of parsed text.
The architecture can representsyntactic onstituents, and can learn generalizationsover syntactic onstituents, thereby addressing thesparse data problems of previous connectionist ar-chitectures.
We apply these Simple Synchrony Net-works to mapping sequences of word tags to parsetrees.
After training on parsed samples of the BrownCorpus, the networks achieve precision and recallon constituents that approaches that of statisticalmethods for this task.1 In t roduct ionConnectionist networks are popular for many of thesame reasons as statistical techniques.
They are ro-bust and have effective learning algorithms.
Theyalso have the advantage of learning their own inter-nal representations, so they are less constrained bythe way the system designer formulates the prob-lem.
These properties and their prevalence in cog-nitive modeling has generated significant interest inthe application of connectionist networks to natu-ral language processing.
However the results havebeen disappointing, being limited to artificial do-mains and oversimplified subproblems (e.g.
(Elman,1991)).
Many have argued that these kinds of con-nectionist networks are simply not computationallyadequate for learning the complexities ofreal naturallanguage (e.g.
(Fodor and Pylyshyn, 1988), (Hender-son, 1996)).Work on extending connectionist architectures forapplication to complex domains uch as natural an-guage syntax has developed a theoretically moti-vated technique called Temporal Synchrony VariableBinding (Shastri and Ajjanagadde, 1993; Henderson,1996).
TSVB allows syntactic constituency to berepresented, but to date there has been no empiricaldemonstration of how a learning algorithm can beeffectively applied to such a network.
In this paperwe propose an architecture for TSVB networks andempirically demonstrate its ability to learn syntac-tic parsing, producing results approaching currentstatistical techniques.In the next section of this paper we present heproposed connectionist architecture, Simple Syn-chrony Networks (SSNs).
SSNs are a natural ex-tension of Simple Kecurrent Networks (SRNs) (El-man, I99I), which are in turn a natural extensionof Multi-Layered Perceptrons (MLPs) (Rumelhartet al, 1986).
SRNs are an improvement over MLPsbecause they generalize what they have learned overwords in different sentence positions.
SSNs are animprovement over SKNs because the use of TSVBgives them the additional ability to generalize overconstituents in different structural positions.
Thecombination of these generalization abilities is whatmakes SSNs adequate for syntactic parsing.Section 3 presents experiments demonstratingSSNs' ability to learn syntactic parsing.
The taskis to map a sentence's sequence of part of speechtags to either an unlabeled or labeled parse tree,as given in a preparsed sample of the Brown Cor-pus.
A network input-output format is developedfor this task, along with some linguistic assump-tions that were used to simplify these initial ex-periments.
Although only a small training set wasused, an SSN achieved 63% precision and 69% re-call on unlabeled constituents for previously unseensentences.
This is approaching the 75% precisionand recall achieved on a similar task by Probabilis-tic Context Free Parsers (Charniak, forthcoming),which is the best current method for parsing basedon part of speech tags alone.
Given that these arethe very first results produced with this method, fu-ture developments are likely to improve on them,making the future for this method very promising.2 A Connect ion is t  Arch i tec ture  thatGenera l i zes  over  Const i tuentsSimple Synehrony Networks (SSNs) are designed toextend the learning abilities of standard eonnec-tionist networks o that they can learn generaliza-tions over linguistic constituents.
This generaliza-tion ability is provided by using Temporal SynchronyVariable Binding (TSVB) (Shastri and Ajjanagadde,1993) to represent constituents.
With TSVB, gener-531HiddenI nput  ~ u tII |1 I I I ; ; ; copy ,' ,' ,' ,' links#1 is eS SS: \[ , ' / ;, I ss~sr  t:--.
:_._- :..-.-gg-: -Figure 1: A Simple Recurrent Network.alization over constituents is achieved in an exactlyanalogous way to the way Simple Recurrent Net-works (SRNs) (Elman, 1991) achieve generalizationover the positions of words in a sentence.
SRNs area standard connectionist method for processing se-quences.
As the name implies, SSNs are one way ofextending SRNs with TSVB.2.1 Simple Recurrent  NetworksSimple Recurrent Networks (Elman, 1991) are a sim-ple extension of the most popular form of connec-tionist network, Multi-Layered Perceptrons (MLPs)(Rumelhart et al, 1986).
MLPs are popular becausethey can approximate any finite mapping, and be-cause training them with the Backpropagation learn-ing algorithm (Rumelhart et al, 1986) has beendemonstrated to be effective in a wide variety ofapplications.
Like MLPs, SRNs consist of a finiteset of units which are connected by weighted links,as illustrated in figure 1.
The output of a unit issimply a scalar activation value.
Information is in-put to a network by placing activation values on theinput units, and information is read out of a net-work by reading off activation values from the out-put units.
Computation is performed by the inputactivation being scaled by the weighted links andpassed through the activation functions of the "hid-den" units, which are neither part of the input noroutput.
The only parameters in this computationare the weights of the links and how many hiddenunits are used.
The number of hidden units is cho-sen by the system designer, but the link weights areautomatically trained using a set of example input-output mappings and the Backpropagation learningalgorithm.Unlike MLPs, SRNs process equences of inputsand produce sequences of outputs.
To store infor-mation about previous inputs, SRNs use a set ofcontext units, which simply record the activationsof the hidden units during the previous time step(shown as dashed links in figure 1).
When the SRNis done computing the output for one input in the se-quence, the vector of activations on the hidden unitsis copied to the context units.
Then the next inputis processed with this copied pattern in the contextunits.
Thus the hidden pattern computed for oneinput is used to represent the context for the subse-quent input.
Because the hidden pattern is learned,this method allows SRNs to learn their own inter-nal representation f this context.
This context isthe state of the network.
A number of algorithmsexist for training such networks with loops in theirflow of activation (called recurrence), for exampleBackpropagation Through Time (Rumelhart et al,1986).The most important characteristic of any learning-based model is the way it generalizes from the ex-amples it is trained on to novel testing examples.
Inthis regard there is a crucial difference between SRNsand MLPs, namely that SRNs generalize across se-quence positions.
At each position in a sequence anew context is copied, a new input is read, and anew output is computed.
However the link weightsthat perform this computation are the same for allthe positions in the sequence.
Therefore the infor-mation that was learned for an input and context inone sequence position will inherently be generalizedto inputs and contexts in other sequence positions.This generalization ability is manifested in the factthat SRNs can process arbitrarily long sequences;even the inputs at the end, which are in sequencepositions that the network has never encounteredbefore, can be processed appropriately.
This gener-alization ability is a direct result of SRNs using timeto represent sequence position.Generalizing across sequence positions is crucialfor syntactic parsing, since a word tends to have thesame syntactic role regardless of its absolute positionin a sentence, and there is no practical bound on thelength of sentences.
However this ability still doesn'tmake SRNs adequate for syntactic parsing.
BecauseSRNs have a bounded number of output units, andtherefore an effectively bounded output for each in-put, the space of possible outputs hould be linearin the length of the input.
For syntactic parsing, thetotal number of constituents is generally consideredto be linear in the length of the input, but each con-stituent has to choose its parent from amongst allthe other constituents.
This gives us a space of pos-sible parent-child relationships that is proportionalto the square of the length of the input.
For exam-ple, the attachment of a prepositional phrase needsto be chosen from all the constituents on the rightfrontier of the current parse tree.
There may be anarbitrary number of these constituents, but an SRNwould have to distinguish between them using onlya bounded number of output units.
While in the-ory such a representation can be achieved using ar-bitrary precision continuous activation values, this532bounded nature is symptomatic of a limitation inSRNs' generalization abilities.
What we really wantis for the network to learn what kinds of constituentssuch prepositional phrases like to attach to, and ap-ply these generalizations independently of the abso-lute position of the constituent in the parse tree.
Inother words, we want the network to generalize overconstituents.
There is no apparent way for SRNs toachieve such generalization.
This inability to gener-alize results in the network having to be trained ona set of sentences in which every kind of constituentappears in every position in the parse tree, result-ing in serious parse data problems.
We believe thatit is this difficulty that has prevented the successfulapplication of SRNs to syntactic parsing.2.2 Simple Synchrony NetworksThe basic technique which we use to solve SRNs'inability to generalize over constituents i exactlyanalogous to the technique SRNs use to generalizeover sentence positions; we process constituents oneat a time.
Words are still input to the network oneat a time, but now within each input step the net-work cycles through the set of constituents.
Thisdual use of time does not introduce any new compli-cations for learning algorithms, o, as for SRNs, wecan use Backpropagation Through Time.
The useof timing to represent constituents (or more gener-ally entities) is the core idea of Temporal SynchronyVariable Binding (Shastri and Ajjanagadde, 1993).Simple Synchrony Networks are an application ofthis idea to SRNs.
1As illustrated in figure 2, SSNs use the samemethod of representing state as do SRNs, namelycontext units.
The difference is that SSNs havetwo of these memories, while SRNs have one.
Onememory is exactly the same as for SRNs (the fig-ure's lower recurrent component).
This memoryhas no representation f constituency, so we callit the "gestalt" memory.
The other memory hashad TSVB applied to it (the figure's upper recur-rent component, depicted with "stacked" units).This memory only represents information about con-stituents, so we call it the constituent memory.These two representations are then combined via an-other set of hidden units to compute the network'soutput.
Because the output is about constituents,these combination and output units have also hadTSVB applied to them.The application of TSVB to the output units al-lows SSNs to solve the problems that SR.Ns havewith representing the output of a syntactic parser.For every step in the input sequence, TSVB unitscycle through the set of constituents.
To output1 There are a variety of ways to extend SRNs using TSVB.The architecture presented here was selected based on previ-ous experiments using a toy grammar.ColConConGe.
?CorFigure 2: A Simple Synchrony Network.
The unitsto which TSVB has been applied are depicted asseveral units stacked on top of each other, becausethey store activations for several constituents.something about a particular constituent, it is sim-ply necessary to activate an output unit at thatconstituent's time in the cycle.
For example, whena prepositional phrase is being processed, the con-stituent which that prepositional phrase attaches tocan be specified by activating a "parent" output unitin synchrony with the chosen constituent.
Howevermany constituents there are for the prepositionalphrase to choose between, there will be that manytimes in the cycle that the "parent" unit can be acti-vated in.
Thereby we can output information aboutan arbitrary number of constituents using only abounded number of units.
We simply require anarbitrary amount of time to go through all the con-stituents.Just as SRNs' ability to input arbitrarily long sen-tences was symptomatic of their ability to generalizeover sentence position, the ability of SSNs to outputinformation about arbitrarily many constituents isymptomatic of their ability to generalize over con-stituents.
Having more constituents han the net-work has seen before is not a problem because out-puts for the extra constituents are produced on thesame units by the same link weights as for otherconstituents.
The training that occurred for theother constituents modified the link weights so asto produce the constituents' outputs appropriately,and now these same link weights are applied to the533extra constituents.
So the SSN has generalized whatit has learned over constituents.
For example, oncethe network has learned what kinds of constituents apreposition likes to attach to, it can apply these gen-eralizations to each of the constituents in the currentparse and choose the best match.In addition to their ability to generalize over con-stituents, SSNs inherit from SRNs the ability to gen-eralize over sentence positions.
By generalizing inboth these ways, the amount of data that is nec-essary to learn linguistic generalizations is greatlyreduced, thus addressing the sparse data problemswhich we believe are the reasons connectionist net-works have not been successfully applied to syntacticparsing.
The next section empirically demonstratesthat SSNs can be successfully applied to learning thesyntactic parsing of real natural anguage.3 Exper iments  in Learn ing  to  ParseAdding the theoretical bility to generalize over lin-guistic constituents is an important step in connec-tionist natural anguage processing, but theoreticalarguments are not sufficient o address the empir-ical question of whether these mechanisms are ef-fective in learning to parse real natural anguage.In this section we present experiments on trainingSimple Synchrony Networks to parse naturally oc-curring sentences.
First we present he input-outputformat for the SSNs used in these experiments, thenwe present he corpus, then we present he results,and finally we discuss likely future developments.Despite the fact that these are the first such ex-periments to be designed and run, an SSN achieved63% precision and 69% recall on constituents.
Be-cause these results are approaching the results forcurrent statistical methods for parsing from part ofspeech tags (around 75% precision and recall), weconclude that SSNs are effective in learning to parse.We anticipate that future developments u ing largertraining sets, words as inputs, and a less constrainedinput-output format will make SSNs a real alterna-tive to statistical methods.3.1 SSNs for Pars ingThe networks that are used in the experiments allhave the same design.
They all use the internalstructure discussed in section 2.2 and illustrated infigure 2, and they all use the same input-output for-mat.
The input-output format is greatly simplifiedby SSNs' ability to represent constituents, but forthese initial experiments some simplifying assump-tions are still necessary.
In particular, we want todefine a single fixed input-output mapping for ev-ery sentence.
This gives the network a stable pat-tern to learn, rather than having the network itselfmake choices such as when information should beoutput or which output constituent should be asso-ciated with which words.
To achieve this we maketwo assumptions, namely that outputs hould occuras soon as theoretically possible, and that the headof each constituent is its first terminal child.As shown in figure 2, SSNs have two sets of in-put units, constituent input units and gestalt inputunits.
Defining a fixed input pattern for the gestaltinputs is straightforward, since these inputs per-tain to information about the sentence as a whole.Whenever a tag is input to the network, the activa-tion pattern for that tag is presented to the gestaltinput units.
The information from these tags isstored in the gestalt context units, forming a holisticrepresentation f the preceding portion of the sen-tence.
The use of this holistic representation is a sig-nificant distinction between SSNs and current sym-bolic statistical methods, giving SSNs some of theadvantages of finite state methods.
Figure 3 showsan example parse, and depicts the gestalt inputs asa tag just above its associated word.
First NP is in-put to the gestalt component, hen VVZ, then AT,and finally NN.Defining a fixed input pattern for the constituentinput units is more difficult, since the input must beindependent ofwhich tags are grouped together intoconstituents.
For this we make use of the assumptionthat the first terminal child of every constituent is itshead.
When a tag is input we add a new constituentto the set of constituents hat the network cyclesthrough and assume that the input tag is the headof that constituent.
The activation pattern for thetag is input in synchrony with this new constituent,but nothing is input to any of the old constituents.In the parse depicted in figure 3, these constituentinputs are shown as predications on new variables.First constituent w is introduced and given the inputNP, then z is introduced and given VVZ, then y isintroduced and given AT, and finally z is introducedand given NN.Because the only input to a constituent is its headtag, the only thing that the constituent context unitsdo is remember information about each constituent'sfirst terminal child.
This is not a very realistic as-sumption about the nature of the linguistic gener-alizations that the network needs to learn, but itis adequate for these initial experiments.
This as-sumption simply means that more burden is placedon the network's gestalt memory, which can store in-formation about any tag.
Provided the appropriateconstituent can be identified based on its first termi-nal child, this gestalt information can be transferredto the constituent through the combination units atthe time when an output needs to be produced.We also want to define a single fixed output pat-tern for each sentence.
This is necessary since we usesimple Backpropagation Through Time, plus it givesthe network a stable mapping to learn.
This desiredoutput pattern is called the target pattern.
The net-534Input Output AccumulatedOutputNP(w) w wNP I I(John) NP NPXvvz(X)wz w ~\ ](loves) VVZ NP VVZX XAT(y) ~y  ~ ~AT I(a) AT NP VVZ ATX(woman) NN NP VVZ AT NNFigure 3: A parse of "John loves a woman".work is trained to try to produce this exact pattern,even though other patterns may be interpretable asthe correct parse.
To define a unique target outputwe need to specify which constituents in the corpusmap to which constituents in the network, and atwhat point in the sentence ach piece of informa-tion in the corpus needs to be output.
The firstproblem is solved by the assumption that the firstterminal child of a constituent is its head.
2 We mapeach constituent in the corpus to the constituent inthe network that has the same head.
Network con-stituents whose head is not the first terminal child ofany corpus constituent are simply never mentionedin the output, as is true of z in figure 3.
The secondproblem is solved by assuming that outputs houldoccur as soon as theoretically possible.
As soon asall the constituents involved in a piece of informationhave been introduced into the network, that pieceof information is required to be output.
Althoughthis means that there is no point at which the entireparse for a sentence is being output by the network,we can simply accumulate the network's incremen-tal outputs and thereby interpret he output of theparser as a complete parse.To specify an unlabeled parse tree it is sufficientto output he tree's et of parent-child relationships.For parent-child relationships that are between aconstituent and a terminal, we know the constituentwill have been introduced by the time the termi-nal's tag is input because a constituent is headedby its first terminal child.
Thus this parent-childrelationship should be output when the terminal's2The cases  where  const i tuents  in the  corpus  have  no  te r -mina l  ch i ld ren  axe d iscussed  in the  next  subsect ion .tag is input.
This is done using a "parent" outputunit, which is active in synchrony with the parentconstituent when the terminal's tag is input.
In fig-ure 3, these parent outputs are shown structurally asparent-child relationships with the input tags.
Thefirst three tags all designate the constituents intro-duced with them as their parents, but the fourth tag(NN) designates the constituent introduced with theprevious tag (y) as its parent.For parent-child relationships that are betweentwo nonterminal constituents, the earliest his in-formation can be output is when the head of thesecond constituent is input.
This is done using a"grandparent" output unit and a "sibling" outputunit.
The grandparent output unit is used whenthe child comes after the parent's head (i.e.
rightbranching constituents like objects).
In this casethe grandparent output unit is active in synchronywith the parent constituent when the head of thechild constituent is input.
This is illustrated in thethird row in figure 3, where AT is shown as havingthe grandparent z.
The sibling output unit is usedwhen the child precedes the parent's head (i.e.
leftbranching constituents like subjects).
In this casethe sibling output unit is active in synchrony withthe child constituent when the head of the parentconstituent is input.
This is illustrated in the sec-ond row in figure 3, where VVZ is shown as havingthe sibling w. These parent, grandparent, and sib-ling output units are sumcient to specify any of theparse trees that we require.While training the networks requires having aunique target output, in testing we can allow anyoutput pattern that is interpretable as the correctparse.
Interpreting the output of the network hastwo stages.
First, the continuous unit activationsare mapped to discrete parent-child relationships.For this we simply take the maximums across com-peting parent outputs (for terminal's parents), andacross competing randparent and sibling outputs(for nonterminal's parents).
Second, these parent-child relationships are mapped to their equivalentparse "tree".
This process is illustrated in the right-most column of figure 3, where the network's incre-mental output of parent-child relationships i  accu-mulated to form a specification of the complete tree.This second stage may have some unexpected re-sults (the constituents may be discontiguous, andthe structure may not be connected), but it willalways specify which words in the sentence achconstituent includes.
By defining each constituentpurely in terms of what words it includes, we cancompare the constituents identified in the network'soutput o the constituents in the corpus.
As is stan-dard, we report the percentage of the output con-stituents that are correct (precision), and percentageof the correct constituents hat are output (recall).5353.2 A Corpus for SaNsThe Susanne 3 corpus is used in this paper as a sourceof preparsed sentences.
The Susanne corpus consistsof a subset of the Brown corpus, preparsed accord-ing to the Susanne classification scheme describedin (Sampson, 1995).
This data must be convertedinto a format suitable for the learning experimentsdescribed below.
This section describes the conver-sion of the Susanne corpus sentences and the preci-sion/recall evaluation functions.We begin by describing the part of speech tags,which form the input to the network.
The tagsin the Susanne scheme are a detailed extension ofthe tags used in the Lancaster-Leeds Treebank (seeGarside et al 1987).
For the experiments describedbelow the simpler Lancaster-Leeds scheme is used.Each tag is a two or three letter sequence, .g.
'John'would be encoded 'NP', the articles 'a' and 'the' areencoded 'AT', and verbs such as 'is' encoded 'VBZ'.These are input to the network by setting one bit ineach of three banks of inputs; each bank representingone letter position, and the set bit indicating whichletter or space occupies that position.The network's output is an incremental represen-tation of the unlabeled parse tree for the currentsentence.
The Susanne scheme uses a detailed clas-sification of constituents, and some changes are nec-essary before the data can be used here.
Firstly, theexperiments in this paper are only concerned withparsing sentences, and so all constituents referringto the meta-sentence level have been discarded.
Sec-ondly, the Susanne scheme allows for 'ghost' mark-ers.
These elements are also discarded, as the 'ghost'elements do not affect the boundaries of the con-stituents present in the sentence.Finally, it was noted in the previous ubsectionthat the SSNs used for these learning experimentsrequire every constituent to have at least one termi-nal child.
There are very few constructions in thecorpus that violate this constraint, but one of themis very common, namely the S-VP division.
The lin-guistic head of the S (the verb) is within the VP,and thus the S often occurs without any tags as im-mediate children.
For example, this occurs when Sexpands to simply NP VP.
To address this problem,we collapse the S and VP into a single constituent,as is illustrtated in figure 3.
The same is done forother such constructions, which include adjective,noun, determiner and prepositional phrases.
Thismove is not linguistically unmotivated, since the re-sult is equivalent to a form of dependency grammar(Mel~uk, 1988), which have a long linguistic tradi-tion.
The constructions are also well defined enough3 We acknowledge the roles of the Economic and Social Re-search Council (UK) as sponsor and the University of Sussexas grantholder in providing the Susanne corpus used in theexperiments described in this paper.Expt Training Cross val TestPrec Rec Prec P~c Prec Rec/~ / 75.6 79.1  66.7 71.9 60.4 66.571 .675 .868 .273 .862 .669 .4(3) 64 .271 .458 .666 .959 .868 .5Table 1: Results of experiments on Susanne corpus.that the collapsed constituents could be separatedat the interpretation stage, but we don't do that inthese experiments.
Also note that this limitation isintroduced by a simplifying assumption, and is notinherent to the architecture.3.3 Experimental ResultsThe experiments in this paper use one of the Susannegenres (genre A, press reportage) for the selectionof training, cross-validation a d test data.
We de-scribe three sets of experiments, training SSNs withthe input-output format described in section 3.1.
Ineach experiment, a variety of networks was trained,varying the number of units in the hidden and com-bination layers.
Each network is trained using anextension of Backpropagation Through Time untilthe sum-squared error reaches a minimum.
A cross-validation data set is used to choose the best net-works, which are then given the test data, and pre-cision/recall figures obtained.For experiments (1) and (2), the first twelve files inSusanne genre A were used as a source for the train-ing data, the next two for the cross-validation set(4700 words in 219 sentences, average length 21.56),and the final two for testing (4602 words in 176 sen-tences, average length 26.15).For experiment (1), only sentences of length lessthan twenty words were used for training, resultingin a training set of 4683 words in 334 sentences.
Theprecision and recall results for the best network canbe seen in the first row of table 1.
For experiment(2), a larger training set was used, containing sen-tences of length less than thirty words, resulting ina training set of 13,523 words in 696 sentences.
Weaveraged the performance of the best two networksto obtain the figures in the second row of table 1.For experiment (3), labeled parse trees were usedas a target output, i.e.
for each word we also outputthe label of its parent constituent.
The output forthe constituent labels uses one output unit for eachof the 15 possible labels.
For calculating the preci-sion and recall results, the network must also outputthe correct label with the head of a constituent inorder to count that constituent as correct.
Further,this experiment uses data sets selected at randomfrom the total set, rather than taking blocks fromthe corpus.
Therefore, the cross-validation set inthis case consists of 4551 words in 186 sentences,average length 24.47 words.
The test set consists of5364485 words in 181 sentences, average length 24.78words.
As in experiment (2), we used a training setof sentences with less than 30 words, producing a setof 1079 sentences, 27,559 words.
For this experimentnone of the networks we tried converged to nontriv-ial solutions on the training set, but one networkachieved reasonable performance before it collapsedto a trivial solution.
The results for this network areshown in the third row of table 1.From current corpus based statistical work onparsing, we know that sequences of part of speechtags contain enough information to achieve around75% precision and recall on constituents (Charniak,forthcoming).
On the other extreme, the simplisticparsing strategy of producing a purely right branch-ing structure only achieves 34% precision and 61%recall on our test set.
The fact that SSNs can achieve63% precision and 69% recall using much smallertraining sets than (Charniak, forthcoming) demon-strates that SSNs can be effective at learning therequired generalizations from the data.
While thereis still room for improvement, we conclude that SSNscan learn to parse real natural anguage.3.4 Extendabi l l tyThe initial results reported above are very promis-ing for future developments with Simple SynchronyNetworks, as they are likely to improve in both thenear and long term.
Significant improvements arelikely with larger training sets and longer trainingsentences.
While other approaches typically use overa million words of training data, the largest rainingset we use is only 13,500 words.
Also, fine tuning ofthe training methodology and architecture often im-proves network performance.
For example we shouldbe using larger networks, since our best results camefrom the largest networks we tried.
Currently thebiggest obstacle to exploring these alternatives i  thelong training times that are typical of Backpropa-gation Through Time, but there are a number ofstandard speedups which we will be trying.Another source of possible improvements i tomake the networks' input-output format more lin-guistically motivated.
As an example, we retestedthe networks from experiment 2 above with a dif-ferent mapping from the output of the network toconstituents.
If a word chooses an earlier word'sconstituent as its parent, then we treat these twowords as being in the same constituent, even if theearlier word has itself chosen an even earlier wordas its parent.
10% of the constituents are changedby this reinterpretation, with precision improving by1.6% and recall worsening by 0.6%.In the longer term the biggest improvement islikely to come from using words, instead of tags,as the input to the network.
Currently all the bestparsing systems use words, and back off to using tagsfor infrequent words (Charniak, forthcoming).
Be-cause connectionist networks automatically exhibit afrequency by regularity effect where infrequent casesare all pulled into the typical pattern, we would ex-pect such backing off to be done automatically, andthus we would expect SSNs to perform well withwords as inputs.
The performance we have achievedwith such small training sets supports this belief.4 Conc lus ionThis paper demonstrates for the first time that aconnectionist network can learn syntactic parsing.This improvement is the result of extending a stan-dard architecture (Simple Recurrent Networks) witha technique for representing linguistic constituents(Temporal Synchrony Variable Binding).
This ex-tension allows Simple Synchrony Networks to gen-eralize what they learn across constituents, therebysolving the sparse data problems of previous connec-tionist architectures.
Initial experiments have em-pirically demonstrated this ability, and future ex-tensions are likely to significantly improve on theseresults.
We believe that the combination of this gen-eralization ability with the adaptability of connec-tionist networks holds great promise for many areasof Computational Linguistics.Re ferencesEugene Charniak.
forthcoming.
Statistical tech-niques for natural anguage parsing.
AI Magazine.Jeffrey L. Elman.
1991.
Distributed representa-tions, simple recurrent networks, and grammaticalstructure.
Machine Learning, 7:195-225.Jerry A. Fodor and Zenon W. Pylyshyn.
1988.
Con-nectionism and cognitive architecture: A criticalanalysis.
Cognition, 28:3-71.R.
Garside, G. Leech, and G. Sampson (eds).
1987.The Computational Analysis of English: a corpus-based approach.
Longman Group UK Limited.James Henderson.
1996.
A connectionist architec-ture with inherent systematicity.
In Proceedingsof the Eighteenth Conference of the Cognitive Sci-ence Society, pages 574-579, La Jolla, CA.I.
Mel~uk.
1988.
Dependency Syntax: Theory andPractice.
SUNY Press.D.
E. Rumelhart, G. E. Hinton, and R. J. Williams.1986.
Learning internal representations by errorpropagation.
In D. E. Rumelhart and J. L. Mc-Clelland, editors, Parallel Distributed Processing,Vol 1.
MIT Press, Cambridge, MA.Geoffrey Sampson.
1995.
English for the Computer.Oxford University Press, Oxford, UK.Lokendra Shastri and Venkat Ajjanagadde.
1993.From simple associations to systematic reasoning:A connectionist representation f rules, variables,and dynamic bindings using temporal synchrony.Behavioral and Brain Sciences, 16:417-451.537
