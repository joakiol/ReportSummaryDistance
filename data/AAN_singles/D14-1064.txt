Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589?599,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLearning to Translate: A Query-Specific Combination Approach forCross-Lingual Information RetrievalFerhan TureRaytheon BBN Technologies10 Moulton StCambridge, MA, 02138 USAfture@bbn.comElizabeth BoscheeRaytheon BBN Technologies10 Moulton StCambridge, MA, 02138 USAeboschee@bbn.comAbstractWhen documents and queries are pre-sented in different languages, the com-mon approach is to translate the query intothe document language.
While there area variety of query translation approaches,recent research suggests that combiningmultiple methods into a single ?structuredquery?
is the most effective.
In this pa-per, we introduce a novel approach forproducing a unique combination recipe foreach query, as it has also been shownthat the optimal combination weights dif-fer substantially across queries and othertask specifics.
Our query-specific combi-nation method generates statistically sig-nificant improvements over other combi-nation strategies presented in the litera-ture, such as uniform and task-specificweighting.
An in-depth empirical anal-ysis presents insights about the effect ofdata size, domain differences, labeling andtuning on the end performance of our ap-proach.1 IntroductionCross-lingual information retrieval (CLIR) is aspecial case of information retrieval (IR) in whichdocuments and queries are presented in differentlanguages.
In order to overcome the languagebarrier, the most commonly adopted method isto translate queries into the document language.Many methods have been introduced for translat-ing queries for CLIR, ranging from word-by-worddictionary lookups (Xu and Weischedel, 2005;Darwish and Oard, 2003) to sophisticated use ofmachine translation (MT) systems (Magdy andJones, 2011; Ma et al., 2012).
Previous researchhas shown that combining evidence from differ-ent translation approaches is superior to any sin-gle query translation method (Braschler, 2004;Herbert et al., 2011).
While there are numer-ous combination-of-evidence techniques for bothmono-lingual and cross-lingual IR, recent worksuggests that there is no one-size-fits-all solution.In fact, the optimal combination weights (i.e.,weights assigned to each piece of evidence in alinear combination) differ greatly across queries,tasks, languages, and other variants (Ture et al.,2012; Berger and Savoy, 2007).In this paper, we introduce a novel method forlearning optimal combination weights when build-ing a linear combination of existing query transla-tion approaches.
From standard query-documentrelevance judgments we train a set of classifiers,which produce a unique combination recipe foreach query, based on a large set of features ex-tracted from the query and collection.
Experi-mental results show that the effectiveness of ourmethod is significantly higher than state-of-the-artquery translation methods and other combinationstrategies.2 Related WorkThe earliest approaches to query translation forCLIR used machine-readable bilingual dictio-naries (Hull and Grefenstette, 1996; Balles-teros and Croft, 1996), achieving around up to60% of monolingual IR effectiveness.
Xu andWeischedel (2005) showed that effectiveness canbe increased to around 80% by weighting eachtranslation proportional to its rank in the dictio-nary.
The practice of weighting translation candi-dates was later formulated as a ?structured query?,in which each query term is represented by a prob-ability distribution over its translations in the doc-ument language (Pirkola, 1998; Kwok, 1999; Dar-wish and Oard, 2003).
Our approach is based onthe structured query formulation.Some of the earliest studies in IR discoveredthat with different underlying models, the re-trieved document set would vary substantially, al-589though the effectiveness was similar (McGill etal., 1979).
Later studies showed that combiningdifferent representations of the query and/or doc-ument often produced superior output (Rajashekarand Croft, 1995; Turtle and Croft, 1990; Fox,1983).
This intuitive idea was supported theoret-ically by Pearl (1988), concluding that multiplepieces of evidence estimates relevance more accu-rately, but that the benefit strongly depends on thequality and independence of each piece.
Experi-ments by Belkin et al.
(1995) indicated the need toproperly weight each representation with respectto its effectiveness.
These so-called ?combination-of-evidence?
techniques became more powerfulwith the introduction of Indri, a probabilistic re-trieval framework specifically designed for com-bining multiple query and document representa-tions (Metzler and Croft, 2005).
Croft (2000) pro-vides a detailed summary of earlier query combi-nation approaches in IR, while Peters et al.
(2012)cites more recent related work.The benefits of combination-of-evidence trans-fer to the cross-lingual case especially well, sincethe inherent ambiguity of translation readily pro-vides a diverse set of representations.
Most CLIRapproaches implement a post-retrieval mergingof ranked lists, each generated from differentquery (Hiemstra et al., 2001; Savoy, 2001; Geyet al., 2001; Chen and Gey, 2004) or docu-ment (Lopez and Romary, 2009) representations,also called ?data fusion?.
In contrast, we focus ona pre-retrieval combination at the modeling stage,so that a single complex query is used in retrieval,instead of multiple simpler ones.
Two advantagesof the former are easier implementation (sincethe approach requires no changes to the modelingside) and the possibly greater diversity that can beachieved by having separate retrieval runs.
How-ever, each ranked list needs to be limited in size,which might cause some potentially useful docu-ments not to be considered in the combination atall.
Since the focus of this paper is on the model-ing end of retrieval, pre-retrieval combination wasa more suitable choice, though we think that thetwo approaches have complementary benefits.The idea of combining query translationsbefore retrieval has been explored previously.Braschler (2004) combines three translation ap-proaches: output of an MT system, a novel trans-lation approach based on a similarity thesaurusbuilt automatically from a comparable corpus,and a dictionary-based translation.
The mainreason that this combination does not providemuch benefit is due to the lower coverage ofthe thesaurus-based and dictionary-based trans-lation methods.
A similar approach by Herbertet al.
(2011) uses Wikipedia to provide transla-tions of certain phrases and entities, and combin-ing that with the Google Translate MT sys-tem yields statistically significant improvementsin English-to-German retrieval.
More recently,Ture et al.
(2012) presented a more sophisti-cated translation approach using the internal rep-resentation of an MT system, and reported sta-tistically significant improvements when a pre-retrieval combination was performed.All of the previously cited approaches eitheruse uniform weights for combination, or selectweights based on collection-level information.However, as stated previously, numerous stud-ies suggest that certain methods work better oncertain queries, collections, languages.
In fact,when weights are optimized separately on eachcollection, they differ substantially across differ-ent collections (Ture et al., 2012).
For monolin-gual retrieval, there has been a series of learning-to-rank (LTR) papers that determine weights forquery concepts (Bendersky et al., 2011), suchthat retrieval effectiveness is maximized.
A re-cent study extends this idea to the cross-lingualcase, by learning how to weight each translatedword for English-Persian CLIR (Azarbonyad etal., 2013).
In contrast, we extract translated wordweights from diverse and sophisticated translationmethods, then learn how to weight each trans-lated structured query, We call this ?learning-to-translate?
(LTT), which can be formulated as asimpler learning problem.
In CLIR, both LTR andLTT are under-explored problems, with a commongoal of applying machine learning techniques toimprove query translation, yet with complemen-tary benefits.To our knowledge, there has been one prior LTTapproach: a classifier was trained to predict ef-fectiveness of each query translation, using fea-tures based on statistics of the query terms (Bergerand Savoy, 2007).
Instead of weighting, thetranslations with highest classifier scores wereconcatenated, yielding statistically significant im-provements over using the single-best translationmethod.
However, the translation methods ex-plored in this paper are all based on one-best MT590systems, making it difficult to draw strong conclu-sions.3 Query TranslationThe primary contribution of this paper is to showhow a diverse set of query translation (QT) meth-ods can be combined effectively into a singleweighted structured query, with improved retrievaleffectiveness.
While our approach can applied toany set of translation methods, we focus on threemethods that have complementary strengths andthat have shown promise in CLIR: word-basedprobabilistic translation, one-best MT, and n-bestprobabilistic MT.
We briefly present our imple-mentation of each method; more details can befound in earlier work (Darwish and Oard, 2003;Ture et al., 2012).Each QT method generates a representation ofthe query in the document language.
In the case ofword-based and n-best MT approaches, the repre-sentation is a structured query itself, where eachquery word is represented by a probability distri-bution over translation alternatives.
For one-bestMT, the query is represented by a bag of translatedwords.3.1 One-Best MTA query translation approach that has becomemore popular recently is to simply run the querythrough an MT system, and use the best output asthe query:t1t2.
.
.
tl= MT(s1s2.
.
.
sk) (1)where s = s1s2.
.
.
skis the query and t =t1t2.
.
.
tlis the translated query.Since modern statistical MT systems generatehigh-quality translations for many language pairs,this one-best strategy works reasonably well forretrieval and provides a competitive baseline.
Apractical advantage of this approach is the ease ofimplementation ?
one can simply use any MT in-terface (e.g., Google Translate) as a blackbox in their CLIR system.3.2 Probabilistic n-best MTThe top translation might sometimes be incorrect,or might lack some of the alternative representa-tions that are very useful in retrieval.
Therefore,considering the n highest scored translations (alsoreferred to as the n-best list in MT literature) hasbecome increasingly popular in CLIR approaches.In order to benefit from the diversity amongstthe n-best translations, one can simply concate-nate them together, forming a large list of queryterms.
However, statistical MT systems alsoassign probabilities to each translation, whichcan be incorporated into the query representationfor better effectiveness, as suggested by Ture etal.
(2012).In this approach, each of the top n transla-tion candidates from the MT system are processedone by one.
For each translation candidate, theMT system provides a translation probability, andalignments between words in the query and itstranslation.
As we process each of the n transla-tions, for each query word si, we accumulate prob-abilities on each translated word tijaligned to si.Finally, we normalize the translation probabilitiesto get Prnbest(tij|si).3.3 Word-basedOne of the most widely used approaches in CLIRis based on translating each query word siin-dependently, with probabilities assigned to eachtranslation candidate tij.
Translations are de-rived automatically from a bilingual corpus usingstatistical word alignment techniques, which areused as part of the training of statistical MT sys-tems (Brown et al., 1993).
These probabilities canbe exploited for retrieval based on the techniqueof Darwish and Oard (2003) for ?projecting?
textinto the document language.
After cleaning up theautomatically learned translation probabilities (de-tails omitted for space considerations), we end upwith the translation probabilities Prword(tij|si).4 Combination of EvidenceOnce we have multiple ways to represent the queryq in the document language (QTi(q), i = 1 .
.
.m),it is possible to combine these ?pieces of evi-dence?
into a single representation as follows:QT(q) =m?i=1wi(q)QTi(q)and each combination-of-evidence approach dif-fers by how the combination weights wiare com-puted:Uniform In this baseline method, we ignore anyinformation we have about the collection or queryand assign equal weights to each method (i.e.,wi(q) = 1/m).
In our case, this means a weight591of 33.3% to each of the one-best, probabilistic n-best, and word-based QT methods.Task-specific We can optimize the combinationweights by overall effectiveness on a specific re-trieval task.
Given a query set and collection,we perform a grid search on combination weights(with a step interval of 0.1) and select the weightsthat maximize retrieval effectiveness.
The trainingis performed in a leave-one-out manner: weightsfor test query q are optimized on all queries exceptfor q.Query-specific We propose a novel method tocompute combination weights specifically foreach query, resulting in a more customized op-timization that can take into account how effec-tiveness of each translation method varies acrossqueries.In the remainder of this section, we describethe details of our novel query-specific combina-tion method.4.1 Overview of Query-Specific CombinationWe present a novel approach for determiningquery-specific combination weights by training aclassifier for each QT method.
Prior to train-ing the classifier, we first run retrieval using eachQT method, and evaluate the effectiveness of theretrieved documents.
The effectiveness of theithmethod on query q (i.e., fi(q)) is then con-verted into a binary label (further described inSection 4.2).
Treating each query as a separateinstance, a classifier is trained for each method,generating classifiers C1, .
.
.
, Cm.
During re-trieval (i.e., at test time), for each query q, eachtrained classifier Ciis applied to the query, re-sulting in a predicted label li(q) and the classi-fier?s confidence in a positive label, Ci(q).1Thesevalues are then used to determine combinationweights w1(q), .
.
.
, wm(q) that are custom-fit forthe query.4.2 LabelingFirst of all, we discard queries in which the dif-ference between the best and worst performingmethods is small (specifically, the worst perform-ing method scores at least k1% of the best per-forming one).
For such queries, generating fairtraining labels is more difficult and therefore more1The confidence in a negative label is 1?
Ci(q).likely to introduce noise into the process.2More-over, these are exactly the queries where choos-ing optimal combination weights is less important(since all methods perform relatively similarly), soit is reasonable to exclude them from training.
Infact, a high number of such queries would indi-cate lower potential for combination-of-evidenceapproaches.For each QT method i, we create training in-stances per query, per retrieval task.
Since ourgoal is to select the best among existing methods,the training label should reflect the effectivenessof method i relative to other methods.
A strategythat we call best-by-measure assigns a label of 1if the effectiveness of the ithmethod (i.e., fi(q)) isat least k2% of the maximum effectiveness for thatquery, and 0 otherwise.
While this directly corre-lates with retrieval effectiveness, labels might bedistributed in an unbalanced manner, which mightaffect the training process negatively.
A balancedlabeling requires sorting all training instances byhow much better the ithmethod is than other meth-ods (maxi?6=i(fi?
(q)/fi(q))), and then assigning alabel of 1 to the lower half and 0 to the higher half.This strategy is called best-by-rank.4.3 FeaturesWe introduce a diverse set of features, in order totrain a robust classifier for predicting when eachQT method performs better and worse than others.We split the feature set into four meaningful cate-gories, so that we can measure the impact of eachsubset separately:Surface features These features do not requirea deep analysis of the query: (a) Number of wordsin query and the translated query, (b) Type ofquery that we automatically classify based on pre-defined templates (e.g., fact question, cause-effect,etc.
), and (c) Number of stop words in the queryand the translated query.Parse-based features These features are ex-tracted from a deeper syntactic analysis of thequery text: (a) Number of related names found ina named entity database, and (b) Existence of syn-tactic constituents in query and its translation (e.g.,?is there a VVB in the query parse tree?
).2We also experimented with including these queries witha third label (e.g., ?same?)
and train a ternary classifier.
Hav-ing more labels requires more training data, which is not easyto obtain for this task.
Also, obtaining a balanced label dis-tribution becomes even more difficult with three labels.592Translation-based features These featuresconsist of statistics computed from the query andits translation: (a) Number of query words thatwere unaligned in at least half of the n-best querytranslations, (b) Number of query words that werealigned to multiple target words in at least halfof the n-best query translations, (c) Number ofquery words that were self-aligned (i.e., targetword is exactly same string) in at least half of then-best query translations, (d) Average / Standarddeviation / Maximum / Minimum of entropy ofPrnbestof each query word, and (e) Average /Standard deviation / Maximum / Minimum ofentropy of Prwordof each query word.Index-based features These features are basedon frequency statistics from a representative col-lection:3(a) Average / Standard deviation / Max-imum / Minimum of document frequency (df) ofquery words and their translations, (b) Average /Standard deviation / Maximum / Minimum of termfrequency averaged across query words and theirtranslations, and (c) Sum / Maximum / Minimumof total probability assigned to words that do notappear in the collection (df = 0).Additionally, the target language is a defaultfeature in all of our experiments.
For each clas-sification task, we train a separate classifier oneach subset of these four feature categories, so thatthere are 16 different sets (including the emptyset).
After we select which categories to pull fea-tures from, we optionally perform feature selec-tion to reduce the number of features by a pre-defined percentage.In our experimentation, we observed thatcollection-based features were most useful forclassifying the one-best method, whereas parse-based features were most discriminative for prob-abilistic 10-best.
For the word-based QT method,the translation-based features were most effectivein our experiments.
We further analyze the effectof various features in Section 5.4.4 Training and Tuning ClassifiersThe scikit-learn package was used for thetraining pipeline (Pedregosa et al., 2012).
Usingan established toolkit allowed us to experimentwith many options for classification, such as thelearner type (support vector machine, maximumentropy, decision tree), feature set (16 subsets of3We used the BOLT collection in our experiments.the four categories described earlier) and two fea-ture selection methods (recursive elimination orselection based on univariate statistical tests).
Inthe end, we get 96 different parameter combina-tions while training a classifier for a particular QTmethod, resulting in the need for tuning ?
pickingthe parameters that produce highest accuracy on arepresentative tuning set.Given that we have a set of queries for testingpurposes, there are few strategies for selecting atraining and tuning set.
One approach is to apply aleave-one-out strategy, so that a classifier is trainedand tuned on all but one of the test queries, andthen applied on the remaining query to predict itslabel.
We call this the fully-open setting.In a more realistic scenario, there will not berelevance judgments for the test queries, yet theremight be a small amount of labeled data similar tothe test task (e.g., different queries on same col-lection) that can be utilized for tuning purposes,and a larger set of training queries from differentcollections.
We call this the half-blind setting.If testing in a new domain, queries of similartype are not available for training and tuning pur-poses.
This is a more challenging scenario thanthe previous two, yet it is important for real-worldapplications.
In order to demonstrate the effec-tiveness of the training pipeline in this case, wehold out test queries entirely, then train and tuneon queries from a completely different task (i.e.,different queries and collection).
We call this thefully-blind setting.4.5 RetrievalOnce we have classifiers trained for all QT meth-ods, we can apply them to a given query on-the-fly,and compute query-specific combination weights.One approach is hard weighting, putting all weightonto a single method ?
when there are more thanone methods classified with label 1, we can ei-ther pick one randomly or use the classifier con-fidence value as a tie-breaker.
An alternative issoft weighting, where the weight of the ithmethodcan be computed either using classifier confidenceCi(i.e., how confident the model is that the ithmethod will perform well), precision on tuning setprecisioni(i.e., how precise the model is at its pre-593dictions for the ithmethod), or both:ws1i(q) =Ci(q)ws2i(q) =precisioni(1)?
li(q)+(1?precisioni(0))?
(1?
li(q))ws3i(q) =precisioni(1)?
Ci(q)+(1?precisioni(0))?
(1?
Ci(q))The intuition behind all of these weightingschemes is to produce a weight for each QTmethod, by taking into account the confidence ofthe classifier, and/or the precision of the classifieron tuning instances.The computed weights are normalized beforeconstructing the final query for retrieval:wfinali(q) = wi(q)/?mj=1wj(q)When compared empirically, we noticed thatsoft weighting is more effective than hard weight-ing, as the latter is more sensitive to classifier er-rors.
Among the three soft weighting functions,differences were mostly negligible in our exper-iments.
Hence, we decided to use the simplestweighting function ws1.4.6 Analytical ModelIt is time-consuming to implement variouscombination-of-evidence approaches and run re-trieval experiments.
Therefore, it is useful to havean analytical model of the process that can pro-vide a rough estimate of how fruitful it would beto spend this effort, given certain details about thetask.
The model we present in this section esti-mates the effectiveness of combining QT methods1 .
.
.m on a query set Q, given (1) the effective-ness of each method on Q and (2) error rate ofbinary classifiers C1.
.
.
Cmon Q.
Using this for-mulation, one can assess the benefit of combina-tion without running retrieval, based only on er-ror rates ?
this saves precious time during de-velopment.
Moreover, even without trained clas-sifiers, this model can be used to estimate poten-tial benefits by plugging in hypothetical error val-ues.
In other words, one can ask the question ?IfI had classifiers with x% error on this query set,what would be the benefit of using these classi-fiers to combine QT methods??
before developingany combination approach at all.The analytical model considers a special case ofweighted combination: for each query q, we pick asingle QT method i = 1 .
.
.m, for which the clas-sifier predicts a label of 1.
If there are more thanone such method, one of them is picked randomly.This simplified version allows us to compute ex-pected effectiveness for q as follows:E[f(q)] =?method iPr(pick i|q)fi(q)While fi(q) is an observed value (the effective-ness of the ithmethod on query q), Pr(pick i|q)needs to be estimated (the probability of selectingthe ithmethod).
Since this depends on the pre-dicted labels, we consider all possible scenariosl = l1l2.
.
.
lm, where each value is the predictionof a classifier.
For instance, ?l=010?
means thatclassifiers C1and C3predicted a label of 0, whileC2predicted a positive label.
Marginalizing overthe 2mpossible scenarios gives us the followingestimate:Pr(pick i|q)=??1?l1=0.
.
.1?lm=0Pr(l|q)???
Pr(pick i|l, q)=??1?l1=0.
.
.1?lm=0m?i=1Pr(li|q)???
Pr(i|l, q)In the final step, we assumed that classifiers makepredictions independent of each other, which isa desired property for successful combination.Pr(li|q) can be estimated using classifier errorstatistics:Pr(li|q) ?count(predicted = li, true = lq)count(true = lq)where lqis the true label of q.
If li= lq, this ex-pression becomes the true positive or true negativerate, depending on the value.
Similarly, if li6= lq,it is either the false positive or false negative rate.Finally, the probability that the ithmethod is se-lected in a particular scenario depends solely onthe predicted labels, since it is a random selection:Pr(pick i|l) = li/?mj=1ljThis concludes the derivation of the analyticalmodel of query evidence combination, which weuse in Section 5.1 to evaluate the effectiveness oflabeling approaches.5 EvaluationWe evaluated our approach on four different CLIRtasks: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Infor-594mation Access (ACLIA), and two forum post re-trieval tasks as part of the DARPA Broad Oper-ational Language Technologies (BOLT) program:English-Arabic (BOLTar) and English-Chinese(BOLTch).
The query language is English in allcases, and we preprocess the queries using BBN?sinformation extraction toolkit SERIF (Ramshaw etal., 2011).
State-of-the-art English-Arabic (En-Ar) and English-Chinese (En-Ch) MT systemswere trained on parallel corpora released in NISTOpenMT 2012, in addition to parallel forum datacollected as part of the BOLT program (10m En-Ar words; 30m En-Ch words).
From these data,word alignments were learned with GIZA++ (Ochand Ney, 2003), using five iterations of each ofIBM Models 1?4 and HMM.3-gram Chinese and 5-gram Arabic Kneser-Neylanguage models were trained from the Gigawordcorpus (1b words each) and non-English side ofthe training corpus.
Chinese and English paralleltext were preprocessed through the Treebank Tok-enizer,4while no special treatment was performedon Arabic.For retrieval, we used Indri, a state-of-the-art probabilistic relevance model that supportsweighted query representations through operators#combine and #weight (Metzler and Croft,2005).
A character-based index was built forChinese collections, whereas Arabic text wasstemmed using Lucene before indexing.5En-glish text was preprocessed by Indri?s imple-mentation of the Porter stemmer (Porter, 1997).Statistics for each collection and query set aresummarized in Table 1.Before performing any combination, we firstran the three baseline QT methods individuallyand evaluated the retrieved documents.
Meanaverage precision (MAP) was used to measureretrieval effectiveness, which is a widely usedand stable metric, estimating the area under theprecision-recall curve.
We set n = 10 for then-best probabilistic translation method.
Baselinescores are reported in Table 2.
The average preci-sion (AP) of each query in these tasks was used tolabel the query and construct training data accord-ingly.In subsequent sections, we evaluate the effect ofseveral variants in the training pipeline.4http://www.cis.upenn.edu/?treebank5http://lucene.apache.org5.1 Effect of LabelingIn Section 4.2, we introduced two ways to labelinstances.
In our evaluation, we set the free pa-rameters k1= k2= 90, which filters out 33% ofqueries from the training set of the BOLTartask;this percentage is 29% in BOLTch, 44% in TREC,and 27% in NTCIR.Labeling determines which query translationmethod is considered effective or not, which con-sequently determines what the ?learning problem?is (since the objective of the classifier is to sep-arate differently labeled instances).
As a result,there are two dimensions to consider when com-paring labeling strategies.
One is the accuracy ofthe classifiers on held-out data, and the other ishow well the trained classifier reflects this accu-racy when used in retrieval.
To clarify the dis-tinction, consider a case where every instance islabeled 1.
This generates a trivial learning prob-lem with no test errors, yet this does not entail thatusing these classifiers in retrieval will be more ef-fective than other labeling strategies.
If, even withhigh classifier accuracy, the retrieval effectivenessis low, that indicates a bad choice for labeling.We can theoretically analyze how suitable eachlabeling method is by applying the analyticalmodel to each CLIR task, setting parameters basedon a perfect classifier: true positive/negative rateof 1 and false positive/negative rate of 0 (see Sec-tion 4.6).
Table 2 shows these results in the ?Per-fect?
column, since these scores represent whatcould be achieved if classifiers were trained to pre-dict labels perfectly (no training or retrieval is ac-tually performed).
There are two values in eachrow of the ?Perfect?
column, one for each labelingstrategy.
In each row, we found these two values tobe statistically significantly higher than any of thebaseline scores.
This shows that both labeling ap-proaches have the potential to improve effective-ness significantly.We also made an empirical comparison of thetwo labeling approaches by actually training clas-sifiers with each labeling, and then using the clas-sifiers to combine query translations in retrieval.The ?Trained?
column in Table 2 shows the MAPwe get on each CLIR task (and average classifieraccuracies), using either labeling.6Based on these results, we conclude that best-6For a fair comparison, we fixed the train-tune setting tofully-open, trained classifiers on the test collection and re-ported leave-one-out accuracies.595LangCollectionTopicsMT Training dataSource Size (docs) Source (domain) Size (words)Arabic TREC-02 383,872 50 OpenMT-12 (news/web)10mArabic BOLT 12,258,904 45 BOLT (forum)Chinese NTCIR-8 388,589 100 OpenMT-12 (news/web)30mChinese BOLT 6,693,951 45 BOLT (forum)Table 1: Summary of the CLIR tasks in our evaluations.TaskBaseline Perfect Trainedone-best ten-best word measure rank measure rankBOLTar0.296 0.311 0.318 0.341 0.341 0.342 (74) 0.330 (72)BOLTch0.370 0.406 0.407 0.458 0.462 0.438 (68) 0.426 (60)TREC 0.292 0.298 0.301 0.327 0.330 0.305 (59) 0.316 (59)NTCIR 0.146 0.152 0.141 0.180 0.177 0.163 (56) 0.162 (61)Table 2: Retrieval effectiveness of baseline QT methods is presented on the left side, and a comparison oflabeling strategies is provided on the right side.
All numbers represent MAP values, except for classifieraccuracy shown in percentage values (in parantheses).
Analytically computed values are shown in italics.by-measure labeling is more useful in practice,supported by typically higher accuracy and effec-tiveness.
Best-by-rank yields better results onlyon TREC, but a closer look reveals that the in-crease in MAP is due to only two outlier queries.For BOLTar, on the other hand, retrieval with best-by-measure labeling is more effective (statisticallysignificant) than best-by-rank; hence, the former isused in remaining parts of our evaluation.5.2 Effect of Train-Tune SettingIn Section 4.4, we introduced three major train-tune settings: fully-open, half-blind, and fully-blind.
In order to implement these settings, wetreat each of the three query sets (BOLT, TREC,NTCIR) as a separate training dataset and experi-ment with a variety of combinations.For simplicity, let us demonstrate the variety ofexperiments assuming the test collection is BOLT.For the fully-open case, the default training data isall of the BOLT queries (this training set is referredto as b).
Additionally, one can include queriesfrom TREC (referred to as t) and NTCIR (referredto as n) into the training data.
This gives us fourdifferent training datasets for the fully-open case:b, b + n, b + t, b + t + n. Similarly, each of thehalf-blind and fully-blind settings can be appliedto three different training sets: For BOLT, theseare t, n, t + n.7This results in ten different ex-periments run for each task ?
in each experiment,7In the case of half-blind, b is split into two: 20% is usedfor tuning and the remainder is used for testing.we train a classifier for each QT method, select thebest meta-parameters on the tuning set, and thencompute combination weights for retrieval usingthe classifiers.Each cell on the left side of Table 3 (under col-umn ?Query-specific Combination?)
shows the re-sults of the most effective experiment for a partic-ular task and train-tune setting.
Accuracy valuesfor classifiers varied widely across these experi-ments.
Still, even when accuracies dropped closeto or below 50% (i.e.
random baseline), combinedretrieval was always more effective than any singleQT approach, which emphasizes the robustness ofour approach.
For instance, in the fully-blind set-ting for the NTCIR task, the individual classifiershad accuracies of only 56%, 49%, and 44% butMAP was 0.163, which is higher than the MAP ofany individual method for that collection (0.146,0.152, or 0.141).Another key observation in Table 3 is thatthe domain effect (i.e., training and/or tuning onqueries similar to test queries) is only noticeableon the two BOLT tasks.
For NTCIR and TREC,we do not observe a boost in MAP when queriesfrom the same task are included in training (i.e.,fully-open setting).
This can be explained by theBOLT-centric nature of our system components:the text analysis tool and MT systems are tunedmainly for forum data, and the collection-basedfeatures are extracted from BOLT.
Due to this bias,BOLT queries were most useful in our experi-ments, supported by the fact that BOLT is always596TaskQuery-specific CombinationUniformTask-Maxfully-open half-blind fully-blind specificBOLTar0.342?
?b 0.330 t+n 0.329 t+n 0.324120.32910.346BOLTch0.438?
?b 0.428 n 0.426 t+n 0.42210.43110.466TREC 0.321 b+t 0.324?
?b+n 0.321 b+n 0.31410.31810.332NTCIR 0.164?b+n 0.163 b+t 0.163 b 0.162130.162130.182Table 3: A comparison of query combination approaches.
For query-specific combination, MAP andtraining data are shown for the most effective experiment of each train-tune setting.
For each task, thehighest MAP achieved with our approach is shown in bold.
Superscripts 1, 2, and 3 indicate statisti-cally significant improvements over baseline methods one-best, probabilistic 10-best, and word-based,whereas * indicates improvements over all three.
Superscript ?
indicates results significantly better thanuniform and task-specific combination methods.included in the train set when testing on TREC orNTCIR (see lowest two rows in Table 3).
Also,when there is no domain effect (i.e., half-blind andfully-blind ), more data yields higher effectivenessin 6 out of 8 cases (see two right columns on theleft side of Table 3).5.3 Retrieval EffectivenessIn this section, we compare our novel query-specific combination-of-evidence approach to thebaseline CLIR approaches, as well as comparablecombination methods (uniform and task-specificcombination) in terms of retrieval effectiveness.Based on a randomized significance test (Smuckeret al., 2007), the best query-specific combina-tion method (shown in boldface in Table 3) out-performs all baseline QT methods in all taskswith 95% confidence (indicated by superscript *in Table 3).
This is not the case for uniform ortask-specific query combination, which are statis-tically indistinguishable from at least one of theQT methods, depending on the task (indicated bysuperscripts 1, 2, and 3 for one-best, probabilis-tic 10-best, and word-based QT methods, respec-tively).
When we directly compare our query-specific combination approach to other combina-tion methods, the differences are statistically sig-nificant for all tasks but NTCIR (indicated by su-perscript ?
).For reference, we also computed effectivenessfor a hypothetical system (denoted by ?Max?
inTable 3) that could select the best QT method foreach query and use only that for retrieval.
This isnot a strict upper bound, since correctly weight-ing each method can produce better results, butit is still a reasonable target for effectiveness.
Inour experiments, Arabic retrieval runs were veryclose to this target with our combination approach,while the gap for Chinese is still substantial, whichis worth further exploration.6 Conclusions and Future WorkIn this paper, we introduced a novel combination-of-evidence approach for CLIR, which learns acustom combination recipe for each query.
We for-mulate this as a set of binary classification prob-lems, and show that trained classifiers can be usedto produce query-specific combination weights ef-fectively.
Our deep exploration of many variants(e.g., labeling, training-tuning, weight computa-tion, analytical formulation) and extensive empiri-cal analysis on four different tasks provide insightsfor future research on the under-studied problemof combining translations for CLIR.Our approach advances the state of the art ofCLIR, yielding higher effectiveness than three ad-vanced query translation approaches, all basedon state-of-the-art MT systems.
Furthermore, onthree of the four tasks, our combination strategy isstatistically significantly better than two compara-ble combination techniques.
Experimental resultsalso suggest that even a uniform combination ofquery translations is consistently better than anyindividual method.
While it is known that com-bining translations helps CLIR, we confirm this ona set of modern CLIR tasks, including two targetlanguages and a variety of text domains.Having a simple linear learning problem allowsus to train robust models with relatively simplerfeatures.
Nevertheless, we are interested in ex-perimenting with more sophisticated learning ap-proaches.
In terms of non-linear classifiers, ourexperience with decision trees in this paper indi-cated a higher tendency to overfit.
In terms of597combining queries in a non-linear fashion, our fu-ture plans include integrating our approach into aLTR framework, and directly optimize MAP.
Thiswill also allow us to explore more complex fea-tures extracted from query and document text, aswell as external sources.Another possible future endeavor is to extendthese ideas to (i) other query translation ap-proaches and (ii) document translation.
While theexact same problem can be formulated for learningto translate documents effectively, a more compli-cated infrastructure and longer running times aretwo challenges that need to be considered.Finally, we hope this to be a significant step to-wards more context-dependent and robust CLIRmodels, by taking advantage of modern translationtechnologies, as well as machine learning tech-niques.AcknowledgmentsThis work was supported by DARPA/I2O Con-tract No.
HR0011-12-C-0014 under the BOLTprogram (Approved for Public Release, Distribu-tion Unlimited).
The views, opinions, and/or find-ings contained in this article are those of the authorand should not be interpreted as representing theofficial views or policies, either expressed or im-plied, of the Defense Advanced Research ProjectsAgency or the Department of Defense.ReferencesHosein Azarbonyad, Azadeh Shakery, and HeshaamFaili.
2013.
Exploiting multiple translation re-sources for english-persian cross language informa-tion retrieval.
In Proceedings of the Cross-LanguageEvaluation Forum on Cross-Language InformationRetrieval and Evaluation, CLEF ?13, pages 93?99.Lisa Ballesteros and W. Bruce Croft.
1996.
Dictionarymethods for cross-lingual information retrieval.
InProceedings of the 7th International DEXA Confer-ence on Database and Expert Systems Applications,pages 791?801.Nicholas J. Belkin, Paul Kantor, Edward A.
Fox, andJoseph A. Shaw.
1995.
Combining the evidenceof multiple query representations for informationretrieval.
Information Processing & Management,31(3):431?448, May.Michael Bendersky, Donald Metzler, and W. BruceCroft.
2011.
Parameterized concept weighting inverbose queries.
In Proceedings of the 34th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR ?11,pages 605?614, New York, NY, USA.
ACM.Pierre-Yves Berger and Jacques Savoy.
2007.
Se-lecting automatically the best query translations.
InLarge Scale Semantic Access to Content (Text, Im-age, Video, and Sound), RIAO ?07, pages 287?300,Paris, France, France.
Le Centre de Hautes EtudesInternationales D?Informatique Documentaire.Martin Braschler.
2004.
Combination approaches formultilingual text retrieval.
Information Retrieval,7(1-2):183?204, January.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:parameter estimation.
Computational Linguistics,19(2):263?311.Aitao Chen and Fredric C. Gey.
2004.
Multilingual in-formation retrieval using machine translation, rele-vance feedback and decompounding.
Inf.
Retr., 7(1-2):149?182, January.W.
Bruce Croft.
2000.
Combining approaches to in-formation retrieval.
In W. Bruce Croft, editor, Ad-vances in Information Retrieval, volume 7 of TheInformation Retrieval Series, pages 1?36.
Springer.Kareem Darwish and Douglas W. Oard.
2003.
Proba-bilistic structured query methods.
In Proceedings ofthe 26th Annual International ACM SIGIR Confer-ence on Research and Development in InformaionRetrieval, SIGIR ?03, pages 338?344.Edward A.
Fox.
1983.
Extending the Booleanand Vector Space Models of Information Retrievalwith P-norm Queries and Multiple Concept Types.Ph.D.
thesis, Cornell University, Ithaca, NY, USA.AAI8328584.Fredric C. Gey, Hailing Jiang, Vivien Petras, andAitao Chen.
2001.
Cross-language retrieval forthe clef collections - comparing multiple meth-ods of retrieval.
In Revised Papers from theWorkshop of Cross-Language Evaluation Forum onCross-Language Information Retrieval and Evalua-tion, CLEF ?00, pages 116?128, London, UK, UK.Springer-Verlag.Benjamin Herbert, Gy?orgy Szarvas, and IrynaGurevych.
2011.
Combining query transla-tion techniques to improve cross-language informa-tion retrieval.
In Proceedings of the 33rd Euro-pean Conference on Advances in Information Re-trieval, ECIR?11, pages 712?715, Berlin, Heidel-berg.
Springer-Verlag.Djoerd Hiemstra, Wessel Kraaij, Ren?ee Pohlmann,and Thijs Westerveld.
2001.
Translation re-sources, merging strategies, and relevance feedbackfor cross-language information retrieval.
In RevisedPapers from the Workshop of Cross-Language Eval-uation Forum on Cross-Language Information Re-trieval and Evaluation, CLEF ?00, pages 102?115,London, UK, UK.
Springer-Verlag.598David A.
Hull and Gregory Grefenstette.
1996.
Query-ing across languages: a dictionary-based approachto multilingual information retrieval.
In Proceed-ings of the 19th Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, SIGIR ?96, pages 49?57.Kui-Lam Kwok.
1999.
English-Chinese cross-language retrieval based on a translation package.In Workshop on Machine Translation for Cross Lan-guage Information Retrieval, Machine TranslationSummit VII, pages 8?13.Patrice Lopez and Laurent Romary.
2009.
Patatras:Retrieval model combination and regression mod-els for prior art search.
In Proceedings of the 10thCross-language Evaluation Forum Conference onMultilingual Information Access Evaluation: TextRetrieval Experiments, CLEF?09, pages 430?437,Berlin, Heidelberg.
Springer-Verlag.Yanjun Ma, Jian-Yun Nie, Hua Wu, and Haifeng Wang.2012.
Opening machine translation black box forcross-language information retrieval.
In InformationRetrieval Technology, pages 467?476.
Springer.Walid Magdy and Gareth J. F. Jones.
2011.
ShouldMT systems be used as black boxes in CLIR?
InProceedings of the 33rd European Conference on In-formation Retrieval, ECIR ?11, pages 683?686.Michael McGill, Matthew Koll, and Terry Noreault.1979.
An Evaluation of Factors Affecting DocumentRanking by Information Retrieval Systems.
ERICreports.
School of Information Studies, SyracuseUniversity.Donald Metzler and W. Bruce Croft.
2005.
A Markovrandom field model for term dependencies.
In Pro-ceedings of the 28th Annual International ACM SI-GIR Conference on Research and Development inInformation Retrieval, SIGIR ?05, pages 472?479.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Judea Pearl.
1988.
Probabilistic Reasoning in In-telligent Systems: Networks of Plausible Inference.Morgan Kaufmann Publishers Inc., San Francisco,CA, USA.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake VanderPlas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and Edouard Duchesnay.
2012.Scikit-learn: Machine learning in python.
CoRR,abs/1201.0490.Carol Peters, Martin Braschler, and Paul Clough.
2012.Multilingual Information Retrieval - From ResearchTo Practice.
Springer.Ari Pirkola.
1998.
The effects of query struc-ture and dictionary-setups in dictionary-based cross-language information retrieval.
In Proceedings ofthe 21st Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, SIGIR ?98, pages 55?63.M.
F. Porter.
1997.
Readings in information retrieval.chapter An Algorithm for Suffix Stripping, pages313?316.
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA.T.
B. Rajashekar and W. Bruce Croft.
1995.
Com-bining automatic and manual index representationsin probabilistic retrieval.
J.
Am.
Soc.
Inf.
Sci.,46(4):272?283, May.Lance Ramshaw, Elizabeth Boschee, Marjorie Freed-man, Jessica MacBride, Ralph Weischedel, and AlexZamanian.
2011.
Serif language processing ?
ef-fective trainable language understanding.
In J. Oliveet al., editor, Handbook of Natural Language Pro-cessing and Machine Translation: DARPA GlobalAutonomous Language Exploitation, pages 626?631.
Springer.Jacques Savoy.
2001.
Report on CLEF-2001 exper-iments: Effective combined query-translation ap-proach.
In CLEF, pages 27?43.Mark D. Smucker, James Allan, and Ben Carterette.2007.
A comparison of statistical significance testsfor information retrieval evaluation.
In Proceedingsof the 16th ACM conference on Conference on In-formation and Knowledge Management, CIKM ?07,pages 623?632.Ferhan Ture, Jimmy Lin, and Douglas W. Oard.2012.
Combining statistical translation techniquesfor cross-language information retrieval.
In Pro-ceedings of the 24th International Conference onComputational Linguistics, COLING ?12, pages2685?2702.Howard Turtle and W. Bruce Croft.
1990.
Inferencenetworks for document retrieval.
In Proceedings ofthe 13th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, SIGIR ?90, pages 1?24, New York, NY,USA.
ACM.Jinxi Xu and Ralph Weischedel.
2005.
Empirical stud-ies on the impact of lexical resources on CLIR per-formance.
Information Processing & Management,41(3):475?487, May.599
