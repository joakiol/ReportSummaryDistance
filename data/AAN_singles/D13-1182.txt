Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765?1775,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Generative Joint, Additive, Sequential Modelof Topics and Speech Acts in Patient-Doctor CommunicationByron C.
Wallace?, Thomas A.
Trikalinos?, M. Barton Laws?,Ira B. Wilson?
and Eugene Charniak??Dept.
of Health Services, Policy & Practice, Brown University, Providence, RI?Dept.
of Computer Science, Brown University, Providence, RI{byron wallace, thomas trikalinos, michael barton lawsira wilson, eugene charniak}@brown.eduAbstractWe develop a novel generative model of con-versation that jointly captures both the top-ical content and the speech act type asso-ciated with each utterance.
Our model ex-presses both token emission and state tran-sition probabilities as log-linear functions ofseparate components corresponding to topicsand speech acts (and their interactions).
Weapply this model to a dataset comprising anno-tated patient-physician visits and show that theproposed joint approach outperforms a base-line univariate model.1 IntroductionCommunication involves at least two aspects: thewords one says and the acts one performs in sayingthem.
Examples of the latter include asking ques-tions, issuing commands, and so on.
These are re-ferred to as speech acts under the sociolinguistic the-ory of Austin (1955), which was further developedby Searle (1969; 1985).
Recognizing speech acts iscrucial to understanding communication because aspeaker?s meaning is only partially captured by thewords they use; much of their intent is expressed im-plicitly via speech acts (Searle, 1969).On this view, conversational utterances can be as-signed both a topic and a speech act.
The formerdescribes the subject matter of what was said andthe latter captures the ?social act?
(e.g., promising)performed by saying it.
For example, the utterance?Obama won the election?
is topically political andis an example of an information giving speech act.
?Did Obama win the election?
?, meanwhile, belongsRole Utterance Topic Speech actD Let me just write down someof these issues here so I getthem straight in my mind.Logistics CommissiveP Doctor you ain?t got to tell menuttin?.Socializing DirectiveP I?m in very good hands whenI?m around you.Socializing Give Info.P If push comes to a shove, youopen the window and throwme out.Socializing Humor/LevityD I wanted to ask you, too - Biomedical Conv.
Mgmt.D you know you had thatcolonic polyp -Biomedical Ask Q.D - is it two years from now thatthey?re going to be doing therepeat?Biomedical Ask Q.P Yeah.
Biomedical Conv.
Mgmt.D We?ll do the repeat coloscopyin about two years.Biomedical Give Info.Table 1: An excerpt from a patient-doctor interaction,annotated with topic and speech act codes.
The D andP roles denote doctor and patient, respectively.
Conv.Mgmt.
abbreviates conversation management; Ask Q. ab-breviates ask question.to the same topic but is a question.
Both aspects arenecessary to understand conversation.Previous computational work on speech acts ?which we review in Section 6 ?
has modeled themin isolation (Perrault and Allen, 1980; Stolcke et al1998; Stolcke et al 2000; Kim et al 2010), i.e.,independent of topical content.
But a richer modelwould account for both speech acts and the contex-tualizing topic of each utterance.
To this end, we de-velop a novel joint, generative model of topics andspeech acts.We focus on physician-patient communication asa motivating domain.
This is of interest because1765it is widely appreciated that effective communica-tion is an integral part of clinical practice (Irwin andRichardson, 2006; Makoul, 2001; Teutsch, 2003).We provide an excerpt of a conversation between apatient and their doctor annotated with topics andspeech acts in Table 1.
Such annotations can providesubstantive insights into how doctors communicatewith patients (Ong et al 1995).A concrete example of this is the use of topicand speech act codes to assess the efficacy of anintervention meant to influence physician-patientcommunication regarding adherence to antiretrovi-ral (ARV) medication (Wilson et al 2010).
Tomeasure the effect of the intervention, investigatorsperformed a randomized control trial in which theyquantified change in communication patterns by tal-lying the number of information giving speech actsthat fell under the ARV adherence topic.
Withoutassigning both topics and speech acts to utterances,this analysis would not have been possible.In this work, we develop a novel component-based generative model for bivariate, sequentiallystructured problems.
Our approach extends the re-cently proposed Sparse Additive Generative (SAGE)model (Eisenstein et al 2011) and similar recentlydeveloped additive models (Paul and Dredze, 2012;Paul et al 2013) to the case of supervised sequen-tial tasks to capture the joint conditional influenceof topics and speech acts, both with respect to tokengeneration and state transitions.
For brevity, we referto this generative Joint, Additive, Sequential modelas JAS.
In contrast to previous work on speech acts,JAS provides a single, coherent generative model ofconversations.
And because it is component-based,this model provides a flexible framework for analyz-ing communication patterns.
We demonstrate thatJAS outperforms a generative univariate baseline intopic/speech act prediction.
Further, we automati-cally reproduce an analysis of the aforementionedrandomized control trial, and in doing so show thatJAS reproduces the results more faithfully than aunivariate approach.2 The Markov-Multinomial ModelWe begin by considering a baseline generative ap-proach to modeling topics and speech acts indepen-dently.
This simple approach was used by Stolcke etal.
(2000) to model speech acts.
It accounts for onlya single output at each time point yt ?
Y , and hencehere we model topics and speech acts independently.A straight-forward (albeit na?
?ve) alternativewould be to treat the Cartesian product of topicsand speech acts as a single output space on whichemissions and transitions are conditioned, but thisspace is too large and sparse for this approach to bepracticable.
We note that the fully coupled HMM(Brand et al 1997) suffers from a similar exponen-tial output state problem.
The related factorial HMM(FHMM) (Ghahramani and Jordan, 1997; Van Gaelet al 2008), meanwhile, imposes unwarranted (inour case) independence assumptions with respect tostate transitions along parallel chains, does not obvi-ously lend itself to discrete observations (typicallyGaussians are assumed), and does not scale wellenough (in terms of training time) to be feasible forour application.The Markov-Multinomial (MM) comprises twocomponents; transitions and emissions.
The formeris modeled by making a first-order Markov assump-tion, specifically:P (yt|y0, ..., yt?1) = P (yt|yt?1) = ?yt?1,yt (1)Emissions can be modeled via a multinomialthat captures the conditional probabilities of to-kens given labels.
Denoting an utterance (an utter-ance comprises the words corresponding to a singlespeech act; see Section 4) at time t by ut and its la-bel by yt, and making the standard na?
?ve assumptionthat words are generated independently conditionedon a label, we have:P (ut|yt) =?w?utP (w|yt) =?w?ut?yt,w (2)Both sets of parameters (the ?
?s and the ?
?s) can beestimated straight-forwardly using maximum like-lihood (i.e., using observed counts).
We can useViterbi decoding (Rabiner and Juang, 1986) to makepredictions for new sequences, as usual.
To makeboth topic and speech act predictions, we simply in-duce models for each and make predictions indepen-dently.3 JAS: A Joint, Additive, Sequential ModelAn obvious shortcoming of the simple MM modeloutlined above is that it treats topics and speech acts1766as statistically independent.
They are not (as con-firmed at statistical significance p < .001 using a?2 test).
One would prefer a more expressive modelthat conditions topic and speech act transitions aswell as the production of utterances jointly on boththe current topic and the current speech act.More specifically, we would like a model that re-flects the assumption that some latent intent givesrise to both the topic and the speech act associatedwith an utterance.
This is consistent with Searle?s(1969) notion of perlocutionary effects; one per-forms speech acts with the aim of getting someoneto do something.
Intent gives rise to the currenttopic and speech act, and the current intent affectsthe next; this induces a correlation between adjacenttopics and speech acts.
This conceptual model is de-picted graphically in the left-half of Figure 1.The latent intent may be, e.g., to encourage a pa-tient to take their medication more regularly.
In ourapplication the topical content may be ARV adher-ence and the type of speech act would be selectedby the provider (presumably to maximize the likeli-hood of patient adherence).
For example, she mayopt to urge imperatively (?You really need to takeyour medicine?)
or to implore with a question (?Willyou please remember to take your medicine??).
Be-cause we have no way of explicitly modeling intent(it is never observed), we instead rely on variablesfor which we have annotations (i.e., the topics andspeech acts; see Figure 1).
We next describe themodel in more detail.We refer to the topic set by Y , the speech actset by S and the vocabulary as W .
We denote the(log of the) background probability of word w by?w, and we will denote components correspondingto deviations from ?w due to a specific topic (speechact) by ?yw (?sw).
Further, we include the component?y,sw to capture interaction effects between topics andspeech acts.
We assume that the conditional proba-bility of word w belonging to an utterance ut withcorresponding topic yt and speech act st is log-linearwith respect to these components, i.e.
:P (w|yt, st) =1Zwexp{?w+?ytw +?stw +?st,ytw } (3)Where Zw is a normalizing term (implicitly condi-tioned on yt and st) defined as:Zw =?w??Wexp{?w?
+ ?ytw?
+ ?stw?
+ ?st,ytw? }
(4)We make the standard na?
?ve assumption that wordsare generated independently, given the topic andspeech act of the utterance to which they belong:P (ut|yt, st) =?w?utP (w|yt, st) (5)The per-token emission probability just describedfalls under the additive generative family of modelsrecently proposed by Eisenstein et al(2011).
How-ever, in addition to conditional token emission prob-abilities, here we need also to model the transitionprobabilities such that the likelihood of transition-ing to topic yt (and to speech act st) reflects both theprevious topic and the previous speech act, captur-ing the dependencies illustrated in Figure 1.
To thisend, we model topic and speech act transition proba-bilities as log-linear functions of the preceding topicand speech act.We denote log of the background topic frequen-cies by piY , and components capturing the influenceof transitioning to topic yt due to the preceding topicand speech act by ?yt?1,yt and ?st?1,yt respectively.We also include a component ?
(yt?1,st?1),yt that cor-responds to the interaction effect on topic transi-tion probability due to the preceding topic/speechact pair.
We then model the topic transition prob-ability (given the preceding states) as:P (yt|yt?1, st?1) =1Zyexp{piYyt +?yt?1,yt +?st?1,yt +?
(yt?1,st?1),yt}(6)Where Zy is a normalizing term for the topic transi-tions (implicitly conditioned on st?1, yt?1):Zy =?y??Yexp{piYy?+?yt?1,y?+?st?1,y?+?(yt?1,st?1),y?
}(7)Similarly, denoting by piS log-transformed speechact background frequencies, and including analo-gous components as above that correspond to the in-fluence of the preceding topic, speech act and theirinteraction on transitioning into speech act st, we1767Topict-1Speech Actt-1Utterancet-1TopictSpeech ActtUtterancetIntentt-1IntenttTopict-1Speech Actt-1Utterancet-1TopictSpeech ActtUtterancetFigure 1: The generative story of utterances, depicted graphically.
On the left we show our motivating conceptualiza-tion: a latent intent gives rise to both the topic and speech acts; these, in turn, jointly induce a distribution over wordsand transitions.
On the right we show our operationalization of this concept.
For clarity, we have denoted arrowscapturing influence due to topics with dotted lines.have:P (st|st?1, yt?1) =1Zsexp{piSst +?st?1,st +?yt?1,st +?
(yt?1,st?1),st}(8)Where Zs is a normalizing constant for speech actsanalogous to Equation 7.
Putting things together:P (yt, st|st?1, yt?1, ut) =P (ut|yt, st) ?
P (yt|yt?1, st?1) ?
P (st|st?1, yt?1)(9)As implied by Figure 1, this model assumes that thetopic and speech act at time t are conditionally in-dependent given the preceding topic and speech act(yt?1 and st?1).
This is intuitively agreeable be-cause time intervenes as a blocking factor; condi-tioning the current topic on the current speech act(or vice versa) would contradict the fact that theseoccur simultaneously.
Instead, the correlation is in-duced by the preceding topic/speech act pair.
(Thatsaid, this is still a simplifying assumption, as onemay instead choose to model speech act selection asconditional on topic (Traum and Larsson, 2003).
)Predictions can again be made via Viterbi de-coding (Rabiner and Juang, 1986) over a matrix ofpairs of joint topic/speech act states.
The strategy ofmodeling (additive) components allows JAS to avoidproblems due to sparsity in this large output space.Model parameters can be estimated using stan-dard optimization techniques.
We fix the ?back-ground?
frequencies ?, piY , piS to the log of thecorresponding observed proportions of words, top-ics and speech acts, respectively.
For the remainingparameters, one can use descent-based optimizationmethods.
The partial derivative for the topic-to-topictransition component ?y,y?
with respect to the likeli-hood, for example, is:???y,y?=?s?SC(y,s),y?
?
P (y?|y, s)C(y,s),?
(10)Where C(y,s),y?
denotes the observed count of tran-sitions from topic/speech act pair (y, s) to y?, andC(y,s),?
denotes the total number of observed transi-tions out of this pair.
The term P (y?|y, s) is withrespect to the current parameter estimates and isdefined in Equation 6.
The partial derivatives forthe other component parameters (both transition andemission) are analogous.
We use a Newton opti-mization method similar to the approach outlined byEisenstein et al(2011).1 We assess convergenceby calculating predictive performance on a held-outportion (5%) of the training dataset at each step,halting the descent when this declines.4 DatasetWe use a corpus of patient-provider visits annotatedwith Generalized Medical Interaction Anaylsis Sys-tem (GMIAS) codes.
The GMIAS has been usedto: characterize interaction processes in physician-patient communication about ARV adherence in the1With the exception that we do not explicitly model the dis-tribution over component variances.1768Topic; Speech act Count (prevalence)ARV Adherence; Ask Q 2939 (0.013)ARV Adherence; Commissive 245 (0.001)ARV Adherence; Continuation 328 (0.001)ARV Adherence; Conv.
Management 4298 (0.018)ARV Adherence; Directive 1650 (0.007)ARV Adherence; Empathy 111 (0.000)ARV Adherence; Give Information 12796 (0.055)ARV Adherence; Humor/Levity 46 (0.000)ARV Adherence; Missing/other 977 (0.004)ARV Adherence; Social-Ritual 15 (0.000)Biomedical; Ask Q 13753 (0.059)Biomedical; Commissive 1049 (0.005)Biomedical; Continuation 1005 (0.004)Biomedical; Conv.
Management 17611 (0.076)Biomedical; Directive 4617 (0.020)Biomedical; Empathy 423 (0.002)Biomedical; Give Information 54231 (0.233)Biomedical; Humor/Levity 255 (0.001)Biomedical; Missing/other 4426 (0.019)Biomedical; Social-Ritual 119 (0.001)Logistics; Ask Q 5517 (0.024)Logistics; Commissive 2308 (0.010)Logistics; Continuation 435 (0.002)Logistics; Conv.
Management 9672 (0.042)Logistics; Directive 5148 (0.022)Logistics; Empathy 100 (0.000)Logistics; Give Information 23351 (0.101)Logistics; Humor/Levity 135 (0.001)Logistics; Missing/other 2732 (0.012)Logistics; Social-Ritual 285 (0.001)Missing/other; Ask Q 820 (0.004)Missing/other; Commissive 70 (0.000)Missing/other; Continuation 1173 (0.005)Missing/other; Conv.
Management 1605 (0.007)Missing/other; Directive 523 (0.002)Missing/other; Empathy 48 (0.000)Missing/other; Give Information 3994 (0.017)Missing/other; Humor/Levity 27 (0.000)Missing/other; Missing/other 12103 (0.052)Missing/other; Social-Ritual 69 (0.000)Psycho-Social; Ask Q 2933 (0.013)Psycho-Social; Commissive 164 (0.001)Psycho-Social; Continuation 208 (0.001)Psycho-Social; Conv.
Management 4433 (0.019)Psycho-Social; Directive 787 (0.003)Psycho-Social; Empathy 262 (0.001)Psycho-Social; Give Information 15521 (0.067)Psycho-Social; Humor/Levity 63 (0.000)Psycho-Social; Missing/other 1199 (0.005)Psycho-Social; Social-Ritual 36 (0.000)Socializing; Ask Q 1283 (0.006)Socializing; Commissive 79 (0.000)Socializing; Continuation 85 (0.000)Socializing; Conv.
Management 2166 (0.009)Socializing; Directive 222 (0.001)Socializing; Empathy 73 (0.000)Socializing; Give Information 8981 (0.039)Socializing; Humor/Levity 306 (0.001)Socializing; Missing/other 849 (0.004)Socializing; Social-Ritual 1685 (0.007)Table 2: Topic/speech act pairs and their counts.context of an intervention trial (Wilson et al 2010);analyze communication about sexual risk behavior(Laws et al 2011a); elucidate the association ofvisit length with constructs of patient-centeredness(Laws et al 2011b); and to describe provider-patient communication regarding ARV adherencecompared with communication about other issues(Laws et al 2012).
GMIAS annotation is describedat length elsewhere,2 but we summarize it here forcompleteness.GMIAS segments conversation into utterances.An utterance is here defined as a single completedspeech act.
Previous coding systems have simplydefined an utterance as conveying a single thought(Roter and Larson, 2002) or any independent or un-restrictive dependent clause of a sentence (Ford andFord, 1995).
Stolcke et al(2000) followed Meteeret al(1995) in using ?sentence-level units?.
Thesedefinitions provide helpful guidance to coders, butmany speech acts are poorly formed grammatically,and cannot be described as a ?clause?.
Further, somespeech acts cannot be said to convey a ?thought?
(orsentence) at all, but rather are pre-syntactical (e.g.,interjections and non-lexical utterances like laugh-ter).
In any case, most natural segmentations of con-versations probably largely agree with intuition, andare not likely to differ substantially.The model we develop in this work assumes thattranscripts have been manually segmented.
Whilethis comes at some cost, segmenting is still muchcheaper than annotating transcripts.
Manually an-notating a single visit with GMIAS codes takes 2-4 hours and must be performed by someone withsubstantive domain expertise.
By contrast, segment-ing transcripts into utterances takes at most 1/4th ofthe time as annotation and can be done by a lesshighly-skilled individual.
That said, in future workwe hope to explore incorporating automatic segmen-tation methods (Galley et al 2003; Eisenstein andBarzilay, 2008) into our approach.Each utterance is assigned a single topic code anda single speech act code.
Inter-rater agreement hasbeen observed to be relatively high for this task:Kappa between three trained annotators and a ref-erence annotation ranged from 0.89 to 1.0 for top-ics and 0.81 to 0.95 for speech acts.
We next de-2https://sites.google.com/a/brown.edu/m-barton-laws/home/gmias1769scribe the topics and speech acts we consider inmore detail; Table 2 enumerates all pairs of theseand their respective counts in the corpus.
We notethat GMIAS defines a hierarchy of both topic andspeech act codes, but here we only attempt to cap-ture the highest level codes in these hierarchies.Topics comprise six major categories: ARVadherence, biomedical, logistics, missing/other,psycho-social and socializing.
Antiretroviral (ARV)adherence applies to utterances that address ARVmedication usage.
Biomedical utterances subsumeclinical observations and diagnostic conclusions.Utterances that concern the business of conducting aphysical examination fall under logistics.
The miss-ing/other topic covers a few cases, including utter-ances that are effectively outside of the GMIAS uni-verse and inaudible utterances; however we note thatmissing/other is a topic explicitly assigned by hu-man annotators.
The psycho-social topic includessuch issues as substance abuse, recovery, employ-ment and relationships.
Finally, socializing refers tocasual conversation unrelated to the business of themedical visit, and to social rituals such as greetings.There are 10 speech acts:3 ask question, commis-sive, continuation, conversation management, direc-tive, empathy, give information, humor/levity, miss-ing/other, and social-ritual.
Ask question is self-explanatory.
Utterances in which the speaker makesa promise or resolves to take action are commissives.A continuation refers to the completion of a previ-ously interrupted speech act (these are rare).
Con-versation management describes utterances that fa-cilitate turn-taking or guide discussion (?talk abouttalk?).
Directives refer to statements that look tocontrol or influence the behavior of the interlocutor.Utterances that express responses to emotions, con-cerns or feelings are coded under empathy.
Com-munication of (purported) facts falls under give in-formation.
Humor/levity captures jokes and jovialconversation.
Missing/other is the same as for top-ics.
Finally, social-ritual utterances represent for-malities (e.g., ?thank you?
).The corpus we use includes 360 GMIAS anno-tated patient-provider interactions (median length:605 utterances).
This data originated as part of3These are high-level speech acts; technically each consti-tutes a category of speech act types.a study designed to assess the role of the patient-provider relationship in explaining racial/ethnic dis-parities in HIV care.
Study subjects were HIV careproviders and their patients at four US care sites.The group responsible for the data are awaiting adecision from the institutional review board (IRB)regarding whether we can make this data publiclyavailable in some form.5 Experimental ResultsMarkov-Multinomial Joint Additive Sequential0.190.200.210.220.230.240.250.26average F-scoreFigure 2: Mean F-scores across all topic/speech act pairsfor the Markov-Multinomial (MM; left) and the proposedJoint Additive Sequential (JAS; right) models.
The thickblack line shows the mean difference over ten differentfolds; the thin grey lines describe per-fold differences.The proposed JAS model outperforms the baseline MMmodel for all foldsOur evaluation includes two parts.4 First, weperform standard cross-validation over the afore-mentioned 360 annotated interactions, evaluating F-measure for each topic/speech act pair.
Second,we look to automatically reproduce an analysis of4Source code at: https://github.com/bwallace/JAS; unfortu-nately we do not yet have permission to post the data.1770a randomized control trial that assessed the efficacyof an intervention meant to alter physician-patientcommunication.
We show that JAS outperforms thebaseline approach with respect to both tasks.We emphasize that while we are here compar-ing predictive performance, we are specifically in-terested in fully generative models of conversationsdue to the longer-term applications we have in mind.We would like, e.g., to use this model to assess thevariation in communicative approaches across dif-ferent doctors, and generative models are more nat-urally amenable to answering such exploratory ques-tions.
Indeed, perhaps the main strength of the ad-ditive component based sequential model we haveproposed here is that it will allow us to easily in-corporate physician-specific parameters that capturedeviations in provider speech act and/or topic tran-sition patterns.
Further, we may soon have accessto many unannotated transcripts, and we would liketo learn from these; generative approaches allowstraight-forward exploitation of unlabeled data.
Forthese reasons, we did not experiment with discrim-inative models, e.g., Dynamic Conditional RandomFields (DCRFs) (Sutton et al 2007) for this work.5.1 Cross-fold ValidationOur aim is to measure model performance in termsof correctly identifying both the topic and speech actcorresponding to each utterance.
We quantify thisvia the F-score calculated for each topic/speech actpair that is observed at least once.
One can see inTable 2 that many such pairs have low prevalence;this can result in undefined F-scores (e.g., when noutterances are assigned to a given pair).
In this case,it is reasonable to treat these as zero values, as iscommonly done (Forman and Scholz, 2010).
Thispenalizes models when they completely fail to iden-tify an entire class of utterances.We first report macro-averages, that is, averagesof the individual topic/speech act pair F-scores.Figure 2 displays the macro-averaged F-score foreach of the 10 folds (grey lines connect folds)and the average of these (thick black line).
TheJAS model achieves an average macro-averaged F-score of .234 versus the .207 achieved by base-line Markov-Multinomial (MM) model; JAS outper-forms MM on every fold.For a more granular picture, Figure 3 displays av-erage F-score differences with respect to every indi-vidual topic and speech act pair for which this differ-ence was non-zero.
This is the (signed) difference ofthe F-score achieved using JAS minus that achievedusing the MM model; black lines thus correspondto pairs for which JAS outperformed MM, and redlines to pairs for which MM outperformed JAS.
Thelatter achieves an improvement of >= .05 for 10pairs, and results in an F-score of > .02 below thatattained by MM only once.The relatively low F-scores for the metrics quanti-fying performance with respect to the cross of topicand speech act codes belie relatively good over-all (marginal) predictive performance.
That is, weachieve much better performance with respect tometrics that measure topic and speech act predic-tions independently of one another.
This is due to thevery large output space under consideration (see Ta-ble 2).
Specifically, averaged over ten runs, the MMmodel achieves a marginal mean topic F-score of.667 and marginal mean speech act F-score of .516.JAS begets a marginal mean topic F-score of .661and a marginal mean speech act F-score of .544;hence the JAS model incurs an F-score loss of .006(a 0.9% decrease) with respect to marginal topiccode prediction, but improves the marginal speechact F-score by .028 (a 5.4% increase).5.2 (Re-)Analysis of Randomized Control TrialWe also evaluated performance by tallying modelpredictions over 116 held-out cases collected froma randomized, cross-over study of an interventionaimed at improving physicians knowledge of pa-tients anti-retroviral (ARV) adherence (Wilson et al2010).
The intervention was a report given to thephysician before a routine office visit that containedinformation regarding the patients ARV usage andtheir beliefs about ARV therapy.
To explore the ef-ficacy of this intervention, 58 paired (116 total) au-dio recorded visits were annotated with GMIAS; 58correspond to visits before which the provider wasnot provided with the report (control cases), whilethe other 58 correspond to visits before which theywere (intervention cases).Wilson et al(2010) demonstrated that the in-tervention indeed increased adherence-related dia-logue, and specifically the number of informationgiving speech acts performed by the physician un-1771Figure 3: Average difference in F-scores corresponding to specific topic/speech act pairs, sorted by magnitude.
Blacklines (extending rightward) represent pairs for which JAS outperforms the baseline model; red lines (leftward) arepairs for which baseline performs better.True MM JAScontrol intervention control intervention control intervention10 (4, 28) 23 (11, 39) 13 (5, 33) 27 (16, 44) 12 (5, 28) 23 (14, 40)Table 3: Utterance counts {Median (25th, 75th per-centile)} for the ARV/information giving topic and speechact pair.
We show the ?gold standard?
(True) tallies,which were assigned by humans, and the counts takenusing the two models, MM and JAS.
The JAS model pre-dictions are closer to the true numbers.derneath this topic.
We attempted to reproduce thisfinding using automated rather manual annotations.To this end, we trained MM and JAS models overthe aforementioned 360 annotated visits and thenused this model to generate topic and speech actcode predictions for the utterances comprising the116 held-out visits used for the analysis (these werenot part of the training set).
We then assessed thedirection and magnitude of the change in the num-ber of ARV adherence/information giving utterancesin the paired control versus intervention cases.
Wecompared the results for this analysis calculated us-ing the true (manually assigned) codes to the resultscalculated using the predicted codes.Following the original analysis (Wilson etal., 2010), we report the median number ofARV/information giving utterances and correspond-ing 25th and 75th percentiles over the 58 control andintervention visits, as counted using the true (hu-man) annotations and using the codes predicted bythe MM and JAS models.
These are reported in Ta-ble 3.
The JAS model predictions better match thetrue labels in all except one case (the lower 25th forthe controls, for which it predicts the same numberas the MM model).6 Related workThere is a relatively long history of research intomodeling conversational speech acts in computa-tional linguistics.
Perrault and Allen (1980) con-ducted pioneering work on computationally formal-izing speech acts, though their work pre-dates statis-tical NLP and is therefore not directly relevant to thepresent work.Stolcke et al(2000; 1998) proposed a probabilis-tic approach to modeling conversational speech actsbased on the Hidden Markov Model (HMM) (Ra-biner and Juang, 1986).
They were interested inmodeling an unrestricted set of conversations, anddid not impose a hierarchy on the speech acts; they1772therefore enumerated many more speech acts (42)than we do in the present work (recall that we use 10?high-level?
speech acts).5 Their model has servedas the baseline approach in the present work.
Stol-cke et alalso considered jointly performing speechrecognition and speech act classification.Others have investigated visual structures ofpatient-provider interactions to qualitatively assesscommunication in care.
Specifically, (Cretchley etal., 2010) leveraged concept maps to explore conver-sations between people with schizophrenia and theircarers.
Briefly, this approach allowed them to (qual-itatively) identify two distinct conversational strate-gies used by care-takers and their patients.
Angus etal.
(Angus et al 2012) presented a similar approachin which they used text visualization software to ex-plore patterns of (inferred) topics in consultations.Another thread of research has investigated classi-fying speech acts in emails into one of a small set of?email speech acts?, e.g., request, propose, commit(Cohen et al 2004; Goldstein et al 2006).
Cohen etal.
(2004) demonstrated that good performance canbe achieved for this task via existing text classifica-tion technologies.
Elsewhere, researchers have ex-plored automatically inferring ?speech acts?
in vari-ous other online social mediums, including messageboard posts (Qadir and Riloff, 2011), Wikipedia talkpages (Ferschke et al 2012) and Twitter (Zhang etal., 2012).A separate line of inquiry concerns classifying di-alogue acts in chat.
Researchers have attempted di-alogue act classification both for 1-on-1 (Kim et al2010) and multi-party (Kim et al 2012; Clark andPopescu-Belis, 2004) online chats.
Ang et al(2005)considered the task of jointly segmenting and clas-sifying utterances comprising multiparty meetings,while Hsueh and Moore (2006) proposed analogousmethods for topic segmentation and labeling (otherworks on topic segmentation include (Galley et al2003) and (Eisenstein and Barzilay, 2008)).
Incor-porating such segmentation methods into the pro-posed model (rather than relying on inputs to bemanually segmented beforehand) would be a natu-ral extension of this work.Additive component models of text have recently5We note that only 8 of the 42 speech acts appeared withgreater than 1% frequency in Stolcke et als corpus.gained traction (Eisenstein et al 2011; Paul, 2012;Paul and Dredze, 2012; Paul et al 2013).
To ourknowledge, this is the first extension of supervisedadditive component models to a sequential task.67 Conclusions and Future DirectionsWe have proposed a novel Joint, Additive, Sequen-tial (JAS) model of conversational topics and speechacts.
In contrast to previous approaches to mod-eling conversational exchanges, this model factorsboth the current topic and the current speech act intotoken emission and state transition probabilities.
Wedemonstrated that this model consistently outper-forms a univariate generative baseline that treatsspeech acts and topics independently.
Furthermore,we showed JAS can automatically re-produce theanalysis of a randomized control trial designed to as-sess the efficacy of an intervention to alter physiciancommunication habits with high-fidelity.The generative component-based framework wehave introduced in this work provides a means ofexploring factors in patient-physician communica-tion.
One limitation of the model we have pre-sented is that it makes several simplifying assump-tions around dialogue.
For example, we have ig-nored non-linearities and ?back-channels?
in con-versation, and we have ignored differences acrossphysicians with respect to communication styles.Going forward, we hope to address these limita-tions.
We also plan on extending this model to in-vestigate qualitative questions surrounding patient-physician communication quantitatively.
For exam-ple, we are interested in investigating how communi-cation varies across hospitals and physicians.
To ex-plore this, we can add additional components to thetransition probability terms corresponding to differ-ent hospitals and doctors.
Ultimately, we would liketo correlate patterns in physician communication (asgleaned from the model) with objective, measuredhealth outcomes (e.g., patient satisfaction and adher-ence to ARVs).6Though Paul (2012) recently proposed ?mixed-membership?
Markov models for unsupervised conversationmodeling.17738 AcknowledgementsThe authors thank members of the Brown Labora-tory for Linguistic Information Processing (BLLIP)and Kevin Small for providing helpful feedback onearlier versions of this work.
We also thank the threeanonymous EMNLP reviewers for insightful com-ments.
This work was partially supported by the Na-tional Institute of Mental Health (2 K24MH092242,R34MH089279 and R01MH083595) and by NIDA(R01DA015679).ReferencesJeremy Ang, Yang Liu, and Elizabeth Shriberg.
2005.Automatic dialog act segmentation and classificationin multiparty meetings.
In Proc.
ICASSP, volume 1,pages 1061?1064.Daniel Angus, Bernadette Watson, Andrew Smith, CindyGallois, and Janet Wiles.
2012.
Visualising con-versation structure across time: Insights into effectivedoctor-patient consultations.
PloS one, 7(6).John Langshaw Austin.
1955.
How to do things withwords, volume 88.
Harvard University Press.Matthew Brand, Nuria Oliver, and Alex Pentland.
1997.Coupled hidden Markov models for complex actionrecognition.
In Computer Vision and Pattern Recog-nition, 1997.
Proceedings., 1997 IEEE Computer So-ciety Conference on, pages 994?999.
IEEE.Alexander Clark and Andrei Popescu-Belis.
2004.Multi-level dialogue act tags.
In Proc.
SIGdial, pages163?170.William W Cohen, Vitor R Carvalho, and Tom MMitchell.
2004.
Learning to classify email into speechacts.
In Proceedings of EMNLP, volume 4. sn.Julia Cretchley, Cindy Gallois, Helen Chenery, and An-drew Smith.
2010.
Conversations between carersand people with schizophrenia: a qualitative analy-sis using leximancer.
Qualitative Health Research,20(12):1611?1628.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 334?343.
Association forComputational Linguistics.J.
Eisenstein, A. Ahmed, and E.P.
Xing.
2011.
Sparseadditive generative models of text.
In Proceedings ofICML, pages 1041?1048.Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar.2012.
Behind the article: Recognizing dialog acts inwikipedia talk pages.
In Proceedings of the 13th Con-ference of the European Chapter of the Association forComputational Linguistics (EACL 2012), pages 777?786.
Citeseer.Jeffrey D Ford and Laurie W Ford.
1995.
The role ofconversations in producing intentional change in or-ganizations.
Academy of Management Review, pages541?570.George Forman and Martin Scholz.
2010.
Apples-to-apples in cross-validation studies: pitfalls in classifierperformance measurement.
volume 12, pages 49?57.ACM.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,and Hongyan Jing.
2003.
Discourse segmentation ofmulti-party conversation.
In Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics, pages 562?569.
Association for Compu-tational Linguistics.Zoubin Ghahramani and Michael I Jordan.
1997.
Facto-rial hidden Markov models.
Machine learning, 29(2-3):245?273.Jade Goldstein, Andrew Kwasinski, Paul Kingsbury,R Sabin, and Albert McDowell.
2006.
Annotatingsubsets of the enron email corpus.
In Proceedings ofthe Third Conference on Email and Anti-Spam.
Cite-seer.P-Y Hsueh and Johanna D Moore.
2006.
Automatictopic segmentation and labeling in multiparty dia-logue.
In Spoken Language Technology Workshop,2006.
IEEE, pages 98?101.
IEEE.Richard S Irwin and Naomi D Richardson.
2006.Patient-focused careusing the right tools.
CHESTJournal, 130(1 suppl):73S?82S.Su Nam Kim, Lawrence Cavedon, and Timothy Bald-win.
2010.
Classifying dialogue acts in one-on-onelive chats.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 862?871.
Association for Computational Lin-guistics.Su Nam Kim, Lawrence Cavedon, and Timothy Bald-win.
2012.
Classifying dialogue acts in multi-partylive chats.Michael Barton Laws, Ylisabyth S Bradshaw, Steven ASafren, Mary Catherine Beach, Yoojin Lee, WilliamRogers, and Ira B Wilson.
2011a.
Discussion of sex-ual risk behavior in HIV care is infrequent and appearsineffectual: a mixed methods study.
AIDS and Behav-ior, 15(4):812?822.Michael Barton Laws, Lauren Epstein, Yoojin Lee,William Rogers, Mary Catherine Beach, and Ira BWilson.
2011b.
The association of visit length andmeasures of patient-centered communication in HIVcare: A mixed methods study.
Patient Education andCounseling, 85(3):e183?e188.1774Michael Barton Laws, Mary Catherine Beach, YoojinLee, William H Rogers, Somnath Saha, P Todd Ko-rthuis, Victoria Sharp, and Ira B Wilson.
2012.Provider-patient adherence dialogue in HIV care: re-sults of a multisite study.
AIDS and Behavior, pages1?12.Gregory Makoul.
2001.
Essential elements of communi-cation in medical encounters: the kalamazoo consen-sus statement.
Academic Medicine, 76(4):390?393.Marie W Meteer, Ann A Taylor, Robert MacIntyre, andRukmini Iyer.
1995.
Dysfluency annotation stylebookfor the switchboard corpus.
University of Pennsylva-nia.Lucille ML Ong, Johanna CJM De Haes, Alaysia MHoos, and Frits B Lammes.
1995.
Doctor-patientcommunication: a review of the literature.
Social sci-ence & medicine, 40(7):903?918.Michael Paul and Mark Dredze.
2012.
Factorial lda:Sparse multi-dimensional text models.
In Advancesin Neural Information Processing Systems 25, pages2591?2599.Michael J. Paul, Byron C. Wallace, and Mark Dredze.2013.
What affects patient (dis)satisfaction?
analyz-ing online doctor ratings with a joint topic-sentimentmodel.
In AAAI Workshop on Expanding the Bound-aries of Health Informatics Using AI (HIAI).Michael J Paul.
2012.
Mixed membership Markov mod-els for unsupervised conversation modeling.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 94?104.Association for Computational Linguistics.C Raymond Perrault and James F Allen.
1980.
A plan-based analysis of indirect speech acts.
ComputationalLinguistics, 6(3-4):167?182.Ashequl Qadir and Ellen Riloff.
2011.
Classifying sen-tences as speech acts in message board posts.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 748?758.
Asso-ciation for Computational Linguistics.Lawrence Rabiner and B Juang.
1986.
An introductionto hidden Markov models.
ASSP Magazine, IEEE,3(1):4?16.Debra Roter and Susan Larson.
2002.
The roter in-teraction analysis system (rias): utility and flexibilityfor analysis of medical interactions.
Patient educationand counseling, 46(4):243?251.John R Searle.
1969.
Speech acts: An essay in the phi-losophy of language.
Cambridge university press.John R Searle.
1985.
Expression and meaning: Studiesin the theory of speech acts.
Cambridge UniversityPress.Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,Noah Coccaro, Daniel Jurafsky, Rachel Martin, MarieMeteer, Klaus Ries, Paul Taylor, and Carol Van Ess-Dykema.
1998.
Dialog act modeling for conversa-tional speech.
In AAAI Spring Symposium on Apply-ing Machine Learning to Discourse Processing, pages98?105.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational linguistics, 26(3):339?373.Charles Sutton, Andrew McCallum, and Khashayar Ro-hanimanesh.
2007.
Dynamic conditional randomfields: Factorized probabilistic models for labeling andsegmenting sequence data.
The Journal of MachineLearning Research, 8:693?723.Carol Teutsch.
2003.
Patient-doctor communication.The medical clinics of North America, 87(5):1115.David R Traum and Staffan Larsson.
2003.
The infor-mation state approach to dialogue management.
InCurrent and new directions in discourse and dialogue,pages 325?353.
Springer.Jurgen Van Gael, Yee Whye Teh, and Zoubin Ghahra-mani.
2008.
The infinite factorial hidden Markovmodel.
In Neural Information Processing Systems,volume 21.Ira B Wilson, M Barton Laws, Steven A Safren, Yoo-jin Lee, Minyi Lu, William Coady, Paul R Skolnik,and William H Rogers.
2010.
Provider-focused inter-vention increases adherence-related dialogue, but doesnot improve antiretroviral therapy adherence in per-sons with HIV.
Journal of acquired immune deficiencysyndromes, 53(3):338.Renxian Zhang, Dehong Gao, and Wenjie Li.
2012.
To-wards scalable speech act recognition in twitter: Tack-ling insufficient training data.
EACL 2012, page 18.1775
