A Comparison of Head Transducers and Transfer for a LimitedDomain Translation ApplicationHiyan  A lshawi  and Adam L. BuchsbaumAT&T Labs180 Park  AvenueF lo rham Park.
NJ 079:32-0971.
USA{hiyan,a lb}.~research.at  t .
comAbst ractWe compare the effectiveness of two related?
machine translation models applied to thesame limited-domain task.
One is a trans-fer model with monolingual head automatafor analysis and generation; the other is adirect transduction model based on bilin-gual head transducers.
We conclude thatthe head transducer model is more effectiveaccording to measures of accuracy, compu-tational requirements, model size, and de-velopment effort.I In t roduct ionIn this paper we describe an experimental ma-chine translation system based on head transducermodels and compare it to a related transfer sys-tem, described in Alshawi 1996a, based on mono-lingual head automata.
Head transducer modelsconsist of collections of finite state machines thatare associated with pairs of lexical items in a bilin-gual lexicon.
The transfer system follows the fa-miliar analysis-transfer-generation architecture (Is-abelle and Macklovitch 1986).
with mapping ofdependency representations (Hudson 1984)in thetransfer phase.
In contrast, the head transducerapproach is more closely aligned with earlier di-rect translation methods: no explicit representa-tions of the source language (interlingua or other-wise) are created in the process of deriving the targetstring.
Despite ~he simple direct architecture, thehead transducer model does embody modern prin-ciples of lexicalized recursive grammars and statis-tical language processing.
The context for evaluat-ing both the transducer and transfer models was thedevelopment of experimental prototypes for speech-to-speech translation.In the case of text translation for publishing, itis reasonable to adopt economic measures of theFei XiaDepar tment  of Computer  andIn format ion  ScienceUnivers i ty  of PennsylvaniaPh i lade lph ia ,  PA 19104.
USAfxia@cis.upenn.edueffectiveness of translation systems.
This involvesassessing the total cost .,f employing a '~ransiationsystem, including, for example, the cost of manualpost-editing.
Post-editing "s not an option in speechtranslation systems for person-to-person communi-cation, and real-time operation is important in thiscontext, so in comparing the two translation modelswe looked at a variety of other measures, includingtranslation accuracy, speed, and system complexity.Both models underlying the translation systemscan be characterized as statistical translation mod-els, but unlike the models proposed by Brown etal.
(1990, 1993), these models have non-uniform lin-guistically motivated structure, at present coded byhand.
In fact, the original motivation for the headtransducer models was that they are simpler andmore amenable to automatic model structure acqui-sition, while the transfer component of the tradi-tional system was designed with regard to allowingmaximum flexibility in mapping between source andtarget representations to overcome translation diver-gences (Lindop and Tsujii 1991: Dorr 1994).
In prac-tice, it turned out that adopting the simpler trans-ducer models did not invoive sacrificing accuracy, atleast for our limited domain application.We first describe the transfer and head transducerapproaches in Sections 2 and 3 and the method usedto assign the numerical parameters of the models inSection 4.
In Section 5. we compare experimentalsystems, based on the two approaches, for English-to-Chinese translation of air travel enquiries, and weconclude in Section 6.2 Mono l ingua l  Automata  andTrans ferIn this section we review the approach based ollmonolingual head automata together with transfermapping.
Further details of this approach, includ-ing the analysis, transfer, and generation algorithmsappear in Alshawi 1996a.3602.1 Mono l ingua l  Relat ional  ModelsWe can characterize the language models used foranalysis and generation in the transfer system asquantitative generative models of ordered depen-dency trees.
In the dependency trees generated bythese models, each node is labeled with a word wfrom the vocabulary V of the language in question:the nodes (and their word labels) immediately dom-inated by such a node are the dependents of w inthe dependency derivation.
Dependency tree arcsare labeled with symbols taken from a set R of de-pendency rei~iorss.
These monolingual models arereversible, in the sense they can be used for analy-sis or generation.
The motivation for these models issimilar to that for Probabilistic Link Grammar (Laf-ferry, Sleator, and Temperley 1992).
one differencebeing that the head automata derivations are alwaystrees.The models are quantitative in that they assign areal-number cost to derivations.
Various cost func-tions are possible, though in the experiments re-ported in this paper, a discriminative cost functionis used, as discussed in Section 4.
In the monolin-gual models, derivation events are actions performedby relational head acceptors, a particular type of fi-nite state automata ssociated with each word in thelanguage.A relational head acceptor writes (or accepts) apair of symbol sequences, a left sequence and a rightsequence.
The symbols in these sequences are takenfrom the set R of dependency relations.
In a de-pendency derivation, an acceptor is associated witha node with word w, and the sequences written bythe acceptor correspond to the relation labels of thearcs to the left and right of the node.
In other words,they are the dependency relations between w and thedependents of w to its left and right.
The possibleactions taken by a relational head acceptor m. ins ta te  qi are:?
Left transition: write a symbol r onto the rightend of the left sequence and eater state qi+l.?
Right transition: write a symbol r onto the leftend of the right sequence and enter state qi+l.?
Stop: stop in state q, at which point the se-quences are considered complete.Derivation of ordered dependency trees proceedsrecursively by generating the dependent relations fora node according to the word and acceptor at thatnode, and then generating the trees dominated bythese relation edges.
This process involves the fol-lowing actions in addition to the acceptor actionsabove:) Selection of a word and acceptor to start anentire derivation.?
Selection of a dependent word and acceptorgiven a head word and a dependency relation.2.2 TransferTransfer in this model is a mapping between un-ordered dependency trees.
Surface ordering of de-pendent phrases of either the source or target is nottaken into account in the transfer mapping.
This or-dering is completely defined by the source and targetmonolingual models.Our transfer model involves a bilingual lexiconspecifying paired source-target fragments of depen-dency trees.
A bilingual iexical entry (see Alshawi1996a for more details) includes a mapping functionbetween the source and target nodes of the frag-ments.
Valid transfer mappings are defined in termsof a tiling of the source dependency tree with sourcefragments from bilingual exicon entries so that thepartial mappings defined in entries are extended toa mapping for the entire source tree.
This tiling pro-cess has the side effect of creating an unordered tar-get dependency representation.
The following non-deterministic actions are involved in the tiling pro-cess:?
Selection of a bilingual entry given a source lan-guage word, w.?
Matching the nodes and arcs of the source frag-ment of an entry against a local subgraph in-cluding a node labeled by w.3 B i l ingua l  Head Transduct ion3.1 Bi l ingual  Head TransducersA head transducer is a transduction version of thefinite state head acceptors employed in the transfermodel.
Such a transducer M is associated with apair of words, a source word w and a target wordt,.
In fact.
w is taken from the set ~,~ consisting ofthe source language vocabulary augmented by the"'empty word" e, and t, is taken from !,~, the tar-get language vocabulary augmented with e. A headtransducer eads from a pair of source sequences, aleft source sequence Lt and a right source sequenceRI; it writes to a pair of target sequences, a lefttarget sequence L.~ and a right target sequence R,(Figure 1).Head transducers were introduced in Alshawi1996b, where the symbols in the source and targetsequences are source and target words respectively.In the experiment described in this paper the sym-bols written are dependency relation symbols or the361l ?11 1 L., r~ r~ ~ r~ R~ ?
.
.
r j +  t ?
.
."
\[ 'Figure 1: Head transducer M converts the sequencesof left and right relations (r~ ... r~) and ( r~+l .
.
.
rn  1)of w into left and right relations ( r~.
.
.
r \ ] )  andempty symbol e. While it is possible to construct atranslator based on head transduction models with-out relation symbols, using a version of head trans-ducers with relation symbols allowed for a more di-rect comparison between the transfer and transducersystems, as discussed in Section 5We can think of the transducer as simultaneouslyderiving the source and target sequences through aseries of transitions followed by a stop action.
Froma state qi these actions are as follows:?
Left transition: write a symbol rl onto the rightend of L1, write symbol r2 to position a in thetarget sequences, and enter state qi+l.
* Right transition: write a symbol rl onto the leftend of R1, write a symbol r~ to position a inthe target sequences, and enter state qi+t.. Stop: stop in state qi, at which point the se-quences Lt, R1, L~ and R,.
are considered com-plete.In simple head transducers, the target positionsa can be restricted in a similar way to the sourcepositions, i.e., the right end of L~ or the left end ofR.~.
The version used in the experiment allows ad-ditional positions, including the left end of L2 andthe right end R~..
Allowing additional target posi-tions increases the flexibility of transducers in thetranslation application without an adverse ffect oncomputational complexity?
On the other hand, werestrict the source side positions as indicated aboveto keep the transduction search similar in nature tohead-outward context free parsing.3.2 Recurs ive  Head Transduct ionWe can apply a set of head transducers recursivelyto derive a pair of source-target ordered dependencytrees?
This is a recursive process in which the depen-dency relations for corresponding nodes in the twotrees are derived by a head transducer.
In additionto the actions performed by the head transducers.this derivation process involves the actions:Selection of a pair of words wo E V1 and vo E V2,and a head transducer 3,10 to start the entirederivation.Selection of a pair of dependent words w I andv ~ and transducer M I given head words w and vand source and target dependency relations eland r2.
(w,w' E V1; v,v' e V2.
)The recursion takes place by running a head trans-ducer (M' in the second action above) to derive localdependency trees for corresponding pairs of depen-dent words (w', v').4 Event Cost AssignmentThe transfer and head transduction derivation mod-els can be formulated as probabilistic generativemodels; such formulations were given in Alshawi1996a and 1996b respectively.
Under such a for-mulation, negated log probabilities can be used asthe costs for the actions listed in Sections 2 and 3.However, experimentation reported in Alshawi andBuchsbaum 1997 suggests that improved translationaccuracy can be achieved by adopting cost functionsother than log probability.
This is true in particularfor a family of discriminative cost functions.We define a cost function f as a real valued func-tion taking two arguments, a event e and a contextc.
The context c is an equivalence class of states un-der which an action is taken, and the event e is anequivalence class of actions possible from that set ofstates.
We write the value of the function as f(elc ),borrowing notation from the special case of condi-tional probabilities.
The pair (elc) is referred to as achoice.
The cost of a solution (i.e., a possible trans-lation of an input string) is the sum of costs for allchoices in the derivation of that solution.Discriminative cost functions, including likelihoodratios (cf.
Dunning 1993), make use of both positiveand negative instances of performing a task.
Herewe take a positive instance to be the derivation ofa "'correct" translation, and a negative instance thederivation of an "incorrect" translation, where cor-rectness is judged by a speaker of both languages.Let n + (e\]c) be the count of taking choice (elc) in pos-itive instances resulting from processing the sourcesentences in a training corpus.
Similarly, let n-(elc )be the count of taking (elc) for negative instances.362The cost function" used in the experiments i com-puted as:/(elc) = log(n+(el c) + n-(elc)) -log(n+(ele)).
(By comparison, the usual "logprob" cost functionusing only positive instances would be log(n+(c)) -log(n+(elc)).)
For unseen choices, we replace thecontext c and event e with larger equivalence classes.5 Effectiveness Comparison5.1 Engl ish-Chinese ATIS ModelsBoth the transfer and transducer systems weretrained and evaluated on English-to-Mandarin Chi-nese translation of transcribed utterances from theATIS corpus (Hirschman et al 1993).
By train-ing here we simply mean assignment of the costfunctions for fixed model structures.
These modelstructures were coded by hand as monolingual headacceptor and bilingual dependency lexicons for thetransfer system and a head transducer lexicon forthe transducer system.Positive and negative counts for cost assignmentwere collected from two sources for both systems andan additional third source for the transfer system.The first set of counts was derived by processingtraces using around 1200 sample utterances fromthe ATIS corpus.
This involved running the sys-tems on the sample utterances, tarting initially withuniform costs, and presenting the resulting trans-lations to a human judge for classification as cor-rect or incorrect.
The second source of counts washand-tagging around 800 utterance transcriptionsto identify correct and incorrect attachment pointsfor prepositional phrases, PP-attachment being im-portant for English-Chinese translation (Chen andChen 1992).
This attachment information was con-verted to corresponding counts for head-dependentchoices involving prepositional phrase attachment.The additional source of counts used in the trans-fer system was an unsupervised training methodin which 13000 training utterances were translatedfrom English to Chinese, and then back again; thederivations were classified as positive (otherwise neg-ative) if the resulting back-translation was suffi-ciently close to the original English, as described inAlshawi and Buchsbaum 1997.There was a strong systematic relationship be-tween the structure of the models used in the twosystems in the following sense.
The head transducerswere built by modifying the English head acceptorsdefined for the transfer system.
This involved theaddition of target relations, including some epsilonrelations, to automaton transitions.
In some cases,Transfer Head TransducerWord error rate 16.2 11.7(per cent)Time 1.09 0.17(seconds/sent.
)Space 1.67 0.14(Mbytes/sent.
)Table 1: Accuracy.
time, and space comparisonthe automata needed to be modified to include addi-tional states, and also some transitions with epsilonrelations on the English (source) side.
Typically,such cases arise when an additional particle needsto be generated on the target side, for example theyes-no question particle in Chinese.
The inclusion ofsuch particles often depended on additional distinc-tions not present in the original English automata.hence the requirement for additional states in thebilingual transducer versions.5.2 Per fo rmanceTo evaluate the relative performance of the twotranslators, 200 utterances were chosen at randomfrom a previously unseen test sample of ATIS utter-ances having no overlap with samples used in modelbuilding and cost assignment.
There was no restric-tion on utterance length or ATIS "class" (dialogue orone-off queries, etc.)
in making this selection.
TheseEnglish test utterances were processed by both sys-tems, yielding lowest cost Chinese translations.Three measures of performance--accuracy, com-putation time, and memory usage--were compared,with the results in Table 1, showing improvementsby the transducer system for all three measures.
Theaccuracy figures are given in terms of translationword error rate, a measure we believe to be some-what less subjective than sentence level measures ofgrammaticality and meaning preservation.
Trans-lation word error rate is defined as the number ofwords in the source which are judged to have beenmistranslated.
For the purposes of this definition,mistranslation of a source word includes choice ofthe wrong target word (or words), the absence (orincorrect addition) of a particle related to the word,and the generation of a correct target word in thewrong position.The improvement in word error rates of the trans-ducer system was achieved without the benefit of theadditional counts from unsupervised training, men-tioned above, with 13,000 utterances.
Earlier experi-ments (Alshawi and Buschbaum 1997) show that theunsupervised training does lead to an improvement363in the performance of the transfer system.
How-ever, this improvement is relatively small: around2% reduction in the number of utterances contain-ing translation errors.
(Word error rates for directcomparison with the results above are not available.
)We also know that some additional improvement ofthe transducer system can be achieved by increasingthe amount of training data: with a further 600 su-pervised training samples (for a total of 1800), theerror rate for the transducer system falls to 11.0%.The processing times reported above are averagesover the same 200 test utterances used in the accu-racy evaluation.
These timings are for an implemen-tation of the search algorithms in Lisp on a SiliconGraphics machine with a 150MHz R4400 processor.The space figures give the average amount of mem-ory allocated in processing each utterance.5.3 Mode l  Size and  Deve lopment  EffortThe performance comparison above is, of course, notthe whole story, particularly since manual effort wasrequired to build the model structures before train-ing for cost assignment.
However, we believe theconclusion for the improvement in performance ofthe transducer system is valid because the amountof effort in building and training the transfer modelsexceeded that for the the transducer systems.
Afterconstruction of the English head acceptor models,common to both systems, a rough estimate of theeffort required for completing the models for Englishto Chinese translation is 12 person-months for thetransfer system and 3 person-months for the trans-ducer system.
With respect to training effort, asnoted, the amount of supervised training effort inthe main experiment was the same for both systems(supervised iscriminative training for 1200 utter-auces plus tagging of prepositional attachments for800 utterances), while the transfer system also ben-efited from unsupervised training with 13000 utter-ances.In comparing models for language processing, orindeed other tasks, it is reasonable to ask if per-formance improvements by one model over anotherwere achieved through an increase in model complex-ity.
We looked at three measures of model complex-ity for the two systems, with the results shown inTable 2.
The first was the number of lexical entries.For the transfer model this includes both monolin-gual entries and the bilingual entries required for theEnglish to Chinese direction; there are only bilin-gual entries in the transducer model.
Comparing thestructural complexity of the two models is somewhatmore difficult but we can make a graph-theoretic ab-straction and count the number of edges in modelTransfer Head TransducerLexical entries 3,250 1,201Edges 72,180 47,910Choices 100,472 67,011Table 2: Lexicon and model size comparisoncomponents.
Both systems include edges for au-tomaton state transitions.
The edge count for thetransfer system includes the number of dependencygraph edges in bilingual entries.
Finally, we alsolooked at the number of choices for which train-ing counts were available, i.e., the number of modelnumerical parameters for which direct evidence waspresent in training data.
As can be seen from Ta-ble 2, the transducer system has a lower model com-plexity according to all three measures.6 Conc lus ionThere are many aspects to the effectiveness of thetranslation component of a speech translator, mak-ing comparisons between systems difficult.
There isalso an inherent difficulty in evaluating the transla-tion task: a single source utterance has many validtranslations and the validity of translations i a mat-ter of degree.
Despite this, we believe that in thecomparison considered in this paper, it is reason-able to make an overall assessment that the headtransducer system is more effective that the transfer-based system.
One justification for this conclusionis that the systems were closely related, having iden-tical sublanguage domain and test data, and usingsimilar automata for analysis in the transfer systemand transduction i  the transducer system.
Anotherjustification is that it was not necessary to makedifficult comparisons between different aspects of ef-fectiveness: the transducer system performed betterwith respect to all the measures we looked at foraccuracy, speed, memory, development effort andmodel complexity.
Looking forward, the relativesimplicity of head transducer models makes themmore promising for further automating the develop-ment of translation applications.AcknowledgmentWe are grateful to Jishen He for building the Chinesemodel and bilingual lexicon of the earlier transfersystem that we used in this work for comparisonwith the head transducer system.364ReferencesAlshawi, H. and A.L.
Buchsbaum.
1997.
"State-Transition Cost Functions and an Application toLanguage Translation".
In Proceedings of the In-ternational Conference on Acoustics, Speech, andSignal Processing, IEEE, Munich, Germany.Alshawi, H. 1996a.
"Head Automata and Bilin-gual Tiling: Translation with Minimal Represen-tations".
In Proceedings ofthe 34th Annual Meet-ing of the Association for Computational Linguis-tics, Santa Cruz, California, 167-176.Alshawi, H. 1996b.
"Head Automata for SpeechTranslation".
In Proceedings of the Interna-tional Conference on Spoken Language Processing,Philadelphia, Pennsylvania.Brown, P., J. Cocke, S. Della Pietra, V. Della Pietra,F.
Jelinek, J. Lafferty, R. Mercer and P. Rossin.1990.
"A Statistical Approach to Machine Trans-lation".
Computational Linguistics 16:79-85.Brown, P.F., S.A. Della Pietra, V.J.
Della Pietra,and R.L.
Mercer.
1993.
"The Mathematics ofStatistical Machine Translation: Parameter Esti-mation".
Computational Linguistics 19:263-312.Chen, K.H.
and H. H. Chen.
1992.
"Attachment andTransfer of Prepositional Phrases with ConstraintPropagation".
Computer Processing of Chineseand Oriental Languages, Vol.
6, No.
2, 123-142.Dorr, B.J.
1994.
"Machine Translation Divergences:A Formal Description and Proposed Solution".Computational Linguistics 20:597-634.Dunning, T. 1993.
"Accurate Methods for Statis-tics of Surprise and Coincidence."
ComputationalLinguistics 19:61-74.Hudson, R.A. 1984.
Word Grammar.
Blackwell,Oxford.Hirschman, L., M. Bates, D. Dahl, W. Fisher, J.Garofolo, D. Pallett, K. Hunicke-Smith, P. Price,A.
Rudnicky, and E. Tzoukermann.
1993.
"Multi-Site Data Collection and Evaluation in SpokenLanguage Understanding".
In Proceedings of theHuman Language Technology Workshop, MorganKaufmann, San Francisco, 19-24.Isabelle, P. and E. Macklovitch.
1986.
"Transferand MT Modularity", In Eleventh InternationalConference on Computational Linguistics, Bonn,Germany, 115-117.Jelinek, F., R.L.
Mercer and S. Roukos.
1992.
"Principles of Lexical Language Modeling forSpeech Recognition".
In S. Furui and M.M.Sondhi (eds.
), Advances in Speech Signal Process-ing, Marcel Dekker, New York.Lafferty, J., D. Sleator and D. Temperley.
1992.
"Grammatical Trigrams: A Probabilistic Model ofLink Grammar".
In Proceedings ofthe 1992 AAAIFall Symposium on Probabilistic Approaches toNatural Language, 89-97.Kay, M. 1989.
"Head Driven Parsing".
In Pro-ceedings of the Workshop on Parsing Technolo-gies, Pittsburgh, 1989.Lindop, J, and J. Tsujii.
1991.
"Complex Transferin MT: A Survey of Examples".
Technical Re-port 91/5, Centre for Computational Linguistics,UMIST, Manchester, UK.Sata, G. and O.
Stock.
1989.
"Heacl-Driven Bidirec-tional Parsing".
In Proceedings of the Workshopon Parsing Technologies, Pittsburgh.Younger, D. 1967.
Recognition and Parsing ofContext-Free Languages in Time n 3.
Informationand Control, 10, 189-208.365
