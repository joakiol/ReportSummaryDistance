Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 264?273,Honolulu, October 2008. c?2008 Association for Computational LinguisticsHotSpots: Visualizing Edits to a TextSrinivas BangaloreAT&T Labs ?
Research180 Park AveFlorham Park, NJ 07932srini@research.att.comDavid SmithAT&T Labs ?
Research180 Park AveFlorham Park, NJ 07932dsmith@research.att.comAbstractCompared to the telephone, email based cus-tomer care is increasingly becoming the pre-ferred channel of communication for corpora-tions and customers.
Most email-based cus-tomer care management systems provide amethod to include template texts in order to re-duce the handling time for a customer?s email.The text in a template is suitably modifiedinto a response by a customer care agent.
Inthis paper, we present two techniques to im-prove the effectiveness of a template by pro-viding tools for the template authors.
First,we present a tool to track and visualize the ed-its made by agents to a template which servesas a vital feedback to the template authors.Second, we present a novel method that au-tomatically extracts potential templates fromresponses authored by agents.
These meth-ods are investigated in the context of an emailcustomer care analysis tool that handles over amillion emails a year.1 IntroductionEmail based customer care is increasingly becom-ing the preferred channel of communication for cor-porations and customers compared to the conven-tional telephone-based customer care.
For cus-tomers, email channel offers several advantages ?there are no tedious menus to navigate, there is nowaiting time to reach an operator, the request canbe formulated at the customer?s pace and additionalmaterial supporting the case can be attached to theemail.
There is also a record of the service re-quest for the customer unlike the telephone-basedcustomer care.
However, there are also limitationsof the email channel.
The most significant one isthat the customer-agent interaction could be drawnout over successive emails spanning over severaldays as opposed to being resolved in one or twotelephone calls.
For corporations, the asynchronousnature of email-based customer care offers signifi-cant opportunities to reduce operations cost by ef-fective load balancing compared to telephone-basedcustomer care.
It is quite common for an email cus-tomer care agent to work on several cases simulta-neously over a period of a few hours.
Email chan-nel also offers higher bandwidth for corporations tosend additional information in the form of web links,images and video or audio instructions.The effectiveness of customer care in the emailchannel is measured using two competing metrics:Average Handling Time (AHT) and Customer Ex-perience Evaluation (CEE).
AHT measures the timetaken from when a customer email is opened to thetime when the response is sent out.
This time is typ-ically averaged over a period of a week or a monthfor reporting purposes.
CEE measures customer sat-isfaction through a survey of a random subset of cus-tomers who have interacted with the email customercare center.
These surveys typically involve qual-itative and quantitative questions and measure thequality of the interactions along a number of differ-ent dimensions.
As is the case in many surveys thepopulation responding to such questionnaires is typ-ically small and very often quite biased.
We do notuse the CEE metric for the work we report in thispaper.As is evident from the definitions of AHT andCEE, it is in the interest of a corporation to minimizeAHT while maximizing CEE.
In order to reduceAHT, most email customer care systems (Kana,2008; Genesys, 2008) provide a mechanism for anagent to respond to a customer?s email by selectinga predefined template text that can be quickly cus-tomized to serve as the response.
The template textis usually associated with a problem category it is in-tended to address and might even be suggested to theagent automatically using classification techniques264applied to the customer?s email.
Once the templateis selected, the agent edits the template text to per-sonalize as well as add case specific details as partof composing a response.
Each of the text edits con-tributes to the handling time of the email.
Hence,it is in the interest of the template designer to mini-mize the number of edits of the template in order tolower AHT.Although most email management systems pro-vide a mechanism to author the template text, thereis typically no mechanism to monitor and track howthese templates are modified by the agents whenthey compose a response.
This information is vitalto the template authors when creating new versionsof the templates that reduce the number of edits andconsequently reduce AHT.In this paper, we present two methods for improv-ing the templates in a principled manner.
After de-scribing the related work in Section 2, we present abrief description of the email tracking tool we havedeveloped in Section 3.
In Section 4, we presenta tool called HotSpots that helps visualize the editsbeing made by the customer care agents to the tem-plates.
This tool provides a visual feedback to thetemplate authors and suggests means of improvingthe template text based on the edits made by agents.In Section 5, we present a new approach to automat-ically identify emerging templates ?
texts that arerepeatedly created by agents and are similar to eachother but distinct from the current template text.
Weuse AHT as the metric to minimize for automaticidentification of emerging templates.
We discusssome of the issues concerning this work in Section 6and conclude in Section 7.2 Related WorkThere are few threads of research that are relevant tothe work presented in this paper.
First, the topic ofemail response generation in the context of customercare has been investigated by (Coch, 1996; Lapalmeand Kosseim, 2003; Zukerman and Marom, 2007).In (Coch, 1996), the authors model multi-sentencegeneration of response letters to customer com-plaints in French.
The generation model is carefullycrafted for the domain using domain-specific rulesfor conceptual planning, rhetorical relations and sur-face word order operators.
They show that theirapproach performs better than predefined templatesand slightly worse than human generated responses.In (Lapalme and Kosseim, 2003), the authors ex-plore three different approaches based on classifica-tion, case-based reasoning and question-answeringto compose responses to queries in an email cus-tomer care application for the telecommunication in-dustry.
The case-based reasoning approach is themost similar to the template approach we follow.
In(Zukerman and Marom, 2007), the authors investi-gate an approach to assembling a response by firstpredicting the clusters of sentences to be included inthe response text and then applying multi-documentsummarization techniques to collate the representa-tive sentences into a single response.
In contrast, inthis paper, due to constraints from the deploymentenvironment, we rely on a template-based approachto response generation.
We focus on providing toolsfor investigating how the templates are modified andsuggest techniques for evolving more effective tem-plates based on quantitative criteria.Another thread of relevant research are methodsfor visualizing texts.
There are several methods thathave been proposed to provide a visual map of a setof text documents with the focus of illustrating therelatedness of these texts (Card et al, 1999).
Us-ing a metric for comparing texts (e.g.
n-gram over-lap) , the texts are clustered and the resulting clus-ters are visualized as two or three dimensional colormaps.
These approaches are useful to depict similar-ities in a static repository of documents or the returnresults of a search query.
These maps are primar-ily designed for exploration and navigation throughthe document space.
While the underlying algorithmwe use to illustrate the text edits is similar to the oneused in text map visualizations, our focus in this pa-per is to provide a mechanism for template designersto quickly identify the variants of a template sen-tence created by the agents.A third thread is in the context of human-assisted machine translation, where a human trans-lator post-edits the output of a machine translationsystem (Foster et al, 1997; Foster et al, 2002; Ochet al, 2003).
In order to improve the efficiency ofa human translator, the k-best output of a translationsystem could be displayed as word or phrase choiceswhich are color coded based on the confidence valueassigned by the translation model.
While the ap-265proach we follow is partly motivated by the post-editing paradigm, there are significant differences inthe context we apply this approach.
In the contextof this paper, the template designer is presented asummary of the set of variants created by each agentfor each sentence of the template.
The task of thetemplate designer is to use this tool to select (or con-struct) a new variant for the template sentence withthe aim of minimizing the need for editing that sen-tence in future uses of the template.3 Email Customer CareTypically, a large email customer care managementcenter receives over 100,000 emails a month.
Thesecenters typically use a customer care managementsystem that offer not only logging and tracking ofemails but also tools for improving the efficiencyof agents responding to emails.
Usually, an incom-ing customer email is categorized into a set of fewtopics/issues.
The categorization might be done au-tomatically based on regular expressions involvingkeywords in the email or using weighted classifiersthat are trained on data.
In order for an agent to re-spond to an incoming email, these systems provide atext box which allows the agent to author a responsefrom scratch.
However, most email customer caresystems offer the ability to store a prefabricated re-sponse (also called templates), instead of agents hav-ing to author a response from scratch.
These tem-plates are typically associated with a problem cate-gory or an issue that they are intended to address.A template helps an agent compose a well-formedresponse quickly.
It contains hints for informationthat the agent should enter as well as indications ofwhere that information should be entered in the tem-plate.
The template might also contain helpful infor-mation to the customer in addition to legal verbiagethat the customer needs to be aware of.An agent receives a customer email and aftercomprehending the issues and consulting the cus-tomer records in the database, selects one of the pre-defined set of templates that best addresses the is-sues raised in the email.
Less frequently, she mighteven select more than one template to compose theresponse.
She then proceeds to edit and personal-ize the chosen templates to better suit the customer?semail.
An example of a ?generic?
template ?
not as-sociated with a specific problem category is shownin Figure 1.Greetings Contact.FirstName,Thank you for your email in regard toXXXXXXXX.I will be happy to assist you with your in-quiry.XXX BODY XXXIf I can be of any further assistance,please reply directly to this email.Thank you for using our company.We appreciate your business and contin-ued loyalty.Regards,Agent.FirstNameFigure 1: An example of a generic templateThe process of selecting an appropriate templatethat addresses the customer?s inquiries could bequite tedious when there are hundreds of templates.Email management systems offer tools that suggestappropriate template to use based on the content ofthe customer?s email.
These tools are trained usingclassification techniques on previous email interac-tions.As mentioned earlier, there are two metrics thatare typically used to measure the effectiveness andefficiency of email responses.
Customers are sur-veyed after their email interaction to assess theirlevel of satisfaction for the service they received.This is usually called the Customer ExperienceEvaluation (CEE) and includes an evaluation of thecustomer?s total interaction experience with the cor-poration, not just the last email interaction.
A smallsubset of customers who had an interaction with theemail center is randomly chosen (typically in the or-der of about 10% of customers) and are invited totake part in the follow-up survey.
Typically, onlya small percent (about 10%) of the customers whoreceive these invitations respond to the survey; ef-fectively about 1% of the total emails have customersurvey scores.A second metric that is also used to measure theefficiency of an operation is called the average han-dling time (AHT) which measures the average of266times taken by agents to respond to emails.
Thehandling time includes the time to comprehend theemail, the time for database lookup and the time forresponse composition.
It is in the interest of theemail customer care operation to minimize AHT andmaximize CEE scores.3.1 Email Customer Care Analysis ToolWe have designed and developed an Email CustomerCare Analysis Tool (ECAT) to help analyze the op-erations of the email care center.
It provides an end-to-end view from the activities involved in answer-ing emails to the results of subsequent customer caresurveys.
In addition, ECAT also provides insightsinto how the agents are editing templates as well asguides template authors in designing more effectivetemplates.ECAT is a web-based tool and offers a birds-eyesummary of the operations aggregated by region,the template used, and the customer satisfaction sur-vey results.
Using this tool, analysts can drill downthrough a series of views until they are eventuallypresented with the results of a single survey or a sin-gle email interaction.One of the most useful functions of the tool is thatit shows the extent to which agents edit the tem-plates in the process of creating responses to cus-tomer emails.
The degree to which a template isedited is based on Levenshtein string edit distancemetric (Levenshtein, 1966).
This metric measuresthe number of edits (substitution, deletion and inser-tions) of words that are needed to transform a tem-plate into a response.
The number of edits is normal-ized by the number of words in the template.
Thesemorphing scores can be viewed for a single email oraveraged per agent or per template used.
The scoresrange from 100 to 0, with 100 representing a tem-plate which hadn?t been edited at all.The tool also allows the morphing score to beviewed alongside the handling time for an email, inother words the amount of time that the agent spendsgathering data and actually composing a response.Handling time is an important metric since it is di-rectly related to cost of operating the email customercare center.
The more editing an agent does, themore time they take to respond to a customer.
So,the number of templates, their precise wording andthe ease with which agents can distinguish them ob-viously have significant influences on overall han-dling time.Beyond the confines of the email centers them-selves, the CEE is the most important elements ingauging the effectiveness of the agent.
The surveyasks customers to rate their overall satisfaction withthe email reply to their question.
Five is the high-est score which equates with ?Extremely Satisfied?while a one equals ?Extremely Dissatisfied.?
Cus-tomers are also asked to rank the email in terms ofit?s content, clarity, professionalism and the lengthof time it took to receive a reply.
The customer isalso allowed to enter some free text so that they cansay how satisfied they were, or not, with how an in-quiry or problem was dealt with.
Customers can alsosay whether they called the company using a tele-phone channel before turning to the email channel.The survey files, all of which can be accessed intheir entirety from within the ECAT tool, also con-tain information on what templates were used whenreplying to the customer.
They also tell the analystwho the replying agent was and whether this wasthe first or a subsequent email in communicationsbetween the customer and the company.The ECAT tool juxtaposes this CEE score withthe template morphing score to show correlationsbetween customer satisfaction and the degree towhich the template had been edited.
This data isgraphed so that the analyst can immediately see ifheavy editing of a template is leading to higher CEE.Heavy editing with a low customer rating couldmean that the template is not helping the agent torespond correctly to the customer.4 HotSpotsWe designed the HotSpots tool that provides in-sights to the template authors on how templates arebeing edited by the agents when creating responses.It suggests methods for improving the next versionof the template so as to reduce edits by agents andhence reduce the handling time for an email.
In thissection, we discuss the algorithm and the visualiza-tion of the information that aids template authors inimproving the efficacy of the templates.The HotSpots algorithm proceeds in two steps asshown in Algorithm 1.
The first step creates analignment between the template string and the re-267Algorithm 1 Compute HotSpots for a template Tgiven a response set R1: EdEv = ?2: T = s1s2 .
.
.
sn3: Ts = {si|1 ?
i ?
n}4: Rs = {rji |Rj ?
R, Rj = rj1rj2 .
.
.
rjmj , 1 ?
i ?mj}5: Index : {Ts ?Rs} ?
I6: Tin = Index(s1)Index(s2) .
.
.
Index(sn)7: for all R ?
R do8: R = r1r2 .
.
.
rnR9: Rin = Index(r1)Index(r2) .
.
.
Index(rnR)// compute distance with sentences as tokensand return the alignment and score10: (alignment, score) = IndDist(Tin, Rin)// for each of the sentences in T, update itsmap11: for all si ?
T do12: in = Index(si)13: if (si, ) ?
alignment then14: EdEv[in].map = EdEv[in].map ?{?delete?
}15: else // (si, rj) ?
alignment16: EdEv[in].map = EdEv[in].map ?
{rj}17: end if18: end for19: end for// Cluster the response sentences aligned foreach template sentence20: for all si ?
T do21: in = Index(si)22: Cl = KmedianCl(EdEv[in].map, ncl)23: end forAlgorithm 2KmedianCl: Compute k centroids for aset of strings S using k-median clustering algorithm1: cs = ?
// centroid of string s?s cluster2: cei = ?
// centroid of cluster i3: numcl = 0 // number of cluster created so far4: Cli = ?
// members of cluster i5: while (numcl ?
k) ?
(numcl ?
|S|) do6: if numcl = 0 then7: ce0 = argminc?S?s?SDist(c, s)8: else // select the string (s) that is farthest from itscentroid (cs)9: cenumcl = argmaxs?SDist(cs, s)10: end if// Move strings to the closest cluster and computecentroids until the set of centroids don?t change11: repeat12: for all s ?
S do13: i?
= argmin0?i?numclDist(cei, s)14: cs = cei?15: Cli?
= Cli?
?
{s}// Computed the closest cluster centroid ceito s.16: end for// Recompute the cluster centroids cei17: for all i such that 0 ?
i ?
numcl do18: cei = argminc ?
Cli?s?CliDist(c, s)19: end for20: until set of centroids does not change21: numcl = numcl + 1 // new cluster added22: end while268sponse string.
For the purposes of this paper, weconsider the alignments between the template textand the response text with sentences as tokens in-stead of a word-based alignment.
The rationalefor this tokenization is that for template develop-ers the visualization of the edits is expected to bemore meaningful when aggregated at the sentencelevel rather than at the word level.
In the secondstep, using the sentence-level alignment we computethe edit events (insertion, deletion and substitution)of the template sentences in order to create the re-sponse.
All the edits events associated with a tem-plate sentence are then clustered into k clusters andthe centroids of the k clusters are displayed as thepotential changes to that sentence.
We next describethese two steps in detail as illustrated in Algorithm1 and Algorithm 2.Given a set of responses R that agents create us-ing a template T , Algorithm 1 proceeds as follows.Each of the sentences in the template and the set ofresponses are mapped into an integer index (Line1).
The template T and each of the responses inR are split into sentences and mapped into indexsequences (Line 6 and Line 9).
The alignment be-tween the two index strings is computed in Line10.
This is a dynamic programming algorithm sim-ilar to computing Levenshtein distance between twostrings, except the cost function used to compute thematch between tokens is as shown below.From the alignment that maps si to rj , we collectthe set of response sentences associated with eachtemplate sentence (Line 13-16).
These sentences arethen clustered using k-median clustering method (il-lustrated in Algorithm 2) in Line 22.In Algorithm 2, we illustrate the method of clus-tering we use to summarize the set of sentences wehave collected for each template sentence after thealignment step.
The algorithm is similar to the k-means algorithm (Duda et al, 2001), however, giventhat we are clustering strings instead of real num-bers (as is typical in applications of k-means), we re-strict the centroid of a cluster to be one of the mem-bers of the set being clustered, hence the name k-median algorithm (Martnez-Hinarejos et al, 2003).The distance function to measure the closeness oftwo strings is instantiated to be an n-gram overlapbetween the two strings.1The algorithm iterates over three steps until thedata is partitioned into k clusters (Line 5).
The firststep (Lines 6-10) is the initialization of a centroidfor a new cluster.
Initially when the data is not parti-tioned into any cluster, the median string of the dataset is used as the initial centroid.
For subsequentiterations, the farthest point from all the centroidscomputed thus far is used as the centroid for the newcluster.
In the second step (Lines 11-16), each mem-ber of the data set is assigned to the nearest clus-ter based on its distance to that cluster?s centroid.Finally, in the third step (Lines 17-20), the clustercentroids are recomputed based on the new clustermemberships.
Steps two and three are repeated untilthere are no changes in the cluster memberships andcluster centroids.
This completes the introduction ofa new cluster for the data.For the purposes of our task, we use up to afour-gram overlap to measure distance between twostrings and use k = 5 for clustering the data.4.1 Visualizing the HotSpotsThe HotSpots page was created within the ECATtool to surgically dissect the way in which templateswere being morphed.
For a given template, as shownin Figure 2, the analyst is presented with a copy ofthe texts from the current and previous versions ofthat template.
Each sentence in the two versionsof the template are color coded to show how fre-quently the agents have changed that sentence.
Thisinvolved running the HotSpots algorithm against ap-proximately 1,000 emails per template version.
Asentence that is colored red is one that was changedin over 50% of the emails that were responded tousing that template.
An orange sentence is one thatwas edited in between 30% and 50%, green is be-tween 10% to 30% and blue is between 0% and 10%.The more often a sentence is edited the ?hotter?
thecolor.The analyst can see the typical substitutions for asentence by hovering the mouse over that sentence.The typical sentences computed as the centroids ofthe clusters created using Algorithm 2 are them-selves color coded using the same identification sys-1We have also experimented with a symmetric version ofLevenshtein distance, but we prefer the n-gram overlap scoredue to its linear run time complexity.269Figure 2: Example of two versions of a template and the edit score (Avg.
Morph.
Score) and centroids associated witheach sentence of the template.tem.
A typical sentence that occurred in over 50%of the emails is colored red.
A typical sentence thatoccurred in 30% to 50% of the emails was orangeand so on.In seeing the two versions side by side, the an-alyst can visually inspect the agents?
edits on thecurrent version of a template relative to the previ-ous version.
If the previous version of the templateis a ?hotter?
document (with more red sentences), itmeans that the changes made to the template by theauthor had led to less editing by agents thus speedingup the process of creating a customer response.
Ifthe current template looks hotter, it suggests that thechanges made to the template were increasing theagents?
edits and probably the email handling time.5 Automatic Extraction of PotentialTemplatesThe goal of the template author is to minimize thenumber of edits done to a template and thus in-directly lowering the handling time for an email.In the preceding section, we discussed a tool thataids the template authors to identify sentences wherechanges are most often made by agents to a tem-plate.
This information could be used by the tem-plate authors to create a new version of the templatethat achieve the goal.In this section, we investigate a technique that au-tomatically identifies a possible template with thepotential of directly minimizing the average han-dling time for an email.
We use the set of responsescreated by the agents using a given template and se-lect one of the responses to be generalized and storedas a new template.
The response to be convertedinto a template is chosen so as to directly minimizethe average handling time.
In essence, we seek topartition the set of responses R generated from tem-plate T into two clusters R1 and R2.
These clus-ters have centroids T (current template) and T ?
(newtemplate) such that constraint shown in 1 holds.
(?r?R1AHT (T, r) < AHT (T?, r)) ?
(?r?R2AHT (T?, r) < AHT (T, r)) (1)Now, the quantity AHT (T, r) is logged as partof the email management system and correspondsto the time taken to respond to a customer?s email.22Although typically this time includes the time to look up a270Cluster Number of Centroid (Template/Response)members1 1799 GREETINGSPHR, Thank you for your recent email.On behalf of the company, I would like to extend my sincereapology for the problems you encountered when (XXX over keywith appropriate response XXX).
It is our goal to provideexcellent customer service, and I am sorry that we did notmeet that objective.
Your input is very valuable, and we willtake your feedback into consideration.
Regards, Agent.FirstName2 206 GREETINGSPHR, Thank you for letting me know that you?ve beenunable to send an online order to upgrade your NAMEDENTITY service.Please accept my apologies for any problems this issue may have causedyou.
You?re a highly valued customer.
I understand yourconcerns and I?ll be happy to address them.
I am investigating thisissue.
I have already made a personal commitment to email youtomorrow, with the resolution.
Thank you for your patience and forchoosing the company.
We appreciate your business and continuedloyalty.
Sincerely, Agent.FirstNameTable 1: Result of clustering responses using the AHT model as the distance metric.However, we do not have access to AHT (T ?, r) forany T ?
6= T .
We propose a model to estimate this inthe next section.5.1 Modeling Average Handling TimeWe model AHT as a linear combination of sev-eral factors which we believe would influence thehandling time for an email.
These factors in-clude the length in words of the customer?s inputemail (inplen), the length in words of the template(templatelen), the length in words of the response(resplen), the total number of edits between thetemplate and the response (edit), the normalized editscore (nedit), the number of individual events ofthe edit distance ?
substitution (sub), insertion (ins),deletion (del) and identity (id), the number of block(contiguous) substitution (blksub), block insertion(blkins) and block deletion (blkdel).
Using these in-dependent variables, we fit a linear regression modelusing the AHT values for 6175 responses createdfrom one particular template (say G).
The result ofthe regression fit is shown in Equation 2 and the dataand error statistics are shown in Table 2.
It must benoted that the coefficients for the variables are notnecessarily reflective of the importance of the vari-ables, since they compensate for the different rangesin variable values.
We have also tried several differ-the customer?s account etc., we assume that time is quite similarfor all responses created from the same template.ent regression fits with fewer variables, but find thatthis fit gives us the best correlation with the data.
?AHT = 0.5314 ?
inplen?
2.7648 ?
templatelen+1.9982 ?
resplen?
0.5822 ?
edit+2900.5242 ?
nedit+4.7499 ?
id?
1.6647 ?
del?1.6021 ?
ins + 26.6704 ?
blksub?15.239 ?
blkins + 24.3931 ?
blkdel?261.6627 (2)Mean AHT 675.74 secondsMedian AHT 543 secondsMode AHT 366 secondsStandard Deviation 487.72 secondsCorrelation coefficient 0.3822Mean absolute error 320.2 secondsRoot mean squared error 450.64 secondsTotal Number of Instances 6175Table 2: Data statistics and the goodness of the regressionmodel for 6175 AHT data points.Based on the goodness statistics of the regressionfit, it is clear the AHT model could be improvedfurther.
However, we acknowledge that AHT doesnot depend solely on the editing of a template to a271response but involves several other components in-cluding the user interface, the complexity of cus-tomer?s email, the database retrieval to access thecustomer?s account and so forth.Nevertheless, we use this model to cluster a newset of 2005 responses originating from the sametemplate (G), as shown in Equation 1.
Using thek-median clustering as described earlier, we parti-tion the responses into two clusters.
We restrict thefirst cluster centroid to be the template and searchfor the best centroid for the second cluster.
The re-sults are shown in Table 1.
The centroid for clus-ter 1 with 1799 members is the template itself whilethe centroid for cluster 2 with 206 members is a re-sponse that could be suitably generalized to serve asa template.
The overall AHT for the 2005 responsesusing the template was 989.2 seconds, while the av-erage AHT for the members of cluster 1 and 2 was971.9 seconds and 1140 seconds, indicating that thetemplate had to be edited considerably to create themembers of cluster 2.6 DiscussionFor the purposes of this paper, it is assumed thatAHT is the same as or correlates well with the timeto compose a response for an email.
However, inmost cases the email care agent might have to per-form several verification, validation, and problemresolution phases by consulting the specifics of acustomer account before formulating and compos-ing a response.
The time taken for each of thesephases typically varies depending on the customer?saccount and the problem category.
Nevertheless, weassume that the times for these phases is mostly aconstant for a given problem category, and hence theresults presented in this paper need to be interpretedon a per problem category basis.A second limitation of the approach presented inthis paper is that the metric used to measure the sim-ilarity between strings (n-gram overlap) is only acrude approximation of an ideal semantic similaritymetric.
There are however other similarity metrics(e.g.
BLEU (Papineni et al, 2002)) which could beused equally well.
The purpose of this paper is to il-lustrate the possibility of analysis of responses usingone particular instantiation of the similarity metric.In spite of the several directions that this work canbe improved, the system and algorithms describedin this paper have been deployed in an operationalcustomer care center.
The qualitative feedback wehave received are extremely positive and analystshave greatly improved the efficiency of the opera-tion using this tool.7 ConclusionsIn this paper, we have presented two approaches thathelp template authors in designing effective tem-plates for email customer care agents.
In the first ap-proach, we have presented details of a graphical toolthat provides vital feedback to the template authorson how their templates are being modified by agentswhen creating responses.
The template authors canaccommodate this information when designing thenext version of the template.
We also presented anovel technique for identifying responses that canpotentially serve as templates and reduce AHT.
To-wards this end, we discussed a method to modelAHT based on the characteristics of the customer?semail, the template text and the response text.8 AcknowledgmentsWewould like to thanks Mazin Gilbert, Junlan Feng,Narendra Gupta and Wenling Hsu for the discus-sions during the course of this work.
We also thankthe members who generously offered to their sup-port to provide us with data used in this study with-out which this work would not have been possible.We thank the anonymous reviewers for their usefulsuggestions in improving the quality of this paper.ReferencesS.K.
Card, J. Mackinlay, and B. Shneiderman.
1999.Readings in Information Visualization: Using Visionto Think.
Morgan Kaufmann.J.
Coch.
1996.
Evaluating and comparing three text-production techniques.
In Proceedings of Coling-96,pages 249?254, Copenhagen, Denmark.R.O.
Duda, P.E.
Hart, and D.G.
Stork.
2001.
PatternClassification.
Wiley, New York.G.
Foster, P. Isabelle, and P. Plamondon.
1997.
Tar-get text mediated interactive machine translation.
Ma-chine Translation, 12(1):175?194.G.
Foster, P. Langlais, and G. Lampalme.
2002.
User-friendly text prediction for translators.
In EMNLP-02,pages 46?51, Philadelphia, USA.Genesys.
2008. http://www.genesys.com.
Genesys Cor-poration.272Kana.
2008. http://www.kana.com.
Kana Corporation.G.
Lapalme and L. Kosseim.
2003.
Mercure: Towardsan automatic e-mail follow-up system.
IEEE Compu-tational Intelligence Bulletin, 2(1):14?18.V.I.
Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertion and reversals.
Soviet PhysicsDoklady, 10:707?710.C.D.
Martnez-Hinarejos, A. Juan, and F. Casacuberta.2003.
Generalized k-medians clustering for strings.Lecture Notes in Computer Science, 2652/2003:502?509.F.J.
Och, R. Zens, and H. Ney.
2003.
Efficient search forinteractive statistical machine translation.
In EACL-03.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.Bleu: A method for automatic evaluation of machinetranslation.
In Proceedings of 40th Annual Meetingof the Association of Computational Linguistics, pages313?318, Philadelphia, PA, July.I.
Zukerman and Y. Marom.
2007.
Evaluation of a large-scale email response system.
In Proceedings of IJ-CAI07, Hyderabad, India.273
