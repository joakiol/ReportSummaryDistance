Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 47?55,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsAnveshan: A Framework for Analysis of Multiple Annotators?
LabelingBehaviorVikas Bhardwaj,Rebecca J. Passonneau and Ansaf Salleb-AouissiColumbia UniversityNew York, NY, USAvsb2108@columbia.edu(becky@cs|ansaf@ccls).columbia.eduNancy IdeVassar CollegePoughkeepsie, NY, USAide@cs.vassar.eduAbstractManual annotation of natural language tocapture linguistic information is essen-tial for NLP tasks involving supervisedmachine learning of semantic knowledge.Judgements of meaning can be more orless subjective, in which case instead ofa single correct label, the labels assignedmight vary among annotators based on theannotators?
knowledge, age, gender, intu-itions, background, and so on.
We intro-duce a framework ?Anveshan,?
where weinvestigate annotator behavior to find out-liers, cluster annotators by behavior, andidentify confusable labels.
We also in-vestigate the effectiveness of using trainedannotators versus a larger number of un-trained annotators on a word sense annota-tion task.
The annotation data comes froma word sense disambiguation task for pol-ysemous words, annotated by both trainedannotators and untrained annotators fromAmazon?s Mechanical turk.
Our resultsshow that Anveshan is effective in uncov-ering patterns in annotator behavior, andwe also show that trained annotators aresuperior to a larger number of untrainedannotators for this task.1 CreditsThis work was supported by a research supple-ment to the National Science Foundation CRIaward 0708952.2 IntroductionManual annotation of language data in order tocapture linguistic knowledge has become increas-ingly important for semantic and pragmatic an-notation tasks.
A very short list of a few suchtasks illustrates the range of types of annotation,in varying stages of development: predicate ar-gument structure (Palmer et al, 2005b), dialogueacts (Hu et al, 2009), discourse structure (Carboneet al, 2004), opinion (Wiebe and Cardie, 2005),emotion (Alm et al, 2005).
The number of ef-forts to create corpus resources that include man-ual annotations has also been growing.
A commonapproach in assessing the resulting manual anno-tations is to report a single quantitative measurereflecting the quality of the annotations, either asummary statistic such as percent agreement, oran agreement coefficient from the family of met-rics that include Krippendorff?s alpha (Krippen-dorff, 1980) and Cohen?s kappa (Cohen, 1960).We present some new assessment methods to usein combination with an agreement coefficient forunderstanding annotator behavior when there aremultiple annotators and many annotation values.Anveshan (Annotation Variance Estimation)1 isa suite of procedures for analyzing patterns ofagreement and disagreement among annotators,as well as the distributions of annotation valuesacross annotators.
Anveshan thus makes it pos-sible to explore annotator behavior in more detail.Currently, it includes three types of analysis: inter-annotator agreement (IA) among all subsets of an-notators, leverage of annotation values for outlierdetection, and metrics for comparing annotators?distributions of annotation values (e.g., Kullbach-Liebler divergence).As an illustration of the utility of Anveshan, wecompare two groups of annotators on the same an-notation word sense annotation tasks: a half dozentrained annotators and fourteen Mechanical Turk-ers.
Previous work has argued that it can be costeffective to collect multiple labels from untrainedlabelers at a low cost per label, and to combinethe multiple labels through a voting method, ratherthan to collect single labels from highly trained la-1Anveshan is a Sanskrit word which literally meanssearch or exploration.47belers (Snow et al, 2008; Sheng et al, 2008; Lamand Stork, 2003).
The tasks included in (Snow etal., 2008), for example, include word sense an-notation; in contrast to our case, where the av-erage number of senses per word is 9.5, the oneword sense annotation task had three senses.
Wefind that the same half dozen trained annotatorscan agree well or not on sense labels for poly-semous words.
When they agree less well, wefind that it is possible to distinguish between prob-lems in the labels (e.g., confusable senses) andsystematic differences of interpretation among an-notators.
When we use twice the number of Me-chanical Turkers as trained annotators for three ofour ten polysemous words, we find inconsistent re-sults.The next section of the paper presents the moti-vation for Anveshan and its relevance to the wordsense annotation task, followed by a section onrelated work.
The word sense annotation data isgiven in section 5.
Anveshan is described in thesubsequent section, followed by the results of itsapplication to the two data sets.
We discuss thecomparison of trained annotators and MechanicalTurkers, as well as differences among words, insection 7.
Section 7 concludes with a short recapof Anveshan in general, and its application to wordsense annotations in particular.3 Beyond Interannotator Agreement (IA)Assessing the reliability of an annotation typicallyaddresses the question of whether different anno-tators (effectively) assign the same annotation la-bels.
Various measures can be used to comparedifferent annotators, including agreement coeffi-cients such as Krippendorff?s alpha (Krippendorff,1980).
Extensive reviews of the properties of suchcoefficients have been presented elsewhere, e.g.,(Artstein and Poesio, 2008).
Briefly, an agree-ment produce values in the interval [-1,1] indicat-ing how much of the observed agreement is above(or below) agreement that would be predicted bychance (value of 0).
To measure reliability in thisway is to assume that for most of the instances inthe data, there is a single correct response.
Herewe present the use of reliability metrics and othermeasures for word sense annotation, and we as-sume that in some cases there may not be a singlecorrect response.
When annotators have less thanexcellent agreement, we aim to examine possiblecauses.We take word sense to be a problematic anno-tation to perform, thus requiring a deeper under-standing of the conditions under which annotatorsmight disagree.
The many reasons can only betouched on here.
For example, word senses arenot discrete, atomic units that can be delimited andenumerated.
While dictionaries and other lexicalresoures, such as WordNet (Miller et al, 1993) orthe Hector lexicon (cf.
SENSEVAL-1 (Kilgarriffand Palmer, 2000)), do provide enumerations ofthe senses for a given word, and their interrela-tions (e.g., a list of senses, a tree of senses), it iswidely agreed that this is a convenient abstraction,if for no other reason than the fact that words shiftmeanings along with the communicative needs ofthe groups of individuals who use them.
The con-text in which a word is used plays a significant rolein restricting the current sense.
As a result, it isoften argued that the best representation for wordmeaning would consist in clustering the contextsin which words are used (Kilgarriff, 1997).
Yeteven this would be insufficient because new com-munities arise, new behaviors and artifacts emergealong with them, hence new contexts of use andnew clusters.
At the same time, contexts of useand the senses that go along with them can fadeaway (cf.
the use of handbag discussed in (Kilgar-riff, 1997) pertaining to disco dancing).
Becausean enumeration of word senses is somewhat arti-ficial, annotators might disagree on word sensesbecause they disagree on the boundaries betweenone sense and another, just as professional lexi-cographers do.Apart from the artificiality of creating flat orhierarchical sense inventories, the meanings ofwords can vary in their subjectivity, due to differ-ences in the perception or experience of individu-als.
This can be true for word senses that are inher-ently relative, such as cold (as in, turn up the ther-mostat, it?s too cold in here); or that derive theirmeaning from cultural norms that may differ fromcommunity to community, such as justice; or thatchange as one grows older, e.g., whether a longtime to wait pertains to hours versus days.Despite the arguments against using word senseinventories, until they are replaced with an equallyconvenient and more representative abstraction,they are an extremely convenient computationalrepresentation.
We rely on WordNet senses, whichare presented to annotators with a gloss (defini-tion) and with example uses.
In order to better un-48derstand reasons for disagreement on senses, wecollect labels from multiple annotators.
When an-notators agree, having multiple annotators is re-dundant.
But when annotators disagree, havingmultiple annotators is necessary in order to de-termine whether the disagreement is due to noisebased on insufficiently clear sense definitions ver-sus a systematic difference between individuals,e.g., those who see a glass as half empty whereothers see it as half full.
To insure the opportu-nity to observe how varied the labeling of a singleword can be, we collect word sense annotationsfrom multiple annotators.
One potential benefit ofsuch investigation might be a better understandingof how to model word meaning.In sum, we hypothesize the following cases:?
Outliers: A small proportion of annotatorsmay assign senses in a manner that differsmarkedly from the remaining annotators.?
Confusability of senses: If multiple annota-tors assign multiple senses in an apparentlyrandom fashion, it may be that the senses arenot sufficiently distinct.?
Systematic differences among subsets of an-notators: If the same 50% of annotators al-ways pick sense X where the remaining an-notators always pick sense Y, it may be thatproperties of the annotators, such as their agecohort, account for the disagreement.4 Related WorkThere has been a decade-long community-wide ef-fort to evaluate word sense disambiguation (WSD)systems across languages in the four Senseval ef-forts (1998, 2001, 2004, and 2007, cf.
(Kilgarriff,1998; Pedersen, 2002a; Pedersen, 2002b; Palmeret al, 2005a)), with a corollary effort to investi-gate the issues pertaining to preparation of man-ually annotated gold standard corpora tagged forword senses (Palmer et al, 2005a).Differences in IA and system performanceacross part-of-speech have been examined, asin (Ng et al, 1999; Palmer et al, 2005a).
Fac-tors that have been proposed as affecting agree-ment include whether annotators are allowed to as-sign multilabels (Ve?ronis, 1998; Ide et al, 2002;Passonneau et al, 2006), the number or granu-larity of senses (Ng et al, 1999), merging of re-lated senses (Snow et al, 2007), sense similar-ity (Chugur et al, 2002), entropy (Diab, 2004;Palmer et al, 2005a), and reactions times requiredto distinguish senses (Klein and Murphy, 2002;Ide and Wilks, 2006).We anticipate that one of the ways in which thedata will be used will be to train machine learningapproaches to WSD.
Noise in labeling and the im-pact on machine learning has been discussed fromvarious perspectives.
In (Reidsma and Carletta,2008), it is argued that machine learning perfor-mance does not vary consistently with interannota-tor agreement.
Through a simulation study, the au-thors find that machine learning performance candegrade or not with lower agreement, dependingon whether the disagreement is due to noise or sys-tematic behavior.
Noise has relatively little impactcompared with systematic disagreements.
In (Pas-sonneau et al, 2008), a similar lack of correla-tion between interannotator agreement and ma-chine learning performance is found in an empiri-cal investigation.5 Word Sense Annotation Data5.1 Trained Annotator dataThe Manually Annotated Sub-Corpus (MASC)project (Ide et al, 2010) is creating a small,representative corpus of American English writtenand spoken texts drawn from the Open AmericanNational Corpus (OANC).2 The MASC corpusincludes hand-validated or manual annotationsfor a variety of linguistic phenomena.
The firstMASC release, available as of May 2010, consistsof 82K words.3 One of the goals of MASC isto support efforts to harmonize WordNet (Milleret al, 1993) and FrameNet (Ruppenhofer et al,2006), in order to bring the sense distinctions eachmakes into better alignment.We chose ten fairly frequent, moderately poly-semous words for sense tagging.
One hundred oc-currences of each word were sense annotated byfive or six trained annotators.
The ten words areshown in Table 1, the words are grouped by part ofspeech, with the number of WordNet senses, thenumber of senses used by the trained annotators(TAs), the number of annotators, and Alpha.
Wecall this the Trained annotator (TA) data.We find that interannotator agreement (IA)among half a dozen annotators varies dependingon the word.
For ten words nearly balanced with2http://www.anc.org3http://www.anc.org/MASC/Home.html49SensesWord-pos Avail.
Used Ann Alphalong-j 9 4 6 0.67fair-j 10 6 5 0.54quiet-j 6 5 6 0.49time-n 10 8 5 0.68work-n 7 7 5 0.62land-n 11 9 6 0.49show-v 12 10 5 0.46tell-v 8 8 6 0.46know-v 11 10 5 0.37say-v 11 10 6 0.37Table 1: Interannotator agreement on ten poly-semous words: three adjectives, three nouns andfour verbs among trained annotatorsrespect to part of speech, we find a range of about0.50 to 0.70 for nouns and adjectives, and about0.37 to 0.46 for verbs.
Table 1 shows the ten wordsand the alpha scores for the same five or six an-notators.
The layout of the table illustrates boththat verbs have lower agreement than adjectivesor nouns, and that within each part of speech, an-notators achieve varying levels of agreement, de-pending on the word.
The annotators, their levelof training, the number of sense choices, the anno-tation tool, and other factors remain constant fromword to word.
Thus we hypothesize that the differ-ences in IA reflect differences in the degree of sub-jectivity of the sense choices, the sense similarity,or both.
Anveshan is a data exploration frameworkto help understand the differences in the ability ofthe same annotators to agree well on sense anno-tation for some words and not others.As shown, annotators achieve respectableagreement on long, time and work, and loweragreement on the remaining words.
Verbs havelower agreement overall.Figure 1 shows WordNet senses for long in theform displayed to annotators, who used an annota-tion GUI developed in Java.
The sense number ap-pears in the first column, followed by the glosses,then sample phrases; only three senses are shown,to conserve space.
Note that annotators did not seethe WordNet synsets (sets of synonymous words)for a given sense.5.2 Mechanical Turk dataAmazon?s Mechanical Turk is a crowd-sourcingmarketplace where Human Intelligence TasksSensesWord-pos Avail.
Used Ann Alphalong-j 9 9 14 0.15fair-j 10 10 14 0.25quiet-j 6 6 15 0.08Table 2: Interannotator agreement on adjectivesamong Mechanical Turk annotators(HITs) such as sense annotation for words in asentence, can be set up and results from a largenumber of annotators (or turkers) can be obtainedquickly.
We used Mechanical Turk to obtain anno-tations from 14 annotators on the set of adjectivesto analyze IA for a larger set of untrained annota-tors.The task was set up to get 150 occurrences an-notated for each of the three adjectives: fair, longand quiet, by 14 mechanical turk annotators each.100 of these occurrences were the same as thosedone by the trained annotators.
For each word,the 150 instances were divided into 15 HITs of 10instances each.
The average submit time of a HITwas 200 seconds.
We report the IA among the Me-chanical Turk annotators using Krippendorff?s Al-pha in Table 2.
As shown, the turkers have pooragreement, particularly on long and quiet, whichis at the chance level.6 AnveshanAnveshan: Annotation Variance Estimation, isour approach to perform a more subtle analysisof inter-annotator agreement.
Anveshan uses sim-ple statistical methods to achieve the three goalsidentified in section 3: outlier detection, confus-able senses, and distinct subsets of annotators thatagree with each other.6.1 MethodThis section uses the following notation to explainAnveshan?s methodology:We assume that we have n annotators annotat-ingm senses.
The probability of annotator a usingsense si is given byPa(S = si) =count(si, a)?mj=1 count(sj , a)where, count(si, a) is number of times si wasused by a.501 primarily temporal sense; being or indicating a relatively great or greater than average duration or passage of timeor a duration as specified: ?a long life?
; ?a long boring speech?
; ?a long time?
; ?a long friendship?
;?a long game?
; ?long ago?
; ?an hour long?2 primarily spatial sense; of relatively great or greater than average spatial extension or extension as specified:?a long road?
; ?a long distance?
; ?contained many long words?
; ?ten miles long?3 of relatively great height: ?a race of long gaunt men?
(Sherwood Anderson); ?looked out the long French windows?Figure 1: Three of the WordNet senses for ?Long?Anveshan uses the Kullbach-Liebler divergence(KLD), Jensen-Shannon divergence (JSD) andLeverage to compare probability distributions.The KLD of two probability distributions P andQ is given by:KLD(P,Q) =?iP (i) logP (i)Q(i)JSD is a modified version of KLD, it is alsoknown as total divergence to the average, and isgiven by:JSD(P,Q) =12KLD(P,M) +12KLD(Q,M)whereM = (P + Q)/2We define Leverage Lev of probability distribu-tion P over Q as:Lev(P,Q) =?k|P (k) ?Q(k)|We now compute the following statistics:?
For each annotator ai, we compute Pai .?
We compute Pavg, which is (?i Pai)/n.?
We compute Lev(Pai , Pavg),?i?
Then we compute JSD(Pai , Paj ) ?
(i, j),where i, j ?
n and i 6= j?
Lastly, we compute a distance measure foreach annotator, by computing the KLD be-tween each annotator and the average ofthe remaining annotators, i.e.
we get?i,Dai = KLD(Pai , Q), where Q =(?j 6=i Paj )/(n?
1)These statistics give us a deeper understandingof annotator behavior.
Looking at the sense us-age probabilities, we can identify how frequentlysenses are used by an annotator.
We can see howmuch an annotator deviates from the average sense0?0.2?0.4?0.6?0.8?1?A107?
A101?
A103?
A102?
A105?
A108?Figure 2: Distance measure (KLD) for Annotatorsof long in TA Data0?0.2?0.4?0.6?0.8?A101?
A102?
A103?
A105?
A107?
A108?101?102?999?103?108?Figure 3: Sense Usage distribution for long by an-notators in TA Datausage distribution by looking at Leverage.
JSD be-tween two annotators gives us a measure of howclose they are to each other.
KLD of an annota-tor with the remaining annotators shows us howdifferent the annotator is from the rest.
In the fol-lowing section we show results, which illustratethe effectiveness of Anveshan in identifying use-ful patterns in the data from the trained annotators(TAs) and Mechanical Turkers (MTs).6.2 ResultsWe used Anveshan on all data from TAs and MTs.We were successful in correctly identifying out-liers on many words.
Also, analyzing the senseusage patterns and observing the JSD and KLDscores gave us useful insights on annotator differ-ences.
In the figures for this section, the six TAsare represented by their unique identifiers (A101,A102, A103, A105, A107, A108).
Word sensesare identified by adding 100 to the WordNet sense51Word Old Alpha Ann Dropped New Alphalong 0.67 1 0.80land 0.49 1 0.54know 0.377 1 0.48tell 0.45 2 0.52say 0.37 2 0.44fair 0.54 2 0.63Table 3: Increase in IA score by dropping annota-tors (TA Data)0?0.05?0.1?0.15?0.2?0.25?105?
102?
104?
103?
101?
999?
108?
106?
107?
110?
109?A102?A105?Figure 4: Sense usage patterns of annotators ?102?and ?105?
for show in TA Datanumber.
An additional ?None of the Above?
labelis represented as 999; annotators select this whenno sense applies, when the word occurs as part ofa large lexical unit (collocation) with a clearly dis-tinct meaning, or when the sentence is not a cor-rect example for other reasons (e.g., wrong part ofspeech).Figure 2 shows the distance measure (KLD) foreach annotator from the rest of the annotators forthe word long with respect to the probability foreach of the four senses used (cf.
Table 1).
It canbe clearly seen that annotator A108 is an outlier.A108 differs in her excessive use of label 999, asshown in Figure 3.
Indeed, by dropping A108,we see that the IA score (Alpha) jumps from 0.67to 0.8 for long.
Similar results were obtainedfor annotations for other words as well.
Table 3shows the jump in IA score after outlier(s) weredropped.Anveshan helps us differentiate between noisydisagreement versus systematic disagreement.The word show with 5 annotators has a lowagreement score of 0.45.
By looking at thesense distributions for the various annotators,and observing annotation preferences for eachannotator, we can see that annotators A102 andA105 have similar behavior (Figure 4, with apairwise alpha of 0.52 versus 0.46 for all five0?0.05?0.1?0.15?0.2?0.25?0.3?105?
102?
104?
103?
101?
999?
108?
106?
107?
110?
109?A107?A108?Figure 5: Sense usage patterns of annotators ?107?and ?108?
for show in TA Data0?0.05?0.1?0.15?0.2?0.25?0.3?0.35?105?
102?
104?
103?
101?
999?
108?
106?
107?
110?
109?Overall?A101?Figure 6: Sense usage distribution of annotator?101?
vs. the average of all annotators for showin TA Dataannotators), and annotators A107 and A108 havesimilar behavior (Figure 5, with a pairwise alphaof 0.53).
In contrast, Annotator A101 has verydistinct preferences (Figure 6).
This behavioris captured by computing JSD scores among allpairs of annotators.
As can be seen in Figure 7,the pairs A102-A105 and A107-A108 have verylow JSD values, indicating similarity in annotatorbehavior.
At the same time we also see the pairshaving A101 in them have a much higher JSDscore, which is attributed to the fact that A101is different from everyone else.
If we look atcorresponding Alpha scores, we see that pairshaving low JSD values have higher agreementscores and vice versa.Observing the sense usage distributions alsohelps us identify confusable senses.
For example,Figure 8 shows us the differences in sense usagepatterns of A101, A103 and the average of allannotators for the word say.
We can see thatA101 and A103 deviate in distinct ways from theaverage.
A101 prefers sense 101 whereas A103prefers sense 102.
This indicates that sense 101and 102 might be confusable.
Sense 1 is givenas ?expressing words?
; sense 2 as ?report ormaintain?.520?0.1?0.2?0.3?0.4?0.5?0.6?A105?
A108?
A105?
A102?
A107?
A108?A102?
A107?
A101?
A101?
A101?
A101?JSD?Alpha?Figure 7: JSD and Alpha scores for pairs of anno-tators for show in TA Data0?0.1?0.2?0.3?0.4?0.5?0.6?Overall?
A101?
A103?102?101?108?103?Figure 8: Sense usage distribution for say in TAData for annotators ?101?
and ?103?0?0.05?0.1?0.15?0.2?0.25?0.3?0.35?A102?
A105?
A108?
A101?
A107?Figure 9: Distance measure (KLD) for annotatorsof work in TA Data0?0.2?0.4?0.6?0.8?1?1.2?Overall???A101???A102???A104???A107???A108???A111???A112???A115???A116???A117???A118???A119???A120??
?A121?106?109?999?108?103?102?101?Figure 10: Sense usage distribution among MTsfor long0?0.2?0.4?0.6?0.8?1?1.2?A101-??TA?A102-??TA?A103-??TA?A104-??TA?A105-??TA?A102-??MT?A106-??MT?A107-??MT?A108-??MT?A114-?
?MT?105?999?102?101?Figure 11: Sense usage distribution among TAsand MTs for fairAnveshan not only helps us understand under-lying patterns in annotator behavior and removenoise from IA scores, but also helps identifycases where there is no noise and no systematicsubsets of annotators that agree with each other.An example can be seen in for the noun work.
Weobserved that the annotators do not have largelydifferent behavior, which is reflected in Figure 9.As none of the annotators are significantly differ-ent from the others, the KLD scores are low andthe plotted line does not have any steep rises, asseen in Figure 2.Similar to the results for TA data, Anveshanwas successful in identifying outliers in Mechan-ical Turk data as well.
In order to compare theagreement among TAs and MTs, we looked at IAscores of all subsets of annotators for the three ad-jectives in the Mechanical Turk data.
We observedthat MTs used much more senses than TAs for allwords and that there was a lot of noise in sense us-age distribution.
Figure 10 illustrates the sense us-age statistics for long among MTs, for frequentlyused senses.We also looked at agreement scores among allsubsets of MTs to see if there are any subsets ofannotators who agree as much as TAs, and we ob-served that for both long and quiet, there were no53subsets of MT annotators whose agreement wascomparable or greater than the same number of theTAs, however for fair, we found one set of 5 an-notators whose IA score (0.61) was greater thanthe IA score (0.54) of trained annotators.
We alsoobserved that among both these pairs of annota-tors, the frequently used senses were the same, asillustrated in Figure 11.
Still, the two groups of an-notators have sufficiently distinct sense usage thatthe overall IA for the combined set drops to 0.43.7 Conclusion and Future WorkFor annotations on a subjective task, there arecases where there is no single correct label.
Inthis paper, we presented Anveshan, an approach tostudy annotator behavior and to explore datasetswith multiple annotators, and with a large set ofannotation values.
Here we looked at data fromhalf a dozen trained annotators and fourteen un-trained Mechanical Turkers on word sense anno-tation for polysemous words.
The analysis usingAnveshan provided many insights into sources ofdisagreement among the annotators.We learn that IA Scores do not give us a com-plete picture and it is necessary to delve deeperand study annotator behavior in order to identifynoise possibly due to sense confusability, to elim-inate noise due to outliers, and to identify system-atic differences where subsets of annotators havemuch higher IA than the full set.The results from Anveshan are encouraging andthe methodology can be readily extended to studypatterns in human behavior.
We plan to extendour work by looking at JSD scores of all subsetsof annotators instead of pairs, to identify largersubsets of annotators who have similar behavior.We also plan to investigate other statistical meth-ods of outlier detection such as the orthogonalizedGnanadesikan-Kettenring estimator.ReferencesCecilia Ovesdotter Alm, Dan Roth, and RichardSproat.
2005.
Emotions from text: machine learn-ing for text-based emotion prediction.
In HLT ?05:Proceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, pages 579?586, Morristown, NJ,USA.
Association for Computational Linguistics.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Marco Carbone, Yaakov Gal, Stuart Shieber, and Bar-bara Grosz.
2004.
Unifying annotated discourse hi-erarchies to create a gold standard.
In Proceedingsof the 5th Sigdial Workshop on Discourse and Dia-logue.Irina Chugur, Julio Gonzalo, and Felisa Verdejo.
2002.Polysemy and sense proximity in the senseval-2 testsuite.
In Proceedings of the SIGLEX/SENSEVALWorkshop on Word Sense Disambiguation: Re-cent Successes and Future Directions, pages 32?39,Philadelphia.Jacob Cohen.
1960.
A coeffiecient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20:37?46.Mona Diab.
2004.
Relieving the data acquisition bot-tleneck in word sense disambiguation.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, pages 303?311.Jun Hu, Rebecca J. Passonneau, and Owen Rambow.2009.
Contrasting the interaction structure of anemail and a telephone corpus: A machine learningapproach to annotation of dialogue function units.In Proceedings of the 10th SIGDIAL on Dialogueand Discourse.Nancy Ide and Yorick Wilks.
2006.
Making senseabout sense.
In E. Agirre and P. Edmonds, editors,Word Sense Disambiguation: Algorithms and Appli-cations, pages 47?74, Dordrecht, The Netherlands.Springer.Nancy Ide, Tomaz Erjavec, and Dan Tufis.
2002.Sense discrimination with parallel corpora.
In Pro-ceedings of ACL?02 Workshop on Word Sense Dis-ambiguation: Recent Successes and Future Direc-tions, pages 54?60, Philadelphia.Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-becca Passonneau.
2010.
The manually annotatedsub-corpus: A community resource for and by thepeople.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, Uppsala, Sweden.Adam Kilgarriff and Martha Palmer.
2000.
Introduc-tion to the special issue on senseval.
Computers andthe Humanities, 34:1?2.Adam Kilgarriff.
1997.
I don?t believe in word senses.Computers and the Humanities, 31:91?113.Adam Kilgarriff.
1998.
SENSEVAL: An exercise inevaluating word sense disambiguation programs.
InProceedings of the First International Conferenceon Language Resources and Evaluation (LREC),pages 581?588, Granada.Devra Klein and Gregory Murphy.
2002.
Paper hasbeen my ruin: Conceptual relations of polysemouswords.
Journal of Memory and Language, 47:548?70.54Klaus Krippendorff.
1980.
Content Analysis: An In-troduction to Its Methodology.
Sage Publications,Beverly Hills, CA.Chuck P. Lam and David G. Stork.
2003.
Evaluatingclassifiers by means of test data with noisy labels.In Proceedings of the 18th International Joint Con-ference on Artificial Intelligence (IJCAI-03), pages513?518, Acapulco.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1993.
In-troduction to WordNet: An on-line lexical database(revised).
Technical Report Cognitive ScienceLaboratory (CSL) Report 43, Princeton University,Princeton.
Revised March 1993.Hwee Tou Ng, Chung Yong Lim, and Shou King Foo.1999.
A case study on inter-annotator agreement forword sense disambiguation.
In SIGLEX WorkshopOn Standardizing Lexical Resources.Martha Palmer, Hoa Trang Dang, and Christiane Fell-baum.
2005a.
Making fine-grained and coarse-grained sense distinctions.
Journal of Natural Lan-guage Engineering, 13.2:137?163.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005b.
The proposition bank: An annotated corpusof semantic roles.
Comput.
Linguist., 31(1):71?106.Rebecca J. Passonneau, Nizar Habash, and Owen Ram-bow.
2006.
Inter-annotator agreement on a mul-tilingual semantic annotation task.
In Proceedingsof the International Conference on Language Re-sources and Evaluation (LREC), pages 1951?1956,Genoa, Italy.Rebecca Passonneau, Tom Lippincott, Tae Yano, andJudith Klavans.
2008.
Relation between agreementmeasures on human labeling and machine learningperformance: results from an art history domain.
InProceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC),pages 2841?2848.Ted Pedersen.
2002a.
Assessing system agreementand instance difficulty in the lexical sample tasks ofSenseval-2.
In Proceedings of the ACL-02Workshopon Word Sense Disambiguation: Recent Successesand Future Directions, pages 40?46.Ted Pedersen.
2002b.
Evaluating the effectiveness ofensembles of decision trees in disambiguating SEN-SEVAL lexical samples.
In Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Re-cent Successes and Future Directions, pages 81?87.Dennis Reidsma and Jean Carletta.
2008.
Reliabil-ity measurement without limits.
Comput.
Linguist.,34(3):319?326.Josef Ruppenhofer, Michael Ellsworth, MiriamR.
L. Petruck, Christopher R. Johnson, andJan Scheffczyk.
2006.
Framenet ii: Ex-tended theory and practice.
Available fromhttp://framenet.icsi.berkeley.edu/index.php.Victor S. Sheng, Foster Provost, and Panagiotis G.Ipeirotis.
2008.
Get another label?
improving dataquality and data mining using multiple, noisy label-ers.
In Proceeding of the 14th ACM SIG KDD Inter-national Conference on Knowledge Discovery andData Mining, pages 614?622, Las Vegas.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2007.Learning to merge word senses.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Compu-tational Natural Language Learning, pages 1005?1014, Prague.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast - butis it good?
evaluating non-expert annotations fornatural language tasks.
In Proceedings of Em-pirical Methods in Natural Language Processing(EMNLP), pages 254?263, Honolulu.Jean Ve?ronis.
1998.
A study of polysemy judgementsand inter-annotator agreement.
In SENSEVAL Work-shop, pages Sussex, England.Janyce Wiebe and Claire Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.language resources and evaluation.
In LanguageResources and Evaluation (formerly Computers andthe Humanities, page 2005.55
