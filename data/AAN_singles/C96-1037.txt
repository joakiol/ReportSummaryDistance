Aligning More Words with High Precision for Small Bilingual CorporaSur-Jin KerDepartment ofComputer ScienceNational Tsing Hua UniversityHsinchu, Taiwan, ROC 30043j schang@cs.nthu.edu.twJason J. S. ChangDepartment ofComputer ScienceNational Tsing tlua UniversityHsinchu, Taiwan, ROC 30043jschang@cs.nthu.edu.twAbstractIn this paper, we propose an algorithm foraligning words with their translation in abilingual corpus.
Conventional algorithms arebased on word-by-word models which requirebilingual data with hundreds of thousandsentences for training.
By using a word-basedapproach, less frequent words or words withdiverse translations generally do not havestatistically significant evidence for confidentalignment.
Consequently, incomplete orincorrect alignments occur.
Our algorithmattempts to handle the problem using class-based rules which are automatic acquired frombilingual materials uch as a bilingual corpus ormachine readable dictionary.
The procedures foracquiring these rules is also described.
We foundthat the algorithm can align over 80% of wordpairs while maintaining a comparably highprecision rate, even when a small corpus wasused in .training.
The algorithm also poses theadvantage of producing a tagged corpus forword sense disambiguation.1.
IntroductionBrown et al (1990) initiated much of the recentinterest in bilingual corpora.
They advocated applyinga statistical approach to machine translation (SMT).The SMT approach can be understood as a word byword model consisting of two submodels: a languagemodel for generating a source text segment ST and atranslation model for translating ST to a target extsegment TT.
They recommended using an alignedbilingual corpus to estimate the parameters oftranslation probability, Pr(ST \[TT) in the translationmodel.
The resolution of alignment can vat3, from lowto high: section, paragraph, sentence, phrase, andword (Gale and Church 1993; Matsumoto et al 1993).In addition to machine translation, manyapplications tbr aligped corpora have been proposed,including bilingual lexicography (Gale and Church199l, Smadja 1992, Dallie, Gaussier and Lange 1994),and word-sense disambiguation (Gale, Church andYarowsky 1992, Chen and Chang 1994).In the context of statistical machine translation,Brown et al (1993) presented a series of five modelsfor Pr(ST \[TT).
The first two models have been usedin research on word alignment.
Model 1 assumes thatPr(ST\[TT) depends only on lexical translationprobability t(s I t), i.e., the probability of the i-th wordin ST producing the j-th word t in TT as its translation.The pair of words (s, t) is called a connection.
Model2 enhances Model 1 by considering the dependence ofPr(ST ITT) on the distortion probability, d(i l J, 1, m)where I and m are the numbers of words in ST and TT,respectively.Using an EM algorithm for Model 2, Brown et al(1990) reported the model produced seventeenacceptable translations for twenty-six testingsentences.
However, the degree of success in wordalignment was not reported.Dagan, Church and Gale (1992) proposed irectlyaligning words without the preprocessing phase ofsentence alignment.
Under this proposal, a roughchm'acter-by-character alignment is first performed.From this rough character alignment, words arealigned using an EM algorithm for Model 2 in afashion quite similar to the method presented byBrown.
Instead of d(i \[ j, 1, m), a smaller set of offsetprobabilities, o(i - i') were used where the i-th word ofST was connected to the j-th word of TT in the roughalignment.
This algorithm was evaluated on a noisyEnglish-French technical document.
The authorsclaimed that 60.5% of 65,000 words in the documentwere correctly aligned.
For 84% of the words, theoffset from correct alignment was at most 3.Motivated by the need to reduce on the memoryrequirement and to insure robustness in estimation ofprobability, Gale and Church (1991) proposed analternative algorithm in which probabilities are notestimated and stored for all word pairs.
Instead, onlystrongly associated word pairs are Ibund and stored.This is achieved by applying dO 2 test, a x~-like statistic.The extracted word pairs are used to match words inST and TT.
The algorithm works from left to right inST, using a dynamic programming procedure tomaximize Pr(ST ITT).
The probability t(s \] t) isapproximated as a function of thn-in, the number ofmatches (s', t) for all s' ~ ST, while distortion d(i I J, l,m) is approximated as a probability function,Pr(matchlj'-j) of slope, j' j, where (i', j') is the positionsof the nearest connection to the left of s. The authorsclaim that when a relevant threshold is set, thealgorithm can recommend connections for 61% for210the words in 800 sentence pairs.
Approximately 95%of the suggested connections are correct.in this paper, we propose a word-alignmentalgorithm based on classes derived from sense-relatedcategories in existing thesauri.
We refer to thisalgorithm as SenseAlign.
The proposed algorithmrelies on an automatic procedure to acquire class-based rules for alignment.
It does not employ word-by-word translation probabilities; nor does it use alengthy iterative EM algorithm for converging to suchprobabilities.
Results obtained fiom the algorithmsdemonstrate that classification based on existingthesauri is very effective in broadening coverage whilemaintaining high precision.
When trained with acorpus only one-tenth the size of the corpus used inGale and Church (1991), the algorithm aligns over80% of word pairs with comparable precision (93%).Besides, since the rules are based on sense distinction,word sense ambiguity can be resolved in favor of thecorresponding senses of rules applied in the alignmentprocess.The rest of this paper is organized as tbllows.
Inthe next section, we describe SenseAlign and discussits main components.
Examples of its output areprovided in Section 3.
All examples and theirtranslations are taken from the l~ongman English-Chinese Dictionary of Contemporary English (Procter1988, I,ecDOCE, hencetbrth).
Section 4 summarizesthe results of inside and outside tests.
In Section 5, wecompare SenseAlign to several other approaches thathave been proposed in literature involvingcomputational linguistics.
Finally, Section 6summarized the paper.2.
The Word Alignment Algorithm2.1 Preliminary details.
SenseAlign is a class-basedword alignment system that utilizes both existing andacquired lexical knowledge.
The system contains thefollowing components and distinctive t~atures.A A greedy algorithm for aligning words.
Thealgorithm is a greedy decision procedure forselecting preferred connections.
The evaluation isbased on composite scores of various factors:applicability, specificity, fan-out relative distortionprobabilities, and evidence from bilingualdictionaries.B.
Lexieal preprocessing.
Morphological analysis,part-of-speech tagging, ktioms identification areperformed for the two languages involved.
Inaddition, certain morpho-syntactic analyses areperformed to handle structures that are specificonly to one of the two languages involved.
Bydoing so, the sentences are brought closer to eachother in the number of words.C.
Two thesauri for classifying words.
(McArthur1992; Mei et al 1993) Classification allows aword to align with a target word using thecollective translation tendency of words in thesame class.
Class-base roles obviously have muchless parameters, are easier to acquire and can beapplied more broadly.1).
Two different ways of learning class-basedrules.
The class-based can be acquired either frombilingual materials uch as example sentences andtheir translations or definition sentences tbr sensesin a machine readable dictionary.E.
Similarity between connection target anddictionary translations.
In 40% of the correctconnections, the target of the connection anddictionary translation have at least one Chinesecharacter in common.
To exploit this thesaury teffect in translation, we include similarity betweentarget and dictionary translation as one of thefactors.F.
Relative distortion.
Translation process tends topreserve contiguous syntactical structures.
Thetarget position in a connection high depends thatof adjacent connections.
Therelbre, parameters inan model of distortion based on absolute positionare highly redundant.
Replacing probabilities of thefbrm d(iLj, 1, m) with relative distortion is a feasiblealternative.
By relative distortion, rd for theconnection (s,t), we mean (j-j')-(i-i') where i'thword, s' in the same syntactical structure of s, isconnected to the j'th word, t' in TT,2.2.
Acquisition of alignment rules.
Class-basedalignment rules can be acquired from a bilingualcorpus.
Table i presents the ten rules with thehighest applicability acquired from the examplesentences and their translations in LecDOCE.Alternatively, we can acquire rules from the bilingualdefinition text for senses in a bilingual dictionary.The definition sentence are disambiguated using asense division based on thesauri for the two languageinvolved.
Each sense is assigned codes fi'om the twothesauri according to its definition in both languages.See Table 2 lbr examples of sense definition andacquired rules.2.3 Evaluation of connection candidates.Connection candidates can be evaluated using variousfactors of confidence.
The probabilities of having acorrect connection as fimctions of these fhctors areestimated empirically to reflect their relativecontribution to the total confidence of a connection1 From one aspect those words sharing commoncharacters can be considered as synonyms tha wouldappear in a thesaurus.
Fujii and Croft (1993) pointedout that this thesaury effect of Kanji in Japanese helpsbroaden tile query lhvorably for character-basedinformation retrieval of Japanese documents.211candidate, fable 3 lists the empirical probabilities ofvarious factors.2.4.
Alignmen!
algorithm.
Our algorithm fbr wordaligmnent is a decision procedure tbr selecting thepreferred connection fiom a list of candidates.
Theinitial list of selected connection contains two dummyconnections.
This establishes the initial anchor pointstbr calculating relative distortion.
The highest scoredcandidate is selected and added to the list of solution.The newly added connection serves as an additionalanchor for a more accurate estimation of relativedistortion.
The connection candidates that areinconsistent with the selected connection are removedfrom the list.
Subsequently, the rest of  the candidatesare re-evaluated again.
Figure 1 presents theSenseAlign algorithm.3.
Example of running SenseAlign.To illustrate how SenseAlign works, consider the pairof sentences (1 e, 1 c).
( I e) I caught a fish yesterday.
(lc) Zhuotian wuo budao yitiao yu.yesterday I catch one fish.Table 4 shows the connections that are considered ineach iteration of  the SenseAlign algorithm.
Variousfactors used to evaluate connections are also given.Table 5 lists the connection in the final solution ofalignment.4.
Experiments with SenseAlignIn this sect ion,  we  present  the exper imenta l  results o fan implementation of  SenseAlign and relatedalgorithms.
Approximately 25,000 bilingual examplesentences from LecDOCE are used here as thetraining data.
Here, the training data were usedprimarily to acquire rules by a greedy learner and todetermine mpMcally probability thnctions of variousfactors.
The algorithnfs pertbrmance was then testedon the two sets of reside and outside data.
The insidetest consists of  fitty sentence pairs from LecDOCE asinput.
The outside test are 416 sentence pairs fiom abook on English sentence patterns containing acomprehensive fifty-five sets of  typical sentencepatterns, l lowever, the words in this outside test issomewhat more common, and, thereby, easier to align.
"fhis is evident from the slightly higher hit rate basedon simple dictionary lookup.The first experiment is designed to demonstrate heeffectiveness of  an naive algorithm (DictAlign) basedon a bilingual dictionary.
According to our results,although DictAlign produces high precision alignmentthe coverage for both test sets is below 20%.However, if the thesaury eft}act is exploited, thecoverage can be increased nearly three tblds to about40%, at the expense of a decrease around 10% inprecision,Table 1#~I234567891o, Ten rules with theRule642 Ma001, Hj63459 ,lh210.
Dil9440 Md108, Be214t8 L8202 , Eb28367 DaO03, Bn01362 Gc060, Hil6349 Fc050, Ed03310 Lh226, TII 8303 Ca002, Ab04302 'Fb020.
Gb09highest applicabilityGloss lbr classesmoving / come, and g ojobs, trade / worktrams/carnew/ne~, fleshhuildmg, house/buildingspcaking/ introducequalities / good.
badn icot ine  / timeman and ~oman / babxlikin% loving / like, loveTable 2.
Rules acquired from bilingual definitions for 12senses of"bank'" m LDOCIE.SgllbC & 1)cfinitionI I.n.
11 land along the side of a river, lakeetc.
i'/: ; ~i~II .n.2\] earth which is heaped up m a field orgarden, often making a border or division.II IJ:~l.n.3\] a mass ofsnmv, clouds, mud, etc.
-~I\[i : -l'JflI.n.4\[ a slope made at bends in a road orirace-track, so that they are safer for cars tcgo round.
~\]J~'l.n.51 = SANDBANK.
, f,~ll'l'";12.v.
II (of a car or aircraft) to move ~ithone side higher than the other, esp.
Mamamaking a turn {~'\[$;I.qi~g,~il3 n. I I a row, csp.
of OARs in an ancientboat or KEYs on a TYPEWRfFER ~Uf14.n.
II a placc in which money is kept andpaid out on demand, and where relatedaetMties go on.
,JI\[47 j:14.n.2i (usu.
in comb.)
a place wheresomething is held ready for use, esp.ORGANIC producls o1" lmman origin formedical use.
{i~{(igi!
( "14.n.31 (a person Mlo keeps) a supply ofmoncy or pieces for paymcnt or use in agame of chance.
;~p~15.v.
11 to put or kcep (money) in a bank (%:Rules1,d099, 13e03I x1{}99.
Bn 12Ib, Bb(i3Ix10!}9.
I ~c(},}I,d(}99, I~c02N.j295, I:d{}2l Ib, I )n{}8Jel04, l)m0,1.cI04 Ih~17.le104.
I )m04Jc 106.
I Ij,l{}15.v.21 \[csp.
with\] to keel} one's money .lelO6,1lj40!esp.
in the stated bank) (/di?Table 3.
Factor types with empirical probabilityFactor condition and probabilityFo f=  1Prob 0.85App A >.
1Prob 0.95Spec 3' _> 12Prob 0.95R.D.
rd  = 0Prob 0.26Sire 3'ira = IeTg-,7 o.94/":  2 f=  3 f>  30.61 0.44 0.42?
I>A k .01 .0I>A > .001 10-% A0.90 0.85 0.4312>3"#11 11> S_>I00.85 0.77rd :: I rd = 2O.
1 I 0.071 =Sire>.66 .66~57m ~,20.42 0.35'10 > ,S0.35rd> 20.04bhm < .20.12212IterationEnglish EnglishWord POS'Fable 4.
Various factors for covmection candidatesChinese Chinese Fan-Word P()S Rule Out Sire rd Spcc App=yesterday NR I J \ [~ Nd l,h225 Tq23 I-fish.
NN fi), Na Ab032 Bil,l 1-1 PP ~j':~; Nh Gh280 Na02 l-l PP ~~ Nh (\]h280 Na05 I -fish NN ~(t Na Af100 Bil4 l-fish NN ,((t Na Ah 120 gi 14 I -fish NN f,(t Na Ea017 I?,i 14 l-fish NN f,(/ Na Eb031 Bi 14 I-a AT '/L~ Nc Nd098 Qa04 l -yesterday NR ~,), Na Lh225 Bil4 l-caught VB 4\]1\] J~J Vq Di Dc098 lhn05 l-fish NN t l~l~ Nd Al l00 Tq23 l-fish NN IlJl(Z~ Nd Ah 120 Tq23 1 -fish NN {l~-:i~ Nd Ea017 Tq23 1 -fish NN \[l'\[)~ Nd Eb031 Tq23 1 -fish NN \[l~\[; ~ Nd Ab032 fq23 1 - 1I 4 \[ 1.2 0.00970.75 l 15.3 0.00171 l 0 0l l 0 0(}.15 1 0 0(}.75 1 0 00.75 1 0 00.75 1 0 00.5 1 0 00 0 0 00 1 0 00 3 0 00 3 0 00 3 0 00 3 0 0o 3 o o!2 fish NN ~0, Na Ab032 P,i 14 1 - 1 0.752 1 PP -1;3~ Nh Gh280 Na02 1- I I2 \[ PP ~ Nh Gh280 Na(}5 1-1 12 fish NN f,(~, Na Afl00 Bi 14 1-1 0.752 lish NN ~(i Na Ah I20  Bil4 I-1 0.752 fish NN ~(( Na Ea017 Bi 14 I - 1 0.752 tish NN ~(I Na Eb031 P, il4 I-1 0.752 a A'\[" -~  Nc Nd098 Qa04 t- 1 0.52 caught VB ~l{iJil\] V I Di Dc098 lhn05 l- 1 015,3 ~ i -Y0 (\]0 00 00 010 0\[0 0 i0 ()io o!3 I PP :J:~ Nh Gh280 Na02 I- 1 I 0 () ((~3 I PP 4-~ Nh Gh280 Na05 1- I I 0 03 a AT -{~ Ne Nd098 Qa04 1-1 0.5 0 0 03 caught VB 41115i~j V+Di Dc098 lhn05 1- I 0 0 0 0:4 a AT "ti~ Nc Nd098 Qa04 1- I 0.5 0 0 ()4 caught VB l\]\[\]j!I\] V ~.\[)i 1)e098 Hm05 l-1 0 0 05 caught VB ~II\] 5_!r\] V+Di  De098 Hm05 l - 1 0 0 0 0In our second experiment, we use SenseAligndescribed above for word aligmnent except that nobilingual diclionary is used.
In our thiM expet+iment,we use the full SenseAlign to align the testing data.Table 6 indicates that acquired lexical infornmtionaugmented and existing lexical information such as abilingual dictionary can supplement each other toproduce optimum aligmnent results.
The generality ofthe approach is evident fi-om the thct that the coverageand precision for the ovtside test are comparable withthose of the inside test.5.
Discussions5.1 Machine-readable  lexieal resources vs. corporaWe believe the proposed algorithm addresses tileproblem of  knowledge engineering bottleneck byusing both corpora and machine readable lexicalresources such as dictionaries and thesauri.
Thecorpora provide us with training and testing materials,so that empirical knowledge can be derived andevaluated objectively.
The thesauri provideclassification that can be utilized to generalize theempirical knowledge gleaned fi-om corporaSenseAlign achieves a degree of  generality since aword pair can be accurately aligned, even when theyoccur rarely or only once ill the corpus.
This kind ofgenerality is unattainable by statistically trained word-based lnodels.
Class-based models obviously offeradvantages of  smaller storage requirement and hi vhersystem efficiency.
Such advantages do have their costs,tot' class-based models may be over-generalized andmiss word-specif ic rules.
However, work on class-based systems have indicated that the advantagesoulweigh the disadvantages.5.2 Mutua l  in format ion,  and frequency.
Gale andChurch (1990) shows a near-miss example where (\]2 aZ2-1ike statistic works better than mutual infimnationfor selecting strongly associated woM pairs to use inword alignment.
In their study, they contend that 2like statistic works better because it uses co-213nonoccurrence and the number of sentences whereone word occurs while the other does not which areoften larger, more stable, and more indicative than co-occurrence used in mutual information.The above-cited work's discussions of the Z2-1ikestatistic and the fan-in factor provide a valuablereference for this work.
In our attempt to improve onlow coverage of word-based approaches, we usesimple filtering according to fan-out in the acquisitionof class-based rules, in order to maximize bothcoverage and precision.
The rules that provide themost instances of plausible connection is selected.This contrasts with approaches based on word-specific statistic where strongly associated word pairsselected may not have a strong presence in the data.This generally corresponds to the results from a recentwork on a variety of tasks such as terminologyextraction and structural disambiguation.
Dallie,Gaussier and Lange (1994) demonstrated that simplecriteria related to frequency coupled with a linguisticfilter works better than mutual information tbrterminology extraction.
Recent work involvingstructural disambiguation (Brill and Resnik 1994) alsoindicated that statistics related to frequencyoutperform utual intbrmation and q~2 statistic.6.
Concluding remarksThis paper has presented an algorithm capable ofidentit~,ing words and their translation in a bilingualcorpus.
It is effective for specific linguistic reasons.The significant majority of words in bilingualsentences have diverging translation; thosetranslations are not often tbund in a bilingualdictionaly.
However, those deviation are largelylimited within the classes defined by thesauri.Therefore, by using a class-based approach, theproblem's complexity can reduced in the sense thatless number of candidates need to be considered witha greater likelihood of finding the correct ranslation.In general, a slight amotmt of precision canapparently be expended to gain a substantial increasein applicability.
Our results suggest that mixedstrategies can yield a broad coverage and highprecision word alignment and sense tagging systemwhich can produce richer information fbr MT andNLP tasks such as word sense disambiguation.
Theword sense information can provide a certain degreeof generality which is lacking in most statisticalprocedures.
The algorithm's performance discussedhere can definitely be improved by enhancing thevarious components of the algorithm, e.g.,morphological analyses, bilingual dictionary,monolingual thesauri, and rule acquisition.
However,this work has presented a workable core forprocessing bilingual corpus.
The proposed algorithmcan produce ffective word-alignment results with1.
Read a pair of English-Chinese s ntences.2.
Two dummies are replace to the left of the firstand to the right of the last word of the sourcesentence.
Similar two dummies are added tothe target sentence.
The left dummy in thesource and target sentences align with eachother.
Similarly, the right dummies align witheach other.
\]'his establishes anchor points forcalculating the relative distortion score.3.
Perfbrm the part-of-speech tagging andanalysis tbr sentences inboth languages.4.
Lookup the words in LEXICON and C1LIN todetermine the classes consistent with the part-of-speech analyses.5.
Follow the procedure in Section 2.3 tocalculate a composite probability tbr eachconnection candidate according to fan-out,applicability, specificity of alignment rules,relative distortion, and dictionary evidence.6.
The highest scored candidate is selected andadded to the list of alignment.7.
The connection candidates that areinconsistent with the selected connection arealso removed from the candidate list.8.
The rest of the candidates are evaluated againaccording to the new list of connections.9.
The procedure iterates until all words in thesource sentence are a l ib i .Figure 1.
Alignment Algorithm of SenseAlignTable 5. q'he final alignmentEnglish EnglishWord CodeChineseWordChineseCodeI Gh280 wuo Na05Hm05 caughti aI)e098 bu-daoNd098 yi-tiao Qa04Ab032Lh225!fish iYUzuotian yesterdayBil4Tq23Table 6.
Experimental Results" Inside TestNo.
Matched # ftk CoverageDictAlign with sim = 1.0 59 56 15.3%I DictAlign withsim > 0.67 113 100 29 .4%Dici/klign with sire > 0.5 l 51 124 39.2%ScnseAlign wilhout sire 237 213 61.7%Full ScnseAlign 314 293 81.8%Outside TestNo.
MatchedDictAlign with sire : 1.0 499DictAlign with sin; > 0.67 970:DictAlign with sim > 0.5 1221SenscAlign wifllout sire 1913:Full SenseAlign 2424fl Hit486865104617212265(~ovcta~c16.8%32.7%41.1%66.8%84.7%Pmcision94 .9%88.5%82.1%89.9%93.3%Prccisi()n97.4%89.2%85.7%90.0%93.4%214sense tagging which can provide a basis for such N I,Ptasks as word sense disambiguation (Chen and Chang1994) and PP attachment (Chen and Chang 199"5).While this paper has specifically addressed onlyEnglish-Chinese corpora, the linguistic issues thatmotivated the algorithm are quite general and are to agreat degree language independent.
If such a case istrue, the algorithm presented here should be adaptableto other language pairs.
The prospects tbr Japanese, inparticular, seem highly promising There arc somework on alignment of l?nglish-Japanese t xts usingboth dictionaries and statistics (Utsuro, lkeda,Yamane, Matsumoto and Nagao 1994).AcknowledgmentsThe authors would like to thank the National ScienceConcil of the Republic of China for financial supportof this manuscript under Contract No.
NSC 84-102-1211.
Zebra Corporation and l,ongnmn Group arcappreciated tbr the machine readable dictionary.Special thanks are due to Mathis H. C Chen lbr workof preprocessing the Mill).
Thanks are also due toKeh-Yih Su tbr many helpful comments on an earlydrall of this paper.References1.
Brill, Eric and P. Resnik, (1994).
A Rule bascdApproach to l'repositional Phrase Attachment, Inl~roceedings oJ" Ihe 15lh hJlernaliotml ("ot!/L, renceon ( 7omlmlalional Linguistics, \[ 198-1205, KyotoJapan.2.
Brown, P., J. Cocke, S. Della Pietra, V. \[)ellal'ietra, F. Jelinek, \].
l,afl~rty, R. Mercer, and P.Roosin, (1990).
A Statistical Approach to MachineTranslation, Computational Linguislies, 16:2, page79-85.3.
l?,mwn, l'., S. Della l'ietra, V. Della Pietra, and R.Mercer, (1993).
The Mathematics of StatisticalMachine Translation: Paranteter Estimation,Compulalional l, inguistics, Vo\[.
19, No.
2, page263-31 l.4.
Chang, J. S. amt M. 11.
C. Chen, (1995).
StructureAmbiguity and Conceptual Information Retrieval,In l'roceeding oJ" t'aciJic Asia (7ot(/~,rence onLattguage, lqfi)rmalion and ( 7omlmlaliott , page 16-23.5.
Chert, \].
N. anti J. S. Chang, (1994).
TowardsGenerality and Modularity in Statistical WordSense Disambiguation, In l'roceeding of t'acificAsia Coq/i, rence on I, brmal arm (7ompulaionalLingl#slic's, page 45-48.6.
I)agan, ldo, K. W. Church and W. A. Gale, (1993).Robust Bilingual Word Aligmnent lbr MachineAided Translation, In l~roceedings o)ihe WorkshopOn Uer F \],arge (?orl)ora : Academic and lnduslrialI'er,vwclives, page I-8.7.
Daille, B., E. Gaussier and J.-M. 1,ange, (1994).Towards automatic extraction of monolinguaI andbilingual terminology, in t'roceedillgs of thehllernational ( "o/~/'erence on ( 'ompulatiomdLinguistics, 515-52 I.8.
Fuji< \[\[ideo and W. Bruce Cro\['t, (1093).
AComparison of Indexing Techniques for JapaneseText Retrieval, In Iq'oceedings of Ihe 16Ihhtlernaliomtl A ( 7A/I SI( ;IR ( "ollfi, re l tee  (711 leesearchamt Development in ht/ormation I&,lrieval, page237-246.Gale, W. and K. Church, (\[993).
A Program forAligning Sentence in Bilingual Corpora,( 7o#qmtalional Littguislics, 19( I ), page 75-102.10Gale, W. A. and K. W. Church.
(1991).
IdentifyingWord Correspondences in Parallel Texts, inlq'oceedmgs o\[" lhe bourlh IMRt.
'A Speech andNatural l,anguage Workshop, page 152-157,Pacific Grove, CA., February.l I.Gale, W. A., K. W. Church, and l)avid Yarowsky,(1992).
Using bilingual materials to develop wordsense disambiguation methods.
In Proceedings oJlhe I,'ottrlh \]ttlernatiomd ( 7ot?fi,,rettce on7 heoreti~xd and Methodological L~sues it, MachineTrattsktlion, 101-112, Montreal, CanadaKay,Martin and Martin Iloscheisen, (1993).
Text-Translation Aligmnent, Computational Linguistics,Vol.
19, No.
1, page 121-142.12.1,ongman, (1993).
lxmgman English-ChineseDictionary of Contemporary English, Published byl,ongnmn Group (Far l,;ast) I,td., I long Kong.13.Matsumoto, Y. et al (1993).
Structural Matchingof Parallet Texts, In l'roceedmgs of Ihe 31s1Atmual Meet#; L, o) c the Association ./or( ?omlmtaliottal Linguistics, page 1-30, Ohio, USA.\[4.McArthur, T. (1992) Longman l,exicon ofContemporary English, Published hy l,ongmanGroup (Far East)Ltd., Hong Kong\[5.Mei, J.\].
el al., (1993).
Tongyici Cilin (WordForest of Synonyms), Tong Hua Publishing, Taipei,(traditional Chinese edition of a simplilied Chineseedition published in 1984).16.Proclor, Paul, (1988).
\[,ongman English-Chinesel)ictionary of Contemporary English, LongmanGroup (l"ar East), Hong Kong17.Utsuro, T., hi.
Ikeda, M. Yamane, M. Matsumoto,aml M. Nagao, (1994).
Bilingual text matchingusing bilingual dictionary anti statistics, Inl~roceedin?<~ ' (?\]" the 151h Inlernational (?oql'ereuceon ('~ompulaliona\[ Linguistics, page 1076-1083,Kyoto, Japan.9.215
