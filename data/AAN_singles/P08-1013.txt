Proceedings of ACL-08: HLT, pages 106?113,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsApplying a Grammar-based Language Modelto a Simplified Broadcast-News Transcription TaskTobias KaufmannSpeech Processing GroupETH Zu?richZu?rich, Switzerlandkaufmann@tik.ee.ethz.chBeat PfisterSpeech Processing GroupETH Zu?richZu?rich, Switzerlandpfister@tik.ee.ethz.chAbstractWe propose a language model based ona precise, linguistically motivated grammar(a hand-crafted Head-driven Phrase StructureGrammar) and a statistical model estimatingthe probability of a parse tree.
The languagemodel is applied by means of an N-best rescor-ing step, which allows to directly measure theperformance gains relative to the baseline sys-tem without rescoring.
To demonstrate thatour approach is feasible and beneficial fornon-trivial broad-domain speech recognitiontasks, we applied it to a simplified Germanbroadcast-news transcription task.
We reporta significant reduction in word error rate com-pared to a state-of-the-art baseline system.1 IntroductionIt has repeatedly been pointed out that N-gramsmodel natural language only superficially: an Nth-order Markov chain is a very crude model of thecomplex dependencies between words in an utter-ance.
More accurate statistical models of naturallanguage have mainly been developed in the fieldof statistical parsing, e.g.
Collins (2003), Charniak(2000) and Ratnaparkhi (1999).
Other linguisticallyinspired language models like Chelba and Jelinek(2000) and Roark (2001) have been applied to con-tinuous speech recognition.These models have in common that they explic-itly or implicitly use a context-free grammar inducedfrom a treebank, with the exception of Chelba andJelinek (2000).
The probability of a rule expansionor parser operation is conditioned on various con-textual information and the derivation history.
Animportant reason for the success of these models isthe fact that they are lexicalized: the probability dis-tributions are also conditioned on the actual wordsoccuring in the utterance, and not only on their partsof speech.
Most statistical parsers achieve a high ro-bustness with respect to out-of-grammar sentencesby allowing for arbitrary derivations and rule expan-sions.
On the other hand, they are not suited to reli-ably decide on the grammaticality of a given phrase,as they do not accurately model the linguistic con-straints inherent in natural language.We take a completely different position.
In thefirst place, we want our language model to reliablydistinguish between grammatical and ungrammati-cal phrases.
To this end, we have developed a pre-cise, linguistically motivated grammar.
To distin-guish between common and uncommon phrases, weuse a statistical model that estimates the probabilityof a phrase based on the syntactic dependencies es-tablished by the parser.
We achieve some degree ofrobustness by letting the grammar accept arbitrarysequences of words and phrases.
To keep the gram-mar restrictive, such sequences are penalized by thestatistical model.Accurate hand-crafted grammars have been ap-plied to speech recognition before, e.g.
Kiefer etal.
(2000) and van Noord et al (1999).
However,they primarily served as a basis for a speech un-derstanding component and were applied to narrow-domain tasks such as appointment scheduling orpublic transport information.
We are mainly con-cerned with speech recognition performance onbroad-domain recognition tasks.Beutler et al (2005) pursued a similar approach.106However, their grammar-based language model didnot make use of a probabilistic component, and itwas applied to a rather simple recognition task (dic-tation texts for pupils read and recorded under goodacoustic conditions, no out-of-vocabulary words).Besides proposing an improved language model,this paper presents experimental results for a muchmore difficult and realistic task and compares themto the performance of a state-of-the-art baseline sys-tem.In the following Section, we will first describe ourgrammar-based language model.
Next, we will turnto the linguistic components of the model, namelythe grammar, the lexicon and the parser.
We willpoint out some of the challenges arising from thebroad-domain speech recognition application andpropose ways to deal with them.
Finally, we will de-scribe our experiments on broadcast news data anddiscuss the results.2 Language Model2.1 The General ApproachSpeech recognizers choose the word sequenceW?
which maximizes the posterior probabilityP (W |O), where O is the acoustic observation.
Thisis achieved by optimizingW?
= argmaxWP (O|W ) ?
P (W )?
?
ip|W | (1)The language model weight ?
and the word inser-tion penalty ip lead to a better performance in prac-tice, but they have no theoretical justification.
Ourgrammar-based language model is incorporated intothe above expression as an additional probabilityPgram(W ), weighted by a parameter ?:W?
= argmaxWP (O|W )?P (W )?
?Pgram(W )?
?ip|W |(2)Pgram(W ) is defined as the probability of the mostlikely parse tree of a word sequence W :Pgram(W ) = maxT?parses(W )P (T ) (3)To determine Pgram(W ) is an expensive operationas it involves parsing.
For this reason, we pursue anN-best rescoring approach.
We first produce the Nbest hypotheses according to the criterion in equa-tion (1).
From these hypotheses we then choose thefinal recognition result according to equation (2).2.2 The Probability of a Parse TreeThe parse trees produced by our parser are binary-branching and rather deep.
In order to compute theprobability of a parse tree, it is transformed to a flatdependency tree similar to the syntax graph repre-sentation used in the TIGER treebank Brants et al(2002).
An inner node of such a dependency treerepresents a constituent or phrase.
Typically, it di-rectly connects to a leaf node representing the mostimportant word of the phrase, the head child.
Theother children represent phrases or words directlydepending on the head child.
To give an example,the immediate children of a sentence node are thefinite verb (the head child), the adverbials, the sub-ject and the all other (verbal and non-verbal) com-plements.This flat structure has the advantage that the in-formation which is most relevant for the head childis represented within the locality of an inner node.Assuming statistical independence between the in-ternal structures of the inner nodes ni, we can factorP (T ) much like it is done for probabilistic context-free grammars:P (T ) ?
?niP ( childtags(ni) | tag(ni) ) (4)In the above equation, tag(ni) is simply the labelassigned to the tree node ni, and childtags(ni) de-notes the tags assigned to the child nodes of ni.Our statistical model for German sentences distin-guishes between eight different tags.
Three tags areused for different types of noun phrases: pronomi-nal NPs, non-pronominal NPs and prenominal gen-itives.
Prenominal genitives were given a dedicatedtag because they are much more restricted than or-dinary NPs.
Another two tags were used to dis-tinguish between clauses with sentence-initial finiteverbs (main clauses) and clauses with sentence-finalfinite verbs (subordinate clauses).
Finally, there arespecific tags for infinitive verb phrases, adjectivephrases and prepositional phrases.P was modeled by means of a dedicated prob-ability distribution for each conditioning tag.
Theprobability of the internal structure of a sentencewas modeled as the trigram probability of the cor-responding tag sequence (the sequence of the sen-tence node?s child tags).
The probability of an ad-jective phrase was decomposed into the probability107of the adjective type (participle or non-participle andattributive, adverbial or predicative) and the proba-bility of its length in words given the adjective type.This allows the model to directly penalize long ad-jective phrases, which are very rare.
The model fornoun phrases is based on the joint probability of thehead type (either noun, adjective or proper name),the presence of a determiner and the presence of pre-and postnominal modifiers.
The probabilities of var-ious other events are conditioned on those four vari-ables, namely the number of prepositional phrases,relative clauses and adjectives, as well as the pres-ence of appositions and prenominal or postnominalgenitives.The resulting probability distributions weretrained on the German TIGER treebank which con-sists of about 50000 sentences of newspaper text.2.3 Robustness IssuesA major problem of grammar-based approachesto language modeling is how to deal with out-of-grammar utterances.
Obviously, the utterance to berecognized may be ungrammatical, or it could begrammatical but not covered by the given grammar.But even if the utterance is both grammatical andcovered by the grammar, the correct word sequencemay not be among the N best hypotheses due toout-of-vocabulary words or bad acoustic conditions.In all these cases, the best hypothesis available islikely to be out-of-grammar, but the language modelshould nevertheless prefer it to competing hypothe-ses.
To make things worse, it is not unlikely thatsome of the competing hypotheses are grammatical.It is therefore important that our language modelis robust with respect to out-of-grammar sentences.In particular this means that it should provide a rea-sonable parse tree for any possible word sequenceW .
However, our approach is to use an accurate,linguistically motivated grammar, and it is undesir-able to weaken the constraints encoded in the gram-mar.
Instead, we allow the parser to attach any se-quence of words or correct phrases to the root node,where each attachment is penalized by the proba-bilistic model P (T ).
This can be thought of asadding two probabilistic context-free rules:S ??
S?
S with probability qS ??
S?
with probability 1?qIn order to guarantee that all possible word se-quences are parseable, S?
can produce both satu-rated phrases and arbitrary words.
To include sucha productive set of rules into the grammar wouldlead to serious efficiency problems.
For this reason,these rules were actually implemented as a dynamicprogramming pass: after the parser has identifiedall correct phrases, the most probable sequence ofphrases or words is computed.2.4 Model ParametersBesides the distributions required to specify P (T ),our language model has three parameters: the lan-guage model weight ?, the attachment probabilityq and the number of hypotheses N .
The parame-ters ?
and q are considered to be task-dependent.For instance, if the utterances are well-covered bythe grammar and the acoustic conditions are good,it can be expected that ?
is relatively large and thatq is relatively small.
The choice of N is restrictedby the available computing power.
For our experi-ments, we chose N = 100.
The influence of N onthe word error rate is discussed in the results section.3 Linguistic Resources3.1 Particularities of the Recognizer OutputThe linguistic resources presented in this Sectionare partly influenced by the form of the recog-nizer output.
In particular, the speech recognizerdoes not always transcribe numbers, compoundsand acronyms as single words.
For instance, theword ?einundzwanzig?
(twenty-one) is transcribedas ?ein und zwanzig?, ?Kriegspla?ne?
(war plans) as?Kriegs Pla?ne?
and ?BMW?
as ?B.
M. W.?
Thesetranscription variants are considered to be correctby our evaluation scheme.
Therefore, the grammarshould accept them as well.3.2 Grammar and ParserWe used the Head-driven Phrase Structure Grammar(HPSG, see Pollard and Sag (1994)) formalism todevelop a precise large-coverage grammar for Ger-man.
HPSG is an unrestricted grammar (Chomskytype 0) which is based on a context-free skeletonand the unification of complex feature structures.There are several variants of HPSG which mainlydiffer in the formal tools they provide for stating lin-108guistic constraints.
Our particular variant requiresthat constituents (phrases) be continuous, but it pro-vides a mechanism for dealing with discontinuitiesas present e.g.
in the German main clause, seeKaufmann and Pfister (2007).
HPSG typically dis-tinguishes between immediate dominance schemata(rough equivalents of phrase structure rules, butmaking no assumptions about constituent order) andlinear precedence rules (constraints on constituentorder).
We do not make this distinction but rather letimmediate dominance schemata specify constituentorder.
Further, the formalism allows to express com-plex linguistic constraints by means of predicates orrelational constraints.
At parse time, predicates arebacked by program code that can perform arbitrarycomputations to check or specify feature structures.We have implemented an efficient Java parser forour variant of the HPSG formalism.
The parser sup-ports ambiguity packing, which is a technique formerging constituents with different derivational his-tories but identical syntactic properties.
This is es-sential for parsing long and ambiguous sentences.Our grammar incorporates many ideas from ex-isting linguistic work, e.g.
Mu?ller (2007), Mu?ller(1999), Crysmann (2005), Crysmann (2003).
In ad-dition, we have modeled a few constructions whichoccur frequently but are often neglected in formalsyntactic theories.
Among them are prenominal andpostnominal genitives, expressions of quantity andexpressions of date and time.
Further, we haveimplemented dedicated subgrammars for analyzingwritten numbers, compounds and acronyms that arewritten as separate words.
To reduce ambiguity, onlynoun-noun compounds are covered by the grammar.Noun-noun compounds are by far the most produc-tive compound type.The grammar consists of 17 rules for gen-eral linguistic phenomena (e.g.
subcategorization,modification and extraction), 12 rules for model-ing the German verbal complex and another 13construction-specific rules (relative clauses, genitiveattributes, optional determiners, nominalized adjec-tives, etc.).
The various subgrammars (expressionsof date and time, written numbers, noun-noun com-pounds and acronyms) amount to a total of 43 rules.The grammar allows the derivation of ?interme-diate products?
which cannot be regarded as com-plete phrases.
We consider complete phrases to besentences, subordinate clauses, relative and interrog-ative clauses, noun phrases, prepositional phrases,adjective phrases and expressions of date and time.3.3 LexiconThe lexicon was created manually based on a list ofmore than 5000 words appearing in the N-best listsof our experiment.
As the domain of our recognitiontask is very broad, we attempted to include any pos-sible reading of a given word.
Our main source ofdictionary information was Duden (1999).Each word was annotated with precise morpho-logical and syntactic information.
For example, theroughly 2700 verbs were annotated with over 7000valency frames.
We distinguish 86 basic valencyframes, for most of which the complement types canbe further specified.A major difficulty was the acquisition of multi-word lexemes.
Slightly deviating from the commonnotion, we use the following definition: A syntac-tic unit consisting of two or more words is a multi-word lexeme, if the grammar cannot derive it fromits parts.
English examples are idioms like ?by andlarge?
and phrasal verbs such as ?to call sth off?.Such multi-word lexemes have to be entered into thelexicon, but they cannot directly be identified in theword list.
Therefore, they have to be extracted fromsupplementary resources.
For our work, we used anewspaper text corpus of 230M words (FrankfurterRundschau and Neue Zu?rcher Zeitung).
This cor-pus included only articles which are dated before thefirst broadcast news show used in the experiment.
Inthe next few paragraphs we will discuss some typesof multiword lexemes and our methods of extractingthem.There is a large and very productive class of Ger-man prefix verbs whose prefixes can appear sepa-rated from the verb, similar to English phrasal verbs.For example, the prefix of the verb ?untergehen?
(tosink) is separated in ?das Schiff geht unter?
(the shipsinks) and attached in ?weil das Schiff untergeht?
(because the ship sinks).
The set of possible va-lency frames of a prefix verb has to be looked upin a dictionary as it cannot be derived systematicallyfrom its parts.
Exploiting the fact that prefixes are at-tached to their verb under certain circumstances, weextracted a list of prefix verbs from the above news-paper text corpus.
As the number of prefix verbs is109very large, a candidate prefix verb was included intothe lexicon only if there is a recognizer hypothesisin which both parts are present.
Note that this pro-cedure does not amount to optimizing on test data:when parsing a hypothesis, the parser chart containsonly those multiword lexemes for which all parts arepresent in the hypothesis.Other multi-word lexemes are fixed word clus-ters of various types.
For instance, some preposi-tional phrases appearing in support verb construc-tions lack an otherwise mandatory determiner, e.g.
?unter Beschuss?
(under fire).
Many multi-wordlexemes are adverbials, e.g.
?nach wie vor?
(still),?auf die Dauer?
(in the long run).
To extract suchword clusters we used suffix arrays proposed in Ya-mamoto and Church (2001) and the pointwise mu-tual information measure, see Church and Hanks(1990).
Again, it is feasible to consider only thoseclusters appearing in some recognizer hypothesis.The list of candidate clusters was reduced using dif-ferent filter heuristics and finally checked manually.For our task, split compounds are to be consid-ered as multi-word lexemes as well.
As our gram-mar only models noun-noun compounds, other com-pounds such as ?unionsgefu?hrt?
(led by the union)have to be entered into the lexicon.
We appliedthe decompounding algorithm proposed in Adda-Decker (2003) to our corpus to extract such com-pounds.
The resulting candidate list was again fil-tered manually.We observed that many proper nouns (e.g.
per-sonal names and geographic names) are identical tosome noun, adjective or verb form.
For example,about 40% of the nouns in our lexicon share in-flected forms with personal names.
Proper nounsconsiderably contribute to ambiguity, as most ofthem do not require a determiner.
Therefore, aproper noun which is a homograph of an open-classword was entered only if it is ?relevant?
for ourtask.
The ?relevant?
proper nouns were extractedautomatically from our text corpus.
We used smalldatabases of unambiguous given names and formsof address to spot personal names in significant bi-grams.
Relevant geographic names were extractedby considering capitalized words which significantlyoften follow certain local prepositions.The final lexicon contains about 2700 verbs (in-cluding 1900 verbs with separable prefixes), 3500nouns, 450 adjectives, 570 closed-class words and220 multiword lexemes.
All lexicon entries amountto a total of 137500 full forms.
Noun-noun com-pounds are not included in these numbers, as theyare handled in a morphological analysis component.4 Experiments4.1 Experimental SetupThe experiment was designed to measure how mucha given speech recognition system can benefit fromour grammar-based language model.
To this end,we used a baseline speech recognition system whichprovided the N best hypotheses of an utterancealong with their respective scores.
The grammar-based language model was then applied to the Nbest hypotheses as described in Section 2.1, yieldinga new best hypothesis.
For a given test set we couldthen compare the word error rate of the baseline sys-tem with that of the extended system employing thegrammar-based language model.4.2 Data and PreprocessingOur experiments are based on word lattice out-put from the LIMSI German broadcast news tran-scription system (McTait and Adda-Decker, 2003),which employs 4-gram backoff language models.From the experiment reported in McTait and Adda-Decker (2003), we used the first three broadcastnews shows1 which corresponds to a signal lengthof roughly 50 minutes.Rather than applying our model to the origi-nal broadcast-news transcription task, we used theabove data to create an artificial recognition taskwith manageable complexity.
Our primary aim wasto design a task which allows us to investigate theproperties of our grammar-based approach and tocompare its performance with that of a competitivebaseline system.As a first simplification, we assumed perfect sen-tence segmentation.
We manually split the originalword lattices at the sentence boundaries and mergedthem where a sentence crossed a lattice boundary.This resulted in a set of 636 lattices (sentences).
Sec-ond, we classified the sentences with respect to con-tent type and removed those classes with an excep-1The 8 o?clock broadcasts of the ?Tagesschau?
from the14th of April, 21st of April and 7th of Mai 2002.110tionally high baseline word error rate.
These classesare interviews (a word error rate of 36.1%), sportsreports (28.4%) and press conferences (25.7%).
Thebaseline word error rate of the remaining 447 lattices(sentences) is 11.8%.From each of these 447 lattices, the 100 best hy-potheses were extracted.
We next compiled a listcontaining all words present in the recognizer hy-potheses.
These words were entered into the lexiconas described in Section 3.3.
Finally, all extractedrecognizer hypotheses were parsed.
Only 25 of the44000 hypotheses2 caused an early termination ofthe parser due to the imposed memory limits.
How-ever, the inversion of ambiguity packing (see Sec-tion 3.2) turned out to be a bottleneck.
As P (T )does not directly apply to parse trees, all possiblereadings have to be unpacked.
For 24 of the 447lattices, some of the N best hypotheses containedphrases with more than 1000 readings.
For these lat-tices the grammar-based language model was sim-ply switched off in the experiment, as no parse treeswere produced for efficiency reasons.To assess the difficulty of our task, we inspectedthe reference transcriptions, the word lattices andthe N-best lists for the 447 selected utterances.
Wefound that for only 59% of the utterances the correcttranscription is among the 100-best hypotheses.
Thefirst-best hypothesis is completely correct for 34%of the utterances.
The out-of-vocabulary rate (es-timated from the number of reference transcriptionwords which do not appear in any of the lattices) is1.7%.
The first-best word error rate is 11.79%, andthe 100-best oracle word error rate is 4.8%.We further attempted to judge the grammatical-ity of the reference transcriptions.
We consideredonly 1% of the sentences to be clearly ungrammat-ical.
19% of the remaining sentences were foundto contain general grammatical constructions whichare not handled by our grammar.
Some of theseconstructions (most notably ellipses, which are om-nipresent in broadcast-news reports) are notoriouslydifficult as they would dramatically increase ambi-guity when implemented in a grammar.
About 45%of the reference sentences were correctly analyzedby the grammar.2Some of the word lattices contain less than 100 differenthypotheses.4.3 Training and TestingThe parameter N , the maximum number of hy-potheses to be considered, was set to 100 (the ef-fect of choosing different values of N will be dis-cussed in section 4.4).
The remaining parameters?
and q were trained using the leave-one-out cross-validation method: each of the 447 utterances servedas the single test item once, whereas the remaining446 utterances were used for training.
As the er-ror landscape is complex and discrete, we could notuse gradient-based optimization methods.
Instead,we chose ?
and q from 500 equidistant points withinthe intervals [0, 20] and [0, 0.25], respectively.
Theword error rate was evaluated for each possible pairof parameter values.The evaluation scheme was taken from McTaitand Adda-Decker (2003).
It ignores capitalization,and written numbers, compounds and acronymsneed not be written as single words.4.4 ResultsAs shown in Table 1, the grammar-based languagemodel reduced the word error rate by 9.2% rela-tive over the baseline system.
This improvementis statistically significant on a level of < 0.1% forboth the Matched Pairs Sentence-Segment Word Er-ror test (MAPSSWE) and McNemar?s test (Gillickand Cox, 1989).
If the parameters are optimized onall 447 sentences (i.e.
on the test data), the worderror rate is reduced by 10.7% relative.For comparison, we redefined the probabilisticmodel as P (T ) = (1?
q)qk?1, where k is the num-ber of phrases attached to the root node.
This re-duced model only considers the grammaticality ofa phrase, completely ignoring the probability of itsinternal structure.
It achieved a relative word errorreduction of 5.9%, which is statistically significanton a level of < 0.1% for both tests.
The improve-ment of the full model compared to the reducedmodel is weakly significant on a level of 2.6% forthe MAPSSWE test.For both models, the optimal value of q was 0.001for almost all training runs.
The language modelweight ?
of the reduced model was about 60%smaller than the respective value for the full model,which confirms that the full model provides morereliable information.111experiment word error ratebaseline 11.79%grammar, no statistics 11.09% (-5.9% rel.
)grammar 10.70% (-9.2% rel.
)grammar, cheating 10.60% (-10.7% rel.
)100-best oracle 4.80%Table 1: The impact of the grammar-based languagemodel on the word error rate.
For comparison, the resultsfor alternative experiments are shown.
In the experiment?grammar, cheating?, the parameters were optimized ontest data.Figure 1 shows the effect of varying N (the max-imum number of hypotheses) on the word error rateboth for leave-one-out training and for optimizingthe parameters on test data.
The similar shapes ofthe two curves suggest that the observed variationsare partly due to the problem structure.
In fact, if Nis increased and new hypotheses with a high valueof Pgram(W ) appear, the benefit of the grammar-based language model can increase (if the hypothe-ses are predominantly good with respect to word er-ror rate) or decrease (if they are bad).
This horizoneffect tends to be reduced with increasing N (withthe exception of 89 ?
N ?
93) because hypothe-ses with high ranks need a much higher Pgram(W )in order to compensate for their lower value ofP (O|W ) ?P (W )?.
For small N , the parameter esti-mation is more severely affected by the rather acci-dental horizon effects and therefore is prone to over-fitting.5 Conclusions and OutlookWe have presented a language model based on a pre-cise, linguistically motivated grammar, and we havesuccessfully applied it to a difficult broad-domaintask.It is a well-known fact that natural language ishighly ambiguous: a correct and seemingly unam-biguous sentence may have an enormous number ofreadings.
A related ?
and for our approach evenmore relevant ?
phenomenon is that many weird-looking and seemingly incorrect word sequences arein fact grammatical.
This obviously reduces the ben-efit of pure grammaticality information.
A solutionis to use additional information to asses how ?natu-ral?
a reading of a word sequence is.
We have done a0 20 40 60 80 100?12?10?8?6?4?20N?WER(relative)leave?one?outoptimized on test dataFigure 1: The word error rate as a function of the maxi-mum number of best hypotheses N .first step in this direction by estimating the probabil-ity of a parse tree.
However, our model only looks atthe structure of a parse tree and does not take the ac-tual words into account.
As N-grams and statisticalparsers demonstrate, word information can be veryvaluable.
It would therefore be interesting to investi-gate ways of introducing word information into ourgrammar-based model.AcknowledgementsThis work was supported by the Swiss National Sci-ence Foundation.
We cordially thank Jean-Luc Gau-vain of LIMSI for providing us with word latticesfrom their German broadcast news transcription sys-tem.112ReferencesM.
Adda-Decker.
2003.
A corpus-based decompoundingalgorithm for German lexical modeling in LVCSR.
InProceedings of Eurospeech, pages 257?260, Geneva,Switzerland.R.
Beutler, T. Kaufmann, and B. Pfister.
2005.
Integrat-ing a non-probabilistic grammar into large vocabularycontinuous speech recognition.
In Proceedings of theIEEE ASRU 2005 Workshop, pages 104?109, San Juan(Puerto Rico).S.
Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.2002.
The TIGER treebank.
In Proceedings of theWorkshop on Treebanks and Linguistic Theories, So-zopol, Bulgaria.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the NAACL, pages 132?139, SanFrancisco, USA.C.
Chelba and F. Jelinek.
2000.
Structured languagemodeling.
Computer Speech & Language, 14(4):283?332.K.
W. Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational Linguistics, 16(1):22?29.M.
Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguistics,29(4):589?637.B.
Crysmann.
2003.
On the efficient implementation ofGerman verb placement in HPSG.
In Proceedings ofRANLP.B.
Crysmann.
2005.
Relative clause extraposition inGerman: An efficient and portable implementation.Research on Language and Computation, 3(1):61?82.Duden.
1999. ?
Das gro?e Wo?rterbuch der deutschenSprache in zehn Ba?nden.
Dudenverlag, dritte Auflage.L.
Gillick and S. Cox.
1989.
Some statistical issues inthe comparison of speech recognition algorithms.
InProceedings of the ICASSP, pages 532?535.T.
Kaufmann and B. Pfister.
2007.
Applying licenserrules to a grammar with continuous constituents.
InStefan Mu?ller, editor, The Proceedings of the 14th In-ternational Conference on Head-Driven Phrase Struc-ture Grammar, pages 150?162, Stanford, USA.
CSLIPublications.B.
Kiefer, H.-U.
Krieger, and M.-J.
Nederhof.
2000.
Ef-ficient and robust parsing of word hypotheses graphs.In Wolfgang Wahlster, editor, Verbmobil.
Founda-tions of Speech-to-Speech Translation, pages 280?295.Springer, Berlin, Germany, artificial intelligence edi-tion.K.
McTait and M. Adda-Decker.
2003.
The 300k LIMSIGerman broadcast news transcription system.
In Pro-ceedings of Eurospeech, Geneva, Switzerland.S.
Mu?ller.
1999.
Deutsche Syntax deklarativ.
Head-Driven Phrase Structure Grammar fu?r das Deutsche.Number 394 in Linguistische Arbeiten.
Max NiemeyerVerlag, Tu?bingen.S.
Mu?ller.
2007.
Head-Driven Phrase Structure Gram-mar: Eine Einfu?hrung.
Stauffenburg Einfu?hrungen,Nr.
17.
Stauffenburg Verlag, Tu?bingen.G.
Van Noord, G. Bouma, R. Koeling, and M.-J.
Neder-hof.
1999.
Robust grammatical analysis for spo-ken dialogue systems.
Natural Language Engineer-ing, 5(1):45?93.C.
J. Pollard and I.
A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press,Chicago.A.
Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34(1-3):151?175.B.
Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.M.
Yamamoto and K. W. Church.
2001.
Using suffixarrays to compute term frequency and document fre-quency for all substrings in a corpus.
ComputationalLinguistics, 27(1):1?30.113
