Proceedings of the 12th Conference of the European Chapter of the ACL, pages 42?50,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsSupervised Domain Adaption for WSDEneko Agirre and Oier Lopez de LacalleIXA NLP GroupUniversity of the Basque CountryDonostia, Basque Contry{e.agirre,oier.lopezdelacalle}@ehu.esAbstractThe lack of positive results on super-vised domain adaptation for WSD havecast some doubts on the utility of hand-tagging general corpora and thus devel-oping generic supervised WSD systems.In this paper we show for the first timethat our WSD system trained on a generalsource corpus (BNC) and the target corpus,obtains up to 22% error reduction whencompared to a system trained on the tar-get corpus alone.
In addition, we showthat as little as 40% of the target corpus(when supplemented with the source cor-pus) is sufficient to obtain the same resultsas training on the full target data.
The keyfor success is the use of unlabeled datawith SVD, a combination of kernels andSVM.1 IntroductionIn many Natural Language Processing (NLP)tasks we find that a large collection of manually-annotated text is used to train and test supervisedmachine learning models.
While these modelshave been shown to perform very well when testedon the text collection related to the training data(what we call the source domain), the perfor-mance drops considerably when testing on textfrom other domains (called target domains).In order to build models that perform well innew (target) domains we usually find two settings(Daume?
III, 2007).
In the semi-supervised setting,the training hand-annotated text from the sourcedomain is supplemented with unlabeled data fromthe target domain.
In the supervised setting, weuse training data from both the source and targetdomains to test on the target domain.In (Agirre and Lopez de Lacalle, 2008) westudied semi-supervised Word Sense Disambigua-tion (WSD) adaptation, and in this paper we fo-cus on supervised WSD adaptation.
We comparethe performance of similar supervised WSD sys-tems on three different scenarios.
In the sourceto target scenario the WSD system is trained onthe source domain and tested on the target do-main.
In the target scenario the WSD systemis trained and tested on the target domain (usingcross-validation).
In the adaptation scenario theWSD system is trained on both source and targetdomain and tested in the target domain (also usingcross-validation over the target data).
The sourceto target scenario represents a weak baseline fordomain adaptation, as it does not use any exam-ples from the target domain.
The target scenariorepresents the hard baseline, and in fact, if the do-main adaptation scenario does not yield better re-sults, the adaptation would have failed, as it wouldmean that the source examples are not useful whenwe do have hand-labeled target examples.Previous work shows that current state-of-the-art WSD systems are not able to obtain better re-sults on the adaptation scenario compared to thetarget scenario (Escudero et al, 2000; Agirre andMart?
?nez, 2004; Chan and Ng, 2007).
This wouldmean that if a user of a generic WSD system (i.e.based on hand-annotated examples from a genericcorpus) would need to adapt it to a specific do-main, he would be better off throwing away thegeneric examples and hand-tagging domain exam-ples directly.
This paper will show that domainadaptation is feasible, even for difficult domain-related words, in the sense that generic corporacan be reused when deploying WSD systems inspecific domains.
We will also show that, giventhe source corpus, our technique can save up to60% of effort when tagging domain-related occur-rences.We performed on a publicly available corpuswhich was designed to study the effect of domainsin WSD (Koeling et al, 2005).
It comprises 4142nouns which are highly relevant in the SPORTSand FINANCES domains, with 300 examples foreach.
The use of two target domains strengthensthe conclusions of this paper.Our system uses Singular Value Decomposi-tion (SVD) in order to find correlations betweenterms, which are helpful to overcome the scarcityof training data in WSD (Gliozzo et al, 2005).This work explores how this ability of SVD anda combination of the resulting feature spaces im-proves domain adaptation.
We present two waysto combine the reduced spaces: kernel combina-tion with Support Vector Machines (SVM), and kNearest-Neighbors (k-NN) combination.The paper is structured as follows.
Section 2 re-views prior work in the area.
Section 3 presentsthe data sets used.
In Section 4 we describethe learning features, including the application ofSVD, and in Section 5 the learning methods andthe combination.
The experimental results are pre-sented in Section 6.
Section 7 presents the discus-sion and some analysis of this paper and finallySection 8 draws the conclusions.2 Prior workDomain adaptation is a practical problem attract-ing more and more attention.
In the supervisedsetting, a recent paper by Daume?
III (2007) showsthat a simple feature augmentation method forSVM is able to effectively use both labeled tar-get and source data to provide the best domain-adaptation results in a number of NLP tasks.
Hismethod improves or equals over previously ex-plored more sophisticated methods (Daume?
IIIand Marcu, 2006; Chelba and Acero, 2004).
Thefeature augmentation consists in making three ver-sion of the original features: a general, a source-specific and a target-specific versions.
That waythe augmented source contains the general andsource-specific version and the augmented targetdata general and specific versions.
The idea be-hind this is that target domain data has twice theinfluence as the source when making predictionsabout test target data.
We reimplemented thismethod and show that our results are better.Regarding WSD, some initial works made a ba-sic analysis of domain adaptation issues.
Escud-ero et al (2000) tested the supervised adaptationscenario on the DSO corpus, which had examplesfrom the Brown corpus and Wall Street Journalcorpus.
They found that the source corpus didnot help when tagging the target corpus, show-ing that tagged corpora from each domain wouldsuffice, and concluding that hand tagging a largegeneral corpus would not guarantee robust broad-coverage WSD.
Agirre and Mart?
?nez (2000) usedthe DSO corpus in the supervised scenario to showthat training on a subset of the source corpora thatis topically related to the target corpus does allowfor some domain adaptation.More recently, Chan and Ng (2007) performedsupervised domain adaptation on a manually se-lected subset of 21 nouns from the DSO corpus.They used active learning, count-merging, andpredominant sense estimation in order to save tar-get annotation effort.
They showed that addingjust 30% of the target data to the source exam-ples the same precision as the full combination oftarget and source data could be achieved.
Theyalso showed that using the source corpus allowedto significantly improve results when only 10%-30% of the target corpus was used for training.Unfortunately, no data was given about the targetcorpus results, thus failing to show that domain-adaptation succeeded.
In followup work (Zhong etal., 2008), the feature augmentation approach wascombined with active learning and tested on theOntoNotes corpus, on a large domain-adaptationexperiment.
They reduced significantly the ef-fort of hand-tagging, but only obtained domain-adaptation for smaller fractions of the source andtarget corpus.
Similarly to these works we showthat we can save annotation effort on the targetcorpus, but, in contrast, we do get domain adap-tation when using the full dataset.
In a way ourapproach is complementary, and we could also ap-ply active learning to further reduce the number oftarget examples to be tagged.Though not addressing domain adaptation,other works on WSD also used SVD and areclosely related to the present paper.
Ando (2006)used Alternative Structured Optimization.
Shefirst trained one linear predictor for each targetword, and then performed SVD on 7 carefully se-lected submatrices of the feature-to-predictor ma-trix of weights.
The system attained small butconsistent improvements (no significance data wasgiven) on the Senseval-3 lexical sample datasetsusing SVD and unlabeled data.Gliozzo et al (2005) used SVD to reduce thespace of the term-to-document matrix, and thencomputed the similarity between train and test43instances using a mapping to the reduced space(similar to our SMA method in Section 4.2).
Theycombined other knowledge sources into a complexkernel using SVM.
They report improved perfor-mance on a number of languages in the Senseval-3 lexical sample dataset.
Our present paper dif-fers from theirs in that we propose an additionalmethod to use SVD (the OMT method), and thatwe focus on domain adaptation.In the semi-supervised setting, Blitzer et al(2006) used Structural Correspondence Learningand unlabeled data to adapt a Part-of-Speech tag-ger.
They carefully select so-called ?pivot fea-tures?
to learn linear predictors, perform SVD onthe weights learned by the predictor, and thus learncorrespondences among features in both sourceand target domains.
Our technique also uses SVD,but we directly apply it to all features, and thusavoid the need to define pivot features.
In prelim-inary work we unsuccessfully tried to carry alongthe idea of pivot features to WSD.
On the contrary,in (Agirre and Lopez de Lacalle, 2008) we showthat methods closely related to those presented inthis paper produce positive semi-supervised do-main adaptation results for WSD.The methods used in this paper originated in(Agirre et al, 2005; Agirre and Lopez de Lacalle,2007), where SVD over a feature-to-documentsmatrix improved WSD performance with andwithout unlabeled data.
The use of several k-NN classifiers trained on a number of reduced andoriginal spaces was shown to get the best resultsin the Senseval-3 dataset and ranked second in theSemEval 2007 competition.
The present paper ex-tends this work and applies it to domain adapta-tion.3 Data setsThe dataset we use was designed for domain-relatedWSD experiments by Koeling et al (2005),and is publicly available.
The examples comefrom the BNC (Leech, 1992) and the SPORTS andFINANCES sections of the Reuters corpus (Roseet al, 2002), comprising around 300 examples(roughly 100 from each of those corpora) for eachof the 41 nouns.
The nouns were selected be-cause they were salient in either the SPORTS orFINANCES domains, or because they had senseslinked to those domains.
The occurrences werehand-tagged with the senses from WordNet (WN)version 1.7.1 (Fellbaum, 1998).
In our experi-ments the BNC examples play the role of generalsource corpora, and the FINANCES and SPORTSexamples the role of two specific domain targetcorpora.Compared to the DSO corpus used in prior work(cf.
Section 2) this corpus has been explicitly cre-ated for domain adaptation studies.
DSO con-tains texts coming from the Brown corpus and theWall Street Journal, but the texts are not classi-fied according to specific domains (e.g.
Sports,Finances), which make DSO less suitable to studydomain adaptation.
The fact that the selectednouns are related to the target domain makesthe (Koeling et al, 2005) corpus more demandingthan the DSO corpus, because one would expectthe performance of a generic WSD system to dropwhen moving to the domain corpus for domain-related words (cf.
Table 1), while the performancewould be similar for generic words.In addition to the labeled data, we also useunlabeled data coming from the three sourcesused in the labeled corpus: the ?written?
partof the BNC (89.7M words), the FINANCES partof Reuters (32.5M words), and the SPORTS part(9.1M words).4 Original and SVD featuresIn this section, we review the features and twomethods to apply SVD over the features.4.1 FeaturesWe relied on the usual features used in previousWSD work, grouped in three main sets.
Localcollocations comprise the bigrams and trigramsformed around the target word (using either lem-mas, word-forms, or PoS tags) , those formedwith the previous/posterior lemma/word-form inthe sentence, and the content words in a ?4-wordwindow around the target.
Syntactic dependen-cies use the object, subject, noun-modifier, prepo-sition, and sibling lemmas, when available.
Fi-nally, Bag-of-words features are the lemmas ofthe content words in the whole context, plus thesalient bigrams in the context (Pedersen, 2001).We refer to these features as original features.4.2 SVD featuresApart from the original space of features, we haveused the so called SVD features, obtained fromthe projection of the feature vectors into the re-duced space (Deerwester et al, 1990).
Basically,44we set a term-by-document or feature-by-examplematrix M from the corpus (see section below formore details).
SVD decomposes M into three ma-trices, M = U?V T .
If the desired number ofdimensions in the reduced space is p, we select prows from ?
and V , yielding ?p and Vp respec-tively.
We can map any feature vector ~t (whichrepresents either a train or test example) into thep-dimensional space as follows: ~tp = ~tTVp?
?1p .Those mapped vectors have p dimensions, andeach of the dimensions is what we call a SVD fea-ture.
We have explored two different variants inorder to build the reduced matrix and obtain theSVD features, as follows.Single Matrix for All target words (SVD-SMA).
The method comprises the following steps:(i) extract bag-of-word features (terms in this case)from unlabeled corpora, (ii) build the term-by-document matrix, (iii) decompose it with SVD, and(iv) map the labeled data (train/test).
This tech-nique is very similar to previous work on SVD(Gliozzo et al, 2005; Zelikovitz and Hirsh, 2001).The dimensionality reduction is performed once,over the whole unlabeled corpus, and it is then ap-plied to the labeled data of each word.
The re-duced space is constructed only with terms, whichcorrespond to bag-of-words features, and thus dis-cards the rest of the features.
Given that the WSDliterature shows that all features are necessary foroptimal performance (Pradhan et al, 2007), wepropose the following alternative to construct thematrix.OneMatrix per Target word (SVD-OMT).
Foreach word: (i) construct a corpus with its occur-rences in the labeled and, if desired, unlabeled cor-pora, (ii) extract all features, (iii) build the feature-by-example matrix, (iv) decompose it with SVD,and (v) map all the labeled training and test datafor the word.
Note that this variant performs oneSVD process for each target word separately, henceits name.When building the SVD-OMT matrices we canuse only the training data (TRAIN) or both the trainand unlabeled data (+UNLAB).
When building theSVD-SMA matrices, given the small size of the in-dividual word matrices, we always use both thetrain and unlabeled data (+UNLAB).
Regarding theamount of data, based also on previous work, weused 50% of the available data for OMT, and thewhole corpora for SMA.
An important parameterwhen doing SVD is the number of dimensions inthe reduced space (p).
We tried two different val-ues for p (25 and 200) in the BNC domain, andset a dimension for each classifier/matrix combi-nation.4.3 MotivationThe motivation behind our method is that althoughthe train and test feature vectors overlap suffi-ciently in the usual WSD task, the domain dif-ference makes such overlap more scarce.
SVDimplicitly finds correlations among features, as itmaps related features into nearby regions in the re-duced space.
In the case of SMA, SVD is appliedover the joint term-by-document matrix of labeled(and possibly unlabeled corpora), and it thus canfind correlations among closely related words (e.g.cat and dog).
These correlations can help reducethe gap among bag-of-words features from thesource and target examples.
In the case of OMT,SVD over the joint feature-by-example matrix oflabeled and unlabeled examples of a word allowsto find correlations among features that show sim-ilar occurrence patterns in the source and targetcorpora for the target word.5 Learning methodsk-NN is a memory based learning method, wherethe neighbors are the k most similar labeled exam-ples to the test example.
The similarity among in-stances is measured by the cosine of their vectors.The test instance is labeled with the sense obtain-ing the maximum sum of the weighted vote of thek most similar contexts.
We set k to 5 based onprevious results published in (Agirre and Lopez deLacalle, 2007).Regarding SVM, we used linear kernels, butalso purpose-built kernels for the reduced spacesand the combinations (cf.
Section 5.2).
We usedthe default soft margin (C=0).
In previous ex-periments we learnt that C is very dependent onthe feature set and training data used.
As wewill experiment with different features and train-ing datasets, it did not make sense to optimize itacross all settings.We will now detail how we combined the origi-nal and SVD features in each of the machine learn-ing methods.5.1 k-NN combinationsOur k-NN combination method (Agirre et al,2005; Agirre and Lopez de Lacalle, 2007) takes45advantage of the properties of k-NN classifiers andexploit the fact that a classifier can be seen ask points (number of nearest neighbor) each cast-ing one vote.
This makes easy to combine sev-eral classifiers, one for each feature space.
For in-stance, taking two k-NN classifiers of k = 5, C1andC2, we can combine them into a single k = 10classifier, where five votes come from C1 and fivefrom C2.
This allows to smoothly combine classi-fiers from different feature spaces.In this work we built three single k-NN classi-fiers trained on OMT, SMA and the original fea-tures, respectively.
In order to combine them weweight each vote by the inverse ratio of its positionin the rank of the single classifier, (k ?
ri + 1)/k,where ri is the rank.5.2 Kernel combinationThe basic idea of kernel methods is to find a suit-able mapping function (?)
in order to get a bettergeneralization.
Instead of doing this mapping ex-plicitly, kernels give the chance to do it inside thealgorithm.
We will formalize it as follows.
First,we define the mapping function ?
: X ?
F .
Oncethe function is defined, we can use it in the kernelfunction in order to become an implicit functionK(x, z) = ??
(x) ?
?
(z)?, where ???
denotes a in-ner product between vectors in the feature space.This way, we can very easily define mappingsrepresenting different information sources and usethis mappings in several machine learning algo-rithm.
In our work we use SVM.We defined three individual kernels (OMT, SMAand original features) and the combined kernel.The original feature kernel (KOrig) is given bythe identity function over the features ?
: X ?
X ,defining the following kernel:KOrig(xi,xj) =?xi ?
xj??
?xi ?
xi?
?xj ?
xj?where the denominator is used to normalize andavoid any kind of bias in the combination.The OMT kernel (KOmt) and SMA kernel(KSma) are defined using OMT and SMA projec-tion matrices, respectively (cf.
Section 4.2).
Giventhe OMT function mapping ?omt : Rm ?
Rp,where m is the number of the original featuresand p the reduced dimensionality, then we defineKOmt(xi,xj) as follows (KSma is defined simi-larly):?
?omt(xi) ?
?omt(xj)???
?omt(xi) ?
?omt(xi)?
?
?omt(xj) ?
?omt(xj)?BNC ?
X SPORTS FINANCESMFS 39.0 51.2k-NN 51.7 60.4SVM 53.9 62.9Table 1: Source to target results: Train on BNC,test on SPORTS and FINANCES.Finally, we define the kernel combination:KComb(xi,xj) =n?l=1Kl(xi,xj)?Kl(xi,xi)Kl(xj,xj)where n is the number of single kernels explainedabove, and l the index for the kernel type.6 Domain adaptation experimentsIn this section we present the results in our two ref-erence scenarios (source to target, target) and ourreference scenario (domain adaptation).
Note thatall methods presented here have full coverage, i.e.they return a sense for all test examples, and there-fore precision equals recall, and suffices to com-pare among systems.6.1 Source to target scenario: BNC ?
XIn this scenario our supervised WSD systems aretrained on the general source corpus (BNC) andtested on the specific target domains separately(SPORTS and FINANCES).
We do not perform anykind of adaptation, and therefore the results arethose expected for a generic WSD system whenapplied to domain-specific texts.Table 1 shows the results for k-NN and SVMtrained with the original features on the BNC.
Inaddition, we also show the results for the MostFrequent Sense baseline (MFS) taken from theBNC.
The second column denotes the accuraciesobtained when testing on SPORTS, and the thirdcolumn the accuracies for FINANCES.
The low ac-curacy obtained with MFS, e.g.
39.0 of precisionin SPORTS, shows the difficulty of this task.
Bothclassifiers improve over MFS.
These classifiers areweak baselines for the domain adaptation system.6.2 Target scenario X ?
XIn this scenario we lay the harder baseline whichthe domain adaptation experiments should im-prove on (cf.
next section).
The WSD systemsare trained and tested on each of the target cor-pora (SPORTS and FINANCES) using 3-fold cross-validation.46SPORTS FINANCESX ?
X TRAIN +UNLAB TRAIN +UNLABMFS 77.8 - 82.3 -k-NN 84.5 - 87.1 -SVM 85.1 - 87.0 -k-NN-OMT 85.0 86.1 87.3 87.6SVM-OMT 82.9 85.1 85.3 86.4k-NN-SMA - 81.1 - 83.2SVM-SMA - 81.3 - 84.1k-NN-COMB 86.
0 86.7 87.9 88.6SVM-COMB - 86.5 - 88.5Table 2: Target results: train and test on SPORTS,train and test on FINANCES, using 3-fold cross-validation.Table 2 summarizes the results for this scenario.TRAIN denotes that only tagged data was used totrain, +UNLAB denotes that we added unlabeleddata related to the source corpus when computingSVD.
The rows denote the classifier and the featurespaces used, which are organized in four sections.On the top rows we show the three baseline clas-sifiers on the original features.
The two sectionsbelow show the results of those classifiers on thereduced dimensions, OMT and SMA (cf.
Section4.2).
Finally, the last rows show the results of thecombination strategies (cf.
Sections 5.1 and 5.2).Note that some of the cells have no result, becausethat combination is not applicable (e.g.
using thetrain and unlabeled data in the original space).First of all note that the results for the base-lines (MFS, SVM, k-NN) are much larger thanthose in Table 1, showing that this dataset is spe-cially demanding for supervised WSD, and partic-ularly difficult for domain adaptation experiments.These results seem to indicate that the examplesfrom the source general corpus could be of littleuse when tagging the target corpora.
Note spe-cially the difference in MFS performance.
The pri-ors of the senses are very different in the sourceand target corpora, which is a well-known short-coming for supervised systems.
Note the high re-sults of the baseline classifiers, which leave smallroom for improvement.The results for the more sophisticated methodsshow that SVD and unlabeled data helps slightly,except for k-NN-OMT on SPORTS.
SMA de-creases the performance compared to the classi-fiers trained on original features.
The best im-provements come when the three strategies arecombined in one, as both the kernel and k-NNcombinations obtain improvements over the re-spective single classifiers.
Note that both the k-NNBNC + X SPORTS FINANCES?
X TRAIN + UNLAB TRAIN + UNLABBNC ?
X 53.9 - 62.9 -X ?
X 86.0 86.7 87.9 88.5MFS 68.2 - 73.1 -k-NN 81.3 - 86.0 -SVM 84.7 - 87.5 -k-NN-OMT 84.0 84.7 87.5 86.0SVM-OMT 85.1 84.7 84.2 85.5k-NN-SMA - 77.1 - 81.6SVM-SMA - 78.1 - 80.7k-NN-COMB 84.5 87.2 88.1 88.7SVM-COMB - 88.4 - 89.7SVM-AUG 85.9 - 88.1 -Table 3: Domain adaptation results: Train onBNC and SPORTS, test on SPORTS (same for FI-NANCES).and SVM combinations perform similarly.In the combination strategy we show that unla-beled data helps slightly, because instead of onlycombining OMT and original features we have theopportunity to introduce SMA.
Note that it was notour aim to improve the results of the basic classi-fiers on this scenario, but given the fact that we aregoing to apply all these techniques in the domainadaptation scenario, we need to show these resultsas baselines.
That is, in the next section we will tryto obtain results which improve significantly overthe best results in this section.6.3 Domain adaptation scenarioBNC + X ?
XIn this last scenario we try to show that our WSDsystem trained on both source (BNC) and tar-get (SPORTS and FINANCES) data performs betterthan the one trained on the target data alone.
Wealso use 3-fold cross-validation for the target data,but the entire source data is used in each turn.
Theunlabeled data here refers to the combination ofunlabeled source and target data.The results are presented in table 3.
Again, thecolumns denote if unlabeled data has been used inthe learning process.
The rows correspond to clas-sifiers and the feature spaces involved.
The firstrows report the best results in the previous scenar-ios: BNC ?
X for the source to target scenario,and X ?
X for the target scenario.
The restof the table corresponds to the domain adaptationscenario.
The rows below correspond to MFS andthe baseline classifiers, followed by the OMT andSMA results, and the combination results.
The lastrow shows the results for the feature augmentationalgorithm (Daume?
III, 2007).47SPORTS FINANCESBNC ?
XMFS 39.0 51.2SVM 53.9 62.9X ?
XMFS 77.8 82.3SVM 85.1 87.0k-NN-COMB (+UNLAB) 86.7 88.6BNC +X ?
XMFS 68.2 73.1SVM 84.7 87.5SVM-AUG 85.9 88.1SVM-COMB (+UNLAB) 88.4 89.7Table 4: The most important results in each sce-nario.Focusing on the results, the table shows thatMFS decreases with respect to the target scenario(cf.
Table 2) when the source data is added, prob-ably caused by the different sense distributions inBNC and the target corpora.
The baseline classi-fiers (k-NN and SVM) are not able to improve overthe baseline classifiers on the target data alone,which is coherent with past research, and showsthat straightforward domain adaptation does notwork.The following rows show that our reductionmethods on themselves (OMT, SMA used by k-NN and SVM) also fail to perform better than inthe target scenario, but the combinations usingunlabeled data (k-NN-COMB and specially SVM-COMB) do manage to improve the best results forthe target scenario, showing that we were able toattain domain adaptation.
The feature augmenta-tion approach (SVM-AUG) does improve slightlyover SVM in the target scenario, but not over thebest results in the target scenario, showing the dif-ficulty of domain adaptation for WSD, at least onthis dataset.7 Discussion and analysisTable 4 summarizes the most important results.The kernel combination method with unlabeleddata on the adaptation scenario reduces the erroron 22.1% and 17.6% over the baseline SVM onthe target scenario (SPORTS and FINANCES re-spectively), and 12.7% and 9.0% over the k-NNcombination method on the target scenario.
Thesegains are remarkable given the already high base-line, specially taking into consideration that the41 nouns are closely related to the domains.
Thedifferences, including SVM-AUG, are statisticallysignificant according to the Wilcoxon test with%25 %32 %50 %62 %75 %82 %100sports (%)8082848688accuracy(%)SVM-COMB (+UNLAB, BNC + SPORTS -> SPORTS)SVM-AUG (BNC + SPORTS -> SPORTS)SVM-ORIG (SPORTS -> SPORTS)y=85.1Figure 1: Learning curves for SPORTS.
The Xaxis denotes the amount of SPORTS data and theY axis corresponds to accuracy.%25 %32 %50 %62 %75 %82 %100finances (%)84868890accuracy(%)SVM-COMB (+UNLAB, BNC + FIN.
-> FIN.
)SVM-AUG (BNC + FIN.
-> FIN.
)SVM-ORIG (FIN.
-> FIN.
)y=87.0Figure 2: Learning curves for FINANCES.
The Xaxis denotes the amount of FINANCES data and Yaxis corresponds to the accuracy.p < 0.01.In addition, we carried extra experiments to ex-amine the learning curves, and to check, giventhe source examples, how many additional ex-amples from the target corpus are needed to ob-tain the same results as in the target scenario us-ing all available examples.
We fixed the sourcedata and used increasing amounts of target data.We show the original SVM on the target scenario,and SVM-COMB (+UNLAB) and SVM-AUG as thedomain adaptation approaches.
The results areshown in figure 1 for SPORTS and figure 2 for FI-NANCES.
The horizontal line corresponds to theperformance of SVM on the target domain.
Thepoint where the learning curves cross the horizon-tal line show that our domain adaptation methodneeds only around 40% of the target data in orderto get the same performance as the baseline SVMon the target data.
The learning curves also shows48that the domain adaptation kernel combination ap-proach, no matter the amount of target data, is al-ways above the rest of the classifiers, showing therobustness of our approach.8 Conclusion and future workIn this paper we explore supervised domain adap-tation for WSD with positive results, that is,whether hand-labeling general domain (source)text is worth the effort when training WSD sys-tems that are to be applied to specific domains (tar-gets).
We performed several experiments in threescenarios.
In the first scenario (source to targetscenario), the classifiers were trained on sourcedomain data (the BNC) and tested on the target do-mains, composed by the SPORTS and FINANCESsections of Reuters.
In the second scenario (tar-get scenario) we set the main baseline for our do-main adaptation experiment, training and testingour classifiers on the target domain data.
In the lastscenario (domain adaptation scenario), we com-bine both source and target data for training, andtest on the target data.We report results in each scenario for k-NN andSVM classifiers, for reduced features obtained us-ing SVD over the training data, for the use of un-labeled data, and for k-NN and SVM combinationsof all.Our results show that our best domain adap-tation strategy (using kernel combination of SVDfeatures and unlabeled data related to the trainingdata) yields statistically significant improvements:up to 22% error reduction compared to SVM onthe target domain data alone.
We also show thatour domain adaptation method only needs 40% ofthe target data (in addition to the source data) inorder to get the same results as SVM on the targetalone.We obtain coherent results in two target scenar-ios, and consistent improvement at all levels ofthe learning curves, showing the robustness or ourfindings.
We think that our dataset, which com-prises examples for 41 nouns that are closely re-lated to the target domains, is specially demand-ing, as one would expect the performance of ageneric WSD system to drop when moving tothe domain corpus, specially on domain-relatedwords, while we could expect the performance tobe similar for generic or unrelated words.In the future we would like to evaluateour method on other datasets (e.g.
DSO orOntoNotes), to test whether the positive results areconfirmed.
We would also like to study word-by-word behaviour, in order to assess whether targetexamples are really necessary for words which areless related to the domain.AcknowledgmentsThis work has been partially funded by the EU Commission(project KYOTO ICT-2007-211423) and Spanish ResearchDepartment (project KNOW TIN2006-15049-C03-01).
OierLopez de Lacalle has a PhD grant from the Basque Govern-ment.ReferencesEneko Agirre and Oier Lopez de Lacalle.
2007.
Ubc-alm: Combining k-nn with svd for wsd.
In Pro-ceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 342?345, Prague, Czech Republic, June.
Association forComputational Linguistics.Eneko Agirre and Oier Lopez de Lacalle.
2008.
Onrobustness and domain adaptation using SVD forword sense disambiguation.
In Proceedings of the22nd International Conference on ComputationalLinguistics (Coling 2008), pages 17?24, Manch-ester, UK, August.
Coling 2008 Organizing Com-mittee.Eneko Agirre and David Mart??nez.
2004.
The effectof bias on an automatically-built word sense corpus.Proceedings of the 4rd International Conference onLanguages Resources and Evaluations (LREC).E.
Agirre, O.Lopez de Lacalle, and David Mart??nez.2005.
Exploring feature spaces with svd and un-labeled data for Word Sense Disambiguation.
InProceedings of the Conference on Recent Advanceson Natural Language Processing (RANLP?05),Borovets, Bulgaria.Rie Kubota Ando.
2006.
Applying alternating struc-ture optimization to word sense disambiguation.
InProceedings of the 10th Conference on Computa-tional Natural Language Learning (CoNLL), pages77?84, New York City.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 120?128, Sydney, Australia, July.Association for Computational Linguistics.Yee Seng Chan and Hwee Tou Ng.
2007.
Do-main adaptation with active learning for word sensedisambiguation.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 49?56, Prague, Czech Republic,June.
Association for Computational Linguistics.49Ciprian Chelba and Alex Acero.
2004.
Adaptationof maximum entropy classifier: Little data can helpa lot.
In Proceedings of of th Conference on Em-pirical Methods in Natural Language Processing(EMNLP), Barcelona, Spain.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26:101?126.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Scott Deerwester, Susan Dumais, Goerge Furnas,Thomas Landauer, and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
Journalof the American Society for Information Science,41(6):391?407.Gerard Escudero, Lluiz Ma?rquez, and German Rigau.2000.
An Empirical Study of the Domain Depen-dence of Supervised Word Sense DidanbiguationSystems.
Proceedings of the joint SIGDAT Con-ference on Empirical Methods in Natural LanguageProcessing and Very Large Corpora, EMNLP/VLC.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Alfio Massimiliano Gliozzo, Claudio Giuliano, andCarlo Strapparava.
2005.
Domain Kernels for WordSense Disambiguation.
43nd Annual Meeting of theAssociation for Computational Linguistics.
(ACL-05).R.
Koeling, D. McCarthy, and J. Carroll.
2005.Domain-specific sense distributions and predomi-nant sense acquisition.
In Proceedings of the Hu-man Language Technology Conference and Confer-ence on Empirical Methods in Natural LanguageProcessing.
HLT/EMNLP, pages 419?426, Ann Ar-bor, Michigan.G.
Leech.
1992.
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.David Mart?
?nez and Eneko Agirre.
2000.
One Senseper Collocation and Genre/Topic Variations.
Con-ference on Empirical Method in Natural Language.T.
Pedersen.
2001.
A Decision Tree of Bigrams is anAccurate Predictor of Word Sense.
In Proceedingsof the Second Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL-01), Pittsburgh, PA.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
Semeval-2007 task-17: En-glish lexical sample, srl and all words.
In Proceed-ings of the Fourth International Workshop on Se-mantic Evaluations (SemEval-2007), pages 87?92,Prague, Czech Republic.Tony G. Rose, Mark Stevenson, and Miles Whitehead.2002.
The reuters corpus volumen 1 from yester-day?s news to tomorrow?s language resources.
InProceedings of the Third International Conferenceon Language Resources and Evaluation (LREC-2002), pages 827?832, Las Palmas, Canary Islands.Sarah Zelikovitz and Haym Hirsh.
2001.
Using LSIfor text classification in the presence of backgroundtext.
In Henrique Paques, Ling Liu, and DavidGrossman, editors, Proceedings of CIKM-01, 10thACM International Conference on Information andKnowledge Management, pages 113?118, Atlanta,US.
ACM Press, New York, US.Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan.
2008.Word sense disambiguation using OntoNotes: Anempirical study.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1002?1010, Honolulu, Hawaii,October.
Association for Computational Linguistics.50
