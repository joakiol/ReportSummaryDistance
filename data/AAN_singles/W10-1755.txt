Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 360?364,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsThe Parameter-optimized ATEC Metric for MT EvaluationBilly T-M Wong               Chunyu KitDepartment of Chinese, Translation and LinguisticsCity University of Hong KongTat Chee Avenue, Kowloon, Hong Kong{ctbwong, ctckit}@cityu.edu.hkAbstractThis paper describes the latest version of theATEC metric for automatic MT evaluation,with parameters optimized for word choiceand word order, the two fundamental featuresof language that the metric relies on.
Theformer is assessed by matching at variouslinguistic levels and weighting the informa-tiveness of both matched and unmatchedwords.
The latter is quantified in term ofword position and information flow.
We alsodiscuss those aspects of language not yetcovered by other existing evaluation metricsbut carefully considered in the formulation ofour metric.1 IntroductionIt is recognized that the proposal of the BLEUmetric (Papineni et al, 2002) has piloted a para-digm evolution to MT evaluation.
It provides acomputable solution to the task and turns it intoan engineering problem of measuring text simi-larity and simulating human judgments of trans-lation quality.
Related studies in recent yearshave extensively revealed more essential charac-teristics of BLEU, including its strengths andweaknesses.
This has aroused the proposal ofdifferent new evaluation metrics aimed at ad-dressing such weaknesses so as to find some oth-er hopefully better alternatives for the task.
Ef-fort in this direction brings up some advancedmetrics such as METEOR (Banerjee and Lavie,2005) and TERp (Snover et al, 2009) that seemto have already achieved considerably strongcorrelations with human judgments.
Nevertheless,few metrics have really nurtured our understand-ing of possible parameters involved in our lan-guage comprehension and text quality judgment.This inadequacy limits, inevitably, the applica-tion of the existing metrics.The ATEC metric (Wong and Kit, 2008) wasdeveloped as a response to this inadequacy, witha focus to account for the process of humancomprehension of sentences via two fundamentalfeatures of text, namely word choice and wordorder.
It integrates various explicit measures forthese two features in order to provide an intuitiveand informative evaluation result.
Its previousversion (Wong and Kit, 2009b) has already illu-strated a highly comparable performance to thefew state-of-the-art evaluation metrics, showinga great improvement over its initial version forparticipation in MetricsMATR081.
It is also ap-plied to evaluate online MT systems for legaltranslation, to examine its applicability for layusers?
use to select appropriate MT systems(Wong and Kit, 2009a).In this paper we describe the formulation ofATEC, including its new features and optimiza-tion of parameters.
In particular we will discusshow the design of this metric can complementthe inadequacies of other metrics in terms of itstreatment of word choice and word order and itsutilization of multiple references in the evalua-tion process.2 The ATEC Metric2.1 Word ChoiceIn general, word is the basic meaning bearingunit of language.
In a semantic theory such asLatent Semantic Analysis (LSA) (Landauer et al,1998), lexical selection is even the sole consider-ation of the meaning of a text.
A recent study ofthe major errors in MT outputs by Vilar et al(2006) also reveals that different kinds of errorrelated to word choices constitute a majority oferror types.
It is therefore of prime importance1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2008/360for MT evaluation metrics to diagnose the ade-quacy of word selection by an MT system.It is a general consensus that the performanceof an evaluation metric can be improved bymatching more words between MT outputs andhuman references.
Linguistic resources likestemmer and WordNet are widely applied bymany metrics for matching word stems and syn-onyms.
ATEC is equipped with these two mod-ules as well, and furthermore, with two measuresfor word similarity, including a WordNet-based(Wu and Palmer, 1994) and a corpus-basedmeasure (Landauer et al, 1998) for matchingword pairs of similar meanings.
Our previouswork (Wong, 2010) shows that the inclusion ofsemantically similar words results in a positivecorrelation gain comparable to the use of Word-Net for synonym identification.In addition to increasing the number of legiti-mate matches, we also consider the importanceof each match.
Although most metrics scoreevery matched word with equal weight, differentwords indeed contribute different amount of in-formation to the meaning of a sentence.
In Ex-ample 1 below, both C1 and C2 contain the samenumber of words matched with Ref, but thematches in C1 are more informative and there-fore should be assigned higher weights.Example 1C1: it was not first time that prime minister con-fronts northern league ?C2: this is not the prime the operation with thenorth ?Ref: this is not the first time the prime ministerhas faced the northern league ?The informativeness of a match is weighted bythe tf-idf measure, which has been widely used ininformation retrieval to assess the relative impor-tance of a word as an indexing term for a docu-ment.
A word is more important to a documentwhen it occurs more frequently in this documentand less in others.
In ATEC, we have ?document?to refer to ?sentence?, the basic text unit in MTevaluation.
This allows a more sensitive measurefor words in different sentences, and gets aroundthe problem of an evaluation dataset containingonly one or a few long documents.
Accordingly,the tf-idf measure is formulated as:)log(),( ,iji sfNtfjitfidf ?=where tfi,j is the occurrences of word wi in sen-tence sj, sfi the number of sentences containingword wi, and N the total number of sentences inthe evaluation set.
In case of a high-frequencyword whose tf-idf weight is less than 1, it is thenrounded up to 1.In addition to matched words, unmatchedwords are also considered to have a role to playin determining the quality of word choices of anMT output.
As illustrated in Example 1, the un-matched words in Ref for C1 and C2 are [this | is| the | the | has | faced | the] and [first | time | mi-nister | has | faced | northern | league] respective-ly.
One can see that the words missing in C2 aremore significant.
It is therefore necessary to ap-ply the tf-idf weighting to unmatched referencewords so as to quantify the information missed inthe MT outputs in question.2.2 Word OrderIn MT evaluation, word order refers to the extentto which an MT output is interpretable followingthe information flow of its reference translation.It is not rare that an MT output has manymatched words but does not make sense becauseof a problematic word order.
Currently it is ob-served that consecutive matches represent a legi-timate local ordering, causing some metrics toextend the unit of matching from word to phrase.Birch et al (2010) show, however, that the cur-rent metrics including BLEU, METEOR andTER are highly lexical oriented and still cannotdistinguish between sentences of different wordorders.
This is a serious problem in MT evalua-tion, for many MT systems have become capableof generating more and more suitable words intranslations, resulting in that the quality differ-ence of their outputs lies more and more crucial-ly in the variances of word order.ATEC uses three explicit features for word or-der, namely position distance, order distance andphrase size.
Position distance refers to the diver-gence of the locations of matches in an MT out-put and its reference.
Example 2 illustrates twocandidates with the same match, whose positionin C1 is closer to its corresponding position inRef than that in C2.
We conceive this as a signif-icant indicator of the accuracy of word order: thecloser the positions of a matched word in thecandidate and reference, the better match it is.Example 2C1: non-signatories these acts victims but itcaused to incursion transcendantC2: non-signatories but it caused to incursiontranscendant these acts victimsRef: there were no victims in this incident butthey did cause massive damage361The calculation of position distance is basedon the position indices of words in a sentence.
Inparticular, we align every word in a candidate toits closest counterpart in a reference.
In Example3, all the candidate words have a match in thereference.
As illustrated by the two ?a?
in thecandidate, the shortest alignments (strict lines)are preferred over any farther alternatives (dashlines).
In a case like this, only two matches, i.e.,thief and police, vary in position by a distance of3.Example 3Candidate: a thief chases a policePos distance:        0     3         0      0      3Pos index: 1    2         3      4      5Reference: a police chases a thiefPos index: 1     2         3        4       5This position distance is sensitive to sentencelength as it simply makes use of word positionindices without any normalization.
Example 4illustrates two cases of different lengths.
The po-sition distance of the bold matched words is 3 inC1 but 14 in C2.
Indeed, the divergence of wordorder in C1 does not hinder our understanding,but in C2 it poses a serious problem.
This exces-sive length inevitably magnifies the interferenceeffect of word order divergence.Example 4C1: Short1 and2 various3 international4 news5R1: International1 news2 brief3C2: Is1 on2 a3 popular4 the5 very6 in7 Iraq8 to9those10 just11 like12 other13 world14 in15which16 young17 people18 with19 the20 and21flowers22 while23 awareness24 by25 other26times27 of28 the29 countries30 of31 the32R2: Valentine?s1 day2 is3 a4 very5 popular6 day7in8 Iraq9 as10 it11 is12 in13 the14 other15 coun-tries16 of17 the18 world19.
Young20 men21 ex-change22 with23 their24 girlfriends25 sweets26,flowers27, perfumes28 and29 other30 gifts31.Another feature, the order distance, concernsthe information flow of a sentence in the form ofthe sequence of matches.
Each match in a candi-date and a reference is first assigned an orderindex in a sequential manner.
Then, the differ-ence of two counterpart indices is measured, soas to see if a variance exists.
Examples 5a and 5bexemplify two kinds of order distance and theircorresponding position distance.
Both cases havetwo matches with the same sum of position dis-tance.
However, the matches are in an identicalsequence in 5a but cause a cross in 5b, resultingin a larger order distance for the latter.Example 5aPosition indexOrder indexCandidate:Reference:Order indexPosition indexPosition distanceOrder distance1      2     3     41            2A    B    C    DB    E    D    F1           21     2     3     4(2-1) + (4-3) = 2(1-1) + (2-2) = 0Example 5bPosition indexOrder indexCandidate:Reference:Order indexPosition indexPosition distanceOrder distance1      2     3     41     2A    B    C    DC    B    E    F1      21      2     3     4(2-2) + (3-1) = 2(2-1) + (2-1) = 2In practice, ATEC operates on phrases likemany other metrics.
But unlike these metrics thatcount only the number of matched phrases,ATEC gives extra credit to a longer phrase toreward its valid word sequence.
In Example 6,C1 and C2 represent two MT outputs of the samelength, with matched words underlined.
Bothhave 10 matches in 3 phrases, and will receivethe same evaluation score from a metric likeMETEOR or TERp, ignoring the subtle differ-ence in the sizes of the matched phrases, whichare [8,1,1] and [4,3,3] words for C1 and C2 re-spectively.
In contrast, ATEC uses the size of aphrase as a reduction factor to its position dis-tance, so as to raise the contribution of a largerphrase to the metric score.Example 6C1: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13C2: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w132.3 Multiple ReferencesThe availability of multiple references allowsmore legitimate word choices and word order ofan MT output to be accounted.
Some existingmetrics only compute the scores of a candidateagainst each reference and select the highest one.362This deficit can be illustrated by a well-knownexample from Papineni et al (2002), as repli-cated in Example 7 with slight modification.
Itshows that nearly all candidate words can findtheir matches in either reference.
However, if weresort to single reference, only around half ofthem can have a match, which would seriouslyunderrate the quality of the candidate.Example 7C:   It is a guide to action which ensures that themilitary always obeys the commands of theparty.R1: It is a guide to action that ensures that themilitary will forever heed Party commands.R2: It is the guiding principle which guaranteesthe military forces always being under thecommands of the party.ATEC exploits multiple references in this fa-shion to maximize the number of matches in acandidate.
It begins with aligning the longestmatches with either reference.
The one with theshortest position distance is preferred if morethan one alternative available in the same phrasesize.
This process repeats until no more candi-date word can find a match.2.4 Formulation of ATECThe computation of an ATEC score begins withalignment of phrases, as described above.
Foreach matched phase, we first sum up the score ofeach word i in the phrase as??
?=}{)(phrasei imatchtypematch tfidfInfowWwhere wtype refers to a basic score of a matchedword depending on its match type.
It is thenminus its information load, i.e., the tf-idf score ofthe matched word with a weight factor, Infomatch.There is also a distance penalty for a phrase,orderorderepospos diswcpdiswDis +?= )||||1(,where dispos and disorder refer to the positiondistance and order distance, and wpos and worderare their corresponding weight factors,respectively.
The position distance is furtherweighted according to the size of phrase |p| withan exponential factor e, in proportion to thelength of candidate |c|.The score of a matched phrase is thencomputed by????
?=,,DisWLimitWPhrasematchdismatch  if  Dis > Wmatch?Limitdis;otherwise,Limitdis is an upper limit for the distance penalty.Accordingly, the score C of all phrases in a can-didate is?
?=}{candidatejjPhraseC.Then, we move on to calculating the informa-tion load of unmatched reference words Wunmatch,approximated as)(}{??
?=unmatchk kunmatchtypeunmatch tfidfInfowW.The overall score M accounting for both thematched and unmatched is defined as????
?=,,unmatchInfoWCLimitCMif  Wunmatch > C?LimitInfo;otherwise,LimitInfo is an upper limit for the informationpenalty of the unmatched words.Finally, the ATEC score is computed using theconventional F-measure in terms of precision Pand recall R asRPPRATEC)1( ??
?+=where             || cMP =,|| rMR =.The parameter ?
adjusts the weights of P and R,and |c| and |r| refer to the length of candidate andreference, respectively.
In the case of multiplereferences, |r| refers to the average length of ref-erences.We have derived the optimized values for theparameters involved in ATEC calculation usingthe development data of NIST MetricsMATR10with adequacy assessments by a simple hillclimbing approach.
The optimal parameter set-ting is presented in Table 1 below.3 ConclusionIn the above sections we have presented the lat-est version of our ATEC metric with particularemphasis on word choice and word order as twofundamental features of language.
Each of thesefeatures contains multiple parameters intended to363have a comprehensive coverage of different tex-tual factors involved in our interpretation of asentence.
The optimal offsetting for the parame-ters is expected to report an empirical observa-tion of the relative merits of each factor in ade-quacy assessment.
We are currently exploringtheir relation with the errors of MT outputs, toexamine the potential of automatic error analysis.The ATEC package is obtainable at:http://mega.ctl.cityu.edu.hk/ctbwong/ATEC/AcknowledgmentsThe research work described in this paper wassupported by City University of Hong Kongthrough the Strategic Research Grant (SRG)7002267.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
Pro-ceedings of Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summariza-tion at the 43th Annual Meeting of the Associationof Computational Linguistics (ACL), pages 65-72,Ann Arbor, Michigan, June 2005.Alexandra Birch, Miles Osborne and Phil Blunsom.2010.
Metrics for MT Evaluation: EvaluatingReordering.
Machine Translation (forthcoming).Thomas Landauer, Peter W. Foltz and Darrell Laham.1998.
Introduction to Latent Semantic Analysis.Discourse Processes 25: 259?284.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: A Method for Auto-matic Evaluation of Machine Translation.
Proceed-ings of 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318,Philadelphia, PA, July 2002.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, Adequacy, orHTER?
Exploring Different Human Judgmentswith a Tunable MT Metric.
Proceedings of theFourth Workshop on Statistical Machine Transla-tion at the 12th Meeting of the European Chapterof the Association for Computational Linguistics(EACL), pages 259-268, Athens, Greece, March,2009.David Vilar, Jia Xu, Luis Fernando D'Haro and Her-mann Ney.
2006.
Error Analysis of Statistical Ma-chine Translation Output.
Proceedings of the 5thInternational Conference on Language Resourcesand Evaluation (LREC), pages 697-702, Genova,Italy, May 2006.Billy T-M Wong.
2010.
Semantic Evaluation of Ma-chine Translation.
Proceedings of the 7th Interna-tional Conference on Language Resources andEvaluation (LREC), Valletta, Malta, May, 2010.Billy T-M Wong and Chunyu Kit.
2008.
Word choiceand Word Position for Automatic MT Evaluation.AMTA 2008 Workshop: MetricsMATR, 3 pages,Waikiki, Hawai'i, October, 2008.Billy T-M Wong and Chunyu Kit.
2009a.
Meta-Evaluation of Machine Translation on Legal Texts.Proceedings of the 22nd International Conferenceon the Computer Processing of Oriental Languag-es (ICCPOL), pages 343-350, Hong Kong, March,2009.Billy Wong and Chunyu Kit.
2009b.
ATEC: Automat-ic Evaluation of Machine Translation via WordChoice and Word Order.
Machine Translation,23(2):141-155.Zhibiao Wu and Martha Palmer.
1994.
Verb Seman-tics and Lexical Selection.
Proceedings of the 32ndAnnual Meeting of the Association for Computa-tional Linguistics, pages 133-138, Las Cruces,New Mexico.Parameters Valueswtype 1        (exact match),0.95  (stem / synonym /semantically close),0.15  (unmatch)Infomatch 0.34Infounmatch 0.26wpos 0.02worder 0.15e 1.1Limitdis 0.95LimitInfo 0.5?
0.5Table 1  Optimal parameter values for ATEC364
