Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1728?1739, Dublin, Ireland, August 23-29 2014.Towards Semantic Validation of a Derivational LexiconBritta D. Zeller?Sebastian Pad?o?Jan?Snajder?
?Heidelberg University, Institut f?ur Computerlinguistik69120 Heidelberg, Germany?Stuttgart University, Institut f?ur maschinelle Sprachverarbeitung70569 Stuttgart, Germany?University of Zagreb, Faculty of Electrical Engineering and ComputingUnska 3, 10000 Zagreb, Croatiazeller@cl.uni-heidelberg.de pado@ims.uni-stuttgart.de jan.snajder@fer.hrAbstractDerivationally related lemmas like friendN?
friendlyA?
friendshipNare derived from a commonstem.
Frequently, their meanings are also systematically related.
However, there are also manyexamples of derivationally related lemma pairs whose meanings differ substantially, e.g., objectN?
objectiveN.
Most broad-coverage derivational lexicons do not reflect this distinction, mixing upsemantically related and unrelated word pairs.In this paper, we investigate strategies to recover the above distinction by recognizing semanticallyrelated lemma pairs, a process we call semantic validation.
We make two main contributions:First, we perform a detailed data analysis on the basis of a large German derivational lexicon.
Itreveals two promising sources of information (distributional semantics and structural informationabout derivational rules), but also systematic problems with these sources.
Second, we developa classification model for the task that reflects the noisy nature of the data.
It achieves animprovement of 13.6% in precision and 5.8% in F1-score over a strong majority class baseline.Our experiments confirm that both information sources contribute to semantic validation, and thatthey are complementary enough that the best results are obtained from a combined model.1 IntroductionMorphological processing forms the first step of virtually all linguistic processing toolchains in naturallanguage processing (NLP) and precedes other analyses such as part of speech tagging, parsing, ornamed entity recognition.
There are three major types of morphological processes: (a) Inflection modifiesword forms according to the grammatical context; (b) derivation constructs new words from individualexisting words, typically through affixation; (c) composition combines multiple words into new lexicalitems.
Computational treatment of morphology is often restricted to normalization, such as lemmatization(covering inflection only) or stemming (covering inflection and derivation heuristically, Porter (1980)).An important reason is that English is morphologically a relatively simple language.
Composition isnot marked morphologically (zoo gate) and an important derivational pattern is zero derivation where theinput and output terms are identical surface forms (a fish / to fish).
Thus, lemmatization or stemming go along way towards treating the aspects of English morphology relevant for NLP.
The situation is differentfor languages with a complex morphology that calls for explicit treatment.
In fact, recent years have seena growing body of computational work in particular on derivation, which is a very productive process ofword formation in Slavic languages but also in languages more closely related to English, like German(?Stekauer and Lieber, 2005).Derivation comprises a large number of distinct patterns, many of which cross part of speech boundaries(nominalization, verbalization, adjectivization), but some of which do not (gender indicators like master /mistress, approximations like red / reddish).
A simple way to conceptualize derivation is that it partitions alanguage?s vocabulary into derivational families of derivationally related lemmas (cf.
Zeller et al.
(2013),Gaussier (1999)).
In WordNet, this type of information has been included to some extent by so-called?morpho-semantic?
relations (Fellbaum et al., 2009), and the approach has been applied to languages otherThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1728lachen Lacher l?acherlichsfx ?er?V NAppend suffix ?er?
to the stem ofthe verb to obtain a nountry uml &sfx ?lich?NATry to turn the noun?s vowels into umlauts, thenappend suffix ?lich?
to obtain an adjectiveto laugh laugh laughableFigure 1: (Part of) a derivational family from DERIVBASE including derivational rulesthan English (Bilgin et al., 2004; Pala and Hlav?a?ckov?a, 2007).
Another source of derivational informationare stand-alone derivational lexicons such as CatVar (Habash and Dorr, 2003) for English, DERIVBASE(Zeller et al., 2013) for German, or the multilingual CELEX (Baayen et al., 1996).Recent work has demonstrated that NLP can benefit from derivational knowledge.
Shnarch et al.
(2011)employ derivational knowledge in recognizing English textual entailment to better gauge the semanticsimilarity of text and hypothesis.
Pad?o et al.
(2013) improve the prediction of German semantic similarityjudgments for lemma pairs by backing off to derivational families for infrequent lemmas.
Luong et al.
(2013) and Lazaridou et al.
(2013) improve distributional semantic representations.Note that all of these applications make use of derivational knowledge to address various semantic tasks,working on the assumption that derivationally related words, as represented in derivational lexicons, arestrongly semantically related.
This assumption is not completely warranted, though.
The development ofwide-coverage derivational lexicons is generally driven by morphological information, using for examplefinite-state technology (Karttunen and Beesley, 2005) to characterize known derivational patterns in termsof string transformations.
Even though there is a strong correlation in derivation between morphologyand semantics, it is not perfect.
The absence of (synchronic) semantic relatedness can have a number ofreasons, including accidental instantiation of derivational patterns (corn ?
corner) and diachronic meaningdrift (dog (animal) ?
dogged (determined)).
In other words, a substantial number of the lemma pairs inthose lexicons are false positives regarding the level of semantic relatedness.Our goal in this paper is to ameliorate this situation by developing strategies for the semantic validationof derivational lexicons, i.e., methods to determine, for lemma pairs that are derivationally relatedat the morphological level, whether they are in fact semantically related.
We base our study on theGerman derivational lexicon DERIVBASE, and start by assessing which strategies can be used for itssemantic validation (Section 2).
In Sections 3 and 4, we analyze the contributions of semantic information(distributional semantics) as well as structural information (derivational rules).
On the basis of ourobservations, we train a classifier that is able to semantically validate DERIVBASE at 89.9% F1-score(Section 5), significantly outperforming a majority-class baseline of 84.1%.
Section 6 reviews relatedwork.
Section 7 concludes the paper and outlines future work.2 A Lexicon for German Derivation2.1 DERIVBASEDERIVBASE (Zeller et al., 2013) is a freely available derivational lexicon for German.1We used arule-based framework to define derivation rules that cover suffixation, prefixation, and zero derivation aswell as stem changes.
Following the work of?Snajder and Dalbelo Ba?si?c (2010), derivational processes aredefined using derivational rules and higher-order string transformation functions.
The only requirementsfor this method are (a) a comprehensive set of lemmas and (b) knowledge about admissible derivationalrules, which can be gathered, for example, from linguistics textbooks.Figure 1 shows a small sample from a derivational family with three lemmas and two derivational rules,one turning a verb into the corresponding event noun (in this case a semelfactive), and one turning theevent into an adjective associated with it.
Note that there are two perspectives on such a family: It can1http://www.cl.uni-heidelberg.de/?zeller/res/derivbase/1729?Positive?
Precision RecallDERIVBASE release class % %1.2 (Zeller et al., 2013)3R and M 83.0 71.01.4 (our analysis) R and M 85.1 91.41.4 (our analysis) R only 76.7 93.8Table 1: DERIVBASE evaluation across releases on the DERIVBASE release 1.2 P and R sampleseither be seen as a set of lemmas, or as a set of (independent) lemma pairs.
We will assume the latterperspective in this paper, leaving questions of global coherence for future work.DERIVBASE is a good example for the problems sketched in Section 1.
It is defined purely onmorphological grounds, without semantic validation of derivational families.
Consequently, it contains asubstantial number of words that are not semantically related.2.2 Morphological and Semantic Relatedness in DERIVBASEOur original evaluation of the quality of DERIVBASE in Zeller et al.
(2013) was based on manuallyclassified samples of lemma pairs.
We introduce two samples, the ?R sample?, drawn from a largepopulation of lemma pairs with high string similarity, in order to calculate recall, and the ?P sample?,drawn from the DERIVBASE families, in order to compute precision.
Each lemma pair was classified intoone of five categories (R: morphologically and semantically related; M: only morphologically related;N: not related; L: lemmatization errors; C: compounds) and inter-annotator agreement was checked tobe substantial.2The overall best model (L123) showed 83% precision and 71% recall.
However, thisevaluation is limited in two important respects.
First, it refers to DERIVBASE release 1.2 from 2013.Since then, we have extended DERIVBASE, e.g., with rules covering particle verbs, a very productivearea of German derivation.
Secondly, and more seriously, the previous evaluation considered all instancesof R and M as true positives.
In other words, in Zeller et al.
(2013) we only evaluated the morphologicalrelatedness of the lemma pairs but not the semantic relatedness.We therefore start by presenting an evaluation of DERIVBASE focusing on the R instances in Table 1,reusing the DERIVBASE 1.2 ?P?
and ?R?
samples introduced in Zeller et al.
(2013, see there for evaluationdetails).
Between DERIVBASE 1.2 and 1.4, precision increased marginally and recall substantially, duemainly to the inclusion of rules that cover particle verbs.
However, the numbers change substantially whenonly R (truly semantically related pairs) are counted as true positives.
Recall increases by about 2.5%, butprecision drops about 8.5%.
Almost one quarter of all pairs in the lexicon are not semantically related.A possible confounder of this analysis is that the ?P sample?
was drawn on DERIVBASE 1.2 andtherefore does not include the novel items in DERIVBASE 1.4.
We therefore created a novel DERIVBASE1.4 extended sample by combining the existing ?P sample?
with those pairs from the ?R sample?
that arein the coverage of a DERIVBASE rule as of DERIVBASE 1.4, resulting in 2,545 lemma pairs.This DERIVBASE 1.4 extended sample will form the basis of all our analyses in this paper.
The classdistribution in the new sample is similar, but not identical, to the old P sample, as shown in Table 2.
Therelative frequency of R drops another 2%.
Since this number also corresponds to the precision of theresource, the precision of the extended sample is 74.6%.There are almost no compound errors C, which is not surprising given the rule-based construction ofthe lexicon, and only a relatively small number (about 5%) of lemmatization errors L, which fall outsidethe scope of our work.
In contrast, both N and M occur with substantial frequency: Each class accountsfor around 10% of the pairs.
An analysis of N shows many cases of rule overgeneration: These are oftenpairs of lemmas whose stems are sufficiently similar that they might be related, e.g., by stem-changingderivation rules.
Although such rules are valid in other contexts (VerkaufN?
Verk?auferN(selling ?
seller)),2Although we believe semantic relatedness to be fundamentally a graded scale, we adopt a binary notion of it as a convenientoperational simplification that is supported by the good inter-annotator agreement for manual labeling in DERIVBASE.3DERIVBASE 1.2 corresponds to DERIVBASE ?L123?
in (Zeller et al., 2013, p. 1207).1730R M N L CFrequency 1899 265 240 131 8Percentage overall 74.6 10.4 9.5 5.2 0.3Percentage on dev.
set 75.5 10.3 9.0 4.8 0.3Percentage of test set 72.6 10.6 10.6 5.9 0.3Table 2: Class distribution in our new DERIVBASE 1.4 extended sampleerroneous application leads to N cases like BlaseN?
Bl?aserN(bubble ?
blower).
Also, we find falsematches of common noun rules with named entities (EmpireN?
EmpirismusN(Empire ?
empiricism)).In contrast, many cases of M (as sketched in Section 1) refer to different senses of the same stem.
Asan example, consider beruhenV?
unruhigA(to rest on ?
restless), both related to RuheN(rest).
In othercases, one of the two lemmas appears to have undergone a meaning shift (RappelN?
rappelnV(craze ?to rattle)).
This is particularly prominent for particle verbs (bauenV?
erbaulichA(build ?
edifying)).We divide the DERIVBASE 1.4 extended sample into a development and a test partition (70:30 ratio);the subsequent analyses consider only the development set.2.3 Hypotheses for Semantic ValidationThe preceding analysis of DERIVBASE has established that the lexicon contains a substantial number(around one fourth) of lemma pairs that are not semantically related.
Therefore, it is in need of semanticvalidation, i.e., a computational procedure that can filter out semantically unrelated words.In this paper, we frame semantic validation as a binary classification task that classifies all lemma pairswithin one derivational family as either semantically related or unrelated.
We consider this a first steptowards splitting the current, morphologically motivated, DERIVBASE families into smaller, semanticallycoherent, families.
We base our work on two general hypotheses about the types of information that mightbe helpful in this endeavor.Hypothesis 1.
Distributional similarity indicates semantic relatedness between derivationally relatedwords.
The instances of polysemy and meaning shift that we observe, in particular in the M class,motivate the use of distributional similarity (Turney and Pantel, 2010) since we expect these lemmapairs to be distributionally less related than cases of true semantic relatedness.Hypothesis 2.
Derivational rules differ in their reliability.
Both the evidence from M and N indicatethat some rules are more meaning-preserving than others.
We expect this to be tied to both lexicalproperties of the rules (particle verbs are more likely than diminutives to radically change meaning)as well as structural properties (more specific rules are presumably more precise than generic rules).In the two following Sections, we will operationalize these hypotheses and analyze the development set ofthe DERIVBASE 1.4 extended sample with respect to their empirical adequacy.3 Analysis 1: Distributional Similarity for Semantic Validation3.1 Measuring Distributional SimilarityWe examine semantic similarities as predicted by simple bag-of-words semantic space models built fromthe lemmatized SDeWaC (Faa?
et al., 2010), a large German web corpus containing about 880 millionwords.
We compute vectors for all words covered in DERIVBASE using a window of ?5 words withinsentence boundaries and considering the 10k most frequent lemma-part of speech combinations of nouns,verbs, and adjectives in SDeWaC as contexts.
Distributional vectors are built from co-occurrences whichare measured with Local Mutual Information (Evert, 2005).
The semantic similarity is measured by thecosine similarity between the vectors.
Despite the size of the corpus, many lemmas from DERIVBASEoccur very infrequently, and due to the inflection in German, it is important to retrieve as many occurrencesof each lemma as possible.1731We therefore use a very permissive two-step lemmatization scheme that starts from lemmas fromthe lexicon-based TreeTagger (Schmid, 1994), which provides reliable lemmas but with relatively lowcoverage, and supplements them with lemmas and parts of speech produced by the probabilistic MATEtoolkit (Bohnet, 2010) when TreeTagger abstains.3.2 Frequency ConsiderationsThe advantage of the string transformation-based construction of DERIVBASE is its ability to includeinfrequent lemmas in the lexicon, and in fact DERIVBASE includes more than 250,000 content lemmas,some of which occur not more than three times in SDeWaC.
However, this is a potential problem whenwe build distributional representations for all lemmas in DERIVBASE since it is known from the literaturethat similarity predictions for infrequent lemmas are often unreliable (Bullinaria and Levy, 2007).Our data conform to expectations in this regard ?
infrequent lemmas are indeed problematic forvalidating the semantic relatedness of lemma pairs.
More specifically, the semantic similarity of relatedlemmas (R) is systematically underestimated, because the lemma pairs from our sample are often tooinfrequent to share any dimensions.
Consequently, they receive a low or zero cosine even when they aresemantically strongly related.
For example, each of the lemmas DrogenverkaufN?
Drogenverk?auferN(drug selling ?
drug seller) has only nine lemmas as dimensions, and those are completely disjoint.
Thisunderestimation constitutes a general trend.
The model assigns cosine scores below 0.1 to 64% of therelated pairs in the development set, cosines below 0.2 to 81%, and cosines below 0.3 to 87%.
Such lowscores are problematic for separating related from unrelated pairs.Two-step lemmatization is important for the proper handling of infrequent words.
Compared tojust using TreeTagger, the TreeTagger+MATE vectors for auferstehenV?
auferstehendA(to resurrect ?resurrecting) share seven more dimensions, including Jesus, Lord, myth, and suffering.
Correspondingly,the cosine value of this pair rises by 50%.
Generally, the amount of zero cosines in the DERIVBASE1.4 extended sample drops by 45% using two-step lemmatization compared to one-step TreeTaggerlemmatization.3.3 Conceptual ConsiderationsIn addition to the frequency considerations discussed above, we find three conceptual phenomena thataffect distributional similarity independently of the frequency aspects.The first one is the influence of parts of speech.
Derivational rules often change the part of speech of theinput lemma, and the parts of speech of its context words change as well.
This decreases context overlap.For example,?Ubersch?atzungN?
?ubersch?atztA(overestimate ?
overestimated) is assigned a cosine ofmerely 0.09.
The upper half of Table 3 shows the top ten individual and shared context words for thispair, ranked by LMI.
The context words of the noun are mainly nominal heads of genitive complements(overestimation of possibility/force/.
.
.
), while the context words of the adjective comprise many adverbs(totally, widely, .
.
.
).
None of the shared contexts rank among of the top ten for both target lemmas.
Thisis even more surprising considering that German adjectives and adverbs have the same surface realization(as opposed to English) and are more likely to form matching context words.The second phenomenon that we identified as influencing semantic similarity is markedness (Battistella,1996).
A considerable number of derivational rules systematically produce marked terms.
A strikingexample is the feminine suffix ?-in?
as in EntertainerN?
EntertainerinN: Although the lemmas areintuitively very similar, their cosine is as low as 0.1.
The reason is that the female versions tend to beused in contexts where the gender of the entertainer is relevant.
This is illustrated in the lower half ofTable 3.
The first two contexts for both words (actor, singer) stem from frequent enumerations (actor andentertainer X) and are almost identical, but again the female versions are marked for gender.
We also findtwo female given names.
As a result, the target lemmas receive a low distributional similarity.The third example are cases of mild meaning shifts that were tagged by the annotators as R. Theseare lemmas where the semantic relatedness is intuitively clearly recognizable but may be accompaniedby pretty substantial changes in the distribution of contexts.
Consider the semantically related pairAbsteigerN?
absteigendA(descender (person) ?
descending/decreasing).
It achieves only a cosine of1732word pair (l1, l2) context(l1) context(l2) shared contexts(l1, l2)?Ubersch?atzung ?
?ubersch?atzt(overestimation ?overestimated),cos = 0.09eigen (own) v?ollig (totally) v?ollig (totally)warnen (to alert) Problem (problem) M?oglichkeit (possibility)M?oglichkeit (possibility) Gefahr (danger) Bedeutung (meaning)f?uhren (to lead) Autor (author) Gefahr (danger)Kraft (force) weit (widely) Einflu?
(influence)Bedeutung (meaning) total (totally) ?uberh?oht (excessive)F?ahigkeit (ability) ernst (seriously) Macht (power)Leistungsf?ahigkeit (performance) ?uberh?oht (excessive) gnadenlos (mercilessly)neigen (to tend) gnadenlos (mercilessly) Kraft (force)Einflu?
(influence) Hollywood (Hollywood) h?aufig (frequent)Entertainer ?Entertainerin(entertainer ?
femaleentertainer),cos = 0.1S?anger (singer) S?angerin (female singer) Schauspieler (actor)Schauspieler (actor) Schauspielerin (actress) Musiker (musician)Musiker (musician) Helga (female given name) Talent (talent)Harald (male given name) Mutter (mother) bekannt (well-known)Moderator (anchorman) ber?uhmt (famous) S?angerin (female singer)Schmidt (surname) brillant (brilliant) beliebt (popular)gro?
(big) Lisa (female given name) gro?
(big)K?unstler (artist) K?unstlerin (female artist) ber?uhmt (famous)Talent (talent) verstorben (deceased) Sportler (sportsman)gut (good) Talent (talent) Schauspielerin (actress)Table 3: Top ten individual and shared context words for?Ubersch?atzungN?
?ubersch?atztA(overestimation?
overestimated) and EntertainerN?
EntertainerinN.
Individual context words are ranked by LMI, sharedcontext words by the product of their LMIs for the two target words.
Shared context words that occur inthe top ten contexts for both words are marked in boldface.0.005, because Absteiger is almost exclusively used to refer to relegated sport teams while absteigend isused as a general verb of scalar change.3.4 Ranking of Distributional InformationGiven the results reported above, the standard distributional approach of using plain cosine scores tomeasure the absolute amount of co-occurrences does not seem very promising, due to the low absolutenumbers of shared dimensions of the two lemmas.
We expect other similarity measures, e.g., the Linmeasure (Lin, 1998), to perform equally poorly since they do not change the fundamental approach.
Also,although using a large corpus for semantic space construction might ameliorate the situation, we wouldprefer to make improvements on the modeling side of semantic validation.We follow the ideas of Hare et al.
(2009) and Lapesa and Evert (2013) who propose to consider semanticsimilarity in terms of ranks rather than absolute values.
The advantage of rank-based similarity is thatit takes the density of regions in the semantic space into account.
That is, a low cosine value does notnecessarily indicate low semantic relatedness ?
provided that the two words are located in a ?sparse?region.
Conversely, a high cosine value can be meaningless in a densely populated region.
A secondconceptual benefit of rank-based similarity is that it is directed: It is possible to distinguish the ?forward?rank (the rank of l1in the neighborhood of l2) and the ?backward?
rank (the rank of l2in the neighborhoodof l1).
The previous studies found rank-based similarity to be beneficial for the prediction of primingresults.
In our case, it suggests a refined version of our Hypothesis 1:Hypothesis 1?.
High rank-based distributional similarity indicates semantic relatedness between deriva-tionally related words.4 Analysis 2: Derivational Rules for Semantic ValidationAs discussed in Section 2.3, a second source of information that should be able to complement theproblematic distributional similarity is provided by the derivational rules that are encoded in DERIVBASE(cf.
the arrows in Figure 1).
Our intuition is that some rules are ?semantically stable?, meaning that theyreliably connect semantically similar lemmas, while other rules tend to cause semantic drifts.
To examine1733this situation, we perform a qualitative analysis on all lemma pairs connected by rule paths of length one(?simplex paths?
), which are easy to analyze.
Longer paths (?complex paths?)
are considered below.We find that rules indeed behave differently.
For example, the ?-in?
female marking rule from Section 3.3is very reliable: every lemma pair connected by this rule is semantically related.
At the other end of thescale, there are rules that consistently lead to semantically unrelated lemmas, e.g., the ?ver-?
noun-verbprefixation: ZweifelN?
verzweifelnV(doubt ?
to despair).
Foreign suffixes like ?-ktiv?
in instruierenV?
instruktivA(to instruct ?
instructive) retain semantic relatedness in most cases, but sometimes linkactually unrelated lemmas (N, C, L).
For example, ObjektivN?
ObjektivismusN(lens ?
objectivism),is an N pair for the suffix ?-ismus?.
Finally, zero derivations and very short suffixes are less reliable:Since they easily match, they are often applied to incorrectly lemmatized words (L).
For example, the?-n?
suffix, which relates nationalities with countries (SchwedeN?
SchwedenN(Swede ?
Sweden)).
Itmatches many wrongly lemmatized nouns due to its syncretism with the plural dative/accusative suffix -n,as in SchweineschnitzelN?
SchweineschnitzelnN(pork cutlet ?
pork cutletsdat/acc-pl).
This suggests thatrule-specific reliability is a promising feature for semantic validation.
Fortunately, due to its construction,DERIVBASE provides a rule chain for each lemma pair so that these reliabilities can be ?read off?.
Forother rules, however, the variance of the individual lemma pairs that instantiate the rule is large, and theapplicability of the rule is influenced by the particular combination of rule and lemma pair.
Such casessuggest that distributional knowledge and structural rule information should be combined, a direction thatwe will pursue in the next section.On word pairs that are linked by ?complex paths?, i.e., more than one rule (lachenV?
l?acherlichAinFigure 1), our main observation in this respect is that rule paths show a clear ?weakest link?
property.
Oneunreliable rule can be sufficient to cause a semantic drift, and only a sequence of reliable rules is likely tolink two semantically related words.
We will act on this observation in the next section.5 A Machine Learning Model for Semantic Validation5.1 ClassificationThe findings of our analyses suggest that the decision to classify lemma pairs as semantically relatedor unrelated can draw on a range of considerations.
We therefore decided to adopt a machine learningapproach and phrase semantic validation as a binary classification task, using the analyses we performedin Sections 3 and 4 as motivation for feature definition.We train a classifier on the development portion of the DERIVBASE 1.4 extended sample (1,780training instances, cf.
Section 2.2).
We learn a binary decision: Semantic relatedness (R) vs. non-semanticrelatedness (M, N, C, L) within derivationally related pairs.
For classification, we use a nonlinear model:Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel.
Using the RBF kernel allowsus to capture the non-linear dependencies between the features.4We rely on LIBSVM (Chang and Lin,2011), a well-known SVM implementation.
We optimize the C and ?
hyperparameters of the SVM modelusing 3-fold cross-validation on the training data (i.e., the development portion of the extended sample).5.2 FeaturesOur analyses motivate three feature groups comprising 35 individual features: Distributional, derivationrule-based (?structural?
), and hybrid features.
Table 4 gives a list.Distributional features.
All distributional features apply to the lemma or pair level.
They are calculatedfrom our BOW model with permissive lemmatization (Section 3.1).
We use absolute and rank-based cosinesimilarity (Section 3.4) as well as the number of shared contexts (computed with LMI, cf.
Section 3.3)and lemma frequency.
To speed up processing, we compute the forward rank similarity for a lemma pair(l1, l2) not on the complete vocabulary but by pairing l1with a random sample of 1,000 lemmas fromDERIVBASE (plus l2if it is not included).
We do the computation analogously for the backward rank.4The nonlinear SVM model outperforms a linear SVM.
The difference is 0.8% F-Score, statistically significant at p=0.05.1734Feature group Type Feature name Description(# features) (# features)Distributio- l Lemma frequency (2) Normalized SDeWaC corpus lemma frequenciesnal (6) p Cosine similarity Standard cosine lemma similarityp Dimensions shared Number of shared context wordsp Cos. rank similarity (2) Rank-based forward and backward similarityStructural (25) r Rule identity (11) Indicator features for the top ten rules in the devset + one aggregate feature for the restr Rule reliability Percentage of rule applications on R pairs amongall applications of the rule in dev setr Rule frequency rank (2) Rank-based rule frequency in DERIVBASEr Avg.
string distance (2) Avg.
Levenshtein distance for all rule instancesp POS combinations (6) Indicator features for lemma POS combinationsp Path length Length of the shortest path between the lemmasp String distance (2) Dice bigram coefficient; Levenshtein distanceHybrid (4) r Average rank sim (2) Frequency-weighted average rank similarity ofrules on shortest pathp Rank sim deviation (2) Difference between lemma pair rank similarityand average rule rank similarityTable 4: Features used to characterize derivationally related lemma pairs.
?Type?
indicates the level atwhich each feature applies: l lemma level, p pair level, r rule level.Structural features.
The structural features encode properties of the rules and paths in DERIVBASE.Most features apply to the level of derivation rules.
This includes the identity of the rule; its reliability(estimated as the ratio of its application on R pairs among all its applications on the dev set); its frequencyrank among all rules (as a measure of specificity)5; and the average Levenshtein distance between theinput and output lemmas (estimating rule complexity by measuring the amount of string modification).For lemma pairs linked by complex paths (i.e., more than one rule, cf.
Figure 1), the question ariseshow the rule-level features should be computed.
Following our observations on ?weakest link?
behaviorin Section 4, we always combine the feature values for the individual rules adopting the most pessimisticcombination function (e.g., minimum in the case of reliability, maximum in the case of frequency rank).Three more structural features are computed directly at the lemma pair level: their part of speechcombination (e.g., ?NV?
for oxideN?
oxidateV), the length of the shortest path connecting them, and theLevenshtein and Dice string distances between the two lemmas.Hybrid features.
Hybrid features combine rule-based and distributional information to avoid theirrespective shortcomings.
We work with two hybrid features, one at rule level and one at pair level.
Therule-level feature models the reliability of the rule.
It is the average rank similarity for each rule (computedas a log frequency-weighted average over rule instances).
This feature is a counterpart to rule reliabilitythat is unsupervised in that it does not require class labels.
We compute it by randomly drawing 200lemma pairs for each rule from DERIVBASE (less if the rule has fewer instances).
The pair-level featureis the difference between the rule?s average rank similarity and the rank similarity for the current pair.
Itmeasures the rank of a pair relative to the rule?s ?baseline?
rank and indicates how similar and dissimilarlemma pairs are compared to the rule average.
In parallel to the structural features, values for complexrule paths are computed by minimum.
Since the rank similarity is directional, we compute both hybridfeatures in two variants, one for each direction.65We compute this feature once only on simplex paths and once on all instances of the rule in DERIVBASE, trading reliabilityagainst noise.6We also tested hybrid features based on raw cosine; however, this yielded worse results than the rank-based hybrid features.1735Validation method Precision Recall F1AccuracyMajority baseline 72.6 100 84.1 72.6Classifier, only ?cosine similarity?
feature 72.6 100 84.1 72.6Classifier only ?similarity rank?
feature 80.3 90.3 85.0 76.8Classifier, only ?rule identity?
feature 73.7 99.5 84.6 73.8Classifier, hybrid group 80.4 95.3 87.2 79.7Classifier, distributional group 80.5 96.6 87.8 80.5Classifier, structural group 82.7 93.1 87.6 80.9Classifier, hybrid + distributional groups 82.6 93.3 87.6 80.9Classifier, hybrid + structural groups 84.9 93.7 89.1 83.4Classifier, distributional + structural groups 85.3 94.6 89.7 84.3Classifier, all features 86.2 93.9 89.9 84.7Table 5: Accuracy, precision, recall, and F1on the test portion of the DERIVBASE 1.4 extended sample.5.3 Results and DiscussionWe applied the trained classifier to the test portion of the DERIVBASE 1.4 extended sample (cf.
Section 2.2).Table 5 summarizes precision, recall, and F1-score of the classifier for various combinations of featuresand feature groups.
Recall that since our motivation is semantic validation, i.e., the removal of falsepositives, we are in particular interested in improving the precision of our predictions.
We test significanceof F1differences among models with bootstrap resampling (Efron and Tibshirani, 1993).Our baseline is the majority class in the sample, R. Due to the sample?s skewed class distribution (cf.Table 2), the frequency baseline is quite high (precision 72.6, F1-score 84.1).
We next consider the threemost prominent individual features: Distributional similarity measured as cosine, distributional similaritymeasured as similarity rank, and rule identity.
As expected from our analyses, the cosine similarity onits own is not reliable; in fact, it performs at baseline level.
The rank-based similarity already leads to aconsiderable gain (precision +7.7%), but only a slight F1-score increase of 0.9% that is not statisticallysignificant at p=0.05.
These results provide good empirical evidence for Hypothesis 1?
(Section 3.4)and underscore that 1?
is a more accurate statement than Hypothesis 1 (Section 2.3).
On the structuralside, rule identity alone improves the precision by 1.1%, with an F1-score increase in 0.5% (again notsignificant).We now proceed to complete feature groups, all of which perform at least 3% F1-score better than thebaseline, proving that the features within these groups are complementary.
The hybrid feature group is theworst among the three.
The distributional feature group is able to improve only slightly over the individualrank-based similarity feature in precision (80.5 vs. 80.3), but gains 6.3% in recall.
This is sufficient fora significant improvement in F1(+3.7%, significant at p=0.01).
The structural feature group performssurprisingly well, given that these features are very simple and most are computed only on the relativelysmall training set.
It yields by far the highest precision (82.7), and its F1-score is only slightly lower thanthe one of the distributional group (87.6 vs. 87.8).
We take this as further evidence for the usefulness ofstructural information, as expressed by Hypothesis 2 (cf.
Section 2.3).Ultimately, all three feature groups turn out to be complementary.
We obtain an improvement inF1-score for two out of the three feature group combinations, and a clear improvement in precision in allcases.
Finally, the best overall result is shown by the combination of all three feature groups.
It attains anF1-score of 89.9, an improvement of 5.8% over the baseline and 2.1% over the best feature group (bothdifferences significant at p=0.01).
Crucially, this model gains over 13% in precision while losing only 6%of recall compared to the baseline.
This corresponds to a reduction of false positives in the sample byabout half (from 27% to 14%) while the true positives were reduced only by 5% (from 73% to 68%).Table 6 shows a breakdown of the predictions by the best model in terms of the five gold standard classes1736R M N L C totalGold annotation 554 81 81 45 2 763Classified as R 520 36 16 29 2 603Classified as not R 34 45 65 16 0 160Table 6: Predictions on the test set of the all features Classifier per annotation class.
(R, M, N, L, C).
Ignoring compounds (C), of which there are too few cases to analyze, we first find thatthe classifier achieves a high R recall.
It is also very good in filtering out unrelated cases (N), of which itdiscards around 80%.
The model recognizes morphologically but not semantically related word pairs (M)fairly well and manages to remove more than half of these.
It has the hardest time with lemmatizationerrors (L), of which only about 35% were removed.
However, this is not surprising: Lemmatization errorsdo not form a coherent category that would be easy to retrieve with the kinds of features that we havedeveloped.
We believe that such errors should be handled in an earlier stage, i.e., during preprocessing.6 Related WorkGiven that many derivational lexicons were only developed in recent years, we are only aware of one study(Jacquemin, 2010) that semantically validates the output of an existing derivational lexicon (Gaussier,1999) to apply it to Question Answering.
In contrast to our study, it requires elaborate dictionaryinformation to look up which derivations are permitted for a specific lemma, as well as word sensedisambiguation to determine the meaning of ambiguous words in context.
Other related work comes fromtwo areas: unsupervised morphology induction and semantic clustering.Unsupervised morphology induction is concerned with the automatic identification of morphologicalrelations (cf.
Hammarstr?om and Borin (2011) for an overview).
Most approaches in this area do not differ-entiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception)and restrict themselves to the string level.
Only a small number of studies (Schone and Jurafsky, 2000;Baroni et al., 2002) take distributional information into account.Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributionalinformation (Turney and Pantel, 2010; im Walde, 2006).
Boleda et al.
(2012) include derivationalproperties in their feature set to learn Catalan adjective classes.
However, the input to such studies isalmost always a set of words from the same part of speech with no prior morphological constraints, whileour input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent,and exhibit systematical variation in parts of speech.
To our knowledge, this challenging situation has notbeen addressed in previous studies.Recent work has also considered the opposite problem, namely using derivational morphology forimproving distributional similarity predictions.
Luong et al.
(2013) use recursive neural networks to learnrepresentations of morphologically complex words and demonstrate the usefulness of their approachon word similarity tasks across different datasets.
Similarly, Lazaridou et al.
(2013) improve the wordrepresentations of derivationally related words by composing vector space representations of stems andderivational suffixes.7 ConclusionsAlmost all existing derivational lexicons do not distinguish between only morphologically related wordson one hand and words that are both morphologically and semantically related words on the other hand.In this paper, we have addressed the task of recovering this distinction and called it semantic validation.We have used DERIVBASE, a German derivation lexicon, as the basis of our investigation.We have made two contributions: (a) providing a detailed analysis of the types of information availablefor this task (distributional similarity as well as structural information about derivation rules) and the prob-lems associated with each information type; and (b) training a machine learning classifier on linguistically1737motivated features.
The classifier, although not perfect, can substantially improve the precision of theword pairs in DERIVBASE and thus help to filter the derivational families in the lexicon.
We are makingthis semantic validation information available in the DERIVBASE lexicon by attaching a probability forthe class R to each lemma pair (see footnote 1 for the DERIVBASE URL).The approach that we have described should transfer straightforwardly to other derivational lexiconsand other languages on the conceptual level.
The practical requirements are an appropriate corpus (for thedistributional features) and derivational rule information (for the structural features).There are two clear directions for future work.
First, we plan to broaden our attention from word pairsto clusters and use the relatedness probabilities to cluster the derivational families in DERIVBASE intosemantically coherent subfamilies.
Second, we will demonstrate the impact of semantic validation onapplications of derivational knowledge such as derivation-driven smoothing of distributional models (Pad?oet al., 2013).Acknowledgments.
We gratefully acknowledge partial funding by the European Commission (projectEXCITEMENT (FP7 ICT-287923), first and second authors) as well as the Croatian Science Foundation(project 02.03/162: ?Derivational Semantic Models for Information Retrieval?, third author).
We thankthe reviewers for their valuable feedback.ReferencesHarald R. Baayen, Richard Piepenbrock, and Leon Gulikers.
1996.
The CELEX Lexical Database.
Release 2.LDC96L14.
Linguistic Data Consortium, University of Pennsylvania, Philadelphia, Pennsylvania.Marco Baroni, Johannes Matiasek, and Harald Trost.
2002.
Unsupervised Discovery of Morphologically RelatedWords Based on Orthographic and Semantic Similarity.
Computing Research Repository, cs.CL/0205006.Edwin L. Battistella.
1996.
The Logic of Markedness.
Oxford University Press.Orhan Bilgin, Ozlem C?etino?glu, and Kemal Oflazer.
2004.
Morphosemantic relations in and across Wordnets.
InProceedings of the Global Wordnet Conference, pages 60?66, Brno, Czech Republic.Bernd Bohnet.
2010.
Top accuracy and fast dependency parsing is not a contradiction.
In Proceedings of theInternational Conference on Computational Linguistics, pages 89?97, Beijing, China.Gemma Boleda, Sabine Schulte im Walde, and Toni Badia.
2012.
Modeling Regular Polysemy: A Study on theSemantic Classification of Catalan Adjectives.
Computational Linguistics, 38(3):575?616.John A. Bullinaria and Joe P. Levy.
2007.
Extracting Semantic Representations from Word Co-occurrence Statis-tics: A Computational Study.
Behavior Research Methods, 39(3):510?526.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM: A library for support vector machines.
ACM Transactionson Intelligent Systems Technology, 2(3):27:1?27:27.Bradley Efron and Robert J. Tibshirani.
1993.
An Introduction to the Bootstrap.
Chapman and Hall, New York.Stefan Evert.
2005.
The Statistics of Word Cooccurrences Word Pairs and Collocations.
Ph.D. thesis, Universityof Stuttgart.Gertrud Faa?, Ulrich Heid, and Helmut Schmid.
2010.
Design and Application of a Gold Standard for Morpholog-ical Analysis: SMOR in Validation.
In Proceedings of the Conference on Language Resources and Evaluation,pages 803?810, Valletta, Malta.Christiane Fellbaum, Anne Osherson, and Peter Clark.
2009.
Putting semantics into WordNet?s ?morphosemantic?links.
In Proceedings of Human Language Technology.
Challenges of the Information Society, pages 350?358,Pozna?n, Poland.
?Eric Gaussier.
1999.
Unsupervised learning of derivational morphology from inflectional lexicons.
In ACLWorkshop Proceedings on Unsupervised Learning in Natural Language Processing, pages 24?30, College Park,Maryland.Nizar Habash and Bonnie Dorr.
2003.
A categorial variation database for English.
In Proceedings of the NorthAmerican Association for Computational Linguistics, pages 96?102, Edmonton, Canada.1738Harald Hammarstr?om and Lars Borin.
2011.
Unsupervised Learning of Morphology.
Computational Linguistics,37(2):309?350.Mary Hare, Michael Jones, Caroline Thomson, Sarah Kelly, and Ken McRae.
2009.
Activating Event Knowledge.Cognition, 111(2):151?167.Sabine Schulte im Walde.
2006.
Experiments on the Automatic Induction of German Semantic Verb Classes.Computational Linguistics, 32(2):159?194.Bernard Jacquemin.
2010.
A derivational rephrasing experiment for question answering.
In Proceedings of theConference on Language Resources and Evaluation, pages 2380?2387, Valletta, Malta.Lauri Karttunen and Kenneth R. Beesley.
2005.
Twenty-five Years of Finite-state Morphology.
In Inquiries intoWords, Constraints and Contexts.
Festschrift for Kimmo Koskenniemi on his 60th Birthday, pages 71?83.
CSLIPublications, Stanford, California.Gabriella Lapesa and Stefan Evert.
2013.
Evaluating neighbor rank and distance measures as predictors of se-mantic priming.
In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages66?74, Sofia, Bulgaria.Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, and Marco Baroni.
2013.
Compositional-ly derived rep-resentations of morphologically complex words in distributional semantics.
In Proceedings of the Associationfor Computational Linguistics, pages 1517?1526, Sofia, Bulgaria.Dekang Lin.
1998.
An information-theoretic definition of similarity.
In Proceedings of the International Confer-ence on Machine Learning, ICML, pages 296?304, San Francisco, California.Minh-Thang Luong, Richard Socher, and Christopher D. Manning.
2013.
Better word representations with recur-sive neural networks for morphology.
In Proceedings of the Conference on Natural Language Learning, pages104?113, Sofia, Bulgaria.Sebastian Pad?o, Jan?Snajder, and Britta Zeller.
2013.
Derivational smoothing for syntactic distributional semantics.In Proceedings of the Association for Computational Linguistics, pages 731?735, Sofia, Bulgaria.Karel Pala and Dana Hlav?a?ckov?a.
2007.
Derivational relations in Czech wordnet.
In Proceedings of the Workshopon Balto-Slavonic Natural Language Processing: Information Extraction and Enabling Technologies, pages75?81, Prague, Czech Republic.Martin Porter.
1980.
An Algorithm for Suffix Stripping.
Program, 14(3):130?137.Helmut Schmid.
1994.
Probabilistic part-of-speech tagging using decision trees.
In Proceedings of the Interna-tional Conference on Natural Language Processing, Manchester, UK.Patrick Schone and Daniel Jurafsky.
2000.
Knowledge-free induction of morphology using latent semantic analy-sis.
In Proceedings of the Conference on Natural Language Learning, pages 67?72.
Lisbon, Portugal.Eyal Shnarch, Jacob Goldberger, and Ido Dagan.
2011.
A probabilistic modeling framework for lexical entailment.In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human LanguageTechnologies, pages 558?563, Portland, Oregon.Jan?Snajder and Bojana Dalbelo Ba?si?c.
2010.
A computational model of Croatian derivational morphology.
InProceedings of the International Conference on Formal Approaches to South Slavic and Balkan Languages,pages 109?118, Dubrovnik, Croatia.Pavol?Stekauer and Rochelle Lieber, editors.
2005.
Handbook of Word-Formation, volume 64 of Studies in NaturalLanguage and Linguistic Theory.
Springer.Peter D. Turney and Patrick Pantel.
2010.
From Frequency to Meaning: Vector Space Models of Semantics.Journal of Artificial Intelligence Research, 37(1):141?188.Britta Zeller, Jan?Snajder, and Sebastian Pad?o.
2013.
DErivBase: Inducing and evaluating a derivational mor-phology resource for German.
In Proceedings of the Annual Meeting of the Association for ComputationalLinguistics, pages 1201?1211, Sofia, Bulgaria.1739
