Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 17?25Manchester, UK.
August 2008Simple is Best: Experiments with Different Document SegmentationStrategies for Passage RetrievalJo?rg TiedemannInformation ScienceUniversity of Groningenj.tiedemann@rug.nlJori MurInformation ScienceUniversity of Groningenj.mur@rug.nlAbstractPassage retrieval is used in QA to fil-ter large document collections in orderto find text units relevant for answeringgiven questions.
In our QA system we ap-ply standard IR techniques and index-timepassaging in the retrieval component.
Inthis paper we investigate several ways ofdividing documents into passages.
In par-ticular we look at semantically motivatedapproaches (using coreference chains anddiscourse clues) compared with simplewindow-based techniques.
We evaluateretrieval performance and the overall QAperformance in order to study the impactof the different segmentation approaches.From our experiments we can concludethat the simple techniques using fixed-sized windows clearly outperform the se-mantically motivated approaches, whichindicates that uniformity in size seems tobe more important than semantic coher-ence in our setup.1 IntroductionPassage retrieval in question answering is differ-ent from information retrieval in general.
Extract-ing relevant passages from large document col-lections is only one step in answering a naturallanguage question.
There are two main differ-ences: i) Passage retrieval queries are generatedfrom complete sentences (questions) compared tobag-of-keyword queries usually used in IR.
ii) Re-trieved passages have to be processed further in or-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.der to extract concrete answers to the given ques-tion.
Hence, the size of the passages retrieved isimportant and smaller units are preferred.
Here,the division of documents into passages is crucial.The textual units have to be big enough to en-sure IR works properly and they have to be smallenough to enable efficient and accurate QA.
In thisstudy we investigate whether semantically moti-vated passages in the retrieval component lead tobetter QA performance compared to the use ofdocument retrieval and window-based segmenta-tion approaches.1.1 Index-time versus Search-time PassagingIn this paper, we experiment with various possi-bilities of dividing documents into passages beforeindexing them.
This is also called index-time pas-saging and refers to a one-step process of retriev-ing appropriate textual units for subsequent an-swer extraction modules (Roberts and Gaizauskas,2004; Greenwood, 2004).
This is in contrast toother strategies using a two-step procedure consist-ing of document retrieval and search-time passag-ing thereafter.
Here, we can distinguish betweenapproaches that only return one passage per rel-evant document (see, for example, (Robertson etal., 1992)) and the ones that allow multiple pas-sages per document (see, for example (Moldovanet al, 2000)).
In general, allowing multiple pas-sages per document is preferable for QA as possi-ble answers can be contained at various positionsin a document (Roberts and Gaizauskas, 2004).For this, an index-time approach has the advan-tage that the retrieval of multiple passages per doc-uments is straightforward because all of them com-pete which each other in the same index using thesame metric for ranking.A comparison between index-time and search-17time passaging has been carried out in (Robertsand Gaizauskas, 2004).
In their experiments,index-time passaging performs similarly to search-time passaging in terms of coverage and redun-dancy (measures which have been introduced inthe same paper; see section 4.2 for more informa-tion).
Significant differences between the variousapproaches can only be observed in redundancy onhigher ranks (above 50).
However, as we will seelater in our experiments (section 4.2), redundancyis not as important as coverage for our QA system.
Furthermore, retrieving more than about 40 pas-sages does not produce significant improvementsof the QA system anymore but slows down the pro-cessing time substantially.Another argument for our focus on a one-stepretrieval procedure can be taken from (Tellex et al,2003).
In this paper, the authors do not actually useany index-time passaging approach but comparevarious search-time passage retrieval algorithms.However, they obtain a huge performance differ-ence when applying an oracle document retriever(only returning relevant documents in the first re-trieval step) instead of a standard IR engine.
Com-pared to this, the differences between the variouspassage retrieval approaches tested is very small.From this we can conclude that much improve-ment can be gained by improving the initial re-trieval step, which seems to be the bottleneck in theentire process.
Unfortunately, the authors do notcompare their results with index-time approaches.However, looking at the potential gain in documentretrieval and keeping in mind that the performanceof index-time and search-time approaches is rathersimilar (as we have discussed earlier) we believethat the index-time approach is preferable.1.2 Passages in IRCertainly, IR performance is effected by chang-ing the size of the units to be indexed.
The taskin document segmentation for our index-time pas-saging approach is to find the proper division ofdocuments into text passages which optimize theretrieval in terms of overall QA performance.The general advantages of passage retrieval overfull-text document retrieval has been investigatedin various studies, e.g., (Kaszkiel and Zobel, 2001;Callan, 1994; Hearst and Plaunt, 1993; Kaszkieland Zobel, 1997).
Besides the argument of de-creasing the search space for subsequent answerextraction modules in QA, passage retrieval alsoimproves standard IR techniques by ?normaliz-ing?
textual units in terms of size which is espe-cially important in cases where documents comefrom very diverse sources.
IR is based on similar-ity measures between documents and queries andstandard approaches have shortcomings when ap-plying them to documents of various sizes and texttypes.
Often there is a bias for certain types raisingproblems of discrimination between documents ofdifferent lengths and content densities.
Passageson the other hand provide convenient units to bereturned to the user avoiding such ranking difficul-ties (Kaszkiel and Zobel, 2001).
For IR, passage-level evidence may be incorporated into documentretrieval (Callan, 1994; Hearst and Plaunt, 1993)or passages may be used directly as retrieval unit(Kaszkiel and Zobel, 2001; Kaszkiel and Zobel,1997).
For QA only the latter is interesting andwill be applied in our experiments.Passages can be defined in various ways.
Themost obvious way is to use existing markup (ex-plicit discourse information) to divide documentsinto smaller units.
Unfortunately, such markup isnot always available or ambiguous with other typesof separators.
For example, headers, list elementsor table cells might be separated in the same way(for example using an empty line) as discourserelated paragraphs.
Also, the division into para-graphs may differ a lot depending on the sourceof the document.
For example, Wikipedia entriesare divided on various levels into rather small unitswhereas newspaper articles often include very longparagraphs.There are several ways of automatically divid-ing documents into passages without relying onexisting markup.
One way is to search for linguis-tic clues that indicate a separation of consecutivetext blocks.
These clues may include lexical pat-terns and relations.
We refer to such approachesas semantically motivated document segmentation.Another approach is to cut documents into arbi-trary pieces ignoring any other type of informa-tion.
For example, we can use fixed-sized win-dows to divide documents into passages of simi-lar size.
Such windows can be defined in terms ofwords and characters (Kaszkiel and Zobel, 2001;Monz, 2003) or sentences and paragraphs (Zobelet al, 1995; Llopis et al, 2002).
It is also possi-ble to allow varying window sizes and overlappingsections to be indexed (Kaszkiel and Zobel, 2001;Monz, 2003).
In this case it is up to the IR engine18to decide which of the competing window types ispreferred and it may even return overlapping sec-tions multiple times.In the following sections we will discusstwo techniques of semantically motivated docu-ment segmentation and compare them to simplewindow-based techniques in terms of passage re-trieval and QA performance.2 Passage Retrieval in our QA systemOur QA system is an open-domain question an-swering system for Dutch.
It includes twostrategies: (1) A table-lookup strategy using factdatabases that have been created off-line, and, (2)an ?on-line?
answer extraction strategy with pas-sage retrieval and subsequent answer identificationand ranking modules.
We will only look at thesecond strategy as we are interested in the passageretrieval component and its impact on QA perfor-mance.The passage retrieval component is imple-mented as an interface to several open-source IRengines.
The query is generated from the givennatural language question after question analysis.Keywords are sent to the IR engine(s) and results(in form of sentence IDs) are returned to the QAsystem.In the experiments described here, we applyZettair (Lester et al, 2006), an open-source IR en-gine developed by the search engine group at theRMIT University in Melbourne, Australia.
It im-plements a very efficient standard IR engine withhigh retrieval performance according to our exper-iments with various alternative systems.
Zettair isoptimized for speed and is very efficient in bothindexing and retrieval.
The outstanding speed inindexing is very fortunate for our experiments inwhich we had to create various indexes with dif-ferent document segmentation strategies.3 Document SegmentationWe now discuss the different methods for docu-ment segmentation, starting with the semanticallymotivated ones and then looking at the window-based techniques.3.1 Using Coreference ChainsCoreference is the relation which holds betweentwo NPs both of which are interpreted as refer-ring to the same unique referent in the contextin which they occur ((Van Deemter and Kibble,2000)).
Since the coreference relation is an equiv-alence relation and consequently a transitive rela-tion chains of coreferring entities can be detectedin arbitrary documents.
We can use these coref-erence chains to demarcate passages in the text.The assumption in this approach is that corefer-ence chains mark semantically coherent passages,which are good candidates for splitting up docu-ments.Figure 1 illustrates chains detected by a resolu-tion system in five successive sentences.1.
[Jim McClements en Susan Sandvig-Shobe]ihebbeneen onrechtmatig argument gebruikt.2.
[De Nederlandse scheidsrechter]j[Jacques de Koning]jbevestigt dit.3.
[Kuipers]kversloeg zondag in een rechtstreeks duel[Shani Davis]m.4.
Toch werd [hij]kin de rangschikking achter [deAmerikaan]mgeklasseerd.5.
[De twee hoofdarbiters]iverklaarden dat [Kuipers?
]kvoorste schaats niet op de grond stond.Cluster i (1,5): [Jim McClements en Susan Sandvig-Shobe][De twee hoofdarbiters]Cluster j (2): [De Nederlandse scheidsrechter][Jacques de Koning]Cluster k (3-5): [Kuipers] [hij] [Kuipers?
]Cluster m (3,4): [Shani Davis] [de Amerikaan]Figure 1: Example of coreference chains used fordocument segmentationThe coreferential units can then be used to formpassages consisting of all sentences the corefer-ence chain spans over, i.e.
the boundaries of pas-sages are sentences containing the first occurrenceof the referent and the last occurrence of a refer-ent.
Thus, in the example in figure 1 we obtainfour passages: 1) sentence one to sentence five, 2)sentence two, 3) sentence three to five, and, 4) sen-tence three and four.
Note that such passages canbe included in others and may overlap with yet oth-ers.
Furthermore, there might be sentences whichare not included in any chain which have to be han-dled by some other techniques.For our purposes we used our own coreferenceresolution system which is based on informationderived from Alpino, a wide-coverage dependencyparser for Dutch (van Noord, 2006).
We ap-proached the task of coreference resolution as a19clustering-based ranking task.
Some NP pairs aremore likely to be coreferent than others.
The sys-tem ranks possible antecedents for each anaphorconsidering syntactic features, semantic featuresand surface structure features from the anaphorand the candidate itself, as well as features fromthe cluster to which the candidate belongs.
It picksthe most likely candidate as the coreferring an-tecedent.References relations are detected between pro-nouns, common nouns and named entities.
Theresolution system yields a precision of 67.9% anda recall of 45.6% (F-score = 54.5%) using MUCscores (Vilain et al, 1993) on the annotated testcorpus developed by (Hoste, 2005) which consistof articles taken from KNACK, a Flemish weeklynews magazine.3.2 TextTilingTextTiling is a well-known algorithm for segment-ing texts into subtopic passages (Hearst, 1997).It is based on the assumption that a significantportion of a set of lexical items in use duringthe course of a given subtopic discussion changeswhen that subtopic in the text changes.Topic shifts are found by searching for lexi-cal co-occurrence patterns and comparing adja-cent blocks.
First the text is subdivided intopseudo-sentences of a predefined size rather thanusing syntactically-determined sentences.
Thesepseudo-sentences are called token-sequences byHearst.The algorithm identifies discourse boundariesby calculating a score for each token-sequencegap.
This score is based on two methods,block comparison and vocabulary introduction.The block comparison method compares adjacentblocks of text to see how similar they are accord-ing to how many words the adjacent blocks havein common.
The vocabulary introduction methodis based on how many new words were seen inthe interval in which the token-sequence gap is themidpoint.The boundaries are assumed to occur at thelargest valleys in the graph that results from plot-ting the token-sequences against their scores.
Inthis way the algorithm produces a flat subtopicstructure from a given document.3.3 Window-basedThe simplest way of dividing documents into pas-sages is to use a fixed-sized window.
Here wedo not take any discourse information nor seman-tic clue into account but split documents at arbi-trary positions.
Windows can be defined in variousways, in terms of characters, words or sentences.In our case it is important to keep sentences to-gether because of the answer extraction compo-nent in our QA system that works on that leveland expects complete sentences.
Window-basedsegmentation techniques may be applied with var-ious amounts of overlaps.
The simplest method isto split documents into passages in a greedy way,starting a new passage immediately after the pre-vious one (and starting the entire process at the be-ginning of each document)1.
Another method is toallow some overlap between consecutive passages,i.e.
starting a new passage at some position withinthe previous one.
If we use the maximum possibleoverlap such an approach is usually called a ?slid-ing window?
in which the difference between twoconsecutive passages is only two basic units (sen-tences) - the first and the last one.4 Experiments4.1 SetupFor our experiments we applied the Dutch news-paper corpus used at the QA track at CLEF, thecross-language evaluation forum.
It contains about190,000 documents consisting of about 4,000,000sentences (roughly 80 million words).
As men-tioned earlier, we applied the open-source IR en-gine, Zettair, in our experiments and used a lan-guage modeling metric with Dirichlet smoothing,which is implemented in the system.The evaluation is based on 778 Dutch CLEFquestions from the QA tracks in the years 2003 ?2005 which are annotated with their answers.
Weuse simple matching of possible answer strings todetermine if a passage is relevant for finding anaccepted answer or not.
Similarly, answer stringmatching is applied to evaluate the output of theentire QA system; i.e.
an answer by the systemis counted as correct if it is identical to one of theaccepted answer strings without looking at the sup-porting sentence/passage.
For evaluation we usedthe standard measure of MRR which is defined asfollows:1Note that in our approach we still keep the documentboundaries intact, i.e.
the segmentation ends at the end ofeach document and starts from scratch at the beginning of thenext document.
In this way, the last passage in a documentmay be smaller than the pre-defined fixed size.20MRRQA=1NN?11rank(first correct answer)Using the string matching strategy for evalu-ation this corresponds to the lenient MRR mea-sures frequently used in the literature.
Strict MRRscores (requiring a match with supporting docu-ments) is less appropriate for our data coming fromthe CLEF QA tracks.
In CLEF there are usuallyonly a few participants and, therefore, only a smallfraction of relevant documents are known for thegiven questions.4.2 Evaluation of Passage RetrievalThere are various metrics that can be employed forevaluating passage retrieval.
Commonly it is ar-gued that passage retrieval for QA is merely a fil-tering task and ranking (precision) is less impor-tant than recall.
Therefore, the measure of redun-dancy has been introduced which is defined as theaverage number of relevant passages retrieved perquestion (independent of any ranking).
Passage re-trieval is, of course, a bottleneck in QA systemsthat make use of such a component.
The systemhas no chance to find an answer if the retrieval en-gine fails to return relevant passages.
Therefore,another measure, coverage is often used in combi-nation with redundancy.
It is defined as the pro-portion of questions for which at least one relevantpassage is found.
In order to validate the use ofthese measures in our setup we experimented withretrieving various amounts of paragraphs.
Figure 2illustrates the relation of coverage and redundancyscores compared to the overall QA performancemeasured in terms of MRR scores.From the figure we can conclude that cover-age is more important than redundancy in our sys-tem.
In other words, our QA system is quite goodin finding appropriate answers if there is at leastone relevant passage in the set of retrieved ones.Redundancy on the other hand does not seem toprovide valuable insides for the end-to-end perfor-mance of our QA system.However, our system also uses the passage re-trieval score (and, hence, the ranking) as a cluefor answer extraction.
Therefore, other standardIR measures might be interesting for our investi-gations as well.
The following three metrics arecommon in the IR literature.0204060801000  20  40  60  80  100coverage/MRR(in%)number of paragraphs retrievedIR coverageredundancy2.557.510IR redundancy       QA MRRFigure 2: The correlation between coverage andredundancy and MRRQAwith varying numbersof paragraphs retrieved.
Note that redundancy andcoverage use different scales on the y-axis whichmakes them not directly comparable.
The inten-tion of this plot is to illustrate the tendency of bothmeasures in comparison with QA performance.Mean average precision (MAP): Average ofprecision scores for top k documents; MAPis the mean of these averages over all the Nqueries.MAP =1NN?n=11KK?k=1Pn(1..k)(Pn(1..k) is the precision of the top k docu-ments retrieved for query qn)Uninterpolated average precision (UAP):Average of precision scores at each relevantdocument retrieved; UAP is the mean ofthese averages over the N queries.UAP =1NN?n=11|Dnr|?k:dk?DnrPn(1..k)(Dnris the set of relevant documents amongthe ones retrieved for question n)Mean reciprocal ranks: The mean of the recip-rocal rank of the first relevant passage re-trieved.MRRIR=1NN?11rank(first relevant passage)In figure 3 the correlation of these measures withthe overall QA performance is illustrated.21152025303540455055600  20  40  60  80  100MRR/UAP/MAP(in%)number of paragraphs retrievedIR MRRIR UAPIR MAP QA MRRFigure 3: The correlation between IR evaluationmeasures (MAP , UAP and MRRIR) and QAevaluation scores (MRRQA) with varying num-bers of paragraphs retrieved.From the picture we can clearly see that theMRRIRscores correlate the most with the QAevaluation scores when retrieving different num-bers of paragraphs.
This, again, confirms the im-portance of coverage as the MRRIRscore onlytakes the first relevant passage into account and ig-nores the fact that there might be more answersto be found in lower ranked passages.
Hence,MRRIRseems to be a good measure that com-bines coverage with an evaluation of the rank-ing and, therefore, we will use it as our main IRevaluation metric instead of coverage, redundancy,MAP & UAP.4.3 BaselinesThe CLEF newspaper corpus comes with para-graph markup which can easily be used as the seg-mentation granularity for passage retrieval.
Table1 shows the scores obtained by different baselineretrieval approaches using either sentences, para-graphs or documents as base units.We can see from the results that document re-trieval (used for QA) is clearly outperformed byboth sentence and paragraph retrieval.
Surpris-ingly, sentence retrieval works even better thanparagraph retrieval when looking at the QA per-formance even though all IR evaluation measures(cov, red, MRRIR) suggest a lower score.
Notethat MRRIRis almost as good as MRRQAforsentence retrieval whereas the difference betweenthem is quite large for the other settings.
This indi-cates the importance of narrowing down the searchspace for the answer extraction modules.
TheMRR#sent cov red IR QA CLEFsent 16,737 0.784 2.95 0.490 0.487 0.430par 80,046 0.842 4.17 0.565 0.483 0.416doc 618,865 0.877 6.13 0.666 0.457 0.387Table 1: Baselines with sentence (sent), paragraph(par) and document (doc) retrieval (20 units).MRRQAis measured on the top 5 answers re-trieved.
CLEF is the accuracy of the QA systemmeasured on the top answer provided by the sys-tem.
cov refers to coverage and red refers to redun-dancy.
#sent gives the total number of sentencesincluded in the retrieved text units to give an im-pression about the amount of text to be processedby subsequent answer extraction modules.amount of data to be processed is much smallerfor sentence retrieval than for the other two whilecoverage is still reasonably high.
The CLEF scores(accuracy measured on the top answer provided bythe system) follow the same pattern.
Here, the dif-ference between sentence retrieval and documentretrieval is even more apparent.Certainly, the success of the retrieval compo-nent depends on the metric used for ranking doc-uments as implemented in the IR engine.
In or-der to verify the importance of document seg-mentation in a QA setting we also ran experi-ments with another standard metric implementedin Zettair, the Okapi BM-25 metric (Robertson etal., 1992).
Similar to the previous setting usingthe LM metric, QA with paragraph retrieval (nowyielding MRRQA= 0.460) outperforms QA withdocument retrieval (MRRQA= 0.449).
How-ever, sentence retrieval does not perform as well(MRRQA= 0.420) which suggests that the Okapimetric is not suited for very small retrieval units.Still, the success of paragraph retrieval supportsthe advantage of passage retrieval compared todocument retrieval and suggests potential QA per-formance gains with improved document segmen-tation strategies.
In the remaining we only reportresults using the LM metric for retrieval due to itssuperior performance.4.4 Semantically Motivated PassagesAs described earlier, coreference chains can beused to extract semantically coherent passagesfrom textual documents.
In our experiments weused several settings for the integration of suchpassages in the retrieval engine.
First of all, coref-22erence chains have been used as the only wayof forming passages.
Sentences which are notincluded in any passage are included as single-sentence passages.
This settings is referred to assent/coref.In the second setting we restrict the passages inlength.
Coreference chains can be arbitrary longand, as we can see in the results in table 2, theIR engine tends to prefer long passages which isnot desirable in the QA setting.
Hence, we definethe constraint that passages have to be longer than200 characters and shorter than 1000.
This setup isreferred to as sent/coref (200-1000).In the third setting we combine paragraphs (us-ing existing markup) and coreference chain pas-sages including the length restriction.
This ismainly to get rid of the single-sentence passagesincluded in the previous settings.
Note that allparagraphs are used even if all sentences withinthem are included in coreferential passages.
Notealso that in all settings passages may refer tooverlapping text units as coreference chains maystretch over various overlapping passages of a doc-ument.We did not perform an exhaustive optimizationof the length restriction.
However, we experi-mented with various settings and 200-1000 was thebest performing one in our experiments.
For illus-tration we include one additional experiment usinga slightly different length constraint (200-400) intable 2.For the document segmentation strategy us-ing TextTiling we used a freely available im-plementation of that algorithm (the Perl ModuleLingua::EN::Segmenter::TextTiling availableat CPAN).
Note that we do not include other pas-sages in this approach (paragraphs using existingmarkup nor single-sentence passages).Table 2 summarizes the scores obtained by thevarious settings when applied for passage retrievaland when embedded into the QA system.It is worth noting that including coreferentialchains without length restriction forced the re-trieval engine to return a lot of very long passageswhich resulted in a degraded QA performance(also in terms of processing time which is notshown here).
The combination of paragraphs andcoreferential passages with length restrictions pro-duced MRRQAscores above the baseline.
How-ever, these improvements are not statistically sig-nificant according to the Wilcoxon matched-pairMRR#sent IR QA CLEFsent/coref 490,968 0.604 0.469 0.405sent/coref (200-1000) 76,865 0.535 0.462 0.395par+coref (200-1000) 82,378 0.560 0.493 0.426par+coref (200-400) 67,580 0.555 0.489 0.422TextTiling 107,879 0.586 ?
0.503 0.434Table 2: Passage retrieval with document segmen-tation using coreference chains and TextTiling (re-trieving a maximum of 20 passages; ?
means sig-nificant with p < 0.05 and Wilcoxon Matched-pairSigned-Ranks Test compared to paragraph base-line ?
only tested for MRRQA)signed-ranks test and looking at the correspondingCLEF scores we can even see a slight drop in per-formance.
Applying TextTiling yielded improvedscores in both passage retrieval and QA perfor-mance (MRRQAand CLEF).
The MRRQAim-provement is statistically significant according tothe same test.4.5 Window-based PassagesIn comparison to the semantically motivated pas-sages discussed above we also looked at simplewindow-based passages as described earlier.
Herewe do not consider any linguistic clues for divid-ing the documents besides the sentence and docu-ment boundaries.
Table 3 summarizes the resultsobtained for various fixed-sized windows used fordocument segmentation.MRR#sent IR QA CLEF2 sentences 33468 0.545 ?
0.506 0.4433 sentences 50190 0.554 0.504 0.4364 sentences 66800 0.581 ?
0.512 0.4475 sentences 83575 0.588 0.493 0.4226 sentences 100110 0.583 0.489 0.4237 sentences 116872 0.572 0.491 0.4228 sentences 133504 0.577 0.481 0.4099 sentences 150156 0.578 0.475 0.40510 sentences 166810 0.596 0.470 0.396Table 3: Passage retrieval with window-based doc-ument segmentation (?
means significant withp < 0.05 and Wilcoxon Matched-pair Signed-Ranks Test)Surprisingly, we can see that window-based seg-mentation approaches with small sizes between 2and 7 sentences yield improved scores comparedto the baseline.
Two of the improvements (using2-sentence passages and 4-sentence passages) arestatistically significant.
Three settings also out-23perform the best semantically motivated segmen-tation approach.
This result was unexpected espe-cially considering the naive way of splitting docu-ments into parts disregarding any discourse struc-ture (besides document boundaries) and other se-mantic clues.We did another experiment using window-basedsegmentation and a sliding window approach.Here, fixed-sized passages are included starting atevery point in the document and, hence, variousoverlapping passages are included in the index.
Inthis way we split documents at various points andleave it to the IR engine to select the most ap-propriate ones for a given query.
The results areshown in table 4.MRR#sent IR QA CLEF2 sent (sliding) 29095 0.548 ?
0.516 0.4563 sent (sliding) 36415 0.549 0.484 0.4114 sent (sliding) 41565 0.546 0.476 0.4095 sent (sliding) 45737 0.534 0.465 0.4036 sent (sliding) 49091 0.528 0.454 0.3907 sent (sliding) 51823 0.529 0.439 0.3728 sent (sliding) 54600 0.535 0.428 0.3609 sent (sliding) 57071 0.531 0.420 0.35110 sent (sliding) 59352 0.542 0.420 0.354Table 4: Passage retrieval with window-based doc-ument segmentation and a sliding windowAgain, we see a significant improvement with2-sentence passages (the overall best score so far)but the performance degrades when increasing thewindow size.
Note that the number of sentences re-trieved is growing very slowly for larger windows.This is because more and more of the overlappingregions are retrieved and, hence, the total numberof unique sentences does not grow with the sizeof the window as we have seen in the non-slidingapproach.5 Discussion & ConclusionsOur experiments show that passage retrieval is in-deed different to general document retrieval.
Im-proved retrieval scores do not necessarily lead tobetter QA performance.
Important for QA is toreduce the search space for subsequent answer ex-traction modules and, hence, passage retrieval hasto balance retrieval accuracy and retrieval size.
Inour setup it seems to be preferable to return verysmall units with a reasonable coverage.
For this,index-time passaging is very effective.In this study we were especially interested in se-mantically motivated approaches to document seg-mentation.
In particular, two techniques have beeninvestigated, one using the well-known TextTil-ing algorithm and one using coreference chains forpassage boundary detection.
We compared themto simple window-based techniques using varioussizes.
From our experiments we can conclude thatsimple document segmentation techniques usingsmall fixed-sized windows work best among theones tested here.
Semantically motivated passagesin the retrieval component helped to slightly im-prove QA performance but do not justify the effortspent in producing them.
One of the main reasonsfor the failure of using coreference chains for seg-mentation might be the fact that this approach pro-duces many overlapping passages which does notseem to be favorable for passage retrieval.
This canalso be seen in the sliding window approach whichdid not perform as well as the one without over-lapping units (except for two-sentence passages).In conclusion, uniformity in terms of length anduniqueness (in terms of non-overlapping contents)seem to be more important than semantic coher-ence for one-step passage retrieval in QA.
A fu-ture direction could be to test an approach that bal-ances both a uniform document segmentation andsemantic coherence.ReferencesCallan, James P. 1994.
Passage-level evidence in doc-ument retrieval.
In SIGIR ?94: Proceedings of the17th annual international ACM SIGIR conference onResearch and development in information retrieval,pages 302?310, New York, NY, USA.
Springer-Verlag New York, Inc.Greenwood, Mark A.
2004.
Using pertainyms to im-prove passage retrieval for questions requesting in-formation about a location.
In Proceedings of theWorkshop on Information Retrieval for Question An-swering (SIGIR 2004), Sheffield, UK.Hearst, Marti A. and Christian Plaunt.
1993.
Subtopicstructuring for full-length document access.
In Re-search and Development in Information Retrieval,pages 59?68.Hearst, Marti A.
1997.
Texttiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.Hoste, V. 2005.
Optimization Issues in Machine Learn-ing of Coreference Resolution.
Ph.D. thesis, Univer-sity of Antwerp.Kaszkiel, Marcin and Justin Zobel.
1997.
Passage re-trieval revisited.
In SIGIR ?97: Proceedings of the20th annual international ACM SIGIR conference on24Research and development in information retrieval,pages 178?185, New York, NY, USA.
ACM Press.Kaszkiel, Marcin and Justin Zobel.
2001.
Ef-fective ranking with arbitrary passages.
Journalof the American Society of Information Science,52(4):344?364.Lester, Nicholas, Hugh Williams, Justin Zobel, FalkScholer, Dirk Bahle, John Yiannis, Bodo vonBillerbeck, Steven Garcia, and William Web-ber.
2006.
The Zettair search engine.http://www.seg.rmit.edu.au/zettair/.Llopis, F., J. Vicedo, and A. Ferra?ndez.
2002.
Pas-sage selection to improve question answering.
InProceedings of the COLING 2002 Workshop on Mul-tilingual Summarization and Question Answering.Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,R.
Girju, R. Goodrum, and V. Rus.
2000.
The struc-ture and performance of an open-domain questionanswering system.Monz, Christof.
2003.
From Document Retrieval toQuestion Answering.
Ph.D. thesis, University ofAmsterdam.Roberts, Ian and Robert Gaizauskas.
2004.
Evaluatingpassage retrieval approaches for question answering.In Proceedings of 26th European Conference on In-formation Retrieval.Robertson, Stephen E., Steve Walker, MichelineHancock-Beaulieu, Aarron Gull, and Marianna Lau.1992.
Okapi at TREC-3.
In Text REtrieval Confer-ence, pages 21?30.Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.2003.
Quantitative evaluation of passage retrieval al-gorithms for question answering.
In Proceedings ofthe SIGIR conference on Research and developmentin informaion retrieval, pages 41?47.
ACM Press.Van Deemter, K. and R. Kibble.
2000.
On coreferring:Coreference in muc and related annotation schemes.Computational Linguistics, 26(4):629?637.van Noord, Gertjan.
2006.
At Last Parsing Is NowOperational.
In TALN 2006 Verbum Ex Machina,Actes De La 13e Conference sur Le TraitementAutomatique des Langues naturelles, pages 20?42,Leuven.Vilain, M., J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1993.
A model-theoretic coreferencescoring scheme.
In Proceedings of the 6th confer-ence on Message understanding (MUC 6), pages 45?52.Zobel, Justin, Alistair Moffat, Ross Wilkinson, and RonSacks-Davis.
1995.
Efficient retrieval of partial doc-uments.
Information Processing and Management,31(3):361?377.25
