Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 312?318,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsExamining the Impacts of Dialogue Content and System Automation onAffect Models in a Spoken Tutorial Dialogue SystemJoanna DrummondDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260jmd73@cs.pitt.eduDiane LitmanDepartment of Computer ScienceLearning Research & Development Ctr.University of PittsburghPittsburgh, PA 15260litman@cs.pitt.eduAbstractMany dialogue system developers use datagathered from previous versions of the dia-logue system to build models which enable thesystem to detect and respond to users?
affect.Previous work in the dialogue systems com-munity for domain adaptation has shown thatlarge differences between versions of dialoguesystems affect performance of ported models.Thus, we wish to investigate how more mi-nor differences, like small dialogue contentchanges and switching from a wizarded sys-tem to a fully automated system, influence theperformance of our affect detection models.We perform a post-hoc experiment where weuse various data sets to train multiple mod-els, and compare against a test set from themost recent version of our dialogue system.Analyzing these results strongly suggests thatthese differences do impact these models?
per-formance.1 IntroductionMany dialogue system developers use data gatheredfrom previous versions of a system to train modelsfor analyzing users?
interactions with later versionsof the system in new ways, e.g.
detecting users?
af-fect enables the system to respond more appropri-ately.
However, this training data does not alwaysaccurately reflect the current version of the system.In particular, differences in the levels of automa-tion and the presentation of dialogue content com-monly vary between versions.
For example, Raux etal (2006) changed dialogue strategies for their Let?sGo bus information system after real-world testing.Previous work in dialogue systems with regards toanalyzing the impact of using differing training datahas primarily been in the domain adaptation field,and has focused on two areas.
First, previous workempirically analyzed the need for domain adapta-tion, i.e.
methods for porting existing classifiersto unrelated domains.
For example, Webb and Liu(2008) developed a cue-phrase-based dialogue actclassifier using the Switchboard corpus, and testedon call center data.
While this performed reason-ably, training on the call center corpus and testingon Switchboard performed poorly.The second research direction involves propos-ing methods for domain adaptation.
Margolis etal.
(2010) observed similar poor performance whenporting their dialogue act classifier between threecorpora: Switchboard, the Meeting Recorder Dia-log Act corpus, and a machine-translated version ofthe Spanish Callhome corpus.
They report promis-ing results through varying their feature set.
Blitzeret al (2007) also observed poor performance andthe need for adaptation when porting product reviewsentiment classifiers.
They used four review corporafrom Amazon (books, DVDs, electronics, and smallappliances), which yielded 12 cross-domain train-ing/testing pairs.
Their algorithmic adaptation meth-ods showed promising results.Our work is in the first direction, as we also em-pirically analyze the impact of differences in train-ing and testing corpora to demonstrate the need foradaptation methods.
However, our work differs fromdomain adaptation, as the corpora in this experimentall come from one intelligent spoken physics tutor.Instead, we analyze differences resulting from vary-312ing levels of automation and small changes in dia-logue content between versions of our system.With respect to analyzing automation, we em-pirically compare the impact of differences in train-ing on data from wizarded (WOZ) versus fully au-tomated systems.
Though many systems use datafrom a WOZ version of the system to train modelswhich are then used in fully automated versions ofthe system, the effectiveness of this method of dia-logue system development has not been tested.
Wehypothesize that models built with automated datawill outperform models built with wizarded data.Additionally, minor dialogue content changestypically exist between versions of systems.
Whilelarge changes, like changing domains, have beenshown to affect model performance, no work has in-vestigated the impact of these more minute changes.We hypothesize that these differences in dialoguecontent presentation will also affect the models.Finally, the amount of training data is a wellknown factor which affects performance of modelsbuilt using supervised machine learning.
We hy-pothesize that combining some, but not all, types oftraining corpora will improve the performance of thetrained models, e.g.
adding automated data to WOZdata will improve performance, as this provides fullyautomated examples.
We hypothesize only provid-ing more WOZ data will not be as useful.2 DataThe data used for this work comes from two priorexperiments using ITSPOKE, a spoken tutorial dia-logue system, which tutors physics novices.
Table1 describes all data used, displaying the number ofusers per data set, the number of dialogues betweenthe system and each user, the total number of userturns per corpus, and the percentage of turns labeleduncertain.
See Appendix A for more information.The first experiment, in 2007, compared twodialogue-based strategies for remediating user un-certainty over and above correctness (Forbes-Rileyand Litman, 2011b).
The goal of this work was tonot only test the hypothesis that this uncertainty re-mediation would improve users?
learning, but to in-vestigate what types of dialogue remediation wouldimprove users?
learning the most.
Since this experi-ment, WOZ-07, was designed to be a gold-standardcase of uncertainty remediation, all natural languageunderstanding and uncertainty annotation was per-formed by a human wizard, in real time (WOZ).
Allannotations were made at the turn-level.For WOZ-07, users?
dialogue interactions withthe system would change based on which remedia-tion strategy they were assigned to.
There were twodifferent dialogue-based remediation strategies.
Inaddition to varying the strategies, the two controlconditions in this experiment also varied when theremediation strategy was applied.The simple remediation dialogue strategy pro-vided additional information about the physics con-cept the user was struggling with, or asked themfurther questions about the concept.
Both controlconditions used the simple remediation strategy; oneonly applied the strategy when the user was incor-rect, the other applied it if the user was incorrect andrandomly when the user was correct.
The simple re-mediation experimental condition applied the reme-diation when the user was incorrect, or correct butuncertain about their answer.
The fourth conditionin WOZ-07 used the second dialogue strategy, com-plex remediation.
This strategy changed the way theremediation was presented, depending on a combi-nation of the user?s correctness and certainty in theiranswer.
Only users in the simple remediation exper-imental condition learned more than users in otherconditions.
Figure 1 shows an example of simple re-mediation; the tutor acknowledges that the user is in-correct, saying ?Well...?, and then explains the con-cept the previous question tested.
Appendix B com-pares simple and complex remediation strategies.Another experiment was performed in 2008,where users interacted with either a fully automated(ASR) version of ITSPOKE or a wizarded version.The goal of this experiment was to see if the learn-ing gains found in the 2007 experiment would holdin the ASR version of the system.
To mimic theWOZ-07 experiment, the wizarded version (WOZ-08) only used the simple remediation experimentalcondition found in WOZ-07, while the ASR ver-sion contained the simple remediation experimentalcondition and both simple remediation control con-ditions.
The complex remediation strategy was notincluded due to its poor performance in WOZ-07.Thus, WOZ-08 and ASR-08 used identical dialoguestrategies, with minor differences in where the reme-313TUTORp5: (Response to an incorrect answer) Well...We just discussed that by Newton?s Third law, when twoobjects collide, the forces they exert on each other areequal in magnitude and opposite in direction.
This is trueregardless of the objects?
differing masses.
So the firstquestion?s answer is that the impact forces on the truckand the car will have the same magnitude but oppositedirection.
Now, the second question asks about the ve-hicles?
change in motion.
We can use Newton?s Secondlaw to answer this.
What does this law say?TUTORp6: (Response to a correct, certain answer) Fine.So the first question?s answer is that the impact forces onthe bus and the motorbike will have the same magnitude,but opposite direction.
Now, the second question asksabout the vehicles?
change in motion.
We can use New-ton?s second law to answer this.
What does this law say?Figure 1: Corpus Excerpt: Remediation in Dialogue 5,and No Remediation in Isomorphic Dialogue 6diation would be applied.
For the ASR conditions,all models were trained on WOZ-07 data; users wererandomly assigned to the WOZ-08 or ASR-08 con-dition as they participated.In addition to eliminating the complex remedia-tion condition, a sixth dialogue, completely isomor-phic to the fifth dialogue, was added to all condi-tions.
See Appendix B dialogue examples, high-lighting their content differences.
Figure 1 displaystwo ASR-08 tutor turns with the same user.
Theseturns are from the fifth problem, and the isomorphicsixth problem.
Note that two things change betweenthese two answers.
First, the system responds to theuser?s incorrectness in the first example.
Had theuser been correct and uncertain, this is also the di-alogue s/he would have seen.
Second, notice thatproblem five discusses a car, while problem six dis-cusses a motorcycle.
To create a completely iso-morphic problem, the scenario for the dialogue waschanged from a car to a motorcycle.For both the 2007 and 2008 corpora, all gold-standard uncertainty annotations were performed bya trained human annotator.
Development and pre-vious testing of the annotation scheme between thisannotator and another trained annotator resulted inkappa = 0.62.
All wizarded conditions were an-notated in real-time; all ASR conditions were anno-Data Set #Usr #Dia #Turn %UncWOZ-07 81 5 6561 22.73WOZ-08 19 6 1812 21.85ASR-08 72 6 7216 20.55ASR-08-Train 19 6 1911 21.51ASR-08-Test 53 6 5305 20.21Table 1: Description of data setstated in a post-hoc manner.In sum, the main differences between the two sys-tems?
data are differences in automation (i.e.
WOZand ASR) and content (i.e.
presentation of content,reflected by differing dialogue strategies, and num-ber of physics dialogues).3 Post-Hoc ExperimentIn this post-hoc analysis, we will analyze the im-pact of content differences by comparing the perfor-mance of models built with WOZ-07 and WOZ-08,and automation differences by comparing modelsbuilt with WOZ-08 and ASR-08 data.
Instead of theoriginal study design, where WOZ-08 and ASR-08subjects were run in parallel, we could have gatheredthe WOZ data first, and used the WOZ data and thefirst few ASR users for system evaluation and devel-opment purposes.
Thus, for the post-hoc analysis,we mimic this by using WOZ-08 as a training set,and splitting ASR-08 into two data sets?ASR-08-Train (the first few users), and ASR-08-Test.
(Pleasesee the last two rows of Table 1.)
We held out thefirst 19 users for ASR-08-Train, since this approx-imates the amount of data used to train the modelbuilt with WOZ-08.
For our post-hoc study, the re-maining 53 ASR users were used as a test set forall training sets, to mimic an authentic developmentlifestyle for a dialogue system.
Additionally, thisguaranteed that no users appear in both the trainingand testing set given any training set.As all uncertainty remediation happens at theturn-level, we classified uncertainty at the turn-level,and compared these automated results with the gold-standard annotations.
We used all the features thatwere designed for the original model.
Since previ-ous experiments with our data showed little variancebetween different machine learning algorithms, wechose a J48 decision tree, implemented by WEKA,11http://www.cs.waikato.ac.nz/ml/weka/314for all experiments due to its easy readability.
Sinceour class distribution is skewed (see Table 1), wealso used a cost matrix which heavily penalizes clas-sifying an uncertain instance as certain.We use simple lexical, prosodic and system-specific features described in (Forbes-Riley and Lit-man, 2011a) to build our models.
These featureswere kept constant through all experiments, so theresults could be directly comparable.
For all lexicalfeatures for all data sets, ASR text was used.2 For allWOZ conditions, we gathered ASR text post-hoc.We trained models on individual training sets, toinspect the impact of content and automation dif-ferences.
We then trained new models on combi-nations of these original training sets, to investigatepossible interactions.
To allow for direct compari-son, we used ASR-08-Test to evaluate all models.Since detecting uncertainty is related to detectingaffective user states, we use the evaluation measuresUnweighted Average (UA) Recall and UA Precision,presented in (Schuller et al, 2009).We also use UAF-measure.
Note that because only one hold-outevaluation set was used, rather than using multiplesets for cross-fold validation, we do not test for sta-tistical significance between models?
results.4 ResultsThe first three rows of Table 2 present the results oftraining a model on each possible training set indi-vidually.
Note that the number of instances per train-ing set varies.
WOZ-07 simply has more users in thetraining set than WOZ-08 or ASR-08-Train.
WhileWOZ-08 and ASR-08-Train have the same numberof users, the number of turns slightly varies, sincedialogues vary depending on users?
answers.When comparing WOZ-08 to WOZ-07, first no-tice that WOZ-08 outperforms WOZ-07 with amuch smaller amount of data.
Both are wiz-arded versions, but content differences exist be-tween these experiments; WOZ-08 only used thesimple remediation strategy, and added a dialogue.When comparing ASR-08-Train to the other twoindividual training sets, note that it best approxi-mates the test set.
This training condition outper-forms all others, while using less data than WOZ-2We used ASR instead of manual transcriptions, to betterapproximate automated data.07.
While WOZ-08 and ASR-08 have the samecontent, the system changes from wizarded to au-tomated language recognition.
This allows us to di-rectly compare how differences due to automation(e.g.
errors in detecting correct answers) can affectperformance of the models.
Note that even thoughwe used ASR transcriptions of WOZ-08 turns, theeffects of ASR errors on later utterances are onlypropagated in ASR-08-Train.
As ASR-08-Train no-ticeably outperforms WOZ-08, with approximatelythe same amount of training data, we conclude thatusing automated data for training better prepares themodel for the data it will be classifying.As we also wish to investigate how incorporat-ing more diverse training data would alter the per-formance of the model, we combined ASR-08-Trainand WOZ-08 with the WOZ-07 training set, shownin Table 2.
We combined these sets practically, aswe wish to test how our model could have performedif we had used our first few 2008 users to train themodel in the actual 2008 experiment.First, note that all combination training sets out-perform individual training sets.
As ASR-08-Trainoutperformed WOZ-08 for individual training sets,it is not surprising that WOZ-07+ASR-08-Train out-performs WOZ-07+WOZ-08.However, we could have used WOZ-07 for featuredevelopment only, and trained on WOZ-08 + ASR-08-Train.
Since the training and testing sets containidentical content, it is unsurprising that the preci-sion for this classifier is high.
This classifier doesnot perform as well with respect to recall, perhapssince its training data is not as varied.
Also note,while this model trained on few data points, we usedadditional data for feature development purposes.Combining all three possible training sets doesnot outperform WOZ-07+ASR-08-Train; it per-forms equivalently, and uses much more data.
Wehypothesize that, since WOZ-07 constitutes the ma-jority of the training set, the benefit of includingWOZ-08 may be mitigated.
Downsampling WOZ-07 could test this hypothesis.
Alternatively, the ben-efit of combining WOZ-07+ASR-08-Train could bethat we provide many varied examples in this com-bined training set.
Since WOZ-07 already accountsfor differences in both content and automation,WOZ-08 doesn?t introduce novel examples for theclassifier, and adding it may not be beneficial.315Training Set n UA Rec.
UA Prec.
UA F1WOZ-07 6561 54.6% 53.0% 53.79%WOZ-08 1812 58.0% 55.4% 56.67%ASR-08-Train 1911 60.5% 57.2% 58.80%WOZ-07 + WOZ-08 8373 66.1% 61.0% 63.45%WOZ-07 + ASR-08-Train 8472 68.3% 63.5% 65.81%WOZ-08 + ASR-08-Train 3723 64.0% 73.4% 68.38%WOZ-07 + WOZ-08 + ASR-08-Train 10284 68.3% 63.6% 65.86%Table 2: Results; Testing on ASR-08-Test (n = 5305).
Bold denotes best performance per metric.In sum, different training set combinations pro-vide different benefits.
With respect to UA F1 andUA Precision, WOZ-08 + ASR-08-Train outper-forms all other training sets.
Using only 3723 turnsto train the model, this configuration uses the leastamount of training data.
However, this requires pre-viously collected data, such as WOZ-07, for fea-ture development purposes.
Alternatively, WOZ-07 + ASR-08-Train performs better than WOZ-08 +ASR-08-Train with respect to UA Recall, and doesnot require a separate feature development set.
Thus,the ?best?
training set would depend on both the ex-perimental design, and the preferred metric.5 Discussion and Future WorkIn this paper, we provided evidence that the degreeof automation of a system used to collect trainingdata can impact the performance of a model whenused in a fully automated system.
Since one com-mon technique of building fully automated dialoguesystems uses a semi-automated wizarded version,this result suggests incorporating a small amount ofautomated data could greatly improve performanceof the models.
Our results also suggest that the typeof data is more important than the quantity whenbuilding these models, since well-performing mod-els were built with small amounts of data.
We alsoinvestigated the impact of building models trainedwith different dialogue content, another commonmethod of developing dialogue systems.
As theWOZ-08 model outperforms the WOZ-07 model, itappears that this has a noticeable impact.However, the WOZ-08 and WOZ-07 experimentsmay not have had identical user population, due tothe timing differences between studies.
We wishto perform further post hoc-experiments to analyzethe impact of population differences in our data.
Todo so, we will eliminate all dialogue strategy dif-ferences between WOZ-07 and WOZ-08.
To fur-ther support our results regarding content differ-ences, we wish to split WOZ-08 into two trainingsets, one including the sixth problem, and one ex-cluding it.
After controlling for differences in quan-tity of data, we will analyze the resulting models.To further strength our results regarding automa-tion differences, we will eliminate all differences inwhen the remediation dialogue strategy was appliedbetween the WOZ-08 and ASR-08-Test corpus, andtry to replicate the results found in this paper.As our results suggest the need for applying do-main adaptation methods to improve models?
per-formance when there are differences in automationand content, future work could investigate applyingalready existing methods for domain adaptation, anddeveloping new ones for this problem.
In particular,the results we presented suggest a method for build-ing a dialogue system that could mitigate the effectsof changes in automation and content.
A small wiz-arded condition, with changes in dialogue content,could be used for feature development.
This data, ordata from another small wizarded condition, couldthen be used to train a preliminary model.
This pre-liminary model could be tested with a small num-ber of users using an automated version.
Then, thedata from the preliminary conditions could be usedto build the final model, which would be used for thecurrent, fully automated version of the system.AcknowledgmentsWe thank Michal Valko, Michael Lipschultz,Wenting Xiong, and the ITSPOKE group for helpfulcomments and suggestions, and an REU supplementto NSF Grant #0631930 for funding this work.316ReferencesJ.
Blitzer, M. Dredze, and F. Pereira.
2007.
Biogra-phies, bollywood, boom-boxes and blenders: Domainadaptation for sentiment classification.
In AnnualMeeting-Association For Computational Linguistics,volume 45, page 440.K.
Forbes-Riley and D. Litman.
2011a.
Benefits andchallenges of real-time uncertainty detection and adap-tation in a spoken dialogue computer tutor.
SpeechCommunication.K.
Forbes-Riley and D. Litman.
2011b.
Designing andevaluating a wizarded uncertainty-adaptive spoken di-alogue tutoring system.
Computer Speech & Lan-guage, 25(1):105?126.A.
Margolis, K. Livescu, and M. Ostendorf.
2010.
Do-main adaptation with unlabeled data for dialog act tag-ging.
In Proceedings of the 2010 Workshop on Do-main Adaptation for Natural Language Processing,pages 45?52.
Association for Computational Linguis-tics.A.
Raux, D. Bohus, B. Langner, A.W.
Black, and M. Es-kenazi.
2006.
Doing research on a deployed spokendialogue system: One year of Lets Go!
experience.
InProc.
Interspeech, pages 65?68.
Citeseer.B.
Schuller, S. Steidl, and A. Batliner.
2009.
The in-terspeech 2009 emotion challenge.
In Tenth AnnualConference of the International Speech Communica-tion Association.N.
Webb and T. Liu.
2008.
Investigating the portabilityof corpus-derived cue phrases for dialogue act classifi-cation.
In Proceedings of the 22nd International Con-ference on Computational Linguistics-Volume 1, pages977?984.
Association for Computational Linguistics.317Data Set Dialogue #5 Dialogue #6Avg.
Avg.
Avg.
Avg.#Turn %Unc #Turn %UncWOZ-07 15.21 26.87 N/A N/AWOZ-08 14.37 28.21 13.42 9.02ASR-08 16.26 19.90 16.28 9.73ASR-08-Train 16.05 19.67 16.00 11.84ASR-08Test 16.38 19.98 16.38 8.99Table 3: Dialogue-level description of corporaAppendix A Differences between corporaWe note that one possible difficulty in using dia-logues with differing content could be differing lev-els of users?
uncertainty between those dialogues.Thus, Table 3 depicts the average percent of uncer-tain turns over all users, per dialogue.
Only Dia-logues #5 and #6 are presented; average number ofturns and average percent uncertainty do change be-tween all problems.
While the average number ofturns are similar, the percentage of uncertain turnsdrops greatly between the two isomorphic problems.Appendix B Dialogue ExamplesWe also present dialogue examples from our cor-pora to illustrate various content differences be-tween versions of ITSPOKE.
The changes betweenDialogues #5 and #6 are as follows: every instanceof ?car?
in Dialogue #5 with ?motorbike,?
and ev-ery instance of ?truck?
is replaced with ?PAT bus.
?Since users discuss Problem #6 with ITSPOKE im-mediately after discussing Problem #5, the drop inpercentage of user uncertainty between Dialogue #5and Dialogue #6 show in Table 3 might be explainedby this.
However, all of the domain-specific wordsand all of the knowledge concepts will remain thesame between these two problems.Figures 1 (in the main paper) and 2 give examplesof the two types of remediation found in the 2007corpus.
ITSPOKE can change its dialogue based onnot only how it presents the content, but also whatfeedback it gives.
Complex remediation changesboth of these attributes.
Table 4 displays possiblefeedbacks given to the user, depending on their lev-els of correctness and certainty.
In Figure 2, the seg-Tutor: What?s the overall net force on the truck equal to?User: Zero??
[Incorrect & Uncertain]Tutor: That?s not correct, but don?t worry.
You seemto be aware of your mistake.
Let?s resolve it.
[FEED-BACK] The net force on the truck is equal to the impactforce on it.
Let?s walk through this answer step by step.
[NEW SHORT ANSWER] We can derive the net force onthe truck by summing the individual forces on it, just likewe did for the car.
First, what horizontal force is exertedon the truck during the collision?
[EXISTING SUBDIA-LOGUE]Figure 2: Example of Complex uncertainty remediation.User Answer Examples ofFeedback PhrasesSimple ComplexCorrect & That?s That?s right.Certain right.Correct & That?s That?s right, but you don?tUncertain right.
sound very certain, so let?srecap.Incorrect & Well... Good try, but that?s notUncertain right.
It sounds like youknew there might be anerror in your answer.Let?s fix it.Incorrect & Well...
I?m sorry, but there?s aCertain mistake in your answer thatwe need to work out.Table 4: Example Feedback Phrases used in Simple andComplex Remediationment of the tutor?s turn is labeled after that segmentis completed (e.g.
the Feedback is ?That?s not cor-rect... resolve it.?).
The type of remediation can alsochange.
While Figure 1 depicts the normal remedi-ation path as if the user had answered incorrectly orcorrect but uncertain, complex remediation, shownin Figure 2, first gives the user a short version of theanswer that they should have given, before movingdown the normal remediation path.318
