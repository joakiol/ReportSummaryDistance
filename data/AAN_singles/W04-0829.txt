WSD Based on Mutual Information and Syntactic PatternsDavid Ferna?ndez-Amoro?sDepartamento de Lenguajes y Sistemas Informa?ticosUNEDdavid@lsi.uned.esAbstractThis paper describes a hybrid system for WSD, pre-sented to the English all-words and lexical-sampletasks, that relies on two different unsupervised ap-proaches.
The first one selects the senses accordingto mutual information proximity between a contextword a variant of the sense.
The second heuristicanalyzes the examples of use in the glosses of thesenses so that simple syntactic patterns are inferred.This patterns are matched against the disambigua-tion contexts.
We show that the first heuristic ob-tains a precision and recall of .58 and .35 respec-tively in the all words task while the second obtains.80 and .25.
The high precision obtained recom-mends deeper research of the techniques.
Resultsfor the lexical sample task are also provided.1 IntroductionWe will describe in this paper the system that wepresented to the SENSEVAL-3 competition in theEnglish all-words and lexical-sample tasks.
It is anunsupervised system that relies only on dictionaryinformation and raw coocurrence data that we col-lected from a large untagged corpus.
There is alsoa supervised extension of the system for the lexicalsample task that takes into account the training dataprovided for the lexical sample task.
We will de-scribe two heuristics; the first one selects the senseof the words?
synset with a synonym with the high-est Mutual Information (MI) with a context word.This heuristic will be covered in section 2.
The sec-ond heuristic relies on a set of syntactic structurerules that support particular senses.
This rules havebeen extracted from the examples in WordNet senseglosses.
Section 3 will be devoted to this technique.In section 4 we will explain the combination of bothheuristics to finish in section 5 with our conclusionsand some considerations for future work.2 Selection of the closest variantIn the second edition of SENSEVAL, we presenteda system, described in (Ferna?ndez-Amoro?s et al,2001), that assigned scores to each word senseadding up Mutual Information estimates between allthe pairs (word-in-context, word-in-gloss).
We haveidentified some problems with this technique.?
This exhaustive use of the mutual informationestimates turned out to be very noisy, given thatthe errors in the individual mutual informationestimates often correlated, thus affecting the fi-nal score for a sense.?
Sense glosses usually contain vocabulary thatis not particularly relevant to the specific sense.?
Another typical problem for unsupervised sys-tems is that the sense inventory contains manysenses with little or no presence in actual texts.This last problem has been addressed in a verystraightforward manner, since we have dis-carded the senses for a word with a relative fre-quency below 10%.The first problem might very well improve by it-self when larger untagged corpora are available andincreasing computing power eliminates the need fora limited controlled vocabulary in the MI calcula-tions.
Anyway, a solution that we have tried to im-plement for this source of problems, that is, cumula-tive errors in estimates biasing the final result, con-sists in restricting the application of the MI measureto promising candidates.An interesting criterion for the selection of thesecandidates is to select those words in the contextthat form a collocation with the word to be disam-biguated, in the sense that is defined in (Yarowsky,1993).
Yarowsky claimed that collocations arenearly monosemous, so identifying them would al-low us to focus on very local context, which shouldmake the disambiguation process, if not more effi-cient, at least easier to interpretate.One example of test item that was incor-rectly disambiguated by the systems described in(Ferna?ndez-Amoro?s et al, 2001) is the word churchin the sentence :Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of SystemsAn ancient stone church stands amid the fields,the sound of bells cascading from its tower, callingthe faithful to evensong.The applicable collocation here would benoun/noun so that stone is the context word to beused.To address the second problem, the use of non-relevant words in the glosses, we have decidedto consider only the variants (the synonyms in asynset,in the case of WordNet) of each sense.
Thesesynonyms (i.e.
variants of a sense) constitute theintimate matter of WordNet synsets, a change in asynset implies a change in the senses of the cor-responding words, while the glosses are just addi-tional information of secondary importance in thedesign of the sense inventory.
To continue withthe example, the synonyms for the three synsetsfor church in WordNet are (excluding church itself,which is obviously common to all the synsets) :?
Christian church ?
Christian (16), Christian-ity (11)?
church building ?
building (187)?
church service ?
service (6)We didn?t compute MI of compound words soinstead we splitted them.
Since church is theword to be disambiguated, Christian church is con-verted to church, church building to building andchurch service to service.
The numbers in paren-thesis indicate the MI 1 between the term and stone.In this case we have a clear and strong preferencefor the second sense, which happens to be in accor-dance with the gold standard.Unfortunately, we didn?t have the time to finish acollocation detection procedure, we just had enoughtime to POS-tag the text with the Brill tagger (Brill,1992) and parse it with the Collins parser (Collins,1999).
That effort was put to use in the syntacticpattern-matching heuristic in the next section, so inthis case we just limited ourselves to detect, for eachvariant, the context word with the highest MI.It is important to note that this heuristic is notdependent on the glosses and it is completely un-supervised, so that it is possible to apply it to anylanguage with a sense inventory based on variants,as is the case with the languages in EuroWordNet,and an untagged corpus.We have evaluated this heuristic and the resultsare shown in table 11for words a and b, MI(a,b)= p(a?b)p(a)?p(b) , the probabilities areestimated in a corpus.Task Attempted Prec Recallall words 1215 / 2041 .58 .35lexical sample 938 / 3944 .45 .11Table 1: Closest variant heuristic resultstheDTartNN TTNPofIN*NP LLPP @@NPFigure 1: Example of syntactic pattern3 Syntactic patternsThis heuristic exploits the regularity of syntacticpatterns in sense disambiguation.
These repetitivepatterns effectively exist, although they might cor-respond to different word meanings .
One exampleis the pattern in figure 1which usually corresponds to a specific sense ofart in the SENSEVAL-2 English lexical sampletask.This regularities can be attached to different de-grees of specificity.
One system that made use ofthese regularities is (Tugwell and Kilgarriff, 2001).The regularities were determined by human inter-action with the system.
We have taken a differentapproach, so that the result is a fully automatic sys-tem.
As in the previous heuristic, we didn?t take intoconsideration the senses with a relative frequencybelow 10%.Due to time constraints we couldn?t devise amethod to identify salient syntactic patterns usefulfor WSD, although the task seems challenging.
In-stead, we parsed the examples in WordNet glosses.These examples are usually just phrases, not com-plete sentences, but they can be used as patternsstraightaway.
We parsed the test instances as welland looked for matches of the example inside theparse tree of the test instance.
Coverage was verylow.
In order to increase it, we adopted the follow-ing strategy : To take a gloss example and go downthe parse tree looking for the word to disambiguate.The subtrees of the visited nodes are smaller andsmaller.
Matching the whole syntactic tree of theexample is rather unusual but chances increase witheach of the subtrees.
Of course, if we go too low inthe tree we will be left with the single target word,which should in principle match all the correspond-architectureNNNPbeVBZtheDTartNN TTNPofINwastingVBGspaceNN  llNP LLPP!!!!
@@NPbeatifullyRBAVDP  XXXXXXXVP((((((((((bbbSFigure 2: Top-level syntactic patternbeVBZtheDTartNNTTNPofINwastingVBGspaceNN  llNP LLPP!!!!
@@NPbeatifullyRBAVDP  XXXXXXXVPFigure 3: Second syntactic patterning trees of the test items of the same word.
Wewill illustrate the idea with an example.
An exam-ple of an art sense gloss is : Architecture is the artof wasting space beatifully.
We can see the parsetree depicted in figure 2.We could descend from the root, looking for theoccurrence of the target word and obtain a second,simpler, pattern, shown in figure 3.Following the same procedure we would acquirethe patterns shown in figures 4 y 5, and the wewould be left with mostly useless pattern shown infigure 6Since there is an obvious tradeoff between cover-age and precision, we have only made disambigua-tion rules based on the first three syntactic levels,and rejected rules with a pattern with only one word.Still, coverage seems to be rather low and thereare areas of the pattern that look like they could begeneralized without much loss of precision, evenwhen it might be difficult to identify them.
OurtheDTartNN TTNPofINwastingVBGspaceNN  llNPLLPP!!!!
@@NPFigure 4: third syntactic patterntheDTartNNTTNPFigure 5: fourth syntactic patternhypothesis is that function words play an impor-tant role in the discovery of these syntactic patterns.We had no time to further investigate the fine-tuningof these patterns, so we added a series of transfor-mations for the rules already obtained.
In the firstplace, we replaced every tagged pronoun form witha wildcard meaning that every word tagged as a pro-noun would match.
In order to increase even morethe number of rules we derive more rules keepingthe part-of-speech tags and replacing content wordswith wilcards.We wanted to derive a larger set of rules, with thetwo-fold intention of achieving increased coverageand also to test if the approach was feasible with arule set in the order of the hundreds of thousandsor even millions.
Every rule specifies the word forwhich it is applicable (for the sake of efficiency) andthe sense the rule supports, as well as the syntacticpattern.
We derived new rules in which we substi-tuted the word to be disambiguated for each of itsvariants in the corresponding sense (i.e.
the syn-onyms in the corresponding synset).
The substitu-tion was carried out sensibly in all the four fields ofthe rule, with the new word-sense (corresponding toartNNFigure 6: fifth syntactic patternthe same synset as the old one), the new variant andthe new syntactic pattern.
This way we were able toeffectively multiply the size of the rule set.We have also derived a set of disambiguationrules based on the training examples for the Englishlexical sample task.
The final rule set consists ofmore than 300000 rules.
The score for a sense isdetermined by the total number of rules it matches.We only take the sense with the highest score.The results of the evaluation for this heuristic areshown in table 2Task Attempted Prec Recallall words 648 / 2041 .80 .25lexical sample 821 / 3944 .51 .11Table 2: Syntactic pattern heuristic results4 CombinationSince we are interested in achieving a high recalland both our heuristics have low coverage, we de-cided to combine the results in a blind way withthe first sense heuristic.
We did a linear combina-tion of the three heuristics, weighting the three ofthem equally, and returned the sense with the high-est score.5 Conclusions and future workThe official results clearly show that the dependencyof the system on the first sense heuristic is verystrong.
We should have been more confident in ourheuristics so that maybe a linear combination giv-ing more weight to them in opposition to the firstsense baseline would have produced better results.The supervised extension of the algorithm, in whichthe syntactic patterns are learnt from the training ex-amples as well as from the synset?s glosses doesn?toffer any improvement at all.
The simple explana-tion is that the increase in the number of rules fromthe unsupervised heuristic to the supervised exten-sion is only 17% so no changes are noticeable at theanswer level.The results for the two heuristics are very encour-aging.
There are several points that deserve furtherinvestigation.
It should be relatively easy to detectYarowsky?s collocations from a parse tree and thatis likely to offer even better results in terms of preci-sion, although the potential for increased coverageis unclear.
As far as the other heuristic is concerned,it seems worthwhile to spend some time determin-ing syntactic patterns more accurately.
A good pointto start could be statistical language modeling overlarge corpora, now that we have adapted the existingresources and parsing massive text collections is rel-atively easy.
Of course, a WSD system aimed for fi-nal applications should also take advantage of otherknowledge sources researched in previous work.ReferencesEric Brill.
1992.
A simple rule-based part-of-speech tagger.
In Proceedings of ANLP-92, 3rdConference on Applied Natural Language Pro-cessing, pages 152?155, Trento, IT.Michael Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania.D.
Ferna?ndez-Amoro?s, J. Gonzalo, and F. Verdejo.2001.
The UNED systems at SENSEVAL-2.In Second International Workshop on Evaluat-ing Word Sense Disambiguation Systems (SEN-SEVAL), Toulouse, pages 75?78.David Tugwell and Adam Kilgarriff.
2001.
Wasp-bench : A lexicographic tool supporting wordsense disambiguation.
In David Yarowsky andJudita Preiss, editors, Second International Work-shop on Evaluating Word Sense DisambiguationSystems (SENSEVAL), Toulouse, pages 151?154.David Yarowsky.
1993.
One Sense per Collocation.In Proceedings, ARPA Human Language Tech-nology Workshop, pages 266?271, Princeton.
