Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 62?69,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe Hinoki Sensebank?
A Large-Scale Word Sense Tagged Corpus of Japanese ?Takaaki Tanaka, Francis Bond and Sanae Fujita{takaaki,bond,fujita}@cslab.kecl.ntt.co.jpNTT Communication Science Laboratories,Nippon Telegraph and Telephone CorporationAbstractSemantic information is important forprecise word sense disambiguation systemand the kind of semantic analysis used insophisticated natural language processingsuch as machine translation, questionanswering, etc.
There are at least twokinds of semantic information: lexicalsemantics for words and phrases andstructural semantics for phrases andsentences.We have built a Japanese corpus of overthree million words with both lexicaland structural semantic information.
Inthis paper, we focus on our method ofannotating the lexical semantics, that isbuilding a word sense tagged corpus andits properties.1 IntroductionWhile there has been considerable research onboth structural annotation (such as the PennTreebank (Taylor et al, 2003) or the Kyoto Corpus(Kurohashi and Nagao, 2003)) and semanticannotation (e.g.
Senseval: Kilgariff andRosenzweig, 2000; Shirai, 2002), there are almostno corpora that combine both.
This makes itdifficult to carry out research on the interactionbetween syntax and semantics.Projects such as the Penn Propbank are addingstructural semantics (i.e.
predicate argumentstructure) to syntactically annotated corpora,but not lexical semantic information (i.e.
wordsenses).
Other corpora, such as the EnglishRedwoods Corpus (Oepen et al, 2002), combineboth syntactic and structural semantics in amonostratal representation, but still have nolexical semantics.In this paper we discuss the (lexical) semanticannotation for the Hinoki Corpus, which ispart of a larger project in psycho-linguisticand computational linguistics ultimately aimed atlanguage understanding (Bond et al, 2004).2 Corpus DesignIn this section we describe the overall design ofthe corpus, and is constituent corpora.
The basicaim is to combine structural semantic and lexicalsemantic markup in a single corpus.
In order tomake the first phase self contained, we startedwith dictionary definition and example sentences.We are currently adding other genre, to make thelangauge description more general, starting withnewspaper text.2.1 Lexeed: A Japanese Basic LexiconWe use word sense definitions from Lexeed:A Japanese Semantic Lexicon (Kasahara et al,2004).
It was built in a series of psycholinguisticexperiments where words from two existingmachine-readable dictionaries were presented tosubjects and they were asked to rank them on afamiliarity scale from one to seven, with sevenbeing the most familiar (Amano and Kondo,1999).
Lexeed consists of all words with afamiliarity greater than or equal to five.
Thereare 28,000 words in all.
Many words havemultiple senses, there were 46,347 differentsenses.
Definition sentences for these sentenceswere rewritten to use only the 28,000 familiarwords.
In the final configuration, 16,900 differentwords (60% of all possible words) were actuallyused in the definition sentences.
An exampleentry for the word ??
{? doraiba?
?driver?is given in Figure 1, with English glosses added.This figure includes the sense annotation andinformation derived from it that is described in thispaper.Table 1 shows the relation between polysemyand familiarity.
The #WS column indicates theaverage number of word senses that polysemous62????????????????????????????????????????????????????????
?INDEX ??
{? doraiba-POS noun Lexical-Type noun-lexFAMILIARITY 6.5 [1?7] (?
5) Frequency 37 Entropy 0.79SENSE 1(0.11)???????????
?DEFINITION F11/k/F0?e1/8/  /e&1<1/8/2d/?1/a tool for inserting and removing screws .EXAMPLE ?Hf??
{?<?GF1k2Z8he used a small screwdriver to tighten the screws on his glasses.HYPERNYM ?1 equipment ?tool?SEM.
CLASS ?942:tool/implement?
(?
?893:equipment?
)WORDNET screwdriver1???????????
?SENSE 2(0.84)???????????
?DEFINITION ?
1/k/?U1/2d/01/Someone who drives a car.EXAMPLE ?H,?C??{?A0??
?.e8my father was given an award as a good driver.HYPERNYM 01 hito ?person?SEM.
CLASS ?292:chauffeur/driver?
(?
?5:person?
)WORDNET driver1???????????
?SENSE 3(0.05)??????????????
?DEFINITION ??
?1// /2??
1/X/G/???3/??/}?
?/In golf, a long-distance club.
A number one wood.EXAMPLE ?H??{??????
?I08he hit (it) 30 yards with the driver.HYPERNYM ??
?3 kurabu ?club?SEM.
CLASS ?921:leisure equipment?
(?
921)WORDNET driver5DOMAIN ??
?1 gorufu ?golf????????????????????????????????????????????????????????????????????????
?Figure 1: Entry for the Word doraiba?
?driver?
(with English glosses)words have.
Lower familiarity words tend tohave less ambiguity and 70 % of words with afamiliarity of less than 5.5 are monosemous.
Mostpolysemous words have only two or three sensesas seen in Table 2.Fam #WordsPoly-semous #WS#Mono-semous(%)6.5 - 368 182 4.0 186 (50.5)6.0 - 4,445 1,902 3.4 2,543 (57.2)5.5 - 9,814 3,502 2.7 6,312 (64.3)5.0 - 11,430 3,457 2.5 7,973 (69.8)Table 1: Familiarity vs Word Sense Ambiguity2.2 OntologyWe also have an ontology built from the parseresults of definitions in Lexeed (Nichols and Bond,2005).
The ontology includes more than 50thousand relationship between word senses, e.g.synonym, hypernym, abbreviation, etc.2.3 Goi-TaikeiAs part of the ontology verification, all nominaland most verbal word senses in Lexeed were#WS #Words1 184602 62123 20404 7995 3116 1877 998 539 3510 1511 1912 1313 1314 615 616 317 218 319 120 2?
21 19Table 2: Number of Word Senseslinked to semantic classes in the Japanesethesaurus, Nihongo Goi-Taikei (Ikehara et al,1997).
Common nouns are classified into about2,700 semantic classes which are organized into a63semantic hierarchy.2.4 Hinoki TreebankLexeed definition and example sentences aresyntactically and semantically parsed with HPSGand correct results are manually selected (Tanakaet al, 2005).
The grammatical coverage over allsentences is 86%.
Around 12% of the parsedsentences were rejected by the treebankers dueto an incomplete semantic representation.
Thisprocess had been done independently of wordsense annotation.2.5 Target CorporaWe chose two types of corpus to mark up: adictionary and two newspapers.
Table 3 showsbasic statistics of the target corpora.The dictionary Lexeed, which defined wordsenses, is also used for a target for sense tagging.Its definition (LXD-DEF) and example (LXD-EX)sentences consist of basic words and functionwords only, i.e.
it is self-contained.
Therefore,all content words have headwords in Lexeed, andall word senses appear in at least one examplesentence.Both newspaper corpora where taken from theMainichi Daily News.
One sample (Senseval2)was the text used for the Japanese dictionarytask in Senseval-2 (Shirai, 2002), which has somewords marked up with word sense tags definedin the Iwanami lexicon (Nishio et al, 1994).The second sample was those sentences used inthe Kyoto Corpus (Kyoto), which is marked upwith dependency analyses (Kurohashi and Nagao,2003).
We chose these corpora so that we cancompare our annotation with existing annotation.Both these corpora were thus already segmentedand annotated with parts-of-speech.
However,they used different morphological analyzers tothe one used in Lexeed, so we had to do someremapping.
E.g.
in Kyoto the copula is not splitfrom nominal-adjectives, whereas in Lexeed it is:?
[9 genkida ?lively?
vs ?
[9 genki da.
Thiscould be done automatically after we had writtena few rules.Although the newspapers contain many wordsother than basic words, only basic words havesense tags.
Also, a word unit in the newspapersdoes not necessarily coincide with the headwordin Lexeed since part-of-speech taggers used forannotation are different.
We do not adjust the wordsegmentation and leave it untagged at this stage,even if it is a part of a basic word or consists ofmultiple basic words.
For instance, Lexeed has thecompound entry |+^?
kahei-kachi ?monetaryvalue?, however, this word is split into two basicwords in the corpora.
In this case, both two words|+ kahei ?money?
and ^?
kachi ?value?
aretagged individually.Corpus TokensContentWordsBasicWords%Mono-semousLXD-DEF 691,072 318,181 318,181 31.7LXD-EX 498,977 221,224 221,224 30.5Senseval2 888,000 692,069 391,010 39.3Kyoto 969,558 526,760 472,419 36.3Table 3: Corpus StatisticsThe corpora are not fully balanced, butallow some interesting comparisons.
There areeffectively three genres: dictionary definitions,which tend to be fragments and are oftensyntactically highly ambiguous; dictionaryexample sentences, which tend to be shortcomplete sentences, and are easy to parse; andnewspaper text from two different years.3 AnnotationEach word was annotated by five annotators.We actually used 15 annotators, divided into 3groups.
None were professional linguists orlexicographers.
All of them had a score above60 on a Chinese character based vocabulary test(Amano and Kondo, 1998).
We used multipleannotators to measure the confidence of tags andthe degree of difficulty in identifying senses.The target words for sense annotation arethe 9,835 headwords having multiple senses inLexeed (?
2.1).
They have 28,300 senses inall.
Monosemous words were not annotated.Annotation was done word by word.
Annotatorsare presented multiple sentences (up to 50) thatcontain the same target word, and they keeptagging that word until occurrences are done.
Thisenables them to compare various contexts wherea target word appears and helps them to keep theannotation consistent.3.1 ToolA screen shot of the annotation tool is given inFigure 2.
The interface uses frames on a browser,with all information stored in SQL tables.
The lefthand frame lists the words being annotated.
Eachword is shown with some context: the surrounding64paragraph, and the headword for definition andexample sentences.
These can be clicked on toget more context.
The word being annotated ishighlighted in red.
For each word, the annotatorchooses its senses or one or more of the other tagsas clickable buttons.
It is also possible to chooseone tag as the default for all entries on the screen.The right hand side frame has the dictionarydefinitions for the word being tagged in the topframe, and a lower frame with instructions.
Asingle word may be annotated with senses frommore than one headword.
For example ??
isdivided into two headwords basu ?bus?
and basu?bass?, both of which are presented.As we used a tab-capable browser, it was easyfor the annotators to call up more information indifferent tabs.
This proved to be quite popular.3.2 MarkupAnnotators choose the most suitable sense in thegiven context from the senses that the word havein lexicon.
Preferably, they select a single sensefor a word, although they can mark up multipletags if the words have multiple meanings or aretruly ambiguous in the contexts.When they cannot choose a sense in somereasons, they choose one or more of the followingspecial tags.o other sense: an appropriate sense is not foundin a lexicon.
Relatively novel concepts (e.g.??
{? doraiba?
?driver?
for ?softwaredriver?)
are given this tag.c multiword expressions (compound / idiom): thetarget word is a part of a non-compositionalcompound or idiom.p proper noun: the word is a proper noun.x homonym: an appropriate entry is not foundin a lexicon, because a target is differentfrom head words in a lexicon (e.g.
only aheadword ??
bass ?bus?
is present in alexicon for ?
?basu ?bass?
).e analysis error: the word segmentation or part-of-speech is incorrect due to errors in pre-annotation of the corpus.3.3 FeedbackOne of the things that the annotators found hardwas not knowing how well they were doing.
Asthey were creating a gold standard, there wasinitially no way of knowing how correct they were.We also did not know at the start of the annotationhow fast senses could or should be annotated (atest of the tool gave us an initial estimate of around400 tokens/day).To answer these questions, and to providefeedback for the annotators, twice a day wecalculated and graphed the speed (in words/day)and majority agreement (how often an annotatoragrees with the majority of annotators for eachtoken, measured over all words annotated sofar).
Each annotator could see a graph withtheir results labelled, and the other annotatorsmade anonymous.
The results are grouped intothree groups of five annotators.
Each groupis annotating a different set of words, but weincluded them all in the feedback.
The orderwithin each group is sorted by agreement, as wewished to emphasise the importance of agreementover speed.
An example of a graph is givenin Figure 3.
When this feedback was given,this particular annotator has the second worstagreement score in their subgroup (90.27%) and isreasonably fast (1799 words/day) ?
they shouldslow down and think more.The annotators welcomed this feedback, andcomplained when our script failed to produce it.There was an enormous variation in speed: thefastest annotator was 4 times as fast as the slowest,with no appreciable difference in agreement.After providing the feedback, the average speedincreased considerably, as the slowest annotatorsagonized less over their decisions.
The finalaverage speed was around 1,500 tokens/day, withthe fastest annotator still almost twice as fast as theslowest.4 Inter-Annotator AgreementWe employ inter-annotator agreement as our coremeasure of annotation consistency, in the sameway we did for treebank evaluation (Tanaka et al,2005).
This agreement is calculated as the averageof pairwise agreement.
Let wi be a word in a set ofcontent words W and wi, j be the jth occurrence ofa word wi.
Average pairwise agreement betweenthe sense tags of wi, j each pair of annotatorsmarked up a(wi, j) is:a(wi, j) =?k (mi, j(sik)C2)nwi, j C2(1)where nwi, j(?
2) is the number of annotators thattag the word wi, j, and mi, j(sik) is the number65Figure 2: Sense Annotation tool (word ?
( shibaraku ?briefly?
)Figure 3: Sample feedback provided to an annotatorof sense tags sik for the word wi, j.
Hence, theagreement of the word wi is the average of awi, jover all occurrences in a corpus:a(wi) =?
j a(wi, j)Nwi(2)66where Nwi is the frequency of the word wi in acorpus.Table 4 shows statistics about the annotationresults.
The average numbers of word senses inthe newspapers are lower than the ones in thedictionary and, therefore, the token agreementof the newspapers is higher than those of thedictionary sentences.
%Unanimous indicates theratio of tokens vs types for which all annotators(normally five) choose the same sense.
Snyderand Palmer (2004) report 62% of all word typeson the English all-words task at SENSEVAL-3were labelled unanimously.
It is hard to directlycompare with our task since their corpus has only2,212 words tagged by two or three annotators.4.1 FamiliarityAs seen in Table 5, the agreement per typedoes not vary much by familiarity.
This wasan unexpected result.
Even though the averagepolysemy is high, there are still many highlyfamiliar words with very good agreement.FamAgreementtoken (type) #WS %Monosem6.5 - .723 (.846) 7.00 22.66.0 - .780 (.846) 5.82 28.05.5 - .813 (.853) 3.79 42.45.0 - .821 (.850) 3.84 46.2ALL .787 (.850) 5.18 34.5Table 5: Inter-Annotator Agreement (LXD-DEF)4.2 Part-of-SpeechTable 6 shows the agreement according to part ofspeech.
Nouns and verbal nouns (vn) have thehighest agreements, similar to the results for theEnglish all-words task at SENSEVAL-3 (Snyderand Palmer, 2004).
In contrast, adjectives have aslow agreement as verbs, although the agreementof adjectives was the highest and that of verbswas the lowest in English.
This partly reflectsdifferences in the part of speech divisions betweenJapanese and English.
Adjectives in Japanese aremuch close in behaviour to verbs (e.g.
they canhead sentences) and includes many words that aretranslated as verbs in English.4.3 EntropyEntropy is directly related to the difficulty inidentifing senses as shown in Table 7.POS Agreement (type) #WS %Monosemousn .803 (.851) 2.86 62.9v .772 (.844) 3.65 34.0vn .849 (.865) 2.54 61.0adj .770 (.810) 3.58 48.3adv .648 (.833) 3.08 46.4others .615 (.789) 3.19 50.8Table 6: POS vs Inter-Annotator Agreement (LXD-DEF)Entropy Agreement (type) #Words #WS2 - .672 84 14.21 - .758 1096 4.380.5 - .809 1627 2.880.05 - .891 495 3.190 - .890 13778 2.56Table 7: Entropy vs Agreement4.4 Sense LumpingLow agreement words have some senses that aredifficult to distinguish from each other: thesesenses often have the same hypernyms.
Forexample, the agreement rate of s kusabana?grass/flower?
in LXD-DEF is only 33.7 %.It has three senses whose semantic class issimilar: kusabana1 ?flower that blooms in grass?,kusabana2 ?grass that has flowers?
and souka1?grass and flowers?
(hypernyms flower1, grass1and flower1 & grass1 respectively).In order to investigate the effect of semanticsimilarity on agreement, we lumped similar wordsenses based on hypernym and semantic class.We use hypernyms from the ontology (?
2.1) andsemantic classes in Goi-Taikei (?
2.3), to regardthe word senses that have the same hypernyms orbelong to the same semantic classes as the samesenses.Table 8 shows the distribution after senselumping.
Table 9 shows the agreement withlumped senses.
Note that this was done with anautomatically derived ontology that has not beenfully hand corrected.As is expected, the overall agreement increased,from 0.787 to 0.829 using the ontology, andto 0.835 using the coarse-grained Goi-Taikeisemantic classes.
For many applications, weexpect that this level of disambiguation is all thatis required.4.5 Special TagsTable 10 shows the ratio of special tags andmultiple tags to all tags.
These results show67CorpusAnnotatedTokens #WSAgreementtoken (type)%Unanimoustoken (type) KappaLXD-DEF 199,268 5.18 .787 (.850) 62.8 (41.1) 0.58LXD-EX 126,966 5.00 .820 (.871) 69.1 (53.2) 0.65Senseval2 223,983 4.07 .832 (.833) 73.9 (45.8) 0.52Kyoto 268,597 3.93 .833 (.828) 71.5 (46.1) 0.50Table 4: Basic Annotation StatisticsCorpus %Other Sense %MWE %Homonym %Proper Noun %Error %Multiple TagsLXD-DEF 4.2 1.5 0.084 0.046 0.92 11.9LXD-EX 2.3 0.44 0.035 0.0018 0.43 11.6Senseval2 9.3 5.6 4.1 8.7 5.7 7.9Kyoto 9.8 7.9 3.3 9.0 5.5 9.3Table 10: Special Tags and Multiple TagsFamAgreementtoken (type) #WS %Monosem6.5 - .772 (.863) 6.37 25.66.0 - .830 (.868) 5.16 31.55.5 - .836 (.872) 3.50 45.65.0 - .863 (.866) 3.76 58.7ALL .829 (.869) 4.72 39.1Lumping together Hypernyms(4,380 senses compressed into 1,900 senses)FamAgreementtoken (type) #WS %Monosem6.5 - .775 (.890) 6.05 26.86.0 - .835 (.891) 4.94 36.45.5 - .855 (.894) 3.29 50.65.0 - .852 (.888) 3.46 49.9ALL .835 (.891) 4.48 41.7Lumping together Semantic Classes(8,691 senses compressed into 4,030 senses)Table 8: Sense Lumping Results (LXD-DEF)(LXD-DEF)Agreementtoken (type) #WS %Monosemno lumping .698 (.816) 8.81 0.0lumping .811 (.910) 8.24 20.0Hypernum Lumping(LXD-DEF)Agreementtoken (type) #WS %Monosemno lumping .751 (.814) 7.09 0.0lumping .840 (.925) 5.99 21.9Semantic Class LumpingTable 9: Lumped Sense Agreement (LXD-DEF)the differences in corpus characteristics betweendictionary and newspaper.
The higher ratios ofOther Sense and Homonym at newspapers indicatethat the words whose surface form is in adictionary are frequently used for the differentmeanings in real text, e.g.
?
gin ?silver?
isused for the abbrebiation of ? ginkou ?bank?.%Multiple Tags is the percentage of tokens forwhich at least one annotator marked multiple tags.5 Discussion5.1 Comparison with Senseval-2 corpusThe Senseval-2 Japanese dictionary taskannotation used senses from a different dictionary(Shirai, 2002).
In the evaluation, 100 test wordswere selected from three groups with differententropy bands (Kurohashi and Shirai, 2001).
Dais the highest entropy group, which contains themost hard to tag words, and Dc is the lowestentropy group.We compare our results with theirs in Table 11.The Senseval-2 agreement figures are slightlyhigher than our overall.
However, it is impossibleto make a direct comparison as the numbers ofannotators (two or three annotators in Senseval vsmore than 5 annotators in our work) and the senseinventories are different.5.2 ProblemsTwo main problems came up when buildingthe corpora: word segmentation and sensesegmentation.
Multiword expressions likecompounds and idioms are tied closely to bothproblems.The word segmentation is the problem of howto determine an unit expressing a meaning.
Atthe present stage, it is based on headword inLexeed, in particular, only compounds in Lexeedare recognized, we do not discriminate non-decomposable compounds with decomposableones.
However, if the headword unit in thedictionary is inconsistent, word sense tagginginherits this problem.
For examples, ? ichibuhas two main usage: one + classifier and a partof something.
Lexeed has an entry including bothtwo senses.
However, the former is split into two68POS Da Db Dc TotalHinoki Senseval Hinoki Senseval Hinoki Senseval Hinoki Sensevalnoun .768 .809 .784 .786 .848 .957 .806 .85914.4 13.1 5.0 4.1 3.1 3.8 5.9 5.1verb .660 .699 .722 .896 .738 .867 .723 .86716.7 21.8 10.3 9.3 5.2 5.9 9.6 10.9total .710 .754 .760 .841 .831 .939 .768 .86315.6 18.8 7.0 6.2 4.2 4.9 7.6 7.9Table 11: Comparison of Agreement for the Senseval-2 Lexical Sample Task Corpus ( upper row:agreement, lower row: the number of word senses)words by our morphological analyser in the sameway as other numeral + classifier.The second problem is how to mark offmetaphorical meaning from literal meanings.Currently, this also depends on the Lexeeddefinition and it is not necessarily consistenteither.
Some words in institutional idioms (Saget al, 2002) have the idiom sense in the lexiconwhile most words do not.
For instance, ?shippo ?tail of animal?)
has a sense for the reading?weak point?
in an idiom ?kY shippo-otsukamu ?lit.
to grasp the tail, idiom.
to find one?sweak point?, while  ase ?sweat?
does not havea sense for the applicable meaning in the idiom k?2 ase-o nagasu ?lit.
to sweat, idiom, to workhard?.6 ConclusionsWe built a corpus of over three million wordswhich has lexical semantic information.
We arecurrently using it to build a model for word sensedisambiguation.AcknowledgementWe thank the 15 sense annotators and 3 treebankers(Takayuki Kuribayashi, Tomoko Hirata and Koji Yamashita).ReferencesAnne Abeille?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer Academic Publishers.Shigeaki Amano and Tadahisa Kondo.
1998.
Estimationof mental lexicon size with word familiarity database.In International Conference on Spoken LanguageProcessing, volume 5, pages 2119?2122.Shigeaki Amano and Tadahisa Kondo.
1999.
Nihongo-noGoi-Tokusei (Lexical properties of Japanese).
Sanseido.Francis Bond, Sanae Fujita, Chikara Hashimoto, KanameKasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,Takaaki Tanaka, and Shigeaki Amano.
2004.
TheHinoki treebank: A treebank for text understanding.
InProceedings of the First International Joint Conference onNatural Language Processing (IJCNLP-04), pages 554?559.
Hainan Island.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentaro Ogura, YoshifumiOoyama, and Yoshihiko Hayashi.
1997.
Goi-Taikei?
A Japanese Lexicon.
Iwanami Shoten, Tokyo.
5volumes/CDROM.Kaname Kasahara, Hiroshi Sato, Francis Bond, TakaakiTanaka, Sanae Fujita, Tomoko Kanasugi, and ShigeakiAmano.
2004.
Construction of a Japanese semanticlexicon: Lexeed.
SIG NLC-159, IPSJ, Tokyo.
(inJapanese).Adam Kilgariff and Joseph Rosenzweig.
2000.
Frameworkand results for English SENSEVAL.
Computers andthe Humanities, 34(1?2):15?48.
Special Issue onSENSEVAL.Sadao Kurohashi and Makoto Nagao.
2003.
Building aJapanese parsed corpus ?
while improving the parsingsystem.
In Abeille?
(2003), chapter 14, pages 249?260.Sadao Kurohashi and Kiyoaki Shirai.
2001.
SENSEVAL-2Japanese task.
SIG NLC 2001-10, IEICE.
(in Japanese).Eric Nichols and Francis Bond.
2005.
Acquiring ontologiesusing deep and shallow processing.
In 11th AnnualMeeting of the Association for Natural LanguageProcessing, pages 494?498.
Takamatsu.Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.1994.
Iwanami Kokugo Jiten Dai Go Han [IwanamiJapanese Dictionary Edition 5].
Iwanami Shoten, Tokyo.
(in Japanese).Stephan Oepen, Kristina Toutanova, Stuart Shieber,Christoper D. Manning, Dan Flickinger, and ThorstenBrant.
2002.
The LinGO redwoods treebank: Motivationand preliminary applications.
In 19th InternationalConference on Computational Linguistics: COLING-2002, pages 1253?7.
Taipei, Taiwan.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake,and Dan Flickinger.
2002.
Multiword expressions: Apain in the neck for NLP.
In Alexander Gelbuk,editor, Computational Linguistics and Intelligent TextProcessing: Third International Conference: CICLing-2002, pages 1?15.
Springer-Verlag, Hiedelberg/Berlin.Kiyoaki Shirai.
2002.
Construction of a word sense taggedcorpus for SENSEVAL-2 Japanese dictionary task.
InThird International Conference on Language Resourcesand Evaluation (LREC-2002), pages 605?608.Benjamin Snyder and Martha Palmer.
2004.
The English all-words task.
In Proceedings of Senseval-3, pages 41?44.ACL, Barcelona.Takaaki Tanaka, Francis Bond, Stephan Oepen, and SanaeFujita.
2005.
High precision treebanking ?
blazing usefultrees using POS information.
In ACL-2005, pages 330?337.Ann Taylor, Mitchel Marcus, and Beatrice Santorini.
2003.The Penn treebank: an overview.
In Abeille?
(2003),chapter 1, pages 5?22.69
