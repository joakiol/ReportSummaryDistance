A System for Searching and Browsing Spoken CommunicationsLee BegejaBernard RengerMurat SaraclarAT&T Labs ?
Research180 Park AveFlorham Park, NJ 07932{lee, renger, murat}@research.att.comDavid GibbonZhu LiuBehzad ShahrarayAT&T Labs ?
Research200 Laurel Ave SMiddletown, NJ 07748{dcg, zliu, behzad}@research.att.comAbstractAs the amount of spoken communications ac-cessible by computers increases, searching andbrowsing is becoming crucial for utilizing suchmaterial for gathering information.
It is desir-able for multimedia content analysis systemsto handle various formats of data and to servevarying user needs while presenting a simpleand consistent user interface.
In this paper,we present a research system for searching andbrowsing spoken communications.
The systemuses core technologies such as speaker segmen-tation, automatic speech recognition, transcrip-tion alignment, keyword extraction and speechindexing and retrieval to make spoken commu-nications easy to navigate.
The main focus ison telephone conversations and teleconferenceswith comparisons to broadcast news.1 IntroductionArchiving and organizing multimedia communicationsfor easy user access is becoming more important as suchinformation sources are becoming available in amountsthat can easily overwhelm a user.
As storage and ac-cess become cheaper, the types of multimedia communi-cations are also becoming more diverse.
Therefore, it isnecessary for multimedia content analysis and navigationsystems to handle various forms of data.In this paper we present SpeechLogger, a research sys-tem for searching and browsing spoken communications,or the spoken component of multimedia communications.In general, the information contained in a spoken com-munication consists of more than just words.
Our goal isto make use of all the information within a spoken com-munication.
Our system uses automatic speech recogni-tion (ASR) to convert speech into a format which makesword and phonetic searching of the material possible.
Italso uses speaker segmentation to aid navigation.We are interested in a wide range of spoken communi-cations with different characteristics, including broadcastmaterial, lectures, meetings, interviews, telephone con-versations, call center recordings, and teleconferences.Each of these communication types presents interestingopportunities, requirements and challenges.
For example,lectures might have accompanying material that can aidASR and navigation.
Prior knowledge about the speakersand the topic may be available for meetings.
Call centerrecordings may be analyzed to create aggregate reports.Spoken document retrieval (SDR) for Broadcast Newstype of content has been well studied and there are manyresearch and commercial systems.
There has also beensome interest in the Voicemail domain (Hirschberg et al,2001) which consists of typically short duration human-to-machine messages.
Our focus here is on telephoneconversations and teleconferences with comparisons tobroadcast news.The paper is organized as follows.
In Section 2, wemotivate our approach by describing the user needs un-der various conditions.
Then we describe our system inSection 3, giving the details of various components.
Ex-perimental results for some components are given in Sec-tion 4.
Finally, in Section 5 we present a summary.2 User NeedsWe are primarily interested in situations in which a per-son needs to gather information from audio data but thequality of that data is not always sufficient to producegood ASR results.
In the case of telephone conversations,the information gatherer needs to know who was on thecall, how long the call was, what was said, a summary ofthe call, the ability to listen to any part of the call basedon search parameters that s/he specifies, etc.
Our userswant to be able to scan a database of many calls, across along period of time to look for specific phrases, speakers,or patterns of speech.In many cases, it is difficult to gather this type of infor-mation from teleconference calls since the audio qualityis poor because of speaker phones, cell phones and linenoise.
All of these combine to lower ASR results to apoint where the text of the call is not fully representativeof the conversation.
Thus, using standard information re-trieval techniques may not provide sufficient informationto the user.
We focus on the navigation aspect of infor-mation gathering with the goal of compensating for lowerASR accuracy by presenting user interface elements rel-evant to the specific task at hand (Stark et al, 2000).Rather than looking at the recorded conversation asmerely audio information, we view it as a source of lin-guistic information to which we can apply informationretrieval and data mining techniques.
We use all avail-able metadata to enhance the search and the presentation.We wanted to have a set of interface elements thatwould be useful no matter what the ASR accuracy was.The main interface elements are:?
Timeline with tick marks indicates search hits withinthe spoken document which allows for many searchresults to be displayed without overwhelming theuser.
This is particularly useful for cases where thereare many false positives.?
Keyword extraction summarizes a given communi-cation, enables differentiation among a collection ofmany spoken documents, and detects subtopics in alarge spoken document.?
Speaker segmentation and speaker identificationseparate a long spoken document into inherentlyuseful pieces.?
Lattice search and phoneme search expand the pos-sible search space.In this paper we examine three classes of spoken doc-uments and consider what tasks a user might want to per-form on them.?
Broadcast News - excellent ASR conditions, onespeaker at a time, good audio quality and gener-ally a good speaker.
Task involves primarily inter-document navigation.
User needs to search text forinformation with metadata possibly used to enhancethe search.?
Telephone Conversations - fair ASR conditions, twospeakers, decent quality audio.
User needs to searchtext but also wants speaker identification and someclassification (call type, urgency, importance).?
Teleconferences - poor ASR conditions, multiplespeakers, mixed to poor audio quality.
Most timeis spent in intra-document navigation.
User needs tonavigate through the calls and find relevant informa-tion in the audio.3 System DescriptionThe system overview is shown in Figure 1.
Our sys-tem is flexible enough to support various forms of live(via a VoiceXML Gateway) or prerecorded spoken com-munications including the three classes of spoken docu-ments discussed above.
It can record the audio via tele-phone for two-party or multi-party calls.
Alternatively,the system can support prerecorded audio input from var-ious sources including telephone conversations or videocontent in which case the audio is extracted from thevideo.
Once various speech processing techniques are ap-plied and the speech is indexed, it is possible to searchand browse the audio content.
Our system is scalableand supports open source/industry standard components(J2EE, VXML, XML, Microsoft SAMI, Microsoft MediaPlayer).
It is also flexible enough to support other formsof audio as input or to support new speech processingtechniques as they become available.
The system was de-signed with modularity in mind.
For instance, it shouldbe possible to add a speaker identification module to theprocessing.Figure 1: System OverviewOnce a new audio recording is available on theFile Server, the following processing steps can begin:speaker segmentation, speech recognition, transcriptionalignment, keyword extraction, audio compression, andspeech indexing.
Each step will be described in more de-tail below.
We attempt to distinguish the different speak-ers from each other in the speaker segmentation compo-nent.
The speech recognition component converts the au-dio into a word or phone based representation includingalternative hypotheses in the form of a lattice.
If a tran-script is available, the transcript can be synchronized (oraligned) in time with the speech recognition output.
Thekeyword extraction component generates the most salientwords found in the speech recognition output (one-bestword) or transcript (if available) and can be used to de-termine the nature of the spoken communications.
Theaudio compression component compresses the audio fileand creates an MP3 audio file which is copied to the Me-dia Server.
The final step in the processing is text andlattice indexing.
This includes creating indices based onone-best word and one-best phone strings or word andphone lattices.After processing, the user can search and browse theaudio using either the text index or the lattice index.
Theaudio is played back via media streaming.
Alternatively,the user can playback the audio file over the phone usingthe VoiceGenie VoiceXML Gateway.3.1 Speaker SegmentationSpeaker-based segmentation of multi-speaker audio datahas received considerable attention in recent years.
Ap-plications that have been considered include: indexingarchived recorded spoken documents by speaker to facil-itate browsing and retrieval of desired portions; taggingspeaker specific portions of data to be used for adapt-ing speech models in order to improve the quality ofautomatic speech recognition transcriptions, and track-ing speaker specific segments in audio streams to aid insurveillance applications.
In our system, speaker segmen-tation is used for more effective visualization of the audiodocument and speaker-based audio playback.Figure 2 gives an overview of the speaker segmenta-tion algorithm we developed.
It consists of two steps:preprocessing and iterative speaker segmentation.
Dur-ing the preprocessing step, the input audio stream is seg-mented into frames and acoustic features are computedfor each frame.
The features we extracted are energy, 12Mel-frequency cepstral coefficients (MFCC), pitch, andthe first and second order temporal derivatives.
Then,all speaker boundary candidates are located, which in-clude silent frames and frames with minimum energy ina window of neighboring frames.
The preprocessing stepgenerates a set of over-segmented audio segments, whosedurations may be as short as a fraction of a second to aslong as a couple of seconds.The iterative speaker segmentation step, as depicted inthe bigger dotted rectangle in Figure 2, detects all seg-ments of each speaker in an iterative way and then marksthe boundaries where speakers change.
At the beginning,all segments produced by the preprocessing step are un-labeled.
Assuming that the features within each segmentfollow a Gaussian distribution, we compute the distancesbetween each pair of segments using the Kullback Leiblerdistance (KLD) (Cover and Thomas, 1991).
Here, we justconsider features extracted from voiced frames since onlyvoiced frames have pitch information.
Based on the seg-ment distance matrix, a hierarchical agglomerative clus-tering (HAC) (Jain and Dubes, 1988) algorithm is appliedFigure 2: Overview of the Speaker Segmentation Algo-rithm.to all unlabeled segments.
The biggest cluster will be hy-pothesized as the set of segments for a new speaker andthe rest of the segments will be considered as backgroundaudio.
Accordingly, each unlabeled segment is labeled aseither the target speaker or background.
Then an embed-ded speaker segment refinement substep is activated toiteratively refine the segments of the target speaker.The refinement substep is depicted in the smaller dot-ted rectangle in Figure 2.
For each iteration, two Gaus-sian mixture models (GMM) are built based on currentsegment labels, one for the target speaker, one for back-ground audio.
Then all segments are relabeled as eitherthe target speaker or background audio using the maxi-mum likelihood method based on the two GMM models.If the set of segments for the target speaker convergesor the refinement iteration number reaches its maximum,the refinement iteration stops.
Otherwise, a new itera-tion starts.
Before the refinement substep terminates, itassigns a new speaker label for all segments of the tar-get speaker, and sets the background audio as unlabeled.Then the iterative speaker segmentation step needs to testfor more speakers or needs to stop.
The termination cri-teria could be the given number of speakers (or majorspeakers) in an audio document, the percentage of unla-beled segments to the number of all segments, or the max-imum distance among all pairs of unlabeled segments.
Ifany of the criteria are met, the speaker segmentation algo-rithm merges all adjacent segments if their speaker labelsare the same, and then outputs a list of audio segmentswith corresponding speaker labels.Obviously, one advantage of our speaker segmentationmethod is that the speaker labels are also extracted.
Al-though the real speaker identities are not available, theFigure 3: Presentation of Speaker Segmentation Results.labels are very useful for presenting, indexing, and re-trieving audio documents.
For more detailed descriptionof the speaker segmentation algorithm, please refer toRosenberg et al (2002).Figure 3 illustrates a graphic interface for presentingthe speaker segmentation results.
The audio stream isshown in colored blocks along a timeline which goesfrom top to bottom, and from left to right.
Color is used todifferentiate the speaker labels.
There are two layers foreach line: the bottom layer shows the manually labeledspeaker segments and the top layer displays the automat-ically generated segments.
This allows the segmentationperformance to be clearly observed.3.2 Automatic Speech RecognitionWe use two different state-of-the-art HMM based largevocabulary continuous speech recognition (LVCSR) sys-tems for telephone and microphone recordings.
In bothcases the front-end uses 9 frames of 12 MFCC compo-nents and energy summarized into a feature vector vialinear discriminant analysis.
The acoustic models consistof decision tree state clustered triphones and the outputdistributions are mixtures of Gaussians.
The models arediscriminatively trained using maximum mutual informa-tion estimation.
The language models are pruned backofftrigram models.For narrow-band telephone recordings we use the firstpass of the Switchboard evaluation system developedby Ljolje et al (2002).
The calls are automatically seg-mented prior to ASR.
The acoustic models are trained on265 hours of speech.
The recognition vocabulary of thesystem has 45K words.For wide-band recordings, we use the real-timeBroadcast News transcription system developed bySaraclar et al (2002).
The acoustic models are trained on140 hours of speech.
The language models are estimatedon a mixture of newspaper text, closed captions and high-accuracy transcriptions from LDC.
Since the system wasdesigned for SDR, the recognition vocabulary of the sys-tem has over 200K words.Both systems use the same Finite State Machine (FSM)based LVCSR decoder (Allauzen et al, 2003).
The out-put of the ASR system is represented as a FSM and maybe in the form of a one-best hypothesis string or a latticeof alternate hypotheses.
The lattices are normalized sothat the probability of the set of all paths leading fromany state to the final state is 1.
The labels on the arcs ofthe FSM may be words or phones and the conversion be-tween the two can easily be done using FSM compositionusing the AT&T FSM Library (Mohri et al, 1997).
Thecosts on the arcs of the FSM are negative log likelihoods.Additionally, timing information can also be present inthe output.3.3 Alignment with TranscriptsManual transcriptions of spoken communications areavailable for certain application domains such as medicaldiagnosis, legal depositions, television and radio broad-casts.
Most audio and video teleconferencing providersoffer transcription as an optional service.
In thesecases, we can take advantage of this additional informa-tion to create high quality multimedia representations ofthe archived spoken communications using parallel textalignment techniques (Gibbon, 1998).
The obvious ad-vantage is increased retrieval accuracy due to the lowerword error rate (manual transcriptions are seldom com-pletely error free.)
What is more compelling, however, isthat we can construct much more evolved user interfacesfor browsing speech by leveraging the fact that the tran-scription is by its nature readable whereas the one-besthypothesis from ASR is typically useful only in smallsegments to establish context for a search term occur-rence.There are several methods for aligning text withspeech.
We use dynamic programming techniques tomaximize the number or word correspondences betweenthe manual transcription and the one-best ASR word hy-pothesis.
For most applications, finding the start and endtimes of the transcript sentences is sufficient; but we doalignment at the word level and then derive the sentencealignment from that.
In cases where the first or last wordof a sentence is not recognized, we expand to the near-est recognized word to avoid cropping even though wemay include small segments from neighboring sentencesduring playback.
The accuracy of the resulting align-ment is directly related to the ASR word error rate; moreprecisely it can be thought of as a sentence error ratewhere we impose a minimum percentage of correspond-ing words per sentence (typically 20%) before declar-ing a sentence a match to avoid noise words triggeringfalse matches.
For sentences without correspondences,we must fall back to deriving the timings from the near-est neighboring sentences with correspondences.Figure 4: Illustration of Keyword Extraction.3.4 Keyword ExtractionPlaying back a spoken document or linearly skimmingthe corresponding text transcript, either from automaticspeech recognition or manual transcription, is not an ef-ficient way for a user to grasp the central topics of thedocument within a short period of time.
A list of repre-sentative keywords, which serve as a dense summary for adocument, can effectively convey the essence of the docu-ment to the user.
The keywords have been widely used forindexing and retrieval of documents in large databases.
Inour system, we extract a list of keywords for each audiodocument based on its transcript (ASR or manual tran-script).There are different ways to automatically extract key-words for a text document within a corpus.
A popularapproach is to select keywords that frequently occur inone document but do not frequently occur in other doc-uments based on the term frequency - inverse documentfrequency (TF-IDF) feature.
Our task is slightly differ-ent.
We are interested in choosing keywords for a sin-gle document, independent of the remaining documentsin the database.
Accordingly, we adopt a different fea-ture, which is term frequency - inverse term probability(TF-ITP) to serve our purpose.
The term probability mea-sures the probability that a term may appear in a generaldocument and it is a language dependent characteristic.Assuming that a term Tk occurs tfk times in a docu-ment, and its term probability is tpk, the TF-ITP of Tk isdefined as wk = tfk/tpk.Figure 4 illustrates the keyword extraction method thatwe have developed.
For the transcript of a given doc-ument, we first apply the Porter stemming algorithm(Porter, 1980) to remove word variations.
Then, the stopwords, which are common words that have no impact onthe document content (also called noise words), are re-moved.
Here we use two lists of noise words, one for gen-eral purposes, which apply to all varieties of documents,and one for specific domains, which can be customizedby the user when prior knowledge about the document isavailable.
For each remaining term in the document, avalue of TF-ITP is calculated.
A vocabulary is createdbased on the transcripts of 600 hours of broadcast newsdata and corresponding term probabilities are estimatedusing the same corpus.
If a term in the document is notin the vocabulary, and its term frequency is more than2, then a default term probability value tpd will be used.The tpd we use is the minimum term probability in thevocabulary.
After we get a list of terms and their TF-ITPvalues, we sort the terms based on their TF-ITP values,such that the most representative terms (highest TF-ITPvalues) are on the top of the list.
Depending on certaincriteria, for example, the number of keywords desired orthe minimum TF-ITP value required, a list of keywordscan be chosen from the top of the term list.
In our sys-tem, we choose the top ten terms as the keywords for adocument.3.5 Speech Indexing and RetrievalTwo different indexing and retrieval modules are uti-lized depending on the type of ASR output.
Inthe case of one-best word or phone strings, we usean off-the-shelf text-based index server called Lucene(http://jakarta.apache.org/lucene).
In the case of wordand phone lattices, we use the method described inSaraclar and Sproat (2004).
Here we give a brief descrip-tion of the latter.The lattice output is a compact representation of likelyalternative hypotheses of an ASR system.
Each path inthe lattice corresponds to a word (or phone) string andhas a probability attached to it.
The expected count fora substring can be defined as the sum of the probabilitiesof all paths which contain that substring.
Lattice basedretrieval makes the system more robust to recognition er-rors, whereas phonetic search allows for retrieving wordsthat are not in the vocabulary of the recognizer.The lattice index is similar to a standard inverted indexbut contains enough information to compute the expectedcount of an arbitrary substring for each lattice.
This canbe achieved by storing a set of index files, one for eachlabel (word or phone) l. For each arc labeled with l in alattice, the index file for l records the lattice number, theprevious and next states of the arc, along with the prob-ability mass leading to the previous state of the arc andthe probability of the arc itself.
For a lattice, which isnormalized so that the probability of the set of all pathsleading from any state to the final state is 1, the poste-rior probability of an arc is given by the multiplication ofthe probability mass leading to the previous state and theprobability of the arc itself.
The expected count of a labelgiven a lattice is equal to the sum of the posterior proba-bilities of all arcs in the index for that label with the samelattice number.To search for a multi-label expression (e.g., a multi-word phrase) w1w2 .
.
.
wn we seek on each label in theexpression and then for each (wi, wi+1) join the nextstates of wi with the matching previous states of wi+1.In this way, we retrieve just those path segments in eachlattice that match the entire multi-label expression.
Theprobability of each match is defined as the multiplicationof the probability mass leading to the previous state ofthe first arc and the probabilities of all the arcs in the pathsegment.
The expected count of a multi-label expressionfor the lattice is computed as above.The answer to a query contains an audio segment onlyif the expected count of the query for the lattice corre-sponding to that audio segment is higher than a threshold.3.6 User InterfaceThe user interface description will apply for the threetypes of spoken communications (Telephone Conversa-tions, Teleconferences, Broadcast News) although the au-dio and speaker quality do vary for each of these typesof spoken communications.
Once the user has found thedesired call (or spoken communication) using one of theretrieval modules (one-best word, one-best phone string,word lattice, phone lattice, or both word and phone lat-tice), the user can navigate the call using the user inter-face elements described below.For the one-best word index, the Web page in Fig-ure 5 shows the user interface for searching, browsing,and playing back this call.
The user can browse the callat any time by clicking on the timeline to start playing atthat location on the timeline.
The compressed audio file(MP3) that was created during the processing would bestreamed to the user.
The user can at any time either entera word (or word phrase) in the Search box or use one ofthe common keywords generated during the keyword ex-traction process.
The text index would be queried and theresults of the search would be shown.
The timeline plotat the top would show all the hits or occurrences of theword as thin tick marks.
The list of hits would be foundunder the keyword list.
In this case, the word ?chap-ter?
was found 4 times and the time stamps are shown.The time stamps come from the results of the automaticspeech recognition process when the one-best words andtime stamps were generated.
The search term ?chapter?is shown in bold with 5 context words on either side.
Theuser can click on any of these 4 hits to start playing wherethe hit occurred.
The solid band in the timeline indicatesthe current position of the audio being played back.
Theentire call, in this case, is 9:59 minutes long and the au-dio is playing at the beginning of the fourth hit at 5:20minutes.
As part of the processing, caption data is gener-ated in Microsoft?s SAMI (Synchronized Accessible Me-dia Interchange) format from the one-best word output inorder to show caption text during the playback.
The cap-tion text under the timeline will be updated as the audiois played.
At this point in the call, the caption text is ?buti did any chapter in a?.
This caption option can be dis-Figure 5: User Interface for ASR One-Best Word Search.Figure 6: User Interface for Lattice Search.abled by clicking on the CC icon and can be enabled byclicking on the CC icon again.
The user can also speedup or slow down the playback at any time by using the?Speed?
button.
The speed will toggle from 50% (slow)to 100% to 150% (fast) to 200% (faster) and then startover at 50%.
The speed, which is currently ?fast?, will beshown next to the current time above the ?Stop?
button.This allows the user to more quickly peruse the audio file.A similar Web page in Figure 6 shows the user inter-face for searching a lattice index.
Note that for the sameaudio file (or call) and the same search term ?chapter?,the results of the query show 6 hits compared to the 4hits in the text index in Figure 5.
In this particular case,the manual transcript does indeed contain these 6 occur-rences of the word ?chapter?.
The search terms werefound in audio segments, which is why the time of thehit is a time range.
The information in brackets is the ex-pected count and can exceed 1.0 if the search term occursmore than once in the audio segment.
The time range isreflected in the timeline since the thin tick marks havebeen replaced with colored segments.
The colors of thesegments correspond to the colors of the hits in the list.The darker the color, the higher the count and the lighterthe color, the lower the count.
Finally, the search can berefined by altering the threshold using the ?Better Hits?and ?More Hits?
buttons.
In this example, the thresholdis set to 0.2 as can be seen under the timeline.
If theuser clicks on the ?Better Hits?
button, the threshold isincreased so that only better hits are shown.
If the ?MoreHits?
button is used, the threshold is decreased so morehits are shown although the hits may not be as good.
Thelattice index only returns hits where each hit has a countabove the threshold.The lattice search user interface allows the user to moreeasily find what the user wants and has additional controls(threshold adjustments) and visual feedback (colored seg-ments/hits) that are not possible for the text search userinterface.4 Experimental ResultsWe used three different corpora to assess the effectivenessof different techniques.The first corpus is the DARPA Broadcast News cor-pus consisting of excerpts from TV or radio programsincluding various acoustic conditions.
The test set isthe 1998 Hub-4 Broadcast News (hub4e98) evaluationtest set (available from LDC, Catalog no.
LDC2000S86)which is 3 hours long and was manually segmented into940 segments.
It contains 32411 word tokens and 4885word types.The second corpus is the Switchboard corpus consist-ing of two-party telephone conversations.
The test set isthe RT02 evaluation test set which is 5 hours long, has120 conversation sides and was manually segmented into6266 segments.
It contains 65255 word tokens and 3788word types.The third corpus is named Teleconference since it con-sists of multi-party teleconferences on various topics.
Atest set of six teleconferences (about 3.5 hours) was tran-scribed.
It contains 31106 word tokens and 2779 wordtypes.
Calls are automatically segmented into a total of1157 segments prior to ASR.4.1 Speaker SegmentationThe performance of the speaker segmentation is evalu-ated as follows.
For an audio document, assume thereare N true boundaries, and the algorithm generates Mspeaker boundaries.
If a detected boundary is within1 second of a true boundary, it is a correctly detectedboundary, otherwise it is a falsely detected boundary.
LetC denote the number of correctly detected boundaries,the recall and precision of the boundary detection can becomputed as R = C/N and P = C/M , respectively.We can combine these two values using the F-measureF = 2 ?
P ?
R/(P + R) to measure the speaker seg-mentation performance.We evaluated the developed method on three differenttypes of audio documents: Broadcast News recordings(16KHz sampling rate, 16 bits/sample), two-party tele-phone conversations (8KHz, 16bps), and multi-party tele-conference recordings (8KHz, 16bps).
Due to the highaudio quality and well controlled structure of the broad-cast news program, the achieved F-measure for broadcastnews data is 91%.
Teleconference data has the worst au-dio quality given the various devices (headset, speaker-phone, etc.)
used and different channels (wired and wire-less) involved.
There are also a lot of spontaneous speechsegments less than 1 second long, for example, ?Yes?,?No?, ?Uh?, etc.
These characteristics make the telecon-ference data the most challenging one to segment.
TheF-measure we achieved for this type of data is 70%.
TheF-measure for two-party telephone conversations is in themiddle at 82%.4.2 Automatic Speech RecognitionFor evaluating ASR performance, we use the standardword error rate (WER) as our metric.
Since we are in-terested in retrieval, we use OOV (Out Of Vocabulary)rate by type to measure the OOV word characteristics.In Table 1, we present the ASR performance on thesethree tasks as well as the OOV Rate by type of the cor-pora.
It is important to note that the recognition vocabu-lary for the Switchboard and Teleconference tasks are thesame and no data from the Teleconference task was usedwhile building the ASR systems.Task WER OOV Rate by TypeBroadcast News ?20% 0.6%Switchboard ?40% 6%Teleconference ?50% 12%Table 1: Word Error Rate and OOV Rate Comparison.4.3 RetrievalOur task is to retrieve the audio segments in which theuser query appears.
For evaluating retrieval performance,we use precision and recall with respect to manual tran-scriptions.
Let C(q) be the number of times the queryq is found correctly, M(q) be the number of answersto the query q, and N(q) be the number of times q isfound in the reference.
We compute precision and re-call rates for each query as P (q) = C(q)/M(q) andR(q) = C(q)/N(q).
We report the average of thesequantities over a set of queries Q, P =?q?Q P (q)/|Q|and R =?q?Q R(q)/|Q|.
The set of queries Q includesall the words seen in the reference except for a stop list ofthe 100 most common words.For lattice based retrieval methods, different operatingpoints can be obtained by changing the threshold.
Theprecision and recall at these operating points can be plot-ted as a curve.
In addition to individual precision-recallvalues we also compute the F-measure defined above andreport the maximum F-measure (maxF) to summarize theinformation in a precision-recall curve.In Table 2, a comparison of the maximum F-measure(maxF) is given for various corpora.
Using word latticesyields a relative gain of 3-5% in maxF over using one-best word hypotheses.
Using both word and phone lat-tices, the relative gain over the baseline increases to 8-12%.
In this approach, we first search the word index;if no matches are found then we search the phone index.This allows the system to return matches even if the userquery is not in the ASR vocabulary.Task System1-best W Lats W+P LatsBroadcast News 84.0 84.8 86.0Switchboard 57.1 58.4 60.5Teleconference 47.4 50.3 52.8Table 2: Maximum F-measure Comparison.In Figure 7, we present the precision-recall curves.The gain from using better techniques utilizing wordand phone lattices increases as retrieval performance getsworse.0 20 40 60 80 100020406080100PrecisionRecallTeleconferencesSwitchboardBroadcast News1?best Word HypothesesWord LatticesWord and Phone LatticesFigure 7: Precision vs Recall Comparison.5 SummaryWe presented a system for searching and browsing spo-ken communications.
The system is flexible enough tosupport various forms of spoken communications.
In thispaper, our focus was on telephone conversations and tele-conferences.
We also presented experimental results forthe speaker segmentation, ASR and retrieval componentsof the system.AcknowledgmentsWe would like to thank Richard Sproat for useful dis-cussions and for making his lattice indexing software(lctools) available for our system.ReferencesC.
Allauzen, M. Mohri, and M. Riley.2003.
DCD Library ?
Decoder Library.http://www.research.att.com/sw/tools/dcd.T.
M. Cover and J.
A. Thomas.
1991.
Elements of Infor-mation Theory.
John Wiley & Sons.D.
Gibbon.
1998.
Generating hypermedia documentsfrom transcriptions of television programs using paral-lel text alignment.
In B. Furht, editor, Handbook of In-ternet and Multimedia Systems and Applications.
CR-CPress.J.
Hirschberg, M. Bacchiani, D. Hindle, P. Isenhour,A.
Rosenberg, L. Stark, L. Stead, S. Whittaker, andG.
Zamchick.
2001.
Scanmail: Browsing and search-ing speech data by content.
In Proceedings of theEuropean Conference on Speech Communication andTechnology (Eurospeech), Aalborg, Denmark.A.
K. Jain and R. C. Dubes.
1988.
Algorithms for Clus-tering Data.
Prentice-Hall.A.
Ljolje, M. Saraclar, M. Bacchiani, M. Collins, andB.
Roark.
2002.
The AT&T RT-02 STT system.
InProc.
RT02 Workshop, Vienna, Virginia.M.
Mohri, F. C. N. Pereira, and M. Riley.
1997.AT&T FSM Library ?
Finite-State Machine Library.http://www.research.att.com/sw/tools/fsm.M.
F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.A.
Rosenberg, A. Gorin, Z. Liu, and S. Parthasarathy.2002.
Unsupervised speaker segmentation of tele-phone conversations.
In Proceedings of the Inter-national Conference on Spoken Language Processing(ICSLP), Denver, Colorado, USA.M.
Saraclar and R. Sproat.
2004.
Lattice-based searchfor spoken utterance retrieval.
In Proc.
HLT-NAACL.M.
Saraclar, M. Riley, E. Bocchieri, and V. Goffin.
2002.Towards automatic closed captioning: Low latencyreal time broadcast news transcription.
In Proceedingsof the International Conference on Spoken LanguageProcessing (ICSLP), Denver, Colorado, USA.L.
Stark, S. Whittaker, and J. Hirschberg.
2000.
ASRsatisficing: the effects of ASR accuracy on speech re-trieval.
In Proceedings of the International Conferenceon Spoken Language Processing (ICSLP).
