Web Search Queries as a CorpusTutorial at the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)Marius Pa?caGoogle Inc.mars@google.comACL 2011                                                        June 2011Portland, Oregon _           _Overview?
Part One: Introduction?
Part Two: Queries as a Corpus?
Part Three: Extraction from QueriesPart One: Introduction?
Open-domain information extraction?
Instances, concepts, relationsUnweaving the World Wide Web of Facts?
The Web is a repository of implicitly-encoded human knowledge?
some text fragments contain easier-to-extract knowledge?
More knowledge leads to better answers?
acquire facts from a fraction of the knowledge on the Web?
exploit available facts during search?
Open-domain information extraction?
extract knowledge (facts, relations) applicable to a wide range,rather than closed, pre-defined set of domains (e.g., medical,financial etc.)?
no need to specify set of concepts and relations of interest inadvance?
rely on as little manually-created input data as possibleInstances, Concepts and Relations?
A concept (class) is a placeholder for a set of instances(objects) that share similar properties?
set of instances?
{matrix, kill bill, ice age, pulp fiction, inception, cidade de deus,...}?
class label?
movies, films?
definition?
a series of pictures projected on a screen in rapid succession withobjects shown in successive positions slightly changed so as to producethe optical effect of a continuous picture in which the objects move(Merriam Webster)?
a form of entertainment that enacts a story by sound and a sequence ofimages giving the illusion of continuous movement (WordNet)Instances, Concepts and Relations?
Relations are assertions linking two (binary relation) or more (n-ary relation) concepts?
actors-act in-movies; cities-capital of-countries?
Facts are instantiations of relations, linking two or moreinstances?
leonardo dicaprio-act in-inception; cairo-capital of-egypt?
Attributes correspond to facts capturing quantifiableproperties of a class or an instance?
actors --> awards, birth date, height?
movies --> producer, release date, budgetOpen-Domain Informationdiseaseschemical elementsfoodscurrenciescountriesdrugsyellow fever, influenza,bipolar disorder, rockymountain spotted fever,anosmia, myxedema,...potassium, magnesium,gold, sulfur, palladium,argon, carbon, borium,ruthenium, zinc, lead,...fish, turkey, rice, milk,chicken, cheese, eggs,corn, beans, wheat,asparagus, grapes,...euro, won, lire, pounds,rand, us dollars, yen,pesos, pesetas, kroner,escudos, shillings,...australia, south korea,kenya, greece, sudan,portugal, argentina,mexico, cuba, kuwait,...paxil, lipitor, ibuprofen,prednisone, albuterol,effexor, azithromycin,fluconazole, advil,...flagclimatepopulation densitygeographycurrencyside effectsdosagepricewithdrawal symptomsgeneric equivalentmasssymbollewis dot diagramatomic numberelectron configurationtreatmentsymptomscausesdiagnosisincidencesizecolorcaloriestasteallergiesdenominationscountrycurrency convertersymbolexchange rateOpen-Domain Informationused in thetreatment ofdecayproduct ofdepletes thebody ofworth millionsofcurrencyofgood sourcesofcan reducerisk ofbrand nameofis aform ofdiseaseschemical elementsfoodscurrenciescountriesdrugsyellow fever, influenza,bipolar disorder, rockymountain spotted fever,anosmia, myxedema,...potassium, magnesium,gold, sulfur, palladium,argon, carbon, borium,ruthenium, zinc, lead,...fish, turkey, rice, milk,chicken, cheese, eggs,corn, beans, wheat,asparagus, grapes,...euro, won, lire, pounds,rand, us dollars, yen,pesos, pesetas, kroner,escudos, shillings,...australia, south korea,kenya, greece, sudan,portugal, argentina,mexico, cuba, kuwait,...paxil, lipitor, ibuprofen,prednisone, albuterol,effexor, azithromycin,fluconazole, advil,...Terminology and Scope?
Terminology?
concept vs. class: used interchangeably?
instance vs. entity: used interchangeably?
Scope?
discussing methods using queries to extract open-domaininformation?
not discussing methods using queries in other tasks such as Websearch in general (e.g., query suggestion, spelling correction,improving search results)Sources of Open-Domain Information?
Human-compiled knowledge resources?
resources created by experts?
resources created collaboratively by non-experts?
Sources of textual data?
text documents (unstructured or semi-structured text)?
(Web) search queriesExpert Resources?
WordNet?
[Fel98]: C. Fellbaum.
WordNet: An Electronic Lexical Database.
MIT Press1998.?
lexical database of English created by experts?
wide-coverage of upper-level conceptual hierarchies?
replicated or extended to other languages?
Cyc?
[Len95]: D. Lenat.
CYC: A Large-Scale Investment in KnowledgeInfrastructure.
Communications of the ACM 1995.?
knowledge base of common-sense knowledge created by experts over 100+person-years?
terms and assertions capturing ground assertions and (inference) rulesCollaborative, Non-Expert Resources?
Wikipedia?
[Rem02]: M. Remy.
Wikipedia: The Free Encyclopedia.
Journal of Online InformationReview 2002.?
free online encyclopedia developed collaboratively by Web volunteers?
among top 20 most popular Web sites (according to comScore: Top 50 US WebProperties, Aug 2009)?
DBpedia?
[BLK+09] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer et al DBpedia ?
A CrystallizationPoint for the Web of Data.
Journal of Web Semantics 2009.?
community effort to convert Wikipedia articles into structured data?
manually-created ontology, mappings from subset of Wikipedia infoboxes to ontology,mappings from Wikipedia articles to WordNet concepts?
Freebase?
[BEP+08]: K. Bollacker, C. Evans, P. Paritosh et al Freebase: A Collaboratively CreatedGraph Database for Structuring Human Knowledge.
SIGMOD-08.?
repository for storing structured data from Wikipedia and other sources, as well asfrom user contributions?
collaboratively created, structured and maintained?
Open Mind?
[SLM+02]: P. Singh, T. Lin, E. Mueller, G. Lim, T. Perkins and W. Zhu.
Open Mind CommonSense: Knowledge Acquisition from the General Public.
Lecture Notes In ComputerScience 2002.?
collect common-sense knowledge from non-expert Web users?
unlike Cyc, collect and represent knowledge in natural language rather than throughformal assertionsWikipediaWikipedia infoboxWikipedia articleDBpedia, FreebaseWikipedia infobox Wikipedia infobox source code<Sears_Tower, previous_building, World_Trade_Center><Sears_Tower, construction_period, 1970-1973>...DBpedia entriesQuantitative Comparison ofHuman-Compiled Resources?
Wikipedia?
3.5+ million articles in English?
articles also available in 200+ other languages?
DBpedia?
2.5+ million instances, 250+ million relations?
Freebase?
20+ million instances, 300+ million relations?
Cyc?
ResearchCyc: 300,000+ concepts and 3+ million assertions?
OpenCyc 2.0: add mappings from Cyc concepts to Wikipedia articles?
Open Mind?
800,000+ facts in English?
facts also available in other languagesSources of Open-Domain Information?
Human-compiled knowledge resources?
resources created by experts?
resources created collaboratively by non-experts?
Sources of textual data?
text documents (unstructured or semi-structured text)?
(Web) search queriesDocumentsSemi-structured textUnstructured textDocumentsSemi-structured textSemi-structured textAlternative to Documents?
Conventionally: data for textual information extraction isavailable as (some sort of) a document collection?
documents capture knowledge, or assertions about the world?
assertions are often ?hidden?
in expository text?
the goal is to derive some of that knowledge from text?
Alternatively: textual information extraction may be pursuedeven without a document collection?
to find new knowledge within a document collection, users formulatetheir search queries based on the knowledge that they alreadypossess at the time of the search--> query logs collectively capture knowledge, through requests that maybe answered by knowledge asserted in document collectionsNext Topic?
Part One: Introduction?
Part Two: Queries as a Corpus?
Part Three: Extraction from QueriesQueries as a Corpus?
Structure of queries?
Comparison with other textual sources?
Usage, demographics and privacyStructure of Queries?
[SW07]: S. Bergsma and Q. Wang.
Learning Noun Phrase Query Segmentation.EMNLP-07.?
identify segments of contiguous query tokens corresponding to semantic concepts, usingmanually annotated queries as training data?
[TP08]: B. Tan and F. Peng.
Unsupervised Query Segmentation Using GenerativeLanguage Models and Wikipedia.
WWW-08.?
identify segments of contiguous query tokens corresponding to semantic concepts, usingevidence from queries and from Wikipedia documents?
[BJR08]: C. Barr, R. Jones and M. Regelson.
The Linguistic Structure of EnglishWeb-Search Queries.
EMNLP-08.?
identify structural characteristics of queries in the task of part of speech tagging?
[ML09]: M. Manshadi and X. Li.
Semantic Tagging of Web Search Queries.
ACL-IJCNLP-09.?
classify queries into domains, and identify query fragments corresponding to pre-specified, per-domain schema of tags?
[GXC+09]: J. Guo and G. Xu and X. Cheng and H. Li.
Named Entity Recognition inQuery.
SIGIR-09.?
detect instances within queries, and classify instances into coarse-grained classes?
[Li10]: X. Li.
Understanding the Semantic Structure of Noun Phrase Queries.ACL-10.?
represent noun phrase queries as a combination of intent heads and intent modifiers,and identify those components automaticallyFinding Structure in Queries?
[BJR08]: C. Barr, R. Jones and M. Regelson.
The Linguistic Structure ofEnglish Web-Search Queries.
EMNLP-08.Part-of-Speech Tags of Query Tokens?
Task?
investigate the task of part-of-speech (POS) tagging when applied to queries?
Input data?
set of 3.2K (2.5K unique) Web search queries, after automatic spell checkingand tokenization?
Manual annotation of POS tags of query tokens is unreliable?
inter-annotator agreement: 0.79 (token-level), 0.65 (query-level)?
main cause of annotation errors (70% of cases): actual query ambiguity (e.g.,download may be a noun or a verb) rather than human annotation mistakes?
POS tags have a different distribution in queries than in documents?
in documents (Brown corpus): ~90 distinct tags, of which 15 for determiners,and 35 for verbs?
in queries: ~20 distinct tags are sufficient, of which 1 for determiners and 1for verbsSuggested Part-of-Speech Tags2.4%getverb2.5%yunknown.........prepositionURIadjectivecommon nounproper nounPart-of-SpeechTaginebay.combigpicturestexasExampleToken3.7%5.9%7.1%30.9%40.2%Percentage ofQuery Tokens?
Nouns are predominant in queries?
most frequent tags in documents: 13% of tokens are common nouns?
most frequent tags in queries: 40% of tokens are proper nouns, 71% oftokens are common nouns or proper nouns?
Verbs are infrequent in queries?
in documents: at least one verb in most sentences?
in queries: less than 3% of tokens(Courtesy R. Jones)Part-of-Speech Tagging Experiments?
Use of capitalization in queries is inconsistent?
17% queries contain capitalization, of which 4% are all-caps?
when a query contains mixed capitalization, first-letter token capitalizationis indicative of an actual proper noun for 73% of cases?
other uses of capitalization in queries: acronyms, capitalization for firsttoken of query, first-letter capitalization for all tokens--> cannot rely on capitalization to identify proper nouns in queriestagger trained and evaluated on queries withautomatically-induced capitalizationtagger trained and evaluated on queries withperfect capitalizationtagger trained on annotated queriestagger trained on annotated documentstagger that assigns most frequent tag (overseparate training lexicon) of each tokenExperimental Setting70.9%89.4%69.7%48.2%65.4%Per-Token TaggingAccuracyComparison with Other Textual Sources?
[CGC+09]: M. Carman, R. Gwadera, F. Crestani and M. Baillie.
A StatisticalComparison of Tag and Query Logs.
SIGIR-09.?
investigate similarity between vocabularies of tokens from search queries vs. tagsassigned by users to Web documents?
[GNL+10]: J. Gao, P. Nguyen, X. Li, C. Thrasher, M. Li and K. Wang.
A ComparativeStudy of Bing Web N-gram Language Models for Web Search and NaturalLanguage Processing.
SIGIR 2010, Web N-gram Workshop.?
generate a repository of n-grams from Web data, including from queries, and evaluateit in various text processing tasksCharacteristics of Documents vs. Queries2-3 words25 words or moreAverage lengthbag of keywordsnatural languageGrammatical stylelowhigh (varies)Average qualityself-containedsurrounding textAvailable contextrequest info.convey info.PurposetexttextType of mediumQueriesDocument SentencesData SourceCharacteristicQueries vs. Other Textual Sources?
[CGC+09]: M. Carman, R. Gwadera, F. Crestani and M. Baillie.
AStatistical Comparison of Tag and Query Logs.
SIGIR-09.Queries vs.
Tags?
Task?
investigate the similarity between query logs and user-generated tags(entered by users to annotate documents)?
Input data?
from query logs containing click-through data, and from Delicious (socialbookmark) tags, select queries and tags associated with a set of 4K Webdocuments?
each document clicked at least 50 times, and associated with a tag at least20 times?
generate respective vocabularies (i.e., sets) of tokens for tags and queries,after removing stop words and stemming all tokens with the Porter stemmerVocabulary SizeToken OccurrencesMetricMedianStd deviationMean278.06464.7955.3Queries393.01533.41105.8Tags15.012.817.6Queries83.0137.7139.6TagsQuery vs. Tag Vocabulary?
Compute overlap between query tokens and tag tokens?
Optionally, remove low frequency tokens or keep high frequency tokens?
Over more than half ofdocuments, overlap ?
0.5--> query vocabulary is verysimilar to tag vocabulary(Courtesy M. Carman)Query vs. Tag vs.
Document Vocabulary?
Include vocabulary of Web documents in comparison of relative overlap?
Similarity between queryand document vocabularyis higher than betweenquery and tag vocabulary?
since documents areclicked search results,they are likely to containquery tokens?
Similarity is lowestbetween tag anddocument vocabulary?
users do not necessarilyenter tags that appear indocument content(Courtesy M. Carman)Repositories of Distilled Query Data?
[GNL+10]: J. Gao, P. Nguyen, X. Li, C. Thrasher, M. Li and K. Wang.
AComparative Study of Bing Web N-gram Language Models for WebSearch and Natural Language Processing.
SIGIR 2010, Web N-gramWorkshop.Web N-Gram CollectionQueriesDocumentsN-gram Length4.6B5.1B2.3B148.5B4-grams1.3B1.1B464.1M11.7B2-grams3.1B3.1B1.4B60.0B3-grams5-grams1-grams230.0B1.2BBodyN/A60.3MAnchor TextN/A150MTitleN/A251.5M?
Language models found to be more similar between queries anddocument title (and queries and document anchor text) than betweenqueries and document body?
Language models of n-grams, from Web documents and search queriesQueries as a Corpus?
Structure of queries?
Comparison with other textual sources?
Usage, demographics and privacyUsage, Demographics and Privacy?
[MC08]: Q. Mei and K. Church.
Entropy of Search Logs: How Hard is Search?
WithPersonalization?
With Backoff?
WSDM-08.?
investigate Web search from the perspective of entropy in search logs, and assess the impact ofaggregated data about users (e.g., from IP addresses) on the outcome of Web search?
[JBS08]: B. Jansen and D. Booth and A. Spink.
Determining the Informational, Navigational,and Transactional Intent of Web Queries.
Journal of Information Processing andManagement 2008.?
investigate the distribution of queries from the point of view of intent type (and subtypes), andautomatically classify queries accordingly?
[JBS09]: B. Jansen, D. Booth and A. Spink.
Patterns of Query Reformulation During WebSearching.
Journal of the American Society for Information Science and Technology 2009.?
develop models to classify various types of query reformulations and identify the most frequent onesamong Web users?
[WC10]: Ingmar Weber and Carlos Castillo.
The Demographics of Web Search.
Sigir-10.?
study the impact of various user demographics factors on the users?
choice of queries?
[JKP+07]: R. Jones, R. Kumar, B. Pang and A. Tomkins.
?I Know What You did Last Summer?
:Query Logs and User Privacy.
CIKM-07.?
study the possibility of uncovering user identity from query logs, despite attempts to remove basicpersonally identifiable information from queries?
[GBG+10]: S. Goel, A. Broder, E. Gabrilovich and B. Pang.
Anatomy of the Long Tail: OrdinaryPeople with Extraordinary Tastes.
WSDM-10.?
[KKM+09]: A. Korolova, K. Kenthapadi, N. Mishra and A. Ntoulas.
Releasing Search Queriesand Clicks Privately.
WWW-09.?
investigate methods to generate modified query log data that preserves user privacyQuery Usage?
Search zeitgeist?
capture ?the general intellectual, moral, and cultural climate of an era?
(Merriam Webster), as reflected in the aggregation of search queriessubmitted by Web usersnokia n900htc evo 4gnokia 5530iphone 4ipadConsumerElectronicsyoutube videosnetflixeminemshakirajustin bieberEntertainmentTop Rising Queries (2010)Top  GlobalEvents (2010)ash cloudoil spillhaiti earthquakeolympicsworld cup(Google Zeitgeist)Geographical Distribution?
For: ash cloud(Google Zeitgeist)Temporal Distribution?
For: circuit city(Google Trends)More queriessubmitted laterduring the year(s)(shopping season)More queriessubmitted, due tounusual event withhigh news coverageQuery Demographics?
[WC10]: Ingmar Weber and Carlos Castillo.
The Demographics of WebSearch.
Sigir-10.Query Demographics?
Task?
investigate impact of user demographics on Web search?
Input data?
user profile data (birth year, gender, zip code)?
set of pairs of (query, clicked URL) from query logs?
census demographic data for various zip codesUSAvg.Query Log DataFeature197414.02.35.788.125.610.922.460%198227.35.115.594.437.616.527.780%17.917.37.94.5Non-English (%)1974196819661956Year of birth12.34.02.40.9Afric.
Amer.
(%)3.64.01.10.4Asian (%)61.912.84.516.020%78.818.17.218.940%76.925.511.122.7Avg.White (%)BA degree (%)Below poverty (%)Per-capita income ($k)75.124.412.421.6(Courtesy I. Weber)Role of Demographics in Web Search?
Highly-discriminant queries for various user demographicsspencer stuart executive searchinsight venture partnersfederal circuitfour seasons jackson holewww.unitnet.comslakerkipasawww.tokbox.comchris jordanelectric candle warmerwww.popsugar.comns4w.orgQueryFeatureBA degree (%)Below poverty (%)Per-capita income ($k)(Courtesy I. Weber)Role of Demographics in Web Search?
Highly-discriminant queries for various user demographicssinabig bang lyricstvb seriesjay chou lyricstrey songz biodef jam records addresss2s magazinemadinaonlinepulloff.comcentral boiler wood furnacefirewood processorsmidwest super cubQueryFeatureAfric.
Amer.
(%)Asian (%)White (%)Role of Demographics in Web Search?
Highly-discriminant queries for various user demographicsfree teen chatroomswet sealtottaly layoutsphotofiltre brusheswww.johnshopkinshealthalerts.comwww.envisionreports.com/vzyahoo free bridge gamesbnymellon.mobular.net/bnymellon/frpQueryFeatureYear of birth, oldYear of birth, youngQueries and User Privacy?
[JKP+07]: R. Jones, R. Kumar, B. Pang and A. Tomkins.
?I Know What Youdid Last Summer?
: Query Logs and User Privacy.
CIKM-07.Queries and User Privacy?
Task?
investigate the vulnerability of narrowing down the identify (demographics)of users submitting search queries, even after removal of personallyidentifiable information (names, numbers) from query logs?
Input data?
from user profile data (anonymized id, birth year, gender, zip code), select100M profiles?
from query logs, select query sessions issued by users with available profiledata, for 744K users?
Assessment of vulnerability?
arrange data into buckets by age, gender, zip code?
arrange buckets into bins, by conjunctions of age, gender, zip code?
smaller bin size makes it easier to identify a particular user from the bin(especially when additional information, e.g., hobbies, is available about theuser)?
e.g., if input data is arranged into bins that share gender bucket, age bucket,and first 3 of 5 zip code digits (e.g., males, age 25-29, living in zip code950xx) --> almost 100K of the 744K users fit into a bin of 100 users or lessDeriving Demographics from Queries?
Identifying user gender and age?
classifiers using bag-of-words features?
gender identification: accuracy of 83.8%?
examples of discriminative features: {bridal, makeup, hair, women?s,..} for women;{nfl, poker, male, compusa,..} for men?
age identification: absolute error of 7 years (predicted vs. actual), betterthan always guessing the middle age point?
examples of discriminative features: {myspace, pregnancy, wikipedia, mall,..} forlower age; {aarp, lottery, amazon.com, senior, repair,..} for higher age?
if personally identifiable information (names and numbers) are removed fromqueries, both gender and age classification remain about as accurate?
Identifying location (zip code)?
existing classifier for locations: given query as input, output list of locations?
convert list of locations into zip code buckets of known first 3, 4 or 5 digits?
if personally identifiable information (names and numbers) are removed fromqueries, location classification becomes much less accurate13.1%6.2%First 554.1%34.9%First 3251.%13.7%First 4Known Digits of Zip CodeCorrect among top threeCorrect at top oneDeriving Queries from Known Information?
Identifying query sessions submitted by a known user?
use demographics, conversations with, lifestyle changes of user, in order toguess queries that may have been submitted by user?
as an approximation, manually create a set of guessed queriesbassmaster (388)skulling (17)skiing (9618)football (123802)Sportsassam (747)pizza (104888)italian restaurant (4998)brie (39325)Foodharry potter (27838)danielle steele (238)freakonomics (574)volkswagen beetle (478)honda odyssey (1504)toyota prius (1070)Commonholly lisle (20)elizabeth moon (27)triumph tr23 (23)e-type jaguar (5)RareCategoryBooksCars?
use combinations of guessed queriesKnowing that auser submittedthe query e-type jaguarnarrows downthe identity ofthe user to abin of 5possible users(Courtesy R. Jones)Deriving Queries from Known Information1brie, holly lisle, pizza27harry potter, volkswagen beetle......2pizza, triumph tr32430football, skiing1441italian restaurant, pizza14855Bin SizeQuery Combinationdanielle steele, volkswagen beetleharry potter, pizza--> even if individual bits of information are far from unique among users,putting them together can uniquely identify a userNext Topic?
Part One: Introduction?
Part Two: Queries as a Corpus?
Part Three: Extraction from QueriesExtraction Methods?
Methods for extraction of:?
instances and concepts?
attributes and relationsInstances and Conceptsdiseaseschemical elementsfoodscurrenciescountriesdrugsyellow fever, influenza,bipolar disorder, rockymountain spotted fever,anosmia, myxedema,...potassium, magnesium,gold, sulfur, palladium,argon, carbon, borium,ruthenium, zinc, lead,...fish, turkey, rice, milk,chicken, cheese, eggs,corn, beans, wheat,asparagus, grapes,...euro, won, lire, pounds,rand, us dollars, yen,pesos, pesetas, kroner,escudos, shillings,...australia, south korea,kenya, greece, sudan,portugal, argentina,mexico, cuba, kuwait,...paxil, lipitor, ibuprofen,prednisone, albuterol,effexor, azithromycin,fluconazole, advil,...Instances and Concepts?
[Pas07]: M. Pa?ca.
Weakly-Supervised Discovery of Named Entities using WebSearch Queries.
CIKM-07.?
expand sets of instances using Web search queries?
[VP08]: B.
Van Durme and M. Pa?ca.
Finding Cars, Goddesses and Enzymes:Parametrizable Acquisition of Labeled Instances for Open-Domain InformationExtraction.
AAAI-08.?
extract labeled sets of instances from Web documents, by merging clusters ofdistributionally similar phrases with IsA pairs extracted with lexico-syntactic patterns?
[PP09]: M. Pennacchiotti and P. Pantel.
Entity Extraction via Ensemble Semantics.EMNLP-09.?
expand sets of instances using multiple sources of text including queries?
[AHH09]: E. Alfonseca and K. Hall and S. Hartmann.
Large-Scale Computation ofDistributional Similarities for Queries.
NAACL-HLT-2009.?
apply vector-space model of distributional similarities to queries rather than documents?
[JP10]: A. Jain and P. Pantel.
Open Entity Extraction from Web Search QueryLogs.
COLING-10.?
extract clusters of distributionally similar phrases from Web search queries and click-through dataInstances and Concepts?
[VP08]: B.
Van Durme and M. Pa?ca.
Finding Cars, Goddesses andEnzymes: Parametrizable Acquisition of Labeled Instances for Open-Domain Information Extraction.
AAAI-08.Extraction from Documents and Queries?
Input?
target relation, available as a small set of extraction patterns?
e.g., <C [such as|including] I>?
Data sources?
collection of Web documents?
collection of anonymized Web search queries?
Output?
sets of instances, each set associated with a class label?
e.g., marine animals = {whales, seals, dolphins, turtles, sea lions, fishes,penguins, squids, pacific walrus, aquatic birds, comb jellies, starfish,florida manatees, walruses,...}?
each set alo associated with lists of attributesAcquisition of Open-Domain Classes?
Define a closed vocabulary of potential class instances, as theset of most frequently-submitted Web search queries?
textual data source: Web query logs?
output: noisy set of potential class instances?
Acquire class labels for potential class instances, via hand-written extraction patterns?
textual data source: Web documents?
<C [such as|including] I>, where C is a potential class label (e.g.,zoonotic diseases) and I is a potential instance (e.g., brucellosis)?
output: noisy pairs of an instance and a class label?
Organize potential class instances into sets of distributionallysimilar phrases?
output: noisy sets of distributionally similar instancesMerge into labeled sets of instancesExtraction of Labeled InstancesInput:   - pairs of an instance and a class label- unlabeled sets of distributionally similar instancesOutput: - sets of instances, each set associated with a class labelFor each unlabeled set of distributionally-similar instances SFor each class label L assigned to some instance(s) of set SA=set of instances of S whose class label is LB=set of sets that contain some instance(s) whose label is LIf |A| > J?|S|:If |B| < K:Collect instances of A, associated with the class label Ltfidf?
Note: J, K are weighting parameters controlling precision/recall?
J in [0,1); higher J --> higher precision?
K is non-negative integer; lower K --> higher precisiongeorge w. bushj.
carterbill clintonnixonronald reaganal sharptonhillary clintongmvolvofordschwinntoyotalettucecornbroccolicarrotappleorangerosebananamangobenjamin franklingeorge washingtonpaul reverejeffersonjohn adamsabe lincolnPresidentsFruitsCar CompaniesPatterns and Distributional Similarities(Courtesy B.
Van Durme)Instances and Concepts?
[PP09]: M. Pennacchiotti and P. Pantel.
Entity Extraction via EnsembleSemantics.
EMNLP-09.Extraction from Multiple Sources?
Input?
target classes, available as small sets of seed instances?
e.g., {jodie foster, humphrey bogart, anthony hopkins} for Actor?
target classes, also available as small sets of seed relations with otherclasses?
e.g., < leonardo dicaprio, inception>, <nicole kidman, eyes wide shut> for Actor(corresponding to relation Actor-act in-Movie)?
Data sources?
collection of Web documents?
collection of Web search queries?
HTML tables identified within the collection of Web documents?
collection of articles from Wikipedia?
Output?
ranked lists of instances, one per class?
e.g., [gordon tootoosis, rosalind chao, john hawkes, jeffrey dean morgan,...] forActorEnsemble SemanticsS1SKS2KEnKE2KE1FG1 FG2 FGmKBFEATURE GENERATORSRANKERKNOWLEDGEEXTRACTORSAGGREGATORMODELERDECODER(Courtesy P. Pantel, M. Pennacchiotti)Extraction Components?
Sources (S1, S2,..., Sk)?
data sources from which instances and their relevant features areextracted?
Knowledge extractors (KE1, KE2,..., KEn)?
extract candidate instances from sources, using various algorithms?
Feature generators (FG1, FG2,..., FGm)?
collect evidence/features relevant to deciding whether candidateinstances are correct or not?
Aggregator?
combine evidence available from multiple sources for candidateinstances?
Ranker?
rank candidate instances extracted by knowledge extractors, basedon features available from feature generatorsRanking Features?
Collected by feature generators?
4 feature families: from Web documents, queries, tables, Wikipedia?
5 feature types: frequency, co-occurrence, distributional, pattern,termness (i.e., checking whether extracted terms are well-formed)(Courtesy P. Pantel, M. Pennacchiotti)Extraction Results?
Input data = collection of 600 million Web documents; tables identifiedwithin the documents; one year of queries; 2 million Wikipedia articles?
Evaluate lists of instances extracted for 3 classes: Actor, Athlete andMusician?
create gold standard from samples of 500 instances selected randomly foreach class?
compute precision of extracted lists of instances, relative to and over thegold standards?
Average precision: 0.860 (Actor), 0.915 (Athlete), 0.788 (Musician)?
Precision@100: 0.99 (Athlete)?
Estimated precision@22000: 0.97 (Athlete)Instances and Concepts?
[JP10]: A. Jain and P. Pantel.
Open Entity Extraction from Web SearchQuery Logs.
COLING-10.Extraction from Queries?
Data sources?
anonymized search queries along with frequencies and click-through data(clicked search results)?
Web documents?
Output?
clusters of similar instances?
e.g., {basic algebra, numerical analysis, discrete math, lattice theory, nonlinearphysics, ...}, {aaa insurance, roadside assistance, personal liability insurance,international driving permits, ...}?
Steps?
collect set of candidate instances from queries?
cluster instances using context in queries or click-through data or bothSimilarity in Documents vs. Queries?
Contextual space of Web documents?
an instance is represented by the contexts in which it appears in textdocuments?
instances are modeled ?objectively?, according to descriptions of the world?
Contextual space of Web search queries?
an instance is represented by the contexts in which it appears in a searchqueries?
instances are modeled ?subjectively?, according to users?
perception of theworldbritney spearsceline dionbruce springsteenparis hiltonserena williamsbritney spears galapagos islandssouth america cruisekauai snorkelingContextual space of Web documents Contextual space of Web search queriesgalapagos islandstasmaniaguineaOther singers Other celebritiesOther regions Other island travel topicsExtraction of Instances?
Identify candidate instances?
intuition: in queries composed by copying fragments from Web documentsand pasting them into queries, capitalization of instances is preserved?
from queries containing capitalization, extract contiguous sequences ofcapitalized tokens as instancesQueries Candidate InstancesBritney Spears new song --> Britney Spearstravel to Italy Roma --> Italy Romarestaurant Cascal in Mountain View --> Cascal, Mountain View?
Retain set of best candidate instances?
first criterion: promote candidate instances whose capitalization is frequentin Web documents?
second criterion: promote candidate instances that occur as full-lengthqueries?
retain set of candidate instances that score highly (above some thresholds)according to both criteria(Courtesy A. Jain)Clustering of Instances?
Induce unlabeled classes of instances, by clustering instances usingfeatures collected from queries?
as an alternative to collecting features from unstructured text in documents?
for efficiency, no attempt to parse the queries?
Context features?
vector of elements corresponding to contexts, where a context is the prefixand postfix around the instance, from queries containing the instance?
Click-through features?
vector of elements corresponding to documents, where a document is onethat is clicked by a user submitting the instance as a full-length query?
Hybrid features?
normalized combination of context and click-through vectorsImpact of Clustering Features?
Given an instance, manuallyjudge each co-clusteredinstance:?
?If you were interested ininstance I, would you alsobe interested in instance Icin any intent???
also, annotate with type ofrelation between instanceand co-clustered instance?
Compute precision, over aset of evaluation instances?
CL-CTX: context?
CL-CLK: click-through?
CL-HYB: hybrid?
CL-Web: context collectedfrom Web documentsrather than queries0.46CL-CTX0.85CL-HYB0.73CL-Web0.81CL-CLKPrecisionMethodMethodRelationType0.020.01-0.01child0.01-0.720.27CL-Web0.030.090.430.46CL-CTX0.120.130.290.46CL-CLK0.32sibling0.16synonym0.40topic0.09parentCL-HYBExtraction Methods?
Methods for extraction of:?
instances and concepts?
attributes and relationsAttributes and Relationsdiseaseschemical elementsfoodscurrenciescountriesdrugsyellow fever, influenza,bipolar disorder, rockymountain spotted fever,anosmia, myxedema,...potassium, magnesium,gold, sulfur, palladium,argon, carbon, borium,ruthenium, zinc, lead,...fish, turkey, rice, milk,chicken, cheese, eggs,corn, beans, wheat,asparagus, grapes,...euro, won, lire, pounds,rand, us dollars, yen,pesos, pesetas, kroner,escudos, shillings,...australia, south korea,kenya, greece, sudan,portugal, argentina,mexico, cuba, kuwait,...paxil, lipitor, ibuprofen,prednisone, albuterol,effexor, azithromycin,fluconazole, advil,...flagclimatepopulation densitygeographycurrencyside effectsdosagepricewithdrawal symptomsgeneric equivalentmasssymbollewis dot diagramatomic numberelectron configurationtreatmentsymptomscausesdiagnosisincidencesizecolorcaloriestasteallergiesdenominationscountrycurrency convertersymbolexchange rateAttributes and Relationsused in thetreatment ofdecayproduct ofdepletes thebody ofworth millionsofcurrencyofgood sourcesofcan reducerisk ofbrand nameofis aform ofdiseaseschemical elementsfoodscurrenciescountriesdrugsyellow fever, influenza,bipolar disorder, rockymountain spotted fever,anosmia, myxedema,...potassium, magnesium,gold, sulfur, palladium,argon, carbon, borium,ruthenium, zinc, lead,...fish, turkey, rice, milk,chicken, cheese, eggs,corn, beans, wheat,asparagus, grapes,...euro, won, lire, pounds,rand, us dollars, yen,pesos, pesetas, kroner,escudos, shillings,...australia, south korea,kenya, greece, sudan,portugal, argentina,mexico, cuba, kuwait,...paxil, lipitor, ibuprofen,prednisone, albuterol,effexor, azithromycin,fluconazole, advil,...Attributes and Relations?
[PV07]: M. Pa?ca and B.
Van Durme.
What You Seek is What You Get: Extractionof Class Attributes from Query Logs.
IJCAI-07.?
apply small set of patterns to extract attributes from queries?
[PVG07]: M. Pa?ca, B.
Van Durme and N. Garera.
The Role of Documents vs.Queries in Extracting Class Attributes from Text.
CIKM-07.?
apply patterns to extract attributes from unstructured text in documents vs. queries?
[Pas07]: M. Pa?ca.
Organizing and Searching the World Wide Web of Facts -Step Two: Harnessing the Wisdom of the Crowds.
WWW-07.?
expand sets of seed attributes using queries?
[LWA09]: X. Li, Y. Wang and A. Acero.
Extracting Structured Information fromUser Queries with Semi-Supervised Conditional Random Fields.
SIGIR-09.?
detect relevant fields in product-search queries, using click data and document content?
[PER+10]: M. Pa?ca, E. Alfonseca, E. Robledo-Arnuncio, R. Martin-Brualla and K.Hall.
The Role of Query Sessions in Extracting Instance Attributes from WebSearch Queries.
ECIR-10.?
extract attributes of instances, from sequences of queries within query sessions?
[YTT10]: X. Yin, W. Tan and Y. Tu.
Automatic Extraction of Clickable StructuredWeb Contents for Name Entity Queries.
WWW-10.?
given a query containing an instance, extract structured data from click data andcontents of subsequently visited documents?
[SJY11]: A. Das Sarma, A. Jain and C. Yu.
Dynamic Relationship and EventDiscovery.
WSDM-11.?
acquire temporally-anchored relations that apply within a given set of instances, usingqueries and (news) documentsAttributes and Relations?
[Pas07]: M. Pa?ca.
Organizing and Searching the World Wide Web ofFacts - Step Two: Harnessing the Wisdom of the Crowds.
WWW-07.Extraction from Queries?
Input?
target classes, available as sets of representative instances?
e.g., {Delphi, Apple Computer, Honda, Oracle, Coca Cola, Toyota, WashingtonMutual, Delta, Reuters, Target, ...} for Company?
small sets of seed attributes, one per class?
e.g., {headquarters, stock price, ceo, location, chairman} for Company?
Data source?
anonymized search queries along with frequencies?
Output?
ranked (longer) lists of attributes, one per class?
e.g., {headquarters, mission statement, stock price, ceo, code of conduct, stocksymbol, organizational structure, corporate address, cio, ...} for Company?
Steps?
select candidate attributes, from queries containing an instance?
create internal representation of candidate attributes, from queriescontaining an instance and a candidate attribute?
rank candidate attributes, from similarity between internal representationof a candidate attribute and combined internal representation of all seedattributesClass Attribute ExtractionCompany: {Delphi, Apple Computer, Honda, Oracle, Coca Cola,Toyota, Washington Mutual, Delta, Reuters, Target,...}Company: {headquarters, stock price, ceo, location, chairman}Seed attributesTarget classesCompany: installingCompany: stock priceCompany: accordCompany: headquartersCompany: mission statementReference search-signature vectors (one per class)Company[ ]      [ ]      [8.1-7 on solaris 8]prefix    infix                                      postfix[ ]      [ ]      [cressida water pump]prefix    infix                                       postfix[ ]      [company one year]      [target]prefix                                    infix             postfix[ ]      [air lines]      [history]prefix                 infix               postfix[ ]      [ ]      [1989 sei]prefix    infix             postfix[new]      [ ]      [ ]prefix    infix     postfix[where is the world]      [for]      [corporation]prefix        infix       postfix[ ]    [new]    [impact]prefix        infix          postfix[ ]      [for the]      [corporation]prefix               infix                      postfix[ ]      [for]      [airlines]prefix       infix                   postfixinstalling toyota cressida water pumporacle 8.1-7 on solaris 8coc  cola company one ye r stock rice targetdel air lines stock price his o yh nda acc rd 1989 sein w honda cordwhere s th  worl headquart s for delphi corporationashington u ual n w h dquarters imp tmissio  s tement for the r cle r orationlta i inesQuery logsCompany: {installing, stock price, accord,headquarters, mission statement,...}Pool of candidate attributesSearch-signature vectors (one per candidate attribute)Company: {headquarters, mission statement, stock price, ceo,code of conduct, stock symbol, organizationalstructure, corporate address, cio,...}Ranked list of extracted class attributesTop Extracted Attributescostume, voice, creator, first appearance, funny pictures, origins,cartoon images, cartoon pics, color pagesCartoonChar6...754321......features, battery life, retail price, mobile review, specification,price list, functions, ratings, tips, tricksCellPhoneModeltransmission, top speed, acceleration, transmission problems,owners manual, gas mileage, towing capacity, stalling, maintenanceschedule, performance partsCarModelcalories, color, size, allergies, taste, carbs, nutritionalinformation, nutrition facts, nutritional value, nutritionBasicFoodrecipients, date, winners list, result, gossip, printable ballot,nominees, winners, location, announcementsAwardweight, length, history, fuel consumption, interior photos,specifications, photographs, interior pictures, seatingarrangement, flight deckAircraftModelawards, height, age, date of birth, weight, b** ****, birthdate,birthplace, cause of death, real nameActorTop Extracted AttributesClassTop Extracted Attributes40393837363534...date, location, significance, images, importance, timeline,summary, pics, maps, photographsWorldWarBattlevintage, color, cost, style, taste, vintage chart, pronunciation,shelf life, wine ratings, wine reviewsWineprice, system requirements, creator, official site, officialwebsite, free game download, concept art, download demo, pccheat codes, reviewsVideoGamealumni, mascot, dean, economics department, career center,graduation 2005, department of psychology, school colors, tuitioncosts, campus mapUniversitycountries, ratification, date, definition, summary, purpose, pros,cons, members, pictureTreatyattacks, leader, goals, meaning, website, leadership, photos,images, definition, flagTerroristGrouplocation, seating capacity, architect, address, seating map,dimensions, tours, pics, poster, box officeStadium......Top Extracted AttributesClassExtraction Results?
Input data = 50 million anonymized queries?
Evaluate attributes extracted with hand-written patterns vs. based onseeds40393837...43210.760.530.850.640.900.72Average (40 Classes)0.660.000.820.000.850.00WorldWarBattle0.570.290.870.421.000.40Wine0.900.440.900.570.900.70VideoGame0.740.650.850.820.850.90University.....................0.860.650.950.901.001.00BasicFood0.690.240.770.150.950.30Award0.710.680.850.770.800.80AircraftModel0.960.741.000.821.000.85ActorSeedPattSeedPattSeedPatt@50@20@10PrecisionClassSummary?
Do ask, do tell?
if knowledge is prominent, someone will eventually write about it?
if knowledge is prominent, someone will eventually ask about it?
Web search queries are cursory reflections of knowledge encodeddeeply within unstructured and structured content available indocuments?
Queries are useful in open-domain information extraction?
each user searches for something; collectively, all users search formany (most?)
things?
queries often reflect the relative popularity of people, topics,events etc.--> useful in the extraction and ranking of instances, classes andrelations
