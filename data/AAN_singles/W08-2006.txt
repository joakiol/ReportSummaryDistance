Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 41?48Manchester, August 2008Affinity Measures based on the Graph LaplacianDelip RaoDept.
of Computer ScienceJohns Hopkins Universitydelip@cs.jhu.eduDavid YarowskyDept.
of Computer ScienceJohns Hopkins Universityyarowsky@cs.jhu.eduChris Callison-BurchDept.
of Computer ScienceJohns Hopkins Universityccb@cs.jhu.eduAbstractSeveral language processing tasks can beinherently represented by a weighted graphwhere the weights are interpreted as ameasure of relatedness between two ver-tices.
Measuring similarity between ar-bitary pairs of vertices is essential in solv-ing several language processing problemson these datasets.
Random walk basedmeasures perform better than other pathbased measures like shortest-path.
Weevaluate several random walk measuresand propose a new measure based on com-mute time.
We use the psuedo inverseof the Laplacian to derive estimates forcommute times in graphs.
Further, weshow that this pseudo inverse based mea-sure could be improved by discarding theleast significant eigenvectors, correspond-ing to the noise in the graph constructionprocess, using singular value decomposi-tion.1 IntroductionNatural language data lend themselves to a graphbased representation.
Words could be linked byexplicit relations as in WordNet (Fellbaum, 1989)or documents could be linked to one another viahyperlinks.
Even in the absence of such a straight-forward representation it is possible to derivemeaningful graphs such as the nearest neighborgraphs as done in certain manifold learning meth-ods (Roweis and Saul, 2000; Belkin and Niyogi,c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.2001).
All of these graphs share the followingproperties:?
They are edge-weighted.?
The edge weight encodes some notion of re-latedness between the vertices.?
The relation represented by edges is at leastweakly transitive.
Examples of such rela-tions include, ?is similar to?, ?is more generalthan?, and so on.
It is important that the re-lations selected are transitive for the randomwalk to make sense.Such graphs present several possibilities in solv-ing language problems on the data.
One such taskis, given two vertices in the graph we would liketo know how related the two vertices are.
Thereis an abundance of literature on this topic, someof which will be reviewed here.
Finding similaritybetween vertices in a graph could be an end in it-self, as in the lexical similarity task, or could be astage before solving other problems like clusteringand classification.2 Contributions of this paperThe major contributions of this paper are?
A comprehensive evaluation of various ran-dom walk based measures?
Propose a new similarity measure based oncommute time.?
An improvement to the above measure byeliminating noisy features via singular valuedecomposition.413 Problem settingConsider an undirected graph G(V,E,W) withvertices V , edges E, and W = [wij] be the sym-metric adjacency weight matrix with wijas theweight of the edge connecting vertices i and j. Theweight, wij= 0 for vertices i and j that are notneighbors and when wij> 0 it is interpreted as anindication of relatedness between i and j.
In ourcase, we consider uniformly weighted graphs, i.e,wij= 1 for neighbors but this need not be the case.Let n = |V | be the order of the graph.
We definea relation sim : V ?
V ?
R+such that sim(i, j)is the relatedness between vertices i and j. Thereare several ways to define sim; the ones exploredin this paper are:?
simG(i, j) is the reciprocal of the shortestpath length between vertices i and j. Notethat this is not a random walk based mea-sure but a useful baseline for comparison pur-poses.?
simB(i, j) is the probability of a random walkfrom vertex i to vertex j using all paths oflength less than m.?
simP(i, j) is the probability of a random walkfrom vertex i to vertex j defined via a pager-ank model.?
simC(i, j) is a function of the commute timebetween vertex i and vertex j.4 Data and EvaluationWe evaluate each of the similarity measure weconsider by using a linguistically motivated taskof finding lexical similarity.
Deriving lexicalrelatedness between terms has been a topic ofinterest with applications in word sense disam-biguation (Patwardhan et al, 2005), paraphras-ing (Kauchak and Barzilay, 2006), question an-swering (Prager et al, 2001), and machine trans-lation (Blatz et al, 2004) to name a few.
Lex-ical relatedness between terms could be derivedeither from a thesaurus like WordNet or fromraw monolingual corpora via distributional simi-larity (Pereira et al, 1993).
WordNet is an inter-esting graph-structured thesaurus where the ver-tices are the words and the edges represent rela-tions between the words.
For the purpose of thiswork, we only consider relations like hypernymy,hyponymy, and synonymy.
The importance of thisproblem has generated copious literature in thepast ?
see (Pedersen et al, 2004) or (Budanitskyand Hirst, 2006) for a detailed review of variouslexical relatedness measures on WordNet.
Our fo-cus in this paper is not to derive the best similar-ity measure for WordNet but to use WordNet andthe lexical relatedness task as a method to evalu-ate the various random walk based similarity mea-sures.
Following the tradition in previous litera-ture we evaluate on the Miller and Charles (1991)dataset.
This data consists of 30 word-pairs alongwith human judgements which is a real value be-tween 1 and 4.
For every measure we consider,we derive similarity scores and compare with thehuman judgements using the Spearman rank cor-relation coefficient.5 Graph constructionFor the purpose of evaluation of the random walkmeasures, we construct a graph for every pair ofwords for which similarity has to be computed.This graph is derived from WordNet as follows:?
For each word w in the pair (w1, w2):?
Add an edge between w and all of itsparts of speech.
For example, if the wordis coast, add edges between coast andcoast#noun and coast#verb.?
For each word#pos combination,add edges to all of its senses (Forexample, coast#noun#1 throughcoast#noun#4.?
For each word sense, add edges to all ofits hyponyms?
For each word sense, add edges to all ofits hypernyms recursively.In this paper we consider uniform weights on alledges as our main aim is to illustrate the differ-ent random walk measures rather than fine tune thegraph construction process.6 Shortest path based measureThe most obvious measure of distance in a graph isthe shortest path between the vertices which is de-fined as the minimum number of intervening edgesbetween two vertices.
This is also known as thegeodesic distance.
To convert this distance mea-sure to a similarity measure, we take the recipro-cal of the shortest-path length.
We refer to this asthe geodesic similarity.
This is not a random walk42Figure 1: Shortest path distances on graphsmeasure but will serve as an important baseline forour work.
As can be observed from Table 1, theMethod Spearman correlationGeodesic 0.275Table 1: Similarity using shortest-path measure.correlation is rather poor for the shortest path mea-sure.7 Why are shortest path distances bad?While shortest-path distances are useful in manyapplications, it fails to capture the following obser-vation.
Consider the subgraph of WordNet shownin Figure 1.
The term moon is connected to theterms religious leader and satellite1.Observe that both religious leader andsatellite are at the same shortest path dis-tance from moon.
However, the connectivitystructure of the graph would suggest satelliteto be ?more?
similar than religious leaderas there are multiple senses, and hence multiplepaths, connecting satellite and moon.Thus it is desirable to have a measure that cap-tures not only path lengths but also the connectiv-ity structure of the graph.
This notion is elegantlycaptured using random walks on graphs.7.1 Similarity via Random walksA random walk is a stochastic process that consistsof a sequence of discrete steps taken at random de-fined by a distribution.
Random walks have inter-esting connections to Brownian motion, heat diffu-sion and have been used in semi-supervised learn-ing ?
for example, see (Zhu et al, 2003).
Certainproperties of random walks are defined for ergodicprocesses only2.
In our work, we assume these1The religious leader sense of moon is due to SunMyung Moon, a US religious leader.2A stochastic process is ergodic if the underlying Markovchain is irreducible and aperiodic.
A Markov chain is irre-hold true as the graphs we deal with are connected,undirected, and non-bipartite.7.1.1 Bounded length walksAs our first random walk measure, we considerthe bounded length walk ?
i.e., all random walks oflength less than or equal to a bound m. We derivea probability transition matrix P from the weightmatrix W as follows:P = D?1Wwhere, D is a diagonal matrix with dii=?nj = 1wij.
Observe that:?
pij= P[i, j] ?
0, and?
?nj = 1pij= 1Hence pijcan be interpreted as the probabilityof transition from vertex i to vertex j in one step.
Itis easy to observe that Pkgives the transition prob-ability from vertex i to vertex j in k steps.
Thisleads to the following similarity measure:S = P + P2+ P3+ ...+ PmObserve that S[i, j] derives the total probability oftransition from vertex i to vertex j in at most msteps3.
Given S, we can derive several measures ofsimilarity:1.
Bounded Walk: S[i, j]2.
Bounded Walk Cosine: dot product ofrowvectors Siand Sj.When we evaluate these measures on the Miller-Charles data the results shown in Table 2. are ob-served.
For this experiment, we consider all walksthat are at most 20 steps long, i.e., m = 20.
Ob-serve that these results are significantly better thanthe Geodesic similarity based on shortest-paths.ducible if there exists a path between any two states and it isaperiodic if the GCD of all cycle lengths is one.3The matrix S is row normalized to ensure that the entriescan be interpreted as probabilities.43Method Spearman correlationBounded Walk 0.346Bounded Walk Cosine 0.365Table 2: Similarity using bounded random walks(m = 20).7.1.2 How many paths are sufficient?In the previous experiment, we arbitrarily fixedm = 20.
However, as observed in Figure 2. , be-yond a certain value the choice of m does not affectthe result as the random walk converges to its sta-tionary distribution.
The choice of m depends onFigure 2: Effect of m in Bounded walkthe amount of computation available.
A reason-ably large value of m (m > 10) should be suffi-cient for most purposes and one could use lowervalues of m to derive an approximation for thismeasure.
One could derive an upper bound on thevalue of m using the mixing time of the underlyingMarkov chain (Aldous and Fill, 2001).7.1.3 Similarity via pagerankPagerank (Page et al, 1998) is the celebrated ci-tation ranking algorithm that has been applied toseveral natural language problems from summa-rization (Erkan and Radev, 2004) to opinion min-ing (Esuli and Sebastiani, 2007) to our task oflexical relatedness (Hughes and Ramage, 2007).Pagerank is yet another random walk model with adifference that it allows the random walk to ?jump?to its initial state with a nonzero probability (?
).Given the probability transition matrix P as definedabove, a stationary distribution vector for any ver-tex (say i) could be derived as follows:1.
Let eibe a vector of all zeros with ei(i) = 12.
Let v0= ei3.
Repeat until ?vt?
vt?1?F< ??
vt+1= ?vtP + (1?
?)v0?
t = t+ 14.
Assign vt+1as the stationary distribution forvertex i.Armed with the stationary distribution vectors forvertices i and j, we define pagerank similarity ei-ther as the cosine of the stationary distribution vec-tors or the reciprocal Jensen-Shannon (JS) diver-gence4between them.
Table 3. shows results onthe Miller-Charles data.
We use ?
= 0.1, the bestvalue on this data.
Observe that these results areMethod Spearman correlationPagerank JS-Divergence 0.379Pagerank Cosine 0.393Table 3: Similarity via pagerank (?
= 0.1).better than the best bounded walk result.
We fur-ther note that our results are different from thatof (Hughes and Ramage, 2007) as they use exten-sive feature engineering and weight tuning duringthe graph generation process that we have not beenable to reproduce.
Hence for simplicity we stuck toa simpler graph generation process.
Nevertheless,the result in Table 3. is still useful as we are in-terested in the performance of the various spectralsimilarity measures rather than achieving the bestperformance on the lexical relatedness task.
Thegraphs we use in all methods are identical makingcomparisons across methods possible.7.2 Similarity via Hitting TimeGiven a graph with the transition probability ma-trix P as defined above, the hitting time betweenvertices i and j, denoted as h(i, j), is defined asthe expected number of steps taken by a randomwalker to first encounter vertex j starting from ver-tex i.
This can be recursively defined as follows:h(i, j) =??
?1 +?k : wik> 0pikh(k, j) if i 6= j0 if i = j(1)4The Jensen-Shannon divergence between two distribu-tions p and q is defined as D(p ?
a)+D(q ?
a), where D(.
?.)
is the Kullback-Liebler divergence and a = (p + q)/2.Note that unlike KL-divergence this measure is symmetric.See (Lin, 1991) for additional details.44The lower the hitting times of two vertices, themore similar they are.
It can be easily verifiedthat hitting time is not a symmetric relation hencegraph theory literature suggests another symmet-ric measure ?
the commute time.5The commutetime, c(i, j), is the expected number of steps takento leave vertex i, reach vertex j, and return back toi.
Thus,c(i, j) = h(i, j) + h(j, i) (2)Observe that, the commute time is a metric in thatit is positive definite, symmetric, and satisifies tri-angle inequality.
Hence, commute time could beused as a distance measure as well.
We derive asimilarity measure from this distance measure us-ing the following lemma.Lemma 1.
For every edge (i, j), c(i, j) ?
2lwhere l = |E|, the number of edges.Proof.
This can be easily observed by defining aMarkov chain on the edges with probability tran-sition matrix Q with 2l states, such that Qe1e2=1/degree(e1?
e2).
Since this matrix is doublystochastic, the stationary distribution on this chainwill be uniform with a probability 1/2l.
Nowc(i, j) = h(i, j)+h(j, i), is the expected time for awalk to start at i, visit j, and return back to i. Whenthe stationary probability at each edge is 1/2l, thisexpected time evaluates to 2l.
Hence the commutetime can be at most 2l.This lemma allows us to define a similarity mea-sure as follows:simC(i, j) = 1?c(i, j)2l(3)Observe that the measure defined in Equation 3 isa metric and further its range is defined in [0, 1].We now only need a way to compute the commutetimes to use Equation 3.
One could compute thehitting times and hence the commute times fromthe Equations 1 and 2 using dynamic program-ming, akin to shortest paths in graphs.
In this pa-per, we instead choose to derive commute timesvia the graph Laplacian.
This also allows us tohandle ?noise?
in the graph construction processwhich cannot be taken care by naive dynamic pro-gramming.5Note that distance measures, in general, need not be sym-metric but we interpret distance as proximity which mandatessymmetry.Chandra et.
al.
(1989) show that the commutetime between two vertices is equal to the resis-tance distance between them.
Resistance distance,as proposed by Klein and Randic (1993), is theeffective resistance between two vertices in theelectrical network represented by the graph, wherethe edges have resistance 1/wij.
Xiao and Gut-man (2003), show the relation between resistancedistances in graphs to the Laplacian spectrum, thusenabling a way to derive commute times from thegraph Laplacian in closed form.We now introduce graph Laplacians, which areinteresting in their own right besides being relatedto commute time.
The Laplacian of a graph couldbe viewed as a discrete version of the Laplace-Beltrami operator on Riemannian manifolds.
It isdefined asL = D ?
WThe graph Laplacian has interesting properties anda wide range of applications, in semi-supervisedlearning (Zhu et al, 2003), non-linear dimension-ality reduction (Roweis and Saul, 2000; Belkin andNiyogi, 2001), and so on.
See (Chung, 1997) fora thorough introduction on Laplacians and theirproperties.
We depend on the fact that L is:1. symmetric (since D and W are for undirectedgraphs)2. positive-semidefinite : since it is symmet-ric, all of the eigenvalues are real and bythe Greshgorin circle theorem, the eigenval-ues must also be non-negative and hence L ispositive-semidefinite.Throughout this paper we use normalized Lapla-cians as defined below:L = D?1/2LD?1/2= I ?
D?1/2WD?1/2The normalized Laplacians preserve all propertiesof the Laplacian by construction.As noted in Xiao and Gutman (2003), the re-sistance distances can be derived from the gener-alized Moore-Penrose pseudo-inverse of the graphLaplacian(L?)
?
also called the inverse Laplacian.Like Laplacians, their pseudo inverse counterpartsare also symmetric, and positive semi-definite.Lemma 2.
L?is symmetricProof.
The Moore-Penrose pseudo-inverse is de-fined as L?= (LTL)?1LT.
From this definition,it is clear that (L?
)T= (LT)?.
By the symmetry45property of graph Laplacians, LT= L.
Hence,(L?
)T= L?.Lemma 3.
L?is positive semi-definiteProof.
We make use of the following propertiesfrom (Chung, 1997):?
The Laplacian, L, is positive semi-definite(also shown above).?
If the Eigen-decomposition of L is Q?QT,then the Eigen-decomposition of the pseudo-inverse L?is Q??1QT.
If any of the eigenval-ues of L is zero then the corresponding eigen-value for L?is also zero.Since L is positive semi-definite, and the eigen-values of L?have the same sign as L, the pseudoinverse L?has to be positive semi-definite.Lemma 4.
The inverse Laplacian is a gram matrixProof.
To prove this, we use the fact that theLaplacian Matrix is symmetric and positive semi-definite.
Hence by Cholesky decomposition wecan write L = UUT.Therefore L?= (UT)?U?= (U?)T(U?
).Hence L?is a matrix of dot-products or a gram-matrix.Thus, from Lemmas 2, 3 and 4, the inverseLaplacian L?is a valid Kernel.7.2.1 Similarity measures from the LaplacianThe pseudo inverse of the Laplacian allows usto compute the following similarity measures.1.
Since L?is a kernel, L?ijcan be interpreted asimilarity value of vertices i and j.2.
Commute time: This is due to (Aldous andFill, 2001).
The commute time, c(i, j) ?
(L?ii+ L?jj?
2L?ij).
This allows us to derivesimilarities using Equation 3.Evaluating the above measures with the Miller-Charles data yields results shown in Table 4.Again, these results are better than the other ran-dom walk methods compared in the paper.Method Spearman correlationL?ij0.469Commute Time (simC) 0.520Table 4: Similarity via inverse Laplacian.7.2.2 Noise in the graph construction processThe graph construction process outlined in Sec-tion 5 is not necessarily the best one.
In fact, anymethod that constructs graphs from existing dataincorporates ?noise?
or extraneous features.
Thesecould be spurious edges between vertices, miss-ing edges, or even improper edge weights.
It ishowever impossible to know any of this a prioriand some noise is inevitable.
The derivation ofcommute times via the pseudo inverse of a noisyLaplacian matrix makes it even worse because thepseudo inverse amplifies the noise in the originalmatrix.
This is because the largest singular valueof the pseudo inverse of a matrix is equal to the in-verse of the smallest singular value of the originalmatrix.
A standard technique in signal processingand information retrieval to eliminate noise or han-dle missing values is to use singular value decom-position (Deerwester et al, 1990).
We apply SVDto handle noise in the graph construction process.For a given matrix A, SVD decomposes A intothree matrices U, S, and V such that A = USVT,where S is a diagonal matrix of eigenvalues of A,and U and V are orthonormal matrices containingthe left and the right eigenvectors respectively.
Thetop-k eigenvectors and eigenvalues are computedusing the iterative method by Lanczos-Arnoldi (us-ing LAPACK) and the product of these matricesrepresents a ?smoothed?
version of the originalLaplacian.
The pseudo inverse is then computedon this smooth Laplacian.
Table 5., shows the im-provements obtained by discarding bottom 20% ofthe eigenvalues.Method Original After SVDL?ij0.469 0.472Commute Time (simC) 0.520 0.542Table 5: Denoising graph Laplacian via SVDFigure 3. shows the dependence on the num-ber of eigenvalues selected.
As can be observed inboth curves there is a reduction in performance byadding the last few eigenvectors and hence may besafely discarded.
This observation is true in othertext processing tasks like document clustering orclassification using Latent Semantic Indexing.8 Related WorkApart from the related work cited throughout thispaper, we would also like to note the paper by Yen46Figure 3: Noise reduction via SVD.et al(2007) on using sigmoid commute time kernelon a graph for document clustering but our workdiffers in that our goal was to study various ran-dom walk measures rather than a specific task andwe provide a new similarity measure (ref.
Eqn3) based on an upper bound on the commute time(Lemma 1).
Our work also suggests a way to han-dle noise in the graph construction process.9 Conclusions and Future WorkThis paper presented an evaluation of randomwalk based similarity measures on weighted undi-rected graphs.
We provided an intuitive explana-tion of why random walk based measures performbetter than shortest-path or geodesic measures,and backed it with empirical evidence.
The ran-dom walk measures we consider include boundedlength walks, pagerank based measures, and a newmeasure based on the commute times in graphs.We derived the commute times via pseudo inverseof the graph Laplacian.
This enables a new methodof graph similarity using SVD that is robust to thenoise in the graph construction process.
Further,the inverse Laplacian is also interesting in that it isa kernel by itself and could be used for other taskslike word clustering, for example.AcknowledgementsThe authors would like to thank David Smith andPetros Drineas for useful discussions and to FanChung for the wonderful book on Spectral Graphtheory.ReferencesAldous and Fill.
2001.
Reversible Markov Chains andRandom Walks on Graphs.
In preparation.Belkin, Mikhail and Partha Niyogi.
2001.
Laplacianeigenmaps and spectral techniques for embeddingand clustering.
In Proceedings of the NIPS.Blatz, John, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, Alberto San-chis, and Nicola Ueffing.
2004.
Confidence estima-tion for machine translation.
In Proceeding of theCOLING.Budanitsky, Alexander and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.Chandra, Ashok, Prabhakar Raghavan, Walter Ruzzo,Roman Smolensky, and Prasoon Tiwari.
1989.
Theelectrical resistance of a graph captures its commuteand cover times.
In Proceedings of the STOC.Chung, Fan.
1997.
Spectral graph theory.
In CBMS:Conference Board of the Mathematical Sciences, Re-gional Conference Series.Deerwester, Scott, Susan Dumais, George Furnas,Thomas Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41.Erkan, G?unes and Dragomir Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
Journal of Artificial Intelligence Re-search (JAIR), 22:457?479.Esuli, Andrea and Fabrizio Sebastiani.
2007.
Pager-anking wordnet synsets: An application to opinionmining.
In Proceedings of the ACL, pages 424?431.Fellbaum, Christaine, editor.
1989.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.Hughes, Thad and Daniel Ramage.
2007.
Lexicalsemantic relatedness with random graph walks.
InProceedings of the EMNLP.Kauchak, David and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In ProceedingsHLT-NAACL.Klein, D. and M. Randic.
1993.
Resistance distance.Journal of Mathematical Chemistry, 12:81?95.Lin, Jianhua.
1991.
Divergence measures based on theshannon entropy.
IEEE Transactions on InformationTheory, 37(1).Miller, G. and W. Charles.
1991.
Contextual correlatesof semantic similarity.
In Language and CognitiveProcess.Page, Larry, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1998.
The pagerank citation ranking:Bringing order to the web.
Technical report, Stan-ford University, Stanford, CA.47Patwardhan, Siddharth, Satanjeev Banerjee, and TedPedersen.
2005.
Senserelate:: Targetword-A gen-eralized framework for word sense disambiguation.In Proceedings of the ACL.Pedersen, Ted, Siddharth Patwardhan, and JasonMichelizzi.
2004.
Wordnet::similarity - measuringthe relatedness of concepts.
In Proceedings of theAAAI.Pereira, Fernando, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.
InProceedings of the ACL.Prager, John M., Jennifer Chu-Carroll, and KrzysztofCzuba.
2001.
Use of wordnet hypernyms for an-swering what-is questions.
In Proceedings of theText REtrieval Conference.Roweis, Sam and Lawrence Saul.
2000.
Nonlinear di-mensionality reduction by locally linear embedding.Science, 290:2323?2326.Xiao, W. and I. Gutman.
2003.
Resistance distance andlaplacian spectrum.
Theoretical Chemistry Associa-tion, 110:284?289.Yen, Luh, Francois Fouss, Christine Decaestecker, Pas-cal Francq, and Marco Saerens.
2007.
Graph nodesclustering based on the commute-time kernel.
InProceedings of the PAKDD.Zhu, Xiaojin, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised learning using gaussianfields and harmonic functions.
In Proceedings of theICML.48
