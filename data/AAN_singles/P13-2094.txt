Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 526?531,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCo-Regression for Cross-Language Review Rating PredictionXiaojun WanInstitute of Computer Science and Technology, The MOE Key Laboratory ofComputational Linguistics, Peking University, Beijing 100871, Chinawanxiaojun@pku.edu.cnAbstractThe task of review rating prediction can bewell addressed by using regression algorithmsif there is a reliable training set of reviewswith human ratings.
In this paper, we aim toinvestigate  a more challenging task of cross-language review rating prediction, whichmakes use of only rated reviews in a sourcelanguage (e.g.
English) to predict the ratingscores of unrated reviews in a target language(e.g.
German).
We propose a new co-regression algorithm to address this task byleveraging unlabeled reviews.
Evaluation re-sults on several datasets show that our pro-posed co-regression algorithm can consistentlyimprove the prediction results.1 IntroductionWith the development of e-commerce, more andmore people like to buy products on the web andexpress their opinions about the products bywriting reviews.
These reviews usually containvaluable information for other people?s referencewhen they buy the same or similar products.
Insome applications, it is useful to categorize a re-view into either positive or negative, but in manyreal-world scenarios, it is important to providenumerical ratings rather than binary decisions.The task of review rating prediction aims toautomatically predict the rating scores of unratedproduct reviews.
It is considered as a finer-grained task than the binary sentiment classifica-tion task.
Review rating prediction has beenmodeled as a multi-class classification or regres-sion task, and the regression based methods haveshown better performance than the multi-classclassification based methods in recent studies (Liet al 2011).
Therefore, we focus on investigatingregression-based methods in this study.Traditionally, the review rating prediction taskhas been investigated in a monolingual setting,which means that the training reviews with hu-man ratings and the test reviews are in the samelanguage.
However, a more challenging task is topredict the rating scores of the reviews in a targetlanguage (e.g.
German) by making use of therated reviews in a different source language (e.g.English), which is called Cross-Language Re-view Rating Prediction.
Considering that the re-sources (i.e.
the rated reviews) for review ratingprediction in different languages are imbalanced,it would be very useful to make use of the re-sources in resource-rich languages to help ad-dress the review rating prediction task in re-source-poor languages.The task of cross-language review rating pre-diction can be typically addressed by using ma-chine translation services for review translation,and then applying regression methods based onthe monolingual training and test sets.
However,due to the poor quality of machine translation,the reviews translated from one language A toanother language B are usually very differentfrom the original reviews in language B, becausethe words or syntax of the translated reviewsmay be erroneous or non-native.
This phenome-non brings great challenges for existing regres-sion algorithms.In this study, we propose a new co-regressionalgorithm to address the above problem by lever-aging unlabeled reviews in the target language.Our algorithm can leverage both views of thereviews in the source language and the targetlanguage to collaboratively determine the confi-dently predicted ones out of the unlabeled re-views, and then use the selected examples toenlarge the training set.
Evaluation results onseveral datasets show that our proposed co-regression algorithm can consistently improvethe prediction results.2 Related WorkMost previous works on review rating predictionmodel this problem as a multi-class classificationtask or a regression task.
Various features havebeen exploited from the review text, includingwords, patterns, syntactic structure, and semantictopic (Qu et al 2010; Pang and Lee, 2005; Leunget al 2006; Ganu et al 2009).
Traditional learn-526ing models, such as SVM, are adopted for ratingprediction.
Most recently, Li et al (2011) pro-pose a novel tensor-based learning framework toincorporate reviewer and product informationinto the text based learner for rating prediction.Saggion et al (2012) study the use of automatictext summaries instead of the full reviews formovie review rating prediction.
In addition topredicting the overall rating of a full review,multi-aspect rating prediction has also been in-vestigated (Lu et al 2011b; Snyder and Barzilay,2007; Zhu et al 2009; Wang et al 2010; Lu et al2009; Titov and McDonald, 2008).
All the aboveprevious works are working under a monolingualsetting, and to the best of our knowledge, thereexists no previous work on cross-language re-view rating prediction.It is noteworthy that a few studies have beenconducted for the task of cross-lingual sentimentclassification or text classification, which aims tomake use of labeled data in a language for thebinary classification task in a different language(Mihalcea et al, 2007; Banea et al, 2008; Wan2009; Lu et al 2011a; Meng et al 2012; Shi etal., 2010; Prettenhofer and Stein 2010).
However,the binary classification task is very differentfrom the regression task studied in this paper,and the proposed methods in the above previousworks cannot be directly applied.3 Problem Definition and Baseline Ap-proachesLet L={(x1, y1), ?, (xi, yi), ?, (xn, yn)} denote thelabeled training set of reviews in a source lan-guage (e.g.
English), where xi is the i-th reviewand yi is its real-valued label, and n is the numberof labeled examples; Let T denote the test reviewset in a different target language (e.g.
German);Then the task of cross-language review ratingprediction aims at automatically predicting therating scores of the reviews in T by leveragingthe labeled reviews in L. No labeled reviews inthe target language are allowed to be used.The task is a regression problem and it is chal-lenging due to the language gap between the la-beled training dataset and the test dataset.
Fortu-nately, due to the development of machine trans-lation techniques, a few online machine transla-tion services can be used for review translation.We adopt Google Translate1 for review transla-tion.
After review translation, the training re-views and the test reviews are now in the same1 http://translate.google.comlanguage, and any regression algorithm (e.g.
lo-gistic regression, least squares regression, KNNregressor) can be applied for learning and predic-tion.
In this study, without loss of generality, weadopt the widely used regression SVM (Vapnik1995; Joachims 1999) implemented in theSVMLight toolkit 2  as the basic regressor.
Forcomparative analysis, we simply use the defaultparameter values in SVMLight with linear kernel.The features include all unigrams and bigrams inthe review texts, and the value of each feature issimply set to its frequency (TF) in a review.Using features in different languages, we havethe following baseline approaches for addressingthe cross-language regression problem.REG_S:  It conducts regression learning andprediction in the source language.REG_T: It conducts regression learning andprediction in the target language.REG_ST: It conducts regression learning andprediction with all the features in both languages.REG_STC: It combines REG_S and REG_Tby averaging their prediction values.However, the above regression methods do notperform very well due to the unsatisfactory ma-chine translation quality and the various lan-guage expressions.
Therefore, we need to findnew approaches to improve the above methods.4 Our Proposed Approach4.1 OverviewOur basic idea is to make use of some amountsof unlabeled reviews in the target language toimprove the regression performance.
Consider-ing that the reviews have two views in two lan-guages and inspired by the co-training style algo-rithms (Blum and Mitchell, 1998; Zhou and Li,2005), we propose a new co-training style algo-rithm called co-regression to leverage the unla-beled data in a collaborative way.
The proposedco-regression algorithm can make full use ofboth the features in the source language and thefeatures in the target language in a unifiedframework similar to (Wan 2009).
Each reviewhas two versions in the two languages.
Thesource-language features and the target-languagefeatures for each review are considered two re-dundant views of the review.
In the trainingphase, the co-regression algorithm is applied tolearn two regressors in the two languages.
In theprediction phase, the two regressors are appliedto predict two rating scores of the review.
The2 http://svmlight.joachims.org527final rating score of the review is the average ofthe two rating scores.4.2 Our Proposed Co-Regression AlgorithmIn co-training for classification, some confidentlyclassified examples by one classifier are pro-vided for the other classifier, and vice versa.Each of the two classifiers can improve by learn-ing from the newly labeled examples providedby the other classifier.
The intuition is the samefor co-regression.
However, in the classificationscenario, the confidence value of each predictioncan be easily obtained through consulting theclassifier.
For example, the SVM classifier pro-vides a confidence value or probability for eachprediction.
However, in the regression scenario,the confidence value of each prediction is notprovided by the regressor.
So the key question ishow to get the confidence value of each labeledexample.
In (Zhou and Li, 2005), the assumptionis that the most confidently labeled example of aregressor should be with such a property, i.e.
theerror of the regressor on the labeled example set(i.e.
the training set) should decrease the most ifthe most confidently labeled example is utilized.In other words, the confidence value of each la-beled example is measured by the decrease of theerror (e.g.
mean square error) on the labeled setof the regressor utilizing the information pro-vided by the example.
Thus, each example in theunlabeled set is required to be checked by train-ing a new regression model utilizing the example.However, the model training process is usuallyvery time-consuming for many regression algo-rithms, which significantly limits the use of thework in (Zhou and Li, 2005).
Actually, in (Zhouand Li, 2005), only the lazy learning based KNNregressor is adopted.
Moreover, the confidenceof the labeled examples is assessed based only onthe labeled example set (i.e.
the training set),which makes the generalization ability of theregressor not good.In order to address the above problem, wepropose a new confidence evaluation strategybased on the consensus of the two regressors.Our intuition is that if the two regressors agreeon the prediction scores of an example very well,then the example is very confidently labeled.
Onthe contrary, if the prediction scores of an exam-ple by the two regressors are very different, wecan hardly make a decision whether the exampleis confidently labeled or not.
Therefore, we usethe absolute difference value between the predic-tion scores of the two regressors as the confi-dence value of a labeled example, and if the ex-ample is chosen, its final prediction score is theaverage of the two prediction scores.
Based onthis strategy, the confidently labeled examplescan be easily and efficiently chosen from theunlabeled set as in the co-training algorithm, andthese examples are then added into the labeledset for re-training the two regressors.Given:- Fsource and Ftarget are redundantly sufficientsets of features, where Fsource representsthe source language features, Ftarget repre-sents the target language features;- L is a set of labeled training reviews;- U is a set of unlabeled reviews;Loop for I iterations:1.
Learn the first regressor Rsource from Lbased on Fsource;2.
Use Rsource to label reviews from U basedon Fsource; Letsourceiy?
denote the predic-tion score of review xi;3.
Learn the second classifier Rtarget from Lbased on Ftarget;4.
Use Rtarget to label reviews from U basedon Ftarget; Letettiyarg?
denote the predic-tion score of review xi;5.
Choose m most confidently predicted re-views E={ top m reviews with the small-est value of sourceietti yy ??arg? }
from U,where the final prediction score of eachreview in E is 2??
arg sourceietti yy + ;6.
Removes reviews E from U and add re-views E with the corresponding predic-tion scores to L;Figure 1.
Our proposed co-regression algorithmOur proposed co-regression algorithm is illus-trated in Figure 1.
In the proposed co-regressionalgorithm, any regression algorithm can be usedas the basic regressor to construct Rsource and Rtar-get, and in this study, we adopt the same regres-sion SVM implemented in the SVMLight toolkitwith default parameter values.
Similarly, the fea-tures include both unigrams and bigrams and thefeature weight is simply set to term frequency.There are two parameters in the algorithm: I isthe iteration number and m is the growth size ineach iteration.
I and m can be empirically set ac-cording to the total size of the unlabeled set U,and we have I?m?
|U|.Our proposed co-regression algorithm is muchmore efficient than the COREG algorithm (Zhouand Li, 2005).
If we consider the time-consuming regression learning process as one528(a) Target language=German & Category=books1.11.121.141.161.181.21.221 10 20 30 40 50 60 70 80 90 100110120130140150Iteration Number (I)MSE(b) Target language=German & Category=dvd1.11.121.141.161.181.21.221.241.261 10 20 30 40 50 60 70 80 90 100110120130140150Iteration Number (I)MSE(c) Target language=German & Category=music1.191.211.231.251.271.291.311.331.351 10 20 30 40 50 60 70 80 90 100110120130140150Iteration Number (I)MSERsourceRtargetco-regressionREG_SREG_TREG_STREG_STCCOREGFigure 2.
Comparison results vs. Iteration Number (I) (Rsource and Rtarget are the two component regressors)basic operation and make use of all unlabeledexamples in U, the computational complexity ofCOREG is O(|U|+I).
By contrast, the computa-tional complexity of our proposed co-regressionalgorithm is just O(I).
Since |U| is much largerthan I, our proposed co-regression algorithm ismuch more efficient than COREG, and thus ourproposed co-regression algorithm is more suit-able to be used in applications with a variety ofregression algorithms.Moreover, in our proposed co-regression algo-rithm, the confidence of each prediction is de-termined collaboratively by two regressors.
Theselection is not restricted by the training set, andit is very likely that a portion of good examplescan be chosen for generalize the regressor to-wards the test set.5 Empirical EvaluationWe used the WEBIS-CLS-10 corpus3 providedby (Prettenhofer and Stein, 2010) for evaluation.It consists of Amazon product reviews for threeproduct categories (i.e.
books, dvds and music)written in different languages including English,German, etc.
For each language-category pairthere exist three sets of training documents, testdocuments, and unlabeled documents.
The train-ing and test sets comprise 2000 documents each,whereas the number of unlabeled documents var-ies from 9000 ?
170000.
The dataset is providedwith the rating score between 1 to 5 assigned byusers, which can be used for the review ratingprediction task.
We extracted texts from both thesummary field and the text field to represent areview text.
We then extracted the rating score asa review?s corresponding real-valued label.
Inthe cross-language scenario, we regarded Englishas the source language, and regarded German asthe target language.
The experiments were con-ducted on each product category separately.Without loss of generality, we sampled and used3 http://www.uni-weimar.de/medien/webis/research/corpora/corpus-webis-cls-10.htmlonly 8000 unlabeled documents for each productcategory.
We use Mean Square Error (MSE) asthe evaluation metric, which penalizes more se-vere errors more heavily.In the experiments, our proposed co-regressionalgorithm (i.e.
?co-regression?)
is compared withthe COREG algorithm in (Zhou and Li, 2005)and a few other baselines.
For our proposed co-regression algorithm, the growth size m is simplyset to 50.
We implemented the COREG algo-rithm by replacing the KNN regressor with theregression SVM and the pool size is also set to50.
The iteration number I varies from 1 to 150.The comparison results are shown in Figure 2.We can see that on all product categories, theMSE values of our co-regression algorithm andthe two component regressors tend to declineover a wide range of I, which means that the se-lected confidently labeled examples at each itera-tion are indeed helpful to improve the regressors.Our proposed co-regression algorithm outper-forms all the baselines (including COREG) overdifferent iteration members, which verifies theeffectiveness of our proposed algorithm.
We canalso see that the COREG algorithm does not per-form well for this cross-language regression task.Overall, our proposed co-regression algorithmcan consistently improve the prediction results.6 Conclusion and Future WorkIn this paper, we study a new task of cross-language review rating prediction and propose anew co-regression algorithm to address this task.In future work, we will apply the proposed co-regression algorithm to other cross-language orcross-domain regression problems in order toverify its robustness.AcknowledgmentsThe work was supported by NSFC (61170166),Beijing Nova Program (2008B03) and NationalHigh-Tech R&D Program (2012AA011101).529ReferencesCarmen Banea, Rada Mihalcea, Janyce Wiebe, andSamer Hassan.
2008.
Multilingual subjectivityanalysis using machine translation.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pp.
127-135.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classifi-cation.
In Annual Meeting-Association For Com-putational Linguistics.Avrim Blum and Tom Mitchell.
1998.
Combininglabeled and unlabeled data with co-training.
In Pro-ceedings of the eleventh annual conference onComputational learning theory, pp.
92-100.Hang Cui, Vibhu Mittal, and Mayur Datar.
2006.Comparative experiments on sentiment classifica-tion for online product reviews.
In Proceedings ofthe National Conference on Artificial Intelligence.Gayatree Ganu, Noemie Elhadad, and Am?lie Marian.2009.
Beyond the stars: Improving rating predic-tions using review text content.
In WebDB.Thorsten Joachims, 1999.
Making large-Scale SVMLearning Practical.
Advances in Kernel Methods -Support Vector Learning, MIT-Press.CaneWing Leung, Stephen Chi Chan, and Fu Chung.2006.
Integrating collaborative filtering and senti-ment analysis: A rating inference approach.
InECAI Workshop, pages 300?307.Fangtao Li, Nathan Liu, Hongwei Jin, Kai Zhao,Qiang Yang and Xiaoyan Zhu.
2011.
Incorporatingreviewer and product information for review ratingprediction.
In Proceedings of the Twenty-SecondInternational Joint Conference on Artificial Intelli-gence (IJCAI2011).Yue Lu, ChengXiang Zhai, Neel Sundaresan.
2009.Rated Aspect Summarization of Short Comments.Proceedings of the World Wide Conference 2009( WWW'09), pages 131-140.Bin Lu, Chenhao Tan, Claire Cardie, Ka Yin Benja-min TSOU.
2011a.
Joint bilingual sentiment classi-fication with unlabeled parallel corpora.
In Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: HumanLanguage Technologies, pp.
320-330.Bin Lu, Myle Ott, Claire Cardie and Benjamin K.Tsou.
2011b.
Multi-aspect sentiment analysis withtopic models.
In Proceedings of Data MinigWorkshps (ICDMW), 2011 IEEE 11th Interna-tional Conference on, pp.
81-88, IEEE.Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,Ge Xu, and Houfeng Wang.
2012.
Cross-LingualMixture Model for Sentiment Classification.
InProceedings of ACL-2012.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.2007.
Learning multilingual subjective languagevia cross-lingual projections.
In Proceedings ofACL-2007.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in natu-ral language processing-Volume 10, pp.
79-86,2002.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
In Proceedings of theACL, pages 115?124.Peter Prettenhofer and Benno Stein.
2010.
Cross-Language Text Classification using Structural Cor-respondence Learning.
In 48th Annual Meeting ofthe Association of Computational Linguistics(ACL 10), 1118-1127.Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.2010.
The bag-of-opinions method for review rat-ing prediction from sparse text patterns.
In COL-ING, pages 913?921, Stroudsburg, PA, USA, 2010.ACL.Horacio Saggion, Elena Lloret, and Manuel Palomar.2012.
Can text summaries help predict ratings?
acase study of movie reviews.
Natural LanguageProcessing and Information Systems (2012): 271-276.Lei Shi, Rada Mihalcea, and Mingjun Tian.
2010.Cross language text classification by model transla-tion and semi-supervised learning.
In Proceedingsof the 2010 Conference on Empirical Methods inNatural Language Processing, pp.
1057-1067, 2010.Benjamin Snyder and Regina Barzilay.
2007.
Multi-ple aspect ranking using the good grief algorithm.Proceedings of the Joint Human Language Tech-nology/North American Chapter of the ACL Con-ference (HLT-NAACL).Ivan Titov and Ryan McDonald.
2008.
A joint modelof text and aspect ratings for sentiment summariza-tion.
In Proceedings of ACL-08:HLT, pages 308-316.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of the 40th An-nual Meeting on Association for ComputationalLinguistics, pp.
417-424.Vladimir N. Vapnik, 1995.
The Nature of StatisticalLearning Theory.
Springer.Xiaojun Wan.
2009.
Co-training for cross-lingualsentiment classification.
In Proceedings of the JointConference of the 47th Annual Meeting of theACL and the 4th International Joint Conference on530Natural Language Processing of the AFNLP, pp.235-243.Hongning Wang, Yue Lu, ChengXiang Zhai.
2010.Latent Aspect Rating Analysis on Review TextData: A Rating Regression Approach.
Proceedingsof the 17th ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Mining(KDD'10), pages 115-124.Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, andMuhua Zhu.
2009.
Multi-aspect opinion pollingfrom textual reviews.
In Proceedings of the 18thACM conference on Information and knowledgemanagement, pp.
1799-1802.
ACM.Zhi-Hua Zhou and Ming Li.
2005.
Semi-supervisedregression with co-training.
In Proceedings of the19th international joint conference on Artificial in-telligence, pp.
908-913.
Morgan Kaufmann Pub-lishers Inc.531
