LAW VIII - The 8th Linguistic Annotation Workshop, pages 70?74,Dublin, Ireland, August 23-24 2014.A Corpus Study for Identifying Evidence on MicroblogsPaul Reisert1Junta Mizuno2Miwa Kanno1Naoaki Okazaki1,3Kentaro Inui11Gradute School of Information Sciences, Tohoku University / Miyagi, Japan2Resilient ICT Research Center, NICT / Miyagi, Japan3Japan Science and Technology Agency (JST) / Tokyo, Japanpreisert@ecei.tohoku.ac.jp junta-m@nict.go.jp {meihe, okazaki, inui}@ecei.tohoku.ac.jpAbstractMicroblogs are a popular way for users to communicate and have recently caught the attentionof researchers in the natural language processing (NLP) field.
However, regardless of their risingpopularity, little attention has been given towards determining the properties of discourse rela-tions for the rapid, large-scale microblog data.
Therefore, given their importance for various NLPtasks, we begin a study of discourse relations on microblogs by focusing on evidence relations.As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus studyto identify such relations on Twitter, a popular microblogging service.
We create annotationguidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidencerelations.
Finally, we report our observations, annotation difficulties, and data statistics.1 IntroductionMicroblogs have become a popular method for users to express their ideas and communicate with otherusers.
Twitter1, a popular microblogging service, has recently been the attraction of many natural lan-guage processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to genderinference for its users (Ciot et al., 2013).
While various tasks are available, despite its daily, rapid large-scale data, evidence relation studies have yet to be explored using Twitter data.
Previous research existsfor determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of thiswork is to determine and annotate evidence relations on microblogs.Our primary motivation behind focusing on evidence relations includes the possibility of discoveringsupport for a claim which can support the debunking of false information.
During the March 2011Great East Japan Earthquake and Tsunami disaster, victims turned to the Internet in order to obtaininformation on current conditions, such as family member whereabouts, refuge center information, andgeneral information (Sakaki et al., 2011).
However, false information, such as the popular Cosmo Oilexplosion causing toxic rain, interfered with those looking to find correct information on the status ofthe disaster areas (Okazaki et al., 2013).
This is a scenario in which identification of potentially falseinformation is necessary in order to provide accurate information to victims and others relying on andtrusting in the Internet.
Therefore, as a start to find support for counterclaims for false information suchas the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a postwith no parent; and a respondent who provides either an agreeing or disagreeing claim and support fortheir claim.
An example is provided in Figure 1.We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013;Mrozinski et al., 2008), which attempts to discover the answer for Why questions.
Given our task ofdiscovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we endup with a Why question similar to Why is it true/not true that X, where X is the contents of the claimfound in the parent post.
However, we consider source mentions or hyperlinks, which can either standalone or be contained in a statement, question, or request, as a way to answer the above question.To the best of our knowledge, no corpora for evidence relations on microblogs currently exists.
Interms of argumentation corpora, the Araucaria Argumentation Corpus2exists which utilizes variousargumentation schemes (Walton, 1996; Katzav and Reed, 2004; Pollock, 1995).
In this work, we1https://twitter.com2http://araucaria.computing.dundee.ac.uk/doku.phpThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/70# ,' (416;% !!&)0+"My cat is limping due to the heat.
But, if I keep the air conditioner on, he?ll catch a cold.$"+!
/.
-#$3297 :58; -*!Topic Starter TweetRespondent TweetA cat won?t catch a cold that way.
A cat cold is caused by a virus preventable by vaccine.
Figure 1: topic starter post and respondent post on the microbloging service Twitter.manually annotate evidence relation claim and support.
We conduct a corpus study that uses both currentdata and March 2011 data from Twitter, manually observing its structure and evidence, and devisingguidelines based on our findings.
We utilize these guidelines for conducting a large-scale annotationstage and develop a corpus with our results.
We present our findings, challenges in annotation, and alsothe result statistics in the later sections.
The corpora and annotation guidelines are currently available at:http://www.cl.ecei.tohoku.ac.jp/index.php?Open%20Resources%2FEvidence%20Relation%20Corpusw2 Annotation MethodIn this section, we describe evidence relation structure, target data, and our annotation method outline.Evidence relations, defined by Mann and Thompson (1988) consist of a claim, or something that anauthor wishes for a reader to believe, and support, or something that increases the believability of theclaim, and it can be understood by the following: The program as published for calendar year 1980really works.
In only a few minutes, I entered all the figures from my 1980 tax return and got a resultwhich agreed with my hand calculations to the penny., where the latter is support to the former claim.With this in mind, we aim to explore what type of claims and support units exist on microblogs.
Ourmicroblog choice is Twitter, where users post tweets containing up to 140 characters.
Tweets may thenbe replied to by other users.
Each pair in our corpus consists of, what we refer to as, a topic starter?stweet and all of its direct reply tweets, or respondent?s tweets.
The topic starter?s tweet is a top-leveltweet not in response to another tweet, and the respondent tweet consists of a tweet directly in reply tothe topic starter?s tweet.
We then discover respondent claims that agree or disagree with the topic starter.In addition, we target only pairs which contain an evidence relation.The outline for annotation is as follows: 1) Given two tweets (topic starter and respondent), detectrelation at agreeing or disagreeing level 2) Mark the claim and support in the respondent tweet??????????????????????????????????
?ClaimI?ve heard that rain will be dangerous due to the fire in Chiba, but is that true?Support?????????????????????http://...???????
?Counter ClaimThat?s false information spread from a few days ago.
Be cautious.
http://...Topic StarterRespondentEvidence RelationFigure 2: Evidence relation within a response.We target this scenario because: 1) we assume that a topic will be presented in a topic starter?s tweetand a respondent?s direct reply will be responding to the topic content, and 2) it is possible to loseimportant information, such as topic keywords, for a reply tweet not in reply to a topic starter tweet.Using this outline, we utilized Japanese Twitter data from the 2011 Great Eastern Tohoku Earthquake,specifically hottolink3data, and created a list of guidelines to use for our large-scale annotation in thenext section by manually observing roughly 6,000 tweet pairs.3 Large-scale AnnotationIn order to obtain and observe more evidence relations, we conducted a large-scale annotation stage.
Wediscuss the data and our statistics and observations.3.1 DataUsing the guidelines in the previous section, we composed a list of 56,033 filtered tweets from varioustime periods, shown in the table below.3http://www.hottolink.co.jp/press/93671Table 1: Data for large-scale annotation phase (A = Agree, D = Disagree, P = Partly A/D, O = Other)# Set Pairs Evidence A D P O1 3-11 False Rumor topic starter Data 5753 1029 177 637 74 1412 Togetter Controversial Category Data 2410 283 164 105 12 23 Togetter Negative Tag Data 1233 129 51 71 7 04 Twitter Random Controversial Topic Filtered Data 6918 277 168 94 14 15 3-11 Random Data 13064 381 241 115 21 46 3-11 Negative respondent Keyword Data 26655 1543 836 521 126 60Total 56033 3642 1637 1543 254 208For Sets 1, 5, 6, we utilize the hottolink corpus mentioned briefly in the previous section.
Set 1consists of filtered pairs containing a well-known rumored topic from a list of 10 topics, such as CosmoOil Toxic Rain and Drinking Isodine for Radiation Prevention, and also contained a negative keyword inthe respondent?s tweet.
We also included all other direct replies for the topic starter?s tweet.
Similarly,Set 5 contains random data from the hottolink corpus, unfiltered, and Set 6 contains pairs filtered via anegative keyword in the reply only.Set 2 consists of crawled data from Togetter4.
Togetter offers a summarization of popular, and poten-tially controversial, tweets for various categories, such as news, society, and sports.
We first crawled allpopular categories around January 2014 and obtained unique tweet IDs.
We then used the Twitter API5to extract the tweet information from its ID in order to determine if it was a direct respondent tweet.
Ifso, we obtained its topic starter tweet and thus created our pairs.For Set 3, we appended negative keywords to the Togetter hyperlink (e.g.
http://togetter.com/t/??
)in order to obtain tweets that had been tagged with a negative keyword.
We then used the same procedureas Set 2 in order to obtain topic starter and respondent tweet pairs.Finally, Set 4 consists of 6,918 tweet pairs randomly selected from a collection of tweet pairs fromTogetter, where each topic starter tweet is filtered by a topic from a list of around 300 controversialtopics.3.2 Statistics and ObservationsIn this section, we summarize the results of the annotated large-scale corpus by first providing infor-mation on the discovered evidence relations.
Of 56,033 pairs, 3,642, or roughly 6.5%, were labeled ascontaining an evidence relation.
Shown in Table 1 are the specific amount of evidence relations foundin each set, along with the exact amount of claims that either agree, disagree, partly agree and disagree,and other.
Also shown in Table 1 is the number of agreeing, disagreeing, partly agreeing/disagreeing,and other statistics for pairs labeled as evidence for each set.3.2.1 Type DistributionSince an important goal of this paper was to deter-mine what types of claims and support we would dis-cover, we classified random annotated tweet pairs byclaim type and support type.attitude Claim contains only reply user attitude(e.g., ?I agree with you?
or ?It?s false information?
)request Claim requests some action (e.g., ?Pleasedelete and correct your tweet immediately?
)question Claim is a question regarding the originaltweet (e.g., ?Why do you think so??
or only ???
)statement Claim is an opinion of a reply user (e.g.,?Radiation cannot be reduced by a normal filter.?
)Table 2: Type distribution resultsClaim Type Support Type Disaster Generalattitudecausality 36 27elaboration 45 40source 35 7requestcausality 13 9elaboration 4 8source 15 0statementcausality 21 22elaboration 49 31source 12 7questioncausality 1 2elaboration 3 9source 2 0summary 236 1624http://togetter.com5https://dev.twitter.com/docs/api72Each of the three types of support (below) are in square brackets.causality Support is a reason of a claim (e.g., ?Isodine is no good because [it will ruin your health]?
)elaboration Support is not a reason of a claim, but an elaboration (e.g., ?topic starter: I definitely donot ride side by side with a car when I?m on my bicycle.
respondent: Me too.
[I do not ride side by sideeven when I ride a motorbike]?
)source Support contains source information of the claim, such as hyperlink and name of the media(e.g., ?Please read this web site [URL]?
or ?I saw it on the TV?
)From Table 2, we found many source samples during disaster times but not for non-disaster periods.For our second finding, we discovered that attitude and statement were tweeted with support, whilerequest and question were not.
This indicates that people require some action without any support.
Forour third finding, we found that there were many replies which contain a statement and its support, whileTwitter allows only 140 characters.
This indicates many informative support segments on Twitter.3.2.2 Annotation IssuesBelow we enumerate issues that were encountered during our annotation process.Reliability For determining annotation reliability, we had 10% of random samples from Set 1 anno-tated by another annotator and found that the inter-annotator agreement Cohen?s kappa value was only.476.
Both annotators marked 45 of the same pairs as evidence.
Annotator A marked 60 other pairsas evidence, while Annotator B marked 15 other pairs as evidence.
We believe this statistic is becausetweets with evidence were infrequent and that many examples contained implicit relations, opposed tocontaining a discourse marker.
From Annotator A?s results, we found that only 9 examples contained anexplicit discourse marker and 96 did not.
Prasad et al.
(2008) has already recognized that it is difficultto annotate relations when no discourse marker is present.
We plan to automatically annotate evidencerelations via machine learning and provide a probability that a pair is evidence to help manual annotation.Multiple Claims With Twitter?s character constraints, we expected to discover only one claim perreply with multiple support segments.
However, we found that a few of our annotated segments containedmultiple claim and multiple support segments.Range Annotation range was a problem we discovered after observing our annotated data.
Althoughsuch annotated cases were small (only 2 respondent tweets), most likely due to annotators avoiding suchannotations, we still believe this type of annotation is important for future work.
The example below waslabeled as unsure:{????????}CLAIM{??HP?}SUPPORT{?????????????
}CLAIM({TheCosmo Oil case,}CLAIM{on the official HP,}SUPPORT{is publicly announced false.
}CLAIM)4 Conclusion and Future WorkAs no corpora exists for evidence relations on microblogs, we conducted a corpus study usingthe popular microblogging service, Twitter.
We created a list of guidelines for evidence rela-tion annotation by observing roughly 6,000 tweet pairs from March 2011 Twitter data, or disaster-specific data.
Next, we conducted a large-scale annotation stage, consisting of 56,033 tweets, anddiscovered 3,642 contained a type of evidence relation.
Our annotated data set is available at:http://www.cl.ecei.tohoku.ac.jp/index.php?Open%20Resources%2FEvidence%20Relation%20CorpusWe manually observed that the presence of evidence relations do indeed exist on microblogs; however,their existence is rather infrequent.
To address this sparsity issue for future annotation, we plan toincrease the number of pairs containing an evidence relation per data set by constructing a model thatcan automatically annotate evidence relations and provide a probability that a pair contains an evidencerelation.
In this work, we did not analyze the quality of evidence we discovered.
Therefore, we aimtowards determining the factuality, or degree of certainty, for a given claim and support in order todetermine the evidence relation?s overall quality.73AcknowledgmentsWe would like to acknowledge MEXT (Ministry of Education, Culture, Sports, Science and Technol-ogy) for their generous financial support via the Research Student Scholarship.
This study was partlysupported by Japan Society for the Promotion of Science (JSPS) KAKENHI Grant No.
23240018 andJapan Science and Technology Agency (JST).
Furthermore, we would like to also thank Eric Nichols(Honda Research Institute Japan Co., Ltd.) for his discussions on the topic of evidence relations.ReferencesEiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011.
Twitter catches the flu: Detecting influenza epi-demics using Twitter.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing,EMNLP ?11, pages 1568?1576.Carlos Castillo, Marcelo Mendoza, and Barbara Poblete.
2011.
Information credibility on Twitter.
In Proceedingsof the 20th International Conference on World Wide Web, WWW ?11, pages 675?684.Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013.
Gender inference of Twitter users in non-Englishcontexts.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages1136?1145.Joel Katzav and Chris Reed.
2004.
A classification system for arguments.
Technical Report.William C. Mann and Sandra A. Thompson.
1988.
Rhetorical structure theory: Toward a functional theory of textorganization.
Text, 8(3):243?281.Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.
2008.
Collecting a why-question corpus for develop-ment and evaluation of an automatic QA-system.
In Proceedings of ACL-08: HLT, pages 443?451.Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013.Why-question answering using intra- and inter-sentential causal relations.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1733?1743.Naoaki Okazaki, Keita Nabeshima, Kento Watanabe, Junta Mizuno, and Kentaro Inui.
2013.
Extracting andaggregating false information from microblogs.
In Proceedings of the Workshop on Language Processing andCrisis Information, pages 36?43.John L. Pollock.
1995.
Cognitive Carpentry: A Blueprint for How to Build a Person.
MIT Press.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.2008.
The penn discourse treebank 2.0.
In Proceedings of the Sixth International Conference on LanguageResources and Evaluation (LREC?08).Takeshi Sakaki, Fujio Toriumi, and Yutaka Matsuo.
2011.
Tweet trend analysis in an emergency situation.
InProceedings of the Special Workshop on Internet and Disasters, SWID ?11, pages 3:1?3:8.Suzan Verberne.
2006.
Developing an approach for why-question answering.
In Proceedings of the Eleventh Con-ference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop,EACL ?06, pages 39?46.Douglas N. Walton.
1996.
Argumentation Schemes for Presumptive Reasoning.
Psychology Press.74
