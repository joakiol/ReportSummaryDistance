Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 175?182,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Gaming Fluency: Evaluating the Bounds and Expectations ofSegment-based Translation MemoryJohn Henderson and William MorganThe MITRE Corporation202 Burlington RoadBedford, MA 01730{jhndrsn,wmorgan}@mitre.orgAbstractTranslation memories provide assis-tance to human translators in produc-tion settings, and are sometimes usedas first-pass machine translation in as-similation settings because they pro-duce highly fluent output very rapidly.In this paper, we describe and eval-uate a simple whole-segment transla-tion memory, placing it as a new lowerbound in the well-populated space ofmachine translation systems.
The re-sult is a new way to gauge how far ma-chine translation has progressed com-pared to an easily understood baselinesystem.The evaluation also sheds light on theevaluation metric and gives evidenceshowing that gaming translation withperfect fluency does not fool bleu theway it fools people.1 Introduction and backgroundTranslation Memory (TM) systems provideroughly concordanced results from an archive ofpreviously translated materials.
They are typ-ically used by translators who want computerassistance for searching large archives for trickytranslations, and also to help ensure a groupof translators rapidly arrive at similar terminol-ogy (Macklovitch et al, 2000).
Several compa-nies provide commercial TMs and systems forusing and sharing them.
TMs can add value tocomputer assisted translation services (Drugan,2004).Machine Translation (MT) developers makeuse of similar historical archives (parallel texts,bitexts), to produce systems that perform a taskvery similar to TMs.
But while TM systemsand MT systems can appear strikingly simi-lar, (Marcu, 2001) key differences exist in howthey are used.TMs often need to be fast because they aretypically used interactively.
They aim to pro-duce highly readable, fluent output, usable indocument production settings.
In this setting,errors of omission are more easily forgiven thanerrors of commission so, just like MT, TM out-put must look good to users who have no accessto the information in source texts.MT, on the other hand, is often used in as-similation settings, where a batch job can of-ten be run on multiple processors.
This permitsvariable rate output and allows slower systemsthat produce better translations to play a part.Batch MT serving a single user only needs to runat roughly the same rate the reader can consumeits output.Simple TMs operate on an entire translationsegment, roughly the size of a sentence or two,while more sophisticated TMs operate on unitsof varying size: a word, a phrase, or an entiresegment (Callison-Burch et al, 2004).
Mod-ern approaches to MT, especially statistical MT,typically operate on more fine-grained units,words and phrases (Och and Ney, 2004).
The re-lationship between whole segment TM and MTcan be viewed as a continuum of translationgranularity:175-ffsegmentssimple TMwordsMThybrid TMSimple TM systems, focusing on segment-levelgranularity, lie at one extreme, and word-for-word, IBM-model MT systems on theother.
Example-Based MT (EBMT), phrase-based, and commercial TM systems likely liesomewhere in between.This classification motivates our work here.MT systems have well-studied and popular eval-uation techniques such as bleu (Papineni et al,2001).
In this paper we lay out a methodologyfor evaluating TMs along the lines of MT evalu-ation.
This allows us to measure the raw relativevalue of TM and MT as translation tools, and todevelop expectations for how TM performanceincreases as the size of the memory increases.There are many ways to perform TM segmen-tation and phrase extraction.
In this study, weuse the most obvious and simple condition?afull segment TM.
This gives a lower bound onreal TM performance, but a lower bound whichis not trivial.Section 2 details the architecture of our simpleTM.
Section 3 describes experiments involvingdifferent strategies for IR, oracle upper boundson TM performance as the memory grows, andtechniques for rescoring the retrievals.
Section 4discusses the results of the experiments.2 A Simple Chinese-EnglishTranslation MemoryFor our experiments below, we constructed asimple translation memory from a sentence-aligned parallel corpus.
The system consists ofthree stages.
A source-language input string isrewritten to form an information retrieval (IR)query.
The IR engine is called to return a listof candidate translation pairs.
Finally a singletarget-language translation as output is chosen.2.1 Query rewritingTo retrieve a list of translation candidates fromthe IR engine, we first create a query which isa concatenation of all possible ngrams of thesource sentence, for all ngram sizes from 1 toa fixed n.We rely on the fact that the Chinese datain the translation memory is tokenized and in-dexed at the unigram level.
Each Chinese char-acter in the source sentence is tokenized indi-vidually, and we make use of the IR engine?sphrase query feature, which matches documentsin which all terms in the phrase appear in con-secutive order, to create the ngrams.
For exam-ple, to produce a trigram + bigram + unigramquery for a Chinese sentence of 10 characters, wewould create a query consisting of eight three-character phrases, nine two-character phrases,and 10 single-character ?phrases?.
All phrasesare weighted equally in the query.This approach allows us to perform lookupsfor arbitrary ngram sizes.
Depending on thespecifics of how idf is calculated, this may yielddifferent results from indexing ngrams directly,but it is advantageous in terms of space con-sumed and scalability to different ngram sizeswithout reindexing.This is a slight generalization of the success-ful approach to Chinese information retrieval us-ing bigrams (Kwok, 1997).
Unlike that work,we perform no second stage IR after query ex-pansion.
Using a segmentation-independent en-gineering approach to Chinese IR allows us tosidestep the lack of a strong segmentation stan-dard for our heterogeneous parallel corpus andprepares us to rapidly move to other languageswith segmentation or lemmatization challenges.2.2 The IR engineSimply for performance reasons, an IR engine,or some other sort of index, is needed to imple-ment a TM (Brown, 2004).
We use the open-source Lucene v1.4.3, (Apa, 2004) as our IR en-gine.
Lucene scores candidate segments fromthe parallel text using a modified tf-idf formulathat includes normalizations for the input seg-ment length and the candidate segment length.We did not modify any Lucene defaults for theseexperiments.To form our translation memory, we indexedall sentence pairs in the translation memory cor-pora, each pair as a separate document.
We176SourceTM outputHowever , everything depended on the missions to be decided by the Security Council .The presentations focused on the main lessons learned from their activities in the field .It is wrong to commit suicide or to use ones own body as a weapon of destruction .There was practically full employment in all sectors .One reference translation (of four)Doug Collins said, ?He may appear any time.
It really depends on how he feels.
?At present, his training is defense oriented but he also practices shots.He is elevating the intensity to test whether his body can adapt to it.So far as his knee is concerned, he thinks it heals a hundred percent after the surgery.
?Table 1: Typical TM output.
Excerpt from a story about athlete Michael Jordan.indexed in such a way that IR searches can berestricted to just the source language side or justthe target language side.2.3 RescoringThe IR engine returns a list of candidate trans-lation pairs based on the query string, and thefinal stage of the TM process is the selection ofa single target-language output sentence fromthat set.We consider a variety of selection metrics inthe experiments below.
For each metric, thesource-language side of each pair in the candi-date list is evaluated against the original sourcelanguage input string.
The target language seg-ment of the pair with the highest score is thenoutput as the translation.In the case of automated MT evaluation met-rics, which are not necessarily symmetric, thesource-language input string is treated as thereference and the source-language side of eachpair returned by the IR engine as the hypothe-sis.All tie-breaking is done via tf-idf , i.e.
if multi-ple entries share the same score, the one rankedhigher by the search engine will be output.Table 1 gives a typical example of how the TMperforms.
Four contiguous source segments arepresented, followed by TM output and finallyone of the reference translations for those sourcesegments.
The only indicator of the translationquality available to monolingual English speak-ers is the awkwardness of the segments as agroup.
By design, the TM performs with perfectfluency at the segment level.3 ExperimentsWe performed several experiments in the courseof optimizing this TM, all using the same setof parallel texts for the TM database andmultiple-reference translation corpus for eval-utation.
The parallel texts for the TM comefrom several Chinese-English parallel corpora,all available from the Linguistic Data Consor-tium (LDC).
These corpora are described in Ta-ble 2.
We discarded any sentence pairs thatseemed trivially incomplete, corrupt, or other-wise invalid.
In the case of LDC2002E18, inwhich sentences were aligned automatically andconfidence scores produced for each alignment,we dropped all pairs with scores above 9, indi-cating poor alignment.
No duplication checkswere performed.
Our final corpus contained ap-proximately 7 million sentence pairs and con-tained 3.2 GB of UTF-8 data.Our evaluation corpus and reference corpus177come from the data used in the NIST 2002 MTcompetition.
(NIST, 2002).
The evaluation cor-pus is 878 segments of Chinese source text.
Thereference corpus consists of four independenthuman-generated reference English translationsof the evaluation corpus.All performance measurements were made us-ing a fast reimplementation of NIST?s bleu.bleu exhibits a high correlation with humanjudgments of translation quality when measur-ing on large sections of text (Papineni et al,2001).
Furthermore, using bleu allowed us tocompare our performance to that of other sys-tems that have been tested with the same eval-uation data.3.1 An upper bound on whole-segmenttranslation memoryOur first experiment was to determine an upperbound for the entire translation memory corpus.In other words, given an oracle that picks thebest possible translation from the translationmemory corpus for each segment in the evalu-ation corpus, what is the bleu score for the re-sulting document?
This score is unlikely to ap-proach the maximum, bleu =100 because thisoracle is constrained to selecting a translationfrom the target language side of the parallel cor-pus.
All of the calculations for this experimentare performed on the target language side of theparallel text.We were able to take advantage of a traitparticular to bleu for this experiment, avoid-ing many of bleu score calculations requiredto assess all of the 878 ?
7.5 million combina-tions.
bleu produces a score of 0 for any hy-pothesis string that doesn?t share at least one4-gram with one reference string.
Thus, foreach set of four references, we created a Lucenequery that returned all translation pairs whichmatched at least one 4-gram with one of the ref-erences.
We picked the top segment by calcu-lating bleu scores against the references, andcreated a hypothesis document from these seg-ments.Note that, for document scores, bleu?sbrevity penalty (BP) is applied globally to anentire document and not to individual segments.Thus, the document score does not necessarilyincrease monotonically with increases in scoresof individual segments.
As more than 99% ofthe segment pairs we evaluated yielded scores ofzero, we felt this would not have a significanteffect on our experiments.
Also, the TM doesnot have much liberty to alter the length of thereturned segments.
Individual segments werechosen to optimize bleu score, and the result-ing documents exhibited appropriately increas-ing scores.
While there is no efficient strategyfor whole-document bleu maximization, an it-erative rescoring of the entire document whileoptimizing the choice of only one candidate seg-ment at a time could potentially yield higherscores than those we report here.3.2 TM performance with variedNgram lengthThe second experiment was to determine the ef-fect that different ngram sizes in the Chinese IRquery have on the IR engine?s ability to retrievegood English translations.We considered cumulative ngram sizes from 1to 7, i.e.
unigram, unigram + bigram, unigram+ bigram + trigram, and so on.
For each setof ngram sizes, we created a Lucene query forevery segment of the (Chinese) evaluation cor-pus.
We then produced a hypothesis documentby combining the English sides of the top re-sults returned by Lucene for each query.
Thehypothesis document was evaluated against thereference corpora by calculating a bleu score.While it was observed that IR perfor-mance is maximized by performing bigramqueries (Kwok, 1997), we had reason to believethe TM would not be similar.
TMs must at-tempt to match short sequences of stop wordsthat indicate grammar as well as more tradi-tional content words.
Note that our systemperformed neither stemming nor stop word (orngram) removal on the input Chinese strings.3.3 An upper bound on TM N-best listrescoringThe next experiment was to determine an upperbound on the performance of tf-idf for differ-ent result set sizes, i.e.
for different (maximum)178LDC Id Description PairsLDC2002E18 Xinhua Chinese-English Parallel News Text v. 1.0 beta 2 64,371LDC2002E58 Sinorama Chinese-English Parallel Text 103,216LDC2003E25 Hong Kong News Parallel Text 641,308LDC2004E09 Hong Kong Hansard Parallel Text 1,247,294LDC2004E12 UN Chinese-English Parallel Text v. 2 4,979,798LDC2000T47 Hong Kong Laws Parallel Text 302,945Total 7,338,932Table 2: Sentence-aligned parallel corpora used for the creation of the translation memory.
The?pairs?
column gives the number of translation pairs available after trivial pruning.numbers of translation pairs returned by the IRengine.
This experiment describes the trade-offbetween more time spent in the IR engine cre-ating a longer list of returns and the potentialincrease in translation score.To determine how much IR was ?enough?
IR,we performed an oracle experiment on differentIR query sizes.
For each segment of the evalua-tion corpus, we performed a cumulative 4-gramquery as described in Section 4.2.
We producedthe n-best list oracle?s hypothesis document byselecting the English translation from this resultset with the highest bleu score when evaluatedagainst the corresponding segment from the ref-erence corpus.
We then evaluated the hypoth-esis documents against the reference corpus bycomputing bleu scores.3.4 N-best list rescoring with severalMT evaluation metricsThe fourth experiment was to determinewhether we could improve upon tf-idf by apply-ing automated MT metrics to pick the best sen-tence from the top n translation pairs returnedby the IR engine.
We compared a variety ofmetrics from MT evaluation literatures.
All ofthese were run on the tokens in the source lan-guage side of the IR result, comparing againstthe single pseudo-reference, the original sourcelanguage segment.
While many of these metricsaren?t designed to perform well with one refer-ence, they stand in as good approximate stringmatching algorithms.The score that the IR engine associates witheach segment is retained and marked as tf-idfin this experiment.
Naturally, bleu (Papineniet al, 2001) was the first choice metric, as itwas well-matched to the target language evalu-ation function.
rouge was a reimplementationof ROUGE-L from (Lin and Och, 2004).
It com-putes an F-measure from precision and recallthat are both based on the longest common sub-sequence of the hypothesis and reference strings.wer-g is a variation on traditional word errorrate that was found to correlate very well withhuman judgments (Foster et al, 2003), and peris the traditional position-independent error ratethat was also shown to correlate well with hu-man judgments (Leusch et al, 2003).
Finally,a random metric was added to show the bleuvalue one could achieve by selecting from the topn strictly by chance.After the individual metrics are calculatedfor these segments, a uniform-weight log-linearcombination of the metrics is calculated andused to produce a new rank ordering under thebelief that the different metrics will make pre-dictions that are constructive in aggregate.4 Results4.1 An upper bound for whole-sentenceTMFigure 1 shows the maximum possible bleuscore that can an oracle can achieve by selectingthe best English-side segment from the paralleltext.
The upper bound achieved here is a bleuscore of 17.7, and this number is higher thanthe best performing system in the correspond-ing NIST evaluation.Note the log-linear growth in the resulting17978910111213141516171810000  100000  1e+06  1e+07OracleBLEUscoreCorpus size (segments)Size bleu73389 7.88366947 10.82733893 12.583669466 16.277338932 17.69Figure 1: Oracle bounds on TM performance ascorpus size increases.2.533.544.555.561  2  3  4  5  6  7BLEUscoreof TMMax n-gram lengthNgrams in query bleu1 2.721,2 4.731,2,3 5.681,2,3,4 5.871,2,3,4,5 5.801,2,3,4,5,6 5.521,2,3,4,5,6,7 5.48Figure 2: bleu scores for different cumulativengram sizes, when retrieving only the first trans-lation pair.bleu score of the TM with increasing databasesize.
As the database is increased by a factorof ten, the TM gains approximately 5 points ofbleu.
While this trend has a natural limit at20 orders of magnitude, it is unlikely that thisamount of text, let alne parallel text, will be aindexed in the foreseeable future.
This rate ismore useful in interpolation, giving an idea ofhow much could be gained from adding to cor-pora that are smaller than 7.5 million segments.4.2 The effect of ngram size on Chinesetf-idf retrievalFigure 2 shows that our best performance isrealized when IR queries are composed of cu-mulative 4-grams (i.e.
unigrams + bigrams +trigrams + 4-grams).
As hypothesized, whilelonger sequences are not important in documentretrieval in Chinese IR, they convey informationthat is useful in segment retrieval in the trans-lation memory.
For the remainder of the ex-periments, we restrict ourselves to cumulative4-gram queries.Note that the 4-gram result here (bleu of5.87) provides the baseline system performancemeasure as well as the value when the segmentsare reranked according to tf-idf .4.3 Upper bounds for tf-idfFigure 3 gives the n-best list rescoring bounds.The upper bound continues to increase up tothe top 1000 results.
The plateau achieved af-ter 1000 IR results suggests that is little to begained from further IR engine retrieval.Note the log-linear growth in the bleu scorethe oracle achieves as the n-best list extends onthe left side of the figure.
As the list lengthis increased by a factor of ten, the oracle up-per bound on performance increases by roughly3 points of bleu.
Of course, for a system toperform as well as the oracle does becomes pro-gressively harder as the n-best list size increases.Comparing this result with the experimentin section 4.1 indicates that making the oraclechoose among Chinese source language IR re-sults and limiting its view to the 1000 resultsgiven by the IR engine incurs only a minor re-duction of the oracle?s bleu score, from 17.7 to18016.3.
This is one way to measure the impactof crossing this particular language barrier andusing IR rather than exhaustive search.46810121416180.1  1  10  100  1000  10000  100000  1e+06  1e+07OracleBLEUscoreN-best list sizeSize bleu score1 5.875 8.4710 9.5150 12.09100 13.18500 15.361000 16.297338932 17.69Figure 3: bleu scores for different (maximum)numbers of translation pairs returned by IR en-gine, where the optimal segment is chosen fromthe results by an oracle.4.4 Using automated MT metrics topick the best TM sentenceEach metric was run on the top 1000 resultsfrom the IR engine, on cumulative 4-gramqueries.
Each metric was given the (Chinese)evaluation corpus segment as the single refer-ence, and scored the Chinese side of each of the1000 resulting translation pairs against that ref-erence.
The hypothesis document for each met-ric consisted of the English side of the transla-tion pair with the best score for each segment.These documents were scored with bleu againstthe reference corpus.
Ties (e.g.
cases where ametric gave all 1000 pairs the same score) werebroken with tf-idf .Results of the rescoring experiment run onMetric bleubleu 6.20wer-g 5.90rouge 5.88tf-idf 5.87per 5.72random 3.32log(tf-idf )+log(bleu)+log(rouge)-log(wer-g)-log(per) 6.56Table 3: bleu scores for different metrics whenpicking the best translation from 100 translationpairs returned by the IR engine.an n-best list of size 100 are given in Table 3.Choosing from 1000 pairs did not give betterresults.
Choosing from only 10 gave worse re-sults.
The random baseline given in the tablerepresents the expected score from choosing ran-domly among the top 100 IR returns.
While thescores of the individual metrics aside from perand bleu reveal no differences, bleu and thecombination metric performed better than theindividual metrics.Surprisingly, tf-idf was outperformed only bybleu and the combination metric.
While wehoped to gain much more from n-best list rescor-ing on this task, reaching toward the limits dis-covered in section 4.3, the combination metricwas less than 0.5 bleu points below the lowerrange of systems that were entered in the NIST2002 evals.
The bleu scores of research systemsin that competition roughly ranged between 7and 15.
Of course, each of the segments pro-duced by the TM exhibit perfect fluency.5 DiscussionThe maximum bleu score attained by a TM wedescribe (6.56) would place it in last place in theNIST 2002 evals, but by less than 0.5 bleu.
Suc-cessive NIST competitions have exhibited im-pressive system progress, but each year therehave been newcomers who score near (or in somecases lower than) our simple TM baseline.181We have presented several experiments thatquantitatively describe how well a simple TMperforms when measured with a standard MTevaluation measure, bleu.
We showed that thetranslation performance of a TM grows as a log-linear function of corpus size below 7.5 millionsegments.
We showed, somewhat surprisingly,only 1000 IR returns need be evaluated by arescorer to get within 1 bleu point of the max-imum possible score attainable by the TM.In future work, we expect to validate theseresults with other language pairs.
One questionis: how well does this simple IR query expansionaddress segmented languages and languages thatallow more liberal word order?
Supervised train-ing of n-best reranking schemes would also de-termine how far the oracle bound can be pushed.The computationally more expensive rerankingprocedure that attempts to optimize bleu onthe entire document should be investigated todetermine how much can be gained by betterglobal management of the brevity penalty.Finally, we believe it?s worth noting the degreeto which high fluency of the TM output couldpotentially mislead target-language-only readersin their estimation of the system?s performance.Table 1 is representative of system output, andis a good example of why translations should notbe judged solely on the fluency of a few segmentsof target language output.ReferencesApache Software Foundation, 2004.
Lucene 1.4.3API.
http://lucene.apache.org/java/docs/api/.Ralf D. Brown.
2004.
A modified burrows-wheeler transform for highly-scalable example-based translation.
In Machine Translation: FromReal Users to Research, Proceedings of the 6thConference of the Association for Machine Trans-lation (AMTA-2004), Washington, D.C., USA.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
2004.
Searchable translation memo-ries.
In Proceedings of ASLIB Translation and theComputer 26.Joanna Drugan.
2004.
Multilingual document man-agement and workflow in the european institu-tions.
In Proceedings of ASLIB Translation andthe Computer 26.George Foster, Simona Gandrabur, Cyril Goutte,Erin Fitzgerald, Alberto Sanchis, Nicola Ueffing,John Blatz, and Alex Kulesza.
2003.
Confidenceestimation for machine translation.
Technical re-port, JHU Center for Language and Speech Pro-cessing.K.
L. Kwok.
1997.
Comparing representations inchinese information retrieval.
In SIGIR ?97: Pro-ceedings of the 20th annual international ACM SI-GIR conference on Research and development ininformation retrieval, pages 34?41, New York, NY,USA.
ACM Press.G.
Leusch, N. Ueffing, and H. Ney.
2003.
Anovel string-to-string distance measure with ap-plications to machine translation evaluation.
InProc.
of the Ninth MT Summit, pages 240?247.Chin-Yew Lin and Franz Josef Och.
2004.
Or-ange: a method for evaluating automatic evalu-ation metrics for machine translation.
In Proceed-ings of the 20th International Conference on Com-putational Linguistics (COLING 2004), Geneva,Switzerland, August.E.
Macklovitch, M. Simard, and Ph.
Langlais.
2000.Transsearch: A free translation memory on theworld wide web.
In Second International Con-ference On Language Resources and Evaluation(LREC), volume 3, pages 1201?1208, AthensGreece, jun.Daniel Marcu.
2001.
Towards a unified approachto memory- and statistical-based machine trans-lation.
In ACL, pages 378?385.NIST.
2002.
The NIST 2002 machine trans-lation evaluation plan (MT-02).
NISTweb site.
http://www.nist.gov/speech/tests/mt/doc/2002-MT-EvalPlan-v1.3.pdf.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machinetranslation.
Computational Linguistics, 30(4).K.
Papineni, S. Roukos, T. Ward, and W-J.
Zhu.2001.
BLEU: a method for automatic evalua-tion of machine translation.
Technical ReportRC22176 (W0109-022), IBM Research Division,Thomas J. Watson Research Center.182
