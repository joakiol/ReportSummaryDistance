HEURISTICS FOR BROAD-COVERAGENATURAL LANGUAGE PARSINGMichael C. McCordIBM T. J. Watson  Research  CenterPOB 704 York town Heights ,  NY  10598ABSTRACTThe Slot Grammar system isinteresting for natural language applica-tions because it can deliver parses with deep grammatical informationon a reasonably broad scale.
The paper describes anumerical scoringsystem used in Slot Grammar for ambiguity resolution, which notonly ranks parses but also contributes to parsing efficiency through aparse space pruning algorithm.
Details of the method of computingparse scores are given, and test results for the English Slot Grammarare presented.1.
INTRODUCTIONAs everyone who has tried it knows, the hardest part of build-ing a broad-coverage parser is not simply covering all theconstructions of the language, but dealing with ambiguity.One approach to ambiguity resolution is to "understand" thetext well enough - to have a good semantic interpretationsystem, to use real-world modeling, inference, etc.
This canwork well in small domains, and it is, in this author's opin-ion, ultimately necessary for the highest quality of naturallanguage processing in any domain; but it is probably notfeasible on a broad scale today.
So some kind of heuristicmethod is needed for disambiguation, some way of rankinganalyses and choosing the best.
Even in the ideal model ofhuman language processing (which would use a great deal ofknowledge representation a d inference), ranking heuristicsseem appropriate as a mechanism since humans must workwith incomplete knowledge most of the time.Two major questions that can be asked about a heuristicmethod for ambiguity resolution are these:1.
What level of representation is used for disambiguationand is involved in the statements of the heuristic rules -lexical/morphological, surface syntactic, deep syntactic,or logical/semantic?2.
Where do the heuristic rules come from?
Are they largelycreated through human linguistic insight, or are theyinduced by processing corpora?This paper describes the heuristic method used in the SlotGrammar (SG)system \[10, 11, 13, 16, 17\] for ambiguity res-olution - the SG parse scoring system.
This scoring systemoperates during parsing (with a bottom-up chart parser), as-signing real number scores to partial analyses as well as toanalyses of the complete sentence.
The scores are used notonly for ranking the final analyses but also for pruning theparse space during parsing, thus increasing time and spaceefficiency.The level of representation being disambiguated is thus thelevel of SG parses.
SG parse structures are dependency-or head-oriented, and include, in a single tree, both surfacestructure and deep syntactic information such as predicate-argument structure, remote dependencies, control informa-tion, and unwinding of passives.
1SG parse structures also include a choice of word senses.
Theextent to which these represent semantic sense distinctionsdepends on the lexicon.
The SG system is set up to dealwith semantic word-sense distinctions and to resolve them bydoing semantic type-checking during parsing.
However, inthe lexicon for ESG (English Slot Grammar), nearly all wordsense distinctions are a matter of part of speech or syntacticslot frame.
Some semantic types are shown in the lexiconand are used in parsing, but generally very few.
Thus onewould say that ESG parse structures are basically syntacticstructures, although the deep information like argument struc-ture, passive unwinding, etc., counts for "semantics" in somepeople's books.Where do the SG scoring rules come from - human linguisticinsight or induction from corpus processing?
The score ofan SG parse, which will be described in Section 4, is thesum of several components.
Most of these come completelyfrom human linguistic insight, though some of them get theirnumeric values from corpus processing.
In the tests reportedin the final section, only the "linguistic-insight"rules are used.Some previous tests using the corpus-based heuristic rulestogether with the main SG heuristic rules showed that theformer could improve the parse rate by a few percentagepoints.
It is definitely worth pursuing both approaches, andmore work will be done with a combination of the two.I No attempt is made to resolve quantifier scoping in SG parses, althoughthere is a post-processing system that produces a logical form with scoperesolution for quantifiers and other "focalizers"\[12\].
Anaphora resolution\[8, 9\] is also done by post-processing SG parses.127In the next section we give a brief overview of Slot Grammar.In Section 3 we describe the scoring system generally and itsuse in parse space pruning, and in Section 4 we give some de-tails of the computation of scores.
Finally, Section 5 presentsthe results of some tests of ESG.2.
OVERVIEW OF SLOT GRAMMARThe slots that figure in Slot Grammar rules and parsing come intwo varieties: complement slots and adjunct slots.
Analysis isword-oriented, and slots are associated with word senses.
Thecomplement slots for a word sense are associated with it in thelexicon.
The adjunct slots depend only on the part of speechof the word sense and are listed for that part of speech in thegrammar.
Slots have names like subj and obj and should bethought of basically as syntactic relations, though complementslot frames in the lexicon can be viewed as corresponding toarguments in logical form.The notion that a phrase fills a slot of a word (sense) is primi-tive in the grammar, and the conditions under which this canhappen are given by the slot-fillerrules.
Grammatical naly-sis of a phrase consists basically of choosing, for each wordof the phrase, (1) a word sense, (2) a feature structure, and (3)filler subphrases for its slots.
A slot is obligatory or optionalaccording as it must be, or need not be, filled in order for theanalysis to be complete.
Adjunct slots are normally optional.A complement slot can be filled at most once, but adjunct slotscan, by default, be filled multiply.The parser works bottom-up, beginning with one-wordphrases and attaching other phrases as left and right modi-fiers as they can fill slots.
As a phrase is built up in this way,it retains a distinguished head word, and the slots associatedwith this head word are considered slots of the phrase.An example of a slot-filler ule is the following for the subjectslot (in simplified form):subj ~ f ( noun( nom, N um) )& h f ( verb( fin( N urn) ) ).A goal f(Feat) on the right hand side of a filler rule tests thatthe feature structure of the filler phrase matches Feat.
A goalhf(Feat) tests the feature structure of the higher phrase- thephrase (with possibly other modifiers attached) with whichthe slot is associated.
The SG formalism includes a rich set ofspecial predicates like f and hf that can be used for examiningany aspects of the filler phrase and higher phrase for a slotfilling.Slot-filler ules normally do not constrain left-to-right order-ing of the phrases involved.
Instead, there are modularlystated ordering rules, which are applied as constraints inpars-ing after slot-filler ules apply.Generally, there is a modular treatment of different gram-matical phenomena in a Slot Grammar.
There are separaterule systems not only for slot-filling and ordering, but alsofor coordination, unbounded ependencies, obligatory slots,adjunct slot declaration, "or-slots", punctuation, and parsescoring.
All these rule types make use of the same system ofspecial predicates (mentioned above for slot-filler ules) forexamining the phrases involved in slot filling.
Modularizationof the rule system makes large grammars more manageableand also makes it easier to adapt a grammar for one languageto another language.There are currently Slot Grammars (in various tates of com-pleteness) for English, German, Spanish, Danish, Norwegian,and Hebrew.
A great deal of attention has been paid to thedevelopment of a large, language-universal component of thesystem, the Slot Grammar shell.
For a particular language,the shell represents roughly 65% of the rules/facts, not count-ing lexicons.
All of the rule types mentioned above have partof their treatment in the shell, but there are especially largelanguage-universal components for coordination, unboundeddependencies, punctuation, and parse scoring.
Nevertheless,for all of these, there can be rules in the language-specificgrammar that override or augment he language-universalrules.The lexicon for ESG consists of a hand-coded portion forapproximately 6,000 lemmas (basically most frequent words),plus a large back-up lexicon of approximately 60,000 lemmasderived from UDICT \[1, 7\] and other sources.
Mary Neff isworking on improvements of the large ESG lexicon throughextraction from standard ictionaries.Slot Grammars are used for source analysis in the MT systemLMT \[14, 15\].For a more detailed escription of current version of the SGsystem, see \[16, 17, 18\].
In this paper we concentrate on thescoring system, in its latest form.3.
SCORING AND PARSE SPACEPRUNINGDuring parsing, each analysis P of a subphrase is assigned areal number score(P).
A larger number epresents a worsescore.
As described in the next section, most of the ingredientsthat go into scores are positive numbers that are like penaltiesfor unusual structure, and total scores are normally positive.Parse space pruning involves comparison of scores of partialanalyses and pruning away analyses that have relatively badscores; but the comparisons are made only within certainequivalence classes of analyses.
Two partial analyses areequivalent when they have the same boundaries, the samehead word, and the same basic feature.
For most categories,the basic feature is just the part of speech, but for verbs a finerdistinction is made according to the inflection type (finite,128infinitive, etc.)
of the verb.
The notion of equivalence isloosened in certain ways for coordinated phrases that will notbe described here.Pruning is done as follows.
Suppose P is a new, candi-date partial analysis obtained in parsing.
Let compar(P)denote the set of existing partial analyses that are equivalentto P (not including P itself).
Because of previous pruning,all members of compar(P) have the same score; call thisnumber scompar(P).
(If corn/mr(P) = 0 then considerscompar(P) = +oo.)
The system stores this best scorescompar(P) for the equivalence class of P in a way that canimmediately be computed from P without searching the chart.Now three things can happen: (1) If score(P) >scornpar(P), then P is discarded.
(2) If score(P) =scompar(P), then P is simply added to the chart.
(3) Ifscore(P) < scorapar(P), then P is added to the chart andall members of compar(P) are discarded.This parse space pruning can be turned on or off at run time,but scoring is done in any case, and final parses are ranked bythe scoring system whether pruning is on or off.
Generally,parse space pruning is crucial to the running of the system forlarge sentences because of space and time problems if it isnot used.
When pruning is turned on, there are generally veryfew final parses obtained for a sentence - on average about1.3 (per successfully parsed sentence).When parsing of a sentence fails, the system pieces together a"fitted parse" in a manner somewhat similar to that in \[5\].
Thescores obtained for partial analyses figure heavily in choosingthe pieces for this result.4.
COMPUTATION OF  SCORESThe score of a partial analysis is obtained incrementally inbuilding up the analysis.
The initial score, for a one-wordanalysis, is associated with the word sense, and the score isincremented whenever a slot is filled or a coordinate structureis formed.
All in all, there are eight main components of thetotal score (the score is the sum of them).
We first list themwith a brief description and then discuss them individually.The list is in decreasing order of"importance" - the amount ofeffort put into the rules/data for the component and roughly thecurrent contribution of the component to successful parsing.Components 1, 2, 3, 4, 5, 8 are totally "human-coded", andcomponents 6, 7 get their data from corpus processing.Most of the heuristics described in \[4\] are covered by theserules (and were developed independently).1.
SlotPref.
Computed when a modifier is attached by fill-ing a slot.
Measures the preference for using that slot vs.other slots for attaching the given modifier to the givenhigher phrase..3.4.....ParallelCoord.
Favors parallelism in coordination.CloseAttach.
Favors close attachment.PhrasePref.
Tests the characteristics of a "completed"phrase - a phrase that becomes a modifier or is taken asan analysis of the complete input segment.
Similar toSlotPref, but independent of the slot.WordSensePref.
Favors one sense of a word over othersenses.HeadSlotFiller.
Used, like SlotPref, when a slot is filled,but tests for specific choices for the head words of thehigher phrase and the filler phrase, as well their parts ofspeech, and tests only these things.POSPref.
Similar to WordSensePref, but tests only forpart of speech.LexSlotPref.
Another score associated with filling agiven slot, but coded in the lexicon in a given slot frameand can test semantic type conditions on the filler.In the following more detailed escription of the scoring com-ponents, we will use "XSG" to refer to the language-specificpart of the Slot Grammar of language X.
Thus the rules forgrammar of X reside in both the SG shell and XSG.SlotPref The rules for SlotPref are coded in both the shelland XSG.
The default score, given in the shell, is +I for anadjunct slot and 0 for a complement slot, so that complementslots are preferred over adjuncts.For an example, consider the sentence John sent the file toBill.
The PP to Bill can attach tofile by an adjunct slot or tosent by a complement slot (its indirect object), but the defaultSlotPrefscore is 1 for the adjunct and 0 for the complement,2 so the analysis with the complement wins and the other ispruned away.
3Slot-scoringrules in XSG can override the default.
Currently,out of a total of 678 rules of various types in ESG, 216 areslot-scoring rules.
Most of the day-to-day effort in improvingESG consists of work on these scoring rules.A slot-scoring rule is of the form:Slot + E ( 4-- Body).E is normally a real number and is the contribution of thisrule to SlotPref.
The Body, if present, can contain specialpredicates like those mentioned in Section 2 for slot-filler2Actually, a slot-scoring rule in ESG gives the adjunct ascore of 2 in thisinstance.3The CloseAttach component by itself favors the closer attachment ofthe PP tofile, but this score component is dominated by SlotPref.129rules that test any characteristics of the filler phrase and thehigher' phrase.Two examples of slot-scoring rules in ESG are as follows(given: in simplified form).ndet + 0 ~-- hf(noun(T, ., *))&(T = chiT = gerund).vadv + 0 /(noun(*, *, *))&,t(tm).The first says that the determiner slot for nouns is rewarded ifthe higher noun is a common oun or a gerund.
The secondsays that the vadv slot for verbs is rewarded when it is tilledby a time noun phrase.
The special goal st(Type) says thatType is a semantic type of the (sense of) the filler phrase, andira is a semantic type used in ESG for time words.A slot-scoring rule might be used to penalize the use of acomplement slot under certain conditions, by assigning it ascore higher than the default 0.
An example of this in ESG iscomp(bin f)  + 1 ~ headf(noun( *, *, * ) )&hrrnods(nil).Here comp(binf) is a verb complement slot filled by a bareinfinitive VP.
This is penalized if the head word of the filler isalso a noun and the higher verb has (so far) no right modifiers.This is to discourage use of the cornp(binf) analysis whenthe phrase may really be an NP, maybe an object.
Severalof the slot-scoring rules in ESG involve checks on existingalternative analyses of words, as in this example.
It is quiteuseful to have such heuristics because of the great ambiguityof English in part of speech of words.Slot-scoring may use conditions on punctuation.
For example,for the slot nprep that finds adjunct PP modifiers of nouns,we might have:nprep + 2 ~ ~sep(nil)&~hrmodf( *, prep(*, *, .
)).This penalizes nprep if the separator is not nil (say, if thereis a comma separator) and there is no other PP postmodifieralready.
Thus, in an example like John noticed his neighbor,from across the street, there will be a preference for the PP tomodify noticed instead of neighbor.ParalleICoord Most of the rules for this score componentare in the shell.
Parallelism in coordinate structures i mea-sured by similarity of the conjuncts with respect o severaldifferent characteristics.
Explicitly, when a coordinate struc-ture is formed, the increment in the total score due to theParallelCoord component is currently given by a formula(slightly simplified):P F ea + P Frarne-F P Sense 4- P M ods + P Len-F PConj +PXSG.Here the first five ingredients measure similarity of the con-juncts with respect to the following: (1) PFea: feature struc-tures; (2) PFrame: complement slot frames; (3) PSense:word senses; (4) PMods: modifier list lengths; (5) PLen:word list lengths.The ingredient PConj  tests for certain characteristics of theconjunction itself (which can include punctuation).The ingredient PXSG represents a contribution fromlanguage-specific coordination rules in XSG.CloseAttach This score is essentially the same as that de-veloped by Heidorn \[3\], and is designed to favor close attach-ment, although SlotPrefand ParallelCoordcan easily overrideit.For a phrase P, the default for CloseAttaeh(P) is definedrecursively as the sum of all terms0.1, (CloseAttaeh(Mod) + 1),where MOd varies over the modifiers of P. (One need notstate the base of this recursive formula separately, since onearrives eventually at phrases with no modifiers, and then thesum over the empty list is understood to be zero.)
The factor0.1 used in the recursive formula is the default, and it can beoverridden by an option in slot-scoring rules.
Also, a slot-scoring rule can change the basic formula applied, in a waythat will not be described here.The combination of SlotPref and CloseAttach isclosely relatedto preference rules discussed in \[19\].PhrasePref Some rules for this component are coded in theshell and have to do with preferences for the feature structureof the analysis of a complete input phrase, for example pref-erence of finite clauses over noun phrases (except in certainenvironments).Phrase-scoring rules in XSG contribute to PhrasePref, andare of a form similar to slot-scoring rules- without mentioninga slot:+E ~ Body.The real number E is added to the total score whenever aphrase satisfying Body fills any slot, or is used as a conjunctin a coordinate structure, or is taken as a top-level analysis.A sample phrase-scoring rule in ESG is-I-I ~-- f (noun(  , ,  , ,  * ) )&-~Imod(ndet,  )&t od(nadj, P)&h odst(P, qu nt d ).This penalizes (by +1) a complete noun phrase that has nodeterminer but does have an adjective modifier which hassome analysis with the feature quantadv.
This rule penalizesfor example the analysis of even Bill in which even is anadjective.130WordSensePref All of the rules for this component arecoded in the lexicon.
An example is a lexical entry for theword man:man < n( human&male, nil) < v( ev( 2 ), *, obj l ).The first lexical analysis element shows man as a noun withfeatures human and male.
The second analysis hows a verbword sense with a WordSensePref penalty of +2.These scores for word sense choices can also be coded con-ditionally on subject area codes, and there is an if-then-elseformalism for expressing this.The WordSensePrefscore is added when an initial (one-word)phrase analysis is formed.HeadSlotFiller Following a method ue largely to Ido Da-gan \[2\], counts of head-slot-filler occurrences are obtainedby parsing a corpus with ESG.
Actually parts of speech arestored along with the head words of the higher and modifierphrases, so the counts are of quintuples:( HWord, H POS, Slot, MWord, M POS).These counts are then used (with certain coefficients) to add areward (a negative number) to the score each time a modifieris attached with a match to a stored quintuple.POSPref Using an idea of Ido Dagan and Herbert Leass,ESG corpus parsing is used to obtain counts of occurrencesof pairs(Word, PartO f Speech).When an initial (one-word)phrase analysis is formed, and theword and its part of speech match an entry in the table justmentioned, then the count, with a certain egative coefficient,is added as the POSPrefcontribution to the phrase score.
Thisis of course similar to WordSensePref, taken from the lexicon,and there is an overlap.LexSIotPref Rules for this component are coded in the lex-icon.
A slot apprearing in a slot frame in a lexicai entry canhave an associated semantic type test on its filler.
For exampleconsider the following entry for give (not an actual entry forESG):give < v(obj .
iobj : human).Here the iobj slot requires a filler with the semantic typehuman.
(In general, any Boolean combination of type testscan be coded.)
If this analysis is used, then aLexSlotPrefscoreof -1 is added.
As it is stated, this semantic type requirementis absolute.
But if one writesgive < v( obj .
iobj : pre f ( human) ).then the test is not an absolute requirement, but merely givesa score increment of -1 if it is satisfied.
In both the absoluteand the "fallsoft" forms of semantic type tests, the formalismallows one to specify arbitrary score increments.5.
TESTS OF  ESGThree recent ests of ESG coverage will be described, two oncomputer manual text and one on Wall Street Journal (WSJ)text.
In all of the tests, there were no restrictions placed onvocabulary or length of test segments.
Only the first parsegiven by ESG for each segment was considered.
4For each segment, parse output was rated with one of threecategories- p: perfect parse, pa: approximate parse, or bad:not p or pa. To get a p rating, all of the SG structure had to becorrect, including for example slot labels; so this is a stricterrequirement than just getting surface structure or bracketingcorrect.
An approximate parse is a non-perfect one for whichnevertheless all the feature structures are correct and surfacestructureis correct except for level of attachment ofmodifiers.In MT applications, one can often get reasonable translationsusing approximate parses.This way of rating parses is not an ideal one, because a parsefor a very long sentence can be rated bad even when it hasa single word with a wrong feature or slot.
A combinationof measures of partial success, such as those obtained bycounting bracketing crossings, would be reasonable, sincepartially correct parses may still be useful.
I can make upfor this partially by reporting results as a function of segmentlength.Test 1 This was done using a set of approximately 88,000segments from computer manuals on which no training ofESG had been done.
Half of the corpus, simply consistingof the odd-numbered segments, was used for some lexicaltraining.
Slava Katz's terminology identification program \[6\]was run on this portion as well as a program that finds candi-date terms by looking (roughly) for sequences of capitalizedwords.
About one day was spent editing this auxiliary multi-word lexicon; theedited result consisted of 2176 entries.
Then100 segments were selected (automatically) at random fromthe (blind) even-numbered segments.
The segments rangedin token list length from 2 to 38.
The following table showsrating percentages for the segments of token list length < Nfor selected _h r.N %p %porpa10 75  7517 71 7925 66 7638 61 734"nlis first parse had the best score, but when more than one parse had thebest, only the first one output by the system was used.131Test 2 From a set of about 2200 computer manual segments,20% had been selected automatically at random, removed,and kept as a blind test set, and some ESG grammatical ndlexicaAt work had been done on the remaining.
The test was on100 of the blind test sentences, which happened to have thesame range in token list length, 2 to 38, as in the precedingtest.
The following table, similar in form to the preceding,shows results.N %p %porpa10 72 7517 74 8425 70 8038 67 80Test 3 This used a corpus of over 4 million segments fromthe WSJ.
No attempt was made to isolate a blind test set.However, little work on ESG has been done for WSJ text -maybe looking at a total of 500 sentences over the span ofwork on ESG, with most of these obtained in other ways (Ido not know if they were in the corpus in question).
At anyrate, automatic random choice from the 4M-segment corpuspresumably resulted in segments that ESG had never seen inits life.Prior to selection of the test set, Katz's terminology identifica-tion was run on approximately 40% of the corpus.
A portionof the results (based on frequency) underwent about a day'sworth of editing, giving an auxiliary multiword lexicon with1513 entries.Then 100 segments were selected at random from the 4M-segment corpus.
They ranged in token list length from 6 to57.
ESG was run, with the following results, shown again aspercentages for segments of length < N:N %p %p orpa10 75 7517 48 5625 45 5538 33 4857 29 45ESG delivered some kind of analysis for all of the segments inthe three tests, with about 11% fitted parses for the computermanual texts, and 26% fitted parses for the WSJ.
The averageparse time per segment was 1.5 seconds for the computermanuals and 5.6 seconds for the WSJ-  on an IBM mainframewith a Prolog interpreter (not compiled).References1.
Byrd, R. J.
"Word Formation in Natural Language ProcessingSystems," Proceedings oflJCAI-VIlI, 1983, pp.
704-706.2.
Dagan, I. and Itai, A.
"Automatic Acquisition of Constraints forthe Resolution of Anaphoric References and Syntactic Ambi-guities," Proceedings ofColing -90, vol.
3, 1990, pp.
162-167.3.
Heidom, G. E. "Experience with an Easily Computed Met-tic for Ranking Alternative Parses," Proceedings of the 20thAnnualMeeting oftheACL, 1982, pp.
82-84.4.
Hobbs, J. R. and Bear, J.
"Two Principles of Parse Preference,"Proceedings ofColing-90, vol.
3, 1990, pp.
162-167.5.
Jensen, K. and Heidorn, G. E. "The Fitted Parse: 100% ParsingCapability in a Syntactic Grammar of English," Research Re-port RC9729, 1982, IBM T.J. Watson Research Center, York-town Heights, NY 10598.6.
Justeson, J.S.
and Katz, S.M.
"Technical Terminology: ItsLinguistic Properties and an Algorithm for Identification iText" (to appear).7.
Klavans, J. L. and Wacbolder, N. "Documentation f Featuresand Attributes in UDICT," Research Report RC14251, 1989,IBM T.J. Watson Research Center, Yorktown Heights, N.Y.8.
Lappin, S. and McCord, M.C.
"A Syntactic Filter on Pronom-inal Anaphora in Slot Grammar" in Proceedings of the 28thAnnual Meeting of the Association for Computational Linguis-tics, 1990,pp.
135-142.9.
Lappin, S. and McCord, M.C.
"Anaphora Resolution in SlotGrammar," Computational Linguistics 16, 1990, pp.
197-212.10.
McCord, M. C. "Slot Grammars," Computational Linguistics,vol.
6, 1980, pp.
31-43.11.
McCord, M. C. "Using Slots and Modifiers in Logic Grammarsfor Natural Language," Artificial Intelligence, vol.
18, 1982,pp.
327-367.12.
McCord, M. C. "Natural Language Processing in Prolog," inWalker, A.
(Ed.
), McCord, M., Sowa, J. F., and Wilson, W. G.,Knowledge Systems and Prolog : A Logical Approach to ExpertSystems and Natural Language Processing, Addison-Wesley,Reading, Mass., 1987.13.
McCord, M. C. "A New Version of Slot Grammar," ResearchReport RC14506, 1989, IBM T.J. Watson Research Center,Yorktown Heights, NY.14.
McCord, M. C. "Design of LMT: A Prolog-based MachineTranslation System," Computational Linguistics, 15, 1989, pp.33-52.15.
McCord, M. C. "LMT," Proceedings of MT Summit II, 1989,pp.
94-99, Deutsche Gesellschaft fur Dokumentation, Frank-furt.16.
McCord, M. C. "Slot Grammar: A System for Simpler Con-struction of Practical Natural Language Grammars," In R.Studer (ed.
), Natural Language and Logic: InternationalScientific Symposium, Lecture Notes in Computer Science,Springer Verlag, Berlin, 1990, pp.
118-145.17.
McCord, M. C. "The Slot Grammar System," Research ReportRC17313, 1991, IBM T.J. Watson Research Center, YorktownHeights, NY.
To appear in J. Wedekind and C. Rohrer (Eds.
),Unification in Grammar, M1T Press.18.
McCord, M. C., Bemth, A., Lappin, S., and Zadrozny, W."Natural Language Processing within a Slot Grammar Frame-work," International Journal on Artificial Intelligence Tools,vol.
1, 1992, pp.
229-277.19.
Wilks, Y., Huang, X-M., and Fass, D. "Syntax, Preference andRight-Attachment," Proceedings of the 9th International JointConference on Artificial Intelligence, 1985, pp.
779-784.132
