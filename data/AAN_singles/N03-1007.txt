An Analysis of Clarification Dialogue for Question AnsweringMarco De BoniSchool of ComputingLeeds Metropolitan UniversityLeeds LS6 3QS, UKDepartment of Computer ScienceUniversity of YorkYork Y010 5DD, UKmdeboni@cs.york.ac.ukSuresh ManandharDepartment of Computer ScienceUniversity of YorkYork Y010 5DD, UKsuresh@cs.york.ac.ukAbstractWe examine clarification dialogue, amechanism for refining user questions withfollow-up questions, in the context of opendomain Question Answering systems.
Wedevelop an algorithm for clarification dialoguerecognition through the analysis of collecteddata on clarification dialogues and examinethe importance of clarification dialoguerecognition for question answering.
Thealgorithm is evaluated and shown tosuccessfully recognize the occurrence ofclarification dialogue in the majority of casesand to simplify the task of answer retrieval.1 Clarification dialogues in QuestionAnsweringQuestion Answering Systems aim to determine ananswer to a question by searching for a response in acollection of documents (see Voorhees 2002 for anoverview of current systems).
In order to achieve this(see for example Harabagiu et al 2002), systems narrowdown the search by using information retrievaltechniques to select a subset of documents, orparagraphs within documents, containing keywordsfrom the question and a concept which corresponds tothe correct question type (e.g.
a question starting withthe word ?Who??
would require an answer containing aperson).
The exact answer sentence is then sought byeither attempting to unify the answer semantically withthe question, through some kind of logicaltransformation (e.g.
Moldovan and Rus 2001) or bysome form of pattern matching (e.g.
Soubbotin 2002;Harabagiu et al 1999).Often, though, a single question is not enough to meetuser?s goals and an elaboration or clarification dialogueis required, i.e.
a dialogue with the user which wouldenable the answering system to refine its understandingof the questioner's needs (for reasons of space we shallnot investigate here the difference between elaborationdialogues, clarification dialogues and coherent topicalsubdialogues and we shall hence refer to this type ofdialogue simply as ?clarification dialogue?, noting thatthis may not be entirely satisfactory from a theoreticallinguistic point of view).
While a number of researchershave looked at clarification dialogue from a theoreticalpoint of view (e.g.
Ginzburg 1998; Ginzburg and Sag2000; van Beek at al.
1993), or from the point of viewof task oriented dialogue within a narrow domain (e.g.Ardissono and Sestero 1996), we are not aware of anywork on clarification dialogue for open domain questionanswering systems such as the ones presented at theTREC workshops, apart from the experiments carriedout for the (subsequently abandoned) ?context?
task inthe TREC-10 QA workshop (Voorhees 2002; Harabagiuet al 2002).
Here we seek to partially address thisproblem by looking at some particular aspect ofclarification dialogues in the context of open domainquestion answering.
In particular, we examine theproblem of recognizing that a clarification dialogue isoccurring, i.e.
how to recognize that the current questionunder consideration is part of a previous series (i.e.clarifying previous questions) or the start of a newseries; we then show how the recognition that aclarification dialogue is occurring can simplify theproblem of answer retrieval.Edmonton, May-June 2003Main Papers , pp.
48-55Proceedings of HLT-NAACL 20032 The TREC Context ExperimentsThe TREC-2001 QA track included a "context" taskwhich aimed at testing systems' ability to track contextthrough a series of questions (Voorhees 2002).
In otherwords, systems were required to respond correctly to akind of clarification dialogue in which a fullunderstanding of questions depended on anunderstanding of previous questions.
In order to test theability to answer such questions correctly, a total of 42questions were prepared by NIST staff, divided into 10series of related question sentences which thereforeconstituted a type of clarification dialogue; thesentences varied in length between 3 and 8 questions,with an average of 4 questions per dialogue.
Theseclarification dialogues were however presented to thequestion answering systems already classified and hencesystems did not need to recognize that clarification wasactually taking place.
Consequently systems that simplylooked for an answer in the subset of documentsretrieved for the first question in a series performed wellwithout any understanding of the fact that the questionsconstituted a coherent series.In a more realistic approach, systems would not beinformed in advance of the start and end of a series ofclarification questions and would not be able to use thisinformation to limit the subset of documents in whichan answer is to be sought.3 Analysis of the TREC context questionsWe manually analysed the TREC context questioncollection in order to determine what features could beused to determine the start and end of a question series,with the following conclusions:?
Pronouns and possessive adjectives: questions suchas ?When was it born?
?, which followed ?What wasthe first transgenic mammal?
?, were referring tosome previously mentioned object through apronoun (?it?).
The use of personal pronouns (?he?,?it?, ?)
and possessive adjectives (?his?, ?her?,?
)which did not have any referent in the questionunder consideration was therefore considered anindication of a clarification question..?
Absence of verbs: questions such as ?On what bodyof water??
clearly referred to some previousquestion or answer.?
Repetition of proper nouns: the question seriesstarting with ?What type of vessel was the modernVaryag??
had a follow-up question ?How long wasthe Varyag?
?, where the repetition of the propernoun indicates that the same subject matter is underinvestigation.?
Importance of semantic relations: the first questionseries started with the question ?Which museum inFlorence was damaged by a major bombexplosion??
; follow-up questions included ?Howmany people were killed??
and ?How muchexplosive was used?
?, where there is a clearsemantic relation between the ?explosion?
of theinitial question and the ?killing?
and ?explosive?
ofthe following questions.
Questions belonging to aseries were ?about?
the same subject, and thisaboutness could be seen in the use of semanticallyrelated words.4 Experiments in Clarification DialogueRecognitionIt was therefore speculated that an algorithm whichmade use of these features would successfully recognizethe occurrence of clarification dialogue.
Given that theonly available data was the collection of ?context?questions used in TREC-10, it was felt necessary tocollect further data in order to test our algorithmrigorously.
This was necessary both because of thesmall number of questions in the TREC data and thefact that there was no guarantee that an algorithm builtfor this dataset would perform well on ?real?
userquestions.
A collection of 253 questions was thereforeput together by asking potential users to seekinformation on a particular topic by asking a prototypequestion answering system a series of questions, with?cue?
questions derived from the TREC questioncollection given as starting points for the dialogues.These questions made up 24 clarification dialogues,varying in length from 3 questions to 23, with anaverage length of 12 questions (the data is availablefrom the main author upon request).The differences between the TREC ?context?collection and the new collection are summarized in thefollowing table:Groups Qs Av.
len Max MinTREC 10 41 4 8 4New 24 253 12 23 3The questions were recorded and manually tagged torecognize the occurrence of clarification dialogue.The questions thus collected were then fed into asystem implementing the algorithm, with no indicationas to where a clarification dialogue occurred.
Thesystem then attempted to recognize the occurrence of aclarification dialogue.
Finally the results given by thesystem were compared to the manually recognizedclarification dialogue tags.
In particular the algorithmwas evaluated for its capacity to:?
recognize a new series of questions (i.e.
to tell thatthe current question is not a clarification of anyprevious question) (indicated by New in the resultstable)?
recognize that the current question is clarifying aprevious question (indicated by Clarification in thetable)5 Clarification Recognition AlgorithmOur approach to clarification dialogue recognitionlooks at certain features of the question currently underconsideration (e.g.
pronouns and proper nouns) andcompares the meaning of the current question with themeanings of previous questions to determine whetherthey are ?about?
the same matter.Given a question q0  and n  previously askedquestions q-1..q-n  we have a functionClarification_Question which is true if a question isconsidered a clarification of a previously askedquestion.
In the light of empirical work such as(Ginzburg 1998), which indicates that questioners donot usually refer back to questions which are verydistant, we only considered the set of the previouslymentioned 10 questions.A question is deemed to be a clarification of aprevious question if:1.
There are direct references to nouns mentioned inthe previous n  questions through  the use ofpronouns (he, she, it, ?)
or possessive adjectives(his, her, its?)
which have no references in thecurrent question.2.
The question does not contain any verbs3.
There are explicit references to proper and commonnouns mentioned in the previous n  questions, i.e.repetitions which refer to an identical object; orthere is a strong sentence similarity between thecurrent question and the previously askedquestions.In other words:Clarification_Question(qn,q-1..q-n)is true if1.
q0  has pronoun andpossessive adjectivereferences to q-1..q-n2.
q0  does not contain anyverbs3.
q0 has repetition ofcommon or proper nounsin q-1..q-n or q0  has astrong semanticsimilarity to some q ?q-1..q-n6 Sentence Similarity MetricA major part of our clarification dialogue recognitionalgorithm is the sentence similarity metric which looksat the similarity in meaning between the currentquestion and previous questions.
WordNet (Miller 1999;Fellbaum 1998), a lexical database which organizeswords into synsets, sets of synonymous words, andspecifies a number of relationships such as hypernym,synonym, meronym which can exist between the synsetsin the lexicon, has been shown to be fruitful in thecalculation of semantic similarity.
One approach hasbeen to determine similarity by calculating the length ofthe path or relations connecting the words whichconstitute sentences (see for example Green 1997 andHirst and St-Onge 1998); different approaches havebeen proposed (for an evaluation see (Budanitsky andHirst 2001)), either using all WordNet relations(Budanitsky and Hirst 2001) or only is-a relations(Resnik 1995; Jiang and Conrath 1997; Mihalcea andMoldvoan 1999).
Miller (1999), Harabagiu et al (2002)and De Boni and Manandhar (2002) found WordNetglosses, considered as micro-contexts, to be useful indetermining conceptual similarity.
(Lee et al 2002)have applied conceptual similarity to the QuestionAnswering task, giving an answer A  a score dependenton the number of matching terms in A  and the question.Our sentence similarity measure followed on theseideas, adding to the use of WordNet relations, part-of-speech information, compound noun and wordfrequency information.In particular, sentence similarity was considered as afunction which took as arguments a sentence s1  and asecond sentence s2 and returned a value representing thesemantic relevance of s1  in respect of s2  in the context ofknowledge B, i.e.semantic-relevance( s1, s2, B  ) = n ? semantic-relevance(s1,s,B) < semantic-relevance(s2,s, B) represents the fact that sentence s1  isless relevant than s2  in respect to the sentence s and thecontext B.
In our experiments, B  was taken to be the setof semantic relations given by WordNet.
Clearly, theuse of a different knowledge base would give differentresults, depending on its completeness and correctness.In order to calculate the semantic similarity betweena sentence s1  and another sentence s2, s1  and s2  wereconsidered as sets P  and Q  of word stems.
Thesimilarity between each word in the question and eachword in the answer was then calculated and the sum ofthe closest matches gave the overall similarity.
In otherwords, given two sets Q  and P, whereQ={qw1,qw2,?,qwn} and P={pw1,pw2,?,pwm}, thesimilarity between Q  and P  is given by1<p<n Argmaxm similarity( qwp, pwm)The function similarity( w1, w2) maps the stems ofthe two words w1 and w2  to a similarity measure mrepresenting how semantically related the two wordsare; similarity( wi, wj)< similarity( wi, wk) represents thefact that the word wj is less semantically related than wkin respect to the word wi.
In particular similarity=0 iftwo words are not at all semantically related andsimilarity=1 if the words are the same.similarity( w1, w2) = h  ? where 0 ?
h  ?
1.
In particular, similarity( w1, w2) = 0 ifw1?ST ?
w2?ST, where ST is a set containing a numberof stop-words (e.g.
?the?, ?a?, ?to?)
which are toocommon to be able to be usefully employed to estimatesemantic similarity.
In all other cases,  h  is calculated asfollows: the words w1 and w2  are compared using all theavailable WordNet relationships (is-a, satellite, similar,pertains, meronym, entails, etc.
), with the additionalrelationship, ?same-as?, which indicated that two wordswere identical.
Each relationship is given a weightingindicating how related two words are, with a ?same as?relationship indicating the closest relationship, followedby synonym relationships, hypernym, hyponym, thensatellite, meronym, pertains, entails.So, for example, given the question ?Who went tothe mountains yesterday??
and the second question ?DidFred walk to the big mountain and then to mountPleasant?
?, Q  would be the set {who, go, to, the,mountain, yesterday} and P  would be the set {Did,Fred, walk, to, the, big, mountain, and, then, to, mount,Pleasant}.In order to calculate similarity the algorithm wouldconsider each word in turn.
?Who?
would be ignored asit is a common word and hence part of the list of stop-words.
?Go?
would be related to ?walk?
in a is-arelationship and receive a score h1.
?To?
and ?the?would be found in the list of stop-words and ignored.?Mountain?
would be considered most similar to?mountain?
(same-as relationship) and receive a scoreh2: ?mount?
would be in a synonym relationship with?mountain?
and give a lower score, so it is ignored.?Yesterday?
would receive a score of 0 as there are nosemantically related words in Q.
The similarity measureof Q  in respect to P  would therefore be given by h1  + h2.In order to improve performance of the similaritymeasure, additional information was considered inaddition to simple word matching (see De Boni andManandhar 2003 for a complete discussion):?
Compound noun information.
The motivationbehind is similar to the reason for using chunkinginformation, i.e.
the fact that the word ?United?
in?United States?
should not be considered similar to?United?
as in ?Manchester United?.
As opposed towhen using chunking information, however, whenusing noun compound information, the compoundis considered a single word, as opposed to a groupof words: chunking and compound nouninformation may therefore be combined as in ?
[the[United States] official team]?.?
Proper noun information.
The intuition behind thisis that titles (of books, films, etc.)
should not beconfused with the ?normal?
use of the same words:?blue lagoon?
as in the sentence ?the film BlueLagoon was rather strange?
should not beconsidered as similar to the same words in thesentence ?they swan in the blue lagoon?
as they areto the sentence ?I enjoyed Blue Lagoon when I wasyounger?.?
Word frequency information.
This is a step beyondthe use of stop-words, following the intuition thatthe more a word is common the less it is useful indetermining similarity between sentence.
So, giventhe sentences ?metatheoretical reasoning iscommon in philosophy?
and ?metatheoreticalarguments are common in philosophy?, the word?metatheoretical?
should be considered moreimportant in determining relevance than the words?common?, ?philosophy?
and ?is?
as it is muchmore rare and therefore less probably found inirrelevant sentences.
Word frequency data wastaken from the Given that the questions examinedwere generic queries which did not necessarily referto a specific set of documents, the word frequencyfor individual words was taken to be the wordfrequency given in the British National Corpus (seeBNCFreq 2003).
The top 100 words, making up43% of the English Language, were then used asstop-words and were not used in calculatingsemantic similarity.7 ResultsAn implementation of the algorithm was evaluatedon the TREC context questions used to develop thealgorithm and then on the collection of 500 newclarification dialogue questions.
The results on theTREC data, which was used to develop the algorithm,were as follows (see below for discussion and anexplanation of each method):TREC Meth.0 Meth.1 Meth.2 Meth.3aMeth.3bNew  90 90 90 60 80Clarif.
47 53 59 78 72Where ?New?
indicates the ability to recognizewhether the current question is the first in a new seriesof clarification questions and ?Clarif.?
(for?Clarification?)
indicates the ability to recognizewhether the current question is a clarification question.The results for the same experiments conducted onthe collected data were as follows:Collected Meth.0 Meth.1 Meth.2 Meth.3a Meth.3bNew  100 100 100 67 83Clarif.
64 62 66 91 89Method 0.
This method did not use any linguisticinformation and simply took a question to be aclarification question if it had any words in commonwith the previous n questions, else took the question tobe the beginning of a new series.
64% of questions inthe new collection could be recognized with this simplealgorithm, which did not misclassify any "new"questions.Method 1.
This method employed point 1 of thealgorithm described in section 5: 62% of questions inthe new collection could be recognized as clarificationquestions simply by looking for "reference" keywordssuch as he, she, this, so, etc.
which clearly referred toprevious questions.
Interestingly this did not misclassifyany "new" questions.Method 2.
This method employed points 1 and 2 ofthe algorithm described in section 5: 5% of questions inthe new collection could be recognized simply bylooking for the absence of verbs, which, combined withkeyword lookup (Method 1), improved performance to66%.
Again this did not misclassify any "new"questions.Method 3a.
This method employed the fullalgorithm described in section 5 (point 3 is thesimilarity measure algorithm described in section 6):clarification recognition rose to 91% of the newcollection by looking at the similarity between nouns inthe current question and nouns in the previousquestions, in addition to reference words and theabsence of verbs.
Misclassification was a seriousproblem, however with correctly classified "new"questions falling to 67%.Method 3b.
This was the same as method 3a, butspecified a similarity threshold when employing thesimilarity measure described in section 6: this requiredthe nouns in the current question to be similar to nounsin the previous question beyond a specified similaritythreshold.
This brought clarification questionrecognition down to 89% of the new collection, butmisclassification of "new" questions was reducedsignificantly, with "new" questions being correctlyclassified 83% of the time.Problems noted were:?
False positives: questions following a similar butunrelated question series.
E.g.
"Are they all Muslimcountries?"
(talking about religion, but in thecontext of a general conversation about SaudiArabia) followed by "What is the chief religion inPeru?"
(also about religion, but in a totallyunrelated context).?
Questions referring to answers, not previousquestions (e.g.
clarifying the meaning of a wordcontained in the answer, or building upon a conceptdefined in the answer: e.g.
"What did AntonioCarlos Tobim play?"
following "Which famousmusicians did he play with?"
in the context of aseries of questions about Fank Sinatra: AntonioCarlos Tobim was referred to in the answer to theprevious question, and nowhere else in theexchange.
These made up 3% of the missedclarifications.?
Absence of relationships in WordNet, e.g.
between"NASDAQ" and "index" (as in share index).Absence of verb-noun relationships in WordNet,e.g.
between to die and death, between "battle" and"win" (i.e.
after a battle one side generally wins andanother side loses), "airport" and "visit" (i.e.
peoplewho are visiting another country use an airport toget there)As can be seen from the tables above, the sameexperiments conducted on the TREC context questionsyielded worse results; it was difficult to say, however,whether this was due to the small size of the TREC dataor the nature of the data itself, which perhaps did notfully reflect ?real?
dialogues.As regards the recognition of question in a series(the recognition that a clarification I taking place), thenumber of sentences recognized by keyword alone wassmaller in the TREC data (53% compared to 62%),while the number of questions not containing verbs wasroughly similar (about 6%).
The improvement given bycomputing noun similarity between successivequestions gave worse results on the TREC data: usingmethod 3a resulted in an improvement to the overallcorrectness of 19 percentage points, or a 32% increase(compared to an improvement of 25 percentage points,or a 38% increase on the collected data); using method3b resulted in an improvement of 13 percentage points,or a 22% increase (compared to an improvement of 23percentage points or a 35% increase on the collecteddata), perhaps indicating that in "real" conversationspeakers tend to use simpler semantic relationships thanwhat was observed in the TREC data.8 Usefulness of Clarification DialogueRecognitionRecognizing that a clarification dialogue is occurringonly makes sense if this information can then be used toimprove answer retrieval performance.We therefore hypothesized that noting that aquestioner is trying to clarify previously asked questionsis important in order to determine the context in whichan answer is to be sought: in other words, the answers tocertain questions are constrained by the context inwhich they have been uttered.
The question ?What doesattenuate mean?
?, for example, may require a genericanswer outlining all the possible meanings of?attenuate?
if asked in isolation, or a particular meaningif asked after the word has been seen in an answer (i.e.in a definite context which constrains its meaning).
Inother cases, questions do not make sense at all out of acontext.
For example, no answer could be given to thequestion ?where??
asked on its own, while following aquestion such as ?Does Sean have a house anywhereapart from Scotland??
it becomes an easily intelligiblequery.The usual way in which Question Answeringsystems constrain possible answers is by restricting thenumber of documents in which an answer is sought byfiltering the total number of available documentsthrough the use of an information retrieval engine.
Theinformation retrieval engine selects a subset of theavailable documents based on a number of keywordsderived from the question at hand.
In the simplest case,it is necessary to note that some words in the currentquestion refer to words in previous questions or answersand hence use these other words when formulating theIR query.
For example, the question ?Is he married?
?cannot be used as is  in order to select documents, as theonly word passed to the IR engine would be ?married?
(possibly the root version ?marry?)
which would returntoo many documents to be of any use.
Noting that the?he?
refers to a previously mentioned person (e.g.
?SeanConnery?)
would enable the answerer to seek an answerin a smaller number of documents.
Moreover, given thatthe current question is asked in the context of a previousquestion, the documents retrieved for the previousrelated question could provide a context in which toinitially seek an answer.In order to verify the usefulness of constraining theset of documents from in which to seek an answer, asubset made of 15 clarification dialogues (about 100questions) from the given question data was analyzed bytaking the initial question for a series, submitting it tothe Google Internet Search Engine and then manuallychecking to see how many of the questions in the seriescould be answered simply by using the first 20documents retrieved for the first question in a series.The results are summarized in the following diagram(Fig.
1):Fig.
1: Search technique used for QuestionFirst Q in seriesWords in QCoreferenceMini-clarificationOther?
69% of clarification questions could be answeredby looking within the documents used for theprevious question in the series, thus indicating theusefulness of noting the occurrence of clarificationdialogue.?
The remaining 31% could not be answered bymaking reference to the previously retrieveddocuments, and to find an answer a differentapproach had to be taken.
In particular:?
6% could be answered after retrieving documentssimply by using the words in the question as searchterms (e.g.
?What caused the boxer uprising??);?
14% required some form of coreference resolutionand could be answered only by combining thewords in the question with the words to which therelative pronouns in the question referred (e.g.
?What film is he working on at the moment?, withthe reference to ?he?
resolved, which gets passed tothe search engine as ?What film is Sean Conneryworking on at the moment??);?
7% required more than 20 documents to beretrieved by the search engine or other, morecomplex techniques.
An example is a question suchas ?Where exactly??
which requires both anunderstanding of the context in which the questionis asked (?Where??
makes no sense on its own) andthe previously given answer (which was probably aplace, but not restrictive enough for the questioner).?
4% constituted mini-clarification dialogues within alarger clarification dialogue (a slight deviation fromthe main topic which was being investigated by thequestioner) and could be answered by looking atthe documents retrieved for the first question in themini-series.Recognizing that a clarification dialogue isoccurring therefore can simplify the task of retrieving ananswer by specifying that an answer must be in the setof documents used the previous questions.
This isconsistent with the results found in the TREC contexttask (Voorhees 2002), which indicated that systemswere capable of finding most answers to questions in acontext dialogue simply by looking at the documentsretrieved for the initial question in a series.
As in thecase of clarification dialogue recognition, therefore,simple techniques can resolve the majority of cases;nevertheless, a full solution to the problem requiresmore complex methods.
The last case indicates that it isnot enough simply to look at the documents provided bythe first question in a series in order to seek an answer:it is necessary to use the documents found for apreviously asked question which is related to the currentquestion (i.e.
the questioner could "jump" betweentopics).
For example, given the following series ofquestions starting with Q1:Q1: When was the Hellenistic Age?[?
]Q5: How did Alexander the great become ruler?Q6: Did he conquer anywhere else?Q7: What was the Greek religion in the Hellenistic Age?where Q6  should be related to Q5  but Q7  should berelated to Q1, and not Q6.
In this case, given that thesubject matter of Q1  is more immediately related to thesubject matter of Q7  than Q6  (although the subjectmatter of Q6  is still broadly related, it is more of aspecialized subtopic), the documents retrieved for Q1will probably be more relevant to Q7  than thedocuments retrieved for Q6 (which would probably bethe same documents retrieved for Q5)9 ConclusionIt has been shown that recognizing that a clarificationdialogue is occurring can simplify the task of retrievingan answer by constraining the subset of documents inwhich an answer is to be found.
An algorithm waspresented to recognize the occurrence of clarificationdialogue and is shown to have a good performance.
Themajor limitation of our algorithm is the fact that it onlyconsiders series of questions, not series of answers.
Asnoted above, it is often necessary to look at an answer toa question to determine whether the current question is aclarification question or not.
Our sentence similarityalgorithm was limited by the number of semanticrelationships in WordNet: for example, a bigimprovement would come from the use of noun-verbrelationships.
Future work will be directed on extendingWordNet in this direction and in providing other usefulsemantic relationships.
Work also needs to be done onusing information given by answers, not just questionsin recognizing clarification dialogue and on coping withthe cases in which clarification dialogue recognition isnot enough to retrieve an answer and where other, morecomplex, techniques need to be used.
It would also bebeneficial to examine the use of a similarity function inwhich similarity decayed in function of the distance intime between the current question and the pastquestions.ReferencesArdissono, L. and Sestero, D. 1996.
"Using dynamicuser models in the recognition of the plans of theuser".
User Modeling and User-Adapted Interaction,5(2):157-190.BNCFreq.
2003.
English Word Frequency List.http://www.eecs.umich.edu/~qstout/586/bncfreq.html(last accessed March 2003).Budanitsky, A., and Hirst, G. 2001.
?Semantic distancein WordNet: and experimental, application-orientedevaluation of five measures?, in Proceedings of theNAACL 2001 Workshop on WordNet and otherlexical resources, Pittsburgh.De Boni, M. and Manandhar, S. 2003.
?The Use ofSentence Similarity as a Semantic Relevance Metricfor Question Answering?.
Proceedings of the AAAISymposium on New Directions in QuestionAnswering, Stanford.De Boni, M. and Manandhar, S. 2002.
?AutomatedDiscovery of Telic Relations for WordNet?.Proceedings of the First International WordNetConference, India.Fellbaum, C. 1998.
WordNet, An electronic LexicalDatabase, MIT Press.Ginzburg , J.
1998.
"Clarifying Utterances" In: J.Hulstijn and A. Nijholt (eds.)
Proceedings of the 2ndWorkshop on the Formal Semantics and Pragmaticsof Dialogue, Twente.Ginzburg and Sag, 2000.
Interrogative Investigations,CSLI.Green, S. J.
1997.
Automatically generating hypertextby computing semantic similarity, Technical Reportn.
366, University of Toronto.Harabagiu, S., Miller, A. G., Moldovan, D.
1999.?WordNet2 - a morphologically and semanticallyenhanced resource?, In Proceedings of SIGLEX-99,University of Maryland.Harabagiu, S., et al 2002.
?Answering Complex, Listand Context Questions with LCC?s Question-Answering Server?, Proceedings of TREC-10, NIST.Hirst, G., and St-Onge, D. 1998.
?Lexical chains asrepresentations of context for the detection andcorrection of malapropisms?, in Fellbaum (ed.
),WordNet: and electronic lexical database, MITPress.Jiang, J. J., and Conrath, D. W. 1997.
?Semanticsimilarity based on corpus statistics and lexicaltaxonomy?, in Proceedings of ICRCL, Taiwan.Lee, G. G., et al 2002.
?SiteQ: Engineering HighPerformance QA System Using Lexico-SemanticPattern Matching and Shallow NLP?, Proceedings ofTREC-10, NIST.Lin, D. 1998.
?An information-theoretic definition ofsimilarity?, in Proceedings of the 15th InternationalConference on Machine Learning, Madison.Mihalcea, R. and Moldovan, D. 1999.
?A Method forWord Sense Disambiguation of Unrestricted Text?,in Proceedings of ACL ?99, Maryland, NY.Miller, G. A.
1999.
?WordNet: A Lexical Database?,Communications of the ACM, 38 (11).Moldovan, D. and Rus, V. 2001.
?Logic FormTransformation of WordNet and its Applicability toQuestion Answering?, Proceedings of the 39thconference of ACL, Toulouse.Resnik, P. 1995.
?Using information content to evaluatesemantic similarity?, in Proceedings of the 14thIJCAI, Montreal.Soubbotin, M. M. 2002. :?Patterns of Potential AnswerExpressions as Clues to the Right Answers?,Proceedings of TREC-10, NIST.van Beek, P., Cohen, R. and Schmidt, K., 1993.
?Fromplan critiquing to clarification dialogue forcooperative response generation?, ComputationalIntelligence  9:132-154.Voorhees, E. 2002.
?Overview of the TREC 2001Question Answering Track?, Proceedings of TREC-10, NIST.
