Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1384?1393,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsImproved Transliteration Mining Using Graph ReinforcementAli El-Kahky1, Kareem Darwish1, Ahmed Saad Aldein2, Mohamed Abd El-Wahab3,Ahmed Hefny2, Waleed Ammar41 Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar2 Computer Engineering Department, Cairo University, Cairo, Egypt3 Microsoft Research, Microsoft, Cairo, Egypt4 Microsoft Research, Microsoft, Redmond, WA, US{aelkahky,kdarwish}@qf.org.qa1, asaadaldien@hotmail.com2,ahmed.s.hefny@gmail.com2, t-momah@microsoft.com3,i-waamma@microsoft.com4AbstractMining of transliterations from comparableor parallel text can enhance naturallanguage processing applications such asmachine translation and cross languageinformation retrieval.
This paper presentsan enhanced transliteration miningtechnique that uses a generative graphreinforcement model to infer mappingsbetween source and target charactersequences.
An initial set of mappings arelearned through automatic alignment oftransliteration pairs at character sequencelevel.
Then, these mappings are modeledusing a bipartite graph.
A graphreinforcement algorithm is then used toenrich the graph by inferring additionalmappings.
During graph reinforcement,appropriate link reweighting is used topromote good mappings and to demote badones.
The enhanced transliteration miningtechnique is tested in the context of miningtransliterations from parallel Wikipediatitles in 4 alphabet-based languages pairs,namely English-Arabic, English-Russian,English-Hindi, and English-Tamil.
Theimprovements in F1-measure over thebaseline system were 18.7, 1.0, 4.5, and32.5 basis points for the four languagepairs respectively.
The results hereinoutperform the best reported results in theliterature by 2.6, 4.8, 0.8, and 4.1 basispoints for the four language pairsrespectively.IntroductionTransliteration Mining (TM) is the process offinding transliterated word pairs in parallel orcomparable corpora.
TM has many potentialapplications such as mining training data fortransliteration, improving lexical coverage formachine translation, and cross language retrievalvia translation resource expansion.
TM has beengaining some attention lately with a shared task inthe ACL 2010 NEWS workshop (Kumaran, et al2010).One popular statistical TM approach is performedin two stages.
First, a generative model is trainedby performing automatic character level alignmentof parallel transliterated word pairs to findcharacter segment mappings between source andtarget languages.
Second, given comparable orparallel text, the trained generative model is usedto generate possible transliterations of a word inthe source language while constraining thetransliterations to words that exist in the targetlanguage.However, two problems arise in this approach:1.
Many possible character sequence mappingsbetween source and target languages may not beobserved in training data, particularly when limitedtraining data is available ?
hurting recall.2.
Conditional probability estimates of obtainedmappings may be inaccurate, because somemappings and some character sequences may not1384appear a sufficient number of times in training toproperly estimate their probabilities ?
hurtingprecision.In this paper we focus on overcoming these twoproblems to improve overall TM.
To address thefirst problem, we modeled the automaticallyobtained character sequence mappings (fromalignment) as a bipartite graph and then weperformed graph reinforcement to enrich the graphand predict possible mappings that were notdirectly obtained from training data.
The examplein Figure 1 motivates graph reinforcement.
In theexample, the Arabic letter ???
(pronounced as?qa?)
was not aligned to the English letter ?c?
intraining data.
Such a mapping seems probablegiven that another Arabic letter, ???
(pronouncedas ?ka?
), maps to two English letters, ?q?
and ?k?,to which ???
also maps.
In this case, there aremultiple paths that would lead to a mappingbetween the Arabic letter ???
and the English letter?c?, namely ?
?
q ?
?
?
c and ?
?
k ?
?
?c.
By using multiple paths as sources of evidence,we can infer the new mapping and estimate itsprobability.Another method for overcoming the missingmappings problem entails assigning smallsmoothing probabilities to unseen mappings.However, from looking at the graph, it is evidentthat some mappings could be inferred and shouldbe assigned probabilities that are higher than asmall smoothing probability.The second problem has to do primarily with somecharacters in one language, typically vowels,mapping to many character sequences in the otherlanguage, with some of these mappings assumingvery high probabilities (due to limited trainingdata).
To overcome this problem, we used linkreweighting in graph reinforcement to scale downthe likelihood of mappings to target charactersequences in proportion to how many sourcesequences map to them.We tested the proposed method using the ACL2010 NEWS workshop data for English-Arabic,English-Russian, English-Hindi, and English-Tamil (Kumaran et al, 2010).
For each languagepair, the standard ACL 2010 NEWS workshop datacontained a base set of 1,000 transliteration pairsfor training, and set of 1,000 parallel Wikipediatitles for testing.The contributions of the paper are:1.
Employing graph reinforcement to improve thecoverage of automatically aligned data ?
as theyapply to transliteration mining.
This positivelyaffects recall.2.
Applying link reweighting to overcomesituations where certain tokens ?
charactersequences in the case of transliteration ?
tend tohave many mappings, which are often erroneous.This positively affects precision.The rest of the paper is organized as follows:Section 2 surveys prior work on transliterationmining; Section 3 describes the baseline TMapproach and reports on its effectiveness; Section 4describes the proposed graph reinforcement alongwith link reweighting and reports on the observedimprovements; and Section 5 concludes the paper.Figure 1:  Example mappings seen in trainingBackgroundMuch work has been done on TM for differentlanguage pairs such as English-Chinese (Kuo et al,2006; Kuo et al, 2007; Kuo et al, 2008; Jin et al2008;), English-Tamil (Saravanan and Kumaran,2008; Udupa and Khapra, 2010), English-Korean(Oh and Isahara, 2006; Oh and Choi, 2006),English-Japanese (Qu et al, 2000; Brill et al,2001; Oh and Isahara, 2006), English-Hindi (Fei etal., 2003; Mahesh and Sinha, 2009), and English-Russian (Klementiev and Roth, 2006).TM typically involves two main tasks, namely:finding character mappings between twolanguages, and given the mappings ascertainingwhether two words are transliterations or not.When training with a limited number oftransliteration pairs, two additional problemsappear: many possible character sequencemappings between source and target languagesmay not be observed in training data, andconditional probability estimates of obtained1385mappings may be inaccurate.
These two problemsaffect recall and precision respectively.1.1 Finding Character MappingsTo find character sequence mappings between twolanguages, the most common approach entailsusing automatic letter alignment of transliterationpairs.
Akin to phrasal alignment in machinetranslation, character sequence alignment is treatedas a word alignment problem between parallelsentences, where transliteration pairs are treated asif they are parallel sentences and the charactersfrom which they are composed are treated as ifthey are words.
Automatic alignment can beperformed using different algorithms such as theEM algorithm (Kuo et al, 2008; Lee and Chang,2003) or HMM based alignment (Udupa et al,2009a; Udupa et al, 2009b).
In this paper, we useautomatic character alignment betweentransliteration pairs using an HMM aligner.Another method is to use automatic speechrecognition confusion tables to extract phoneticallyequivalent character sequences to discovermonolingual and cross lingual pronunciationvariations (Kuo and Yang, 2005).
Alternatively,letters can be mapped into a common character setusing a predefined transliteration scheme (Oh andChoi, 2006).1.2 Transliteration MiningFor the problem of ascertaining if two words canbe transliterations of each other, a commonapproach involves using a generative model thatattempts to generate all possible transliterations ofa source word, given the character mappingsbetween two languages, and restricting the outputto words in the target language (Fei et al, 2003;Lee and Chang, 2003, Udupa et al, 2009a).
This issimilar to the baseline approach that we used inthis paper.
Noeman and Madkour (2010)implemented this technique using a finite stateautomaton by generating all possibletransliterations along with weighted edit distanceand then filtered them using appropriate thresholdsand target language words.
They reported the bestTM results between English and Arabic with F1-measure of 0.915 on the ACL-2010 NEWSworkshop standard TM dataset.
A relatedalternative is to use back-transliteration todetermine if one sequence could have beengenerated by successively mapping charactersequences from one language into another (Brill etal., 2001; Bilac and Tanaka, 2005; Oh and Isahara,2006).Udupa and Khapra (2010) proposed a method inwhich transliteration candidates are mapped into a?low-dimensional common representation space?.Then, similarity between the resultant featurevectors for both candidates can be computed.Udupa and Kumar (2010) suggested that mappingto a common space can be performed using contextsensitive hashing.
They applied their technique tofind variant spellings of names.Jiampojamarn et al (2010) used classification todetermine if a source language word and targetlanguage word are valid transliterations.
They useda variety of features including edit distancebetween an English token and the Romanizedversions of the foreign token, forward andbackward transliteration probabilities, andcharacter n-gram similarity.
They reported the bestresults for Russian, Tamil, and Hindi with F1-measure of 0.875, 0.924, and 0.914 respectively onthe ACL-2010 NEWS workshop standard TMdatasets.1.3 Training with Limited Training DataWhen only limited training data is available totrain a character mapping model, the resultantmappings are typically incomplete (due tosparseness in the training data).
Further, resultantmappings may not be observed a sufficient oftimes and hence their mapping probabilities maybe inaccurate.Different methods were proposed to solve thesetwo problems.
These methods focused on makingtraining data less sparse by performing some kindof letter conflation.
Oh and Choi (2006) used aSOUNDEX like scheme.
SOUNDEX is used toconvert English words into a simplified phoneticrepresentation, in which vowels are removed andphonetically similar characters are conflated.
Avariant of SOUNDEX along with iterative trainingwas proposed by Darwish (2010).
Darwish (2010)reported significant improvements in TM recall atthe cost of limited drop in precision.
Anothermethod involved expanding character sequencemaps by automatically mining transliteration pairsand then aligning these pairs to generate anexpanded set of character sequence maps (Fei etal., 2003).
In this work we proposed graph1386reinforcement with link reweighting to address thisproblem.
Graph reinforcement was used in thecontext of different problems such as miningparaphrases (Zhao et al, 2008; Kok and Brockett,2010; Bannard and  Callison-Burch 2005) andnamed entity translation extraction (You et al,2010).Baseline Transliteration Mining1.4 Description of Baseline SystemThe basic TM setup that we employed in thiswork used a generative transliteration model,which was trained on a set of transliteration pairs.The training involved automatically aligningcharacter sequences.
The alignment was performedusing a Bayesian learner that was trained on worddependent transition models for HMM based wordalignment (He, 2007).
Alignment producedmappings of source character sequences to targetcharacter sequences along with the probability ofsource given target and vice versa.
Sourcecharacter sequences were restricted to be 1 to 3characters long.For all the work reported herein, given anEnglish-foreign language transliteration candidatepair, English was treated as the target language andthe foreign language as the source.
Given aforeign source language word sequenceand anEnglish target word sequence,couldbe a potential transliteration of.
Anexample of word sequences pair is the Tamil-English pair:  (???????
????
?????
?,Haile Selassie I of Ethiopia), where ????????
?could be transliteration for any or none of theEnglish words {?Haile?, ?Selassie?, ?I?, ?of?,?Ethiopia?}.
The pseudo code below describeshow transliteration mining generates candidates.Basically, given a source language word, allpossible segmentations, where each segment has amaximum length of 3 characters, are producedalong with their associated mappings into thetarget language.
Given all mapping combinations,combinations producing valid target words areretained and sorted according to the product oftheir mapping probabilities.
If the product of themapping probabilities for the top combination isabove a certain threshold, then it is chosen as thetransliteration candidate.
Otherwise, no candidateis chosen.
To illustrate how TM works, considerthe following example: Given the Arabic word???
?, all possible segmentations are (?
?
?)
and (??
).Given the target words {the, best, man} and thepossible mappings for the segments and theirprobabilities:??
= {(m, 0.7), (me, 0.25), (ma, 0.05)}?
= {n, 0.7), (nu, 0.2), (an, 0.1)}??
= {(men, 0.4), (man, 0.3), (mn, 0.3)}The only combinations leading valid targetwords would be:(??)
?
{(man: 0.3)}( ??
?
)?
?
{(m,an: 0.07), (ma, n: 0.035)}Consequently, the algorithm would produce thetuple with the highest probability: (??
, man, 0.3).As the pseudo code suggests, the actualimplementation is optimized via: incremental leftto right processing of source words; the use of aPatricia trie to prune mapping combinations thatdon?t lead to valid words; and the use of a priorityqueue to insure that the best candidate is always atthe top of the queue.1.5 Smoothing and ThresholdingWe implemented the baseline system with andwithout assigning small smoothing probabilitiesfor unseen source character to target charactermappings.
Subsequent to training, the smoothingprobability was selected as the smallest observedmapping probability in training.We used a threshold on the minimum acceptabletransliteration score to filter out unreliabletransliterations.
We couldn?t fix a minimum scorefor reliable transliterations to a uniform value forall words, because this would have caused themodel to filter out long transliterations.
Thus, wetied the threshold to the length of transliteratedwords.
We assumed a threshold d for singlecharacter mappings and the transliterationthreshold for a target word of length l wascomputed as    .
We selected d by sorting themapping probabilities, removing the lowest 10% ofmapping probabilities (which we assumed to beoutliers), and then selecting the smallest observedprobability to be the character threshold d. Thechoice of removing the lowest ranking 10% ofmapping probabilities was based on intuition,because we did not have a validation set.
Thethreshold was then applied to filter outtransliteration with                         .13871.6 Effectiveness of Baseline SystemTo test the effectiveness of the baseline system, weused the standard TM training and test datasetsfrom the ACL-2010 NEWS workshop shared task.The datasets are henceforth collectively referred toas the NEWS dataset.
The dataset included 4alphabet-based language pairs, namely English-Arabic, English-Russian, English-Hindi, andEnglish-Tamil.
For each pair, a dataset included alist of 1,000 parallel transliteration word pairs totrain a transliteration model, and a list of 1,000parallel word sequences to test TM.
The parallelsequences in the test sets were extracted titles fromWikipedia article for which cross language linksexist between both languages.We preprocessed the different languages asfollows:?
Russian: characters were case-folded?
Arabic: the different forms of alef (alef, alefmaad, alef with hamza on top, and alef withhamza below it) were normalized to alef, yaand alef maqsoura were normalized to ya, andta marbouta was mapped to ha.?
English: letters were case-folded and thefollowing letter conflations were performed:?, ?
?
z  ?, ?, ?, ?, ?, ?, ?, ?
?
a?, ?, ?
?
e  ?, ?, ?
?c?
?l  ?, ?, ?, ?
?
i?, ?, ?, ?
?
o ?, ?, ?
?
n?, ?, ?, ?
?
s ?
?
r?
?
y  ?, ?, ?, ?
?
u?
Tamil and Hindi: no preprocessing wasperformed.English/ P R FArabic 0.988 0.983 0.583 0.603 0.733 0.748Russian 0.975 0.967 0.831 0.862 0.897 0.912Hindi 0.986 0.981 0.693 0.796 0.814 0.879Tamil 0.984 0.981 0.274 0.460 0.429 0.626Table 1:  Baseline results for all language pairs.Results with smoothing are shaded.Table 1 reports the precision, recall, and F1-measure results for using the baseline system inTM between English and each of the 4 otherlanguages in the NEWS dataset with and withoutsmoothing.
As is apparent in the results, withoutsmoothing, precision is consistently high for alllanguages, but recall is generally poor, particularlyfor Tamil.
When smoothing is applied, weobserved a slight drop in precision for Arabic,Hindi, and Tamil and a significant drop of 5.61: Input:  Mappings, set of source given target mappings with associated Prob.2: Input:  SourceWord (1), Source language word3: Input:  TargetWords, Patricia trie containing all target language words (1?
)4: Data Structures:  DFS, Priority queue to store candidate transliterations pair ordered by their transliterationscore ?
Each candidate transliteration tuple = (SourceFragment, TargetTransliteration, TransliterationScore).5: StartSymbol = (?
?, ?
?, 1.0)6: DFS={StartSymbol}7: While(DFS is not empty)8:  SourceFragment= DFS.Top().SourceFragment9:  TargetFragment= DFS.Top().TargetTransliteration10:  FragmentProb=DFS.Top().TransliterationScore11:  If (SourceWord == SourceFragment )12:   If(FragmentScore > Threshold)13:    Return (SourceWord, TargetTransliteration, TransliterationScore)14:   Else15:    Return Null16:  DFS.RemoveTop()17:  For SubFragmentLength=1 to 318:   SourceSubString= SubString( SourceWord, SourceFragment.Length , SubFragmentLength)19:   Foreach mapping in Mappings[SourceSubString]20:    If( (TargetFragment + mapping)  is a sub-string in TargetWords)21:     DFS.Add(SourceFragment + SourceSubString, Mapping.Score * FragmentScore)22:  DFS.Remove(SourceFragment)23: End While24: Return NullFigure 2:  Pseudo code for transliteration mining1388basis points for Russian.
However, the applicationof smoothing increased recall dramatically for alllanguages, particularly Tamil.
For the remainder ofthe paper, the results with smoothing are used asthe baseline results.Background1.7 Description of Graph ReinforcementIn graph reinforcement, the mappings deducedfrom the alignment process were represented usinga bipartite graph G = (S, T, M), where S was theset of source language character sequences, T wasthe set of target language character sequences, andM was the set of mappings (links or edges)between S and T. The score of each mappingm(v1|v2), where m(v1|v2) ?
M, was initially set tothe conditional probability of target given sourcep(v1|v2).
Graph reinforcement was performed bytraversing the graph from S ?
T ?
S ?
T inorder to deduce new mappings.
Given a sourcesequence s' ?
S and a target sequence t' ?
?T, thededuced mapping probabilities were computed asfollows (Eq.1):?
(  |  )    ?
(  ?
(  | )?
( | )?
( |  ))where the term (  ?
(  | )?
( | )?
( |  ))computed the probability that a mapping is notcorrect.
Hence, the probability of an inferredmapping would be boosted if it was obtained frommultiple paths.
Given the example in Figure 1,m(c|?)
would be computed as follows:(  ?
( |?)?
(?| )?
( |?
))(  ?
( |?)?
(?| )?
( |?
))We were able to apply reinforcement iteratively onall mappings from S to T to deduce previouslyunseen mappings (graph edges) and to update theprobabilities of existing mappings.1.8 Link ReweightingThe basic graph reinforcement algorithm is proneto producing irrelevant mappings by usingcharacter sequences with many different possiblemappings as a bridge.
Vowels were the mostobvious examples of such character sequences.
Forexample, automatic alignment produced 26 Hindicharacter sequences that map to the English letter?a?, most of which were erroneous such as themapping between ?a?
and ???
(pronounced va).Graph reinforcement resulted in many more suchmappings.
After successive iterations, suchcharacter sequences would cause the graph to befully connected and eventually the link weightswill tend to be uniform in their values.
To illustratethis effect, we experimented with basic graphreinforcement on the NEWS dataset.
The figures ofmerit were precision, recall, and F1-measure.Figures 3, 4, 5, and 6 show reinforcement resultsfor Arabic, Russian, Hindi, and Tamil respectively.The figures show that: recall increased quickly andnearly saturated after several iterations; precisioncontinued to drop with more iterations; and F1-measure peaked after a few iterations and began todrop afterwards.
This behavior was undesirablebecause overall F1-measure values did notconverge with iterations, necessitating the need tofind clear stopping conditions.To avoid this effect and to improve precision, weapplied link reweighting after each iteration.
Linkreweighting had the effect of decreasing theweights of target character sequences that havemany source character sequences mapping to themand hence reducing the effect of incorrectlyinducing mappings.
Link reweighting wasperformed as follows (Eq.
2):?
( | )( | )?
(  | )Where si ?
S is a source character sequence thatmaps to t. So in the case of ?a?
mapping to the ??
?character in Hindi, the link weight from ?a?
to ??
?is divided by the sum of link weights from ?a?
toall 26 characters to which ?a?
maps.We performed multiple experiments on the NEWSdataset to test the effect of graph reinforcementwith link reweighting with varying number ofreinforcement iterations.
Figures 7, 8, 9, and 10compare baseline results with smoothing to resultswith graph reinforcement at different iterations.As can be seen in the figures, the F1-measurevalues stabilized as we performed multiple graphreinforcement iterations.
Except for Russian, theresults across different languages behaved in asimilar manner.For Russian, graph reinforcement marginallyaffected TM F1-measure, as precision and recall1389marginally changed.
The net improvement was 1.1basis points.
English and Russian do not share thesame alphabet, and the number of initial mappingswas bigger compared to the other language pairs.Careful inspection of the English-Russian test set,with the help of a Russian speaker, suggests that:1) the test set reference contained many falsenegatives;2) Russian names often have multiple phoneticforms (or spellings) in Russian with a singlestandard transliteration in English.
For example,the Russian name ?Olga?
is often written andpronounced as ?Ola?
and ?Olga?
in Russian; and3) certain English phones do not exist in Russian,leading to inconsistent character mappings inRussian.
For example, the English phone for ?g?,as in ?George?, does not exist in Russian.For the other languages, graph reinforcementyielded steadily improving recall and consequentlysteadily improving F1-measure.
Mostimprovements were achieved within the first 5iterations, and improvements beyond 10 iterationswere generally small (less than 0.5 basis points inF1-measure).
After 15 iterations, the improvementsin overall F1-measure above the baseline withsmoothing were 19.3, 5.3, and 32.8 basis points forArabic, Tamil, and Hindi respectively.
The F1-measure values seemed to stabilize with successiveiterations.
The least improvements were observedfor Hindi.
This could be attributed to the fact thatHindi spelling is largely phonetic, making letters inwords pronounceable in only one way.
This factmakes transliteration between Hindi and Englisheasier than Arabic and Tamil.
In the case of Tamil,the phonetics of letters change depending on theposition of letters in words.
As for Arabic, multipleletters sequences in English can map to singleletters in Arabic and vice versa.
Also, Arabic hasdiacritics which are typically omitted, butcommonly transliterate to English vowels.
Thus,the greater the difference in phonetics between twolanguages and the greater the phonetic complexityof either, the more TM can gain from the proposedtechnique.1.9 When Graph Reinforcement WorkedAn example where reinforcement worked entailsthe English-Arabic transliteration pair (Seljuq,?????).
In the baseline runs with 1,000 trainingexamples, both were not mapped to each otherbecause there were no mappings between the letter?q?
and the Arabic letter sequence ????
(pronounced as ?qah?).
The only mappings thatwere available for ?q?
were ????
(pronounced as?kah?
), ???
(pronounced as ?q?
), and ???
(pronounced as ?k?)
with probabilities 54.0, 0.10,and 5452 respectively.
Intuitively, the thirdmapping is more likely than the second.
After 3graph reinforcement iterations, the top 5 mappingsfor ?q?
were ???
(pronounced as ?q?
), ????
(pronounced as ?qah?
), ????
(pronounced as?kah?
), ???
(pronounced as ?k?
), and ?????
(pronounced as ?alq?)
with mapping probabilitiesof 0.22, 0.19, 0.15, 0.05, and 0.05 respectively.
Inthis case, graph reinforcement was able to find themissing mapping and properly reorder themappings.
Performing 10 iterations with linkreweighting for Arabic led to 17 false positives.Upon examining them, we found that: 9 wereactually correct, but erroneously labeled as false inthe test set; 6 were phonetically similar like ?????????
(pronounced espanya) and ?Spain?
and ?????????????
(pronounced alteknologya) and ?technology?
; andthe remaining 2 were actually wrong, which were???????
(pronounced beatchi) and ?medici?
and?
?????
(pronounced sidi) and ?taya?.
This seems toindicate that graph reinforcement generallyintroduced more proper mappings than improperones.1.10 Comparing to the State-of-the-ArtTable 2 compares the best reported results in ACL-2010 NEWS TM shared task for Arabic (Noemanand Madkour, 2010) and for the other languages(Jiampojamarn et al 2010) and the results obtainedby the proposed technique using 10 iterations, withlink reweighting.
The comparison shows that theproposed algorithm yielded better results than thebest reported results in the literature by 2.6, 4.8,0.8 and 4.1 F1-measure points in Arabic, Russian,Hindi and Tamil respectively.
For Arabic, theimprovement over the previously reported resultwas due to improvement in precision, while for theother languages the improvements were due toimprovements in both recall and precision.1390Figure 3: Graph reinforcement w/o link reweightingfor ArabicFigure 4: Graph reinforcement w/o link reweightingfor RussianFigure 5: Graph reinforcement w/o link reweightingfor HindiFigure 6: Graph reinforcement w/o link reweightingfor TamilFigure 7:  Graph reinforcement results for ArabicFigure 8: Graph reinforcement results for RussianFigure 9:  Graph reinforcement results for HindiFigure 10:  Graph reinforcement results for Tamil0.4000.5000.6000.7000.8000.9001.000baseline 1 2 3 4 5 6 7 8 910IterationsFRP0.4000.5000.6000.7000.8000.9001.000baseline 1 2 3 4 5 6 7 8 910IterationsFRP0.4000.5000.6000.7000.8000.9001.000baseline 1 2 3 4 5 6 7 8 910IterationsFRP0.4000.5000.6000.7000.8000.9001.000baseline 1 2 3 4 5 6 7 8 910IterationsFRP0.820.840.860.880.900.920.940.960.981.001 2 3 4 5 6 7 8 9 101112131415Number of IterationsFRPbaselineF = 0.748R = 0.603P = 0.9830.820.840.860.880.900.920.940.960.981.001 2 3 4 5 6 7 8 9 101112131415Number of IterationsFRPbaselineF = 0.912R = 0.862P = 0.9670.820.840.860.880.900.920.940.960.981.001 2 3 4 5 6 7 8 9 101112131415Number of IterationsFRPbaselineF = 0.879R = 0.796P = 0.9810.820.840.860.880.900.920.940.960.981.001 2 3 4 5 6 7 8 9 101112131415Number of IterationsFRPbaselineF = 0.626R = 0.460P = 0.9811391Shared Task Proposed AlgorithmEnglish/ P R F P R FArabic 0.887 0.945 0.915 0.979 0.905 0.941Russian 0.880 0.869 0.875 0.921 0.925 0.923Hindi 0.954 0.895 0.924 0.972 0.895 0.932Tamil 0.923 0.906 0.914 0.964 0.945 0.955Table 2: Best results obtained in ACL-2010 NEWS TMshared task compared to graph reinforcement with linkreweighting after 10 iterationsConclusionIn this paper, we presented a graph reinforcementalgorithm with link reweighting to improvetransliteration mining recall and precision bysystematically inferring mappings that were unseenin training.
We used the improved technique toextract transliteration pairs from parallel Wikipediatitles.
The proposed technique solves two problemsin transliteration mining, namely: some mappingsmay not be seen in training data ?
hurting recall,and certain mappings may not be seen a sufficientnumber of times to appropriate estimate mappingprobabilities ?
hurting precision.
The resultsshowed that graph reinforcement yielded improvedtransliteration mining from parallel Wikipediatitles for all four languages on which the techniquewas tested.Generally iterative graph reinforcement was able toinduce unseen mappings in training data ?improving recall.
Link reweighting favoredprecision over recall counterbalancing the effect ofgraph reinforcement.
The proposed systemoutperformed the best reported results in theliterature for the ACL-2010 NEWS workshopshared task for Arabic, Russian, Hindi and Tamil.To extend the work, we would like to trytransliteration mining from large comparable texts.The test parts of the NEWS dataset only containedshort parallel fragments.
For future work, graphreinforcement could be extended to MT to improvethe coverage of aligned phrase tables.
In doing so,it is reasonable to assume that there are multipleways of expressing a singular concept and hencemultiple translations are possible.
Using graphreinforcement can help discover such translationthough they may never be seen in training data.Using link reweighting in graph reinforcement canhelp demote unlikely translations while promotinglikely ones.
This could help clean MT phrasetables.
Further, when dealing with transliteration,graph reinforcement can help find phoneticvariations within a single language, which canhave interesting applications in spelling correctionand information retrieval.
Applying the same tomachine translation phrase tables can help identifyparaphrases automatically.ReferencesColin Bannard, Chris Callison-Burch.
2005.Paraphrasing with Bilingual Parallel Corpora.
ACL-2005, pages 597?604.Slaven Bilac, Hozumi Tanaka.
2005.
Extractingtransliteration pairs from comparable corpora.
NLP-2005.Eric Brill, Gary Kacmarcik, Chris Brockett.
2001.Automatically harvesting Katakana-English termpairs from search engine query logs.
NLPRS 2001,pages 393?399.Kareem Darwish.
2010.
Transliteration Mining withPhonetic Conflation and Iterative Training.
ACLNEWS workshop 2010.Huang Fei, Stephan Vogel, and Alex Waibel.
2003.Extracting Named Entity Translingual Equivalencewith Limited Resources.
TALIP, 2(2):124?129.Xiaodong He.
2007.
Using Word-Dependent TransitionModels in HMM based Word Alignment forStatistical Machine Translation.
ACL-07 2nd SMTworkshop.Sittichai Jiampojamarn, Kenneth Dwyer, ShaneBergsma, Aditya Bhargava, Qing Dou, Mi-YoungKim and Grzegorz Kondrak.
2010.
TransliterationGeneration and Mining with Limited TrainingResources.
ACL NEWS workshop 2010.Chengguo Jin, Dong-Il Kim, Seung-Hoon Na, Jong-Hyeok Lee.
2008.
Automatic Extraction of English-Chinese Transliteration Pairs using DynamicWindow and Tokenizer.
Sixth SIGHAN Workshopon Chinese Language Processing, 2008.Alexandre Klementiev and Dan Roth.
2006.
NamedEntity Transliteration and Discovery fromMultilingual Comparable Corpora.
HLT Conf.
of theNorth American Chapter of the ACL, pages 82?88.Stanley Kok, Chris Brockett.. 2010.
Hitting the RightParaphrases in Good Time.
Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the ACL, June 2010A.
Kumaran, Mitesh M. Khapra, Haizhou Li.
2010.Report of NEWS 2010 Transliteration Mining SharedTask.
Proceedings of the 2010 Named Entities1392Workshop, ACL 2010, pages 21?28, Uppsala,Sweden, 16 July 2010.Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang.
2006.Learning Transliteration Lexicons from the Web.COLING-ACL2006, Sydney, Australia, 1129 ?
1136.Jin-shea Kuo, Haizhou Li, Ying-kuei Yang.
2007.
Aphonetic similarity model for automatic extraction oftransliteration pairs.
TALIP, 2007Jin-Shea Kuo, Haizhou Li, Chih-Lung Lin.
2008.Mining Transliterations from Web Query Results: AnIncremental Approach.
Sixth SIGHAN Workshop onChinese Language Processing, 2008.Jin-shea Kuo, Ying-kuei Yang.
2005.
IncorporatingPronunciation Variation into Extraction ofTransliterated-term Pairs from Web Corpora.
Journalof Chinese Language and Computing, 15 (1): (33-44).Chun-Jen Lee, Jason S. Chang.
2003.
Acquisition ofEnglish-Chinese transliterated word pairs fromparallel-aligned texts using a statistical machinetransliteration model.
Workshop on Building andUsing Parallel Texts, HLT-NAACL-2003, 2003.Sara Noeman and Amgad Madkour.
2010.
LanguageIndependent Transliteration Mining System UsingFinite State Automata Framework.
ACL NEWSworkshop 2010.R.
Mahesh, K. Sinha.
2009.
Automated Mining OfNames Using Parallel Hindi-English Corpus.
7thWorkshop on Asian Language Resources, ACL-IJCNLP 2009, pages 48?54, 2009.Jong-Hoon Oh, Key-Sun Choi.
2006.
Recognizingtransliteration equivalents for enriching domainspecific thesauri.
3rd Intl.
WordNet Conf.
(GWC-06), pages 231?237, 2006.Jong-Hoon Oh, Hitoshi Isahara.
2006.
Mining the Webfor Transliteration Lexicons: Joint-ValidationApproach.
pp.254-261, 2006 IEEE/WIC/ACM Intl.Conf.
on Web Intelligence (WI'06), 2006.Yan Qu, Gregory Grefenstette, David A. Evans.
2003.Automatic transliteration for Japanese-to-English textretrieval.
SIGIR 2003:353-360Robert Russell.
1918.
Specifications of Letters.
USpatent number 1,261,167.K Saravanan, A Kumaran.
2008.
Some Experiments inMining Named Entity Transliteration Pairs fromComparable Corpora.
The 2nd Intl.
Workshop onCross Lingual Information Access: Addressing theNeed of Multilingual Societies, 2008.Raghavendra Udupa, K. Saravanan, Anton Bakalov,Abhijit Bhole.
2009a.
"They Are Out There, If YouKnow Where to Look": Mining Transliterations ofOOV Query Terms for Cross-Language InformationRetrieval.
ECIR-2009, Toulouse, France, 2009.Raghavendra Udupa, K. Saravanan, A. Kumaran, andJagadeesh Jagarlamudi.
2009b.
MINT: A Method forEffective and Scalable Mining of Named EntityTransliterations from Large Comparable Corpora.EACL 2009.Raghavendra Udupa and Mitesh Khapra.
2010a.Transliteration Equivalence using CanonicalCorrelation Analysis.
ECIR-2010, 2010.Raghavendra Udupa, Shaishav Kumar.
2010b.
Hashing-based Approaches to Spelling Correction of PersonalNames.
EMNLP 2010.Gae-won You, Seung-won Hwang, Young-In Song,Long Jiang, Zaiqing Nie.
2010.
Mining NameTranslations from Entity Graph Mapping.Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages430?439.Shiqi Zhao, Haifeng Wang, Ting Liu, Sheng Li.
2008.Pivot Approach for Extracting Paraphrase Patternsfrom Bilingual Corpora.
Proceedings of ACL-08:HLT, pages 780?788.1393
