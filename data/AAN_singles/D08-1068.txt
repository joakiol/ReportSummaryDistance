Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 650?659,Honolulu, October 2008. c?2008 Association for Computational LinguisticsJoint Unsupervised Coreference Resolution with Markov LogicHoifung Poon Pedro DomingosDepartment of Computer Science and EngineeringUniversity of WashingtonSeattle, WA 98195-2350, U.S.A.{hoifung,pedrod}@cs.washington.eduAbstractMachine learning approaches to coreferenceresolution are typically supervised, and re-quire expensive labeled data.
Some unsuper-vised approaches have been proposed (e.g.,Haghighi and Klein (2007)), but they are lessaccurate.
In this paper, we present the first un-supervised approach that is competitive withsupervised ones.
This is made possible byperforming joint inference across mentions,in contrast to the pairwise classification typ-ically used in supervised methods, and by us-ingMarkov logic as a representation language,which enables us to easily express relationslike apposition and predicate nominals.
OnMUC and ACE datasets, our model outper-forms Haghigi and Klein?s one using only afraction of the training data, and often matchesor exceeds the accuracy of state-of-the-art su-pervised models.1 IntroductionThe goal of coreference resolution is to identifymentions (typically noun phrases) that refer to thesame entities.
This is a key subtask in many NLPapplications, including information extraction, ques-tion answering, machine translation, and others.
Su-pervised learning approaches treat the problem asone of classification: for each pair of mentions,predict whether they corefer or not (e.g., McCal-lum & Wellner (2005)).
While successful, theseapproaches require labeled training data, consistingof mention pairs and the correct decisions for them.This limits their applicability.Unsupervised approaches are attractive due to theavailability of large quantities of unlabeled text.However, unsupervised coreference resolution ismuch more difficult.
Haghighi and Klein?s (2007)model, the most sophisticated to date, still lags su-pervised ones by a substantial margin.
Extending itappears difficult, due to the limitations of its Dirich-let process-based representation.The lack of label information in unsupervisedcoreference resolution can potentially be overcomeby performing joint inference, which leverages the?easy?
decisions to help make related ?hard?
ones.Relations that have been exploited in supervisedcoreference resolution include transitivity (McCal-lum & Wellner, 2005) and anaphoricity (Denis &Baldridge, 2007).
However, there is little work todate on joint inference for unsupervised resolution.We address this problem using Markov logic,a powerful and flexible language that combinesprobabilistic graphical models and first-order logic(Richardson & Domingos, 2006).
Markov logicallows us to easily build models involving rela-tions among mentions, like apposition and predi-cate nominals.
By extending the state-of-the-art al-gorithms for inference and learning, we developedthe first general-purpose unsupervised learning al-gorithm for Markov logic, and applied it to unsuper-vised coreference resolution.We test our approach on standard MUC and ACEdatasets.
Our basic model, trained on a minimumof data, suffices to outperform Haghighi and Klein?s(2007) one.
Our full model, using apposition andother relations for joint inference, is often as accu-rate as the best supervised models, or more.650We begin by reviewing the necessary backgroundon Markov logic.
We then describe our Markovlogic network for joint unsupervised coreferenceresolution, and the learning and inference algorithmswe used.
Finally, we present our experiments and re-sults.2 Related WorkMost existing supervised learning approaches forcoreference resolution are suboptimal since they re-solve each mention pair independently, only impos-ing transitivity in postprocessing (Ng, 2005).
More-over, many of them break up the resolution step intosubtasks (e.g., first determine whether a mention isanaphoric, then classify whether it is coreferent withan antecedent), which further forsakes opportunitiesfor joint inference that have been shown to be help-ful (Poon & Domingos, 2007).
Using graph parti-tioning, McCallum & Wellner (2005) incorporatedtransitivity into pairwise classification and achievedthe state-of-the-art result on the MUC-6 dataset, buttheir approach can only leverage one binary relationat a time, not arbitrary relations among mentions.Denis & Baldridge (2007) determined anaphoricityand pairwise classification jointly using integer pro-gramming, but they did not incorporate transitivityor other relations.While potentially more appealing, unsupervisedlearning is very challenging, and unsupervisedcoreference resolution systems are still rare to thisdate.
Prior to our work, the best performance inunsupervised coreference resolution was achievedby Haghighi & Klein (2007), using a nonparamet-ric Bayesian model based on hierarchical Dirichletprocesses.
At the heart of their system is a mixturemodel with a few linguistically motivated featuressuch as head words, entity properties and salience.Their approach is a major step forward in unsuper-vised coreference resolution, but extending it is chal-lenging.
The main advantage of Dirichlet processesis that they are exchangeable, allowing parametersto be integrated out, but Haghighi and Klein forgothis when they introduce salience.
Their model thusrequires Gibbs sampling over both assignments andparameters, which can be very expensive.
Haghighiand Klein circumvent this by making approxima-tions that potentially hurt accuracy.
At the sametime, the Dirichlet process prior favors skewed clus-ter sizes and a number of clusters that grows loga-rithmically with the number of data points, neither ofwhich seems generally appropriate for coreferenceresolution.Further, deterministic or strong non-deterministicdependencies cause Gibbs sampling to break down(Poon & Domingos, 2006), making it difficult toleverage many linguistic regularities.
For exam-ple, apposition (as in ?Bill Gates, the chairman ofMicrosoft?)
suggests coreference, and thus the twomentions it relates should always be placed in thesame cluster.
However, Gibbs sampling can onlymove one mention at a time from one cluster toanother, and this is unlikely to happen, because itwould require breaking the apposition rule.
Blockedsampling can alleviate this problem by samplingmultiple mentions together, but it requires that theblock size be predetermined to a small fixed number.When we incorporate apposition and other regular-ities the blocks can become arbitrarily large, mak-ing this infeasible.
For example, suppose we alsowant to leverage predicate nominals (i.e., the sub-ject and the predicating noun of a copular verb arelikely coreferent).
Then a sentence like ?He is BillGates, the chairman of Microsoft?
requires a blockof four mentions: ?He?, ?Bill Gates?, ?the chair-man of Microsoft?, and ?Bill Gates, the chairmanof Microsoft?.
Similar difficulties occur with otherinference methods.
Thus, extending Haghighi andKlein?s model to include richer linguistic features isa challenging problem.Our approach is instead based on Markov logic,a powerful representation for joint inference withuncertainty (Richardson & Domingos, 2006).
LikeHaghighi and Klein?s, our model is cluster-basedrather than pairwise, and implicitly imposes tran-sitivity.
We do not predetermine anaphoricity of amention, but rather fuse it into the integrated reso-lution process.
As a result, our model is inherentlyjoint among mentions and subtasks.
It shares sev-eral features with Haghighi & Klein?s model, but re-moves or refines features where we believe it is ap-propriate to.
Most importantly, our model leveragesapposition and predicate nominals, which Haghighi& Klein did not use.
We show that this can be donevery easily in our framework, and yet results in verysubstantial accuracy gains.651It is worth noticing that Markov logic is also wellsuited for joint inference in supervised systems (e.g.,transitivity, which tookMcCallum&Wellner (2005)nontrivial effort to incorporate, can be handled inMarkov logic with the addition of a single formula(Poon & Domingos, 2008)).3 Markov LogicIn many NLP applications, there exist rich relationsamong objects, and recent work in statistical rela-tional learning (Getoor & Taskar, 2007) and struc-tured prediction (Bakir et al, 2007) has shown thatleveraging these can greatly improve accuracy.
Oneof the most powerful representations for joint infer-ence is Markov logic, a probabilistic extension offirst-order logic (Richardson & Domingos, 2006).
AMarkov logic network (MLN) is a set of weightedfirst-order clauses.
Together with a set of con-stants, it defines a Markov network with one nodeper ground atom and one feature per ground clause.The weight of a feature is the weight of the first-order clause that originated it.
The probability ofa state x in such a network is given by P (x) =(1/Z) exp (?i wifi(x)), where Z is a normaliza-tion constant, wi is the weight of the ith clause,fi = 1 if the ith clause is true, and fi = 0 other-wise.Markov logic makes it possible to compactlyspecify probability distributions over complex re-lational domains.
Efficient inference can be per-formed using MC-SAT (Poon & Domingos, 2006).MC-SAT is a ?slice sampling?
Markov chain MonteCarlo algorithm.
Slice sampling introduces auxil-iary variables u that decouple the original ones x,and alternately samples u conditioned on x and vice-versa.
To sample from the slice (the set of states xconsistent with the current u), MC-SAT calls Sam-pleSAT (Wei et al, 2004), which uses a combina-tion of satisfiability testing and simulated annealing.The advantage of using a satisfiability solver (Walk-SAT) is that it efficiently finds isolated modes in thedistribution, and as a result the Markov chain mixesvery rapidly.
The slice sampling scheme ensuresthat detailed balance is (approximately) preserved.MC-SAT is orders of magnitude faster than previousMCMC algorithms like Gibbs sampling, making ef-ficient sampling possible on a scale that was previ-Algorithm 1 MC-SAT(clauses, weights,num samples)x(0) ?
Satisfy(hard clauses)for i?
1 to num samples doM ?
?for all ck ?
clauses satisfied by x(i?1) doWith probability 1?
e?wk add ck to Mend forSample x(i) ?
USAT (M)end forously out of reach.Algorithm 1 gives pseudo-code for MC-SAT.
Atiteration i ?
1, the factor ?k for clause ck is ei-ther ewk if ck is satisfied in x(i?1), or 1 otherwise.MC-SAT first samples the auxiliary variable uk uni-formly from (0, ?k), then samples a new state uni-formly from the set of states that satisfy ?
?k ?
ukfor all k (the slice).
Equivalently, for each k, withprobability 1 ?
e?wk the next state must satisfy ck.In general, we can factorize the probability distribu-tion in any way that facilitates inference, sample theuk?s, and make sure that the next state is drawn uni-formly from solutions that satisfy ?
?k ?
uk for allfactors.MC-SAT, like most existing relational inferencealgorithms, grounds all predicates and clauses, thusrequiring memory and time exponential in the pred-icate and clause arities.
We developed a generalmethod for producing a ?lazy?
version of relationalinference algorithms (Poon & Domingos, 2008),which carries exactly the same inference steps as theoriginal algorithm, but only maintains a small sub-set of ?active?
predicates/clauses, grounding moreas needed.
We showed that Lazy-MC-SAT, the lazyversion of MC-SAT, reduced memory and time byorders of magnitude in several domains.
We useLazy-MC-SAT in this paper.Supervised learning for Markov logic maximizesthe conditional log-likelihoodL(x, y) = logP (Y =y|X = x), where Y represents the non-evidencepredicates, X the evidence predicates, and x, y theirvalues in the training data.
For simplicity, from nowon we omit X , whose values are fixed and alwaysconditioned on.
The optimization problem is convexand a global optimum can be found using gradient652descent, with the gradient being?
?wiL(y) = ni(y)??y?
P (Y = y?)ni(y?
)= ni(y)?
EY [ni].where ni is the number of true groundings of clausei.
The expected count can be approximated asEY [ni] ?1NN?k=1ni(yk)where yk are samples generated by MC-SAT.
Tocombat overfitting, a Gaussian prior is imposed onall weights.In practice, it is difficult to tune the learning ratefor gradient descent, especially when the numberof groundings varies widely among clauses.
Lowd& Domingos (2007) used a preconditioned scaledconjugate gradient algorithm (PSCG) to address thisproblem.
This estimates the optimal step size in eachstep as?
=?dT gdTHd + ?dTd.where g is the gradient, d the conjugate update direc-tion, and ?
a parameter that is automatically tunedto trade off second-order information with gradientdescent.
H is the Hessian matrix, with the (i, j)thentry being?2?wi?wjL(y) = EY [ni] ?
EY [nj ]?
EY [ni ?
nj ]= ?CovY [ni, nj ].The Hessian can be approximated with the samesamples used for the gradient.
Its negative inversediagonal is used as the preconditioner.1The open-source Alchemy package (Kok et al,2007) provides implementations of existing algo-rithms for Markov logic.
In Section 5, we developthe first general-purpose unsupervised learning al-gorithm for Markov logic by extending the existingalgorithms to handle hidden predicates.21Lowd & Domingos showed that ?
can be computed moreefficiently, without explicitly approximating or storing the Hes-sian.
Readers are referred to their paper for details.2Alchemy includes a discriminative EM algorithm, but it as-sumes that only a few values are missing, and cannot handlecompletely hidden predicates.
Kok & Domingos (2007) appliedMarkov logic to relational clustering, but they used hard EM.4 An MLN for Joint UnsupervisedCoreference ResolutionIn this section, we present our MLN for joint unsu-pervised coreference resolution.
Our model deviatesfrom Haghighi & Klein?s (2007) in several impor-tant ways.
First, our MLN does not model saliencesfor proper nouns or nominals, as their influence ismarginal compared to other features; for pronounsalience, it uses a more intuitive and simpler def-inition based on distance, and incorporated it as aprior.
Another difference is in identifying heads.
Forthe ACE datasets, Haghighi and Klein used the goldheads; for the MUC-6 dataset, where labels are notavailable, they crudely picked the rightmost token ina mention.
We show that a better way is to determinethe heads using head rules in a parser.
This improvesresolution accuracy and is always applicable.
Cru-cially, our MLN leverages syntactic relations suchas apposition and predicate nominals, which are notused by Haghighi and Klein.
In our approach, whatit takes is just adding two formulas to the MLN.As common in previous work, we assume thattrue mention boundaries are given.
We do not as-sume any other labeled information.
In particu-lar, we do not assume gold name entity recogni-tion (NER) labels, and unlike Haghighi & Klein(2007), we do not assume gold mention types (forACE datasets, they also used gold head words).
Wedetermined the head of a mention either by takingits rightmost token, or by using the head rules in aparser.
We detected pronouns using a list.4.1 Base MLNThe main query predicate is InClust(m, c!
), whichis true iff mention m is in cluster c. The ?t!?
notationsignifies that for each m, this predicate is true for aunique value of c. The main evidence predicate isHead(m, t!
), where m is a mention and t a token, andwhich is true iff t is the head of m. A key componentin our MLN is a simple head mixture model, wherethe mixture component priors are represented by theunit clauseInClust(+m,+c)and the head distribution is represented by the headprediction ruleInClust(m,+c) ?
Head(m,+t).653All free variables are implicitly universally quanti-fied.
The ?+?
notation signifies that the MLN con-tains an instance of the rule, with a separate weight,for each value combination of the variables with aplus sign.By convention, at each inference step we nameeach non-empty cluster after the earliest mention itcontains.
This helps break the symmetry amongmentions, which otherwise produces multiple op-tima and makes learning unnecessarily harder.
Toencourage clustering, we impose an exponentialprior on the number of non-empty clusters withweight ?1.The above model only clusters mentions with thesame head, and does not work well for pronouns.
Toaddress this, we introduce the predicate IsPrn(m),which is true iff the mention m is a pronoun, andadapt the head prediction rule as follows:?IsPrn(m) ?
InClust(m,+c) ?
Head(m,+t)This is always false when m is a pronoun, and thusapplies only to non-pronouns.Pronouns tend to resolve with men-tions that are semantically compatible withthem.
Thus we introduce predicates thatrepresent entity type, number, and gender:Type(x, e!
), Number(x, n!
), Gender(x, g!
),where x can be either a cluster or mention,e ?
{Person, Organization, Location, Other},n ?
{Singular, Plural} and g ?
{Male, Female, Neuter}.
Many of these areknown for pronouns, and some can be inferredfrom simple linguistic cues (e.g., ?Ms.
Galen?is a singular female person, while ?XYZ Corp.?is an organization).3 Entity type assignment isrepresented by the unit clauseType(+x,+e)and similarly for number and gender.
A mentionshould agree with its cluster in entity type.
This isensured by the hard rule (which has infinite weightand must be satisfied)InClust(m, c)?
(Type(m, e)?
Type(c, e))3We used the following cues: Mr., Ms., Jr., Inc., Corp., cor-poration, company.
The proportions of known properties rangefrom 14% to 26%.There are similar hard rules for number and gender.Different pronouns prefer different entity types,as represented byIsPrn(m) ?
InClust(m, c)?Head(m,+t) ?
Type(c,+e)which only applies to pronouns, and whose weight ispositive if pronoun t is likely to assume entity typee and negative otherwise.
There are similar rules fornumber and gender.Aside from semantic compatibility, pronouns tendto resolve with nearby mentions.
To model this, weimpose an exponential prior on the distance (numberof mentions) between a pronoun and its antecedent,with weight ?1.4 This is similar to Haghighi andKlein?s treatment of salience, but simpler.4.2 Full MLNSyntactic relations among mentions often suggestcoreference.
Incorporating such relations into ourMLN is straightforward.
We illustrate this withtwo examples: apposition and predicate nominals.We introduce a predicate for apposition, Appo(x, y),where x, y are mentions, and which is true iff y is anappositive of x.
We then add the ruleAppo(x, y)?
(InClust(x, c)?
InClust(y, c))which ensures that x, y are in the same cluster if y isan appositive of x.
Similarly, we introduce a predi-cate for predicate nominals, PredNom(x, y), and thecorresponding rule.5 The weights of both rules canbe learned from data with a positive prior mean.
Forsimplicity, in this paper we treat them as hard con-straints.4.3 Rule-Based MLNWe also consider a rule-based system that clustersnon-pronouns by their heads, and attaches a pro-noun to the cluster which has no known conflicting4For simplicity, if a pronoun has no antecedent, we definethe distance to be?.
So a pronoun must have an antecedent inour model, unless it is the first mention in the document or it cannot resolve with previous mentions without violating hard con-straints.
It is straightforward to soften this with a finite penalty.5We detected apposition and predicate nominatives usingsimple heuristics based on parses, e.g., if (NP, comma, NP) arethe first three children of an NP, then any two of the three nounphrases are apposition.654type, number, or gender, and contains the closest an-tecedent for the pronoun.
This system can be en-coded in an MLN with just four rules.
Three of themare the ones for enforcing agreement in type, num-ber, and gender between a cluster and its members,as defined in the base MLN.
The fourth rule is?IsPrn(m1) ?
?IsPrn(m2)?Head(m1, h1) ?
Head(m2, h2)?InClust(m1, c1) ?
InClust(m2, c2)?
(c1 = c2?
h1 = h2).With a large but not infinite weight (e.g., 100),this rule has the effect of clustering non-pronounsby their heads, except when it violates the hardrules.
The MLN can also include the apposition andpredicate-nominal rules.
As in the base MLN, weimpose the same exponential prior on the number ofnon-empty clusters and that on the distance betweena pronoun and its antecedent.
This simple MLN isremarkably competitive, as we will see in the exper-iment section.5 Learning and InferenceUnsupervised learning in Markov logic maximizesthe conditional log-likelihoodL(x, y) = logP (Y = y|X = x)= log?z P (Y = y, Z = z|X = x)where Z are unknown predicates.
In our coref-erence resolution MLN, Y includes Head andknown groundings of Type, Number and Gender,Z includes InClust and unknown groundings ofType, Number, Gender, and X includes IsPrn,Appo and PredNom.
(For simplicity, from now onwe drop X from the formula.)
With Z, the opti-mization problem is no longer convex.
However, wecan still find a local optimum using gradient descent,with the gradient being?
?wiL(y) = EZ|y[ni]?
EY,Z [ni]where ni is the number of true groundings of the ithclause.
We extended PSCG for unsupervised learn-ing.
The gradient is the difference of two expec-tations, each of which can be approximated usingsamples generated by MC-SAT.
The (i, j)th entry ofthe Hessian is now?2?wi?wjL(y) = CovZ|y[ni, nj ]?
CovY,Z [ni, nj ]and the step size can be computed accordingly.Since our problem is no longer convex, the nega-tive diagonal Hessian may contain zero or negativeentries, so we first took the absolute values of thediagonal and added 1, then used the inverse as thepreconditioner.
We also adjusted ?
more conserva-tively than Lowd & Domingos (2007).Notice that when the objects form independentsubsets (in our cases, mentions in each document),we can process them in parallel and then gather suf-ficient statistics for learning.
We developed an ef-ficient parallelized implementation of our unsuper-vised learning algorithm using the message-passinginterface (MPI).
Learning in MUC-6 took only onehour, and in ACE-2004 two and a half.To reduce burn-in time, we initialized MC-SATwith the state returned by MaxWalkSAT (Kautz etal., 1997), rather than a random solution to the hardclauses.
In the existing implementation in Alchemy(Kok et al, 2007), SampleSAT flips only one atomin each step, which is inefficient for predicates withunique-value constraints (e.g., Head(m, c!)).
Suchpredicates can be viewed as multi-valued predi-cates (e.g., Head(m) with value ranging over allc?s) and are prevalent in NLP applications.
Weadapted SampleSAT to flip two or more atoms ineach step so that the unique-value constraints areautomatically satisfied.
By default, MC-SAT treatseach ground clause as a separate factor while de-termining the slice.
This can be very inefficientfor highly correlated clauses.
For example, givena non-pronoun mention m currently in cluster c andwith head t, among the mixture prior rules involv-ing m InClust(m, c) is the only one that is satisfied,and among those head-prediction rules involving m,?IsPrn(m)?InClust(m, c)?Head(m, t) is the onlyone that is satisfied; the factors for these rules mul-tiply to ?
= exp(wm,c + wm,c,t), where wm,c is theweight for InClust(m, c), and wm,c,t is the weightfor ?IsPrn(m)?InClust(m, c)?Head(m, t), sincean unsatisfied rule contributes a factor of e0 = 1.
Weextended MC-SAT to treat each set of mutually ex-clusive and exhaustive rules as a single factor.
E.g.,for the above m, MC-SAT now samples u uniformly655from (0, ?
), and requires that in the next state ??
beno less than u. Equivalently, the new cluster andhead for m should satisfy wm,c?
+ wm,c?,t?
?
log(u).We extended SampleSAT so that when it consid-ers flipping any variable involved in such constraints(e.g., c or t above), it ensures that their new valuesstill satisfy these constraints.The final clustering is found using the MaxWalk-SAT weighted satisfiability solver (Kautz et al,1997), with the appropriate extensions.
We first rana MaxWalkSAT pass with only finite-weight formu-las, then ran another pass with all formulas.
Wefound that this significantly improved the quality ofthe results that MaxWalkSAT returned.6 Experiments6.1 SystemWe implemented our method as an extension to theAlchemy system (Kok et al, 2007).
Since our learn-ing uses sampling, all results are the average of fiveruns using different random seeds.
Our optimiza-tion problem is not convex, so initialization is im-portant.
The core of our model (head mixture) tendsto cluster non-pronouns with the same head.
There-fore, we initialized by setting all weights to zero,and running the same learning algorithm on the baseMLN, while assuming that in the ground truth, non-pronouns are clustered by their heads.
(Effectively,the corresponding InClust atoms are assigned toappropriate values and are included in Y rather thanZ during learning.)
We used 30 iterations of PSCGfor learning.
(In preliminary experiments, additionaliterations had little effect on coreference accuracy.
)We generated 100 samples using MC-SAT for eachexpectation approximation.66.2 MethodologyWe conducted experiments on MUC-6, ACE-2004,and ACE Phrase-2 (ACE-2).
We evaluated our sys-tems using two commonly-used scoring programs:MUC (Vilain et al, 1995) and B3 (Amit & Bald-win, 1998).
To gain more insight, we also reportpairwise resolution scores and mean absolute errorin the number of clusters.6Each sample actually contains a large number of ground-ings, so 100 samples yield sufficiently accurate statistics forlearning.The MUC-6 dataset consists of 30 documents fortesting and 221 for training.
To evaluate the contri-bution of the major components in our model, weconducted five experiments, each differing from theprevious one in a single aspect.
We emphasize thatour approach is unsupervised, and thus the data onlycontains raw text plus true mention boundaries.MLN-1 In this experiment, the base MLN wasused, and the head was chosen crudely as therightmost token in a mention.
Our system wasrun on each test document separately, using aminimum of training data (the document itself).MLN-30 Our system was trained on all 30 test doc-uments together.
This tests how much can begained by pooling information.MLN-H The heads were determined using the headrules in the Stanford parser (Klein & Manning,2003), plus simple heuristics to handle suffixessuch as ?Corp.?
and ?Inc.
?MLN-HA The apposition rule was added.MLN-HAN The predicate-nominal rule was added.This is our full model.We also compared with two rule-based MLNs:RULE chose the head crudely as the rightmost tokenin a mention, and did not include the apposition ruleand predicate-nominal rule; RULE-HAN chose thehead using the head rules in the Stanford parser, andincluded the apposition rule and predicate-nominalrule.Past results on ACE were obtained on differentreleases of the datasets, e.g., Haghighi and Klein(2007) used the ACE-2004 training corpus, Ng(2005) and Denis and Baldridge (2007) used ACEPhrase-2, and Culotta et al (2007) used the ACE-2004 formal test set.
In this paper, we used theACE-2004 training corpus and ACE Phrase-2 (ACE-2) to enable direct comparisons with Haghighi &Klein (2007), Ng (2005), and Denis and Baldridge(2007).
Due to license restrictions, we were not ableto obtain the ACE-2004 formal test set and so cannotcompare directly to Culotta et al (2007).
The En-glish version of the ACE-2004 training corpus con-tains two sections, BNEWS and NWIRE, with 220and 128 documents, respectively.
ACE-2 contains a656Table 1: Comparison of coreference results in MUCscores on the MUC-6 dataset.# Doc.
Prec.
Rec.
F1H&K 60 80.8 52.8 63.9H&K 381 80.4 62.4 70.3M&W 221 - - 73.4RULE - 76.0 65.9 70.5RULE-HAN - 81.3 72.7 76.7MLN-1 1 76.5 66.4 71.1MLN-30 30 77.5 67.3 72.0MLN-H 30 81.8 70.1 75.5MLN-HA 30 82.7 75.1 78.7MLN-HAN 30 83.0 75.8 79.2Table 2: Comparison of coreference results in MUCscores on the ACE-2004 (English) datasets.EN-BNEWS Prec.
Rec.
F1H&K 63.2 61.3 62.3MLN-HAN 66.8 67.8 67.3EN-NWIRE Prec.
Rec.
F1H&K 66.7 62.3 64.2MLN-HAN 71.3 70.5 70.9training set and a test set.
In our experiments, weonly used the test set, which contains three sections,BNEWS, NWIRE, and NPAPER, with 51, 29, and17 documents, respectively.6.3 ResultsTable 1 compares our system with previous ap-proaches on the MUC-6 dataset, in MUC scores.Our approach greatly outperformed Haghighi &Klein (2007), the state-of-the-art unsupervised sys-tem.
Our system, trained on individual documents,achieved an F1 score more than 7% higher thantheirs trained on 60 documents, and still outper-formed it trained on 381 documents.
Training onthe 30 test documents together resulted in a signif-icant gain.
(We also ran experiments using moredocuments, and the results were similar.)
Betterhead identification (MLN-H) led to a large improve-ment in accuracy, which is expected since for men-tions with a right modifier, the rightmost tokens con-fuse rather than help coreference (e.g., ?the chair-man of Microsoft?).
Notice that with this improve-ment our system already outperforms a state-of-the-Table 3: Comparison of coreference results in MUCscores on the ACE-2 datasets.BNEWS Prec.
Rec.
F1Ng 67.9 62.2 64.9D&B 78.0 62.1 69.2MLN-HAN 68.3 66.6 67.4NWIRE Prec.
Rec.
F1Ng 60.3 50.1 54.7D&B 75.8 60.8 67.5MLN-HAN 67.7 67.3 67.4NPAPER Prec.
Rec.
F1Ng 71.4 67.4 69.3D&B 77.6 68.0 72.5MLN-HAN 69.2 71.7 70.4Table 4: Comparison of coreference results in B3 scoreson the ACE-2 datasets.BNEWS Prec.
Rec.
F1Ng 77.1 57.0 65.6MLN-HAN 70.3 65.3 67.7NWIRE Prec.
Rec.
F1Ng 75.4 59.3 66.4MLN-HAN 74.7 68.8 71.6NPAPER Prec.
Rec.
F1Ng 75.4 59.3 66.4MLN-HAN 70.0 66.5 68.2art supervised system (McCallum & Wellner, 2005).Leveraging apposition resulted in another large im-provement, and predicate nominals also helped.
Ourfull model scores about 9% higher than Haghighi &Klein (2007), and about 6% higher than McCallum& Wellner (2005).
To our knowledge, this is the bestcoreference accuracy reported on MUC-6 to date.7The B3 scores of MLN-HAN on the MUC-6 datasetare 77.4 (precision), 67.6 (recall) and 72.2 (F1).
(The other systems did not report B3.)
Interest-ingly, the rule-based MLN (RULE) sufficed to out-perform Haghighi & Klein (2007), and by using bet-ter heads and the apposition and predicate-nominalrules (RULE-HAN), it outperformed McCallum &Wellner (2005), the supervised system.
The MLNswith learning (MLN-30 and MLN-HAN), on the7As pointed out by Haghighi & Klein (2007), Luo et al(2004) obtained a very high accuracy on MUC-6, but their sys-tem used gold NER features and is not directly comparable.657Table 5: Our coreference results in precision, recall, andF1 for pairwise resolution.Pairwise Prec.
Rec.
F1MUC-6 63.0 57.0 59.9EN-BNEWS 51.2 36.4 42.5EN-NWIRE 62.6 38.9 48.0BNEWS 44.6 32.3 37.5NWIRE 59.7 42.1 49.4NPAPER 64.3 43.6 52.0Table 6: Average gold number of clusters per documentvs.
the mean absolute error of our system.# Clusters MUC-6 EN-BN EN-NWGold 15.4 22.3 37.2Mean Error 4.7 3.0 4.8# Clusters BNEWS NWIRE NPAPERGold 20.4 39.2 55.2Mean Error 2.5 5.6 6.6other hand, substantially outperformed the corre-sponding rule-based ones.Table 2 compares our system to Haghighi & Klein(2007) on the ACE-2004 training set in MUC scores.Again, our system outperformed theirs by a largemargin.
The B3 scores of MLN-HAN on the ACE-2004 dataset are 71.6 (precision), 68.4 (recall) and70.0 (F1) for BNEWS, and 75.7 (precision), 69.2(recall) and 72.3 (F1) for NWIRE.
(Haghighi &Klein (2007) did not report B3.)
Due to license re-strictions, we could not compare directly to Culottaet al (2007), who reported overall B3-F1 of 79.3 onthe formal test set.Tables 3 and 4 compare our system to two re-cent supervised systems, Ng (2005) and Denis& Baldridge (2007).
Our approach significantlyoutperformed Ng (2005).
It tied with Denis &Baldridge (2007) on NWIRE, and was somewhatless accurate on BNEWS and NPAPER.Luo et al (2004) pointed out that one can ob-tain a very high MUC score simply by lumping allmentions together.
B3 suffers less from this prob-lem but is not perfect.
Thus we also report pairwiseresolution scores (Table 5), the gold number of clus-ters, and our mean absolute error in the number ofclusters (Table 6).
Systems that simply merge allmentions will have exceedingly low pairwise preci-sion (far below 50%), and very large errors in thenumber of clusters.
Our system has fairly good pair-wise precisions and small mean error in the numberof clusters, which verifies that our results are sound.6.4 Error AnalysisMany of our system?s remaining errors involve nom-inals.
Additional features should be considered todistinguish mentions that have the same head but aredifferent entities.
For pronouns, many remaining er-rors can be corrected using linguistic knowledge likebinding theory and salience hierarchy.
Our heuris-tics for identifying appositives and predicate nomi-nals also make many errors, which often can be fixedwith additional name entity recognition capabilities(e.g., given ?Mike Sullivan, VOA News?, it helps toknow that the former is a person and the latter anorganization).
The most challenging case involvesphrases with different heads that are both propernouns (e.g., ?Mr.
Bush?
and ?the White House?
).Handling these cases requires domain knowledgeand/or more powerful joint inference.7 ConclusionThis paper introduces the first unsupervised coref-erence resolution system that is as accurate as su-pervised systems.
It performs joint inference amongmentions, using relations like apposition and predi-cate nominals.
It uses Markov logic as a representa-tion language, which allows it to be easily extendedto incorporate additional linguistic and world knowl-edge.
Future directions include incorporating addi-tional knowledge, conducting joint entity detectionand coreference resolution, and combining corefer-ence resolution with other NLP tasks.8 AcknowledgementsWe thank the anonymous reviewers for their comments.This research was funded by DARPA contracts NBCH-D030010/02-000225, FA8750-07-D-0185, and HR0011-07-C-0060, DARPA grant FA8750-05-2-0283, NSF grantIIS-0534881, and ONR grant N-00014-05-1-0313 andN00014-08-1-0670.
The views and conclusions con-tained in this document are those of the authors andshould not be interpreted as necessarily representing theofficial policies, either expressed or implied, of DARPA,NSF, ONR, or the United States Government.658ReferencesAmit, B.
& Baldwin, B.
1998.
Algorithms for scoringcoreference chains.
In Proc.
MUC-7.Bakir, G.; Hofmann, T.; Scho?lkopf, B.; Smola, A.;Taskar, B. and Vishwanathan, S.
(eds.)
2007.
Pre-dicting Structured Data.
MIT Press.Culotta, A.; Wick, M.; Hall, R. and McCallum, A.
2007.First-order probabilistic models for coreference reso-lution.
In Proc.
NAACL-07.Denis, P. & Baldridge, J.
2007.
Joint determination ofanaphoricity and coreference resolution using integerprogramming.
In Proc.
NAACL-07.Getoor, L. & Taskar, B.
(eds.)
2007.
Introduction toStatistical Relational Learning.
MIT Press.Haghighi, A.
& Klein, D. 2007.
Unsupervised corefer-ence resolution in a nonparametric Bayesian model.
InProc.
ACL-07.Kautz, H.; Selman, B.; and Jiang, Y.
1997.
A generalstochastic approach to solving problems with hard andsoft constraints.
In The Satisfiability Problem: Theoryand Applications.
AMS.Klein, D. & Manning, C. 2003.
Accurate unlexicalizedparsing.
In Proc.
ACL-03.Kok, S.; Singla, P.; Richardson, M.; Domingos,P.
; Sumner, M.; Poon, H. & Lowd, D. 2007.The Alchemy system for statistical relational AI.http://alchemy.cs.washington.edu/.Lowd, D. & Domingos, D. 2007.
Efficient weight learn-ing for Markov logic networks.
In Proc.
PKDD-07.Luo, X.; Ittycheriah, A.; Jing, H.; Kambhatla, N. andRoukos, S. 2004.
A mention-synchronous corefer-ence resolution algorithm based on the bell tree.
InProc.
ACL-04.McCallum, A.
& Wellner, B.
2005.
Conditional modelsof identity uncertainty with application to noun coref-erence.
In Proc.
NIPS-04.Ng, V. 2005.
Machine Learning for Coreference Resolu-tion: From Local Classification to Global Ranking.
InProc.
ACL-05.Poon, H. & Domingos, P. 2006.
Sound and efficientinference with probabilistic and deterministic depen-dencies.
In Proc.
AAAI-06.Poon, H. & Domingos, P. 2007.
Joint inference in infor-mation extraction.
In Proc.
AAAI-07.Poon, H. & Domingos, P. 2008.
A general method forreducing the complexity of relational inference and itsapplication to MCMC.
In Proc.
AAAI-08.Richardson, M. & Domingos, P. 2006.
Markov logicnetworks.
Machine Learning 62:107?136.Vilain, M.; Burger, J.; Aberdeen, J.; Connolly, D. &Hirschman, L. 1995.
A model-theoretic coreferencescoring scheme.
In Proc.
MUC-6.Wei, W.; Erenrich, J. and Selman, B.
2004.
Towardsefficient sampling: Exploiting random walk strategies.In Proc.
AAAI-04.659
