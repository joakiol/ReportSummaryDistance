Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459?468,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsTranslation Model Adaptation for Statistical Machine Translation withMonolingual Topic Information?Jinsong Su1,2, Hua Wu3, Haifeng Wang3, Yidong Chen1, Xiaodong Shi1,Huailin Dong1, and Qun Liu2Xiamen University, Xiamen, China1Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China2Baidu Inc., Beijing, China3{jssu, ydchen, mandel, hldong}@xmu.edu.cn{wu hua, wanghaifeng}@baicu.comliuqun@ict.ac.cnAbstractTo adapt a translation model trained fromthe data in one domain to another, previousworks paid more attention to the studies ofparallel corpus while ignoring the in-domainmonolingual corpora which can be obtainedmore easily.
In this paper, we propose anovel approach for translation model adapta-tion by utilizing in-domain monolingual top-ic information instead of the in-domain bilin-gual corpora, which incorporates the topic in-formation into translation probability estima-tion.
Our method establishes the relationshipbetween the out-of-domain bilingual corpusand the in-domain monolingual corpora vi-a topic mapping and phrase-topic distributionprobability estimation from in-domain mono-lingual corpora.
Experimental result on theNIST Chinese-English translation task showsthat our approach significantly outperformsthe baseline system.1 IntroductionIn recent years, statistical machine translation(SMT)has been rapidly developing with more and morenovel translation models being proposed and put in-to practice (Koehn et al, 2003; Och and Ney, 2004;Galley et al, 2006; Liu et al, 2006; Chiang, 2007;Chiang, 2010).
However, similar to other naturallanguage processing(NLP) tasks, SMT systems of-ten suffer from domain adaptation problem duringpractical applications.
The simple reason is that theunderlying statistical models always tend to closely?Part of this work was done during the first author?s intern-ship at Baidu.approximate the empirical distributions of the train-ing data, which typically consist of bilingual sen-tences and monolingual target language sentences.When the translated texts and the training data comefrom the same domain, SMT systems can achievegood performance, otherwise the translation qualitydegrades dramatically.
Therefore, it is of significantimportance to develop translation systems which canbe effectively transferred from one domain to anoth-er, for example, from newswire to weblog.According to adaptation emphases, domain adap-tation in SMT can be classified into translation mod-el adaptation and language model adaptation.
Herewe focus on how to adapt a translation model, whichis trained from the large-scale out-of-domain bilin-gual corpus, for domain-specific translation task,leaving others for future work.
In this aspect, pre-vious methods can be divided into two categories:one paid attention to collecting more sentence pairsby information retrieval technology (Hildebrand etal., 2005) or synthesized parallel sentences (Ueffinget al, 2008; Wu et al, 2008; Bertoldi and Federico,2009; Schwenk and Senellart, 2009), and the otherexploited the full potential of existing parallel cor-pus in a mixture-modeling (Foster and Kuhn, 2007;Civera and Juan, 2007; Lv et al, 2007) framework.However, these approaches focused on the studies ofbilingual corpus synthesis and exploitation while ig-noring the monolingual corpora, therefore limitingthe potential of further translation quality improve-ment.In this paper, we propose a novel adaptationmethod to adapt the translation model for domain-specific translation task by utilizing in-domain459monolingual corpora.
Our approach is inspired bythe recent studies (Zhao and Xing, 2006; Zhao andXing, 2007; Tam et al, 2007; Gong and Zhou, 2010;Ruiz and Federico, 2011) which have shown that aparticular translation always appears in some spe-cific topical contexts, and the topical context infor-mation has a great effect on translation selection.For example, ?bank?
often occurs in the sentencesrelated to the economy topic when translated into?y?inha?ng?, and occurs in the sentences related to thegeography topic when translated to ?he?a`n?.
There-fore, the co-occurrence frequency of the phrases insome specific context can be used to constrain thetranslation candidates of phrases.
In a monolingualcorpus, if ?bank?
occurs more often in the sentencesrelated to the economy topic than the ones relatedto the geography topic, it is more likely that ?bank?is translated to ?y?inha?ng?
than to ?he?a`n?.
With theout-of-domain bilingual corpus, we first incorporatethe topic information into translation probability es-timation, aiming to quantify the effect of the topicalcontext information on translation selection.
Then,we rescore all phrase pairs according to the phrase-topic and the word-topic posterior distributions ofthe additional in-domain monolingual corpora.
Ascompared to the previous works, our method takesadvantage of both the in-domain monolingual cor-pora and the out-of-domain bilingual corpus to in-corporate the topic information into our translationmodel, thus breaking down the corpus barrier fortranslation quality improvement.
The experimentalresults on the NIST data set demonstrate the effec-tiveness of our method.The reminder of this paper is organized as fol-lows: Section 2 provides a brief description of trans-lation probability estimation.
Section 3 introducesthe adaptation method which incorporates the top-ic information into the translation model; Section4 describes and discusses the experimental results;Section 5 briefly summarizes the recent related workabout translation model adaptation.
Finally, we endwith a conclusion and the future work in Section 6.2 BackgroundThe statistical translation model, which containsphrase pairs with bi-directional phrase probabilitiesand bi-directional lexical probabilities, has a greateffect on the performance of SMT system.
Phraseprobability measures the co-occurrence frequency ofa phrase pair, and lexical probability is used to vali-date the quality of the phrase pair by checking howwell its words are translated to each other.According to the definition proposed by (Koehnet al, 2003), given a source sentence f = fJ1 =f1, .
.
.
, fj , .
.
.
, fJ , a target sentence e = eI1 =e1, .
.
.
, ei, .
.
.
, eI , and its word alignment a whichis a subset of the Cartesian product of word position-s: a ?
(j, i) : j = 1, .
.
.
, J ; i = 1, .
.
.
, I , the phrasepair (f?
, e?)
is said to be consistent (Och and Ney,2004) with the alignment if and only if: (1) theremust be at least one word inside one phrase alignedto a word inside the other phrase and (2) no wordsinside one phrase can be aligned to a word outsidethe other phrase.
After all consistent phrase pairs areextracted from training corpus, the phrase probabil-ities are estimated as relative frequencies (Och andNey, 2004):?(e?|f?)
=count(f?
, e?)?e??count(f?
, e??
)(1)Here count(f?
, e?)
indicates how often the phrase pair(f?
, e?)
occurs in the training corpus.To obtain the corresponding lexical weight, wefirst estimate a lexical translation probability distri-bution w(e|f) by relative frequency from the train-ing corpus:w(e|f) =count(f, e)?e?count(f, e?
)(2)Retaining the alignment a?
between the phrase pair(f?
, e?
), the corresponding lexical weight is calculatedaspw(e?|f?
, a?)
=|e?|?i=11|{j|(j, i) ?
a?}|??
(j,i)?a?w(ei|fj) (3)However, the above-mentioned method onlycounts the co-occurrence frequency of bilingualphrases, assuming that the translation probability isindependent of the context information.
Thus, thestatistical model estimated from the training data isnot suitable for text translation in different domains,resulting in a significant drop in translation quality.4603 Translation Model Adaptation viaMonolingual Topic InformationIn this section, we first briefly review the principleof Hidden Topic Markov Model(HTMM) which isthe basis of our method, then describe our approachto translation model adaptation in detail.3.1 Hidden Topic Markov ModelDuring the last couple of years, topic models suchas Probabilistic Latent Semantic Analysis (Hof-mann, 1999) and Latent Dirichlet Allocation mod-el (Blei, 2003), have drawn more and more attentionand been applied successfully in NLP community.Based on the ?bag-of-words?
assumption that the or-der of words can be ignored, these methods modelthe text corpus by using a co-occurrence matrix ofwords and documents, and build generative model-s to infer the latent aspects or topics.
Using thesemodels, the words can be clustered into the derivedtopics with a probability distribution, and the corre-lation between words can be automatically capturedvia topics.However, the ?bag-of-words?
assumption is anunrealistic oversimplification because it ignores theorder of words.
To remedy this problem, Gruber etal.
(2007) propose HTMM, which models the topicsof words in the document as a Markov chain.
Basedon the assumption that all words in the same sen-tence have the same topic and the successive sen-tences are more likely to have the same topic, HTM-M incorporates the local dependency between wordsby Hidden Markov Model for better topic estima-tion.HTMM can also be viewed as a soft clusteringtool for words in training corpus.
That is, HT-MM can estimate the probability distribution of atopic over words, i.e.
the topic-word distributionP (word|topic) during training.
Besides, HTMMderives inherent topics in sentences rather than indocuments, so we can easily obtain the sentence-topic distribution P (topic|sentence) in trainingcorpus.
Adopting maximum likelihood estima-tion(MLE), this posterior distribution makes it pos-sible to effectively calculate the word-topic distri-bution P (topic|word) and the phrase-topic distribu-tion P (topic|phrase) both of which are very impor-tant in our method.3.2 Adapted Phrase Probability EstimationWe utilize the additional in-domain monolingualcorpora to adapt the out-of-domain translation mod-el for domain-specific translation task.
In detail, webuild an adapted translation model in the followingsteps:?
Build a topic-specific translation model toquantify the effect of the topic information onthe translation probability estimation.?
Estimate the topic posterior distributions ofphrases in the in-domain monolingual corpora.?
Score the phrase pairs according to the prede-fined topic-specific translation model and thetopic posterior distribution of phrases.Formally, we incorporate monolingual topic in-formation into translation probability estimation,and decompose the phrase probability ?(e?|f?
)1 asfollows:?(e?|f?)
=?tf?
(e?, tf |f?)=?tf?(e?|f?
, tf ) ?
P (tf |f?)
(4)where ?(e?|f?
, tf ) indicates the probability of trans-lating f?
into e?
given the source-side topic tf ,P (tf |f?)
denotes the phrase-topic distribution of f?
.To compute ?(e?|f?
), we first apply HTMM to re-spectively train two monolingual topic models withthe following corpora: one is the source part ofthe out-of-domain bilingual corpus Cf out, the oth-er is the in-domain monolingual corpus Cf in in thesource language.
Then, we respectively estimate?(e?|f?
, tf ) and P (tf |f?)
from these two corpora.
Toavoid confusion, we further refine ?(e?|f?
, tf ) andP (tf |f?)
with ?(e?|f?
, tf out) and P (tf in|f?
), respec-tively.
Here, tf out is the topic clustered from thecorpus Cf out, and tf in represents the topic derivedfrom the corpus Cf in.However, the two above-mentioned probabilitiescan not be directly multiplied in formula (4) be-cause they are related to different topic spaces from1Due to the limit of space, we omit the description of the cal-culation method of the phrase probability ?(f?
|e?
), which can beadjusted in a similar way to ?(e?|f?)
with the help of in-domainmonolingual corpus in the target language.461different corpora.
Besides, their topic dimension-s are not assured to be the same.
To solve thisproblem, we introduce the topic mapping probabili-ty P (tf out|tf in) to map the in-domain phrase-topicdistribution into the one in the out-domain topic s-pace.
To be specific, we obtain the out-of-domainphrase-topic distribution P (tf out|f?)
as follows:P (tf out|f?)
=?tf inP (tf out|tf in) ?
P (tf in|f?)
(5)Thus formula (4) can be further refined as the fol-lowing formula:?(e?|f?)
=?tf out?tf in?(e?|f?
, tf out)?P (tf out|tf in) ?
P (tf in|f?)
(6)Next we will give detailed descriptions of the cal-culation methods for the three probability distribu-tions mentioned in formula (6).3.2.1 Topic-Specific Phrase TranslationProbability ?(e?|f?
, tf out)We follow the common practice (Koehn et al,2003) to calculate the topic-specific phrase trans-lation probability, and the only difference is thatour method takes the topical context information in-to account when collecting the fractional counts ofphrase pairs.
With the sentence-topic distributionP (tf out|f) from the relevant topic model of Cf out,the conditional probability ?(e?|f?
, tf out) can be eas-ily obtained by MLE method:?(e?|f?
, tf out)=?
?f ,e?
?Coutcount?f ,e?(f?
, e?)
?
P (tf out|f)?e???
?f ,e?
?Coutcount?f ,e?(f?
, e??)
?
P (tf out|f)(7)where Cout is the out-of-domain bilingual trainingcorpus, and count?f ,e?(f?
, e?)
denotes the number ofthe phrase pair (f?
, e?)
in sentence pair ?f , e?.3.2.2 Topic Mapping Probability P (tf out|tf in)Based on the two monolingual topic models re-spectively trained from Cf in and Cf out, we com-pute the topic mapping probability by using sourceword f as the pivot variable.
Noticing that thereare some words occurring in one corpus only, weuse the words belonging to both corpora during themapping procedure.
Specifically, we decomposeP (tf out|tf in) as follows:P (tf out|tf in)=?f?Cf out?Cf inP (tf out|f) ?
P (f |tf in) (8)Here we first get P (f |tf in) directly from the top-ic model related to Cf in.
Then, considering thesentence-topic distribution P (tf out|f) from the rel-evant topic model of Cf out, we define the word-topic distribution P (tf out|f) as:P (tf out|f)=?f?Cf outcountf (f) ?
P (tf out|f)?tf out?f?Cf outcountf (f) ?
P (tf out|f)(9)where countf (f) denotes the number of the word fin sentence f .3.2.3 Phrase-Topic Distribution P (tf in|f?
)A simple way to compute the phrase-topic distri-bution is to take the fractional counts from Cf inand then adopt MLE to obtain relative probability.However, it is infeasible in our model because somephrases occur in Cf out while being absent in Cf in.To solve this problem, we further compute this pos-terior distribution by the interpolation of two model-s:P (tf in|f?)
= ?
?
Pmle(tf in|f?)
+(1?
?)
?
Pword(tf in|f?)
(10)where Pmle(tf in|f?)
indicates the phrase-topic dis-tribution by MLE, Pword(tf in|f?)
denotes thephrase-topic distribution which is decomposed intothe topic posterior distribution at the word level, and?
is the interpolation weight that can be optimizedover the development data.Given the number of the phrase f?
in sentence fdenoted as countf (f?
), we compute the in-domainphrase-topic distribution in the following way:Pmle(tf in|f?
)=?f?Cf incountf (f?)
?
P (tf in|f)?tf in?f?Cf incountf (f?)
?
P (tf in|f)(11)462Under the assumption that the topics of all word-s in the same phrase are independent, we consid-er two methods to calculate Pword(tf in|f?).
One isa ?Noisy-OR?
combination method (Zens and Ney,2004) which has shown good performance in calcu-lating similarities between bags-of-words in differ-ent languages.
Using this method, Pword(tf in|f?)
isdefined as:Pword(tf in|f?
)= 1?
Pword(t?f in|f?)?
1?
?fj?f?P (t?f in|fj)= 1??fj?f?(1?
P (tf in|fj)) (12)where Pword(t?f in|f?)
represents the probability thattf in is not the topic of the phrase f?
.
Similarly,P (t?f in|fj) indicates the probability that tf in is notthe topic of the word fj .The other method is an ?Averaging?
combinationone.
With the assumption that tf in is the topic of f?if at least one of the words in f?
belongs to this topic,we derive Pword(tf in|f?)
as follows:Pword(tf in|f?)
?
?fj?f?P (tf in|fj)/|f?
| (13)where |f?
| denotes the number of words in phrase f?
.3.3 Adapted Lexical Probability EstimationNow we briefly describe how to estimate the adaptedlexical weight for phrase pairs, which can be adjust-ed in a similar way to the phrase probability.Specifically, adopting our method, each word isconsidered as one phrase consisting of only oneword, sow(e|f) =?tf out?tf inw(e|f, tf out)?P (tf out|tf in) ?
P (tf in|f) (14)Here we obtain w(e|f, tf out) with a simi-lar approach to ?(e?|f?
, tf out), and calculateP (tf out|tf in) and P (tf in|f) by resorting toformulas (8) and (9).With the adjusted lexical translation probability,we resort to formula (4) to update the lexical weightfor the phrase pair (f?
, e?
).4 ExperimentWe evaluate our method on the Chinese-to-Englishtranslation task for the weblog text.
After a brief de-scription of the experimental setup, we investigatethe effects of various factors on the translation sys-tem performance.4.1 Experimental setupIn our experiments, the out-of-domain training cor-pus comes from the FBIS corpus and the Hansard-s part of LDC2004T07 corpus (54.6K documentswith 1M parallel sentences, 25.2M Chinese wordsand 29M English words).
We use the Chinese Sohuweblog in 20091 and the English Blog Authorshipcorpus2 (Schler et al, 2006) as the in-domain mono-lingual corpora in the source language and targetlanguage, respectively.
To obtain more accurate top-ic information by HTMM, we firstly filter the noisyblog documents and the ones consisting of short sen-tences.
After filtering, there are totally 85K Chineseblog documents with 2.1M sentences and 277K En-glish blog documents with 4.3M sentences used inour experiments.
Then, we sample equal numbers ofdocuments from the in-domain monolingual corpo-ra in the source language and the target language torespectively train two in-domain topic models.
Theweb part of the 2006 NIST MT evaluation test da-ta, consisting of 27 documents with 1048 sentences,is used as the development set, and the weblog partof the 2008 NIST MT test data, including 33 docu-ments with 666 sentences, is our test set.To obtain various topic distributions for the out-of-domain training corpus and the in-domain mono-lingual corpora in the source language and the tar-get language respectively, we use HTMM tool devel-oped by Gruber et al(2007) to conduct topic modeltraining.
During this process, we empirically set thesame parameter values for the HTMM training of d-ifferent corpora: topics = 50, ?
= 1.5, ?
= 1.01,iters = 100.
See (Gruber et al, 2007) for themeanings of these parameters.
Besides, we set theinterpolation weight ?
in formula (10) to 0.5 by ob-serving the results on development set in the addi-tional experiments.We choose MOSES, a famous open-source1http://blog.sohu.com/2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html463phrase-based machine translation system (Koehnet al, 2007), as the experimental decoder.GIZA++ (Och and Ney, 2003) and the heuristics?grow-diag-final-and?
are used to generate a word-aligned corpus, from which we extract bilingualphrases with maximum length 7.
We use SRILMToolkits (Stolcke, 2002) to train two 4-gram lan-guage models on the filtered English Blog Author-ship corpus and the Xinhua portion of Gigawordcorpus, respectively.
During decoding, we set thettable-limit as 20, the stack-size as 100, and per-form minimum-error-rate training (Och and Ney,2003) to tune the feature weights for the log-linearmodel.
The translation quality is evaluated bycase-insensitive BLEU-4 metric (Papineni et al,2002).
Finally, we conduct paired bootstrap sam-pling (Koehn, 2004) to test the significance in BLEUscore differences.4.2 Result and Analysis4.2.1 Effect of Different Smoothing MethodsOur first experiments investigate the effect of dif-ferent smoothing methods for the in-domain phrase-topic distribution: ?Noisy-OR?
and ?Averaging?.We build adapted phrase tables with these two meth-ods, and then respectively use them in place of theout-of-domain phrase table to test the system perfor-mance.
For the purpose of studying the generality ofour approach, we carry out comparative experimentson two sizes of in-domain monolingual corpora: 5Kand 40K.AdaptationMethod(Dev) MT06Web(Tst) MT08WeblogBaseline 30.98 20.22Noisy-OR (5K) 31.16 20.45Averaging (5K) 31.51 20.54Noisy-OR (40K) 31.87 20.76Averaging (40K) 31.89 21.11Table 1: Experimental results using different smoothingmethods.Table 1 reports the BLEU scores of the translationsystem under various conditions.
Using the out-of-domain phrase table, the baseline system achievesa BLEU score of 20.22.
In the experiments withthe small-scale in-domain monolingual corpora, theBLEU scores acquired by two methods are 20.45and 20.54, achieving absolute improvements of 0.23and 0.32 on the test set, respectively.
In the exper-iments with the large-scale monolingual in-domaincorpora, similar results are obtained, with absoluteimprovements of 0.54 and 0.89 over the baselinesystem.From the above experimental results, we knowthat both ?Noisy-OR?
and ?Averaging?
combinationmethods improve the performance over the base-line, and ?Averaging?
method seems to be slight-ly better.
This finding fails to echo the promis-ing results in the previous study (Zens and Ney,2004).
This is because the ?Noisy-OR?
method in-volves the multiplication of the word-topic distribu-tion (shown in formula (12)), which leads to muchsharper phrase-topic distribution than ?Averaging?method, and is more likely to introduce bias to thetranslation probability estimation.
Due to this rea-son, all the following experiments only consider the?Averaging?method.4.2.2 Effect of Combining Two Phrase TablesIn the above experiments, we replace the out-of-domain phrase table with the adapted phrase table.Here we combine these two phrase tables in a log-linear framework to see if we could obtain furtherimprovement.
To offer a clear description, we repre-sent the out-of-domain phrase table and the adaptedphrase table with ?OutBP?
and ?AdapBP?, respec-tively.Used PhraseTable(Dev) MT06Web(Tst) MT08WeblogBaseline 30.98 20.22AdapBp (5K) 31.51 20.54+ OutBp 31.84 20.70AdapBp (40K) 31.89 21.11+ OutBp 32.05 21.20Table 2: Experimental results using different phrase ta-bles.
OutBp: the out-of-domain phrase table.
AdapBp:the adapted phrase table.Table 2 shows the results of experiments using d-ifferent phrase tables.
Applying our adaptation ap-proach, both ?AdapBP?
and ?OutBP + AdapBP?consistently outperform the baseline, and the lat-464Figure 1: Effect of in-domain monolingual corpus size ontranslation quality.ter produces further improvements over the former.Specifically, the BLEU scores of the ?OutBP +AdapBP?
method are 20.70 and 21.20, which ob-tain 0.48 and 0.98 points higher than the baselinemethod, and 0.16 and 0.09 points higher than the?AdapBP?
method.
The underlying reason is that theprobability distribution of each in-domain sentenceoften converges on some topics in the ?AdapBP?method and some translation probabilities are over-estimated, which leads to negative effects on thetranslation quality.
By using two tables together, ourapproach reduces the bias introduced by ?AdapBP?,therefore further improving the translation quality.4.2.3 Effect of In-domain Monolingual CorpusSizeFinally, we investigate the effect of in-domainmonolingual corpus size on translation quality.
Inthe experiment, we try different sizes of in-domaindocuments to train different monolingual topic mod-els: from 5K to 80K with an increment of 5K eachtime.
Note that here we only focus on the exper-iments using the ?OutBP + AdapBP?
method, be-cause this method performs better in the previousexperiments.Figure 1 shows the BLEU scores of the transla-tion system on the test set.
It can be seen that themore data, the better translation quality when thecorpus size is less than 30K.
The overall BLEUscores corresponding to the range of great N val-ues are generally higher than the ones correspond-ing to the range of small N values.
For example, theBLEU scores under the condition within the range[25K, 80K] are all higher than the ones within therange [5K, 20K].
When N is set to 55K, the BLEUscore of our system is 21.40, with 1.18 gains on thebaseline system.
This difference is statistically sig-nificant at P < 0.01 using the significance test tooldeveloped by Zhang et al(2004).
For this experi-mental result, we speculate that with the incrementof in-domain monolingual data, the correspondingtopic models provide more accurate topic informa-tion to improve the translation system.
However,this effect weakens when the monolingual corporacontinue to increase.5 Related workMost previous researches about translation modeladaptation focused on parallel data collection.
Forexample, Hildebrand et al(2005) employed infor-mation retrieval technology to gather the bilingualsentences, which are similar to the test set, fromavailable in-domain and out-of-domain training da-ta to build an adaptive translation model.
Withthe same motivation, Munteanu and Marcu (2005)extracted in-domain bilingual sentence pairs fromcomparable corpora.
Since large-scale monolin-gual corpus is easier to obtain than parallel corpus,there have been some studies on how to generateparallel sentences with monolingual sentences.
Inthis respect, Ueffing et al (2008) explored semi-supervised learning to obtain synthetic parallel sen-tences, and Wu et al (2008) used an in-domaintranslation dictionary and monolingual corpora toadapt an out-of-domain translation model for the in-domain text.Differing from the above-mentioned works onthe acquirement of bilingual resource, several stud-ies (Foster and Kuhn, 2007; Civera and Juan, 2007;Lv et al, 2007) adopted mixture modeling frame-work to exploit the full potential of the existing par-allel corpus.
Under this framework, the training cor-pus is first divided into different parts, each of whichis used to train a sub translation model, then thesesub models are used together with different weightsduring decoding.
In addition, discriminative weight-ing methods were proposed to assign appropriateweights to the sentences from training corpus (Mat-soukas et al, 2009) or the phrase pairs of phrase ta-ble (Foster et al, 2010).
Final experimental result-s show that without using any additional resources,these approaches all improve SMT performance sig-465nificantly.Our method deals with translation model adap-tation by making use of the topical context, so letus take a look at the recent research developmen-t on the application of topic models in SMT.
As-suming each bilingual sentence constitutes a mix-ture of hidden topics and each word pair follows atopic-specific bilingual translation model, Zhao andXing (2006,2007) presented a bilingual topical ad-mixture formalism to improve word alignment bycapturing topic sharing at different levels of linguis-tic granularity.
Tam et al(2007) proposed a bilin-gual LSA, which enforces one-to-one topic corre-spondence and enables latent topic distributions tobe efficiently transferred across languages, to cross-lingual language modeling and translation lexiconadaptation.
Recently, Gong and Zhou (2010) alsoapplied topic modeling into domain adaptation inSMT.
Their method employed one additional featurefunction to capture the topic inherent in the sourcephrase and help the decoder dynamically choose re-lated target phrases according to the specific topic ofthe source phrase.Besides, our approach is also related to context-dependent translation.
Recent studies have shownthat SMT systems can benefit from the utiliza-tion of context information.
For example, trigger-based lexicon model (Hasan et al, 2008; Mauser etal., 2009) and context-dependent translation selec-tion (Chan et al, 2007; Carpuat and Wu, 2007; Heet al, 2008; Liu et al, 2008).
The former gener-ated triplets to capture long-distance dependenciesthat go beyond the local context of phrases, and thelatter built the classifiers which combine rich con-text information to better select translation duringdecoding.
With the consideration of various localcontext features, these approaches all yielded stableimprovements on different translation tasks.As compared to the above-mentioned works, ourwork has the following differences.?
We focus on how to adapt a translation mod-el for domain-specific translation task with thehelp of additional in-domain monolingual cor-pora, which are far from full exploitation in theparallel data collection and mixture modelingframework.?
In addition to the utilization of in-domainmonolingual corpora, our method is differen-t from the previous works (Zhao and Xing,2006; Zhao and Xing, 2007; Tam et al, 2007;Gong and Zhou, 2010) in the following aspect-s: (1) we use a different topic model ?
HTMMwhich has different assumption from PLSA andLDA; (2) rather than modeling topic-dependenttranslation lexicons in the training process, weestimate topic-specific lexical probability bytaking account of topical context when extract-ing word pairs, so our method can also be di-rectly applied to topic-dependent phrase proba-bility modeling.
(3) Instead of rescoring phrasepairs online, our approach calculate the transla-tion probabilities offline, which brings no addi-tional burden to translation systems and is suit-able to translate the texts without the topic dis-tribution information.?
Different from trigger-based lexicon model andcontext-dependent translation selection both ofwhich put emphasis on solving the translationambiguity by the exploitation of the context in-formation at the sentence level, we adopt thetopical context information in our method forthe following reasons: (1) the topic informa-tion captures the context information beyondthe scope of sentence; (2) the topical context in-formation is integrated into the posterior prob-ability distribution, avoiding the sparseness ofword or POS features; (3) the topical contextinformation allows for more fine-grained dis-tinction of different translations than the genreinformation of corpus.6 Conclusion and future workThis paper presents a novel method for SMT sys-tem adaptation by making use of the monolingualcorpora in new domains.
Our approach first esti-mates the translation probabilities from the out-of-domain bilingual corpus given the topic information,and then rescores the phrase pairs via topic mappingand phrase-topic distribution probability estimationfrom in-domain monolingual corpora.
Experimentalresults show that our method achieves better perfor-mance than the baseline system, without increasingthe burden of the translation system.In the future, we will verify our method on oth-466er language pairs, for example, Chinese to Japanese.Furthermore, since the in-domain phrase-topic dis-tribution is currently estimated with simple smooth-ing interpolations, we expect that the translation sys-tem could benefit from other sophisticated smooth-ing methods.
Finally, the reasonable estimation oftopic number for better translation model adaptationwill also become our study emphasis.AcknowledgementThe authors were supported by 863 State KeyProject (Grant No.
2011AA01A207), NationalNatural Science Foundation of China (Grant Nos.61005052 and 61103101), Key Technologies R&DProgram of China (Grant No.
2012BAH14F03).
Wethank the anonymous reviewers for their insightfulcomments.
We are also grateful to Ruiyu Fang andJinming Hu for their kind help in data processing.ReferencesMichiel Bacchiani and Brian Roark.
2003.
Unsuper-vised Language Model Adaptation.
In Proc.
of ICAS-SP 2003, pages 224-227.Michiel Bacchiani and Brian Roark.
2005.
ImprovingMachine Translation Performance by Exploiting Non-Parallel Corpora.
Computational Linguistics, pages477-504.Nicola Bertoldi and Marcello Federico.
2009.
DomainAdaptation for Statistical Machine Translation withMonolingual Resources.
In Proc.
of ACL Workshop2009, pages 182-189.David M. Blei.
2003.
Latent Dirichlet Allocation.
Jour-nal of Machine Learning, pages 993-1022.Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, LongNguyen and John Makhoul.
2007.
Language ModelAdaptation in Machine Translation from Speech.
InProc.
of ICASSP 2007, pages 117-120.Marine Carpuat and Dekai Wu.
2007.
Improving Statis-tical Machine Translation Using Word Sense Disam-biguation.
In Proc.
of EMNLP 2007, pages 61-72.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2006.Word sense disambiguation improves statistical ma-chine translation.
In Proc.
of ACL 2007, pages 33-40.Boxing Chen, George Foster and Roland Kuhn.
2010.Bilingual Sense Similarity for Statistical MachineTranslation.
In Proc.
of ACL 2010, pages 834-843.David Chiang.
2007.
Hierarchical Phrase-Based Trans-lation.
Computational Linguistics, pages 201-228.David Chiang.
2010.
Learning to Translate with Sourceand Target Syntax.
In Proc.
of ACL 2010, pages 1443-1452.Jorge Civera and Alfons Juan.
2007.
Domain Adaptationin Statistical Machine Translation with Mixture Mod-elling.
In Proc.
of the Second Workshop on StatisticalMachine Translation, pages 177-180.Matthias Eck, Stephan Vogel and Alex Waibel.
2004.Language Model Adaptation for Statistical MachineTranslation Based on Information Retrieval.
In Proc.of Fourth International Conference on Language Re-sources and Evaluation, pages 327-330.Matthias Eck, Stephan Vogel and Alex Waibel.
2005.Low Cost Portability for Statistical Machine Transla-tion Based on N-gram Coverage.
In Proc.
of MT Sum-mit 2005, pages 227-234.George Foster and Roland Kuhn.
2007.
Mixture ModelAdaptation for SMT.
In Proc.
of the Second Workshopon Statistical Machine Translation, pages 128-135.George Foster, Cyril Goutte and Roland Kuhn.
2010.Discriminative Instance Weighting for Domain Adap-tation in Statistical Machine Translation.
In Proc.
ofEMNLP 2010, pages 451-459.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang and Ignacio Thay-er.
2006.
Scalable Inference and Training of Context-Rich Syntactic Translation Models.
In Proc.
of ACL2006, pages 961-968.Zhengxian Gong and Guodong Zhou.
2010.
ImproveSMT with Source-side Topic-Document Distributions.In Proc.
of MT SUMMIT 2010, pages 24-28.Amit Gruber, Michal Rosen-Zvi and Yair Weiss.
2007.Hidden Topic Markov Models.
In Journal of MachineLearning Research, pages 163-170.Sas?a Hasan, Juri Ganitkevitch, Hermann Ney and Jesu?sAndre?s-Ferrer 2008.
Triplet Lexicon Models for S-tatistical Machine Translation.
In Proc.
of EMNLP2008, pages 372-381.Zhongjun He, Qun Liu and Shouxun Lin.
2008.
Improv-ing Statistical Machine Translation using LexicalizedRule Selection.
In Proc.
of COLING 2008, pages 321-328.Almut Silja Hildebrand.
2005.
Adaptation of the Trans-lation Model for Statistical Machine Translation basedon Information Retrieval.
In Proc.
of EAMT 2005,pages 133-142.Thomas Hofmann.
1999.
Probabilistic Latent SemanticIndexing.
In Proc.
of SIGIR 1999, pages 50-57.Franz Joseph Och and Hermann Ney.
2003.
A Systemat-ic Comparison of Various Statistical Alignment Mod-els.
Computational Linguistics, pages 19-51.Franz Joseph Och and Hermann Ney.
2004.
The Align-ment Template Approach to Statistical Machine Trans-lation.
Computational Linguistics, pages 417-449.467Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
of HLT-NAACL 2003, pages 127-133.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proc.
of EMNLP2004, pages 388-395.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.
ofACL 2007, Demonstration Session, pages 177-180.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proc.
of ACL 2006, pages 609-616.Yajuan Lv, Jin Huang and Qun Liu.
2007.
Improv-ing Statistical Machine Translation Performance byTraining Data Selection and Optimization.
In Proc.of EMNLP 2007, pages 343-350.Arne Mauser, Richard Zens and Evgeny Matusov, Sas?aHasan and Hermann Ney.
2006.
The RWTH Statisti-cal Machine Translation System for the IWSLT 2006Evaluation.
In Proc.
of International Workshop onSpoken Language Translation, pages 103-110.Arne Mauser, Sas?a Hasan and Hermann Ney 2009.
Ex-tending Statistical Machine Translation with Discrimi-native and Trigger-Based Lexicon Models.
In Proc.
ofACL 2009, pages 210-218.Spyros Matsoukas, Antti-Veikko I. Rosti and Bing Zhang2009.
Discriminative Corpus Weight Estimation forMachine Translation.
In Proc.
of EMNLP 2009, pages708-717.Nick Ruiz and Marcello Federico.
2011.
Topic Adapta-tion for Lecture Translation through Bilingual LatentSemantic Models.
In Proc.
of ACL Workshop 2011,pages 294-302.Kishore Papineni, Salim Roukos, Todd Ward and WeiJingZhu.
2002.
BLEU: A Method for Automatic Evalu-ation of Machine Translation.
In Proc.
of ACL 2002,pages 311-318.Jonathan Schler, Moshe Koppel, Shlomo Argamon andJames Pennebaker.
2006.
Effects of Age and Genderon Blogging.
In Proc.
of 2006 AAAI Spring Sympo-sium on Computational Approaches for Analyzing We-blogs.Holger Schwenk and Jean Senellart.
2009.
TranslationModel Adaptation for an Arabic/french News Transla-tion System by Lightly-supervised Training.
In Proc.of MT Summit XII.Andreas Stolcke.
2002.
Srilm - An Extensible LanguageModeling Toolkit.
In Proc.
of ICSLP 2002, pages 901-904.Yik-Cheung Tam, Ian R. Lane and Tanja Schultz.
2007.Bilingual LSA-based adaptation for statistical machinetranslation.
Machine Translation, pages 187-207.Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar.2008.
Semi-supervised Model Adaptation for Statisti-cal Machine Translation.
Machine Translation, pages77-94.Hua Wu, Haifeng Wang and Chengqing Zong.
2008.
Do-main Adaptation for Statistical Machine Translationwith Domain Dictionary and Monolingual Corpora.
InProc.
of COLING 2008, pages 993-1000.Richard Zens and Hermann Ney.
2004.
Improvments inphrase-based statistical machine translation.
In Proc.of NAACL 2004, pages 257-264.Ying Zhang, Almut Silja Hildebrand and Stephan Vogel.2006.
Distributed Language Modeling for N-best ListRe-ranking.
In Proc.
of EMNLP 2006, pages 216-223.Bing Zhao, Matthias Eck and Stephan Vogel.
2004.Language Model Adaptation for Statistical MachineTranslation with Structured Query Models.
In Proc.of COLING 2004, pages 411-417.Bing Zhao and Eric P. Xing.
2006.
BiTAM: BilingualTopic AdMixture Models for Word Alignment.
InProc.
of ACL/COLING 2006, pages 969-976.Bing Zhao and Eric P. Xing.
2007.
HM-BiTAM: Bilin-gual Topic Exploration, Word Alignment, and Trans-lation.
In Proc.
of NIPS 2007, pages 1-8.Qun Liu, Zhongjun He, Yang Liu and Shouxun Lin.2008.
Maximum Entropy based Rule Selection Modelfor Syntax-based Statistical Machine Translation.
InProc.
of EMNLP 2008, pages 89-97.468
