Proceedings of the Second Workshop on Statistical Machine Translation, pages 96?103,Prague, June 2007. c?2007 Association for Computational LinguisticsHuman Evaluation of Machine Translation Through Binary SystemComparisonsDavid Vilar, Gregor Leuschand Hermann NeyLehrstuhl fu?r Informatik 6RWTH Aachen UniversityD-52056 Aachen, Germany{vilar,leusch,ney}@cs.rwth-aachen.deRafael E. BanchsD.
of Signal Theory and CommunicationsUniversitat Polite`cnica de Catalunya08034 Barcelona, Spainrbanchs@gps.tsc.upc.eduAbstractWe introduce a novel evaluation scheme forthe human evaluation of different machinetranslation systems.
Our method is basedon direct comparison of two sentences at atime by human judges.
These binary judg-ments are then used to decide between allpossible rankings of the systems.
The ad-vantages of this new method are the lowerdependency on extensive evaluation guide-lines, and a tighter focus on a typical eval-uation task, namely the ranking of systems.Furthermore we argue that machine transla-tion evaluations should be regarded as sta-tistical processes, both for human and au-tomatic evaluation.
We show how confi-dence ranges for state-of-the-art evaluationmeasures such as WER and TER can becomputed accurately and efficiently withouthaving to resort to Monte Carlo estimates.We give an example of our new evaluationscheme, as well as a comparison with classi-cal automatic and human evaluation on datafrom a recent international evaluation cam-paign.1 IntroductionEvaluation of machine translation (MT) output is adifficult and still open problem.
As in other natu-ral language processing tasks, automatic measureswhich try to asses the quality of the translationcan be computed.
The most widely known are theWord Error Rate (WER), the Position independentword Error Rate (PER), the NIST score (Dodding-ton, 2002) and, especially in recent years, the BLEUscore (Papineni et al, 2002) and the Translation Er-ror Rate (TER) (Snover et al, 2005).
All of the-ses measures compare the system output with oneor more gold standard references and produce a nu-merical value (score or error rate) which measuresthe similarity between the machine translation and ahuman produced one.
Once such reference transla-tions are available, the evaluation can be carried outin a quick, efficient and reproducible manner.However, automatic measures also have big dis-advantages; (Callison-Burch et al, 2006) describessome of them.
A major problem is that a given sen-tence in one language can have several correct trans-lations in another language and thus, the measure ofsimilarity with one or even a small amount of ref-erence translations will never be flexible enough totruly reflect the wide range of correct possibilities ofa translation.
1 This holds in particular for long sen-tences and wide- or open-domain tasks like the onesdealt with in current MT projects and evaluations.If the actual quality of a translation in terms ofusefulness for human users is to be evaluated, humanevaluation needs to be carried out.
This is howevera costly and very time-consuming process.
In thiswork we present a novel approach to human evalu-ation that simplifies the task for human judges.
In-stead of having to assign numerical scores to eachsentence to be evaluated, as is done in current evalu-ation procedures, human judges choose the best oneout of two candidate translations.
We show how thismethod can be used to rank an arbitrary number ofsystems and present a detailed analysis of the statis-tical significance of the method.1Compare this with speech recognition, where apart fromorthographic variance there is only one correct reference.962 State-of-the-artThe standard procedure for carrying out a humanevaluation of machine translation output is based onthe manual scoring of each sentence with two nu-merical values between 1 and 5.
The first one mea-sures the fluency of the sentence, that is its readabil-ity and understandability.
This is a monolingual fea-ture which does not take the source sentence intoaccount.
The second one reflects the adequacy, thatis whether the translated sentence is a correct trans-lation of the original sentence in the sense that themeaning is transferred.
Since humans will be theend users of the generated output,2 it can be ex-pected that these human-produced measures will re-flect the usability and appropriateness of MT outputbetter than any automatic measure.This kind of human evaluation has however addi-tional problems.
It is much more time consumingthan the automatic evaluation, and because it is sub-jective, results are not reproducible, even from thesame group of evaluators.
Furthermore, there canbe biases among the human judges.
Large amountsof sentences must therefore be evaluated and proce-dures like evaluation normalization must be carriedout before significant conclusions from the evalua-tion can be drawn.
Another important drawback,which is also one of the causes of the aforemen-tioned problems, is that it is very difficult to definethe meaning of the numerical scores precisely.
Evenif human judges have explicit evaluation guidelinesat hand, they still find it difficult to assign a numeri-cal value which represents the quality of the transla-tion for many sentences (Koehn and Monz, 2006).In this paper we present an alternative to this eval-uation scheme.
Our method starts from the obser-vation that normally the final objective of a humanevaluation is to find a ?ranking?
of different systems,and the absolute score for each system is not relevant(and it can even not be comparable between differ-ent evaluations).
We focus on a method that aims tosimplify the task of the judges and allows to rank thesystems according to their translation quality.3 Binary System ComparisonsThe main idea of our method relies in the factthat a human evaluator, when presented two differ-ent translations of the same sentence, can normallychoose the best one out of them in a more or less2With the exception of cross-language information retrievaland similar tasks.definite way.
In social sciences, a similar methodhas been proposed by (Thurstone, 1927).3.1 Comparison of Two SystemsFor the comparison of two MT systems, a set oftranslated sentence pairs is selected.
Each of thesepairs consists of the translations of a particularsource sentence from the two systems.
The humanjudge is then asked to select the ?best?
translation ofthese two, or to mark the translations to be equallygood.
We are aware that the definition of ?best?
hereis fuzzy.
In our experiments, we made a point of notgiving the evaluators explicit guidelines on how todecide between both translations.
As a consequence,the judges were not to make a distinction betweenfluency and adequacy of the translation.
This has atwo-fold purpose: on the one hand it simplifies thedecision procedure for the judges, as in most of thecases the decision is quite natural and they do notneed to think explicitly in terms of fluency and ade-quacy.
On the other hand, one should keep in mindthat the final goal of an MT system is its usefulnessfor a human user, which is why we do not want toimpose artificial constraints on the evaluation proce-dure.
If only certain quality aspects of the systemsare relevant for the ranking, for example if we wantto focus on the fluency of the translations, explicitguidelines can be given to the judges.
If the evalua-tors are bilingual they can use the original sentencesto judge whether the information was preserved inthe translation.After our experiment, the human judges providedfeedback on the evaluation process.
We learnedthat the evaluators normally selected the translationwhich preserved most of the information from theoriginal sentence.
Thus, we expect to have a slightpreference for adequacy over fluency in this evalu-ation process.
Note however that adequacy and flu-ency have shown a high correlation3 in previous ex-periments.
This can be explained by noting that alow fluency renders the text incomprehensible andthus the adequacy score will also be low.The difference in the amount of selected sen-tences of each system is an indicator of the differ-ence in quality between the systems.
Statistics canbe carried out in order to decide whether this differ-ence is statistically significant; we will describe thisin more detail in Section 3.4.3At least for ?sensible?
translation systems.
Academiccounter-examples could easily be constructed.973.2 Evaluation of Multiple SystemsWe can generalize our method to find a ranking ofseveral systems as follows: In this setting, we havea set of n systems.
Furthermore, we have defined anorder relationship ?is better than?
between pairs ofthese systems.
Our goal now is to find an orderingof the systems, such that each system is better thanits predecessor.
In other words, this is just a sortingproblem ?
as widely known in computer science.Several efficient sorting algorithms can be foundin the literature.
Generally, the efficiency of sort-ing algorithms is measured in terms of the numberof comparisons carried out.
State-of-the-art sort-ing algorithms have a worst-case running time ofO(n log n), where n is the number of elements tosort.
In our case, because such binary comparisonsare very time consuming, we want to minimize theabsolute number of comparisons needed.
This mini-mization should be carried out in the strict sense, notjust in an asymptotic manner.
(Knuth, 1973) discusses this issue in detail.
It isrelatively straightforward to show that, in the worstcase, the minimum number of comparisons to becarried out to sort n elements is at least dlog n!e(for which n log n is an approximation).
It is notalways possible to reach this minimum, however, aswas proven e.g.
for the case n = 12 in (Wells, 1971)and for n = 13 in (Peczarski, 2002).
(Ford Jr andJohnson, 1959) propose an algorithm called mergeinsertion which comes very close to the theoreticallimit.
This algorithm is sketched in Figure 1.
Thereare also algorithms with a better asymptotic runtime(Bui and Thanh, 1985), but they only take effect forvalues of n too large for our purposes (e.g., morethan 100).
Thus, using the algorithm from Figure 1we can obtain the ordering of the systems with a(nearly) optimal number of comparisons.3.3 Further ConsiderationsIn Section 3.1 we described how to carry out thecomparison between two systems when there is onlyone human judge carrying out this comparison.
Thecomparison of systems is a very time consumingtask.
Therefore it is hardly possible for one judgeto carry out the evaluation on a whole test corpus.Usually, subsets of these test corpora are selectedfor human evaluations instead.
In order to obtaina better coverage of the test corpus, but also to tryto alleviate the possible bias of a single evaluator, itis advantageous to have several evaluators carryingout the comparison between two systems.
However,there are two points that must be considered.The first one is the selection of sentences each hu-man judge should evaluate.
Assume that we have al-ready decided the amount of sentences m each eval-uator has to work with (in our case m = 100).
Onepossibility is that all human judges evaluate the sameset of sentences, which presumably will cancel pos-sible biases of the evaluators.
A second possibility isto give each judge a disjunct set of sentences.
In thisway we benefit from a higher coverage of the corpus,but do not have an explicit bias compensation.In our experiments, we decided for a middlecourse: Each evaluator receives a randomly selectedset of sentences.
There are no restrictions on the se-lection process.
This implicitly produces some over-lap while at the same time allowing for a larger setof sentences to be evaluated.
To maintain the sameconditions for each comparison, we also decidedthat each human judge should evaluate the same setof sentences for each system pair.The other point to consider is how the evaluationresults of each of the human judges should be com-bined into a decision for the whole system.
Onepossibility would be to take only a ?majority vote?among the evaluators to decide which system is thebest.
By doing this, however, possible quantitativeinformation on the quality difference of the systemsis not taken into account.
Consequently, the output isstrongly influenced by statistical fluctuations of thedata and/or of the selected set of sentences to eval-uate.
Thus, in order to combine the evaluations wejust summed over all decisions to get a total count ofsentences for each system.3.4 Statistical SignificanceThe evaluation of MT systems by evaluating trans-lations of test sentences ?
be it automatic evaluationor human evaluation ?
must always be regarded asa statistical process: Whereas the outcome, or scoreR, of an evaluation is considered to hold for ?all?possible sentences from a given domain, a test cor-pus naturally consists of only a sample from all thesesentences.
Consequently, R depends on that sam-ple of test sentences.
Furthermore, both a humanevaluation score and an automatic evaluation scorefor a hypothesis sentence are by itself noisy: Hu-man evaluation is subjective, and as such is subjectto ?human noise?, as described in Section 2.
Eachautomatic score, on the other hand, depends heavilyon the ambiguous selection of reference translations.Accordingly, evaluation scores underly a probability981.
Make pairwise comparisons of bn/2c disjoint pairs of elements.
(If n is odd, leave one element out).2.
Sort the bn/2c larger elements found in step 1, recursively by merge insertion.3.
Name the bn/2c elements found in step 2 a1, a2, .
.
.
, abn/2c and the rest b1, b2, .
.
.
, bdn/2e, such thata1 ?
a2 ?
?
?
?
?
abn/2c and bi ?
ai for 1 ?
i ?
bn/2c.
Call b1 and the a?s the ?main chain?.4.
Insert the remaining b?s into the main chain, using binary insertion, in the following order (ignore thebj such that j > dn/2e): b3, b2; b5, b4; b11, .
.
.
, b6; .
.
.
; btk , .
.
.
, btk?1+1; .
.
.
with tk =2k+1+(?1)k3 .Figure 1: The merge insertion algorithm as presented in (Knuth, 1973).distribution, and each evaluation result we achievemust be considered as a sample from that distribu-tion.
Consequently, both human and automatic eval-uation results must undergo statistical analysis be-fore conclusions can be drawn from them.A typical application of MT evaluation ?
for ex-ample in the method described in this paper ?
is todecide whether a given MT system X , representedby a set of translated sentences, is significantly betterthan another system Y with respect to a given eval-uation measure.
This outcome is traditionally calledthe alternative hypothesis.
The opposite outcome,namely that the two systems are equal, is knownas the null hypothesis.
We say that certain valuesof RX , RY confirm the alternative hypothesis if thenull hypothesis can be rejected with a given levelof certainty, e.g.
95%.
In the case of comparingtwo MT systems, the null hypothesis would be ?bothsystems are equal with regard to the evaluation mea-sure; that is, both evaluation scoresR, R?
come fromthe same distribution R0?.As R is randomly distributed, it has an expecta-tion E[R] and a standard error se[R].
Assuming anormal distribution for R, we can reject the null hy-pothesis with a confidence of 95% if the sampledscore R is more than 1.96 times the standard erroraway from the null hypothesis expectation:R significant ?
|E[R0] ?
R| > 1.96 se[R0] (1)The question we have to solve is: How can we es-timate E[R0] and se[R0]?
The first step is that weconsider R and R0 to share the same standard errorse[R0] = se[R].
This value can then be estimatedfrom the test data.
In a second step, we give an es-timate for E[R0], either inherent in the evaluationmeasure (see below), or from the estimate for thecomparison system R?.A universal estimation method is the bootstrapestimate: The core idea is to create replications ofR by random sampling from the data set (Bisaniand Ney, 2004).
Bootstrapping is generally possi-ble for all evaluation measures.
With a high numberof replicates, se[R] and E[R0] can be estimated withsatisfactory precision.For a certain class of evaluation measures, theseparameters can be estimated more accurately and ef-ficiently from the evaluation data without resortingto Monte Carlo estimates.
This is the class of er-rors based on the arithmetic mean over a sentence-wise score: In our binary comparison experiments,each judge was given hypothesis translations ei,X ,ei,Y .
She could then judge ei,X to be better than,equal to, or worse than ei,Y .
All these judgmentswere counted over the systems.
We define a sentencescore ri,X,Y for this evaluation method as follows:ri,X,Y :=????
?+1 ei,X is better than ei,Y0 ei,X is equal to ei,Y?1 ei,X is worse than ei,Y.
(2)Then, the total evaluation score for a binary com-parison of systems X and Y isRX,Y :=1mm?i=1ri,X,Y , (3)with m the number of evaluated sentences.For this case, namelyR being an arithmetic mean,(Efron and Tibshirani, 1993) gives an explicit for-mula for the estimated standard error of the scoreRX,Y .
To simplify the notation, we will use R in-stead of RX,Y from now on, and ri instead of ri,X,Y .se[R] =1m ?
1???
?m?i=1(ri ?
R)2 .
(4)With x denoting the number of sentences whereri = 1, and y denoting the number of sentences99where ri = ?1,R =x ?
ym(5)and with basic algebrase[R] =1m ?
1?x + y ?
(x ?
y)2m.
(6)Moreover, we can explicitly give an estimate forE[R0]: The null hypothesis is that both systems are?equally good?.
Then, we should expect as manysentences where X is better than Y as vice versa,i.e.
x = y.
Thus, E[R0] = 0.Using Equation 4, we calculate se[R] and thus asignificance range for adequacy and fluency judg-ments.
When comparing two systems X and Y ,we assume for the null hypothesis that se[R0] =se[RX ] and E[R0] = E[RY ] (or vice versa).A very useful (and to our knowledge new) resultfor MT evaluation is that se[R] can also be explic-itly estimated for weighted means ?
such as WER,PER, and TER.
These measures are defined as fol-lows: Let di, i = 1, .
.
.
,m denote the number of ?er-rors?
(edit operations) of the translation candidate eiwith regard to a reference translation with length li.Then, the total error rate will be computed asR :=1Lm?i=1di (7)whereL :=m?i=1li (8)As a result, each sentence ei affects the overall scorewith weight li ?
the effect of leaving out a sen-tence with length 40 is four times higher than thatof leaving out one with length 10.
Consequently,these weights must be considered when estimatingthe standard error of R:se[R] =????
1(m ?
1)(L ?
1)m?i=1(dili?
R)2?
li(9)With this Equation, Monte-Carlo-estimates are nolonger necessary for examining the significance ofWER, PER, TER, etc.
Unfortunately, we do not ex-pect such a short explicit formula to exist for thestandard BLEU score.
Still, a confidence rangefor BLEU can be estimated by bootstrapping (Och,2003; Zhang and Vogel, 2004).Spanish EnglishTrain Sentences 1.2MWords 32M 31MVocabulary 159K 111KSingletons 63K 46KTest Sentences 1 117Words 26KOOV Words 72Table 1: Statistics of the EPPS Corpus.4 Evaluation SetupThe evaluation procedure was carried out on the datagenerated in the second evaluation campaign of theTC-STAR project4.
The goal of this project is tobuild a speech-to-speech translation system that candeal with real life data.
Three translation directionsare dealt with in the project: Spanish to English, En-glish to Spanish and Chinese to English.
For the sys-tem comparison we concentrated only in the Englishto Spanish direction.The corpus for the Spanish?English language pairconsists of the official version of the speeches held inthe European Parliament Plenary Sessions (EPPS),as available on the web page of the European Parlia-ment.
A more detailed description of the EPPS datacan be found in (Vilar et al, 2005).
Table 1 showsthe statistics of the corpus.A total of 9 different MT systems participated inthis condition in the evaluation campaign that tookplace in February 2006.
We selected five representa-tive systems for our study.
Henceforth we shall referto these systems as System A through System E. Werestricted the number of systems in order to keep theevaluation effort manageable for a first experimentalsetup to test the feasibility of our method.
The rank-ing of 5 systems can be carried out with as few as 7comparisons, but the ranking of 9 systems requires19 comparisons.5 Evaluation ResultsSeven human bilingual evaluators (6 native speakersand one near-native speaker of Spanish) carried outthe evaluation.
100 sentences were randomly cho-sen and assigned to each of the evaluators for everysystem comparison, as discussed in Section 3.3.
Theresults can be seen in Table 2 and Figure 2.
Counts4http://www.tc-star.org/1000 10 20 30 40 50 60 70010203040506070lllllll# "First system better"# "Second system better" lB?AD?CA?CE?AE?BB?DD?A(a) Each judge.0 100 200 300 4000100200300400# "First system better"# "Second system better"lB?AD?CA?C E?AE?BB?D D?A(b) All judges.Figure 2: Results of the binary comparisons.
Number of times the winning system was really judged ?better?vs.
number of times it was judged ?worse?.
Results in hatched area can not reject null hypothesis, i.e.
wouldbe considered insignificant.missing to 100 and 700 respectively denote ?samequality?
decisions.As can be seen from the results, in most of thecases the judges clearly favor one of the systems.The most notable exception is found when compar-ing systems A and C, where a difference of only 3sentences is clearly not enough to decide betweenthe two.
Thus, the two bottom positions in the finalranking could be swapped.Figure 2(a) shows the outcome for the binarycomparisons separately for each judge, together withan analysis of the statistical significance of the re-sults.
As can be seen, the number of samples (100)would have been too low to show significant re-sults in many experiments (data points in the hatchedarea).
In some cases, the evaluator even judged bet-ter the system which was scored to be worse by themajority of the other evaluators (data points abovethe bisector).
As Figure 2(b) shows, ?the only thingbetter than data is more data?
: When we summarizeR over all judges, we see a significant difference(with a confidence of 95%) at all comparisons buttwo (A vs. C, and E vs. B).
It is interesting to notethat exactly these two pairs do not show a significantdifference when using a majority vote strategy.Table 3 shows also the standard evaluation met-rics.
Three BLEU scores are given in this table, theone computed on the whole corpus, the one com-puted on the set used for standard adequacy and flu-ency computations and the ones on the set we se-lected for this task5.
It can be seen that the BLEUscores are consistent across all data subsets.
In thiscase the ranking according to this automatic measurematches exactly the ranking found by our method.When comparing with the adequacy and fluencyscores, however, the ranking of the systems changesconsiderably: B D E C A.
However, the differencebetween the three top systems is quite small.
Thiscan be seen in Figure 3, which shows some auto-matic and human scores for the five systems in ourexperiments, along with the estimated 95% confi-dence range.
The bigger difference is found whencomparing the bottom systems, namely System Aand System C. While our method produces nearlyno difference the adequacy and fluency scores indi-cate System C as clearly superior to System A.
It isworth noting that the both groups use quite differenttranslation approaches (statistical vs. rule-based).5Regretfully these two last sets were not the same.
This isdue to the fact that the ?AF Test Set?
was further used for eval-uating Text-to-Speech systems, and thus a targeted subset ofsentences was selected.101Sys E1 E2 E3 E4 E5 E6 E7?A 29 19 38 17 32 29 41 205B 40 59 48 53 63 64 45 372C 32 22 29 23 32 34 42 214D 39 61 59 50 64 58 46 377A 32 31 31 31 47 38 40 250C 37 29 32 22 39 45 43 247A 36 28 17 28 34 37 31 211E 41 47 44 43 53 45 58 331B 26 29 18 24 43 36 33 209E 34 33 28 27 32 29 43 226B 34 28 30 31 40 41 48 252D 23 17 23 17 24 28 38 170A 36 14 27 9 31 30 34 181D 34 50 40 50 57 61 57 349Final ranking (best?worst): E B D A CTable 2: Result of the binary system comparison.Numbers of sentences for which each system wasjudged better by each evaluator (E1-E7).Subset: Whole A+F BinarySys BLEU BLEU A F BLEUA 36.3 36.2 2.93 2.46 36.3B 49.4 49.3 3.74 3.58 49.2C 36.3 36.2 3.53 3.31 36.1D 48.2 46.8 3.68 3.48 47.7E 49.8 49.6 3.67 3.46 49.4Table 3: BLEU scores and Adequacy and Fluencyscores for the different systems and subsets of thewhole test set.
BLEU values in %, Adequacy (A)and Fluency (F) from 1 (worst) to 5 (best).6 DiscussionIn this section we will review the main drawbacks ofthe human evaluation listed in Section 2 and analyzehow our approach deals with them.
The first onewas the use of explicit numerical scores, which aredifficult to define exactly.
Our system was mainlydesigned for the elimination of this issue.Our evaluation continues to be time consuming.Even more, the number of individual comparisonsneeded is in the order of log(n!
), in contrast with thestandard adequacy-fluency evaluation which needs2n individual evaluations (two evaluations per sys-tem, one for fluency, another one for adequacy).
Forn in the range of 1 up to 20 (a realistic number ofsystems for current evaluation campaigns) these twoquantities are comparable.
And actually each of ourCADBECADBECADBECADBEllllllllllllllllll llFluencyAdequacy1?WERBLEU0.3 0.4 0.5 0.6 0.7worse <?
normalized score  ?> betterMeasure &SystemFigure 3: Normalized evaluation scores.
Higherscores are better.
Solid lines show the 95% con-fidence range.
Automatic scores calculated on thewhole test set, human scores on the A+F subset.evaluations should be simpler than the standard ad-equacy and fluency ones.
Therefore the time neededfor both evaluation procedures is probably similar.Reproducibility of the evaluation is also an impor-tant concern.
We computed the number of ?errors?in the evaluation process, i.e.
the number of sen-tences evaluated by two or more evaluators wherethe evaluators?
judgement was different.
Only in10% of the cases the evaluation was contradictory,in the sense that one evaluator chose one sentence asbetter than the other, while the other evaluator chosethe other one.
In 30% of the cases, however, oneevaluator estimated both sentences to be of the samequality while the other judged one sentence as supe-rior to the other one.
As comparison, for the fluency-adequacy judgement nearly one third of the com-mon evaluations have a difference in score greater orequal than two (where the maximum would be four),and another third a score difference of one point6.With respect to biases, we feel that it is almost im-possible to eliminate them if humans are involved.
Ifone of the judges prefers one kind of structure, therewill a bias for a system producing such output, in-dependently of the evaluation procedure.
However,the suppression of explicit numerical scores elimi-nates an additional bias of evaluators.
It has beenobserved that human judges often give scores within6Note however that possible evaluator biases can have agreat influence in these statistics.102a certain range (e.g.
in the mid-range or only ex-treme values), which constitute an additional diffi-culty when carrying out the evaluation (Leusch etal., 2005).
Our method suppresses this kind of bias.Another advantage of our method is the possibil-ity of assessing improvements within one system.With one evaluation we can decide if some modi-fications actually improve performance.
This eval-uation even gives us a confidence interval to weightthe significance of an improvement.
Carrying outa full adequacy-fluency analysis would require a lotmore effort, without giving more useful results.7 ConclusionWe presented a novel human evaluation techniquethat simplifies the task of the evaluators.
Our methodrelies on two basic observations.
The first one is thatin most evaluations the final goal is to find a rankingof different systems, the absolute scores are usuallynot so relevant.
Especially when considering humanevaluation, the scores are not even comparable be-tween two evaluation campaigns.
The second oneis the fact that a human judge can normally choosethe best one out of two translations, and this is amuch easier process than the assessment of numeri-cal scores whose definition is not at all clear.
Takingthis into consideration we suggested a method thataims at finding a ranking of different MT systemsbased on the comparison of pairs of translation can-didates for a set of sentences to be evaluated.A detailed analysis of the statistical significanceof the method is presented and also applied to somewide-spread automatic measures.
The evaluationmethodology was applied for the ranking of 5 sys-tems that participated in the second evaluation cam-paign of the TC-STAR project and comparison withstandard evaluation measures was performed.8 AcknowledgementsWe would like to thank the human judges who par-ticipated in the evaluation.
This work has beenfunded by the integrated project TC-STAR?
Tech-nology and Corpora for Speech-to-Speech Transla-tion ?
(IST-2002-FP6-506738).ReferencesM.
Bisani and H. Ney.
2004.
Bootstrap estimates forconfidence intervals in ASR performance evaluationx.IEEE ICASSP, pages 409?412, Montreal, Canada,May.T.
Bui and M. Thanh.
1985.
Significant improvements tothe Ford-Johnson algorithm for sorting.
BIT Numeri-cal Mathematics, 25(1):70?75.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.
Re-evaluating the role of BLEU in machine translation re-search.
Proceeding of the 11th Conference of the Eu-ropean Chapter of the ACL: EACL 2006, pages 249?256, Trento, Italy, Apr.G.
Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
Proc.
ARPA Workshop on Human LanguageTechnology.B.
Efron and R. J. Tibshirani.
1993.
An Introductionto the Bootstrap.
Chapman & Hall, New York andLondon.L.
Ford Jr and S. Johnson.
1959.
A Tournament Problem.The American Mathematical Monthly, 66(5):387?389.D.
E. Knuth.
1973.
The Art of Computer Programming,volume 3.
Addison-Wesley, 1st edition.
Sorting andSearching.P.
Koehn and C. Monz.
2006.
Manual and automaticevaluation of machine translation between europeanlanguages.
Proceedings of the Workshop on Statisti-cal Machine Translation, pages 102?121, New YorkCity, Jun.G.
Leusch, N. Ueffing, D. Vilar, and H. Ney.
2005.Preprocessing and normalization for automatic evalu-ation of machine translation.
43rd ACL: Proc.
Work-shop on Intrinsic and Extrinsic Evaluation Measuresfor MT and/or Summarization, pages 17?24, Ann Ar-bor, Michigan, Jun.F.
J. Och.
2003.
Minimum error rate training in statisti-cal machine translation.
Proc.
of the 41st ACL, pages160?167, Sapporo, Japan, Jul.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
Proc.
of the 40th ACL, pages 311?318,Philadelphia, PA, Jul.M.
Peczarski.
2002.
Sorting 13 elements requires 34comparisons.
LNCS, 2461/2002:785?794, Sep.M.
Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci-ulla, and R. Weischedel.
2005.
A study of translationerror rate with targeted human annotation.
TechnicalReport LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58, University of Maryland, College Park, MD.L.
Thurstone.
1927.
The method of paired comparisonsfor social values.
Journal of Abnormal and Social Psy-chology, 21:384?400.D.
Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.2005.
Statistical Machine Translation of EuropeanParliamentary Speeches.
Proceedings of MT SummitX, pages 259?266, Phuket, Thailand, Sep.M.
Wells.
1971.
Elements of combinatorial computing.Pergamon Press.Y.
Zhang and S. Vogel.
2004.
Measuring confidenceintervals for the machine translation evaluation met-rics.
Proceedings of the 10th International Conferenceon Theoretical and Methodological Issues in MachineTranslation, pages 4?6, Baltimore, MD.103
