Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 28?29,Vancouver, October 2005.THE MIT SPOKEN LECTURE PROCESSING PROJECTJames R. Glass, Timothy J. Hazen, D. Scott Cyphers, Ken Schutte and Alex ParkThe MIT Computer Science and Artificial Intelligence Laboratory32 Vassar Street, Cambridge, Massachusetts, 02476, USA{hazen,jrg,cyphers}@csail.mit.eduAbstractWe will demonstrate the MIT Spoken LectureProcessing Server and an accompanying lecturebrowser that students can use to quickly locate andbrowse lecture segments that apply to their query.We will show how lecturers can upload recordedlectures and companion text material to our serverfor automatic processing.
The server automaticallygenerates a time-aligned word transcript of the lec-ture which can be downloaded for use within abrowser.
We will also demonstrate a browser wehave created which allows students to quickly lo-cate and browse audio segments that are relevant totheir query.
These tools can provide students witheasier access to audio (or audio/visual) lectures,hopefully improving their educational experience.1 IntroductionOver the past decade there has been increasingamounts of educational material being made avail-able on-line.
Projects such as MIT OpenCourse-Ware provide continuous worldwide access toeducational materials to help satisfy our collectivethirst for knowledge.
While the majority of suchmaterial is currently text-based, we are beginningto see dramatic increases in the amount of audioand visual recordings of lecture material.
Unliketext materials, untranscribed audio data can be te-dious to browse, making it difficult to utilize theinformation fully without time-consuming datapreparation.
Moreover, unlike some other forms ofspoken communication such as telephone conver-sations or television and radio broadcasts, lectureprocessing has until recently received little atten-tion or benefit from the development of humanlanguage technology.
The single biggest effort, todate, is on-going work in Japan using the Corpusof Spontaneous Japanese [1,3,4].Lectures are particularly challenging for auto-matic speech recognizers because the vocabularyused within a lecture can be very technical andspecialized, yet the speaking style can be veryspontaneous.
As a result, even if parallel text mate-rials are available in the form of textbooks or re-lated papers, there are significant linguisticdifferences between written and oral communica-tion styles.
Thus, it is a challenge to predict how awritten passage might be spoken, and vice versa.By helping to focus a research spotlight on spokenlecture material, we hope to begin to overcomethese and many other fundamental issues.While audio-visual lecture processing will per-haps be ultimately most useful, we have initiallyfocused our attention on the problem of spokenlecture processing.
Within this realm there aremany challenging research issues pertaining to thedevelopment of effective automatic transcription,indexing, and summarization.
For this project, ourgoals have been to a) help create a corpus of spo-ken lecture material for the research community, b)analyze this corpus to better understand the lin-guistic characteristics of spoken lectures, c) per-form speech recognition and information retrievalexperiments on these data to benchmark perform-ance on these data, d) develop a prototype spokenlecture processing server that will allow educatorsto automatically annotate their recorded lecturedata, and e) develop prototype software that willallow students to browse the resulting annotatedlectures.2 Project DetailsAs mentioned earlier, we have developed a web-based Spoken Lecture Processing Server(http://groups.csail.mit.edu/sls/lectures) in whichusers can upload audio files for automatic tran-scription and indexing.
In our work, we have ex-28perimented with collecting audio data using asmall personal digital audio recorder (an iRiverN10).
To help the speech recognizer, users canprovide their own supplemental text files, such asjournal articles, book chapters, etc., which can beused to adapt the language model and vocabularyof the system.
Currently, the key steps of the tran-scription process are as follows: a) adapt a topic-independent vocabulary and language model usingany supplemental text materials, b) automaticallysegment the audio file into short chunks of pause-delineated speech, and c) automatically annotatethese chunks using a speech recognition system.Language model adaptation is performed is twosteps.
First the vocabulary of any supplemental textmaterial is extracted and added to an existingtopic-independent vocabulary of nearly 17Kwords.
Next, the recognizer merges topic-independent word sequence statistics from anexisting corpus of lecture material with the topic-dependent statistics of the supplemental material tocreate a topic-adapted language model.The segmentation algorithm is performed in twosteps.
First the audio file is arbitrarily broken into10-second chunks for speech detection processingusing an efficient speaker-independent phoneticrecognizer.
To help improve its speech detectionaccuracy, this recognizer contains models for non-lexical artifacts such as laughs and coughs as wellas a variety of other noises.
Contiguous regions ofspeech are identified from the phonetic recognitionoutput (typically 6 to 8 second segments of speech)and passed alone to our speech recognizer forautomatic transcription.
The speech segmentationand transcription steps are currently performed in adistributed fashion over a bank of computationservers.
Once recognition is completed, the audiodata is indexed (based on the recognition output) inpreparation for browsing by the user.The lecture browser provides a graphical user in-terface to one or more automatically transcribedlectures.
A user can type a text query to thebrowser and receive a list of hits within the in-dexed lectures.
When a hit is selected, it is shownin the context of the lecture transcription.
The usercan adjust the duration of context preceding andfollowing the hit, navigate to and from the preced-ing and following parts of the lecture, and listen tothe displayed segment.
Orthographic segments arehighlighted as they are played.3 Experimental ResultsTo date we have collected and analyzed a corpusof approximately 300 hours of audio lectures in-cluding 6 full MIT courses and 80 hours of semi-nars from the MIT World web site [2].
We arecurrently in the process of expanding this corpus.From manual transcriptions we have generated andverified time-aligned transcriptions for 169 hoursof our corpus, and we are in the process of time-aligning transcriptions for the remainder of ourcorpus.We have performed initial speech recognitionexperiments using 10 computer science lectures.
Inthese experiments we have discovered that, despitehigh word error rates (in the area of 40%), retrievalof short audio segments containing important key-words and phrases can be performed with a high-degree of reliability (over 90% F-measure whenexamining precision and recall results) [5].
Theseresults are similar in nature to the findings in theSpeechBot project (which performs a similar ser-vice for online broadcast news archives) [6].References[1] S. Furui, ?Recent advances in spontaneous speechrecognition and understanding,?
in Proc.
ISCA & IEEEWorkshop on Spontaneous Speech Processing and Rec-ognition (SSPR), pp.
1-6, Tokyo, April 2003.
[2] J.
Glass, T. Hazen, L. Hetherington, and C. Wang,?Analysis and Processing of Lecture Audio Data: Pre-liminary Investigations,?
in Proc.
HLT/NAACL SpeechIndexing Workshop, 9-12, Boston, May 2004.
[3] T. Kawahara, H. Nanjo.
And S. Furui,  ?Automatictranscription of spontaneous lecture speech,?
in IEEEWorkshop on Automatic Speech Recognition and Un-derstanding, pp.
186-189, Trento, Italy, December2001.
[4] H. Nanjo and T. Kawahara, ?Language model andspeaking rate adaptation for spontaneous speech recog-nition,?
IEEE Transactions of Speech and Audio Proc-essing, vol.
12, no.
4, pp.
391-400, July 2004.
[5] A.
Park, T. Hazen, and J.
Glass, "AutomaticProcessing of Audio Lectures for Information Retrieval:Vocabulary Selection and Language Modeling," Proc.ICASSP, Philadelphia, PA, March 2005.
[6] J.-M. Van Thong, et al ?SpeechBot: An experimen-tal speech-based search engine for multimedia contenton the web.
IEEE Transactions of Multimedia, vol.
4,no.
1, pp.
88-96, March 2002.29
