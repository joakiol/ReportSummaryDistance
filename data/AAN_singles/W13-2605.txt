Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 37?46,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsAn Analysis of Memory-based Processing Costs using IncrementalDeep Syntactic Dependency Parsing?Marten van SchijndelThe Ohio State Universityvanschm@ling.osu.eduLuan NguyenUniversity of Minnesotalnguyen@cs.umn.eduWilliam SchulerThe Ohio State Universityschuler@ling.osu.eduAbstractReading experiments using naturalisticstimuli have shown unanticipated facili-tations for completing center embeddingswhen frequency effects are factored out.To eliminate possible confounds due tosurface structure, this paper introduces aprocessing model based on deep syntac-tic dependencies.
Results on eye-trackingdata indicate that completing deep syntac-tic embeddings yields significantly morefacilitation than completing surface em-beddings.1 IntroductionSelf-paced reading and eye-tracking experimentshave often been used to support theories aboutinhibitory effects of working memory operationsin sentence processing (Just and Carpenter, 1992;Gibson, 2000; Lewis and Vasishth, 2005), but itis possible that many of these effects can be ex-plained by frequency (Jurafsky, 1996; Hale, 2001;Karlsson, 2007).
Experiments on large naturalis-tic text corpora (Demberg and Keller, 2008; Wu etal., 2010; van Schijndel and Schuler, 2013) haveshown significant memory effects at the ends ofcenter embeddings when frequency measures havebeen included as separate factors, but these mem-ory effects have been facilitatory rather than in-hibitory.Some of the memory-based measures that pro-duce these facilitatory effects (Wu et al 2010; vanSchijndel and Schuler, 2013) are defined in termsof initiation and integration of connected compo-nents of syntactic structure,1 with the presumption?
*Thanks to Micha Elsner and three anonymous review-ers for their feedback.
This work was funded by an Ohio StateUniversity Department of Linguistics Targeted Investmentfor Excellence (TIE) grant for collaborative interdisciplinaryprojects conducted during the academic year 2012?13.1Graph theoretically, the set of connected componentsthat referents that belong to the same connectedcomponent may cue one another using content-based features, while those that do not must relyon noisier temporal features that just encode howrecently a referent was accessed.
These measures,based on left-corner parsing processes (Johnson-Laird, 1983; Abney and Johnson, 1991), abstractcounts of unsatisfied dependencies from noun orverb referents (Gibson, 2000) to cover all syntacticdependencies, motivated by observations of Dem-berg and Keller (2008) and Kwon et al(2010) ofthe inadequacies of Gibson?s narrower measure.But these experiments use naturalistic stimuliwithout constrained manipulations and thereforemight be susceptible to confounds.
It is possiblethat the purely phrase-structure-based connectedcomponents used previously may ignore some in-tegration costs associated with filler-gap construc-tions, making them an unsuitable generalization ofGibson-style dependencies.
It is also possible thatthe facilitatory effect for integration operations innaturally-occurring stimuli may be driven by syn-tactic center embeddings that arise from modifiers(e.g.
The CEO sold [[the shares] of the com-pany]), which do not require any dependenciesto be deferred, but which might be systematicallyunder-predicted by frequency measures, produc-ing a confound with memory measures when fre-quency measures are residualized out.In order to eliminate possible confounds due toexclusion of unbounded dependencies in filler-gapconstructions, this paper evaluates a processingmodel that calculates connected components ondeep syntactic dependency structures rather thansurface phrase structure trees.
This model ac-counts unattached fillers and gaps as belongingto separate connected components, and thereforeperforms additional initiation and integration op-of a graph ?V, E?
is the set of maximal subsets ofit {?V1, E1?, ?V2, E2?, ...} such that any pair of vertices ineach Vi can be connected by edges in the corresponding Ei.37a) Noun PhraseRelative ClauseSentence w. GapVerb Phrase w. GapSentence w. GapmillionsstolesayofficialswhoNoun Phrasepersontheb)i1i2 i3i4i5i6i7121211sayofficialsstolewhopersonmillionsthe0000000Figure 1: Graphical representation of (a) a singleconnected component of surface syntactic phrasestructure corresponding to (b) two connected com-ponents of deep syntactic dependency structure forthe noun phrase the person who officials say stolemillions, prior to the word say.
Connections es-tablished prior to the word say are shown in black;subsequent connections are shown in gray.erations in filler-gap constructions as hypothesizedby Gibson (2000) and others.
Then, in order tocontrol for possible confounds due to modifier-induced center embedding, this refined model isapplied to two partitions of an eye-tracking cor-pus (Kennedy et al 2003): one consisting of sen-tences containing only non-modifier center em-beddings, in which dependencies are deferred, andthe other consisting of sentences containing nocenter embeddings or containing center embed-dings arising from attachment of final modifiers,in which no dependencies are deferred.
Processingthis partitioned corpus with deep syntactic con-nected components reveals a significant increasein facilitation in the non-modifier partition, whichlends credibility to the observation of negativeintegration cost in processing naturally-occurringsentences.2 Connected ComponentsThe experiments described in this paper evalu-ate whether inhibition and facilitation in readingcorrelate with operations in a hierarchic sequen-tial prediction model that initiate and integrateconnected components of hypothesized syntacticstructure during incremental parsing.
The modelused in these experiments refines previous con-nected component models by allowing fillers andgaps to occur in separate connected componentsof a deep syntactic dependency graph (Mel?c?uk,1988; Kintsch, 1988), even when they belong tothe same connected component when defined onsurface structure.For example, the surface syntactic phrase struc-ture and deep syntactic dependency structure forthe noun phrase the person who officials say stolemillions are shown in Figure 1.2 Notice that af-ter the word officials, there is only one connectedcomponent of surface syntactic phrase structure(from the root noun phrase to the verb phrase withgap), but two disjoint connected components ofdeep syntactic dependency structure (one endingat i3, and another at i5).
Only the deep syntacticdependency structure corresponds to familiar (Justand Carpenter, 1992; Gibson, 1998) notions ofhow memory is used to store deferred dependen-cies in filler-gap constructions.
The next sectionwill describe a generalized categorial grammar,which (i) can be viewed as context-free, to seed alatent-variable probabilistic context-free grammarto accurately derive parses of filler-gap construc-tions, and (ii) can be viewed as a deep syntacticdependency grammar, defining dependencies forconnected components in terms of function appli-cations.3 Generalized Categorial GrammarIn order to evaluate memory effects for hypothe-sizing unbounded dependencies between referentsof fillers and referents of clauses containing gaps,a memory-based processor must define connectedcomponents in terms of deep syntactic dependen-cies (including unbounded dependencies) ratherthan in terms of surface syntactic phrase structuretrees.
To do this, at least some phrase structureedges must be removed from the set of connec-tions that define a connected component.Because these unbounded dependencies are notrepresented locally in the original Treebank for-mat, probabilities for operations on these modified2Following Mel?c?uk (1988) and Kintsch (1988),the graphical dependency structure adopted here usespositionally-defined labels (?0?
for the predicate label, ?1?for the first argument ahead of a predicate, ?2?
for the lastargument behind, etc.)
but includes unbounded dependen-cies between referents of fillers and referents of clausescontaining gaps.
It is assumed that semantically-labeledstructures would be isomorphic to the structures definedhere, but would generalize across alternations such as activeand passive constructions, for example.38connected components are trained on a corpus an-notated with generalized categorial grammar de-pendencies for ?gap?
arguments at all categoriesthat subsume a gap (Nguyen et al 2012).
Thisrepresentation is similar to the HPSG-like repre-sentation used by Hale (2001) and Lewis and Va-sishth (2005), but has a naturally-defined depen-dency structure on which to calculate connectedcomponents.
This generalized categorial grammaris then used to identify the first sign that introducesa gap, at which point a deep syntactic connectedcomponent containing the filler can be encoded(stored), and a separate deep syntactic connectedcomponent for a clause containing a gap can beinitiated.A generalized categorial grammar (Bach, 1981)consists of a set U of primitive category types;a set O of type-constructing operators allowing arecursive definition of a set of categories C =defU ?
(C ?
O ?
C); a set X of vocabulary items;a mapping M from vocabulary items in X to se-mantic functions with category types in C; anda set R of inference rules for deriving functionswith category types inC from other functions withcategory types in C. Nguyen et al(2012) useprimitive category types for clause types (e.g.
Vfor finite verb-headed clause, N for noun phraseor nominal clause, D for determiners and pos-sessive clauses, etc.
), and use the generalized setof type-constructing operators to characterize notonly function application dependencies betweenarguments immediately ahead of and behind afunctor (-a and -b, corresponding to ?\?
and ?/?
inAjdukiewicz-Bar-Hillel categorial grammars), butalso long-distance dependencies between fillersand categories subsuming gaps (-g), dependenciesbetween relative pronouns and antecedent modif-icands of relative clauses (-r), and dependenciesbetween interrogative pronouns and their argu-ments (-i), which remain unsatisfied in derivationsbut function to distinguish categories for contentand polar questions.
A lexicon can then be de-fined in M to introduce lexical dependencies andobligatory pronominal dependencies using num-bered functions for predicates and deep syntacticarguments, for example:the ?
(?i (0 i)=the) : Dperson ?
(?i (0 i)=person) : N-aDwho ?
(?k i (0 i)=who ?
(1 i)=k) : N-rNofficials ?
(?i (0 i)=officials) : NtheDpersonN-aDN AawhoN-rNofficialsNsayV-aN-bVstoleV-aN-bNmillionsNV-aN AeV-gN GaV-aN-gNAgV-gN AcV-rN FcN RFigure 2: Example categorization of the nounphrase the person who officials say stole millions.say ?
(?i (0 i)=say) : V-aN-bVstole ?
(?i (0 i)=stole) : V-aN-bNmillions ?
(?i (0 i)=millions) : NInference rules in R are then defined to com-pose arguments and modifiers and propagate gaps.Arguments g of type d ahead of functors h oftype c-ad are composed by passing non-local de-pendencies ?
?
{-g, -i, -r} ?
C from premises toconclusion in all combinations:g:d h: c-ad ?
( fc-ad g h): c (Aa)g:d?
h: c-ad ?
?k ( fc-ad (g k) h): c?
(Ab)g:d h: c-ad?
?
?k ( fc-ad g (h k)): c?
(Ac)g:d?
h: c-ad?
?
?k ( fc-ad (g k) (h k)): c?
(Ad)Similar rules compose arguments behind functors:g: c-bd h:d ?
( fc-bd g h): c (Ae)g: c-bd?
h:d ?
?k ( fc-bd (g k) h): c?
(Af)g: c-bd h:d?
?
?k ( fc-bd g (h k)): c?
(Ag)g: c-bd?
h:d?
?
?k ( fc-bd (g k) (h k)): c?
(Ah)These rules use composition functions fc-adand fc-bd for initial and final arguments, which de-fine dependency edges numbered v from referentsof predicate functors i to referents of arguments j,where v is the number of unsatisfied arguments?1...?v ?
{-a, -b} ?C in a category label:fu?1..v?1-acdef= ?g h i ?
j (v i)= j ?
(g j) ?
(h i) (1a)fu?1..v?1-bcdef= ?g h i ?
j (v i)= j ?
(g i) ?
(h j) (1b)R also contains inference rules to compose mod-ifier functors g of type u-ad ahead of modifi-cands h of type d:g: u-ad h:c ?
( fIM g h):c (Ma)g: u-ad?
h:c ?
?k ( fIM (g k) h):c?
(Mb)g: u-ad h:c?
?
?k ( fIM g (h k)):c?
(Mc)39?i1 j1.. i?
j?
... ?
(g?
:c/d { j?}
i?)
xt?i1 j1.. i?
... ?
((g?
f ):c i?
)xt ?
f :d (?Fa)?i1 j1.. i?
j?
... ?
(g?
:c/d { j?}
i?)
xt?i1 j1.. i?
j?i?+1 ... ?
(g?
:c/d { j?}
i?)
?
( f :e i?+1)xt ?
f :e (+Fa)?i1 j1..
i?
?1 j??1i?
... ?
(g?
:d i?
)?i1 j1.. i?
j?
... ?
(( f g?
):c/e { j?}
i?)??????????????????
?g:d h:e ?
( f g h):c org:d h:e ?
?k( f (g k) h):c org:d h:e ?
?k( f g (h k)):c org:d h:e ?
?k( f (g k) (h k)):c(?La)?i1 j1..
i?
?1 j??1i?
... ?
(g?
?1:a/c { j?
?1} i?
?1) ?
(g?
:d i?
)?i1 j1..
i?
?1 j?
?1 ... ?
(g?
?1 ?
( f g?
):a/e { j?
?1} i??1)??????????????????
?g:d h:e ?
( f g h):c org:d h:e ?
?k( f (g k) h):c org:d h:e ?
?k( f g (h k)):c org:d h:e ?
?k( f (g k) (h k)):c(+La)Figure 3: Basic processing productions of a right-corner parser.g: u-ad?
h:c?
?
?k ( fIM (g k) (h k)):c?
(Md)or for modifier functors behind a modificand:g:c h: u-ad ?
( fFM g h):c (Me)g:c?
h: u-ad ?
?k ( fFM (g k) h):c?
(Mf)g:c h: u-ad?
?
?k ( fFM g (h k)):c?
(Mg)g:c?
h: u-ad?
?
?k ( fFM (g k) (h k)):c?
(Mh)These rules use composition functions fIM and fFMfor initial and final modifiers, which define depen-dency edges numbered ?1?
from referents of mod-ifier functors i to referents of modificands j:fIMdef= ?g h j ?i (1 i)= j ?
(g i) ?
(h j) (2a)fFMdef= ?g h j ?i (1 i)= j ?
(g j) ?
(h i) (2b)R also contains inference rules for hypothesiz-ing gaps -gd for arguments and modifiers:3g: c-ad ?
?k ( fc-ad {k} g): c-gd (Ga)g: c-bd ?
?k ( fc-ad {k} g): c-gd (Gb)g:c ?
?k ( fIM {k} g):c-gd (Gc)and for attaching fillers e, d-re, d-ie as gaps -gd:g:e h: c-gd ?
?i ?
j (g i) ?
(h i j):e (Fa)g:d-re h: c-gd ?
?k j ?i (g k i) ?
(h i j): c-re (Fb)g:d-ie h: c-gd ?
?k j ?i (g k i) ?
(h i j): c-ie (Fc)3Since these unary inferences perform no explicit compo-sition, they are defined to use only initial versions composi-tion functions fc-ad and fIM.and for attaching modificands as antecedents ofrelative pronouns:g:e h:c-rd ?
?i ?
j (g i) ?
(h i j):e (R)An example derivation of the noun phrase the per-son who officials say stole millions using theserules is shown in Figure 2.
The semantic expres-sion produced by this derivation consists of a con-junction of terms defining the edges in the graphshown in Figure 1b.This GCG formulation captures many of the in-sights of the HPSG-like context-free filler-gap no-tation used by Hale (2001) or Lewis and Vasishth(2005): inference rules with adjacent premises canbe cast as context-free grammars and weighted us-ing probabilities, which allow experiments to cal-culate frequency measures for syntactic construc-tions.
Applying a latent variable PCFG trainer(Petrov et al 2006) to this formulation was shownto yield state-of-the-art accuracy for recovery ofunbounded dependencies (Nguyen et al 2012).Moreover, the functor-argument dependencies ina GCG define deep syntactic dependency graphsfor all derivations, which can be used in incremen-tal parsing to calculate connected components formemory-based measures.4 Incremental ProcessingIn order to obtain measures of memory opera-tions used in incremental processing, these GCGinference rules are combined into a set of parser40?i1 j1.. in jn.. i?
j?
... ?
(gn:y/z?
{ jn} in) ?
... ?
(g?
:c/d { j?}
i?)
xt?i1 j1.. in jn.. i?
... ?
(gn:y/z?
{ jn} in) ?
... ?
((g?
( f ?
{ jn} f )):c i?
)xt ?
?k( f ?
{k} f ):d(?Fb)?i1 j1.. in jn.. i?
j?
... ?
(gn:y/z?
{ jn} in) ?
... ?
(g?
:c/d { j?}
i?)
xt?i1 j1.. in jn.. i?
j?i?+1 ... ?
(gn:y/z?
{ jn} in) ?
... ?
(g?
:c/d { j?}
i?)
?
(( f ?
{ jn} f ):e i?+1)xt ?
?k( f ?
{k} f ):e(+Fb)?i1 j1.. in jn..
i?
?1 j??1i?
... ?
(gn:y/z?
{ jn} in) ?
... ?
(g?
:d i?
)?i1 j1.. in jn.. i?
j?
... ?
(gn:y/z?
{ jn} in) ?
... ?
(( f g?)
?
( f ?
{ jn}):c?/e { j?}
i?
)g:d h:e ?
?k( f g ( f ?
{k} h)):c?
(?Lb)?i1 j1.. in jn..
i?
?1 j??1i?
... ?
(gn:y/z?
{ jn} in) ?
... ?
(g??1:a/c?
{ j?
?1} i?
?1) ?
(g?
:d i?
)?i1 j1.. in jn..
i?
?1 j?
?1 ... ?
(gn:y/z?
{ jn} in) ?
... ?
(g?
?1 ?
( f g?)
?
( f ?
{ jn}):a/e { j?
?1} i?
?1)g:d h:e ?
?k( f g ( f ?
{k} h)):c?
(+Lb)Figure 4: Additional processing productions for attaching a referent of a filler jn as the referent of a gap.productions, similar to those of the ?right corner?parser of van Schijndel and Schuler (2013), ex-cept that instead of recognizing shallow hierarchi-cal sequences of connected components of surfacestructure, the parser recognizes shallow hierarchi-cal sequences of connected components of deepsyntactic dependencies.
This parser exploits theobservation (van Schijndel et al in press) that left-corner parsers and their variants do not need to ini-tiate or integrate more than one connected compo-nent at each word.
These two operations are thenaugmented with rules to introduce fillers and at-tach fillers as gaps.This parser is defined on incomplete connectedcomponent states which consist of an active sign(with a semantic referent and syntactic form orcategory) lacking an awaited sign (also with a ref-erent and category) yet to come.
Semantic func-tions of active and awaited signs are simplified todenote only sets of referents, with gap arguments(?k) stripped off and handled by separate con-nected components.
Incomplete connected com-ponents, therefore, always denote semantic func-tions from sets of referents to sets of referents.This paper will notate semantic functions ofconnected components using variables g and h, in-complete connected component categories as c/d(consisting of an active sign of category c and anawaited sign of category d), and associations be-tween them as g:c/d.
The semantic representa-tion used here is simply a deep syntactic depen-dency structure, so a connected component func-tion is satisfied if it holds for some output ref-erent i given input referent j.
This can be no-tated ?i j (g:c/d { j} i), where the set { j} is equiva-lent to (?
j?
j?= j).
Connected component functionsthat have a common referent j can then be com-posed into larger connected components:4?i jk (g { j} i) ?
(h {k} j) ?
?i j (g?h {k} i) (3)Hierarchies of ?
connected compo-nents can be represented as conjunctions:?i1 j1... i?
j?
(g1:c1/d1 { j1} i1) ?
... ?
(g?:c?/d?
{ j?}
i?
).This allows constraints such as unbounded depen-dencies between referents of fillers and referentsof clauses containing gaps to be specified acrossconnected components by simply plugging vari-ables for filler referents into argument positionsfor gaps.A nondeterministic incremental parser can nowbe defined as a deductive system, given an inputsequence consisting of an initial connected com-ponent state of category T/T, corresponding to anexisting discourse context, followed by a sequenceof observations x1, x2, .
.
.
, processed in time order.As each xt is encountered, it is connected to an ex-isting connected component or it introduces a newdisjoint component using the productions shownin Figures 3, 4, and 5.4These are connected components of dependency struc-ture resulting from one or more composition functions beingcomposed, with each function?s output as the previous func-tion?s second argument.
This uses a standard definition offunction composition: (( f ?
g) x) = ( f (g x)).41?i1 j1..
i?
?1 j??1i?
... ?
(g?
:d i?
)?i1 j1.. i?
j?
... ?
(( f g?)
?
(?h k i (h k)):a/e?
{ j?}
i?
)g:d h:e?
?
( f g h):c (?Lc)?i1 j1..
i?
?1 j??1i?
... ?
(g?
?1:a/c { j?
?1} i?
?1) ?
(g?
:d i?
)?i1 j1..
i?
?1 j?
?1 ... ?
(g?
?1 ?
( f g?)
?
(?h k i (h k)):a/e?
{ j?
?1} i?
?1)g:d h:e?
?
( f g h):c (+Lc)?i1 j1.. i?
j?
... ?
(g??1:c/d?
{ j?
?1} i?
?1) ?
(g?
:d?/e { j?}
i?
)?i1 j1..
i?
?1 j?
?1 ... ?
(g?
?1 ?
(?h i?
j(h j)) ?
g?
:c/e { j?
?1} i?
?1)(+N)Figure 5: Additional processing productions for hypothesizing filler-gap attachment.Operations on dependencies that can be derivedfrom surface structure (see Figure 3) are takendirectly from van Schijndel and Schuler (2013).First, if an observation xt can immediately fillthe awaited sign of the last connected componentg?
:c/d, it is hypothesized to do so, turning thisincomplete connected component into a completeconnected component (g?
f ):c (Production ?Fa); orif the observation can serve as an initial sub-signof this awaited sign, it is hypothesized to form anew complete sign f :e in a new component with xtas its first observation (Production +Fa).
Then,if either of these resulting complete signs g?
:dcan immediately attach as an initial child of theawaited sign of the most recent connected com-ponent g?
?1:a/c, it is hypothesized to merge andextend this connected component, with xt as thelast observation of the completed connected com-ponent (Production +La); or if it can serve as aninitial sub-sign of this awaited sign, it is hypoth-esized to remain disjoint and form its own con-nected component (Production ?La).
The sideconditions of La productions are defined to unpackgap propagation (instances of ?k that distinguishrules Aa?h and Ma?h) from the inference rulesin Section 3, because this functionality will be re-placed with direct substitution of referent variablesinto subordinate semantic functions, below.The Nguyen et al(2012) GCG was definedto pass up unbounded dependencies, but in in-cremental deep syntactic dependency processing,unbounded dependencies are accounted as sepa-rate connected components.
When hypothesizingan unbounded dependency, the processing modelsimply cues the active sign of a previous connectedcomponent containing a filler without completingthe current connected component.
The four +F,?F, +L, and ?L operations are therefore combinedwith applications of unary rules Ga?c for hypoth-esizing referents as fillers for gaps (providing f ?in the equations in Figure 4).
Productions ?Fband +Fb fill gaps in initial children, and Produc-tions ?Lb and +Lb fill gaps in final children.
Notethat the Fb and Lb productions apply to the sametypes of antecedents as Fa and La productions re-spectively, so members of these two sets of pro-ductions cannot be applied together.Applications of rules Fa?c and R for introduc-ing fillers are applied to store fillers as existentiallyquantified variable values in Lc productions (seeFigure 5).
These Lc productions apply to the sametype of antecedent as La and Lb productions, sothese also cannot be applied together.Finally, connected components separated bygaps which are no longer hypothesized (?)
arereattached by a +N production.
This +N pro-duction may then be paired with a ?N productionwhich yields its antecedent unchanged as a conse-quent.
These N productions apply to antecedentsand consequents of the same type, so they may beapplied together with one F and one L production,but since the +N production removes in its conse-quent a ?
argument required in its antecedent, itmay not apply more than once in succession (andapplying the ?N production more than once in suc-cession has no effect).An incremental derivation of the noun phrasethe person who officials say stole millions, usingthese productions, is shown in Figure 6.5 EvaluationThe F, L, and N productions defined in the pre-vious section can be made probabilistic by firstcomputing a probabilistic context-free grammar(PCFG) from a tree-annotated corpus, then trans-forming that PCFG model into a model of prob-abilities over incremental parsing operations us-ing a grammar transform (Schuler, 2009).
Thisallows the intermediate PCFG to be optimized us-ing an existing PCFG-based latent variable trainer42?i0 (.. :T/T {i0} i0) the?i0 i2 (.. :T/T {i0} i0) ?
(.. :N/N-aD {i2} i2)+Fa,?La,?Nperson?i0 i2 (.. :T/T {i0} i0) ?
(.. :N/V-rN {i2} i2)?Fa,?La,?Nwho?i0 i2 i3 (.. :T/T {i0} i0) ?
(.. :N/V-gN {i3} i2)+Fa,+Lc,?Nofficials?i0 i2 i3 i5 (.. :T/T {i0} i0) ?
(.. :N/V-gN {i3} i2) ?
(.. :V-gN/V-aN-gN {i5} i5)+Fa,?La,?Nsay?i0 i2 i6 (.. :T/T {i0} i0) ?
(.. :N/V-aN {i6} i2)+Fb,+La,+Nstole?i0 i2 i7 (.. :T/T {i0} i0) ?
(.. :N/N {i7} i2)+Fa,+La,?Nmillions?i0 (.. :T/T {i0} i0)?Fa,+La,?NFigure 6: Derivation of the person who officials say stole millions, showing connected components withunique referent variables (calculated according to the equations in Section 4).
Semantic functions areabbreviated to ?..?
for readability.
This derivation yields the following lexical relations: (0 i1)=the,(0 i2)=person, (0 i3)=who, (0 i4)=officials, (0 i5)=say, (0 i6)=stole, (0 i7)=millions, and the followingargument relations: (1 i2)=i1, (1 i3)=i2, (1 i5)=i4, (2 i5)=i6, (1 i6)=i3, (2 i6)=i7.
(Petrov et al 2006).
When applied to the outputof this trainer, this transform has been shown toproduce comparable accuracy to that of the origi-nal Petrov et al(2006) CKY parser (van Schijn-del et al 2012).
The transform used in these ex-periments diverges from that of Schuler (2009), inthat the probability associated with introducing agap in a filler-gap construction is reallocated froma ?F?L operation to a +F?L operation (to encodethe previously most subordinate connected com-ponent with the filler as its awaited sign and be-gin a new disjoint connected component), and theprobability associated with resolving such a gap isreallocated from an implicit ?N operation to a +Noperation (to integrate the connected componentcontaining the gap with that containing the filler).In order to verify that the modifications to thetransform correctly reallocate probability mass forgap operations, the goodness of fit to readingtimes of a model using this modified transformis compared against the publicly-available base-line model from van Schijndel and Schuler (2013),which uses the original Schuler (2009) transform.5To ensure a valid comparison, both parsers aretrained on a GCG-reannotated version of the WallStreet Journal portion of the Penn Treebank (Mar-cus et al 1993) before being fit to reading timesusing linear mixed-effects models (Baayen et al2008).6 This evaluation focuses on the process-ing that can be done up to a given point in a sen-tence.
In human subjects, this processing includesboth immediate lexical access and regressions that5The models used here also use random slopes to reducetheir variance, which makes them less anticonservative.6The models are built using lmer from the lme4R package(Bates et al 2011; R Development Core Team, 2010).aid in the integration of new information, so thereading times of interest in this evaluation are log-transformed go-past durations.7The first and last word of each line in theDundee corpus, words not observed at least 5times in the WSJ training corpus, and fixations af-ter long saccades (>4 words) are omitted from theevaluation to filter out wrap-up effects, parser in-accuracies, and inattention and track loss of theeyetracker.
The following predictors are centeredand used in each baseline model: sentence posi-tion, word length, whether or not the previous ornext word were fixated upon, and unigram and bi-gram probabilities.8 Then each of the followingpredictors is residualized off each baseline beforebeing centered and added to it to help residualizethe next factor: length of the go-past region, cumu-lative total surprisal, total surprisal (Hale, 2001),and cumulative entropy reduction (Hale, 2003).9All 2-way interactions between these effects are7Go-past durations are calculated by summing all fixa-tions in a region of text, including regressions, until a newregion is fixated, which accounts for additional processingthat may take place after initial lexical access, but before thenext region is processed.
For example, if one region ends atword 5 in a sentence, and the next fixation lands on word 8,then the go-past region consists of words 6-8 while go-pastduration sums all fixations until a fixation occurs after word8.
Log-transforming eye movements and fixations may maketheir distributions more normal (Stephen and Mirman, 2010)and does not substantially affect the results of this paper.8For the n-gram model, this study uses the Brown corpus(Francis and Kucera, 1979), the WSJ Sections 02-21 (Mar-cus et al 1993), the written portion of the British NationalCorpus (BNC Consortium, 2007), and the Dundee corpus(Kennedy et al 2003) smoothed with modified Kneser-Ney(Chen and Goodman, 1998) in SRILM (Stolcke, 2002).9Non-cumulative metrics are calculated from the finalword of the go-past region; cumulative metrics are summedover the go-past region.43included as predictors along with the predictorsfrom the previous go-past region (to account forspillover effects).
Finally, each model has sub-ject and item random intercepts added in additionto by-subject random slopes (cumulative total sur-prisal, whether the previous word was fixated, andlength of the go-past region) and is fit to centeredlog-transformed go-past durations.10The Akaike Information Criterion (AIC)indicates that the gap-reallocating model(AIC = 128,605) provides a better fit to readingtimes than the original model (AIC = 128,619).11As described in Section 1, previous findings ofnegative integration cost may be due to a confoundwhereby center-embedded constructions causedby modifiers, which do not require deep syntac-tic dependencies to be deferred, may be drivingthe effect.
Under this hypothesis, embeddingsthat do not arise from final adjunction of mod-ifiers (henceforth canonical embeddings) shouldyield a positive integration cost as found by Gib-son (2000).To investigate this potential confound, theDundee corpus is partitioned into two parts.
First,the model described in this paper is used to anno-tate the Dundee corpus.
From this annotated cor-pus, all sentences are collected that contain canon-ical embeddings and lack modifier-induced em-beddings.12 This produces two corpora: one con-sisting entirely of canonical center-embeddingssuch as those used in self-paced reading exper-iments with findings of positive integration cost(e.g.
Gibson 2000), the other consisting of theremainder of the Dundee corpus, which containssentences with canonical embeddings but also in-cludes modifier-caused embeddings.The coefficient estimates for integration oper-ations (?F+L and +N) on each of these corporaare then calculated using the baseline describedabove.
To ensure embeddings are driving any ob-served effect rather than sentence wrap-up effects,the first and last words of each sentence are ex-cluded from both data sets.
Integration cost ismeasured by the amount of probability mass theparser allocates to ?F+L and +N operations, accu-10Each fixed effect that has an absolute t-value greater than10 when included in a random-intercepts only model is addedas a random slope by-subject.11The relative likelihood of the original model to the gap-sensitive model is 0.0009 (n = 151,331), which suggests theimprovement is significant.12Modifier-induced embeddings are found by looking forembeddings that arise from inference rules Ma-h in Section 3.Model coeff std err t-scoreCanonical -0.040 0.010 -4.05Other -0.017 0.004 -4.20Table 1: Fixed effect estimates for integration costwhen used to fit reading times over two partitionsof the Dundee corpus: one containing only canon-ical center embeddings and the other composed ofthe rest of the sentences in the corpus.mulated over each go-past region, and this cost isadded as a fixed effect and as a random slope bysubject to the mixed model described earlier.13The fixed effect estimate for cumulative inte-gration cost from fitting each corpus is shownin Table 1.
Application of Welch?s t-test showsthat the difference between the estimated distri-butions of these two parameters is highly signif-icant (p < 0.0001).14 The strong negative corre-lation of integration cost to reading times in thepurely canonical corpus suggests canonical (non-modifier) integrations contribute to the finding ofnegative integration cost.6 ConclusionThis paper has introduced an incremental parsercapable of using GCG dependencies to distinguishbetween surface syntactic embeddings and deepsyntactic embeddings.
This parser was shown toobtain a better fit to reading times than a surface-syntactic parser and was used to parse the Dundeeeye-tracking corpus in two partitions: one consist-ing of canonical embeddings that require deferreddependencies and the other consisting of sentencescontaining no center embeddings or center em-beddings arising from the attachment of clause-final modifiers, in which no dependencies are de-ferred.
Using linear mixed effects models, com-pletion (integration) of canonical center embed-dings was found to be significantly more nega-tively correlated with reading times than comple-tion of non-canonical embeddings.
These resultssuggest that the negative integration cost observedin eye-tracking studies is at least partially due todeep syntactic dependencies and not due to con-founds related to surface forms.13Integration cost is residualized off the baseline before be-ing centered and added as a fixed effect.14Integration cost is significant as a fixed effect (p = 0.001)in both partitions: canonical (n = 16,174 durations) andnon-canonical (n = 131,297 durations).44ReferencesSteven P. Abney and Mark Johnson.
1991.
Memoryrequirements and local ambiguities of parsing strate-gies.
J. Psycholinguistic Research, 20(3):233?250.R.
Harald Baayen, D. J. Davidson, and Douglas M.Bates.
2008.
Mixed-effects modeling with crossedrandom effects for subjects and items.
Journal ofMemory and Language, 59:390?412.Emmon Bach.
1981.
Discontinuous constituents ingeneralized categorial grammars.
Proceedings ofthe Annual Meeting of the Northeast Linguistic So-ciety (NELS), 11:1?12.Douglas Bates, Martin Maechler, and Ben Bolker,2011.
lme4: Linear mixed-effects models using S4classes.BNC Consortium.
2007.
The british national corpus.Stanley F. Chen and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical report, Harvard University.Vera Demberg and Frank Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109(2):193?210.W.
Nelson Francis and Henry Kucera.
1979.
Thebrown corpus: A standard corpus of present-dayedited american english.Edward Gibson.
1998.
Linguistic complexity: Local-ity of syntactic dependencies.
Cognition, 68(1):1?76.Edward Gibson.
2000.
The dependency locality the-ory: A distance-based theory of linguistic complex-ity.
In Image, language, brain: Papers from the firstmind articulation project symposium, pages 95?126,Cambridge, MA.
MIT Press.John Hale.
2001.
A probabilistic earley parser as apsycholinguistic model.
In Proceedings of the sec-ond meeting of the North American chapter of theAssociation for Computational Linguistics, pages159?166, Pittsburgh, PA.John Hale.
2003.
Grammar, Uncertainty and SentenceProcessing.
Ph.D. thesis, Cognitive Science, TheJohns Hopkins University.Philip N. Johnson-Laird.
1983.
Mental models: to-wards a cognitive science of language, inference,and consciousness.
Harvard University Press, Cam-bridge, MA, USA.Daniel Jurafsky.
1996.
A probabilistic model of lexicaland syntactic access and disambiguation.
CognitiveScience: A Multidisciplinary Journal, 20(2):137?194.Marcel Adam Just and Patricia A. Carpenter.
1992.
Acapacity theory of comprehension: Individual differ-ences in working memory.
Psychological Review,99:122?149.Fred Karlsson.
2007.
Constraints on multiple center-embedding of clauses.
Journal of Linguistics,43:365?392.Alan Kennedy, James Pynte, and Robin Hill.
2003.The Dundee corpus.
In Proceedings of the 12th Eu-ropean conference on eye movement.Walter Kintsch.
1988.
The role of knowledge in dis-course comprehension: A construction-integrationmodel.
Psychological review, 95(2):163?182.Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,Robert Kluender, and Maria Polinsky.
2010.
Cog-nitive and linguistic factors affecting subject/objectasymmetry: An eye-tracking study of pre-nominalrelative clauses in korean.
Language, 86(3):561.Richard L. Lewis and Shravan Vasishth.
2005.An activation-based model of sentence processingas skilled memory retrieval.
Cognitive Science,29(3):375?419.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Igor Mel?c?uk.
1988.
Dependency syntax: theory andpractice.
State University of NY Press, Albany.Luan Nguyen, Marten van Schijndel, and WilliamSchuler.
2012.
Accurate unbounded dependencyrecovery using generalized categorial grammars.
InProceedings of the 24th International Conference onComputational Linguistics (COLING ?12), Mumbai,India.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of the44th Annual Meeting of the Association for Compu-tational Linguistics (COLING/ACL?06).R Development Core Team, 2010.
R: A Language andEnvironment for Statistical Computing.
R Foun-dation for Statistical Computing, Vienna, Austria.ISBN 3-900051-07-0.William Schuler.
2009.
Parsing with a bounded stackusing a model-based right-corner transform.
In Pro-ceedings of NAACL/HLT 2009, NAACL ?09, pages344?352, Boulder, Colorado.
Association for Com-putational Linguistics.Damian G. Stephen and Daniel Mirman.
2010.
Inter-actions dominate the dynamics of visual cognition.Cognition, 115(1):154?165.Andreas Stolcke.
2002.
Srilm ?
an extensible languagemodeling toolkit.
In Seventh International Confer-ence on Spoken Language Processing.Marten van Schijndel and William Schuler.
2013.
Ananalysis of frequency- and recency-based processingcosts.
In Proceedings of NAACL-HLT 2013.
Associ-ation for Computational Linguistics.45Marten van Schijndel, Andy Exley, and WilliamSchuler.
2012.
Connectionist-inspired incrementalPCFG parsing.
In Proceedings of CMCL 2012.
As-sociation for Computational Linguistics.Marten van Schijndel, Andy Exley, and WilliamSchuler.
in press.
A model of language processingas hierarchic sequential prediction.
Topics in Cogni-tive Science.Stephen Wu, Asaf Bachrach, Carlos Cardenas, andWilliam Schuler.
2010.
Complexity metrics in anincremental right-corner parser.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics (ACL?10), pages 1189?1198.46
