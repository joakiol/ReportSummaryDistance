Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 75?80,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsTagging Opinion Phrases and their Targets in User Generated TextualReviewsNarendra GuptaAT&T Labs - Research, Inc.Florham Park, NJ 07932 - USAngupta@research.att.comAbstractWe discuss a tagging scheme to tag data fortraining information extraction models whichcan extract the features of a product/serviceand opinions about them from textual reviews,and which can be used across different do-mains with minimal adaptation.
A simple tag-ging scheme results in a large number of do-main dependent opinion phrases and impedesthe usefulness of the trained models across do-mains.
We show that by using minor mod-ifications to this simple tagging scheme thenumber of domain dependent opinion phrasesare reduced from 36% to 17%, which leads tomodels more useful across domains.1 IntroductionA large number of opinion-rich reviews about mostproducts and services are available on the web.These reviews are often summarized by star rat-ings to help consumers in making buying decisions.While such a summarization is very useful, oftenconsumers like to know about specific features of theproduct/service.
For example in the case of restau-rants consumers might want to know what peoplethink about their chicken dish.
There are many re-search papers on both supervised (Li et al 2010)and unsupervised(Liu et al 2012),(Hu and Liu,2004), (Popescu and Etzioni, 2005), (Baccianellaet al 2009) methods for extracting reviewer?s opin-ions and their targets (features of products/services)from textual reviews.
Unsupervised methods arepreferred as they can be used across domains, how-ever their performance is limited by the assump-tions they make about lexical and syntactic proper-ties of opinion and target phrases.
We would liketo use supervised methods to develop informationextraction models that can also be used across do-mains with minimum adaptation.
We hope to suc-ceed in our goal because: a) even though there aredomain specific opinion phrases, we believe a largeproportion of opinion phrases can be used acrossthe domains with the same semantic interpretation;b) target phrases mostly contain domain dependentwords, but have domain independent syntactic rela-tionships with opinion phrases.
Obviously for a do-main containing large number of domain dependentopinion phrases, our models will perform poorly andadditional domain adaptation will be necessary.In this paper we discuss a tagging scheme to man-ually tag the necessary training data.
In section2 we show that simply tagging opinion and targetphrases, forces a large number of opinion phrases tocontain domain dependent vocabulary.
This makesthem domain dependent, even when domain inde-pendent opinion words are used.
In section 3 wepropose a modification to the simple tagging schemeand show that this modification allows tagging ofopinion phrases without forcing them to contain do-main dependent vocabulary.
We also identify manylinguistic structures used to express opinions thatcannot be captured even with our modified taggingscheme.
In section 4 we experimentally show theimprovement in the coverage of tagged domain inde-pendent opinion phrases due to our proposed modifi-cation.
In section 5 we discuss the relationship withother work.
We conclude this paper in section 6 bysummarizing the contribution of this work.752 A Simple Tagging SchemeOur goal is to only extract author?s current opinionsby using the smallest possible representation.
Pastopinions or those of other agents are not of interest.As shown in Table 1, in this simple taggingscheme we tag opinions, their targets, and pronomi-nal references in each sentence without consideringthe review or the domain the sentence is part of1.Opinion phrases are further categorized to representtheir polarity and their domain dependence2.There are two relations in this scheme viz.Target(Opinionphrase, Target|Referencephrase), andReference(Referencephrase, Targetphrase).Finally, we tag only the contiguous non-overlapping spans in the sentences.Phrase Type Domain Dependent Tag SymbolOpinionPositiveNo PYes PDNegativeNo NYes NDNeutral Yes UDTarget Yes TPronomial Reference No RTable 1: Different types of phrases to be tagged.Figure 1: Examples tagged by the simple tagging scheme.Figure 13 shows examples of tagged sentencesusing the simple tagging scheme.
It illustrates: a)a sentence can have multiple ?Target?
relationshipsb) pronominal references can be used as targets; c)many opinion phrases can have the same target and1Once context independent information is extracted from in-dividual sentences, post processing (not discussed in this paper)can aggregate information (e.g.
resolve all identified pronomi-nal references) within the context of the entire review.2An opinion phrase is domain independent if its interpreta-tion remains unchanged across domains3Figures showing examples annotations are extracted fromBrat Annotation Tool (http://brat.nlplab.org/).
In Brat tags aredisplayed as square boxes and relations as directed arcs.vice versa; d) opinion phrases are not always adjec-tives and/or adverbs; e) in the sixth sentence ?his?opinion about chocolate is not tagged instead, au-thor?s opinion about the opinion holder is tagged; f)in the last sentence fragmented opinion phrase ?notrecommend for a large group?
is not tagged.Figure 2: Examples where the simple tagging scheme isnot discriminating enough.Figure 2 shows examples where our simple tag-ging scheme is not discriminating enough.
As a re-sult majority of the opinion phrases are tagged asdomain dependent.
Example 1, 2 show that the tag-ging scheme cannot express attributes of a target.Therefore, they are lumped with the opinion phrases,making them domain dependent.
In example 5 theopinion about ?wines they have?
is embedded in thetagged opinion phrase.
In example 6 the fact that?we do not love this place?
is not captured.
Example7 shows that our scheme can only tag one of the twotargets of a comparative opinion expression.
Exam-ple 8 shows a complex opinion expression involv-ing multiple agents, opinions, expectations, analo-gies and modalities.
To accurately represent opin-ions expressed in the infinitely many compositions,natural languages offer, a more complex representa-tion is required.
Instead of solving this knowledgerepresentation problem, we introduce two additionaltags and relations, and show that our modified tag-ging scheme is able to capture opinions expressedthrough some commonly used expressions.3 A Modified Tagging SchemeIn our modified tagging scheme, we add 2 moretags and relations.
We add an ?Embedded Target?76(symbol ET) tag to represent attributes of thetargets, embedded in the opinion phrases taggedby the simple tagging scheme.
These attributescould have any relationship e.g.
part-of, feature-ofand instance-of, with the target of the opinion.More specifically in the modified tagging schemewe break the opinion phrases as tagged in thesimple tagging scheme into opinion phrases and theembedded target phrases.We also add a ?Negation?
(symbol NO) tag to capture the negation of anopinion which often is located far from the opinionphrases (example 3 and 6 in Figure 2).
The cor-responding relations in our modified scheme areEmbeddedtarget(OpinionPhrase,EmbeddedTarget) andNegation(NegationPhrase,OpinionPhrase).Figure 3: Example sentences tagged with modified tag-ging scheme.Figure 3 shows the examples in Figure 2 taggedwith the modified tagging scheme.
From this tag-ging we can put together fragmented components ofopinion and target phrases (Table 2) using the rule:Target(Op, Tp)&Emb Target(Op,ETp) ?
Target(Op, Tp :ETp) i.e.
if Tp is tagged as target phrase of theopinion phrase Op and ETp is tagged as its em-bedded target phrase then Tp : ETp4 is the targetof the opinion phrase Op.
Similarly if a relationNegation(Np,Op) exists, the complete specifica-tion of the opinion is derived by adding the negationphrase Np to the opinion phrase Op .As can be seen in Table 2, the modified tag-ging scheme is able to capture the opinions andtheir targets more precisely than the simple taggingscheme.
In addition, opinion phrases become mostlydomain independent.
Still, there is some informa-tion loss.
For example in sentence 4 ?for?
rela-4The colon in this expression is intended to join specifica-tions of the target.Sentence Opinion phrase Target Phrase1good This place: foodgood This place: entertainment2 does not realize how poor The server: service3 not anymore: a great This: place to eat4great This: placeromantic This: evening5knowledgeable My server: winegreat they: wine6 have given up: love this placeTable 2: Tagged information in Figure 3.tionship between the two opinion phrases is ignored(?place is great for romantic evening?
), instead weextract ?This:place?
is ?great?
and ?This5:evening?is ?Romantic?.
This although not exact, captures theessence of the reviewer?s opinion without additionalcomplexity in the tagging scheme.
In the rest of thissection, we describe other natural language struc-tures used to express opinions and also show howthey are handled in our tagging scheme.3.1 Ambiguous TargetsIn many situations it is difficult to distinguish be-tween the target and the embedded target.
In Figure4 two possible tags on a sentence are shown.
In theFigure 4: Examples sentences showing ambiguity in tag-ging targets and embedded targets.first version, the neutral opinion about the ?discern-ing diners?
is tagged.
In the second version, domaindependent negative opinion about ?this restaurant?is tagged.
If the context of tagging i.e.
interest inopinions about the restaurants, was known, this am-biguity is resolved.
In our context free tagging, weresolve this ambiguity by preferring the subject ofthe sentence as the main target of the opinions.3.2 Conditional OpinionsOpinions are also expressed in conditional form andsometimes, like in example 1 and 2 in Figure 5, itis difficult to separate the opinion phrases from thetarget/embedded target phrases and the only choiceis to tag entire sentence/segement as domain depen-dent neutral.
Even though in the first sentence opin-ion about the loud music is expressed, and in the sec-5Anphora resoultion will bind ?This?
to the reviewed restau-rant.77ond sentence opinion about the food of the restau-rant is expressed, they cannot be tagged as such evenwith our modified scheme.
Examples 3 and 4 asFigure 5: Examples of conditional opinion phrases.shown in Figure 5 however, can be segmented intoopinions and their targets/embedded targets.
Theseexamples illustrate that when there are no negationsin the conditional opinions they typically can be seg-mented into opinion and target phrases.3.3 Opinion Referencing Other OpinionPhrasesFigure 6 shows examples where opinions aboutother opinions are expressed.
In the first example,the opinion ?most impressive?
reinforces other opin-ions; such reinforcement cannot be represented inour tagging scheme.
In the second example, how-ever, the pronoun ?it?
references the magazine?sopinion, which is ignored in our tagging scheme.Figure 6: Examples where opinion expressions referenceother opinion expressions.3.4 Implicit Target SwitchIn the first part of the sentence shown in Figure 7an opinion about ?this: steak?
is expressed and inthe second part an opinion about ?this: ambiance?is expressed.
Clearly if ?this?
refers to a steak, itcannot have ambiance.
It must be the ambiance ofthe restaurant serving the steak.
Our tagging schemedoes not capture this implicit target switching.Figure 7: Example of implicit target switching.4 Coverage experimentTable 3 shows the counts of domain dependent opin-ion phrases tagged on a small sample of data from 3different domains, using both simple and modifiedschemes.
The number of domain dependent opinionphrases in case of the modified tagging scheme is re-duced by more than half.
Even for the MP3 playerswith a large domain dependent vocabulary, 73% ofopinion phrases are tagged as domain independent.This will make models trained on different domainsuseful even for MP3 players.Domain Num.
SentencesNumber of Tagged Opinion PhrasesTotalDomain DependentSimple ModifiedRestaurant 68 101 31(30%) 13(13%)Hotels 147 111 39(35%) 15(14%)MP3 Plyr.
350 287 103(36%) 48(17%)Table 3: Comparison of simple and modified scheme.5 Relationship to other workKessler et al(2010)6 have tagged automobile data(JDPA Corpus) with sentiment expressions (ouropinion phrases) and mentions (our target and em-bedded target phrases).
JDPA representation is moreextensive than ours.
It explicitly represents many re-lationships among mentions and a number of mod-ifiers of sentiment expressions.
The strength of ourscheme however, is in the way we choose the targets.In JDAP, mentions are tagged as targets of theirmod-ifying sentiment expressions.
In our scheme we tagthe main object as the target of opinions.
For somecases both JDPA and our schemes result in equiv-alent representations, but for others we believe ourscheme results in a more accurate representation.As can be verified for Example 1 in Figure 2both schemes result in an equivalent representation.For example in Figure 8, on the other hand ourscheme represents the opinion expressions more ac-curately then JDPA.
This example contains an opin-ion about any good camera.
Therefore, the targetof the opinion in our scheme is ?good camera?
andnot ?camera?
by itself, and the opinion is ?musthave a great zoom?, ?zoom?
being embedded targetwe can drive Target(must have a great, good cam-era:zoom).
In JDPA this will be represented as Tar-get(good,camera), Target(must have a great,zoom),6Author is thankful to the reviewers of the paper to point outthis reference.78part-of(camera, zoom).
Notice that JDPA explicitlyrepresents that the ?camera is good?, which is nottrue, and is not represent in our scheme.Figure 8: Example where our scheme captures the opin-ions more accurately than JDPA.The tagged data by Hu and Liu (2004)(H-L data)is the another data that has opinions and their tar-gets labeled.
It has been used by many researchersto benchmark their work.
We randomly selected re-views from the H-L data and tagged them with ourmodified tagging scheme (Figure 9).
Several obser-vations can be made from Table 4, showing infor-mation tagged by our scheme and by the labels inthe H-L data.
First, not all opinion and targets aretagged in the H-L data.
Instead of tagging the opin-ion phrases directly, the H-L data relies on labeler?sassessments for polarity strengths of the opinion.
Inthe H-L data even the targets may or may not bepresent in the sentence (example 2).
Again the H-L data relies on the labeler?s assessment of what thetarget is.
Clearly in the H-L data the labeling is per-formed with a specific context in mind while ourscheme makes no such assumption.
The main rea-son for this difference is that Hu and Liu (2004) usedthis data only to test their un-supervised technique,while our motivation is to use the tagged data for su-pervised training of models that could be used acrossdomains.
With the contextual assumptions made inthe labeling, the models trained by using the H-Ldata will perform very poorly when used across do-mains.Modified Tagging Scheme H-L LabelOpinion Pol.
Target Pol.
Targetincredibly overpriced neg apple i-podnot(regret) pos the purchase 3 playerNot(any doubts) pos this playereasy pos software: to use 2 softwaremuch cheaper pos player 2 pricegood looking pos playerbeautiful pos blue back-lit screengood pos this?lack of a viewing hole for ..not(damaged/scratched) pos the facefast pos transfer ratesuck neg the stock ones?headphones -3 headphonewill out sell pos this playerTable 4: Side by side comparison of tagged informationwith our modified tagging scheme and H-L dataWiebe et al(2005) describe the MPQA taggingFigure 9: Tagging a review from Hu and Liu (2004) datausing the our modified tagging scheme.scheme for identifying private states of agents, in-cluding those of the author and any other agent re-ferred in the text.
The MPAQ tags direct subjectiveexpressions (DSE) e.g.
?faithful?
and ?criticized?,and expressive subjective elements (ESE) e.g.
?high-way robbery?
and ?illegitimate?, to identify the pri-vate states.
We only tag author?s opinions.
For ex-ample in ?
?The US fears a spill-over,?
said Xirao-Nima?
the MPQA will identify the private states of?US?
and of ?Xirao-Nima?.
We, however, will nottag this sentence since the author is not expressingany opinion.Opinions are part of an agent?s private state, butnot all private states are opinions.
For example in thesentence ?I am happy?
the author is describing hisprivate state and not an opinion.
In the MPQA theauthor?s private state will be identified by ?happy?but, in our tagging scheme this sentence will not betagged.
However, in the sentence ?I am happy withtheir service?
author is expressing an opinion about?their service?
and will be tagged in our scheme.Another difference between MPQA and ourscheme is that MPQA tags only the private states ofagents, causing some inconsistencies as illustratedby the following example.
In the sentence ?The U.S.is full of absurdities?, ?absurdities?
is tagged as aprivate state of the U.S. At the same time in sen-tence ?The report is full of absurdities?, ?absurdi-ties?
is tagged as a private state of the author, and79?the report?
is relegated to its target.
In our taggingscheme both ?the US?
and ?the report?
are consis-tently tagged as targets of the opinion phrase ?ab-surdities?.
Because of these differences we believethat the MPQA data is less suitable for opinion min-ing research.6 ConclusionWe discussed a tagging scheme to tag data for train-ing information extraction models to extract fromtextual reviews the features of a product/service andopinions about them, and which can be used acrossdomains with minimal adaptation.
We demonstratedthat a) by using a simple tagging scheme a large pro-portion of opinion phrases are tagged as domain de-pendent, defeating our goal to train models usableacross domains; b) even when a domain indepen-dent vocabulary is used, a more complex taggingscheme is needed to fully disambiguate opinion andtarget phrases.
Instead of addressing this complexrepresentation problem, we show that by introducingtwo additional tags the number of domain dependentopinion phrases is reduced from 36% to 17%.
Thiswill lead to information extraction models that per-form better when used across domains.7 AcknowledgmentsAuthor is thankful to Amanda Stent and to theanonymous reviewers for their comments and sug-gestions, which significantly contributed to improv-ing the quality of the publication.ReferencesBaccianella, Stefano, Andrea Esuli, and Fabrizio Se-bastiani.
2009.
Multi-facet rating of product re-views.
In Proceedings of the 31th European Con-ference on IR Research on Advances in Informa-tion Retrieval (ECIR 09).
pages 462?472.Hu, Minqing and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In Proceedings of theACM SIGKDD Conference on Knowledge Dis-covery and Data Mining (KDD).
pages 168?177.Kessler, Jason S., Miriam Eckert, Lyndsay Clark,and Nicolas Nicolov.
2010.
The icwsm 2010 jdpasentiment corpus for the automotive domain.
InProceedings of the 4th International AAAI Con-ference on Weblogs and Social Media Data Chal-lenge Workshop (ICWSM-DCW 2010).Li, Fangtao, Chao Han, Minlie Huang, XiaoyanZhu, Yingju Xia, Shu Zhang, and Hao Yu.
2010.Structure-aware review mining and summariza-tion.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (COL-ING 2010).
pages 653?661.Liu, Kang, Liheng Xu, and Jun Zhao.
2012.
Opin-ion target extraction using word-based translationmodel.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, EMNLP-CoNLL 2012. pages 1346?1356.Popescu, Ana-Maria and Oren Etzioni.
2005.
Ex-tracting product features and opinions from re-views.
In Proceedings of the Human LanguageTechnology Conference and the Conference onEmpirical Methods in Natural Language Process-ing (HLT/EMNLP).Wiebe, Janyce, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions andemotions in language.
Language Resources andEvaluation 39(2-3):165?210.80
