Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 1?9,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsGraph-based Clustering for Computational Linguistics: A SurveyZheng ChenThe Graduate CenterThe City University of New Yorkzchen1@gc.cuny.eduHeng JiQueens College and The Graduate CenterThe City University of New Yorkhengji@cs.qc.cuny.eduAbstractIn this survey we overview graph-based clus-tering and its applications in computationallinguistics.
We summarize graph-based clus-tering as a five-part story: hypothesis, model-ing, measure, algorithm and evaluation.
Wethen survey three typical NLP problems inwhich graph-based clustering approacheshave been successfully applied.
Finally, wecomment on the strengths and weaknesses ofgraph-based clustering and envision thatgraph-based clustering is a promising solu-tion for some emerging NLP problems.1 IntroductionIn the passing years, there has been a tremend-ous body of work on graph-based clustering,either done by theoreticians or practitioners.Theoreticians have been extensively investigat-ing cluster properties, quality measures and var-ious clustering algorithms by taking advantageof elegant mathematical structures built in graphtheory.
Practitioners have been investigating thegraph clustering algorithms for specific applica-tions and claiming their effectiveness by takingadvantage of the underlying structure or otherknown characteristics of the data.
Althoughgraph-based clustering has gained increasingattentions from Computational Linguistic (CL)community (especially through the series ofTextGraphs workshops), it is studied case bycase and as far as we know, we have not seenmuch work on comparative study of variousgraph-based clustering algorithms for certainNLP problems.
The major goal of this survey isto ?bridge?
the gap between theoretical aspectand practical aspect in graph-based clustering,especially for computational linguistics.From the theoretical aspect, we state that thefollowing five-part story describes the generalmethodology of graph-based clustering:(1) Hypothesis.
The hypothesis is that a graphcan be partitioned into densely connected sub-graphs that are sparsely connected to each other.
(2) Modeling.
It deals with the problem of trans-forming data into a graph or modeling the realapplication as a graph.
(3) Measure.
A quality measure is an objectivefunction that rates the quality of a clustering.
(4) Algorithm.
An algorithm is to exactly orapproximately optimize the quality measure.
(5) Evaluation.
Various metrics can be used toevaluate the performance of clustering by com-paring with a ?ground truth?
clustering.From the practical aspect, we focus on threetypical NLP applications, including coreferenceresolution, word clustering and word sense dis-ambiguation, in which graph-based clusteringapproaches have been successfully applied andachieved competitive performance.2 Graph-based Clustering MethodologyWe start with the basic clustering problem.
Let??
= {?
?1, ?
, ????}
be a set of data points, ??
=???????
???
,??=1,?,??
be the similarity matrix in whicheach element indicates the similarity ??????
?
0 be-tween two data points ????
and ????
.
A nice way torepresent the data is to construct a graph onwhich each vertex represents a data point andthe edge weight carries the similarity of twovertices.
The clustering problem in graph pers-pective is then formulated as partitioning thegraph into subgraphs such that the edges in thesame subgraph have high weights and the edgesbetween different subgraphs have low weights.In the next section, we define essential graphnotation to facilitate discussions in the rest ofthis survey.12.1 Graph NotationA graph is a triple G=(V,E,W)  where ??
={?
?1, ?
, ????}
is a set of vertices, E?V?V is a setof edges, and ??
= ???????
???
,??=1,?,??
is called adja-cency matrix in which each element indicates anon-negative weight ( ??????
?
0)  between twovertices ????
and ????
.In this survey we target at hard clusteringproblem which means we partition vertices ofthe graph into non-overlapping clusters, i.e., let??
= (?
?1, ?
,????)
be a partition of ??
such that(1) ????
?
?
for ??
?
{1, ?
,??}.
(2) ????
?
????
= ?
for ?
?, ??
?
{1, ?
,??}
and ??
?
??
(3) ?
?1 ???
????
= ?
?2.2 HypothesisThe hypothesis behind graph-based clusteringcan be stated in the following ways:(1) The graph consists of dense subgraphs suchthat a dense subgraph contains more well-connected internal edges connecting thevertices in the subgraph than cutting edgesconnecting the vertices across subgraphs.
(2) A random walk that visits a subgraph willlikely stay in the subgraph until many of itsvertices have been visited (Dongen, 2000).
(3) Among all shortest paths between all pairsof vertices, links between different densesubgraphs are likely to be in many shortestpaths (Dongen, 2000).2.3 ModelingModeling addresses the problem of transform-ing the problem into graph structure, specifical-ly, designating the meaning of vertices andedges in the graph, computing the edge weightsfor weighted graph, and constructing the graph.Luxburg (2006) stated three most common me-thods to construct a graph: ??
-neighborhoodgraph, ?
?-nearest neighbor graph, and fully con-nected graph.
Luxburg analyzed different beha-viors of the three graph construction methods,and stated that some graph-cluster algorithms(e.g., spectral clustering) can be quite sensitiveto the choice of graphs and parameters (??
and ??
).As a general recommendation, Luxburg sug-gested exploiting ?
?-nearest neighbor graph asthe first choice, which is less vulnerable to thechoices of parameters than other graphs.
Unfor-tunately, theoretical justifications on the choicesof graphs and parameters do not exist and as aresult, the problem has been ignored by practi-tioners.2.4 MeasureA measure is an objective function that rates thequality of a clustering, thus called quality meas-ure.
By optimizing the quality measure, we canobtain the ?optimal?
clustering.It is worth noting that quality measure shouldnot be confused with vertex similarity measurewhere it is used to compute edge weights.
Fur-thermore, we should distinguish quality meas-ure from evaluation measure which will be dis-cussed in section 2.6.
The main difference isthat cluster quality measure directly identifies aclustering that fulfills a desirable property whileevaluation measure rates the quality of a cluster-ing by comparing with a ground-truth clustering.We summarize various quality measures inTable 1, from the basic density measures (intra-cluster and inter-cluster), to cut-based measures(ratio cut, ncut, performance, expansion, con-ductance, bicriteria), then to the latest proposedmeasure modularity.
Each of the measures hasstrengths and weaknesses as commented in Ta-ble 1.
Optimizing each of the measures is NP-hard.
As a result, many efficient algorithms,which have been claimed to solve the optimalproblem with polynomial-time complexity,yield sub-optimal clustering.2.5  AlgorithmWe categorize graph clustering algorithms intotwo major classes: divisive and agglomerative(Table 2).
In the divisive clustering class, wecategorize algorithms into several subclasses,namely, cut-based, spectral clustering, multile-vel, random walks, shortest path.
Divisive clus-tering follows top-down style and recursivelysplits a graph into subgraphs.
In contrast, ag-glomerative clustering works bottom-up anditeratively merges singleton sets of vertices intosubgraphs.
The divisive and agglomerative al-gorithms are also called hierarchical since theyproduce multi-level clusterings, i.e., one cluster-ing follows the other by refining (divisive) orcoarsening (agglomerative).
Most graph cluster-ing algorithms ever proposed are divisive.
Welist the quality measure and the running com-plexity for each algorithm in Table 2.2Measures Commentsintra-cluster densityinter-cluster density?
Maximizing intra-cluster density is equivalent to minimizing inter-clusterdensity and vice versa?
Drawback: both favor cutting small sets of isolated vertices in the graph(Shi and Malik, 2000)ratio cut (Hagan and Kahng,1992)ncut (Shi and Malik, 2000)?
Ratio cut is suitable for unweighted graph, and ncut is a better choice forweighted graph?
Overcome the drawback of intra-cluster density or inter-cluster density?
Drawback: both favor clusters with equal sizeperformance (Dongen, 2000;Brandes et al, 2003)?
Performance takes both intra-cluster density and inter-cluster densityinto considerations simultaneouslyexpansion, conductance,bicriteria(Kannan et al, 2000)?
Expansion is suitable for unweighted graph, and conductance is a betterchoice for weighted graph?
Both expansion and conductance impose quality within clusters, but notinter-cluster quality; bicriteria takes both into considerationsmodularity (Newman andGirvan,2004)?
Evaluates the quality of clustering with respect to a randomized graph?
Drawbacks: (1) It requires global knowledge of the graph?s topology,i.e., the number of edges.
Clauset (2005) proposed an improved measureLocal Modularity.
(2) Resolution limit problem: it fails to identify clus-ters smaller than a certain scale.
Ruan and Zhang (2008) proposed animproved measure HQcut.
(3) It fails to distinguish good from bad clus-tering between different graphs with the same modularity value.
Chen etal.
(2009) proposed an improved measure Max-Min ModularityTable 1.
Summary of Quality MeasuresCategory Algorithms optimizedmeasurerunningcomplexitydivisive cut-based Kernighan-Lin algorithm(Kernighan and Lin, 1970)intercluster ??(|?
?|3)cut-clustering algorithm(Flake et al, 2003)bicriteria ??(|?
?|)spectral unnormalized spectral clustering(Luxburg, 2006)ratiocut ??(|??||?
?|)normalized spectral clustering I(Luxburg, 2006; Shi and Malik, 2000)ncut ??(|??||?
?|)normalized spectral clustering II(Luxburg, 2006; Ng, 2002)ncut ??(|??||?
?|)iterative conductance cutting (ICC)(Kannan et al,2000)conductance ??(|??||?
?|)geometric MST clustering (GMC)(Brandes et al, 2007)pluggable(anyquality measure)??(|??||?
?|)modularity oriented(White and Smyth,2005)modularity ??(|??||?
?|)multilevel multilevel recursive bisection(Karypis and Kumar, 1999)intercluster ??(|??|????????
)multilevel ?
?-way partitioning(Karypis and Kumar, 1999)intercluster ??(|?
?|+ ??????????
)random Markov Clustering Algorithm (MCL)(Dongen, 2000)performance ??(??2|?
?|)shortestpathbetweenness(Girvan and Newman, 2003)modularity ??(|??||?
?|2)information centrality(Fortunato et al, 2004)modularity ??(|??||?
?|3)agglomerative modularity oriented(Newman, 2004)modularity ??(|??||?
?|)Table 2.
Summary of Graph-based Clustering Algorithms (|?
?|: the number of vertices, |?
?|: thenumber of edges, ??
: the number of clusters, ??
: the number of resources allocated for each vertex)3The first set of algorithms (cut-based) is asso-ciated with max-flow min-cut theorem (Ford andFulkerson, 1956) which states that ?the value ofthe maximum flow is equal to the cost of theminimum cut?.
One of the earliest algorithm,Kernighan-Lin algorithm (Kernighan and Lin,1970) splits the graph by performing recursivebisection (split into two parts at a time), aimingto minimize inter-cluster density (cut size).
Thehigh complexity of the algorithm ( ??(|?
?|3)makes it less competitive in real applications.Flake et al (2003) proposed a cut-clustering al-gorithm which optimizes the bicriterion measureand the complexity is proportional to the numberof clusters ??
using a heuristic, thus the algorithmis competitive in practice.The second set of algorithms is based on spec-tral graph theory with Laplacian matrix as themathematical tool.
The connection between clus-tering and spectrum of Laplacian matrix (??)
bas-ically lies in the following important proposition:the multiplicity ??
of the eigenvalue 0 of ??
equalsto the number of connected components in thegraph.
Luxburg (2006) and Abney (2007) pre-sented a comprehensive tutorial on spectral clus-tering.
Luxburg (2006) discussed three forms ofLaplacian matrices (one unnormalized form andtwo normalized forms) and their three corres-ponding spectral clustering algorithms (unnorma-lized, normalized I and normalized II).
Unnorma-lized clustering aims to optimize ratiocut meas-ure while normalized clustering aims to optimizencut measure (Shi and Malik, 2000), thus spec-tral clustering actually relates with cut-basedclustering.
The success of spectral clustering ismainly based on the fact that it does not makestrong assumptions on the form of the clustersand can solve very general problems like intert-wined spirals which k-means clustering handlesmuch worse.
Unfortunately, spectral clusteringcould be unstable under different choices ofgraphs and parameters as mentioned in section2.3.
Luxburg et al (2005) compared unnorma-lized clustering with normalized version andproved that normalized version always convergesto a sensible limit clustering while for unnorma-lized case the same only holds under strong addi-tional assumptions which are not always satisfied.The running complexity of spectral clusteringequals to the complexity of computing the eigen-vectors of Laplacian matrix which is ??(|?
?|3) .However, when the graph is sparse, the complex-ity is reduced to ??(|??||?
?|) by applying efficientLanczos algorithm.The third set of algorithms is based on multi-level graph partitioning paradigm (Karypis andKumar, 1999) which consists of three phases:coarsening phase, initial partitioning phase andrefinement phase.
Two approaches have beendeveloped in this category, one is multilevel re-cursive bisection which recursively splits intotwo parts by performing multilevel paradigmwith complexity of ??(|??|????????)
; the other ismultilevel ??
-way partitioning which performscoarsening and refinement only once and directlypartitions the graph into ??
clusters with com-plexity of ??(|?
?| + ??????????).
The latter approachis superior to the former one for less runningcomplexity and comparable (sometimes better)clustering quality.The fourth set of algorithms is based on thesecond interpretation of the hypothesis in section2.2, i.e., a random walk is likely to visit manyvertices in a cluster before moving to the othercluster.
An outstanding approach in this categoryis presented in Dogen (2000), named Markovclustering algorithm (MCL).
The algorithm itera-tively applies two operators (expansion and infla-tion) by matrix computation until convergence.Expansion operator simulates spreading of ran-dom walks and inflation models demotion of in-ter-cluster walks; the sequence matrix computa-tion results in eliminating inter-cluster interac-tions and leaving only intra-cluster components.The complexity of MCL is ??(??2|?
?|) where ??
isthe number of resources allocated for each vertex.A key point of random walk is that it is actuallylinked to spectral clustering (Luxburg, 2006),e.g., ncut can be expressed in terms of transitionprobabilities and optimizing ncut can beachieved by computing the stationary distribu-tion of a random walk in the graph.The final set of algorithms in divisive categoryis based on the third interpretation of the hypo-thesis in section 2.2, i.e., the links between clus-ters are likely to be in the shortest paths.
Girvanand Newman (2003) proposed the concept ofedge betweenness which is the number of short-est paths connecting any pair of vertices that passthrough the edge.
Their algorithm iteratively re-moves one of the edges with the highest bet-weenness.
The complexity of the algorithm is??(|??||??|2).
Instead of betweenness, Fortunato etal.
(2004) used information centrality for eachedge and stated that it performs better than bet-weenness but with a higher complexity of??(|??||?
?|3).The agglomerative category contains muchfewer algorithms.
Newman (2004) proposed an4algorithm that starts each vertex as singletons,and then iteratively merges clusters together inpairs, choosing the join that results in the greatestincrease (or smallest decrease) in modularityscore.
The algorithm converges if there is onlycluster left in the graph, then from the clusteringhierarchy, we choose the clustering with maxi-mum modularity.
The complexity of the algo-rithm is ??(|??||?
?|).The algorithms we surveyed in this section areby no means comprehensive as the field is long-standing and still evolving rapidly.
We also referreaders to other informative references, e.g.,Schaeffer (2007), Brandes et al (2007) andNewman (2004).A natural question arises: ?which algorithmshould we choose??
A general answer to thisquestion is that no algorithm is a panacea.
First,as we mentioned earlier, a clustering algorithm isusually proposed to optimize some quality meas-ure, therefore, it is not fair to compare an algo-rithm that favors one measure with the other onethat favors some other measure.
Second, there isnot a perfect measure that captures the full cha-racteristics of cluster structures; therefore a per-fect algorithm does not exist.
Third, there is nodefinition for so called ?best clustering?.
The?best?
depends on applications, data characteris-tics, and granularity.2.6 EvaluationWe discussed various quality measures in section2.4, however, a clustering optimizing somequality measure does not necessarily translateinto effectiveness in real applications with re-spect to the ground truth clustering and thus anevaluation measure plays the role of evaluatinghow well the clustering matches the gold stan-dard.
Two questions arise: (1) what constraints(properties, criteria) should an ideal evaluationmeasure satisfy?
(2) Do the evaluation measuresever proposed satisfy the constraints?For the first question, there have been severalattempts on it: Dom (2001) developed a parame-tric technique for describing the quality of a clus-tering and proposed five ?desirable properties?based on the parameters; Meila (2003) listed 12properties associated with the proposed entropymeasure; Amigo et al (2008) proposed four con-straints including homogeneity, completeness,rag bag, and cluster size vs. quantity.
A parallelcomparison shows that the four constraints pro-posed by Amigo et al (2008) have advantagesover the constraints proposed in the other twopapers, for one reason, the four constraints candescribe all the important constraints in Dom(2001) and Meila (2003), but the reverse doesnot hold; for the other reason, the four con-straints can be formally verified for each evalua-tion measure, but it is not true for the constraintsin Dom (2001).Table 3 lists the evaluation measures ever pro-posed (including those discussed in Amigo et al,2008 and some other measures known for corefe-rence resolution).
To answer the second questionproposed in this section, we conclude the find-ings in Amigo et al (2008) plus our new findingsabout MUC and CEAF as follows: (1) all themeasures except B-Cubed fail the rag bag con-straint and only B-Cubed measure can satisfy allthe four constraints; (2) two entropy based meas-ures (VI and V) and MUC only fail the rag bagconstraint; (3) all the measures in set mappingcategory fail completeness constraint (4) all themeasures in pair counting category fail clustersize vs. quantity constraint; (5) CEAF, unfortu-nately, fails homogeneity, completeness, rag bagconstraints.Category Evaluation Measuresset mapping  purity, inverse purity, F-measurepair counting rand index, Jaccard Coefficient,Folks and Mallows FMentropy entropy, mutual information, VI,Veditingdistanceediting distancecoreferenceresolutionMUC (Vilain et al,1995),B-Cubed (Bagga and Baldwin,1998), CEAF (Luo, 2005)Table 3.
Summary of Evaluation Measures3 Applying Graph Clustering to NLPA variety of structures in NLP can be naturallyrepresented as graphs, e.g., co-occurrence graphs,coreference graphs, word/sentence/ documentgraphs.
In recent years, there have been an in-creasing amount of interests in applying graph-based clustering to some NLP problems, e.g.,document clustering (Zhong and Ghosh, 2004),summarization (Zha, 2002), coreference resolu-tion (Nicolae and Nicolae, 2006), word sensedisambiguation (Dorow and Widdows, 2003;V?ronis, 2004; Agirre et al, 2007), word cluster-ing (Matsuo et al, 2006; Biemann, 2006).
Manyauthors chose one or two their favorite graphclustering algorithms and claimed the effective-ness by comparing with supervised algorithms(which need expensive annotations) or other non-5graph clustering algorithms.
As far as we know,there is not much work on the comparative studyof various graph-based clustering algorithms forcertain NLP problems.
As mentioned at the endof section 2.5, there is not a graph clustering al-gorithm that is effective for all applications.However, it is interesting to find out, for a spe-cific NLP problem, if graph clustering methodscan be applied, (1) how the parameters in thegraph model affects the performance?
(2) Doesthe NLP problem favor some quality measureand some graph clustering algorithm rather thanthe others?
Unfortunately, this survey neitherprovides answers for these questions; instead, weoverview a few NLP case studies in which somegraph-based clustering methods have been suc-cessfully applied.3.1 Coreference ResolutionCoreference resolution is typically defined as theproblem of partitioning a set of mentions intoentities.
An entity is an object or a set of objectsin the real world such as person, organization,facility, while a mention is a textual reference toan entity.
The approaches to solving coreferenceresolution have shifted from earlier linguistics-based (rely on domain knowledge and hand-crafted rules) to machine-learning based ap-proaches.
Elango (2005) and Chen (2010) pre-sented a comprehensive survey on this topic.
Oneof the most prevalent approaches for coreferenceresolution is to follow a two-step procedure: (1) aclassification step that computes how likely onemention corefers with the other and (2) aclustering step that groups the mentions intoclusters such that all mentions in a cluster referto the same entity.
In the past years, NLPresearchers have explored and enriched thismethodogy from various directions (either inclassification or clustering step).
Unfortunately,most of the proposed clustering algorithms, e.g.,closest-first clustering (Soon et al, 2001), best-first clustering (Ng and Cardie, 2002), sufferfrom a drawback: an instant decision is made (ingreedy style) when considering two mentions arecoreferent or not, therefore, the algorithm makesno attempt to search through the space of allpossible clusterings, which results in a sub-optimal clustering (Luo et al, 2004).
Variousapproaches have been proposed to alleviate thisproblem, of which graph clustering methodologyis one of the most promising solutions.The problem of coreference resolution can bemodeled as a graph such that the vertexrepresents a mention, and the edge weight carriesthe coreference likelihood between two mentions.Nicolae and Nicolae (2006) proposed a newquality measure named BESTCUT which is tooptimize the sum of ?correctly?
placed verticesin the graph.
The BESTCUT algorithm works byperforming recursive bisection (similar to Ker-nighan-Lin algorithm) and in each iteration, itsearches the best cut that leads to partition intohalves.
They compared BESTCUT algorithmwith (Luo et al, 2004)?s Belltree and (Ng andCardie, 2002)?s Link-Best algorithm and showedthat using ground-truth entities, BESTCUT out-performs the other two with statistical signific-ance (4.8% improvement over Belltree and Link-Best algorithm in ECM F-measure).
Nevertheless,we believe that the BESTCUT algorithm is notthe only choice and the running complexity ofBESTCUT,??(|??||?
?| + |??|2??????|?
?|), is not com-petitive, thus could be improved by other graphclustering algorithms.Chen and Ji (2009a) applied normalized spec-tral algorithm to conduct event coreference reso-lution: partitioning a set of mentions into events.An event is a specific occurrence involving par-ticipants.
An event mention is a textual referenceto an event which includes a distinguished trig-ger (the word that most clearly expresses anevent occurs) and involving arguments (enti-ties/temporal expressions that play certain rolesin the event).
A graph is similarly constructed asin entity coreference resolution except that it in-volves quite different feature engineering (mostfeatures are related with event trigger and argu-ments).
The graph clustering approach yieldscompetitive results by comparing with an agglo-merative clustering algorithm proposed in (Chenet al, 2009b), unfortunately, a scientific compar-ison among the algorithms remains unexplored.3.2 Word ClusteringWord clustering is a problem defined as cluster-ing a set of words (e.g., nouns, verbs) into groupsso that similar words are in the same cluster.Word clustering is a major technique that canbenefit many NLP tasks, e.g., thesaurus construc-tion, text classification, and word sense disam-biguation.
Word clustering can be solved by fol-lowing a two-step procedure: (1) classificationstep by representing each word as a feature vec-tor and computing the similarity of two words; (2)clustering step which applies some clusteringalgorithm, e.g., single-link clustering, complete-link clustering, average-link clustering, such thatsimilar words are grouped together.Matsuo et al (2006) presented a graph cluster-6ing algorithm for word clustering based on wordsimilarity measures by web counts.
A word co-occurrence graph is constructed in which the ver-tex represents a word, and the edge weight iscomputed by applying some similarity measure(e.g., PMI, ?2) on a co-occurrence matrix, whichis the result of querying a pair of words to asearch engine.
Then an agglomerative graphclustering algorithm (Newman, 2004), which issurveyed in section 2.5, is applied.
They showedthat the similarity measure  ?2  performs betterthan PMI, for one reason, PMI performs worsewhen a word group contains rare or frequentwords, for the other reason, PMI is sensitive toweb output inconsistency, e.g., the web count of?
?1 is below the web count of ??1???????
?2 in ex-treme case.
They also showed that their graphclustering algorithm outperforms average-linkagglomerative clustering by almost 32% using ?2similarity measure.
The concern of their ap-proach is the running complexity for constructingco-occurrence matrix, i.e., for ??
words, ??(?
?2)queries are required which is intractable for alarge graph.Ichioka and Fukumoto (2008) applied similarapproach as Matsuo et al (2006) for JapaneseOnomatopoetic word clustering, and showed thatthe approach outperforms ?
?-means clustering by16.2%.3.3 Word Sense Disambiguation (WSD)Word sense disambiguation is the problem ofidentifying which sense of a word (meaning) isconveyed in the context of a sentence, when theword is polysemic.
In contrast to supervisedWSD which relies on pre-defined list of sensesfrom dictionaries, unsupervised WSD inducesword senses directly from the corpus.
Amongthose unsupervised WSD algorithms, graph-based clustering algorithms have been foundcompetitive with supervised methods, and inmany cases outperform most vector-based clus-tering methods.Dorow and Widdows (2003) built a co-occurrence graph in which each node representsa noun and two nodes have an edge betweenthem if they co-occur more than a given thre-shold.
They then applied Markov Clustering al-gorithm (MCL) which is surveyed in section 2.5,but cleverly circumvent the problem of choosingthe right parameters.
Their algorithm not onlyrecognizes senses of polysemic words, but alsoprovides high-level readable cluster name foreach sense.
Unfortunately, they neither discussedfurther how to identify the sense of a word in agiven context, nor compared their algorithm withother algorithms by conducting experiments.V?ronis (2004) proposed a graph based modelnamed HyperLex based on the small-world prop-erties of co-occurrence graphs.
Detecting the dif-ferent senses (uses) of a word reduces to isolat-ing the high-density components (hubs) in theco-occurrence graph.
Those hubs are then used toperform WSD.
To obtain the hubs, HyperLexfinds the vertex with highest relative frequencyin the graph at each iteration and if it meets somecriteria, it is selected as a hub.
Agirre (2007)proposed another method based on PageRank forfinding hubs.
HyperLex can detect low-frequencysenses (as low as 1%) and most importantly, itoffers an excellent precision (97% compared to73% for baseline).
Agirre (2007) further con-ducted extensive experiments by comparing thetwo graph based models (HyperLex and Page-Rank) with other supervised and non-supervisedgraph methods and concluded that graph basedmethods perform close to supervised systems inthe lexical sample task and yield the second-bestWSD systems for the Senseval-3 all-words task.4 ConclusionsIn this survey, we organize the sparse relatedliterature of graph clustering into a structuredpresentation and summarize the topic as a fivepart story, namely, hypothesis, modeling, meas-ure, algorithm, and evaluation.
The hypothesisserves as a basis for the whole graph clusteringmethodology, quality measures and graph clus-tering algorithms construct the backbone of themethodology, modeling acts as the interface be-tween the real application and the methodology,and evaluation deals with utility.
We also surveyseveral typical NLP problems, in which graph-based clustering approaches have been success-fully applied.We have the following final comments on thestrengths and weaknesses of graph clusteringapproaches:(1) Graph is an elegant data structure that canmodel many real applications with solid ma-thematical foundations including spectraltheory, Markov stochastic process.
(2) Unlike many other clustering algorithmswhich act greedily towards the final clusteringand thus may miss the optimal clustering,graph clustering transforms the clusteringproblem into optimizing some quality meas-ure.
Unfortunately, those optimization prob-lems are NP-Hard, thus, all proposed graph7clustering algorithms only approximatelyyield ?optimal?
clustering.
(3) Graph clustering algorithms have been criti-cized for low speed when working on largescale graph (with millions of vertices).
Thismay not be true since new graph clusteringalgorithms have been proposed, e.g., the mul-tilevel graph clustering algorithm (Karypisand Kumar, 1999) can partition a graph withone million vertices into 256 clusters in a fewseconds on current generation workstationsand PCs.
Nevertheless, scalability problem ofgraph clustering algorithm still needs to beexplored which is becoming more importantin social network study.We envision that graph clustering methods canlead to promising solutions in the followingemerging NLP problems:(1) Detection of new entity types, relation typesand event types (IE area).
For example, theeight event types defined in the ACE 1(2) Web people search (IR area).
The main issuein web people search is the ambiguity of theperson name.
Thus by extracting attributes(e.g., attended schools, spouse, children,friends) from returned web pages, construct-ing person graphs (involving those attributes)and applying graph clustering, we are opti-mistic to achieve a better person search en-gine.pro-gram may not be enough for wider usage andmore event types can be induced by graphclustering on verbs.AcknowledgmentsThis work was supported by the U.S. NationalScience Foundation Faculty Early Career Devel-opment (CAREER) Award under Grant IIS-0953149, the U.S. Army Research Laboratoryunder Cooperative Agreement NumberW911NF-09-2-0053, Google, Inc., CUNY Re-search Enhancement Program, Faculty Publica-tion Program and GRTI Program.
The views andconclusions contained in this document are thoseof the authors and should not be interpreted asrepresenting the official policies, either ex-pressed or implied, of the Army Research Labor-atory or the U.S. Government.
The U.S. Gov-ernment is authorized to reproduce and distributereprints for Government purposes notwithstand-ing any copyright notation here on.1 http://www.nist.gov/speech/tests/ace/ReferencesA.
Bagga and B. Baldwin.1998.
Algorithms for scor-ing coreference chains.
Proc.
The First Interna-tional Conference on Language Resources andEvaluation Workshop on Linguistics Coreference.A.
Clauset.
2005.
Finding local community structurein networks.
Physical Review E, 72:026132.B.
Dom.
2001.
An information-theoretic externalcluster-validity measure.
IBM Research Report.B.
Dorow, D. Widdows.
2003.
Discovering corpus-specific word-senses.
In Proc.
EACL.B.
W. Kernighan and S. Lin.1970.
An efficient heuris-tic procedur for partitioning graphs.
Bell Syst.Techn.
J.,Vol.
49, No.
2, pp.
291?307.C.
Biemann.
2006.
Chinese Whispers - an Efficient-Graph Clustering Algorithm and its Application toNatural Language Processing Problems.
In Proc.
ofthe HLT-NAACL-06 Workshop on Textgraphs-06.C.
Nicolae and G. Nicolae.
2006.
Bestcut: A graphalgorithm for coreference resolution.
In EMNLP,pages 275?283, Sydney, Australia.E.
Agirre, D. Martinez, O.L.
de Lacalle and A.Soroa.2007.
Two graph-based algorithms for state-of-the-art WSD.
In Proc.
EMNLP.E.
Amigo,  J. Gonzalo,  J. Artiles  and F. Verdejo.2008.
A comparison of extrinsic clustering evalua-tion metrics based on formal constraints.
Informa-tion Retrieval.E.
Terra and C. L. A. Clarke.
Frequency Estimates forStatistical Word Similarity Measures.
In Proc.HLT/NAACL 2003.G.
Karypis and V. Kumar.
1999.
Multilevel algo-rithms for multiconstraint graph partitioning.
inProceedings of the 36th ACM/IEEE conference onDesign automation conference, (New Orleans,Louisiana), pp.
343 ?
348.G.
W. Flake, R. E. Tarjan and K. Tsioutsiouliklis.2003.
Graph clustering and minimum cut trees.
In-ternet Mathematics, 1(4):385?408.H.
Zha.2002.
Generic summarization and keyphraseextraction using mutual reinforcement principleand sentence clustering.
In Proc.
of SIGIR2002, pp.113-120.J.
Chen, O. R. Za?ane, R. Goebel.
2009.
DetectingCommunities in Social Networks Using Max-MinModularity.
SDM 2009: 978-989.J.
Ruan and W. Zhang.2008.
Identifying networkcommunities with a high resolution.
Physical Re-view E, 77:016104.J.
Shi and J. Malik.
2000.
Normalized Cuts and ImageSegmentation.
IEEE Trans.
Pattern Analysis andMachine Intelligence, vol.
22, no.
8, pp.
888-905.8J.
V?ronis.
2004.
HyperLex: Lexical Cartography forInformation Retrieval.
Computer Speech & Lan-guage 18(3).K.
Ichioka  and F. Fukumoto.
2008.
Graph-basedclustering for semantic classification of onomato-poetic words.
In Proc.
of the 3rd Textgraphs Work-shop on Graph-based Algorithms for Natural Lan-guage Processing.L.
Hagen and A.
B. Kahng.
1992.
New spectral me-thods for ratio cut partitioning and clustering.
IEEETransactions Computer-Aided Design, Santa ClaraCA, 422-427.L.
R. Ford, D. R. Fulkerson.
1956.
Maximal flowthrough a network.
Canadian Journal of Mathe-matics 8: 399?404.M.
E. J. Newman.
2004.
Detecting community struc-ture in networks.
Eur.
Phys.
J.
B, 38, 321?330.M.
E. J. Newman.
2004.
Fast algorithm for detectingcommunity structure in networks.
Phys Rev E. 69,2004.M.
E. J. Newman and M. Girvan.
2004.
Finding andevaluating community structure in networks.
Phys.Rev.
E 69,026113.M.
Girvan and M. E. J. Newman.
2002.
Communitystructure in social and biological networks.
Proc.Natl.
Acad.
Sci.
USA 99, 7821-7826.M.
Meila.
2003.
Comparing clusterings.
In Proceed-ings of COLT03.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly  and L.Hirschman.1995.
A model-theoretic coreferencescoring scheme.
In Proceedings of the Sixth Mes-sage Understanding Conference (MUC-6).P.
Elango.
2005.
Coreference Resolution: A Survey.Technical Report, University of Wisconsin Madi-son.R.
Kannan, S. Vempala, and A. Vetta.
2000.
On clus-terings:good, bad and spectral.
In Proceedings ofthe 41st Annual Symposium on Foundations ofComputer Science.S.
Abney.
2007.
Semi-supervised Learning for Com-putational Linguistics, Chapman and Hall.S.
E. Schaeffer.
2007.
Graph clustering.
ComputerScience Review, 1(1):27?64.S.
Fortunato, V. Latora, and M. Marchiori.
2004.
AMethod to Find Community Structures Based onInformation Centrality.
Phys Rev E.70, 056104.S.
van Dongen.
2000.
Graph Clustering by Flow Si-mulation.
PhD thesis, University of Utrecht.S.
White and P. Smyth.
2005.
A spectral clusteringapproach to finding communities in graphs.
InSIAM International Conference on Data Mining.U.
Brandes, M. Gaertler, and D. Wagner.
2003.
Expe-riments on graph clustering algorithms.
Proc.
11thEuropean Symp.
Algorithms, LNCS 2832:568-579.U.
Brandes, M. Gaertler, and D.Wagner.
2007.
Engi-neering graph clustering: Models and experimentalevaluation.
J. Exp.
Algorithmics, 12:1.1.U.
Luxburg, O. Bousquet, M. Belkin.
2005.
Limits ofspectral clustering.
In L. K. Saul, Y. Weiss and L.Bottou (Eds.
), Advances in neural informationprocessing systems 17.
Cambridge, MA: MIT Press.U.
Luxburg.2006.
A tutorial on spectral clustering.Technical Report 149, Max Plank Institute for Bio-logical Cybernetics.V.
Ng and C. Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
In Proc.Of the ACL, pages 104?111.W.
M. Soon, H. T. Ng and D. Lim.2001.
A machinelearning approach to coreference resolution ofnoun phrases.
Computational Linguistics,27(4):521?544.X.
Luo.
2005.
On coreference resolution performancemetrics.
Proc.
of HLT-EMNLP.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla and S.Roukos.
2004.
A mention-synchronous coreferenceresolution algorithm based on the Bell Tree.
InProc.
of ACL-04, pp.136?143.Y.
Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka.2006.
Graph-based word clustering using websearch engine.
In Proc.
of EMNLP 2006.Z.
Chen and H. Ji.
2009a.
Graph-based Event Corefe-rence Resolution.
In Proc.
ACL-IJCNLP 2009workshop on TextGraphs-4: Graph-based Methodsfor Natural Language Processing.Z.
Chen, H. Ji, R. Haralick.
2009b.
A Pairwise Core-ference Model, Feature Impact and Evaluation forEvent Coreference Resolution.
In Proc.
RANLP2009 workshop on Events in Emerging Text Types.Z.
Chen.
2010.
Graph-based Clustering and its Appli-cation in Coreference Resolution.
Technical Report,the Graduate Center, the City University of NewYork.9
