Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 202?210,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsTranslation Memory Retrieval MethodsMichael BloodgoodCenter for Advanced Study of LanguageUniversity of MarylandCollege Park, MD 20742 USAmeb@umd.eduBenjamin StraussCenter for Advanced Study of LanguageUniversity of MarylandCollege Park, MD 20742 USAbstrauss@umd.eduAbstractTranslation Memory (TM) systems areone of the most widely used translationtechnologies.
An important part of TMsystems is the matching algorithm that de-termines what translations get retrievedfrom the bank of available translationsto assist the human translator.
Althoughdetailed accounts of the matching algo-rithms used in commercial systems can?tbe found in the literature, it is widelybelieved that edit distance algorithms areused.
This paper investigates and eval-uates the use of several matching algo-rithms, including the edit distance algo-rithm that is believed to be at the heartof most modern commercial TM systems.This paper presents results showing howwell various matching algorithms corre-late with human judgments of helpfulness(collected via crowdsourcing with Ama-zon?s Mechanical Turk).
A new algorithmbased on weighted n-gram precision thatcan be adjusted for translator length pref-erences consistently returns translationsjudged to be most helpful by translators formultiple domains and language pairs.1 IntroductionThe most widely used computer-assisted transla-tion (CAT) tool for professional translation of spe-cialized text is translation memory (TM) technol-ogy (Christensen and Schjoldager, 2010).
TMconsists of a database of previously translated ma-terial, referred to as the TM vault or the TM bank(TMB in the rest of this paper).
When a trans-lator is translating a new sentence, the TMB isconsulted to see if a similar sentence has alreadybeen translated and if so, the most similar pre-vious translation is retrieved from the bank tohelp the translator.
The main conceptions of TMtechnology occurred in the late 1970s and early1980s (Arthern, 1978; Kay, 1980; Melby and oth-ers, 1981).
TM has been widely used since thelate 1990s and continues to be widely used to-day (Bowker and Barlow, 2008; Christensen andSchjoldager, 2010; Garcia, 2007; Somers, 2003).There are a lot of factors that determine howhelpful TM technology will be in practice.
Someof these include: quality of the interface, speed ofthe back-end database lookups, speed of networkconnectivity for distributed setups, and the com-fort of the translator with using the technology.A fundamentally important factor that determineshow helpful TM technology will be in practice ishow well the TM bank of previously translatedmaterials matches up with the workload materialsto be translated.
It is necessary that there be a highlevel of match for the TM technology to be mosthelpful.
However, having a high level of match isnot sufficient.
One also needs a successful methodfor retrieving the useful translations from the (po-tentially large) TM bank.TM similarity metrics are used for both evalu-ating the expected helpfulness of previous transla-tions for new workload translations and the met-rics also directly determine what translations getprovided to the translator during translation of newmaterials.
Thus, the algorithms that compute theTM similarity metrics are not only important, butthey are doubly important.The retrieval algorithm used by commercial TMsystems is typically not disclosed (Koehn andSenellart, 2010; Simard and Fujita, 2012; Why-man and Somers, 1999).
However, the best-performing method used in current systems iswidely believed to be based on edit distance (Bald-win and Tanaka, 2000; Simard and Fujita, 2012;Whyman and Somers, 1999; Koehn and Senellart,2010; Christensen and Schjoldager, 2010; Man-dreoli et al., 2006; He et al., 2010).
Recently202Simard and Fujita (2012) have experimented withusing MT (machine translation) evaluation metricsas TM fuzzy match, or similarity, algorithms.
Alimitation of the work of (Simard and Fujita, 2012)was that the evaluation of the performance of theTM similarity algorithms was also conducted us-ing the same MT evaluation metrics.
Simardand Fujita (2012) concluded that their evalua-tion of TM similarity functions was biased sincewhichever MT evaluation metric was used as theTM similarity function was also likely to obtainthe best score under that evaluation metric.The current paper explores various TM fuzzymatch algorithms ranging from simple baselinesto the widely used edit distance to new methods.The evaluations of the TM fuzzy match algorithmsuse human judgments of helpfulness.
An algo-rithm based on weighted n-gram precision consis-tently returns translations judged to be most help-ful by translators for multiple domains and lan-guage pairs.
In addition to being able to retrieveuseful translations from the TM bank, the fuzzymatch scores ought to be indicative of how helpfula translation can be expected to be.
Many transla-tors find it counter-productive to use TM when thebest-matching translation from the TM is not simi-lar to the workload material to be translated.
Thus,many commercial TM products offer translatorsthe opportunity to set a fuzzy match score thresh-old so that only translations with scores above thethreshold will ever be returned.
It seems to be awidely used practice to set the threshold at 70%but again it remains something of a black-box as towhy 70% ought to be the setting.
The current pa-per uncovers what expectations of helpfulness canbe given for different threshold settings for variousfuzzy match algorithms.The rest of this paper is organized as follows.Section 2 presents the TM similarity metrics thatwill be explored; section 3 presents our experi-mental setup; section 4 presents and analyzes re-sults; and section 5 concludes.2 Translation Memory SimilarityMetricsIn this section we define the methods for measur-ing TM similarity for which experimental resultsare reported in section 4.
All of the metrics com-pute scores between 0 and 1, with higher scoresindicating better matches.
All of the metrics taketwo inputs: M and C, where M is a workload sen-tence from the MTBT (Material To Be Translated)and C is the source language side of a candidatepre-existing translation from the TM bank.
Themetrics range from simple baselines to the sur-mised current industrial standard to new methods.2.1 Percent MatchPerhaps the simplest metric one could conceive ofbeing useful for TM similarity matching is percentmatch (PM), the percent of tokens in the MTBTsegment found in the source language side of thecandidate translation pair from the TM bank.Formally,PM(M,C) =|Munigrams?Cunigrams||Munigrams|, (1)where M is the sentence from the MTBT that isto be translated, C is the source language sideof the candidate translation from the TM bank,Munigramsis the set of unigrams in M , andCunigramsis the set of unigrams in C.2.2 Weighted Percent MatchA drawback of PM is that it weights the match-ing of each unigram in an MTBT segment equally,however, it is not the case that the value of assis-tance to the translator is equal for each unigramof the MTBT segment.
The parts that are mostvaluable to the translator are the parts that he/shedoes not already know how to translate.
Weightedpercent match (WPM) uses inverse document fre-quency (IDF) as a proxy for trying to weight wordsbased on how much value their translations are ex-pected to provide to translators.
The use of IDF-based weighting is motivated by the assumptionthat common words that permeate throughout thelanguage will be easy for translators to translatebut words that occur in relatively rare situationswill be harder to translate and thus more valuableto match in the TM bank.
For our implementa-tion of WPM, each source language sentence inthe parallel corpus we are experimenting with istreated as a ?document?
when computing IDF.Formally,WPM(M,C) =?u?
{MunigramsTCunigrams}idf(u,D)?u?Munigramsidf(u,D), (2)where M , C, Munigrams, and Cunigramsare asdefined in Eq.
1, D is the set of all source language203sentences in the parallel corpus, and idf(x,D) =log(|D||{d?D:x?d}|).2.3 Edit DistanceA drawback of both the PM and WPM metricsare that they are only considering coverage of thewords from the workload sentence in the candi-date sentence from the TM bank and not takinginto account the context of the words.
However,words can be translated very differently dependingon their context.
Thus, a TM metric that matchessentences on more than just (weighted) percentagecoverage of lexical items can be expected to per-form better for TM bank evaluation and retrieval.Indeed, as was discussed in section 1, it is widelybelieved that most TM similarity metrics used inexisting systems are based on string edit distance.Our implementation of edit distance (Leven-shtein, 1966), computed on a word level, is sim-ilar to the version defined in (Koehn and Senellart,2010).Formally, our TM metric based on Edit Dis-tance (ED) is defined asED = max(1?edit-dist(M,C)|Munigrams|, 0), (3)where M , C, and Munigramsare as defined inEq.
1, and edit-dist(M,C) is the number of worddeletions, insertions, and substitutions required totransform M into C.2.4 N-Gram PrecisionAlthough ED takes context into account, it doesnot emphasize local context in matching certainhigh-value words and phrases as much as metricsthat capture n-gram precision between the MTBTworkload sentence and candidate source-side sen-tences from the TMB.
We note that n-gram preci-sion forms a fundamental subcomputation in thecomputation of the corpus-level MT evaluationmetric BLEU score (Papineni et al., 2002).
How-ever, although TM fuzzy matching metrics are re-lated to automated MT evaluation metrics, thereare some important differences.
Perhaps the mostimportant is that TM fuzzy matching has to be ableto operate at a sentence-to-sentence level whereasautomated MT evaluation metrics such as BLEUscore are intended to operate over a whole cor-pus.
Accordingly, we make modifications to howwe use n-gram precision for the purpose of TMmatching than how we use it when we computeBLEU scores.
The rest of this subsection and thenext two subsections describe the innovations wemake in adapting the notion of n-gram precision tothe TM matching task.Our first metric along these lines, N-Gram Pre-cision (NGP), is defined formally as follows:NGP =N?n=11Npn, (4)where the value of N sets the upper bound on thelength of n-grams considered1, andpn=|Mn-grams?
Cn-grams|Z ?
|Mn-grams|+ (1?
Z) ?
|Cn-grams|, (5)where M and C are as defined in Eq.
1, Mn-gramsis the set of n-grams in M , Cn-gramsis the set ofn-grams in C, and Z is a user-set parameter thatcontrols how the metric is normalized.2As seen by equation 4, we use an arithmeticmean of precisions instead of the geometric meanthat BLEU score uses.
An arithmetic mean is bet-ter than a geometric mean for use in translationmemory metrics since translation memory metricsare operating at a segment level and not at theaggregate level of an entire test set.
At the ex-treme, the geometric mean will be zero if any ofthe n-gram precisions pnare zero.
Since large n-gram matches are unlikely on a segment level, us-ing a geometric mean can be a poor method to usefor matching on a segment level, as has been de-scribed for the related task of MT evaluation (Dod-dington, 2002; Lavie et al., 2004).
Additionally,for the related task of MT evaluation at a segmentlevel, Lavie et al.
(2004) have found that usingan arithmetic mean correlates better with humanjudgments than using a geometric mean.Now we turn to discussing the parameter Z forcontrolling how the metric is normalized.
At oneextreme, setting Z=1 will correspond to having nopenalty on the length of the candidate retrievedfrom the TMB and leads to getting longer trans-lation matches retrieved.
At the other extreme,1We used N = 4 in our experiments.2Note that the n in n-grams is intended to be substitutedwith the corresponding integer.
Accordingly, for p1, n = 1and therefore Mn-grams= M1-gramsis the set of unigramsin M and Cn-grams= C1-gramsis the set of unigrams in C;for p2, n = 2 and therefore Mn-grams= M2-gramsis theset of bigrams in M and Cn-grams= C2-gramsis the set ofbigrams in C; and so on.204setting Z=0 will correspond to a normalizationthat penalizes relatively more for length of theretrieved candidate and leads to shorter transla-tion matches being retrieved.
There is a preci-sion/recall tradeoff in that one wants to retrievecandidates from the TMB that have high recallin the sense of matching what is in the MTBTsentence yet one also wants the retrieved candi-dates from the TMB to have high precision in thesense of not having extraneous material not rele-vant to helping with the translation of the MTBTsentence.
The optimal setting of Z may differfor different scenarios based on factors like thelanguages, the corpora, and translator preference.We believe that for most TM applications therewill usually be an asymmetric valuation of pre-cision/recall in that recall will be more importantsince the value of getting a match will be morethan the cost of extra material up to a point.
There-fore, we believe a Z setting in between 0.5 and 1.0will be an optimal default.
We use Z=0.75 in allof our experiments described in section 3 and re-ported on in section 4 except for the experimentsexplicitly showing the impact of changing the Zparameter.2.5 Weighted N-Gram PrecisionAnalogous to how we improved PM with WPM,we seek to improve NGP in a similar fashion.
Ascan be seen from the numerator of Equation 5,NGP is weighting the match of all n-grams asuniformly important.
However, it is not the casethat each n-gram is of equal value to the transla-tor.
Similar to WPM, we use IDF as the basis ofour proxy for weighting n-grams according to thevalue their translations are expected to provide totranslators.
Specifically, we define the weight ofan n-gram to be the sum of the IDF values for eachconstituent unigram that comprises the n-gram.Accordingly, we formally define methodWeighted N-Gram Precision (WNGP) as follows:WNGP =N?n=11Nwpn, (6)where N is as defined in Equation 4, andwpn=?i?{Mn-grams?
Cn-grams}w(i)Z[?i?Mn-gramsw(i)]+ (1?
Z)[?i?Cn-gramsw(i)],(7)where Z, Mn-grams, and Cn-gramsare as definedin Equation 5, andw(i) =?1-gram?iidf(1-gram,D), (8)where i is an n-gram and idf(x,D) is as definedabove for Equation 2.2.6 Modified Weighted N-gram PrecisionNote that in Equation 6 each wpncontributesequally to the average.
Modified Weighted N-Gram Precision (MWNGP) improves on WNGPby weighting the contribution of each wpnso thatshorter n-grams contribute more than longer n-grams.
The intuition is that for TM settings, get-ting more high-value shorter n-gram matches atthe expense of fewer longer n-gram matches willbe more helpful since translators will get relativelymore assistance from seeing new high-value vo-cabulary.
Since the translators already presumablyknow the rules of the language in terms of howto order words correctly, the loss of the longer n-gram matches will be mitigated.Formally we define MWNGP as follows:MWNGP =2N2N?
1N?n=112nwpn, (9)where N and wpnare as they were defined forEquation 6.3 Experimental SetupWe performed experiments on two corpora fromtwo different technical domains with two languagepairs, French-English and Chinese-English.
Sub-section 3.1 discusses the specifics of the corporaand the processing we performed.
Subsection 3.2discusses the specifics of our human evaluations ofhow helpful retrieved segments are for translation.2053.1 CorporaFor Chinese-English experiments, we used theOpenOffice3 (OO3) parallel corpus (Tiedemann,2009), which is OO3 computer office productiv-ity software documentation.
For French-Englishexperiments, we used the EMEA parallel cor-pus (Tiedemann, 2009), which are medical docu-ments from the European Medecines Agency.
Thecorpora were produced by a suite of automatedtools as described in (Tiedemann, 2009) and comesentence-aligned.The first step in our experiments was to pre-process the corpora.
For Chinese corpora we to-kenize each sentence using the Stanford ChineseWord Segmenter (Tseng et al., 2005) with the Chi-nese Penn Treebank standard (Xia, 2000).
For allcorpora we remove all segments that have fewerthan 5 tokens or more than 100 tokens.
We callthe resulting set the valid segments.
For the pur-pose of computing match statistics, for French cor-pora we remove all punctuation, numbers, and sci-entific symbols; we case-normalize the text andstem the corpus using the NLTK French snowballstemmer.
For the purpose of computing matchstatistics, for Chinese corpora we remove all butvalid tokens.
Valid tokens must include at leastone Chinese character.
A Chinese character is de-fined as a character in the Unicode range 0x4E00-0x9FFF or 0x4000-0x4DFF or 0xF900-0xFAFF.The rationale for removing these various tokensfrom consideration for the purpose of comput-ing match statistics is that translation of numbers(when they?re written as Arabic numerals), punc-tuation, etc.
is the same across these languagesand therefore we don?t want them influencing thematch computations.
But once a translation is se-lected as being most helpful for translation, theoriginal version (that still contains all the numbers,punctuation, case markings, etc.)
is the versionthat is brought back and displayed to the transla-tor.For the TM simulation experiments, we ran-domly sampled 400 translations from the OO3corpus and pretended that the Chinese sides ofthose 400 translations constitute the workloadChinese MTBT.
From the rest of the corpus werandomly sampled 10,000 translations and pre-tended that that set of 10,000 translations consti-tutes the Chinese-English TMB.
We also did simi-lar sampling from the EMEA corpus of a workloadFrench MTBT of size 300 and a French-EnglishTMB of size 10,000.After the preprocessing and selection of theTMB and MTBT, we found the best-matchingsegment from the TMB for each MTBT seg-ment according to each TM retrieval metric de-fined in section 2.3The resulting sets of(MTBT segment,best-matching TMB segment)pairs formed the inputs on which we conductedour evaluations of the performance of the variousTM retrieval metrics.3.2 Human EvaluationsTo conduct evaluations of how helpful the transla-tions retrieved by the various TM retrieval metricswould be for translating the MTBT segments, weused Amazon Mechanical Turk, which has beenused productively in the past for related work inthe context of machine translation (Bloodgood andCallison-Burch, 2010b; Bloodgood and Callison-Burch, 2010a; Callison-Burch, 2009).For each (MTBT segment,best-matching TMBsegment) pair generated as discussed in subsec-tion 3.1, we collected judgments from Turkers(i.e., the workers on MTurk) on how helpfulthe TMB translation would be for translating theMTBT segment on a 5-point scale.
The 5-pointscale was as follows:?
5 = Extremely helpful.
The sample is so sim-ilar that with trivial modifications I can do thetranslation.?
4 = Very helpful.
The sample included a largeamount of useful words or phrases and/orsome extremely useful words or phrases thatoverlapped with the MTBT.?
3 = Helpful.
The sample included some use-ful words or phrases that made translating theMTBT easier.?
2 = Slightly helpful.
The sample containedonly a small number of useful words orphrases to help with translating the MTBT.?
1 = Not helpful or detrimental.
The samplewould not be helpful at all or it might even beharmful for translating the MTBT.After a worker rated a (MTBT segment,TMBsegment) pair the worker was then required to give3If more than one segment from the TMB was tied forbeing the highest-scoring segment, the segment located firstin the TMB was considered to be the best-matching segment.206metric PM WPM ED NGP WNGP MWNGPPM 100.0 69.5 23.0 32.0 31.5 35.5WPM 69.5 100.0 25.8 37.0 39.0 44.2ED 23.0 25.8 100.0 41.5 35.8 35.0NGP 32.0 37.0 41.5 100.0 77.8 67.0WNGP 31.5 39.0 35.8 77.8 100.0 81.2MWNGP 35.5 44.2 35.0 67.0 81.2 100.0Table 1: OO3 Chinese-English: The percent of thetime that each pair of metrics agree on the mosthelpful TM segmentmetric PM WPM ED NGP WNGP MWNGPPM 100.0 64.7 30.3 40.3 38.3 41.3WPM 64.7 100.0 32.0 46.3 47.0 54.3ED 30.3 32.0 100.0 42.3 40.3 39.3NGP 40.3 46.3 42.3 100.0 76.3 67.7WNGP 38.3 47.0 40.3 76.3 100.0 81.3MWNGP 41.3 54.3 39.3 67.7 81.3 100.0Table 2: EMEA French-English: The percent ofthe time that each pair of metrics agree on the mosthelpful TM segmentan explanation for their rating.
These explanationsproved quite helpful as discussed in section 4.
Foreach (MTBT segment,TMB segment) pair, we col-lected judgments from five different Turkers.
Foreach (MTBT segment,TMB segment) pair thesefive judgments were then averaged to form a meanopinion score (MOS) on the helpfulness of the re-trieved TMB translation for translating the MTBTsegment.
These MOS scores form the basis of ourevaluation of the performance of the different TMretrieval metrics.4 Results and Analysis4.1 Main ResultsTables 1 and 2 show the percent of the time thateach pair of metrics agree on the choice of themost helpful TM segment for the Chinese-EnglishOO3 data and the French-English EMEA data, re-spectively.
A main observation to be made is thatthe choice of metric makes a big difference inthe choice of the most helpful TM segment.
Forexample, we can see that the surmised industrialstandard ED metric agrees with the new MWNGPmetric less than 40% of the time on both sets ofdata (35.0% on Chinese-English OO3 and 39.3%on French-English EMEA data).Tables 3 and 4 show the number of times eachmetric found the TM segment that the Turkersjudged to be the most helpful out of all the TMsegments retrieved by all of the different metrics.From these tables one can see that the MWNGPMetric Found Best Total MTBT SegmentsPM 178 400WPM 200 400ED 193 400NGP 251 400WNGP 271 400MWNGP 282 400Table 3: OO3 Chinese-English: The number oftimes that each metric found the most helpful TMsegment (possibly tied).Metric Found Best Total MTBT SegmentsPM 166 300WPM 184 300ED 148 300NGP 188 300WNGP 198 300MWNGP 201 300Table 4: EMEA French-English: The number oftimes that each metric found the most helpful TMsegment (possibly tied).method consistently retrieves the best TM segmentmore often than each of the other metrics.
Scat-terplots showing the exact performance on everyMTBT segment of the OO3 dataset for variousmetrics are shown in Figures 1, 2, and 3.
To con-serve space, scatterplots are only shown for met-rics PM (baseline metric), ED (strong surmisedindustrial standard metric), and MWNGP (newhighest-performing metric).
For each MTBT seg-ment, there is a point in the scatterplot.
The y-coordinate is the value assigned by the TM metricto the segment retrieved from the TM bank andthe x-coordinate is the MOS of the five Turkerson how helpful the retrieved TM segment wouldbe for translating the MTBT segment.
A pointis depicted as a dark blue diamond if none ofthe other metrics retrieved a segment with higherMOS judgment for that MTBT segment.
A pointis depicted as a yellow circle if another metric re-trieved a different segment from the TM bank forthat MTBT segment that had a higher MOS.A main observation from Figure 1 is that PM isfailing as evidenced by the large number of pointsin the upper left quadrant.
For those points, themetric value is high, indicating that the retrievedsegment ought to be helpful.
However, the MOSis low, indicating that the humans are judging itto not be helpful.
Figure 2 shows that the ED207metric does not suffer from this problem.
How-ever, Figure 2 shows that ED has another prob-lem, which is a lot of yellow circles in the lowerleft quadrant.
Points in the lower left quadrant arenot necessarily indicative of a poorly performingmetric, depending on the degree of match of theTMB with the MTBT workload.
If there is noth-ing available in the TMB that would help withthe MTBT, it is appropriate for the metric to as-sign a low value and the humans to correspond-ingly agree that the retrieved sentence is not help-ful.
However, the fact that so many of ED?s pointsare yellow circles indicates that there were bettersegments available in the TMB that ED was notable to retrieve yet another metric was able to re-trieve them.
Observing the scatterplots for ED andthose for MWNGP one can see that both methodshave the vast majority of points concentrated inthe lower left and upper right quadrants, solvingthe upper left quadrant problem of PM.
However,MWNGP has a relatively more densely populatedupper right quadrant populated with dark blue di-amonds than ED does whereas ED has a moredensely populated lower left quadrant with yel-low circles than MWNGP does.
These results andtrends are consistent across the EMEA French-English dataset so those scatterplots are omittedto conserve space.Examining outliers where MWNGP assigns ahigh metric value yet the Turkers indicated that thetranslation has low helpfulness such as the pointin Figure 3 at (1.6,0.70) is informative.
Lookingonly at the source side, it looks like the translationretrieved from the TMB ought to be very help-ful.
The Turkers put in their explanation of theirscores that the reason they gave low helpfulnessis because the English translation was incorrect.This highlights that a limitation of MWNGP, andall other TM metrics we?re aware of, is that theyonly consider the source side.4.2 Adjusting for length preferencesAs discussed in section 2, the Z parameter can beused to control for length preferences.
Table 5shows how the average length, measured by num-ber of tokens of the source side of the translationpairs returned by MWNGP, changes as the Z pa-rameter is changed.Table 6 shows an example of how the opti-mal translation pair returned by MWNGP changesfrom Z=0.00 to Z=1.00.
The example illustrates1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0MOS0.00.20.40.60.81.0MetricValueFigure 1: OO3 PM scatterplot1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0MOS0.00.20.40.60.81.0MetricValueFigure 2: OO3 ED scatterplot1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0MOS0.00.20.40.60.81.0MetricValueFigure 3: OO3 MWNGP scatterplot208MTBT French: Ne pas utiliser durant la gestation et la lactation, car l?
innocuit?e dum?edicament v?et?erinaire n?
a pas ?et?e ?etablie pendant la gestation oula lactation.English: Do not use during pregnancy and lactation because the safety of theveterinary medicinal product has not been established duringpregnancy and lactation.MWNGP French: Peut ?etre utilis?e pendant la gestation et la lactation.
(Z=0.00) English: Can be used during pregnancy and lactation.MWNGP French: Ne pas utiliser chez l?
animal en gestation ou en p?eriode de lactation,(Z=1.00) car la s?ecurit?e du robenacoxib n?
a pas ?et?e ?etablie chez les femelles gestantes ouallaitantes ni chez les chats et chiens utilis?es pour la reproduction.English: Do not use in pregnant or lactating animals because the safety ofrobenacoxib has not been established during pregnancy and lactation or in catsand dogs used for breeding.Table 6: This table shows for an example MTBT workload sentence from the EMEA French-English datahow the optimal translation pair returned by MWNGP changes when going from Z = 0.00 to Z = 1.00.We provide the English translation of the MTBT workload sentence for the convenience of the readersince it was available from the EMEA parallel corpus.
Note that in a real setting it would be the job ofthe translator to produce the English translation of the MTBT-French sentence using the translation pairsreturned by MWNGP as help.Z Value Avg Length0.00 9.92980.25 13.2040.50 16.01340.75 19.63551.00 27.8829(a) EMEA French-EnglishZ Value Avg Length0.00 7.24750.25 9.56000.50 11.12500.75 14.18251.00 25.0875(b) OO3 Chinese-EnglishTable 5: Average TM segment length, measuredby number of tokens of the source side of the trans-lation pairs returned by MWNGP, for varying val-ues of the Z parameterthe impact of changing the Z value on the na-ture of the translation matches that get returnedby MWNGP.
As discussed in section 2, smallersettings of Z are appropriate for preferences forshorter matches that are more precise in the sensethat a larger percentage of their content will berelevant.
Larger settings of Z are appropriate forpreferences for longer matches that have higher re-call in the sense that they will have more matcheswith the content in the MTBT segment overall, al-though at the possible expense of having more ir-relevant content as well.5 ConclusionsTranslation memory is one of the most widelyused translation technologies.
One of the mostimportant aspects of the technology is the systemfor assessing candidate translations from the TMbank for retrieval.
Although detailed descriptionsof the apparatus used in commercial systems arelacking, it is widely believed that they are basedon an edit distance approach.
We have definedand examined several TM retrieval approaches, in-cluding a new method using modified weighted n-gram precision that performs better than edit dis-tance according to human translator judgments ofhelpfulness.
The MWNGP method is based on thefollowing premises: local context matching is de-sired; weighting words and phrases by expectedhelpfulness to translators is desired; and allowingshorter n-gram precisions to contribute more to thefinal score than longer n-gram precisions is de-sired.
An advantage of the method is that it can beadjusted to suit translator length preferences of re-turned matches.
A limitation of MWNGP, and allother TM metrics we are aware of, is that they onlyconsider the source language side.
Examples fromour experiments reveal that this can lead to poorretrievals.
Therefore, future work is called for toexamine the extent to which the target languagesides of the translations in the TM bank influenceTM system performance and to investigate waysto incorporate target language side information toimprove TM system performance.209ReferencesPeter J Arthern.
1978.
Machine translation and com-puterized terminology systems: a translator?s view-point.
In Translating and the Computer: Proceed-ings of a Seminar, pages 77?108.Timothy Baldwin and Hozumi Tanaka.
2000.
The ef-fects of word order and segmentation on translationretrieval performance.
In Proceedings of the 18thconference on Computational linguistics-Volume 1,pages 35?41.
Association for Computational Lin-guistics.Michael Bloodgood and Chris Callison-Burch.
2010a.Bucking the trend: Large-scale cost-focused activelearning for statistical machine translation.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, pages 854?864.Association for Computational Linguistics.Michael Bloodgood and Chris Callison-Burch.
2010b.Using mechanical turk to build machine translationevaluation sets.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 208?211.
Association for Computational Linguistics.Lynne Bowker and Michael Barlow.
2008.
Acomparative evaluation of bilingual concordancersand translation memory systems.
Topics in Lan-guage Resources for Translation and Localization,?Amsterdam-Filadelfia: John Benjamins, pages 1?22.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: Evaluating translation quality using Amazon?sMechanical Turk.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 286?295, Singapore, August.
As-sociation for Computational Linguistics.Tina Paulsen Christensen and Anne Gram Schjoldager.2010.
Translation-memory (tm) research: what dowe know and how do we know it?
Hermes, 44:89?101.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the sec-ond international conference on Human LanguageTechnology Research, HLT ?02, pages 138?145, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Ignacio Garcia.
2007.
Power shifts in web-based trans-lation memory.
Machine Translation, 21(1):55?68.Yifan He, Yanjun Ma, Andy Way, and Josef Van Gen-abith.
2010.
Integrating n-best smt outputs into atm system.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,pages 374?382.
Association for Computational Lin-guistics.Martin Kay.
1980.
The proper place of men and ma-chines in language translation.
In Research ReportCSL-80-11, Xerox PARC, Palo Alto, CA.
Reprintedin Machine Translation 12, 3-23, 1997.Philipp Koehn and Jean Senellart.
2010.
Convergenceof translation memory and statistical machine trans-lation.
In Proceedings of AMTA Workshop on MTResearch and the Translation Industry, pages 21?31.Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-man.
2004.
The significance of recall in auto-matic metrics for mt evaluation.
In In Proceedingsof the 6th Conference of the Association for MachineTranslation in the Americas (AMTA-2004.Vladimir I Levenshtein.
1966.
Binary codes capableof correcting deletions, insertions and reversals.
InSoviet physics doklady, volume 10, page 707.Federica Mandreoli, Riccardo Martoglia, and PaoloTiberio.
2006.
Extra: a system for example-based translation assistance.
Machine Translation,20(3):167?197.Alan K Melby et al.
1981.
A bilingual concordancesystem and its use in linguistic studies.
In TheEighth Lacus Forum, pages 541?549, Columbia, SC.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Michel Simard and Atsushi Fujita.
2012.
A poor man?stranslation memory using machine translation eval-uation metrics.
In Conference of the Associationfor Machine Translation in the Americas 2012, SanDiego, California, USA, October.Harold L Somers.
2003.
Computers and translation:a translator?s guide, volume 35.
John BenjaminsPublishing Company.J?org Tiedemann.
2009.
News from OPUS - A col-lection of multilingual parallel corpora with toolsand interfaces.
In N. Nicolov, K. Bontcheva,G.
Angelova, and R. Mitkov, editors, RecentAdvances in Natural Language Processing, vol-ume V, pages 237?248.
John Benjamins, Amster-dam/Philadelphia, Borovets, Bulgaria.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, volume171.
Jeju Island, Korea.Edward K. Whyman and Harold L. Somers.
1999.Evaluation metrics for a translation memory system.Software-Practice and Experience, 29:1265?1284.Fei Xia.
2000.
The segmentation guidelines forthe penn chinese treebank (3.0).
Technical ReportIRCS-00-06, University of Pennsylvania.210
