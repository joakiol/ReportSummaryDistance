Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539?549,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCollective Annotation of Linguistic Resources:Basic Principles and a Formal ModelUlle Endriss and Raquel Ferna?ndezInstitute for Logic, Language & ComputationUniversity of Amsterdam{ulle.endriss|raquel.fernandez}@uva.nlAbstractCrowdsourcing, which offers new waysof cheaply and quickly gathering largeamounts of information contributed byvolunteers online, has revolutionised thecollection of labelled data.
Yet, to createannotated linguistic resources from thisdata, we face the challenge of having tocombine the judgements of a potentiallylarge group of annotators.
In this paperwe investigate how to aggregate individualannotations into a single collective anno-tation, taking inspiration from the field ofsocial choice theory.
We formulate a gen-eral formal model for collective annotationand propose several aggregation methodsthat go beyond the commonly used major-ity rule.
We test some of our methods ondata from a crowdsourcing experiment ontextual entailment annotation.1 IntroductionIn recent years, the possibility to undertake large-scale annotation projects with hundreds or thou-sands of annotators has become a reality thanks toonline crowdsourcing methods such as Amazon?sMechanical Turk and Games with a Purpose.
Al-though these techniques open the door to a truerevolution for the creation of annotated corpora,within the computational linguistics communitythere so far is no clear understanding of how theso-called ?wisdom of the crowds?
could or shouldbe used to develop useful annotated linguistic re-sources.
Those who have looked into this increas-ingly important issue have mostly concentrated onvalidating the quality of multiple non-expert an-notations in terms of how they compare to ex-pert gold standards; but they have only used sim-ple aggregation methods based on majority votingto combine the judgments of individual annotators(Snow et al, 2008; Venhuizen et al, 2013).In this paper, we take a different perspective andinstead focus on investigating different aggrega-tion methods for deriving a single collective an-notation from a diverse set of judgments.
For thiswe draw inspiration from the field of social choicetheory, a theoretical framework for combining thepreferences or choices of several individuals intoa collective decision (Arrow et al, 2002).
Our aimis to explore the parallels between the task of ag-gregating the preferences of the citizens participat-ing in an election and the task of combining theexpertise of speakers taking part in an annotationproject.
Our contribution consists in the formula-tion of a general formal model for collective an-notation and, in particular, the introduction of sev-eral families of aggregation methods that go be-yond the commonly used majority rule.The remainder of this paper is organised as fol-lows.
In Section 2 we introduce some basic termi-nology and argue that there are four natural formsof collective annotation.
We then focus on one ofthem and present a formal model for it in Sec-tion 3.
We also formulate some basic principlesof aggregation within this model in the same sec-tion.
Section 4 introduces three families of ag-gregation methods: bias-correcting majority rules,greedy methods for identifying (near-)consensualcoalitions of annotators, and distance-based aggre-gators.
We test the former two families of aggrega-tors, as well as the simple majority rule commonlyused in similar studies, in a case study on data ex-tracted from a crowdsourcing experiment on tex-tual entailment in Section 5.
Section 6 discussesrelated work and Section 7 concludes.2 Four Types of Collective AnnotationAn annotation task consists of a set of items, eachof which is associated with a set of possible cate-gories (Artstein and Poesio, 2008).
The categoriesmay be the same for all items or they may be item-specific.
For instance, dialogue act annotation539(Allen and Core, 1997; Carletta et al, 1997) andword similarity rating (Miller and Charles, 1991;Finkelstein et al, 2002) involve choosing fromamongst a set of categories?acts in a dialogueact taxonomy or values on a scale, respectively?which remains fixed for all items in the annotationtask.
In contrast, in tasks such as word sense la-belling (Kilgarriff and Palmer, 2000; Palmer et al,2007; Venhuizen et al, 2013) and PP-attachmentannotation (Rosenthal et al, 2010; Jha et al, 2010)coders need to choose a category amongst a set ofoptions specific to each item?the possible sensesof each word or the possible attachment points ineach sentence with a prepositional phrase.In either case (one set of categories for all itemsvs.
item-specific sets of categories), annotators aretypically asked to identify, for each item, the cat-egory they consider the best match.
In addition,they may be given the opportunity to indicate thatthey cannot judge (the ?don?t know?
or ?unclear?category).
For large-scale annotation projects runover the Internet it is furthermore very likely thatan annotator will not be confronted with every sin-gle item, and it makes sense to distinguish itemsnot seen by the annotator from items labelled as?don?t know?.
We refer to this form of annotation,i.e., an annotation task where coders have the op-tion to (i) label items with one of the available cat-egories, to (ii) choose ?don?t know?, or to (iii) notlabel an item at all, as plain annotation.Plain annotation is the most common form ofannotation and it is the one we shall focus on inthis paper.
However, other, more complex, formsof annotation are also possible and of interest.
Forinstance, we may ask coders to rank the avail-able categories (resulting in, say, a weak or par-tial order over the categories); we may ask them toprovide a qualitative ratings of the available cat-egories for each item (e.g., excellent match, goodmatch, etc.
); or we may ask for quantitative rat-ings (e.g., numbers from 1 to 100).1 We refer tothese forms of annotation as complex annotation.We want to investigate how to aggregate theinformation available for each item once annota-tions by multiple annotators have been collected.In line with the terminology used in social choicetheory and particularly judgment aggregation (Ar-1Some authors have combined qualitative and quantitativeratings; e.g., for the Graded Word Sense dataset of Erk et al(2009) coders were asked to classify each relevant WordNetsense for a given item on a 5-point scale: 1 completely differ-ent, 2 mostly different, 3 similar, 4 very similar, 5 identical.row, 1963; List and Pettit, 2002), let us call an ag-gregation method independent if the outcome re-garding a given item j only depends on the cate-gories provided by the annotators regarding j it-self (but not on, say, the categories assigned to adifferent item j?).
Independent aggregation meth-ods are attractive due to their simplicity.
They alsohave some conceptual appeal: when deciding onj maybe we should only concern ourselves withwhat people have to say regarding j?
On the otherhand, insisting on independence prevents us fromexploiting potentially useful information that cutsacross items.
For instance, if a particular anno-tator almost always chooses category c, then weshould maybe give less weight to her selecting cfor the item j at hand than when some other anno-tator chooses c for j.
This would call for methodsthat do not respect independence, which we shallrefer to as general aggregation.
Note that whenstudying independent aggregation methods, with-out loss of generality, we may assume that eachannotation task consists of just a single item.In view of our discussion above, there are fourclasses of approaches to collective annotation:(1) Independent aggregation of plain annota-tions.
This is the simplest case, resulting in afairly limited design space.
When, for a givenitem, each annotator has to choose betweenk categories (or abstain) and we do not per-mit ourselves to use any other information,then the only reasonable choice is to imple-ment the plurality rule (Taylor, 2005), underwhich the winning category is the categorychosen by the largest number of annotators.In case there are exactly two categories avail-able, the plurality rule is also called the ma-jority rule.
The only additional considerationto make here (besides how to deal with ties)is whether or not we may want to declare nowinner at all in case the plurality winner doesnot win by a sufficiently significant margin ordoes not make a particular quota.
This is themost common approach in the literature (see,e.g., Venhuizen et al, 2013).
(2) Independent aggregation of complex annota-tions.
This is a natural generalisation of thefirst approach, resulting in a wider range ofpossible methods.
We shall not explore ithere, but only point out that in case annotatorsprovide linear orders over categories, there isa close resemblance to classical voting the-540ory (Taylor, 2005); in case only partial orderscan be elicited, recent work in computationalsocial choice on the generalisation of classi-cal voting rules may prove helpful (Pini et al,2009; Endriss et al, 2009); and in case an-notators rate categories using qualitative ex-pressions such as excellent match, the methodof majority judgment of Balinski and Laraki(2011) should be considered.
(3) General aggregation of plain annotations.This is the approach we shall discuss be-low.
It is related to voting in combinato-rial domains studied in computational socialchoice (Chevaleyre et al, 2008), and to bothbinary aggregation (Dokow and Holzman,2010; Grandi and Endriss, 2011) and judg-ment aggregation (List and Pettit, 2002).
(4) General aggregation of complex annotations.While appealing due to its great level of gen-erality, this approach can only be tackled suc-cessfully once approaches (2) and (3) are suf-ficiently well understood.3 Formal ModelNext we present our model for general aggregationof plain annotations into a collective annotation.3.1 Terminology and NotationAn annotation task is defined in terms of m items,with each item j ?
{1, .
.
.
,m} being associatedwith a finite set of possible categories Cj .
Anno-tators are asked to provide an answer for each ofthe items of the annotation task.
In the context ofplain annotations, a valid answer for item j is anelement of the set Aj = Cj ?
{?,?
}.2 Here ?represents the answer ?don?t know?
and we use ?to indicate that the annotator has not answered (oreven seen) the item at all.
An annotation is a vec-tor of answers by one annotator, one answer foreach item of the annotation task at hand, i.e., anannotation is an element of the Cartesian productA = A1 ?
A2 ?
?
?
?
?
Am.
A typical element ofA will be denoted as A = (a1, .
.
.
, am).Let N = {1, .
.
.
, n} be a finite set of n anno-tators (or coders).
A profile A = (A1, .
.
.
, An) ?An, for a given annotation task, is a vector of an-notations, one for each annotator.
That is, A is an2As discussed earlier, in the context of complex annota-tions, an answer could also be, say, a partial order on Cj or afunction associating elements of Cj with numerical ratings.Item 1 Item 2 Item 3Annotator 1 B A AAnnotator 2 B B BAnnotator 3 A B AMajority B B ATable 1: A profile with a collective annotation.n?m-matrix; e.g., a3,7 is the answer that the 3rdannotator provides for the 7th item.We want to aggregate the information providedby the annotators into a (single) collective anno-tation.
For the sake of simplicity, we use A alsoas the domain of possible collective annotations(even though the distinction between ?
and?maynot be strictly needed here; they both indicate thatwe do not want to commit to any particular cate-gory).
An aggregator is a function F : An ?
A,mapping any given profile into a collective annota-tion, i.e., a labelling of the items in the annotationtask with corresponding categories (or ?
or?).
Anexample is the plurality rule (also known as themajority rule for binary tasks with |Cj | = 2 forall items j), which annotates each item with thecategory chosen most often.Note that the collective annotation need notcoincide with any of the individual annotations.Take, for example, a binary annotation task inwhich three coders label three items with categoryA or B as shown in Table 1.
Here using the major-ity rule to aggregate the annotations would resultin a collective annotation that does not fully matchany annotation by an individual coder.3.2 Basic PropertiesA typical task in social choice theory is to formu-late axioms that formalise specific desirable prop-erties of an aggregator F (Arrow et al, 2002).
Be-low we adapt three of the most basic axioms thathave been considered in the social choice litera-ture to our setting and we briefly discuss their rel-evance to collective annotation tasks.We will require some additional notation: forany profileA, item j, and possible answer a ?
Aj ,let NAj:a denote the set of annotators who choseanswer a for item j under profile A.?
F is anonymous if it treats coders symmetri-cally, i.e., if for every permutation pi : N ?
N ,F (A1, .
.
.
, An) = F (Api(1), .
.
.
, Api(n)).
In so-cial choice theory, this is a fairness constraint.For us, fairness per se is not a desideratum,541but when we do not have any a priori informa-tion regarding the expertise of annotators, thenanonymity is a natural axiom to adopt.?
F is neutral if it treats all items symmetri-cally, i.e., if for every two items j and j?
withthe same set of possible categories (i.e., withCj = Cj?)
and for every profile A, it is the casethat whenever NAj:a = NAj?
:a for all answersa ?
Aj = Aj?
, then F (A)j = F (A)j?
.
Thatis, if the patterns of individual annotations of jand j?
are the same, then also their collectiveannotation should coincide.
In social choicetheory, neutrality is also considered a basic fair-ness requirement (avoiding preferential treat-ment one candidate in an election).
In the con-text of collective annotation there may be goodreasons to violate neutrality: e.g., we may usean aggregator that assigns different default cat-egories to different items and that can overridesuch a default decision only in the presence ofa significant majority (note that this is differentfrom anonymity: we will often not have any in-formation on our annotators, but we may havetangible information on items).3?
F is independent if the collective annotation ofany given item j only depends on the individualannotations of j.
Formally, F is independent if,for every item j and every two profiles A andA?, it is the case that wheneverNAj:a = NA?j:a forall answers a ?
Aj , then F (A)j = F (A?
)j .In social choice theory, independence is oftenseen as a desirable albeit hard (or even impos-sible) to achieve property (Arrow, 1963).
Forcollective annotation, we strongly believe thatit is not a desirable property: by consideringhow annotators label other items we can learnabout their biases and we should try to exploitthis information to obtain the best possible an-notation for the item at hand.Note that the plurality/majority rule is indepen-dent.
All of the methods we shall propose in Sec-tion 4 are both anonymous and neutral?except tothe extent to which we have to violate basic sym-metry requirements in order to break ties betweencategories chosen equally often for a given item.None of our aggregators is independent.3It would also be of interest to formulate a neutrality ax-iom w.r.t.
categories (rather than items).
For two categories,this idea has been discussed under the name of domain-neutrality in the literature (Grandi and Endriss, 2011), butfor larger sets of categories it has not yet been explored.Some annotation tasks might be subject to in-tegrity constraints that determine the internal con-sistency of an annotation.
For example, if ouritems are pairs of words and the possible cate-gories include synonymous and antonymous, thenif item 1 is about words A and B, item 2 aboutwords B and C, and item 3 about words A andC, then any annotation that labels items 1 and 2as synonymous should not label item 3 as antony-mous.
Thus, a further desirable property that willplay a role for some annotation tasks is collectiverationality (Grandi and Endriss, 2011): if all in-dividual annotations respect a given integrity con-straint, then so should the collective annotation.We can think of integrity constraints as impos-ing top-down expert knowledge on an annotation.However, for some annotation tasks, no integrityconstraints may be known to us in advance, eventhough we may have reasons to believe that theindividual annotators do respect some such con-straints.
In that case, selecting one of the indi-vidual annotations in the profile as the collectiveannotation is the only way to ensure that these in-tegrity constraints will be satisfied by the collec-tive annotation (Grandi and Endriss, 2011).
Ofcourse, to do so we would need to assume thatthere is at least one annotator who has labelled allitems (and to be able to design a high-quality ag-gregator in this way we should have a sufficientlylarge number of such annotators to choose from),which may not always be possible, particularly inthe context of crowdsourcing.4 Three Families of AggregatorsIn this section we instantiate our formal model byproposing three families of methods for aggrega-tion.
Each of them is inspired, in part, by standardapproaches to desigining aggregation rules devel-oped in social choice theory and, in part, by thespecific needs of collective annotation.
Regard-ing the latter point, we specifically emphasise thefact that not all annotators can be expected to beequally reliable (in general or w.r.t.
certain items)and we try to integrate the process of aggregationwith a process whereby less reliable annotators areeither given less weight or are excluded altogether.4.1 Bias-Correcting Majority RulesWe first want to explore the following idea: If agiven annotator annotates most items with 0, thenwe might want to assign less significance to that542choice for any particular item.4 That is, if an an-notator appears to be biased towards a particularcategory, then we might want to try to correct forthis bias during aggregation.What follows applies only to annotation taskswhere every item is associated with the same set ofcategories.
For ease of exposition, let us further-more assume that there are only two categories, 0and 1, and that annotators do not make use of theoption to annotate with ?
(?don?t know?
).For every annotator i ?
N and every cate-gory X ?
{0, 1}, fix a weight wXi ?
R. Thebias-correcting majority (BCM) rule for this fam-ily of weights is defined as follows.
Given profileA, the collective category for item j will be 1 incase?ai,j=1w1i >?ai,j=0w0i , and 0 otherwise.5That is, we compute the overall weight for cate-gory 1 by adding up the corresponding weights forthose coders that chose 1 for item j, and we doaccordingly for the overall weight for category 0;finally, we choose as collective category that cate-gory with the larger overall weight.
Note that forwXi ?
1 we obtain the simple majority rule.Below we define three intuitively appealingfamilies of weights, and thereby three BCM rules.However, before we do so, we first require someadditional notation.
Fix a profile of annotations.For X ?
{0, 1}, let Freqi(X) denote the relativefrequency with which annotator i has chosen cat-egory X .
For instance, if i has annotated 20 itemsand has chosen 1 in five cases, then Freqi(1) =0.25.
Similarly, let Freq(X) denote the frequencyof X across the entire profile.Here are three ways of making the intuitive ideaof bias correction concrete:(1) The complement-based BCM rule (ComBCM)is defined by weights wXi = Freqi(1?X).That is, the weight of annotator i for cate-gory X is equal to her relative frequency ofhaving chosen the other category 1?X .
Forexample, if you annotate two items with 1 andeight with 0, then each of your 1-annotationswill have weight 0.8, while each of your0-annotations will only have weight 0.2.
(2) The difference-based BCM rule (DiffBCM) isdefined by weights wXi = 1 + Freq(X) ?4A similar idea is at the heart of cumulative voting, whichrequires a voter to distribute a fixed number of points amongstthe candidates (Glasser, 1959; Brams and Fishburn, 2002).5For the sake of simplicity, our description here presup-poses that ties are always broken in favour of 0.
Other tie-breaking rules (e.g., random tie-breaking) are possible.Freqi(X).
Recall that Freq(X) is the rela-tive frequency ofX in the entire profile, whileFreqi(X) is the relative frequency of X inthe annotation of i.
Hence, if i assigns cat-egory X less often than the general popula-tion, then her weight on X-choices will be in-creased by the difference (and vice versa incase she assigns X more often than the popu-lation at large).
For example, if you assign 1in two out of ten cases, while in general cat-egory 1 appears in exactly 50% of all annota-tions, then your weight for a choice of 1 willbe 1 + 0.5?
0.2 = 1.3, while you weight fora choice of 0 will only be 0.7.
(3) The relative BCM rule (RelBCM) is definedby weights wXi = Freq(X)Freqi(X) .
The idea is verysimilar to the DiffBCM rule.
For the exam-ple given above, your weight for a choice of1 would be 0.5/0.2 = 2.5, while your weightfor a choice of 0 would be 0.5/0.8 = 0.625.The main difference between the ComBCM ruleand the other two rules is that the former only takesinto account the possible bias of individual anno-tators, while the latter two factor in as well thepossible skewness of the data (as reflected by thelabelling behaviour of the full set of annotators).In addition, while ComBCM is specific to thecase of two categories, DiffBCM and RelBCMimmediately generalise to any number of cate-gories.
In this case, we add up the category-specific weights as before and then choose the cat-egory with maximal support (i.e., we generalisethe majority rule underlying the family of BCMrules to the plurality rule).We stress that our bias-correcting majority rulesdo not violate anonymity (nor neutrality for thatmatter).
If we were to give less weight to a givenannotator based on, say, her name, this would con-stitute a violation of anonymity; if we do so due toproperties of the profile at hand and if we do so ina symmetric manner, then it does not.4.2 Greedy Consensus RulesNow consider the following idea: If for a givenitem there is almost complete consensus amongstthose coders that annotated it with a proper cate-gory (i.e., those who did not choose ?
or ?
), thenwe should probably adopt their choice for the col-lective annotation.
Indeed, most aggregators willmake this recommendation.
Furthermore, the factthat there is almost full consensus for one item543may cast doubts on the reliability of coders whodisagree with this near-consensus choice and wemight want to disregard their views not only w.r.t.that item but also as far as the annotation of otheritems is concerned.
Next we propose a family ofaggregators that implement this idea.For simplicity, suppose that the only proper cat-egories available are 0 and 1 and that annotatorsdo not make use of ?
(but it is easy to generaliseto arbitrary numbers of categories and scenarioswhere different items are associated with differentcategories).
Fix a tolerance value t ?
{0, .
.
.
,m}.The greedy consensus rule GreedyCRt works asfollows.
First, initialise the set N ?
with the fullpopulation of annotators N .
Then iterate the fol-lowing two steps:(1) Find the item with the strongest majority foreither 0 or 1 amongst coders in N ?
and lockin that value for the collective annotation.
(2) Eliminate all coders from N ?
who disagreeon more than t items with the values lockedin for the collective annotation so far.Repeat this process until the categories for all mitems have been settled.6 We may think of this asa ?greedy?
way of identifying a coalitionN ?
withhigh inter-annotator agreement and then applyingthe majority rule to this coalition to obtain the col-lective annotation.To be precise, the above is a description of anentire family of aggregators: Whenever there ismore than one item with a majority of maximalstrength, we could choose to lock in any one ofthem.
Also, when there is a split majority betweenannotators in N ?
voting 0 and those voting 1, wehave to use a tie-breaking rule to make a decision.Additional heuristics may be used to make theselocal decisions, or they may be left to chance.Note that in case t = m, GreedyCRt is sim-ply the majority rule (as no annotator will ever geteliminated).
In case t = 0, we end up with a coali-tion of annotators that unanimously agree with allof the categories chosen for the collective annota-tion.
However, this coalition of perfectly aligned6There are some similarities to Tideman?s Ranked Pairsmethod for preference aggregation (Tideman, 1987), whichworks by fixing the relative rankings of pairs of alternativesin order of the strength of the supporting majorities.
In pref-erence aggregation (unlike here), the population of voters isnot reduced in the process; instead, decisions against the ma-jority are taken whenever this is necessary to guarantee thetransitivity of the resulting collective preference order.annotators need not be the largest such coalition(due to the greedy nature of our rule).Note that greedy consensus rules, as definedhere, are both anonymous and neutral.
Specifi-cally, it is important not to confuse possible skew-ness of the data with a violation of neutrality of theaggregator.4.3 Distance-based AggregationOur third approach is based on the notion of dis-tance.
We first define a metric on choices to beable to say how distant two choices are.
This in-duces an aggregator that, for a given profile, re-turns a collective choice that minimises the sumof distances to the individual choices in the pro-file.7 This opens up a wide range of possibilities;we only sketch some of them here.A natural choice is the adjusted Hamming dis-tanceH : A?A ?
R>0, which counts how manyitems two annotations differ on:H(A,A?)
=m?j=1?
(aj , a?j)Here ?
is the adjusted discrete distance defined as?
(x, y) = 0 if x = y or x ?
{?,?}
or y ?
{?,?
},and as ?
(x, y) = 1 in all other cases.8Once we have fixed a distance d on A (suchas H), this induces an aggregator Fd:Fd(A) = argminA?An?i=1d(A,Ai)To be precise, Fd is an irresolute aggregator thatmight return a set of best annotations with minimaldistance to the profile.Note that FH is simply the plurality rule.
Thisis so because every element of the Cartesian prod-uct is a possible annotation.
In the presence of in-tegrity constraints excluding some combinations,however, a distance-based rule allows for more so-phisticated forms of aggregation (by choosing theoptimal annotation w.r.t.
all feasible annotations).We may also try to restrict the computation ofdistances to a subset of ?reliable?
annotators.
Con-sider the following idea: If a group of annota-tors is (fairly) reliable, then they should have a7This idea has been used in voting (Kemeny, 1959), beliefmerging (Konieczny and Pino Pe?rez, 2002), and judgmentaggregation (Miller and Osherson, 2009).8This ?, divided by m, is the same thing as what Artsteinand Poesio (2008) call the agreement value agrj for item j.544(fairly) high inter-annotator agreement.
By thisreasoning, we should choose a group of annota-tors ANN ?
N that maximises inter-annotatoragreement in ANN and work with the aggrega-tor argminA?A?i?ANN d(A,Ai).
But this is toosimplistic: any singleton ANN = {i} will resultin perfect agreement.
That is, while we can eas-ily maximise agreement, doing so in a na?
?ve waymeans ignoring most of the information collected.In other words, we face the following dilemma:?
On the one hand, we should choose a small setANN (i.e., select few annotators to base our col-lective annotation on), as that will allow us toincrease the (average) reliability of the annota-tors taken into account.?
On the other hand, we should choose a large setANN (i.e., select many annotators to base ourcollective annotation on), as that will increasethe amount of information exploited.One pragmatic approach is to fix a minimum qual-ity threshold regarding one of the two dimensionsand optimise in view of the other.95 A Case StudyIn this section, we report on a case study inwhich we have tested our bias-correcting major-ity and greedy consensus rules.10 We have usedthe dataset created by Snow et al (2008) forthe task of recognising textual entailment, orig-inally proposed by Dagan et al (2006) in thePASCAL Recognizing Textual Entailment (RTE)Challenge.
RTE is a binary classification task con-sisting in judging whether the meaning of a pieceof text (the so-called hypothesis) can be inferredfrom another piece of text (the entailing text).The original RTE1 Challenge testset consists of800 text-hypothesis pairs (such as T : ?Chre?tienvisited Peugeot?s newly renovated car factory?,H: ?Peugeot manufactures cars?)
with a goldstandard annotation that classifies each item as ei-ther true (1)?in case H can be inferred from T?or false (0).
Exactly 400 items are annotated as0 and exactly 400 as 1.
Bos and Markert (2006)performed an independent expert annotation of9GreedyCRt is a greedy (rather than optimal) implemen-tation of this basic idea, with the tolerance value t fixing athreshold on (a particular form of) inter-annotator agreement.10Since the annotation task and dataset used for our casestudy do not involve any interesting integrity constraints, wehave not tested any distance-based aggregation rules.this testset, obtaining 95% agreement between theRTE1 gold standard and their own annotation.The dataset of Snow et al (2008) includes 10non-expert annotations for each of the 800 itemsin the RTE1 testset, collected with Amazon?s Me-chanical Turk.
A quick examination of the datasetshows that there are a total of 164 annotators whohave annotated between 20 items (124 annotators)and 800 items each (only one annotator).
Non-expert annotations with category 1 (rather than 0)are slightly more frequent (Freq(1) ?
0.57).We have applied our aggregators to this data andcompared the outcomes with each other and to thegold standard.
The results are summarised in Ta-ble 2 and discussed in the sequel.
For each pairwe report the observed agreement Ao (proportionof items on which two annotations agree) and, inbrackets, Cohen?s kappa ?
= Ao?Ae1?Ae , with Ae be-ing the expected agreement for independent anno-tators (Cohen, 1960; Artstein and Poesio, 2008).Note that there are several variants of the major-ity rule, depending on how we break ties.
In Ta-ble 2, Maj10 is the majority rule that chooses 1 incase the number of annotators choosing 1 is equalto the number of annotators choosing 0 (and ac-cordingly for Maj01).
For 65 out of the 800 itemsthere has been a tie (i.e., five annotators choose 0and another five choose 1).
This means that the tie-breaking rule used can have a significant impacton results.
Snow et al (2008) work with a major-ity rule where ties are broken uniformly at randomand report an observed agreement (accuracy) be-tween the majority rule and the gold standard of89.7%.
This is confirmed by our results: 89.7%is the mean of 87.5% (our result for Maj10) and91.9% (our result for Maj01).
If we break tiesin the optimal way (in view of approximating thegold standard (which of course would not actu-ally be possible without having access to that goldstandard), then we obtain an observed agreementof 93.8%, but if we are unlucky and ties happen toget broken in the worst possible way, we obtain anobserved agreement of only 85.6%.For none of our bias-correcting majority rulesdid we encounter any ties.
Hence, for these ag-gregators the somewhat arbitrary choices we haveto make when breaking ties are of no significance,which is an important point in their favour.
Ob-serve that all of the bias-correcting majority rulesapproximate the gold standard better than the ma-jority rule with uniformly random tie-breaking.545Annotation Maj10 Maj01 ComBCM DiffBCM RelBCM GreedyCR0 GreedyCR15Gold Standard 87.5% (.75) 91.9% (.84) 91.1% (.80) 91.5% (.81) 90.8% (.80) 86.6% (.73) 92.5% (.85)Maj10 91.9% (.84) 88.9% (.76) 94.3% (.87) 94.0% (.87) 87.6% (.75) 91.5% (.83)Maj01 96.0% (.91) 97.6% (.95) 96.9% (.93) 89.0% (.78) 96.1% (.92)ComBCM 94.6% (.86) 94.4% (.86) 88.8% (.75) 93.9% (.86)DiffBCM 98.8% (.97) 88.6% (.75) 94.8% (.88)RelBCM 88.4% (.74) 93.8% (.86)GreedyCR0 90.6% (.81)Table 2: Observed agreement (and ?)
between collective annotations and the gold standard.Recall that the greedy consensus rule is in facta family of aggregators: whenever there is morethan one item with a maximal majority, we maylock in any one of them.
Furthermore, when thereis a split majority, then ties may be broken eitherway.
The results reported here refer to an imple-mentation that always chooses the lexicographi-cally first item amongst all those with a maximalmajority and that breaks ties in favour of 1.
Theseparameters yield neither the best or the worst ap-proximations of the gold standard.
We tested arange of tolerance values.
As an example, Table 2includes results for tolerance values 0 and 15.
Thecoalition found for tolerance 0 consists of 46 an-notators who all completely agree with the col-lective annotation; the coalition found for toler-ance 15 consists of 156 annotators who all dis-agree with the collective annotation on at most15 items.
While GreedyCR0 appears to performrather poorly, GreedyCR15 approximates the goldstandard particularly well.
This is surprising andsuggests, on the one hand, that eliminating onlythe most extreme outlier annotators is a usefulstrategy, and on the other hand, that a high-qualitycollective annotation can be obtained from a groupof annotators that disagree substantially.116 Related WorkThere is an increasing number of projects usingcrowdsourcing methods for labelling data.
On-line Games with a Purpose, originally conceivedby von Ahn and Dabbish (2004) to annotate im-ages, have been used for a variety of linguis-tic tasks: Lafourcade (2007) created JeuxDeMotsto develop a semantic network by asking playersto label words with semantically related words;Phrase Detectives (Chamberlain et al, 2008) hasbeen used to gather annotations on anaphoric co-reference; and more recently Basile et al (2012)11Recall that 124 out of 164 coders only annotated 20 itemseach; a tolerance value of 15 thus is fairly lenient.have developed the Wordrobe set of games forannotating named entities, word senses, homo-graphs, and pronouns.
Similarly, crowdsourcingvia microworking sites like Amazon?s MechanicalTurk has been used in several annotation experi-ments related to tasks such as affect analysis, eventannotation, sense definition and word sense disam-biguation (Snow et al, 2008; Rumshisky, 2011;Rumshisky et al, 2012), amongst others.12All these efforts face the problem of how to ag-gregate the information provided by a group ofvolunteers into a collective annotation.
However,by and large, the emphasis so far has been on is-sues such as experiment design, data quality, andcosts, with little attention being paid to the aggre-gation methods used, which are typically limitedto some form of majority vote (or taking averagesif the categories are numeric).
In contrast, our fo-cus has been on investigating different aggregationmethods for arriving at a collective annotation.Our work has connections with the literature oninter-annotator agreement.
Agreement scores suchas kappa are used to assess the quality of an anno-tation but do not play a direct role in constructingone single annotation from the labellings of sev-eral coders.13 The methods we have proposed, incontrast, do precisely that.
Still, agreement playsa prominent role in some of these methods.
In ourdiscussion of distance-based aggregation, we sug-gested how agreement can be used to select a sub-set of annotators whose individual annotations areminimally distant from the resulting collective an-notation.
Our greedy consensus rule also makesuse of agreement to ensure a minimum level ofconsensus.
In both cases, the aggregators have theeffect of disregarding some outlier annotators.12See also the papers presented at the NAACL 2010 Work-shop on Creating Speech and Language Data with Amazon?sMechanical Turk (tinyurl.com/amtworkshop2010).13Creating a gold standard often involves adjudication ofdisagreements by experts, or even the removal of cases withdisagreement from the dataset.
See, e.g., the papers cited byBeigman Klebanov and Beigman (2009).546Other researchers have explored ways to di-rectly identify ?low-quality?
annotators.
For in-stance, Snow et al (2008) and Raykar et al (2010)propose Bayesian methods for identifying and cor-recting annotators?
biases, while Ipeirotis et al(2010) propose an algorithm for assigning a qual-ity score to annotators that distinguishes intrinsicerror rate from an annotator?s bias.
In our ap-proach, we do not directly rate annotators or re-calibrate their annotations?rather, some outlierannotators get to play a marginal role in the re-sulting collective annotation as a side effect of theaggregation methods themselves.Although in our case study we have tested ouraggregators by comparing their outcomes to a goldstandard, our approach to collective annotation it-self does not assume that there is in fact a groundtruth.
Instead, we view collective annotations asreflecting the views of a community of speakers.14This contrasts significantly with, for instance, themachine learning literature, where there is a fo-cus on estimating the hidden true label from a setof noisy labels using maximum-likelihood estima-tors (Dawid and Skene, 1979; Smyth et al, 1995;Raykar et al, 2010).In application domains where it is reasonable toassume the existence of a ground truth and wherewe are able to model the manner in which individ-ual judgments are being distorted relative to thisground truth, social choice theory provides tools(using again maximum-likelihood estimators) forthe design of aggregators that maximise chancesof recovering the ground truth for a given model ofdistortion (Young, 1995; Conitzer and Sandholm,2005).
In recent work, Mao et al (2013) have dis-cussed the use of these methods in the context ofcrowdsourcing.
Specifically, they have designedan experiment in which the ground truth is definedunambiguously and known to the experiment de-signer, so as to be able to extract realistic modelsof distortion from the data collected in a crowd-sourcing exercise.7 ConclusionsWe have presented a framework for combiningthe expertise of speakers taking part in large-scale14In some domains, such as medical diagnosis, it makesperfect sense to assume that there is a ground truth.
However,in tasks related to linguistic knowledge and language use suchan assumption seems far less justified.
Hence, a collectiveannotation may be the closest we can get to a representationof the linguistic knowledge/use of a linguistic community.annotation projects.
Such projects are becomingmore and more common, due to the availabilityof online crowdsourcing methods for data annota-tion.
Our work is novel in several respects.
Wehave drawn inspiration from the field of socialchoice theory to formulate a general formal modelfor aggregation problems, which we believe shedslight on the kind of issues that arise when tryingto build annotated linguistic resources from a po-tentially large group of annotators; and we haveproposed several families of concrete methods foraggregating individual annotations that are morefine-grained that the standard majority rule that sofar has been used across the board.
We have testedsome of our methods on a gold standard testset forthe task of recognising textual entailment.Our aim has been conceptual, namely to pointout that it is important for computational linguiststo reflect on the methods used when aggregat-ing annotation information.
We believe that so-cial choice theory offers an appropriate generalmethodology for supporting this reflection.
Im-portantly, this does not mean that the concrete ag-gregation methods developed in social choice the-ory are immediately applicable or that all the ax-ioms typically studied in social choice theory arenecessarily relevant to aggregating linguistic an-notations.
Rather, what we claim is that it is themethodology of social choice theory which is use-ful: to formally state desirable properties of ag-gregators as axioms and then to investigate whichspecific aggregators satisfy them.
To put it dif-ferently: at the moment, researchers in compu-tational linguistics simply use some given aggre-gation methods (almost always the majority rule)and judge their quality on how they fare in specificexperiments?but there is no principled reflectionon the methods themselves.
We believe that thisshould change and hope that the framework out-lined here can provide a suitable starting point.In future work, the framework we have pre-sented here should be tested more extensively, notonly against a gold standard but also in terms ofthe usefulness of the derived collective annotationsfor training supervised learning systems.
On thetheoretial side, it would be interesting to study theaxiomatic properties of the methods of aggrega-tion we have proposed here in more depth and todefine axiomatic properties of aggregators that arespecifically tailored to the task of collective anno-tation of linguistic resources.547ReferencesJames Allen and Mark Core, 1997.
DAMSL: DialogueAct Markup in Several Layers.
Discourse ResourceInitiative.Kenneth J. Arrow, Armatya K. Sen, and Kotaro Suzu-mura, editors.
2002.
Handbook of Social Choiceand Welfare.
North-Holland.Kenneth J. Arrow.
1963.
Social Choice and IndividualValues.
John Wiley and Sons, 2nd edition.
Firstedition published in 1951.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Michel Balinski and Rida Laraki.
2011.
MajorityJudgment: Measuring, Ranking, and Electing.
MITPress.Valerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012.
A platform for collaborative se-mantic annotation.
In Proc.
13th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL-2012), pages 92?96.Beata Beigman Klebanov and Eyal Beigman.
2009.From annotator agreement to noise models.
Com-putational Linguistics, 35(4):495?503.Johan Bos and Katja Markert.
2006.
Recognising tex-tual entailment with robust logical inference.
In Ma-chine Learning Challenges, volume 3944 of LNCS,pages 404?426.
Springer-Verlag.Steven J. Brams and Peter C. Fishburn.
2002.
Votingprocedures.
In Kenneth J. Arrow, Armartya K. Sen,and Kotaro Suzumura, editors, Handbook of SocialChoice and Welfare.
North-Holland.Jean Carletta, Stephen Isard, Anne H. Anderson,Gwyneth Doherty-Sneddon, Amy Isard, and Jacque-line C. Kowtko.
1997.
The reliability of a dialoguestructure coding scheme.
Computational Linguis-tics, 23:13?31.Jon Chamberlain, Massimo Poesio, and Udo Kr-uschwitz.
2008.
Addressing the resource bottleneckto create large-scale annotated texts.
In Semanticsin Text Processing.
STEP 2008 Conference Proceed-ings, volume 1 of Research in Computational Se-mantics, pages 375?380.
College Publications.Yann Chevaleyre, Ulle Endriss, Je?ro?me Lang, andNicolas Maudet.
2008.
Preference handling in com-binatorial domains: From AI to social choice.
AIMagazine, 29(4):37?46.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20:37?46.Vincent Conitzer and Tuomas Sandholm.
2005.
Com-mon voting rules as maximum likelihood estimators.In Proc.
21st Conference on Uncertainty in ArtificialIntelligence (UAI-2005).Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entail-ment challenge.
In Machine Learning Challenges,volume 3944 of LNCS, pages 177?190.
Springer-Verlag.Alexander Philip Dawid and Allan M. Skene.
1979.Maximum likelihood estimation of observer error-rates using the EM algorithm.
Applied Statistics,28(1):20?28.Elad Dokow and Ron Holzman.
2010.
Aggregationof binary evaluations.
Journal of Economic Theory,145(2):495?511.Ulle Endriss, Maria Silvia Pini, Francesca Rossi, andK.
Brent Venable.
2009.
Preference aggrega-tion over restricted ballot languages: Sincerity andstrategy-proofness.
In Proc.
21st International JointConference on Artificial Intelligence (IJCAI-2009).Katrin Erk, Diana McCarthy, and Nicholas Gaylord.2009.
Investigations on word senses and word us-ages.
In Proc.
47th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-2009),pages 10?18.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Gerald J. Glasser.
1959.
Game theory and cumula-tive voting for corporate directors.
Management Sci-ence, 5(2):151?156.Umberto Grandi and Ulle Endriss.
2011.
Binary ag-gregation with integrity constraints.
In Proc.
22ndInternational Joint Conference on Artificial Intelli-gence (IJCAI-2011).Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.2010.
Quality Management on Amazon MechanicalTurk.
In Proc.
2nd Human Computation Workshop(HCOMP-2010).Mukund Jha, Jacob Andreas, Kapil Thadani, SaraRosenthal, and Kathleen McKeown.
2010.
Corpuscreation for new genres: A crowdsourced approachto PP attachment.
In Proc.
NAACL-HLT Workshopon Creating Speech and Language Data with Ama-zon?s Mechanical Turk, pages 13?20.John Kemeny.
1959.
Mathematics without numbers.Daedalus, 88:577?591.Adam Kilgarriff and Martha Palmer.
2000.
Introduc-tion to the special issue on senseval.
Computers andthe Humanities, 34(1):1?13.Se?bastien Konieczny and Ramo?n Pino Pe?rez.
2002.Merging information under constraints: A logicalframework.
Journal of Logic and Computation,12(5):773?808.548Mathieu Lafourcade.
2007.
Making people play forlexical acquisition with the JeuxDeMots prototype.In Proc.
7th International Symposium on NaturalLanguage Processing.Christian List and Philip Pettit.
2002.
Aggregating setsof judgments: An impossibility result.
Economicsand Philosophy, 18(1):89?110.Andrew Mao, Ariel D. Procaccia, and Yiling Chen.2013.
Better human computation through principledvoting.
In Proc.
27th AAAI Conference on ArtificialIntelligence.George A. Miller and Walter G. Charles.
1991.
Con-textual correlates of semantic similarity.
Languageand Cognitive Processes, 6(1):1?28.Michael K. Miller and Daniel Osherson.
2009.
Meth-ods for distance-based judgment aggregation.
SocialChoice and Welfare, 32(4):575?601.Martha Palmer, Hoa Trang Dang, and ChristianeFellbaum.
2007.
Making fine-grained andcoarse-grained sense distinctions, both manuallyand automatically.
Natural Language Engineering,13(2):137?163.Maria Silvia Pini, Francesca Rossi, K. Brent Venable,and Toby Walsh.
2009.
Aggregating partially or-dered preferences.
Journal of Logic and Computa-tion, 19(3):475?502.Vikas Raykar, Shipeng Yu, Linda Zhao, Gerardo Her-mosillo Valadez, Charles Florin, Luca Bogoni, andLinda Moy.
2010.
Learning from crowds.
The Jour-nal of Machine Learning Research, 11:1297?1322.Sara Rosenthal, William Lipovsky, Kathleen McKe-own, Kapil Thadani, and Jacob Andreas.
2010.
To-wards semi-automated annotation for prepositionalphrase attachment.
In Proc.
7th International Con-ference on Language Resources and Evaluation(LREC-2010).Anna Rumshisky, Nick Botchan, Sophie Kushkuley,and James Pustejovsky.
2012.
Word sense inven-tories by non-experts.
In Proc.
8th InternationalConference on Language Resources and Evaluation(LREC-2012).Anna Rumshisky.
2011.
Crowdsourcing word sensedefinition.
In Proc.
ACL-HLT 5th Linguistic Anno-tation Workshop (LAW-V).Padhraic Smyth, Usama Fayyad, Michael Burl, PietroPerona, and Pierre Baldi.
1995.
Inferring groundtruth from subjective labelling of venus images.
Ad-vances in Neural Information Processing Systems,pages 1085?1092.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In Proc.
Conference on EmpiricalMethods in Natural Language Processing (EMNLP-2008), pages 254?263.Alan D. Taylor.
2005.
Social Choice and the Math-ematics of Manipulation.
Cambridge UniversityPress.T.
Nicolaus Tideman.
1987.
Independence of clonesas a criterion for voting rules.
Social Choice andWelfare, 4(3):185?206.Noortje Venhuizen, Valerio Basile, Kilian Evang, andJohan Bos.
2013.
Gamification for word sense la-beling.
In Proc.
10th International Conference onComputational Semantics (IWCS-2013), pages 397?403.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proc.
SIGCHI Con-ference on Human Factors in Computing Systems,pages 319?326.
ACM.H.
Peyton Young.
1995.
Optimal voting rules.
Journalof Economic Perspectives, 9(1):51?64.549
