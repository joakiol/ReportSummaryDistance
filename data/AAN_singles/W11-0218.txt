Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 136?145,Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational LinguisticsSimSem: Fast Approximate String Matching in Relation to SemanticCategory DisambiguationPontus Stenetorp??
Sampo Pyysalo?
and Jun?ichi Tsujii??
Tsujii Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan?
Aizawa Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan?
Microsoft Research Asia, Beijing, People?s Republic of China{pontus,smp}@is.s.u-tokyo.ac.jpjtsujii@microsoft.comAbstractIn this study we investigate the merits offast approximate string matching to addresschallenges relating to spelling variants and toutilise large-scale lexical resources for seman-tic class disambiguation.
We integrate stringmatching results into machine learning-baseddisambiguation through the use of a novel setof features that represent the distance of agiven textual span to the closest match in eachof a collection of lexical resources.
We col-lect lexical resources for a multitude of se-mantic categories from a variety of biomedi-cal domain sources.
The combined resources,containing more than twenty million lexicalitems, are queried using a recently proposedfast and efficient approximate string match-ing algorithm that allows us to query largeresources without severely impacting systemperformance.
We evaluate our results on sixcorpora representing a variety of disambigua-tion tasks.
While the integration of approxi-mate string matching features is shown to sub-stantially improve performance on one corpus,results are modest or negative for others.
Wesuggest possible explanations and future re-search directions.
Our lexical resources andimplementation are made freely available forresearch purposes at: http://github.com/ninjin/simsem1 IntroductionThe use of dictionaries for boosting performance hasbecome commonplace for Named Entity Recogni-tion (NER) systems (Torii et al, 2009; Ratinov andRoth, 2009).
In particular, dictionaries can give aninitial improvement when little or no training datais available.
However, no dictionary is perfect, andall resources lack certain spelling variants and lagbehind current vocabulary usage and thus are un-able to cover the intended domain in full.
Further,due to varying dictionary curation and corpus anno-tation guidelines, the definition of what constitutesa semantic category is highly unlikely to preciselymatch for any two specific resources (Wang et al,2009).
Ideally, for applying a lexical resource to anentity recognition or disambiguation task to serve asa definition of a semantic category there would bea precise match between the definitions of the lexi-cal resource and target domain, but this is seldom ornever the case.Most previous work studying the use of dictionaryresources in entity mention-related tasks has focusedon single-class NER, in particular this is true forBioNLP where it has mainly concerned the detec-tion of proteins.
These efforts include Tsuruoka andTsujii (2003), utilising dictionaries for protein de-tection by considering each dictionary entry using anovel distance measure, and Sasaki et al (2008), ap-plying dictionaries to restrain the contexts in whichproteins appear in text.
In this work, we do notconsider entity mention detection, but instead focussolely on the related task of disambiguating the se-mantic category for a given continuous sequence ofcharacters (a textual span), doing so we side-step theissue of boundary detection in favour of focusing onnovel aspects of semantic category disambiguation.Also, we are yet to see a high-performing multi-classbiomedical NER system, this motivates our desire toinclude multiple semantic categories.1362 MethodsIn this section we introduce our approach and thestructure of our system.2.1 SimSemMany large-scale language resources are availablefor the biomedical domain, including collectionsof domain-specific lexical items (Ashburner et al,2000; Bodenreider, 2004; Rebholz-Schuhmann etal., 2010).
These resources present obvious opportu-nities for semantic class disambiguation.
However,in order to apply them efficiently, one must be ableto query the resources taking into consideration bothlexical variations in dictionary entries compared toreal-world usage and the speed of look-ups.We can argue that each resource offers a differ-ent view of what constitutes a particular semanticcategory.
While these views will not fully overlapbetween resources even for the same semantic cate-gory, we can expect a certain degree of agreement.When learning to disambiguate between semanticcategories, a machine learning algorithm could beexpected to learn to identify a specific semantic cat-egory from the similarity between textual spans an-notated for the category and entries in a related lex-ical resource.
For example, if we observe the text?Carbonic anhydrase IV?
marked as PROTEIN andhave an entry for ?Carbonic anhydrase 4?
in a lexicalresource, a machine learning method can learn to as-sociate the resource with the PROTEIN category (atspecific similarity thresholds) despite syntactic dif-ferences.In this study, we aim to construct such a systemand to demonstrate that it outperforms strict stringmatching approaches.
We refer to our system asSimSem, as in ?Similarity?
and ?Semantic?.2.2 SimStringSimString1 is a software library utilising the CP-Merge algorithm (Okazaki and Tsujii, 2010) to en-able fast approximate string matching.
The softwaremakes it possible to find matches in a collection withover ten million entries using cosine similarity anda similarity threshold of 0.7 in approximately 1 mil-lisecond with modest modern hardware.
This makesit useful for querying a large collection of strings to1http://www.chokkan.org/software/simstring/find entries which may differ from the query stringonly superficially and may still be members of thesame semantic category.As an example, if we construct a SimStringdatabase using an American English wordlist2 andquery it using the cosine measure and a threshold of0.7.
For the query ?reviewer?
SimString would re-turn the following eight entries: review, viewer, pre-view, reviewer, unreviewed, televiewer, and review-eress.
We can observe that most of the retrieved en-tries share some semantic similarity with the query.2.3 Machine LearningFor the machine learning component of our systemwe use the L2-regularised logistic regression im-plementation of the LIBLINEAR3 software library(Fan et al, 2008).
We do not normalise our featurevectors and optimise our models?
penalty parameterusing k-fold cross-validation on the training data.
Inorder to give a fair representation of the performanceof other systems, we use a rich set of features that arewidely applied for NER (See Table 1).Our novel SimString features are generated as fol-lows.
We query each SimString database using thecosine measure with a sliding similarity threshold,starting at 1.0 and ending at 0.7, lowering the thresh-old by 0.1 per query.
If a query is matched, we gen-erate a feature unique for that database and thresh-old, we also generate the same feature for each stepfrom the current threshold to the cut-off of 0.7 (amatch at e.g.
0.9 similarity also implies matches at0.8 and 0.7).The cut-off is motivated by the fact that verylow thresholds introduces a large degree of noise.For example, for our American English wordlistthe query ?rejection?
using threshold 0.1 and thecosine measure will return 13,455 results, amongthem ?questionableness?
which only have a singlesequence ?ion?
in common.It is worthwhile to note that during our prelimi-nary experiments we failed to establish a consistentbenefit from contextual features across our develop-ment sets.
Thus, contextual features are not includedin our feature set and instead our study focuses only2/usr/share/dict/web2 under FreeBSD 8.1-RELEASE, basedon Webster?s Second International dictionary from 19343We used version 1.7 of LIBLINEAR for our experiments137Feature Type Input Value(s)Text Text Flu FluLower-cased Text DNA dnaPrefixes: sizes 3 to 5 Text bull bul, .
.
.Suffixes: sizes 3 to 5 Text bull ull, .
.
.Stem (Porter, 1993) Text performing performIs a pair of digits Bool 42 TrueIs four digits Bool 4711 TrueLetters and digits Bool C4 TrueDigits and hyphens Bool 9-12 TrueDigits and slashes Bool 1/2 TrueDigits and colons Bool 3,1 TrueDigits and dots Bool 3.14 TrueUpper-case and dots Bool M.C.
TrueInitial upper-case Bool Pigeon TrueOnly upper-case Bool PMID TrueOnly lower-case Bool pure TrueOnly digits Bool 131072 TrueOnly non-alpha-num Bool #*$!
TrueContains upper-case Bool gAwn TrueContains lower-case Bool After TrueContains digits Bool B52 TrueContains non-alpha-num Bool B52;s TrueDate regular expression4 Bool 1989-01-30 TruePattern Text 1B-zz 0A-aaCollapsed Pattern Text 1B-zz 0A-aTable 1: Basic features used for classificationthe features that are generated solely from the tex-tual span which has been annotated with a semanticcategory (span-internal features) and the comparisonof approximate and strict string matching.3 ResourcesThis section introduces and discusses the prepro-cessing and statistics of the lexical and corpus re-sources used in our experiments.3.1 Lexical ResourcesTo generate a multitude of SimString databases cov-ering a wide array of semantic categories we employseveral freely available lexical resources (Table 2).The choice of lexical resources was initially madewith the aim to cover commonly annotated domainsemantic categories: the CHEBI and CHEMICALsubsets of JOCHEM for chemicals, LINNAEUS forspecies, Entrez Gene and SHI for proteins.
We then4A simple regular expression matching dates:?
(19|20)\d\d[- /.
](0[1-9]|1[012])[- /.
](0[1-9]|[12][0-9]|3[01])$from http://www.regular-expressions.info/dates.htmlexpanded the selection based on error analysis to in-crease our coverage of a wider array of semantic cat-egories present in our development data.We used the GO version from March 2011, ex-tracting all non-obsolete terms from the ontologyand separating them into the three GO subontolo-gies: biological process (BP), cellular component(CC) and molecular function (MF).
We then createdan additional three resources by extracting all exactsynonyms for each entry.
Lastly, we expanded thesesix resources into twelve resources by applying theGO term variant generation technique described byBeisswanger et al (2008).UMLS, a collection of various resources, contain135 semantic categories (e.g.
Body Location or Re-gion and Inorganic Chemical) which we use to cre-ate a database for each category.For Entrez Gene we extracted all entries for thefollowing types: gene locus, protein name, proteindescription, nomenclature symbol and nomenclaturefullname, creating a SimString database for each.This leaves some parts of Entrez Gene unutilised,but we deemed these categories to be sufficient forour experiments.The Turku Event Corpus is a resource created byapplying an automated event extraction system onthe full release of PubMed from 2009.
As a pre-condition for the event extraction system to operate,protein name recognition is necessary; for this cor-pus, NER has been performed by the corpus curatorsusing the BANNER (Leaman and Gonzalez, 2008)NER system trained on GENETAG (Tanabe et al,2005).
We created a database (PROT) containingall protein annotations, extracted all event triggers(TRIG) and created a database for each of the eventtypes covered by the event extraction system.For the AZDC corpus, we extracted each anno-tated textual span since the corpus covers only a sin-gle semantic category.
Similarly, the LINNAEUSdictionary was converted into a single database sinceit covers the single category ?species?.Table 3 contains the statistics per dictionary re-source and the number of SimString databases cre-ated for each resource.
Due to space requirementswe leave out the full details for GO BP, GO CC,GO MF, UMLS, Entrez Gene and TURKU TRIG,and instead give the total entries for all the databasesgenerated from these resources.138Name Abbreviation Semantic Categories PublicationGene Ontology GO Multiple Ashburner et al (2000)Protein Information Resource PIR Proteins Wu et al (2003)Unified Medical Language System UMLS Multiple Bodenreider (2004)Entrez Gene ?
Proteins Maglott et al (2005)Automatically generated dictionary SHI Proteins Shi and Campagne (2005)Jochem JOCHEM Multiple Hettne et al (2009)Turku Event Corpus TURKU Proteins and biomolecular events Bjo?rne et al (2010)Arizona Disease Corpus AZDC Diseases Chowdhury and Lavelli (2010)LINNAEUS Dictionary LINNAEUS Species Gerner et al (2010)Webster?s International Dictionary WID Multiple ?Table 2: Lexical resources gathered for our experimentsResource Unique Entries DatabasesGO BP 67,411 4GO CC 5,993 4GO MF 55,595 4PIR 691,577 1UMLS 5,902,707 135Entrez Gene 3,602,757 5SHI 61,676 1CHEBI 187,993 1CHEMICAL 1,527,751 1TURKU PROT 4,745,825 1TURKU TRIG 130,139 10AZDC 1,195 1LINNAEUS 3,119,005 1WID 235,802 1Total: 20, 335, 426 170Table 3: Statistics per dictionary resource3.2 CorporaTo evaluate our approach we need a variety of cor-pora annotated with multiple semantic categories.For this purpose we selected the six corpora listedin Table 4.The majority of our corpora are available in thecommon stand-off style format introduced for theBioNLP 2009 Shared Task (BioNLP?09 ST) (Kimet al, 2009).
The remaining two, NLPBA andCALBC CII, were converted into the BioNLP?09 STformat so that we could process all resources in thesame manner for our experimental set-up.In addition to physical entity annotations, theGREC, EPI, ID and GENIA corpora incorporateevent trigger annotations (e.g.
Gene RegulatoryEvent (GRE) for GREC).
These trigger expressionscarry with them a specific semantic type (e.g.
?in-teract?
can carry the semantic type BINDING forGENIA), allowing us to enrich the data sets withadditional semantic categories by including thesetypes in our dataset as distinct semantic categories.This gave us the following increase in semantic cat-egories: GREC one, EPI 15, ID ten, GENIA nine.The original GREC corpus contains an exception-ally wide array of semantic categories.
While thisis desirable for evaluating the performance of ourapproach under different task settings, the sparsityof the data is a considerable problem; the majorityof categories do not permit stable evaluation as theyhave only a handful of annotations each.
To alleviatethis problem we used the five ontologies defined inthe GREC annotation guidelines5, collapsing the an-notations into five semantic super categories to cre-ate a resource we refer to as Super GREC.
This pre-processing conforms with how the categories wereused when annotating the GREC corpus (Thompsonet al, 2009).
This resource contains sufficient anno-tations for each semantic category to enable evalua-tion on a category-by-category basis.
Also, for thepurpose of our experiments we removed all ?SPAN?type annotations since they themselves carry no se-mantic information (cf.
GREC annotation guide-lines).CALBC CII contains 75,000 documents, whichis more than enough for our experiments.
In orderto maintain balance in size between the resources inour experiments, we sampled a random 5,000 docu-ments and used these as our CALBC CII dataset.5http://www.nactem.ac.uk/download.php?target=GREC/Event annotation guidelines.pdf139Name Abbreviation PublicationBioNLP/NLPBA 2004 Shared Task Corpus NLPBA Kim et al (2004)Gene Regulation Event Corpus GREC Thompson et al (2009)Collaborative Annotation of a Large Biomedical Corpus CALBC CII Rebholz-Schuhmann et al (2010)Epigenetics and Post-Translational Modifications EPI Ohta et al (2011)Infectious Diseases Corpus ID Pyysalo et al (2011)Genia Event Corpus GENIA Kim et al (2011)Table 4: Corpora used for evaluation3.3 Corpus StatisticsIn this section we present statistics for each of ourdatasets.
For resources with a limited number of se-mantic categories we use pie charts to illustrate theirdistribution (Figure 1).
For the other corpora we usetables to illustrate this.
Tables for the corpora forwhich pie charts are given has been left out due tospace requirements.The NLPBA corpus (Figure 1a) with 59,601 to-kens annotated, covers five semantic categories, witha clear majority of protein annotations.
WhileNLPBA contains several semantic categories, theyare closely related, which is expected to pose chal-lenges for disambiguation.
This holds in particularfor proteins, DNA and RNA, which commonly sharenames.Our collapsed version of GREC, Super GREC(see Figure 1b), contains 6,777 annotated tokens andcovers a total of six semantic categories: RegulatoryEvent (GRE), nucleic acids, proteins, processes, liv-ing system and experimental.
GREC is an interest-ing resource in that its classes are relatively distinctand four of them are evenly distributed.CALBC CII is balanced among its annotated cat-egories, as illustrated in Figure 1c.
The 6,433 to-kens annotated are of the types: proteins and genes(PRGE), species (SPE), disorders (DISO) and chem-icals and drugs (CHED).
We note that we have in-troduced lexical resources covering each of theseclasses (Section 3.1).For the BioNLP?11 ST resources EPI (Table 5),GENIA (Figure 1d and contains 27,246 annotatedtokens) and ID (Table 6), we observe a very skeweddistribution due to our decision to include eventtypes as distinct classes; The dominating class forall the datasets are proteins.
For several of thesecategories, learning accurate disambiguation is ex-Type Ratio AnnotationsAcetylation 2.3% 294Catalysis 1.4% 186DNA demethylation 0.1% 18DNA methylation 2.3% 301Deacetylation 0.3% 43Deglycosylation 0.2% 26Dehydroxylation 0.0% 1Demethylation 0.1% 12Dephosphorylation 0.0% 3Deubiquitination 0.1% 13Entity 6.6% 853Glycosylation 2.3% 295Hydroxylation 0.9% 116Methylation 2.5% 319Phosphorylation 0.9% 112Protein 77.7% 10,094Ubiquitination 2.3% 297Total: 12,983Table 5: Semantic categories in EPIpected to be very challenging if not impossible dueto sparsity: For example, Dehydroxylation in EPIhas a single annotation.ID is of particular interest since it contains a con-siderable amount of annotations for more than onephysical entity category, including in addition toprotein also organism and a minor amount of chem-ical annotations.4 ExperimentsIn this section we introduce our experimental set-upand discuss the outcome of our experiments.4.1 Experimental Set-upTo ensure that our results are not biased by over-fitting on a specific set of data, all data sets wereseparated into training, development and test sets.140(a) NLPBA(b) Super GREC(c) CALBC CII(d) GENIAFigure 1: Semantic category distributionsNLPBA defines only a training and test set, GRECand CALBC CII are provided as resources and lackany given division, and for the BioNLP?11 ST datathe test sets are not distributed.
Thus, we combinedall the available data for each dataset and separatedthe documents into fixed sets with the following ra-tios: 1/2 training, 1/4 development and 1/4 test.Type Ratio AnnotationsBinding 1.0% 102Chemical 6.8% 725Entity 0.4% 43Gene expression 3.3% 347Localization 0.3% 36Negative regulation 1.6% 165Organism 25.5% 2,699Phosphorylation 0.5% 54Positive regulation 2.5% 270Process 8.0% 843Protein 43.1% 4,567Protein catabolism 0.0% 5Regulation 1.8% 188Regulon-operon 1.1% 121Transcription 0.4% 47Two-component-system 3.7% 387Total: 10,599Table 6: Semantic categories in IDWe use a total of six classifiers for our experi-ments.
First, a naive baseline (Naive): a majorityclass voter with a memory based on the exact textof the textual span.
The remaining five are ma-chine learning classifiers trained using five differ-ent feature sets: gazetteer features constituting strictstring matching towards our SimString databases(Gazetteer), SimString features generated from ourSimString databases (SimString), the span internalfeatures listed in Table 1 (Internal), the span inter-nal and gazetteer features (Internal-Gazetteer) andthe span internal and SimString features (Internal-SimString).We evaluate performance using simple instance-level accuracy (correct classifications / all classifica-tions).
Results are represented as learning curves foreach data set.4.2 ResultsFrom our experiments we find that ?
not surpris-ingly ?
the performance of the Naive, Gazetteer andSimString classifiers alone is comparatively weak.Their performance is illustrated in Figure 2.
We canbriefly summarize the results for these methods bynoting that the SimString classifier outperforms theGazetteer by a large margin for every dataset.6 From6Due to space restrictions we do not include further analysisor charts.141Figure 2: SimString, Gazetteer and Naive for IDFigure 3: Learning curve for NLPBAhere onwards we focus on the performance of the In-ternal classifier in combination with Gazetteer andSimString features.For NLPBA (Figure 3), GENIA (Figure 4) and ID(Figure 5) our experiments show no clear systematicbenefit from either SimString or Gazetteer features.For Super GREC (Figure 6) and EPI (Figure 7)classifiers with Gazetteer and SimString featuresconsistently outperform the Internal classifier, andthe SimString classifier further shows some benefitover Gazetteer for EPI.The only dataset for which we see a clear benefitfrom SimString features over Gazetteer and Internalis for CALBC CII (Figure 8).5 Discussion and ConclusionsWhile we expected to see clear benefits from bothusing Gazetteers and SimString features, our exper-Figure 4: Learning curve for GENIAFigure 5: Learning curve for IDiments returned negative results for the majority ofthe corpora.
For NLPBA, GENIA and ID we areaware that most of the instances are either proteinsor belong to event trigger classes for which we maynot have had adequate lexical resources for disam-biguation.
By contrast, for Super GREC there areseveral distinct classes for which we expected lex-ical resources to have fair coverage for SimStringand Gazetteer features.
While an advantage over In-ternal was observed for Super GREC, SimString fea-tures showed no benefit over Gazetteer features.
Themethods exhibited the expected result on only one ofthe six corpora, CALBC CII, where there is a clearadvantage for Gazetteer over Internal and a furtherclear advantage for SimString over Gazetteer.Disappointingly, we did not succeed in establish-ing a clear improvement for more than one of the sixcorpora.
Although we have not been successful in142Figure 6: Learning curve for Super GRECFigure 7: Learning curve for EPIproving our initial hypothesis we argue that our re-sults calls for further study due to several concernsraised by the results remaining unanswered.
It maybe that our notion of distance to lexical resource en-tries is too naive.
A possible future direction wouldbe to compare the query string to retrieved results us-ing a method similar to that of Tsuruoka and Tsujii(2003).
This would enable us to retain the advantageof fast approximate string matching, thus being ableto utilise larger lexical resources than if we were tocalculate sophisticated alignments for each lexicalentry.Study of the confusion matrices revealed thatsome event categories such as negative regulation,positive regulation and regulation for ID are com-monly confused by the classifiers.
Adding addi-tional resources or contextual features may alleviatethese problems.Figure 8: Learning curve for CALBC CIITo conclude, we have found a limited advantagebut failed to establish a clear, systematic benefitfrom approximate string matching for semantic classdisambiguation.
However, we have demonstratedthat approximate string matching can be used to gen-erate novel features for classifiers and allow for theutilisation of large scale lexical resources in new andpotentially interesting ways.
It is our hope that bymaking our findings, resources and implementationavailable we can help the BioNLP community toreach a deeper understanding of how best to incor-porate our proposed features for semantic categorydisambiguation and related tasks.Our system and collection of resources are freelyavailable for research purposes at http://github.com/ninjin/simsemAcknowledgementsThe authors would like to thank Dietrich Rebholz-Schuhmann and the CALBC organisers for allowingus the use of their data.
and Jari Bjo?rne for answer-ing questions regarding the Turku Event Corpus.
Wewould also like to thank the anonymous reviewersand Luke McCrohon for their insightful and exten-sive feedback, which has considerably helped us toimprove this work.
Lastly the first author wouldlike to thank Makoto Miwa and Jun Hatori for theirtimely and helpful advice on machine learning meth-ods.This work was supported by the Swedish RoyalAcademy of Sciences and by Grant-in-Aid for Spe-cially Promoted Research (MEXT, Japan).143ReferencesM.
Ashburner, C.A.
Ball, J.A.
Blake, D. Botstein, H. But-ler, J.M.
Cherry, A.P.
Davis, K. Dolinski, S.S. Dwight,J.T.
Eppig, et al 2000.
Gene ontology: tool for theunification of biology.
The Gene Ontology Consor-tium.
Nature genetics, 25(1):25.E.
Beisswanger, M. Poprat, and U. Hahn.
2008.
LexicalProperties of OBO Ontology Class Names and Syn-onyms.
In 3rd International Symposium on SemanticMining in Biomedicine.J.
Bjo?rne, F. Ginter, S. Pyysalo, J. Tsujii, andT.
Salakoski.
2010.
Scaling up biomedical event ex-traction to the entire PubMed.
In Proceedings of the2010 Workshop on Biomedical Natural Language Pro-cessing, pages 28?36.
Association for ComputationalLinguistics.O Bodenreider.
2004.
The unified medical language sys-tem (umls): integrating biomedical terminology.
Nu-cleic Acids Research, 32:D267?D270.M.F.M.
Chowdhury and A. Lavelli.
2010.
Disease Men-tion Recognition with Specific Features.
ACL 2010,page 83.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.M.
Gerner, G. Nenadic, and C.M.
Bergman.
2010.LINNAEUS: A species name identification system forbiomedical literature.
BMC bioinformatics, 11(1):85.K.M.
Hettne, R.H. Stierum, M.J. Schuemie, P.J.M.
Hen-driksen, B.J.A.
Schijvenaars, E.M. Mulligen, J. Klein-jans, and J.A.
Kors.
2009.
A dictionary to identifysmall molecules and drugs in free text.
Bioinformat-ics, 25(22):2983.Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,Yuka Tateisi, and Nigel Collier.
2004.
Introductionto the bio-entity recognition task at JNLPBA.
In Pro-ceedings of the International Joint Workshop on Nat-ural Language Processing in Biomedicine and its Ap-plications (JNLPBA), pages 70?75.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 Shared Task on Event Extraction.In Proceedings of Natural Language Processing inBiomedicine (BioNLP) NAACL 2009 Workshop, pages1?9.Jin-Dong Kim, Yue Wang, Toshihasi Takagi, and Aki-nori Yonezawa.
2011.
Overview of genia eventtask in bionlp shared task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.R.
Leaman and G. Gonzalez.
2008.
BANNER: an exe-cutable survey of advances in biomedical named entityrecognition.
In Pacific Symposium on Biocomputing,volume 13, pages 652?663.
Citeseer.D.
Maglott, J. Ostell, K.D.
Pruitt, and T. Tatusova.
2005.Entrez Gene: gene-centered information at NCBI.
Nu-cleic Acids Research, 33(suppl 1):D54.Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii.
2011.Overview of the Epigenetics and Post-translationalModifications (EPI) task of BioNLP Shared Task2011.
In Proceedings of the BioNLP 2011 WorkshopCompanion Volume for Shared Task, Portland, Oregon,June.
Association for Computational Linguistics.Naoaki Okazaki and Jun?ichi Tsujii.
2010.
Simple andefficient algorithm for approximate dictionary match-ing.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (Coling 2010),pages 851?859, Beijing, China, August.M.F.
Porter.
1993.
An algorithm for suffix stripping.Program: electronic library and information systems,14(3):130?137.Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-livan, Chunhong Mao, Chunxia Wang, Bruno So-bral, Jun?ichi Tsujii, and Sophia Ananiadou.
2011.Overview of the Infectious Diseases (ID) task ofBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.L.
Ratinov and D. Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning, pages 147?155.Association for Computational Linguistics.D.
Rebholz-Schuhmann, A.J.J.
Yepes, E.M. Van Mul-ligen, N. Kang, J. Kors, D. Milward, P. Corbett,E.
Buyko, E. Beisswanger, and U. Hahn.
2010.CALBC silver standard corpus.
Journal of bioinfor-matics and computational biology, 8(1):163?179.Y.
Sasaki, Y. Tsuruoka, J. McNaught, and S. Ananiadou.2008.
How to make the most of NE dictionaries instatistical NER.
BMC bioinformatics, 9(Suppl 11):S5.L.
Shi and F. Campagne.
2005.
Building a protein namedictionary from full text: a machine learning term ex-traction approach.
BMC bioinformatics, 6(1):88.L.
Tanabe, N. Xie, L. Thom, W. Matten, and W.J.Wilbur.
2005.
GENETAG: a tagged corpus forgene/protein named entity recognition.
BMC bioinfor-matics, 6(Suppl 1):S3.P.
Thompson, S.A. Iqbal, J. McNaught, and S. Anani-adou.
2009.
Construction of an annotated corpusto support biomedical information extraction.
BMCbioinformatics, 10(1):349.144M.
Torii, Z. Hu, C.H.
Wu, and H. Liu.
2009.
BioTagger-GM: a gene/protein name recognition system.
Jour-nal of the American Medical Informatics Association,16(2):247.Y.
Tsuruoka and J. Tsujii.
2003.
Boosting precision andrecall of dictionary-based protein name recognition.In Proceedings of the ACL 2003 workshop on Naturallanguage processing in biomedicine-Volume 13, pages41?48.
Association for Computational Linguistics.Yue Wang, Jin-Dong Kim, Rune Saetre, Sampo Pyysalo,and Jun?ichi Tsujii.
2009.
Investigating heteroge-neous protein annotations toward cross-corpora uti-lization.
BMC Bioinformatics, 10(1):403.C.H.
Wu, L.S.L.
Yeh, H. Huang, L. Arminski, J. Castro-Alvear, Y. Chen, Z. Hu, P. Kourtesis, R.S.
Ledley, B.E.Suzek, et al 2003.
The protein information resource.Nucleic Acids Research, 31(1):345.145
