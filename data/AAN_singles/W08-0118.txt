Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 104?111,Columbus, June 2008. c?2008 Association for Computational LinguisticsOptimal Dialog in Consumer-Rating Systems using a POMDP FrameworkZhifei LiCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USAzhifei.work@gmail.comPatrick Nguyen, Geoffrey ZweigMicrosoft Corporation1 Microsoft Way,Redmond, WA 98052, USA{panguyen,gzweig}@microsoft.comAbstractVoice-Rate is an experimental dialog systemthrough which a user can call to get prod-uct information.
In this paper, we describean optimal dialog management algorithm forVoice-Rate.
Our algorithm uses a POMDPframework, which is probabilistic and cap-tures uncertainty in speech recognition anduser knowledge.
We propose a novel methodto learn a user knowledge model from a reviewdatabase.
Simulation results show that thePOMDP system performs significantly betterthan a deterministic baseline system in termsof both dialog failure rate and dialog interac-tion time.
To the best of our knowledge, ourwork is the first to show that a POMDP canbe successfully used for disambiguation in acomplex voice search domain like Voice-Rate.1 IntroductionIn recent years, web-based shopping and rating sys-tems have provided a valuable service to consumersby allowing them to shop products and share theirassessments of products online.
The use of thesesystems, however, requires access to a web interface,typically through a laptop or desktop computer, andthis restricts their usefulness.
While mobile phonesalso provide some web access, their small screensmake them inconvenient to use.
Therefore, therearises great interests in having a spoken dialog in-terface through which a user can call to get productinformation (e.g., price, rating, review, etc.)
on thefly.
Voice-Rate (Zweig et al, 2007) is such a sys-tem.
Here is a typical scenario under which showsthe usefulness of the Voice-Rate system.
A user en-ters a store and finds that a digital camera he hasnot planned to buy is on sale.
Before he decidesto buy the camera, he takes out his cell phone andcalls Voice-Rate to see whether the price is reallya bargain and what other people have said aboutthe camera.
This helps him to make a wise deci-sion.
The Voice-Rate system (Zweig et al, 2007) in-volves many techniques, e.g., information retrieval,review summarization, speech recognition, speechsynthesis, dialog management, etc.
In this paper, wemainly focus on the dialog management component.When a user calls Voice-Rate for the informationof a specific product, the system needs to identify,from a database containing millions of products, theexact product the user intends.
To achieve this, thesystem first solicits the user for the product name.Using the product name as a query, the system thenretrieves from its database a list of products relatedto the query.
Ideally, the highest-ranked productshould be the one intended by the user.
In reality,this is often not the case due to various reasons.
Forexample, there might be a speech recognition erroror an information retrieval ranking error.
Moreover,the product name is usually very ambiguous in iden-tifying an exact product.
The product name that theuser says may not be exactly the same as the namein the product database.
For example, while the usersays ?Canon Powershot SD750?, the exact namein the product database may be ?Canon PowershotSD750 Digital Camera?.
Even the user says the ex-act name, it is possible that the same name may becorresponding to different products in different cat-egories, for instance books and movies.Due to the above reasons, whenever the Voice-Rate system finds multiple products matching theuser?s initial speech query, it initiates a dialog proce-dure to identify the intended product by asking ques-tions about the products.
In the product database,104many attributes can be used to identify a product.For example, a digital camera has the product name,category, brand, resolution, zoom, etc.
Given a listof products, different attributes may have differentability to distinguish the products.
For example, ifthe products belong to many categories, the categoryattribute is very useful to distinguish the products.
Incontrast, if all the products belong to a single cate-gory, it makes no sense to ask a question on the cat-egory.
In addition to the variability in distinguishingproducts, different attributes may require differentknowledge from the user in order for them to an-swer questions about these attributes.
For example,while most users can easily answer a question oncategory, they may not be able to answer a questionon the part number of a product, though the partnumber is unique and perfect to distinguish prod-ucts.
Other variabilities are in the difficulty that theattributes impose on speech recognition and speechsynthesis.
Clearly, given a list of products and a setof attributes, what questions and in what order to askis essential to make the dialog successful.
Our goalis to dynamically find such important attributes ateach stage/turn.The baseline system (Zweig et al, 2007) asksquestions only on product name and category.
Theorder of questions is fixed: first ask questions onproduct category, and then on name.
Moreover, itis deterministic and does not model uncertainly inspeech recognition and user knowledge.
Partiallyobservable Markov decision process (POMDP) hasbeen shown to be a general framework to capture theuncertainty in spoken dialog systems.
In this paper,we present a POMDP-based probabilistic system,which utilizes rich product information and capturesuncertainty in speech recognition and user knowl-edge.
We propose a novel method to learn a userknowledge model from a review database.
Our sim-ulation results show that the POMDP-based systemimproves the baseline significantly.To the best of our knowledge, our work is the firstto show that a POMDP can be successfully used fordisambiguation in a complex voice search domainlike Voice-Rate.2 Voice-Rate Dialog System OverviewFigure 1 shows the main flow in the Voice-Rate sys-tem with simplification.
Specifically, when a usercalls Voice-Rate for the information of a specificYesBeginInformation RetrievalDialog ManagerEndInitial Speech QueryList of ProductsCorrupted User ActionHumanSpeech recognizerUser ActionFoundproduct?
NoPlay RatingQuestion?
Intended productFigure 1: Flow Chart of Voice-Rate SystemStep-1: remove products that do not matchthe user actionStep-2: any category question to ask?yes: ask the question and returnno: go to step-3Step-3: ask a product name questionTable 1: Baseline Dialog Manager Algorithmproduct, the system first solicits the user for theproduct name.
Treating the user input as a queryand the product names in the product database asdocuments, the system retrieves a list of productsthat match the user input based on TF-IDF mea-sure.
Then, the dialog manager dynamically gener-ates questions to identify the specific intended prod-uct.
Once the product is found, the system playsback its rating information.
In this paper, we mainlyfocus on the dialog manager component.Baseline Dialog Manager: Table 1 shows thebaseline dialog manager.
In Step-1, it removes allthe products that are not consistent with the user re-sponse.
For example, if the user answers ?camera?when given a question on category, the system re-moves all the products that do not belong to category?camera?.
In Step-2 and Step-3, the baseline systemasks questions about product name and product cat-egory, and product category has a higher priority.3 Overview of POMDP3.1 Basic DefinitionsA Partially Observable Markov Decision Process(POMDP) is a general framework to handle uncer-tainty in a spoken dialog system.
Following nota-105tions in Williams and Young (2007), a POMDP isdefined as a tuple {S,A, T,R,O,Z, ?,~b0} where Sis a set of states s describing the environment; A isa set of machine actions a operating on the environ-ment; T defines a transition probability P (s?
|s, a);R defines a reward function r(s, a); O is a set of ob-servations o, and an observation can be thought asa corrupted version of a user action; Z defines anobservation probability P (o?
|s?
, a); ?
is a geometricdiscount factor; and~b0 is an initial belief vector.The POMDP operates as follows.
At each time-step (a.k.a.
stage), the environment is in some unob-served state s. Since s is not known exactly, a distri-bution (called a belief vector ~b) over possible statesis maintained where~b(s) indicates the probability ofbeing in a particular state s. Based on the current be-lief vector ~b, an optimal action selection algorithmselects a machine action a, receives a reward r, andthe environment transits to a new unobserved states?
.
The environment then generates an observationo?
(i.e., a user action), after which the system updatethe belief vector ~b.
We call the process of adjustingthe belief vector~b at each stage ?belief update?.3.2 Applying POMDP in PracticeAs mentioned in Williams and Young (2007), it isnot trivial to apply the POMDP framework to aspecific application.
To achieve this, one normallyneeds to design the following three components:?
State Diagram Modeling?
Belief Update?
Optimal Action SelectionThe state diagram defines the topology of thegraph, which contains three kinds of elements: sys-tem state, machine action, and user action.
To drivethe transitions, one also needs to define a set ofmodels (e.g., user goal model, user action model,etc.).
The modeling assumptions are application-dependent.
The state diagram, together with themodels, determines the dynamics of the system.In general, the belief update depends on the ob-servation probability and the transition probability,while the transition probability itself depends on themodeling assumptions the system makes.
Thus, theexact belief update formula is application-specific.Optimal action selection is essentially an opti-mization algorithm, which can be defined as,a?
= argmaxa?AG(P (a)), (1)where A refers to a set of machine actions a.Clearly, the optimal action selection requires threesub-components: a goodness measure function G, aprediction algorithm P , and a search algorithm (i.e.,the argmax operator).
The prediction algorithm isused to predict the behavior of the system in thefuture if a given machine action a was taken.
Thesearch algorithm can use an exhaustive linear searchor an approximated greedy search depending on thesize of A (Murphy, 2000; Spaan and Vlassis, 2005).4 POMDP Framework in Voice-RateIn this section, we present our instantiation ofPOMDP in the Voice-Rate system.4.1 State Diagram Modeling4.1.1 State Diagram DesignTable 2 summarizes the main design choices inthe state diagram for our application, i.e., identifyingthe intended product from a large list of products.As in Williams and Young (2007), we incorporateboth the user goal (i.e., the intended product) andthe user action in the system state.
Moreover, to ef-ficiently update belief vector and compute optimalaction, the state space is dynamically generated andpruned.
In particular, instead of listing all the possi-ble combinations between the products and the useractions, at each stage, we only generate states con-taining the products and the user actions that are rel-evant to the last machine action.
Moreover, at eachstage, if the belief probability of a product is smallerthan a threshold, we prune out this product and allits associated system states.
Note that the intendedproduct may be pruned away due to an overly largethreshold.
In the simulation, we will use a develop-ment set to tune this threshold.As shown in Table 2, five kinds of machine ac-tions are defined.
The questions on product namesare usually long, imposing difficulty in speech syn-thesis/recgonition and user input.
Thus, short ques-tions (e.g., questions on category or simple at-tributes) are preferable.
This partly motivate us toexploit rich product information to help the dialog.Seven kinds of user actions are defined as shownin Table 2.
Among them, the user actions ?others?,?not related?, and ?not known?
are special.
Specif-ically, to limit the question length and to ensure the106Component Design CommentsSystem State (Product, User action) e.g., (HP Computer, Category: computer)Machine Action Question on Category e.g., choose category: Electronics, Movie, BookQuestion on Product name e.g., choose product name: Canon SD750 digital cam-era, Canon Powershot A40 digital camera, CanonSD950 digital camera, OthersQuestion on Attribute e.g., choose memory size: 64M, 128M, 256MConfirmation question e.g., you want Canon SD750 camera, yes or no?Play Rating e.g., I think you want Canon SD750 digital camera,here is the rating!User Action Category e.g., MovieProduct name e.g., Canon SD750 digital cameraAttribute value e.g., memory size: 64MOthers used when a question has too many possible optionsYes/No used for a confirmation questionNot related used if the intended product is unrelated to the questionNot known used if the user does not have required knowledge toanswer the questionTable 2: State Diagram Design in Voice-Ratehuman is able to memorize all the options, we re-strict the number of options in a single question to athreshold N (e.g., 5).
Clearly, given a list of prod-ucts and a question, there might be more than N pos-sible options.
In such a case, we need to merge someoptions into the ?others?
class.
The third example inTable 2 shows an example with the ?others?
option.One may exploit a clustering algorithm (e.g., an it-erative greedy search algorithm) to find an optimalmerge.
In our system, we simply take the top-(N -1)options (ranked by the belief probabilities) and treatall the remaining options as ?others?.The ?not related?
option is required when somecandidate products are irrelevant to the question.
Forexample, when the system asks a question regardingthe attribute ?cpu speed?
while the products containboth books and computers, the ?not related?
optionis required in case the intended product is a book.Lastly, while some attributes are very useful todistinguish the products, a user may not have enoughknowledge to answer a question on these attributes.For example, while there is a unique part number foreach product, however, the user may not know theexact part number for the intended product.
Thus,?not known?
option is required whenever the systemexpects the user is unable to answer the question.4.1.2 ModelsWe assume that the user does not change his goal(i.e., the intended product) along the dialog.
Wealso assume that the user rationally answers thequestion to achieve his goal.
Additionally, we as-sume that the speech synthesis is good enough suchthat the user always gets the right information thatthe system intends to convey.
The two main mod-els that we consider include an observation modelthat captures speech recognition uncertainty, and auser knowledge model that captures the variabilityof user knowledge required for answering questionson different attributes.Observation Model: Since the speech recogni-tion engine we are using returns only a one-best andits confidence value C ?
[0, 1].
We define the obser-vation function as follows,P (a?u|au) ={C if a?u = au,1?C|Au|?1 otherwise.
(2)where au is the true user action, a?u is the speechrecognition output (i.e., corrupted user action), andAu is the set of user actions related to the last ma-chine action.User Knowledge Model: In most of the appli-cations (Roy et al, 2000; Williams, 2007) where107the POMDP framework got applied, it is normallyassumed that the user needs only common sense toanswer the questions asked by the dialog system.Our application is more complex as the product in-formation is very rich.
A user may have differentdifficulty in answering different questions.
For ex-ample, while a user can easily answer a question oncategory, he may not be able to answer a questionon the part number.
Thus, we define a user knowl-edge model to capture such uncertainty.
Specifically,given a question (say am) and an intended product(say gu) in the user?s mind, we want to know howlikely the user has required knowledge to answer thequestion.
Formally, the user knowledge model is,P (au|gu, am) =????
?P (unk|gu, am) if au=unk,1?
P (unk|gu, am) if au=truth,0 otherwise.
(3)where unk represents the user action ?not known?.Clearly, given a specific product gu and a specificquestion am, there is exactly one correct user ac-tion (represented by truth in Equation 3), and itsprobability is 1 ?
P (unk|gu, am).
Now, to obtaina user knowledge model, we only need to obtainP (unk|gu, am).
As shown in Table 2, there are fourkinds of question-type machine actions am.
We as-sume that the user always has knowledge to answera question regarding the category and product name,and thus P (unk|gu, am) for these types of machineactions are zero regardless of what the specific prod-uct gu is.
Therefore, we only need to considerP (unk|gu, am) when am is a question about an at-tribute (say attr).
Moreover, since there are millionsof products, to deal with the data sparsity issue, weassume P (unk|gu, am) does not depends on a spe-cific product gu, instead it depends on only the cate-gory (say cat) of the product gu.
Therefore,P (unk|gu, am) ?
P (unk|cat,attr).
(4)Now, we only need to get the probabilityP (unk|cat,attr) for each attribute attr in each cate-gory cat.
To learn P (unk|cat,attr), one may collectdata from human, which is very expensive.
Instead,we learn this model from a database of online re-views for the products.
Our method is based on thefollowing intuition: if a user cares/knows about anattribute of a product, he will mention either the at-tribute name, or the attribute value, or both in hisreview of this product.
With this intuition, the occur-rence frequency of a given attr in a given categorycat is collected from the review database, followedby proper weighting, scaling and normalization, andthus P (unk|cat,attr) is obtained.4.2 Belief UpdateBased on the model assumptions in Section 4.1.2,the belief update formula for the state (gu, a?u) is,~b(gu, a?u) = (5)k ?
P (a?
?u|a?u)P (a?u|gu, am)?au?A(gu)~b(gu, au)where k is a normalization constant.
The P (a?
?u|a?u)is the observation function as defined in Equation 2,while P (a?u|gu, am) is the user knowledge model asdefined in Equation 3.
The A(gu) represents the setof user actions au related to the system states forwhich the intended product is gu.In our state representation, a single product guis associated with several states which differ in theuser action au, and the belief probability of gu is thesum of the probabilities of these states.
Therefore,even there is a speech recognition error or an un-intentional user mistake, the true product still getsa non-zero belief probability (though the true/idealuser action au gets a zero probability).
Moreover,the probability of the true product will get promotedthrough later iterations.
Therefore, our system haserror-handling capability, which is one of the majoradvantages over the deterministic baseline system.4.3 Optimal Action SelectionAs mentioned in Section 3.2, the optimal action se-lection involves three sub-components: a predictionalgorithm, a goodness measure, and a search algo-rithm.
Ideally, in our application, we should mini-mize the time required to successfully identify theintended product.
Clearly, this is too difficult asit needs to predict the infinite future and needs toencode the time into a reward function.
Therefore,for simplicity, we predict only one-step forward, anduse the entropy as a goodness measure1.
Formally,1Due to this approximation, one may argue that our modelis more like the greedy information theoretic model in Paek andChickering (2005), instead of a POMDP model.
However, webelieve that our model follows the POMDP modeling frame-work in general, though it does not involve reinforcement learn-ing currently.108the optimization function is as follows:a?
= argmina?AH(Products | a), (6)where H(Products | a) is the entropy over the beliefprobabilities of the products if the machine actiona was taken.
When predicting the belief vector us-ing Equation 5, we consider only the user knowledgemodel and ignore the observation function2.In the above, we consider only the question-typemachine actions.
We also need to decide whento take the play rating action such that the dialogwill terminate.
Specifically, we take the play ratingaction whenever the belief probability of the mostprobable product is greater than a threshold.
More-over, the threshold should depend on the number ofsurviving products.
For example, if there are fiftysurviving products and the most probable producthas a belief probability greater than 0.3, it is reason-able to take the play rating action.
This is not trueif there are only four surviving products.
Also notethat if we set the thresholds to too small values, thesystem may play the rating for a wrong product.
Wewill use a development set to tune these thresholds.4.3.1 Machine Action Filtering during SearchWe use an exhaustive linear search for the opera-tor argmin in Equation 6.
However, additional filter-ing during the search is required.Repeated Question: Since the speech responsefrom the user to a question is probabilistic, it is quitepossible that the system will choose the same ques-tion that has been asked in previous stages3.
Sinceour product information is very rich, many differ-ent questions have the similar capability to reduceentropy.
Therefore, during the search, we simply ig-nore all the questions asked in previous stages.
?Not Related?
Option: While reducing entropyhelps to reduce the confusion at the machine side, itdoes not measure the ?weirdness?
of a question tothe human.
For example, when the intended productis a book and the candidate products contain bothbooks and computers, it is quite possible that theoptimal action, based solely on entropy reduction,2Note that we ignore the observation function only in theprediction, not in real belief update.3In a regular decision tree, the answer to a question is deter-ministic.
It never asks the same question as that does not lead toany additional reduction of entropy.
This problem is also due tothe fact we do not have an explicit reward function.is a question on the attribute ?cpu speed?.
Clearly,such a question is very weird to the human as he islooking for a book that has nothing related to ?cpuspeed?.
Though the user may be able to choose the?not related?
option correctly after thinking for awhile, it degrades the dialog quality.
Therefore, fora given question, whenever the system predicts thatthe user will have to choose the ?not related?
optionwith a probability greater than a threshold, we sim-ply ignore such questions in the search.
Clearly, ifwe set the threshold as zero, we essentially elimi-nates the ?not related?
option.
That is, at each stage,we generate questions only on attributes that applyto all the candidate products.
Since we dynamicallyremove products whose probability is smaller thana threshold at each stage, the valid question set dy-namically expands.
Specifically, at the beginning,only very general questions (e.g., questions on cate-gory) are valid, then more refined questions becomevalid (e.g., questions on product brand), and finallyvery specific questions are valid (e.g, questions onproduct model).
This leads to very natural behav-ior in identifying a product, i.e., coarse to fine4.
Italso makes the system adapt to the user knowledge.Specifically, as the user demonstrates deeper knowl-edge of the products by answering the questions cor-rectly, it makes sense to ask more refined questionsabout the products.5 Simulation ResultsTo evaluate system performance, ideally one shouldask people to call the system, and manually collectthe performance data.
This is very expensive.
Al-ternatively, we develop a simulation method, whichis automatic and thus allow fast evaluation of thesystem during development5.
In fact, many designchoices in Section 4 are inspired by the simulation.5.1 Simulation ModelFigure 2 illustrates the general framework for thesimulation.
The process is very similar to that inFigure 1 except that the human user and the speech4While the baseline dialog manager achieves the similar be-havior by manually enforcing the order of questions, the sys-tem here automatically discovers the order of questions and thequestion set is much more richer than that in the baseline.5However, we agree that simulation is not without its limi-tations and the results may not precisely reflect real scenarios.109YesBeginInformation RetrievalDialog Manager?
Baseline?
POMDPEndInitial QueryList of ProductsCorrupted User ActionSimulated User?
Intended product?
User knowledge modelSimulatedSpeech RecognizerUser ActionFoundproduct?
NoPlay RatingQuestionFigure 2: Flow Chart in Simulationrecognizer are replaced with a simulated compo-nent, and that the simulated user has access to a userknowledge model.
In particular, we generate theuser action and its corrupted version using randomnumber generators by following the models definedin Equations 3 and 2, respectively.
We use a fixedvalue (e.g., 0.9) for C in Equation 2.Clearly, our goal here is not to evaluate the good-ness of the user knowledge model or the speech rec-ognizer.
Instead, we want to see how the probabilis-tic dialog manger (i.e., POMDP) performs comparedwith the deterministic baseline dialog manager, andto see whether the richer attribute information helpsto reduce the dialog interaction time.5.2 Data ResourcesIn the system, we use three data resources: a prod-uct database, a review database, and a query-clickdatabase.
The product database contains detailed in-formation for 0.2 million electronics and computerrelated products.
The review database is used forlearning the user knowledge model.
The query-click database contains 2289 pairs in the format (textquery, product clicked).
One example pair is (CanonPowershot A700, Canon Powershot A700 6.2MPdigital camera).
We divide it into a development set(1308 pairs) and a test set (981 pairs).5.3 Results on Information RetrievalFor each initial query, the information retrieval(IR) engine returns a list of top-ranked products.Whether the intended product is in the returned listdepends on the size of the list.
If the intended prod-uct is in the list, the IR successfully recalled theproduct.
Table 3 shows the correlation between therecall rate and the size of the returned list.
Clearly,the larger the list size is, the larger the recall rate is.One may notice that the IR recall rate is low.
Thisis because the query-click data set is very noisy, thatis, the clicked product may be nothing to do withthe query.
For example, (msn shopping, HandspringTreo 270) is one of the pairs in our data set.List Size Recall Rate (%)50 38.36100 41.46150 43.5Table 3: Information Retrieval Recall Rates on Test set5.4 Dialog System Configuration and TuningAs mentioned in Section 4, several parameters in thesystem are configurable and tunable.
Specifically,we set the max number of options in a question as5, and the threshold for ?not related?
option as zero.We use the development set to tune the following pa-rameters: the threshold of the belief probability be-low which the product is pruned, and the thresholdsabove which the most probable product is played.The parameters are tuned in a way such that no dia-log error is made on the development set.5.5 Results on Error HandlingEven the IR succeeds, the dialog system may notfind the intended product successfully.
In particu-lar, the baseline system does not have error handlingcapability.
Whenever the system makes a speechrecognition error or the user mistakenly answers aquestion, the dialog system fails (either plays the rat-ing for a wrong product or fails to find any product).On the contrary, our POMDP framework has errorhandling functionality due to its probabilistic na-ture.
Table 5 compares the dialog error rate betweenthe baseline and the POMDP systems.
Clearly,the POMDP system performs much better to han-dle errors.
Note that the POMDP system does noteliminate dialog failures on the test set because thethresholds are not perfect for the test set6.
This isdue to two reasons: the system may prune the in-tended product (reason-1), and the system may playthe rating for a wrong product (reason-2).6Note that the POMDP system does not have dialog failureson the development set as we tune the system in this way.110System Size Average MaxStages Characters Words Stages Characters WordsBaseline50 2.44 524.0 82.3 11 2927 546100 3.37 765.4 120.4 25 7762 1369150 3.90 906.4 143.0 30 9345 1668POMDP50 1.57 342.8 54.3 4 2659 466100 2.36 487.9 76.6 18 3575 597150 2.59 541.3 85.0 19 4898 767Table 4: Interaction Time Results on Test SetSize Baseline POMDP (%)(%) Total Reason-1 Reason-250 13.8 8.2 4.2 4.0100 17.7 2.7 1.2 1.5150 19.3 4.7 0.7 4.0Table 5: Dialog Failure Rate on Test Set5.6 Results on Interaction TimeIt is quite difficult to measure the exact interactiontime, so instead we measure it through the number ofstages/characters/words required during the dialogprocess.
Clearly, the number of characters is the onethat matches most closely to the true time.
Table 4reports the average and maximum numbers.
In gen-eral, the POMDP system performs much better thanthe baseline system.
One may notice the differencein the number of stages between the baseline andthe POMDP systems is not as significant as in thenumber of characters.
This is because the POMDPsystem is able to exploit very short questions whilethe baseline system mainly uses the product namequestion, which is normally very long.
The longquestion on product name also imposes difficulty inspeech synthesis, user input, and speech recognition,though this is not reflected in the simulation.6 ConclusionsIn this paper, we have applied the POMDP frame-work into Voice-Rate, a system through which auser can call to get product information (e.g., price,rating, review, etc.).
We have proposed a novelmethod to learn a user knowledge model from a re-view database.
Compared with a deterministic base-line system (Zweig et al, 2007), the POMDP systemis probabilistic and is able to handle speech recogni-tion errors and user mistakes, in which case the de-terministic baseline system is doomed to fail.
More-over, the POMDP system exploits richer product in-formation to reduce the interaction time required tocomplete a dialog.
We have developed a simulationmodel, and shown that the POMDP system improvesthe baseline system significantly in terms of both di-alog failure rate and dialog interaction time.
We alsoimplement our POMDP system into a speech demoand plan to carry out tests through humans.AcknowledgementThis work was conducted during the first author?sinternship at Microsoft Research; thanks to Dan Bo-hus, Ghinwa Choueiter, Yun-Cheng Ju, Xiao Li,Milind Mahajan, Tim Paek, Yeyi Wang, and DongYu for helpful discussions.ReferencesK.
Murphy.
2000.
A survey of POMDP solution tech-niques.
Technical Report, U. C. Berkeley.T.
Paek and D. Chickering.
2005.
The Markov assump-tion in spoken dialogue management.
In Proc of SIG-dial 2005.N.
Roy, J. Pineau, and S. Thrun.
2000.
Spoken dialogmanagement for robots.
In Proc of ACL 2000.M.
Spaan and N. Vlassis.
2005.
Perseus: randomizedpoint-based value iteration for POMDPs.
Journal ofArtificial Intelligence Research, 24:195-220.J.
Williams.
2007.
Applying POMDPs to DialogSystems in the Troubleshooting Domain.
In ProcHLT/NAACL Workshop on Bridging the Gap: Aca-demic and Industrial Research in Dialog Technology.J.
Williams and S. Young.
2007.
Partially ObservableMarkov Decision Processes for Spoken Dialog Sys-tems.
Computer Speech and Language 21(2): 231-422.G.
Zweig, P. Nguyen, Y.C.
Ju, Y.Y.
Wang, D. Yu, andA.
Acero.
2007.
The Voice-Rate Dialog System forConsumer Ratings.
In Proc of Interspeech 2007.111
