The BBN BYBLOS ContinuousSpeech Recognition SystemRichard SchwartzChris Barry, Yen-Lu Chow, Alan Deft, Ming-Whei Feng,Owen Kimball, Francis Kubala, John Makhoul, Jeffrey VandegriftBBN Systems and Technologies10 Moulton StreetCambridge, MA 02138AbstractIn this paper we describe the algorithms used in theBBN BYBLOS Continuous Speech Recognition system.The BYBLOS system uses context-dependent hiddenMarkov models of phonemes to provide a robust model ofphonetic coarticulation.
We provide an update of theongoing research aimed at improving the recognitionaccuracy.
In the first experiment we confirm the largeimprovement in accuracy that can be derived by usingspectral derivative parameters in the recognition.
Inparticular, the word error rate is reduced by a factor of two.Currently the system achieves a word error rate of 2.9%when tested on the speaker-dependent part of the standard1000-Word DARPA Resource Management Databaseusing the Word-Pair grammar supplied with the database.When no grammar was used, the error rate is 15.3%.Finally, we present a method for smoothing the discretedensities on the states of the HMM, which is intended toalleviate the problem of insufficient raining for detailedphonetic models.i.
IntroductionAt BBN we have been involved in the development ofSpoken Language Systems for almost wo decades.
As partof DARPA's Speech Understanding Research Programfrom 1971-1976, we developed a system that integratedcontinuous speech recognition with natural languageunderstanding in a 1000-word travel management task; wecall the system HWIM (Hear What I Mean).
As part ofanother DARPA program, we have been working since1982 on a more advanced speech recognition system basedon using Hidden Markov Models.
The result of this workis the BYBLOS Continuous Speech Recognition System.The basic algorithms used in the BYBLOSContinuous Speech Recognition system have beendescribed in several papers \[1, 2, 3\].
In Section 2 we give abrief review of the techniques currently used in theBYBLOS system.
The two features that have made thelargest improvements in recognition accuracy since 1982were the use of robust context-dependent phonetic models,and the addition of derivative spectral parameters inmultiple codebooks.
Each of these features used separatelyreduces the recognition error rate by a factor of two.
Takentogether, they reduce the error rate by a factor of four.
InSection 3 we present he latest recognition results for theBYBLOS system.
In particular, we compare therecognition results with and without spectral derivativeparameters.
We also demonstrate, by testing the system ontraining data, that the recognition accuracy is likely toimprove as more training data is made available.
Sinceseveral similar systems have provided test results on thisdatabase it is possible to determine the benefits ofparticular algorithms.
In particular, we compare the errorrate for using discrete densities with that using continuousdensities.
We also compare the recognition accuracy forspeaker-dependent models with that for speaker-independent models derived from a large number, ofspeakers.
Finally, in Section 4, we present a method forsmoothing the discrete densities on the states of the HNIM.The smoothing is intended to alleviate the problem ofinsufficient training for detailed phonetic models.2.
The BYBLOS systemThe BYBLOS system uses context-dependent hiddenMarkov models (HMM) of phonemes to provide a robustmodel of coarticulation \[ 1, 2\].
Each phoneme is typicallymodeled as a HMM with three states that correspondroughly to the acoustics of the beginning, middle, and endof the phoneme.
To model the acoustic coarticulationbetween phonemes, we define a separate HMM for eachphoneme in each of its possible contexts.
Since many ofthese phonetic ontexts do not occur frequently enough toallow robust estimation of model parameters, weinterpolate the detailed context-dependent phonetic modelswith models of the same phoneme that are dependent onless context.
In this way we derive the benefit of word-based models for words with sufficient training and thegenerality of phoneme-based models for the rest.
Forexample, we use triphone models that depend jointly on thepreceding and following phonemes, we use diphone modelsthat depend separately on the preceding or following94context, and we use context-independent models that arepooled across all instances of  the phoneme.
We have alsoexperimented with models of the phoneme that depend onthe particular word that the phoneme is in \[3\].
We averagethe probabilities of the different context-dependent modelswith weights that depend on the state within the phonemeand on the number of occurrences of each type of contextin the training set.With each state of the HMM we associate aconditional probability density of the spectral featuresgiven that state.
The basic spectral features are reel-scaledcepstral coefficients (MFCC)\[4\] and the log of thenormalized total energy.
We derive the MFCC by warpingthe log power spectrum of each frame of speech beforecomputing the cepstrum (by inverse Fourier transform).
Aportion of the training set of MFCC vectors is clustered toproduce a codebook of spectral prototypes \[5\].
Wetypically use a codebook with 256 prototypes.
Then foreach frame we find the index of the nearest vector quantizer(VQ) prototype.
The discrete probability density istherefore represented asa vector of 256 numbers indicatingthe probability of each VQ index given the state.The decoding algorithm used in the BYBLOS systemhas been described in \[2\].
The algorithm is a time-synchronous beam search for the most likely sequence ofwords, given the observed speech parameters.
Thealgorithm is similar to the commonly used Viterhialgorithm with the exception that, when performing thestate update within a word, the probability of being in aparticular state is derived from the sum of the probabilitiesat each of the preceding states.
This is contrasted with thestandard Viterbi algorithm, in which we use the maximumover the preceding states.
This algorithm more nearlycomputes the correct likelihood function for each sequenceof words and was found to result in a small but consistentimprovement over the standard Viterbi algorithm.
As withthe Viterbi algorithm, the search can be constrained by anyf'mite-state grammar.
It has also been used in a top-downsearch using context-free grammars.Mu l t ip le  CodebooksAs shown by Furui \[6\], even though the sequence ofspectral parameter vectors may be sufficient o reproduce areasonable facsimile of the original speech, it is beneficialto explicitly include the derivatives of the spectralparameters in the recognition algorithm.
To avoidproblems associated with trying to estimate probabilitydensities of large dimensional spaces, we use a separateVQ codebook and probability distribution for the steadystate and derivative parameter sets.
We multiply theprobabilities for the different parameter sets as if they wereindependent \[7l.
During the past year we modified theBYBLOS system to use multiple sets of features.Currently, the BYBLOS system uses three sets of spectralfeatures: 14 reel-scale cepstral coefficients, the 14derivatives of these parameters (computed as the derivativeof a linear fit to 5 successive frames), and a third setcontaining the normalized total energy and the derivative ofthe energy.3.
ResultsIn this section we present he recognition results forthe BYBLOS system under several different conditions.But first, we describe the database and the testingprocedure used for all the results in this paper.DARPA Resource Management DatabaseMost of the recent research with the system has beenperformed using the standard 1000-word DARPA ResourceManagement Database \[8\].
Tests were performed on thespeaker-dependent portion of the database which containsthe speech of 12 speakers.
The training set for eachspeaker consists of 600 sentences averaging eight words orthree seconds in length, for a total of about 30 minutes ofspeech.
There are two test sets of 100 sentences each.
Thefirst test set is designated as "development test" and wasdistributed by the National Bureau of Standards for formaltests.
25 sentences from 8 of the 12 speakers weredistributed in October, 1987; 25 different sentences fromall 12 speakers were distributed in May, 1988.
After thesetwo formal tests, all I00 of the development test sentenceswere released for development purposes.
The remaining100 test sentences, which were designated as "evaluationtest", were also divided into 25 sentence groups and arebeing distributed in a similar manner for further formaltesting.
The first set of 25 was distributed for February,1989.
In the remainder of this paper, we will refer to thedifferent formal test sets by their date of distribution, (e.g.the Oct. '87 test set, etc.
).In addition to the speech data itself, the database alsocontains a specification of two grammars to be used fortesting and testing procedures to be used to assure thatresults from different research sites can be compared.Recognition runs typically are performed using an artificial"Word-Pair" grammar with perplexity 60 that allows allpairs of word classes that appear in the database, and withno grammar or perplexity 1000.
The recognized strings ateautomatically aligned to the tree word strings, and thenumber of word substitutions, insertions, and deletions arecomputed.
The standard single-number measure ofperformance is the total word error, which is defined as%error = 100 substitutions + deletions + insertionstotal wordsWhen sentence rror rate is quoted, (typically only whenusing a grammar) it is defined as the percentage ofsentences with any error at all.95Mul t ip le  Codebook  Resu l tsWe compared the recognition accuracy when thesystem used 14 steady state cepstral coefficients with thatwhen it used three codebooks (including derivative andenergy parameters).
The comparison was made on the Oct.'87 test set of 8 speakers under the two grammarconditions.
Table 1 below contains the results of thiscomparison.
As can be seen, the use of derivative (andenergy) information reduced the error rate by about a factorof two under both grammar conditions.1 codebook3codebooksWord-Pair7.53.6\[ No Grammar32.418.0Table I: Recognition error rate with 1 codebookwith steady state parameters vs 3 codebooks withadded derivative and energy parameters.The results given above for 3 codebooks weredevelopment results, in that the t~st set had been usedseveral times.
Therefore, we present below in Table 2 theresults of testing the system on all 12 speakers on the May'88 and Feb '89 test sets for the first time, using the samephonetic word models as used above.The average word error rates were 3.4% and 2.9%when the Word-Pair grammar was used, and 16.2% and15.3% when no grammar was used.
The differenceMay '88Feb '89Word-Pair3.42.9No Grammar16.215.3Table 2: Recognition Results With and WithoutGrammar for May '88 and Feb '89 Test Setsbetween the error rates of the two test sessions is onlymarginally significant especially given the variationbetween speakers.
Table 3 below shows the detailedresults for the February '89 test sets for each of the 12speakers.
The table gives the percent substitution, deletion,and insertion errors, in addition to the total word andsentence rror rates.NO GRAMMAR WORD PA IR\] Word Word  i SentSub  Del  Ins Err Sub Del  Ins Err I~rrBEF 10.8 6.1 0.5 17.4 1.9 0.9 0.5 i 3.3 24.0CMR 14.6 3.3 3.3 21.2 2.8 0.9 0.5 I 4.2 24.0DAS 2.9 1.0 l 1.0 4.9 0.5 0.0 0.5 1.0 8.0DMS 8.6 2.3 1.1 12.0 I. I  1.1 0.0 2.2 12.0DTB 15.3 2.4 1.0 18.7 2.4 0.5 0.0 2.9 20.0DTD 13.1 2.3 1.4 16.8 2.3 0.0 0.9 3.2i 20.0ERS !
I7.7 2.9 2.9 33.5 1.1 1.7 0.6 3.4 16.0HXS I 6.1 1.9 1.9 9,9 0.9 1.4 0.0 2.3 16.0JWS i 7.7 1.7 0.0 0.4 1.7 0.4 0.0 2.1 20.0PGH I 8.8 1.4 2.8 15.0 1.8 0.5 0.0 2.3 20.0RKM 13.2 4.7 4.3 22.2 3.4!
1.3 0.4 5.1 36.0TAB 10.6 2.3 2.3 15.2 1.8 i 0.9 0.0 2.7 24.0IAvg 10.8 2.7 1.9 15.3 1.5 0.5 10.3 2.9 i 20.0Table 3: Detailed Recognition Results for Each ofthe Twelve Speakers on the Feb '89 Test Set.Results are given with and without he Word-Pairgrammar.
For each condition and speaker, the tableshows the percent substitution, deletion, andinsertion errors, and total word error.
Percentsentence rror is also given for the Word-Pairgrammar.Tes t  on Tra in ingIt is frequently instructive to measure the recognitionperformance of a system when it is tested on data that wasincluded m the training set.
In Table 4 below we comparethe word and sentence recognition error rate when thesystem is tested on the training set versus when it is testedon an independent test set.
The same acoustic models wereused in both cases.
Results are given for the Word-Pairgrammar only.As can be seen, when the system is tested on trainingdata, the error rates are very small.
This large difference inperformance indicates that there is not enough training datafor the number of free parameters we have in our phoneticmodels.
Therefore, we might expect that recognitionaccuracy would improve considerably as we add moretraining data.WordErrorSentenceErrorIndependentTest2.9 - 3.420.0TrainingDan0.52.7Table 4: Comparison of Results on IndependentTest vs on Training Data.
The Word-Pair grammarwas used.96Compar i son  of  MethodsSeveral other research groups have also reported theirrecognition results on this same database.
Since, in manycases, the algorithms differ in only one or two aspects, it ispossible to identify differences in performance withparticular aspects of a system.
In this section, we attemptto make two such comparisons: discrete vs continuousdensities, and speaker-dependent vs speaker-independentmodels.
The comparisons are made on the results providedfor the May '88 test set because the different systems weremost similar for this test set.
We note that each of thesesystems has evolved since testing on this particular test set,and as a result their results have improved considerably ascan be seen in the results presented for those systemselsewhere in this volume.The continuous peech recognition system developedat MIT Lincoln Labs by Doug Paul uses Gaussianprobability densities to represent each of the states of theHMM instead of the discrete densities used in BYBLOS.In most other respects, the two systems are quite similar.The recognition accuracy for the speaker-dependent test onthe May '88 test set was 5.5%, as compared with 3.4% forthe BYBLOS system.
It would appear, then, thatcontinuous HMM densities do not necessarily provideimproved results over discrete densities.Another comparison of interest is the relativeperformance of speaker-dependent models versus speaker-independent models.
While it is clear that, for any givenduration of training, a speaker-dependent model (trainedfor the particular speaker using the system) should alwaysresult in much higher recognition accuracy, the practicalquestion remains, "How much more training does aspeaker-independent system need to give the sameaccuracy as a speaker-dependent system?"
Three systemsthat are almost identical to BYBLOS have been used on thespeaker-independent portion of the Resource ManagementDatabase.
Two different raining sets have been used in thetests on the May '88 test set: one with 72 differentspeakers containing 2880 sentences, and a larger one with105 speakers containing 4200 sentences.
The test data usedwas the same as for the speaker-dependent test describedabove.When trained on 72 speakers the word error rate withthe Word-Pair grammar was 10.1% for the Sphinx systemof Carnegie Mellon University, I 1.4% for the Deciphersystem of Stanford Research Institute, and 13.1% for theLincoln Labs system.
The Sphinx system and the Deciphersystem both use discrete densities imilar to those used inBYBLOS.
When trained on 105 speakers, the error ratesfor Sphinx and the Lincoln system were 8.9% and 10.1%respectively.
Thus, the BYBLOS system with speaker-dependent training with five to seven times less trainingdata has roughly 1/2 to 1/3 the error rate of the speaker-independent trained systems.
It would be interesting tofred out how much additional speech is needed for speaker-independent training to result in the same performance as30-minute speaker-dependent training.4.
Robust Smoothing forDiscrete Probability DensitiesMuch of the research in speech recognition is devotedto improving the structure of the statistical model ofspeech.
Frequently, improving the model involvesincreasing the complexity or dimensionality of the model.For example, we use context-dependent phonetic models,which increases the number of models.
We add features,such as spectral derivatives, which increases thedimensionality of the feature space.
We use a non-parametric probability density function (pdf) to haveflexibility in the model, but we lose the benefit of thecompactness of a parametric model.
Each of theseimprovements comes with an increase in the effectivenumber of degrees of freedom in our model.Unfortunately, more training data is needed to estimatereliably the increased number of free parameters.Conversely, faced with a fixed amount of training data, wemust limit the number of free parameters or else our"improvements" will not be realized.As described above the BBN BYBLOS ContinuousSpeech Recognition system uses discrete nonparametricpdfs of context-dependent phonetic models.
Most of thesepdfs are trained with only a few tokens of speech (typicallybetween 1 and 1'0).
These discrete distributions worksurprisingly well, given the small amount of training.However, they are certainly prone to the problem ofspectral types that do not appear in the training set for agiven model, but are, in fact, likely to occur for that model.The results presented in Table 4 in Section 2 indicate thatthere is a large difference in recognition rate when thesystem is tested on the training data and on independenttest data.
Therefore, we tried to find a smoothing algorithmthat would reduce the number of probabilities that are lowpurely due to a lack of training.
Below we describe ageneral smoothing method based on using a probabilisticsmoothing matrix \[9\].For each state of a discrete HMM, we have a discreteprobability density function (pdf) defined over a fixed set,N, of spectral templates.
For example, in the BYBLOSsystem we typically use a vector quantization (VQ)codebook of size N=256 \[5\].
The index of the closesttemplate is referred to below as the VQ index or thespectral bin.
We can view the discrete pdf for each state sas a probability row vectorp(s) = \[P(klls), P(k21s) .
.
.
.
.
p(k~s)\],  (2)where P(kils) is the probability of spectral template ki at97state s. We can imagine that the probabilities of differentspectra re related in that, for each spectrum that has a highprobability for a given lxtf, there are several other spectrathat are also likely to have high probabilities.
These mightbe "nearby" spectra, or they might just be statisticallyrelated.
We represent this relation by p(kj4ki), theprobability that if spectrum ki occurs, the spectrum kj willoccur also.
The set of probabilities p(k~4k i) for all i and jform an NxN smoothing matrix, T, where Tij = p(k14ki).If we multiply the original pdf row vector p(s) by thesmoothing matrix, we get a smoothed pdf row vector.Psmooth(S) = Porig(S) ?
T. (3)In our experiments we use a separate smoothing matrix foreach phoneme.
This matrix is combined with thephoneme-independent ma rix to ensure robustness.The amount of training available for different modelsvaries considerably, from one or two tokens for themajority of the triphone-dependent models to hundreds oftokens for the more common models.
Clearly, we don'twant to smooth a model as much ff it was estimated from alarge number of training tokens.
Therefore we recombinethe smoothed pdf above with the original pdf using aweight w(s) that depends on the number of training tokensof the model.
Thus the final pdf used is given byp//nafls) = w(s)Porig(s) + \[l-w(s)\]Psmooth(S).
(4)The weight w is made proportional to the log of the numberof training tokens, N T.w(s) = rain\[0.99, 0.5 lOgl0NT(S)\] (5)This equation is illustrated in Figure 1.11.oo.so.o10 100Number  of  Occur rences  (N)Figure I: Weight w for original model as afunction of the number of training tokens, N TEstimating the MatrixWe have tried three techniques for estimating thesmoothing matrix: Parzen smoothing, self adaptationcooccurrence smoothing, and triphone cooccurrencesmoothing.
These methods were presented in a talk atArden House in May 1988 and are described in detail in\[10\].
Since the third method worked best in our initialexperiments, we will discuss only that method.After performing forward-backward training, we havea large number of context-dependent phonetic models.Most of these (about 2,500) are triphone-dependem models.Each model has three different pdfs.
These models containa record of all of the VQ-index spectra that occurred forone part (one state) of a particular triphone.
Thus,according to the Markov model, these spectra freelycooccur.
For each pdf of each triphone model we count allpermutations of two VQ spectra in that pdf, weighted bytheir probabilities and by the number of training tokens ofthe model.
Figure 2 illustrates this process for one pdf ofone modelNo.
of  Oeeurren?4.
l  27 112 198 2OI ? '
I ?
S .2 112 3.00 S.00 2.00 J, , !
.
t -  ??
.ojFigure 2: Triphone Cooccurrence MatrixEstimation.
pdf shown results in matrix incrementsshown.For example the pdf shown has VQ indices 27, 112,and 198 with probabilities 0.3, 0.5, 0.2 respectively.
Themodel occurred 20 times in the training set.
Therefore, weadd 0.3 * 0.5 * 20 = 3.0 to entries (27,112) and (112.27) mthe matrix.
As with the second method, we keep a separatematrix for each phoneme and one phoneme-independentmatrix.
Each row is normalized to create probabilisticmatrices.
A method similar to this was developedindependently by Lee \[11\].
However, in his method therewas only one smoothing matrix, instead of one for eachphoneme, and he estimated the matrix from context-independent models instead of triphone-dependent models.We believe that these differences result in too muchsmoothing.Recognition experiments using the word-pairgrammar were performed with and without triphonecooccurrence smoothing on all three test sets.
These resultsare shown below in Table 5.98Test SotTot  word ,trot rm ft.)mWord-Pair No GrammarBaseline Smooth Baseline SmoothOo  t~r~ ' I, 3.6 3.0 18.0 19.2 (8 spkrs) IMay '88(12 spkrs) 3.4* 2.7 16.2 * 15.2Feb.
'89(12 spkrs) 2.9 * 3.1 * 15.3 * 13.8 ** Official testTable 5: Recognition Results With and WithoutSmoothing5.
Conc lus ions2.3.4.5.We have described the BYBLOS Continuous SpeechRecognition System.
As expected, we found that addingthe derivative and energy parameters in separate codebooksreduced the error rate by a factor of two, relative to usingthe steady state spectral parameters alone.
The resulting 6.word error rate was 3.4% and 2.9% on two successiveformal tests.
We presented an algorithm for smoothingdiscrete probability densities when the training isinsufficient.
However the algorithm provided only a smallgain in recognition accuracy when 30 minutes were 7.available for training.
The HMM systems based onnonparametric discrete densities resulted in higher accuracythan the system that used continuous densities, leavingopen the question of whether it is harmful to quantize thespectral parameters.
The error rate of the speaker-dependent system when trained with 30 minutes of speech 8.was less than half that of similar speaker-independentsystems trained on over 100 speakers with five to seventimes the amount of speech.AcknowledgementThis work was supported by the Defense AdvancedResearch Projects Agency and monitored by the Office ofNaval Research under contract number N00014-85-C-0279.ReferencesR.M.
Schwartz,.Y.
Chow, S. Roucos, M. Krasner,and J. Makhoul, "Improved Hidden MarkovModeling of Phonemes for Continuous SpeechRecognition", IEEE Int.
Conf.
Acoust., Speech,Signal Processing, San Diego, CA, March 1984, pp.35.6.1-35.6.4.1.9.10.11.R.M.
Schwartz, Y.L.
Chow, O.A.
Kimball,S.
Roucos, M. Krasner, and J. Makhoul, "Context-Dependent Modeling for Acoustic-PhoneticRecognition of Continuous Speech", IEEE Int.Conf.
Acoust., Speech, Signal Processing, Tampa,FL, March 1985, pp.
1205-1208, Paper No.
31.3.Y.L.
Chow, R.M.
Schwartz, S. Roucos, O.A.Kimball, P.J.
Price, G.F. Kubala, M.O.
Dunham,M.A.
Krasner, and J. Makhoul, "The Role ofWord-Dependent Coartieulatory Effects in aPhoneme-Based Speech Recognition System",IEEE Int.
Conf.
Acoust., Speech, Signal Processing,Tokyo, Japan, April 1986, pp.
1593-1596, PaperNo.
30.9.1.S.
Davis and P. Mermelstein, "Comparison ofParametric Representations forMonosyllabic WordRecognition in Continuously Spoken Sentences",IEEE Trans.
Acoust., Speech, Signal Processing,Vol.
ASSP-28, No.
4, August 1980, pp.
357-366.J.
Makhoul, S. Roucos, and H. Gish, "VectorQuantization i Speech Coding", Proc.
IEEE, Vol.73, No.
11, November 1985, pp.
1551-1588, SpecialIssue on Man-Machine Speech Communication.S.
Fumi, "Speaker-Independent Isolated WordRecognition Based on Emphasized SpectralDynamics", IEEE Int.
Conf.
Acoust., Speech,Signal Processing, Tokyo, Japan, April 1986, pp.1991-1994, Paper no.
37.10.V.N.
Gupta, M. Lennig, and P. Mermelstein,"Integration of Acoustic Information in a LargeVocabulary Word Recognizer", IEEE Int.
Conf.Acoust., Speech.
Signal Processing, Dallas, TX,1987, pp.
697-700.P.
Price, W.M.
Fisher, J. Bemstein and D.S.
Pallett,"The DARPA 1000-Word Resource ManagementDatabase for Continous Speech Recognition",IEEE Int.
Conf.
Acoust., Speech, Signal Processing,New York, NY, April 1988, pp.
651-654.K.
Sugawara, M. Nishimura, K. Toshioka,M.
Okochi, and T. Kaneko, "Isolated WordRecognition Using Hidden Markov Models", IEEEInt.
Conf.
Acoust., Speech, Signal Processing,Tampa FL, March 1985, pp.
1.-4.R.
Schwartz, O. Kimball, F. Kubala, M. Feng, Y.L.Chow, C. Barry, J. Makhoul, "Robust SmoothingMethods for Discrete Hidden Markov Models",IEEE Int.
Conf.
Acoust., Speech, Signal Processing,Glasgow, Scotland, May 1989.K.F.
Lee, Large-Vocabulary Speaker-IndependentContinuous Speech Recognition: The SphinxSystem, PhD dissertation, Camagie-MellonUniversity, April 1988, CMU-CS-88-14899
