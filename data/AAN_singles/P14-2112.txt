Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 687?692,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsExponential Reservoir Sampling for Streaming Language ModelsMiles Osborne?School of InformaticsUniversity of EdinburghAshwin LallMathematics and Computer ScienceDenison UniversityBenjamin Van DurmeHLTCOEJohns Hopkins UniversityAbstractWe show how rapidly changing textualstreams such as Twitter can be modelled infixed space.
Our approach is based upona randomised algorithm called Exponen-tial Reservoir Sampling, unexplored bythis community until now.
Using languagemodels over Twitter and Newswire as atestbed, our experimental results based onperplexity support the intuition that re-cently observed data generally outweighsthat seen in the past, but that at times,the past can have valuable signals enablingbetter modelling of the present.1 IntroductionWork by Talbot and Osborne (2007), Van Durmeand Lall (2009) and Goyal et al (2009) consid-ered the problem of building very large languagemodels via the use of randomized data structuresknown as sketches.1While efficient, these struc-tures still scale linearly in the number of itemsstored, and do not handle deletions well: if pro-cessing an unbounded stream of text, with newwords and phrases being regularly added to themodel, then with a fixed amount of space, errorswill only increase over time.
This was pointedout by Levenberg and Osborne (2009), who inves-tigated an alternate approach employing perfect-hashing to allow for deletions over time.
Theirdeletion criterion was task-specific and based onhow a machine translation system queried a lan-guage model.
?Corresponding author: miles@inf.ed.ac.uk1Sketches provide space efficiencies that are measured onthe order of individual bits per item stored, but at the costof being lossy: sketches trade off space for error, where theless space you use, the more likely you will get erroneousresponses to queries.Here we ask what the appropriate selectioncriterion is for streaming data based on a non-stationary process, when concerned with an in-trinsic measure such as perplexity.
Using Twitterand newswire, we pursue this via a sampling strat-egy: we construct models over sentences based ona sample of previously observed sentences, thenmeasure perplexity of incoming sentences, all ona day by day, rolling basis.
Three sampling ap-proaches are considered: A fixed-width slidingwindow of most recent content, uniformly at ran-dom over the stream and a biased sample thatprefers recent history over the past.We show experimentally that a moving windowis better than uniform sampling, and further thatexponential (biased) sampling is best of all.
Forstreaming data, recently encountered data is valu-able, but there is also signal in the previous stream.Our sampling methods are based on reser-voir sampling (Vitter, 1985), a popularly knownmethod in some areas of computer science, butwhich has seen little use within computational lin-guistics.2Standard reservoir sampling is a methodfor maintaining a uniform sample over a dynamicstream of elements, using constant space.
Novelto this community, we consider a variant owing toAggarwal (2006) which provides for an exponen-tial bias towards recently observed elements.
Thisexponential reservoir sampling has all of the guar-antees of standard reservoir sampling, but as weshow, is a better fit for streaming textual data.
Ourapproach is fully general and can be applied to anystreaming task where we need to model the presentand can only use fixed space.2Exceptions include work by Van Durme and Lall (2011)and Van Durme (2012), aimed at different problems than thatexplored here.6872 BackgroundWe address two problems: language changes overtime, and the observation that space is a problem,even for compact sketches.Statistical language models often assume eithera local Markov property (when working with ut-terances, or sentences), or that content is gener-ated fully i.i.d.
(such as in document-level topicmodels).
However, language shows observablepriming effects, sometimes called triggers, wherethe occurrence of a given term decreases the sur-prisal of some other term later in the same dis-course (Lau et al, 1993; Church and Gale, 1995;Beeferman et al, 1997; Church, 2000).
Conven-tional cache and trigger models typically do notdeal with new terms and can be seen as adjustingthe parameters of a fixed model.Accounting for previously unseen entries in alanguage model can be naively simple: as they ap-pear in new training data, add them to the model!However in practice we are constrained by avail-able space: how many unique phrases can westore, given the target application environment?Our work is concerned with modeling languagethat might change over time, in accordance withcurrent trending discourse topics, but under a strictspace constraint.
With a fixed amount of memoryavailable, we cannot allow our list of unique wordsor phrases to grow over time, even while new top-ics give rise to novel names of people, places, andterms of interest.
Thus we need an approach thatkeeps the size of the model constant, but that isgeared to what is being discussed now, as com-pared to some time in the past.3 Reservoir Sampling3.1 Uniform Reservoir SamplingThe reservoir sampling algorithm (Vitter, 1985) isthe classic method of sampling without replace-ment from a stream in a single pass when thelength of the stream is of indeterminate or un-bounded length.
Say that the size of the desiredsample is k. The algorithm proceeds by retain-ing the first k items of the stream and then sam-pling each subsequent element with probabilityf(k, n) = k/n, where n is the length of the streamso far.
(See Algorithm 1.)
It is easy to show via in-duction that, at any time, all the items in the streamso far have equal probability of appearing in thereservoir.The algorithm processes the stream in a singlepass?that is, once it has processed an item in thestream, it does not revisit that item unless it isstored in the reservoir.
Given this restriction, theincredible feature of this algorithm is that it is ableto guarantee that the samples in the reservoir are auniformly random sample with no unintended bi-ases even as the stream evolves.
This makes it anexcellent candidate for situations when the streamis continuously being updated and it is computa-tionally infeasible to store the entire stream or tomake more than a single pass over it.
Moreover,it is an extremely efficient algorithm as it requiresO(1) time (independent of the reservoir size andstream length) for each item in the stream.Algorithm 1 Reservoir Sampling AlgorithmParameters:k: maximum size of reservoir1: Initialize an empty reservoir (any containerdata type).2: n := 13: for each item in the stream do4: if n < k then5: insert current item into the reservoir6: else7: with probability f(n, k), eject an ele-ment of the reservoir chosen uniformlyat random and insert current item into thereservoir8: n := n+ 13.2 Non-uniform Reservoir SamplingHere we will consider generalizations of the reser-voir sampling algorithm in which the sampleitems in the reservoir are more biased towards thepresent.
Put another way, we will continuouslydecay the probability that an older item will ap-pear in the reservoir.
Models produced using suchbiases put more modelling stress on the presentthan models produced using data that is selecteduniformly from the stream.
The goal here is tocontinuously update the reservoir sample in sucha way that the decay of older items is done consis-tently while still maintaining the benefits of reser-voir sampling, including the single pass and mem-ory/time constraints.The time-decay scheme we will study in thispaper is exponential bias towards newer items inthe stream.
More precisely, we wish for items that6880 2000 4000 6000 8000 10000time0.00.20.40.60.81.0probability of appearingin reservoiruniformexponential (various beta)Figure 1: Different biases for sampling a streamhave age a in the stream to appear with probabilityg(a) = c ?
exp (?a/?
),where a is the age of the item, ?
is a scale param-eter indicating how rapidly older items should bedeemphasized, and c is a normalization constant.To give a sense of what these time-decay proba-bilities look like, some exponential distributionsare plotted (along with the uniform distribution)in Figure 1.Aggarwal (2006) studied this problem andshowed that by altering the sampling probability(f(n, k) in Algorithm 1) in the reservoir samplingalgorithm, it is possible to achieve different age-related biases in the sample.
In particular, heshowed that by setting the sampling probability tothe constant function f(n, k) = k/?, it is possibleto approximately achieve exponential bias in thesample with scale parameter ?
(Aggarwal, 2006).Aggarwal?s analysis relies on the parameter ?
be-ing very large.
In the next section we will makethe analysis more precise by omitting any such as-sumption.3.3 AnalysisIn this section we will derive an expression for thebias introduced by an arbitrary sampling functionf in Algorithm 1.
We will then use this expressionto derive the precise sampling function needed toachieve exponential decay.3Careful selection off allows us to achieve anything from zero decay(i.e., uniform sampling of the entire stream) toexponential decay.
Once again, note that sincewe are only changing the sampling function, the3Specifying an arbitrary decay function remains an openproblem.one-pass, memory- and time-efficient propertiesof reservoir sampling are still being preserved.In the following analysis, we fix n to be the sizeof the stream at some fixed time and k to be thesize of the reservoir.
We assume that the ith el-ement of the stream is sampled with probabilityf(i, k), for i ?
n. We can then derive the proba-bility that an element of age a will still be in thereservoir asg(a) = f(n?
a, k)n?t=n?a+1(1?f(t, k)k),since it would have been sampled with probabilityf(n?
a, k) and had independent chances of beingreplaced at times t = n?a+1, .
.
.
, n with proba-bility f(t, k)/k.
For instance, when f(x, k) =kx,the above formula simplifies down to g(a) =kn(i.e., the uniform sampling case).For the exponential case, we fix the samplingrate to some constant f(n, k) = pk, and we wishto determine what value to use for pkto achievea given exponential decay rate g(a) = ce?a/?,where c is the normalization constant (to make g aprobability distribution) and ?
is the scale param-eter of the exponential distribution.
Substitutingf(n, k) = pkin the above formula and equatingwith the decay rate, we get that pk(1 ?
pk/k)a?ce?a/?, which must hold true for all possible val-ues of a.
After some algebra, we get that whenf(x, k) = pk= k(1 ?
e?1/?
), the probabilitythat an item with age a is included in the reser-voir is given by the exponential decay rate g(a) =pke?a/?.
Note that, for very large values of ?, thisprobability is approximately equal to pk?
k/?
(by using the approximation e?x?
1 ?
x, when|x| is close to zero), as given by Aggarwal, but ourformula gives the precise sampling probability andworks even for smaller values of ?.4 ExperimentsOur experiments use two streams of data to illus-trate exponential sampling: Twitter and a moreconventional newswire stream.
The Twitter data isinteresting as it is very multilingual, bursty (for ex-ample, it talks about memes, breaking news, gos-sip etc) and written by literally millions of differ-ent people.
The newswire stream is a lot more wellbehaved and serves as a control.4.1 Data, Models and EvaluationWe used one month of chronologically orderedTwitter data and divided it into 31 equal sized689Stream Interval Total (toks) Test (toks)Twitter Dec 2013 3282M 105MGiga 1994 ?
2010 635.5M 12MTable 1: Stream statisticsblocks (roughly corresponding with days).
Wealso used the AFP portion of the Giga Word corpusas another source of data that evolves at a slowerpace.
This data was divided into 50 equal sizedblocks.
Table 1 gives statistics about the data.
Ascan be seen, the Twitter data is vastly larger thannewswire and arrives at a much faster rate.We considered the following models.
Each one(apart from the exact model) was trained using thesame amount of data:?
Static.
This model was trained using datafrom the start of the duration and never var-ied.
It is a baseline.?
Exact.
This model was trained using allavailable data from the start of the stream andacts as an upper bound on performance.?
Moving Window.
This model used all datain a fixed-sized window immediately beforethe given test point.?
Uniform.
Here, we use uniform reservoirsampling to select the data.?
Exponential.
Lastly, we use exponen-tial reservoir sampling to select the data.This model is parameterised, indicating howstrongly biased towards the present the sam-ple will be.
The ?
parameter is a multiplierover the reservoir length.
For example, a ?value of 1.1 with a sample size of 10 meansthe value is 11.
In general, ?
always needs tobe bigger than the reservoir size.We sample over whole sentences (or Tweets)and not ngrams.4Using ngrams instead wouldgive us a finer-grained control over results, butwould come at the expense of greatly complicat-ing the analysis.
This is because we would need toreason about not just a set of items but a multisetof items.
Note that because the samples are large5,variations across samples will be small.4A consequence is that we do not guarantee that each sam-ple uses exactly the same number of grams.
This can be tack-led by randomly removing sampled sentences.5Each day consists of approximately four million Tweetsand we evaluate on a whole day.Day Uniform ?
value?
1.1 1.3 1.5 2.05 619.4 619.4 619.4 619.4 619.46 601.0 601.0 603.8 606.6 611.17 603.0 599.4 602.7 605.6 612.18 614.6 607.7 611.9 614.3 621.69 623.3 611.5 615.0 620.0 628.110 656.2 643.1 647.2 650.1 658.012 646.6 628.9 633.0 636.5 644.615 647.7 628.7 630.4 634.5 641.620 636.7 605.3 608.4 610.8 618.425 631.5 601.9 603.3 604.4 610.0Table 2: Perplexities for different ?
values overTwitter (sample size = five days).
Lower is better.We test the model on unseen data from all of thenext day (or block).
Afterwards, we advance to thenext day (block) and repeat, potentially incorpo-rating the previously seen test data into the currenttraining data.
Evaluation is in terms of perplexity(which is standard for language modelling).We used KenLM for building models and eval-uating them (Heafield, 2011).
Each model wasan unpruned trigram, with Kneser-Ney smoothing.Increasing the language model order would notchange the results.
Here the focus is upon whichdata is used in a model (that is, which data is addedand which data is removed) and not upon makingit compact or making retraining efficient.4.2 Varying the ?
ParameterTable 2 shows the effect of varying the ?
param-eter (using Twitter).
The higher the ?
value, themore uniform the sampling.
As can be seen, per-formance improves when sampling becomes morebiased.
Not shown here, but for Twitter, evensmaller ?
values produce better results and fornewswire, results degrade.
These differences aresmall and do not affect any conclusions made here.In practise, this value would be set using a devel-opment set and to simplify the rest of the paper, allother experiments use the same ?
value (1.1).4.3 Varying the Amount of DataDoes the amount of data used in a model affect re-sults?
Table 3 shows the results for Twitter whenvarying the amount of data in the sample and us-ing exponential sampling (?
= 1.1).
In paren-theses for each result, we show the correspondingmoving window results.
As expected, using moredata improves results.
We see that for each samplesize, exponential sampling outperforms our mov-ing window.
In the limit, all sampling methodswould produce the same results.690Day Sample Size (Days)1 2 35 652.5 (661.2) 629.1 (635.8) 624.8 (625.9)6 635.4 (651.6) 611.6 (620.8) 604.0 (608.7)7 636.0 (647.3) 611.0 (625.2) 603.7 (612.5)8 654.8 (672.7) 625.6 (641.6) 614.6 (626.9)9 653.9 (662.8) 628.3 (643.0) 618.8 (632.2)10 679.1 (687.8) 654.3 (666.8) 646.6 (659.7)12 671.1 (681.9) 645.8 (658.6) 633.8 (647.5)15 677.7 (697.9) 647.4 (668.0) 636.4 (652.6)20 648.1 (664.6) 621.4 (637.9) 612.2 (627.6)25 657.5 (687.5) 625.3 (664.4) 613.4 (641.8)Table 3: Perplexities for different sample sizesover Twitter.
Lower is better.4.4 Alternative Sampling StrategiesTable 4 compares the two baselines against the twoforms of reservoir sampling.
For Twitter, we seea clear recency effect.
The static baseline getsworse and worse as it recedes from the currenttest point.
Uniform sampling does better, but itin turn is beaten by the Moving Window Model.However, this in turn is beaten by our exponentialreservoir sampling.Day Static Moving Uniform Exp Exact5 619.4 619.4 619.4 619.4 619.46 664.8 599.7 601.8 601.0 597.67 684.4 602.8 603.0 599.3 595.68 710.1 612.0 614.6 607.7 603.59 727.0 617.9 623.3 613.0 608.710 775.6 651.2 656.2 642.0 640.512 776.7 639.0 646.6 628.7 627.515 777.1 638.3 647.7 626.7 627.320 800.9 619.1 636.7 604.9 607.325 801.4 621.7 631.5 601.5 597.6Table 4: Perplexities for differently selected sam-ples over Twitter (sample size = five days, ?
=1.1).
Results in bold are the best sampling results.Lower is better.4.5 GigaWordTwitter is a fast moving, rapidly changing multi-lingual stream and it is not surprising that our ex-ponential reservoir sampling proves beneficial.
Isit still useful for a more conventional stream thatis drawn from a much smaller population of re-porters?
We repeated our experiments, using thesame rolling training and testing evaluation as be-fore, but this time using newswire for data.Table 5 shows the perplexities when using theGigaword stream.
We see the same general trends,albeit with less of a difference between exponen-tial sampling and our moving window.
Perplexityvalues are all lower than for Twitter.Block Static Moving Uniform Exp11 416.5 381.1 382.0 382.015 436.7 353.3 357.5 352.820 461.8 347.0 354.4 344.625 315.6 214.9 222.2 211.330 319.1 200.5 213.5 199.540 462.5 304.4 313.2 292.9Table 5: Perplexities for differently selected sam-ples over Gigaword (sample size = 10 blocks, ?
=1.1).
Lower is better.4.6 Why does this work for Twitter?Although the perplexity results demonstrate thatexponential sampling is on average beneficial, itis useful to analyse the results in more detail.
Fora large stream size (25 days), we built models us-ing uniform, exponential (?
= 1.1) and our movingwindow sampling methods.
Each approach usedthe same amount of data.
For the same test set(four million Tweets), we computed per-Tweet loglikelihoods and looked at the difference betweenthe model that best explained each tweet and thesecond best model (ie the margin).
This gives usan indication of how much a given model betterexplains a given Tweet.
Analysing the results, wefound that most gains came from short grams andvery few came from entire Tweets being reposted(or retweeted).
This suggests that the Twitter re-sults follow previously reported observations onhow language can be bursty and not from Twitter-specific properties.5 ConclusionWe have introduced exponential reservoir sam-pling as an elegant way to model a stream of un-bounded size, yet using fixed space.
It naturally al-lows one to take account of recency effects presentin many natural streams.
We expect that our lan-guage model could improve other Social Mediatasks, for example lexical normalisation (Han andBaldwin, 2011) or even event detection (Lin etal., 2011).
The approach is fully general and notjust limited to language modelling.
Future workshould look at other distributions for sampling andconsider tasks such as machine translation overSocial Media.Acknowledgments This work was carried outwhen MO was on sabbatical at the HLTCOE andCLSP.691ReferencesCharu C Aggarwal.
2006.
On biased reservoir sam-pling in the presence of stream evolution.
In Pro-ceedings of the 32nd international conference onVery large data bases, pages 607?618.
VLDB En-dowment.Doug Beeferman, Adam Berger, and John Lafferty.1997.
A model of lexical attractions and repulsion.In Proceedings of the 35th Annual Meeting of the As-sociation for Computational Linguistics and EighthConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 373?380.Association for Computational Linguistics.K.
Church and W. A. Gale.
1995.
Poisson mixtures.Natural Language Engineering, 1:163?190.Kenneth W Church.
2000.
Empirical estimates ofadaptation: the chance of two noriegas is closer top/2 than p 2.
In Proceedings of the 18th conferenceon Computational linguistics-Volume 1, pages 180?186.
Association for Computational Linguistics.Amit Goyal, Hal Daum?e III, and Suresh Venkatasub-ramanian.
2009.
Streaming for large scale NLP:Language Modeling.
In Proceedings of NAACL.Bo Han and Timothy Baldwin.
2011.
Lexical normal-isation of short text messages: Makn sens a #twit-ter.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies - Volume 1, HLT ?11,pages 368?378, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proceedings of theEMNLP 2011 Sixth Workshop on Statistical Ma-chine Translation, pages 187?197, Edinburgh, Scot-land, United Kingdom, July.Raymond Lau, Ronald Rosenfeld, and SaIim Roukos.1993.
Trigger-based language models: A maximumentropy approach.
In Acoustics, Speech, and SignalProcessing, 1993.
ICASSP-93., 1993 IEEE Interna-tional Conference on, volume 2, pages 45?48.
IEEE.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for smt.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing: Volume 2-Volume 2, pages 756?764.
Association for Compu-tational Linguistics.Jimmy Lin, Rion Snow, and William Morgan.
2011.Smoothing techniques for adaptive online languagemodels: topic tracking in tweet streams.
In Proceed-ings of the 17th ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 422?429.
ACM.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine transla-tion.
In Proceedings of ACL.Benjamin Van Durme and Ashwin Lall.
2009.
Proba-bilistic Counting with Randomized Storage.
In Pro-ceedings of IJCAI.Benjamin Van Durme and Ashwin Lall.
2011.
Effi-cient online locality sensitive hashing via reservoircounting.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies: short papers-Volume 2, pages 18?23.
Association for Computa-tional Linguistics.Benjamin Van Durme.
2012.
Streaming analysis ofdiscourse participants.
In Proceedings of the 2012Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 48?58.
Association forComputational Linguistics.Jeffrey S. Vitter.
1985.
Random sampling with a reser-voir.
ACM Trans.
Math.
Softw., 11:37?57, March.692
