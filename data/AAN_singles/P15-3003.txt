Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 16?21,Beijing, China, July 28, 2015.c?2015 Association for Computational LinguisticsLearning representations for text-level discourse parsingGregor WeissFaculty of Computer and Information ScienceUniversity of LjubljanaVe?cna pot 113, Ljubljana, Sloveniagregor.weiss@student.uni-lj.siAbstractIn the proposed doctoral work we will de-sign an end-to-end approach for the chal-lenging NLP task of text-level discourseparsing.
Instead of depending on mostlyhand-engineered sparse features and in-dependent components for each subtask,we propose a unified approach completelybased on deep learning architectures.
Totrain more expressive representations thatcapture communicative functions and se-mantic roles of discourse units and rela-tions between them, we will jointly learnall discourse parsing subtasks at differentlayers of our architecture and share theirintermediate representations.
By combin-ing unsupervised training of word embed-dings with our layer-wise multi-task learn-ing of higher representations we hope toreach or even surpass performance of cur-rent state-of-the-art methods on annotatedEnglish corpora.1 IntroductionModern algorithms for natural language process-ing (NLP) are based on statistical machine learn-ing and require a computationally convenient rep-resentation of input data.
Unfortunately real-worldplain text is usually represented as an unstruc-tured sequence of words with complex relationsbetween them.
Therefore it is extremely impor-tant to discover good representations in the formof informative text features.In NLP such features are almost always hand-engineered sparse features and require expensivehuman labor and expert knowledge to construct.They are usually based on lexicons or featuresextracted by other NLP subtasks and have theform of hand-engineered extraction rules, regularexpressions, lemmatization, part-of-speech (POS)tags, positions or lengths of arguments, tenseforms, syntactic parse trees, and similar.
Althoughsuch features are specific for a given language, do-main, and task, they work well enough for sim-ple NLP tasks, like named entity recognition orPOS tagging.
Nevertheless, the ability to learn textfeatures and representations automatically wouldhave a lot of potential to improve state-of-the-artperformance on more challenging NLP tasks, suchas text-level discourse parsing.
This may even bemore important for languages where progress inNLP is still lacking.Variants of deep learning architectures havebeen shown to provide a different approach tolearning in which latent features are automaticallylearned as distributed dense vectors.
They man-aged to represent meaningful relations with word(Collobert, 2011), POS and dependency tag (Chenand Manning, 2014), sentence (Guo and Diab,2012), and document (Socher et al., 2012) embed-dings and achieved surprising results for a numberof NLP tasks.
It has been shown that both unsuper-vised pre-training (Hinton et al., 2006) and multi-task learning (Collobert and Weston, 2008) signif-icantly improve their performance in the absenceof hand-engineered features.
This makes them es-pecially interesting for the problem of text-leveldiscourse parsing.2 Text-level discourse parsingIn natural language, a piece of text meant to com-municate specific information, function, or knowl-edge (clauses, sentences, or even paragraphs) iscalled a discourse.
They are often understoodonly in relation to other discourse units (at anylevel of grouping) and their combination creates ajoint meaning larger than individual unit?s mean-ing alone (Mann and Thompson, 1988).Discourse parsing is the task of determininghow these units are related to each other (likein Figure 1) and plays a central role in a num-16ber of high-impact natural language processing(NLP) applications, including text summarization,sentence compression, sentiment analysis, andquestion-answering.
For analyzing different per-spectives of discourse analysis researchers pro-posed a number of theoretical frameworks and re-leased annotated corpora, such as RST DiscourseTreebank (RST-DT) (Carlson et al., 2003) andPenn Discourse Treebank (PDTB) (Prasad et al.,2008).
Both of these decompose discourse pars-ing into a few subtasks and, like in most of NLP,their success depends on expert knowledge of eachsubtask and hand-engineering of more powerfulfeatures (Feng and Hirst, 2012; Lin et al., 2014),representations, and heuristics (Joty et al., 2013;Prasad et al., 2010).Despite recent progress in automatic discoursesegmentation and sentence-level parsing (Fisherand Roark, 2007; Joty et al., 2012; Soricut andMarcu, 2003), text-level discourse parsing re-mains a significant challenge (Feng and Hirst,2012; Ji and Eisenstein, 2014; Lin et al., 2014).Traditional hand-engineering approaches unfortu-nately seem to be insufficient, as discourses andrelations between them do not follow any strictgrammar or obvious rules.Two main theoretical frameworks with Englishcorpus have been proposed to capture differentrhetorical characteristics, and serve different ap-plications.The Penn Discourse Treebank (PDTB) (Prasadet al., 2008) is currently the largest discourse-annotated corpus, consisting of 2159 articles fromWall Street Journal.
It strives to maintain atheory-neutral approach by adopting the predicate-argument view and independence of discourse re-lations.
In it either explicitly or implicitly givendiscourse connectives, such as coordinating con-junction (e.g.
"and", "but"), subordinating con-junction (e.g.
"if", "because"), or discourse ad-verbial (e.g.
"however", "also"), combine pairsof discourse arguments into relations.
For PDTB-style discourse parsing, extracting argument spansseems to be the most difficult subtask (Lin et al.,2014), resulting in the best overall performance ofonly 34.80% in F1-measure (Kong et al., 2014).The RST Discourse Treebank (RST-DT) (Carl-son et al., 2003) follows the theoretical frame-work of Rhetorical Structure Theory (RST) (Mannand Thompson, 1988).
It contains 385 annotateddocuments from the Wall Street Journal with 18high-level categories and 110 fine-grained rela-tions.
Any coherent text can be represented asa RST discourse tree structure (like in Figure 1)whose leaves are minimal non-overlapping textspans called elementary discourse units.
Adja-cent nodes are joined depending on their discourserelations to form a tree.
In a mono-nuclear dis-course relation one of the text spans is the nucleus,which is more salient than the satellite, while ina multi-nuclear relation all text spans are equallyimportant for interpretation.
Performance of RST-style discourse parsing is evaluated based on theirability to locate spans of text that serve as argu-ments (best 85.7% in F1-measure (Feng and Hirst,2012)), identify which of the arguments is the nu-cleus (best 71.1% in F1-measure (Ji and Eisen-stein, 2014)), and tag the sense and location of dis-course relations (best 61.6% in F1-measure (Ji andEisenstein, 2014)).3 Related workEarly work on linguistic and computationaldiscourse analysis produced several theoreticalframeworks and one of the most influential isRhetorical Structure Theory (RST) (Mann andThompson, 1988).
In order to automatically builda hierarchical structure of a text, first approaches(Marcu, 2000) relied mainly on discourse markers,hand-engineered rules, and heuristics.
Learning-based approaches were first applied to identifywithin-sentence discourse relations (Soricut andMarcu, 2003), and only later to cross-sentencetext-level relations (Baldridge and Lascarides,2005).
They largely focused on lexical, syntac-tic, and structural features, but the close rela-tionship between discourse structure and seman-tic meaning suggests that this may not be suf-ficient (Prasad et al., 2008; Subba and Di Eu-genio, 2009).
Further work on discourse pars-ing focused first on having a binary classifierfor determining whether two adjacent discourseunits should be merged, followed by a multi-classclassifier for determining which discourse rela-tion should be assigned to the new subtree (Du-Verle and Prendinger, 2009).
Improved results(Feng and Hirst, 2012) have been achieved byincorporating rich linguistic features (Hernault etal., 2010), including lexical semantics, and spe-cific discourse production rules (Lin et al., 2009).An alternative approach is based on jointly per-forming detection and classification in a bottom-17?
[The dollar finished lower yesterday,]e1[after another session on Wall Street.]e2?
[Concern about the volatile U.S. stock market had faded in recent sessions,]e3[and traders letthe dollar languish in a narrow range until tomorrow,]e4[when the preliminary report on U.S.gross national product is released.]e5?
[But movements in the Dow Jones Industrial Average yesterday put Wall Street back in thespotlight]e6[and inspired participants to bid the U.S. unit lower.
]e7Figure 1: An example of seven elementary discourse units (e1-e7), and (mono- or multi-nuclear) relationsbetween them in an RST discourse tree representation (Feng et al., 2014).up fashion while distinguishing within-sentenceand cross-sentence relations (Joty et al., 2013) andimproved with discriminative reranking of dis-course trees using tree kernels (Joty and Moschitti,2014).
It has been shown that constituent- anddependency-based syntax and features based oncoreference links improve performance (Surdeanuet al., 2015).
The first PDTB-style end-to-end dis-course parser (Lin et al., 2014) uses a connec-tive list to identify explicit candidates, followedby simple features and parse trees to extract ar-guments and identify discourse relations.
Classi-fying implicit discourse relations can be improvedby combining distributed representations of parsetrees with coreferent entity mentions (Ji and Eisen-stein, 2015).
Extracting discourse arguments hasbeen attempted by using classic linear word tag-ging with conditional random fields and globalfeatures (Ghosh et al., 2012), identifying nodes inconstituent subtrees (Lin et al., 2014), and hybridmerging and pruning of parse trees with integerlinear programming (Kong et al., 2014).Deep learning architectures consist of multiplelayers of simple learning blocks stacked on eachother and, when well trained, tend to do a bet-ter job at disentangling the underlying factors ofvariation.
Beginning with raw data, its represen-tation is transformed into increasingly higher andmore abstract forms in each layer, until the finallow-dimensional features or representation usefulfor a given task is reached.
Their success is possi-ble with breakthroughs and improvements in train-ing techniques (like AdaGrad or Adam optimiza-tion, rectifier function, dropout regularization) andwith initialization using unsupervised pre-training(Hinton et al., 2006; Collobert, 2011) on massivedatasets (such as Wikipedia or Wall Street Jour-nal).
Pre-training helps deep networks to developnatural abstractions and combined with multi-tasklearning (Collobert and Weston, 2008) it can sig-nificantly improve their performance in the ab-sence of hand-engineered features.Classic feed-forward architectures are inappro-priate for processing text documents, because oftheir variable length and natural representation asa sequence of words.
One approach to solve thisis to specify a transition-based processing mech-anism (Chen and Manning, 2014; Ji and Eisen-stein, 2014) and train a neural network classifierto make parsing decisions.
Recurrent neural net-works (RNNs) (Elman, 1990) or their generaliza-tion, recursive neural networks (Goller and K?ch-ler, 1996), represent a more direct approach by re-cursively applying the same set of weights over thesequence (temporal dimension) or structure (tree-based).
Li et al.
(Li et al., 2015) have recently18showed that only some NLP tasks benefit fromrecursive models applied on syntactic parse treesand recurrent models seem to be sufficient for dis-course parsing.
By stacking multiple hidden lay-ers into a deep RNN makes them represent a tem-poral hierarchy with multiple layers operating atdifferent time scales (Hermans and Schrauwen,2013).
Learning to store information over ex-tended time intervals has been achieved with longshort-term memory (Hochreiter and Schmidhu-ber, 1997), time delay neural network (Waibel etal., 1989), or neural Turing machines (Graves etal., 2014).
Bidirectional variants of these mod-els can incorporate information from precedingas well as following tokens (Schuster and Pali-wal, 1997).
Recursive neural networks have alsobeen shown to support different task-specific rep-resentations, such as matrix-vector representationof words (Socher et al., 2012) or recurrent neu-ral tensor networks (Socher et al., 2013).
Forour discourse parsing task such deeper models,that can learn abstract representations on differenttime scales, might better model the discourse re-lations between input vectors and (hopefully) cap-ture their communicative functions and semanticmeaning.A few initial attempts of applying representa-tion learning to our task have already shown sub-stantial performance improvements over previousstate-of-the-art.
Ji and Eisenstein (Ji and Eisen-stein, 2014) implement a shift-reduce discourseparser on top of given RST-style discourse unitsto simultaneously learn parsing and a discourse-driven projection of features using support vectormachines with gradient-based updates.
Li et al.
(Li, 2014) produce a distributed representation ofRST-style discourse units using recursive convolu-tion on sentence parse trees and apply a classifierto determine relations between them.
Ji and Eisen-stein (Ji and Eisenstein, 2014) also improved clas-sification of PDTB-style implicit discourse rela-tions by combining distributed representations ofparse trees with coreferent entity mentions.4 Contribution to scienceBecause text-level discourse parsing is an impor-tant, yet still challenging NLP task, it is the focusof our doctoral dissertation.Method for text-level discourse parsing.Instead of depending on mostly hand-engineeredsparse features and independent separately-developed components for each subtask, wepropose a unified end-to-end approach for text-level discourse parsing completely based ondeep learning architectures.
First each of thediscourse parsing subtasks, such as argumentboundary detection, labeling, discourse relationidentification and sense classification, need tobe formulated in terms of RNNs and similarderivable learning architectures.
To benefit fromtheir ability to learn intermediate representationsthey will be partially stacked on top of each order,such that the last but one layer (i.e.
output layer)for each subtask is shared with other subtasks.By placing increasingly more difficult subtasksat different layers in one deep architecture,they can benefit from each others intermediaterepresentations, improve robustness and trainingspeed.
Figure 2 further combines unsupervisedtraining of word embeddings with our layer-wisemulti-task learning of higher representationsand illustrates our goal of a unified end-to-endapproach for text-level discourse parsing utilizingdifferent layers of representations.Figure 2: Illustration of our unified end-to-end ap-proach for text-level discourse parsing with layer-wise multi-task learning of higher representations.5 Work planTo accomplish this we will, on one hand, need tofind the best deep learning models for each of thediscourse parsing subtasks, suitable architecture,activation functions, and figure out how to adaptthem to operate on sequential data and with eachother.
This includes analyzing deep learning archi-tectures, identifying their strengths, useful compo-nents, and their suitability for our NLP task.Afterwards combine them into one unified deeplearning architecture with shared intermediate rep-19resentations and unsupervised training of wordembeddings.
Developing a prototype for shal-low discourse parsing will open the door for findthe best initialization procedures, training func-tions, learning rates, and similar.
Shallow PDTB-style discourse parsing is also a challenge on thisyears CoNLL 2015 conference, where adjacenttext spans are not necessarily connected with dis-course relations to form a tree.Additionally we will experiment with new andmore expressive representations and structures(like neural tensor networks) that could capturecommunicative functions and semantic roles ofdiscourse units and relations between them.Even though our method could be applied toany plain text, we plan on evaluating it on stan-dard annotated English corpora.
After applyingour approach on at lest one of the corpora, weintend to qualitatively analyze the identified dis-course units and relations between them to gain in-sights about its strengths and weaknesses.
On theother hand, the dataset will allow us to also quan-titatively compare its performance to current state-of-the-art methods.
The procedure for our methodwill begin by pre-training the weights in our deeparchitecture on external unlabeled datasets (likeWikipedia), than jointly train on all discourse pars-ing subtasks on the training set, use a separate val-idation set to optimize hyper-parameters, and es-timate its performance on the test set.
For eval-uation purposes standard evaluation measures forsubtasks based on F1-scores will be used.6 ConclusionTo increase the generality of our unified end-to-end approach for text-level discourse parsing, wewill try to depend as little as possible on back-ground knowledge in the form of hand-engineeredfeatures for a specific language, domain, or task.By incorporating various improvements in auto-matic learning of features and representations wehope to reach or even surpass performance of cur-rent state-of-the-art methods on annotated Englishcorpora.ReferencesJason Baldridge and Alex Lascarides.
2005.
Proba-bilistic head-driven parsing for discourse structure.In Proc.
9th Conf.
Comput.
Nat.
Lang.
Learn., pages96?103.
Association for Computational Linguistics.Lynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2003.
Building a discourse-tagged cor-pus in the framework of Rhetorical Structure Theory.Curr.
New Dir.
Discourse Dialogue, 22:85?112.Danqi Chen and Christopher D Manning.
2014.
AFast and Accurate Dependency Parser using NeuralNetworks.
In Proc.
2014 Conf.
Empir.
Methods Nat.Lang.
Process., pages 740?750.Ronan Collobert and Jason Weston.
2008.
A Uni-fied Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning.
InProc.
25th Int.
Conf.
Mach.
Learn., volume 20,pages 160?167.Ronan Collobert.
2011.
Deep Learning for EfficientDiscriminative Parsing.
Int.
Conf.
Artif.
Intell.
Stat.,15:224?232.David A. DuVerle and Helmut Prendinger.
2009.
Anovel discourse parser based on support vector ma-chine classification.
In Proc.
Jt.
Conf.
47th Annu.Meet.
ACL 4th Int.
Jt.
Conf.
Nat.
Lang.
Process.AFNLP, pages 665?673.
Association for Computa-tional Linguistics.Jeffrey L. Elman.
1990.
Finding structure in time* 1.Cogn.
Sci., 14(1990):179?211.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-levelDiscourse Parsing with Rich Linguistic Features.
InProc.
50th Annu.
Meet.
Assoc.
Comput.
Linguist.,pages 60?68.
Association for Computational Lin-guistics.Vanessa Wei Feng, Ziheng Lin, and Graeme Hirst.2014.
The Impact of Deep Hierarchical DiscourseStructures in the Evaluation of Text Coherence.
InProc.
25th Int.
Conf.
Comput.
Linguist.Seeger Fisher and Brian Roark.
2007.
The utility ofparse-derived features for automatic discourse seg-mentation.
In Proc.
45th Annu.
Meet.
Assoc.
Com-put.
Linguist., volume 45, pages 488?495.Sucheta Ghosh, Giuseppe Riccardi, and Richard Jo-hansson.
2012.
Global features for shallow dis-course parsing.
In Annu.
Meet.
Spec.
Interes.
Gr.Discourse Dialogue, pages 150?159.Christoph Goller and Andreas K?chler.
1996.
Learn-ing Task-Dependent Distributed Representations byBackpropagation Through Structure.
In IEEE Int.Conf.
Neural Networks, pages 347?352.Alex Graves, Greg Wayne, and Ivo Denihelka.2014.
Neural Turing Machines.
arXiv Prepr.arXiv410.5401, pages 1?26.Weiwei Guo and Mona Diab.
2012.
Modeling Sen-tences in the Latent Space.
In Proc.
50th Annu.Meet.
Assoc.
Comput.
Linguist., pages 864?872.
As-sociation for Computational Linguistics.20Michiel Hermans and Benjamin Schrauwen.
2013.Training and Analyzing Deep Recurrent Neural Net-works.
In Adv.
Neural Inf.
Process.
Syst., volume 26,pages 190?198.Hugo Hernault, Helmut Prendinger, David A. DuVerle,and Mitsuru Ishizuka.
2010.
HILDA: A DiscourseParser Using Support Vector Machine Classification.Dialogue and Discourse, 1(3):1?33.Geoffrey E. Hinton, Simon Osindero, and Yee-WhyeTeh.
2006.
A Fast Learning Algorithm for DeepBelief Nets.
Neural Comput., 18:1527?1554.Sepp Hochreiter and J?rgen Schmidhuber.
1997.
LongShort-Term Memory.
Neural Comput., 9(8):1735?1780.Yangfeng Ji and Jacob Eisenstein.
2014.
Representa-tion Learning for Text-level Discourse Parsing.
InProc.
52nd Annu.
Meet.
Assoc.
Comput.
Linguist.,pages 13?24.Yangfeng Ji and Jacob Eisenstein.
2015.
One Vector isNot Enough: Entity-Augmented Distributed Seman-tics for Discourse Relations.
Trans.
Assoc.
Comput.Linguist.Shafiq Joty and Alessandro Moschitti.
2014.
Discrim-inative Reranking of Discourse Parses Using TreeKernels.
In Proc.
2014 Conf.
Empir.
Methods Nat.Lang.
Process., pages 2049?2060.Shafiq Joty, Giuseppe Carenini, and Raymond T.Ng.
2012.
A novel discriminative framework forsentence-level discourse analysis.
In Proc.
2012 Jt.Conf.
Empir.
Methods Nat.
Lang.
Process.
Comput.Nat.
Lang.
Learn., pages 904?915.
Association forComputational Linguistics.Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, andYashar Mehdad.
2013.
Combining Intra- andMulti-sentential Rhetorical Parsing for Document-level Discourse Analysis.
In Proc.
51st Annu.
Meet.Assoc.
Comput.
Linguist., pages 486?496.Fang Kong, Hwee Tou, and Ng Guodong.
2014.
AConstituent-Based Approach to Argument Labelingwith Joint Inference in Discourse Parsing.
In Conf.Empir.
Methods Nat.
Lang.
Process., pages 68?77.Jiwei Li, Dan Jurafsky, and Eduard Hovy.
2015.
WhenAre Tree Structures Necessary for Deep Learning ofRepresentations?
Arxiv.Junyi Jessy Li.
2014.
Reducing Sparsity Improvesthe Recognition of Implicit Discourse Relations.
InProc.
SIGDIAL 2014 Conf., number June, pages199?207.Ziheng Lin, Min-yen Kan, and Hwee Tou Ng.
2009.Recognizing Implicit Discourse Relations in thePenn Discourse Treebank.
In Proc.
2009 Conf.
Em-pir.
Methods Nat.
Lang.
Process., pages 343?351.Association for Computational Linguistics.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.A PDTB-Styled End-to-End Discourse Parser.
Nat.Lang.
Eng., 20(2):151?184.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text-Interdisciplinary J.Study Discourse, 8(3):243?281.Daniel Marcu.
2000.
The Rhetorical Parsing of Unre-stricted Texts: A Surface-based Approach.
Comput.Linguist., 26(38):395?448.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse TreeBank 2.0.Proc.
Sixth Int.
Conf.
Lang.
Resour.
Eval., pages2961?2968.Rashmi Prasad, Aravind Joshi, and Bonnie Webber.2010.
Exploiting Scope for Shallow Discourse Pars-ing.
In Int.
Conf.
Lang.
Resour.
Eval., pages 2076?2083.Mike Schuster and Kuldip K Paliwal.
1997.
Bidirec-tional recurrent neural networks.
IEEE Trans.
Sig-nal Process., 45(11):2673?2681.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Compo-sitionality through Recursive Matrix-Vector Spaces.In Proc.
2012 Jt.
Conf.
Empir.
Methods Nat.
Lang.Process.
Comput.
Nat.
Lang.
Learn., pages 1201?1211.
Association for Computational Linguistics.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proc.
Conf.
Empir.
Methods Nat.
Lang.Process., pages 1631?1642.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical infor-mation.
Proc.
2003 Conf.
North Am.
Chapter Assoc.Comput.
Linguist.
Hum.
Lang.
Technol., 1:228?235.Rajen Subba and Barbara Di Eugenio.
2009.
An effec-tive discourse parser that uses rich linguistic infor-mation.
In Proc.
Hum.
Lang.
Technol.
2009 Annu.Conf.
North Am.
Chapter Assoc.
Comput.
Linguist.,pages 566?574.
Association for Computational Lin-guistics.Mihai Surdeanu, Thomas Hicks, and Marco A.Valenzuela-Escarcega.
2015.
Two Practical Rhetor-ical Structure Theory Parsers.
In Proc.
North Am.Chapter Assoc.
Comput.
Linguist.Alexander Waibel, Toshiyuki Hanazawa, Geoffrey E.Hinton, Kiyohiro Shikano, and Kevin J. Lang.
1989.Phoneme recognition using time-delay neural net-works.
IEEE Trans.
Acoust., 37(3):328?339.21
