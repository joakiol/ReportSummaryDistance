Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1610?1620,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLearning Structured Predictors from Bandit Feedbackfor Interactive NLPArtem Sokolov,?and Julia Kreutzer?and Christopher Lo?,?and Stefan Riezler?,?
?Computational Linguistics &?IWR, Heidelberg University, Germany{sokolov,kreutzer,riezler}@cl.uni-heidelberg.de?Department of Mathematics, Tufts University, Boston, MA, USAchris.aa.lo@gmail.comAmazon Development Center, Berlin, GermanyAbstractStructured prediction from bandit feed-back describes a learning scenario whereinstead of having access to a gold standardstructure, a learner only receives partialfeedback in form of the loss value of a pre-dicted structure.
We present new learningobjectives and algorithms for this inter-active scenario, focusing on convergencespeed and ease of elicitability of feed-back.
We present supervised-to-banditsimulation experiments for several NLPtasks (machine translation, sequence la-beling, text classification), showing thatbandit learning from relative preferenceseases feedback strength and yields im-proved empirical convergence.1 IntroductionStructured prediction from partial information canbe described by the following learning protocol:On each of a sequence of rounds, the learning al-gorithm makes a prediction, and receives partialinformation in terms of feedback on the predictedpoint.
This single-point feedback is used to con-struct a parameter update that is an unbiased esti-mate of the respective update rule for the full in-formation objective.
In difference to the full infor-mation scenario, the learner does not know whatthe correct prediction looks like, nor what wouldhave happened if it had predicted differently.
Thislearning scenario has been investigated under thenames of learning from bandit feedback1or rein-?The work for this paper was done while the authorswere at Heidelberg University.1The name is inherited from a model where in each rounda gambler pulls an arm of a different slot machine (?one-armed bandit?
), with the goal of maximizing his reward rel-ative to the maximal possible reward, without apriori knowl-edge of the optimal slot machine.
See Bubeck and Cesa-Bianchi (2012) for an overview.forcement learning2, and has (financially) impor-tant real world applications such as online adver-tising (Chapelle et al, 2014).
In this application,the probability that an ad will be clicked (and theadvertiser has to pay) is estimated by trading offexploration (a new ad needs to be displayed in or-der to learn its click-through rate) and exploitation(displaying the ad with the current best estimateis better in the short term) in displaying ads tousers.
Similar to the online advertising scenario,there are many potential applications to interac-tive learning in NLP.
For example, in interactivestatistical machine translation (SMT), user feed-back in form of post-edits of predicted transla-tions is used for model adaptation (Bertoldi et al,2014; Denkowski et al, 2014; Green et al, 2014).Since post-editing feedback has a high cost andrequires professional expertise of users, weakerforms of feedback are desirable.
Sokolov et al(2015) showed in a simulation experiment thatpartial information in form of translation qualityjudgements on predicted translations is sufficientfor model adaptation in SMT.
However, one draw-back of their bandit expected loss minimization al-gorithm is the slow convergence speed, meaningthat impractically many rounds of user feedbackwould be necessary for learning in real-world in-teractive SMT.
Furthermore, their algorithms re-quires feedback in form of numerical assessmentsof translation quality.
Such absolute feedback isarguably harder to elicit from human users thanrelative judgements.The goal of this work is a preparatory studyof different objectives and algorithms for struc-tured prediction from partial information withreal-world interactive scenarios in mind.
Since thealgorithm of Sokolov et al (2015) can be charac-terized as stochastic optimization of a non-convex2See Szepesv?ari (2009) for an overview of algorithms forreinforcement learning and their relation to bandit learning.1610objective, a possible avenue to address the prob-lem of convergence speed is a (strong) convexifi-cation of the learning objective, which we formal-ize as bandit cross-entropy minimization.
To theaim of easing elicitability of feedback, we presenta bandit pairwise preference learning algorithmthat requires only relative feedback in the form ofpairwise preference rankings.The focus of this paper is on an experimentalevaluation of the empirical performance and con-vergence speed of the different algorithms.
Wefollow the standard practice of early stopping bymeasuring performance on a development set, andpresent results of an extensive evaluation on sev-eral tasks with different loss functions, includingBLEU for SMT, Hamming loss for optical char-acter recognition, and F1 score for chunking.
Inour experiments, we use a standard supervised-to-bandit transformation where a reward signal issimulated by evaluating a task loss against goldstandard structures without revealing them to thelearning algorithm (Agarwal et al, 2014).
Fromthe perspective of real-world interactive applica-tions, bandit pairwise preference learning is thepreferred algorithm since it only requires compar-ative judgements for learning.
This type of rela-tive feedback been shown to be advantageous forhuman decision making (Thurstone, 1927).
How-ever, in our simulation experiments we found thatrelative feedback also results in improved empir-ical convergence speed for bandit pairwise pref-erence learning.
The picture of fastest empiricalconvergence of bandit pairwise preference learn-ing is consistent across different tasks, both com-pared to bandit expected loss minimization andbandit cross-entropy minimization.
Given the im-proved convergence and the ease of elicitabilityof relative feedback, the presented bandit pairwisepreference learner is an attractive choice for inter-active NLP tasks.2 Related WorkReinforcement learning (RL) has the goal of max-imizing the expected reward for choosing an ac-tion at a given state in a Markov Decision Pro-cess (MDP) model, where rewards are receivedat each state or once at the final state.
The al-gorithms in this paper can be seen as one-stateMDPs where choosing an action corresponds topredicting a structured output.
Most closely re-lated are RL approaches that use gradient-basedoptimization of a parametric policy for action se-lection (Bertsekas and Tsitsiklis, 1996; Sutton etal., 2000).
Policy gradient approaches have beenapplied to NLP tasks by Branavan et al (2009),Chang et al (2015) or Ranzato et al (2016).Bandit learning operates in a similar scenario ofmaximizing the expected reward for selecting anarm of a multi-armed slot machine.
Similar to ourcase, the models consist of a single state, however,arms are usually selected from a small set of op-tions while structures are predicted over exponen-tial output spaces.
While bandit learning is mostlyformalized as online regret minimization with re-spect to the best fixed arm in hindsight, we inves-tigate asymptotic convergence of our algorithms.In the spectrum of stochastic (Auer et al, 2002a)versus adversarial bandits (Auer et al, 2002b), ourapproach takes a middle path by making stochasticassumptions on inputs, but not on rewards.
Mostclosely related are algorithms that optimize para-metric models, e.g., contextual bandits (Langfordand Zhang, 2007; Li et al, 2010) or combinatorialbandits (Dani et al, 2007; Cesa-Bianchi and Lu-gosi, 2012).
To the best of our knowledge, thesetypes of algorithms have not yet been applied inthe area of NLP.Pairwise preference learning has been studiedin the full information supervised setting (see Her-brich et al (2000), Joachims (2002), Freund etal.
(2003), Cortes et al (2007), F?urnkranz andH?ullermeier (2010), inter alia) where given pref-erence pairs are assumed.
Stochastic optimizationfrom two-point (or multi-point) feedback has beeninvestigated in the framework of gradient-free op-timization (see Yue and Joachims (2009), Agarwalet al (2010), Ghadimi and Lan (2012), Jamiesonet al (2012), Duchi et al (2015), inter alia), whileour algorithms can be characterized as stochasticgradient descent algorithms.3 Probabilistic Structured Prediction3.1 Full Information vs. Bandit FeedbackThe objectives and algorithms presented in this pa-per are based on the well-known expected loss cri-terion for probabilistic structured prediction (seeOch (2003), Smith and Eisner (2006), Gimpel andSmith (2010), Yuille and He (2012), He and Deng(2012), inter alia).
The objective is defined as aminimization of the expectation of a given taskloss function with respect to the conditional dis-tribution over structured outputs.
This criterion1611has the form of a continuous, differentiable, and ingeneral, non-convex objective function.
More for-mally, let X be a structured input space, let Y(x)be the set of possible output structures for input x,and let ?y: Y ?
[0, 1] quantify the loss ?y(y?
)suffered for predicting y?instead of the gold stan-dard structure y; as a rule, ?y(y?)
= 0 iff y = y?.In the full information setting, for a data distri-bution p(x, y), the learning criterion is defined asminimization of the expected loss with respect tow ?
RdwhereEp(x,y)pw(y?|x)[?y(y?
)](1)=?x,yp(x, y)?y??Y(x)?y(y?
)pw(y?|x).Assume further that output structures given inputsare distributed according to an underlying Gibbsdistribution (a.k.a.
conditional exponential or log-linear model)pw(y|x) = exp(w>?
(x, y))/Zw(x),where ?
: X ?
Y ?
Rdis a joint feature rep-resentation of inputs and outputs, w ?
Rdis anassociated weight vector, and Zw(x) is a normal-ization constant.
For this model, the gradient ofobjective (1) is as follows:?Ep(x,y)pw(y?|x)[?y(y?
)]= Ep(x,y)pw(y?|x)[?y(y?)(?
(x, y?)?Epw(y?|x)[?
(x, y?)])].
(2)Unlike in the full information scenario, banditfeedback in structured prediction means that thegold standard output structure y, with respect towhich the objective function is evaluated, is not re-vealed to the learner.
Thus we can neither evaluatethe task loss ?
nor calculate the gradient (2) of theobjective function (1).
A solution to this problemis to pass the evaluation of the loss function to theuser, i.e, we access the loss directly through userfeedback without assuming existence of a fixedreference y.
We indicate this by dropping the sub-script referring to the gold standard structure in thedefinition of ?.
In all algorithms presented belowwe need to make the following assumptions:1.
We assume a sequence of input structuresxt, t = 1, .
.
.
, T that are generated by a fixed,unknown distribution p(x).Algorithm 1 Bandit Expected Loss Minimization1: Input: sequence of learning rates ?t2: Initialize w03: for t = 0, .
.
.
, T do4: Observe xt5: Calculate Epwt(y|xt)[?
(xt, y)]6: Sample y?t?
pwt(y|xt)7: Obtain feedback ?
(y?t)8: wt+1= wt?
?t?
(y?t)9: ?(?
(xt, y?t)?
Epwt[?
(xt, y)])Algorithm 2 Bandit Pairwise Preference Learning1: Input: sequence of learning rates ?t2: Initialize w03: for t = 0, .
.
.
, T do4: Observe xt5: Calculate Epwt(?yi,yj?|xt)[?
(xt, ?yi, yj?
)]6: Sample ?y?i, y?j?t?
pwt(?yi, yj?
|xt)7: Obtain feedback ?
(?y?i, y?j?t)8: wt+1= wt?
?t?
(?y?i, y?j?t)9: ?(?
(xt, ?y?i, y?j?t)?Epwt[?
(xt, ?yi, yj?)])2.
We use a Gibbs model as sampling distri-bution to perform simultaneous exploitation(use the current best estimate) / exploration(get new information) on output structures.3.
We use feedback to the sampled output struc-tures to construct a parameter update rule thatis an unbiased estimate of the true gradient ofthe respective objective.3.2 Learning Objectives and AlgorithmsBandit Expected Loss Minimization.
Algo-rithm 1 has been presented in Sokolov et al (2015)and minimizes the objective below by stochasticgradient descent optimization.
It is non-convex forthe specific instantiations in this paper:Ep(x)pw(y|x)[?
(y)] (3)=?xp(x)?y?Y(x)?
(y)pw(y|x).Intuitively, the algorithm compares the sampledfeature vector to the average feature vector, andperforms a step into the opposite direction of thisdifference, the more so the higher the loss of thesampled structure is.
In the extreme case, if thesampled structure is correct (?
(y?t) = 0), no up-date is performed.1612Algorithm 3 Bandit Cross-Entropy Minimization1: Input: sequence of learning rates ?t2: Initialize w03: for t = 0, .
.
.
, T do4: Observe xt5: Sample y?t?
pwt(y|xt)6: Obtain feedback g(y?t)7: wt+1= wt?
?tg(y?t)pwt(y?t|xt)8: ?(?
?
(xt, y?t) + Epwt[?
(xt, y?t)])Bandit Pairwise Preference Learning.
De-composing complex problems into a series of pair-wise comparisons has been shown to be advan-tageous for human decision making (Thurstone,1927) and for machine learning (F?urnkranz andH?ullermeier, 2010).
For our case, this idea canbe formalized as an expected loss objective withrespect to a conditional distribution of pairs ofstructured outputs.
Let P(x) = {?yi, yj?
|yi, yj?Y(x)} denote the set of output pairs for an inputx, and let ?
(?yi, yj?)
: P(x) ?
[0, 1] denote atask loss function that specifies a dispreference ofyicompared to yj.
Instantiating objective (3) tothe case of pairs of output structures defines thefollowing objective:Ep(x)pw(?yi,yj?|x)[?
(?yi, yj?)]
.
(4)Stochastic gradient descent optimization of thisobjective leads to Algorithm 2.
The objectiveis again non-convex in the use cases in this pa-per.
Minimization of this objective will assure thathigh probabilities are assigned to pairs with lowloss due to misranking yjover yi.
Stronger as-sumptions on the learned probability ranking canbe made if assumptions of transitivity and asym-metry of the ordering of feedback structures aremade.
For efficient sampling and calculation ofexpectations, we assume a Gibbs model that fac-torizes as follows:pw(?yi, yj?
|x) =ew>(?(x,yi)??(x,yj))??yi,yj??P(x)ew>(?(x,yi)??
(x,yj))= pw(yi|x)p?w(yj|x).If a sample from the p?wdistribution is preferredover a sample from the pwdistribution, this is astrong signal for model correction.Bandit Cross-Entropy Minimization.
Thestandard theory of stochastic optimization pre-dicts considerable improvements in convergencespeed depending on the functional form of theobjective.
This motivates the formalization ofconvex upper bounds on expected normalized lossas presented in Green et al (2014).
Their objec-tive is based on a gain function g : Y ?
[0, 1](in this work, g(y) = 1 ?
?
(y)) that is normal-ized over n-best lists where g?
(y) =g(y)Zg(x)andZg(x) =?y?n-best(x)g(y).
It can be seen as thecross-entropy of model pw(y|x) with respect the?true?
distribution g?(y):Ep(x)g?(y)[?
log pw(y|x)] (5)= ??xp(x)?y?Y(x)g?
(y) log pw(y|x).For a proper probability distribution g?
(y), an ap-plication of Jensen?s inequality to the convex neg-ative logarithm function shows that objective (5) isa convex upper bound on objective (3).
However,normalizing the gain function is prohibitive in abandit setting since it would require to elicit userfeedback for each structure in the output space orn-best list.
We thus work with an unnormalizedgain function which sacrifices the upper bound butpreserves convexity.
This can be seen by rewritingthe objective as the sum of a linear and a convexfunction in w:Ep(x)g(y)[?
log pw(y|x)] (6)= ??xp(x)?y?Y(x)g(y)w>?
(x, y)+?xp(x)(log?y?Y(x)exp(w>?
(x, y)))?
(x),where ?
(x) =?y?Y(x)g(y) is a constant factornot depending on w. The gradient of objective (6)is as follows:?(?
?xp(x)?y?Y(x)g(y) log pw(y|x))= Ep(x)ps(y|x)[g(y)ps(y|x)(?
?
(x, y)+ Epw(y|x)[?
(x, y)])].Minimization of this objective will assign highprobabilities to structures with high gain, as de-sired.
Algorithm 3 minimizes this objective bysampling from a distribution ps(y|x), receivingfeedback, and updating according to the ratio ofgain versus current probability of the sampledstructure.
A positive ratio expresses a preference1613of the sampled structure under the gain functioncompared to the current probability estimate.
Wecompare the sampled feature vector to the averagefeature vector, and we update towards the sampledfeature vector relative to this ratio.
We instanti-ate ps(y|x) to the current update of pwt(y|x) inorder to present progressively more useful struc-tures to the user.
In contrast to Algorithms 1 and 2,each update is thus affected by a probability thatchanges over time and is unreliable when train-ing is started.
This further increases the variancealready present in stochastic optimization.
Wedeal with this problem by clipping too small sam-pling probabilities (Ionides, 2008) or by reduc-ing variance using momentum techniques (Polyak,1964).3.3 Remarks on Theoretical AnalysisConvergence of our algorithms can be analyzedusing results of standard stochastic approximationtheory.
For example, Sokolov et al (2015) analyzethe convergence of Algorithm 1 in the pseudogra-dient framework of Polyak and Tsypkin (1973),relying on the fact that a positive inner product ofthe update vector with the gradient in expectationsuffices for convergence.
Sokolov et al (2016) an-alyze convergence in the framework of stochas-tic first-order optimization of Ghadimi and Lan(2012), relying on the fact that the update vectorsof the algorithms are stochastic gradients of therespective objectives, that is, the update vectorsare unbiased gradient measurements that equal thegradient of the full information objective in expec-tation.
Note that the latter analysis covers the useof constant learning rates.Convergence speed is analyzed in standardstochastic approximation theory in terms of thenumber of iterations needed to reach an accuracyof  for a gradient-based criterionE[?
?J(wt)?2] ?
, (7)where J(wt) denotes the objective to be mini-mized.
Following Ghadimi and Lan (2012), theiteration complexity of the non-convex objectivesunderlying our Algorithms 1 and 2 can be givenas O(1/2) (see Sokolov et al (2016)).
Algo-rithm 3 can be seen as stochastic optimizationof a strongly convex objective that is attainedby adding an `2regularizer?2?w?2with constant?
> 0 to objective (6).
In the standard stochas-tic approximation theory, the iteration complexityof stochastic gradient algorithms using decreasinglearning rates can be given as O(1/) for an ob-jective value-based criterionE[J(wt)]?
J(w?)
?
,where w?= arg minwJ(w) (Polyak, 1987).
Forconstant learning rates, even faster convergencecan be shown provided certain additional condi-tions are met (Solodov, 1998).While the asymptotic iteration complexitybounds predict faster convergence for Algorithm 3compared to Algorithms 1 and 2, and equal con-vergence speed for the latter two, Sokolov et al(2016) show that the hidden constant of varianceof the stochastic gradient can offset this advan-tage empirically.
They find smallest variance ofstochastic updates and fastest empirical conver-gence under the gradient-based criterion (7) forAlgorithm 2.
In the next section we will presentexperimental results that show similar relations offastest convergence of Algorithm 2 under a con-vergence criterion based on task loss evaluation onheldout data.4 ExperimentsExperimental design.
Our experiments followan online learning protocol where on each of a se-quence of rounds, an output structure is randomlysampled, and feedback to it is used to update themodel (Shalev-Shwartz, 2012).
We simulate ban-dit feedback by evaluating ?
against gold stan-dard structures which are never revealed to thelearner (Agarwal et al, 2014).
Training is startedfrom w0= 0 or from an out-of-domain model(for SMT).Following the standard practice of early stop-ping by performance evaluation on a developmentset, we compute convergence speed as the num-ber of iterations needed to find the point of op-timal performance before overfitting on the de-velopment set occurs.
The convergence criterionis thus based on the respective task loss func-tion ?
(y?wt(x)) under MAP prediction y?w(x) =arg maxy?Y(x)pw(y|x), microaveraged on the de-velopment data.
This lets us compare conver-gence across different objectives, and is justifiedby the standard practice of performing online-to-batch conversion by early stopping on a develop-ment set (Littlestone, 1989), or by tolerant train-ing to avoid overfitting (Solodov, 1998).
As afurther measure for comparability of convergence1614task Algorithm 1 Algorithm 2 Algorithm 3Text classification ?t= 1.0 ?t= 10?0.75?t= 10?1CRFOCR T0= 0.4, ?t= 10?3.5T0= 0.1, ?t= 10?4?
= 10?5, k = 10?2, ?t= 10?6Chunking ?t= 10?4?t= 10?4?
= 10?6, k = 10?2, ?t= 10?6SMTNews (n-best, dense) ?t= 10?5?t= 10?4.75?
= 10?4, ?
= 0.99, ?t= 10?6/?tNews (h-graph, sparse) ?t= 10?5?t= 10?4?
= 10?6, k = 5 ?
10?3, ?t= 10?6Table 1: Metaparameter settings determined on dev sets for constant learning rate ?t, temperature co-efficient T0for annealing under the schedule T = T0/3?epoch + 1 (Rose, 1998; Arun et al, 2010),momentum coefficient min{1 ?
1/(t/2 + 2), ?}
(Polyak, 1964; Sutskever et al, 2013), clipping con-stant k used to replace pwt(y?t|xt) with max{pwt(y?t|xt), k} in line 7 of Algorithm 3 (Ionides, 2008), `2regularization constant ?.
Unspecified parameters are set to zero.speeds across algorithms, we employ small con-stant learning rates in all experiments.
The useof constant learning rates for Algorithms 1 and 2is justified by the analysis of Ghadimi and Lan(2012).
For Algorithm 3, the use of constant learn-ing rates effectively compares convergence speedtowards an area in close vicinity of a local mini-mum in the search phase of the algorithm (Bottou,2004).The development data are also used for meta-parameter search.
Optimal configurations arelisted in Table 1.
Final testing was done by com-puting ?
on a further unseen test set using themodel found by online-to-batch conversion.
Forbandit-type algorithms, final results are averagedover 3 runs with different random seeds.
For sta-tistical significance testing of results against base-lines we use Approximate Randomization testing(Noreen, 1989).Multiclass classification.
Multiclass text clas-sification on the Reuters RCV1 dataset (Lewiset al, 2004) is a standard benchmark for (sim-plified) structured prediction that has been usedin a bandit setup by Kakade et al (2008).
Thesimplified problem uses a binary ?
function in-dicating incorrect assignment of one out of 4classes.
Following Kakade et al (2008), we useddocuments with exactly one label from the setof labels {CCAT, ECAT, GCAT, MCAT} and con-verted them to tfidf word vectors of dimension244,805 in training.
The data were split intothe sets train (509,381 documents from originaltest pt[0-2].dat files), dev (19,486 docs:every 8th entry from test pt3.dat and test(19,806 docs from train.dat).As shown in Table 2 (row 1), all loss results aresmall and comparable since the task is relativelyeasy.
For comparison, the partial informationclassification algorithm Banditron (Kakade et al,2008) (after adjusting the exploration/exploitationconstant on the dev set) scored 0.047 on the testset.
However, our main interest is in convergencespeed.
Table 3 (row 1) shows that pairwise rank-ing (Algorithm 2) yields fastest convergence by afactor of 2-4 compared to the other bandit algo-rithms.
Table 1 confirms that this improvementis not attributable to larger learning rates (Algo-rithm 2 employs a similar or smaller learning ratethan Algorithms 1 and 3, respectively.
)Sequence labeling for OCR and chunking.Handwritten optical character recognition (OCR)is a standard benchmark task for structured pre-diction (Taskar et al, 2003), where the Ham-ming distance between the predicted word andthe gold standard labeling (normalized by wordlength) is assumed as the ?
function.
We usedtheir dataset of 6,876 handwritten words, from 150human subjects, under a split where 5,546 exam-ples (folds 2-9) were used as train set, 704 exam-ples (fold 1) as dev, and 626 (fold 0) as test set.We assumed the classical linear-chain ConditionalRandom Field (CRF) (Lafferty et al, 2001) modelwith input images xiat every ith node, tabularstate-transition probabilities between 28 possiblelabels of the (i ?
1)th and ith node (Latin lettersplus two auxiliary start and stop states).3To test the CRF-based model also with sparsefeatures, we followed Sha and Pereira (2003) inapplying CRFs to the noun phrase chunking task3The feature set is composed of a 16 ?
8 binary pixelrepresentation for each character, yielding 28?16?8+282=4, 368 features for the training set.
We based our code on thepystruct kit (M?uller and Behnke, 2014).1615task gain/lossfull information partial informationAlg.
1 Alg.
2 Alg.
3Text classification 0/1 ?
percep., ?
= 10?60.040 0.0306?0.00040.083?0.0020.035?0.001CRFOCR (dense) Hamming ?
likelihood 0.099 0.261?0.0030.332?0.0110.257?0.004Chunking (sparse) F1-score ?
likelihood 0.935 0.923?0.0020.914?0.0020.891?0.005out-of-domain in-domain Alg.
1 Alg.
2 Alg.
3SMTNews (n-best list, dense)BLEU ?0.2588 0.2841 0.2689?0.00030.2745?0.00040.2763?0.0005News (hypergraph, sparse) 0.2651 0.2831 0.2667?0.000080.2733?0.00050.2713?0.001Table 2: Test set evaluation for full information lower and upper bounds and partial information banditlearners (expected loss, pairwise loss, cross-entropy).
?
and ?
indicate the direction of improvement forthe respective evaluation metric.on the CoNLL-2000 dataset4.
We split the origi-nal training set into a dev set (top 1,000 sent.)
andused the rest as train set (7,936 sent.
); the test setwas kept intact (2,012 sent.).
For an input sentencex, each CRF node xicarries an observable wordand its part-of-speech tag, and has to be assigneda chunk tag ciout of 3 labels: Beginning, Inside,or Outside (of a noun phrase).
Chunk labels arenot nested.
As in Sha and Pereira (2003), we usesecond order Markov dependencies (bigram chunktags), such that for sentence position i, the state isyi= ci?1ci, increasing the label set size from 3to 9.
Out of the full list of Sha and Pereira (2003)?sfeatures we implemented all except two featuretemplates, yi= y and c(yi) = c, to simplify im-plementation.
Impossible bigrams (OI) and labeltransitions of the pattern ?O ?
I?
were prohib-ited by setting the respective potentials to??.
Asthe active feature count in the train set was just un-der 2M, we hashed all features and weights into asparse array of 2M entries.
Despite the reducedtrain size and feature set, and hashing, our full in-formation baseline trained with log-likelihood at-tained the test F1-score of 0.935, which is compa-rable to the original result of 0.9438.Table 2 (rows 2-3) and Table 3 (rows 2-3) showevaluation and convergence results for the OCRand chunking tasks.
For the chunking task, the F1-score results obtained for bandit learning are closeto the full-information baseline.
For the OCR task,bandit learning does decrease Hamming loss, butit does not quite achieve full-information perfor-mance.
However, pairwise ranking (Algorithm 2)again converges faster than the alternative banditalgorithms by a factor of 2-4, despite similar learn-ing rates for Algorithms 1 and 2 and a compensa-4http://www.cnts.ua.ac.be/conll2000/chunking/task Alg.
1 Alg.
2 Alg.
3Text classification 2.0M 0.5M 1.1MCRFOCR 14.4M 9.3M 37.9MChunking 7.5M 4.7M 5.9MSMTNews (n-best, dense) 3.8M 1.2M 1.2MNews (h-graph, sparse) 370k 115k 281kTable 3: Number of iterations required to meetstopping criterion on development data.tion of smaller learning rates in Algorithm 3 byvariance reduction and regularization.Discriminative ranking for SMT.
FollowingSokolov et al (2015), we apply bandit learningto simulate personalized MT where a given SMTsystem is adapted to user style and domain basedon feedback to predicted translations.
We per-form French-to-English domain adaptation fromEuroparl to NewsCommentary domains using thedata of Koehn and Schroeder (2007).
One differ-ence of our experiment compared to Sokolov etal.
(2015) is our use of the SCFG decoder cdec(Dyer et al, 2010) (instead of the phrase-basedMoses decoder).
Furthermore, in addition to ban-dit learning for re-ranking on unique 5,000-bestlists, we perform ranking on hypergraphs with re-decoding after each update.
Sampling and com-putation of expectations on the hypergraph usesthe Inside-Outside algorithm over the expectationsemiring (Li and Eisner, 2009).
The re-rankingmodel used 15 dense features (6 lexicalized re-ordering features, two (out-of- and in-domain) lan-guage models, 5 translation model features, dis-tortion and word penalty).
The hypergraph ex-periments used additionally lexicalized sparse fea-tures: rule-id features, rule source and target bi-gram features, and rule shape features.16160.250.2550.260.2650.270  100000  200000  300000  400000  500000  600000BLEUondev#training samples1) expected-loss 2) pairwise-ranking 3) cross-entropyFigure 1: Learning curves for task loss BLEU on development data for SMT hypergraph re-decodingmodels, together with averages over three runs of the respective algorithms.For all SMT experiments we tokenized, lower-cased and aligned words using cdec tools, trained4-gram in-domain and out-of-domain languagemodels (on the English sides of Europarl andin-domain NewsCommentary) For dense featuremodels, the out-of-domain baseline SMT modelwas trained on 1.6M parallel Europarl data andtuned with cdec?s lattice MERT (Och, 2003)on out-of-domain Europarl dev2006 dev set(2,000 sent.).
The full-information in-domainSMT model tuned by MERT on news in-domainsets (nc-dev2007, 1,057 sent.)
gives the rangeof possible improvements by the difference ofits BLEU score to the one of the out-of-domainmodel (2.5 BLEU points).
For sparse featuremodels, in-domain and out-of-domain baselineswere trained on the same data using MIRA (Chi-ang, 2012).
The in-domain MIRA model contains133,531 active features, the out-of-domain MIRAmodel 214,642.
MERT and MIRA runs for bothsettings were repeated 7 times and median resultsare reported.Learning under bandit feedback starts at thelearned weights of the out-of-domain medianmodels.
It uses the parallel in-domain data(news-commentary, 40,444 sent.)
to simu-late bandit feedback, by evaluating the sampledtranslation against the reference using as loss func-tion ?
a smoothed per-sentence 1 ?
BLEU (zeron-gram counts being replaced with 0.01).
Forpairwise preference learning we use binary feed-back resulting from the comparison of the BLEUscores of the sampled translations.
To speed uptraining for hypergraph re-decoding, the train-ing instances were reduced to those with at most60 words (38,350 sent.).
Training is distributedacross 38 shards using multitask-based feature se-lection for sparse models (Simianer et al, 2012),where after each epoch of distributed training,the top 10k features across all shards are se-lected, all other features are set to zero.
Themeta-parameters were adjusted on the in-domaindev sets (nc-devtest2007, 1,064 parallel sen-tences).
The final results are obtained on separatein-domain test sets (nc-test2007, 2,007 sen-tences) by averaging three independent runs forthe optimal dev set meta-parameters.The results for n-best re-ranking in Table 2(4th row) show statistically significant improve-ments of 1-2 BLEU points over the out-of-domainSMT model (that includes an in-domain languagemodel) for all bandit learning methods, confirm-ing the results of Sokolov et al (2015) for a differ-ent decoder.
Similarly, the results for hypergraphre-coding with sparse feature models (row 5 inTable 2) show significant improvements over theout-of-domain baseline for all bandit learners.
Ta-ble 3 (row 4) shows the convergence speed for n-best re-ranking, which is similar for Algorithms 2and 3, and improved over Algorithm 1 by a factorof 3.
For hypergraph re-decoding, Table 3 (row 5)shows fastest convergence for Algorithm 2 com-1617pared to Algorithms 1 and 3 by a factor of 2-4.5Again, we note that for both n-best re-ranking andhypergraph re-decoding, learning rates are similarfor Algorithms 1 and 2, and smaller learning ratesin Algorithm 3 are compensated by variance re-duction or regularization.Figure 1 shows the learning curves of BLEU forSMT hypergraph re-decoding on the developmentset that were used to find the stopping points.
Foreach algorithm, we show learning curves for threeruns with different random seeds, together with anaverage learning curve.
We see that Algorithm 2,optimizing the pairwise preference ranking objec-tive, reaches the stopping point of peak perfor-mance on development data fastest, followed byAlgorithms 1 and 3.
Furthermore, the larger vari-ance of the runs of Algorithm 3 is visible, despitethe smallest learning rate used.5 ConclusionWe presented objectives and algorithms for struc-tured prediction from bandit feedback, with a fo-cus on improving convergence speed and ease ofelicitability of feedback.
We investigated the per-formance of all algorithms by test set performanceon different tasks, however, the main interest ofthis paper was a comparison of convergence speedacross different objectives by early stopping on aconvergence criterion based on heldout data per-formance.
Our experimental results on differentNLP tasks showed a consistent advantage of con-vergence speed under this criterion for bandit pair-wise preference learning.
In light of the standardstochastic approximation analysis, which predictsa convergence advantage for strongly convex ob-jectives over convex or non-convex objectives, thisresult is surprising.
However, the result can be ex-plained by considering important empirical factorssuch as the variance of stochastic updates.
Ourexperimental results support the numerical resultsof smallest stochastic variance and fastest conver-gence in gradient norm (Sokolov et al, 2016) byconsistent fastest empirical convergence for ban-dit pairwise preference learning under the criterionof early stopping on heldout data performance.Given the advantages of faster convergence andthe fact that only relative feedback in terms ofcomparative evaluations is required, bandit pair-5The faster convergence speed hypergraph re-decodingcompared to n-best re-ranking is due to the distributed featureselection and thus orthogonal to the comparison of objectivefunctions that is of interest here.wise preference learning is a promising frameworkfor future real-world interactive learning.AcknowledgmentsThis research was supported in part by the Ger-man research foundation (DFG), and in part by aresearch cooperation grant with the Amazon De-velopment Center Germany.ReferencesAlekh Agarwal, Ofer Dekel, and Liu Xiao.
2010.
Opti-mal algorithms for online convex optimization withmulti-point bandit feedback.
In COLT, Haifa, Israel.Alekh Agarwal, Daniel Hsu, Satyen Kale, John Lang-ford, Lihong Li, and Robert E. Schapire.
2014.Taming the monster: A fast and simple algorithmfor contextual bandits.
In ICML, Beijing, China.Abhishek Arun, Barry Haddow, and Philipp Koehn.2010.
A unified approach to minimum risk train-ing and decoding.
In Workshop on SMT and Metrics(MATR), Uppsala, Sweden.Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.2002a.
Finite-time analysis of the multiarmed ban-dit problem.
Machine Learning, 47:235?256.Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, andRobert E. Schapire.
2002b.
The nonstochastic mul-tiarmed bandit problem.
SIAM J. on Computing,32(1):48?77.Nicola Bertoldi, Patrick Simianer, Mauro Cettolo,Katharina W?aschle, Marcello Federico, and StefanRiezler.
2014.
Online adaptation to post-edits forphrase-based statistical machine translation.
Ma-chine Translation, 29:309?339.Dimitri P. Bertsekas and John N. Tsitsiklis.
1996.Neuro-Dynamic Programming.
Athena Scientific.L?eon Bottou.
2004.
Stochastic learning.
In OlivierBousquet, Ulrike von Luxburg, and Gunnar R?atsch,editors, Advanced Lectures on Machine Learning,pages 146?168.
Springer, Berlin.S.R.K.
Branavan, Harr Chen, Luke S. Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In ACL, Suntec,Singapore.S?ebastian Bubeck and Nicol`o Cesa-Bianchi.
2012.
Re-gret analysis of stochastic and nonstochastic multi-armed bandit problems.
Foundations and Trends inMachine Learning, 5(1):1?122.Nicol`o Cesa-Bianchi and G?abor Lugosi.
2012.
Com-binatorial bandits.
J. of Computer and System Sci-ences, 78:1401?1422.1618Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agar-wal, Hal Daume, and John Langford.
2015.
Learn-ing to search better than your teacher.
In ICML,Lille, France.Olivier Chapelle, Eren Masnavoglu, and Romer Ros-ales.
2014.
Simple and scalable response predictionfor display advertising.
ACM Trans.
on IntelligentSystems and Technology, 5(4).David Chiang.
2012.
Hope and fear for discrimina-tive training of statistical translation models.
JMLR,12:1159?1187.Corinna Cortes, Mehryar Mohri, and Asish Rastogi.2007.
Magnitude-preserving ranking algorithms.
InICML, Corvallis, OR.Varsha Dani, Thomas P. Hayes, and Sham M. Kakade.2007.
The price of bandit information for online op-timization.
In NIPS, Vancouver, Canada.Michael Denkowski, Chris Dyer, and Alon Lavie.2014.
Learning from post-editing: Online modeladaptation for statistical machine translation.
InEACL, Gothenburg, Sweden.John C. Duchi, Michael I. Jordan, Martin J. Wain-wright, and Andre Wibisono.
2015.
Optimal ratesfor zero-order convex optimization: The power oftwo function evaluations.
IEEE Translactions on In-formation Theory, 61(5):2788?2806.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In ACL Demo, Uppsala, Sweden.Yoav Freund, Ray Iyer, Robert E. Schapire, and YoramSinger.
2003.
An efficient boosting algorithm forcombining preferences.
JMLR, 4:933?969.Johannes F?urnkranz and Eyke H?ullermeier.
2010.Preference learning and ranking by pairwise com-parison.
In Johannes F?urnkranz and EykeH?ullermeier, editors, Preference Learning.
Springer.Saeed Ghadimi and Guanghui Lan.
2012.
Stochas-tic first- and zeroth-order methods for nonconvexstochastic programming.
SIAM J. on Optimization,4(23):2342?2368.Kevin Gimpel and Noah A. Smith.
2010.
Softmax-margin training for structured log-linear models.Technical Report CMU-LTI-10-008, Carnegie Mel-lon University, Pittsburgh, PA.Spence Green, Sida I. Wang, Jason Chuang, JeffreyHeer, Sebastian Schuster, and Christopher D. Man-ning.
2014.
Human effort and machine learnabil-ity in computer aided translation.
In EMNLP, Doha,Qatar.Xiaodong He and Li Deng.
2012.
Maximum ex-pected BLEU training of phrase and lexicon trans-lation models.
In ACL, Jeju Island, Korea.Ralf Herbrich, Thore Graepel, and Klaus Obermayer.2000.
Large margin rank boundaries for ordinal re-gression.
In Advances in Large Margin Classifiers,pages 115?132.
Cambridge, MA.Edward L. Ionides.
2008.
Truncated importance sam-pling.
J. of Comp.
and Graph.
Stat., 17(2):295?311.Kevin G. Jamieson, Robert D. Nowak, and BenjaminRecht.
2012.
Query complexity of derivative-freeoptimization.
In NIPS, Lake Tahoe, CA.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In KDD, New York, NY.Sham M. Kakade, Shai Shalev-Shwartz, and AmbujTewari.
2008.
Efficient bandit algorithms for onlinemulticlass prediction.
In ICML, Helsinki, Finland.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In WMT, Prague, Czech Republic.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In ICML, San Francisco, CA.John Langford and Tong Zhang.
2007.
The epoch-greedy algorithm for contextual multi-armed ban-dits.
In NIPS, Vancouver, Canada.David D. Lewis, Yiming Yang, Tony G. Rose, and FanLi.
2004.
RCV1: A new benchmark collection fortext categorization research.
JMLR, 5:361?397.Zhifei Li and Jason Eisner.
2009.
First-and second-order expectation semirings with applications tominimum-risk training on translation forests.
InEMNLP, Edinburgh, UK.Lihong Li, Wei Chu, John Langford, and Robert E.Schapire.
2010.
A contextual-bandit approachto personalized news article recommendation.
InWWW, Raleigh, NC.Nick Littlestone.
1989.
From on-line to batch learning.In COLT, Santa Cruz, CA.Andreas C. M?uller and Sven Behnke.
2014. pystruct- learning structured prediction in python.
JMLR,15:2055?2060.Eric W. Noreen.
1989.
Computer Intensive Meth-ods for Testing Hypotheses.
An Introduction.
Wiley,New York.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In HLT-NAACL,Edmonton, Canada.Boris T. Polyak and Yakov Z. Tsypkin.
1973.
Pseu-dogradient adaptation and training algorithms.
Au-tomation and remote control, 34(3):377?397.Boris T. Polyak.
1964.
Some methods of speeding upthe convergence of iteration methods.
USSR Comp.Math.
and Math.
Phys., 4(5):1?17.1619Boris T. Polyak.
1987.
Introduction to Optimization.Optimization Software, Inc., New York.Marc?Aurelio Ranzato, Sumit Chopra, Michael Auli,and Wojciech Zaremba.
2016.
Sequence level train-ing with recurrent neural networks.
In ICLR, SanJuan, Puerto Rico.Kenneth Rose.
1998.
Deterministic annealing for clus-tering, compression, classification, regression andrelated optimization problems.
IEEE, 86(11).Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In NAACL, Edmon-ton, Canada.Shai Shalev-Shwartz.
2012.
Online learning and on-line convex optimization.
Foundations and Trendsin Machine Learning, 4(2):107?194.Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012.Joint feature selection in distributed stochastic learn-ing for large-scale discriminative training in SMT.In ACL, Jeju Island, Korea.David A. Smith and Jason Eisner.
2006.
Minimumrisk annealing for training log-linear models.
InCOLING-ACL, Sydney, Australia.Artem Sokolov, Stefan Riezler, and Tanguy Urvoy.2015.
Bandit structured prediction for learning fromuser feedback in statistical machine translation.
InMT Summit XV, Miami, FL.Artem Sokolov, Julia Kreutzer, and Stefan Riezler.2016.
Stochastic structured prediction under banditfeedback.
CoRR, abs/1606.00739.Mikhail V. Solodov.
1998.
Incremental gradi-ent algorithms with stepsizes bounded away fromzero.
Computational Optimization and Applica-tions, 11:23?35.Ilya Sutskever, James Martens, George E. Dahl, andGeoffrey E. Hinton.
2013.
On the importance ofinitialization and momentum in deep learning.
InICML, Atlanta, GA.Richard S. Sutton, David McAllester, Satinder Singh,and Yishay Mansour.
2000.
Policy gradient meth-ods for reinforcement learning with function approx-imation.
In NIPS, Vancouver, Canada.Csaba Szepesv?ari.
2009.
Algorithms for Reinforce-ment Learning.
Morgan & Claypool.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003.Max-margin markov networks.
In NIPS, Vancouver,Canada.Louis Leon Thurstone.
1927.
A law of comparativejudgement.
Psychological Review, 34:278?286.Yisong Yue and Thorsten Joachims.
2009.
Interac-tively optimizing information retrieval systems asa dueling bandits problem.
In ICML, Montreal,Canada.Alan Yuille and Xuming He.
2012.
Probabilistic mod-els of vision and max-margin methods.
Frontiers ofElectrical and Electronic Engineering, 7(1):94?106.1620
