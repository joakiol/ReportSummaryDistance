Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 181?184,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsEvolving new lexical association measures using genetic programmingJan ?Snajder Bojana Dalbelo Bas?ic?
Sas?a Petrovic?
Ivan Sikiric?Faculty of Electrical Engineering and Computing, University of ZagrebUnska 3, Zagreb, Croatia{jan.snajder, bojana.dalbelo, sasa.petrovic, ivan.sikiric}@fer.hrAbstractAutomatic extraction of collocations fromlarge corpora has been the focus of many re-search efforts.
Most approaches concentrateon improving and combining known lexicalassociation measures.
In this paper, we de-scribe a genetic programming approach forevolving new association measures, which isnot limited to any specific language, corpus,or type of collocation.
Our preliminary experi-mental results show that the evolved measuresoutperform three known association measures.1 IntroductionA collocation is an expression consisting of two ormore words that correspond to some conventionalway of saying things (Manning and Schu?tze, 1999).Related to the term collocation is the term n-gram,which is used to denote any sequence of n words.There are many possible applications of colloca-tions: automatic language generation, word sensedisambiguation, improving text categorization, in-formation retrieval, etc.
As different applicationsrequire different types of collocations that are of-ten not found in dictionaries, automatic extractionof collocations from large textual corpora has beenthe focus of much research in the last decade; see,for example, (Pecina and Schlesinger, 2006; Evertand Krenn, 2005).Automatic extraction of collocations is usuallyperformed by employing lexical association mea-sures (AMs) to indicate how strongly the wordscomprising an n-gram are associated.
However, theuse of lexical AMs for the purpose of collocationextraction has reached a plateau; recent researchin this field has focused on combining the existingAMs in the hope of improving the results (Pecinaand Schlesinger, 2006).
In this paper, we proposean approach for deriving new AMs for collocationextraction based on genetic programming.
A simi-lar approach has been usefully applied in text min-ing (Atkinson-Abutridy et al, 2004) as well as ininformation retrieval (Gordon et al, 2006).Genetic programming is an evolutionary compu-tational technique designed to mimic the process ofnatural evolution for the purpose of solving complexoptimization problems by stochastically searchingthrough the whole space of possible solutions (Koza,1992).
The search begins from an arbitrary seedof possible solutions (the initial population), whichare then improved (evolved) through many iterationsby employing the operations of selection, crossover,and mutation.
The process is repeated until a termi-nation criterion is met, which is generally defined bythe goodness of the best solution or the expiration ofa time limit.2 Genetic programming of AMs2.1 AM representationIn genetic programming, possible solutions (in ourcase lexical AMs) are mathematical expressions rep-resented by a tree structure (Koza, 1992).
The leavesof the tree can be constants, or statistical or linguisticinformation about an n-gram.
A constant can be anyreal number in an arbitrarily chosen interval; our ex-periments have shown that variation of this intervaldoes not affect the performance.
One special con-stant that we use is the number of words in the cor-pus.
The statistical information about an n-gram canbe the frequency of any part of the n-gram.
For ex-181ample, for a trigram abc the statistical informationcan be the frequency f(abc) of the whole trigram,frequencies f(ab) and f(bc) of the digrams, andthe frequencies of individual words f(a), f(b), andf(c).
The linguistic information about an n-gram isthe part-of-speech (POS) of any one of its words.Inner nodes in the tree are operators.
The binaryoperators are addition, subtraction, multiplication,and division.
We also use one unary operator, thenatural logarithm, and one ternary operator, the IF-THEN-ELSE operator.
The IF-THEN-ELSE nodehas three descendant nodes: the left descendant isthe condition in the form ?i-th word of the n-gramhas a POS tag T,?
and the other two descendants areoperators or constants.
If the condition is true, thenthe subexpression corresponding to the middle de-scendant is evaluated, otherwise the subexpressioncorresponding to the right descendant is evaluated.The postfix expression of an AM can be obtainedby traversing its tree representation in postorder.Figure 1 shows the representation of the Dice co-efficient using our representation.2.2 Genetic operatorsThe crossover operator combines two parent solu-tions into a new solution.
We defined the crossoveroperator as follows: from each of the two parents,one node is chosen randomly, excluding any nodesthat represent the condition of the IF-THEN-ELSEoperator.
A new solution is obtained by replacingthe subtree of the chosen node of the first parent withthe subtree of the chosen node of the second parent.This method of defining the crossover operator is thesame as the one described in (Gordon et al, 2006).The mutation operator introduces new ?geneticmaterial?
into a population by randomly changinga solution.
In our case, the mutation operator cando one of two things: either remove a randomly se-lected inner node (with probability of 25%), or insertan inner node at a random position in the tree (withprobability of 75%).
If a node is being removedfrom the tree, one of its descendants (randomly cho-sen) takes its place.
An exception to this rule is theIF-THEN-ELSE operator, which cannot be replacedby its condition node.
If a node is being inserted,a randomly created operator node replaces an exist-ing node that then becomes a descendant of the newnode.
If the inserted node is not a unary operator,the required number of random leaves is created.The selection operator is used to copy the best so-lutions into the next iteration.
The goodness of thesolution is determined by the fitness function, whichassigns to each solution a number indicating howgood that particular solution actually is.
We mea-sure the goodness of an AM in terms of its F1 score,obtained from the precision and recall computed ona random sample consisting of 100 positive n-grams(those considered collocations) and 100 negative n-grams (non-collocations).
These n-grams are rankedaccording to the AM value assigned to them, afterwhich we compute the precision and recall by con-sidering first n best-ranked n-grams as positives andthe rest as negatives, repeating this for each n be-tween 1 and 200.
The best F1 score is then taken asthe AM?s goodness.Using the previous definition of the fitness func-tion, preliminary experiments showed that solutionssoon become very complex in terms of number ofnodes in the tree (namely, on the order of tensof thousands).
This is a problem both in termsof space and time efficiency; allowing unlimitedgrowth of the tree quickly consumes all computa-tional resources.
Also, it is questionable whetherthe performance benefits from the increased size ofthe solution.
Thus, we modified the fitness func-tion to also take into account the size of the tree(that is, the less nodes a tree has, the better).
Fa-voring shorter solutions at the expense of some lossin performance is known as parsimony, and it hasalready been successfully used in genetic program-ming (Koza, 1992).
Therefore, the final formula forthe fitness function we used incorporates the parsi-mony factor and is given byfitness(j) = F1(j) + ?Lmax ?
L(j)Lmax, (1)where F1(j) is the F1 score (ranging from 0 to 1) ofthe solution j, ?
is the parsimony factor, Lmax is themaximal size (measured in number of nodes), andL(j) is the size of solution j.
By varying ?
we cancontrol how much loss of performance we will tol-erate in order to get smaller, more elegant solutions.Genetic programming algorithms usually iterateuntil a termination criterion is met.
In our case, thealgorithm terminates when a certain number, k, ofiterations has passed without an improvement in the182Dice(a, b, c) = f(abc)f(a)+f(b)+f(c)Figure 1: Dice coefficient for digrams represented by treeresults.
To prevent the overfitting problem, we mea-sure this improvement on another sample (valida-tion sample) that also consists of 100 collocationsand 100 non-collocations.3 Preliminary results3.1 Experimental settingWe use the previously described genetic program-ming approach to evolve AMs for extracting collo-cations consisting of three words from a corpus of7008 Croatian legislative documents.
Prior to this,words from the corpus were lemmatized and POStagged.
Conjunctions, propositions, pronouns, in-terjections, and particles were treated as stop-wordsand tagged with a POS tag X .
N-grams starting orending with a stopword, or containing a verb, werefiltered out.
For evaluation purposes we had a hu-man expert annotate 200 collocations and 200 non-collocations, divided into the evaluation and valida-tion sample.
We considered an n-gram to be a collo-cation if it is a compound noun, terminological ex-pression, or a proper name.
Note that we could haveadopted any other definition of a collocation, sincethis definition is implicit in the samples provided.In our experiments, we varied a number of ge-netic programming parameters.
The size of the ini-tial population varied between 50 and 50 thousandrandomly generated solutions.
To examine the ef-fects of including some known AMs on the perfor-mance, the following AMs had a 50% chance ofbeing included in the initial population: pointwisemutual information (Church and Hanks, 1990), theDice coefficient, and the heuristic measure definedin (Petrovic?
et al, 2006):H(a, b, c) =??
?2 log f(abc)f(a)f(c) if POS (b) = X,log f(abc)f(a)f(b)f(c) otherwise.For the selection operator we used the well-knownthree-tournament selection.
The probability of mu-tation was chosen from the interval [0.0001, 0.3],and the parsimony factor ?
from the interval[0, 0.05], thereby allowing a maximum of 5% lossof F1 in favor of smaller solutions.
The maximalsize of the tree in nodes was chosen from the inter-val [20, 1000].
After the F1 score for the validationsample began dropping, the algorithm would con-tinue for another k iterations before stopping.
Theparameter k was chosen from the interval [104, 107].The experiments were run with 800 different randomcombinations of the aforementioned parameters.3.2 ResultsAround 20% of the evolved measures (that is, the so-lutions that remained after the algorithm terminated)achieved F1 scores of over 80% on both the evalu-ation and validation samples.
This proportion was13% in the case when the initial population did notinclude any known AMs, and 23% in the case whenit did, thus indicating that including known AMs inthe initial population is beneficial.
The overall bestsolution had 205 nodes and achieved an F1 score of88.4%.
In search of more elegant AMs, we singledout solutions that had less than 30 nodes.
Amongthese, a solution that consisted of 13 nodes achievedthe highest F1.
This measure is given byM13(a, b, c) =???
?0.423f(a)f(c)f2(abc) if POS (b) = X,1 ?
f(b)f(abc) otherwise.The association measure M13 is particularly inter-esting because it takes into account whether themiddle word in a trigram is a stopword (denotedby the POS tag X).
This supports the claim laidout in (Petrovic?
et al, 2006) that the trigrams con-taining stopwords (e.g., cure for cancer) should betreated differently, in that the frequency of the stop-word should be ignored.
It is important to note thatthe aforementioned measure H was not included inthe initial population from which M13 evolved.
Itis also worthwhile noting that in such populations,out of 100 best evolved measures, all but four ofthem featured a condition identical to that of M13(POS (b) = X).
In other words, the majority ofthe measures evolved this condition completely in-dependently, without H being included in the initialpopulation.1831 2 3 4 5 6 7 8 9 100102030405060708090100Number of n?grams (?
105)F 1scoreDicePMIHM13M205Figure 2: Comparison of association measures on a cor-pus of 7008 Croatian documentsFigure 2 shows the comparison of AMs in termsof their F1 score obtained on the corpus of 7008documents.
The x axis shows the number of nbest ranked n-grams that are considered positives(we show only the range of n in which all the AMsachieve their maximum F1; all measures tend to per-form similarly with increasing n).
The maximumF1 score is achieved if we take 5 ?
105 n-gramsranked best by the M205 measure.
From Fig.
2 wecan see that the evolved AMs M13 and M205 outper-formed the other three considered AMs.
For exam-ple, collocations kosilica za travu (lawn mower) anddigitalna obrada podataka (digital data processing)were ranked at the 22th and 34th percentile accord-ing to Dice, whereas they were ranked at the 97thand 87th percentile according to M13.4 ConclusionIn this paper we described a genetic programmingapproach for evolving new lexical association mea-sures in order to extract collocations.The evolved association measure will perform atleast as good as any other AM included in the initialpopulation.
However, the evolved association mea-sure may be a complex expression that defies inter-pretation, in which case it may be treated as a black-box suitable for the specific task of collocation ex-traction.
Our approach only requires an evaluationsample, thus it is not limited to any specific type ofcollocation, language or corpus.The preliminary experiments, conducted on a cor-pus of Croatian documents, showed that the bestevolved measures outperformed other considered as-sociation measures.
Also, most of the best evolvedassociation measures took into account the linguis-tic information about an n-gram (the POS of the in-dividual words).As part of future work, we intend to apply our ap-proach to corpora in other languages and comparethe results with existing collocation extraction sys-tems.
We also intend to apply our approach to collo-cations consisting of more than three words, and toexperiment with additional linguistic features.AcknowledgmentsThis work has been supported by the Governmentof Republic of Croatia, and Government of Flan-ders under the grant No.
036-1300646-1986 andKRO/009/06.ReferencesJohn Atkinson-Abutridy, Chris Mellish, and StuartAitken.
2004.
Combining information extraction withgenetic algorithms for text mining.
IEEE IntelligentSystems, 19(3):22?30.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22?29.Stephan Evert and Brigitte Krenn.
2005.
Using smallrandom samples for the manual evaluation of statisti-cal evaluation measures.
Computer Speech and Lan-guage, 19(4):450?466.Michael Gordon, Weiguo Fan, and Praveen Pathak.2006.
Adaptive web search: Evolving a programthat finds information.
IEEE Intelligent Systems,21(5):72?77.John R. Koza.
1992.
Genetic programming: On the pro-gramming of computers by means of natural selection.MIT Press.Christopher Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.MIT Press, Cambridge, MA, USA.Pavel Pecina and Pavel Schlesinger.
2006.
Combin-ing association measures for collocation extraction.
InProc.
of the COLING/ACL 2006, pages 651?658.Sas?a Petrovic?, Jan ?Snajder, Bojana Dalbelo Bas?ic?, andMladen Kolar.
2006.
Comparison of collocation ex-traction measures for document indexing.
J. of Com-puting and Information Technology, 14(4):321?327.184
