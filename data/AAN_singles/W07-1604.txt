Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 25?30,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsDetection of Grammatical Errors Involving PrepositionsMartin ChodorowHunter College of CUNY695 Park AvenueNew York, NY, 10021mchodoro@hunter.cuny.eduJoel R. Tetreault and Na-Rae HanEducational Testing ServicesRosedale RoadPrinceton, NJ, 08541jtetreault|nzhan@ets.orgAbstractThis paper presents ongoing work on the de-tection of preposition errors of non-nativespeakers of English.
Since prepositionsaccount for a substantial proportion of allgrammatical errors by ESL (English as aSecond Language) learners, developing anNLP application that can reliably detectthese types of errors will provide an invalu-able learning resource to ESL students.
Toaddress this problem, we use a maximumentropy classifier combined with rule-basedfilters to detect preposition errors in a corpusof student essays.
Although our work is pre-liminary, we achieve a precision of 0.8 witha recall of 0.3.1 IntroductionThe National Clearinghouse for English LanguageAcquisition (2002) estimates that 9.6% of the stu-dents in the US public school population speak alanguage other than English and have limited En-glish proficiency.
Clearly, there is a substantial andincreasing need for tools for instruction in Englishas a Second Language (ESL).In particular, preposition usage is one of the mostdifficult aspects of English grammar for non-nativespeakers to master.
Preposition errors account fora significant proportion of all ESL grammar errors.They represented the largest category, about 29%,of all the errors by 53 intermediate to advanced ESLstudents (Bitchener et al, 2005), and 18% of all er-rors reported in an intensive analysis of one Japanesewriter (Murata and Ishara, 2004).
Preposition errorsare not only prominent among error types, they arealso quite frequent in ESL writing.
Dalgish (1985)analyzed the essays of 350 ESL college studentsrepresenting 15 different native languages and re-ported that preposition errors were present in 18%of sentences in a sample of text produced by writ-ers from first languages as diverse as Korean, Greek,and Spanish.The goal of the research described here is to pro-vide software for detecting common grammar andusage errors in the English writing of non-native En-glish speakers.
Our work targets errors involvingprepositions, specifically those of incorrect preposi-tion selection, such as arrive to the town, and thoseof extraneous prepositions, as in most of people.We present an approach that combines machinelearning with rule-based filters to detect prepositionerrors in a corpus of ESL essays.
Even though thisis work in progress, we achieve precision of 0.8 witha recall of 0.3.
The paper is structured as follows: inthe next section, we describe the difficulty in learn-ing English preposition usage; in Section 3, we dis-cuss related work; in Sections 4-7 we discuss ourmethodology and evaluation.2 Problem of Preposition UsageWhy are prepositions so difficult to master?
Perhapsit is because they perform so many complex roles.
InEnglish, prepositions appear in adjuncts, they markthe arguments of predicates, and they combine withother parts of speech to express new meanings.The choice of preposition in an adjunct is largelyconstrained by its object (in the summer, on Friday,25at noon) and the intended meaning (at the beach,on the beach, near the beach, by the beach).
Sinceadjuncts are optional and tend to be flexible in theirposition in a sentence, the task facing the learner isquite complex.Prepositions are also used to mark the argumentsof a predicate.
Usually, the predicate is expressedby a verb, but sometimes it takes the form of an ad-jective (He was fond of beer), a noun (They havea thirst for knowledge), or a nominalization (Thechild?s removal from the classroom).
The choice ofthe preposition as an argument marker depends onthe type of argument it marks, the word that fills theargument role, the particular word used as the pred-icate, and whether the predicate is a nominalization.Even with these constraints, there are still variationsin the ways in which arguments can be expressed.Levin (1993) catalogs verb alternations such as Theyloaded hay on the wagon vs.
They loaded the wagonwith hay, which show that, depending on the verb,an argument may sometimes be marked by a prepo-sition and sometimes not.English has hundreds of phrasal verbs, consist-ing of a verb and a particle (some of which arealso prepositions).
To complicate matters, phrasalverbs are often used with prepositions (i.e., give upon someone; give in to their demands).
Phrasalverbs are particularly difficult for non-native speak-ers to master because of their non-compositionalityof meaning, which forces the learner to commit themto rote memory.3 Related WorkIf mastering English prepositions is a daunting taskfor the second language learner, it is even moreso for a computer.
To our knowledge, only threeother groups have attempted to automatically de-tect errors in preposition usage.
Eeg-Olofsson et al(2003) used 31 handcrafted matching rules to detectextraneous, omitted, and incorrect prepositions inSwedish text written by native speakers of English,Arabic, and Japanese.
The rules, which were basedon the kinds of errors that were found in a trainingset of text produced by non-native Swedish writers,targeted spelling errors involving prepositions andsome particularly problematic Swedish verbs.
In atest of the system, 11 of 40 preposition errors werecorrectly detected.Izumi et al (2003) and (2004) used error-annotated transcripts of Japanese speakers in aninterview-based test of spoken English to train amaximum entropy classifier (Ratnaparkhi, 1998) torecognize 13 different types of grammatical and lex-ical errors, including errors involving prepositions.The classifier relied on lexical and syntactic features.Overall performance for the 13 error types reached25.1% precision with 7.1% recall on an independenttest set of sentences from the same source, but theresearchers do not separately report the results forpreposition error detection.
The approach taken byIzumi and colleagues is most similar to the one wehave used, which is described in the next section.More recently, (Lee and Seneff, 2006) used alanguage model and stochastic grammar to replaceprepositions removed from a dialogue corpus.
Eventhough they reported a precision of 0.88 and recallof 0.78, their evaluation was on a very restricted do-main with only a limited number of prepositions,nouns and verbs.4 The Selection ModelA preposition error can be a case of incorrect prepo-sition selection (They arrived to the town), use of apreposition in a context where it is prohibited (Theycame to inside), or failure to use a preposition in acontext where it is obligatory (e.g., He is fond thisbook).
To detect the first type of error, incorrectselection, we have employed a maximum entropy(ME) model to estimate the probability of each of34 prepositions, based on the features in their lo-cal contexts.
The ME Principle says that the bestmodel will satisfy the constraints found in the train-ing, and for those situations not covered in the train-ing, the best model will assume a distribution ofmaximum entropy.
This approach has been shownto perform well in combining heterogeneous formsof evidence, as in word sense disambiguation (Rat-naparkhi, 1998).
It also has the desirable property ofhandling interactions among features without havingto rely on the assumption of feature independence,as in a Naive Bayesian model.Our ME model was trained on 7 million ?events?consisting of an outcome (the preposition that ap-peared in the training text) and its associated con-26text (the set of feature-value pairs that accompa-nied it).
These 7 million prepositions and their con-texts were extracted from the MetaMetrics corpus of1100 and 1200 Lexile text (11th and 12th grade) andnewspaper text from the San Jose Mercury News.The sentences were then POS-tagged (Ratnaparkhi,1998) and then chunked into noun phrases and verbphrases by a heuristic chunker.The maximum entropy model was trained with25 contextual features.
Some of the features repre-sented the words and tags found at specific locationsadjacent to the preposition; others represented thehead words and tags of phrases that preceded or fol-lowed the preposition.
Table 1 shows a subset of thefeature list.Some features had only a few values while oth-ers had many.
PHR pre is the ?preceding phrase?feature that indicates whether the preposition waspreceded by a noun phrase (NP) or a verb phrase(VP).
In the example in Table 2, the prepositioninto is preceded by an NP.
In a sentence that be-gins After the crowd was whipped up into a frenzyof anticipation, the preposition into is preceded bya VP.
There were only two feature#value pairs forthis feature: PHR pre#NP and PHR pre#VP.
Otherfeatures had hundreds or even thousands of differ-ent values because they represented the occurrenceof specific words that preceded or followed a prepo-sition.
Any feature#value pairs which occurred withvery low frequency in the training (less than 10 timesin the 7 million contexts) were eliminated to avoidthe need for smoothing their probabilities.
Lemmaforms of words were used as feature values to fur-ther reduce the total number and to allow the modelto generalize across inflectional variants.
Even afterincorporating these reductions, the number of val-ues was still very large.
As Table 1 indicates, TGR,the word sequence including the preposition and thetwo words to its right, had 54,906 different values.Summing across all features, the model contained atotal of about 388,000 feature#value pairs.
Table 2shows an example of where some of the features arederived from.5 Evaluation on Grammatical TextThe model was tested on 18,157 preposition con-texts extracted from 12 files randomly selected froma portion of 1100 Lexile text (11th grade) that hadnot been used for training.
For each context, themodel predicted the probability of each preposi-tion given the contextual representation.
The high-est probability preposition was then compared tothe preposition that had actually been used by thewriter.
Because the test corpus consisted of pub-lished, edited text, we assumed that this materialcontained few, if any, errors.
In this and subsequenttests, the model was used to classify each context asone of 34 classes (prepositions).Results of the comparison between the classifierand the test set showed that the overall proportionof agreement between the text and the classifier was0.69.
The value of kappa was 0.64.
When we ex-amined the errors, we discovered that, frequently,the classifier?s most probable preposition (the oneit assigned) differed from the second most probableby just a few percentage points.
This correspondedto a situation in which two or more prepositionswere likely to be found in a given context.
Con-sider the context They thanked him for his consider-ation this matter, where either of or in could fillthe blank.
Because the classifier was forced to makea choice in this and other close cases, it incurred ahigh probability of making an error.
To avoid thissituation, we re-ran the test allowing the classifierto skip any preposition if its top ranked and sec-ond ranked choices differed by less than a specifiedamount.
In other words, we permitted it to respondonly when it was confident of its decision.
Whenthe difference between the first and second rankedchoices was 0.60 or greater, 50% of the cases re-ceived no decision, but for the remaining half of thetest cases, the proportion of agreement was 0.90 andkappa was 0.88.
This suggests that a considerableimprovement in performance can be achieved by us-ing a more conservative approach based on a higherconfidence level for the classifier.6 Evaluation on ESL EssaysTo evaluate the ME model?s suitability for analyzingungrammatical text, 2,000 preposition contexts wereextracted from randomly selected essays written onESL tests by native speakers of Chinese, Japanese,and Russian.
This set of materials was used to lookfor problems that were likely to arise as a conse-27Feature Description No.
of values with freq ?
10BGL Bigram to left; includes preceding word and POS 23,620BGR Bigram to right; includes following word and POS 20,495FH Headword of the following phrase 19,718FP Following phrase 40,778PHR pre Preceding phrase type 2PN Preceding noun 18,329PNMod Adjective modifying preceding noun 3,267PNP Preceding noun phrase 29,334PPrep Preceding preposition 60PV Preceding verb 5,221PVP Preceding verb phrase 23,436PVtag POS tag of the preceding verb 24PVword Lemma of the preceding verb 5,221PW Lemma of the preceding word 2,437TGL Trigram to left; includes two preceding words and POS 44,446TGR Trigram to right; includes two following words and POS 54,906Table 1: Some features used in ME ModelAfter whipping the crowd up into a frenzy of anticipation...PVword PN PW FHBGL BGR??TGL??
??TGR?
?Table 2: Locations of some features in the local context of a prepositionquence of the mismatch between the training cor-pus (edited, grammatical text) and the testing corpus(ESL essays with errors of various kinds).
When themodel was used to classify prepositions in the ESLessays, it became obvious, almost immediately, thata number of new performance issues would have tobe addressed.The student essays contained many misspelledwords.
Because misspellings were not in the train-ing, the model was unable to use the features associ-ated with them (e.g., FHword#frinzy) in its decisionmaking.
The tagger was also affected by spellingerrors, so to avoid these problems, the classifierwas allowed to skip any context that contained mis-spelled words in positions adjacent to the preposi-tion or in its adjacent phrasal heads.
A second prob-lem resulted from punctuation errors in the studentwriting.
This usually took the form of missing com-mas, as in I disagree because from my point of viewthere is no evidence.
In the training corpus, commasgenerally separated parenthetical expressions, suchas from my point of view, from the rest of the sen-tence.
Without the comma, the model selected ofas the most probable preposition following because,instead of from.
A set of heuristics was used to lo-cate common sites of comma errors and skip thesecontexts.There were two other common sources of clas-sification error: antonyms and benefactives.
Themodel very often confused prepositions with op-posite meanings (like with/without and from/to), sowhen the highest probability preposition was anantonym of the one produced by the writer, weblocked the classifier from marking the usage as anerror.
Benefactive phrases of the form for + per-son/organization (for everyone, for my school) werealso difficult for the model to learn, most likely be-cause, as adjuncts, they are free to appear in manydifferent places in a sentence and the preposition isnot constrained by its object, resulting in their fre-quency being divided among many different con-texts.
When a benefactive appeared in an argumentposition, the model?s most probable preposition wasgenerally not the preposition for.
In the sentenceThey described a part for a kid, the preposition ofhas a higher probability.
The classifier was pre-vented from marking for + person/organization asa usage error in such contexts.To summarize, the classifier consisted of the MEmodel plus a program that blocked its application28Rater 1 vs. Classifier vs. Classifier vs.Rater 2 Rater 1 Rater 2Agreement 0.926 0.942 0.934Kappa 0.599 0.365 0.291Precision N/A 0.778 0.677Recall N/A 0.259 0.205Table 3: Classifer vs. Rater Statisticsin cases of misspelling, likely punctuation errors,antonymous prepositions, and benefactives.
An-other difference between the training corpus and thetesting corpus was that the latter contained grammat-ical errors.
In the sentence, This was my first experi-ence about choose friends, there is a verb error im-mediately following the preposition.
Arguably, thepreposition is also wrong since the sequence aboutchoose is ill-formed.
When the classifier marked thepreposition as incorrect in an ungrammatical con-text, it was credited with correctly detecting a prepo-sition error.Next, the classifier was tested on the set of 2,000preposition contexts, with the confidence thresholdset at 0.9.
Each preposition in these essays wasjudged for correctness of usage by one or two humanraters.
The judged rate of occurrence of prepositionerrors was 0.109 for Rater 1 and 0.098 for Rater 2,i.e., about 1 out of every 10 prepositions was judgedto be incorrect.
The overall proportion of agreementbetween Rater1 and Rater 2 was 0.926, and kappawas 0.599.Table 3 (second column) shows the results for theClassifier vs. Rater 1, using Rater 1 as the gold stan-dard.
Note that this is not a blind test of the clas-sifier inasmuch as the classifier?s confidence thresh-old was adjusted to maximize performance on thisset.
The overall proportion of agreement was 0.942,but kappa was only 0.365 due to the high level ofagreement expected by chance, as the Classifier usedthe response category of ?correct?
more than 97%of the time.
We found similar results when com-paring the judgements of the Classifier to Rater 2:agreement was high and kappa was low.
In addition,for both raters, precision was much higher than re-call.
As noted earlier, the table does not include thecases that the classifier skipped due to misspelling,antonymous prepositions, and benefactives.Both precision and recall are low in these com-parisons to the human raters.
We are particularlyconcerned about precision because the feedback thatstudents receive from an automated writing analy-sis system should, above all, avoid false positives,i.e., marking correct usage as incorrect.
We tried toimprove precision by adding to the system a naiveBayesian classifier that uses the same features foundin Table 1.
As expected, its performance is not asgood as the ME model (e.g., precision = 0.57 andrecall = 0.29 compared to Rater 1 as the gold stan-dard), but when the Bayesian classifier was given aveto over the decision of the ME classifier, overallprecision did increase substantially (to 0.88), thoughwith a reduction in recall (to 0.16).
To address theproblem of low recall, we have targeted another typeof ESL preposition error: extraneous prepositions.7 Prepositions in Prohibited ContextsOur strategy of training the ME classifier on gram-matical, edited text precluded detection of extrane-ous prepositions as these did not appear in the train-ing corpus.
Of the 500-600 errors in the ESL test set,142 were errors of this type.
To identify extraneouspreposition errors we devised two rule-based filterswhich were based on analysis of the developmentset.
Both used POS tags and chunking information.Plural Quantifier Constructions This filter ad-dresses the second most common extraneous prepo-sition error where the writer has added a preposi-tion in the middle of a plural quantifier construction,for example: some of people.
This filter works bychecking if the target word is preceded by a quanti-fier (such as ?some?, ?few?, or ?three?
), and if thehead noun of the quantifier phrase is plural.
Then, ifthere is no determiner in the phrase, the target wordis deemed an extraneous preposition error.Repeated Prepositions These are cases such aspeople can find friends with with the same interestswhere a preposition occurs twice in a row.
Repeatedprepositions were easily screened by checking if thesame lexical item and POS tag were used for bothwords.These filters address two types of extraneouspreposition errors, but there are many other types(for example, subcategorization errors, or errorswith prepositions inserted incorrectly in the begin-ning of a sentence initial phrase).
Even though thesefilters cover just one quarter of the 142 extraneous29errors, they did improve precision from 0.778 to0.796, and recall from 0.259 to 0.304 (comparingto Rater 1).8 Conclusions and Future WorkWe have presented a combined machine learningand rule-based approach that detects preposition er-rors in ESL essays with precision of 0.80 or higher(0.796 with the ME classifier and Extraneous Prepo-sition filters; and 0.88 with the combined ME andBayesian classifiers).
Our work is novel in that weare the first to report specific performance results fora preposition error detector trained and evaluated ongeneral corpora.While the training for the ME classifier was doneon a separate corpus, and it was this classifier thatcontributed the most to the high precision, it shouldbe noted that some of the filters were tuned on theevaluation corpus.
Currently, we are in the courseof annotating additional ESL essays for prepositionerrors in order to obtain a larger-sized test set.While most NLP systems are a balancing act be-tween precision and recall, the domain of designinggrammatical error detection systems is distinguishedin its emphasis on high precision over high recall.Essentially, a false positive, i.e., an instance of an er-ror detection system informing a student that a usageis incorrect when in fact it is indeed correct, must bereduced at the expense of a few genuine errors slip-ping through the system undetected.
Given this, wechose to set the threshold for the system so that it en-sures high precision which in turn resulted in a recallfigure (0.3) that leaves us much room for improve-ment.
Our plans for future system development in-clude:1.
Using more training data.
Even a cursory ex-amination of the training corpus reveals that thereare many gaps in the data.
Seven million seemslike a large number of examples, but the selectionof prepositions is highly dependent on the presenceof other specific words in the context.
Many fairlycommon combinations of Verb+Preposition+Nounor Noun+Preposition+Noun are simply not attested,even in a sizable corpus.
Consistent with this, thereis a strong correlation between the relative frequencyof a preposition and the classifier?s ability to predictits occurrence in edited text.
That is, prediction isbetter for prepositions that have many examples inthe training set and worse for those with fewer ex-amples.
This suggests the need for much more data.2.
Combining classifiers.
Our plan is to use theoutput of the Bayesian model as an input feature forthe ME classifier.
We also intend to use other classi-fiers and let them vote.3.
Using semantic information.
The MEmodel in this study contains no semantic informa-tion.
One way to extend and improve its cover-age might be to include features of verbs and theirnoun arguments from sources such as FrameNet(http://framenet.icsi.berkeley.edu/), which detail thesemantics of the frames in which many Englishwords appear.ReferencesJ.
Bitchener, S. Young, and D. Cameron.
2005.
The ef-fect of different types of corrective feedback on esl stu-dent writing.
Journal of Second Language Writing.G.
Dalgish.
1985.
Computer-assisted esl research andcourseware development.
Computers and Composi-tion.J.
Eeg-Olofsson and O. Knuttson.
2003.
Automaticgrammar checking for second language learners - theuse of prepositions.
In Nodalida.National Center for Educational Statistics.
2002.
Publicschool student counts, staff, and graduate counts bystate: School year 2000-2001.E.
Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-hara.
2003.
Automatic error detection in the japaneseleaners?
english spoken data.
In ACL.E.
Izumi, K. Uchimoto, and H. Isahara.
2004.
Theoverview of the sst speech corpus of japanese learnerenglish and evaluation through the experiment on au-tomatic detection of learners?
errors.
In LREC.J.
Lee and S. Seneff.
2006.
Automatic grammar correc-tion for second-language learners.
In Interspeech.B.
Levin.
1993.
English verb classes and alternations: apreliminary investigation.
Univ.
of Chicago Press.M.
Murata and H. Ishara.
2004.
Three english learnerassistance systems using automatic paraphrasing tech-niques.
In PACLIC 18.A.
Ratnaparkhi.
1998.
Maximum Entropy Models fornatural language ambiguity resolution.
Ph.D. thesis,University of Pennsylvania.30
