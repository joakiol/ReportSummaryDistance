A LINEAR LEAST SQUARES FIT MAPPING METHOD FORINFORMATION RETRIEVAL FROM NATURAL LANGUAGE TEXTSYIMING YANGCHRISTOPHER G. CHUTESection of Medical Information ResourcesMayo Clinic/FoundationRochester, Minnesota 55905 USAABSTRACTThis paper describes a unique method for mapping nat-ural language texts to canonical terms that identify thecontents of the texts.
This method learns empirical as-sociations between free-form texts and canonical termsfrom human-assigned matches and determines a Lin-ear Least Squares Fit (LLSF) mapping function whichrepresents weighted connections between words in thetexts and the canonical terms.
The mapping functionenables us to project an arbitrary text to the canon-ical term space where the "transformed" text is com-pared with the terms, and similarity scores are obtainedwhich quantify the relevance between the the text andthe terms.
This approach has superior power to dis-cover synonyms or related terms and to preserve thecontext sensitivity of the mapping.
We achieved a rateof 84~ in both the recall and the precision with a test-ing set of 6,913 texts, outperforming other techniquesincluding string matching (15%), morphological parsing(17%) and statistical weighting (21%).1.
In t roduct ionA common need in natural language information re-trieval is to identify the information in free-form textsusing a selected set of canonical terms, so that the textscan be retrieved by conventional database techniquesusing these terms as keywords.
In medical classifica-tion, for example, original diagnoses written by physi-cians in patient records need to be classified into canon-ical disease categories which are specified for the pur-poses of research, quality improvement, or billing.
Wewill use medical examples for discussion although ourmethod is not limited to medical applications.String matching is a straightforward solution to auto-matic mapping from texts to canonical terms.
Here weuse "term" to mean a canonical description of a con-cept, which is often a noun phrase.
Given a text (a"query ~) and a set of canonical terms, string matchingcounts the common words or phrases in the text andthe terms, and choo~s the term containing the largestoverlap as most relevant.
Although it is a simple andtherefore widely used technique, a poor success rate(typically 15% - 20%) is observed \[1\].
String-matching-based methods uffer from the problems known as "toolittle" and "too many".
As an example of the former,high blood pressure and hypertension are synonyms buta straightforward string matching cannot capture theequivalence in meaning because there is no commonword in these two expressions.
On the other hand, thereare many terms which do share some words with thequery high blood pressure, such as high head at term,fetal blood loss, etc.
; these terms would be found by astring matcher although they are conceptually distantfrom the query,Human-defined synonyms or terminology thesauri havebeen tried as a semantic solution for the "too little"problem \[2\] \[3\].
It may significantly improve the map-ping if the right set of synonyms or thesaurus is avail-able.
However~ as Salton pointed out \[4\], there is "noguarantee that a thesaurus tailored to a particular textcollection can be usefully adapted to another collec-tion.
As a result, it has not been possible to obtainreliable improvements in retrieval effectiveness by us-ing thesauruses with a variety of different documentcollections".Salton has addressed the problem from a different an-gle, using statistics of word frequencies in a corpus to es-timate word importance and reduce the "too many" ir-relevant terms \[5\].
The idea is that "meaningful" wordsshould count more in the mapping while unimportantwords should count less.
Although word counting istechnically simple and this idea is commonly used inexisting information retrieval systems, it inherits thebasic weakness of surface string matching.
That is,words used in queries but not occurring in the term col-lection have no affect on the mapping, even if they aresynonyms of important concepts in the term collection.Besides, these word weights are determined regardlessof the contexts where words have been used, so the lackof sentitivity to contexts is another weakness.We focus our efforts on an algorithmic solution for achiev-ing the functionality of terminology thesauri and se-mantic weights without requiring human effort in iden-tifying synonyms.
We seek to capture such knowledgethrough samples representing its usage in various con-texts, e.g.
diagnosis texts with expert-assigned canoni-cal terms collected from the Mayo Clinic patient recordarchive.
We propose a numerical method, a "LinearACRES DE COLING-92, NANTES, 23-28 AOUT 1992 4 4 7 Paoc, OF COL1NG-92, NANTES, AUG. 23-28, 1992(a) text/term pairs and the matrix representationtagh grade cmx~id ulceratipn I dr, cry ruplure "-"'7highgmdegLi?rnit / I maliss~"~"e?vtasmlstom~hm~um I I  / gastdcinjL~y, \[0 1 l l  g ' / jhigh o 1 11 i~j~-y l 1 0 0 lrapture 1 0 01 malignant | 0 1 01stornaeh 1 0 O\[ neoplasm \[ 0 1 0 lul~ration 0 0 1 .J rupture L 0 0 1 /matrix A matrix B(b) an LLSF solution W of the linear system WA = Bcarotid glioma grade high rupture stomach ulceration~.
I '0 .375  -0.25 0.t25 0.125 0 0 0.375-\]8as~c / 0 0 0 0 0.5 0.5 0 linjta'Y / 0 0 0 0 0.5 0.5 0 lmalignant / -0.25 0.5 0.25 0.25 0 0 -0.25 1neoplasm | -0.25 0.5 0.25 0.25 0 0 -0.25 /rupture10.375 -0.25 0.125 0.125 0 0 0.375.\]IPisure 1.
The nmn'ix rep~scntmlon of ?
text/term pair collection and the mapping function W computed from the collection.Least Squares Fit" mapping model, which enables usto obtain mapping functions based on the large collec-tion of known matches and then use these functions todetermine the relevant canonical terms for an arbitrarytext.2.
Comput ing  an LLSF  mapp ing  functionWe consider a mapping between two languages, i.e.from a set of texts to a set of canonical terms.
Wecall the former the source language and the latter thetarget language.
For convenience we refer to an itemin the source language (a diagnosis) as "text", and anitem in the target language (a canonical description ofa disease category) as "canonical term" or "term".
Weuse "text" or "term" in a loose sense, in that it may bea paragraph, a sentence, one or more phrases, or simplya word.
Since we do not restrict the syntax, there is nodifference between a text and a term, both of them aretreated as a set of words.2.1 A numer ica l  representat ion  o f  textsIn mathematics, there are well-established numericalmethods to approximate unknown functions using knowndata.
Applying this idea to our text-to-term apping,the known data  are text / term pairs and the unknownfunction we want to determine is a correct (or nearlycorrect) text-to-term apping for not only the texts in-cluded in the given pairs, but also for the texts whichare not included.
We need a numerical representationfor such a computation.Vectors and matrices have been used for representingnatural  anguage texts in information retrieval systemsfor decades \[5\].
We employ such a representation iour model as shown in Figure 1 (a).
Matrix A is aset of texts, matr ix B is a set of terms, each columnin A represents an individual text and the correspond-ing column of B represents the matched term.
Rowsin these matrices correspond to words and cells con-taln the numbers of times words occur in correspondingtexts or terms.2.2 The  mapp ing  funct ionHaving matrix .4 and E, we are ready to compute themapping function by solving the equation WA = Bwhere W is the unknown function.
The solution W, ifit exists, should satisfy all the given text/term pairs,i.e.
the equation WE~ = b~ holds for i = 1, ...,k, wherek is the number of text/ term pairs, Ei(n x 1) is a textvector, a column of A; bi(rn x 1) is a term vector, thecorresponding column in B; n is the number of distinctsource words and m is the number of distinct targetwords.Solving WA = B can be straightforward using tech-niques of solving linear equations if the system is con-sistent.
Unfortunately the linear system WA = B doesnot always have a solution because there are only m x nunknowns in W,  but the number of given vector pairsmay be arbitrarily large and form an inconsistent sys-tem.
The problem therefore needs to be modified as aLinear Least Squares Fit which always has at least onesolution.Definition 1.
The LLSF problem is to find W whichminimizes the sumk ki=l i=1where ~ d=~ Wgl - b'i is the mapping error of the ithtext/term pair; the notation 11...112 is vector 2-norm,defined as 11712 x \ ] r~ ' 2 = =iv~ and ~'is m x 1; II .
.
.
l i t  isthe Frobenius matrix norm, defined asIIMIIF = m 2 qi=1  j= land M is m x k.The meaning of the LLSF problem is to find the map-ping function W that  minimizes the total mapping er-rors for a given text/term pair collection (the "trainingAcrEs DE COLING-92, NANTES, 23-28 AOt~r 1992 4 4 8 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992set").
The underlying semantics of the transformationW~ = b'~ is to "translate" the meaning of each sourceword in the text into a set of target words with weights,and then linearly combine the translations of individ-ual words to obtain the translation of the whole text,Figure 1 (b) is the W obtained from matrix A and Bin (a).
The columns of W correspond to source words,the rows correspond to target words, and the ceils arethe weights of word-to-word connections between thetwo languages.
A little algebra will show that vectorbi = WS"i is the sum of the column vectors in W, whichcorrespond to the source words in the text.The weights in W are optimally determined accordingto the training set.
Note that the weights do not de-pend on the literal meanings of words.
For example, thesource word glioma has positive connections of 0.5 toboth the target words malignant and neoplasm, show~ing that these different words are related to a certaindegree.
On the other hand, ruptur~ is a word shared byboth the source language and the target language, butthe source word rupture and the target word rupfurehave a connection weight of 0 because the two wordsdo not co-occur in any of the text/term pairs in thetraining set.
Negative weight is also possible for wordsthat do not co-occur and its function is to preserve thecontext sensitivity of the mapping.
For example, highgrade in the context of high grade carotid ulcerationdoes not lead to a match with malignan~ neoplasm,as it would if it were used in the context high gradeglioma, because this ambiguity is cancelled by the neg-ative weights.
Readers can easily verify this by addingthe corresponding column vectors of W for these twodifferent contexts.2.3 The  computat ionA conventional method for solving the LLSF is to usesingular value decomposition (SVD) \[6\] [7\].
Since math-ematics is not the focus of this paper, we simply outlinethe computation without proof.Given matrix A (n x k) and B (mx k), the computationof an LLSF for WA = B consists of the following steps:(1) Compute an SVD of A, yielding matrices U, S andV:if n > k, decompose A such that A = USV T,if n < k, decompose the transpose AT such that.A T = VSU T,where U (n x p) sad V (k x p) contain the left andright singular vectors, respectively, and V ~r isthe transpose of V; Sis  a diagonal (pxp) whichcontains p non-zero singular values al > s2... > sp > 0 and p < rain (k,n);(2) Compute the mapping function W = BVS-1U T,where S - t  = diag ( l /s1, 1/s:~ ..... 1/sl, ).3, Mapp ing  arb i t ra ry  queries to canonical  te rmsThe LLSF mapping consists of the following steps:(1) Given an arbitrary text (a "query"), first form aquery vector, ~, in the source vector space.A query vector is similar to a eolunm of matrix A, whoseelements contain the numbers of times source wordsoccur in the query.
A query may Mso contain somewords which are not in the source language; we ignorethese words because no meaningful connections withthem are provided by the mapping function.
As anexample, query severe stomach ulcers*ion is convertedinto vector ~=(0  0 0 0 0 1 1).
(2) Transform the source vector a7 into t7 = W:~ in thetarget space.In our example, 17 = W?
- (0.375 0.5 0.5 -0.25 -0.250.375).
Differing from text vectors in A and term vec-tors in B, the elements (coefficients) of17 are not limitedto non-negative integers.
These numbers how how themeaning of a query distributes over the words in thetarget language.
(3) Compare query-term similarity for all the term vec-tors and find the relevant erms.In linear algebra, eosine-theta (or dot-product) is acommon measure for obtaining vector similarity.
It isalso widely accepted by the information retrieval com-munity using vector-based techniques because of thereasonable underlying intuition: it captures the siufi-larity of texts by counting the similarity of individualwords and then summarizing them.
We use the cosinevalue to evaluate query-term similarity, defined as be-low;De\]tuition 2.
Let ~ = (Yl , y2, ..., y,n) be the query vectorin the target space and g = (vl,v2, ...,vm) be a termvector in the target space,similarity(~, v-') = cos(~',y lV l  + y2V2 + ... + ymVm= 2 ; ~ .
.
.
.
2 ...+~ VV~SrV~+.. .+Yo~x/ 11+v2+\]In order to find the closest match, we need to comparewith all the term vectors.
We use C to denote thematrix of these vectors distinct from matrix B whichrepresents the term collection in the training set.
Ingeneral only a subset of terms are contained in a train-ing set, so (7 has more columns than the unique columnsof B.
Furthermore, C could have more rows than B be-cause of the larger vocabulary.
However, since only thewords in B have meaningful connections in the LLSFmapping function, we use the words in B to form a re-duced target language and trim C into the same rowsas B.
Words not in the reduced target language areignored.An exhaustive comparison of the query-term similarityAcll~:S DE COLING-92, NANTES, 23-28 Ao~r 1992 4 4 9 PROC.
OF COLING-92, NAN'IXS, AUG. 23-28, 1992values provides a ranked list of all the terms with re-spect to a query.
A retrieval threshold can be chosen fordrawing a line between relevant and irrelevant.
Sincerelevance is often a relative concept, the choice of thethreshold is left to the application or experiment.A potential weakness of this method is that the termvectors in matrix C are all surface-based (representingword occurrence frequency only) and are not affectedby the training set or the mapping function.
This weak-ness can be attenuated by a refined mapping methodusing a reverse mapping function R which is an LLSFsolution of the linear system RB = A.
The refinementis described in a separate paper \[8\].4.
The resu l ts4.1 The pr imary  testWe tested our method with texts collected from patientrecords of Mayo Clinic.
The patient records include di-agnoses (DXs) written by physicians, operative reportswritten by surgeons, etc.
The original texts need to beclassified into canonical categories and about 1.5 mil-lion patient records are coded by human experts eachyear.
We arbitrarily chose the cardiovascular diseasesubset from the 1990 surgical records for our primarytest.
After human editing to separate these texts fromirrelevant parts in the patient records and to clarify theone-to-one correspondence b tween DXs and canonicalterms, we obtained a set of 6,913 DX/term pairs.
Thetarget language consists of 376 canonical names of car-diovascular diseases as defined in the classification sys-tem ICD-9-CM \[9\].
A simple preproceseing was appliedto remove punctuation and numbers, but no stemmingor removal of non-discriminative words were used.We split the 6,913 DXs into two halves, called "odd-half" and "even-half".
The odd-half was used as thetraining set, the even-half was used as queries, and theexpert-assigned canonical terms of the even-half wereused to evaluate the effectiveness of the LLSF mapping.We used conventional measures in the evaluation: recalland precision, defined asrecal l  = j;erms ret r ieved and re levantto ta l  te rms re levantprec is ion = terms re t r ieved  and re levanttota l  te rms re t r ievedFor the query set of the even-half, we had a recall rateof 84% when the top choice only was counted and 96%recall among the top five choices.
We also tested theodd-half, i.e.
the training set itself, as queries and hada recall of 92% with the top choice and 99% with thetop five.
In our testing set, each text has one and onlyone relevant (or correct) canonical term, so the recall isalways the same as the precision at the top choice.Our experimental system is implemented as a combi-nation of C++, Perl and UNIX shell programming.For SVD, currently we use a matrix library in C++\[10\] which implements the same algorithm as in LIN-PACK\[Ill. A test with 3,457 pairs in the training settook about 4.45 hours on a SUN SPARCstation 2 tocompute the mapping function W and R. Since thecomputation of the mapping function is only neededonce until the data collection is renewed, a real time re-sponse is not required.
Term retrieval took 0.45 sec orle~ per query and was satisfactory for practical needs.Two person-days ofhuman editing were needed for prepar-ing the testing set of the 6,913 DXs.4.2 The compar isonFor comparing our method with other approaches, wedid additional tests with the same query set, the even-half (3,456 DXs), and matched it against the same termset, the 376 ICD-9-CM disease categories.For the test of a string matching method, we formed onematrix for all the 3,456 texts and the 376 terms, andused the cosine measure for computing the similarities.Only a 15% recall and precision rate was obtained atthe top choice threshold.For testing the effect of linguistic anonicalization, weemployed a morphological parser developed by the Evansgroup at CMU \[12\] (and refined by our group by addingsynonyms) which covers over 10,000 lexical variants.We used it as a preprocessor which converted lexicalvariants to word roots, expanded abbreviations to fullspellings, recognized non-discriminative categories suchas conjunctions and prepositions and removed them,and converted synonyms into canonical terms.
Both thetexts and the terms were parsed, and then the stringmatching as mentioned above was applied.
The recall(and precision) rate was 17% (i.e.
only 2% improve-ment), indicating that lexical canonicalization does notsolve the crucial part of the problem; obviously, verylittle information was captured.
Although synonymswere also used, they were a small collection and notespecially favorable for the cardiovascular diseases.For testing the effectiveness of statistical weighting, weran the SMART system (version 10) developed at Cor-nell by Salton's group on our testing set.
Two weightingschemes, one using term frequency and another using acombination of term frequency and "inverse documentfrequency", were tested with default parameters; 20%and 21% recall rates (top choice) were obtained, re-spectively.
An interactive scheme using user feedbackfor improvement is also provided in SMART, but ourtests did not include that option.For further analysis we checked the vocabulary over-lap between the query set and the term set.
Only 20%of the source words were covered by the target words,which partly explains the unsatisfactory esults of theabove methods.
Since they are all surface-based up-AcrEs DE COLING-92, NA~rn~s, 23-28 Aotrr 1992 4 5 0 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992Table l.The test summtwMethodstring matching~ring matching ~ by ?
morphological p~rsingSMART: atathai~ weighting using IDFLLSF: training act = odd-halfLLSF: uainin 8 met = odd-half, query set = o&l-lud fof different methodsmcall of recall ofthe top choice ~ five choices15% 42%17% 46%21% 48%84% 96%92% 99%(1) The "cven-hLlf" (3,456 D~) was used as the query set for testis 8all the mothuds above, except the last one;(2) the "odd-ludf' (3,457 DXs) was used as the Iraining sa in the LLSF tests, which formed asource l~8uage including945 distinct wolds and t lascar language (reduc.ed) including 376 unique canonical terms and 224 distinct words;O) the refined mapping method mentioned in Section 3was u~d in the I\]~SF tests.\]DIAGNOSISWRITTFNIIyPHYSICIAN~ TI~d~IFOUNDHYAS~IRINGMATCHING TERM FOUNI\] BY THE LLSF MAPPING // vasculitis Itft elbow tn~oimr~ve left heart failure art~fiti, unspecifiedr up/ured fight fe~noral p seudoaneurytm dxlominal oeurysm r Ul~ured aneurym of urtery of low~ extreanit y |unmpturexl cJcutld 5ifmr~on emeurym amaic ueur,/sm anent\]urn of artery of neck /ruptured abdominal aortic m~eurysm abdominal neurysm ruptured abdominal aneurysm ruptured |abdominal ortic mncaryamunruptured I~lomlnal aneurysm abdominal neurysm without mention /of luptule /bold: word effective in the staSng matching \]JHgttre 2.
Sasnple ~ult~ of file DX--to-tenn mapping using the LLSF and a string matching methodproaches, only 20% of the query words were effectivelyused and roughly 80% of the information was ignored.The~e approaches hare a common weakness in thatthey can not capture the implicit meaning of words (oronly captured a little), and this seems to be a crucialproblem.The LLSF method, on the other hand, does not havesuch disadvantages.
First, since the training set andthe query set were from the sanle data collection, amuch higher vocabulary coverage of 67% was obtained.Second, the 67% source words were further connectedto their synonyms or related words by the LLSF map-ping, according to the matches in the training set.
Notonly word co-occurrence, "but also the contexts (sets ofwords) where the words have been used, were taken intoaccount in the computation of weights; these connec-tions were therefore context-sensitive.
As a result, the{~7% word coverage achieved an 84% recall and preci-sion rate (top choice), outperforming the other methodsby 63% or more.
Table 1 summarizes these tests.Figure 2 shows some sample results where each query islisted with the top choice by the LLSF mapping and thetop choice by the string matching.
All the terms cho-sen by the LLSF mapping agreed with expert-aesignedmatches.
It is evident hat  the LLSF mapping succem-fully captures the semantic associations between thedifferent surface xpressions where a~ the string match-ing failed completely or missed important information.,5.
D iscuss ion5.1 Impact  to  computat iona l  l inguist icsltecognizing word meanings or underlying concepts innatural language texts is a major focus in computa-tional linguistics, especially in applied natural anguageprocessing such as information retrieval.
Lexico-syntaeticapproaches have had limited achievement because lexoicai canonicalization and syntactic categorization cannot capture much information about the implicit mean-ing of words and surface xpressions.
Knowledge-basedapproaches using semantic thesauri or networks, on theother hand, lead to the fundamental question aboutwhat should be put in a knowledge base.
Is a gen-eral knowledge base for unrestricted subject areas re~aiistic?
If unlikely, then what should be chosen for adomain-specific or application-specific knowledge bane?ls there a systematic way to avoid ad hoe decisions orthe inconsistency that have often been involved in hu-man development of semantic lasses and the relation-ships between them?
No clear answers have been givenfor these questions.The LLSF method gives an effective solution for captur-ing semantic implications between surface expressions.The word-to-word connections between two languagescapture synonyms and related terms with respect o thecontexts given in the text/ term pairs of the training set.Furthermore, by taking a training set from the samedata collection as the queries the knowledge (semm~-tic~) is self-restricted, i.e.
domain-specific, application-specific and user-group-specific.
No symbolic represen-tation of the knowledge is involved nor necessary, sosubjective decisions by humans are avoided.
As a re-Ac.q'ES DE COLING-92, NANTES, 23-28 Aotrr 1992 4 5 1 PROC.
OF COL1NG-92, NARrEs, AuG. 23-28, 1992suit, the 6%69% improvement over the string matchingand the morphological parsing is evidence of our asser-tions.5.2 Dif ference f rom other  vector -based methodsThe use of vector/matrix representation, cosine mea-sure and SVD makes our approach look similar to othervector-based methods, e.g.
Saiton's tatistical weight-ing scheme and Deerwester's Latent Semantic Index-ing (LSI) \[13\] which uses a word-document matrix andtruncated SVD technique to adjust word weights in adocument retrieval.
However, there is a fundamentaldifference in that they focus on word weights based oncounting word occurrence frequencies in a text collec-tion, so only the words that appeared in queries anddocuments (terms in our context) have an affect on theretrieval.
On the other hand, we focus on the weightsof word-to-word connections between two languages,not weight of words; our computation is based on theinformation of human-assigned matches, the word co-occurrence and the contexts in the text/term pairs, notsimply word occurrence frequencies.
Our approach hasan advantage in capturing synonyms or terms seman-tically related at various degrees and this makes a sig-nificant difference.
As we discussed above, only 20% ofquery words were covered by the target words.
So evenif the statistical methods could find optimal weights forthese words, the majority of the information was stillignored, and as a result, the top choice recall and preci-sion rate of SMART did not exceed 20% by much.
Ourtests with the LSI were mentioned in a separate paper\[14\]; the results were not better than SMART or thestring matching method discussed above.In short, besides the surface characteristics such as us-ing matrix, cosine-theta and SVD,  the LLSF  mappinguses different information and solves the problem on adifferent scale.5.3 Potent ia l  app l icat ionsWe have demonstrated the success of the LLSF map-ping in medical cP, ssification, but our method is notlimited to this application.
An attractive and practi-cal application is automatic indexing of text databasesand a retrieval using these indexing terms.
As mostexisting text databmms use human-assigned keywordsfor indexing documents, numerous amounts of docu-ment/term pairs can be easily collected and used astraining sets.
The obtained LLSF mapping functionsthen can be used for automatic document indexing withor without human monitoring and refinement.
Queriesfor retrieval can be mapped to the indexing terms usingthe same mapping functions and the rest of the task issimply a keyword-based search.Another interesting potential is machine translation.Brown\[15\] proposed a statistical approach for machinetranslation which used word-to-word translation prob-ability between two languages.
They had about threemillion pairs of English-French sentences but the dif-ficult problem was to break the sentence-to-sentenceassociation down to word-to-word.
While they hada sophisticated algorithm to determine an alignmentof word connections with maximum probability, it re-quired estimation and re-estimation about possible align-ments.
Our LLSF mapping appears to have a great op-portunity to discover the optimal word-to-word trans-lation probability, according to the English-French sen-tence pairs but without requiring any subjective sti-mations.5.4 Other  aspectsSeveral quastion~ deserve ashort discussion: is the worda good choice for the basis of the LLSF vector space?Is the LLSF the only choice or the best choice for anumerical mapping?The word is not the only choice as the basis.
We use itas a suitable starting point and for computational effi-ciency.
We also treat some special phrases uch as Ac-gulfed Immunod~ficiency Syndrome as a single word, byputting hyphens between the words in a pre-formatting.An alternative choice to using words is to use nounphrases for invoking more syntactic onstraints.
Whileit may improve the precision of the mapping (how muchis unclear), a combinatorial increase of the problem sizeis the trade-off.Linear fit is a theoretical limitation of the LLSF map-ping method.
More powerful mapping functions areused in some neural networks\[16\].
However, the factthat the LLSF mapping is simple, fast to compute,and has well known mathematical properties makes itpreferable at this stage of research.
There are other nu-merical methods possible, e.g.
using polynomial fit in-stead of linear fit, or using interpolation (going throughpoints) instead of least squares fit, etc.
The LLSFmodel demonstrated the power of numerical extrac-tion of the knowledge from human-assigned mappingresults, and finding the optimal solution among differ-ent fitting methods is a matter of implementation a dexperimentation.AcknowledgementWe would like to thank Tony Plate and Kent Baileyfor fruitful discussions and Geoffrey Atkin for program-ruing.References1.
Blair DC, Maron ME.
An evaluation of retrieval effec-tiveness of a full-text document-retrieval system.
Com.rauaications of the ACM 1985;28:289-299.2.
Chute CG, Yang Y, Evans DA.
Latent semantic in-ACRES DE COLING-92, NANTZS, 23-28 ^ o~-r 1992 4 5 2 Pgoc.
OF COLING-92, NANTES, AUG. 23-28.
1992dexing of medical diagnoses using UMLS semantic struc-tures.
Proceedings of the 15th Annual Symposium onComputer Applications in Medical Care 1991;15:185-189.3.
Evans DA, Handeraon SK, Monarch IA, Pereiro J,Delon L, Hersh WR.
Mapping vocabularies using "La-tent Semantics."
TechnicaI Report No.
CMU-LCL-91-1.Pittsburgh, PA: Carnegie Mellon University, 1991.4.
Salton G, Development inAutomatic Text Retrieval,Science 1991:253:974-980.5.
Salton G, Yang CS, Wu CT. A theory of term im-portance in automatic text analysis.
J Amer Soc Inf Sci1975;26:33-44.6.
Lawson CL, and Hanson RJ.
Solving Least SquaresProblems.
Englewood Cliffs, N.J.: Prentice-Hall, 1974.7.
Golub GH, Van Loan CE.
Matrix Computations, ~ndEdition.
The Johns Hopkins University Press, 1989,8.
Yang Y, Chute CG.
A Numerical Solution for Text in-formation Retrieval and its Application in Patient DataClassification.
Technical Report Series, No.
50, Sectionof Biostatistics, Mayo Clinic 1992.9. International Classification of Diseases, 9th Revi-sion, Clinical Modifications.
Ann Arbor, MI: Commis-sion on Professional and Hospital Activities, 1986.10.
M-t-+ Class Library, User Guide, Release 8.
DyadSoftware Corporation; Bellevue, WA: 1991.11.
Dongaxra JJ, Moler CB, Bunch JR, Stewart GW.LINPACK Users' Guide.
Philadelphia, PA: SIAM, 1979.12.
Evans DA, Hersh WR, Monarch IA, Lefferts RG,Handerson SK.
Automatic indexing of abstracts vianatural-language processing using a simple thesaurus.Medical Decision Making 1991;11/4 Suppl;1O8-115.13.
Deerwester S., Dumals ST, Furnas GW, LandauerTK, Harshman R. Indexing by Latent Semantic Anal-ysis.
J Amer Soc lnf Sci 1990;41(6):391-407.14.
Chute CG, Y~ng Y.
An Evaluation of Concept BasedLatent Semantic Indexing for Clinical Information Ke-trieval.
Proceedings of the 16th Annual Symposium onComputer Applications in Medical Care 1991;submit-ted.15.
Brown PG, Cocke J, Pietra SD, Pietra VJD, JelinekF, Lafferty JD, Mercer RL, Roossin PS.
A StatisticalApproach to Machine "lYanslation.
Computational Lin-guistics, 1990;16(2): 79-85.16.
Rumelhart DE, McClelland ~L and the PDP Re-search Group.
Parallel Distributed Processing: Explo-rations in the Microstrncture of Cognition.
Cambridge,Mas~.
: MIT Press, 1986.ACRES DE COLING-92, NAb~rES, 23-28 AO(;r 1992 4 5 3 l'ROC.
OF COLING-92, NANTES.
AUG. 23-28, 1992
