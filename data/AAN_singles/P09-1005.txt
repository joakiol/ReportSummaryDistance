Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 37?45,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPBrutus: A Semantic Role Labeling System Incorporating CCG, CFG, andDependency FeaturesStephen A. Boxwell, Dennis Mehay, and Chris BrewDepartment of LinguisticsThe Ohio State University{boxwe11,mehay,cbrew}@1ing.ohio-state.eduAbstractWe describe a semantic role labeling systemthat makes primary use of CCG-based fea-tures.
Most previously developed systemsare CFG-based and make extensive use of atreepath feature, which suffers from data spar-sity due to its use of explicit tree configura-tions.
CCG affords ways to augment treepath-based features to overcome these data sparsityissues.
By adding features over CCG word-word dependencies and lexicalized verbal sub-categorization frames (?supertags?
), we canobtain an F-score that is substantially betterthan a previous CCG-based SRL system andcompetitive with the current state of the art.
Amanual error analysis reveals that parser errorsaccount for many of the errors of our system.This analysis also suggests that simultaneousincremental parsing and semantic role labelingmay lead to performance gains in both tasks.1 IntroductionSemantic Role Labeling (SRL) is the process of assign-ing semantic roles to strings of words in a sentence ac-cording to their relationship to the semantic predicatesexpressed in the sentence.
The task is difficult becausethe relationship between syntactic relations like ?sub-ject?
and ?object?
do not always correspond to seman-tic relations like ?agent?
and ?patient?.
An effectivesemantic role labeling system must recognize the dif-ferences between different configurations:(a) [The man]Arg0 opened [the door]Arg1 [forhim]Arg3 [today]ArgM?TMP .
(b) [The door]Arg1 opened.
(c) [The door]Arg1 was opened by [a man]Arg0.We use Propbank (Palmer et al, 2005), a corpus ofnewswire text annotated with verb predicate semanticrole information that is widely used in the SRL litera-ture (Ma`rquez et al, 2008).
Rather than describe se-mantic roles in terms of ?agent?
or ?patient?, Propbankdefines semantic roles on a verb-by-verb basis.
For ex-ample, the verb open encodes the OPENER as Arg0, theOPENEE as Arg1, and the beneficiary of the OPENINGaction as Arg3.
Propbank also defines a set of adjunctroles, denoted by the letter M instead of a number.
Forexample, ArgM-TMP denotes a temporal role, like ?to-day?.
By using verb-specific roles, Propbank avoidsspecific claims about parallels between the roles of dif-ferent verbs.We follow the approach in (Punyakanok et al, 2008)in framing the SRL problem as a two-stage pipeline:identification followed by labeling.
During identifica-tion, every word in the sentence is labeled either asbearing some (as yet undetermined) semantic role ornot .
This is done for each verb.
Next, during label-ing, the precise verb-specific roles for each word aredetermined.
In contrast to the approach in (Punyakanoket al, 2008), which tags constituents directly, we tagheadwords and then associate them with a constituent,as in a previous CCG-based approach (Gildea andHockenmaier, 2003).
Another difference is our choiceof parsers.
Brutus uses the CCG parser of (Clark andCurran, 2007, henceforth the C&C parser), Charniak?sparser (Charniak, 2001) for additional CFG-based fea-tures, and MALT parser (Nivre et al, 2007) for de-pendency features, while (Punyakanok et al, 2008)use results from an ensemble of parses from Char-niak?s Parser and a Collins parser (Collins, 2003; Bikel,2004).
Finally, the system described in (Punyakanok etal., 2008) uses a joint inference model to resolve dis-crepancies between multiple automatic parses.
We donot employ a similar strategy due to the differing no-tions of constituency represented in our parsers (CCGhaving a much more fluid notion of constituency andthe MALT parser using a different approach entirely).For the identification and labeling steps, we traina maximum entropy classifier (Berger et al, 1996)over sections 02-21 of a version of the CCGbank cor-pus (Hockenmaier and Steedman, 2007) that has beenaugmented by projecting the Propbank semantic anno-tations (Boxwell and White, 2008).
We evaluate ourSRL system?s argument predictions at the word stringlevel, making our results directly comparable for eachargument labeling.1In the following, we briefly introduce the CCGgrammatical formalism and motivate its use in SRL(Sections 2?3).
Our main contribution is to demon-strate that CCG ?
arguably a more expressive and lin-1This is guaranteed by our string-to-string mapping fromthe original Propbank to the CCGbank.37guistically appealing syntactic framework than vanillaCFGs ?
is a viable basis for the SRL task.
This is sup-ported by our experimental results, the setup and detailsof which we give in Sections 4?10.
In particular, usingCCG enables us to map semantic roles directly ontoverbal categories, an innovation of our approach thatleads to performance gains (Section 7).
We concludewith an error analysis (Section 11), which motivatesour discussion of future research for computational se-mantics with CCG (Section 12).2 Combinatory Categorial GrammarCombinatory Categorial Grammar (Steedman, 2000)is a grammatical framework that describes syntacticstructure in terms of the combinatory potential of thelexical (word-level) items.
Rather than using standardpart-of-speech tags and grammatical rules, CCG en-codes much of the combinatory potential of each wordby assigning a syntactically informative category.
Forexample, the verb loves has the category (s\np)/np,which could be read ?the kind of word that would bea sentence if it could combine with a noun phrase onthe right and a noun phrase on the left?.
Further, CCGhas the advantage of a transparent interface between theway the words combine and their dependencies withother words.
Word-word dependencies in the CCG-bank are encoded using predicate-argument (PARG)relations.
PARG relations are defined by the functorword, the argument word, the category of the functorword and which argument slot of the functor categoryis being filled.
For example, in the sentence John lovesMary (figure 1), there are two slots on the verbal cat-egory to be filled by NP arguments.
The first argu-ment (the subject) fills slot 1.
This can be encodedas <loves,john,(s\np)/np,1>, indicating the head ofthe functor, the head of the argument, the functor cat-egory and the argument slot.
The second argument(the direct object) fills slot 2.
This can be encoded as<loves,mary,(s\np)/np,2>.
One of the potential ad-vantages to using CCGbank-style PARG relations isthat they uniformly encode both local and long-rangedependencies ?
e.g., the noun phrase the Mary thatJohn loves expresses the same set of two dependencies.We will show this to be a valuable tool for semanticrole prediction.3 Potential Advantages to using CCGThere are many potential advantages to using the CCGformalism in SRL.
One is the uniformity with whichCCG can express equivalence classes of local and long-range (including unbounded) dependencies.
CFG-based approaches often rely on examining potentiallylong sequences of categories (or treepaths) between theverb and the target word.
Because there are a number ofdifferent treepaths that correspond to a single relation(figure 2), this approach can suffer from data sparsity.CCG, however, can encode all treepath-distinct expres-sions of a single grammatical relation into a singlepredicate-argument relationship (figure 3).
This fea-ture has been shown (Gildea and Hockenmaier, 2003)to be an effective substitute for treepath-based features.But while predicate-argument-based features are veryeffective, they are still vulnerable both to parser er-rors and to cases where the semantics of a sentencedo not correspond directly to syntactic dependencies.To counteract this, we use both kinds of features withthe expectation that the treepath feature will providelow-level detail to compensate for missed, incorrect orsyntactically impossible dependencies.Another advantage of a CCG-based approach (andlexicalist approaches in general) is the ability to en-code verb-specific argument mappings.
An argumentmapping is a link between the CCG category and thesemantic roles that are likely to go with each of its ar-guments.
The projection of argument mappings ontoCCG verbal categories is explored in (Boxwell andWhite, 2008).
We describe this feature in more detailin section 7.4 Identification and Labeling ModelsAs in previous approaches to SRL, Brutus uses a two-stage pipeline of maximum entropy classifiers.
In ad-dition, we train an argument mapping classifier (de-scribed in more detail below) whose predictions areused as features for the labeling model.
The samefeatures are extracted for both treebank and automaticparses.
Automatic parses were generated using theC&C CCG parser (Clark and Curran, 2007) with itsderivation output format converted to resemble that ofthe CCGbank.
This involved following the derivationalbracketings of the C&C parser?s output and recon-structing the backpointers to the lexical heads using anin-house implementation of the basic CCG combina-tory operations.
All classifiers were trained to 500 iter-ations of L-BFGS training ?
a quasi-Newton methodfrom the numerical optimization literature (Liu and No-cedal, 1989) ?
using Zhang Le?s maxent toolkit.2 Toprevent overfitting we used Gaussian priors with globalvariances of 1 and 5 for the identifier and labeler, re-spectively.3 The Gaussian priors were determined em-pirically by testing on the development set.Both the identifier and the labeler use the followingfeatures:(1) Words.
Words drawn from a 3 word windowaround the target word,4 with each word asso-ciated with a binary indicator feature.
(2) Part of Speech.
Part of Speech tags drawnfrom a 3 word window around the target word,2Available for download at http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.3Gaussian priors achieve a smoothing effect (to preventoverfitting) by penalizing very large feature weights.4The size of the window was determined experimentallyon the development set ?
we use the same window sizesthroughout.38John loves Marynp (s[dcl]\np)/np np>s[dcl]\np<s[dcl]Figure 1: This sentence has two depen-dencies: <loves,mary,(s\np)/np,2> and<loves,john,(s\np)/np,1>Saaa!!
!NPRobinVPbb""VfixedNP@ DettheNcarNPHHHDettheNHHHNcarRCHHHRelthatSZZNPRobinVPVfixedFigure 2: The semantic relation (Arg1) between ?car?and ?fixed?
in both phrases is the same, but thetreepaths ?
traced with arrows above ?
are differ-ent: (V>VP<NP<N and V>VP>S>RC>N<N, re-spectively).Robin fixed the carnp (s\np)/np np/n n>np>s\np<sthe car that Robin fixednp/n n (np\np)/(s/np) np (s\np)/np>Ts/(s\np)> >Bnp s/np>np\np<npFigure 3: CCG word-word dependencies are passedup through subordinate clauses, encoding the rela-tion between car and fixed the same in both cases:(s\np)/np.2.?
(Gildea and Hockenmaier, 2003)with each associated with a binary indicatorfeature.
(3) CCG Categories.
CCG categories drawn froma 3 word window around the target word, witheach associated with a binary indicator feature.
(4) Predicate.
The lemma of the predicate we aretagging.
E.g.
fix is the lemma of fixed.
(5) Result Category Detail.
The grammatical fea-ture on the category of the predicate (indicat-ing declarative, passive, progressive, etc).
Thiscan be read off the verb category: declarativefor eats: (s[dcl]\np)/np or progressive for run-ning: s[ng]\np.
(6) Before/After.
A binary indicator variable indi-cating whether the target word is before or afterthe verb.
(7) Treepath.
The sequence of CCG categoriesrepresenting the path through the derivationfrom the predicate to the target word.
Forthe relationship between fixed and car in thefirst sentence of figure 3, the treepath is(s[dcl]\np)/np>s[dcl]\np<np<n, with > and< indicating movement up and down the tree,respectively.
(8) Short Treepath.
Similar to the above treepathfeature, except the path stops at the highestnode under the least common subsumer thatis headed by the target word (this is the con-stituent that the role would be marked on if weidentified this terminal as a role-bearing word).Again, for the relationship between fixed andcar in the first sentence of figure 3, the shorttreepath is (s[dcl]\np)/np>s[dcl]\np<np.
(9) NP Modified.
A binary indicator feature indi-cating whether the target word is modified byan NP modifier.55This is easily read off of the CCG PARG relationships.39(10) Subcategorization.
A sequence of the cate-gories that the verb combines with in the CCGderivation tree.
For the first sentence in fig-ure 3, the correct subcategorization would benp,np.
Notice that this is not necessarily a re-statement of the verbal category ?
in the secondsentence of figure 3, the correct subcategoriza-tion is s/(s\np),(np\np)/(s[dcl]/np),np.
(11) PARG feature.
We follow a previous CCG-based approach (Gildea and Hockenmaier,2003) in using a feature to describe the PARGrelationship between the two words, if one ex-ists.
If there is a dependency in the PARGstructure between the two words, then this fea-ture is defined as the conjunction of (1) the cat-egory of the functor, (2) the argument slot thatis being filled in the functor category, and (3)an indication as to whether the functor (?)
orthe argument (?)
is the lexical head.
For ex-ample, to indicate the relationship between carand fixed in both sentences of figure 3, the fea-ture is (s\np)/np.2.
?.The labeler uses all of the previous features, plus thefollowing:(12) Headship.
A binary indicator feature as towhether the functor or the argument is the lex-ical head of the dependency between the twowords, if one exists.
(13) Predicate and Before/After.
The conjunctionof two earlier features: the predicate lemmaand the Before/After feature.
(14) Rel Clause.
Whether the path from predicateto target word passes through a relative clause(e.g., marked by the word ?that?
or any otherword with a relativizer category).
(15) PP features.
When the target word is a prepo-sition, we define binary indicator features forthe word, POS, and CCG category of the headof the topmost NP in the prepositional phraseheaded by a preposition (a.k.a.
the ?lexicalhead?
of the PP).
So, if on heads the phrase ?onthe third Friday?, then we extract features re-lating to Friday for the preposition on.
This isnull when the target word is not a preposition.
(16) Argument Mappings.
If there is a PARG rela-tion between the predicate and the target word,the argument mapping is the most likely pre-dicted role to go with that argument.
Thesemappings are predicted using a separate classi-fier that is trained primarily on lexical informa-tion of the verb, its immediate string-level con-text, and its observed arguments in the train-ing data.
This feature is null when there isno PARG relation between the predicate andthe target word.
The Argument Mapping fea-ture can be viewed as a simple prediction aboutsome of the non-modifier semantic roles that averb is likely to express.
We use this informa-tion as a feature and not a hard constraint toallow other features to overrule the recommen-dation made by the argument mapping classi-fier.
The features used in the argument map-ping classifier are described in detail in section7.5 CFG based FeaturesIn addition to CCG-based features, features can bedrawn from a traditional CFG-style approach whenthey are available.
Our motivation for this is twofold.First, others (Punyakanok et al, 2008, e.g.
), have foundthat different parsers have different error patterns, andso using multiple parsers can yield complementarysources of correct information.
Second, we noticedthat, although the CCG-based system performed wellon head word labeling, performance dropped whenprojecting these labels to the constituent level (see sec-tions 8 and 9 for more).
This may have to do with thefact that CCG is not centered around a constituency-based analysis, as well as with inconsistencies betweenCCG and Penn Treebank-style bracketings (the latterbeing what was annotated in the original Propbank).Penn Treebank-derived features are used in the iden-tifier, labeler, and argument mapping classifiers.
Forautomatic parses, we use Charniak?s parser (Charniak,2001).
For gold-standard parses, we remove func-tional tag and trace information from the Penn Tree-bank parses before we extract features over them, so asto simulate the conditions of an automatic parse.
ThePenn Treebank features are as follows:(17) CFG Treepath.
A sequence of traditionalCFG-style categories representing the pathfrom the verb to the target word.
(18) CFG Short Treepath.
Analogous to the CCG-based short treepath feature.
(19) CFG Subcategorization.
Analogous to theCCG-based subcategorization feature.
(20) CFG Least Common Subsumer.
The cate-gory of the root of the smallest tree that domi-nates both the verb and the target word.6 Dependency Parser FeaturesFinally, several features can be extracted from a de-pendency representation of the same sentence.
Au-tomatic dependency relations were produced by theMALT parser.
We incorporate MALT into our col-lection of parses because it provides detailed informa-tion on the exact syntactic relations between word pairs(subject, object, adverb, etc) that is not found in otherautomatic parsers.
The features used from the depen-dency parses are listed below:40(21) DEP-Exists A binary indicator feature show-ing whether or not there is a dependency be-tween the target word and the predicate.
(22) DEP-Type If there is a dependency betweenthe target word and the predicate, what type ofdependency it is (SUBJ, OBJ, etc).7 Argument Mapping ModelAn innovation in our approach is to use a separate clas-sifier to predict an argument mapping feature.
An ar-gument mapping is a mapping from the syntactic argu-ments of a verbal category to the semantic argumentsthat should correspond to them (Boxwell and White,2008).
In order to generate examples of the argumentmapping for training purposes, it is necessary to em-ploy the PARG relations for a given sentence to identifythe headwords of each of the verbal arguments.
That is,we use the PARG relations to identify the headwords ofeach of the constituents that are arguments of the verb.Next, the appropriate semantic role that corresponds tothat headword (given by Propbank) is identified.
Thisis done by climbing the CCG derivation tree towardsthe root until we find a semantic role corresponding tothe verb in question ?
i.e., by finding the point wherethe constituent headed by the verbal category combineswith the constituent headed by the argument in ques-tion.
These semantic roles are then marked on the cor-responding syntactic argument of the verb.As an example, consider the sentence The boy lovesa girl.
(figure 4).
By examining the arguments that theverbal category combines with in the treebank, we canidentify the corresponding semantic role for each argu-ment that is marked on the verbal category.
We then usethese tags to train the Argument Mapping model, whichwill predict likely argument mappings for verbal cate-gories based on their local surroundings and the head-words of their arguments, similar to the supertaggingapproaches used to label the informative syntactic cat-egories of the verbs (Bangalore and Joshi, 1999; Clark,2002), except tagging ?one level above?
the syntax.The Argument Mapping Predictor uses the followingfeatures:(23) Predicate.
The lemma of the predicate, as be-fore.
(24) Words.
Words drawn from a 5 word windowaround the target word, with each word associ-ated with a binary indicator feature, as before.
(25) Parts of Speech.
Part of Speech tags drawnfrom a 5 word window around the target word,with each tag associated with a binary indicatorfeature, as before.
(26) CCG Categories.
CCG categories drawn froma 5 word window around the target word, witheach category associated with a binary indica-tor feature, as before.the boy loves a girlnp/n n (s[dcl]\npArg0)/npArg1 np/n n> >np ?
Arg0 np ?
Arg1>s[dcl]\np<s[dcl]Figure 4: By looking at the constituents that the verbcombines with, we can identify the semantic roles cor-responding to the arguments marked on the verbal cat-egory.
(27) Argument Data.
The word, POS, and CCGcategory, and treepath of the headwords of eachof the verbal arguments (i.e., PARG depen-dents), each encoded as a separate binary in-dicator feature.
(28) Number of arguments.
The number of argu-ments marked on the verb.
(29) Words of Arguments.
The head words of eachof the verb?s arguments.
(30) Subcategorization.
The CCG categories thatcombine with this verb.
This includes syntacticadjuncts as well as arguments.
(31) CFG-Sisters.
The POS categories of the sis-ters of this predicate in the CFG representation.
(32) DEP-dependencies.
The individual depen-dency types of each of the dependencies re-lating to the verb (SBJ, OBJ, ADV, etc) takenfrom the dependency parse.
We also incorpo-rate a single feature representing the entire setof dependency types associated with this verbinto a single feature, representing the set of de-pendencies as a whole.Given these features with gold standard parses, ourargument mapping model can predict entire argumentmappings with an accuracy rate of 87.96% on the testset, and 87.70% on the development set.
We found thefeatures generated by this model to be very useful forsemantic role prediction, as they enable us to make de-cisions about entire sets of semantic roles associatedwith individual lemmas, rather than choosing them in-dependently of each other.8 Enabling Cross-System ComparisonThe Brutus system is designed to label headwords ofsemantic roles, rather than entire constituents.
How-ever, because most SRL systems are designed to labelconstituents rather than headwords, it is necessary toproject the roles up the derivation to the correct con-stituent in order to make a meaningful comparison ofthe system?s performance.
This introduces the poten-tial for further error, so we report results on the ac-curacy of headwords as well as the correct string ofwords.
We deterministically move the role to the high-est constituent in the derivation that is headed by the41a man with glasses spokenp/n n (np\np)/np np s\np> >np np\np<np ?
speak.Arg0<sFigure 5: The role is moved towards the root until theoriginal node is no longer the head of the marked con-stituent.P R FG&H (treebank) 67.5% 60.0% 63.5%Brutus (treebank) 88.18% 85.00% 86.56%G&H (automatic) 55.7% 49.5% 52.4%Brutus (automatic) 76.06% 70.15% 72.99%Table 1: Accuracy of semantic role prediction usingonly CCG based features.originally tagged terminal.
In most cases, this corre-sponds to the node immediately dominated by the low-est common subsuming node of the the target word andthe verb (figure 5).
In some cases, the highest con-stituent that is headed by the target word is not imme-diately dominated by the lowest common subsumingnode (figure 6).9 ResultsUsing a version of Brutus incorporating only the CCG-based features described above, we achieve better re-sults than a previous CCG based system (Gildea andHockenmaier, 2003, henceforth G&H).
This could bedue to a number of factors, including the fact that oursystem employs a different CCG parser, uses a morecomplete mapping of the Propbank onto the CCGbank,uses a different machine learning approach,6 and has aricher feature set.
The results for constituent taggingaccuracy are shown in table 1.As expected, by incorporating Penn Treebank-basedfeatures and dependency features, we obtain better re-sults than with the CCG-only system.
The results forgold standard parses are comparable to the winningsystem of the CoNLL 2005 shared task on semanticrole labeling (Punyakanok et al, 2008).
Other systems(Toutanova et al, 2008; Surdeanu et al, 2007; Johans-son and Nugues, 2008) have also achieved comparableresults ?
we compare our system to (Punyakanok etal., 2008) due to the similarities in our approaches.
Theperformance of the full system is shown in table 2.Table 3 shows the ability of the system to predictthe correct headwords of semantic roles.
This is a nec-essary condition for correctness of the full constituent,but not a sufficient one.
In parser evaluation, Carroll,Minnen, and Briscoe (Carroll et al, 2003) have argued6G&H use a generative model with a back-off lattice,whereas we use a maximum entropy classifier.P R FP.
et al(treebank) 86.22% 87.40% 86.81%Brutus (treebank) 88.29% 86.39% 87.33%P.
et al(automatic) 77.09% 75.51% 76.29%Brutus (automatic) 76.73% 70.45% 73.45%Table 2: Accuracy of semantic role prediction usingCCG, CFG, and MALT based features.P R FHeadword (treebank) 88.94% 86.98% 87.95%Boundary (treebank) 88.29% 86.39% 87.33%Headword (automatic) 82.36% 75.97% 79.04%Boundary (automatic) 76.33% 70.59% 73.35%Table 3: Accuracy of the system for labeling semanticroles on both constituent boundaries and headwords.Headwords are easier to predict than boundaries, re-flecting CCG?s focus on word-word relations ratherthan constituency.for dependencies as a more appropriate means of eval-uation, reflecting the focus on headwords from con-stituent boundaries.
We argue that, especially in theheavily lexicalized CCG framework, headword evalu-ation is more appropriate, reflecting the emphasis onheadword combinatorics in the CCG formalism.10 The Contribution of the New FeaturesTwo features which are less frequently used in SRLresearch play a major role in the Brutus system: ThePARG feature (Gildea and Hockenmaier, 2003) andthe argument mapping feature.
Removing them hasa strong effect on accuracy when labeling treebankparses, as shown in our feature ablation results in ta-ble 4.
We do not report results including the Argu-ment Mapping feature but not the PARG feature, be-cause some predicate-argument relation information isassumed in generating the Argument Mapping feature.P R F+PARG +AM 88.77% 86.15% 87.44%+PARG -AM 88.42% 85.78% 87.08%-PARG -AM 87.92% 84.65% 86.26%Table 4: The effects of removing key features from thesystem on gold standard parses.The same is true for automatic parses, as shown in ta-ble 5.11 Error AnalysisMany of the errors made by the Brutus system can betraced directly to erroneous parses, either in the auto-matic or treebank parse.
In some cases, PP attachment42with even brief exposures causing symptoms(((vp\vp)/vp[ng])/np n/n n/n n (s[ng]\np)/np np> >n s[ng]\np>nnp ?
cause.Arg0>(vp\vp)/vp[ng]>vp\vpFigure 6: In this case, with is the head of with even brief exposures, so the role is correctly marked on even briefexposures (based on wsj 0003.2).P R F+PARG +AM 74.14% 62.09% 67.58%+PARG -AM 70.02% 64.68% 67.25%-PARG -AM 73.90% 61.15% 66.93%Table 5: The effects of removing key features from thesystem on automatic parses.ambiguities cause a role to be marked too high in thederivation.
In the sentence the company stopped usingasbestos in 1956 (figure 7), the correct Arg1 of stoppedis using asbestos.
However, because in 1956 is erro-neously modifying the verb using rather than the verbstopped in the treebank parse, the system trusts the syn-tactic analysis and places Arg1 of stopped on using as-bestos in 1956.
This particular problem is caused by anannotation error in the original Penn Treebank that wascarried through in the conversion to CCGbank.Another common error deals with genitive construc-tions.
Consider the phrase a form of asbestos usedto make filters.
By CCG combinatorics, the relativeclause could either attach to asbestos or to a form ofasbestos.
The gold standard CCG parse attaches therelative clause to a form of asbestos (figure 8).
Prop-bank agrees with this analysis, assigning Arg1 of useto the constituent a form of asbestos.
The automaticparser, however, attaches the relative clause low ?
toasbestos (figure 9).
When the system is given the au-tomatically generated parse, it incorrectly assigns thesemantic role to asbestos.
In cases where the parser at-taches the relative clause correctly, the system is muchmore likely to assign the role correctly.Problems with relative clause attachment to genitivesare not limited to automatic parses ?
errors in gold-standard treebank parses cause similar problems whenTreebank parses disagree with Propbank annotator in-tuitions.
In the phrase a group of workers exposed toasbestos (figure 10), the gold standard CCG parse at-taches the relative clause to workers.
Propbank, how-ever, annotates a group of workers as Arg1 of exposed,rather than following the parse and assigning the roleonly to workers.
The system again follows the parseand incorrectly assigns the role to workers instead of agroup of workers.
Interestingly, the C&C parser optsfor high attachment in this instance, resulting in thea form of asbestos used to make filtersnp (np\np)/np np np\np>np\np<np ?
Arg1<npFigure 8: CCGbank gold-standard parse of a relativeclause attachment.
The system correctly identifies aform of asbestos as Arg1 of used.
(wsj 0003.1)a form of asbestos used to make filtersnp (np\np)/np np ?
Arg1 np\np<np>np\np<npFigure 9: Automatic parse of the noun phrase in fig-ure 8.
Incorrect relative clause attachment causes themisidentification of asbestos as a semantic role bearingunit.
(wsj 0003.1)correct prediction of a group of workers as Arg1 of ex-posed in the automatic parse.12 Future WorkAs described in the error analysis section, a large num-ber of errors in the system are attributable to errors inthe CCG derivation, either in the gold standard or inautomatically generated parses.
Potential future workmay focus on developing an improved CCG parser us-ing the revised (syntactic) adjunct-argument distinc-tions (guided by the Propbank annotation) described in(Boxwell and White, 2008).
This resource, togetherwith the reasonable accuracy (?
90%) with which ar-gument mappings can be predicted, suggests the possi-bility of an integrated, simultaneous syntactic-semanticparsing process, similar to that of (Musillo and Merlo,2006; Merlo and Musillo, 2008).
We expect this wouldimprove the reliability and accuracy of both the syntac-tic and semantic analysis components.13 AcknowledgmentsThis research was funded by NSF grant IIS-0347799.We are deeply indebted to Julia Hockenmaier for the43the company stopped using asbestos in 1956np ((s[dcl]\np)/(s[ng]\np)) (s[ng]\np)/np np (s\np)\(s\np)>s[ng]\np<s[ng]\np ?
stop.Arg1>s[dcl]\np<s[dcl]Figure 7: An example of how incorrect PP attachment can cause an incorrect labeling.
Stop.Arg1 should cover us-ing asbestos rather than using asbestos in 1956.
This sentence is based on wsj 0003.3, with the structure simplifiedfor clarity.a group of workers exposed to asbestosnp (np\np)/np np ?
exposed.Arg1 np\np<np>np\np<npFigure 10: Propbank annotates a group of workers as Arg1 of exposed, while CCGbank attaches the relative clauselow.
The system incorrectly labels workers as a role bearing unit.
(Gold standard ?
wsj 0003.1)use of her PARG generation tool.ReferencesSrinivas Bangalore and Aravind Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational Linguistics, 25(2):237?265.Adam L. Berger, S. Della Pietra, and V. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39?71.D.M.
Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Stephen A. Boxwell and Michael White.
2008.Projecting propbank roles onto the ccgbank.
InProceedings of the Sixth International LanguageResources and Evaluation Conference (LREC-08),Marrakech, Morocco.J.
Carroll, G. Minnen, and T. Briscoe.
2003.
Parserevaluation.
Treebanks: Building and Using ParsedCorpora, pages 299?316.E.
Charniak.
2001.
Immediate-head parsing for lan-guage models.
In Proc.
ACL-01, volume 39, pages116?123.Stephen Clark and James R. Curran.
2007.
Wide-coverage Efficient Statistical Parsing with CCG andLog-linear Models.
Computational Linguistics,33(4):493?552.Stephen Clark.
2002.
Supertagging for combinatorycategorial grammar.
In Proceedings of the 6th In-ternational Workshop on Tree Adjoining Grammarsand Related Frameworks (TAG+6), pages 19?24,Venice, Italy.M.
Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguis-tics, 29(4):589?637.Daniel Gildea and Julia Hockenmaier.
2003.
Identi-fying semantic roles using Combinatory CategorialGrammar.
In Proc.
EMNLP-03.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33(3):355?396.R.
Johansson and P. Nugues.
2008.
Dependency-based syntactic?semantic analysis with PropBankand NomBank.
Proceedings of CoNLL?2008.D C Liu and Jorge Nocedal.
1989.
On the limitedmemory method for large scale optimization.
Math-ematical Programming B, 45(3).Llu?
?s Ma`rquez, Xavier Carreras, Kenneth C. Litowski,and Suzanne Stevenson.
2008.
Semantic Role La-beling: An Introduction to the Special Issue.
Com-putational Linguistics, 34(2):145?159.Paola Merlo and Gabrile Musillo.
2008.
Semanticparsing for high-precision semantic role labelling.
InProceedings of CONLL-08, Manchester, UK.Gabriele Musillo and Paola Merlo.
2006.
Robust pars-ing of the proposition bank.
In Proceedings of theEACL 2006 Workshop ROMAND, Trento.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13(02):95?135.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.44Vasin Punyakanok, Dan Roth, and Wen tau Yih.
2008.The Importance of Syntactic Parsing and Inferencein Semantic Role Labeling.
Computational Linguis-tics, 34(2):257?287.Mark Steedman.
2000.
The Syntactic Process.
MITPress.M.
Surdeanu, L. Ma`rquez, X. Carreras, and P. Comas.2007.
Combination strategies for semantic role la-beling.
Journal of Artificial Intelligence Research,29:105?151.K.
Toutanova, A. Haghighi, and C.D.
Manning.
2008.A global joint model for semantic role labeling.Computational Linguistics, 34(2):161?191.45
