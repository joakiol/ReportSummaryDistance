Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 49?52, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsLanguage Independent Extractive SummarizationRada MihalceaDepartment of Computer Science and EngineeringUniversity of North Texasrada@cs.unt.eduAbstractWe demonstrate TextRank ?
a system forunsupervised extractive summarization thatrelies on the application of iterative graph-based ranking algorithms to graphs encod-ing the cohesive structure of a text.
An im-portant characteristic of the system is thatit does not rely on any language-specificknowledge resources or any manually con-structed training data, and thus it is highlyportable to new languages or domains.1 IntroductionGiven the overwhelming amount of information avail-able today, on the Web and elsewhere, techniquesfor efficient automatic text summarization are essen-tial to improve the access to such information.
Al-gorithms for extractive summarization are typicallybased on techniques for sentence extraction, and at-tempt to identify the set of sentences that are mostimportant for the understanding of a given document.Some of the most successful approaches to extractivesummarization consist of supervised algorithms thatattempt to learn what makes a good summary by train-ing on collections of summaries built for a relativelylarge number of training documents, e.g.
(Hirao etal., 2002), (Teufel and Moens, 1997).
However, theprice paid for the high performance of such super-vised algorithms is their inability to easily adapt tonew languages or domains, as new training data arerequired for each new type of data.
TextRank (Mi-halcea and Tarau, 2004), (Mihalcea, 2004) is specifi-cally designed to address this problem, by using an ex-tractive summarization technique that does not requireany training data or any language-specific knowledgesources.
TextRank can be effectively applied to thesummarization of documents in different languageswithout any modifications of the algorithm and with-out any requirements for additional data.
Moreover,results from experiments performed on standard datasets have demonstrated that the performance of Text-Rank is competitive with that of some of the best sum-marization systems available today.2 Extractive SummarizationRanking algorithms, such as Kleinberg?s HITS al-gorithm (Kleinberg, 1999) or Google?s PageRank(Brin and Page, 1998) have been traditionally and suc-cessfully used in Web-link analysis, social networks,and more recently in text processing applications.
Inshort, a graph-based ranking algorithm is a way of de-ciding on the importance of a vertex within a graph,by taking into account global information recursivelycomputed from the entire graph, rather than relyingonly on local vertex-specific information.
The basicidea implemented by the ranking model is that of vot-ing or recommendation.
When one vertex links to an-other one, it is basically casting a vote for that othervertex.
The higher the number of votes that are castfor a vertex, the higher the importance of the vertex.These graph ranking algorithms are based on arandom walk model, where a walker takes randomsteps on the graph, with the walk being modeled as aMarkov process ?
that is, the decision on what edge tofollow is solely based on the vertex where the walkeris currently located.
Under certain conditions, this49model converges to a stationary distribution of prob-abilities associated with vertices in the graph, repre-senting the probability of finding the walker at a cer-tain vertex in the graph.
Based on the Ergodic theoremfor Markov chains (Grimmett and Stirzaker, 1989),the algorithms are guaranteed to converge if the graphis both aperiodic and irreducible.
The first conditionis achieved for any graph that is a non-bipartite graph,while the second condition holds for any strongly con-nected graph.
Both these conditions are achieved inthe graphs constructed for the extractive summariza-tion application implemented in TextRank.While there are several graph-based ranking algo-rithms previously proposed in the literature, we fo-cus on two algorithms, namely PageRank (Brin andPage, 1998) and HITS (Kleinberg, 1999).Let G = (V, E) be a directed graph with the set ofvertices V and set of edges E, where E is a subsetof V ?
V .
For a given vertex Vi, let In(Vi) be theset of vertices that point to it (predecessors), and letOut(Vi) be the set of vertices that vertex Vi points to(successors).2.1 PageRankPageRank (Brin and Page, 1998) is perhaps oneof the most popular ranking algorithms, and wasdesigned as a method for Web link analysis.
Un-like other graph ranking algorithms, PageRank inte-grates the impact of both incoming and outgoing linksinto one single model, and therefore it produces onlyone set of scores:PR(Vi) = (1 ?
d) + d ?
?Vj?In(Vi)PR(Vj)|Out(Vj)|(1)where d is a parameter that is set between 0 and 1,and has the role of integrating random jumps into therandom walking model.2.2 HITSHITS (Hyperlinked Induced Topic Search) (Klein-berg, 1999) is an iterative algorithm that was designedfor ranking Web pages according to their degree of?authority?.
The HITS algorithm makes a distinc-tion between ?authorities?
(pages with a large num-ber of incoming links) and ?hubs?
(pages with a largenumber of outgoing links).
For each vertex, HITSproduces two sets of scores ?
an ?authority?
score, anda ?hub?
score:HITSA(Vi) =?Vj?In(Vi)HITSH(Vj) (2)HITSH(Vi) =?Vj?Out(Vi)HITSA(Vj) (3)Starting from arbitrary values assigned to each nodein the graph, the ranking algorithm iterates until con-vergence below a given threshold is achieved.
Afterrunning the algorithm, a score is associated with eachvertex, which represents the importance of that ver-tex within the graph.
Note that the final values arenot affected by the choice of the initial value, only thenumber of iterations to convergence may be different.When the graphs are built starting with natural lan-guage texts, it may be useful to integrate into the graphmodel the strength of the connection between two ver-tices Vi and Vj , indicated as a weight wij added tothe corresponding edge.
Consequently, the rankingalgorithm is adapted to include edge weights, e.g.
forPageRank the score is determined using the follow-ing formula (a similar change can be applied to theHITS algorithm):PRW (Vi) = (1?d)+d?
?Vj?In(Vi)wjiPRW (Vj)?Vk?Out(Vj)wkj(4)While the final vertex scores (and therefore rank-ings) for weighted graphs differ significantly as com-pared to their unweighted alternatives, the number ofiterations to convergence and the shape of the con-vergence curves is almost identical for weighted andunweighted graphs.For the task of single-document extractive summa-rization, the goal is to rank the sentences in a giventext with respect to their importance for the overallunderstanding of the text.
A graph is therefore con-structed by adding a vertex for each sentence in thetext, and edges between vertices are established us-ing sentence inter-connections, defined using a simplesimilarity relation measured as a function of contentoverlap.
Such a relation between two sentences can beseen as a process of recommendation: a sentence thataddresses certain concepts in a text gives the readera recommendation to refer to other sentences in the50text that address the same concepts, and therefore alink can be drawn between any two such sentencesthat share common content.The overlap of two sentences can be determinedsimply as the number of common tokens between thelexical representations of the two sentences, or it canbe run through filters that e.g.
eliminate stopwords,count only words of a certain category, etc.
Moreover,to avoid promoting long sentences, we use a normal-ization factor and divide the content overlap of twosentences with the length of each sentence.The resulting graph is highly connected, with aweight associated with each edge, indicating thestrength of the connections between various sentencepairs in the text.
The graph can be represented as: (a)simple undirected graph; (b) directed weighted graphwith the orientation of edges set from a sentence tosentences that follow in the text (directed forward);or (c) directed weighted graph with the orientation ofedges set from a sentence to previous sentences in thetext (directed backward).After the ranking algorithm is run on the graph,sentences are sorted in reversed order of their score,and the top ranked sentences are selected for inclu-sion in the summary.
Figure 1 shows an example of aweighted graph built for a short sample text.
[1] Watching the new movie, ?Imagine: John Lennon,?
was verypainful for the late Beatle?s wife, Yoko Ono.
[2] ?The only reason why I did watch it to the end is because I?mresponsible for it, even though somebody else made it,?
she said.
[3] Cassettes, film footage and other elements of the acclaimedmovie were collected by Ono.
[4] She also took cassettes of interviews by Lennon, which wereedited in such a way that he narrates the picture.
[5] Andrew Solt (?This Is Elvis?)
directed, Solt and David L.Wolper produced and Solt and Sam Egan wrote it.
[6] ?I think this is really the definitive documentary of JohnLennon?s life,?
Ono said in an interview.3 EvaluationEnglish document summarization experiments are runusing the summarization test collection provided inthe framework of the Document Understanding Con-ference (DUC).
In particular, we use the data set of567 news articles made available during the DUC2002 evaluations (DUC, 2002), and the correspond-ing 100-word summaries generated for each of thesedocuments.
This is the single document summariza-tion task undertaken by other systems participating in1234560.160.300.46 0.15[1.34][1.75][0.70][0.74][0.52][0.91]0.150.290.320.15Figure 1: Graph of sentence similarities built on asample text.
Scores reflecting sentence importance areshown in brackets next to each sentence.the DUC 2002 document summarization evaluations.To test the language independence aspect of the al-gorithm, in addition to the English test collection, wealso use a Brazilian Portuguese data set consisting of100 news articles and their corresponding manuallyproduced summaries.
We use the TeMa?rio test col-lection (Pardo and Rino, 2003), containing newspa-per articles from online Brazilian newswire: 40 docu-ments from Jornal de Brasil and 60 documents fromFolha de Sa?o Paulo.
The documents were selected tocover a variety of domains (e.g.
world, politics, for-eign affairs, editorials), and manual summaries wereproduced by an expert in Brazilian Portuguese.
Unlikethe summaries produced for the English DUC docu-ments ?
which had a length requirement of approxi-mately 100 words, the length of the summaries in theTeMa?rio data set is constrained relative to the lengthof the corresponding documents, i.e.
a summary hasto account for about 25-30% of the original document.Consequently, the automatic summaries generated forthe documents in this collection are not restricted to100 words, as in the English experiments, but are re-quired to have a length comparable to the correspond-ing manual summaries, to ensure a fair evaluation.For evaluation, we are using the ROUGE evaluationtoolkit1, which is a method based on Ngram statistics,found to be highly correlated with human evaluations(Lin and Hovy, 2003).
The evaluation is done usingthe Ngram(1,1) setting of ROUGE, which was foundto have the highest correlation with human judgments,at a confidence level of 95%.Table 2 shows the results obtained on these two datasets for different graph settings.
The table also listsbaseline results, obtained on summaries generated by1ROUGE is available at http://www.isi.edu/?cyl/ROUGE/.51GraphAlgorithm Undirected Forward BackwardHITSWA 0.4912 0.4584 0.5023HITSWH 0.4912 0.5023 0.4584PageRankW 0.4904 0.4202 0.5008Baseline 0.4799Table 1: English single-document summarization.GraphAlgorithm Undirected Forward BackwardHITSWA 0.4814 0.4834 0.5002HITSWH 0.4814 0.5002 0.4834PageRankW 0.4939 0.4574 0.5121Baseline 0.4963Table 2: Portuguese single-document summarization.taking the first sentences in each document.
By waysof comparison, the best participating system in DUC2002 was a supervised system that led to a ROUGEscore of 0.5011.For both data sets, TextRank applied on a directedbackward graph structure exceeds the performanceachieved through a simple (but powerful) baseline.These results prove that graph-based ranking algo-rithms, previously found successful in Web link anal-ysis and social networks, can be turned into a state-of-the-art tool for extractive summarization when ap-plied to graphs extracted from texts.
Moreover, dueto its unsupervised nature, the algorithm was alsoshown to be language independent, leading to similarresults and similar improvements over baseline tech-niques when applied on documents in different lan-guages.
More extensive experimental results with theTextRank system are reported in (Mihalcea and Tarau,2004), (Mihalcea, 2004).4 ConclusionIntuitively, iterative graph-based ranking algorithmswork well on the task of extractive summarization be-cause they do not only rely on the local context of atext unit (vertex), but they also take into account infor-mation recursively drawn from the entire text (graph).Through the graphs it builds on texts, a graph-basedranking algorithm identifies connections between var-ious entities in a text, and implements the concept ofrecommendation.
In the process of identifying impor-tant sentences in a text, a sentence recommends othersentences that address similar concepts as being use-ful for the overall understanding of the text.
Sentencesthat are highly recommended by other sentences arelikely to be more informative for the given text, andwill be therefore given a higher score.An important aspect of the graph-based extractivesummarization method is that it does not require deeplinguistic knowledge, nor domain or language specificannotated corpora, which makes it highly portable toother domains, genres, or languages.AcknowledgmentsWe are grateful to Lucia Helena Machado Rino formaking available the TeMa?rio summarization test col-lection and for her help with this data set.ReferencesS.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual Web search engine.
Computer Networksand ISDN Systems, 30(1?7).DUC.
2002.
Document understanding conference 2002.http://www-nlpir.nist.gov/projects/duc/.G.
Grimmett and D. Stirzaker.
1989.
Probability and Ran-dom Processes.
Oxford University Press.T.
Hirao, Y. Sasaki, H. Isozaki, and E. Maeda.
2002.
Ntt?stext summarization system for duc-2002.
In Proceed-ings of the Document Understanding Conference 2002.J.M.
Kleinberg.
1999.
Authoritative sources in a hyper-linked environment.
Journal of the ACM, 46(5):604?632.C.Y.
Lin and E.H. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProceedings of Human Language Technology Confer-ence (HLT-NAACL 2003), Edmonton, Canada, May.R.
Mihalcea and P. Tarau.
2004.
TextRank ?
bringing orderinto texts.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing (EMNLP2004), Barcelona, Spain.R.
Mihalcea.
2004.
Graph-based ranking algorithms forsentence extraction, applied to text summarization.
InProceedings of the 42nd Annual Meeting of the Associ-ation for Computational Lingusitics (ACL 2004) (com-panion volume), Barcelona, Spain.T.A.S.
Pardo and L.H.M.
Rino.
2003.
TeMario: a cor-pus for automatic text summarization.
Technical report,NILC-TR-03-09.S.
Teufel and M. Moens.
1997.
Sentence extraction as aclassification task.
In ACL/EACL workshop on ?Intel-ligent and scalable Text summarization?, pages 58?65,Madrid, Spain.52
