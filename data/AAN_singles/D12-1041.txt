Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 445?454, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsForced Derivation Tree based Model Training toStatistical Machine TranslationNan DuanMicrosoft Research Asiananduan@microsoft.comMu LiMicrosoft Research Asiamuli@microsoft.comMing ZhouMicrosoft Research Asiamingzhou@microsoft.comAbstractA forced derivation tree (FDT) of a sentencepair {f, e} denotes a derivation tree that cantranslate f into its accurate target translatione.
In this paper, we present an approach thatleverages structured knowledge contained inFDTs to train component models for statisticalmachine translation (SMT) systems.
We firstdescribe how to generate different FDTs foreach sentence pair in training corpus, and thenpresent how to infer the optimal FDTs basedon their derivation and alignment qualities.
Asthe first step in this line of research, we verifythe effectiveness of our approach in a BTG-based phrasal system, and propose four FDT-based component models.
Experiments arecarried out on large scale English-to-Japaneseand Chinese-to-English translation tasks, andsignificant improvements are reported on bothtranslation quality and alignment quality.1 IntroductionMost of today?s SMT systems depends heavily onparallel corpora aligned at the word-level to traintheir different component models.
However, suchannotations do have their drawbacks in training.On one hand, word links predicted by automaticaligners such as GIZA++ (Och and Ney, 2004) oftencontain errors.
This problem gets even worse on lan-guage pairs that differ substantially in word orders,such as English and Japanese/Korean/German.
Thedescent of the word alignment quality will lead toinaccurate component models straightforwardly.On the other hand, several component modelsare designed to supervise the decoding procedures,which usually rely on training examples extractedfrom word-aligned sentence pairs, such as distortionmodels (Tillman, 2004; Xiong et al2006; Galleyand Manning, 2008) and sequence models (Banchset al2005; Quirk and Menezes, 2006; Vaswani etal., 2011).
Ideally, training examples of models areexpected to match most of the situations that couldbe met in decoding procedures.
But actually, plainstructures of word alignments are too coarse to pro-vide enough knowledge to ensure this expectation.This paper presents an FDT-based model trainingapproach to SMT systems by leveraging structuredknowledge contained in FDTs.
An FDT of a sen-tence pair {f, e} denotes a derivation tree that cantranslate f into its accurate target translation e. Theprinciple advantage of this work is two-fold.
First,using alignments induced from the 1-best FDTs ofall sentence pairs, the overall alignment quality oftraining corpus can be improved.
Second, compar-ing to word alignments, FDTs can provide richerstructured knowledge for various component modelsto extract training instances.
Our FDT-based mod-el training approach performs via three steps: (1)generation, where an FDT space composed of dif-ferent FDTs is generated for each sentence pair intraining corpus by the forced decoding technique;(2) inference, where the optimal FDTs are extract-ed from the FDT space of each sentence pair basedon both derivation and alignment qualities measuredby a memory-based re-ranking model; (3) training,where various component models are trained basedon the optimal FDTs extracted in the inference step.Our FDT-based model training approach can beadapted to SMT systems with arbitrary paradigms.445As the first step in this line of research, our approachis verified in a phrase-based SMT system on bothEnglish-to-Japanese and Chinese-to-English transla-tion tasks .
Significant improvements are reportedon both translation quality (up to 1.31 BLEU) andword alignment quality (up to 3.15 F-score).2 Forced Derivation Tree for SMTA forced derivation tree (FDT) of a sentence pair{f, e} can be defined as a pair G =< D,A >:?
D denotes a derivation that can translate f intoe accurately, using a set of translation rules.?
A denotes a set of word links (i, j) indicatingthat ei ?
e aligns to fj ?
f .In this section, we first describe how to gener-ate FDTs for each sentence pair in training corpus,which is denoted as the generation step, and thenpresent how to select the optimal FDT for each sen-tence pair, which is denoted as the inference step.We leave a real application of FDTs to the modeltraining in a phrase-based SMT system in Section 3.2.1 GenerationWe first describe how to generate multiple FDTs foreach sentence pair in training corpus C based on theforced decoding (FD) technique, which performs viathe following four steps:1.
Train component models needed for a specificSMT paradigm M based on training corpus C;2.
Perform MERT on the development data set toobtain a set of optimized feature weights;3.
For each {f, e} ?
C, translate f into accurate ebased onM, component models trained in step1, and feature weights optimized in step 2;4.
For each {f, e} ?
C, output the hypergraph(Huang and Chiang, 2005) H(f, e) generatedin step 3 as its FDT space.In step 3: (1) all partial hypotheses that do not matchany sequence in e will be discarded; (2) derivationscovering identical source and target words but withdifferent alignments will be kept as different partialcandidates, as they can produce different FDTs forthe same sentence pair.
For each {f, e}, the proba-bility of each G ?
H(f, e) is computed as:p(G|H(f, e)) = exp{?(G)}?G?
?H(f,e) exp{?(G?
)}(1)where ?
(G) is the FD model score assigned to G.For each sentence pair, different alignment candi-dates can be induced from its different forced deriva-tion trees generated in the generation step, becauseFD can use phrase pairs with different internal wordlinks extracted from other sentence pairs to recon-struct the given sentence pair, which could lead tobetter word alignment candidates.2.2 InferenceGiven an FDT spaceH(f, e), we propose a memory-based re-ranking model (MRM), which selects thebest FDT G?
as follows:G?
= argmaxG?H(f,e)exp{?i ?ihi(G)}?G?
?H(f,e) exp{?i ?ihi(G?
)}= argmaxG?H(f,e)?i?ihi(G) (2)where hi(G) is feature function and ?i is its featureweight.
Here, memory means the whole translationhistory that happened in the generation step will beused as the evidence to help us compute features.From the definition we can see that the quality ofan FDT directly relates to two aspects: its derivationD and alignments A.
So two kinds of features areused to measure the overall quality of each FDT.
(I) The features in the first category measure thederivation quality of each FDT, including:?
h(e?|f?
), source-to-target translation probabilityof a translation rule r = {f?
, e?}.h(e?|f?)
=?
{f,e}?C fracH(f,e)(f?
, e?)?{f,e}?C?e??
fracH(f,e)(f?
, e??)(3)fracH(f,e)(f?
, e?)
denotes the fractional count ofr used in generating H(f, e):fracH(f,e)(f?
, e?)
=?G?H(f,e)1r(G)p(G|H(f, e))1r(G) is an indicator function that equals 1when r is used in G and 0 otherwise.
In prac-tice, we use pH(f,e)(r) of r to approximate446fracH(f,e)(f?
, e?)
when the size of H(f, e) is toolarge to enumerate all FDTs:pH(f,e)(r) =?
(r)O(head(r))?v?tail(r) I(v)Z(f)where ?
(r) is the weight of translation rule rin the FDT space H(f, e), Z is a normalizationfactor that equals to the inside probability ofthe root node in H(f, e), I(v) and O(v) arethe standard inside and outside probabilities ofa node v inH(f, e), head(r) and tail(r) are thehead node and a set of tail nodes of a translationrule r in H(f, e) respectively.?
h(f?
|e?
), target-to-source translation probabilityof a translation rule r = {f?
, e?}.h(f?
|e?)
=?
{f,e}?C fracH(f,e)(f?
, e?)?{f,e}?C?f?
?
fracH(f,e)(f?
?, e?)(4)?
h#(r), smoothed usage count for translationrule r = {f?
, e?}
in the whole generation step.h#(r) =11 + e{??
{f,e}?C fracH(f,e)(f?
,e?
)}(5)In this paper, the sigmoid function is used tomake sure that the feature values of differenttranslation rules are in a proper value range.?
hr(G), number of translation rules used in G.?
hd(G), structure-based score of G. For FDTsgenerated by phrase-based paradigms, it can becomputed by distortion models; while for FDTsgenerated by syntax-based paradigms, it can becomputed by either parsing models or syntacticLMs (Charniak et al2003).The overfitting issue in the generation step can bealleviated by leveraging memory-based features inthe inference step.
h#(r) is used to penalize thoselong translation rules which tend to occur in only afew training sentences and are used few times in FD,hr(G) adjust our MRM to prefer FDTs consisting ofmore translation rules, hd(G) is used to select FDTswith better parse tree-like structures, which can beinduced from their derivations directly.
(II) The features in the second category measurethe alignment quality of each FDT, including:?
word pair translation probabilities trained fromIBM models (Brown et al1993);?
log-likelihood ratio (Moore, 2005);?
conditional link probability (Moore, 2005);?
count of unlinked words;?
counts of inversion and concatenation.Many alignment-inspired features can be used inMRM.
This paper only uses those commonly-usedones that have already been proved useful in manyprevious work (Moore, 2005; Moore et al2006;Fraser and Marcu, 2006; Liu et al2010).Following the common practice in SMT research,the MERT algorithm (Och, 2003) is used to tune fea-ture weights in MRM.
Due to the fact that all FDTsof each sentence pair share identical translation, wecannot use BLEU as the error criterion any more.Instead, alignment F-score is used as the alterna-tive.
We will show in Section 5 that after the in-ference step, alignment quality can be improved byreplacing original alignments of each sentence pairwith alignments induced from its 1-best FDT.
Futurework could experiment with other error criterions,such as reordering-based loss functions (Birch et al2010; Talbot et al2011; Birch and Osborne, 2011)or span F1 (DeNero and Uszkoreit, 2011).3 Training in Phrase-based SMTAs the first step in this line of research, we explorethe usage of FDT-based model training method ina phrase-based SMT system (Xiong et al2006),which employs Bracketing Transduction Grammar(BTG) (Wu, 1997) to parse parallel sentences.
Thereason of choosing this system is due to the promi-nent advantages of BTG, such as the simplicity ofthe grammar and the good coverage of syntactic di-versities between different language pairs.
We firstdescribe more details of FDTs under BTG.
Then,four FDT-based component models are presented.3.1 BTG-based FDTGiven a sentence pair f = {f0, ..., fJ} and e ={e0, ..., eI} in training corpus, its FDT G generat-ed based on BTG is a binary tree, which is presentedby a set of terminal translation states T and a set ofnon-terminal translation states N , where:447Figure 1: S = {f?
[i,j), e?[i?,j?
), A?,m,m?,R} is denotedby the dark-shaded rectangle pair.
It can be split into twochild translation states, Sl, which is denoted by the light-shaded rectangle pair, and Sr, which is denoted by thewhite rectangle pair.
Dash lines within rectangle pairsdenote their internal alignments and solid lines with rowsdenote BTG rules.
(a) uses [?]
to combine two translationstates, while (b) uses ???.
Both Sl and Sr belong to T ?N .?
each terminal translation state S ?
T is a 3-tuple {f?
[i,j), e?[i?,j?
), A?
}, in which f?
[i,j) denotesthe word sequence that covers the source span[i, j) of f , e?[i?,j?)
denotes the target translationof f?
[i,j), which is the word sequence that coversthe target span [i?, j?)
of e at the same time, A?
isa set of word links that aligns f?
[i,j) and e?[i?,j?).?
each non-terminal translation state S ?
N is a5-tuple {f?
[i,j), e?[i?,j?
), A?,m,m?,R}1.
The first3 elements have the same meanings as in T ,whilem andm?
denote two split points that di-vide S into two child translation states, Sl andSr, R denotes a BTG rule, which is either a [?
]operation or a ???
operation2.
The relationshipbetween Sl, Sr and S is illustrated in Figure 1.All terminal translation states of the sentence pair{f, e} are disjoint but cover f[0,J+1) and e[0,I+1) atthe same time, where J = |f | and I = |e|, andall non-terminal translation states correspond to thepartial decoding states generated during decoding.3.2 FDT-based Translation ModelFirst, an FDT-based translation model (FDT-TM) ispresented for our BTG-based system.1We sometimes omit m, m?
and R for a simplicity reason.2A [?]
operation combines the translations of two consecu-tive source spans [i,m) and [m, j) in a monotonic way; whilea ???
operation combines them in an inverted way.Given sentence pairs in training corpus with theircorresponding FDT spaces, we train FDT-TM in t-wo different ways: (1) The first only uses the 1-bestFDT of each sentence pair.
Based on each align-ment A induced from each 1-best FDT G, all possi-ble bilingual phrases are extracted.
Then, the max-imum likelihood estimation (MLE) is used to com-pute probabilities and generate an FDT-TM.
(2) Thesecond uses the n-best FDTs of each sentence pair,which is motivated by several studies (Venugopal etal., 2008; Liu et al2009).
For each sentence pair{f, e}, we first induce n alignments {A1, ...,An}from the top n FDTs ?
= {G1, ...,Gn} ?
H(f, e).Each Ak is annotated with the posterior probabilityof its corresponding FDT Gk as follows:p(Ak|Gk) =exp{?i ?ihi(Gk)}?Gk??
?exp{?i ?ihi(Gk?
)}(6)where?i ?ihi(Gk) is the model score assigned toGk by MRM.
Then, all possible bilingual phrasesare extracted from the expanded training corpus builtusing n-best alignments for each sentence pair.
Thecount of each phrase pair is now computed as thesum of posterior probabilities, instead of the sum ofabsolute frequencies.
Last, MLE is used to computeprobabilities and generate an FDT-TM.3.3 FDT-based Distortion ModelIn Xiong?s BTG system, training instances of thedistortion model (DM) are pruned based on heuris-tic rules, aiming to keep the training size acceptable.But this will cause the examples remained cannotcover all reordering cases that could be met in realdecoding procedures.
To overcome this drawback,we propose an FDT-based DM (FDT-DM).Given the 1-best FDT G of a sentence pair {f, e},all non-terminal translation states {S1, ...,SK} arefirst extracted.
For each Sk, we split it into twochild translation states Skl and Skr.
A training in-stance can be then obtained, using the BTG opera-tionR ?
Sk as its class label and boundary words oftwo translation blocks (f?Skl , e?Skl) and (f?Skr , e?Skr)contained in Skl and Skr as its features.
Last, theFDT-DM is trained based on all training instancesby a MaxEnt toolkit, which can cover both local andglobal reordering situations due to its training in-stance extraction mechanism.
Figure 2 shows an ex-ample of extracting training instances from an FDT.448Figure 2: An example of extracting training instancesfrom an FDT, where solid lines with rows denote BTGoperations and dash lines denote alignments.
Two in-stances can be extracted from this FDT, where 0 and 1denote a [?]
operation and a ???
operation respectively.
InDM training, the number (0 or 1) in each instance is usedas a label, while boundary words are extracted from eachinstance?s two phrase pairs and used as lexical features.3.4 FDT-based Source Language ModelWe next propose an FDT-based source languagemodel (FDT-SLM).Given the 1-best FDT G of a sentence pair {f, e},we first extract a reordered source word sequencef ?
= {f ?0, ..., f ?J} from G, based on the order of ter-minal translation states in G which covers the targettranslation e from left to right.
This procedure canbe illustrated by Algorithm 1.
Then, all reorderedsource sentences of training corpus are used to traina source LM.
During decoding, each time when anew hypothesis is generated, we obtain its reorderedsource word sequence as well, compute a LM scorebased on FDT-SLM and use it as a new feature:hSLM (f ?)
=J?k=1p(f ?k|f ?k?n+1, ..., f ?k?1) (7)3.5 FDT-based Rule Sequence ModelThe last contribution in this section is an FDT-basedrule sequence model (FDT-RSM).Given the 1-best FDT G of a sentence pair {f, e},we first extract a sequence of translation rule appli-cations {r1, ..., rK} based on Algorithm 2, whereAlgorithm 1: Sequence Extraction in FDT-SLM1 let f ?
= ?
;2 let S?
= {S1?
, ...,SK?}
represents an orderedsequence of terminal translation states whosetarget phrases cover e from left to right orderly;3 foreach S ?
S?
in the left-to-right order do4 extract f?
[i,j) from S;5 append f?
[i,j) to f ?
;6 append a blank space to f ?
;7 end8 return f ?
as a reordered source word sequence.rk = (f?
[i,j), e?[i?,j?))
denotes the kth phrase pair.
Fig-ure 3 gives an example of extracting a rule sequencefrom an FDT.
An FDT-RSM is trained based on allrule sequences extracted from training corpus.
Dur-ing decoding, each time when a new hypothesis isgenerated, we compute an FDT-RSM score based onits rule sequence and use it as a new feature:hRSM (f, e) =K?k=1p(rk|rk?n+1, ..., rk?1) (8)Algorithm 2: Sequence Extraction in FDT-RSM1 let r?
= ?
;2 let S?
= {S1?
, ...,SK?}
represents an orderedsequence of terminal translation states whosetarget phrases cover e from left to right orderly;3 foreach S ?
S?
in the left-to-right order do4 extract a phrase pair (f?
[i,j), e?[i?,j?))
from S;5 add rk = (f?
[i,j), e?[i?,j?))
to r?
;6 end7 return r?
as a rule sequence.The main difference between FDT-SLM andFDT-RSM is that the former is trained based onmonolingual n-grams; while the latter is trainedbased on bilingual phrases.
Although these twomodels are trained and computed in an LM style,they are used as reordering features, because theyhelp SMT decoder find better decoding sequences.Of course, the usage of FDTs need not be limit-ed to the BTG-based system, and we consider usingFDTs generated by SCFG-based systems or tradi-tional left-to-right phrase-based systems in future.449Figure 3: An example of extracting a rule sequence froman FDT.
In order to generate the correct target translation,the desired rule sequence should be r2 ?
r3 ?
r1.4 Related Work4.1 Forced Decoding/AlignmentSchwartz (2008) used forced decoding to leveragemultilingual corpus to improve translation quality;Shen et al2008) used forced alignment to traina better phrase segmentation model; Wuebker et al(2010) used forced alignment to re-estimate trans-lation probabilities using a leaving-one-out strategy.We consider the usage of FD in Section 2.1 to bea direct extension of these approaches, but one thatgenerates FDTs for parallel data rather than focusingon phrase segmentation or probability estimation.4.2 Pre-reorderingPre-reordering (PRO) techniques (Collins et al2005; Xu et al2009; Genzel et al2010; Lee etal., 2010) used features from syntactic parse treesto reorder source sentences at training and transla-tion time.
A parser is often indispensable to providesyntactic information for such methods.
Recently,DeNero and Uszkoreit (2011) proposed an approachthat induced parse trees automatically from word-aligned training corpus to perform PRO for a phrase-based SMT system, instead of relying on treebanks.First, binary parse trees are induced from word-aligned training corpus.
Based on them, a monolin-gual parsing model and a tree reordering model aretrained to pre-reorder source words into the target-language-like order.
Their work is distinct from oursbecause it focused on inducing sentence structuresfor the PRO task, but mirrors ours in demonstratingthat there is a potential role for structure-based train-ing corpus in SMT model training.4.3 Distortion ModelsLexicalized distortion models (Tillman, 2004; Zensand Ney, 2006; Xiong et al2006; Galley and Man-ning, 2008;) are widely used in phrase-based SMTsystems.
Training instances of these models are ex-tracted from word-aligned sentence pairs.
Due to ef-ficiency reasons, only parts of all instances are keptand used in DM training, which cannot cover all pos-sible reordering situations that could be met in de-coding.
In FDT-DM, by contrast, training instancesare extracted from FDTs.
Such instances take bothlocal and global reordering cases into consideration.4.4 Sequence ModelsFeng et al2010) proposed an SLM in a phrase-based SMT system.
They used it as a reorderingfeature in the sense that it helped the decoder to findcorrect decoding sequences.
The difference betweentheir model and our FDT-SLM is that, in their work,the reordered source sequences are extracted basedon word alignments only; while in our FDT-SLM,such sequences are obtained based on FDTs.Quirk and Menezes (2006) proposed a MinimalTranslation Unit (MTU) -based sequence model andused it in their treelet system; Vaswani et al2011)proposed a rule Markov model to capture dependen-cies between minimal rules for a top-down tree-to-string system.
The key difference between FDT-RSM and previous work is that the rule sequencesare extracted from FDTs, and no parser is needed.5 Experiments5.1 Data and MetricExperiments are carried out on English-to-Japanese(E-J) and Chinese-to-English (C-E) MT tasks.For E-J task, bilingual data used contains 13.3Msentence pairs after pre-processing.
The Japaneseside of bilingual data is used to train a 4-gram LM.The development set (dev) which contains 2,000sentences is used to optimize the log-linear SMTmodel.
Two test sets are used for evaluation, whichcontain 5,000 sentences (test-1) and 999 sentences450(test-2) respectively.
In all evaluation data sets, eachsource sentence has only one reference translation.For C-E task, bilingual data used contains 0.5Msentence pairs with high translation quality, includ-ing LDC2003E07, LDC2003E14, LDC2005T06,LDC2005T10, LDC2005E83, LDC2006E26, LD-C2006E34, LDC2006E85 and LDC2006E92.
A 5-gram LM is trained on the Xinhua portion of LDCEnglish Gigaword Version 3.0.
NIST 2004 (MT04)data set is used as dev set, and evaluation resultsare measured on NIST 2005 (MT05) and NIST 2008(MT08) data sets.
In all evaluation data sets, eachsource sentence has four reference translations.Default word alignments for both SMT tasks areperformed by GIZA++ with the intersect-diag-growrefinement.
Translation quality is measured in termsof case-insensitive BLEU (Papineni et al2002) andreported in percentage numbers.5.2 Baseline SystemThe phrase-based SMT system proposed by Xionget al2006) is used as the baseline system, with aMaxEnt principle-based lexicalized reordering mod-el integrated, which is used to handle reorderings indecoding.
The maximum lengths for the source andtarget phrases are 5 and 7 on E-J task, and 3 and 5on C-E task.
The beam size is set to 20.5.3 Translation Quality on E-J TaskWe first evaluate the effectiveness of our FDT-basedmodel training approach on E-J translation task, andpresent evaluation results in Table 1, in which BTGdenotes the performance of the baseline system.FDT-TM denotes the improved system that usesFDT-TM proposed in Section 3.2 instead of originalphrase table.
As described in Section 3.2, we trieddifferent sizes of n-best FDTs to induce alignmentsfor phrase extraction and found the optimal choiceis 5.
Besides, in order to make full use of the train-ing corpus, for those sentence pairs that are failed inFD, we just use their original word alignments to ex-tract bilingual phrases.
We can see from Table 1 thatFDT-TM outperforms the BTG system significantly.FDT-DM denotes the improved system that us-es FDT-DM proposed in Section 3.3 instead of o-riginal distortion model.
Comparing to baseline D-M which has length limitation on training instances,training examples of FDT-DM are extracted from 1-best FDTs without any restriction.
This makes ournew DM can cover both local and global reorderingsituations that might be met in decoding procedures.We can see from Table 1 that using FDT-DM, sig-nificant improvements can be achieved.FDT-SLM denotes the improved system that usesFDT-SLM proposed in Section 3.4 as an addition-al feature, in which the maximum n-gram order is4.
However, from Table 1 we notice that with FDT-SLM integrated, only 0.2 BLEU improvements canbe obtained.
We analyze decoding-logs and find thatthe reordered source sequences of n-best translation-s are very similar, which, we think, can explain whyimprovements of using this model are so limited.FDT-RSM denotes the improved system that us-es FDT-RSM proposed in Section 3.5 as an addi-tional feature.
The maximum order of this modelis 3.
From Table 1 we can see that FDT-RSM out-performs BTG significantly, with up to 0.48 BLEUimprovements.
Comparing to FDT-SLM, FDT-RSMperforms slightly better as well.
We think it is dueto the fact that bilingual phrases can provide morediscriminative power than monolingual n-grams do.Last, all these four FDT-based models (FDT-TM,FDT-DM, FDT-SLM and FDT-RSM) are put togeth-er to form an improved system that is denoted asFDT-ALL.
It can provide an averaged 1.2 BLEU im-provements on these three evaluation data sets.BLEU dev test-1 test-2BTG 20.60 20.27 13.15FDT-TM 21.21 20.71(+0.44) 13.98(+0.83)FDT-DM 21.13 20.79(+0.52) 14.25(+1.10)FDT-SLM 20.84 20.50(+0.23) 13.36(+0.21)FDT-RSM 21.07 20.75(+0.48) 13.59(+0.44)FDT-ALL 21.83 21.34(+1.07) 14.46(+1.31)PRO 21.89 21.81 14.69Table 1: FDT-based model training on E-J task.Pre-reordering (PRO) is often used on languagepairs, e.g.
English and Japanese, with very differentword orders.
So we compare our method with PROas well.
We re-implement the PROmethod proposedby Genzel (2010) and show its results in Table 1.
Ondev and test-2, FDT-ALL performs comparable toPRO, with no syntactic information needed at all.4515.4 Translation Quality on C-E TaskWe then evaluate the effectiveness of our FDT-basedmodel training approach on C-E translation task, andpresent evaluation results in Table 2, from which wecan see significant improvements as well.BLEU MT03 MT05 MT08BTG 38.73 38.01 23.78FDT-TM 39.14 38.31(+0.30) 24.30(+0.52)FDT-DM 39.27 38.56(+0.55) 24.50(+0.72)FDT-SLM 38.97 38.22(+0.21) 24.04(+0.26)FDT-RSM 39.06 38.33(+0.32) 24.13(+0.35)FDT-ALL 39.59 38.72(+0.71) 24.67(+0.89)Table 2: FDT-based model training on C-E taskComparing to numbers in Table 1, the gains com-ing from the first two FDT-based models become s-mall on C-E task.
This might be due to the fact thatthe word alignment quality in C-E task is more reli-able than that in E-J task for TM and DM training.5.5 Effect on Alignment QualityWe compare the qualities of alignments predicted byGIZA++ and alignments induced from 1-best FDTs.For E-J task, 575 English-Japanese sentence pairsare manually annotated with word alignments.
382sentence pairs are used as the dev set, and the other193 sentence pairs are used as the test set.
For C-Etask, 491 Chinese-English sentence pairs are manu-ally annotated with word alignments.
250 sentencepairs are used as the dev set, and the other 241 sen-tence pairs are used as the test set.
Both Japaneseand Chinese sentences are adapted to our own wordsegmentation standards respectively.
Table 3 showsthe comparison results.
Comparing to C-E languagepair (S-V-O), E-J language pair (S-O-V) has muchlower F-scores, due to its very different word order.F-score from GIZA++ from 1-best FDTsdevEJ 54.75% 57.93%(+3.18%)testEJ 55.32% 58.47%(+3.15%)devCE 81.32% 83.37%(+2.05%)testCE 80.61% 82.51%(+1.90%)Table 3: Comparison of alignment qualities predicted byGIZA++ and induced from 1-best FDTs.From Table 3 we can see that the F-score im-proves on all language pairs when using alignmentsinduced from 1-best FDTs, rather than GIZA++.5.6 Effect on Classification AccuracyIn the BTG system, the MaxEnt model is used as abinary classifier to predict reordering operations ofneighbor translation blocks.
As the baseline DM andour FDT-DM have different mechanisms on traininginstance extraction procedures, we compare the clas-sification accuracies of these two DMs in Table 4 toshow the effect of different training instances.
TheMaxEnt toolkit (Zhang, 2004) is used to optimizefeature weights using the l-BFGS method (Byrd etal., 1995).
We set the iteration number to 200 andGaussian prior to 1 for avoiding overfitting.
Table4 shows that when using training instances extract-ed from FDTs, classification accuracy of reorderingsimproves on both E-J and C-E tasks.
This is becauseFDTs can provide more deterministic and structuredknowledge for training instance extraction, whichcan cover both local and global reordering cases.baseline DM FDT-based DME-J 93.67% 95.60%(+1.93%)C-E 95.85% 97.52%(+1.67%)Table 4: Comparison of classification accuracies of DMsbased on instances extracted by different mechanisms.6 ConclusionsIn this paper, we have presented an FDT-based mod-el training approach to SMT.
As the first step in thisresearch direction, we have verified our method on aphrase-based SMT system, and proposed four FDT-based component models.
Experiments on both E-Jand C-E tasks have demonstrated the effectivenessof our approach.
Summing up, comparing to plainword alignments, FDTs provide richer structuredknowledge for more accurate SMT model training.Several potential research topics can be explored infuture.
For example, FDTs can be used in a pre-reordering framework.
This is feasible in the sensethat FDTs can provide both tree-like structures andreordering information.
We also plan to adapt ourFDT-based model training approach to SCFG-basedand traditional left-to-right phrase-based systems.452ReferencesPeter Brown, Stephen Pietra, Vincent Pietra, and RobertMercer.
1993.
The Mathematics of Statistical Ma-chine Translation: Parameter Estimation, Computa-tional Linguistics.Rafael Banchs, Josep Crego, Adri?a Gispert, Patrik Lam-bert, and Jos Mario.
2005.
Statistical Machine Trans-lation of Euparl Data by using Bilingual N-grams, InProceedings of the ACL Workshop on Building andUsing Parallel Texts.Alexandra Birch, Phil Blunsom, and Miles Osborne.2010.
Metrics for MT evaluation: Evaluating reorder-ing, Machine Translation.Alexandra Birch and Miles Osborne.
2011.
Reorderingmetrics for MT, In Proceedings of the Association forComputational Linguistics.Richard Byrd, Peihuang Lu, Jorge Nocedal, and CiyouZhu.
1995.
A limited memory algorithm for boundconstrained optimization, SIAM Journal of Scienceand Statistical Computing.Eugene Charniak, Kevin Knight, and Kenji Yamada.2003.
Syntax-based Language Models for StatisticalMachine Translation, MT Summit.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation, In Proceedings of the Association forComputational Linguistics.John DeNero and Jakob Uszkoreit.
2011.
Inducing Sen-tence Structure from Parallel Corpora for Reordering,In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing.Minwei Feng, Arne Mauser, and Hermann Ney.
2010.
ASource-side Decoding Sequence Model for StatisticalMachine Translation, In Proceedings of the Confer-ence of the Association for Machine Translation.Alexander Fraser and Daniel Marcu.
2006.
Semi-Supervised Training for Statistical Word Alignment, InProceedings of the International Conference on Com-putational Linguistics and Annual Meeting of the As-sociation for Computational Linguistics.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion, In Proceedings of the Conference on Computa-tional Linguistics.Liang Huang and David Chiang.
2005.
Better k-bestParsing, In Proceedings of International Conferenceon Parsing Technologies,.Young-Suk Lee, Bing Zhao, and Xiaoqiang Luo.2010.
Constituent Reordering and Syntax Models forEnglish-to-Japanese Statistical Machine Translation,In Proceedings of the Conference on ComputationalLinguistics.Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu.
2009.Weighted Alignment Matrices for Statistical Ma-chineTranslation, In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.Yang Liu, Qun Liu, and Shouxun Lin.
2010.
Discrimi-native Word Alignment by Linear Modeling, Compu-tational Linguistics.Robert Moore.
2005.
A Discriminative Framework forBilingual Word Alignment, In Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing.Robert Moore, Wen-tau Yih, and Andreas Bode.
2006.Improved Discriminative Bilingual Word Alignmen-t, In Proceedings of the International Conference onComputational Linguistics and Annual Meeting of theAssociation for Computational Linguistics.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based Translation, In Proceedings of the Associationfor Computational Linguistics.Galley Michel and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase ReorderingModel, In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Franz Och.
2003.
Minimum Error Rate Training in S-tatistical Machine Translation, In Proceedings of theAssociation for Computational Linguistics.Franz Och and Hermann Ney.
2004.
The AlignmentTemplate Approach to Statistical Machine Translation,Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation, In Proceedings of theAssociation for Computational Linguistics.Chris Quirk and Arul Menezes.
2006.
Do we need phras-es?
Challenging the conventional wisdom in Statisti-cal Machine Translation, In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.Lane Schwartz.
2008.
Multi-Source Translation Method-s, In Proceedings of the Conference of the Associationfor Machine Translation.Wade Shen, Brian Delaney, Tim Anderson, and Ray Sly-h. 2008.
The MIT-LL/AFRL IWSLT-2008 MT System,International Workshop on Spoken Language Transla-tion.David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-son Katz-Brown, Masakazu Seno, and Franz Och.2011.
A lightweight evaluation framework for ma-chine translation reordering, In Proceedings of theSixth Workshop on Statistical Machine Translation.Ashish Vaswani, Haitao Mi, Liang Huang, and DavidChiang.
2011.
Rule Markov Models for Fast Tree-to-String Translation, In Proceedings of the Associationfor Computational Linguistics.453Ashish Venugopal, Andreas Zollmann, Noah Smith, andStephan Vogel.
2008.
Wider Pipelines: N-best Align-ments and Parses in MT Training, In Proceedings ofthe Conference of the Association for Machine Trans-lation.Joern Wuebker, Arne Mauser, and Hermann Ney.
2010.Training Phrase Translation Models with Leaving-One-Out, In Proceedings of the Association for Com-putational Linguistics.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora,Computational Linguistics.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum entropy based phrase reordering model for s-tatistical machine translation, In Proceedings of theAssociation for Computational Linguistics.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a Dependency Parser to ImproveSMT for Subject-Object-Verb Languages, In Proceed-ings of the North American Chapter of the Associationfor Computational Linguistics.Richard Zens and Hermann Ney.
2006.
DiscriminativeReordering Models for Statistical Machine Transla-tion, In Proceedings of the Workshop on StatisticalMachine Translation.Le Zhang.
2004.
Maximum Entropy Modeling Toolkitfor Python and C++.454
