EXPANDING THE SCOPE OF THE ATIS TASK: THE ATIS-3 CORPUSDeborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, ChristinePao, Alexander Rudnicky, and Elizabeth ShribergContact: Deborah DahlUnisys CorporationP.O.
Box 517Paoli, PA 19301email:dahl @vfl.paramax.comABSTRACTThe Air Travel Information System (ATIS) domain serves as thecommon evaluation task for ARPA"spoken language systemdevelopers.
1 To support this task, the Multi-Site ATIS Data COl-lection Working group (MADCOW) coordinates data collectionactivities.
This paper describes recent MADCOW activities.
Inparticular, this paper describes the migration of the ATIS task to aricher elational database and development corpus (ATIS-3) anddescribes the ATIS-3 corpus.
The expanded atabase, whichincludes information on 46 US and Canadian cities and 23,457flights, was released inthe fall of 1992, and data collection for theATIS-3 corpus began shortly thereafter.
The ATIS-3 corpus nowconsists of a total of 8297 released training utterances and 3211utterances reserved for testing, collected at BBN, CMU, MIT,NIST and SRI.
2906 of the training utterances have been anno-tated with the correct information from the database.
This paperdescribes the ATIS-3 corpus in detail, including breakdowns ofdata by type (e.g.
context-independent, context-dependent, andunevaluable)and variations in the data collected at different sites.This paper also includes a description of the ATIS-3 database.Finally, we discuss future data collection and evaluation plans.1.
BACKGROUNDThe ATIS task was first used as a common ARPA spokenlanguage valuation task in 1990 \[1,2\].
In the ATIS task,subjects obtain air travel information such as flight sched-ules, fares, and ground transportation from a relationaldatabase using spoken atural language, and use it to solveair travel planning scenarios.
Although the core air travelplanning task has remained the same since the beginning,its use in evaluation has gradually evolved over the yearswith the general objectives of increasing tile match between1.
This paper was prepared under the auspices of theMulti-Site ATIS Data Collection Working group(MADCOW).
In addition to the authors, many otherpeople, listed under the Acknowledgments section,made important contributions tothis work.the evaluation and a real task as well as increasing theaccuracy of the metric.The first official evaluation took place in February of 1991,following a dry run in June of 1990.
In the 1991 evaluation,context-independent (Class A) queries as well as dialogpairs (D1) were evaluated.
The score for a system was theweighted-error metric which included a penalty for incor-rect answers as opposed to "No Answer".
Further efine-ments took place in the November 1992 evaluation, whereClass D (utterances with context dependencies throughoutthe dialog) queries were evaluated.
Another variation intro-duced in 1992 was the rain-max criterion, in which theinformation provided by systems in the answer wasrequired to fall between a minimum and a maximumamount.
In the most recent evaluation, December 1993, themain change has been to drop the weighted error metricand report results based on the unweighted error, or 100-%T.The 1993 ATIS spoken language understanding evaluationis the first evaluation based on the ATIS-3 corpus (\[5\]).
TheATIS-3 corpus will also supply test data for tile December1994 ATIS evaluation.
In addition, test data has also beenreserved for a dry run of a semantic evaluation \[6\].2.
THE EXPANDED ATIS RELATIONAL DATABASEThe initial ATIS task was based on a relational databasecontaining air travel information for 11 cities.
Three cor-pora of spontaneous spoken language utterances (ATIS-0,ATIS-1 and ATIS-2) were collected with this databaseusing a variety of paradigms, as described in \[3,4\].
AsATIS technology developed, it was felt that the initial ATIStask was unrealistically imited because of the small size oftile database.
Consequently, the database was expanded toinclude air travel information for 46 cities.
The expandeddatabase was released in tile fall of 1992, and data collec-tion began shortly thereafter.The new database isbased on air travel data obtained fromtile Official Airline Guide (OAG) in June 1992 and currentat that time.
The database includes information for 46 cities43and 52 airports in the US and Canada.
The largest able inthe exp;mded atabase, the flight table, includes informa-tion on 23,457 flights.
This compares to 11 cities, 9 air-ports, and 765 flights in the earlier ATIS databases andclearly represents a significant scaling up of the ATIS task.Despite the fact that the number of flights in the databasehas been increased by over a factor of thirty, the conversionto the larger database has not caused any serious difficultiesfor the :sites doing data collection, the sites doing evalua-tion, or the annotators.This result is encouraging, since itindicates that the SLS technology developed on a smalldatabase can scale up to a significantly bigger task.Cities mid airports included in the new database are listed inthe Appendix.3.
ATIS-3 DATA COLLECTION AND ANNOTATIONThe AT1S-3 data was collected at BBN, CMU, MIT, NIST,and SRI.
NIST participated in ATIS data collection for thefirst time in this round of data collection, using data collec-tion software from both BBN and SRI.Since the beginning of the ATIS task data collection para-digms have moved toward increasingly automatedapproaches.
The original ATIS-0 corpus was collectedusing human wizards to both transcribe the subjects' utter-ances as well as to interpret them (the so-called "wizard ofOZ" paradigm).
In the ATIS-3 corpus, nearly all transcrip-tion and interpretation f the subjects' speech was done bythe sites' ATIS systems themselves.
The only exceptionwas MIT, which collected ata using a transcription wizardinstead of a speech recognizer, while using MIT's naturallanguage system to interpret the utterances.
Automatic datacollection has the advantage of reduced cost.
In addition,the data is more realistic in the sense that it is obtained fromsubjects who are really talking to a computer.
The disad-vantage of automatic data collection is that imperfect pro-cessing by the spoken language system sometimes leads tothe presence of artifacts in data collection, such as utter-ances repeated over and over again.The general process of data collection and annotation asdescribed in \[3\] has not changed in the ATIS-3 data collec-tion effort.
We summarize this process here for conve-nience.Collected ata is transcribed at the collecting site and sentto NIST, where it is logged and potential test data is heldout.
The data is then released to sites participating in theATIS evaluations as initial, i.e.
unannotated, ata, and issimultaneously sent to SRI for annotation.
During annota-tion, the data is classified into three categories:?
Class A: not dependent on context for interpretation?
Class D: dependent on context for interpretation?
Class X: unevaluableThe Principles of Interpretation document is used to cate-gorize utterances in these three classes and also specificshow to interpret vague expressions which occur in utter-ances in Class A and D.Annotated ata is returned from SRI to NIST and releasedby NIST.
A full set of data for a subject session includes thefollowing files:?
.wav: speech waveform?
.log: session log?
.sro: detailed transcription?
.cat: categorization f query (A, D, X)Table 1: Total ATIS-3 DataTraining Pool (including 1993Test Data)Site #Spkr # Sess #UttsBBN 14 55 1101CMU 15 177 1462IMIT 30 146 954NIST 49 253 2510SRI 30 141 2326Total 125 , 693 8297Test#Spkr #Sess #Utts9 37 3898 70 \] 387ii25 120 41822 179 2016 27 4181813SemEval Dry Run#Spkr #Sess #Utts12 67 50012 67 500Total#Spkr #Sess #Utts23 92 149023 247 184955 266 137271 432 321136 168 27441066644?
.ref: minimal reference answer?
.rf2: maximal reference answer3.1.
The ATIS-3 Corpus3.2.
Initial DataThe total data collected for the ATIS-3 corpus consists of12,047 utterances, of which 3,876 have been annotated.The data breaks down across sites as shown in Table 1.Approximately 20% of submitted ata is allocated to thepool from which evaluation test data is drawn.
In addition,500 utterances from the NIST data collection activity havebeen reserved as test data for a possible dry run for seman-tic evaluation i  ATIS \[6\].
This table does not include 1440additional utterances collected at CMU which have not yetbeen released as initial data.
Two subject-scnearios fromthe ATIS-3 corpus can be seen in Appendix 2.
Note in par-ticular the false starts typical of spontaneous speech.3.3.
Annotated DataSlightly over 1/3, or 36%, of the released initial data hasbeen annotated with the correct answers.
Unannotated dataincludes data reserved for the December 1994 evaluation,which will be annotated just before the evaluation to insurethat it is consistent with the Principles of Interpretation cur-rent at that time.
Other unannotated data includes data fromNIST and SRI which was received too late to be annotated.The full corpus of annotated ata also includes 667 sen-tences collected for the November 1992 logfile experiments\[4\].
Although these utterances were collected using the 11city database, they were annotated using the expanded data-base.
The rationale for this decision was that the annotatorswished to get experience with the expanded atabase, andat the time, the logfile data was the only data available.The annotated data breaks down into Classes A, D, and Xby site as shown in Table 2.If the annotated data is broken down by site as well as byclass, it can be noted that there is a wide range of variationacross ites in tile relative proportion of A, D, and X que-ries, which can be seen in Figure 1.
We believe this islargely attributable tothe effects of different data collectionscenarios used by the different sites.
The practical conse-quences of this effect are that an understanding of how sce-narios achieve this effect might lead to the development oftechniques for improving system performance for particu-lar applications.4.
ATIS PLANS4.1.
Development Test DataDespite the fact that nearly 25,000 ATIS utterances havebeen collected since 1990, no standard evelopmeut testdata exists for ATIS.
Sites have individually constructeddevelopment test sets from the training data and evaluationtest sets, but this method makes inter-site comparisons dif-ficult.
While inter-site comparisons are a major goal of theofficial evaluation, variations in the test set from year toyear make comparisons across years problematic.
In addi-tion, if evaluation test data is used after the evaluation asdevelopment test data, it is contaminated by system devel-opers looking at it in detail for the purposes of adjudica-tion.
The existence of a development test corpus will alsoextend the usefulness of the ATIS-3 corpus after all trainingand evaluation test data is released by providing asource ofunseen data.
For these reasons MADCOW has decided tocollect an additional 300-500 utterances from BBN, CMU,MIT, NIST, and SRI, to be designated evelopment testdata.
This data is to be collected in the spring of 1994 andwill have a high priority for early annotation.4.2.
Alternative EvaluationsMADCOW is also interested in exploring evaluation para-digms other than file standard CAS evaluation.
Theseinclude the end-to-end/logfile approach described in \[4\], aswell as the semantic evaluation paradigm described in \[6\].Table 1: Number of A, D, and X utterances in ATIS-3 dataTraining Data  December  93 Evaluat ion Test Data TotalSite Class A Class D Class X Total Class A \] Class D Class X TotalBBNCMUMITSRINIST'lbtal28241739132901419182 193103 1972393519691211065236577177517862911SiteBBNCMUMITSRINIST458911382808444857 5350 3650 3286 34!82 82325 192199 856199 916915 1642002039659862033876End to End: In 1992 MADCOW defined and carried out adry run evaluation of approaches in which a human judgerules on the correctness orappropriateness of each systemresponse and, in which task-level metrics, such as time-to-complete task and correctness of solution are measured \[4\].On the basis of an analysis of the experiment discussed in\[4\] performed by Alex Rudnicky, we have determined thatin onler to obtain statistically reliable results it will be nec-essary to reduce xtraneous sources of variation as much aspossible; consequently, a within-subjects design is highlydesirable.\] Although we have not continued to activelydevelop this approach, we believe that it may be useful inthe fnture as we move to increasingly realistic tasks.Semantic Evaluation: The goal of semantic evaluation isto define a level of representation which focuses pecifi-cally on language understanding, as opposed to task perfor-mance, in a maximally task-independent way.
Thisapproach as the advantage of minimizing the number ofextraneous tasks required of system developers participat-ing in evaluations.
In addition, it is anticipated that much ofthe work done in developing the semantic evaluation willcarry over to new tasks.Aside from the specific representation used, which is dis-cussed in detail for ATIS in \[6\], the infrastructure for carry-ing out a semantic evaluation is remarkably parallel to thatrequired by the current CAS evaluations.
That is, data needsto be collected, annotated according to a set of well-definedrules, and distributed to sites.
In addition ancillary softwareis required for scoring and to assist in annotation.1.
We would like to acknowledge David Pisoni of Indi-ana University and Astrid Schmidt-Nielsen ofthe NavalResearch Lab for their helpful comments on the end-to-end evaluation procedure.4.3.
Beyond AT Is -3MADCOW has also begun to explore follow-on tasks toATIS to be implemented for the 1995 evaluation cycle.Although the details of future tasks remain to be specified,telephone tasks are of high interest, since they stimulateboth research on telephone speech as well as interactive dia-log.
In addition, telephone tasks are useful because the sub-jects do not have to be physically located near the datacollecting system, thus making it possible for subjects indifferent geographic areas to interact with a range of datacollection systems imply by using another telephone num-ber.ACKNOWLEDGEMENTSThe MADCOW committee would like to acknowledge thecontributions of the following people to the shared ata col-lection efforts.
At AT&T, Enrico Bocchieri and BruceBuntschuh, At BBN, Beverly Schwartz, Sandra Peters, andRobert Ingria, at CMU, Robert Weide, Yuzong Chang, andEric Thayer, at MIT, Lynette Hirschman and Joe Polifroni,at NIST, John Garofolo, Jon Fiscus, and Bruce Lund, at SRIGoh Kawai and Tom Kuhn (annotators) and at Unisys, LewNorton.REFERENCES1.
Price, E "Evaluation of spoken language systems: the ATISdomain".
In Proceedings of the speech and natural languageworkshop.
Morgan-Kaufmann, 1990.2.
Hemphill, C. T., J. J. Godfrey, and G. R. Doddington.
"TheATIS spoken language systems pilot corpus".
In Proceed-ings of the speech and natural language workshop.
MorganKaufmann, 1990.Figure 1: Class A, D, and X in Training Data Figure 2: Class A, D, and X in Test DataUtterances0.80.60.4.0.2Class A, D, and X in Training DataBBN CMt MIT SRI NIST TotalCollecting SiteIK I  , Class A\[\]i Class D;DClass XClass A, D, and X in Test Datarancesmi i  i ,-1i" .
.
.
.
_. .
.
.
.
.
.
.
.  "
. '
.
.
.
.  "
. "
?
.BBN CMU MIT SRI NIST TotalCollecting Site\[\]Class A\[\]Class D\[\]Class X463.
MADCOW.
"Multi-site data collection for a spoken lan-guage corpus" Proceedings of the fifth DARPA speech andnatural language workshop.
Morgan Kaufmann, 1992.4.
Hirschman, L. M. Bates, D. Dahl, W. Fisher, D. Pallett, KateHunicke-Smith.
P Price, A. Rudnicky, and E. Tzoukerman-n."Multi-site data collection and evaluation i spoken lan-guage understanding".
Proceedings of the HumanLanguage Teclmology Workshop, March, 1993.5.
Pallett, David, Jonathan Fiscus, William Fisher, John Garo-folo, Bruce Lund, Mark Pryzbocki, "1993 B enclmaark Testsfor the ARPA Spoken Language Program" (this volume).6.
Moore, R. "Semantic Evaluation for Spoken Language Sys-tems" (this volume).APPENDIX 1Cities and airports included in the expanded ATIS rela-tional database:Table 1: CitiesNashville, TN Boston, MA Burbank, CABaltimore, MD Chicago, IL Cleveland, OHCharlotte, NC Cohinlbus, OH Cincinnati, OHDenver, CO Dallas, 'IX Detroit, MIFort Worth, TX Houston, "IX WestchesterCounty, NYIndianapolis, IN Newark, NJ Las Vegas, NVLos Angeles, CA Long Beach, CA Atlanta, GAMemphis, TN Miami, FL Kansas City, MOMilwaukee, WI Minneapolis, MN New York, NYOakland, CA Ontario, CA Orlando, FLPlfiladelphia, PA Phoenix, AZ Pittsburgh, PASt.
Paul, MN San Diego, CA Seattle, WASan Francisco, CA San Jose, CA Salt Lake City, UTSt.
Louis, MO St. Petersburg, FL Tacoma, WATampa.
FL Washington, DC Montreal, PQToronto, ONTable 2: AirportsWilliam B. Hartsfield Atlanta Intl., Atlanta, GeorgiaNashville International, Nashville, TennesseeLogan International, Boston, MassachusettsBurbank, Burbank, CaliforniaBaltimore/Washingtou International, Baltimore, MarylandHopkins International, Cleveland, OhioCharlotte/Douglas International, Charlotte, Norlh CarolinaPort Columbus International, Columbus, OhioCincinnati/Northern Ke tucky Intl., Cincinnati, OhioLove Field, Dallas/Ft.
Worth, TexasWashington National, Washington, D.C.Stapleton I ternational, Denver, ColoradoDetroit City, Detroit, MichiganDallas/Fort Worth International, Dallas/Ft.
Worth, TexasMetropolitan Wayne County, Detroit, MichiganNewark International, Newark, New JerseyHobby, Houston, TexasWestchester County, Westchester County, New YorkDulles International, Washington, D.C.Houston I tercontinental, Houston, TexasIndianapolis International, Indianapolis, IndianaJohn E Kennedy International.
New YorkMccarran I ternational, LasVegas, NevadaLos Angeles International, Los Angeles, CaliforniaLa Guardia, New York NYLong Beach Municipal, Long Beach, CaliforniaKansas City International, Kansas City, MissouriOrlando International, Orlando, FloridaMidway, Chicago, IllinoisMemphis International, Memplfis, TennesseeMiami International, Miami, FloridaGeneral Mitchell International, Milwaukee, WisconsinMinneapolis/St.
Paul International, Minneapolis/St.
Paul, MnMetropolitan Oakland International, Oakland, California47Table 2: AirportsOntario International, Ontario, CaliforniaO'Hare International, Chicago, IllinoisPhiladelphia International, Philadelphia PASky Harbor International, Phoenix, ArizonaSt.
Petersburg/Clearwater In national, Tampa/St.
Petersburg, FloridaGreater Pittsburgh International, Pittsburgh, PennsylvaniaLindbergh Field/San Diego International, San Diego, CaliforniaSeattle/Tacoma International, Seattle/Tacoma, WashingtonSan Francisco International, San Francisco, CaliforniaSan Jose International, San Jose, CaliforniaSalt Lake City International, Salt Lake City, UtahLambert St. Louis International, St. Louis, MissouriTampa International, Tampa/St.
Petersburg, FloridaButtonville, Toronto, OntarioMirabel, Montreal, QuebecToronto Island, Toronto, OntarioDorval International, Montreal.
QuebecLester B. Pearson International, TorontoxO7036sx: i would like an afternoon flight from denver col-orado to dallas texasxO7046sx: what type of ground transportation from tile air-port to dallasxO7056sx: i want a evening flight from dallas to milwaukeexO7066sx: what type of ground transportation from the air-port to milwaukeeExample 2:Scenario:Determine the type of aircraft used on a flight J?om Cleve-land to Dallas that leaves before noon.xO2Ollsx: may i see all file flights from cleveland to, &dlasxO2021sx.sro: can you show me the flights that leave beforenoon, onlyxO2031sx.sro: could you sh- please show me tile types ofaircraft used on these flightsAPPENDIX 24.4.
Sample Subject-Scenarios from the ATIS-3 Corpus(Data collected at NIST using SRI data collection sys-tem)Example 1:ScenarioYou have only three days for job hunting, and you havearranged job interviews in two different cities, t (The inter-view times will depend on your flight schedule.)
Start fromMilwaukee and plan the flight and ground transportationitinerary to City-A and City-B, and back toMilwaukee.xO7016sx: i would like a morning flight from milwaukee toden- to denver colorado please with ground transportaUonxO7026sx: i would like a morning flight from milwaukee todenver colorado pleasexO7036sx: what type of ground transportation from the air-port to denver48
