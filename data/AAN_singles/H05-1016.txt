Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 121?128, Vancouver, October 2005. c?2005 Association for Computational LinguisticsUsing Names and Topics for New Event DetectionGiridhar Kumaran and James AllanCenter for Intelligent Information RetrievalDepartment of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA 01003, USA{giridhar,allan}@cs.umass.eduAbstractNew Event Detection (NED) involvesmonitoring chronologically-ordered newsstreams to automatically detect the storiesthat report on new events.
We comparetwo stories by finding three cosine simi-larities based on names, topics and the fulltext.
These additional comparisons sug-gest treating the NED problem as a bi-nary classification problem with the com-parison scores serving as features.
Theclassifier models we learned show statis-tically significant improvement over thebaseline vector space model system on allthe collections we tested, including the lat-est TDT5 collection.The presence of automatic speech recog-nizer (ASR) output of broadcast news innews streams can reduce performance andrender our named entity recognition basedapproaches ineffective.
We provide a so-lution to this problem achieving statisti-cally significant improvements.1 IntroductionThe instant and automatic detection of new eventsis very useful in situations where novel informa-tion needs to be detected from a real-time streamof rapidly growing data.
These real-life situationsoccur in scenarios like financial markets, news anal-yses, and intelligence gathering.
In this paper wefocus on creating a system to immediately identifystories reporting new events in a stream of news- a daunting task for a human analyst given theenormous volume of data coming in from varioussources.The Topic Detection and Tracking (TDT) pro-gram, a DARPA funded initiative, seeks to developtechnologies that search, organize and structure mul-tilingual news-oriented textual materials from a va-riety of broadcast news media.
One of the tasks inthis program, New Event Detection (NED), involvesconstant monitoring of streams of news stories toidentify the first story reporting topics of interest.A topic is defined as ?a seminal event or activity,along with directly related events and activities?
(Al-lan, 2002).
An earthquake at a particular place isan example of a topic.
The first story on this topicis the story that first carries the report on the earth-quake?s occurrence.
The other stories that make upthe topic are those discussing the death toll, the res-cue efforts, the reactions from different parts of theworld, scientific discussions, the commercial impactand so on.
A good NED system would be one thatcorrectly identifies the article that reports the earth-quake?s occurrence as the first story.NED is a hard problem.
For example, to dis-tinguish stories about earthquakes in two differentplaces, a vector space model system would rely on atf-idf weighting scheme that will bring out the dif-ference by weighting the locations higher.
Moreoften then not, this doesn?t happen as the differ-ences are buried in the mass of terms in commonbetween stories describing earthquakes and their af-termath.
In this paper we reduce the dependence ontf-idf weighting by showing the utility of creating121three distinct representations of each story based onnamed entities.
This allows us to view NED as a bi-nary classification problem - i.e., each story has tobe classified into one of two categories - old or new,based on features extracted using the three differentrepresentations.The paper starts by summarizing the previouswork on NED in Section 2.
In Section 3, we explainthe rationale behind our intuition.
Section 4 de-scribes the experimental setup, data pre-processing,and our baseline NED system.
We then briefly de-scribe the evaluation methodology for NED in Sec-tion 5.
Model creation and the results of applyingthese models to test data are detailed in Section 6.In the same section, we describe the effect on perfor-mance if the manually transcribed version of broad-cast news is replaced with ASR output.
Since itshard to recognize named entities from ASR data,performance expectedly deteriorates.
We follow anovel approach to work around the problem result-ing in statistically significant improvement in per-formance.
The results are analyzed in Section 7.
Wewrap up with conclusions and future work in Sec-tion 8.2 Previous ResearchPrevious approaches to NED have concentrated ondeveloping similarity metrics or better documentrepresentations or both.
A summer workshop ontopic-based novelty detection held at Johns Hop-kins University extensively studied the NED prob-lem.
Similarity metrics, effect of named entities,pre-processing of data, and language and HiddenMarkov Models were explored (Allan et al, 1999).Combinations of NED systems were also discussed.In the context of this paper, selective re-weighting ofnamed entities didn?t bring about expected improve-ment.Improving NED by better comparison of storieswas the focus of following papers.
In an approachto solve on-line NED, when a new document wasencountered it was processed immediately to ex-tract features and build up a query representationof the document?s content (Papka and Allan, 1998).The document?s initial threshold was determined byevaluating it with the query.
If the document did nottrigger any previous query by exceeding this partic-ular threshold, it was marked as a new event.
Un-like the previous paper, good improvements on TDTbenchmarks were shown by extending a basic in-cremental TF-IDF model to include source-specificmodels, similarity score normalization techniques,and segmentation of documents (Brants et al, 2003).Other researchers have attempted to build betterdocument models.
A combination of evidence de-rived from two distinct representations of a docu-ment?s content was used to create a new representa-tion for each story (Stokes and Carthy, 2001).
Whileone of the representations was the usual free textvector, the other made use of lexical chains (createdusing WordNet) to obtain the most prevalent topicsdiscussed in the document.
The two vectors werecombined in a linear fashion and a marginal increasein effectiveness was observed.NED approaches that rely on exploiting existingnews tracking technology were proved to inevitablyexhibit poor performance (Allan et al, 2000).
Giventracking error rates, the lower and upper boundson NED error rates were derived mathematically.These values were found to be good approximationsof the true NED system error rates.
Since track-ing and filtering using full-text similarity compar-ison approaches were not likely to make the sortof improvements that are necessary for high-qualityNED results, the paper concluded that an alternateapproach to NED was required.
This led to a se-ries of research efforts that concentrated on buildingmulti-stage NED algorithms and new ways to com-bine evidence from different sources.In the topic-conditioned novelty detection ap-proach, documents were classified into broad top-ics and NED was performed within these categories(Yang et al, 2002).
Additionally, named entitieswere re-weighted relative to the normal words foreach topic, and a stop list was created for each topic.The experiments were done on a corpus differentfrom the TDT corpus and, apparently didn?t scalewell to the TDT setting.The DOREMI research group treated named enti-ties like people and locations preferentially and de-veloped a new similarity measure that utilized thesemantics classes they came up with (Makkonen etal., 2002).
They explored various definitions of theNED task and tested their system accordingly.
Morerecently, they utilized a perceptron to learn a weight122function on the similarities between different seman-tic classes to obtain a final confidence score for eachstory (Makkonen et al, 2004).The TDT group at UMass introduced multipledocument models for each news story and modifiedsimilarity metrics by splitting up stories into onlynamed entities and only terms other than named en-tities (Kumaran and Allan, 2004).
They observedthat certain categories of news were better tackledusing only named entities, while using only topicterms for the others helped.In approaches similar to named entity tagging,part-of-speech tagging (Farahat et al, 2003) has alsobeen successfully used to improve NED.Papers in the TDT2003 and TDT2004 work-shops validated the hypothesis that ensemble single-feature classifiers based on majority voting exhibitedbetter performance than single classifiers workingwith a number of features on the NED task (Braunand Kaneshiro, 2003; Braun and Kaneshiro, 2004).Examples of features they used are cosine similarity,text tiling output and temporally-weighted tf-idf.Probabilistic models for online clustering of doc-uments, with a mechanism for handling creation ofnew clusters have been developed.
Each cluster wasassumed to correspond to a topic.
Experimental re-sults did not show any improvement over baselinesystems (Zhang et al, 2005).3 Features for NEDPinning down the character of new stories is a toughprocess.
New events don?t follow any periodic cy-cle, can occur at any instant, can involve only oneparticular type of named entity (people, places, or-ganizations etc.)
or a combination, can be reportedin any language, and can be reported as a story ofany length by any source1 .
Apart from the source,date, and time of publication or broadcast of eachnews story, the TDT corpora do not contain anyother clues like placement in the webpage, the num-ber of sources reporting the same news and so on.Given all these factors, we decided that the best fea-1It could be argued that articles from a source, say NYTimes,are much longer than news stories from CNN, and hence thelength of stories is a good candidate for use as a feature.
How-ever, when there is no pattern that indicates that either of thetwo sources reports new stories preferentially, the use of lengthas a feature is moot.tures to use would be those that were not particularto the story in question only, but those that measuredifferences between the story and those it is com-pared with.Category-specific rules that modified the baselineconfidence score assigned to each story have beendeveloped (Kumaran and Allan, 2004).
The mod-ification was based on additional evidence in theform of overlap of named entities and topic terms(terms in the document not identified as named en-tities) with the closest story reported by a base-line system.
We decided to use these three scores:namely the baseline confidence score, named en-tity overlap, and topic-term overlap as features.
Thenamed entities considered were Event, GeopoliticalEntity, Language, Location, Nationality, Organiza-tion, Person, Cardinal, Ordinal, Date, and Time.These named entities were detected in stories usingBBN IdentiFinderTM(Bikel et al, 1999).
Irrespec-tive of their type, all named entities were pooled to-gether to form a single named entity vector.The intuition behind using these features is thatwe believe every event is characterized by a set ofpeople, places, organizations, etc.
(named entities),and a set of terms that describe the event.
Whilethe former can be described as the who, where, andwhen aspects of an event, the latter relates to thewhat aspect.
If two stories were on the same topic,they would share both named entities as well as topicterms.
If they were on different, but similar, topics,then either named entities or topic terms will matchbut not both.We illustrate the above intuition with examples.Terms in bold face are named entities common toboth stories, while those in italics are topic termsin common.
We start with an example showingthat for old stories both the named entities as wellas topic terms overlap with a story on the same topic.Story 1. : Story on a topic already reportedWhile in Croatia today, Pope John Paul II calledon the international community to help end thefighting in the Yugoslavia?s Kosovo province.Story 2. : Story on the same topicPope John Paul II is urging the internationalcommunity to quickly help the ethnic Albanians inKosovo.
He spoke in the coastal city of Split, wherehe ended a three-day visit to Croatia.123Story 1 is an old story about Pope John Paul II?svisit to Yugoslavia.
Story 2 was the first story on thetopic and it shares both named entities likes PopeJohn Paul II and Croatia and also topic terms likeinternational community and help.Our next example shows that for new stories,either the named entities or topic terms match withan earlier story.Story 3. : Topic not seen beforeTurkey has sent 10,000 troops to its southern borderwith Syria amid growing tensions between the twoneighbors, newspapers reported Thursday.
DefenseMinister Ismet Sezgin denied any troop movementalong the border, but said Turkey?s patience wasrunning out.
Turkey accuses Syria of harboringTurkish Kurdish rebels fighting for autonomy inTurkey?s southeast; it says rebel leader AbdullahOcalan lives in Damascus.Story 4. : Closest Story due to Named EntitiesA senior Turkish government official called Mon-day for closer military cooperation with neighboringBulgaria.
After talks with President Petar Stoyanovat the end of his four-day visit, Turkish Deputy Pre-mier and National Defense Minister Ismet Sezginexpressed satisfaction with the progress of bilateralrelations and the hope that Bulgarian-Turkishmilitary cooperation will be promoted.Story 3 is a new story about the rising ten-sions between Turkey and Syria.
The closest storyas reported by a (baseline) basic vector space modelNED system using cosine similarity is Story 4,a story about Turkish-Bulgarian relations.
Thenamed entities Turkey and Ismet Sezgin causedthis match.
We see that none of the topic termsmatch.
However, the system reported with a highconfidence score that Story 3 is old.
This is becauseof the matching of high IDF-valued named entities.Determining that the topic terms didn?t match wouldhave helped the system avoid this mistake.4 Experimental Setup and BaselineWe used the TDT2, TDT3, TDT4, and TDT5 cor-pora for our experiments.
They contain a mix ofbroadcast news (bn) and newswire (nwt) stories.Only the English stories in the multi-lingual collec-tions were considered for the NED task.
The broad-cast news material is provided in the form of an au-dio sampled data signal, a manual transcription ofthe audio signal (bn-man), and a transcription cre-ated using an automatic speech recognizer (bn-asr).We used version 3.0 of the open source Lemursystem2 to tokenize the data, remove stop words,stem and create document vectors.
We used the 418stopwords included in the stop list used by InQuery(Callan et al, 1992), and the Krovetz-stemmer algo-rithm implementation provided as part of Lemur.Documents were represented as term vectors withincremental TF-IDF weighting (Brants et al, 2003;Yang et al, 1998).
We used the cosine similaritymetric to judge the similarity of a story S with thoseseen in the past.Sim(S, X) =?w weight(w, S) ?
weight(w, X)?
?w weight(w,S)2?
?w weight(w, X)2(1)whereweight(w, d) =tf ?
idftf =log(termfrequency + 1.0)idf = log((docCount+1)(documentfreq+0.5)The maximum similarity of the story S with storiesseen in the past was taken as the confidence scorethat S was old.
This constituted our baseline system.We extracted three features for each incomingstory S. The first was the confidence score reportedby the baseline system.
The second and third fea-tures were the cosine similarity between only thenamed entities in S and X and the cosine similaritybetween only the topic terms in S and X.
We traineda Support Vector Machine (SVM) (Burges, 1998)classifier on these features.
We chose to use SVMsas they are considered state-of-the-art for text clas-sification purposes (Mladeni et al, 2004), and pro-vide us with options to consider both linear and non-linear decision boundaries.
To develop SVM modelswe used SV MLight(Joachims, 1999), which is animplementation of SVMs in C. SV MLight is an im-plementation of Vapnik?s Support Vector Machine(Vapnik, 1995).For training, we used the TDT3 and TDT4 cor-pora.
There were 115 and 70 topics respectively giv-ing us a total of 185 positive examples (new stories)2http://www.lemurproject.org124and 7800 negative examples (old stories).
We bal-anced the number of positive and negative examplesby oversampling the minority class until there wereequal number of positive and negative training in-stances.
Testing was done on the TDT2 and TDT5corpora (96 and 126 topics resp.
).5 NED EvaluationThe official TDT evaluation requires a NED systemto assign a confidence score between 0 (new) and1 (old) to every story upon its arrival in the time-ordered news stream.
This assignment of scoresis done either immediately upon arrival or after afixed look-ahead window of stories.
To evaluate per-formance, the stories are sorted according to theirscores, and a threshold sweep is performed.
Allstories with scores above the threshold are declaredold, while those below it are considered new.
Ateach threshold value, the misses and false alarms areidentified, and a cost Cdet is calculated as follows.Cdet = Cmiss ?Pmiss ?Ptarget + CFA ?
PFA ?
Pnon?targetwhere CMiss and CFA are the costs of a Missand a False Alarm, respectively, PMiss and PFAare the conditional probabilities of a Miss and aFalse Alarm, respectively, and Ptarget and Pnon?targetare the a priori target probabilities (Pnon?target = 1 -Ptarget).The threshold that results in the least cost is se-lected as the optimum one.
Different NED systemsare compared based on their minimum cost.
In otherwords, the lower the Cdet score reported by a systemon test data, the better the system.6 ResultsOur first set of experiments were performed on dataconsisting of newswire text and manual transcrip-tion of broadcast news (nwt+bn-man).
We usedthe features mentioned in Section 3 to build SVMmodels in the classification mode.
We experimentedwith linear, polynomial, and RBF kernels.
The out-put from the SVM classifiers was normalized to fallwithin the range zero and one.We found that using certain kernels improved per-formance over the baseline system significantly.
Theresults for both corpora, TDT2 and TDT5, wereconsistently and significantly improved by using theTDT2 TDT5Kernel Type (nwt+bn-man) (nwt)Baseline System 0.585 0.701Linear Kernel 0.548 0.696Poly.
of deg.
1 0.548 0.696Poly.
of deg.
2 0.543 0.688Poly.
of deg.
3 0.545 0.684Poly.
of deg.
4 0.535 0.694Poly.
of deg.
5 0.533 0.688Poly.
of deg.
6 0.534 0.693RBF with ?
= 1 0.540 0.661RBF with ?
= 5 0.530 0.699Table 1: Summary of the results of using SVM classifier mod-els for NED on the TDT2 and TDT5 collections.
The numbersare the minimum cost (Cdet) values (lower is better).
The signtest, with ?
= 0.05, was performed to compare the baseline sys-tem with only a classifier using RBF kernels with ?
= 1.
Forboth collections, the improvements were found to be statisti-cally significant (shown in bold).
While there are better per-forming kernels for TDT2, we chose to perform significancetests for only one kernel to show that significant improvementover the baseline can be obtained using a single kernel acrossdifferent test collections.classification models.
The 2004 NED evaluationsconducted by the National Institute of Standards andTechnology was on the TDT5 collection.
The largesize of the collection and existence of a large num-ber of topics with a single story made the task verychallenging.
The best system fielded by the partici-pating sites was the baseline system used here.
Ta-ble 1 summarizes the results we obtained.All statistical significance testing was done usingthe sign test.
We counted the number of topics forwhich using the SVM classifier improved over thebaseline (in terms of detecting more previously un-detected new and old stories), and also the num-ber of topics for which using the SVM classifieractually converted originally correct decisions intowrong ones.
These were used as input for the signtest.
The test were used to check whether improve-ment in performance using the classifier-based sys-tem was spread across a significant number of top-ics, and not confined to a few.
Table 2 gives someexamples of topics and the associated improvementsin detecting them.125Topic ID Number of Num.
detected Num.
detected Improvementold stories by baseline system by SVM classifier (Higher the better)55105 420 407 403 -455010 21 21 20 -155023 5 5 4 -155089 226 226 225 -155125 120 114 120 655107 331 327 331 755106 808 787 795 855200 196 185 193 8Table 2: Examples of improvements due to using the SVM classifier on a per-topic basis.
Shown here are thefour topics each in which the greatest degradation and improvements in performance were seen.
The topicsvary in size.
The SVM classifier resulted in overall (statistically significant, refer Table 1) improvement asit corrected more errors than introduced them.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 105101520253035Confidence ScoreNumber of StoriesTDT5 ?
New ScoresBaseline New Story ScoresClassifier New Story ScoresFigure 1: Distribution of new story scores for thebaseline and SVM model systems.7 AnalysisThe main goal of our effort was to come up with away to correctly identify new stories based on fea-tures we thought characterized them.
To understandwhat we had actually achieved by using these mod-els, we studied the distribution of the confidencescores assigned to new and old stories for the base-line and a classifier-based NED system for the TDT5collection (Figures 1 and 2 respectively).We observe that the scores for a small fractionof new stories that were initially missed (betweenscores 0.8 and 1) are decreased by the model-basedNED system while a larger fraction (between scores0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1050010001500200025003000Confidence ScoreNumber of StoriesTDT5 ?
Old StoriesBaseline Old Story ScoresClassifier Old Story ScoresFigure 2: Distribution of old story scores for thebaseline and SVM model systems.0.1 and 0.4) is also decreased by a small amount.However, the major impact of using SVM model-based NED systems appears to be in detecting oldstories.
We observe that the scores of a significantnumber of old stories (between scores 0.2 and 0.55)have been increased to be closer to one.
This had theeffect of increasing the score difference between oldand new stories, and hence improved classificationaccuracy as measured by the minimum cost.We investigated the relative importance of thethree features by looking that the linear kernel SVMmodel.
While the original cosine similarity metricCS remained the prominent feature, the contribution126of the third feature non-NE-CS was slightly morethan if not equal to the contribution of named en-tities NE-CS (Table 3).
This explains why simplere-weighting of named entities alone (Allan et al,1999) doesn?t suffice to improved performance.Feature CS NE-CS non-NE-CSWeight 5.4 1.58 1.83Table 3: Weights assigned to features by the linearkernel SVM.If this method of harnessing named entities andtopic terms were indeed so effective, then we shouldhave been able to detect every old story in everytopic.
However, analysis reveals that this approachmakes an assumption about the way stories in atopic are related.
Not all topics are dense, with bothnamed entities and topic terms threading the storiestogether.
Examples of such topics are natural dis-aster topics.
While the first story might report onthe actual calamity and the region it affected, suc-cessive stories might report on individual survivortales.
These stories might be connected to the orig-inal story of the topic by as tenuous a link as onlythe name of the calamity, or the place.
Such topicstructures are very common in newswire.
Hence ourapproach will fail in such topics with loosely con-nected stories.
Much more advanced processing ofstory content is required in such cases.
Mistakesmade by the named entity recognizer also impedeperformance.Given that its impractical to expect manual tran-scriptions of all broadcast news, we tested our base-line and classifier systems on a version of TDT2with newswire stories and ASR output of the broad-cast news (nwt+bn-asr).
TDT5 was left out as itdoesn?t have any broadcast news.
As shown in Ta-ble 4, the baseline system performed significantlyworse when manual transcription was replaced withASR output.
The classifier systems did even worsethan the nwt +bn-asr baseline result.
An analysisof the named entities extracted revealed that the ac-curacy was very poor - worse than extraction frombn-man documents.
This was primarily because theversion of IdentiFinder (IdentiFinder-man) we usedwas by default trained on nwt.To alleviate this problem we re-trained Identi-Kernel Type TDT2 (nwt+bn-asr)Baseline System 0.640IdentiFinder-man IdentiFinder-asrLinear Kernel 0.653 0.608Poly.
of deg.
1 0.654 0.608Poly.
of deg.
2 0.658 0.619Poly.
of deg.
3 0.659 0.616Poly.
of deg.
4 0.671 0.632Poly.
of deg.
5 0.676 0.640Poly.
of deg.
6 0.682 0.652RBF with ?
= 1 0.649 0.636RBF with ?
= 5 0.668 0.679Table 4: The baseline system was the same used forthe nwt+bn-man collection.
We find that using a lin-ear kernel for the procedure using IdentiFinder-asrto tag named entities results in statistically signifi-cant improvement.Finder using a simulated ASR corpus with namedentities identified correctly.
Since the amount oftraining data required was huge, we obtained thetraining data from the bn-man version of TDT3.We ran IdentiFinder-man on the bn-man version ofTDT3 and tagged the named entities.
We then re-moved punctuation and converted all the text to up-percase to simulate ASR to a limited degree.
Were-trained IdentiFinder on this simulated ASR cor-pus and used it to tag named entities in only thebn-asr stories in TDT2.
We retained the use ofIdentiFinder-man for the nwt stories.
The same threefeatures were then extracted and we re-ran the classi-fiers.
The results are shown in Table 4 in the columntitled IdentiFinder-asr.8 Conclusions and Future WorkWe have shown the applicability of machine learn-ing classification techniques to solve the NED prob-lem.
Significant improvements were made over thebaseline systems on all the corpora tested on.
Thefeatures we engineered made extensive use of namedentities, and reinforced the importance and need toeffectively harness their utility to solve problems inTDT.
NED requires not only detection and report-ing of new events, but also suppression of storiesthat report old events.
From the study of the distri-butions of scores assigned to stories by the baseline127and SVM model systems, we can see that we now doa better job of detecting old stories (reducing falsealarms).
Thus we believe that attacking the prob-lem as ?old story detection?
might be a better andmore fruitful approach.
We have shown the effectsof ASR output in the news stream, and demonstrateda procedure to alleviate the problem.A classifier with RBF kernel with ?
set to one ex-hibited the best performance.
The reason for thissuperior performance over other kernels needs to beinvestigated.
Engineering of better features is alsoa definite priority.
In the future NED can also beextended to other interesting domains like scientificliterature to detect the emerge of new topics and in-terests.AcknowledgementsThis work was supported in part by the Center for In-telligent Information Retrieval, in part by NSF grant#IIS-9907018, and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903.
Any opin-ions, findings and conclusions or recommendationsexpressed in this material are the author(s) and donot necessarily reflect those of the sponsor.ReferencesJ.
Allan, Hubert Jin, Martin Rajman, Charles Wayne, DanielGildea, Victor Lavrenko, Rose Hoberman, and David Ca-puto.
1999.
Topic-based novelty detection.
Technical re-port, Center for Language and Speech Processing, JohnsHopkins University.
Summer Workshop Final Report.J.
Allan, Victor Lavrenko, and Hubert Jin.
2000.
First storydetection in tdt is hard.
In Proceedings of the Ninth Interna-tional Conference on Information and Knowledge Manage-ment, pages 374?381.
ACM Press.J.
Allan.
2002.
Topic Detection and Tracking: Event-BasedInformation Organization.
Kluwer Academic Publishers.Daniel M. Bikel, Richard L. Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?s in aname.
Machine Learning, 34(1-3):211?231.Thorsten Brants, Francine Chen, and Ayman Farahat.
2003.
Asystem for new event detection.
In Proceedings of the 26thAnnual International ACM SIGIR Conference, pages 330?337, New York, NY, USA.
ACM Press.Ronald K. Braun and Ryan Kaneshiro.
2003.
Exploiting topicpragmatics for new event detection in tdt-2004.
Techni-cal report, National Institute of Standards and Technology.Topic Detection and Tracking Workshop.Ronald K. Braun and Ryan Kaneshiro.
2004.
Exploiting topicpragmatics for new event detection in tdt-2004.
Techni-cal report, National Institute of Standards and Technology.Topic Detection and Tracking Workshop.Christopher J. C. Burges.
1998.
A tutorial on support vectormachines for pattern recognition.
Data Mining and Knowl-edge Discovery, 2(2):121?167.James P. Callan, W. Bruce Croft, and Stephen M. Harding.1992.
The INQUERY retrieval system.
In Proceedings ofDEXA-92, 3rd International Conference on Database andExpert Systems Applications, pages 78?83.Ayman Farahat, Francine Chen, and Thorsten Brants.
2003.Optimizing story link detection is not equivalent to optimiz-ing new event detection.
In ACL, pages 232?239.Thorsten Joachims.
1999.
Making large-scale support vectormachine learning practical.
MIT Press, Cambridge, MA,USA.Giridhar Kumaran and J. Allan.
2004.
Text classificationand named entities for new event detection.
In Proceedingsof the 27th Annual International ACM SIGIR Conference,pages 297?304, New York, NY, USA.
ACM Press.Juha Makkonen, Helena Ahonen-Myka, and MarkoSalmenkivi.
2002.
Applying semantic classes in eventdetection and tracking.
In Proceedings of InternationalConference on Natural Language Processing (ICON 2002),pages 175?183.Juha Makkonen, Helena Ahonen-Myka, and MarkoSalmenkivi.
2004.
Simple semantics in topic detec-tion and tracking.
Information Retrieval, 7(3?4):347?368.Dunja Mladeni, Janez Brank, Marko Grobelnik, and NatasaMilic-Frayling.
2004.
Feature selection using linear classi-fier weights: interaction with classification models.
In Pro-ceedings of the 27th Annual International ACM SIGIR Con-ference, pages 234?241, New York, NY, USA.
ACM Press.R.
Papka and J. Allan.
1998.
On-line new event detection usingsingle pass clustering.
Technical Report UM-CS-1998-021.Nicola Stokes and Joe Carthy.
2001.
Combining semantic andsyntactic document classifiers to improve first story detec-tion.
In Proceedings of the 24th Annual International ACMSIGIR Conference, pages 424?425, New York, NY, USA.ACM Press.Vladimir N. Vapnik.
1995.
The nature of statistical learningtheory.
Springer-Verlag New York, Inc.Yiming Yang, Tom Pierce, and Jaime Carbonell.
1998.
A studyof retrospective and on-line event detection.
In Proceedingsof the 21st Annual International ACM SIGIR Conference,pages 28?36, New York, NY, USA.
ACM Press.Yiming Yang, Jian Zhang, Jaime Carbonell, and Chun Jin.2002.
Topic-conditioned novelty detection.
In Proceedingsof the 8th ACM SIGKDD International Conference, pages688?693.
ACM Press.Jian Zhang, Zoubin Ghahramani, and Yiming Yang.
2005.
Aprobabilistic model for online document clustering with ap-plication to novelty detection.
In Lawrence K. Saul, YairWeiss, and Le?on Bottou, editors, Advances in Neural In-formation Processing Systems 17, pages 1617?1624.
MITPress, Cambridge, MA.128
