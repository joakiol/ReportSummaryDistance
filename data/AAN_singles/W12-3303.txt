Proceedings of the 2012 Student Research Workshop, pages 13?18,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsActive Learning with Transfer LearningChunyong Luo, Yangsheng Ji, Xinyu Dai, Jiajun ChenState Key Laboratory for Novel Software Technology,Department of Computer Science and Technology,Nanjing University, Nanjing, 210046, China{luocy,jiys,daixy,chenjj}@nlp.nju.edu.cnAbstractIn sentiment classification, unlabeled userreviews are often free to collect for newproducts, while sentiment labels are rare.
In thiscase, active learning is often applied to build ahigh-quality classifier with as small amount oflabeled instances as possible.
However, whenthe labeled instances are insufficient, theperformance of active learning is limited.
Inthis paper, we aim at enhancing active learningby employing the labeled reviews from adifferent but related (source) domain.
Wepropose a framework Active Vector Rotation(AVR), which adaptively utilizes the sourcedomain data in the active learning procedure.Thus, AVR gets benefits from source domainwhen it is helpful, and avoids the negativeaffects when it is harmful.
Extensiveexperiments on toy data and review texts showour success, compared with other state-of-the-art active learning approaches, as well asapproaches with domain adaptation.1 IntroductionTo get a good generalization in traditionalsupervised learning, we need sufficient labeledinstances in training, which are drawn from thesame distribution as testing instances.
When thereare plenty of unlabeled instances but labels areinsufficient and expensive to obtain, activelearning (Settles, 2009) selects a small set ofcritical instances from target domain to be labeled,but costs are incurred for each label.
On the otherhand, transfer learning (Ji et al, 2011), also knownas domain adaptation (Blitzer et al, 2006), aims atleveraging instances from other related sourcedomains to construct high-quality models in thetarget domain.
For example, we may employlabeled user reviews of similar products, to predictsentiment labels of new product reviews.
When thedistributions of source and target domain aresimilar, transfer learning would work well.
Butsignificant distribution divergence might causenegative transfer (Rosenstein et al, 2005).To further reduce the labeling cost and avoidnegative transfer, we propose a framework, namelyActive Vector Rotation (AVR), which takesadvantage of both active learning and transferlearning techniques.
Basically, AVR makesmodel?s parameter vector ?
actively rotate towardsits optimal direction with as few labeled instancesin target domain as possible.
Specifically, AVRfirst applies certain unsupervised learningtechniques to make source and target domain?sdistributions more ?similar?, and then leveragessource domain information to query the mostinformative instances of target domain.
Mostimportantly, it carefully reweights instances tomitigate the risk of negative transfer.
AVR isgeneral enough to incorporate various activelearning and transfer learning modules, as well asvaried basic learners such as LR and SVM.2 Related WorkShi et al (2008) proposed an approach AcTraK,using labeled source and target domain instances tobuild a so-called ?transfer classifier?
to help labelactively selected target domain instances.
AcTraKinitially requires labeled target domain instances,13and relies too much on the transfer classifier.
Thusit might be degenerated by negative transfer.An ALDA framework was proposed in (Saha etal., 2011).
ALDA employs source domainclassifier ?????
to help label actively selected target domain instances.
When conditional distributions???|??
are a bit different (Chattopadhyay et al,2011) or marginal distributions ????
aresignificantly different between source and targetdomain, ALDA would perform poorly.
ALDAdoesn?t discuss the negative transfer problem andgets hurts when it happens, while AVR activelyavoids it by its projection and reweighting strategy.Liao et al (2005) proposed a method M-Logit,utilizing auxiliary data to help train LR.
They alsoproposed actively sampling target domaininstances using Fisher Information Matrix(Fedorov, 1972; Mackay, 1992).
Besides, instanceweighting was used to mitigate distributiondifference between source and target domain in(Huang et al, 2006; Jiang and Zhai, 2007;Sugiyama et al, 2008).
These can work as amodule in our framework.3 AVR: Active Vector RotationWithout loss of generalization, we will constrainthe discussion of AVR to binary classificationtasks.
But in fact, AVR can also be applied tomulti-class classification and regression.Given training set ???
?
???
?, ???|?
?
1,?
,?
?, ??
?
??
, ??
?
??1,?1?
, traditional supervised learning tries to optimize (Fan et al, 2008; Lin etal., 2008):min???
||?|| ?
?
?
???
; ?
?, ???????
,        (1) where the penalty parameter ?
?
0 , controls theimportance ratio between loss function ???
; ?
?, ???
and regularization parameter ||?||.
Loss function?sdefinition is diverse for different basic learners, e.g.LR uses log?1 ?
?????????
, while L2-SVM usesmax?
?1 ?
?????
?, 0??.
In the paper, we have the following assumptions:1) Target domain ????
?
?????
, ????|?
?1,?
, ????
?, ???
?
???
, ???
?
?
?1,?1?, ???
?is the size of ????
;2) Source domain ????
?
????
?, ????|?
?1, ?
, ????
?, ???
?
???
, ???
?
?
?1,?1?, ????
is the size of ????
; 3) ?????
?
?????
;4) ????
and ????
are large enough;5) Testing set ?????
and ????
are i.i.d..Under maximum labeling budget ?
?, our goal is to employ source and target domain instances tomaximize model accuracy:max?
?????????
?
?
?????????????????,?????????
,  (2) where the hypothesis is:?????
?
?
?1, ???
?
0?1, ???
?
0.
(3)So, we design the machine learning framework,Active Vector Rotation, to optimize ?:min???
||?|| ?
?
c????
; ?
?, ???????
,         (4) where the weight variables c?
?
0 ,  control the importance of each instance in training.
Larger c?
means more necessity of ?
to fit ??
?, ???
.
Intuitively, ?
of ???
should try harder to fit the instances from ????
than the instances from ????
,so that the corresponding c?
of instances from ???
?should be larger.
The algorithm of AVR isdescribed in Table 1, which is discussed in detail inthe following subsections.Input: ???
?, ???
?, ????
?, ??
; Output: ?, ?????????1.
Project ??
and ??
to a common latent semanticspace, where ??
?, ???
?
??.2.
Actively select the least source domain instances,which can characterize source domain classifier????
, into training set ???
?
?????
?, ?????|?
?1,?
, ?????
?.
3.
Initialize ?
using ???.
4.
For ??
?
?????
?
1 ?
?????
?
??
1) Actively select the most informativeinstance ????
?, ?????
from ???
?.2) Insert the new labeled instance intotraining set, ???
?
???
?
????
?, ????
?.3) Update c?
for ?
?
1: ?
?.4) Retrain ?
using ???
and (4).
end5.
Compute ?????????
.Table 1: AVR algorithm3.1 Projection of Source and Target Domain??
and ??
might be in different vector spaces.
Toemploy ????
in the training of ????
?s optimal ?
,we?d better project ??
and ??
into a common n-dimensional latent semantic space, where thedistributions of the projected ??
?, ???
?
??
wouldbe more similar.
Varied projection approachescould be employed in different tasks.
For example,Hardoon et al (2004) used CCA to project text and14image to a latent semantic space, where imagecould be retrieved by text.
Blitzer et al (2007) andJi et al (2011) utilized SCL and VMVPCArespectively in sentiment classification.
Huang etal.
(2006) applied RKHS and KMM in breastcancer prediction.Regarding the case where ??
and ??
are in thesame vector space but certain approach is appliedto make their distributions more similar, we alsoconsider it as a kind of projection of ????
and ???
?.3.2 Initialization of Training setTo reduce training cost and risk of negativetransfer, AVR actively selects a relatively small setof instances from ????
into ???
.
Transfer learning mainly leverages ????
?s separating hyperplane information, i.e.
????
, while only a small set of critical instances from ????
can characterize the statistics of ????
.
AVR initializes ???
by these critical instances.
Different tasks may employdifferent selection strategy.
E.g.
in our experiments,the text classification task employs uncertaintysampling (Settles, 2009), while sentimentclassification task selects the least ?????
instances which can accurately characterize ????
?, such that:min?????????
?
?????
?????????????
.
(5)3.3 Query Strategy in Target DomainAfter initialization of ??
?, AVR uses certain basic learner, such as LR and SVM, to get ?
?
??????
.
As the labeling budget ??
is limited, we need iteratively query the most informative instance andadd the new labeled instance into ???
to retrain ?.
AVR revises the query strategy of traditionalactive learning.
After a few new labeled instancesadded to ???
, the retrained ?
would be different from ??????
and closer to the optimum.
Traditional active learning queries the instance in ????
w.r.t.
?,e.g.
uncertainty sampling queries the instanceclosest to separating hyperplane, such that:min??????????
????????.
(6)However, AVR queries the most informativeinstance from which are identically classified by ?and ??????
, e.g.
for uncertainty sampling, AVR queries the instance such that:min?????????,???????????????
????????
????????
.
(7)The instance queried by AVR makes ?
morequickly approach to its optimum, as to some extent,part of the statistics of the instances which aredifferently classified by ?
and ?????
, can be characterized by the new queried instances.
Butwhen ?
is very close to the optimum, AVR willquery by traditional active learning strategy.3.4 Reweighting ?
?Appropriate reweighting can help accelerate ?rotating to the optimum and avoid negative transfer.Intuitively, the instances from ????
and theinstances which have similar distribution with ???
?should be given higher weight.
Varied reweightingstrategy, e.g.
TrAdaBoost (Dai et al, 2007), couldbe applied in AVR framework.
In our experiments,AVR employs a simple but efficient reweightingstrategy, without iteration:??
?
?
?1, ?
?
?????
, ????????????
????
?
0?
?0, ?
?
?????
, ????????????
????
?
0??
?, ?????????.
(8)4 ExperimentsWe perform AVR on a set of toy data and two realworld datasets, 20 Newsgroups Dataset 1  andMulti-Domain Sentiment Dataset 2 , comparing itwith several baseline methods.
In this paper, weuse model accuracy ?????????
under fixed labeling budget ??
as the evaluation.
We used LR and L2-SVM as basic learner respectively, but due tospace limit, we only report the results of LR.4.1 Toy DataWe generate four bivariate Gaussian distributionsas the positive and negative instances of ????
and ????
respectively as illustrated in Figure 1.Figure 1: Distribution of toy data and AVR process1 http://people.csail.mit.edu/jrennie/20Newsgroups/.2 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/.15As shown in Figure 1, ????
and ????
randomlysample 1000 instances respectively, then ?????
randomly samples 200 instances from ????.
Circleand diamond, big plus and cross, small plus andcross, represent positive and negative instances of???
?, ????
and ?????
respectively.To this toy data, AVR?s configuration is:1) ???
?
?
?, ???
?
?
?.2) AVR uses uncertainty sampling to select theleast 5 instances which can characterize????
?, to initialize ???
and ?????.
In Figure 1, the 5 instances are marked by big filledcircles or diamonds, the dash line draws theseparating hyperplane ??????
?
?
0.
3) Then AVR queries instances as described inSection 3.3, the first 10 queried instances aremarked by large numerals, with the first 3are queried w.r.t.
(7).
The small numeralsmark the first 3 instances which would bequeried w.r.t.
(6).4) AVR reweights ??
by (8), where ?
?
4.
The black filled circles mark the instances whosecorresponding c?
?
0.
The solid line draws the current hyperplane ???
?
0.Baseline methods are briefly described in Table2.
Details about AcTraK and ALDA can be foundin (Shi et al, 2008) and (Saha et al, 2011)respectively.Method NoteRandomActiveRandomly sample instances from ???
?,without use of ????
Uncertainty sampling, without use of ????
AcTraKO-ALDAInitiated by one positive and one negativeinstances from ???
?, followed by uncertaintysampling from ???
?Stream-based sampling, without instancereweightingB-ALDASource-APool-based sampling, without instancereweightingInitialize ???
by ???
?, following uncertainty sampling without instance reweightingAVR-UAVR-W?Uncertainty sampling with instancereweightingGive all instances from ????
the same weight, regardless prediction differencebetween ??and??????.
?Table 2: Brief description of baseline methodsThe first 4 methods referring randomness are run1000 times to average results as shown in Table 3.Method Target Domain Labeling Budget ??
1 2 3 4 5 6 7 8 9 10RandomActive50.0549.9069.3575.6579.8890.4186.0495.9290.2696.3093.0197.2394.4197.4195.3097.5996.0397.6496.4197.72AcTraKO-ALDA93.157795.237796.1077.0196.6977.0797.0377.1597.3077.2497.5377.3397.6877.3797.7877.4297.8277.48B-ALDASource-AAVR-UAVR-W777780.5080.507777959477778594.5077779697777798.5098.507777969777779898.5077.5077.509897.5077.5077.509798.5077.5077.5096.5097AVR 80.50 94 94.50 97 98.50 97 98.50 97.50 98.50 98.50Table 3: Performance of different methods on toydata, where AcTraK unfairly uses two more labels.4.2 20 Newsgroups Dataset20 Newsgroups Dataset is commonly used inmachine learning and NLP tasks.
It contains about20000 newsgroup documents which arecategorized into 6 top categories and 20subcategories.
We split it into 6 pair of ????
and ????
, with each pair includes only two topcategories documents, such as ?comp?
and ?rec?,but ????
and ????
are drawn from differentsubcategories, e.g.
????
has ?comp.graphics?
and ?comp.graphics?, but ????
has ?comp.windows.x?and ?sci.autos?.
The task is to leverage ????
to distinguish the top categories of documents in ???
?.Our settings of 20 Newsgroups Dataset is identicalwith Dai et al (2007), details can be found there.On this dataset, AVR?s configuration is similarwith that on toy data, with ?????
varies from 500 to 800 on different pairs.Due to space limit, we only report results on thepair of ?comp vs. rec?
in Figure 2, with allmethods are averaged over 30 runs.
The results onother pairs are similar.
Since AVR-U and AVR-Ware variants of AVR, with similar performance, weonly report the results of AVR.Figure 2: AVR outperforms others on the ?compvs.
rec?
pair.164.3 Multi-Domain Sentiment DatasetThe sentiment dataset consists of user reviewsabout several products (Book, DVD, Electronic,Kitchen) from Amazon.com, the task is to classifya review?s sentiment label as positive or negative.We have 12 pairs with each pair has two productsas ????
and ????
respectively.
On this dataset,AVR employs VMVPCA (Ji et al, 2011) to project????
and ????
, and initializes ???
with ?????
?1000  instances from ????
w.r.t.
(5), while the other configuration is the same as that described inSection 4.1.
To be comparable, the baselinemethods which leverage ????
are preprocessed by VMVPCA.
We also add another baseline methodSource-A?
here, which is identical with Source-A,except that it is not projected by VMVPCA.
Givenspace limit, we only report the results on the pair?DVD?Kitchen?, with other pairs have similarperformance.Figure 3: AVR does better than previous work onthe ?DVD?Kitchen?
dataset for all budget sizes.4.4 DiscussionFrom inspection of experimental results, we get thefollowing remarks.Why to combine active learning and transferlearning??
Active learning such as uncertainty samplingcan significantly reduce the labeling cost.
Butwhen ?
is far from the optimum, uncertaintysampling may oversample instances near adirection.
For example, in Figure 2, Activemethod is worse than Random method when???
?
50.?
????
could help ????
in learning accurate ?
,e.g.
in Figure 2, when ???
?
200, Source-A method with the help of ????
outperforms Random and Active methods which never use????.
But inappropriate use of ????
may cause negative transfer, e.g.
in Figure 2, when???
?
200 , Source-A, ALDA and AcTraK methods, which overuse ????
, underperform Active method.?
Thus, we realize that appropriate combinationof transfer learning and active learning couldadvance and complement each other.Especially when ????
has scarce labels, ???
?could help avoid oversample instances near adirection.
But with the increase of labels in????
, ????
should decrease its weight intraining to avoid negative transfer.Does each component of AVR work??
Appropriate Projection of ????
and ????
couldmitigate distribution divergence, e.g.
in oursentiment classification task, Source-A andAVR which applied VMVPCA significantlyand consistently outperforms Source-A?.?
Initialize ???
by a small set of critical instances from ????
can significantly reduce training cost without loss of accuracy.
E.g.
inour experiments, when ???
?
1 , AVR has better or comparable performance w.r.t.Source-A which initializes ???
by whole ????.
More importantly, AVR trims initial ???
size from 1000 to 5 in toy data, from 4000 to 500in Newsgroups dataset, and from 2000 to1000 in Sentiment dataset.?
The query strategy of AVR described inSection 3.3 advances traditional activelearning, which is supported by theperformance of AVR over AVR-U.?
Appropriately reweighting instances from????
and ????
could result in accurate ?
andavoid negative transfer meanwhile.
Forexample, in our experiments, the reweightingstrategy of (8) makes AVR outperform allbaseline methods, while some of which sufferfrom negative transfer.How about AcTraK?s performance??
AcTraK works well on our toy data, justbecause it unfairly uses too much more labelsof ????
, even though, it underperforms AVRwhen ?
???
?
3 .
Besides, AcTraK performs poorly on high dimensional data like text inour experiments.175 Conclusion and Future WorkOur proposed machine learning framework AVRactively and carefully leverages information ofsource domain to query the most informativeinstances in target domain, as well as to train thebest possible model of target domain.
The fouressential components of AVR, which establish itsefficacy and help it avoid negative transfer, arevalidated in experiments.In the future, we are planning to apply AVR inmore tasks with appropriate specification ofprojection, query and reweighting strategy.Especially for sentiment classification, we willcombine prior domain knowledge, such as domainsentiment lexicon, with AVR framework to furtherreduce labeling cost.AcknowledgementsThis work is supported by the NationalFundamental Research Program of China(2010CB327903) and the Doctoral Fund ofMinistry of Education of China (20110091110003).We also thank Shujian Huang, Ning Xi, YinggongZhao, and anonymous reviewers for their greatlyhelpful comments.ReferencesJohn Biltzer, Ryan Mcdonald, Fernando Pereira.
2006.Domain adaptation with structural correspondencelearning.
In Proc.EMNLP, pp.120-128.John Biltzer, Mark Dredze, Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:domain adaptation for sentiment classification.
InProc.
ACL, pp.432-439.Rita Chattopadhyay, Jieping Ye, SethuramanPanchanathan, Wei Fan, Ian Davidson.
2011.
Multi-source domain adaptation and its application to earlydetection of fatigue.
In Proc.
KDD, pp.717-725.Wenyuan Dai, Qiang Yang, Gui-Rong Xue, Yong Yu.2007.
Boosting for transfer learning.
In Proc.
ICML,pp.93-200.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin.
2008.
Liblinear: a libraryfor large linear classification.
JMLR, 9:1871-1874.Valerii?
Vadimovich Fedorov.
1972.
Theory of optimalexperiments.
Academic Press.David R. Hardoon, Sandor Szedmak, John Shaew-Taylor.
2004.
Canonical correlation analysis: Anoverview with application to learning methods.Neural Computation, 16(12): 2639-2664.Jiayuan Huang, Alexander J. Smola, Arthur Gretton,Karsten M. Borgwardt, Bernhard Scho?
lkpf.
2006.Correcting sample selection bias by unlabeled data.In Proc.
NIPS, pp.601-608.Yangsheng Ji, Jiajun Chen, Gang Niu, Lin Shang,Xinyu Dai.
2011.
Transfer learning via multi-viewprincipal component analysis.
JCST, 26(1):81-98.Jing Jiang, ChengXiang Zhai.
2007.
Instance weightingfor domain adaptation in NLP.
In proc.
ACL, pp.264-271.Xuejun Liao, Ya Xue, Lawrence Cain.
2005.
Logisticregression with an auxiliary data source.
In Proc.ICML,  pp.505-512.Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi.
2008.Trust region newton method for large-scale logisticregression.
JMLR, 9:627-650.David J. C. Mackay.
1992.
Information-based objectivefunctions for active data selection.
NeuralComputation, 5:590-604.Michael T. Rosenstein, Zvika Marx, Leslie PackKaelbling, Thomas G. Dietterich.
2005.
To transferor not to transfer.
In Proc.
NIPS, December 9-10.Avishek Saha, Piyush Rai, Hal Daum e?
III, SureshVenkatasubramanian, Scott L. DuVall.
2011.
Activesupervised domain adaptation.
In Proc.
ECML-PKDD, pp.97-112.Burr Settles.
2009.
Active learning Literature Survey.
InComputer Sciences Technology Report 1648,University of Wisconsin-Madison.Xiaoxiao Shi, Wei Fan, Jiangtao Ren.
2008.
Activelytransfer domain knowledge.
In Proc.
ECML-PKDDpp.342-357.Masashi Sugiyama, Shinichi Nakajima, HisashiKashima, Paul von B?
?nau, Motoaki Kawanabe.
2008.Direct importance estimation with model selectionand its application to covariate shift adaptation.NIPS, pp.1433-1440.18
