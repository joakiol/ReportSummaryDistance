Y. Glass,A ?
B i l ingua l  VOYAGER System 1D.
Goodine, M. Phillips, S. Sakai 2, S. Seneff,Spoken Language Systems Group ?Laboratory  for Computer  Sc ienceMassachuset ts  Institute of Techno logyCambr idge ,  Massachuset ts  02139and V. Zue 3ABSTRACTThis paper describes our initial efforts at porting the VOY-AGER spoken language system to Japanese.
In the processwe have reorganized the structure of the system so that lan-guage dependent information is separated from the core en-gine as much as possible.
For example, this information isencoded in tabular or rule-based form for the natural lan-guage understanding and generation components.
The inter-nal system manager, discourse and dialogue component, anddatabase are all maintained in language transparent form.Once the generation component was ported, data were col-lected from 40 native speakers of Japanese using a wizardcollection paradigm.
A portion of these data was used totrain the natural language and segment-based speech recog-nition components.
The system obtained an overall under-standing accuracy of 52~0 on the test data, which is similarto our earlier reported results for English \[i\].INTRODUCTIONIn the fall of 1989, our group first demonstrated VOY-AGER, a system that can engage in verbal dialogues withusers about a geographical region within Cambridge, Mas-sachusetts \[2\].
The system can provide users with infor-mation about distances, travel times, or directions be-tween objects located within this area (e.g., restaurants,hotels, post offices, subway stops), as well as informa-tion such as addresses or telephone numbers of the ob-jects themselves.
While VOYAGER is constrained bothin its capabilities and domain of knowledge, it containsall the essential components of a spoken-language sys-tem, including discourse maintenance and language gen-eration.
The VOYAGER application provided us with ourfirst experience with the development ofspoken languagesystems, helped us understand the issues related to thisendeavor, and provided a framework for our subsequentsystem development efforts \[3, 4\].1This research was supported by DARPA under ContractN00014-89-J-1332, monitored through the Office of Naval Research.2Currently a visiting scientist from NEC Corp, Kawasaki,Japan.3The authors are listed in alphabetical order.49Over the past few years, we have become increas-ing ly  interested in developing multilingual spoken lan-guage systems.
There are several ongoing internationalspoken language translation projects whose goal is to en-able humans to communicate with each other in theirimtive tongues \[5, 6\].
Our objective, however, is some-what different.
Specifically, we are interested in devel-oping multilingual human-computer interfaces, such thatthe information stored in the database can be accessedand received in multiple spoken languages.
We believethat there is great utility in having such systems, sinceinformation is fast becoming globally accessible.
Fur-thermore, we suspect hat this type of multilingual sys-tem may be easier to develop than speech translationsystems, since the system only needs to anticipate thediversity of one side of the conversation, i.e., the humanside.
During the past year, we have begun to developa multilingual version of VOYAGER.
This paper will de-scribe our work in extending VOYAGER'S capability fromEnglish to Japanese.Since VOYAGER was originally designed only for En-glish, a number  of changes were necessary to accommo-date multiple languages.
In the next section, we describeour approach to developing multilingual systems, and themodifications made to the original system.
A discussionof the specific implementation of the various componentsfor Japanese will follow.
Finally, performance evaluationof the Japanese VOYAGER system will be presented, fol-lowed by a brief description of future plans.SYSTEM DESCRIPT IONFigure 1 shows a block diagram of a prototypical MITspoken language system.
The speech signal is convertedto words using our SUMMIT segment-based speech recog-nition system \[7\].
Language understanding makes use ofTINA, a probabilistic natural anguage system that inter-leaves syntactic and semantic information in the parsetree \[8\].
Data exchange between SUMMIT and TINA iscurrently achieved via an N-best interface, in which therecognizer produces the top-N sentence hypotheses, andTINA screens them for syntactic and semantic well-formed-ness within the domain \[1\].
The parse-tree producedFigure 1:system.Schematic of prototypical MIT spoken-languageby TINA iS subsequently converted to a semantic framewhich is intended to  capture the meaning of the inpututterance in a language independent form \[4\].The semantic frame is passed to the system manage rwhich uses it, along with contextual information stored inthe discourse component, o access information stored inthe database, and provide a response \[2\].
The VOYAGERapplication uses an object-oriented database, althoughwe have also accessed ata in SQL and other configura-tions \[3\].
Responses to the user consist of displays, text,and synthetic speech.
The latter two are derived via alanguage generation component which generates noun-phrases from the internal semantic representation andembeds them into context-dependent messages.In order to develop a multilingual capability for ourspoken language systems, we have adopted the approachthat each component in the system be as language trans-parent as possible.
In the VOYAGER system for instance,the system manager, discourse component, and the data-base are all structured so as to be independent of theinput or output language.
Where language-dependentinformation is required we have attempted to isolate itin the form of external tables or rules, as illustrated inFigure 1 for both the language understanding and gener-ation components.
As will be described in more detail inthe next section, we trained a version of the basic SUM-MIT system for both Japanese and English, using datarecorded from native speakers for each language.
Thecurrent user interface is very similar to that of the orig-inal VOYAGER system, except hat a separate recordingicon is used for each language.
For text-to-speech syn-thesis we use a DECtalk system for English, and an NECtext-to-speech system for Japanese.If we are to attain a multilingual capability within asingle system framework, the task of porting to a newlanguage should involve only adapting existing tables ormodels, without requiring any modification of the indi-50vidual components.
By incrementally porting the systemto new languages we hope to slowly generalize the archi-tecture of each component to achieve this result.
Thefollowing sections provide more detailed descriptions ofthe work done in the different areas to achieve a bilingualstatus of VOYAGER.JAPANESEIMPLEMENTATIONTo allow VOYAGER to converse with a user in Japanese,the following steps were taken.
We first converted thesystem so that it could generate responses in Japanese.This enabled us to collect data from native speakers ofJapanese in a wizard mode whereby an experimentorwould translate the subjects' spoken input and type theresulting English queries to the system \[3, 9\].
Once datawere available we were able to port the speech recog-nition and language understanding components.
In theprocess of augmenting the system components to han-dle Japanese, we made many changes to the system corestructure, separating out the language-dependent aspectsinto external tables and rules.Data  Co l lec t ionOne of the most time-consuming aspects of the port-ing process was the acquisition of appropriate user datacapturing the many different ways users can ask questionswithin the VOYAGER domain: We started with transla-tions from available English sentences, but these aloneare not nearly adequate for closure on coverage of actualdata.
Although in theory a grammar developer can usehis/her innate knowledge of the language to write appro-priate grammar ules, in practice such an approach fallsfar short of complete coverage of actual user utterances.For data collection from Japanese subjects we recordeddata from 40 native speakers, recruited from the generalMIT community.
In a manner similar to data collec-tion techniques used for the ATIS domain \[3\], subjectswere asked to solve four problem scenarios.
At the endof the session subjects were also allowed to ask randomquestions of the system.
The resulting corpus of 1426utterances was partitioned into a 34 speaker training setand a 6 speaker test set which was subsequently used toevaluate system components.Speech RecognitionMajor tasks in porting SUMMIT to Japanese includeacoustic-phonetic, lexical-phonological and language mod-eling.
In an earlier paper, we described these compo-nents and reported on a speaker-dependent evaluation\[10\].
Will briefly summarize our previous work, and de-scribe all subsequent developments, including improvedlanguage modeling and speaker-independent training.Phonetic Modeling In the current version, we use acontext-independent mixture (up to 16) diagonal Gaus-sian model to represent each label in the lexical net-work \[7\].
Starting from seed models, the phonetic mod-els are iteratively trained using a segmental K-means-like procedure whereby the forced alignments of the pre-vious iteration are used to train the current iteration.In the English version, the seed models were trainedfrom the manually-aligned phonetic transcriptions of theTIMIT corpus \[11\].
Rather than obtaining aligned pho-netic transcriptions for a Japanese corpus, we found thatwe could achieve reasonable initial alignments by seed-ing our Japanese phonetic models from their phoneticallymost similar English counterparts.
Based on an inspec-tion of the alignments, we confirmed that the resultingJapanese models were converging to the intended labelsafter a few training iterations.Word IDta t atara t a r aQ qte t ede d edesu  d e s uto t oto(p-j) t oLeft RightPronunciation Category Categoryaux-taiaux- tarainf-v-sokuaux-tep-c-deaux-desup-c-top-j-toadj-raux-tarav-p-sokuaux-tep-c-deaux-desu-fp-c-top-j-toFigure 2: Example lexical entries.
Each lexical entry con-sists of a word ID, a pronunciation, and left and right mor-phological categories.which may not be well suited to spontaneous speech \[13\].Phonological Modeling Words in the lexicon mustbe mapped from the abstract phonemic representationto the possible acoustic realizations, taking into accountcontextual variations.
We have adopted the procedure ofmodeling some of these variations through aset of phono-logical transformation rules, some of which are unique toJapanese.
One of the typical phonological effects that wemust account for in Japanese is the different phonetic re-alizations of the so-called mora  (syllabic) phonemes/Q/and/N/ .
For example, the phoneme/Q/ is  regarded tooccupy one higher-level temporal unit (mora) and is re-alized as a lengthening ofthe closure interval before stopconsonants.
When it is followed by fricatives, it may berealized instead as a lengthening of the following frica-tion.
Another major phonological phenomenon is thedevoicing o f / i /  and /u / ,  which typically occurs whenthey are preceded and followed by voiceless consonants.In the English version of SUMMIT, phonological trans-formation rules have been used to generate alternativepronunciations based on low-level phonological effects uchas flapping, palatalization, and gemination.
For the Japan-ese version, we have been able to use the same frameworkfor the conversion of mora phonemes into different pho-netic realizations as well as describing lower-level phono-logical effects uch as gemination and devocalization.
Aset of approximately 60phonological rules has been de-veloped to account for the possible acoustic realizationsof word sequences.
These rules produce a total of 56distinct acoustic labels in the resulting lexical network.Language Modeling Language modeling is an impor-tant aspect of speech recognition since it can dramati-cally reduce the difficulty of a task.
Many speech recog-nition systems developed for English, particularly thosedeveloped for spontaneous speech, employ n-gram lan-guage models which capture local word constraints inan utterance \[4, 12\].
On the other hand, most speechrecognition systems for Japanese speech currently employonly small and rather constrained context-free grammars51Compared to English, the choice of lexical units forJapanese speech recognition is less clear.
In particu-lar, Japanese orthography does not have spacing betweenwords, making it difficult to have a common agreementon where word boundaries are in a sentence, specially inthe case of certain function word sequences.
The choice ofunits impacts both the compactness of the lexical repre-sentation and the effectiveness of local grammatical con-straints.
If we choose units that are too large, the lexi-con will need many redundant entries to capture the lin-guistic variation.
On the other hand, choosing smallerunits weakens the constraint available from local lan-guage models such as statistical bigrams.
We have ad-dressed this to some degree by carefully choosing a set ofmorphological units along with left and right adjacencycategories for these units.
For example, lexical entriesare ~lly separated into root and inflectional suffixes, ex-cept for words with irregular inflections, thus providing asystem flexible enough to cope with various expressionsin spontaneous speech.In order to develop sufficiently general grammaticalconstraints o be used for continuous speech recognition,we developed a category bigram grammar, where theclasses are defined by morphological categories.
As illus-trated in Figure 2, each lexical entry is given a left andright morphological djacency category.
The probabilityof the word wj given word wi is defined to bep(wjlw ) P(l(wj)Iw )  ( jlZ(wj))1!5(wjll(wj)) - L( l(wj))where l(w) and r(w) are the categories of word w asviewed from the left and right respectively, an d L(l) isthe number of distinct words in a category I.
By thisdefinition, all words within a category are assumed to beequally probable.As we and others have done previously \[4, 12\], thecategory bigram probability is smoothed by interpolatingthe bigram estimate with the prior probabilities of eachcategory:: (llr) = c(l)~ -  + (1 - ~(r)) c(all word tokens)c(r)=c(r) + Kwhere c(x) is the count of tokens of category x inthe corpus.Language Unders tand ingThe grammar for the English VOYAGER had been en-tered in the form of context-free rules plus constraints.A trace mechanism was used to handle movement phe-nomena, and syntactic and semantic features were unifiedduring parsing to invoke agreement constraints.
Japanesewas in many respects easier than English - we found thatit was unnecessary to mark any syntactic or semantic fea-tures, and Japanese, unlike European languages, appearsnot to make use of constituent movement.
The only dif-ficulty with Japanese was that parse trees tend to beleft-recursive, which can cause infinite-loop problems ina top-down parser.
Noun phrase modifiers are positionedto the left of the modified object, and, furthermore, thepreposition indicating the relationship follows the modi-fier.
Thus a top-down depth-first parser can keep seekinga noun modifier as the next constituent, at the end of aninfinite series of recursive modifiers.Since the main  reason for parsing top-down was  thetrace mechan ism,  wh ich  Japanese does not use, our solu-tion was  to imp lement  a simple bot tom-up  parser withouttrace.
Rules were entered by hand, based on  all of thetraining material we  had  collected.
Figure 3 shows  an ex-ample  parse for the sentence, "Sentoraru Eki no chikakuno toshokan wa doko desu ka?"
("Where is a library inthe vicinity of Central Station?").
The left-recursion isapparent from the shape of the parse tree, and the poten-tial for infinite recursion is clear from the category labelson the left-most branch, since 'cA-PLACE" can rewrite as('cA-PLACE" ...).Mean ing  Representat ionThe Japanese parse tree must be converted to a se-mantic representation i  order to access the informationin the VOYAGER knowledge base.
To do this, we designedthe grammar ules for the Japanese grammar such thatthe resulting parse tree could easily be converted to asemantic frame essentially identical to that of the corre-sponding English sentence.
A table-driven procedure isused to convert he parse tree to the semantic frame forboth languages.
The functions that carry out the con-version are essentially language independent, with thelanguage-dependent information being stored in separatE2QUESTIONQUESYION-WHERE-QA-PLACE WA WHERE-QOBJECT-MODIFIER AN-OBJECTIN-VICINITV-OET A-VICINJTVA-PLACE P-C-NO VICINITVI .AN-OBJECT IOBJECT OBJECTA-T-STOP A-LIBRARYSTOP-NAME STATION P-C-NO LEBRAKVi i !SENTOP, ARU EKI NO CHIRAKU O TOSHOKAN WAWHERE LINK-QUNK QUEBTIO N-KA IIDOKO DESU KAFigure 3: Parse tree for the sentence, "Sentoraru Eki nochikaku no toshokan wa doko desu ka?"
( "Where is a libraryin the vicinity of Central Station?
').Semant ic  Assoc iat ions for Relevant Parse Nodes:Parse Category Semant ic  Category  Funct ionquestion-where-q locate set-sentypea-t-stop station noun-phraselibrary public-building noun-phrasestop-name stop-name proper-namevicinity j-near j-operatorTerminal  Translations:toshokan librarySentoraru centralTab le  I: Control tables required to convert f rom parse treeof Figure 3 to semantic frame of Figure 4.
These includemappings from parse tree categories to semantic categoriesto functional types, as well as translations for critical contentwords.tables for each language.
We have found that the originalsemantic f rame designed for English can accommodateJapanese with only minor  modifications.Given a well-constructed grammar, it is a relativelysimple process to define the conversions from a parse treeto a semantic frame.
Semantic encoding is defined at thelevel of the grammatical category, identified with eachnode in the parse tree, rather than at the level of an en-tire rule.
All of the semantic encoding instructions areentered in the form of simple association lists.
Each se-mantically active category (preterminal or nonterminal)in the parse tree is associated with a corresponding se-mantic name, which is often the same as its given name.Each unique semantic name is in turn associated with afunctional type, defining what function to call when thisnode is encountered in the parse tree during the stageof converting the parse to a semantic frame.
There arefewer than twenty distinct functional types.The function that converts a parse tree to a semanticf rame visits each node  once in a top -down left-to-rightfashion, calling the appropriate functions asdictated bythe mappings.
Table 1 gives the complete set of categorycorrespondences required in order to produce a seman-tic frame from the parse tree in Figure 3.
Notice thatmost of the nodes in the parse tree are ignored.
Thesemantic ategories hown in the table are all identicalto those for English except for the special category "j-near" corresponding to the function "j-operator," spe-cia/ized to handle Japanese postpositional particles.
The"j-operator" function renames the generic key "topic" inthe semantic frame under construction to the specific se-mantic relationship defined by the particular operator,in our case, "near."
In addition to these mappings, atranslation table must also be provided for those wordsthat carry semantic information.
Only two words in thissentence need to be provided, as shown in the table.Ultimately, upon complete analysis of a parse tree, anested semantic frame is produced - a structure with aname, a type, and a set of \[key-value\] pairs, where thevalue could be a string, a symbol, a list of values, a num-ber, or another semantic frame.
The semantic frame forour example sentence is shown in Figure 4.
Entries inthe frame are order-independent, and the same semanticframe is produced from a large pool of questions with dif-ferent phrasings but equivalent meanings, such as "Whatis the distance between MIT and Harvard," and "Howfar is it from MIT to Harvard."
Likewise, Japanese ver-sions of this question produce a semantic frame that isessentially identical to the one produced for English.We had anticipated that the very different Order ofconstituents between Japanese and English might makeit hard to produce an equivalent semantic frame from aJapanese sentence to that produced by an English sen-tence with the same meaning.
This did not turn out to bethe case.
Except for the additional special functions tohandle post-positional particles, along with a few otherminor adaptations, we were able to use the same func-tional procedures for converting Japanese parse trees tosemantic frames as those used for English: By carefullychoosing grammar ules with corrresponden.ces to theirEnglish equivalents, we  were able to exploit the sameprotocol for producing a semantic frame, thus feedinginto the main system with a common interlingual rep-resentation.
We feel that the success of this approach islargely attributable to the fact that we have intentionallydesigned our semantic interpretation procedure to oper-ate at the level of independent parse tree nodes, ratherthan to be explicitly associated with grammar ules orwith complex patterns found in the parse tree.Sys tem Manager  &: D iscourse  ComponentThe system manager and discourse components at-tempt to process an input semantic frame in the contextof a discourse and provide an appropriate response to theuser \[2\].
Normally this will involve accessing the databasefor the set of objects satisfying the input constraints, al-though in the case where a query is ambiguous, some 53(LOCATE CLAUSETOPIC: \[library\] REFERENCEREFTYPE: PUBLIC-BUILDINGPREDICATE: NEAR PREDICATETOPIC: \[central\] REFERENCEREFTYPE: STATION)F igure  4: Semantic frame produced by parse tree of Figure3 using mappings defined in Table 1.sort of clarification might be appropriate.
In the exampleshown in Figures 3 and 4 for instance, the result wouldbe the set of libraries having the property that they arenear a station named "Central".
These components arestructured so that they are language independent (i.e:,-the resulting set would be identical no matter what theinput language was).
The net effect is that the inputand output languages are completely isolated from eachother so that a user could speak in one language andhave the system respond in another.
Additionally, sincecontextual information is stored in a language indepen-dent form, linguistic references to objects in focus can begenerated based on the output languag e of the currentquery.
This means that a user can carry on a dialogue inmixed languages, with the system producing the appro-priate responses to each query.Language .Generat ionOnce the system manager has determined an appro-priate response for the user it will display the result onthe map, and use the language generation component toproduce a verbal answer.
The language generation com-ponent has the ability to generate noun phrases describ-ing object sets produced by the system manager.
Thenoun phrase can be singular or plural, and can contain adefinite or indefinite article.
For the example of the set oflibraries near Central Station, the English noun-phrasegenerator could produce "library near Central Station",or "libraries near Central Station", along with the ar-ticles "a" or "the" depending on the need.
These con2ditions can be specified by the system manager at themoment of generation since the precise context of theresponse is known.The  noun phrases produce d by the generator are em-bedded in language-dependent message strings which arestored in a table.
Each string is given a unique label soit can be referenced by the system manager.
Each lan-guage thus requires an association list of the messagelabel and string pattern.
To produce a response, the sys-tem manager calls the language generation componentwith a particular message label, and the noun phrasesassociated with the response.
In the library example forinstance, the system knows of one library near CentralStation.
It  would therefore call the language generationcomponent with an only message, and pass as argumentsthe noun-phrase "library near Central Station" or "Sen-toraru Eki no chikaku ni aru toshokan" depending onwhether the output language were English or Japanese.The respective unknown messages consist of "I know ofonly one <noun-phrase>."
or "<noun-phrase> wa hi-totsu dake shitte imasu.
"Although the language generation process has beenpresented as a two-stage process; it is actually recursivesince as is the case for our example, a noun-phrase canitself consist of many embedded n0un-phrases.
To buildup the noun-phrase for the set of libraries near CentralStation, the generator would start with the basic vocab-ulary value for library, and embed this string using thenear  message and the string value of the noun-phraseCentral Station.
In English, the near message would beof the form "<noun> near  <ob ject>" .
Using this pro-cedure, the language generation component  can createarbitrarily complicated noun-phrases in the domain.EVALUATIONFor the Japanese VOYAGER system, we defined a vo-cabulary of 495 words comprised of words in the trainingset and words determined by translating 2000 sentencesfrom the English VOYAGER training corpus.
This vocab-ulary covered 99% of the words in the test set (96% ofunique words).
The category bigram was also trained us-ing the training data and had perplexities of 25.9 and 27.5on the training and test sets respectively.
First choiceword and sentence rror rates were 14.9% and 53.3%,respectively, on the test set.The parser covers 82% percent of the training data,and 65% of the test data.
An  inspection of the answersgenerated by the system using text input showed that60% of the responses for the test set was correct.
Theperformance of the system dropped by 8%, to 52%, whenthe input is spoken rather than typed (N  = I0 for theN-best interface).
Note that the system's understandingability actually exceeds its sentence recognition accuracyby 5%, which suggests that a full transcription is notalways necessary for understanding.
Finally, this perfor-mance  is similar to that initially reported for our Englishsystem when using context-independent phone modelswith a word-pair g rammar  of similar perplexity (22) \[i\].FUTURE PLANSIn this paper we described our recent effort at con-verting VOYAGER to a bilingual platform.
We are encour-aged by our preliminary results, and will continue to im-prove its capabilities in all directions, including context-dependent phonetic models, a robust parsing capabilitymodeled after our ATIS system, and an expansion of itsknowledge domain.
We are currently porting the VOY-AGER system to other languages including French, Ital-ian, and German.
We plan to collect data for all lan-guages in scenario collection format in order to acquire54more goal-oriented speech.
We would also like to incor-porate a pointing mechanism into the system, since theVOYAGER application lends itself to this kind of multi-modal input.REFERENCES\[1\] Zue, V., Glass, J., Goddeau, D., Goodine, D., Leung,H., McCandless, M., Phillips, M., Polfroni, J., Seneff,S., and Whitney, D., "Recent Progress on the MIT VOY-AGER.Spoken Language System," Proc.
ICSLP, 1317-1320, Kobe, Japan, November 1990.\[2\] Zue, V., Glass, J., Goodine, D., Leung, H., Phillips, M.,Polifroni, J. and Seneff, S. "Preliminary Evaluation ofthe VOYAGER Spoken Language System," Proc.
DARPASpeech and NL Workshop, 160-167, Harwichport, MA,October 1989.\[3\] Seneff, S., Glass, J., Goddeau, D., Goodine, D.,Hirschman, L., Leung, H., Phillips, M., Polifroni, J., andZue, V., "Development and Preliminary Evaluation ofthe MIT ATIS System," Proc.
DARPA Speech and NLWorkshop, 88-93, Pacific Grove, CA., February 1991.\[4\] Zue, V., Glass, J., Goddeau, D., Goodine, D.,Hirschman, L., Phillips, M., Polfroni, J., Seneff, S. "TheMIT ATIS System: February 1992 Progress Report,"Proc.
DARPA Speech and NL Workshop, 84-88, Har-riman, NY, February 1992.\[5\] Roe, D. B., Pereira, F., Sproat, R. W., Riley, M. D.,Moreno, P. J., and Macarron, A., "Toward a SpokenLanguage Translator for Restricted-domain Context-freeLanguages," Proc.
Eurospeech, 1063-1066, Genova, Italy,September 1991.\[6\] Morimoto, T., Takezawa, T., Ohkura, K., Nagata, M.,Yato, F., Sagayama, S., and Kurematsu, A., "Enhance-ment of ATR's Spoken Language Translation System:SL-TRANS2," Proc.
ICSLP, 397-400, Banff, Canada,October 1992.\[7\] Phillips, M., Glass, J., and Zue, V., "Automatic Learn-ing of Lexical Representations forSub-Word Unit BasedSpeech Recoguition Systems," Proc.
Eurospeech, 577-580, Genova, Italy, September 1991.\[8\] Seneff, S., "TINA: A Natural Language System for Spo-ken Language Applications," Computational Linguistics,Vol.
18, No.
1, 61-86, 1992.\[9\] Zue, V., Daly, N., Glass, J., Goodine, D., Leung, H.,Phillips, M., Polifroni, J., Seneff, S. and Soclof, M. "TheCollection and Preliminary Analysis of a SpontaneousSpeech Database," Proc.
DARPA Speech and NL Work-shop: 126-134, October 1989.\[10\] Sakai, S., and Phillips, M., "J-SUMMIT: A JapaneseSegment-Based Speech Recognition System,", Proc.
IC-SLP, 1515-1518, Banff, Alberta, Canada, October 1992.\[11\] Lamel, L., Kassel, R., and Seneff, S., "Speech DatabaseDevelopment: Design and Analysis of the Acoustic-Phonetic Corpus," Proc.
DARPA Speech RecognitionWorkshop, Report No.
SAIC-86/1546, 100-109, Febru-ary 1986.\[12\] Kubala F., et al "BBN BYBLOS and HARC February1992 ATIS Benchmark Results", Proc.
DARPA Speechand NL Workshop, 72-77, Harriman, NY, February1992.\[13\] Itou, K., Hayamizu, S., Tanaka, H., "Continuous SpeechRecognition by Context-Dependent Phonetic HMM andan Efficient Algorithm for Finding N-best Sentence Hy-potheses," Proc.
ICASSP, 21-24, San Francisco, CA,March 1992.
