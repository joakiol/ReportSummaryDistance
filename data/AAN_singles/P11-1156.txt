Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1556?1565,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsExploiting Web-Derived Selectional Preference to Improve StatisticalDependency ParsingGuangyou Zhou, Jun Zhao?, Kang Liu, and Li CaiNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences95 Zhongguancun East Road, Beijing 100190, China{gyzhou,jzhao,kliu,lcai}@nlpr.ia.ac.cnAbstractIn this paper, we present a novel approachwhich incorporates the web-derived selec-tional preferences to improve statistical de-pendency parsing.
Conventional selectionalpreference learning methods have usually fo-cused on word-to-class relations, e.g., a verbselects as its subject a given nominal class.This paper extends previous work to word-to-word selectional preferences by using web-scale data.
Experiments show that web-scaledata improves statistical dependency pars-ing, particularly for long dependency relation-ships.
There is no data like more data, perfor-mance improves log-linearly with the numberof parameters (unique N-grams).
More impor-tantly, when operating on new domains, weshow that using web-derived selectional pref-erences is essential for achieving robust per-formance.1 IntroductionDependency parsing is the task of building depen-dency links between words in a sentence, which hasrecently gained a wide interest in the natural lan-guage processing community.
With the availabil-ity of large-scale annotated corpora such as PennTreebank (Marcus et al, 1993), it is easy to traina high-performance dependency parser using super-vised learning methods.However, current state-of-the-art statistical de-pendency parsers (McDonald et al, 2005; McDon-ald and Pereira, 2006; Hall et al, 2006) tend to have?Correspondence author: jzhao@nlpr.ia.ac.cnlower accuracies for longer dependencies (McDon-ald and Nivre, 2007).
The length of a dependencyfrom word wi to word wj is simply equal to |i ?
j|.Longer dependencies typically represent the mod-ifier of the root or the main verb, internal depen-dencies of longer NPs or PP-attachment in a sen-tence.
Figure 1 shows the F1 score1 relative to thedependency length on the development set by usingthe graph-based dependency parsers (McDonald etal., 2005; McDonald and Pereira, 2006).
We notethat the parsers provide very good results for adja-cent dependencies (96.89% for dependency length=1), while the dependency length increases, the ac-curacies degrade sharply.
These longer dependen-cies are therefore a major opportunity to improve theoverall performance of dependency parsing.
Usu-ally, these longer dependencies can be parsed de-pendent on the specific words involved due to thelimited range of features (e.g., a verb and its mod-ifiers).
Lexical statistics are therefore needed forresolving ambiguous relationships, yet the lexical-ized statistics are sparse and difficult to estimate di-rectly.
To solve this problem, some information withdifferent granularity has been investigated.
Koo etal.
(2008) proposed a semi-supervised dependencyparsing by introducing lexical intermediaries at acoarser level than words themselves via a clustermethod.
This approach, however, ignores the se-lectional preference for word-to-word interactions,such as head-modifier relationship.
Extra resources1Precision represents the percentage of predicted arcs oflength d that are correct, and recall measures the percentageof gold-standard arcs of length d that are correctly predicted.F1 = 2?
precision ?
recall/(precision + recall)15561 5 10 15 20 25 300.70.750.80.850.90.951Dependency LengthF1 Score(%)MST1MST2Figure 1: F score relative to dependency length.beyond the annotated corpora are needed to capturethe bi-lexical relationship at the word-to-word level.Our purpose in this paper is to exploit web-derived selectional preferences to improve the su-pervised statistical dependency parsing.
All of ourlexical statistics are derived from two kinds of web-scale corpus: one is the web, which is the largestdata set that is available for NLP (Keller and Lap-ata, 2003).
Another is a web-scale N-gram corpus,which is a N-gram corpus with N-grams of length 1-5 (Brants and Franz, 2006), we call it Google V1 inthis paper.
The idea is very simple: web-scale datahave large coverage for word pair acquisition.
Byleveraging some assistant data, the dependency pars-ing model can directly utilize the additional informa-tion to capture the word-to-word level relationships.We address two natural and related questions whichsome previous studies leave open:Question I: Is there a benefit in incorporatingweb-derived selectional preference features for sta-tistical dependency parsing, especially for longer de-pendencies?Question II: How well do web-derived selec-tional preferences perform on new domains?For Question I, we systematically assess the valueof using web-scale data in state-of-the-art super-vised dependency parsers.
We compare dependencyparsers that include or exclude selectional prefer-ence features obtained from web-scale corpus.
Tothe best of our knowledge, none of the existing stud-ies directly address long dependencies of depen-dency parsing by using web-scale data.Most statistical parsers are highly domain depen-dent.
For example, the parsers trained on WSJ textperform poorly on Brown corpus.
Some studies haveinvestigated domain adaptation for parsers (Mc-Closky et al, 2006; Daume?
III, 2007; McClosky etal., 2010).
These approaches assume that the parsersknow which domain it is used, and that it has ac-cess to representative data in that domain.
How-ever, in practice, these assumptions are unrealisticin many real applications, such as when processingthe heterogeneous genre of web texts.
In this paperwe incorporate the web-derived selectional prefer-ence features to design our parsers for robust open-domain testing.We conduct the experiments on the English PennTreebank (PTB) (Marcus et al, 1993).
The resultsshow that web-derived selectional preference canimprove the statistical dependency parsing, partic-ularly for long dependency relationships.
More im-portantly, when operating on new domains, the web-derived selectional preference features show greatpotential for achieving robust performance (Section4.3).The remainder of this paper is divided as follows.Section 2 gives a brief introduction of dependencyparsing.
Section 3 describes the web-derived selec-tional preference features.
Experimental evaluationand results are reported in Section 4.
Finally, we dis-cuss related work and draw conclusion in Section 5and Section 6, respectively.2 Dependency ParsingIn dependency parsing, we attempt to build head-modifier (or head-dependent) relations betweenwords in a sentence.
The discriminative parser weused in this paper is based on the part-factoredmodel and features of the MSTParser (McDonald etal., 2005; McDonald and Pereira, 2006; Carreras,2007).
The parsing model can be defined as a con-ditional distribution p(y|x;w) over each projectiveparse tree y for a particular sentence x, parameter-ized by a vector w. The probability of a parse treeisp(y|x;w) = 1Z(x;w)exp{??
?yw ??
(x, ?
)}(1)where Z(x;w) is the partition function and ?
arepart-factored feature functions that include head-1557modifier parts, sibling parts and grandchild parts.Given the training set {(xi, yi)}Ni=1, parameter es-timation for log-linear models generally resolvearound optimization of a regularized conditionallog-likelihood objective w?
= argminwL(w)whereL(w) = ?CN?i=1logp(yi|xi;w) +12||w||2 (2)The parameter C > 0 is a constant dictating thelevel of regularization in the model.
Since objec-tive function L(w) is smooth and convex, which isconvenient for standard gradient-based optimizationtechniques.
In this paper we use the dual exponenti-ated gradient (EG)2 descent, which is a particularlyeffective optimization algorithm for log-linear mod-els (Collins et al, 2008).3 Web-Derived Selectional PreferenceFeaturesIn this paper, we employ two different feature sets:a baseline feature set3 which draw upon ?normal?information source, such as word forms and part-of-speech (POS) without including the web-derived se-lectional preference4 features, a feature set conjoinsthe baseline features and the web-derived selectionalpreference features.3.1 Web-scale resourcesAll of our selectional preference features describedin this paper rely on probabilities derived from unla-beled data.
To use the largest amount of data possi-ble, we exploit web-scale resources.
one is web, N-gram counts are approximated by Google hits.
An-other we use isGoogle V1 (Brants and Franz, 2006).This N-gram corpus records how often each uniquesequence of words occurs.
N-grams appearing 402http://groups.csail.mit.edu/nlp/egstra/3This kind of feature sets are similar to other feature sets inthe literature (McDonald et al, 2005; Carreras, 2007), so wewill not attempt to give a exhaustive description.4Selectional preference tells us which arguments are plau-sible for a particular predicate, one way to determine the se-lectional preference is from co-occurrences of predicates andarguments in text (Bergsma et al, 2008).
In this paper, theselectional preferences have the same meaning with N-grams,which model the word-to-word relationships, rather than onlyconsidering the predicates and arguments relationships.objdetdetrootobjmodsubjFigure 2: An example of a labeled dependency tree.
Thetree contains a special token ?$?
which is always the rootof the tree.
Each arc is directed from head to modifier andhas a label describing the function of the attachment.times or more (1 in 25 billion) are kept, and appearin the n-gram tables.
All n-grams with lower countsare discarded.
Co-occurrence probabilities can becalculated directly from the N-gram counts.3.2 Web-derived N-gram features3.2.1 PMIPrevious work on noun compounds bracketinghas used adjacency model (Resnik, 1993) and de-pendency model (Lauer, 1995) to compute associa-tion statistics between pairs of words.
In this pa-per we generalize the adjacency and dependencymodels by including the pointwise mutual informa-tion (Church and Hanks, 1900) between all pairs ofwords in the dependency tree:PMI(x, y) = log p(?x y?)p(?x?)p(?y?
)(3)where p(?x y?)
is the co-occurrence probabilities.When use the Google V1 corpus, this probabilitiescan be calculated directly from the N-gram counts,while using the Google hits, we send the queries tothe search engine Google5 and all the search queriesare performed as exact matches by using quotationmarks.6The value of these features is the PMI, if it is de-fined.
If the PMI is undefined, following the workof (Pitler et al, 2010), we include one of two binaryfeatures:p(?x y?)
= 0 or p(?x?)
?
p(?y?)
= 0Besides, we also consider the trigram features be-5http://www.google.com/6Google only allows automated querying through theGoogle Web API, this involves obtaining a license key, whichthen restricts the number of queries to a daily quota of 1000.However, we obtained a quota of 20,000 queries per day bysending a request to api-support@google.com for research pur-poses.1558PMI(?hit with?
)xi-word=?hit?, xj-word=?with?, PMI(?hit with?
)xi-word=?hit?, xj-word=?with?, xj-pos=?IN?, PMI(?hit with?
)xi-word=?hit?, xi-pos=?VBD?, xj-word=?with?, PMI(?hit with?
)xi-word=?hit?, b-pos=?ball?, xj-word=?with?, PMI(?hit with?
)xi-word=?hit?, xj-word=?with?, PMI(?hit with?
), dir=R, dist=3.
.
.Table 1: An example of the N-gram PMI features and the conjoin features with the baseline.tween the three words in the dependency tree:PMI(x, y, z) = log p(?x y z?
)p(?x y?
)p(?y z?
)(4)This kinds of trigram features, for example in MST-Parser, which can directly capture the sibling andgrandchild features.We illustrate the PMI features with an exampleof dependency parsing tree in Figure 2.
In decidingthe dependency between the main verb hit and its ar-gument headed preposition with, an example of theN-gram PMI features and the conjoin features withthe baseline are shown in Table 1.3.2.2 PP-attachmentPropositional phrase (PP) attachment is one ofthe hardest problems in English dependency pars-ing.
An English sentence consisting of a subject, averb, and a nominal object followed by a preposi-tional phrase is often ambiguous.
Ambiguity resolu-tion reflects the selectional preference between theverb and noun with their prepositional phrase.
Forexample, considering the following two examples:(1) John hit the ball with the bat.
(2) John hit the ball with the red stripe.In sentence (1), the preposition with depends on themain verb hit; but in sentence (2), the prepositionalphrase is a noun attribute and the preposition withneeds to depends on the word ball.
To resolve thiskind of ambiguity, there needs to measure the attach-ment preference.
We thus have PP-attachment fea-tures that determine the PMI association across thepreposition word ?IN?7:PMIIN (x, z) = logp(?x IN z?
)p(x)(5)7Here, the preposition word ?IN?
(e.g., ?with?, ?in?, .
.
.)
isany token whose part-of-speech is INN-gram feature templateshw, mw, PMI(hw,mw)hw, ht, mw, PMI(hw,mw)hw, mw, mt, PMI(hw,mw)hw, ht, mw, mt, PMI(hw,mw).
.
.hw, mw, swhw, mw, sw, PMI(hw, mw, sw)hw, mw, gwhw, mw, gw, PMI(hw, mw, gw)Table 2: Examples of N-gram feature templates.
Eachentry represents a class of indicator for tuples of informa-tion.
For example, ?hw, mw?
reprsents a class of indi-cator features with one feature for each possible combi-nation of head word and modifier word.
Abbreviations:hw=head word, ht= head POS.
st, gt=likewise for siblingand grandchild.PMIIN (y, z) = logp(?y IN z?
)p(y)(6)where the word x and y are usually verb and noun,z is a noun which directly depends on the preposi-tion word ?IN?.
For example in sentence (1), wewould include the features PMIwith(hit, bat) andPMIwith(ball, bat).
If both PMI features exist andPMIwith(hit, bat) > PMIwith(ball, bat), indicatingto our dependency parsing model that the preposi-tion word with depends on the verb hit is a goodchoice.
While in sentence (2), the features includePMIwith(hit, stripe) and PMIwith(ball, stripe).3.3 N-gram feature templatesWe generate N-gram features by mimicking thetemplate structure of the original baseline features.For example, the baseline feature set includes indi-cators for word-to-word and tag-to-tag interactionsbetween the head and modifier of a dependency.
Inthe N-gram feature set, we correspondingly intro-duce N-gram PMI for word-to-word interactions.1559The N-gram feature set for MSTParser is shownin Table 2.
Following McDonald et al (2005),all features are conjoined with the direction ofattachment as well as the distance between the twowords creating the dependency.
In between N-gramfeatures, we include the form of word trigramsand PMI of the trigrams.
The surrounding wordN-gram features represent the local context of theselectional preference.
Besides, we also presentthe second-order feature templates, including thesibling and grandchild features.
These features aredesigned to disambiguate cases like coordinatingconjunctions and prepositional attachment.
Con-sider the examples we have shown in section 3.2.2,for sentence (1), the dependency graph path featureball ?
with ?
bat should have a lower weightsince ball rarely is modified by bat, but is oftenseen through them (e.g., a higher weight should beassociated with hit ?
with ?
bat).
In contrast,for sentence (2), our N-gram features will tell usthat the prepositional phrase is much more likelyto attach to the noun since the dependency graphpath feature ball ?
with ?
stripe should have ahigh weight due to the high strength of selectionalpreference between ball and stripe.Web-derived selectional preference featuresbased on PMI values are trickier to incorporateinto the dependency parsing model because theyare continuous rather than discrete.
Since all thebaseline features used in the literature (McDonald etal., 2005; Carreras, 2007) take on binary values of 0or 1, there is a ?mis-match?
between the continuousand binary features.
Log-linear dependency parsingmodel is sensitive to inappropriately scaled feature.To solve this problem, we transform the PMIvalues into a more amenable form by replacing thePMI values with their z-score.
The z-score of aPMI value x is x???
, where ?
and ?
are the meanand standard deviation of the PMI distribution,respectively.4 ExperimentsIn order to evaluate the effectiveness of our proposedapproach, we conducted dependency parsing exper-iments in English.
The experiments were performedon the Penn Treebank (PTB) (Marcus et al, 1993),using a standard set of head-selection rules (Yamadaand Matsumoto, 2003) to convert the phrase struc-ture syntax of the Treebank into a dependency treerepresentation, dependency labels were obtained viathe ?Malt?
hard-coded setting.8 We split the Tree-bank into a training set (Sections 2-21), a devel-opment set (Section 22), and several test sets (Sec-tions 0,9 1, 23, and 24).
The part-of-speech tags forthe development and test set were automatically as-signed by the MXPOST tagger10, where the taggerwas trained on the entire training corpus.Web page hits for word pairs and trigrams are ob-tained using a simple heuristic query to the searchengine Google.11 Inflected queries are performedby expanding a bigram or trigram into all its mor-phological forms.
These forms are then submitted asliteral queries, and the resulting hits are summed up.John Carroll?s suite of morphological tools12 is usedto generate inflected forms of verbs and nouns.
Allthe search terms are performed as exact matches byusing quotation marks and submitted to the searchengines in lower case.We measured the performance of the parsers us-ing the following metrics: unlabeled attachmentscore (UAS), labeled attachment score (LAS) andcomplete match (CM), which were defined by Hallet al (2006).
All the metrics are calculated as meanscores per word, and punctuation tokens are consis-tently excluded.4.1 Main resultsThere are some clear trends in the results of Ta-ble 3.
First, performance increases with the orderof the parser: edge-factored model (dep1) has thelowest performance, adding sibling and grandchildrelationships (dep2) significantly increases perfor-mance.
Similar observations regarding the effect ofmodel order have also been made by Carreras (2007)and Koo et al (2008).Second, note that the parsers incorporating the N-gram feature sets consistently outperform the mod-els using the baseline features in all test data sets,regardless of model order or label usage.
Another8http://w3.msi.vxu.se/ nivre/research/MaltXML.html9We removed a single 249-word sentence from Section 0 forcomputational reasons.10http://www.inf.ed.ac.uk/resources/nlp/local doc/MXPOST.html11http://www.google.com/12http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html.1560Sec dep1 +hits +V1 dep2 +hits +V1 dep1-L +hits-L +V1-L dep2-L +hits-L +V1-L00 90.39 90.94 90.91 91.56 92.16 92.16 90.11 90.69 90.67 91.94 92.47 92.4201 91.01 91.60 91.60 92.27 92.89 92.86 90.77 91.39 91.39 91.81 92.38 92.3723 90.82 91.46 91.39 91.98 92.64 92.59 90.30 90.98 90.92 91.24 91.83 91.7724 89.53 90.15 90.13 90.81 91.44 91.41 89.42 90.03 90.02 90.30 90.91 90.89Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24.
Abbreviation:dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived fromthe Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser.
Unlabeled parsers arescored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.finding is that the N-gram features derived fromGoogle hits are slightly better than Google V1 dueto the large N-gram coverage, we will discuss later.As a final note, all the comparisons between the inte-gration of N-gram features and the baseline featuresin Table 3 are mildly significant using the Z-test ofCollins et al (2005) (p < 0.08).Type Systems UAS CMDYamada and Matsumoto (2003) 90.3 38.7McDonald et al (2005) 90.9 37.5McDonald and Pereira (2006) 91.5 42.1Corston-Oliver et al (2006) 90.9 37.5Hall et al (2006) 89.4 36.4Wang et al (2007) 89.2 34.4Carreras et al (2008) 93.5 -GoldBerg and Elhadad (2010)?
91.32 40.41Ours 92.64 46.61CNivre and McDonald (2008)?
92.12 44.37Martins et al (2008)?
92.87 45.51Zhang and Clark (2008) 92.1 45.4SKoo et al (2008) 93.16 -Suzuki et al (2009) 93.79 -Chen et al (2009) 93.16 47.15Table 4: Comparison of our final results with other best-performing systems on the whole Section 23.
TypeD, C and S denote discriminative, combined and semi-supervised systems, respectively.
?
These papers werenot directly reported the results on this data set, we im-plemented the experiments in this paper.To put our results in perspective, we also com-pare them with other best-performing systems in Ta-ble 4.
To facilitate comparisons with previous work,we only use Section 23 as the test data.
The re-sults show that our second order model incorpo-rating the N-gram features (92.64) performs betterthan most previously reported discriminative sys-tems trained on the Treebank.
Carreras et al (2008)reported a very high accuracy using information ofconstituent structure of TAG grammar formalism,while in our system, we do not use such knowl-edge.
When compared to the combined systems, oursystem is better than Nivre and McDonald (2008)and Zhang and Clark (2008), but a slightly worsethan Martins et al (2008).
We also compare ourmethod with the semi-supervised approaches, thesemi-supervised approaches achieved very high ac-curacies by leveraging on large unlabeled data di-rectly into the systems for joint learning and decod-ing, while in our method, we only explore the N-gram features to further improve supervised depen-dency parsing performance.Table 5 shows the details of some other N-gramsources, where NEWS: created from a large set ofnews articles including the Reuters and Gigword(Graff, 2003) corpora.
For a given number of uniqueN-gram, using any of these sources does not havesignificant difference in Figure 3.
Google hits isthe largest N-gram data and shows the best perfor-mance.
The other two are smaller ones, accuraciesincrease linearly with the log of the number of typesin the auxiliary data set.
Similar observations havebeen made by Pitler et al (2010).
We see that therelationship between accuracy and the number of N-gram is not monotonic for Google V1.
The reasonmay be that Google V1 does not make detailed pre-processing, containing many mistakes in the corpus.Although Google hits is noisier, it has very muchlarger coverage of bigrams or trigrams.Some previous studies also found a log-linearrelationship between unlabeled data (Suzuki andIsozaki, 2008; Suzuki et al, 2009; Bergsma et al,2010; Pitler et al, 2010).
We have shown that thistrend continues well for dependency parsing by us-ing web-scale data (NEWS and Google V1).13Google indexes about more than 8 billion pages and eachcontains about 1,000 words on average.1561Corpus # of tokens ?
# of typesNEWS 3.2B 1 3.7BGoogle V1 1,024.9B 40 3.4BGoogle hits13 8,000B 100 -Table 5: N-gram data, with total number of words in theoriginal corpus (in billions, B).
Following (Brants andFranz, 2006; Pitler et al, 2010), we set the frequencythreshold to filter the data ?, and total number of uniqueN-gram (types) remaining in the data.1e4 1e5 1e6 1e7 1e8 1e991.99292.192.292.392.492.592.692.7Number of Unique N-gramsUASScore (%)NEWSGoogle V1Google hitsFigure 3: There is no data like more data.
UAS accu-racy improves with the number of unique N-grams butstill lower than the Google hits.4.2 Improvement relative to dependency lengthThe experiments in (McDonald and Nivre, 2007)showed a negative impact on the dependency pars-ing performance from too long dependencies.
Forour proposed approach, the improvement relativeto dependency length is shown in Figure 4.
Fromthe Figure, it is seen that our method gives observ-able better performance when dependency lengthsare larger than 3.
The results here show that theproposed approach improves the dependency pars-ing performance, particularly for long dependencyrelationships.4.3 Cross-genre testingIn this section, we present the experiments to vali-date the robustness the web-derived selectional pref-erences.
The intent is to understand how well theweb-derived selectional preferences transfer to othersources.The English experiment evaluates the perfor-mance of our proposed approach when it is trained1 10 20 300.750.80.850.90.951Dependency LengthF1 Score(%)MST2MST2+N-gramFigure 4: Dependency length vs. F1 score.on annotated data from one genre of text (WSJ) andis used to parse a test set from a different genre: thebiomedical domain related to cancer (PennBioIE.,2005) with 2,600 parsed sentences.
We divided thedata into 500 for training, 100 for development andothers for testing.
We created five sets of train-ing data with 100, 200, 300, 400, and 500 sen-tences respectively.
Figure 5 plots the UAS ac-curacy as function of training instances.
WSJ isthe performance of our second-order dependencyparser trained on section 2-21; WSJ+N-gram is theperformance of our proposed approach trained onsection 2-21; WSJ+BioMed is the performance ofthe parser trained on WSJ and biomedical data.WSJ+BioMed+N-gram is the performance of ourproposed approach trained on WSJ and biomedicaldata.
The results show that incorporating the web-scale N-gram features can significantly improve thedependency parsing performance, and the improve-ment is much larger than the in-domain testing pre-sented in Section 4.1, the reason may be that web-derived N-gram features do not depend directly ontraining data and thus work better on new domains.4.4 DiscussionIn this paper, we present a novel method to im-prove dependency parsing by using web-scale data.Despite the success, there are still some problemswhich should be discussed.
(1) Google hits is less sparse than Google V1in modeling the word-to-word relationships, butGoogle hits are likely to be noisier than Google V1.It is very appealing to carry out a correlation anal-1562100 150 200 250 300 350 400 450 500808182838485868788UASScore (%)WSJWSJ+N-gramWSJ+BioMedWSJ+BioMed+N-gramFigure 5: Adapting a WSJ parser to biomedical text.WSJ: performance of parser trained only on WSJ;WSJ+N-gram: performance of our proposed approachtrained only on WSJ; WSJ+BioMed: parser trained onWSJ and biomedical text; WSJ+BioMed+N-gram: ourapproach trained on WSJ and biomedical text.ysis to determine whether Google hits and GoogleV1 are highly correlated.
We will leave it for futureresearch.
(2) Veronis (2005) pointed out that there had beena debate about reliability of Google hits due to theinconsistencies of page hits estimates.
However, thisestimate is scale-invariant.
Assume that when thenumber of pages indexed by Google grows, the num-ber of pages containing a given search term goes toa fixed fraction.
This means that if pages indexedby Google doubles, then so do the bigrams or tri-grams frequencies.
Therefore, the estimate becomesstable when the number of indexed pages grows un-boundedly.
Some details are presented in Cilibrasiand Vitanyi (2007).5 Related WorkOur approach is to exploit web-derived selectionalpreferences to improve the dependency parsing.
Theidea of this paper is inspired by the work of Suzukiet al (2009) and Pitler et al (2010).
The former usesthe web-scale data explicitly to create more data fortraining the model; while the latter explores the web-scale N-grams data (Lin et al, 2010) for compoundbracketing disambiguation.
Our research, however,applies the web-scale data (Google hits and GoogleV1) to model the word-to-word dependency rela-tionships rather than compound bracketing disam-biguation.Several previous studies have exploited the web-scale data for word pair acquisition.
Keller andLapata (2003) evaluated the utility of using websearch engine statistics for unseen bigram.
Nakovand Hearst (2005) demonstrated the effectiveness ofusing search engine statistics to improve the nouncompound bracketing.
Volk (2001) exploited theWWWas a corpus to resolve PP attachment ambigu-ities.
Turney (2007) measured the semantic orienta-tion for sentiment classification using co-occurrencestatistics obtained from the search engines.
Bergsmaet al (2010) created robust supervised classifiersvia web-scale N-gram data for adjective ordering,spelling correction, noun compound bracketing andverb part-of-speech disambiguation.
Our approach,however, extends these techniques to dependencyparsing, particularly for long dependency relation-ships, which involves more challenging tasks thanthe previous work.Besides, there are some work exploring the word-to-word co-occurrence derived from the web-scaledata or a fixed size of corpus (Calvo and Gel-bukh, 2004; Calvo and Gelbukh, 2006; Yates et al,2006; Drabek and Zhou, 2000; van Noord, 2007)for PP attachment ambiguities or shallow parsing.Johnson and Riezler (2000) incorporated the lex-ical selectional preference features derived fromBritish National Corpus (Graff, 2003) into a stochas-tic unification-based grammar.
Abekawa and Oku-mura (2006) improved Japanese dependency pars-ing by using the co-occurrence information derivedfrom the results of automatic dependency parsing oflarge-scale corpora.
However, we explore the web-scale data for dependency parsing, the performanceimproves log-linearly with the number of parameters(unique N-grams).
To the best of our knowledge,web-derived selectional preference has not been suc-cessfully applied to dependency parsing.6 ConclusionIn this paper, we present a novel method which in-corporates the web-derived selectional preferencesto improve statistical dependency parsing.
The re-sults show that web-scale data improves the de-pendency parsing, particularly for long dependencyrelationships.
There is no data like more data,performance improves log-linearly with the num-1563ber of parameters (unique N-grams).
More impor-tantly, when operating on new domains, the web-derived selectional preferences show great potentialfor achieving robust performance.AcknowledgmentsThis work was supported by the National NaturalScience Foundation of China (No.
60875041 andNo.
61070106), and CSIDM project (No.
CSIDM-200805) partially funded by a grant from the Na-tional Research Foundation (NRF) administered bythe Media Development Authority (MDA) of Singa-pore.
We thank the anonymous reviewers for theirinsightful comments.ReferencesT.
Abekawa and M. Okumura.
2006.
Japanese depen-dency parsing using co-occurrence information and acombination of case elements.
In Proceedings of ACL-COLING.S.
Bergsma, D. Lin, and R. Goebel.
2008.
Discriminativelearning of selectional preference from unlabeled text.In Proceedings of EMNLP, pages 59-68.S.
Bergsma, E. Pitler, and D. Lin.
2010.
Creating robustsupervised classifier via web-scale N-gram data.
InProceedings of ACL.T.
Brants and Alex Franz.
2006.
The Google Web 1T5-gram Corpus Version 1.1.
LDC2006T13.H.
Calvo and A. Gelbukh.
2004.
Acquiring selec-tional preferences from untagged text for prepositionalphrase attachment disambiguation.
In Proceedings ofVLDB.H.
Calvo and A. Gelbukh.
2006.
DILUCT: An open-source Spanish dependency parser based on rules,heuristics, and selectional preferences.
In LectureNotes in Computer Science 3999, pages 164-175.X.
Carreras.
2007.
Experiments with a higher-order pro-jective dependency parser.
In Proceedings of EMNLP-CoNLL, pages 957-961.X.
Carreras, M. Collins, and T. Koo.
2008.
TAG, dy-namic programming, and the perceptron for efficient,feature-rich parsing.
In Proceedings of CoNLL.E.
Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.2000.
BLLIP 1987-89 WSJ Corpus Release 1, LDCNo.
LDC2000T43.Linguistic Data Consortium.W.
Chen, D. Kawahara, K. Uchimoto, and Torisawa.2009.
Improving dependency parsing with subtreesfrom auto-parsed data.
In Proceedings of EMNLP,pages 570-579.K.
W. Church and P. Hanks.
1900.
Word associationnorms, mutual information, and lexicography.
Com-putational Linguistics, 16(1):22-29.R.
L. Cilibrasi and P. M. B. Vitanyi.
2007.
The Googlesimilarity distance.
IEEE Transaction on Knowledgeand Data Engineering, 19(3):2007. pages 370-383.M.
Collins, A. Globerson, T. Koo, X. Carreras, and P.L.
Bartlett.
2008.
Exponentiated gradient algorithmfor conditional random fields and max-margin markovnetworks.
Journal of Machine Learning Research,pages 1775?1822.M.
Collins, P. Koehn, and I. Kucerova.
2005.
Clause re-structuring for statistical machine translation.
In Pro-ceedings of ACL, pages 531-540.S.
Corston-Oliver, A. Aue, Kevin.
Duh, and E. Ringger.2006.
Multilingual dependency parsing using bayespoint machines.
In Proceedings of NAACL.H.
Daume?
III.
2007.
Frustrating easy domain adaptation.In Proceedings of ACL.E.
F. Drabek and Q. Zhou.
2000.
Using co-occurrencestatistics as an information source for partial parsing ofChinese.
In Proceedings of Second Chinese LanguageProcessing Workshop, ACL, pages 22-28.Y.
GoldBerg and M. Elhadad.
2010.
An efficient algo-rithm for easy-first non-directional dependency pars-ing.
In Proceedings of NAACL, pages 742-750.D.
Graff.
2003.
English Gigaword, LDC2003T05.J.
Hall, J. Nivre, and J. Nilsson.
2006.
Discrimina-tive classifier for deterministic dependency parsing.
InProceedings of ACL, pages 316-323.M.
Johnson and S. Riezler.
2000.
Exploiting auxiliarydistribution in stochastic unification-based garmmars.In Proceedings of NAACL.T.
Koo, X. Carreras, and M. Collins.
2008.
Simplesemi-supervised dependency parsing.
In Proceedingsof ACL, pages 595-603.F.
Keller and M. Lapata.
2003.
Using the web to ob-tain frequencies for unseen bigrams.
ComputationalLinguistics, 29(3):459-484.M.
Lapata and F. Keller.
2005.
Web-based models fornatural language processing.
ACM Transactions onSpeech and Language Processing, 2(1), pages 1-30.M.
Lauer.
1995.
Corpus statistics meet the noun com-pound: some empirical results.
In Proceedings ofACL.D.
K. Lin, H. Church, S. Ji, S. Sekine, D. Yarowsky, S.Bergsma, K. Patil, E. Pitler, E. Lathbury, V Rao, K.Dalwani, and S. Narsale.
2010.
New tools for web-scale n-grams.
In Proceedings of LREC.M.P.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
Computational Linguistics.1564A.
F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.2008.
Stacking dependency parsers.
In Proceedingsof EMNLP, pages 157-166.D.
McClosky, E. Charniak, and M. Johnson.
2006.Reranking and self-training for parser adaptation.
InProceedings of ACL.D.
McClosky, E. Charniak, and M. Johnson.
2010.
Au-tomatic Domain Adapatation for Parsing.
In Proceed-ings of NAACL-HLT.R.
McDonald and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.
InProceedings of EMNLP-CoNLL.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Pro-ceedings of EACL, pages 81-88.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of ACL, pages 91-98.P.
Nakov and M. Hearst.
2005.
Search engine statis-tics beyond the n-gram: application to noun compoundbracketing.
In Proceedings of CoNLL.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProceedings of ACL, pages 950-958.G.
van Noord.
2007.
Using self-trained bilexical pref-erences to improve disambiguation accuracy.
In Pro-ceedings of IWPT, pages 1-10.PennBioIE.
2005.
Mining the bibliome project, 2005.http:bioie.ldc.upenn.edu/.E.
Pitler, S. Bergsma, D. Lin, and K. Church.
2010.
Us-ing web-scale N-grams to improve base NP parsingperformance.
In Proceedings of COLING, pages 886-894.P.
Resnik.
1993.
Selection and information: a class-based approach to lexical relationships.
Ph.D. thesis,University of Pennsylvania.J.
Suzuki, H. Isozaki, X. Carreras, and M. Collins.
2009.An empirical study of semi-supervised structured con-ditional models for dependency parsing.
In Proceed-ings of EMNLP, pages 551-560.J.
Suzuki and H. Isozaki.
2008.
Semi-supervised sequen-tial labeling and segmentation using giga-word scaleunlabeled data.
In Proceedings of ACL, pages 665-673.P.
D. Turney.
2003.
Measuring praise and criticism:Inference of semantic orientation from association.ACM Transactions on Information Systems, 21(4).J.
Veronis.
2005.
Web: Google adjusts its counts.
JeanVeronis?
blog: http://aixtal.blogsplot.com/2005/03/web-google-adjusts-its-count.html.M.
Volk.
2001.
Exploiting the WWW as corpus to re-solve PP attachment ambiguities.
In Proceedings ofthe Corpus Linguistics.Q.
I. Wang, D. Lin, and D. Schuurmans.
2007.
Simpletraining of dependency parsers via structured boosting.In Proceedings of IJCAI, pages 1756-1762.Yamada and Matsumoto.
2003.
Statistical dependencyanalysis with support vector machines.
In Proceedingsof IWPT, pages 195-206.A.
Yates, S. Schoenmackers, and O. Etzioni.
2006.
De-tecting parser errors using web-based semantic filters.In Proceedings of EMNLP, pages 27-34.Y.
Zhang and S. Clark.
2008.
A tale of two parsers: in-vestigating and combining graph-based and transition-based dependency parsing using beam-search.
In Pro-ceedings of EMNLP, pages 562-571.1565
