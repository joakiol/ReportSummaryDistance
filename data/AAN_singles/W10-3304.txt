Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 28?37,Beijing, August 2010Finding Medical Term Variations using Parallel Corpora andDistributional SimilarityLonneke van der PlasDepartment of LinguisticsUniversity of Genevalonneke.vanderplas@unige.chJo?rg TiedemannDepartment of Linguistics and PhilologyUppsala Universityjorg.tiedemann@lingfil.uu.seAbstractWe describe a method for the identifica-tion of medical term variations using par-allel corpora and measures of distribu-tional similarity.
Our approach is basedon automatic word alignment and stan-dard phrase extraction techniques com-monly used in statistical machine transla-tion.
Combined with pattern-based filterswe obtain encouraging results comparedto related approaches using similar data-driven techniques.1 IntroductionOntologies provide a way to formally representknowledge, for example for a specific domain.Ontology building has received a lot of atten-tion in the medical domain.
This interest is re-flected in the existence of numerous medical on-tologies, such as the Unified Medical LanguageSystem (UMLS) (McCray and Hole, 1990) withits metathesaurus, semantic network, and special-ist lexicon.
Although the UMLS includes infor-mation for languages other than English, the cov-erage for other languages is generally smaller.In this paper we describe an approach to acquirelexical information for the Dutch medical domainautomatically.
In the medical domain variations interminology often include multi-word terms suchas aangeboren afwijking ?birth defect?
for con-genitale aandoening ?congenital disorder?.
Thesemultiple ways to refer to the same concept usingdistinct (multi-word) terms are examples of syn-onymy1 but are often referred to as term varia-1Spelling variants are a type of term variations that arenot included in the definition of synonymy.tions.
These term variations could be used to en-hance existing medical ontologies for the Dutchlanguage.Our technique builds on the distributional hy-pothesis, the idea that semantically related wordsare distributed similarly over contexts (Harris,1968).
This is in line with the Firthian saying that,?You shall know a word by the company it keeps.?
(Firth, 1957).
In other words, you can grasp themeaning of a word by looking at its contexts.Context can be defined in many ways.
Previouswork has been mainly concerned with the syntac-tic contexts a word is found in (Lin, 1998; Cur-ran, 2003).
For example, the verbs that are ina subject relation with a particular noun form apart of its context.
In accordance with the Firthiantradition these contexts can be used to determinethe semantic relatedness of words.
For instance,words that occur in a object relation with the verbto drink have something in common: they are liq-uid.
Other work has been concerned with the bag-of-word context, where the context of a word arethe words that are found in its proximity (Wilks etal., 1993; Schu?tze, 1992).Yet another context, that is much less studied, isthe translational context.
The translational contextof a word is the set of translations it gets in otherlanguages.
For example, the translational contextof cat is kat in Dutch and chat in French.
Thisrequires a rather broad understanding of the termcontext.
The idea is that words that share a largenumber of translations are similar.
For exampleboth autumn and fall get the translation herfst inDutch, Herbst in German, and automne in French.This indicates that autumn and fall are synonyms.28A straightforward place to start looking fortranslational context is in bilingual dictionaries.However, these are not always publicly availablefor all languages.
More importantly, dictionar-ies are static and therefore often incomplete re-sources.
We have chosen to automatically acquireword translations in multiple languages from text.Text in this case should be understood as multi-lingual parallel text.
Automatic alignment givesus the translations of a word in multiple lan-guages.
The so-called alignment-based distribu-tional methods described in Van der Plas (2008)apply the translational context for the discoveryof single word synonyms for the general domain.Any multilingual parallel corpus can be used forthis purpose.
It is thus possible to focus ona special domain, such as the medical domainwe are considering in this paper.
The automaticalignment provides us also with domain-specificfrequency information for every translation pair,which is helpful in case words are ambiguous.Aligned parallel corpora have often been usedin the field of word sense discovery, the task ofdiscriminating the different senses words have.The idea behind it is that a word that receives dif-ferent translations might be polysemous.
For ex-ample, a word such as wood receives the transla-tion woud and hout in Dutch, the former referringto an area with many trees and the latter referringto the solid material derived from trees.
Whereasthis type of work is all built upon the divergence oftranslational context, i.e.
one word in the sourcelanguage is translated by many different words inthe target language, we are interested in the con-vergence of translations, i.e.
two words in thesource language receiving the same translation inthe target language.
Of course these two phenom-ena are not independent.
The alleged conversionof the target language might well be a hidden di-version of the source language.
Since the Englishword might be polysemous, the fact that woud andhout in Dutch are both translated in English bywood does not mean that woud and hout in Dutchare synonyms.
However, the use of multiple lan-guages overshadows the noise resulting from pol-ysemy (van der Plas, 2008).Van der Plas (2008) shows that the way thecontext is defined influences the type of lexico-semantic knowledge that is discovered.
Aftergold standard evaluations and manual inspectionthe author concludes that when using translationalcontexts more tight semantic relations such assynonymy are found whereas the conventionalsyntax-based approaches retrieve hypernyms, co-hyponyms, and antonyms of the target word.
Theperformance on synonym acquisition when usingtranslational contexts is almost twice as good aswhen using syntactic contexts, while the amountof data used is much smaller.
Van der Plas (2008)ascribed the fact that the syntax-based method be-haves in this way to the fact that loosely relatedwords, such as wine and beer, are often found inthe same syntactic contexts.
The alignment-basedmethod suffers less from this indiscriminant ac-ceptance because words are typically translated bywords with the same meaning.
The word wine istypically not translated with a word for beveragenor with a word for beer, and neither is good trans-lated with the equivalence of bad.In this paper we are concerned with medicalterm variations that are in fact (multi-word) syn-onyms.
We will use the translational context tocompute similarity between terms.
The transla-tional context is not only very suitable to findtight relations between words, the transition fromsingle-word synonyms to multi-word term varia-tions is also straightforward due to advances inphrase-based machine translation.
We will useword alignment techniques in combination withphrase extraction techniques from statistical ma-chine translation to extract phrases and their trans-lations from a medical parallel corpus.
We com-bine this approach with Part-of-Speech (PoS) pat-terns from the term extraction literature to extractcandidate terms from the phrase tables.
Usingsimilarity measures used in distributional methodswe finally compute ranked lists of term variations.We already noted that these term variationscould be used to enhance existing ontologies forthe Dutch language.
On top of that we believe thatthe multi-lingual method that uses translations ofmulti-word terms in several languages could beused to expand resources built for English withtranslations in other languages (semi-) automati-cally.
This last point falls outside the scope of thispaper.29In the following section we will describe thealignment-based approaches to distributional sim-ilarity.
In section 3 we will describe the method-ology we followed in this paper in detail.
We de-scribe our evaluation in section 4 and discuss theresults in section 5.
Section 6 concludes this pa-per.2 Alignment-based methodsIn this section we explain the alignment-based ap-proaches to distributional similarity.
We will givesome examples of translational context and wewill explain how measures serve to determine thesimilarity of these contexts.
We end this sectionwith a discussion of related work.2.1 Translational contextThe translational context of a word or a multi-word term is the set of translations it gets in otherlanguages.
For the acquisition of translations forthe Dutch medical terms we rely on automaticword alignment in parallel corpora.Figure 1: Example of bidirectional word align-ments of two parallel sentencesFigure 1 illustrates the automatic word alignmentbetween a Dutch and an English phrase as a re-sult of using the IBM alignment models (Brownet al, 1993) implemented in the open-source toolGIZA++ (Och, 2003).
The alignment of two textsis bi-directional.
The Dutch text is aligned tothe English text and vice versa (dotted lines ver-sus continuous lines).
The alignment models pro-duced are asymmetric.
Several heuristics existto combine directional word alignments which isusually called ?symmetrization?.
In order to covermulti-word terms standard phrase extraction tech-niques can be used to move from word alignmentto linked phrases (see section 3.2 for more de-tails).2.2 Measures for computing similarityTranslational co-occurrence vectors are used tofind distributionally similar words.
For ease ofreading, we give an example of a single-wordterm kat in Table 1.
In our current setting theterms can be both single- or multi-word termssuch as werkzame stof ?active ingredient?.
Ev-ery cell in the vector refers to a particular transla-tional co-occurrence type.
For example, kat ?cat?gets the translation Katze in German.
The valueof these cells indicate the number of times the co-occurrence type under consideration is found inthe corpus.Each co-occurrence type has a cell frequency.Likewise each head term has a row frequency.The row frequency of a certain head term is thesum of all its cell frequencies.
In our example therow frequency for the term kat ?cat?
is 65.
Cut-offs for cell and row frequency can be applied todiscard certain infrequent co-occurrence types orhead terms respectively.DE FR IT EN totalKatze chat gatto catkat 17 26 8 13 64Table 1: Translational co-occurrence vector forkat (?cat?)
based on four languagesThe more similar the vectors are, the more dis-tributionally similar the head terms are.
We need away to compare the vectors for any two head termsto be able to express the similarity between themby means of a score.
Various methods can be usedto compute the distributional similarity betweenterms.
We will explain in section 3 what measureswe have chosen in the current experiments.2.3 Related workMultilingual parallel corpora have mostly beenused for tasks related to word sense disambigua-tion such as separation of senses (Resnik andYarowsky, 1997; Dyvik, 1998; Ide et al, 2002).However, taking sense separation as a basis,Dyvik (2002) derives relations such as synonymyand hyponymy by applying the method of se-mantic mirrors.
The paper illustrates how themethod works.
First, different senses are iden-tified on the basis of manual word translationsin sentence-aligned Norwegian-English data (2,6million words in total).
Second, senses aregrouped in semantic fields.
Third, features are30assigned on the basis of inheritance.
Lastly, se-mantic relations such synonymy and hyponymyare detected based on intersection and inclusionamong feature sets .Improving the syntax-based approach for syn-onym identification using bilingual dictionarieshas been discussed in Lin et al (2003) and Wu andZhou (2003).
In the latter parallel corpora are alsoapplied as a reference to assign translation likeli-hoods to candidates derived from the dictionary.Both of them are limited to single-word terms.Some researchers employ multilingual corporafor the automatic acquisition of paraphrases (Shi-mota and Sumita, 2002; Bannard and Callison-Burch, 2005; Callison-Burch, 2008).
The last twoare based on automatic word alignment as is ourapproach.Bannard and Callison-Burch (2005) use amethod that is also rooted in phrase-based statis-tical machine translation.
Translation probabili-ties provide a ranking of candidate paraphrases.These are refined by taking contextual informa-tion into account in the form of a language model.The Europarl corpus (Koehn, 2005) is used.
It hasabout 30 million words per language.
46 Englishphrases are selected as a test set for manual evalu-ation by two judges.
When using automatic align-ment, the precision reached without using contex-tual refinement is 48.9%.
A precision of 55.3%is reached when using context information.
Man-ual alignment improves the performance by 26%.A precision score of 55% is attained when usingmultilingual data.In a more recent publication Callison-Burch(2008) improved this method by using syntac-tic constraints and multiple languages in parallel.We have implemented a combination of Bannardand Callison-Burch (2005) and Callison-Burch(2008), in which we use PoS filters instead ofsyntactic constraints to compare our results with.More details can be found in the Section 5.Apart from methods that use parallel corporamono-lingual pattern-based methods have beenused to find term variations.
Fahmi (2009) ac-quired term variation for the medical domain us-ing a two-step model.
As a first step an initial listof synonyms are extracted using a method adaptedfrom DIPRE (Brin, 99).
During this step syntacticpatterns guide the extraction of candidate terms inthe same way as they will guide the extraction inthis paper.
This first step results in a list of candi-date synonyms that are further filtered following amethod described in Lin et al (2003), which usesWeb pages as an external source to measure thesynonym compatibility hits of each pair.
The pre-cision and recall scores presented in Fahmi (2009)are high.
We will give results for this methodon our test set in Section 5 and refer to it as thepattern- and web-based approach.3 Materials and methodsIn the following subsections we describe the setupfor our experiments.3.1 Data collectionMeasures of distributional similarity usually re-quire large amounts of data.
For the alignmentmethod we need a parallel corpus of reasonablesize with Dutch either as source or as target lan-guage coming from the domain we are interestedin.
Furthermore, we would like to experimentwith various languages aligned to Dutch.The freely available EMEA corpus (Tiede-mann, 2009) includes 22 languages in parallelwith a reasonable size of about 12-14 million to-kens per language.
The entire corpus is alignedat the sentence level for all possible combinationsof languages.
Thus, for acquiring Dutch syn-onyms we have 21 language pairs with Dutch asthe source language.
Each language pair includesabout 1.1 million sentence pairs.
Note that thereis a lot of repetition in EMEA and the numberof unique sentences (sentence fragments) is muchsmaller: around 350,000 sentence pairs per lan-guage pair with about 6-7 million tokens per lan-guage.3.2 Word alignment and phrase extractionFor sentence alignment we applied hunalign(Varga et al, 2005) with the ?realign?
function thatinduces lexical features from the bitext to be com-bined with length based features.
Word alignmenthas been performed using GIZA++ (Och, 2003).We used standard settings defined in the Mosestoolkit (Koehn et al, 2007) to generate Viterbiword alignments of IBM model 4 for sentences31not longer than 80 tokens.
In order to improvethe statistical alignment we used lowercased to-kens and lemmas in case we had them available(produced by the Tree-Tagger (Schmid, 1994) andthe Alpino parser (van Noord, 2006)).We used the grow heuristics to combine theasymmetric word alignments which starts withthe intersection of the two Viterbi alignments andadds block-neighboring points to it in a secondstep.
In this way we obtain high precision linkswith some many-to-many alignments.
Finally weused the phrase extraction tool from Moses to ex-tract phrase correspondences.
Phrases in statisti-cal machine translation are defined as sequencesof consecutive words and phrase extraction refersto the exhaustive extraction of all possible phrasepairs that are consistent with the underlying wordalignment.
Consistency in this case means thatwords in a legal phrase are only aligned to wordsin the corresponding phrase and not to any otherword outside of that phrase.
The extraction mech-anism can be restricted by setting a maximumphrase length which is seven in the default set-tings of Moses.
However, we set the maximumphrase length to four, because we do not expectmany terms in the medical domain to be longerthan 4 words.As explained above, word alignment is carriedout on lowercased and possibly lemmatised ver-sions of the corpus.
However, for phrase extrac-tion, we used surface wordforms and extractedthem along with the part-of-speech (PoS) tags forDutch taken from the corresponding Alpino parsetrees.
This allows us to lowercase all words exceptthe words that have been tagged as name.
Further-more, the inclusion of PoS tags enabled us to fil-ter the resulting phrase table according to typicalpatterns of multi-word terms.
We also removedphrases that consist of only non-alphabetical char-acters.
Note that we rely entirely on automaticprocessing of our data.
Thus, the results fromautomatic tagging, lemmatisation and word align-ment include errors.
Bannard and Callison-Burch(2005) show that when using manual alignmentthe percentage of correct paraphrases significantlyrises from 48.9% to 74.9%.3.3 Selecting candidate termsAs we explained above we can select thosephrases that are more likely to be good termsby using a regular expression over PoS tags.We apply a pattern using adjectives (A), nouns(NN), names (NM) and prepositions (P) as itscomponents based on Justeson and Katz.
(1995)which was adapted to Dutch by Fahmi (2009):((A|NN|NM)+|(((A|NN|NM)*(NN|NM P)?
)(A|NN|NM)*))NN+To explain this regular expression in words, acandidate term is either a sequence of adjectivesand/or nouns and/or names, ending in a noun orname or it consists of two such strings, separatedby a single preposition.After applying the filters and removing all ha-paxes we are left with 9.76 M co-occurrences of aDutch (multi-word) term and a foreign translation.3.4 Comparing vectorsTo compare the vectors of the terms we need asimilarity measures.
We have chosen to describethe functions used in this paper using an extensionof the notation used by Lin (1998), adapted byCurran (2003).
Co-occurrence data is describedas tuples: ?word, language, word?
?, for example,?kat, EN, cat?.Asterisks indicate a set of values ranging overall existing values of that component of the rela-tion tuple.
For example, (w, ?, ?)
denotes for agiven word w all translational contexts it has beenfound in in any language.
For the example ofkat in, this would denote all values for all transla-tional contexts the word is found in: Katze DE:17,chat FR:26 etc.
Everything is defined in termsof co-occurrence data with non-zero frequencies.The set of attributes or features for a given corpusis defined as:(w, ?, ?)
?
{(r, w?)|?
(w, r, w?
)}Each pair yields a frequency value, and the se-quence of values is a vector indexed by r:w?
val-ues, rather than natural numbers.
A subscriptedasterisk indicates that the variables are bound to-gether:?
(wm, ?r, ?w?)
?
(wn, ?r, ?w?
)32The above refers to a dot product of the vectorsfor term wm and term wn summing over all ther:w?
pairs that these two terms have in common.For example we could compare the vectors for katand some other term by applying the dot productto all bound variables.We have limited our experiments to using Co-sine2.
We chose this measure, since it performedbest in experiments reported in Van der Plas(2008).
Cosine is a geometrical measure.
It re-turns the cosine of the angle between the vectorsof the words and is calculated as the dot productof the vectors:Cosine =?
(W1, ?r, ?w?)
?
(W2, ?r, ?w?)??
(W1, ?, ?
)2 ??
(W2, ?, ?
)2If the two words have the same distribution theangle between the vectors is zero.3.5 Post-processingA well-known problem of phrase-based meth-ods to paraphrase or term variation acquisitionis the fact that a large proportion of the termvariations or paraphrases proposed by the sys-tem are super- or sub-strings of the original term(Callison-Burch, 2008).
To remedy this prob-lem we removed all term variations that are ei-ther super- or sub-strings of the original term fromthe lists of candidate term variations output by thesystem.4 EvaluationThere are several evaluation methods available toassess lexico-semantic data.
Curran (2003) distin-guishes two types of evaluation: direct evaluationand indirect evaluation.
Direct evaluation meth-ods compare the semantic relations given by the2Feature weights have been used in previous work forsyntax-based methods to account for the fact that co-occurrences have different information values.
Selectionallyweak (Resnik, 1993) or light verbs such as hebben ?to have?have a lower information value than a verb such as uitpersen?squeeze?
that occurs less frequently.
Although weights thatpromote features with a higher information value work verywell for syntax-based methods, Van der Plas (2008) showedthat weighting only helps to get better synonyms for very in-frequent nouns when applied in alignment-based approaches.In the current setting we do not consider very infrequentterms so we did not use any weighting.system against human performance or expertise.Indirect approaches evaluate the system by mea-suring its performance on a specific task.Since we are not aware of a task in which wecould test the term variations for the Dutch medi-cal domain and ad-hoc human judgments are timeconsuming and expensive, we decided to com-pare against a gold standard.
Thereby denyingthe common knowledge that the drawback of us-ing gold standard evaluations is the fact that goldstandards often prove to be incomplete.
In previ-ous work on synonym acquisition for the generaldomain, Van der Plas and Tiedemann (2006) usedthe synsets in Dutch EuroWordnet (Vossen, 1998)for the evaluation of the proposed synonyms.
Inan evaluation with human judgments, Van der Plasand Tiedemann (2006) showed that in 37% of thecases the majority of the subjects judged the syn-onyms proposed by the system to be correct eventhough they were not found to be synonyms inDutch EuroWordnet.
For evaluating medical termvariations in Dutch there are not many gold stan-dards available.
Moreover, the gold standards thatare available are even less complete than for thegeneral domain.4.1 Gold standardWe have chosen to evaluate the nearest neighboursof the alignment-based method on the term vari-ations from the Elseviers medical encyclopediawhich is intended for the general audience con-taining 379K words.
The encyclopedia was madeavailable to us by Spectrum B.V.3.The test set is comprised of 848 medical termsfrom aambeeld ?incus?
to zwezerik ?thymus?
andtheir term variations.
About 258 of these entriescontain multiword terms.
For most of the termsthe list from Elseviers medical encyclopedia givesonly one term variation, 146 terms have two termvariations and only one term has three variations.For each of these medical terms in the test set thesystem generates a ranked list of term variationsthat will be evaluated against the term variationsin the gold standard.3http://www.kiesbeter.nl/medischeinformatie/335 Results and DiscussionBefore we present our results and give a detailederror analysis we would like to remind the readerof the two methods we compare our results withand give some more detail on the implementationof the second method.5.1 Two methods for comparisonThe first method is the pattern- and web-based ap-proach described in Fahmi (2009).
Note that wedid not re-implement the method, so we were notable to run the method on the same corpus weare using in our experiments.
The corpus usedin Fahmi (2009) is a medical corpus developedin Tilburg University (http://ilk.uvt.nl/rolaquad).It consists of texts from a medical encyclopediaand a medical handbook and contains 57,004 sen-tences.
The system outputs a ranked list of termvariation pairs.
We selected the top-100 pairsthat are output by the system and evaluated theseon the test set described in Subsection 4.1.
Themethod is composed of two main steps.
In thefirst step candidate terms are extracted from thecorpus using a PoS filter, that is similar to thePoS filter we applied.
In the second step pairs ofcandidate term variations are re-ranked on the ba-sis of information from the Web.
Phrasal patternssuch as XorY are used to get synonym compat-ibility hits as opposed to XandY that points tonon-synonymous terms.The second method we compare with is thephrase-based translation method first introducedby Bannard and Callison-Burch (2005).
Statisti-cal word alignment can be used to measure the re-lation between source language items.
Here, onemakes use of the estimated translation likelihoodsof phrases (p(f |e) and p(e|f)) that are used tobuild translation models in standard phrase-basedstatistical machine translation systems (Koehn etal., 2007).
Bannard and Callison-Burch (2005)define the problem of paraphrasing as the follow-ing search problem:e?2 = argmaxe2:e2 6=e1p(e2|e1) wherep(e2|e1) ?
?fp(f |e1)p(e2|f)Certainly, for paraphrasing we are not only inter-ested in e?2 but for the top-ranked paraphrase can-didates but this essentially does not change the al-gorithm.
In their paper, Bannard and Callison-Burch (2005) also show that systematic errors(usually originating from bad word alignments)can be reduced by summing over several languagepairs.e?2 ?
argmaxe2:e2 6=e1?C?fCp(fC |e1)p(e2|fC)This is the approach that we also adapted for ourcomparison.
The only difference in our imple-mentation is that we applied a PoS-filter to extractcandidate terms as explained in section 3.3.
Insome sense this is a sort of syntactic constraint in-troduced in Callison-Burch (2008).
Furthermore,we set the maximum phrase length to 4 and ap-plied the same post-processing as described inSubsection 3.5 to obtain comparable results.5.2 ResultsTable 2 shows the results for our method com-pared with the method adapted from Bannard andCallison-Burch (2005) and the method by Fahmi(2009).
Precision and recall are given at severalvalues of k. At k=1, only the top-1 term varia-tions the system proposes are taken into account.At k=3 the top-3 candidate term variations are in-cluded in the calculations.The last column shows the coverage of the sys-tem.
A coverage of 40% means that for 40% of the850 terms in the test set one or more term varia-tions are found.
Recall is measured for the termscovered by the system.From Table 2 we can read that the method wepropose is able to get about 30% of the term vari-ations right, when only the top-1 candidates areconsidered.
It is able to retrieve roughly a quarterof the term variations provided in the gold stan-dard4.
If we increase k precision goes down andrecall goes up.
This is expected, because the sys-tem proposes a ranked list of candidate term vari-ations so at higher values of k the quality is lower,but more terms from the gold standard are found.4Note that a recall of 100% is not possible, because someterms have several term variations.34Method k=1 k=2 k=3 CoverageP R P R P RPhrase-based Distr.
Sim 28.9 22.8 21.8 32.7 17.3 37.2 40.0Bannard&Callison-Burch (2005) 18.4 15.3 16.9 27.3 13.7 32.3 48.1Fahmi (2009) 38.2 35.1 37.1 35.1 37.1 35.1 4.0Phrase-based Distr.
Sim (hapaxes) 25.4 20.9 20.4 32.1 16.1 36.8 47.8Table 2: Percent precision and recall at several values of k and percent coverage for the method pro-posed in this paper (plus a version including hapaxes), the method adapted from Bannard and Callison-Burch (2005) and the output of the system proposed by Fahmi (2009)In comparison, the scores resulting from ouradapted implementation of Bannard and Callison-Burch (2005) are lower.
They do however, man-age to find more terms from the test set coveringaround 48% of the words in the gold standard.This is due to the cut-off that we use when cre-ating the co-occurrence vector to remove unreli-able data points.
In our approach we discardedhapaxes, whereas for the Bannard and Callison-Burch approach the entire phrase table is used.We therefore ran our system once again withoutthis cut-off.
As expected, the coverage went upin that setting ?
actually to 48% as well.5 How-ever, we can see that the precision and recall re-mained higher, than the scores we got with theimplementation following Bannard and Callison-Burch (2005).
Hence, our vector-based approachseems to outperform the direct use of probabilitiesfrom phrase-based MT.Finally, we also compare our results with thedata set extracted using the pattern- and web-based approach from Fahmi (2009).
The precisionand recall figures of that data set are the highest inour comparison.
However, since the coverage ofthis method is very low (which is not surprisingsince a smaller corpus is used to get these results)the precision and recall are calculated on the ba-sis of a very small number of examples (35 to beprecise).
The results are therefore not very reli-able.
The precision and recall figures presentedin Fahmi (2009), however, point in the same di-rection.
To get an idea of the actual coverage ofthis method we would need to apply this extrac-tion technique to the EMEA corpus.
This is espe-cially difficult due to the heavy use of web queries5The small difference in coverage is due to some mistakesin tokenisation for our method.which makes it problematic to apply this methodto large data sets.5.3 Error analysisThe most important finding we did, when closelyinspecting the output of the system is that many ofthe term variations proposed by the system are notfound in the gold standard, but are in fact correct.Here, we give some examples below:arts, dokter (?doctor?
)ademnood, ademhalingsnood (?respiratory distress?
)aangezichtsverlamming, gelaatsparalyse (?facial paralysis?
)alvleesklierkanker, pancreaskanker (?cancer of the pan-creas?
)The scores given in Table 2 are therefore pes-simistic and a manual evaluation with domain spe-cialist would certainly give us more realistic andprobably much higher scores.
We also found somespelling variants which are usually not covered bythe gold standard.
Look, for instance, at the fol-lowing examples:astma, asthma (?asthma?
)atherosclerose, Artherosclerosis (?atherosclerosis?
)autonoom zenuwstelsel, autonome zenuwstelsel (?autonomicnervous system?
)Some mistakes could have been avoided usingstemming or proper lemmatisation (plurals thatare counted as wrong):abortus, zwangerschapsafbrekingen (?abortion?
)adenoom, adenomen (?adenoma?
)indigestie, spijsverteringsstoornissen (?indigestion?
)After removing the previous cases from the data,some of the remaining mistakes are related to theproblem we mentioned in section 3.5: Phrase-35based methods to paraphrase or term variation ac-quisition have the tendency to propose term vari-ations that are super- or sub-strings of the origi-nal term.
We were able to filter out these super-or sub-strings, but not in cases where a candidateterm is a term variation of a super- or sub-string ofthe original term.
Consider, for example the termbloeddrukverlaging ?blood pressure decrease?
andthe candidate afname ?decrease?, where afname isa synonym for verlaging.6 ConclusionsIn this article we have shown that translationalcontext together with measures of distributionalsimilarity can be used to extract medical term vari-ations from aligned parallel corpora.
Automaticword alignment and phrase extraction techniquesfrom statistical machine translation can be appliedto collect translational variations across variouslanguages which are then used to identify seman-tically related words and phrases.
In this study, weadditionally apply pattern-based filters using part-of-speech labels to focus on particular patterns ofsingle and multi-word terms.
Our method out-performs another alignment-based approach mea-sured on a gold standard taken from a medical en-cyclopedia when applied to the same data set andusing the same PoS filter.
Precision and recall arestill quite poor according to the automatic evalu-ation.
However, manual inspection suggests thatmany candidates are simply misjudged because ofthe low coverage of the gold standard data.
Weare currently setting up a manual evaluation.
Alto-gether our approach provides a promising strategyfor the extraction of term variations using straight-forward and fully automatic techniques.
We be-lieve that our results could be useful for a range ofapplications and resources and that the approachin general is robust and flexible enough to be ap-plied to various languages and domains.AcknowledgementsThe research leading to these results has receivedfunding from the EU FP7 programme (FP7/2007-2013) under grant agreement nr 216594 (CLAS-SIC project: www.classic-project.org).ReferencesBannard, C. and C. Callison-Burch.
2005.
Paraphras-ing with bilingual parallel corpora.
In Proceedingsof the annual Meeting of the Association for Com-putational Linguistics (ACL).Brin, S. 99.
Extracting patterns and relations from theWorld Wide Web.
In WebDB ?98: Selected papersfrom the International Workshop on The World WideWeb and Databases.Brown, P.F., S.A. Della Pietra, V.J.
Della Pietra, andR.L.
Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?296.Callison-Burch, C. 2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP.Curran, J.R. 2003.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.Dyvik, H. 1998.
Translations as semantic mirrors.In Proceedings of Workshop Multilinguality in theLexicon II (ECAI).Dyvik, H. 2002.
Translations as semantic mirrors:from parallel corpus to wordnet.
Language andComputers, Advances in Corpus Linguistics.
Pa-pers from the 23rd International Conference on En-glish Language Research on Computerized Corpora(ICAME 23), 16:311?326.Fahmi, I.
2009.
Automatic Term and Relation Extrac-tion for Medical Question Answering System.
Ph.D.thesis, University of Groningen.Firth, J.R. 1957.
A synopsis of linguistic theory 1930-1955.
Studies in Linguistic Analysis (special vol-ume of the Philological Society), pages 1?32.Harris, Z.S.
1968.
Mathematical structures of lan-guage.
Wiley.Ide, N., T. Erjavec, and D. Tufis.
2002.
Sense discrim-ination with parallel corpora.
In Proceedings of theACL Workshop on Sense Disambiguation: RecentSuccesses and Future Directions.Justeson, J. and S. Katz.
1995.
Technical terminol-ogy: some linguistic properties and an algorithm foridentification in text.
Natural Language Engineer-ing, 1:9?27.Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,M.Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A.Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedingsof the Annual Meeting of the Association for Com-putational Linguistics.36Koehn, P. 2005.
Europarl: A parallel corpus for statis-tical machine translation.
In Proceedings of the MTSummit, pages 79?86, Phuket, Thailand.Lin, D., S. Zhao, L. Qin, and M. Zhou.
2003.
Identify-ing synonyms among distributionally similar words.In Proceedings of the International Joint Confer-ence on Artificial Intelligence (IJCAI).Lin, D. 1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING/ACL.McCray, A. and W. Hole.
1990.
The scope and struc-ture of the first version of the umls semantic net-work.
In Symposium on Computer Applications inPrimary Care (SCAMC-90), IEEE Computer Soci-ety, pages 126?130, , Washington DC, IEEE Com-puter Society.
126-130.Och, F.J. 2003.
GIZA++: Training of sta-tistical translation models.
Available fromhttp://www.isi.edu/?och/GIZA++.html.Resnik, P. and D. Yarowsky.
1997.
A perspective onword sense disambiguation methods and their eval-uation.
In Proceedings of ACL SIGLEX Workshopon Tagging Text with Lexical Semantics: Why, what,and how?Resnik, P. 1993.
Selection and information.
Unpub-lished doctoral thesis, University of Pennsylvania.Schmid, Helmut.
1994.
Probabilistic part-of-speech tagging using decision trees.
In Pro-ceedings of International Conference on NewMethods in Language Processing, pages 44?49,Manchester, UK, September.
http://www.ims.uni-stuttgart.de/?schmid/.Schu?tze, H. 1992.
Dimensions of meaning.
In Pro-ceedings of the ACM/IEEE conference on Super-computing.Shimota, M. and E. Sumita.
2002.
Automatic para-phrasing based on parallel corpus for normalization.In Proceedings of the International Conference onLanguage Resources and Evaluation (LREC).Tiedemann, Jo?rg.
2009.
News from OPUS - A collec-tion of multilingual parallel corpora with tools andinterfaces.
In Nicolov, N., K. Bontcheva, G. An-gelova, and R. Mitkov, editors, Recent Advancesin Natural Language Processing, volume V, pages237?248, Borovets, Bulgaria.
John Benjamins, Am-sterdam/Philadelphia.van der Plas, L. and J. Tiedemann.
2006.
Finding syn-onyms using automatic word alignment and mea-sures of distributional similarity.
In Proceedings ofCOLING/ACL.van der Plas.
2008.
Automatic lexico-semantic acqui-sition for question answering.
Groningen disserta-tions in linguistics.van Noord, G. 2006.
At last parsing is now oper-ational.
In Actes de la 13eme Conference sur leTraitement Automatique des Langues Naturelles.Varga, D., L. Nmeth, P. Halcsy, A. Kornai, V. Trn, andV.
Nagy.
2005.
Parallel corpora for medium densitylanguages.
In Proceedings of RANLP 2005, pages590?596.Vossen, P. 1998.
EuroWordNet a multilingualdatabase with lexical semantic networks.Wilks, Y., D. Fass, Ch.
M. Guo, J. E. McDonald,and B. M. Slator T. Plate.
1993.
Providing ma-chine tractable dictionary tools.
Machine Transla-tion, 5(2):99?154.Wu, H. and M. Zhou.
2003.
Optimizing synonym ex-traction using monolingual and bilingual resources.In Proceedings of the International Workshop onParaphrasing: Paraphrase Acquisition and Appli-cations (IWP).37
