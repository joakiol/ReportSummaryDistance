Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 29?38,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsLexical Tightness and Text ComplexityMichael Flor Beata Beigman Klebanov Kathleen M. SheehanEducational Testing ServicePrinceton, NJ, 08541, USA{mflor,bbeigmanklebanov,ksheehan}@ets.orgAbstractWe present a computational notion of LexicalTightness that measures global cohesion of con-tent words in a text.
Lexical tightness representsthe degree to which a text tends to use wordsthat are highly inter-associated in the language.We demonstrate the utility of this measure forestimating text complexity as measured by USschool grade level designations of texts.
Lexicaltightness strongly correlates with grade level ina collection of expertly rated reading materials.Lexical  tightness  captures  aspects  of  prosecomplexity that are not covered by classic read-ability indexes, especially for literary texts.
Wealso present initial findings on the utility of thismeasure for automated estimation of complex-ity for poetry.1 IntroductionAdequate estimation of text complexity has a longand rich history.
Various readability metrics havebeen designed in the last 100 years (DuBay, 2004).Recent work on computational  estimation of textcomplexity for school- and college-level texts in-cludes (Vajjala and Meurers 2012; Graesser et al2011;  Sheehan et  al.,  2010;  Petersen  and Osten-dorf, 2009; Heilman et al 2006).
Several commer-cial  systems were recently evaluated in the RaceTo The Top competition (Nelson et al  2012) inrelation to the US Common Core State Standardsfor instruction (CCSSI, 2010).A variety of factors influence text  complexity,including vocabulary, sentence structure, academicorientation,  narrativity,  cohesion,  etc.
(Hiebert,2011)  and  corresponding  features  are  utilized  inautomated  systems  of  complexity  evaluation(Vajjala and Meurers, 2012; Graesser et al 2011;Sheehan et al 2010).We focus on text complexity levels expressed asUS school grade level equivalents1.
Our interest isin  quantifying  the  differences  among  texts  (es-say-length  reading  passages)  at  different  gradelevels, for the purposes of automatically evaluatingtext complexity.
The work described in this paperis part of an ongoing project that investigates novelfeatures indicative of text complexity.The paper is organized as follows.
Section 2.1presents our methodology for building word asso-ciation profiles  for  texts.
Section 2.2 defines  themeasure of lexical tightness (LT).
Section 2.3 de-scribes the datasets used in this study.
Sections 3.1and  3.2  present  our  study  of  the  relationshipbetween LT and text complexity.
Section 3.3 de-scribes application to poetry.
Section 3.4 evaluatesan improved measure (LTR).
Section 4 reviews re-lated work.2 Methodology2.1 Word-Association ProfileWe define WAPT ?
a word association profile of atext T ?
as the distribution of association values forall pairs of content words of text T, where the asso-ciation values are estimated from a very large cor-pus of texts.
In this work, WAP is purely illustrat-ive, and sets the stage for lexical tightness.1 For age equivalents of grade levels seehttp://en.wikipedia.org/wiki/Educational_stage29There exists an extensive literature on the use ofword-association measures for NLP, especially fordetection  of  collocations  (Pecina,  2010;  Evert,2008).
The  use  of  pointwise  mutual  information(PMI) with word-space models is noted in (Zhanget al 2012; Baroni and Lenci, 2010; Mitchell andLapata, 2008; Turney, 2001).
We begin with PMI,and provide a modified measure in later sections.To obtain comprehensive information about co-occurrence behavior of words in English, we builda  first-order  co-occurrence  word-space  model(Turney  and  Pantel,  2010;  Baroni  and  Lenci,2010).
The model was generated from a corpus oftexts  of  about  2.5  billion  word  tokens,  countingnon-directed co-occurrence in  a  paragraph,  usingno  distance  coefficients  (Bullinaria  and  Levy,2007).
About 2 billion word tokens come from theGigaword  2003  corpus  (Graff  and  Cieri,  2003).Additional 500 million word tokens come from anin-house corpus containing texts from the genres offiction and popular science.
The matrix of 2.1x2.1million  word  types  and  their  co-occurrence  fre-quencies, as well as single-word frequencies, is ef-ficiently compressed using the TrendStream tech-nology (Flor, 2013), resulting in a database file of4.7GB.
The  same  toolkit  allows  fast  retrieval  ofword  probabilities  and  statistical  associations  forpairs of words.2In this study we use all content word tokens of atext.
We use the OpenNLP tagger3 to POS-tag atext and only take into account nouns, verbs, ad-jective and adverbs.
We further  apply a stop-list(see Appendix A) to filter out auxiliary verbs.To illustrate why WAP is an interesting notion,consider  this  toy  example:  The  texts  ?The  dogbarked and wagged its tail?
vs. ?Green ideas sleepfuriously?.
Their matrices of pairwise word associ-ations are presented in Table 1.
For the first text,all  the  six  content  word  pairs  score  abovePMI=5.5.
On  the  other  hand,  for  ?Green  ideassleep  furiously?,  all  the  six  content  word  pairsscore below PMI=2.2.
The first text puts togetherwords that often go together in English, and thismight be one of the reasons it seems easier to un-derstand than the second text.We use histograms to illustrate word-associationprofiles  for  real  texts,  containing  hundreds  of2 The distributional word-space model includes counts for 2.1million words and 1279 million word pairs (types).
Associ-ation measures are computed on the fly.3 http://opennlp.apache.orgwords.
For  a 60-bin histrogram spanning all  ob-tained PMI values,  the  lowest  bin contains  pairswith PMI?
?5, the highest bin contains pairs withPMI>4.83, while the rest of the bins contain wordpairs  (a,b)  with  -5<PMI(a,b)?4.83.
Figure  1presents  WAP  histograms  for  two  real  textsamples, one for grade level 3 (age 8-9) and onefor grade level 11 (age 16-17).
We observe that theshape of distribution is normal-like.
The distribu-tion of GL3 text is shifted to the right ?
it containsmore highly associated word-pairs than the text ofGL11.
In  a  separate  study  we  investigated  theproperties of WAP distribution (Beigman-Kleban-ov and Flor,  2013).
The normal-like  shape turnsout to be stable across a variety of texts.The dog barked and wagged its tail:dog barked wagged taildog 7.02 7.64 5.57barked 9.18 5.95wagged 9.45tailGreen ideas sleep furiously:green ideas sleep furiouslygreen 0.44 1.47 2.05ideas 1.01 0.94sleep 2.18furiouslyTable 1.
Word association matrices (PMI values) fortwo illustrative examples.-5 -4 -3 -2 -1 0 1 2 3 4 5012345678910TextGL11 TextGL3 PMIPercentageofpairsofwordtokensFigure  1.
Word  Association  Profiles  for  two  sampletexts,  showing 60-bin histograms with smoothed linesinstead of bars.
The last bin of the histogram containsall pairs with PMI>4.83, hence the uptick at PMI=5.302.2 Lexical TightnessIn this section we consider how to derive a singlemeasure to represent each text for further analyses.Given the stable  normal-like  shape of  WAP,  weuse average (mean) value per text for further in-vestigations.
We experimented with several associ-ation measures.Point-wise mutual information is defined as fol-lows (Church and Hanks, 1990):PMI = log2 p ?a ,b ?p ?a?
p ?b?Normalized PMI (Bouma, 2009):NPMI = 2 2( , )log log ( , )( ) ( )p a b p a bp a p b?
???
??
?Unlike the standard PMI (Manning and Sch?tze,1999), NPMI has the property that its values aremostly constrained in the range {-1,1}, it is less in-fluenced by rare extreme values, which is conveni-ent  for  summing  values  over  multiple  pairs  ofwords.
Additional  experiments  on  our  data  haveshown that ignoring negative NPMI values4.
worksbest.
Thus,  we  define  Positive  Normalized  PMI(PNPMI) for a pair of words  a and b as follows:PNPMI(a,b)=  NPMI(a,b)  if NPMI(a,b)>0=  0  if NPMI(a,b)?0or if database has no data forco-occurrence of a and b.5We define Lexical Tightness (LT) of a text asthe mean value of PNPMI for all pairs of content-word tokens in a text.
Thus, if a text has N words,and after filtering we remain with K content words,the total number of pairs is K*(K-1)/2.Lexical tightness represents the degree to whicha text tends to use words that are highly inter-asso-ciated in the language.
We conjecture that lexicallytight texts (with higher values of LT) are easier toread  and  would  thus  correspond  to  lower  gradelevels.4 Ignoring negative values is described by Bullinaria and Levy(2007), also Mohammad and Hirst (2006).5In our text collection, the average percentage of word-pairsnot found in database is 5.5% per text.2.3 DatasetsOur data consists of two sets of passages.
The firstset consists of 1012 passages (636K words) ?
read-ing materials that were used in various tests in stateand national assessment  frameworks in the USA.Part of this set is taken from Sheehan et al(2007)(from testing programs and US state departmentsof education), and part was taken from the Standar-dized State Test Passages set of the Race To TheTop (RTT)  competition  (Nelson et  al.,  2012).
Adistinguishing feature of this dataset is that the ex-act grade level specification was available for eachtext.
Table 2 provides the breakdown by grade andgenre.
Text length in this set ranged between 27and 2848 words, with average 629 words.
Averagetext length in the literary subset was 689 words andin the informational subset 560 words.GradeLevelGenre TotalInf Lit Other1 2 4 1 72 2 4 3 93 49 63 10 1224 54 77 8 1395 47 48 15 1106 44 43 6 937 39 61 6 1068 73 66 19 1589 25 25 3 5310 29 52 2 8311 18 25 0 4312 47 20 22 89Total 429 488 95 1012Table 2.
Counts of texts by grade level and genre, set #1GradeBand GLGenre TotalInf Lit Other2?3 2.5 6 10 4 204?5 4.5 16 10 4 306?8 7 12 16 13 419?10 9.5 12 10 17 3911+ ' 11.5 8 10 20 38Total 54 56 58 168Table  3.
Counts of texts by grade band and genre, fordataset #2.
GL specifies our grade level designation.The second dataset comprises 168 texts (80.8Kword  tokens)  from Appendix  B of  the  CommonCore State Standards (CCSSI, 2010)6, not includ-6 www.corestandards.org/assets/Appendix_B.pdf31ing  poetry  items.
Exact  grade  level  designationsare not  available for this set,  rather the texts areclassified into grade bands, as established by ex-pert  instructors  (Nelson  et  al.,  2012).
Table  3provides the breakdown by grade and genre.
Textlength  in  this  set  ranged  between  99  and  2073words,  with  average  481  words.
Average  textlength in the literary subset was 455 words and inthe informational subset 373 words.Our  collection  is  not  very  large  in  terms  oftypical datasets used in NLP research.
However, ithas two unique facets: grading and genres.
Ratherthan having grade-ranges, set #1 has exact gradedesignations  for each text.
Moreover,  these  wererated by educational experts and used in state andnationwide testing programs.Previous research has emphasized the importan-ce of genre effects for predicting readability andcomplexity (Sheehan et al 2008) and for text ad-aptation (Fountas and Pinnell, 2001).
For all textsin our collection, genre designations (information-al, literary, or 'other') were provided by expert hu-man  judges  (we  used  the  designations  that  wereprepared for the RTT competition,  Nelson et  al.,2012).
The 'other' category included texts that weresomewhere in between literary and informational(e.g.
biographies), as well as speeches, schedules,and manuals.3 Results3.1 Lexical Tightness and Grade LevelCorrelations of lexical tightness with grade levelare shown in Table 4, for sets 1 and 2, the com-bined set and for literary and informational subsets.Our first finding is that lexical tightness has con-siderable  and  statistically  significant  correlationwith grade level, in each dataset, in the combineddataset  and  for  the  specific  subsets.
Notably thecorrelation  between  lexical  tightness  and  gradelevel is negative.
Texts of higher grade levels arelexically less tight, as predicted.Although in these datasets grade level is mode-rately correlated with text length, lexical tightnessremains  considerably and significantly correlatedwith grade level even after removing the influenceof correlations with text length.Our second finding is that lexical tightness has astronger correlation with grade level for the subsetof literary texts (r=-0.610) than for informationaltexts (r=-0.499) in set #1.
A similar pattern existsfor set #2.Figure 2 shows the average LT for each gradelevel,  for  texts  of  set  #1.
As the grade level  in-creases,  average lexical tightness values decreaseconsistently, especially for informational and liter-ary  texts.
There  are  two  'outliers'.
Informationaltexts for grade 12 show a sudden increase in lexic-al tightness.
Also, for genre 'other', grades 9,10,11are underepresented (see Table 2).Subset N Correlation GL&lengthCorrelationGL&LTPartialCorrelationGL&LTSet #1All 1012 0.362 -0.546 -0.472Inf 429 0.396 -0.499 -0.404Lit 488 0.408 -0.610 -0.549Set #2 (Common Core)All 168 0.360 -0.441 -0.373Inf 54 0.406 -0.313 -0.347Lit 56 0.251 -0.546 -0.505Combined setAll 1180 0.339 -0.528 -0.462Inf 483 0.386 -0.472 -0.369Lit 544 0.374 -0.601 -0.545Table  4.
Correlations  of  grade  level  (GL)  with  textlength  and  lexical  tightness  (LT).
Partial  correlationGL&LT  controls  for  text  length.
All  correlations  aresignificant with p<0.04.Figure 3 shows the average LT for each gradeband, for texts of set #2.
Here as well, decrease oflexical tightness is evident with increase of grade3 4 5 6 7 8 9 10 11 120.0400.0450.0500.0550.0600.0650.070Lexical Tightness by Grade LevelInf Lit otherGrade LevelLexicalTightnessFigure 2.
Lexical tighness by grade level and genre,for texts of grades 3-12 in dataset #1.32level.
In this small set, informational texts show arelatively  smooth  decrease  of  LT,  while  literarytexts  show a  sharp  decrease  of  LT in  transitionfrom grade band 4-5 (4.5) to grade band 6-8 (7).Texts labelled as 'other' genre in set #2 are gener-ally less 'tight' than literary or informational.
Alsofor 'other' genre, bands 7-8, 9-10 and 11-12 haveequal lexical tighness.3.2 Grade Level and Readability IndexesWe have also calculated readability indexes foreach passage in sets 1 and 2.
We used well knownreadability formulae: Flesch-Kincaid Grade Level(FKGL: Kincaid et al 1975), Flesch Reading Ease(FRE:  Flesch,  1948),  Gunning-Fog  Index  (GFI:Gunning, 19527), Coleman Liau Index (CLI: Cole-man and Liau, 1975) and Automated ReadabilityIndex (ARI: Senter and Smith, 1967).
All of themare based on measuring the length of words (in let-ters  or  syllables)  and  length  of  sentences  (meannumber  of  words).
For  our  collection,  we  alsocomputed the average sentence length (avgSL, asword count),  average word frequency8 (avgWF ?over all  words),  and average word frequency foronly  content  words  (avgWFCW).
Results  areshown in Table 5.Word frequency has quite low correlation withgrade  level  in  both  datasets.
Readability  indexes7 Using the modern formula, as referenced at http://en.wikipe-dia.org/wiki/Fog_Index8 For word frequency we use the unigrams data from theGoogle Web1T collection (Brants and Franz, 2006).have a strong and consistent correlation with gradelevel.
For  dataset  #1,  readability  indexes  havemuch stronger correlation with grade level for in-formational  texts  (|r| between  0.7  and  0.81)  ascompared  to  literary  texts  (|r| between 0.53  and0.68), and a similar pattern is seen for dataset #2,with overall lower correlation.The correlation of Flesch-Kincaid (FKGL) val-ues with LT are  r=-0.444 for set #1,  r=-0.499 forthe informational subset and  r=-0.541 for literarysubset.
The correlation is r=-0.182 in set #2.All Inf LitSet #1N (texts): 1012 429 488FKGL 0.705 0.807 0.673FRE -0.658 -0.797 -0.629GFI 0.701 0.810 0.673CLI 0.537 0.722 0.537ARI 0.670 0.784 0.653avgSL 0.667 0.705 0.630avgWF 0.205 0.128 0.249avgWFCW 0.039 -0.039 0.095Set #2 (Common Core)N (texts): 168 54 56FKGL 0.487 0.670 0.312FRE9 -0.503 -0.586 -0.398GFI 0.493 0.622 0.356CLI 0.430 0.457 0.440ARI 0.458 0.658 0.298avgSL 0.407 0.701 0.203avgWF 0.100 0.234 -0.109avgWFCW 0.156 -0.053 -0.038Table 5.
Correlations of grade level with readabilityformulae and word frequency.
All correlations apartfrom the italicized ones are significant with p<0.05.Abbreviations are explained in the text.3.3 Lexical Tightness and Readability IndexesTo  evaluate  the  usefulness  of  LT  in  predictinggrade level of passages, we estimate, using dataset#1, a linear regression model where the grade levelis a dependent variable and Flesch-Kincaid scoreand lexical tightness are the two independent vari-ables (features).
First, we checked whether regres-sion model improves over FKGL in the training set(#1).
Then, we tested the regression model estim-ated on 1012 texts of set #1, on 168 texts of set #2.The  results  of  the  regression  model  on  1012texts  of  set  #1  (R2=0.565,  F(2,1009)=655.85,9 Flesch Reading Ease formula is inversely related to gradelevel, hence the negative correlations.2.5 4.5 7 9.5 11.50.0400.0450.0500.0550.0600.0650.070Lexical Tightness by Grade LevelInf Lit otherGrade LevelLexicalTightnessFigure 3.
Lexical tighness by grade band and genre,for texts in dataset #2 (CommonCore).33p<0.0001)  indicate  that  the  amount  of  explainedvariance in the grade levels, as measured by the ad-justed R2 of the model, improved from 0.497 (withFKGL alone,  multiple  r=0.705)  to  0.564 (FKGLwith LT, r=0.752), that is an absolute improvementof 6.7%, and a relative improvement of 13.5%.A separate regression model  was estimated onthe  informational  texts  of  dataset  #1.
The  result(R2=0.664, F(2,426)=420.3, p<0.0001) reveals thatadjusted  R2 of  the  model  improved  from  0.651(with FKGL alone, r=0.807) to 0.663 (FKGL withLT,  r=0.815).
Similarly,  a  regression  model  wasestimated on the literary texts of set #1.
The result(R2=0.522, F(2,485)=264.6, p<0.0001) reveals thatadjusted R2 of the model improved from .453 (withFKGL alone,  r=0.673) to 0.520 (FKGL with LT,r=0.722).
We observe that Flesch-Kincaid formulaworks well on informational texts, better than onliterary  texts;  while  lexical  tightness  correlateswith grade level in the literary texts better than itdoes in the informational texts.
Thus, for informa-tional texts, adding LT to FKGL provides a small(1.2%) but statistically significant improvement forpredicting  GL.
For  literary  texts,  LT  provides  aconsiderable  improvement  (explaining  additional6.3% in the variance).We use the regression model (FKGL & LT) es-timated on the 1012 texts of set #1 and test it on168 texts of set #2.
In dataset #2, FKGL alone cor-relates with grade level with  r=0.487, and the es-timated regression equation achieves correlation ofr=0.574 (the difference between correlation coeffi-cients  is  statistically  significant10,  p<0.001).
Theamount of explained variance rises from 23.7% to33%,  an  almost  10%  improvement  in  absolutescores, and 39% relative improvement over FKGLreadability index alone.3.4 Analyzing PoetrySince poetry is often included in school curricula,automated estimation of poem complexity can beuseful.
Poetry is notoriously hard to analyze com-putationally.
Many poems do not adhere to stand-ard  punctuation  conventions,  have  peculiar  sen-tence structure  (if  sentence boundaries are  indic-ated at all).
However, poems can be tackled withbag-of-words approaches.We have collected 66 poems from Appendix Bof  the  Common  Core  State  Standards  (CCSSI,10Non-independent correlations test, McNemar (1955), p.148.2010).
Just as other materials from that source, thepoems  are  classified  into  grade  bands,  as  estab-lished by expert instructors.
Table 6 provides thebreakdown by grade band.
Text length in this setranges between 21 and 1100 words, the average is182, total word count is 12,030.Grade Band GL N (texts)K-1 1 122?3 2.5 154?5 4.5 96?8 7 119?10 9.5 711+ ' 11.5 12Total 66Table 6.
Counts of poems by grade band,from Common Core Appendix B.GL specifies our grade level designation.We computed lexical tightness for all 66 poemsusing the same procedure as for the two larger textcollections.
For computing correlations, texts fromeach grade band where assigned grade level as lis-ted in Table 6.
For the poetry dataset, LT has ratherlow  correlation  with  grade  level,  r=-0.271(p<0.002).
Text  length  correlation  with  GL  isr=0.218  (p<0.04).
Correlation  of  LT  and  textlength is  r=-0.261 (p<0.02).
Partial correlation ofLT and GL, controlling for text length, is r=-0.227and only almost significant (p=0.069).
In this data-set,  the  correlation  of  Flesch-Kincaid  index(FKGL) with GL is r=0.291 (p<0.003) and FleschReading Ease (FRE)  has  a  stronger  correlation,r=-0.335 (p<0.003).On examining some of the poems, we noted thatthe LT measure does not assign enough importanceto recurrence of words within a text.
For example,PNPMI(voice,  voice)  is  0.208,  while  the  ceilingvalue is 1.0.
We modify the LT measure in the fol-lowing way.
Revised Association Score (RAS) fortwo words a and b:=1.0   if a=b (token repetition)RAS(a,b) =0.9  if a and b are inflectional variants of same lemma= PNPMI(a,b)  otherwiseRevised Lexical Tightness (LTR) for  a text  isaverage of RAS scores for all accepted word pairsin the text (same filtering as before).34For the set of 66 poems, LTR moderately correl-ates with grade level r=-0.353 (p<0.002).
LTR cor-relates  with  text  length  r=0.28  (p<0.02).
Partialcorrelation  of  LTR and  GL,  controlling  for  textlength,  is  r=-0.312 (p<0.012).
This  suggests  thatthe revised measure captures some aspect of com-plexity of the poems.We  re-estimated  the  regression  model,  usingFRE readability and LTR, on all 1012 texts of set#1.
We then applied this model  for prediction ofgrade levels  in  the  set  of  66  poems.
The modelachieves  a  solid  correlation  with  grade  level,r=0.447 (p<0.0001).3.5 Revisiting ProseWe revisit the analysis of our two main datasets,set #1 and #2, using the revised lexical tightnessmeasure  LTR.
Table  7  presents  correlations  ofgrade level with LT and LTR measures.
Evidently,in each case LTR achieves better correlations.Subset N Correlation GL&LTCorrelationGL&LTRSet #1All 1012 -0.546 -0.605Inf 429 -0.499 -0.561Lit 488 -0.610 -0.659Set #2 (Common Core)All 168 -0.441 -0.492Inf 54 -0.310 -0.336Lit 56 -0.546 -0.662Combined setAll 1180 -0.528 -0.587Inf 483 -0.472 -0.531Lit 544 -0.601 -0.655Table 7.
Pearson correlations of grade level (GL) withlexical tightness (LT) and revised lexical tightness(LTR).
All correlations are significant with p<0.04.We re-estimated a linear regression model usingthe grade level as a dependent variable and Flesch-Kincaid score (FKGL) and LTR as the two inde-pendent variables.
The results of regression modelon  1012  texts  of  dataset  #1,  R2=0.583,F(2,1009)=706.07,  p<0.0001,  indicate  that  theamount of explained variance in the grade levels,as measured by the adjusted R2 of the model, im-proved from 0.497 (with FKGL alone, r=0.705) to0.582 (FKGL with LTR, r=0.764), that is absoluteimprovement of 8.5%.
For comparison, the regres-sion model  with LT explained 0.564 of the vari-ance, with 6.7% improvement over FKGL alone.We re-estimated separate regression models forinformational and literary subsets of set #1.
For in-formational  texts,  the  model  has  R2=0.667,F(2,426)=426.8,  p<0.0001,  R2 improving  from0.651 (with FKGL alone,  r=0.807) to adjusted R20.666  (FKGL  with  LTR,  r=0.817).
Regressionmodel with LT brought an improvement of 1.2%,the model with LTR provides 1.5%.A regression model was estimated on the literarytexts  of  dataset  #1.
The  result  (R2=0.560,F(2,485)=308.5, p<0.0001) reveals that adjusted R2of the  model  rose from .453 (with FKGL alone,r=0.673) to 0.558 (FKGL with LT,  r=0.748), thatis 10.5% absolute improvement.
For comparison,LT brought 6.3% improvement.
As with the origin-al LT measure, LTR provides the bulk of improve-ment for evaluation of literary texts.The  regression  model  (FKGL  with  LTR),estimated on all 1012 texts of set #1, is tested on168  texts  of  set  #2.
In  set  #2,  FKGL  alonecorrelates with grade level with  r=0.487, and theprediction formula achieves correlation of r=0.585(the difference between correlation coefficients isstatistically significant,  p<0.001).
The amount  ofexplained variance rises from 23.7% to 34.3%, thatis 10.6% absolute improvement.
Even better resultof predicting grade level in set #2 is achieved usinga  regression  model  of  Flesch  Readability  Ease(FRE) and LTR, estimated on all 1012 texts of set#1.
This  model  achieves  correlation  of r=0.616(p<0.0001) on the 168 texts of set #2, explaining37.9% of the variance.For  complexity  estimation,  in  both  proze  andpoetry, LTR is more effective than simple LT.4 Related WorkTraditional readability formulae use a small num-ber of surface features,  such as the average sen-tence length (a proxy for syntactic complexity) andthe average word length in syllables or characters(a  proxy to  vocabulary difficulty).
Such featuresare considered linguistically shallow, but they aresurprisingly  effective  and  are  still  widely  used(DuBay, 2004;  ?tajner et al 2012).
The formulaeor their features are incorporated in modern read-ability classification systems (Vajjala and Meurers,2012;  Sheehan et  al.,  2010;  Petersen  and Osten-dorf, 2009).Developments  in  computational  linguistics  en-abled inclusion of multiple features for capturing35various  manifestations  of  text-related  readability.Peterson and Ostendorf (2009) compute a varietyof features: vocabulary/lexical (including the clas-sic 'syllables per word'), parse features, includingaverage parse-tree height, noun-phrase count, verb-phrase  count  and  average  count  of  subordinatedclauses.
They use machine learning to train classi-fiers  for  direct  prediction of  grade level.
Vajjalaand  Meurers  (2012)  also  use  machine  learning,with a wide variety of features, including classicfeatures,  parse  features,  and  features  motivatedfrom studies on second language acquisition, suchas Lexical  Density and Type-Token Ratio.
Wordfrequency and its derivations, such as proportion ofrare words, are utilized in many models of com-plexity (Graesser et al 2011; Sheehan et al2010;Stenner et al 2006; Collins-Thompson and Callan,2004).Inspired by psycholinguistic research, two sys-tems have explicitly set to measure textual cohe-sion for estimations of readability and complexity:Coh-Metrix  (Graesser  et  al.,  2011)  and  Sour-ceRater (Sheehan et al 2010).
One notion of cohe-sion involved in those two systems is lexical cohe-sion ?
the amount of lexically/semantically relatedwords in a text.
Some amount of local lexical cohe-sion can be measured via stem overlap of adjacentsentences, with averaging of such metric per text(McNamara et al 2010).
However, Sheehan et al(submitted) demonstrated that such measure is notwell correlated with grade levels.Perhaps closest to our present study is work re-ported in Foltz et al(1998) and McNamara et al(2010).
These studies used Latent Semantic Ana-lysis,  which  reflects  second  order  co-occurrenceassociative relations, to characterize levels of lex-ical similarity for pairs of adjacent sentences with-in  paragraphs,  and  for  all  possible  pairs  of  sen-tences  within  paragraphs.
McNamara  et  al.
haveshown success in distinguishing lower and highercohesion versions of the same text,  but  have notshown  whether  that  approach  systematically  ap-plies for different texts and across grade levels.Our study is a first demonstration that a measureof  lexical  cohesion  based  on  word-associations,and computed globally for the whole text, is an in-dicative  feature  that  varies  systematically  acrossgrade levels.In the theoretical tradition, our work is closest inspirit to Michael Hoey?s theory of lexical priming(Hoey, 2005, 1991), positing that users of languageinternalize patterns of word co-occurrence and usethem in reading, as well as when creating their owntexts.
We suggest that such patterns become richerwith age and education, beginning with the mosttight patterns at early age.5 ConclusionsIn  this  paper  we  defined  a  novel  computationalmeasure, lexical tightness.
It represents the degreeto which a text tends to use words that are highlyinter-associated  in  the  language.
We  interpretlexical tightness as a measure of intra-text globalcohesion.This  study  presented  the  relationship  betweenlexical  tightness  and  text  complexity,  using  twodatasets of reading materials (1180 texts in total),with  expert-assigned  grade  levels.
Lexical  tight-ness has a significant correlation with grade levels:about  -0.6  overall.
The  correlation  is  negative:texts for lower grades are lexically tight, they use ahigher  proportion  of  mildly  and  strongly  inter-associated words; texts for higher grades are lesstight, they use a lesser amount of inter-associatedwords.
The  correlation  of  lexical  tightness  withgrade level is stronger for texts of the literary genre(fiction and stories) than for text belonging to in-formational genre (expositional).While lexical tightness is moderately correlatedwith  readability  indexes,  it  also  captures  someaspects of prose complexity that are not covered byclassic  readability  indexes,  especially for  literarytexts.
Regression analyses  on a  training set  haveshown  that  lexical  tightness  adds  between  6.7%and 8.5% of explained grade level variance on topof  the  best  readability  formula.
The  utility  oflexical  tightness  was  confirmed  by  testing  theregression formula on a held out set of texts.Lexical  tightness  is  also moderately correlatedwith grade level (-0.353) in a small set of poems.In the same set,  Flesch Reading Ease readabilityformula  correlates  with  grade  level  at  -0.335.
Aregression  model  using  that  formula  and  lexicaltightness achieves correlation of  0.447 with gradelevel.
Thus we have shown that  lexical  tightnesshas good potential for analysis of poetry.In future work, we intend to a) evaluate on lar-ger datasets, and b) integrate lexical tightness withother  features  used  for  estimation  of  readability.We also intend to use this or a related measure forevaluation of writing quality.36ReferencesBaroni M. and Lenci A.
2010.
Distributional Memory:A General Framework for Corpus-Based Semantics.Computational Linguistics, 36(4):673-721.Beigman-Klebanov  B.  and  Flor  M.  2013.
WordAssociation  Profiles  and  their  Use  for  AutomatedScoring of Essays.
To appear in  Proceedings of the51th  Annual  Meeting  of  the  Association  forComputational Linguistics, ACL 2013.Bouma  G.  2009.
Normalized  (Pointwise)  MutualInformation in Collocation Extraction.
In:  Chiarcos,Eckart  de  Castilho  &  Stede  (eds), From  Form  toMeaning:  Processing  Texts  Automatically,Proceedings of the Biennial GSCL Conference 2009,31?40, Gunter Narr Verlag: T?bingen.Brants T. and Franz A.
2006.
?Web 1T 5-gram Version1?.
LDC2006T13.
Linguistic  Data  Consortium,Philadelphia, PA.Bullinaria  J.  and  Levy  J.
2007.
Extracting  semanticrepresentations from word co-occurrence statistics: Acomputational  study.
Behavior  Research  Methods,39:510?526.Church K. and Hanks P. 1990.
Word association norms,mutual information and lexicography.
ComputationalLinguistics, 16(1):22?29.Coleman,  M.  and  Liau,  T.  L.  1975.
A  computerreadability  formula  designed  for  machine  scoring.Journal of Applied Psychology, 60:283-284.Collins-Thompson K. and Callan J.
2004.
A languagemodeling approach  to  predicting reading  difficulty.Proceedings of HLT / NAACL 2004, Boston, USA.Common Core State Standards Initiative (CCSSI) 2010.Common core state standards for English languagearts & literacy in history/social studies, science andtechnical subjects.
Washington, DC: CCSSO &National Governors Association.http://www.corestandards.org/ELA-LiteracyDuBay W.H.
2004.
The principles of readability.
ImpactInformation:  Costa Mesa,  CA.
http://www.impact-information.com/impactinfo/readability02.pdfEvert S. 2008.
Corpora and collocations.
In A. L?delingand  M.  Kyt?
(eds.
),  Corpus  Linguistics:  AnInternational  Handbook,  article  58.
Mouton  deGruyter: Berlin.Flesch R. 1948.
A new readability yardstick.
Journal ofApplied Psychology, 32:221-233.Flor M. 2013.
A fast and flexible architecture for verylarge word n-gram datasets.
Natural LanguageEngineering, 19(1):61-93.Foltz P.W., Kintsch W., and Landauer T.K.
1998.
Themeasurement of textual coherence with LatentSemantic Analysis.
Discourse Processes, 25:285-307.Fountas I. and Pinnell G.S.
2001.
Guiding Readers andWriters, Grades 3?6.
Heinemann, Portsmouth, NH.Graesser, A.C., McNamara, D.S., and Kulikowich, J.M.Coh-Metrix: Providing Multilevel Analyses of TextCharacteristics.
Educational  Researcher,  40(5):223?234.Graff,  D.  and  Cieri,  C.  2003.
English  Gigaword.LDC2003T05.
Linguistic  Data  Consortium,Philadelphia, PA.Gunning  R.  1952.
The  technique  of  clear  writing.McGraw-Hill: New York.Heilman,  M.,  Collins-Thompson,  K.,  Callan,  J.  andEskenazi,  M.  2006.
Classroom  success  of  anintelligent  tutoring  system  for  lexical  practice  andreading comprehension.
In  Proceedings of the NinthInternational  Conference  on  Spoken  LanguageProcessing, Pittsburgh, PA.Hiebert,  E.H.  2011.
Using  multiple  sources  ofinformation in establishing text complexity.
ReadingResearch Report 11.03.
TextProject Inc., Santa Cruz,CA.Hoey  M.  1991.
Patterns  of  Lexis  in  Text.
OxfordUniversity Press.Hoey M. 2005.
Lexical Priming: A new theory of wordsand language.
Routledge, London.Kincaid  J.P.,  Fishburne  R.P.
Jr,  Rogers  R.L.,  andChissom B.S.
1975.
Derivation  of  new readabilityformulas   for  Navy  enlisted  personnel.
ResearchBranch  Report  8-75,  Millington,  TN:  NavalTechnical  Training,  U.S.
Naval  Air  Station,Memphis, TN.Manning,  C.  and  Sch?tze  H.  1999.
Foundations  ofStatistical Natural Language Processing.
MIT Press,Cambridge, MA.McNamara,  D.S.,  Louwerse,  M.M.,  McCarthy,  P.M.and  Graesser  A.C.  2010.
Coh-metrix:  Capturinglinguistic features of cohesion.
Discourse Processes,47:292-330.McNemar,  Q.
1955.
Psychological  Statistics.
NewYork, John Wiley & Sons.Mitchell J. and Lapata M.  2008.
Vector-based modelsof semantic composition.
In Proceedings of the 46thAnnual  Meeting  of  the  Association  forComputational Linguistics, 236?244, Columbus, OH.Mohammad  S.  and  Hirst  G.  2006.
DistributionalMeasures  of  Concept-Distance:  A  Task-orientedEvaluation.
In  Proceedings of the 2006 Conferenceon  Empirical  Methods  in  Natural  LanguageProcessing (EMNLP 2006), 35?43.Nelson  J.,  Perfetti  C.,  Liben  D.,  and Liben  M. 2012.Measures of Text Difficulty: Testing their PredictiveValue  for  Grade  Levels  and  Student  Performance.Student  Achievement  Partners.
Available  fromhttp://www.ccsso.org/Documents/2012/Measures%20ofText%20Difficulty_final.2012.pd  fPecina  P.  2010.
Lexical  association  measures  andcollocation  extraction.
Language  Resources  &Evaluation, 44:137?158.37Petersen  S.E.
and  Ostendorf  M.  2009.
A  machinelearning  approach  to  reading  level  assessment.Computer Speech and Language, 23: 89?109.Senter  R.J.  and  Smith  E.A.
1967.
AutomatedReadability Index.
Report AMRL-TR-6620.
Wright-Patterson Air Force Base, USA.Sheehan K.M.,  Kostin I.,  Napolitano D.,  and Flor  M.TextEvaluator:  Helping  Teachers  and  TestDevelopers  Select  Texts for  Use in Instruction andAssessment.
Submitted  to  The  Elementary  SchoolJournal (Special Issue: Text Complexity).Sheehan K.M., Kostin I., Futagi Y., and Flor M. 2010.Generating automated text complexity classificationsthat  are  aligned  with  targeted  text  complexitystandards.
(ETS RR-10-28).
ETS, Princeton, NJ.Sheehan K.M., Kostin I., and Futagi Y.
2008.
When dostandard  approaches  for  measuring  vocabularydifficulty,  syntactic  complexity  and  referentialcohesion yield biased estimates of text difficulty?
InB.C.
Love,  K.  McRae,  &  V.M.
Sloutsky  (eds.
),Proceedings  of  the 30th Annual  Conference  of  theCognitive Science Society, Washington DC.Sheehan  K.M.,  Kostin  I.,  and  Futagi  Y.
2007.SourceFinder:  A  construct-driven  approach  forlocating  appropriately  targeted  readingcomprehension  source  texts.
In  Proceedings  of  the2007  workshop  of  the  International  SpeechCommunication Association,  Special  Interest  Groupon Speech and Language Technology in Education,Farmington, PA.?tajner S., Evans R., Or?san C., and Mitkov R. 2012.What  Can  Readability  Measures  Really  Tell  UsAbout Text Complexity?
In proceedings of workshopon   Natural  Language  Processing  for  ImprovingTextual Accessibility (NLP4ITA 2012), 14-22.Stenner A.J.,  Burdick H., Sanford E., and Burdick D.2006.
How  accurate  are  Lexile  text  measures?Journal of Applied Measurement, 7(3):307-322.Turney  P.D.
2001.
Mining  the  Web  for  Synonyms:PMI-IR versus  LSA on TOEFL.
In  proceedings  ofEuropean  Conference  on  Machine  Learning,  491?502, Freiburg, Germany.Turney  P.D.
and  Pantel  P.  2010.
From Frequency  toMeaning:  Vector  Space  Models  of  Semantics.Journal  of  Artificial  Intelligence  Research,  37:141-188.Vajjala  S.  and  Meurers  D.  2012.
On  Improving  theAccuracy of Readability Classification using Insightsfrom Second Language Acquisition.
In  proceedingsof  The 7th Workshop on the Innovative Use of NLPfor  Building  Educational  Applications,  (BEA-7),163?173, ACL.Zhang  Z.,  Gentile  A.L.,  Ciravegna  F.  2012.
Recentadvances in methods of lexical semantic relatedness?
a  survey.
Natural  Language  Engineering,  DOI:http://dx.doi.org/10.1017/S1351324912000125Appendix AThe list of stopwords utilized in this study:a, an, the, at, as, by, for, from, in, on, of, off, up,to, out, over, if, then, than, with, have, had, has,can,  could,  do,  did,  does,  be,  am,  are,  is,  was,were, would, will,  it,  this,  that,  no, not,  yes, but,all,  and,  or,  any,  so,  every,  we,  us,  you,  also,  sNote that most of these words would be excludedby POS filtering.
However, the full  stop list  wasapplied anyway.38
