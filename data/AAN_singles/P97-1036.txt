Unification-based Multimodal IntegrationMichae l  Johnston ,  Ph i l ip  R .
Cohen,  Dav id  McGee,Sharon  L. Ov ia t t ,  James  A .
P i t tman,  I ra  Smi thCenter  for Human Computer  Communicat ionDepar tment  of Computer  Science and Engineer ingOregon Graduate  Inst i tute ,  PO BOX 91000, Por t land,  OR 97291, USA.
{johnston, pcohen, dmcgee, oviatt, jay, ira}?cse, ogi.
eduAbst ractRecent empirical research as shown con-clusive advantages of multimodal interac-tion over speech-only interaction for map-based tasks.
This paper describes a mul-timodal language processing architecturewhich supports interfaces allowing simulta-neous input from speech and gesture recog-nition.
Integration of spoken and gesturalinput is driven by unification of typed fea-ture structures representing the semanticcontributions of the different modes.
Thisintegration method allows the componentmodalities to mutually compensate for eachothers' errors.
It is implemented in Quick-Set, a multimodal (pen/voice) system thatenables users to set up and control dis-tributed interactive simulations.1 In t roduct ionBy providing a number of channels through whichinformation may pass between user and computer,multimodal interfaces promise to significantly in-crease the bandwidth and fluidity of the interfacebetween humans and machines.
In this work, we areconcerned with the addition of multimodal input tothe interface.
In particular, we focus on interfaceswhich support simultaneous input from speech andpen, utilizing speech recognition and recognition ofgestures and drawings made with a pen on a complexvisual display, such as a map.Our focus on multimodal interfaces i motivated,in part, by the trend toward portable computing de-vices for which complex graphical user interfaces areinfeasible.
For such devices, speech and gesture willbe the primary means of user input.
Recent em-pirical results (Oviatt 1996) demonstrate clear taskperformance and user preference advantages for mul-timodal interfaces over speech only interfaces, in par-ticular for spatial tasks such as those involving maps.Specifically, in a within-subject experiment duringwhich the same users performed the same tasks invarious conditions using only speech, only pen, orboth speech and pen-based input, users' multimodalinput to maps resulted in 10% faster task comple-tion time, 23% fewer words, 35% fewer spoken dis-fluencies, and 36% fewer task errors compared tounimodal spoken input.
Of the user errors, 48% in-volved location errors on the map--errors that werenearly eliminated by the simple ability to use pen-based input.
Finally, 100% of users indicated a pref-erence for multimodal interaction over speech-onlyinteraction with maps.
These results indicate thatfor map-based tasks, users would both perform bet-ter and be more satisfied when using a multimodalinterface.
As an illustrative example, in the dis-tributed simulation application we describe in thispaper, one user task is to add a "phase line" to amap.
In the existing unimodal interface for this ap-plication (CommandTalk, Moore 1997), this is ac-complished with a spoken utterance such as 'CRE-ATE A LINE FROM COORDINATES NINE FOURTHREE NINE THREE ONE TO NINE EIGHTNINE NINE FIVE ZERO AND CALL IT PHASELINE GREEN'.
In contrast he same task can be ac-complished by saying 'PHASE LINE GREEN' andsimultaneously drawing the gesture in Figure 1.JFigure 1: Line gestureThe multimodal command involves peech recog-nition of only a three word phrase, while the equiva-lent unimodal speech command involves recognitionof a complex twenty four word expression.
Further-more, using unimodal speech to indicate more com-281plex spatial features uch as routes and areas is prac-tically infeasible if accuracy of shape is important.Another significant advantage of multimodal overunimodal speech is that it allows the user to switchmodes when environmental noise or security con-cerns make speech an unacceptable input medium,or for avoiding and repairing recognition errors (Ovi-att and Van Gent 1996).
Multimodality also offersthe potential for input modes to mutually compen-sate for each others' errors.
We will demonstrate:~'~.,, in our system, multimodal integration allowsspeech input to compensate for errors in gesturerecognition and vice versa.Systems capable of integration of speech and ges-ture have existed since the early 80's.
One of thefirst such systems was the "Put-That-There" sys-tem (Bolt 1980).
However, in the sixteen years sincethen, research on multimodal integration has notyielded a reusable scalable architecture for the con-struction of multimodal systems that integrate ges-ture and voice.
There are four major limiting factorsin previous approaches to multimodal integration:(1) The majority of approaches limit the bandwidthof the gestural mode to simple deictic pointinggestures made with a mouse (Neal and Shapiro1991, Cohen 1991, Cohen 1992, Brison andVigouroux (ms.), Wauchope 1994) or with thehand (Koons et al19931).
(ii) Most previous approaches have been primarilyspeech-driven ~ , treating esture as a secondarydependent mode (Neal and Shapiro 1991, Co-hen 1991, Cohen 1992, Brison and Vigouroux(ms.), Koons et al1993, Wauchope 1994).
Inthese systems, integration of gesture is triggeredby the appearance of expressions in the speechstream whose reference needs to be resolved,such as definite and deictic noun phrases (e.g.
'this one', 'the red cube').
(iii) None of the existing approaches provide a well-understood generally applicable common mean-ing representation for the different modes, or,(iv) A general and formally-welldefined mechanismfor multimodal integration.I Koons et al1993 describe two different systems.
Thefirst uses input from hand gestures and eye gaze in orderto aid in determining the reference of noun phrases in thespeech stream.
The second allows users to manipulateobjects in a blocks world using iconic and pantomimicgestures in addition to deictic gestures.~More precisely, they are 'verbal anguage'-driven.Either spoken or typed linguistic expressions are thedriving force of interpretation.We present an approach to multimodal integra-tion which overcomes these limiting factors.
A widebase of continuous gestural input is supported andintegration may be driven by either mode.
Typedfeature structures (Carpenter 1992) are used to pro-vide a clearly defined and well understood commonmeaning representation for the modes, and multi-modal integration is accomplished through unifica-tion.2 Qu ickset :  A Mu l t imoda l  In ter facefo r  D is t r ibuted  In teract iveS imulat ionThe initial application of our multimodal interfacearchitecture has been in the development of theQuickSet system, an interface for setting up andinteracting with distributed interactive simulations.QuickSet provides a portal into LeatherNet 3, a sim-ulation system used for the training of US MarineCorps platoon leaders.
LeatherNet simulates train-ing exercises using the ModSAF simulator (Courte-manche and Ceranowicz 1995) and supports 3D vi-sualization of the simulated exercises using Com-mandVu (Clarkson and Yi 1996).
SRI Interna-tional's CommandTalk provides a unimodal spokeninterface to LeatherNet (Moore et al1997).QuickSet is a distributed system consisting of acollection of agents that communicate through theOpen Agent Architecture 4 (Cohen et al1994).
Itruns on both desktop and hand-held PCs under Win-dows 95, communicating over wired and wirelessLANs (respectively), or modem links.
The wire-less hand-held unit is a 3-1b Fujitsu Stylistic 1000(Figure 2).
We have also developed a Java-basedQuickSet agent that provides a portal to the simula-tion over the World Wide Web.
The QuickSet userinterface displays a map of the terrain on which thesimulated military exercise is to take place (Figure2).
The user can gesture and draw directly on themap with the pen and simultaneously issue spokencommands.
Units and objectives can be laid downon the map by speaking their name and gesturingon the desired location.
The map can also be an-notated with line features uch as barbed wire andfortified lines, and area features uch as minefieldsand landing zones.
These are created by drawing theappropriate spatial feature on the map and speak-3LeatherNet is currently being developed by theNaval Command, Control and Ocean Surveillance Cen-ter (NCCOSC) Research, Development, Test and Eval-uation Division (NRaD) in coordination with a numberof contractors.4Open Agent Architecture is a trademark of SRIInternational.282Figure 2: The QuickSet user interfaceing its name.
Units, objectives, and lines can alsobe generated using unimodal gestures by drawingtheir map symbols in the desired location.
Orderscan be assigned to units, for example, in Figure 2an M1A1 platoon on the bottom left has been as-signed a route to follow.
This order is created mul-timodally by drawing the curved route and saying'WHISKEY FOUR SIX FOLLOW THIS ROUTE'.As entities are created and assigned orders they aredisplayed on the UI and automatically instantiatedin a simulation database maintained by the ModSAFsimulator.Speech recognition operates in either a click-to-speak mode, in which the microphone is activatedwhen the pen is placed on the screen, or open micro-phone mode.
The speech recognition agent is builtusing a continuous peaker-independent recognizercommercially available from IBM.When the user draws or gestures on the map, theresulting electronic 'ink' is passed to a gesture recog-nition agent, which utilizes both a neural networkand a set of hidden Markov models.
The ink is size-normalized, centered in a 2D image, and fed into theneural network as pixels, as well as being smoothed,resampled, converted to deltas, and fed to the HMMrecognizer.
The gesture recognizer currently recog-nizes a total of twenty six different gestures, some ofwhich are illustrated in Figure 3.
They include var-ious military map symbols uch as platoon, mortar,and fortified line, editing gestures uch as deletion,and spatial features uch as routes and areas.i - G linetank mechanizedplatoon companyfo~ied  linearea pointdeletion mortarbarbed wireFigure 3: Example symbols and gesturesAs with all recognition technologies, gesturerecognition may result in errors.
One of the factors283contributing to this is that routes and areas do nothave signature shapes that can be used to identifythem and are frequently confused (Figure 4).O g3Figure 4: Pen drawings of routes and areasAnother contributing factor is that users' pen in-put is often sloppy (Figure 5) and map symbols canbe confused among themselves and with route andarea gestures.mortar tank deletion mechanizedplatoon companyFigure 5: Typical pen input from real usersGiven the potential for error, the gesture recog-nizer issues not just a single interpretation, but aseries of potential interpretations ranked with re-spect to probability.
The correct interpretation isfrequently determined as a result of multimodal in-tegration, as illustrated below 5.3 A Un i f i ca t ion -based  Arch i tec turefo r  Mu l t imoda l  In tegrat ionOne the most significant challenges facing the devel-opment of effective multimodal interfaces concernsthe integration of input from different modes.
In-put signals from each of the modes can be assignedmeanings.
The problem is to work out how to com-bine the meanings contribute d by each of the modesin order to determine what the user actually intendsto communicate.To model this integration, we utilize a unificationoperation over typed feature structures (Carpenter1990, 1992, Pollard and Sag 1987, Calder 1987, KingSSee Wahlster 1991 for discussion of the role of dialogin resolving ambiguous gestures.1989, Moshier 1988).
Unification is an operationthat determines the consistency of two pieces of par-tial information, and if they are consistent combinesthem into a single result.
As such, it is ideally suitedto the task at hand, in which we want to determinewhether a given piece of gestural input is compatiblewith a given piece of spoken input, and if they arecompatible, to combine the two inputs into a singleresult that can be interpreted by the system.The use of feature structures as a semantic rep-resentation framework facilitates the specification ofpartial meanings.
Spoken or gestural input whichpartially specifies a command can be representedas an underspecified feature structure in which cer-tain features are not instantiated.
The adoption oftyped feature structures facilitates the statement ofconstraints on integration.
For example, if a givenspeech input can be integrated with a line gesture,it can be assigned a feature structure with an under-specified location feature whose value is required tobe of type line.IArt  IFigure 6: Multimodal integration architectureFigure 6 presents the main agents involved in theQuickSet system.
Spoken and gestural input orig-inates in the user interface client agent and it ispassed on to the speech recognition and gesturerecognition agents respectively.
The natural lan-guage agent uses a parser implemented in Prolog toparse strings that originate from the speech recog-nition agent and assign typed feature structures to284them.
The potential interpretations of gesture fromthe gesture recognition agent are also represented astyped feature structures.
The multimodal integra-tion agent determines and ranks potential unifica-tions of spoken and gestural input and issues com-plete commands to the bridge agent.
The bridgeagent accepts commands in the form of typed fea-ture structures and translates them into commandsfor whichever applications the system is providingan interface to.For example, if the user utters 'M1A1 PLA-TOON', the name of a particular type of tank pla-toon, the natural anguage agent assigns this phrasethe feature structure in Figure 7.
The type of eachfeature structure is indicated in italics at its bottomright or left corner.object : echelon :platoonun i tcreate_unit location : \] pointFigure 7: Feature structure for 'M1A1 PLATOON'Since QuickSet is a task-based system directed to-ward setting up a scenario for simulation, this phraseis interpreted as a partially specified unit creationcommand.
Before it can be executed, it needs a lo-cation feature indicating where to create the unit,which is provided by the user's gesturing on thescreen.
The user's ink is likely to be assigned a num-ber of interpretations, for example, both a point in-terpretation and a line interpretation, which the ges-ture recognition agent assigns typed feature struc-tures (see Figures 8 and 9).
Interpretations of ges-tures as location features are assigned ageneral com-mand type which unifies with all of commands takenby the system.\[ \[xcoord 9 30 \] \]location : xcoord :94365command pointFigure 8: Point interpretation of gesturecommand\[ icoor it \] 1 \[(95301, 94360), location : (95305, 94365),(95310, 94380)\] ~in?Figure 9: Line interpretation of gestureThe task of the integrator agent is to field incom-ing typed feature structures representing interpreta-tions of speech and of gesture, identify the best po-tential interpretation, multimodal or unimodal, andissue a typed feature structure representing the pre-ferred interpretation to the bridge agent, which willexecute the command.
This involves parsing of thespeech and gesture streams in order to determine po-tential multimodal integrations.
Two factors guidethis: tagging of speech and gesture as either com-plete or partial and examination of time stamps as-sociated with speech and gesture.Speech or gesture input is marked as complete if itprovides a full command specification and thereforedoes not need to be integrated with another mode.Speech or gesture marked as partial needs to be in-tegrated with another mode in order to derive anexecutable command.Empirical study of the nature of multimodal inter-action has shown that speech typically follows ges-ture within a window of a three to four seconds whilegesture following speech is very uncommon (Oviattet al97).
Therefore, in our multimodal architec-ture, the integrator temporally licenses integrationof speech and gesture if their time intervals overlap,or if the onset of the speech signal is within a brieftime window following the end of gesture.
Speechand gesture are integrated appropriately even if theintegrator agent receives them in a different orderfrom their actual order of occurrence.
If speech istemporally compatible with gesture, in this respect,then the integrator takes the sets of interpretationsfor both speech and gesture, and for each pairingin the product set attempts to unify the two fea-ture structures.
The probability of each multimodalinterpretation i the resulting set licensed by unifi-cation is determined by multiplying the probabilitiesassigned to the speech and gesture interpretations.In the example case above, both speech andgesture have only partial interpretations, one forspeech, and two for gesture.
Since the speech in-terpretation (Figure 7) requires its location featureto be of type point, only unification with the pointinterpretation of the gesture will succeed and bepassed on as a valid multimodal interpretation (Fig-ure 10).create_unittype:mlal  \]object : echelon : platoon J =nitxcoord : 95305 \]location : xcoord :94365 J poi,~tFigure 10: Multimodal interpretationThe ambiguity of interpretation of the gesture wasresolved by integration with speech which in thiscase required a location feature of type point.
Ifthe spoken command had instead been 'BARBED285WIRE' it would have been assigned the featurestructure in Figure 11.
This structure would onlyunify with the line interpretation of gesture result-ing in the interpretation i  Figure 12.create_line\[ style:barbed_wire \] \]object : color : redlocation: \[ \]li,~ , .
.
.
.
b.~Figure 11: Feature structure for 'BARBED WIRE'create_lineobject:location :\[ :to~le :: b Tbed-wire \] ,,,~_ob ~\[oorot \]\[(95301, 9436o),(95305, 94365),(95310, 94380)\] .,~Figure 12: Multimodal line creationSimilarly, if the spoken command escribed anarea, for example an 'ANTI TANK MINEFIELD' ,it would only unify with an interpretation f gestureas an area designation.
In each case the unification-based integration strategy compensates for errors ingesture recognition through type constraints on thevalues of features.Gesture also compensates for errors in speechrecognition.
In the open microphone mode, wherethe user does not have to gesture in order to speak,spurious peech recognition errors are more commonthan with click-to-speak, but are frequently rejectedby the system because of the absence of a compatiblegesture for integration.
For example, if the systemspuriously recognizes 'M1A1 PLATOON', but thereis no overlapping or immediately preceding estureto provide the location, the speech will be ignored.The architecture also supports election among n-best speech recognition results on the basis of thepreferred gesture recognition.
In the future, n-bestrecognition results will be available from the recog-nizer, and we will further examine the potential forgesture to help select among speech recognition al-ternatives.Since speech may follow gesture, and since even si-multaneously produced speech and gesture are pro-cessed sequentially, the integrator cannot executewhat appears to be a complete unimodal commandon receiving it, in case it is immediately followed byinput from the other mode suggesting a multimodalinterpretation.
If a given speech or gesture inputhas a set of interpretations including both partialand complete interpretations, the integrator agentwaits for an incoming signal from the other mode.
Ifno signal is forthcoming from the other mode withinthe time window, or if interpretations from the othermode do not integrate with any interpretations inthe set, then the best of the complete unimodalinterpretations from the original set is sent to thebridge agent.For example, the gesture in Figure 13 is used forunimodal specification of the location of a fortifiedline.
If recognition is successful the gesture agentwould assign the gesture an interpretation like thatin Figure 14./kgXdl..OFigure 13: Fortified line gesturecreate J ine?bject: \[ \ ] .b j  .
.
.
.location :style : fortified._finecolor : bluecoordlist :\[(93000, 94360),(93025, 94365),Figure 14: Unimodal fortified line feature structureHowever, it might also receive an additional po-tential interpretation asa location feature of a moregeneral line type (Figure 15).location :command linecoordhst:\[(93000,94360),(93025,94365),i 3112, 94362)\]Figure 15: Line feature structureOn receiving this set of interpretations, the in-tegrator cannot immediately execute the completeinterpretation to create a fortified line, even if it isassigned the highest probability by the recognizer,since speech contradicting this may immediately fol-low.
For example, if overlapping with or just afterthe gesture, the user said 'BARBED WIRE' thenthe line feature interpretation would be preferred.
Ifspeech does not follow within the three to four sec-ond window, or following speech does not integratewith the gesture, then the unimodal interpretation286is chosen.
This approach embodies a preference formultimodal interpretations over unimodal ones, mo-tivated by the possibility of unintended completeunimodal interpretations of gestures.
After moredetailed empirical investigation, this will be refinedso that the possibility of integration weighs in favorof the multimodal interpretation, but it can still bebeaten by a unimodal gestural interpretation with asignificantly higher probability.4 ConclusionWe have presented an architecture for multimodalinterfaces in which integration of speech and ges-ture is mediated and constrained by a unificationoperation over typed feature structures.
Our ap-proach supports a full spectrum of gestural input,not just deixis.
It also can be driven by either modeand enables a wide and flexible range of interactions.Complete commands can originate in a single modeyielding unimodal spoken and gestural commands,or in a combination of modes yielding multimodalcommands, in which speech and gesture are able tocontribute ither the predicate or the arguments ofthe command.
This architecture allows the modesto synergistically mutual compensate for each oth-ers' errors.
We have informally observed that inte-gration with speech does succeed in resolving am-biguous gestures.
In the majority of cases, gestureswill have multiple interpretations, but this is rarelyapparent o the user, because the erroneous inter-pretations of gesture are screened out by the unifi-cation process.
We have also observed that in theopen microphone mode multimodality allows erro-neous speech recognition results to be screened out.For the application tasks described here, we haveobserved a reduction in the length and complexityof spoken input, compared to the unimodal spokeninterface to LeatherNet, informally reconfirming theempirical results of Oviatt et al1997.
For this fam-ily of applications at least, it appears to be the casethat as part of a multimodal architecture, currentspeech recognition technology is sufficiently robustto support easy-to-use interfaces.Vo and Wood 1996 present an approach to mul-timodal integration similar in spirit to that pre-sented here in that it accepts a variety of gesturesand is not solely speech-driven.
However, we be-lieve that unification of typed feature structuresprovides a more general, formally well-understood,and reusable mechanism for multimodal integrationthan the frame merging strategy that they describe.Cheyer and Julia (1995) sketch a system based onOviatt's (1996) results but describe neither the in-tegration strategy nor multimodal compensation.QuickSet has undergone a form of pro-active val-uation in that its design is informed by detailed pre-dictive modeling of how users interact multimodallyand it incorporates the results of existing empiricalstudies of multimodal interaction (Oviatt 1996, Ovi-att et al1997).
It has also undergone participatorydesign and user testing with the US Marine Corpsat their training base at 29 Palms, California, withthe US Army at the Royal Dragon exercise at FortBragg, North Carolina, and as part of the CommandCenter of the Future at NRaD.Our initial application of this architecture hasbeen to map-based tasks such as distributed simula-tion.
It supports a fully-implemented usable systemin which hundreds of different kinds of entities canbe created and manipulated.
We believe that theunification-based method described here will read-ily scale to larger tasks and is sufficiently generalto support a wide variety of other application areas,including raphically-based information systems andediting of textual and graphical content.
The archi-tecture has already been successfully re-deployed inthe construction of multimodal interface to healthcare information.We are actively pursuing incorporation ofstatistically-derived heuristics and a more sophisti-cated dialogue model into the integration architec-ture.
We are also developing a capability for auto-matic logging of spoken and gestural input in orderto collect more fine-grained empirical data on thenature of multimodal interaction.5 AcknowledgmentsThis work is supported in part by the Informa-tion Technology and Information Systems offices ofDARPA under contract number DABT63-95-C-007,in part by ONR grant number N00014-95-1-1164,and has been done in collaboration with the USNavy's NCCOSC RDT&E Division (NRaD), AscentTechnologies, Mitre Corp., MRJ Corp., and SRI In-ternational.Re ferencesBolt, R. A., 1980.
"Put-That-There" :Voice and ges-ture at the graphics interface.
Computer Graph-ics, 14.3:262-270.Brison, E., and N. Vigouroux.
(unpublished ms.).Multimodal references: A generic fusion pro-cess.
URIT-URA CNRS.
Universit Paul Sabatier,Toulouse, France.Calder, J.
1987.
Typed unification for natural an-guage processing.
In E. Klein and J. van Benthem,287editors, Categories, Polymorphisms, and Unifica-tion, pages 65-72.
Centre for Cognitive Science,University of Edinburgh, Edinburgh.Carpenter, R. 1990.
Typed feature structures: In-heritance, (In)equality, and Extensionality.
InW.
Daelemans and G. Gazdar, editors, Proceed-ings of the ITK Workshop: Inheritance in NaturalLanguage Processing, pages 9-18, Tilburg.
Insti-tute for Language Technology and Artificial Intel-ligence, Tilburg University, Tilburg.Carpenter, R. 1992.
The logic of typed feature struc-tures.
Cambridge University Press, Cambridge,England.Cheyer, A., and L. Julia.
1995.
Multimodal maps:An agent-based approach.
In International Con-ference on Cooperative Multimodal Communica-tion (CMC/95), pages 24-26, May 1995.
Eind-hoven, The Netherlands.Clarkson, J. D., and J. Yi.
1996.
LeatherNet: Asynthetic forces tactical training system for theUSMC commander.
In Proceedings of the SixthConference on Computer Generated Forces andBehavioral Representation, pages 275-281.
Insti-tute for simulation and training.
Technical ReportIST-TR-96-18.Cohen, P. R. 1991.
Integrated interfaces for decisionsupport with simulation.
In B. Nelson, W. D. Kel-ton, and G. M. Clark, editors, Proceedings of theWinter Simulation Conference, pages 1066-1072.ACM, New York.Cohen, P. R. 1992.
The role of natural language in amultimodal interface.
In Proceedings of UIST'92,pages 143-149.
ACM Press, New York.Cohen, P. R., A. Cheyer, M. Wang, and S. C. Baeg.1994.
An open agent architecture.
In WorkingNotes of the AAA1 Spring Symposium on Soft-ware Agents (March 21-22, Stanford University,Stanford, California), pages 1-8.Courtemanche, A. J., and A. Ceranowicz.
1995.ModSAF development s atus.
In Proceedingsof the Fifth Conference on Computer GeneratedForces and Behavioral Representation, pages 3-13,May 9-11, Orlando, Florida.
University of CentralFlorida, Florida.King, P. 1989.
A logical formalism for head-drivenphrase structure grammar.
Ph.D. Thesis, Univer-sity of Manchester, Manchester, England.Koons, D. B., C. J. Sparrell, and K. R. Thorisson.1993.
Integrating simultaneous input from speech,gaze, and hand gestures.
In M. T. Maybury, edi-tor, Intelligent Multimedia Interfaces, pages 257-276.
AAAI Press/ MIT Press, Cambridge, Mas-sachusetts.Moore, R. C., J. Dowding, H. Bratt, J. M. Gawron,Y.
Gorfu, and A. Cheyer 1997.
CommandTalk:A Spoken-Language Interface for Battlefield Sim-ulations.
In Proceedings of Fifth Conference onApplied Natural Language Processing, pages 1-7,Washington, D.C. Association for ComputationalLinguistics, Morristown, New Jersey.Moshier, D. 1988.
Extensions to unification gram-mar for the description of programming languages.Ph.D.
Thesis, University of Michigan, Ann Arbor,Michigan.Neal, J. G., and S. C. Shapiro.
1991.
Intelligentmulti-media nterface technology.
In J. W. Sul-livan and S. W. Tyler, editors, Intelligent UserInterfaces, pages 45-68.
ACM Press, Frontier Se-ries, Addison Wesley Publishing Co., New York,New York.Oviatt, S. L. 1996.
Multimodal interfaces for dy-namic interactive maps.
In Proceedings of Con-ference on Human Factors in Computing Systems:CHI '96, pages 95-102, Vancouver, Canada.
ACMPress, New York.Oviatt, S. L., A. DeAngeli, and K. Kuhn.
1997.
In-tegration and synchronization f input modes dur-ing multimodal human-computer interaction.
InProceedings of the Conference on Human Factorsin Computing Systems: CH\[ '97, pages 415-422,Atlanta, Georgia.
ACM Press, New York.Oviatt, S. L., and R. van Gent.
1996.
Error resolu-tion during multimodal human-computer interac-tion.
In Proceedings of International Conferenceon Spoken Language Processing, vol 1, pages 204-207, Philadelphia, Pennsylvania.Pollard, C. J., and I.
A.
Sag.
1987.
Information-based syntax and semantics: Volume I, Funda-mentals., Volume 13 of CSLI Lecture Notes.
Cen-ter for the Study of Language and Information,Stanford University, Stanford, California.Vo, M. T., and C. Wood.
1996.
Building an appli-cation framework for speech and pen input inte-gration in multimodal learning interfaces.
In Pro-ceedings of International Conference on Acoustics,Speech, and Signal Processing, Atlanta, GA.Wahlster, W. 1991.
User and discourse models formultimodal communication.
I  J. Sullivan and S.Tyler, editors, Intelligent User Interfaces, ACMPress, Addison Wesley Publishing Co., New York,New York.Wauchope, K. 1994.
Eucalyptus: Integratingnatural language input with a graphical userinterface.
Naval Research Laboratory, ReportNRL/FR/5510-94-9711.288
