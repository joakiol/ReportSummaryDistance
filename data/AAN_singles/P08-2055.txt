Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 217?220,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsComputing Confidence Scores for All Sub Parse TreesFeng Lin Fuliang WengDepartment of Computer Science and Engineering Research and Technology CenterFudan University Robert Bosch LLCShanghai 200433, P.R.
China Palo Alto, CA, 94303, USAfenglin@fudan.edu.cn fuliang.weng@us.bosch.comAbstractComputing confidence scores for applica-tions, such as dialogue system, informa-tion retrieving and extraction, is an activeresearch area.
However, its focus has beenprimarily on computing word-, concept-,or utterance-level confidences.
Motivatedby the need from sophisticated dialoguesystems for more effective dialogs, wegeneralize the confidence annotation to allthe subtrees, the first effort in this line ofresearch.
The other contribution of thiswork is that we incorporated novel longdistance features to address challenges incomputing multi-level confidence scores.Using Conditional Maximum Entropy(CME) classifier with all the selected fea-tures, we reached an annotation error rateof 26.0% in the SWBD corpus, comparedwith a subtree error rate of 41.91%, aclosely related benchmark with theCharniak parser from (Kahn et al, 2005).1 IntroductionThere has been a good amount of interest in ob-taining confidence scores for improving word orutterance accuracy, dialogue systems, informationretrieving & extraction, and machine translation(Zhang and Rudnicky, 2001; Guillevic et al, 2002;Gabsdil et al, 2003; Ueffing et al, 2007).However, these confidence scores are limited torelatively simple systems, such as command-n-control dialogue systems.
For more sophisticateddialogue systems (e.g., Weng et al, 2007), identi-fication of reliable phrases must be performed atdifferent granularity to ensure effective andfriendly dialogues.
For example, in a request ofMP3 music domain ?Play a rock song by Cher?, ifwe want to communicate to the user that the sys-tem is not confident of the phrase ?a rock song,?the confidence scores for each word, the artistname ?Cher,?
and the whole sentence would not beenough.
For tasks of information extraction, whenextracted content has internal structures, confi-dence scores for such phrases are very useful forreliable returns.As a first attempt in this research, we generalizeconfidence annotation algorithms to all sub parsetrees and tested on a human-human conversationalcorpus, the SWBD.
Technically, we also introducea set of long distance features to address the chal-lenges in computing multi-level confidence scores.This paper is organized as follows:  Section 2 in-troduces the tasks and the representation for parsetrees; Section 3 presents the features used in thealgorithm; Section 4 describes the experiments inthe SWBD corpus; Section 5 concludes the paper.2 Computing Confidence Scores forParse TreesThe confidence of a sub-tree is defined as the pos-terior probability of its correctness, given all theavailable information.
It is )|( xcorrectisspP  ?
theposterior probability that the parse sub-tree sp iscorrect, given related information x.
In real appli-cations, typically a threshold or cutoff t is needed:??
?<?txcorrectisspPifincorrectxcorrectisspPifcorrectissp)|(,)|(,(1)217In this work, the probability )|( xcorrectisspP iscalculated using CME modeling framework:( ) ( ) ( )???????
?= ?jjj yxfxZxyP ,exp1| ?
(2)where y?
{sp is correct, sp is incorrect}, x is thesyntactic context of the parse sub-tree sp, fj are thefeatures, ?j are the corresponding weights, and Z(x)is the normalization factor.The parse trees used in our system are lexical-ized binary trees.
However, the confidence compu-tation is independent of any parsing method usedin generating the parse tree as long as it generatesthe binary dependency relations.
An example ofthe lexicalized binary trees is given in Figure 1,where three important components are illustrated:the left sub-tree, the right sub-trees, and themarked head and dependency relation.Because the parse tree is already given, a bot-tom-up left-right algorithm is used to traversethrough the parse tree: for each subtree, computeits confidence, and annotate it as correct or wrong.3 FeaturesFour major categories of features are used, includ-ing, words, POS tags, scores and syntactic infor-mation.
Due to the space limitation, we only give adetailed description of the most important one1,lexical-syntactic features.The lexical-syntactic features include lexical,POS tag, and syntactic features.
Word and POS tagfeatures include the head and modifier words of theparse sub-tree and the two children of the root, aswell as their combinations.
The POS tags and hier-archical POS tags of the corresponding words are1 The other important one is the dependency score, which isthe conditional probability of the last dependency relation inthe subtree, given its left and right child treesalso considered to avoid data sparseness.
Theadopted hierarchical tags are: Verb-related (V),Noun-related (N), Adjectives (ADJ), and Adverbs(ADV), similar to (Zhang et al 2006).Long distance structural features in statisticalparsing lead to significant improvements (Collinset al, 2000; Charniak et al, 2005).
We incorporatesome of the reported features in the feature spaceto be explored, and they are enriched with differentPOS categories and grammatical types.
Two eam-ples are given below.One example is the Single-Level Joint Headand Dependency Relation (SL-JHD).
This featureis pairing the head word of a given sub-tree with itslast dependency relation.
To address the datasparseness problem, two additional SL-JHD fea-tures are considered: a pair of the POS tag of thehead of a given sub-tree and its dependency rela-tion, a pair of the hierarchical POS tag of the headof a given sub-tree and its dependency relation.
Forexample, for the top node in Figure 2, (restaurantNCOMP), (NN, NCOMP), and (N, NCOMP) arethe examples for the three SL-JHD features.
Tocompute the confidence score of the sub-tree, weinclude the three JHD features for the top node,and the JHD features for its two children.
Thus, forthe sub-tree in Figure 2, the following nine JHDfeatures are included in the feature space, i.e., (res-taurant NCOMP), (NN, NCOMP), (N, NCOMP),(restaurant NMOD), (NN NMOD), (N NMOD),(with POBJ), (IN POBJ), and (ADV POBJ).The other example feature is Multi-Level JointHead and Dependency Relation (ML-JHD), whichtakes into consideration the dependency relationsat multiple levels.
This feature is an extension ofSL-JHD.
Instead of including only single levelhead and dependency relations, the ML-JHD fea-ture includes the hierarchical POS tag of the headand dependency relations for all the levels of agiven sub-tree.
For example, given the sub-tree inFigure 3, (NCOMP, N, NMOD, N, NMOD, N,POBJ, ADV, NMOD, N) is the ML-JHD featurefor the top node (marked by the dashed circle).In addition, three types of features are included:dependency relations, neighbors of the head of thecurrent subtree, and the sizes of the sub-tree and itsleft and right children.
The dependency relationsinclude the top one in the subtree.
The neighborsare typically within a preset distance from the headword.
The sizes refer to the numbers of words ornon-terminals in the subtree and its children.Figure 1.
Example of parse sub-tree?s structure forphrase ?three star Chinese restaurant?starNNNP (star)NMODNP (restaurant)NMODLeft Sub-treethreeCDrestaurantNNNP (restaurant)NMODChineseNNPRight Sub-tree218Figure 3.
ML-JHD Featuresa NNPCUISINENAME restaurantNNNP (restaurant) DTNP (restaurant)NMODwith JJgood serviceNNNP (service)INPP (with)NMODNMOD POBJNP (restaurant)NCOMP4 ExperimentsExperiments were conducted to see the perform-ance of our algorithm in human to human dialogs ?the ultimate goal of a dialogue system.
In our work,we use a version of the Charniak?s parser from(Aug. 16, 2005) to parse the re-segmented SWBDcorpus (Kahn et al, 2005), and extract the parsesub-trees from the parse trees as experimental data.The parser?s training procedure is the same as(Kahn et al, 2005).
The only difference is that theyuse golden edits in the parsing experiments whilewe delete all the edits in the UW Switchboard cor-pus.
The F-score of the parsing result of theCharniak parser without edits is 88.24%.The Charniak parser without edits is used toparse the training data, testing data and tuning data.We remove the sentences with only one word anddelete the interjections in the hypothesis parse trees.Finally, we extract parse sub-trees from these hy-pothesis parse trees.
Based on the gold parse trees,a parse sub-tree is labeled with 1 (correct), if it hasall the words, their POS tags and syntactic struc-tures correct.
Otherwise, it is 0 (incorrect).
Amongthe 424,614 parse sub-trees from the training data,316,182 sub-trees are labeled with 1; among the38,774 parse sub-trees from testing data, 22,521ones are labeled with 1; and among the 67,464parse sub-trees from the tuning data, 38,619 onesare labeled with 1.
In the testing data, there are5,590 sentences, and the percentage of completebracket match2 is 57.11%, and the percentage ofparse sub-trees with correct labels at the sentencelevel is 48.57%.
The percentage of correct parsesub-trees is lower than that of the complete bracketmatch due to its stricter requirements.Table 1 shows our analysis of the testing data.There, the first column indicates the phrase lengthcategories from the parse sub-trees.
Among all theparse trees in the test data, 82.84% (first two rows)have a length equal to or shorter than 10 words.We converted the original parse sub-trees from theCharniak parser into binary trees.Length Sub-tree Types Number RatioCorrect 21,593 55.70%<=10Incorrect 10,525 27.14%Correct 928 2.39%>10Incorrect 5,728 14.77%Table 1.
The analysis of testing data.We apply the model (2) from section 2 on theabove data for all the following experiments.
Theperformance is measured based on the confidenceannotation error rate (Zhang and Rudnicky, 2001).SubtreesOfNumberTotalncorrectnotatedAsISubtreesAnOfNumberErrorAnnot =.Two sets of experiments are designed to demon-strate the improvements of our confidence comput-ing algorithm, as well as the newly introducedfeatures (see Table 2 and Table 3).Experiments were conducted to evaluate the ef-fectiveness of each feature category for the sub-tree level confidence annotation on SWBD corpus(Table 2).
The baseline system uses the conven-tional features: words and POS tags.
Additionalfeature categories are included separately.
The syn-tactic feature category shows the biggest improve-ment among all the categories.To see the additive effect of the feature spacesfor the multi-level confidence annotation, anotherset of experiments were performed (Table 3).Three feature spaces are included incrementally:dependency score, hierarchical tags and syntacticfeatures.
Each category provides sizable reductionin error rate.
Totally, it reduces the error rate by2 Complete bracket match is the percentage of sentences wherebracketing recall and precision are both 100%.Figure 2.
SL-JHD Featuresa NNPCUISINENAME restaurantNNNP (restaurant) DTNP (restaurant)NMODwith JJgood serviceNNNP (service)INPP (with)NMODNMOD POBJNP (restaurant)NCOMP Relation_Head219Feature Space Description Annot.
Error Relative Error DecreaseBaseline Base features: Words, POS tag 36.2% \Set 1 Base features + Dependency score 32.8% 9.4%Set 2 Base features + Hierarchical tags 35.3% 2.5%Set 3 Base features + Syntactic features 29.3% 19.1%Table 2.
Comparison of different feature space (on SWBD corpus).Feature Space Description Annot.
Error Relative Error DecreaseBaseline Base features: Words, POS tag 36.2% \Set 4 + Dependency score 32.8% 9.4%Set 5 + Dependency score + hierarchical tags   32.7% 9.7%Set 6 + Dependency score + hierarchical tags   + syntactic features 26.0% 28.2%Table 3.
Summary of experiment results with different feature space (on SWBD corpus).10.2%, corresponding to 28.2% of a relative errorreduction over the baseline.
The best result of an-notation error rate is 26% for Switchboard data,which is significantly lower than the 41.91% sub-tree parsing error rate (see Table 1: 41.91% =27.14%+14.77%).
So, our algorithm would alsohelp the best parsing algorithms during rescoring(Charniak et al, 2005; McClosky et al, 2006).We list the performance of the parse sub-treeswith different lengths for Set 6 in Table 4, usingthe F-score as the evaluation measure.Length Sub-tree Category F-scoreCorrect 82.3% <=10Incorrect 45.9%Correct 33.1% >10Incorrect 86.1%Table 4.
F-scores for various lengths in Set 15.The F-score difference between the ones withcorrect labels and the ones with incorrect labels aresignificant.
We suspect that it is caused by the dif-ferent amount of training data.
Therefore, we sim-ply duplicated the training data for the sub-treeswith incorrect labels.
For the sub-trees of lengthequal to or less than 10 words, this training methodleads to a 79.8% F-score for correct labels, and a61.4% F-score for incorrect labels, which is muchmore balanced than those in the first set of results.5 ConclusionIn this paper, we generalized confidence annota-tion algorithms to multiple-level parse trees anddemonstrated the significant benefits of using longdistance features in SWBD corpora.
It is foresee-able that multi-level confidence annotation can beused for many other language applications such asparsing, or information retrieval.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.
Proc.ACL, pages 173?180.Michael Collins.
2000.
Discriminative reranking for naturallanguage parsing.
Proc.
ICML, pages 175?182.Malte Gabsdil and Johan Bos.
2003.
Combining Acoustic Con-fidence Scores with Deep Semantic Analysis for Clarifica-tion Dialogues.
Proc.
IWCS, pages 137-150.Didier Guillevic, et al 2002.
Robust semantic confidencescoring.
Proc.
ICSLP,  pages 853-856.Jeremy G. Kahn, et al 2005.
Effective Use of Prosody in Pars-ing Conversational Speech.
Proc.
EMNLP, pages 233-240.David McClosky, Eugene Charniak and Mark Johnson.
2006.Reranking and Self-Training for Parser Adaptation.
Proc.COLING-ACL, pages 337-344.Nicola Ueffing and Hermann Ney.
2007.
Word-Level Confi-dence Estimation for Machine Translation.
ComputationalLinguistics, 33(1):9-40.Fuliang Weng, et al, 2007.
CHAT to Your Destination.
Proc.of the 8th SIGDial workshop on Discourse and Dialogue,pages 79-86.Qi Zhang, Fuliang Weng and Zhe Feng.
2006.
A Pro-gressiveFeature Selection Algorithm for Ultra Large FeatureSpaces.
Proc.
COLING-ACL, pages 561-568.Rong Zhang and Alexander I. Rudnicky.
2001.
Word levelconfidence annotation using combinations of features.
Proc.Eurospeech, pages 2105-2108.220
