Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 28?36,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsIs Unlabeled Data Suitable for Multiclass SVM-based Web PageClassification?Arkaitz Zubiaga, V?
?ctor Fresno, Raquel Mart?
?nezNLP & IR Group at UNEDLenguajes y Sistemas Informa?ticosE.T.S.I.
Informa?tica, UNED{azubiaga, vfresno, raquel}@lsi.uned.esAbstractSupport Vector Machines present an interest-ing and effective approach to solve automatedclassification tasks.
Although it only han-dles binary and supervised problems by na-ture, it has been transformed into multiclassand semi-supervised approaches in severalworks.
A previous study on supervised andsemi-supervised SVM classification over bi-nary taxonomies showed how the latter clearlyoutperforms the former, proving the suitabilityof unlabeled data for the learning phase in thiskind of tasks.
However, the suitability of un-labeled data for multiclass tasks using SVMhas never been tested before.
In this work,we present a study on whether unlabeled datacould improve results for multiclass web pageclassification tasks using Support Vector Ma-chines.
As a conclusion, we encourage to relyonly on labeled data, both for improving (or atleast equaling) performance and for reducingthe computational cost.1 IntroductionThe amount of web documents is increasing in avery fast way in the last years, what makes moreand more complicated its organization.
For this rea-son, web page classification has gained importanceas a task to ease and improve information access.Web page classification can be defined as the taskof labeling and organizing web documents within aset of predefined categories.
In this work, we focuson web page classification based on Support Vec-tor Machines (SVM, (Joachims, 1998)).
This kindof classification tasks rely on a previously labeledtraining set of documents, with which the classi-fier acquires the required ability to classify new un-known documents.Different settings can be distinguished for webpage classification problems.
On the one hand, at-tending to the learning technique the system baseson, it may be supervised, with all the training docu-ments previously labeled, or semi-supervised, whereunlabeled documents are also taken into accountduring the learning phase.
On the other hand, attend-ing to the number of classes, the classification maybe binary, where only two possible categories canbe assigned to each document, or multiclass, wherethree or more categories can be set.
The former iscommonly used for filtering systems, whereas thelatter is necessary for bigger taxonomies, e.g.
topi-cal classification.Although multiple studies have been made fortext classification, its application to the web pageclassification area remains without enough attention(Qi and Davison, 2007).
Analyzing the nature ofa web page classification task, we can consider itto be, generally, multiclass problems, where it isusual to find numerous classes.
In the same way,if we take into account that the number of availablelabeled documents is tiny compared to the size ofthe Web, this task becomes semi-supervised besidesmulticlass.However, the original SVM algorithm supportsneither semi-supervised learning nor multiclass tax-onomies, due to its dichotomic and supervised na-ture.
To solve this issue, different studies forboth multiclass SVM and semi-supervised SVM ap-proaches have been proposed, but a little effort has28been invested in the combination of them.
(Joachims, 1999) compares supervised and semi-supervised approaches for binary tasks using SVM.It shows encouraging results for the transductivesemi-supervised approach, clearly improving the su-pervised, and so he proved unlabeled data to besuitable to optimize binary SVM classifiers?
results.On the other hand, the few works presented forsemi-supervised multiclass SVM classification donot provide clear information on whether the unla-beled data improves the classification results in com-parison with the only use of labeled data.In this work, we performed an experiment amongdifferent SVM-based multiclass approaches, bothsupervised and semi-supervised.
The experimentswere focused on web page classification, andwere carried out over three benchmark datasets:BankSearch, WebKB and Yahoo!
Science.
Usingthe results of the comparison, we analyze and studythe suitability of unlabeled data for multiclass SVMclassification tasks.
We discuss these results andevaluate whether it is worthy to rely on a semi-supervised SVM approach to conduct this kind oftasks.The remainder of this document is organized asfollows.
Next, in section 2, we briefly explain howSVM classifiers work for binary classifications, bothfor a supervised and a semi-supervised view.
In sec-tion 3, we continue with the adaptation of SVM tomulticlass environments, and show what has beendone in the literature.
Section 4 presents the detailsof the experiments carried out in this work, aim atevaluating the suitability of unlabeled data for mul-ticlass SVM classification.
In section 5 we show anddiscuss the results of the experiments.
Finally, insection 6, we conclude with our thoughts and futurework.2 Binary SVMIn the last decade, SVM has become one of the moststudied techniques for text classification, due to thepositive results it has shown.
This technique uses thevector space model for the documents?
representa-tion, and assumes that documents in the same classshould fall into separable spaces of the representa-tion.
Upon this, it looks for a hyperplane that sepa-rates the classes; therefore, this hyperplane shouldmaximize the distance between it and the nearestdocuments, what is called the margin.
The followingfunction is used to define the hyperplane (see Figure1):f(x) = w ?
x+ bFigure 1: An example of binary SVM classification, sep-arating two classes (black dots from white dots)In order to resolve this function, all the possiblevalues should be considered and, after that, the val-ues of w and b that maximize the margin should beselected.
This would be computationally expensive,so the following equivalent function is used to relaxit (Boser et al , 1992) (Cortes and Vapnik, 1995):min[12 ||w||2 + Cl?i=1?di]Subject to: yi(w ?
xi + b) ?
1?
?i, ?i ?
0where C is the penalty parameter, ?i is an stackvariable for the ith document, and l is the number oflabeled documents.This function can only resolve linearly separableproblems, thus the use of a kernel function is com-monly required for the redimension of the space; inthis manner, the new space will be linearly separa-ble.
After that, the redimension is undone, so thefound hyperplane will be transformed to the originalspace, respecting the classification function.
Best-known kernel functions include linear, polynomial,radial basis function (RBF) and sigmoid, among oth-ers.
Different kernel functions?
performance hasbeen studied in (Scho?lkopf and Smola, 1999) and(Kivinen et al, 2002).29Note that the function above can only resolve bi-nary and supervised problems, so different variantsare necessary to perform semi-supervised or multi-class tasks.2.1 Semi-supervised Learning for SVM(S3VM)Semi-supervised learning approaches differ in thelearning phase.
As opposed to supervised ap-proaches, unlabeled data is used during the learn-ing phase, and so classifier?s predictions over themis also included as labeled data to learn.
The fact oftaking into account unlabeled data to learn can im-prove the classification done by supervised methods,specially when its predictions provide new useful in-formation, as shown in figure 2.
However, the noiseadded by erroneus predictions can make worse thelearning phase and, therefore, its final performance.This makes interesting the study on whether relyingon semi-supervised approaches is suitable for eachkind of task.Semi-supervised learning for SVM, also knownas S3VM, was first introduced by (Joachims, 1999)in a transductive way, by modifying the originalSVM function.
To do that, he proposed to add anadditional term to the optimization function:min?
?12 ?
||?||2 + C ?l?i=1?di + C?
?u?j=1??dj?
?where u is the number of unlabeled data.Nevertheless, the adaptation of SVM to semi-supervised learning significantly increases its com-putational cost, due to the non-convex nature of theresulting function, and so obtaining the minimumvalue is even more complicated.
In order to relaxthe function, convex optimization techniques suchas semi-definite programming are commonly used(Xu et al , 2007), where minimizing the functiongets much easier.By means of this approach, (Joachims, 1999)demonstrated a large performance gap between theoriginal supervised SVM and his semi-supervisedproposal, in favour of the latter one.
He showedthat for binary classification tasks, the smaller isthe training set size, the larger gets the differenceamong these two approaches.
Although he workedFigure 2: SVM vs S3VM, where white balls are unla-beled documentswith multiclass datasets, he splitted the problemsinto smaller binary ones, and so he did not demon-strate whether the same performance gap occurs formulticlass classification.
This paper tries to coverthis issue.
(Chapelle et al, 2008) present a compre-hensive study on S3VM approaches.3 Multiclass SVMDue to the dichotomic nature of SVM, it came upthe need to implement new methods to solve multi-class problems, where more than two classes mustbe considered.
Different approaches have been pro-posed to achieve this.
On the one hand, as a directapproach, (Weston, 1999) proposed modifying theoptimization function getting into account all the kclasses at once:min?
?12k?m=1||wm||2 + Cl?i=1?m6=yi?mi?
?Subject to:wyi ?
xi + byi ?
wm ?
xi + bm + 2?
?mi , ?mi ?
0On the other hand, the original binary SVM clas-sifier has usually been combined to obtain a multi-class solution.
As combinations of binary SVM clas-sifiers, two different approaches to k-class classifierscan be emphasized (Hsu and Lin, 2002):?
one-against-all constructs k classifiers definingthat many hyperplanes; each of them separatesthe class i from the rest k-1.
For instance, fora problem with 4 classes, 1 vs 2-3-4, 2 vs 1-3-4, 3 vs 1-2-4 and 4 vs 1-2-3 classifiers would30be created.
New documents will be categorizedin the class of the classifier that maximizes themargin: C?i = argmaxi=1,...,k(wix + bi).
Asthe number of classes increases, the amount ofclassifiers will increase linearly.?
one-against-one constructs k(k?1)2 classifiers,one for each possible category pair.
For in-stance, for a problem with 4 classes, 1 vs 2,1 vs 3, 1 vs 4, 2 vs 3, 2 vs 4 and 3 vs 4 clas-sifiers would be created.
After that, it classi-fies each new document by using all the clas-sifiers, where a vote is added for the winningclass over each classifier; the method will pro-pose the class with more votes as the result.
Asthe number of classes increases, the amount ofclassifiers will increase in an exponential way,and so the problem could became very expen-sive for large taxonomies.Both (Weston, 1999) and (Hsu and Lin, 2002)compare the direct multiclass approach to the one-against-one and one-against-all binary classifiercombining approaches.
They agree concluding thatthe direct approach does not outperform the resultsby one-against-one nor one-against-all, althoughit considerably reduces the computational cost be-cause the number of support vector machines itconstructs is lower.
Among the binary combin-ing approaches, they show the performance of one-against-one to be superior to one-against-all.Although these approaches have been widelyused in supervised learning environments, they havescarcely been applied to semi-supervised learning.Because of this, we believe the study on its appli-cability and performance for this type of problemscould be interesting.3.1 Multiclass S3VMWhen the taxonomy is defined by more than twoclasses and the number of previously labeled doc-uments is very small, the combination of both mul-ticlass and semi-supervised approaches could be re-quired.
That is, a multiclass S3VM approach.
Theusual web page classification problem meets withthese characteristics, since more than two classesare usually needed, and the tiny amount of labeleddocuments requires the use of unlabeled data for thelearning phase.Actually, there are a few works focused on trans-forming SVM into a semi-supervised and multiclassapproach.
As a direct approach, a proposal by (Ya-jima and Kuo, 2006) can be found.
They modify thefunction for multiclass SVM classification and get itusable for semi-supervised tasks.
The resulting op-timization function is as follows:min 12h?i=1?iTK?1?i+Cl?j=1?i6=yjmax{0, 1?
(?yjj ?
?ij)}2where ?
represents the product of a vector of vari-ables and a kernel matrix defined by the author.On the other hand, some other works are based ondifferent approaches to achieve a multiclass S3VMclassifier.
(Qi et al, 2004) use Fuzzy C-Means (FCM) topredict labels for unlabeled documents.
After that,multiclass SVM is used to learn with the augmentedtraining set, classifying the test set.
(Xu y Schu-urmans, 2005) rely on a clustering-based approachto label the unlabeled data.
Afterwards, they ap-ply a multiclass SVM classifier to the fully labeledtraining set.
(Chapelle et al, 2006) present a directmulticlass S3VM approach by using the Continua-tion Method.
On the other hand, this is the onlywork, to the best of our knowledge, that has testedthe one-against-all and one-against-one approachesin a semi-supervised environment.
They apply thesemethods to some news datasets, for which they getlow performance.
Additionally, they show that one-against-one is not sufficient for real-world multi-class semi-supervised learning, since the unlabeleddata cannot be restricted to the two classes underconsideration.It is noteworthy that most of the above worksonly presented their approaches and compared themto other semi-supervised classifying methods, suchas Expectation-Maximization (EM) or Naive Bayes.As an exception, (Chapelle et al, 2006) compareda semi-supervised and a supervised SVM approach,but only over image datasets.
Against this, we feltthe need to evaluate and compare multiclass SVMand multiclass S3VM approaches, for the sake ofdiscovering whether learning with unlabeled web31documents is helpful for multiclass problems whenusing SVM as a classifier.4 Multiclass SVM versus Multiclass S3VMThe main goal of this work is to evaluate the realcontribution of unlabeled data for multiclass SVM-based web page classification tasks.
There are a fewworks using semi-supervised multiclass SVM clas-sifiers, but nobody has demonstrated it improves su-pervised SVM classifier?s performance.
Next, wedetail the experiments we carried out to clear up anydoubts and to ensure which is better for multiclassSVM-based web page classifications.4.1 ApproachesIn order to evaluate and compare multiclass SVMand multiclass S3VM, we decided to use three differ-ent but equivalent approaches for each view, super-vised and semi-supervised.
For further informationon these approaches, see section 3.
We add a suffix,-SVM or -S3VM, to the names of the approaches, todifferentiate whether they are based in a supervisedor a semi-supervised algorithm.On the part of the semi-supervised view, the fol-lowing three approaches were selected:?
2-steps-SVM: we called 2-steps-SVM to thetechnique based on the direct multiclass su-pervised approach exposed in section 3.
Thismethod works, on its first step, with the train-ing collection, learning with the labeled docu-ments and predicting the unlabeled ones; afterthat, the latter documents are labeled based onthe generated predictions.
On the second step,now with a fully labeled training set, the usualsupervised classification process is done, learn-ing with the training documents and predictingthe documents in the test set.This approach is somehow similar to those pro-posed by (Qi et al, 2004) and (Xu y Schu-urmans, 2005).
Nonetheless, the 2-steps-SVMapproach uses the same method for both thefirst and second steps.
A supervised multiclassSVM is used to increase the labeled set and, af-ter that, to classify the test set.?
one-against-all-S3VM: the one-against-all ap-proach has not sufficiently been tested for semi-supervised environments, and seems interest-ing to evaluate its performance.?
one-against-one-S3VM: the one-against-onedoes not seem to be suitable for semi-supervised environments, since the classifier isnot able to ignore the inadecuate unlabeled doc-uments for each 1-vs-1 binary task, as stated by(Chapelle et al, 2006).
Anyway, since it hasscarcely been tested, we also consider this ap-proach.On the other hand, the approaches selected forthe supervised view were these: (1) 1-step-SVM;(2) one-against-all-SVM, and (3) one-against-one-SVM.The three approaches mentioned above are anal-ogous to the semi-supervised approaches, 2-steps-SVM, one-against-all-S3VM and one-against-one-S3VM, respectively.
They differ in the learningphase: unlike the semi-supervised approaches, thesethree supervised approaches only rely on the labeleddocuments for the learning task, but after that theyclassify the same test documents.
These approachesallow to evaluate whether the unlabeled documentsare contributing in a positive or negative way in thelearning phase.4.2 DatasetsFor these experiments we have used three web pagebenchmark datasets previously used for classifica-tion tasks:?
BankSearch (Sinka and Corne, 2002), a col-lection of 11,000 web pages over 11 classes,with very different topics: commercial banks,building societies, insurance agencies, java, c,visual basic, astronomy, biology, soccer, mo-torsports and sports.
We removed the categorysports, since it includes both soccer and motor-sports in it, as a parent category.
This results10,000 web pages over 10 categories.
4,000 in-stances were assigned to the training set, whilethe other 6,000 were left on the test set.?
WebKB1, with a total of 4,518 documents of4 universities, and classified into 7 classes1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/32(student, faculty, personal, department, course,project and other).
The class named other wasremoved due to its ambiguity, and so we finallygot 6 classes.
2,000 instances fell into the train-ing set, and 2,518 to the test set.?
Yahoo!
Science (Tan, 2002), with 788 scien-tific documents, classified into 6 classes (agri-culture, biology, earth science, math, chemistryand others).
We selected 200 documents for thetraining set, and 588 for the test set.Within the training set, for each dataset, multipleversions were created, modifying the number of la-beled documents, while the rest were left unlabeled.Thus, the size of labeled subset within the trainingset changes, ranging from 50 web documents to thewhole training set.4.3 Document RepresentationSVM requires a vectorial representation of the docu-ments as an input for the classifier, both for train andtest phases.
To obtain this vectorial representation,we first converted the original html files into plaintext files, removing all the html tags.
After that, weremoved the noisy tokens, such as URLs, email ad-dresses or some stopwords.
For these edited docu-ments, the tf-idf term weighting function was usedto define the values for the uniterms found on thetexts.
As the term dimensionality became too large,we then removed the least-frequent terms by its doc-ument frequency; terms appearing in less than 0.5%of the documents were removed for the representa-tion.
The remaining uniterms define the vector spacedimensions.
That derived term vectors with 8285 di-mensions for BankSearch dataset, 3115 for WebKBand 8437 for Yahoo!
Science.4.4 ImplementationTo carry out our experiments, we based on freelyavailable and already tested and experimented soft-ware.
Different SVM classifiers were needed to im-plement the methods described in section 4.1.SVMlight2 was used to work with binary semi-supervised classifiers for the one-against-all-S3VMand one-against-one-S3VM approaches.
In the sameway, we implemented their supervised versions,2http://svmlight.joachims.orgone-against-all-SVM and one-against-one-SVM, inorder to evaluate the contribution of unlabeled data.To achieve the supervised approaches, we ignoredthe unlabeled data during the training phase and, af-ter that, tested with the same test set used for semi-supervised approaches.
The default settings usinga polynomial kernel were selected for the experi-ments.SVMmulticlass3 was used to implement the 2-steps-SVM approach, by using it two times.
Firstly,to train the labeled data and classify unlabeled data.After that, to train with the whole training set labeledwith classifier?s predictions, and to test with the testset.
In the same way, the 1-step-SVM method wasimplemented by ignoring unlabeled data and train-ing only the labeled data.
This method allows toevaluate the contribution of unlabeled data for the2-steps-SVM method.4.5 Evaluation MeasuresFor the evaluation of the experiments we used theaccuracy to measure the performance, since it hasbeen frequently used for text classification prob-lems, specially for multiclass tasks.
The accuracyoffers the percent of the correct predictions for thewhole test set.
We have considered the same weightfor all the correct guesses for any class.
A correctprediction in any of the classes has the same value,thus no weighting exists.On the other hand, an averaged accuracy evalu-ation is also possible for the binary combining ap-proaches.
An averaged accuracy makes possible toevaluate the results by each binary classifier, andprovides an averaged value for the whole binaryclassifier set.
It is worth to note that these values donot provide any information for the evaluation of thecombined multiclass results, but only for evaluatingeach binary classifier before combining them.5 Results and DiscussionNext, we show and discuss the results of our experi-ments.
It is remarkable that both one-against-one-SVM and one-against-one-S3VM approaches werevery inferior to the rest, and so we decided not to plotthem in order to maintain graphs?
clarity.
Hence,figures 3, 4 and 5 show the results in accordance3http://www.cs.cornell.edu/People/tj/svm light/svm multiclass.html33with the labeled subset size for the 2-steps-SVM, 1-step-SVM, one-against-all-S3VM and one-against-all-SVM approaches within our experiments.
Forthe results to be more representative, nine execu-tions were done for each subset, obtaining the meanvalue.
These nine executions vary on the labeledsubset within the training set.The fact that one-against-one-S3VM has been theworst approach for our experiments confirms thatthe noise added by the unlabeled documents withineach 1-vs-1 binary classification task is harmful tothe learning phase, and it is not corrected whenmerging all the binary tasks.The averaged accuracy for the combined bi-nary classifiers allows to compare the one-against-one and one-against-all views.
The averaged ac-curacy for one-against-one-S3VM shows very lowperformance (about 60% in most cases), whereasthe same value for one-against-all-S3VM is muchhigher (about 90% in most cases).
This is obvi-ous to happen for the one-against-all view, sinceit is much easier to predict documents not pertain-ing to the class under consideration for each 1-vs-all binary classifier.
Although each binary classifiergets about 90% accuracy for the one-against-one-S3VM approach, this value falls considerably whencombining them to get the multiclass result.
Thisshows the additional difficulty for multiclass prob-lems compared to binary ones.
Hence, the difficultyto correctly predict unlabeled data increases for mul-ticlass tasks, and it is more likely to add noise duringthe learning phase.Figure 3: Results for BankSearch datasetFigure 4: Results for WebKB datasetFigure 5: Results for Yahoo!
Science datasetFor all the datasets we worked with, there is anoticeable performance gap between direct multi-class and binary combining approaches.
Both 2-steps-SVM and 1-step-SVM are always on the topof the graphs, and one-against-all-S3VM and one-against-all-SVM approaches are so far from catch-ing up with their results, except for WebKB dataset,where the gap is not so noticeable.
This seems en-couraging, since considering less support vectors ina direct multiclass approach reduces the computa-tional cost and improves the final results.Comparing the two analogous approaches amongthem, different conclusions could be extracted.On the one hand, one-against-all-S3VM showsslightly better results than one-against-all-SVM, andso considering unlabeled documents seems to be34favourable for the one-against-all view.
On the otherhand, the direct multiclass view shows varying re-sults.
Both 2-steps-SVM and 1-step-SVM show verysimilar results for BankSearch and Yahoo!
Sciencedatasets, but superior for 1-step-SVM over the We-bKB dataset.
As a conclusion of this, ignoring un-labeled documents by means of the 1-step-SVM ap-proach seems to be advisable, since it reduces thecomputation cost, obtaining at least the same resultsthan the semi-supervised 2-steps-SVM.Although their results are so poor, as we saidabove, the supervised approach wins for the one-against-one view; this confirms, again, that the one-against-one view is not an adecuate view to be ap-plied in a semi-supervised environment, due to thenoise existing during the learning phase.When analyzing the performance gaps betweenthe analogous approaches, a general conclusion canbe extracted: the smaller is the labeled subset thebigger is the performance gap, except for the Ya-hoo!
Science dataset.
Comparing the two bestapproaches, 1-step-SVM and 2-steps-SVM, the per-formance gap increases when the number of la-beled documents decrease for BankSearch; for thisdataset, the accuracy by 1-step-SVM is 0.92 timesthe one by 2-steps-SVM when the number of labeleddocuments is only 50, but this proportion goes to0.99 with 500 labeled documents.
This reflects howthe contribution of unlabeled data decreases whilethe labeled set increases.
For WebKB, the perfor-mance gap is in favour of 1-step-SVM, and variesbetween 1.01 and 1.05 times 2-steps-SVM method?saccuracy, even with only 50 labeled documents.Again, increasing the labeled set negatively affectssemi-supervised algorithm?s performance.
Last, forYahoo!
Science, the performance gap among thesetwo approaches is not considerable, since their re-sults are very similar.Our conjecture for the performance difference be-tween 1-step-SVM and 2-steps-SVM for the threedatasets is the nature of the classes.
The accuracyby semi-supervised 2-steps-SVM is slightly higherfor BankSearch and Yahoo!
Science, where theclasses are quite heterogeneous.
On the other hand,the accuracy by supervised 1-step-SVM is clearlyhigher for WebKB, where all the classes are an aca-demic topic, and so more homogeneous.
The semi-supervised classifiers show a major problem for pre-dicting the unlabeled documents when the collectionis more homogeneous, and so more difficult to differbetween classes.In summary, the main idea is that unlabeled doc-uments do not seem to contribute as they would formulticlass tasks using SVM.
Within the approacheswe tested, the supervised 1-step-SVM approachshows the best (or very similar to the best in somecases) results in accuracy and, taking into accountit is the least-expensive approach, we strongly en-courage to use this approach to solve multiclass webpage classification tasks, mainly when the classesunder consideration are homogeneous.6 Conclusions and OutlookWe have studied and analyzed the contribution ofconsidering unlabeled data during the learning phasefor multiclass web page classification tasks usingSVM.
Our results show that ignoring unlabeled doc-ument to learn reduces computational cost and, ad-ditionaly, obtains similar or slightly worse accuracyvalues for heterogeneus taxonomies, but higher forhomogeneous ones.
Therefore we show that, unlikefor binary cases, as was shown by (Joachims, 1999),a supervised view outperforms a semi-supervisedone for multiclass environments.
Our thought is thatpredicting unlabeled documents?
class is much moredifficult when the number of classes increases, andso, the mistaken labeled documents are harmful forclassifier?s learning phase.As a future work, a direct semi-supervised multi-class approach, such as those proposed by (Yajimaand Kuo, 2006) and (Chapelle et al, 2006), shouldalso be considered, as well as setting the classifierwith different parameters or kernels.
Balancing theweight of previously and newly labeled data couldalso be interesting to improve semi-supervised ap-proaches?
results.AcknowledgmentsWe wish to thank the anonymous reviewers for theirhelpful and instructive comments.
This work hasbeen supported by the Research Network MAVIR(S-0505/TIC-0267), the Regional Ministry of Ed-ucation of the Community of Madrid, and by theSpanish Ministry of Science and Innovation projectQEAVis-Catiex (TIN2007-67581-C02-01).35ReferencesB.
E. Boser, I. Guyon and V. Vapnik.
1992.
A TrainingAlgorithm for Optimal Margin Classifiers.
Proceed-ings of the 5th Annual Workshop on computationalLearning Theory.C.
Campbell.
2000.
Algorithmic Approaches to TrainingSupport Vector Machines: A Survey Proceedings ofESANN?2000, European Symposium on Artificial Neu-ral Networks.O.
Chapelle, M. Chi y A. Zien 2006.
A ContinuationMethod for Semi-supervised SVMs.
Proceedings ofICML?06, the 23rd International Conference on Ma-chine Learning.O.
Chapelle, V. Sindhwani, S. Keerthi 2008.
Optimiza-tion Techniques for Semi-Supervised Support VectorMachines.
J. Mach.
Learn.
Res..C. Cortes and V. Vapnik.
1995.
Support Vector Network.Machine Learning.C.-H. Hsu and C.-J.
Lin.
2002.
A Comparison of Meth-ods for Multiclass Support Vector Machines.
IEEETransactions on Neural Networks.T.
Joachims.
1998.
Text Categorization with SupportVector Machines: Learning with many Relevant Fea-tures.
Proceedings of ECML98, 10th European Con-ference on Machine Learning.T.
Joachims.
1999.
Transductive Inference for TextClassification Using Support Vector Machines.
Pro-ceedings of ICML99, 16th International Conferenceon Machine Learning.J.
Kivinen and E.J.
Smola and R.C.
Williamson.
2002.Learning with Kernels.T.
Mitchell.
1997.
Machine Learning.
McGraw Hill.H.-N. Qi, J.-G. Yang, Y.-W. Zhong y C. Deng 2004.Multi-class SVM Based Remote Sensing ImageClassification and its Semi-supervised ImprovementScheme.
Proceedings of the 3rd ICMLC.X.
Qi and B.D.
Davison.
2007.
Web Page Classification:Features and Algorithms.
Technical Report LU-CSE-07-010.B.
Scho?lkopf and A. Smola.
1999.
Advances in KernelMethods: Support Vector Learning.
MIT Press.F.
Sebastiani.
2002.
Machine Learning in AutomatedText Categorization.
ACM Computing Surveys, pp.
1-47.M.P.
Sinka and D.W. Corne.
2002.
A New BenchmarkDataset for Web Document Clustering.
Soft Comput-ing Systems.C.M.
Tan, Y.F.
Wang and C.D.
Lee.
2002.
The Use ofBigrams to Enhance Text Categorization.
InformationProcessing and Management.J.
Weston and C. Watkins.
1999.
Multi-class SupportVector Machines.
Proceedings of ESAAN, the Euro-pean Symposium on Artificial Neural Networks.L.
Xu y D. Schuurmans.
2005.
Unsupervised and Semi-supervised Multiclass Support Vector Machines.
Pro-ceedings of AAAI?05, the 20th National Conference onArtificial Intelligence.Z.
Xu, R. Jin, J. Zhu, I.
King and M. R. Lyu.
2007.
Ef-ficient Convex Optimization for Transductive SupportVector Machine.
Advances in Neural Information Pro-cessing Systems.Y.
Yajima and T.-F. Kuo.
2006.
Optimization Ap-proaches for Semi-Supervised Multiclass Classifica-tion.
Proceedings of ICDM?06 Workshops, the 6th In-ternational Conference on Data Mining.36
