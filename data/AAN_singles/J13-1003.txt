Parsing Morphologically Rich Languages:Introduction to the Special IssueReut Tsarfaty?Uppsala UniversityDjame?
Seddah??Universite?
Paris-Sorbonne/INRIASandra Ku?bler?Indiana UniversityJoakim Nivre?Uppsala UniversityParsing is a key task in natural language processing.
It involves predicting, for each naturallanguage sentence, an abstract representation of the grammatical entities in the sentence andthe relations between these entities.
This representation provides an interface to compositionalsemantics and to the notions of ?who did what to whom.?
The last two decades have seen greatadvances in parsing English, leading to major leaps also in the performance of applications thatuse parsers as part of their backbone, such as systems for information extraction, sentimentanalysis, text summarization, and machine translation.
Attempts to replicate the success ofparsing English for other languages have often yielded unsatisfactory results.
In particular,parsing languages with complex word structure and flexible word order has been shown torequire non-trivial adaptation.
This special issue reports on methods that successfully addressthe challenges involved in parsing a range of morphologically rich languages (MRLs).
Thisintroduction characterizes MRLs, describes the challenges in parsing MRLs, and outlines thecontributions of the articles in the special issue.
These contributions present up-to-date researchefforts that address parsing in varied, cross-lingual settings.
They show that parsing MRLsaddresses challenges that transcend particular representational and algorithmic choices.1.
Parsing MRLsParsing is a central task in natural language processing, where a system accepts asentence in a natural language as input and provides a syntactic representation of the?
Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.E-mail: tsarfaty@stp.lingfil.uu.se.??
Inria?s Alpage project & Universite?
Paris Sorbonne, Maison de la Recherche, 28 rue Serpentes, 75006Paris, France.
E-mail: djame.seddah@paris-sorbonne.fr.?
Indiana University, Department of Linguistics, Memorial Hall 322, Bloomington IN-47405, USA.E-mail: skuebler@indiana.edu.?
Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.E-mail: joakim.nivre@lingfil.uu.se.?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 1entities and grammatical relations in the sentence as output.
The input sentences to aparser reflect language-specific properties (in terms of the order of words, the wordforms, the lexical items, and so on), whereas the output abstracts away from theseproperties in order to yield a structured, formal representation that reflects the functionsof the different elements in the sentence.The best broad-coverage parsing systems to date use statistical models, possiblyin combination with hand-crafted grammars.
They use machine learning techniquesthat allow the system to generalize the syntactic patterns characterizing the data.
Thesemachine learning methods are trained on a treebank, that is, a collection of naturallanguage sentences which are annotated with their correct syntactic analyses.
Based onthe patterns and frequencies observed in the treebank, parsing algorithms are designedto suggest and score novel analyses for unseen sentences, and search for the most likelyanalysis.The release of a large-scale annotated corpus for English, the Wall Street JournalPenn Treebank (PTB) (Marcus, Santorini, and Marcinkiewicz 1993), led to a significantleap in the performance of statistical parsing for English (Magerman 1995; Collins 1997;Charniak 2000; Charniak and Johnson 2005; Petrov et al2006; Huang 2008; Finkel,Kleeman, and Manning 2008; Carreras, Collins, and Koo 2008).
At the time of theirpublication, each of these models improved the state-of-the-art of English parsing,bringing constituency-based parsing performance on the standard test set of the PTBto the level of 92% F1-score using the PARSEVAL evaluation metrics (Black et al1991).The last decade has seen the development of large-scale annotated treebanksfor languages such as Arabic (Maamouri et al2004), French (Abeille?, Cle?ment, andToussenel 2003), German (Uszkoreit 1987; Skut et al1997), Hebrew (Sima?an et al2001), Swedish (Nivre and Megyesi 2007), and others.
The availability of syntacticallyannotated corpora for these languages had initially raised the hope of attaining thesame level of parsing performance on these languages, by simply porting the existingmodels to the newly available corpora.Early attempts to apply the aforementioned constituency-based parsing models toother languages have demonstrated that the success of these approaches was rather lim-ited.
This observation was confirmed for individual languages such as Czech (Collinset al1999), German (Dubey and Keller 2003), Italian (Corazza et al2004), French (Arunand Keller 2005), Modern Standard Arabic (Kulick, Gabbard, andMarcus 2006), ModernHebrew (Tsarfaty and Sima?an 2007), and many more (Tsarfaty et al2010).The same observation was independently confirmed by parallel research efforts ondata-driven dependency-based parsing (Ku?bler, McDonald, and Nivre 2009).
Resultscoming from multilingual parsing evaluation campaigns, such as the CoNLL sharedtasks on multilingual dependency parsing, showed significant variation in the resultsof the same models applied to a range of typologically different languages.
In partic-ular, these results demonstrated that the morphologically rich nature of some of thoselanguages makes them inherently harder to parse, regardless of the parsing techniqueused (Buchholz and Marsi 2006; Nivre et al2007a).Morphologically rich languages (MRLs) express multiple levels of information al-ready at the word level.
The lexical information for each word form in an MRL maybe augmented with information concerning the grammatical function of the word inthe sentence, its grammatical relations to other words, pronominal clitics, inflectionalaffixes, and so on.
In English, many of these notions are expressed implicitly by wordorder and adjacency: The direct object, for example, is generally the first NP after theverb and thus does not necessarily need an explicit marking.
Expressing such functionalinformation morphologically allows for a high degree of word-order variation, since16Tsarfaty et alParsing Morphologically Rich Languages:grammatical functions need no longer be strongly associated with syntactic positions.Furthermore, lexical items appearing in different syntactic contexts may be realized indifferent forms.
This leads to a high level of word-form variation and complicates lexicalacquisition from small sized corpora.2.
The Overarching ChallengesThe complexity of the linguistic patterns found in MRLs was shown to challengeparsing in many ways.
For instance, standard models assume that a word alwayscorresponds to a unique terminal in the parse tree.
In Arabic, Hebrew, Turkish, and otherlanguages, an input word-token may correspond to multiple terminals.
Furthermore,models developed primarily to parse English draw substantial inference based onword-order patterns.
Parsing non-configurational languages such as Hungarian may requirerelying on morphological information to infer equivalent functions.
Parsing Czech orGerman is further complicated by case syncretism, which precludes a deterministiccorrelation between morphological case and grammatical functions.
In languages suchas Hungarian or Finnish, the diversity of word forms leads to a high rate of out-of-vocabulary words unseen in the annotated data.
MRL parsing is thus often associatedwith increased lexical data sparseness.
An MRL parser requires robust statistical meth-ods for analyzing such phenomena.Following Tsarfaty et al(2010) we distinguish three overarching challenges that areassociated with parsing MRLs.
(i) The Architectural Challenge.
Contrary to English, where the input signal uniquelydetermines the sequence of tree terminals, word forms in an MRLmay contain multipleunits of information (morphemes).
These morphemes have to be segmented in order toreveal the basic units of analysis.
Furthermore, morphological analysis of MRL wordsmay be highly ambiguous, and morphological segmentation may be a non-trivial taskfor certain languages.
Therefore, a parsing architecture for an MRL must contain, at thevery least, a morphological component for segmentation and a syntactic component forparsing.
The challenge is thus to determine how these two models should be combinedin the overall parsing architecture: Should we assume a pipeline architecture, where themorphological segmentation is disambiguated prior to parsing?
Or should we constructa joint architecture where the model picks out a parse tree and a segmentation at once?
(ii) The Modeling Challenge.
The design of a statistical parsing model requires specifyingthree formal elements: the formal output representation, the events that can be observedin the data, and the independence assumptions between these events.
For anMRL, com-plex morphosyntactic interactions may impose constraints on the form of events and ontheir possible combination.
In such cases, we may need to incorporate morphologicalinformation in the syntactic model explicitly.
How should morphological informationbe treated in the syntactic model: as explicit tree decoration, as hidden variables, or ascomplex objects in their own right?
Which morphological features should be explicitlyencoded?
Where should we mark morphological features: at the part-of-speech level, atphrase level, on dependency arcs?
How domorphological and syntactic events interact,and how can we exploit these interactions for inferring correct overall structures?
(iii) The Lexical Challenge.
A parsing model for MRLs requires recognizing the morpho-logical information in each word form.
Due to the high level of morphological variation,however, data-driven systems are not guaranteed to observe all morphological variants17Computational Linguistics Volume 39, Number 1of a word form in a given annotated corpus.
How can we assign correct morphologicalsignatures to the lexical items in the face of such extreme data spareseness?
Whendevising a model for parsing MRLs, one may want to make use of whatever additionalresources one has access to?morphological analyzers, unlabeled data, and lexica?inorder to extend the coverage of the parser and obtain robust and accurate predictions.3.
Contributions of this Special IssueThis special issue draws attention to the different ways in which researchers work-ing on parsing MRLs address the challenges described herein.
It contains six stud-ies discussing parsing results for six languages, using both constituency-based anddependency-based frameworks (cf.
Table 1).
The first three studies (Seeker and Kuhn;Fraser et al Kallmeyer and Maier) focus on parsing European languages and dealwith phenomena that lie within their flexible phrase ordering and rich morphology,including problems posed by case syncretism.
The next two papers (Goldberg andElhadad; Marton et al focus on Semitic languages and study the application of general-purpose parsing algorithms (constituency-based and dependency-based, respectively)to parsing such data.
They empirically show gaps in performance between differentarchitectures (pipeline vs. joint , gold vs. machine-predicted input), feature choices, andtechniques for increasing lexical coverage of the parser.
The last paper (Green et al isa comparative study on multi-word expression (MWE) recognition via two specializedparsing models applied to both French and Modern Standard Arabic.
Let us brieflyoutline the individual contributions made by each of the articles in this special issue.Seeker and Kuhn present a comparative study of dependency parsing for threeEuropean MRLs from different typological language families: German (Germanic),Czech (Slavonic), and Hungarian (Finno-Ugric).
Although all these languages possessricher morphological marking than English, there is variation among these languagesin terms of the richness of the morphological information encoded in the word forms,and the ambiguity of these morphological markers.
Hungarian is agglutinating, thatis, morphological markers in Hungarian are non-ambiguous and easy to recognize.German and Czech are fusional languages with different types of case syncretism.Seeker and Kuhn use the Bohnet Parser (Bohnet 2010) to parse all these languages, andshow that not using morphological information in the statistical feature model is detri-mental.
Using gold morphology significantly improves results for all these languages,whereas automatically predicted morphology leads to smaller improvements for thefusional languages, relative to the agglutinating one.
To combat this loss in performance,they add linguistic constraints to the decoder, restricting the possible structures.
Theyshow that a decoding algorithm which filters out dependency parses that do not obeyTable 1Contributions to the CL special issue on parsing morphologically rich languages (CL-PMRL).Constituency-Based Dependency-BasedArabic Green, de Marneffe, and Manning 2013 Marton, Habash, and Rambow 2013Czech Seeker and Kuhn 2013French Green, de Marneffe, and Manning 2013German Kallmeyer and Maier 2013 Seeker and Kuhn 2013Fraser et al2013Hebrew Goldberg and Elhadad 2013Hungarian Seeker and Kuhn 201318Tsarfaty et alParsing Morphologically Rich Languages:predicate-argument constraints allows the authors to obtain more substantial gainsfrom morphology.Fraser et alalso focus on parsing German, though in a constituency-based setting.They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set ofmanual treebank annotations that bring the treebank grammar performance to the levelof automatically predicted states learned by Petrov et al(2006).
As in the previousstudy, syncretism is shown to cause ambiguity that hurts parsing performance.
Tocombat this added ambiguity, they use external information sources.
In particular, theyshow different ways of using information from monolingual and bilingual data setsin a re-ranking framework for improving parsing accuracy.
The bilingual approachis inspired by machine translation studies and exploits the variation in marking thesame grammatical functions differently across languages for increasing the confidenceof a disambiguation decision in one language by observing a parallel non-ambiguousstructure in the other one.These two studies use German corpora stripped of discontinuous constituents inorder to benchmark their parsers.
In each of these cases, the discontinuities are con-verted into pure tree structures, thus ignoring the implied long distance dependencies.Kallmeyer and Maier propose an alternative approach for parsing such languagesby presenting an overall solution for parsing discontinuous structures directly.
Theypresent a parsing model based on Probabilistic Linear Context-Free Rewriting Systems(PLCFRS), which implements many of the technological advances that were developedin the context of parsing with PCFGs.
In particular, they present a decoding algorithmbased on weighted deductive CKY parsing, and use it in conjunction with PLCFRSparameters directly estimated from treebank data.
Because PLCFRS is a powerful for-malism, the parser needs to be tuned for speed.
The authors present several admissibleheuristics that facilitate faster A* parsing.
The authors present parsing results that arecompetitive with constituency-based parsing of German while providing invaluableinformation concerning discontinuous constituents and long distance dependencies.Goldberg and Elhadad investigate constituency parsing for Modern Hebrew(Semitic), a language which is known to have a very rich and ambiguous morpho-logical structure.
They empirically show that an application of the split-and-mergegeneral-purpose model of Petrov et al(2006) for parsing Hebrew does not guaranteeaccurate parsing in and of itself.
In order to obtain competitive parsing performance,they address all three challenges we have noted.
In order to deal with the problem ofword segmentation (the architectural challenge), they extend the chart-based decoderof Petrov et alwith a lattice-based decoder.
In order to handle morphological markingpatterns (the modeling challenge), they refine the initial treebank with particularlytargeted state-splits, and add a set of linguistic constraints that act as a filter rulingout trees that violate agreement.
Finally, they add information from an external wide-coverage lexicon to combat lexical sparseness (the lexical challenge).
They show that thecontribution of these different methods is cumulative, yielding state-of-the-art resultson constituency parsing of Hebrew.Marton et alstudy dependency parsing of Modern Standard Arabic (Semitic)and attend to the same challenges.
They show that for two transition-based parsers,MaltParser (Nivre et al2007b) and EasyFirst (Goldberg and Elhadad 2010), controllingthe architectural and modeling choices leads to similar effects.
For instance, whencomparing parsing performance on gold and machine-predicted input conditions, theyshow that rich informative tag sets are preferred in gold conditions, but smaller tagsets are preferred in machine-predicted conditions.
They further isolate a set of mor-phological features which leads to significant improvements in the machine-predicted19Computational Linguistics Volume 39, Number 1condition, for both frameworks.
They also show that function-based morphologicalfeatures are more informative than surface-based features, and that performance lossthat is due to errors in part-of-speech tagging may be restored by training the modelon a joint set of trees encoding gold tags and machine-predicted tags.
At the same time,undirected parsing of EasyFirst shows better accuracy, possibly due to the flexiblity inphrase ordering.
The emerging insight is that tuning morphological information insidegeneral-purpose parsing systems is of crucial importance for obtaining competitiveperformance.Focusing on Modern Standard Arabic (Semitic) and French (Romance), the lastarticle of this special issue, by Green et al may be seen as an applications paper,treating the task of MWE recognition as a side effect of a joint model for parsing andMWE identification.
The key problem here is knowing what to consider a minimalunit for parsing, and how to handle parsing in realistic scenarios where MWEs havenot yet been identified.
The authors present two parsing models for such a task: afactored model including a factored lexicon that integrates morphological knowledgeinto the Stanford Parser word model (Klein and Manning 2003), and a Dirichlet ProcessTree Substitution Grammar based model (Cohn, Blunsom, and Goldwater 2010).
Thelatter can be roughly described as Data Oriented Parsing (Bod 1992; Bod, Scha, andSima?an 2003) in a Bayesian framework, extended to include specific features that easethe extraction of tree fragments matching MWEs.
Interestingly, those very differentmodels do provide the same range of performance when confronted with predictedmorphology input.
Additional important challenges that are exposed in the context ofthis study concern the design of experiments for cross-linguistic comparison in the faceof delicate asymmetries between the French and Arabic data sets.4.
ConclusionThis special issue highlights actively studied areas of research that address parsingMRLs.
Most approaches described in this issue rely on extending existing parsingmodels to address three overarching challenges.
The joint parsing and segmentationarchitecture scenario can be addressed by extending a general-purpose CKY decoderinto a lattice-based decoder.
The modeling challenge may be addressed by explicitlymarking morphological features as syntactic state-splits, by modeling discontinuitiesin the formal syntactic representation directly, by incorporating hard-coded linguisticconstraints as filters, and so on.
The lexical challenge can be addressed by using externalresources such as a wide-coverage lexicon for analyzing unknown words, and the useof additional monolingual and bilingual data in order to obtain robust statistics in theface of extreme sparseness.An empirical observation reflected in the results presented here is that languageswhich we refer to as MRLs exhibit their own cross-lingual variation and thus should notbe treated as a single, homogeneous class of languages.
Some languages show richermorphology than others; some languages possess more flexible word ordering thanothers; some fusional languages show syncretism (coarse-grained underspecifiedmark-ers) whereas others use a large set of fine-grained and unambiguous morphologicalmarkers.
The next challenge would then be to embrace these variations, and investigatewhether the typological properties of languages can inform us more directly concerningthe adequate methods that can be used to effectively parse them.As the next research goal, we then set out to obtain a deeper understanding ofhow annotation choices paired up with modeling choices systematically correlate withparsing performance for different languages.
Further work in the line of the studies20Tsarfaty et alParsing Morphologically Rich Languages:presented here is required in order to draw relevant generalizations.
Furthermore,the time is ripe for another multilingual parser evaluation campaign, which wouldencourage the community to develop parsing systems that can easily be transferredfrom one language type to another.
By compiling these recent contributions, we hopeto encourage not only the development of novel systems for parsing individual MRLs,but also to facilitate the search for more robust, generic cross-linguistic solutions.AcknowledgmentsAs guest editors of this special issue, wewish to thank the regular members andthe guest members of the ComputationalLinguistics editorial board for their thoroughwork, which allowed us to assemble thisspecial issue of high quality contributions inthe emerging field of parsing MRLs.
We alsowant to thank Marie Candito, Jennifer Foster,Yoav Goldberg, Ines Rehbein, Lamia Tounsi,and Yannick Versley for their contributionto the initial proposal for this special issue.Finally, we want to express our gratitudeto Robert Dale and Suzy Howlett for theirinvaluable support throughout the editorialprocess.ReferencesAbeille?, Anne, Lionel Cle?ment, and Franc?oisToussenel.
2003.
Building a treebank forFrench.
In Anne Abeille?, editor, Treebanks.Kluwer, Dordrecht, pages 165?188.Arun, Abhishek and Frank Keller.
2005.Lexicalization in crosslinguisticprobabilistic parsing: The case of French.In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics,pages 306?313, Ann Arbor, MI.Black, Ezra, Steven Abney, Dan Flickinger,Claudia Gdaniec, Ralph Grishman, PhilipHarrison, Donald Hindle, Robert Ingria,Frederick Jelinek, Judith Klavans, MarkLiberman, Mitchell Marcus, Salim Roukos,Beatrice Santorini, and TomekStrzalkowski.
1991.
A procedure forquantitatively comparing the syntacticcoverage of English grammars.
SpeechCommunication, 33(1,2):306?311.Bod, Rens.
1992.
A computational modelof language performance: Data orientedparsing.
In Proceedings of the 14th Conferenceon Computational linguistics-Volume 3,pages 855?859, Nantes.Bod, Rens, Remko Scha, and Khalil Sima?an,editors.
2003.
Data-Oriented Parsing.
CSLI,Stanford, CA.Bohnet, Bernd.
2010.
Top accuracy and fastdependency parsing is not a contradiction.In Proceedings of CoLing, pages 89?97, Sydney.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings ofthe Tenth Conference on ComputationalLanguage Learning (CoNLL),pages 149?164, New York, NY.Carreras, Xavier, Michael Collins, andTerry Koo.
2008.
TAG, dynamicprogramming, and the perceptronfor efficient, feature-rich parsing.In Proceedings of the Twelfth Conference onComputational Natural Language Learning(CoNLL), pages 9?16, Manchester.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedingsof the 1st Annual Meeting of the NorthAmerican Chapter of the ACL (NAACL),pages 132?139, Seattle, WA.Charniak, Eugene and Mark Johnson.2005.
Coarse-to-fine n-best parsingand maxent discriminative reranking.In Proceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL 2005), pages 173?180,Ann Arbor, MI.Cohn, Trevor, Phil Blunsom, and SharonGoldwater.
2010.
Inducing tree-substitution grammars.
The Journal ofMachine Learning Research, 11:3053?3096.Collins, Michael.
1997.
Three generative,lexicalized models for statistical parsing.In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics,pages 16?23, Madrid.Collins, Michael, Jan Hajic?, Lance Ramshaw,and Christoph Tillmann.
1999.
A statisticalparser for Czech.
In Proceedings of the 37thAnnual Meeting of the ACL, pages 505?512,College Park, MD.Corazza, Anna, Alberto Lavelli, Giogio Satta,and Roberto Zanoli.
2004.
Analyzing anItalian treebank with state-of-the-artstatistical parsers.
In Proceedings of theThird Workshop on Treebanks and LinguisticTheories (TLT 2004), pages 39?50, Tu?bingen.Dubey, Amit and Frank Keller.
2003.Probabilistic parsing for German usingsister-head dependencies.
In Proceedings ofthe 41st Annual Meeting of the Association forComputational Linguistics, pages 96?103,Ann Arbor, MI.21Computational Linguistics Volume 39, Number 1Finkel, Jenny Rose, Alex Kleeman, andChristopher D. Manning.
2008.
Efficient,feature-based, conditional random fieldparsing.
In Proceedings of ACL-08: HLT,pages 959?967, Columbus, OH.Goldberg, Yoav and Michael Elhadad.
2010.An efficient algorithm for easy-firstnon-directional dependency parsing.In Human Language Technologies: The2010 Annual Conference of the NorthAmerican Chapter of the Associationfor Computational Linguistics,pages 742?750, Los Angeles, CA.Huang, Liang.
2008.
Forest reranking:Discriminative parsing with non-localfeatures.
In Proceedings of ACL-08: HLT,pages 586?594, Columbus, OH.Klein, Dan and Christopher D. Manning.2003.
Accurate unlexicalized parsing.In Proceedings of the 41st Annual Meeting onAssociation for Computational Linguistics,pages 423?430, Sapporo.Ku?bler, Sandra, Ryan McDonald, and JoakimNivre.
2009.
Dependency Parsing.
Number 2in Synthesis Lectures on Human LanguageTechnologies.
Morgan & ClaypoolPublishers.Kulick, Seth, Ryan Gabbard, and MitchellMarcus.
2006.
Parsing the Arabic treebank:Analysis and improvements.
In Proceedingsof the 5th International Workshop onTreebanks and Linguistic Theories (TLT),pages 31?42, Prague.Maamouri, Mohamed, Anne Bies, TimBuckwalter, and Wigdan Mekki.
2004.The Penn Arabic Treebank: Building alarge-scale annotated Arabic corpus.
InProceedings of the NEMLAR Conferenceon Arabic Language Resources and Tools,pages 102?109, Cairo.Magerman, David M. 1995.
Statisticaldecision-tree models for parsing.In Proceedings of the 33rd Annual Meeting onAssociation for Computational Linguistics,pages 276?283, Cambridge, MA.Marcus, Mitchell, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English:The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Nivre, Joakim, Johan Hall, Sandra Ku?bler,Ryan McDonald, Jens Nilsson, SebastianRiedel, and Deniz Yuret.
2007a.
TheCoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL 2007Shared Task.
Joint Conference on EmpiricalMethods in Natural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL), pages 915?932, Prague.Nivre, Joakim, Johan Hall, Jens Nilsson,Atanas Chanev, Gu?ls?en Eryig?it,Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007b.
MaltParser:A language-independent system fordata-driven dependency parsing.
NaturalLanguage Engineering, 13(2):95?135.Nivre, Joakim and Beata Megyesi.
2007.Bootstrapping a Swedish Treebankusing cross-corpus harmonization andannotation projection.
In Proceedings of theSixth International Workshop on Treebanksand Linguistic Theories (TLT), pages 97?102,Bergen.Petrov, Slav, Leon Barrett, Romain Thibaux,and Dan Klein.
2006.
Learning accurate,compact, and interpretable treeannotation.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics,pages 433?440, Sydney.Schmid, Helmut.
2004.
Efficient parsing ofhighly ambiguous context-free grammarswith bit vectors.
In Proceedings of the 20thInternational Conference on ComputationalLinguistics (COLING 2004), pages 162?168,Geneva.Sima?an, Khalil, Alon Itai, Yoad Winter,Alon Altmann, and Noa Nativ.
2001.Building a tree-bank of Modern Hebrewtext.
Traitement Automatique des Langues,42:347?380.Skut, Wojciech, Brigitte Krenn, ThorstenBrants, and Hans Uszkoreit.
1997.An annotation scheme for free wordorder languages.
In Proceedingsof the Fifth Conference on AppliedNatural Language Processing (ANLP),pages 88?95, Washington, D.C.Tsarfaty, Reut, Djame?
Seddah, YoavGoldberg, Sandra Ku?bler, MarieCandito, Jennifer Foster, Yannick Versley,Ines Rehbein, and Lamia Tounsi.
2010.Statistical parsing of morphologicallyrich languages (SPMRL): What, how andwhither.
In Proceedings of the NAACLWorkshop on Statistical Parsing ofMorphologically Rich Languages, pages 1?12,Los Angeles, CA.Tsarfaty, Reut and Khalil Sima?an.
2007.Three-dimensional parametrization forparsing morphologically rich languages.In Proceedings of the Tenth InternationalConference on Parsing Technologies,pages 156?167, Prague.Uszkoreit, Hans.
1987.Word Order andConstituent Structure in German.
CSLI,Stanford, CA.22
