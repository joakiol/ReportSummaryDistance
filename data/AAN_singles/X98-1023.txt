Improving Robust Domain Independent SummarizationJim Cowie, Eugene Ludovik, Hugo Molina-SalgadoDept.
3CRL, Box 30001, NMSU, Las Cruces, NM 88003, USA(jcowie, eugene, hsalgado) @crl.nmsu.eduAbstractWe discuss those techniques which, in theopinion of the authors, are needed to supportrobust automatic summarization.
Many ofthese methods are already incorporated in amulti-lingual summarization system, MINDS,developed at CRL.
The approach is sentenceselection, but includes techniques to improvecoherence and also to perform sentence reduc-tion.
Our methods are in distinct contrast othose approaches to summarization by deepanalysis of a document followed by text gener-ation.KEYWORDSSummarization, Multi-lingual Language Engi-neering, Robust Methods1 IntroductionSummarization is the problem of presenting themost important information contained in one ormore documents.
The research described herefocuses on multi-lingual summarization (MLS).Summaries of documents are produced in Spanish,Japanese, English and Russian using the samebasic summarization engine.The core summarization problem is taking asingle text and producing a shorter text in the samelanguage that contains all the main points in theinput text.
We are using a robust, graded approachto building the core engine by incorporating statis-tical, syntactic and document structure analysesamong other techniques.
We have developed a sys-tem design which allows the parameterization bothof the summarization process and of necessaryinformation about he languages being processed.Document structure analysis (Salton & Singal94, Salton et al 95) is important for extracting thetopic of a text.
In a statistical analysis for example(Paice 90, Paice & Jones 93), titles and sub-titleswould be given a more important weight than thebody of the text.
Similarly, the introduction andconclusion for the text itself and for each sectionare more important han other paragraphs, and thefirst and last sentences in each paragraph are moreimportant han others.
The applicability of thesedepends on the style adopted in a particulardomain, and on the language: the stylistic structureand the presentation of arguments vary signifi-cantly across genres and languages.
Structure anal-ysis must be tailored to a particular type of text in aparticular language.
In the MINDS system docu-ment structure analysis involves the following sub-tasks:?
Language Identification?
Document Structure Parsing?
Multilingual Sentence Segmentation?
Text Structure HeuristicsIn order to allow a multitude of techniques tocontribute to sentence selection, the core engineadopts a flexible method of scoring the sentencesin a document by each of the techniques and thenranking them by combining the different scores.Text-structure based heuristics provide the mainmethod for ranking and selecting sentences in adocument.
These are supplemented by word fre-quency analysis methods.The core engine is designed in such a way thatas additional resources, such as lexical and otherknowledge bases or text processing and MTengines, become available from other ongoingresearch efforts they can be incorporated into theoverall multi-engine MINDS system.
The mostpromising components are part of speech tagging,anaphora resolution, and semantic methods toallow concept identification to supplement word171frequency analysis.
Part of speech tagging hasalready been used to perform sentence lengthreduction by stripping out "superfluous" words andphrases.
The other methods will be used to main-tain document coherence, and to improve sentenceselection and reduction.In this paper we describe the architecture andperformance of the current system and our plansfor incorporating new NLP methods.2 MINDS - Mul t i -L ingual  Interact iveDocument  Summar izat ion2.1 BackgroundThe need for summarization tools is especiallystrong if the source text is in a language differentfrom the one(s) in which the reader is most fluent.Interactive summarization of multilingual docu-ments is a very promising approach to improvingproductivity and reducing costs in large-scale doc-ument processing.
This addresses the scenariowhere an analyst is trying to filter through a largeset of documents to decide quickly which docu-ments deserve further processing.
This task is moredifficult and expensive when the documents are ina foreign language in which the analyst may not beas fluent as he or she is in English.
The task is evenmore difficult when the documents are in severaldifferent languages.
For example, the analyst's taskmay be to filter through newspaper a ticles in manydifferent languages published on a particular day togenerate a report on different nations' reactions to acurrent international event, such as a nuclear teston the previous day.
This last task is currentlyinfeasible for a single analyst, unless he or sheunderstands each one of those languages, sincemachine translation (MT) of entire documents can-not yet meet he requirements of such a task.
Multi-lingual summarization (MLS) introduces thepossibility of translating a summary rather than theentire document to the language of the summary(i.e., English).
We hope that MLS and MT canmutually benefit from one another since summari-zation offers MT the benefit of not having to trans-late entire texts and also spares a user from havingto read through an entire document produced by anMT system.2.2 OverviewThe MINDS system is a multilingual domainindependent summarization system, which is ableto summarize documents written in English, Japa-nese, Russian and Spanish.
The system is intendedto be rapidly adaptable to new language and genresby adjusting a set of parameters.
A summarizationsystem for Turkish has just been added to the sys-tem.
This required about one programmer day ofeffort, mostly spent in preprocessing the languageFigure 1.
Overview of the MINDS ArchitecturePlain TextE-mail ~ Input  ~ Document ~ummar izat ion~ Process Structuring Translation Core/ "  ' I I I I I / / 1SGML I I ~____~_~__~ / Statistics"l_l L -  " l~"~eUcI~lent  ~ ~  pOrUotcP~sts SummaryTranslation172Figure 2.
Input Processing StagePlain TextE-mail ~1HTML ~-~'1SGML fParser t~ ~ Plain Text, LanguageE-mail, HTML, RecognitionSGML.I IIIocumont  -.
.~ .yOb jec  tUNICODEConversiont I I IJ IJ FJresources used by the system.
The types of summa-rization information used are also intended to beadjustable by a user "on the fly", to allow the tun-ing of the summarizers output based on length ofsummary needed, type of document structure, topicfocus.The MINDS summarization system is com-posed of four stages.
First we have an Input Pro-cess stage, whose main function is to get therelevant text in the document in UNICODE encod-ing.
The second stage is a Document StructuringStage, where paragraph and sentence recognition,and word tokenization are performed.
All the infor-mation about the document structure is stored in a"Document Object" that will be used in the Sum-marization-Translation stage.
In the Summariza-tion-Translation Stage, the text is summarizedusing sentence xtraction techniques, where thesentence scoring and ranking is mainly based ontext-structure based heuristics supplemented byword frequency analysis methods and in somecases by information from a Name Recognitionmodule.
Once the summary is ready in the originalFigure 3.
Document Structuring StageI ,ParagraphandSentenceRecognitionE /\ \  / /\~t  / /I t. _ .
lp , .
/ /Document~u .
.
.
.
.
.
.
.
k~,,Object ~_Morphology/ /Tagging NameRecognitionI II I IJIJ173Figure 4.
Summarization a d Translation StageFrequency Countand _ ~  Sentence IFrequent Word ScoringSelection\ \  I I  ] Sentence Reduction t_ll~,q Re_Scoring t___.lp,.
(\ \  I I\ \  I I / /'i,~'---~-L~~,~  ~.~~.~~...~..... : ~  RankingSentenceObjectObject ~ ~ "" "" "" _ -- -- -- Summary [.Z --~ -~ Z S S S S S -- -- -- Generation IL ~ Translation ]language, MINDS uses MT engines from otherongoing CRL projects to translate the summary toEnglish.
The final stage is the Output Process thatgenerates the summary output form; SGML,HTML, or Plain text.
This may also involve con-version from UNICODE to the original encodingof the document.2.3 Input Process StageIn the input stage, MINDS can accept docu-ments written in different languages and codesets:currently English, Japanese, Russian, Turkish andSpanish.
Also the documents can be in differentformats such as SGML, HTML, E-mail or Plaintext.
A parsing stage identifies the document'sformat, selects and applies the appropriate parserand extracts the relevant ext from the document.Once we have the text to be summarized a lan-guage recognition module determines the languagein which the document is written and the textencoding used in the document.
Given the encod-ing of the document the text is converted to UNI-CODE and all the rest of the processing is carriedout on the UNICODE version of the text.2.4 Document Structuring StageAfter the text to be summarized is available inUNICODE encoding, its structure needs to bedetermined.
This is the job of the Document Struc-turing Stage.
In this stage, three tokenizationstages are performed.
The first one pose of identi-fies the paragraphs in the document.
The secondtokenization stage identifies entences within eachparagraph.
To identify sentence boundaries formany languages requires a list of abbreviations forthe language.
Languages such as Chinese and Japa-nese have an unambiguous " top" character andthus do not present his problem.
Finally, wordtokenization is carried out to identify individualwords in each sentence.
Here Chinese and Japa-nese which do not use spaces between wordsrequire some segmentation method to be applied.The current system actually uses two characterpairs, bi-grams, for all its calculations for Japanese.These bi-grams are produced starting at every char-acter position in the document.All the structuring information is stored in a"Document Object", which is the main data struc-ture of the system, holding all the information gen-174Figure 5.
Output Process StageocumentS- -  bject \[ "-- --- _.I \ \I \ \ \ \\ \ocument~ \ \  \~anagery  \HTML IGeneratorSGML~ Generator\\Plain TextGeneratorE-MailGenerator\~I~ StatisticsGeneratorffSummary inHTML formatSummaryTranslation inHTML formatt ~ -tP"..IV.
GraphsI~  ~ Scoreserated during the processing.
After thetokenization stage is complete and depending onthe lexical resources available for each language,other stages are performed, such as Morphology,Proper Name Recognition and Tagging.2.5 Summarizat ion-Translat ion StageIn the Summarization-Translation Stage, theimportance of each sentence in the document isdetermined using a scoring procedure, whichassigns scores to the sentences according to theposition of the sentences in the document structureand according to the occurrences of key-words inthe sentence which belong to the set of most fre-quent words in the document that are not in a "stoplist" (the most frequent words in a language areconsidered irrelevant).
We make the assumptionthat these key-word represent or identify the mainconcepts in the document, herefore if a sentencecontains everal of them, its score should be highso it could be selected as part of the summary.
It isimportant to note here that we need a "stop list" foreach language considered in the summarizationsystem.
Also, if a Proper Name Recognition mod-ule is available for a specific language, we use theinformation about person names, organizationnames, places and dates to contribute in the scoresof sentences.At this point if the lexical resources are avail-able, an optional sentence length reduction can becarried out using information from a tagging stage.This sentence length reduction includes the elimi-nation of adjectives from noun phrases, keepingonly the head noun in a noun phrase, eliminatingadverbs from verb phrases and eliminating most ofthe prepositional phrases.
However, if a wordselected for elimination is a key word, proper noun,the name of a place, a date or a number, the word iskept in the sentences.
If this word happens to be ina prepositional phrase, then the prepositionalphrase is kept in the sentence.Once the scoring process is done, the sentencesare ranked and a summary is generated using thesentences with the higher scores that together do175not exceed a predetermined percentage of the doc-ument's length.
This summary is written in thedocument's original language, so a machine trans-lation system is used to produce an English versionof the summary.2.6 Output ProcessAt this point in the summarization process wehave a version of the document's ummary in theoriginal language and a version in English, bothencoded using UNICODE and in plain text format.The Output Process stage takes these two versionsof the summary and converts the one written in theoriginal language to the original encoding of thedocument (identified by the Language Recognitionmodule), then it converts the version in Englishfrom UNICODE to "8859_1" (ISO Latin-l).After the summaries are in the proper outputencoding, the system generates the summary in oneof the following formats: SGML, HTML, E-mailor Plain text according to the user's specification orto system parametrization, for example, if the sum-marization system is being used for web delivery,then the output format will be HTML by default.3 Extending the SummarizationCapabilityOur goal is to improve the usability and flexi-bility of the summarization system, while stillretaining robustness.
This is one of the main rea-sons why we favor the sentence selection methodrather than approaches based on deep analysis andgeneration (Beale 94, Carlson & Nirenburg 90).Though much disparaged for lack of readability,cohesion etc.
systems based in the sentence selec-tion method performed well in the recent Tipstersummarization evaluation.
In fact the readability asassessed by the evaluators was as high for summa-ties of about 30% of the document length as it wasfor the original documents.
We are developingsummarization techniques based on informationextraction and text generation.
These will not givevery good coverage, because of their domain speci-ficity, but do offer advantages, particularly in thearea of cross document summarization.Our experiments have shown for English thatthe inclusion of other language processing tech-niques Can indeed increase the flexibility and per-formance of the summarizer.
In particular propername recognition, co-reference resolution, part ofspeech tagging and partial parsing can all contrib-ute to the performance of the system.The use of proper names allows the summariesto be weighted towards sections of the documentsdiscussing specific individuals or organizationsrather than more general topics.
In terms of pro-duction of informative summaries, rather thanindicative summaries, this may be an importantcapability.
This technique was used to producesummaries evaluated using a "question andanswer" methodology at the Tipster evaluation andproduced ahigh performance here.We have not incorporated co-reference resolu-tion methods in our system yet, but it would seemthat readability can be improved by the ability toreplace pronouns with their referents would be use-ful.
It remains to be seen, however, whether suffi-cient accuracy can be achieved to support thismethod.
In cases like this where an error may becritical for a user of the system we would normallymark the fact that the text had been added by thesystem.Part of speech tagging and phrase recognitionallows us to carry out certain kinds of text compac-tion.
This is particularly important when very shortsummaries (10%) of short documents are required.Our experiments with this kind of compaction haveshowed reductions of about 1/3 of the summarysize with some loss of readability.
A single sen-tence example shows the usefulness of this tech-nique.Original SentenceBrowning-Ferris Industries Inc. was denied anew permit for a huge Los Angeles-area gar-bage dump, threatening more than $1 billion infuture revenue and the value of a $100 millioninvestment.Shortened Sentence176Browning-Ferris Industries Inc. was denied apermit for a Los Angeles-area dump, threaten-ing more than $1 billion in revenue and thevalue of a $100 million investment.We hope eventually to have sentence reductionin place for all the languages we process, and thatthis will also improve the readability of MT outputby allowing it to process significantly simplifiedinput.4 ConclusionsWe feel that further research is warranted onimproving summarization based on sentence selec-tion and that its bad press is largely apocryphal andunjustified.
In fact from a document analysts pointof view material from the original document maybe preferable, carrying as it does, the style and toneof the original document.We see significant opportunities in carrying outfurther research to develop and integrate languageprocessing and other intelligent echniques uch asthose described above.
One particularly challeng-ing type of document is the HTML pages found onthe web.
Here techniques to identify coherent sec-tions of text are required as well as methods forsummarizing tables and groups of frames.References(Beale 94) Beale, S. 1994 Dependency-directed xtgeneration, Technical Report, MCCS-94-272, Com-puting Research Laboratory, New Mexico State Uni-versity, Las Cruces, NM.
(Carlson & Nirenburg 90) Carlson, L. and Nirenburg, S.1990.
World Modeling for NLP.
Center for MachineTranslation, Carnegie Mellon University, TechReport CMU-CMT-90-121.
(Cohen 95) Cohen, J.D.
1995.
Highlights: Language-and domain-independent automatic indexing termsfor abstracting.
Journal of the American Society forInformation Science, 463:162-174.
(Paice & Jones 93) Paice, C.D.
and Jones, RA.
1993.The identification of important concepts in highlystructured technical papers.
In Proceedings of the16th ACM SIGIR conference, Pittsburgh PA, June27-July 1, 1993; pp.69-78.
(Paice 90) Paice, C.D.
1990.
Constructing literatureabstracts by computer: Techniques and prospects.Inf.Proc.
& Management 261,171-186.
(Salton et al 95) Salton, G., Singhal, A., Buckley, C.,and Mitra, M. 1995.
Automatic text decompositionusing text segments and text themes.
TechnicalReport, Department of Computer Science, CornellUniversity, Ithaca, NY.
(Salton & Singal 94) Salton, G. and Singhal, A.
1994.Automatic text theme generation and the analysis oftext structure.
Technical Report TR94-1438, Depart-ment of Computer Science, Cornell University, Ith-aca, N.Y.177
