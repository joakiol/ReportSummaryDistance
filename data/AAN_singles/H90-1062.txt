IMPROVED ACOUSTIC MODELING FOR CONTINUOUS SPEECH RECOGNITIONC.-H. Lee, E. GiachinP , L. R. Rabiner, R. Pieraccini and A. E. RosenbergSpeech Research DepartmentAT&T Bell LaboratoriesMurray Hill, NJ 07974ABSTRACTWe report on some recent improvements o an HMM-based, continuous peech recognition system which isbeing developed at AT&T Bell Laboratories.
Theseadvances, which include the incorporation of inter-word,context-dependent units and an improved featureanalysis, lead to a recognition system which achievesbetter than 95% word accuracy for speaker independentrecognition of the 1000-word, DARPA resourcemanagement task using the standard word-pair grammar(with a perplexity of about 60).
It will he shown that theincorporation of inter-word units into training results inbetter acoustic models of word juncture coarticulationand gives a 20% reduction in error rate.
The effect of animproved set of spectral and log energy features is tofurther reduce word error rate by about 30%.
We alsofound that the spectral vectors, corresponding to the samespeech unit, behave differently statistically, depending onwhether they are at word boundaries or within a word.The results suggest hat intra-word and inter-word unitsshould be modeled independently, even when they appearin the same context.
Using a set of sub-word unitswhich included variants for intra-word and inter-word,context-dependent phones, an additional decrease ofabout 10% in word error rate resulted.1.
INTRODUCTIONIn the past few years there have been proposed a numberof systems for large vocabulary, speaker-independent,continuous peech recognition which have achieved highword recognition accuracy \[1-5\].
The approach to largevocabulary speech recognition we adopt in this paper is apattern recognition based approach.
The basic speechunits in the system use phonetic labels and are modeledacoustically based on a lexical description of words inthe vocabulary.
No assumption is made, a priori, aboutthe mapping between acoustic measurements and sub-word linguistic units such as phonemes; such a mappingt Now with CSELT, Torino, Italy.is entirely learned via a finite training set of utterances.The resulting speech units, which we call phone-likeunits (PLU's) are essentially acoustic descriptions oflinguistically-based units as represented in ttle wordsoccurring in the given training set.In the baseline system reported in \[1\], acoustic modelingtechniques for intra-word context-dependent PLU's werediscussed.
The focus of this paper is to extend the basicacoustic modeling techniques developed in \[1\] to includemodeling of word juncture coarticulation and toincorporate higher-order time derivatives of cepstral andlog energy parameters into the feature vector in order toimprove speech recognition performance.We tested the improved acoustic modeling techniques onspeaker-independent recognition of the DARPA NavalResource Management task using both the word-pair(WP) and the no grammar (NG) conditions.
For theFEB89 test set using the WP grammar, the wordaccuracy improved from 91.3% to 95.0% when both theinter-word context-dependent PLU's and an improvedfeature analysis were incorporated into the baselinesystem.
We also observed that, for the first time, over70% sentence accuracy was achieved.
The same level ofimprovement was also obtained for the OCF89 and theJUN90 test sets.2.
BASELINE RECOGNITION SYSTEMThere are three main modules in the baseline recognitionsystem, namely a feature analysis module, a word-levelacoustic match module and a sentence-level languagematch module.
The speech is filtered from 100 Hz to3.8 kHz, and sampled at an 8 kHz rate and convertedinto a sequence of feature vectors at a frame rate of 10msec.
Each (24-element) feature vector consists of 12liftered cepstral coefficients and 12 delta cepstralcoefficients.
The reader is referred to \[1\] for a detaileddescription of the acoustic analysis procedure.
We willshow later that higher order time derivatives of cepstral319and energy parameters can also be incorporated into thefeature vectors to improve recognition performance.The word-level match module evaluates the similaritybetween the input feature vector sequence and a set ofacoustic word models to determine what words weremost likely spoken.
The word models are generated viaa lexicon and a set of sub-word models.
In our currentimplementation, we use a slightly modified version of alexicon provided by CMU.
Every word in thevocabulary is represented by exactly one entry in thelexicon, and each lexical entry is characterized by alinear sequence of phone units.
Each word model iscomposed as a concatenation of the sequence of sub-word models according to its corresponding lexicalrepresentation.
A set of 47 context-independent phonelabels is extracted from the lexicon and is used as the setof basic speech units throughout his study.
Context-dependent units are also derived from this basic unit set.The sentence-level match module uses a language model(based on a set of syntactic and semantic rules) todetermine the word sequence in a sentence.
In ourcurrent implementation, we assume that the languagemodel is fixed and can be represented by a finite statenetwork (FSN).
The word-level match module and thesentence-level match module work together to producethe mostly likely recognized sentence.2.1 PLU ModelingEach PLU is modeled as a left-to-fight hidden Markovmodel (HMM).
All PLU models have three states, exceptfor the silence PLU model which has only one state.Furthermore, no transition probabilities are used; forwardand self transitions from a state are assumed equallylikely.
The state observation density of every PLU isrepresented by a multivariate Gaussian mixture densitywith a diagonal covariance matrix.To avoid singularity caused by an under-estimation fthe variance, we replaced all estimates whose valueswere in the lowest 20% with an estimate of the 20percentile value of the variance histogram.
It can beshown \[7\] that such a variance clipping strategy results inthe maximum a postedori estimate for the varianceparameter, if we assume that the variance is known apriori to exceed a fixed threshold (which has to beestimated from a set of training data).Training of the set of sub-word units is accomplished bya modified version of the segmental k-means trainingprocedure \[1,8\].
Since acoustic segments for differentPLU labels are assumed independent, models for eachunit can be constructed independently.
By doing so, thecomputation requirement for HMM training is much lessthan the standard forward-backward HMM training.2.2 Creation of Context Dependent PLU'sThe idea behind creating context dependent PLU's is tocapture the local acoustic variability associated with aknown context and thereby reduce the acoustic variabilityof the set of PLU's.
One of the earliest attempts atexploiting context dependent PLU's was in the BBNBYBLOS system where left and right context PLU'swere introduced \[9\].
Given the set of 47 PLU's, a totalof 473 -- 103823 CD PLU's could be obtained m theory.In practice, only a small fraction of them appear inwords for a given task.
However the number of CDPLU's is still very large (on the order of 2000-7000),making even a reasonable amount of training materialinsufficient to estimate all the CD PLU models withacceptable accuracy.
Several techniques have beendevised to overcome these training difficulties.
Perhapsthe simplest way is to use a unit reduction rule \[1\] basedon the number of occurrences of a particular unit in thetraining set.2.3 Experimental Setup and Baseline ResultsThroughout his study, we used the training and testingmaterials for the DARPA naval resource managementtask.
The speech database was originally provided byDARPA at a 16 kHz sampling rate.
To make thedatabase compatible with telephone bandwidth, wefiltered and down-sampled the speech to an 8 kHz ratebefore analysis.
The training set consists of a set of3990 read sentences from 109 talkers (30-40sentences/talker).
For most of our evaluations, we usedthe so-called FEB89 test data which consisted of 300sentences from 10 new talkers (30 sentences/talker) asdistributed by DARPA in February 1989.A detailed performance summary of the baseline systemand the techniques we used to achieve such aperformance is given in \[1\].
In order to provide aconsistent base of performance for comparison with thenew results, we used a threshold of 30 in the unitreduction rule for all performance evaluations.
When nointer-word units were used, a threshold of 30 resulted ina set of 1090 PLU's.
With this set of units, we obtaineda word accuracy of 91.3% and a sentence accuracy of58.7% on the FEB89 test set.3.
WORD JUNCTURE MODELINGCoarticulafion phenomena arising at the boundarybetween words are a major cause of acoustic variabilityfor the initial and final parts of a word when spoken incontinuous peech.
If  this type of contextual variabilityis not adequately represented in the recogmtion system,errors are likely to occur.
This is the case, for instance,when words in the dictionary are phonetically transcribedaccording to their pronunciations in isolation.
The320solution to this problem is to provide a more precisephonetic representation of the region across wordboundaries.
Different approaches may be taken to achievethis goal, depending on which type of coarticulationphenomenon is handled.
In \[10\] we characterized twodifferent types of pronunciation changes at wordjunctures, namely soft and hard changes.
In softchanges, the alteration of a phone due to neighboringphones is comparatively small and the actual realizationis perceived as a variation of the original phone ratherthan a transformation to a different phone.
This alterationis of the same nature as the one observed in intra-wordphones.
By contrast, in hard changes, a boundary phonemay undergo a complete deletion or a substitution by atotally different phone.
It was experimentally shown thatphonological rules were effective in coping with errorsgenerated by hard changes (a 10% error rate reduction\[10\]).
However for soft changes, it has been shown thatthe use of context-dependent phone-like units is mosteffective.
Such units specify context-independent phonesaccording to their context, i.e.
the preceding phone andthe following phone.
Context-dependent phones are notvery effective with hard changes because these changesare comparatively rare and the training material is notsufficient o model the units that would represent them.Soft changes occur more frequently in the training setand hence we can model such changes imilar to the waywe model the set of intra-word units.3.1 Word Juncture Phonological RulesIn the current set-up, phonological rules are employedwith the set of 47 context-independent phone units.
Thephonological rules have been implemented both in thetraining as well as in the recognition procedure.
Thereason for introducing the rules during training is thatthey give a more precise phonetic transcription of thetraining sentences and hence allow a better segmentationand, consequently, better estimation of model parameters.In preliminary experiments, it was seen that about 50%of the rule corrective power is actually due to training.Recognition is based on a modified version of theframe-synchronous beam search algorithm described in\[1\].
A detailed summary of the experimental resultsusing phonological rules can be found in \[10\].
Based onthe set of 47 context-independent PLU's, training on3200 utterances and testing on 750 utterances, we foundthat a 10% error rate reduction can he obtained.Although the set of phonological rules was applied rarelyin both training and in recognition, the application ofphonological rules was shown very effective in dealingwith the hard word juncture changes which are difficultto model statistically.3213.2 Inter-word Context-Dependent UnitsSeveral schemes have been proposed for modeling andincorporating inter-word, context-dependent phones into acontinuous peech recognition system.
The methodologywe adopted for inter-word unit modeling is described indetail in \[11\].
Since the initial phone of each worddepends on the final phone of the preceding word andsimilarly for the final phone of each word, there aremany more context-dependent, inter-word units thanintra-word units.
In the 109 speaker training set, thereare roughly 5500 inter-word units compared to only1,800 intra-word units.
Therefore the training issuesoutlined above become even more complicated.Moreover, in recognition, since it is not known a prioriwhich words will precede or follow any given word, thewords have to be represented with all their possibleinitial and final phones.
In addition, for reasons that willhe explained below, words consisting of one singlephone (e.g.
"a") have to be treated with special care; thusfurther increasing the complication of the recognition andtraining procedures.The complete set of units, including both intra-word andinter-word units, was chosen according to the same unitreduction rule described in \[1\].
By varying the value ofthe threshold T different PLU inventories can be obtainedand tested.
Based on the reduction rules, the inter-wordunits generated include double-context units (triphones),single-context units (diphones) and context-independentunits (monophones).
A typical implementation discussedin the literature \[2\] use only tliphones and monophones;however we found diphones quite helpful in ourimplementation.
Using the same threshold, i.e.
T=30,we obtain a set of 1282 units, including 1101 double-context units, 99 left-context units, 35 right-context unitsand 47 context-independent uni s. It is noted here that inour modeling strategy, every acoustic segment in thetraining data has only one PLU label and is used onlyonce in training to create a sub-word model of theparticular phone unit.Training is based on a modified version of the segmentalk-means training procedure \[1,8\].
However, due to thepresence of inter-word units, cross-word connectionsneed to be handled properly in the segmentation part ofthe procedure.
Segmentation is carried out on the finitestate network (FSN) that represents each utterance interms of PLU's.
Since context-dependent i er-wordPLU's are available, there is no discontinuity betweenwords.
However, optional silences are allowed betweenwords; in such cases PLU's having either a left or rightsilence context are used.The recognizer used in our research is based on amodified version of the flame-synchronous beam searchalgorithm \[1\].
However due to the presence of thecompficated inter-word connection structure, the finitestate network representing the task grammar is convertedinto an efficient compiled network to minimizecomputation and memory overhead.
The reader isreferred to \[12\] for a detailed description of therecognizer implementation.
The recognizer is run withno grammar (i.e.
every word can follow every otherword) or with a word pair grammar.A detailed summary of the experimental results usinginter-word units can be found in \[l l\].
Based on the setof 1282 PLU's, the recognition performance was 93.0%word accuracy and 63.7% sentence accuracy for theFEB89 test set.
A comparison with the results obtainedwithout using inter-word units (i.e.
the 1090 PLU set)shows that a 20% error rate reduction resulted.After a close examination of the recognition results,several observations can be made.
First, there are lesserrors on function words (e.g.
"a", "the", etc.
), because abetter word juncture coarticulation model gives a betterrepresentation of those highly variable short words.Second, we observed a much higher likelihood scorewhen inter-word units are used in both training andrecognition.
We also found that, when an utterance wasmisrecognized, the likelihood difference between therecognized and the correct strings was smaller than thatwith only intra-word units.
This shows that themisrecognized utterance is more likely to be correctedwhen better acoustic modeling techniques areincorporated into the system.
We now discuss sometechniques for obtaining an improved feature set.4.
IMPROVED FEATURE ANALYSISSo far, we have discussed the criteria for the selection ofthe set of fundamental units, shown how to expand theset to include both intra-word and inter-word, context-dependent PLU's and discussed how to properly modelthese units.
In this section, we focus our discussion onan improved front-end feature analysis.
Since we areusing a continuous density HMM approach forcharacterizing each of the sub-word units, it is fairlystraightforward to incorporate new features into ourfeature vectors.
Specifically, we study the incorporationof higher order time derivatives of short-time cepstralfeatures and log energy features, such as the secondcepstral derivatives (delta-delta cepstrum), the log energyderivative (delta energy), and the second log energyderivative (delta-delta energy), into our system.Second Order Cepstral Time DerivativesThe incorporation of first order time derivatives ofcepstral coefficients has been shown useful for bothspeech recognition and speaker verification.
Thus we322were interested in investigating the effects ofincorporating higher order cepstral time derivatives.There are several ways to incorporate the second ordertime-derivative of the cepstral coefficients.
All theexisting approaches evaluate the second derivatives(called delta-delta cepstrum) as the time derivatives ofthe first order time derivative - so called delta cepstrum.The degree of success m using such a strategy for thedelta-delta cepstrum computation was mixed.
The onlycontinuous peech recognition system which used usingthe delta-delta cepstral features was reported by Ney\[13\].
Ney tested the system using speaker independentrecognition of the DARPA naval resource managementtask, and showed a very significant improvement whentesting the recognizer without using any grammar.However, for the word-pair grammar, there was nosignificant improvement in performance.In our evaluation, we used a 3-frame window (i.e.
anoverall window of 70 msecs for both delta and delta-delta cepstrum).
The m 'h delta-delta cepstral coefficientat frame I was approximated asAzct(m) = K - \ [Act+ l (m) -Act_ l (m) l  (1)where Act(m) is the estimated m 'n delta cepstralcoefficient evaluated at frame l, and K is a scalingconstant which was fixed to he 0.375 (no optimizationwas attempted to find a better value of the normalizationconstant o optimize the k-means clustering part of thetraining algorithm).We augment he original 24-dimensional feature vectorwith 12 additional delta-delat cesptral features giving a36-dimensional feature vector.
We then tested this newfeature analysis procedure on the resource managementtask using the word-pair grammar.
We tested cases bothwith and without the use of inter-word units.
The perspeaker word accuracies are summaried in Table 1.
Theeffect of adding the delta-delta cepstral parameters onrecognition performance (using both inter-word andintra-word units) on the set of 1282 PLU's, was that theperformance improved from 93.0% (column 2) to 93.8%(column 3) on the FEB89 test set.
The error ratereduction for the latter case was not as much as theformer case.
For the inter-word case (1282 PLU), weran four iterations of the segmental k-means trainingprocedure.
We also tested models generated in all fouriterations.
It is worth noting that the best averageperformance was achieved using the model generatedfrom the fourth iteration (shown in column 3).
Whencomparing performance on a per speaker basis, theresults showed that the addition of the delta-delta cepstralfeatures to the overall spectral feature vector is notalways beneficial for each speaker.
For example, there isa significant performance degradation for speaker 1(cmhl8) in the 1282 PLU case.
We also observed alarge variability in speaker performance using modelsobtained from various iterations.
We therefore manuallyextracted the best performance among all four iterations,for each speaker, and list the results in column 4 ofTable 1.
It is interesting to note that the average bestperformance is much better (16% reduction in error rate)than the results listed in column 3.
Our conjecture isthat the second order cepstral analysis produces verynoisy observations based on a 30 msec window and a 10msec frame shift.
Another concem is the effectiveness ofeach of the additional features.
In Ney \[13\], a pre-selected set of delta and delta-delta cepstral features wasused.
To be more effective, an automatic featureselection algorithm should be used to determine therelative importance of all spectral analysis features.Speaker ID DCEPcmhl8 90.9dml01 89.7dwa05 89.6esg04 96.7gaw07 95.1grab05 93.5him02 96.3jdh06 92.7kls01 93.4lns03 91.9Average 93.0tTable 1.
Delta-delta1282 PLU setDDCEP BEST88.7 90.992.6 94.590.4 90.497.9 99.296.7 96.795.1 95.996.6 97.693.9 95.193.4 95.591.9 91.993.8 94.8cepstmm test resultsA few observations can be made from the above results.Overall it can be seen that the incorporation of secondorder time derivatives of cepstral parameters improvesrecognition performance significantly.
However thesecond order time derivatives are very noisy in the sensethat they produce features which are not alwaysbeneficial and they are not stable over the segmental k-means training iterations.
From the last column of Table1, it is clear that improved performance could beachieved if we could stablize the features across trainingiterations.
One way to improve the feature analysis is tocarefully select new features o that only features that areuseful for discrimination are included in the featurevector.
Another way is to combine features through aprincipal component analysis.3234.1 Log Energy Time DerivativesThe first order time derivatives of the log energy values,known as delta energy, have been shown useful in anumber of recognition systems.
Most systems use bothenergy and delta energy parameters as features.
In orderto use the energy parameter, careful normalization isrequired.
In our baseline system, the energy parameterwas normalized syllabically.
We did not include theenergy parameter m the feature vector; instead we usedthe energy parameter to assign a penalty term to thelikelihood of the observed feature vector.
However, wehave found that the delta and delta-delta energyparameters are more robust and more effectiverecognition features.
Similar to the evaluation of thedelta cepstrum, the delta energy at frame l isapproximated as a linear combination of the energyparameters m a 5 frame window centered at frame l.Since the energy parameter has a wider dynamic rangein value, we used a smaller constant (0.0375) for theevaluation of the delta energy.
Again, we did notattempt to optimize the k-means clustering part byadjusting the normalization constant.The second order time derivatives of the energyparameters, called the delta-delta energy, are computedsimilar to the way the delta-delta cepstral features areevaluated.
Starting with the 24-element feature vector,by adding delta-delta cepstrum, delta energy and delta-delta energy to the feature set, for every frame 1, we havea 38-element feature vector O~ of the formOt = {cl(l:12), A~t(l:12), A2ct(l:12), Aet, A2et} (2)where ~t(l:12) and A~t(l:12) are the 12 liftered cepstralcoefficients and the 12 delta cepstral coefficients; andA2ct(l:12), Aet and A2e t are the additional 12 delta-deltacepstral coefficients, the delta energy parameter and thedelta-delta energy parameter at frame l respectively.We tested the use of energy time derivatives on theFEB89 test with the word pair grammar.
All the testsused the set of 1282 PLU's, and the test results aresummarized in Table 2.
When compared with the resultsshown in Table 1, we observed a very significant overallimprovement when delta energy is incorporated (shownin column 2).
We also note that the improvement variesfrom speaker to speaker.
For example, delta energyhelps eliminate a lot of word errors for speaker 1(cmhl8).
When delta-delta energy is added to form a38-dimensional feature vector, we observe the sameeffect as shown in Table 1, i.e.
delta-delta energy is notalways beneficial.
However, for some talkers (cmhl8,dwa05 and lns03) the improvement is significant.
Thereis no overall improvement; however all the test speakersachieve over 92% word accuracy.
We show in column 4,the best achievable performance for each talker usingvarious feature combinations.
The best achievableaverage performance is over 96% word accuracy.Speaker ID +DENG +DDENGcmhl8 93.0 95.2dml01 95.5 94.5dwa05 91.5 92.2esg04 !
97.9 97.5gaw07 95.9 96.3gmb05 96.7 95.9him02 96.6 95.9jdh06 94.7 94.3kls01 96.3 94.7Ins03 91.5 92.6IAverage 94.9 94.9BEST95.795.593.099.Z97.196.797.695.597.193.096.1Table 2.
Log energy time derivatives test results5.
POSITION-DEPENDENT UNIT MODELINGFor all our experiments we have selected the set of basicspeech units based on context.
However, it is believedthat the spectral features of units within words behavedifferently acoustically from those of units at the wordboundaries even when the units appear in the samecontext.
We investigated this conjecture by selectingintra-word and inter-word units independently based onthe same unit reduction rule.
We call such a selectionstrategy position-dependent unit selection.
With athreshold of 30, we obtained a total of 1769 units,including 913 intra-word and 856 inter-word units.
Forthe ihtra-word units, we have 639 units in doublecontext, 98 left-context and 176 right-context units and47 context-independent u its.
For the inter-word units,we end up with 480 units in double context, 310 left-context, 2 right-context units and 46 context-independentunits.
We use the same modeling strategy described inSection 3, except that when creating the FSN forsegmentation and recognition, only inter-word units canappear at the word boundaries and only intra-word unitscan appear within words.Using such a position-dependent unit selection strategy,we found that all the sub-word unit models are morefocused in the sense that the spectral variability is lessthan the case in which we combine common intra-wordand inter-word unit models.
Two interestingobservations are worth mentioning.
First, whenrecognition is performed with the beam search algorithm,the number of alive nodes is much less in the 1769 PLUcase than that in the 1282 PLU case.
Second, the unitseparation (in terms of likelihood) distance is larger forthe 1769 PLU set than that for the 1282 PLU set.
Theunit separation distance is measured as follows.
We firstcollect all sets of PLU's such that all the units whichhave the same middle phone symbol p are grouped intothe same set S e regardless of their context andenvironment.
We compute the distance between eachpair of units m a set.
The distance between units P2 andP1 is defined as the difference between averagelikelihoods of observing all acoustic segments with labelP2 and observing all acoustic segments with label P1given the model for unit P l .
For each unit, we thendefine the unit separation distance as the smallestdistance among all other units m the same set.
Whenexaming the histogram for the unit separation distancesfor the 1769 PLU set, we found a quite remarkable unitseparation.
Almost all the unit separation distances arelarger than 2 (in log likelihood).
It is also interesting tonote that units appearing in the same context but in adifferent environment (i.e.
intra-word versus inter-word)show the same behavior as units appearing in differentcontext.
The average unit separation distance is about 9.For the 1282 PLU set, the histogram plot is skewed tothe left and the unit separation characteristics are not aspronounced as those of the 1769 PLU set.Results on the resource management task using the 1769PLU set showed a significant improvement (about 10%error reduction) in performance over the 1282 PLU setusing both the WP grammar and the NG grammar.
Wealso tested the OCT89 set which consists of 300 testutterance (30 each from a group of 10 speakers), and theJUN90 set which consists of 480 test utterances (120each from a group of two female and two male talkers).Detailed results for all three test sets using both the WPand NG cases are summarized in Tables 3 and 4respectively.
A careful examination of the word errorpatterns hows that function words still account for about60% of the word errors.
The second dominant category,which accounts for more than 20% of the errors, areconfusions involving the same root word (e.g.
locationversus locations, six versus sixth, Flint versus Flint's,chop versus chopped, etc.)
appearing in different forms.This type of errors can easily be corrected with a simpleset of syntactic and semantic rules.324Testing Set FEB89 OCT89IWord Corr.
96.1 96.3Sub.
Error 2.9 2.7Del.
Error 1.0 0.9Ins.
Error 0.7 0.9Word Error 4.6 4.5Sent.
Acc.
74.7 73.7Word Acc.
95.4 95.5JUN9095.63.31.10.54.971.595.1Table 3.
WP test summary using the 1769 PLU setTesting Set FEB89 OCT89Word Corr.
81.8 81.9Sub.
ErrorDel.
Error14.33.814.53.8Ins.
Error 2.1 2.6Word Error 20.3 \] 20.7Sent.
Acc.
27.3 26.3Word Acc.
79.7 79.3JUN9078.915.65.51.422.524.277.5Table 4.
NG test summary using the 1769 PLU setIt is interesting to note that for the speaker-independentpart of the evaluation data, we achieved virtually thesame level of performance, 95.5% word accuracy andover 70% sentence accuracy, for both the FEB89 andOCT89 test data.
However, for the JUN90 test set, theword accuracy fell to 94.9%.
The sentence accuracy of70.2% was also considerably lower than that for theFEB89 and OCT89 test sets.
This was due to the factthat one of the female test talkers 0rm08) gave a verypoor result, namely 90.8% word accuracy.
The medianword accuracy among all four JUN90 test talkers wasover 95.5%.
Another possible explanation for thisperformance degradation is that the speaker-dependentpart of the test utterances was generated from a set ofsentence patterns whose context was significantlydifferent from the set of sentences used to generate the109-speaker t aining set.
This bnngs up the issue of theadequacy of training data.
The above discussionsuggests that a task-specific training set is more usefulthan a task-independent training set.
This agreessomewhat with results reported in \[14\], where the socalled "vocabulary-independent" training procedure iseffective only when most of the task-specific tfiphonecontexts appear in the training corpus.Even though only a small improvement in performancewas obtained in our test, we believe the real benefit ofincorporating position-dependent PLU modeling lies inthe area of model prediction for units appearing ratherinfrequently in the training data.
We are nowexperimenting with a unit expansion rule, which uses theunit reduction nile first (based on a lower countthreshold) to get a set of auxiliary speech units which arethe units to be expanded into a complete set.
The onlyremaining issue is how to predict the model for units inthe auxiliary set.
For each existing model, we computethe likelihood of observing all the acoustic segmentscorresponding to each auxiliary unit.
Then we assign themodel that is the closest to each of the auxiliary speechunits.
The effectiveness of such an approach is yet to beevaluated.6.
SUMMARYWe have reported on several improvements to one of thespeaker-independent, continuous speech recognitionsystems developed at AT&T Bell Laboratories.
Theimproved acoustic modeling, including incorporation ofinter-word units and an improved feature analysis,provided high word accuracies for all three DARPAevaluation sets using the word pair grammar.
We havealso developed a unit selection rule for selecting intra-word and inter-word units independently.
We anticipatethat with the proposed unit expansion rule, an even betterset of units can be obtained which will further improveacoustic modeling techniques for continuous speechrecognition.Based on current recognition performance it seems fair tosay that when task-specific training data are provided foracoustic modeling of the set of basic speech units usingHMM's, high performance can be achieved for a largevocabulary, speaker-independent, continuous speechrecognition task with a perplexity of about 60.
However,there are still some open issues that need to beaddressed.
The reader is referred to a recent paper \[15\]for a discussion of some of those issues related to usingHMM's for speech recognition.
We list, in the following,a number of acoustic modeling issues which we believeto be essential for expanding the capabilities of ourcurrent continuous peech recognition system.
They are:(1) Speech unit selection and modeling for task-independent applications; (2) Improved worddiscrimination based on some form of corrective trainingfor continuous density HMM parameters (e.g.
\[16\]); (3)Lexical modeling to deal with lexical variability inbaseform pronunciation; and (4) Improved featureselection so that only those features useful todiscrimination are included in the feature vector.Our tasks so far have been mainly focused on speechrecognition.
We observed that short function words (e.g.
"a", "the") are a major source of recognition errors.However, most of those errors can be corrected using aset of simple syntactic and semantic rules operating in a325post-processing mode.
For example, for the resourcemanagement task, we have developed a language decoder(decoupled from the acoustic decoder) that incorporates aset of simple rules.
Our preliminary results \[17\] indicatethat sentence accuracy for the FEB89 test set improvedfrom 70% to close to 90% (with 98% word accuracy)when the top candidate string decoded using the word-pair grammar is used as input to this language decoder.When no grammatical constraints were used in speechdecoding, the sentence accuracy improved from 24% to67% (with 90% word accuracy).
Except in cases wheresome key content words were misrecognized, the simplelanguage analyzer properly decoded the noisy stringsprovided by the speech decoder without going back tothe acoustic domain to request mismatch information.We believe the language decoder can be more effective ifthe speech decoder can provide more word and stringhypotheses.
One way to get more information is to usethe N-best string search strategies.
Another way is toconstruct, in acoustic decoding, a phone lattice and aword lattice that contain more word hypotheses, and thengenerate recognized strings according to the languageconstraints.
The effectiveness of such approaches in realspoken language tasks, such as the DARPA Air TravelInformation System (ATIS) task, is yet to be evaluated.REFERENCES1.
C.-H. Lee, L. R. Rabiner, R. Pieraccini, J. G.Wilpon, "Acoustic Modeling for Large VocabularySpeech Recognition", Computer Speech andLanguage, 4, pp.
127-165, 1990.2.
K. F. Lee, Automatic Speech Recognition - TheDevelopment of the SPHINX System, KluwerAcademic Publishers, Boston, 1989.3.
D. B. Paul, "The Lincoln Robust ContinuousSpeech Recognizer," Proc.
ICASSP-89, Glasgow,Scotland, pp.
449-452, May 1989.4.
M. Weintraub et al, "Linguistic Constraints inHidden Markov Model Based SpeechRecognition," Proc.
ICASSP-89, Glasgow,Scotland, pp.
699-702, May 1989.5.
V. Zue, J.
Glass, M. Phillips, and S. Seneff, "TheMIT Summit Speech Recognition System: AProgress Report," Proc.
Speech and NaturalLanguage Workshop, pp.
179-189, Feb. 1989.6.
L. R. Rabiner, "A Tutorial on Hidden MarkovModels, and Selected Applications in SpeechRecognition," Proc.
IEEE, Vol.
77, No.
2,pp.
257-286, Feb. 1989.7.
C.-H. Lee, C.-H. Lin and B.-H. Juang, "A Studyon Speaker Adaptation of the Parameters forContinuous Density Hidden Markov Models", toappear in IEEE Trans.
on Acoustic, Speech andSignal Proc.8.
L.R.
Rabiner, J. G. Wilpon, and B. H. Juang, "ASegmental K-Means Training Procedure forConnected Word Recognition," AT&T Tech.
J.,Vol.
65, No.
3, pp.
21-31, May-June 1986.9.
R. Schwartz et al, "Context Dependent Modelingfor Acoustic-Phonetic Recognition of Continuous'Speech," Proc.
ICASSP 85, Tampa, Florida,pp.
1205-1208, March 1985.10.
E. Giachin, A. E. Rosenberg and C.-H. Lee, "WordJuncture Coarticulation Modeling UsingPhonological Rules for HMM-based ContinuousSpeech Recognition", Proc.
ICASSP 90, pp.
737-740, Albuquerque, NM, April 1990.11.
E. Giachin, C.-H. Lee, L. R. Rabiner and R.Pieraccini, "Word Juncture Modeling Using Inter-Word Context-Dependent Phone-Like Units",submitted for publication.12.
R. Pieraccini, C.-H. Lee, E. Giachin and L. R.Rabiner, "Implementation Aspects of LargeVocabulary Recognition Based on Intra-word andInter-word Phonetic Units", Proc.
DARPA Speechand Natural Language Workshop, Somerset, PA,June 1990.13.
H. Ney, "Acoustic-Phonetic Modeling UsingContinuous Mixture Densities for the 991-WordDARPA Speech Recognition Task," Proc.
ICASSP90, pp.
713-716, Albuquerque, NM, April 1990.14.
H.W.
Hon, K. F. Lee, and R. Weide, "TowardsSpeech Recognition Without Vocabulary SpecificTraining," Proc.
EuroSpeech 89, pp.
481-484,Paris, France, September 1989.15.
B.-H. Juang and L. R. Rabiner, "Issues in UsingHidden Markov Models for Speech Recognition,"to appear in Advances in Speech Signal Processing,S.
Furui and M. Sondhi editors, Marcel DekkerInc., New York, 1990.16.
S. Katagifi and C.-H. Lee "A New HMM/LVQHybrid Algorithm for Speech Recognition," toappear in Proc.
GLOBECOM-90, San Diego, CA,December 1990.17.
R. Pieraccini, K.-Y.
Su and C.-H. Lee, unpublishedwork.32G
