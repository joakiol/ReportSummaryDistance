On Using Written Language Training Data forSpoken Language ModelingR.
Schwartz, L. Nguyen, F. Kubala, G. Chou, G. Zavaliagkos t, J. MakhoulBBN Systems and TechnologiesCambridge, MA 02138tNortheastern UniversityABSTRACTWe attemped to improve recognition accuracy by reduc-ing the inadequacies of the lexicon and language model.Specifically we address the following three problems: (1)the best size for the lexicon, (2) conditioning written textfor spoken language recognition, and (3) using additionaltraining outside the text distribution.
We found that in-creasing the lexicon 20,000 words to 40,000 words re-duced the percentage of words outside the vocabularyfrom over 2% to just 0.2%, thereby decreasing the errorrate substantially.
The error rate on words already in thevocabulary did not increase substantially.
We modifiedthe language model training text by applying rules to sim-ulate the differences between the training text and whatpeople actually said.
Finally, we found that using anotherthree years' of training text - even without the appropri-ate preprocessing, substantially improved the languagemodel We also tested these approaches on spontaneousnews dictation and found similar improvements.1.
INTRODUCTIONSpeech recognition accuracy is affected as much by the lan-guage model as by the acoustic model.
In general, the worderror rate is roughly proportional to the square root of theperplexity of the language model.
In addition, in a naturalunlimited vocabulary task, a substantial portion of the worderrors come from words that are not even in the recognitionvocabulary.
These out-of-vocabulary (OOV) words have nochance of being recognized correctly.
Thus, our goal is toestimate a good language model from the available trainingtext, and to determine a vocabulary that is likely to coverthe test vocabulary.The straightforward solution to improving the languagemodel might be to increase the complexity of the model(e.g., use a higher order Markov chain) and/or obtain morelanguage model training text.
But this by itself will not nec-essarily provide a better model, especially if the text is notan ideal model of what people will actltally say.
The simplesolution to increase the coverage of the vocabulary is to in-crease the vocabulary size.
But this also increases the worderror rate and the computation and size of the recognitionprocess.In this paper we consider several simple techniques for im-proving the power of the language model.
First, in Section3, we explore the effect of increasing the vocabulary size onrecognition accuracy in an unlimited vocabulary task.
Sec-ond, in Section 4, we consider ways to model the differencesbetween the language model Iraining text and the way peo-ple actually speak.
And third, in Section 5, we show thatsimply increasing the amount of language model traininghelps significantly.2.
THE WSJ  CORPUSThe November 1993 ARPA Continuous Speech Recognition(CSR) evaluations was based on speech and language takenfrom the Wall Street Journal (WSJ).
The standard languagemodel training text was estimated from about 35 millionwords of text extracted from the WSJ from 1987 to 1989.The text was normalized (preprocessed) with a model forwhat words people use to read open text.
For example,"$234.56" was always assumed to be read as "two hundredthirty four dollars and fifty six cents".
"March 13" wasalways normalized as "March thirteenth" - not "March thethirteenth", nor "March thirteen".
And so on.The original processed text contains about 160,000 uniquewords.
However, many of these are due to misspellings.Therefore, the test corpus was limited to those sentences thatconsisted only of the most likely 64,000 words.
While thisvocabulary is still quite large, it has two beneficial effects.First, it greatly reduces the number of misspellings in thetexts.
Second, it allows implementations to use 2-byte datafields to represent the words rather than having to use 4bytes.The "standard" recognition vocabulary was defined as themost likely 20,000 words in the corpus.
Then, the standardlanguage model was defined as a trigram language modelestimated specifically for these 20K words.
This standardmodel, provided by Lincoln Laboratory, was to be used forthe controlled portion of the recognition tests.
In addition,participants were encouraged to generate an improved lan-guage model by any means (other than examining the testdata).943.
RECOGNIT ION LEX ICONWe find that, typically, over 2% of the word occurrences in adevelopment set are not included in the standard 20K-wordvocabulary.
Naturally, words that are not in the vocabu-lary cannot be recognized accurately.
(At best, we mighttry to detect hat there is one or more unknown words atthis point in a sentence, and then attempt to recognize thephoneme sequence, and then guess a possible letter sequencefor this phoneme sequence.
Unfortunately, in English, evenif we could recognize the phonemes perfectly, there aremany valid ways to spell a particular phoneme sequence.
)However, in addition to this word not being recognized, weoften see that one or two words adjacent to this missing wordare also misrecognized.
This is because the recognition, inchoosing a word in its vocabulary, also now has the wrongcontext for the following or preceding words.
In general,we find that the word error rate increases by about 1.5 to 2times the number of out-of-vocabulary (OOV) words.One simple way to decrease the percentage of OOV wordsis to increase the vocabulary size.
But which words shouldbe added?
The obvious solution is to add words in order oftheir relative frequency within the full text corpus.
Thereare several problems that might result from this:1.
The vocabulary might have to be extremely large be-fore the OOV rate is reduced significantly.2.
If the word error rate for the vast majority of the wordsthat are already in the smaller vocabulary increased byeven a small amount, it might offset any gain obtainedfrom reducing the OOV rate.3.
The language model probabilities for these additionalwords would be quite low, which might prevent themfrom being recognized anyway.We did not have phonetic pronunciations for all of the 64Kwords.
We sent a list of the (approximately 34K) words forwhich we had no pronunciations toBoston University.
Theyfound pronunciations for about half (18K) of the words intheir (expanded Moby) dictionary.
When we added thesewords to our WSJ dictionary, we had a total of 50K wordsthat we could use for recognition.The following table shows the percentage ofOOV words as afunction of the vocabulary size.
The measurement was doneon the WSJ1 Hubl "20K" development test which has 2,464unique words with the total count of 8,227 words.
Due to theunavailability of phonetic pronunciations (mentioned above),the final vocabulary size would be the second column.We were somewhat surprised to see that the percentage ofOOV words was reduced to only 0.17% when the lexiconincluded the most likely 40K words - especially given thatmany of the most likely words were not available becausewe did not have phonetic pronunciations for them.
Thus,Top N Vocab.
#OOV %20k 19998 187 2.2730k 28247 85 1.0340k 35298 39 0.4748k 40213 14 0.1750k 41363 12 0.1564k 48386 1 0.01it was not necessary to increase the vocabulary above 40Kwords.The second worry was that increasing the vocabulary by toomuch might increase the word error rate due to the increasednumber of choices.
For example, normally, if we double thevocabulary, we might expect an increase in word error rateof about 40%!
So we performed an experiment in whichwe used the standard 20K language model for the 5K de-velopment data.
We found, to our surprise, that the errorrate increased only slightly, from 8.7% to 9.3%.
Therefore,we felt confident that we could increase the vocabulary asneeded.We considered possible explanations for the small increasein error due to a larger vocabulary.
We realized that the an-swer was in the language model.
In the first case, when wejust increase the vocabulary, the new words also have thesame probability in the language model as the old words.However, in this case, all the new words that were addedhad lower probabilities (at least for the unigram model) thanthe existing words.
Let us consider two possibilities that wewould not falsely substitute a new word for an old one.
Ifthe new word were acoustically similar to one of the wordsin the test (and therefore similar to a word in the originalvocabulary, then the word would be correctly recognized be-cause the original word would always have a higher languagemodel probability.
If, on the other hand, the new word wereacoustically very different from the word being spoken, thenwe might expect hat our acoustic models would prevent thenew word from being chosen over the old word.
While theargument makes some sense, we did not expect he loss forincreasing the vocabulary from 5K words to 20K words tobe so small.Finally, the third question is whether the new words wouldbe recognized when they did occur, since (as mentionedabove) their language model probabilities were generallylow.
In fact, we found that, even though the error rate forthese new words was higher than for the more likely words,we were still able to recognize about 50% to 70% of themcorrectly, presumably based largely on the acoustic model.Thus, the net effect of this was to reduce the word error rateby about 1% to 1.5%, absolute.954.
MODEL ING SPOKEN LANGUAGEAnother effect hat we worked on was the difference betweenthe l~:ocessed text, as defined by the preprocessor, and thewords that people actually used when reading WSJ text.
Inthe pilot WSJ corpus, the subjects were prompted with textsthat had already been "normalized", so that there was noambiguity about how to read a sentence.
However, in theWSJ1 corpus, subjects were instructed to read the originaltexts and to say whatever seemed most appropriate to them.Since the WSJ1 prompting texts were not normalized todeterministic word sequences, ubjects howed considerablevariability in their reading of the prompting text.However, the standard language model was derived from thenormalized text produced by the preprocessor.
This resultedin a mismatch between the language model and the actualword sequences that were spoken.
While the preprocessorwas quite good at predicting what people said most of thetime, there were several cases where people used differentwords than predicted.
For example, the preprocessor p e-dicted that strings like "$234" would be read as "two hun-dred thirty four dollars".
But in fact, most people read thisas "two hundred AND thirty four dollars".
For another ex-treme example, the preprocessor's prediction of "10.4" was"ten point four", but the subject (in the WSJ1 developmentdata) read this as "ten and four tenths".
There were manyother similar examples.The standard model for the tests was the "nonverbalizedpunctuation" (NVP) model, which assumes that the reeadersnever speak any of the punctuation words.
The other modelthat had been defined was the "verbalized punctuation" (VP)model, which assumed that all of the punctuation was readout loud.
This year, the subjects were instructed that theywere free to read the punctuation out loud or not, in what-ever way they feel most comfortable.
It turns out that peopledidn't verbalize most punctuation.
However, they regularlyverbalized quotation marks in many different ways that wereall different than the ways predicted by the standard prepro-cessor.There were also several words that were read differently bysubjects.
For example, subjects pronounced abbreviationslike, "CORP." and "INC.".
While the preprocessor assumedthat all abbreviations would be read as full words.We used two methods to model the ways people actuallyread text.
The simpler approach was to include the text ofthe acoustic training data in the language model training.That is, we simply added the 37K sentence transcriptionsfrom the acoustic training to the 2M sentences of trainingtext.
The advantage of this method is that it modeled whatpeople actually said.
The system was definitely more likelyto recognize words or sequences that were previously impos-sible.
The problem with this method was that the amount oftranscribed speech was quite small (about 50 times smaller)compared to the original training text.
We tried repeatingthe transcriptions several times, but we found that the effectwas not as strong as we would like.A more powerful approach was to simulate the effects of thedifferent word choices by simple rules which were appliedto all of the 35M words of language training .text.
We choseto use the following rules:Preprocessed TextHUNDRED \[number\]ONE HUNDREDONE DOLLARZERO POINT \[number\]AND ONE HALFAND ONE QUARTERSimulated TextHUNDRED AND \[number\]A HUNDREDA DOLLARPOINT \[number\]AND A HALFAND A QUARTERThus, for example, ff the sentence consists of the pattern"hundred twenty", we repeated the same sentence with "hun-dred AND twenty".The result was that about one fifth of the sentences in theoriginal corpus had some change reflecting a difference inthe way subjects read the original text.
Thus, this was equiv-alent in weight to an equal amount of training text to theoriginal text.We found that this preprocessing of the text was sufficient tocover most of those cases where the readers aid things dif-ferently than the predictions.
The recognition results howedthat the system now usually recognized the new word se-quences and abbreviations correctly.5.
INCREASING THE LANGUAGE MODELTRAIN INGWhile 35M words may seem like a lot of data, it is notenough to cover all of the trigrams that are likely to occurin the testing data.
So we considered other sources for ad-ditional anguage modeling text.
The only easily accessibledata available was an additional 3 years (from 1990-1992)of WSJ data from the TIPSTER corpus produced by theLinguistic Data Consortium (LDC).However, there were two problems with using this data.First, since the test data was known to come from 1987-1989, we were concerned that this might actually hurt per-forrnance due to some differences in the topics during that3-year period.
Second, this text had not been normalizedwith the preprocessor and we did not have available to usthe preprocessor that was used to transform the raw text intoword sequences.We decided to use the new text with minimal processing.The text was filtered to remove all tables, captions, num-bers, etc.
We replaced each initial example of double-quote(") with "QUOTE and the matching token with "UNQUOTEor "ENDQUOTE, which were the most common ways thesewords were said.
No other changes were made.
We just96used the raw text as it was.
One benefit of this was that ab-breviations were left as they appeared in the text rather thanexpanded.
Any numbers, dates, dollar amounts, etc, werejust considered "unknown" words, and did not contribute tothe training.
We assumed that we had sufficient examplesof numbers in the original text.We found that adding this additional language training damreduced the er~r by about 7% of the error, indicating that theoriginal 35 million words was not sufficient for the modelswe were using.
Thus, the addition of plain text, even thoughit was from a different hree years, and had many gaps dueto apparent unknown words, still improved the recognitionaccuracy considerably.6.
RESULTSThe following table shows the benefit of the enlarged 40Klexicon and the enhanced language model training on theOOV rate and the word error for the development test andthe evaluation test.% OOV % Word ErrorTest Set 20K 40K 20K 40KDevelopment 2.27 0.17 16.4 12.9Evaluation 1.83 0.23 14.2 12.2Surprisingly, the addition of three year's LM training (froma period post-dating the test data) improved performance onthe utterances that were completely inside the vocabulary.Evidently, even the common trigrams are poorly trained withonly the 35 million word WSJ0 corpus.
Overall, our mod-ifications to the lexicon and grammar training reduced theword error by 14--22%.7.
Spontaneous DictationAnother area we investigated was spontaneous dictation.The subjects were primarily former or practicing journal-ists with some experience at dictation.
They were instructedto dictate general and financial news stories that would beappropriate for a newspaper like WSJ.
In general, the jour-nalists chose topics of recent interest.
This meant hat theoriginal anguage model was often out of date for the sub-ject.
As a result, the percentage of OOV words increased(to about 4%), and the language model taken from WSJ textwas less appropriate.The OOV words in the spontaneous data were more likelyto be proper nouns from recent events that were not coveredby the LM training material.
To counter this, we added all(1,028) of the new words that were found in the spontaneousportion of the acoustic training data in WSJ1.
This mostlyincluded topical names (e.g., Hillary Rodham, NAFTA, etc.
).In order to account for some of the differences between theread text and the spontaneous text, and to have languagemodel probabilities for the new words, we added the train-ing transcriptions of the spontaneous dictation (about 8Ksentences) to the LM training as well.New weights for the new language model, HMM, and Seg-mental Neural Network were all optimized on spontaneousdevelopment test data.
The table below shows that the OOVremains near 1% even after the enlargement to a 41K lexi-con.% OOV % Word ErrorTest Set 20K 40K 41K 20K 41KDevelopment 2.9 1.4 0.8 - 21.7Evaluation 4.8 1.9 1.5 24.7 19.1As can be seen, increasing the vocabulary size from 20Kto 40K significantly reduced the OOV rate.
It is importantto point out that in this case, we did not have the ben-efit of a word frequency list for spontaneous speech, andthat the source of speech had an unlimited vocabulary.
Sothe reduction in OOV rate is certainly a fair - if not pes-simistic - estimate of the real benefit from increasing thevocabulary.
Adding the few new words observed in thespontaneous speech also helped somewhat, but not nearly asmuch.
The sample of only 8,000 sentences i clearly notsufficient o find all the new words that people might use.Presumably, if the sample of spontaneous speech were largeenough to derive word frequencies, then we could choose amuch better list of 40K words with a lower OOV rate.Overall, the 41K trigram reduces the word error by 23%over the 20K standard trigram on the November '93 CSR $9evaluation test.
We estimate that more than half of this gainwas due to the decreased percentage of OOV words, and theremainder was due to the increased language model training,including specific examples of spontaneous dictation.8.
CONCLUSIONSWe found the following interesting results:?
Expanding the vocabulary with less frequent wordsdoes not substantially increase the word error on thosewords already in the vocabulary, but does eliminatemany errors due to OOV words.?
Doubling the amount of language model training textimproves the language model, even though the textcomes from different years than the test, and eventhough the text was not preprocessed into proper lexi-cal forms.?
It is possible to improve the quality of the languagemodeling text by modeling the differences between the97predicted rre~ding style and some examples of actualtranscriptions.?
Increasing the vocabulary size and language traininghadL a bigger effect on spontaneous speech than it didfor read speech.!9.
ACKKNOWLEDGEMENTThis work was supported by the Advanced Research ProjectsAgency and monitored by the Office of Naval Research un-der conlIact No.
N00014-92-C-0035.References1.
Bates, M., R. Bobrow, P. Fung, R. Ingria, F. Kubala,J.
Makhoul, L. Nguyen, R. Schwartz, D. Stallard,'Whe BBN/HARC Spoken Language UnderstandingSystem", Proc.
of lEEE ICASSP-93, Minneapolis, MN,April 1993, pp.
111-114, vol.
II.2.
PIaceway, P., R. Schwartz, P. Fung, L. Nguyen, "TheEstimation of Powerful Language Models from Smalland Large Corpora", Proc.
of IEEE ICASSP-93, Min-neapolis, MN, April 1993, vol.
II, pp.
33-36.98
