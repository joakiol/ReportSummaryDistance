2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 636?640,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsComparing HMMs and Bayesian Networks for Surface RealisationNina DethlefsHeriot-Watt UniversityEdinburgh, Scotlandn.s.dethlefs@hw.ac.ukHeriberto Cuaya?huitlGerman Research Centre for Artificial IntelligenceSaarbru?cken, Germanyheriberto.cuayahuitl@dfki.deAbstractNatural Language Generation (NLG) systemsoften use a pipeline architecture for sequen-tial decision making.
Recent studies how-ever have shown that treating NLG decisionsjointly rather than in isolation can improve theoverall performance of systems.
We presenta joint learning framework based on Hierar-chical Reinforcement Learning (HRL) whichuses graphical models for surface realisation.Our focus will be on a comparison of BayesianNetworks and HMMs in terms of user satis-faction and naturalness.
While the former per-form best in isolation, the latter present a scal-able alternative within joint systems.1 IntroductionNLG systems have traditionally used a pipeline ar-chitecture which divides the generation process intothree distinct stages.
Content selection chooses?what to say?
and constructs a semantic form.
Ut-terance planning organises the message into sub-messages and surface realisation maps the seman-tics onto words.
Recently, a number of studieshave pointed out that many decisions made at thesedistinct stages require interrelated, rather than iso-lated, optimisations (Angeli et al, 2010; Lemon,2011; Cuaya?huitl and Dethlefs, 2011a; Dethlefs andCuaya?huitl, 2011a).
The key feature of a joint archi-tecture is that decisions of all three NLG stages shareinformation and can be made in an interrelated fash-ion.
We present a joint NLG framework based onHierarchical RL and focus, in particular, on the sur-face realisation component of joint NLG systems.We compare the user satisfaction and naturalnessof surface realisation using Hidden Markov Models(HMMs) and Bayesian Networks (BNs) which bothhave been suggested as generation spaces?spacesof surface form variants for a semantic concept?within joint NLG systems (Dethlefs and Cuaya?huitl,2011a; Dethlefs and Cuaya?huitl, 2011b) and in iso-lation (Georgila et al, 2002; Mairesse et al, 2010).2 Surface Realisation for Situated NLGWe address the generation of navigation instruc-tions, where e.g.
the semantic form (path(target =end of corridor) ?
(landmark = lif t ?
dir =left)) can be expressed as ?Go to the end of thecorridor?, ?Head to the end of the corridor past thelift on your left?
and many more.
The best realisa-tion depends on the space (types and properties ofspatial objects), the user (position, orientation, priorknowledge) and decisions of content selection andutterance planning.
These can be interrelated withsurface realisation, for example:(1) ?Follow this corridor and go past the lift on yourleft.
Then turn right at the junction.?
(2) ?Pass the lift and turn right at the junction.
?Here, (1) is appropriate for a user unfamiliar with thespace and a high information need, so that more in-formation should be given.
For a familiar user, how-ever, who may know where the lift is, it is redundantand (2) is preferable, because it is more efficient.
Anunfamiliar user may get confused with just (2).In this paper, we distinguish navigation of des-tination (?go back to the office?
), direction (?turnleft?
), orientation (?turn around?
), path (?follow the636corridor?)
and straight?
(?go forward?)
in the GIVEcorpus (Gargett et al, 2010).
Users can react to aninstruction by performing the action, performing anundesired action, hesitating or requesting help.3 Jointly Learnt NLG: Hierarchical RLwith Graphical ModelsIn a joint framework, each subtask of content selec-tion, utterance planning and surface realisation hasknowledge of the decisions made in the other twosubtasks.
In an isolated framework, this knowledgeis absent.
In the joint case, the relationship betweenhierarchical RL and graphical models is that the lat-ter provide feedback to the former?s surface realisa-tion decisions according to a human corpus.Hierarchical RL Our HRL agent consists of ahierarchy of discrete-time Semi-Markov DecisionProcesses, or SMDPs, M ij defined as 4-tuples <Sij, Aij , T ij , Rij >, where i and j uniquely identifya model in the hierarchy.
These SMDPs representgeneration subtasks, e.g.
generating destination in-structions.
Sij is a set of states, Aij is a set of ac-tions, and T ij is a probabilistic state transition func-tion that determines the next state s?
from the currentstate s and the performed action a.
Rij(s?, ?
|s, a) isa reward function that specifies the reward that anagent receives for taking an action a in state s last-ing ?
time steps.
Since actions in SMDPs may takea variable number of time steps to complete, the ran-dom variable ?
represents this number of time steps.Actions can be either primitive or composite.
Theformer yield single rewards, the latter correspond toSMDPs and yield cumulative rewards.
The goal ofeach SMDP is to find an optimal policy pi?
that max-imises the reward for each visited state, accordingto pi?ij(s) = argmaxa?A Q?ij(s, a), where Qij(s, a)specifies the expected cumulative reward for execut-ing action a in state s and then following pi?.
Pleasesee (Dethlefs and Cuaya?huitl, 2011b) for details onthe design of the hierarchical RL agent and the inte-gration of graphical models for surface realisation.Hidden Markov Models Representing surface re-alisation as an HMM can be roughly defined as theconverse of POS tagging.
While in POS tagging wemap an observation string of words onto a hiddensequence of POS tags, in NLG we face the oppo-...gowalkintotopointroom room room roompoint point pointto to tointo into intowalk walk walkgo go goprocessspatialrelation relatum detail.
.
.direc.
direc.
direc.
direc.Figure 1: Example trellis for an HMM for destinationinstructions (not all states and transitions are shown).Dashed arrows show paths that occur in the corpus.site scenario.
Given an observation sequence of se-mantic symbols, we want to map it onto a hiddenmost likely sequence of words.
We treat states asrepresenting surface realisations for (observed) se-mantic classes, so that a sequence of states s0...snrepresents phrases or sentences.
An observation se-quence o0...on consists of a finite set of semanticsymbols specific to an instruction type.
Each symbolhas an observation likelihood bs(o)t giving the prob-ability of observing o in state s at time t. We createdthe HMMs and trained the transition and emissionprobabilities from the GIVE corpus using the Baum-Welch algorithm.
Please see Fig.
1 for an exampleHMM and (Dethlefs and Cuaya?huitl, 2011a) for de-tails on using HMMs for surface realisation.Bayesian Networks Representing a surface re-aliser as a BN, we can model the dynamics betweensemantic concepts and their realisations.
A BN mod-els a joint probability distribution over a set of ran-dom variables and their dependencies based on a di-rected acyclic graph, where each node represents avariable Yj with parents pa(Yj).
Due to the Markovcondition, each variable depends only on its parents,resulting in a unique joint probability distributionp(Y ) = ?p(Yj|pa(Yj)), where every variable is as-sociated with a conditional probability distribution637DestinationVerbDestinationDirectionValues: {left/right,straight, empty} Values: {go, keep going,walk, continue, return,get, you need, you want,empty, ... }InformationValues: {high, low}DestinationPrepositionValues:{into, in,to, towards, until,empty, ...}DestinationRelatumValues:{landmark,room}NeedFigure 2: BN for generating destination instructions.p(Yj|pa(Yj)).
The meaning of random variablescorresponds to semantic symbols.
The values of ran-dom variables correspond to surface variants of a se-mantic symbol.
Figure 2 shows an example BN withtwo main dependencies.
First, the random variable?information need?
influences the inclusion of op-tional semantic constituents and the process of theutterance (?destination verb?).
Second, a sequenceof dependencies spans from the verb to the end ofthe utterance (?destination relatum?).
The first de-pendency is based on the intuition that more detailis needed in an instruction for users with high infor-mation need (e.g.
with little prior knowledge).1 Thesecond dependency is based on the hypothesis thatthe value of one constituent can be estimated basedon the previous constituent.
In the future, we maycompare different configurations and effects of wordorder.
Given the word sequence represented by lex-ical and syntactic variables Y0...Yn, and situation-based variables Yn+1...Ym, we can compute the pos-terior probability of a random variable Yj .
The pa-rameters of the BNs were estimated using MLE.Please see (Dethlefs and Cuaya?huitl, 2011b) for de-tails on using BNs for surface realisation within ajoint learning framework.4 Experimental SettingWe compare instructions generated with theHMMs and BNs according to their user sat-isfaction and their naturalness.
The learn-1This is key to the joint treatment of content selection andsurface realisation: if an utterance is not informative in termsof content, it will receive bad rewards, even with good surfacerealisation choices (and vice versa).ing agent is trained using the reward functionReward = User satisfaction ?
P (w0 .
.
.
wn) ?CAS.2 User satisfaction is a function of tasksuccess and the number of user turns based onthe PARADISE framework3 (Walker et al, 1997)and CAS refers to the proportion of repetitionand variation in surface forms.
Our focus inthis short paper is on P (w0 .
.
.
wn) which rewardsthe agent for having generated a surface form se-quence w0 .
.
.
wn.
In HMMs, this corresponds tothe forward probability?obtained from the For-ward algorithm?of observing the sequence in thedata.
In BNs, P (w0 .
.
.
wn) corresponds to P (Yj =vx|pa(Yj) = vy), the posterior probability given thechosen values vx and vy of random variables andtheir dependencies.
We assign a reward of ?1 foreach action to prevent loops.5 Experimental ResultsUser satisfaction Our trained policies learn thesame content selection and utterance planning be-haviour reported by (Dethlefs and Cuaya?huitl,2011b).
These policies contribute to the user sat-isfaction of instructions.
BNs and HMMs howeverdiffer in their surface realisation choices.
Figure3 shows the performance in terms of average re-wards over time for both models within the jointlearning framework and in isolation.4 For ease ofcomparison, a learning curve using a greedy policyis also shown.
It always chooses the most likelysurface form according to the human corpus with-out taking other tradeoffs into account.
Within thejoint framework, both BNs and HMMs learn to gen-erate context-sensitive surface forms that balancethe tradeoffs of the most likely sequence (accord-ing to the human corpus) and the one that best cor-responds to the user?s information need (e.g., usingnick names of rooms for familiar users).
The BNs2This reward function, the simulated environment and train-ing parameters were adapted from (Dethlefs and Cuaya?huitl,2011b) to allow a comparison with related work in using graph-ical models for surface realisation.
Simulation is based on uni-and bigrams for the spatial setting and Naive Bayes Classifica-tion for user reactions to system instructions.3See (Dethlefs et al, 2010) for evidence of the correlationbetween user satisfaction, task success and dialogue length.4In the isolated case, subtasks of content selection, utteranceplanning and surface realisation are blind regarding the deci-sions made by other subtasks, but in the joint case they are not.638103 104 105?20?19?18?17?16?15?14?13?12?11?10AverageRewardEpisodesBNs JointBNs IsolatedHMMs JointHMMs IsolatedGreedyFigure 3: Performance of HMMs, BNs and a greedy base-line in conjunction and isolation of the joint framework.reach an average reward5 of ?11.53 and outper-form the HMMs (average ?11.64) only marginallyby less than one percent.
BNs and HMMs improvethe greedy baseline by 6% (p < 0.0001, r = 0.90).While BNs reach the same performance in isola-tion of the joint framework, the performance ofHMMs deteriorates significantly to an average re-ward of ?12.12.
This corresponds to a drop of 5%(p < 0.0001, r = 0.79) and is nearly as low as thegreedy baseline.
HMMs thus reach a comparableperformance to BNs as a result of the joint learningarchitecture: the HRL agent will discover the non-optimal behaviour that is caused by the HMM?s lackof context-awareness (due to their independence as-sumptions) and learn to balance this drawback bylearning a more comprehensive policy itself.
For themore context-aware BNs this is not necessary.Naturalness We compare the instructions gener-ated with HMMs and BNs regarding their human-likeness based on the Kullback-Leibler (KL) diver-gence.
It computes the difference between two prob-ability distributions.
For evidence of its usefulnessfor measuring naturalness, cf.
(Cuaya?huitl, 2010).We compare human instructions (based on strings)drawn from the corpus against strings generated bythe HMMs and BNs to see how similar both are tohuman authors.
Splitting the human instructions inhalf and comparing them to each other indicates howsimilar human authors are to each other.
It yields aKL score of 1.77 as a gold standard (the lower thebetter).
BNs compared with human data obtain ascore of 2.83 and HMMs of 2.80.
The difference in5The average rewards of agents have negative values due tothe negative reward of ?1 the agent receives for each action.terms of similarity with humans for HMMs and BNsin a joint NLG model is not significant.Discussion While HMMs reach comparable usersatisfaction and naturalness to BNs in a joint system,they show a 5% lower performance in isolation.
Thisis likely caused by their conditional independenceassumptions: (a) the Markov assumption, (b) thestationary assumption, and (c) the observation inde-pendence assumption.
Even though these can makeHMMs easier to train and scale than more structuredmodels such as BNs, it also puts them in a disadvan-tage concerning context-awareness and accuracy asshown by our results.
In contrast, the random vari-ables of BNs allow them to keep a structured modelof the space, user, and relevant content selection andutterance planning choices.
BNs are thus able tocompute the posterior probability of a surface formbased on all relevant properties of the current situa-tion (not just the occurrence in a corpus).
While BNsalso place independence assumptions on their vari-ables, they usually overcome the problem of lackingcontext-awareness by their dependencies across ran-dom variables.
However, BNs also face limitations.Given the dependencies they postulate, they are typ-ically more data intensive and less scalable than lessstructured models such as HMMs.
This can be prob-lematic for large domains such as many real worldapplications.
Regarding their application to surfacerealisation, we can argue that while BNs are the bestperforming model in isolation, HMMs represent acheap and scalable alternative especially for large-scale problems in a joint NLG system.6 Conclusion and Future WorkWe have compared the user satisfaction and natural-ness of instructions generated with HMMs and BNsin a joint HRL model for NLG.
Results showed thatwhile BNs perform best in isolation, HMMs repre-sent a cheap and scalable alternative within the jointframework.
This is particularly attractive for large-scale, data-intensive systems.
While this paper hasfocused on instruction generation, the hierarchicalapproach in our learning framework helps to scaleup to larger NLG tasks, such as text or paragraphgeneration.
Future work could test this claim, com-pare other graphical models, such as dynamic BNs,and aim for a comprehensive human evaluation.639Acknowledgements This research was funded bythe European Commission?s FP7 programmes undergrant agreement no.
287615 (PARLANCE) and no.ICT-248116 (ALIZ-E).ReferencesAngeli, G., Liang, P. and D. Klein (2010).
A simpledomain-independent probabilistic approach to gener-ation , Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP) .Cuaya?huitl, H., Renals, S., Lemon, O. and H. Shimodaira(2010).
Evaluation of a Hierarchical ReinforcementLearning Spoken Dialogue System, Computer Speechand Language 24.Cuaya?huitl, H., and N. Dethlefs (2011a).
Spatially-Aware Dialogue Control Using Hierarchical Rein-forcement Learning, ACM Transactions on Speechand Language Processing (Special Issue on MachineLearning for Robust and Adaptive Spoken DialogueSystems 7(3).Dethlefs, N. and H. Cuaya?huitl, 2011.
Hierarchical Re-inforcement Learning and Hidden Markov Models forTask-Oriented Natural Language Generation, In Proc.of the 49th Annual Meeting of the Association forComputational Linguistics (ACL-HLT).Dethlefs, N. and H. Cuaya?huitl, 2011.
Combining Hi-erarchical Reinforcement Learning and Bayesian Net-works for Natural Language Generation in SituatedDialogue, In Proceedings of the 13th European Work-shop on Natural Language Generation (ENLG).Dethlefs, N., Cuaya?huitl, H., Richter, K.-F., Andonova,E.
and J. Bateman, 2010.
Evaluating Task Success ina Dialogue System for Indoor Navigation, In Proceed-ings of the Workshop on the Semantics and Pragmaticsof Dialogue (SemDial).Gargett, A., Garoufi, K., Koller, A. and K. Striegnitz(2010).
The GIVE-2 Corpus of Giving Instructionsin Virtual Environments, Proc.
of the 7th InternationalConference on Language Resources and Evaluation.Georgila, K., Fakotakis, N. and Kokkinakis, G. (2002).Stochastic Language Modelling for Recognition andGeneration in Dialogue Systems.
TAL (Traitement au-tomatique des langues) Journal, Vol.
43(3).Lemon, O.
(2011).
Learning what to say and how to sayit: joint optimization of spoken dialogue managementand Natural Language Generation, Computer Speechand Language 25(2).Mairesse, F., Gas?ic?, M., Jurc??
?c?ek, F., Keizer, S., Thom-son, B., Yu, K. and S. Young (2010).
Phrase-basedstatistical language generation using graphical modelsand active learning, Proc.
of the 48th Annual Meetingof the Association for Computational Linguistics.Walker, M., Litman, D., Kamm, C. and A. Abella (1997).PARADISE: A Framework for Evaluating Spoken Di-alogue Agents, Proceedings of the Annual Meeting ofthe Association for Computational Linguistic (ACL).640
