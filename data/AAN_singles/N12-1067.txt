2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 568?572,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsBetter Evaluation for Grammatical Error CorrectionDaniel Dahlmeier1 and Hwee Tou Ng1,21NUS Graduate School for Integrative Sciences and Engineering2Department of Computer Science, National University of Singapore{danielhe,nght}@comp.nus.edu.sgAbstractWe present a novel method for evaluatinggrammatical error correction.
The core ofour method, which we call MaxMatch (M2),is an algorithm for efficiently computing thesequence of phrase-level edits between asource sentence and a system hypothesis thatachieves the highest overlap with the gold-standard annotation.
This optimal edit se-quence is subsequently scored using F1 mea-sure.
We test our M2 scorer on the HelpingOur Own (HOO) shared task data and showthat our method results in more accurate eval-uation for grammatical error correction.1 IntroductionProgress in natural language processing (NLP) re-search is driven and measured by automatic eval-uation methods.
Automatic evaluation allows fastand inexpensive feedback during development, andobjective and reproducible evaluation during testingtime.
Grammatical error correction is an importantNLP task with useful applications for second lan-guage learning.
Evaluation for error correction istypically done by computing F1 measure betweena set of proposed system edits and a set of human-annotated gold-standard edits (Leacock et al, 2010).Unfortunately, evaluation is complicated by thefact that the set of edit operations for a given systemhypothesis is ambiguous.
This is due to two reasons.First, the set of edits that transforms one string intoanother is not necessarily unique, even at the tokenlevel.
Second, edits can consist of longer phraseswhich introduce additional ambiguity.
To see howthis can affect evaluation, consider the followingsource sentence and system hypothesis from the re-cent Helping Our Own (HOO) shared task (Dale andKilgarriff, 2011) on grammatical error correction:Source : Our baseline system feeds wordinto PB-SMT pipeline.Hypot.
: Our baseline system feeds a wordinto PB-SMT pipeline.The HOO evaluation script extracts the system edit( ?
a), i.e., inserting the article a. Unfortunately,the gold-standard annotation instead contains the ed-its (word ?
{a word, words}).
Although the ex-tracted system edit results in the same corrected sen-tence as the first gold-standard edit option, the sys-tem hypothesis was considered to be invalid.In this work, we propose a method, called Max-Match (M2), to overcome this problem.
The key ideais that if there are multiple possible ways to arriveat the same correction, the system should be eval-uated according to the set of edits that matches thegold-standard as often as possible.
To this end, wepropose an algorithm for efficiently computing theset of phrase-level edits with the maximum overlapwith the gold standard.
The edits are subsequentlyscored using F1 measure.
We test our method in thecontext of the HOO shared task and show that ourmethod results in a more accurate evaluation for er-ror correction.The remainder of this paper is organized as fol-lows: Section 2 describes the proposed method; Sec-tion 3 presents experimental results; Section 4 dis-cusses some details of grammar correction evalua-tion; and Section 5 concludes the paper.5682 MethodWe begin by establishing some notation.
We con-sider a set of source sentences S = {s1, .
.
.
, sn} to-gether with a set of hypotheses H = {h1, .
.
.
,hn}generated by an error correction system.
Let G ={g1, .
.
.
,gn} be the set of gold standard annota-tions for the same sentences.
Each annotation gi ={g1i , .
.
.
, gri } is a set of edits.
An edit is a triple(a, b, C), consisting of:?
start and end (token-) offsets a and b with re-spect to a source sentence,?
a correction C. For gold-standard edits, C is aset containing one or more possible corrections.For system edits, C is a single correction.Evaluation of the system output involves the follow-ing two steps:1.
Extracting a set of system edits ei for eachsource-hypothesis pair (si,hi).2.
Evaluating the system edits for the completetest set with respect to the gold standard G.The remainder of this section describes a method forsolving these two steps.
We start by describing howto construct an edit lattice from a source-hypothesispair.
Then, we show that finding the optimal se-quence of edits is equivalent to solving a shortestpath search through the lattice.
Finally, we describehow to evaluate the edits using F1 measure.2.1 Edit latticeWe start from the well-established Levenshtein dis-tance (Levenshtein, 1966), which is defined as theminimum number of insertions, deletions, and sub-stitutions needed to transform one string into an-other.
The Levenshtein distance between a sourcesentence si = s1i , .
.
.
, ski and a hypothesis hi =h1i , .
.
.
, hli can be efficiently computed using a twodimensional matrix that is filled using a classic dy-namic programming algorithm.
We assume thatboth si and hi have been tokenized.
The matrix forthe example from Section 1 is shown in Figure 1.
Byperforming a simple breadth-first search, similar tothe Viterbi algorithm, we can extract the lattice ofall shortest paths that lead from the top-left cornerto the bottom-right corner of the Levenshtein ma-trix.
Each vertex in the lattice corresponds to a cellOur baseline system feeds a word into PB-SMT pipeline .0 1 2 3 4 5 6 7 8 9 10Our 1 0 1 2 3 4 5 6 7 8 9baseline 2 1 0 1 2 3 4 5 6 7 8system 3 2 1 0 1 2 3 4 5 6 7feeds 4 3 2 1 0 1 2 3 4 5 6word 5 4 3 2 1 1 1 2 3 4 5into 6 5 4 3 2 2 2 1 2 3 4PB-SMT 7 6 5 4 3 3 3 2 1 2 3pipeline 8 7 6 5 4 4 4 3 2 1 2.
9 8 7 6 5 5 5 4 3 2 1Figure 1: The Levenshtein matrix and the shortest pathfor a source sentence ?Our baseline system feeds wordinto PB-SMT pipeline .?
and a hypothesis ?Our baselinesystem feeds a word into PB-SMT pipeline .
?in the Levenshtein matrix, and each edge in the lat-tice corresponds to an atomic edit operation: insert-ing a token, deleting a token, substituting a token,or leaving a token unchanged.
Each path throughthe lattice corresponds to a shortest sequence of ed-its that transform si into hi.
We assign a unit cost toeach edge in the lattice.We have seen that annotators can use longerphrases and that phrases can include un-changed words from the context, e.g., thegold edit from the example in Section 1 is(4, 5,word, {a word, words}).
However, it seemsunrealistic to allow an arbitrary number of un-changed words in an edit.
In particular, we want toavoid very large edits that cover complete sentences.Therefore, we limit the number of unchanged wordsby a parameter u.
To allow for phrase-level edits,we add transitive edges to the lattice as long as thenumber of unchanged words in the newly added editis not greater than u and the edit changes at least oneword.
Let e1 = (a1, b1, C1) and e2 = (a2, b2, C2)be two edits corresponding to adjacent edges in thelattice, with the first end offset b1 being equal to thesecond start offset a2.
We can combine them into anew edit e3 = (a1, b2, C1 + C2), where C1 + C2 isthe concatenation of strings C1 and C2.
The cost ofa transitive edge is the sum of the costs of its parts.The lattice extracted from the example sentence isshown in Figure 2.2.2 Finding maximally matching edit sequenceOur goal is to find the sequence of edits ei withthe maximum overlap with the gold standard.
LetL = (V,E) be the edit lattice graph from the lastsection.
We change the cost of each edge whose cor-5690,0 1,1Our (1) 2,2baseline (1)3,3system (1) 4,5feeds/feeds a (2)4,4feeds (1) 5,6word (1)?/a (1)word/a word (-45) 6,7into (1) 7,8PB-SMT (1) 8,9pipeline (1) 9,10.
(1)system feeds/system feeds a (3) feeds word/feeds a word (3)word into/a word into (3)Figure 2: The edit lattice for ?Our baseline system feeds (?
a) word into PB-SMT pipeline .?
Edge costs are shownin parentheses.
The edge from (4,4) to (5,6) matches the gold annotation and carries a negative cost.responding edit has a match in the gold standard to?
(u + 1) ?
|E|.
An edit e matches a gold edit giff they have the same offsets and e?s correction isincluded in g:match(e, g)?
e.a = g.a ?
e.b = g.b ?
e.C ?
g.C(1)Then, we perform a single-source shortest pathsearch with negative edge weights from the start tothe end vertex1.
This can be done efficiently, for ex-ample with the Bellman-Ford algorithm (Cormen etal., 2001).
As the lattice is acyclic, the algorithm isguaranteed to terminate and return a shortest path.Theorem 1.
The set of edits corresponding to theshortest path has the maximum overlap with the goldstandard annotation.Proof.
Let e = e1, .
.
.
, ek be the edit sequence cor-responding to the shortest path and let p be the num-ber of matched edits.
Assume that there exists an-other edit sequence e?
with higher total edge weightsbut p?
> p matching edits.
Then we havep(?
(u+ 1)|E|) + q ?
p?(?
(u+ 1)|E|) + q?(2)?
(q ?
q?)
?
(p?
?
p)(?
(u+ 1)|E|),where q and q?
denote the combined cost of all non-matching edits in the two paths, respectively.
Be-cause p?
?
p ?
1, the right hand side is at most?
(u + 1)|E|.
Because q and q?
are positive andbounded by (u+ 1)|E|, the left hand side cannot besmaller than or equal to ?
(u+ 1)|E|.
This is a con-tradiction.
Therefore there cannot exist such an editsequence e?, and e is the sequence with the maxi-mum overlap with the gold-standard annotation.1To break ties between non-matching edges, we add a smallcost ?
1 to all non-matching edges, thus favoring paths thatuse fewer edges, everything else being equal.2.3 Evaluating editsWhat is left to do is to evaluate the set of editswith respect to the gold standard.
This is done bycomputing precision, recall, and F1 measure (vanRijsbergen, 1979) between the set of system edits{e1, .
.
.
, en} and the set of gold edits {g1, .
.
.
,gn}for all sentencesP =?ni=1 |ei ?
gi|?ni=1 |ei|(3)R =?ni=1 |ei ?
gi|?ni=1 |gi|(4)F1 = 2?P ?RP +R, (5)where we define the intersection between ei and giasei ?
gi = {e ?
ei | ?
g ?
gi(match(e, g))}.
(6)3 Experiments and ResultsWe experimentally test our M2 method in the con-text of the HOO shared task.
The HOO test data2consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details).We test our method by re-scoring the best runs ofthe participating teams3 in the HOO shared task withour M2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff4to extract system edits.
We obtain each system?soutput and segment it at the sentence level accord-ing to the gold standard sentence segmentation.
The2Available at http://groups.google.com/group/hoo-nlp/ afterregistration.3Except one team that did not submit any plain text output.4http://www.gnu.org/s/wdiff/570M2 scorer .
.
.
should basic translational unit be (word?
a word) .
.
.HOO scorer .
.
.
should basic translational unit be *(?
a) word .
.
.M2 scorer .
.
.
development set similar (with?
to) (?
the) test set .
.
.HOO scorer .
.
.
development set similar *(with?
to the) test set .
.
.M2 scorer (?
The) *(Xinhua portion of?
xinhua portion of) the English Gigaword3 .
.
.HOO scorer *(Xinhua?
The xinhua) portion of the English Gigaword3 .
.
.Table 2: Examples of different edits extracted by the M2 scorer and the official HOO scorer.
Edits that do not matchthe gold-standard annotation are marked with an asterisk (*).Team HOO scorer M2 scorerP R F1 P R F1JU (0) 10.39 3.78 5.54 12.30 4.45 6.53LI (8) 20.86 3.22 5.57 21.12 3.22 5.58NU (0) 29.10 7.38 11.77 31.09 7.85 12.54UI (1) 50.72 13.34 21.12 54.61 14.57 23.00UT (1) 5.01 4.07 4.49 5.72 4.45 5.01Table 1: Results for participants in the HOO shared task.The run of the system is shown in parentheses.source sentences, system hypotheses, and correc-tions are tokenized using the Penn Treebank stan-dard (Marcus et al, 1993).
The character edit offsetsare automatically converted to token offsets.
We setthe parameter u to 2, allowing up to two unchangedwords per edit.
The results are shown in Table 1.Note that the M2 scorer and the HOO scorer adhereto the same score definition and only differ in theway the system edits are computed.
We can see thatthe M2 scorer results in higher scores than the offi-cial scorer for all systems, showing that the officialscorer missed some valid edits.
For example, theM2 scorer finds 155 valid edits for the UI systemcompared to 141 found by the official scorer, and 83valid edits for the NU system, compared to 78 bythe official scorer.
We manually inspect the outputof the scorers and find that the M2 scorer indeed ex-tracts the correct edits matching the gold standardwhere possible.
Examples are shown in Table 2.4 DiscussionThe evaluation framework proposed in this work dif-fers slightly from the one in the HOO shared task.Sentence-by-sentence.
We compute the editsbetween source-hypothesis sentence pairs, whilethe HOO scorer computes edits at the documentlevel.
As the HOO data comes in a sentence-segmented format, both approaches are equivalent,while sentence-by-sentence is easier to work with.Token-level offsets.
In our work, the start andend of an edit are given as token offsets, while theHOO data uses character offsets.
Character offsetsmake the evaluation procedure very brittle as a smallchange, e.g., an additional whitespace character, willaffect all subsequent edits.
Character offsets also in-troduce ambiguities in the annotation, e.g., whethera comma is part of the preceding token.Alternative scoring.
The HOO shared task de-fines three different scores: detection, recognition,and correction.
Effectively, all three scores are F1measures and only differ in the conditions on whenan edit is counted as valid.
Additionally, each scoreis reported under a ?with bonus?
alternative, wherea system receives rewards for missed optional ed-its.
The F1 measure defined in Section 2.3 is equiv-alent to correction without bonus.
Our method canbe used to compute detection and recognition scoresand scores with bonus as well.5 ConclusionWe have presented a novel method, called Max-Match (M2), for evaluating grammatical error cor-rection.
Our method computes the sequence ofphrase-level edits that achieves the highest over-lap with the gold-standard annotation.
Experi-ments on the HOO data show that our methodovercomes deficiencies in the current evaluationmethod.
The M2 scorer is available for downloadat http://nlp.comp.nus.edu.sg/software/.AcknowledgmentsWe thank Chang Liu for comments on an earlierdraft.
This research is supported by the Singa-pore National Research Foundation under its Inter-national Research Centre @ Singapore Funding Ini-tiative and administered by the IDM Programme Of-fice.571ReferencesT.
Cormen, C.E.
Leiserson, R.L.
Rivest, and C. Stein.2001.
Introduction to Algorithms.
MIT Press, Cam-bridge, MA.R.
Dale and A. Kilgarriff.
2011.
Helping Our Own: TheHOO 2011 pilot shared task.
In Proceedings of the2011 European Workshop on Natural Language Gen-eration.C.
Leacock, M. Chodorow, M. Gamon, and J. Tetreault,2010.
Automated Grammatical Error Detection forLanguage Learners, chapter 5.
Morgan and ClaypoolPublishers.V.
Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertions, and reversals.
Soviet PhysicsDoklady, 10(8):707?710.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of English:The Penn Treebank.
Computational Linguistics, 19.C.
J. van Rijsbergen.
1979.
Information Retrieval.
But-terworth, 2nd edition.572
