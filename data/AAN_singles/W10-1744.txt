Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 301?306,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsCMU Multi-Engine Machine Translation for WMT 2010Kenneth HeafieldCarnegie Mellon UniversityPittsburgh, PA, USA.heafield@cs.cmu.eduAlon LavieCarnegie Mellon UniversityPittsburgh, PA, USA.alavie@cs.cmu.eduAbstractThis paper describes our submission,cmu-heafield-combo, to the WMT2010 machine translation system combi-nation task.
Using constrained resources,we participated in all nine language pairs,namely translating English to and fromCzech, French, German, and Spanish aswell as combining English translationsfrom multiple languages.
Combinationproceeds by aligning all pairs of systemoutputs then navigating the aligned out-puts from left to right where each path isa candidate combination.
Candidate com-binations are scored by their length, agree-ment with the underlying systems, and alanguage model.
On tuning data, improve-ment in BLEU over the best system de-pends on the language pair and rangesfrom 0.89% to 5.57% with mean 2.37%.1 IntroductionSystem combination merges the output of sev-eral machine translation systems into a sin-gle improved output.
Our system combina-tion scheme, submitted to the Workshop on Sta-tistical Machine Translation (WMT) 2010 ascmu-heafield-combo, is an improvementover our previous system (Heafield et al, 2009),called cmu-combo in WMT 2009.
The schemeconsists of aligning 1-best outputs from each sys-tem using the METEOR (Denkowski and Lavie,2010) aligner, identifying candidate combinationsby forming left-to-right paths through the alignedsystem outputs, and scoring these candidates us-ing a battery of features.
Improvements this yearinclude unigram paraphrase alignment, support forall target languages, new features, language mod-eling without pruning, and more parameter opti-mization.
This paper describes our scheme withemphasis on improved areas.2 Related WorkConfusion networks (Rosti et al, 2008) are themost popular form of system combination.
In thisapproach, a single system output acts as a back-bone to which the other outputs are aligned.
Thisbackbone determines word order while other out-puts vote for substitution, deletion, and insertionoperations.
Essentially, the backbone is editedto produce a combined output which largely pre-serves word order.
Our approach differs in thatwe allow paths to switch between sentences, effec-tively permitting the backbone to switch at everyword.Other system combination techniques typicallyuse TER (Snover et al, 2006) or ITGs (Karakoset al, 2008) to align system outputs, meaningthey depend solely on positional information tofind approximate matches; we explicitly use stem,synonym, and paraphrase data to find alignments.Our use of paraphrases is similar to Leusch et al(2009), though they learn a monolingual phrasetable while we apply cross-lingual pivoting (Ban-nard and Callison-Burch, 2005).3 AlignmentSystem outputs are aligned at the token level usinga variant of the METEOR (Denkowski and Lavie,2010) aligner.
This identifies, in decreasing orderof priority: exact, stem, synonym, and unigramparaphrase matches.
Stems (Porter, 2001) areavailable for all languages except Czech, thoughthis is planned for future work and expectedto produce significant improvement.
Synonymscome from WordNet (Fellbaum, 1998) and areonly available in English.
Unigram paraphrasesare automatically generated using phrase table piv-oting (Bannard and Callison-Burch, 2005).
Thephrase tables are trained using parallel data fromEuroparl (fr-en, es-en, and de-en), news commen-tary (fr-en, es-en, de-en, and cz-en), United Na-301tions (fr-en and es-en), and CzEng (cz-en) (Bojarand Z?abokrtsky?, 2009) sections 0?8.
The Germanand Spanish tables also use the German-SpanishEuroparl corpus released for WMT08 (Callison-Burch et al, 2008).
Currently, the generated para-phrases are filtered to solely unigram matches;full use of this table is planned for future work.When alignment is ambiguous (i.e.
?that?
appearstwice in a system output), an alignment is chosento minimize crossing with other alignments.
Fig-ure 1 shows an example alignment.
Compared toour previous system, this replaces heuristic ?arti-ficial?
alignments with automatically learned uni-gram paraphrases.Twice that produced by nuclear plantsDouble that that produce nuclear power stationsFigure 1: Alignment generated by METEORshowing exact (that?that and nuclear?nuclear),stem (produced?produce), synonym (twice?double), and unigram paraphrase (plants?stations)alignments.4 Search SpaceA candidate combination consists of a string of to-kens (words and punctuation) output by the under-lying systems.
Unconstrained, the string could re-peat tokens and assemble them in any order.
Wetherefore have several constraints:Sentence The string starts with the beginning ofsentence token and finishes with the end ofsentence token.
These tokens implicitly ap-pear in each system?s output.Repetition A token may be used at most once.Tokens that METEOR aligned are alterna-tives and cannot both be used.Weak Monotonicity This prevents the schemefrom reordering too much.
Specifically, thepath cannot jump backwards more than r to-kens, where positions are measured relativeto the beginning of sentence.
It cannot makea series of smaller jumps that add up to morethan r either.
Equivalently, once a tokenin the ith position of some system output isused, all tokens before the i?
rth position intheir respective system outputs become un-usable.
The value of r is a hyperparameterconsidered in Section 6.Completeness Tokens may not be skipped unlessthe sentence ends or another constraint wouldbe violated.
Specifically, when a token fromsome system is used, it must be the first (left-most in the system output) available tokenfrom that system.
For example, the first de-coded token must be the first token output bysome system.Together, these define the search space.
The candi-date starts at the beginning of sentence by choos-ing the first token from any system.
Then it caneither continue with the next token from the samesystem or switch to another one.
When it switchesto another system, it does so to the first availabletoken from the new system.
The repetition con-straint requires that the token does not repeat con-tent.
The weak monotonicity constraint ensuresthat the jump to the new system goes at most rwords back.
The process repeats until the end ofsentence token is encountered.The previous version (Heafield et al, 2009) alsohad a hard phrase constraint and heuristics to de-fine a phrase; this has been replaced with newmatch features.Search is performed using beam search wherethe beam contains partial candidates of the samelength, each of which starts with the beginning ofsentence token.
In our experiments, the beam sizeis 500.
When two partial candidates will extendin the same way (namely, the set of available to-kens is the same) and have the same feature state(i.e.
language model history), they are recom-bined.
The recombined partial candidate subse-quently acts like its highest scoring element, untilk-best list extraction when it is lazily unpacked.5 Scoring FeaturesCandidates are scored using three feature classes:Length Number of tokens in the candidate.
Thiscompensates, to first order, for the impact oflength on other features.Match For each system s and small n, featurems,n is the number of n-grams in the candi-date matching the sentence output by systems.
This is detailed in Section 5.1.302Language Model Log probability from a n-gramlanguage model and backoff statistics.
Sec-tion 5.2 details our training data and backofffeatures.Features are combined into a score using a linearmodel.
Equivalently, the score is the dot productof a weight vector with the vector of our featurevalues.
The weight vector is a parameter opti-mized in Section 6.5.1 Match FeaturesThe n-gram match features reward agreement be-tween the candidate combination and underlyingsystem outputs.
For example, feature m1,1 countstokens in the candidate that also appear in sys-tem 1?s output for the sentence being combined.Featurem1,2 counts bigrams appearing in both thecandidate and the translation suggested by system1.
Figure 2 shows example feature values.System 1: Supported Proposal of FranceSystem 2: Support for the Proposal of FranceCandidate: Support for Proposal of FranceUnigram Bigram TrigramSystem 1 4 2 1System 2 5 3 1Figure 2: Example match feature values with twosystems and matches up to length three.
Here,?Supported?
counts because it aligns with ?Sup-port?.The match features count n-gram matches be-tween the candidate and each system.
Thesematches are defined in terms of alignments.
A to-ken matches the system that supplied it as well asthe systems to which it aligns.
This can be seen inFigure 2 where System 1?s unigram match countincludes ?Supported?
even though the candidatechose ?Support?.
Longer matches are defined sim-ilarly: a bigram match consists of two consecutivealignments without reordering.
Since METEORgenerates several types of alignments as shown inFigure 1, we wonder whether all alignment typesshould count as matches.
If we count all typesof alignment, then the match features are blind tolexical choice, leaving only the language model todiscriminate.
If only exact alignments count, thenless systems are able to vote on a word order deci-sion mediated by the bigram and trigram features.We find that both versions have their advantages,and therefore include two sets of match features:one that counts only exact alignments and anotherthat counts all alignments.
We also tried copies ofthe match features at the stem and synonym levelbut found these impose additional tuning cost withno measurable improvement in quality.Since systems have different strengths andweaknesses, we avoid assigning a single systemconfidence (Rosti et al, 2008) or counting n-grammatches with uniform system confidence (Hilde-brand and Vogel, 2009).
The weight on matchfeature ms,n corresponds to our confidence in n-grams from system s. These weights are fully tun-able.
However, there is another hyperparameter:the maximum length of n-gram considered; wetypically use 2 or 3 with little gain seen above this.5.2 Language ModelWe built language models for each of the five tar-get languages with the aim of using all constraineddata.
For each language, we used the providedEuroparl (Koehn, 2005) except for Czech, NewsCommentary, and News monolingual corpora.
Inaddition, we used:Czech CzEng (Bojar and Z?abokrtsky?, 2009) sec-tions 0?7English Gigaword Fourth Edition (Parker et al,2009), Giga-FrEn, and CzEng (Bojar andZ?abokrtsky?, 2009) sections 0?7French Gigaword Second Edition (Mendonca etal., 2009a), Giga-FrEnSpanish Gigaword Second Edition (Mendonca etal., 2009b)Paragraphs in the Gigaword corpora were splitinto sentences using the script provided withEuroparl (Koehn, 2005); parenthesized format-ting notes were removed from the NYT portion.We discarded Giga-FrEn lines containing invalidUTF8, control characters, or less than 90% Latincharacters or punctuation.
Czech training dataand system outputs were preprocessed using Tec-toMT (Z?abokrtsky?
and Bojar, 2008) following theCzEng 0.9 pipeline (Bojar and Z?abokrtsky?, 2009).English training data and system outputs were to-kenized with the IBM tokenizer.
French, Ger-man, and Spanish used the provided tokenizer.303Czech words were truecased based on automati-cally identified lemmas marking names; for otherlanguages, training data was lowercased and sys-tems voted, with uniform weight, on capitalizationof each character in the final output.With the exception of Czech (for which we usedan existing model), all models were built with nolossy pruning whatsoever, including our Englishmodel with 5.8 billion tokens (i.e.
after IBM to-kenization).
Using the stock SRILM (Stolcke,2002) toolkit with modified Kneser-Ney smooth-ing, the only step that takes unbounded memory isfinal model estimation from n-gram counts.
Sincekey parameters have already been estimated at thisstage, this final step requires only counts for thedesired n-grams and all of their single token ex-tensions.
We can therefore filter the n-grams onall but the last token.
Our scheme will only queryan n-gram if all of the tokens appear in the unionof system outputs for some sentence; this strict fil-tering criterion is further described and releasedas open source in Heafield and Lavie (2010).
Thesame technique applies to machine translation sys-tems, with phrase table expansion taking the placeof system outputs.For each language, we built one model by ap-pending all data.
Another model interpolatessmaller models built on the individual sourceswhere each Gigaword provider counts as a distinctsource.
Interpolation weights were learned on theWMT 2009 references.
For English, we also triedan existing model built solely on Gigaword usinginterpolation.
The choice of model is a hyperpa-rameter we consider in Section 6.In the combination scheme, we use the log lan-guage model probability as a feature.
Anotherfeature reports the length of the n-gram matchedby the model; this exposes limited tunable con-trol over backoff behavior.
For Czech, the modelwas built with a closed vocabulary; when an out-of-vocabulary (OOV) word is encountered, it isskipped for purposes of log probability and athird feature counts how often this happens.
Thisamounts to making the OOV probability a tunableparameter.6 Parameter Optimization6.1 Feature WeightsFeature weights are tuned using Minimum ErrorRate Training (MERT) (Och, 2003) on the 455provided references.
Our largest submission, xx-en primary, combines 17 systems with five matchfeatures each plus three other features for a total of88 features.
This immediately raises two concerns.First, there is overfitting and we expect to see aloss in the test results, although our experience inthe NIST Open MT evaluation is that the amountof overfitting does not significantly increase at thisnumber of parameters.
Second, MERT is poor atfitting this many feature weights.
We present onemodification to MERT that addresses part of thisproblem, leaving other tuning methods as futurework.MERT is prone to local maxima, so we applya simple form of simulated annealing.
As usual,the zeroth iteration decodes with some initial fea-ture weights.
Afterward, the weights {?f} learnedfrom iteration 0 ?
j < 10 are perturbed to pro-duce new feature weights?f ?
U[j10?f ,(2?j10)?f]where U is the uniform distribution.
This sam-pling is done on a per-sentence basis, so the firstsentence is decoded with different weights thanthe second sentence.
The amount of random per-turbation decreases linearly each iteration untilthe 10th and subsequent iterations whose learnedweights are not perturbed.
We emphasize thatthe point is to introduce randomness in sentencesdecoded during MERT, and therefore consideredduring parameter tuning, and not on the spe-cific formula presented in this system description.In practice, this technique increases the numberof iterations and decreases the difference in tun-ing scores following MERT.
In our experiments,weights are tuned towards uncased BLEU (Pap-ineni et al, 2002) or the combined metric TER-BLEU (Snover et al, 2006).6.2 HyperparametersIn total, we tried 1167 hyperparameter configura-tions, limited by CPU time during the evaluationperiod.
For each of these configurations, the fea-ture weights were fully trained with MERT andscored on the same tuning set, which we used toselect the submitted combinations.
Because theseconfigurations represent a small fraction of thehyperparameter space, we focused on values thatwork well based on prior experience and tuningscores as they became available:Set of systems Top systems by BLEU.
The num-ber of top systems included ranged from 3 to304Pair Entry #Sys r Match LM Objective ?BLEU ?TER ?METEcz-en main 5 4 2 Append BLEU 2.38 0.99 1.50de-enmain 6 4 2 Append TER-BLEU 2.63 -2.38 1.36contrast 7 3 2 Append BLEU 2.60 -2.62 1.09es-enmain 7 5 3 Append BLEU 1.22 -0.74 0.70contrast 5 6 2 Gigaword BLEU 1.08 -0.80 0.97fr-enmain 9 5 3 Append BLEU 2.28 -2.26 0.78contrast 8 5 3 Append BLEU 2.19 -1.81 0.63xx-enmain 17 5 3 Append BLEU 5.57 -5.60 4.33contrast 16 5 3 Append BLEU 5.45 -5.38 4.22en-cz main 7 5 3 Append TER-BLEU 0.74 -0.26 0.68en-demain 6 6 2 Interpolate BLEU 1.26 0.16 1.14contrast 5 4 2 Interpolate BLEU 1.26 0.30 1.00en-esmain 8 5 3 Interpolate BLEU 2.38 -2.20 0.96contrast 6 7 2 Append BLEU 2.40 -1.85 1.02en-fr main 6 7 2 Append BLEU 2.64 -0.50 1.55Table 1: Submitted combinations chosen from among 1167 hyperparameter settings by tuning datascores.
Uncased BLEU, uncased TER, and METEOR 1.0 with adequacy-fluency parameters are shownrelative to top system by BLEU.
Improvement is seen in all pairs on all metrics except for TER on cz-enand en-de where the top systems are 5% and 2% shorter than the references, respectively.
TER has a wellknown preference for shorter hypotheses.
The #Sys column indicates the number of systems combined,using the top scoring systems by BLEU.
The Match column indicates the maximum n-gram length con-sidered for matching on all alignments; we separately counted unigram and bigram exact matches.
Insome cases, we made a contrastive submission where metrics disagreed or length behavior differed nearthe top; contrastive submissions are not our 2009 scheme.all of them, except on xx-en where we com-bined up to 17.Jump limit Mostly r = 5, with some experi-ments ranging from 3 to 7.Match features Usually unigram and bigram fea-tures, sometimes trigrams as well.Language model Balanced between the ap-pended and interpolated models, with theoccasional baseline Gigaword model forEnglish.Tuning objective Usually BLEU for speed rea-sons; occasional TER-BLEU with typicalvalues for other hyperparameters.7 ConclusionTable 1 shows the submitted combinations andtheir performance.
Our submissions this year im-prove over last year (Heafield et al, 2009) inoverall performance and support for multiple lan-guages.
The improvement in performance we pri-marily attribute to the new match features, whichaccount for most of the gain and allowed us to in-clude lower quality systems.
We also trained lan-guage models without pruning, replaced heuristicalignments with unigram paraphrases, tweaked theother features, and improved the parameter opti-mization process.
We hope that the improvementsseen on tuning scores generalize to significantlyimproved test scores, especially human evaluation.AcknowledgmentsOndr?ej Bojar made the Czech language modeland preprocessed Czech system outputs.
MichaelDenkowski provided the paraphrase tables andwrote the version of METEOR used.
This workwas supported in part by the DARPA GALE pro-gram and by a NSF Graduate Research Fellow-ship.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings ACL.Ondr?ej Bojar and Zdene?k Z?abokrtsky?.
2009.
CzEng3050.9, building a large Czech-English automatic paral-lel treebank.
The Prague Bulletin of MathematicalLinguistics, (92):63?83.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the Third Workshop on Statisti-cal Machine Translation, pages 70?106, Columbus,Ohio, June.
Association for Computational Linguis-tics.Michael Denkowski and Alon Lavie.
2010.
Extend-ing the METEOR machine translation metric to thephrase level.
In Proceedings NAACL 2010, Los An-geles, CA, June.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Kenneth Heafield and Alon Lavie.
2010.
Combiningmachine translation output with open source: TheCarnegie Mellon multi-engine machine translationscheme.
In The Prague Bulletin of MathematicalLinguistics, number 93, pages 27?36, Dublin.Kenneth Heafield, Greg Hanneman, and Alon Lavie.2009.
Machine translation system combinationwith flexible word ordering.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, pages 56?60, Athens, Greece, March.
Associa-tion for Computational Linguistics.Almut Silja Hildebrand and Stephan Vogel.
2009.CMU system combination for WMT?09.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 47?50, Athens, Greece,March.
Association for Computational Linguistics.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine translation sys-tem combination using ITG-based alignments.
InProceedings ACL-08: HLT, Short Papers (Compan-ion Volume), pages 81?84.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof MT Summit.Gregor Leusch, Evgeny Matusov, and Hermann Ney.2009.
The RWTH system combination system forWMT 2009.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation, pages 51?55, Athens, Greece, March.
Association for Compu-tational Linguistics.Angelo Mendonca, David Graff, and Denise DiPer-sio.
2009a.
French gigaword second edition.LDC2009T28.Angelo Mendonca, David Graff, and Denise DiPer-sio.
2009b.
Spanish gigaword second edition.LDC2009T21.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 160?167, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Proceed-ings of 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318,Philadelphia, PA, July.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English gigaword fourthedition.
LDC2009T13.Martin Porter.
2001.
Snowball: A language for stem-ming algorithms.
http://snowball.tartarus.org/.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental hypothe-sis alignment for building confusion networks withapplication to machine translation system combina-tion.
In Proceedings Third Workshop on StatisticalMachine Translation, pages 183?186.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings Seventh Conference of the Associa-tion for Machine Translation in the Americas, pages223?231, Cambridge, MA, August.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the Sev-enth International Conference on Spoken LanguageProcessing, pages 901?904.Zdene?k Z?abokrtsky?
and Ondr?ej Bojar.
2008.
TectoMT,Developer?s Guide.
Technical Report TR-2008-39,Institute of Formal and Applied Linguistics, Facultyof Mathematics and Physics, Charles University inPrague, December.306
