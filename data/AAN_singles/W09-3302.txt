Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 10?18,Suntec, Singapore, 7 August 2009.c?2009 ACL and AFNLPNamed Entity Recognition in WikipediaDominic Balasuriya Nicky Ringland Joel Nothman Tara Murphy James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{dbal7610,nicky,joel,tm,james}@it.usyd.edu.auAbstractNamed entity recognition (NER) is used inmany domains beyond the newswire textthat comprises current gold-standard cor-pora.
Recent work has used Wikipedia?slink structure to automatically generatenear gold-standard annotations.
Until now,these resources have only been evaluatedon newswire corpora or themselves.We present the first NER evaluation ona Wikipedia gold standard (WG) corpus.Our analysis of cross-corpus performanceon WG shows that Wikipedia text maybe a harder NER domain than newswire.We find that an automatic annotation ofWikipedia has high agreement with WGand, when used as training data, outper-forms newswire models by up to 7.7%.1 IntroductionNamed Entity Recognition (NER) is the task ofidentifying and classifying people, organisationsand other named entities (NE) within text.
NER iscentral to many NLP systems, especially informa-tion extraction and question answering.Machine learning approaches now dominateNER, learning patterns associated with individualentity classes from annotated training data.
Thistraining data, including English newswire fromthe MUC-6, MUC-7 (Chinchor, 1998), and CONLL-03 (Tjong Kim Sang and De Meulder, 2003) com-petitive evaluation tasks, and the BBN PronounCoreference and Entity Type Corpus (Weischedeland Brunstein, 2005), is critical to the success ofthese approaches.This data dependence has impeded the adapta-tion or porting of existing NER systems to newdomains, such as scientific or biomedical text,e.g.
Nobata et al (2000).
Similar domain sensi-tivity is exhibited by most tasks across NLP, e.g.parsing (Gildea, 2001), and the adaptation penaltyis still apparent even when the same set of namedentity classes is used in text from similar domains(Ciaramita and Altun, 2005).Wikipedia is an important corpus for informa-tion extraction, e.g.
Bunescu and Pas?ca (2006)and Wu et al (2008) because of its size, cur-rency, rich semi-structured content, and its closerresemblance to web text than newswire.
Recently,Wikipedia?s markup has been exploited to auto-matically derive NE annotated text for training sta-tistical models (Richman and Schone, 2008; Mikaet al, 2008; Nothman et al, 2008).However, without a gold standard, existing eval-uations of these models were forced to compareagainst mismatched newswire corpora or the noisyWikipedia-derived annotations themselves.
Fur-ther, it was not possible to directly ascertain theaccuracy of these automatic extraction methods.We have manually annotated 39,007 tokens ofWikipedia with coarse-grained named entity tags(WG).
We present the first evaluation of Wiki-pedia-trained models on Wikipedia: the C&C NERtagger (Curran and Clark, 2003b) trained on (a)automatically annotated Wikipedia text (WP2) ex-tracted by Nothman et al (2009); and (b) tradi-tional newswire NER corpora (MUC, CONLL andBBN).
The WP2 model, though trained on noisyannotations, outperforms newswire models on WGby 7.7%.
However, every model, including WP2,performs far worse on WG than on the newswire.We examined the quality of WG, and found thatour annotation strategy produced a high-quality,consistent corpus.
Our analysis suggests that it isthe form and distribution of NEs in Wikipedia thatmake it a difficult target domain.Finally, we compared WG with the annotationsextracted by Nothman et al (2009), and foundagreement comparable to our inter-annotatoragreement, demonstrating that NE corpora can bederived very accurately from Wikipedia.102 BackgroundTraditional evaluations of NER have consideredthe performance of a tagger on test data from thesame source as its training data.
Although themajority of annotated corpora available consist ofnewswire text, recent practical applications covera far wider range of genres, including Wikipedia,blogs, RSS feeds, and other data sources.
Cia-ramita and Altun (2005) showed that even whenmoving a short distance, e.g.
annotating WSJ textwith the same scheme as CONLL?s Reuters, the per-formance was 26% worse than on the original text.Similar differences are reported by Nothmanet al (2009) who compared MUC, CONLL andBBN annotations reduced to a common tag-set.They found poor cross-corpus performance to bedue to tokenisation and annotation scheme mis-match, missing frequent lexical items, and namingconventions.
They then compared automatically-annotated Wikipedia text as training data andfound it also differs in otherwise inconsequen-tial ways from the newswire corpora, in particularlacking abbreviations necessary to tag news text.2.1 Automatic Wikipedia annotationWikipedia, a collaboratively-written online ency-clopedia, is readily exploited in NLP, because it islarge, semi-structured and multilingual.
Its arti-cles often correspond to NEs, so it has been usedfor NE recognition (Kazama and Torisawa, 2007)and disambiguation (Bunescu and Pas?ca, 2006;Cucerzan, 2007).
Wikipedia links often span NEs,which may be exploited to automatically createannotated NER training data by determining theentity class of the linked article and then labellingthe link text with it.Richman and Schone (2008) use article clas-sification knowledge from English Wikipedia toproduce NE-annotated corpora in other languages(evaluated against NE gold standards for French,Spanish, and Ukrainian).
Mika et al (2008) ex-plored the use of tags from a CONLL-trained tag-ger to seed the labelling of entities and evaluatethe performance of a Wikipedia-trained model byhand.We make use of an approach described by Noth-man et al (2009) which is engineered to performwell on BBN data with a reduced tag-set (LOC,MISC, ORG, PER).
They derive an annotated cor-pus with the following steps:1.
Classify Wikipedia articles into entity classes2.
Split the articles into tokenised sentences3.
Label expanded links according to target NEs4.
Select sentences for inclusion in a corpusTo prepare the text, they use mwlib (Pedi-aPress, 2007) to parse Wikipedia?s native markupretaining only paragraph text with links, ap-ply Punkt (Kiss and Strunk, 2006) estimated onWikipedia text to perform sentence boundary de-tection, and tokenise the resulting text using regu-lar expressions.Nothman et al (2009) infer additional NEs notprovided by existing links, and apply rules to ad-just link boundaries and classifications to closermatch BBN annotations.2.2 NER evaluationMeaningful automatic evaluation of NER is dif-ficult and a number of metrics have been pro-posed (Nadeau and Sekine, 2007).
Ambiguityleads to entities correctly delimited but misclas-sified, or boundaries mismatched despite correctclassification.Although the MUC-7 evaluation (Chinchor,1998) defined a metric which was less sensitiveto often-meaningless boundary errors, we consideronly exact entity matches as correct, followingthe standard CONLL evaluation (Tjong Kim Sang,2002).
We report precision, recall and F -score foreach entity type.3 Creating the Wikipedia gold standardWe created a corpus by manually annotating thetext of 149 articles from the May 22, 2008 dumpof English Wikipedia.
The articles were selectedat random from all articles describing named en-tities, with a roughly equal proportion of arti-cle topics from each of the four CONLL-03 classes(LOC, MISC, ORG, PER).
We adopted Nothman etal.
?s (2008) preprocessing described above to pro-duce tokenised sentences for annotation.Only body text was extracted from the chosenarticles for inclusion in the corpus.
Four articleswere found not to have any usable text, consistingsolely of tables, lists, templates and section head-ings, which we remove.
Their exclusion leaves acorpus of 145 articles.3.1 AnnotationAnnotation was initially carried out using a fine-grained tag-set which was expanded by the an-11[COMPANY Aero Gare] was a kitplane manufacturerfounded by [PERSON Gary LeGare] in [CITY Mojave] ,[STATE California] to marketed the [PLANE Sea Hawker]amphibious aircraft .
[ORG Aero Gare] was a kitplane manufacturer foundedby [PER Gary LeGare] in [LOC Mojave] , [LOC Califor-nia] to marketed the [MISC Sea Hawker] amphibiousaircraft .
(a) Fine-grained annotation (b) Coarse-grained annotationFigure 1: An example of coarse and fine-grained annotation of Wikipedia text.notators as annotation progressed, and eventuallycontained 96 tags.We created a mapping from these fine-grainedtags to the four coarse-grained tags used in theCONLL-03 data: PER, LOC, MISC and ORG.
Thisenables evaluation with existing NER models.
Webelieve this two-phase approach allowed anno-tators to defer difficult mapping decisions, (e.g.should an airport be classified as a LOC, ORG, orMISC?)
which can then be made after discussion.The mapping could also be modified to suit a par-ticular evaluation task.Figure 1 shows an example of the use of fine andcoarse-grained tags to annotate a sentence.
Tagssuch as PERSON correspond directly to coarse-grained tags, while most map to a more generaltag, such as STATE and CITY mapping to LOC.PLANE is an example of a fine-grained tag thatcannot be mapped to LOC, ORG, or PER.
Thesetags may be mapped to MISC; some are not con-sidered entities under the CONLL scheme and areleft unlabelled in the coarse-grained annotation.Three independent annotators were involved inthe annotation process.
Annotator 1 annotated all145 articles using the fine-grained tags.
Annota-tors 2 and 3 then re-annotated 19 of these articles(316 sentences or 8030 tokens), amounting to 21%of the corpus.
Annotator 2 used the fine-grainedtags described above, while Annotator 3 used thefour coarse-grained CONLL tags.
To measure vari-ation, all three annotations of this common portionwere mapped down to the CONLL tag-set and inter-annotator agreement was calculated.We found that 202 tokens were disagreed uponby at least one annotator (2.5% of all tokensannotated), and these discrepancies were thendiscussed by the three annotators.
The inter-annotator agreement will be analysed in more de-tail in Section 5.Sentences containing grammatical and typo-graphical errors were not corrected, so that the cor-pus would be as close as possible to the sourcetext.
Web text often contains errors, such as toTrain Test P R FWP2 WG 66.5 67.4 66.9BBN WG 59.2 59.1 59.2CONLL WG 54.3 57.2 55.7WP2 * WG * 75.1 67.7 71.2BBN * WG * 57.2 64.1 60.4CONLL * WG * 53.1 62.7 57.5MUC * WG * 52.3 57.2 54.6WP2 BBN 73.4 74.6 74.0WP2 CONLL 73.6 64.9 69.0WP2 * MUC * 86.2 68.9 76.6BBN BBN 85.7 87.3 86.5CONLL CONLL 85.3 86.5 85.9MUC MUC 81.0 83.6 82.3Table 2: Tagger performance on various corpora.Asterisks indicate that MISC tags are ignored.marketed the Sea Hawker from the example in Fig-ure 1, so any NER system must deal with these er-rors.
Sentences with poor tokenisation or sentenceboundary detection were identified and correctedmanually, since these errors are introduced by ourprocessing and annotation, and do not exist in thesource text.The final corpus was created by correcting an-notation mistakes, with annotators 2 and 3 eachcorrecting 50% of the corpus.
The fine-grainedtags were mapped to the four CONLL tags beforethe final corrections were made.
The final WG cor-pus consists of the body text of 145 Wikipedia ar-ticles tagged with the four CONLL-03 tags.4 NER on the Wikipedia gold-standardNothman et al (2009) have previously shown thatthat an NER system trained on automatically anno-tated Wikipedia corpora performs reasonably wellon non-Wikipedia text.
Having created our WGcorpus of gold-standard annotations, we are ableto evaluate the performance of these models onWikipedia text.We compare the C&C NE maximum-entropytagger (Curran and Clark, 2003b) trained ongold-standard newswire corpora (MUC-7, BBN andCONLL-03) with the same tagger trained on auto-matically annotated Wikipedia text, WP2.
WG is12WG WP2 BBN CONLL-03 MUC-7Test Train Train Test Train Test Train TestTokens 39 007 3 500 032 901 849 129 654 203 621 46 435 83 601 60 436Sentences 1 696 146 543 37 843 5 462 14 987 3 453 3 485 2 419Articles 145 ?
1775 238 946 231 102 99NEs 3 558 288 545 49 999 7 307 23 498 5 648 4 315 3 540Table 1: Corpus sizes.too small to train a reasonable NER model on gold-standard Wikipedia annotations.
Part-of-speechtags are added to all corpora using the C&C POStagger (Curran and Clark, 2003a) before trainingand testing.1We evaluate each model on tradi-tional newswire evaluation corpora as well as WG.Table 1 gives the size of each corpus.The results are shown in Table 2.
The WP2 tag-ger performed substantially better on WG than tag-gers trained on newswire text, with a 7?11% in-crease in F -score compared to BBN and CONLL-03, and a 16% increase compared to MUC-7, whenmiscellaneous NEs in the corpus are not consid-ered in the evaluation.
The Wikipedia trainedmodel thus outperforms newswire models on ournew WG corpus even though the training annota-tions were automatically extracted.The WP2 tagger performed worse on WG thanon gold-standard news corpora (BBN and CONLL),with a 2?7% reduction in F -score.
Further, theperformance of WP2 on WG is 11?20% F -scorelower than same-source evaluation results, e.g.BBN on BBN, CONLL on CONLL.
Therefore, de-spite WP2 showing an advantage in tagging WGdue to their common source domain, we find thatWG?s annotations are harder to predict than thenewswire test data commonly used for evaluation.One possible explanation is that our WG corpushas been inconsistently annotated.
When NEs ofmiscellaneous type are not considered in the eval-uation (asterisks in Table 2), the performance of alltaggers on WG improves, with WP2 demonstratinga 4% increase.
This result suggests another par-tial explanation: that MISC NEs in Wikipedia aremore difficult to annotate correctly, due to theirpoor definition and broad coverage.
A third ex-planation is that the automatic conversion processproposed by Nothman et al (2008) produces muchlower quality training data than manual annota-tion.
We explore these three possibilities below.1Both taggers are available from http://svn.ask.it.usyd.edu.au/trac/candc.Token Exact NE onlyA1 and A2 0.95 0.99 0.88A1 and A3 0.91 0.95 0.81A2 and A3 0.91 0.96 0.79Fleiss?
Kappa 0.92 0.97 0.83Table 3: Initial human inter-annotator agreement.5 Quality of the Wikipedia gold standardThe low performance observed on WG may bedue to the poor quality of its annotation.
We en-sure that this is not the case by measuring inter-annotator agreement.
The WG annotation processproduced three independent annotations of a sub-set of WG.
These annotations were compared us-ing Cohen?s ?
(Fleiss and Cohen, 1973) betweenpairs of annotators, and Fleiss?
?
(Fleiss, 1971),which generalises Cohen?s ?
to more than twoconcurrent annotations.Table 3 shows the three types of ?
values cal-culated.
Token is calculated on a per token basis,comparing the agreement of annotators on eachtoken in the corpus; NE only, is calculated onthe agreement between entities alone, excludingagreement in cases where all annotators agreedthat a token was not a NE; Exact refers to theagreement between annotators where all annota-tors have agreed on the boundaries of a NE, butdisagree on the type of NE.Annotator 1 originally annotated the entire cor-pus, and Annotators 2 and 3 then corrected exactlyhalf of the corpus each after a discussion betweenthe three annotators to resolve ambiguities.
Landisand Koch (1977) determine that a ?
value greaterthan 0.81 indicates almost perfect agreement.
Bythis standard, our three annotators were in strongagreement prior to discussion, with our Fleiss?
?values all greater than 0.81.
Inconsistencies in thecorpus due to annotation mistakes by Annotator 1were corrected by Annotators 2 and 3.Inter-annotator agreement for cases where theannotators agreed on NE boundaries was higherthan agreement on each token, which suggeststhat many discrepancies resulted from NE bound-13LOC MISC ORG PER H(C): With O Without O Total NEs % NE tokensWG 28.5 20.0 25.2 26.3 0.98 2.0 3 558 17.1BBN 22.4 9.8 46.4 21.3 0.61 1.7 49 999 9.6MUC 33.3 ?
40.7 26.1 0.52 1.5 4 315 8.1CONLL 30.4 14.6 26.9 28.1 0.98 1.9 23 498 17.1Table 4: NE class distribution, tag entropy and NE density statistics for gold-standard corpora and WG.ary ambiguities, or disagreement as to whethera phrase constituted a NE at all.
Higher inter-annotator agreement between Annotators 1 and 2leads us to believe that the two-phase annotationstrategy, where an initially fine-grained tag-set isreduced, results in more consistent annotation.Our analysis demonstrates that WG is annotatedin a consistent and accurate manner and the smallnumber of errors cannot alone explain the reducedperformance figures.6 Comparing gold-standard corpora6.1 NE class distributionTable 4 compares the distribution of differentclasses of NEs across different corpora on the fourCONLL categories.
WG has a higher proportion ofPER and MISC NEs and a lower proportion of ORGNEs than the BBN corpus.
This is also found in theMUC corpus, although comparisons to MUC are af-fected by its lack of a MISC category.
The CONLL-03 corpus is most similar to WG in terms of thedistribution of the NE classes, although CONLL-03has a smaller proportion of MISC NEs than WG.An analysis of the lengths of NEs in CONLL shows,however, that they are very different to those inWG (see Table 8), perhaps explaining the differ-ence in performance observed.Tag entropy H(C) was calculated for each cor-pus with respect to the 5 possible classes (4 NEclasses, and the O tag, indicating non-entities).H(C) is a measure of the amount of informationrequired to represent the classification of each to-ken in the corpus.
Two calculations are made, in-cluding and excluding the frequent O tag.
Our re-sults (Table 4) suggest that WG?s tags are least pre-dictable, with a tag entropy of 2.0 bits (without theO class) compared to 1.7 and 1.9 bits for BBN andCONLL respectively.6.2 Fine-grained class distributionWhile the CONLL-03 and MUC evaluation corporaare marked up with only very coarse tags, the BBNcorpus uses 29 coarse tags, many with specificsubtypes, including NEs, descriptors of NEs andMapped BBN tag WG BBNPERSON 25.9 19.3ORGANIZATION:OTHER 13.0 2.8ORGANIZATION:CORPORATION 9.2 43.1GPE:CITY 8.0 6.7WORK OF ART:SONG 4.7 0.1NORP 4.3 3.1WORK OF ART:OTHER 4.1 1.3GPE:COUNTRY 3.5 5.1ORGANIZATION:EDUCATIONAL 3.0 0.9GPE:STATE PROVINCE 2.8 2.8ORGANIZATION:POLITICAL 2.6 0.6EVENT:OTHER 2.5 0.4ORGANIZATION:GOVERNMENT 2.0 7.5WORK OF ART:BOOK 1.6 0.4EVENT:WAR 1.6 0.1FAC:OTHER 1.4 0.2LOCATION:REGION 1.3 0.8FAC:ATTRACTION 1.2 0.0Table 5: Distribution of some fine-grained tagsnon-NEs, intended as answer types for questionanswering (Brunstein, 2002).
Non-NE types in-clude MONEY and TIME, which are also tagged inthe MUC corpus, and others such as ANIMAL.
Whenevaluating the performance of the taggers, each ofBBN?s 150 fine-grained tags was mapped to one offour coarse-grained classes or none, using a map-ping described in Nothman (2008).However, since the WG corpus was initially an-notated using 96 distinct classes, we map thesetags to the corresponding fine-grained BBN NEclasses.
In some cases, the tags map exactly(e.g.
COUNTRY mapped to LOCATION:COUNTRY);in other cases, classes have to be merged or notmapped at all, where the BBN and WG annotationsdiffer in granularity.
Where possible, we map tofine-grained BBN categories.We create mappings to a total of 36 BBN entitytypes, and apply them across the WG corpus.
Table5 shows the distribution of the most common tags,calculated as a percentage of all counts of the 36selected tags across each corpus.
Tags for whichthere is at least a two-fold difference in proportionbetween BBN and WG are marked in bold.The comparison is dominated by thepresence of a disproportionate number ofORG:CORPORATIONS in the BBN corpus com-141 2 3 4 5 6 7+ # NEsWG 53.0 77.0 88.9 94.8 96.6 98.2 100 712BBN (train) 75.0 91.0 95.4 97.2 98.2 98.7 100 4 913CONLL (train) 75.0 93.8 98.1 99.5 99.9 99.9 100 3 437Table 6: Comparing MISC NE lengths (cumulative).Feature group WG BBN CONLLCurrent token 0.88 0.89 0.93Current POS 0.43 0.57 0.48Current word-type 0.42 0.49 0.48Previous token 0.46 0.43 0.47Previous POS 0.12 0.19 0.14Previous word-type 0.07 0.14 0.12Table 7: Feature-tag gain ratios.pared to WG.
It also mentions many moregovernmental organisations.
Prominent cases oftags found in higher proportions in WG are worksof art, organisations of type OTHER (e.g.
bands,sports teams, clubs), events and attractions.This comparison demonstrates that there are ob-servable differences in NE types between the newsand Wikipedia domains.
These differences are re-flected in the distribution of both coarse and fine-grained types of NEs.
The more complex entitydistribution in Wikipedia is a likely cause for re-duced NER performance on WG.6.3 Feature-tag gainNobata et al (2000) use gain ratio as an infor-mation-theoretic measure of corpus difficulty:GR(C;F ) =I(C;F )H(C)where I(C;F ) = H(C) ?
H(C|F ) is the infor-mation gain of the NE tag distribution (C) with re-spect to a feature set F .This gain ratio normalises the information gainover the tag entropy, which Nobata et al (2000)suggest allows us to compare gain ratios betweencorpora.
It also makes the impact of including the?O?
tag negligible for our calculations.We apply this approach to measure the relativedifficulty of tagging NEs in the WG corpus.
Ta-ble 7 shows that WG tags seem generally harderto predict than those in newswire, on the basis ofwords, POS tags or orthographic word-types (likethose used in the Curran and Clark (2003b) taggeras proposed by Collins (2002)).In particular, POS tags are less indicative thanin BBN and CONLL, suggesting a wider variety of1 2 3 4 5 6 7+WG 49.9 81.7 93.1 97.4 98.6 99.4 100BBN (train) 57.4 83.3 92.9 97.4 99.1 99.6 100CONLL (train) 63.1 94.5 98.4 99.4 99.8 99.9 100MUC (train) 62.0 89.1 96.1 99.1 99.7 99.8 100Table 8: Comparing all NE lengths (cumulative).grammatical functions in NE names in Wikipedia?
this might be expected with more band names,and song and movie titles.
Alternatively, it may bean indication that the POS tagging is less reliableon Wikipedia using newswire-trained models.The previous word?s orthographic form alsoprovides less information, which may relate to ti-tles like Mr. and Mrs., strong indicators of PER en-tities, which are frequent in BBN and to a lesserextent CONLL, but are almost absent in Wikipedia.6.4 Lengths of named entitiesThe number of tokens in NEs is substantially dif-ferent between WG and other gold-standard cor-pora.
When compared with WG, other gold-standard corpora have a larger proportion ofsingle-word NEs (between 7 and 13% more), asshown in Table 8.
The distribution of NE lengthsin BBN is most similar to WG, but it still differssignificantly in the proportion of single-word NEs.Additionally, WG has a larger number of longmulti-word NEs than the other gold-standard cor-pora.
Longer entities are more difficult to clas-sify, since boundary resolution is more error proneand they typically contain lowercase words witha wider range of syntactic roles.
This adds to thedifficulty of correctly identifying NEs in WG.The difference in entity lengths is most pro-nounced MISC NEs (Table 6), with Wikipedia hav-ing a substantially smaller number of single-wordMISC NEs.
The presence of a large number of longmiscellaneous NEs, including song, film and booktitles, and other works of art are a feature that char-acterises the nature of Wikipedia text in contrastto newswire text.
Typically, longer MISC NEs innewswire text are laws and NORPs, which also ap-pear in Wikipedia text.151 2 3 4 5 6 7+ # NEsWG 49.2 82.9 94.2 98.0 99.2 99.8 100 2 846BBN (train) 55.4 82.4 92.6 97.4 99.2 99.7 100 45 086CONLL (train) 61.1 94.7 98.4 99.4 99.8 99.9 100 20 061MUC (train) 62.0 89.1 96.1 99.1 99.7 99.8 100 4 315Table 9: Comparing non-MISC NE lengths (cumulative).# Sents # with NEs # NEsWG 1 696 1 341 3 558WG WP2-style 571 298 569WG WP4-style 698 425 831Table 10: Size of WG and auto-annotated subsets.7 Evaluation of automatic annotationWe compared the gold-standard annotations in ourWG corpus to those sentences that were automati-cally annotated by Nothman et al (2009).
Theirautomatic annotation process does not retain allWikipedia sentences.
Rather, it selects sentenceswhere, on the basis of capitalisation heuristics,it seems all named entities in the sentence havebeen tagged by the automatic process.
We adoptthis confidence criterion to produce automatically-annotated subsets of the WG corpus.Two variants of their automatic annotation pro-cedure were used: WP2 uses a few rules to infertags for non-linked NEs in Wikipedia; WP4 haslooser criteria for inferring additional links, and itsover-generation typically reduced its performanceas training data (Nothman et al, 2009).A large proportion of sentences in our WG cor-pus cannot be automatically tagged with confi-dence.
Sentence selection leaves 571 sentences(33.7%) after the WP2 process and 698 (41.2%)after the WP4 process (see Table 10).
The use ofthe more permissive WP4 process may lead to thelabelling of more NEs, but many may be spurious.We use three approaches to compare automaticand manual annotations of WG text: (a) treat eachcorpus as test data and evaluate NER performanceon each; (b) treat WP2 and WP4-style subsets asNER predictions on the WG corpus to calculate anF -score; and (c) treat the automatic annotationslike human annotators and calculate ?
values.We first evaluate the WP2 model on eachcorpus and find that performance is higher onautomatically-annotated subsets of WG (Table 11).This is unsurprising given the common automaticannotation process and the effects of the selectioncriterion.
However, Nothman (2008) provides anTRAIN TEST P R FWP2 WG manual 66.5 67.4 66.9WP2 WG WP2-style 76.0 72.9 74.4WP2 WG WP4-style 75.5 71.4 73.4WP2 WP2 ten folds ?
?
83.6WP2 * WG manual * 75.1 67.7 71.2WP2 * WG WP2-style * 81.5 74.4 77.8WP2 * WG WP4-style * 81.9 74.6 78.1WP2 * WP2 ten folds * ?
?
86.1Table 11: NER performance of the WP2-trainedmodel on auto-annotated subsets of WG.?
NE ?
P R FWP2-style 0.94 0.84 89.0 89.0 89.0WP4-style 0.93 0.83 86.8 87.6 87.2Table 12: Comparing WP2-style WG and WP4-style WG on WG.
The automatically annotated datawas treated as predicted annotations on WG.F -score for the WP2 model when evaluated on 10folds of automatically-annotated (WP2-style) testdata.
This F -score is 8?10% higher than WP2?sperformance on the WP2-style subset of WG, sug-gesting that WG?s text is somewhat more difficultto annotate than typical portions of WP2-style text.We compare the annotations of WG text moredirectly by treating the automatic annotations asif they are the output from a tagger run on the 698and 571 sentences that were confidently chosen.
Areasonable agreement between the gold standardand automatic annotation is observed (Table 12),with F -scores of 87.2% and 89.0% achieved byWP2 and WP4.Table 12 also shows inter-annotator agreementcalculated between the automatically annotatedsubsets and the gold-standard annotations in WG,using Cohen?s ?
in the same way as for human an-notators.
The agreement was very high: equal orbetter than the agreement between human annota-tors prior to discussion and correction.8 ConclusionWe have presented the first evaluation of namedentity recognition (NER) on a gold-standard eval-uation of Wikipedia, a resource of increasing16importance in Computational Linguistics.
Weannotated a corpus of Wikipedia articles (WG)with gold-standard NE tags.
Using this new re-source as test data we have evaluated modelstrained on three gold-standard newswire corporafor NER, and compared them to a model trained onWikipedia-derived NER annotations (Nothman etal., 2009).
We found that this WP2 model outper-formed models trained on MUC, CONLL, and BBNdata by more than 7.7% F -score.However, we found that all four models per-formed significantly worse on the WG corpus thanthey did on news text, suggesting that Wikipediaas a textual domain is more difficult for NER.
Weinitially suspected that annotation quality was re-sponsible, but found that we had very high inter-annotator agreement even before further discus-sion and correction of the corpus.
This also val-idates our approach of creating many fine-grainedcategories and then reducing them down to thefour CONLL types.To further examine the difficulty of tagging WG,we compared the distribution of fine-grained entitytypes in WG and BBN, finding a more even dis-tribution over a larger range of types in WG.
Wefound that the standard NER features such as cur-rent and previous POS tags and words had lowerpredictive power on WG.
We also compared thedistribution of NEs lengths and showed that WGentities are longer on average (for instance songand book titles).
This all suggests that Wikipediais genuinely more difficult to automatically anno-tate with named entities than newswire.Finally, we compared the common sentencesbetween Nothman et al?s (2009) automatic NE an-notation of Wikipedia and WG, directly measuringthe quality of automatically deriving NE annota-tions from Wikipedia.We found that WP2 agreed with our finalWG corpus to a high degree, demonstrating thatWikipedia is a viable source of automatically an-notated NE annotated data, reducing our depen-dence on expensive manual annotation for trainingNER systems.AcknowledgementsWe would like to thank the anonymous review-ers for their helpful feedback.
This work wassupported by the Australian Research Council un-der Discovery Project DP0665973.
Dominic Bal-asuriya was supported by a University of Syd-ney Outstanding Achievement Scholarship.
NickyRingland was supported by a Capital MarketsCRC High Achievers Scholarship.
Joel Noth-man was supported by a Capital Markets CRCPhD Scholarship and a University of Sydney Vice-Chancellor?s Research Scholarship.ReferencesAda Brunstein.
2002.
Annotation guidelines for an-swer types.
LDC2005T33, Linguistic Data Consor-tium, Philadelphia.Razvan Bunescu and Marius Pas?ca.
2006.
Using en-cyclopedic knowledge for named entity disambigua-tion.
In Proceedings of the 11th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 9?16.Nancy Chinchor.
1998.
Overview of MUC-7.
In Pro-ceedings of the 7th Message Understanding Confer-ence.Massimiliano Ciaramita and Yasemin Altun.
2005.Named-entity recognition in novel domains with ex-ternal lexical knowledge.
In Proceedings of theNIPS Workshop on Advances in Structured Learningfor Text and Speech Processing.Michael Collins.
2002.
Ranking algorithms fornamed-entity extraction: boosting and the voted per-ceptron.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguis-tics, pages 489?496.Silviu Cucerzan.
2007.
Large-scale named entitydisambiguation based on Wikipedia data.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 708?716.James R. Curran and Stephen Clark.
2003a.
Investigat-ing GIS and smoothing for maximum entropy tag-gers.
In Proceedings of the 10th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 91?98, Budapest, Hungary,12?17 April.James R. Curran and Stephen Clark.
2003b.
Languageindependent NER using a maximum entropy tagger.In Proceedings of the 7th Conference on NaturalLanguage Learning, pages 164?167.Joseph L. Fleiss and Jacob Cohen.
1973.
The equiv-alence of weighted kappa and the intraclass corre-lation coefficient as measures of reliability.
Educa-tional and Psychological Measurement, 33(3):613.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76(5):378?382.17Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In 2001 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),Pittsburgh, PA.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Ex-ploiting Wikipedia as external knowledge for namedentity recognition.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 698?707.Tibor Kiss and Jan Strunk.
2006.
Unsupervised mul-tilingual sentence boundary detection.
Computa-tional Linguistics, 32(4):485?525.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33:159?174.Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza,and Jordi Atserias.
2008.
Learning to tag and tag-ging to learn: A case study on wikipedia.
IEEE In-telligent Systems, 23(5, Sep./Oct.
):26?33.David Nadeau and Satoshi Sekine.
2007.
A sur-vey of named entity recognition and classification.Lingvisticae Investigationes, 30:3?26.Chikashi Nobata, Nigel Collier, and Jun?ichi Tsuji.2000.
Comparison between tagged corpora for thenamed entity task.
In Proceedings of the Workshopon Comparing Corpora, pages 20?27.Joel Nothman, James R Curran, and Tara Murphy.2008.
Transforming Wikipedia into named entitytraining data.
In Proceedings of the AustralasianLanguage Technology Association Workshop 2008,pages 124?132, Hobart, Australia, December.Joel Nothman, Tara Murphy, and James R. Curran.2009.
Analysing Wikipedia and gold-standard cor-pora for NER training.
In Proceedings of the12th Conference of the European Chapter of theACL (EACL 2009), pages 612?620, Athens, Greece,March.Joel Nothman.
2008.
Learning Named Entity Recogni-tion from Wikipedia.
Honours Thesis.
School of IT,University of Sydney.PediaPress.
2007. mwlib MediaWiki parsing library.http://code.pediapress.com.Alexander E. Richman and Patrick Schone.
2008.Mining wiki resources for multilingual named en-tity recognition.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies, pages 1?9, Columbus, Ohio.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 sharedtask: Language-independent named entity recogni-tion.
In Proceedings of the 7th Conference on Natu-ral Language Learning, pages 142?147, Edmonton,Canada.Erik F. Tjong Kim Sang.
2002.
Introduction to theCoNLL-2002 shared task: Language-independentnamed entity recognition.
In Proceedings of the 6thConference on Natural Language Learning, pages1?4, Taipei, Taiwan.Ralph Weischedel and Ada Brunstein.
2005.BBN Pronoun Coreference and Entity Type Cor-pus.
LDC2005T33, Linguistic Data Consortium,Philadelphia.Fei Wu, Raphael Hoffmann, and Daniel S. Weld.
2008.Information extraction from Wikipedia: Movingdown the long tail.
In Proceedings of the 14th In-ternational Conference on Knowledge Discovery &Data Mining, Las Vegas, USA, August.18
