Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 371?374,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsPengYuan@PKU: Extracting Infrequent Sense Instance with theSame N-gram Pattern for the SemEval-2010 Task 15Peng-Yuan Liu1  Shui Liu2  Shi-Wen Yu1  Tie-Jun Zhao21Institute of Computational Linguistics, Peking University, Beijing, China2Department of Computer Science, Harbin Institute of Technology, Harbin, China{liupengyuan,yusw}@pku.edu.cn,{tjzhao,liushui}@mtlab.hit.edu.cnAbstractThis paper describes our infrequent senseidentification system participating in theSemEval-2010 task 15 on Infrequent SenseIdentification for Mandarin Text to SpeechSystems.
The core system is a supervisedsystem based on the ensembles of Na?veBayesian classifiers.
In order to solve theproblem of unbalanced sense distribution, weintentionally extract only instances ofinfrequent sense with the same N-gram patternas the complemental training data from anuntagged Chinese corpus ?
People?s Daily ofthe year 2001.
At the same time, we adjustedthe prior probability to adapt to thedistribution of the test data and tuned thesmoothness coefficient to take the datasparseness into account.
Official result showsthat, our system ranked the first with the bestMacro Accuracy 0.952.
We briefly describethis system, its configuration options and thefeatures used for this task and present somediscussion of the results.1 IntroductionWe participated in the SemEval-2010 task 15 onInfrequent Sense Identification for MandarinText to Speech Systems.
This task requiredsystems to disambiguating the homograph word,a word that has the same POS (part of speech)but different pronunciation.
In this case, we stillconsidered it as a WSD (word sensedisambiguation) problem, but it  is a littledifferent from WSD.
In this task, two or moresenses of the same word may correspond to onepronunciation.
That is, the sense granularity iscoarser than traditional WSD.The challenge of this task is the much skeweddistribution in real text: the most frequentpronunciation accounts for usually over 80%.
Infact, in the training data provided by theorganizer , we found that the sense distributionof some words are distinctly unbalanced.
Foreach of these words, there are fewer than teninstances of one sense whereas the dominantsense instances are hundreds or more.
At thesame time, according to the task description onthe task 15 of SemEval-2010(http://semeval2.fbk.eu/semeval2.php?location=tasks), the test dataset of this task isintentionally divided into the infrequentpronunciation instances and the frequent ones byhalf and half.
Apparently, if we use traditionalmethods and only the provided training datasetto train whatever classifier, it is very likely thatwe will  get an disambiguation result that all (atleast the overwhelming number) the testinstances of these words would be labeled withthe most frequent pronunciation (sense) tag.Then our system is meaningless for the target ofthe task  is focused on the performance ofidentifying the infrequent sense.In order to solve the problem of theunbalanced sense distribution in the training dataand the  fairly balanced sense distribution in thetest data, we designed our PengYuan@PKUsystem, which attempts to extract infrequentsense instances only and adjust the priorprobability so as to counteract the problem as faras possible.
The core system is a supervisedsystem based on the ensembles of Na?veBayesian classifiers.
The complemental trainingdata is extracted from an untagged Chinesecorpus ?
People?s Daily of the year 2001automatically.
Besides the motivation ofinvestigating the function of our method ofcompensating infrequent sense instances, we arealso interested in the role where the smoothnessplays when it encounters with such a datasparseness here.In section 2, we will describe our system thatincludes the core classifier, its configurationoptions and features.
In section 3, we will showthe official results of this task and present someanalyses and discussions.
Section 4 is related371works.
The conclusion and future work are insection 5.2 System Description2.1 Na?ve Bayesian Classifier and FeaturesFor a na?ve Bayesian classifier, the jointprobability of observing a certain combination ofcontext features with a particular sense isexpressed as:1 21( , ,..., , ) ( ) ( | )nn iip F F F S p S p F S== ?
(1)In equation (1), (F1, F2,?, Fn) is featurevariables, S is classification variable and p(S) isthe prior probability of classification variable.Any  parameter  that  has  a  value  of zeroindicates that  the  associated word never occurswith  the  specified  sense  value.
These zerovalues are smoothed by additive smoothingmethod as expressed below:( , )( | ) ( )i ki kkC F SP F S C S Nl+= +    , ??
(0,1)    (2)In equation (2), ?is the smoothness variable.C(Sk) is the times of instances with Sk label.C(Fi,Sk) is the concurrences times of Fi and Sk.
Nis the times of total words in the corpus.The features and their weights of context usedin one single Na?ve Bayesian classifier aredescribed in Table 1.Features Description weightsw-i?wiContent words appearingwithin the window of ?iwords on each side of thetarget word1wj/jj?
[-3,3]Word forms and theirposition information of thewords at fixed positionsfrom the target word.3wk-1wkk?
(-i,i]word bigrams appearingwithin the window of ?i1 wheni>3, else3Pk-1Pkk?
(-i,i]POS bigrams appearingwithin the window of ?i 1Table 1: Features and their weights used in oneNa?ve Bayesian classifier2.2 Ensembles the Na?ve BayesianClassifiersThe ensemble strategy of our system is likePederson (2000).
The windows of context haveseven different sizes (i): 3, 5, 7, 9, 11, 13 and 15words.
The first step in the ensemble approach isto train a separate Na?ve Bayesian classifier foreach of the seven window sizes.Each of the seven member classifiers votes forthe most probable sense given the particularcontext represented by that classifier; theensemble disambiguates by assigning the sensethat receives the majority of the votes.2.3 Infrequent Sense Instances AcquisitionN-gram Increasing Instances Number(-1,1) 246(-2,0) 229 3-gram(0,2) 5511026(9135)(-1,0) 1123 2-gram (0,1) 1844 2967(9135)Table 2: The overview of the training data before andafter the extracting stageSense DistributionAfterTargetWords Before(O)  (O+E3)  (O+E2)?
128 51 128 66 128 2621?
503 83 503 83 503 194??
168 13 168 16 168 23?
175 10 175 27 175 88?
487 42 487 63 487 267??
134 44 134 44 134 49??
125 11 125 11 125 12?
2020 8 2020 12 2020 25?
300 3 300 6 300 32?
268 3 268 4 268 45?
1625 41 1625 346 1625 1625?
144 13 144 15 144 33??
136 8 136 9 136 16?
1666 253 1666 847 1666 1567?
142 17 142 17 142 17?
438 76 438 136 438 414Table 3: The sense distributions of the training databefore and after the extracting stageOur system uses a special heuristic rule to extractthe sense labeled infrequent sense instancesautomatically.
The heuristic rule assumes thatone sense per N-gram which we testified initiallythrough investigating a Chinese sense-taggedcorpus STC (Wu et al, 2006).
Our assumption isinspired by the celebrated one sense percollocation supposition (Yarowsky, 1993).
STCis an ongoing project of building a sense-tagged1 We intentionally control the sense distribution of word(???)
and change it from approximately 2.5:1 to 1:2 so asto investigate the influence.372corpus which contained the sense-tagged 1, 2 and3 months of People?s Daily of the year 2000.According to our investigation, to any targetmulti-sense word, given a specific N-gram (N>1)including the target word, we will expect to seethe same label that range from 88.6% to 99.2%of the time on average.
So, based on the trainingdata, we can extract instance with the same N -gram pattern from the untagged Chinese corpusand we assume if the N-gram is the same thenthe sense-label is the same.For all the 16 multiple-sense target words inthe training data of task 15, we found the N-gramof infrequence sense instances and  extracted2 theinstances with the same N-gram from People?sDaily of the year 2001(about 116M bytes).
Weextracted as many as possible until the totalnumber of them is equal to the dominant senseinstance number.
We appointed the same N-graminstances the same sense tag and (merge?)
it intothe original training corpus.
Table 2 and 3 showthe overview and the sense distribution of thetraining data before and after the extracting stage.Number 9135 in brackets of Table 2 is theinstance number of original training corpus.
O,O+E3, O+E2 in Table 3 mean original trainingdata, original training data plus extracted 3-graminstances and original training data plus extracted2-gram instances respectively.
Limited to thescale of the corpus, the unbalance sensedistribution of some words does not improvemuch.2.4 Other Configuration OptionsSystems Training Data p(S) ?_3.001 O+E3 0.5 0.001_3.1 O+E3 0.5 0.1_2.001 O+E2 0.5 0.001_2.1 O+E2 0.5 0.1Table 4: The system configurationTo formula (1), we tune the prior probability ofclassification variable p(S) as a constant to matchthe sense distribution of test data.
Consideringthe data sparseness as there may have been in thetest stage, to formula (2), we set 2 kinds of?toinvestigate  the effect of smoothness.In total, we develop four systems based onvarious configuration options.
They are showedin Table 4.2 In order to guarantee the extracted instances are notduplicated in the training data or in the test data in case, oursystem filters the repeated instances automatically if theyare already in the original training or test dataset.3 Results and Discussions3.1 Official ResultsSystemIDMicroAccuracyMacroAccuracyRank_3.001 0.974 0.952 1/9_3.1 0.965 0.942 2/9_2.001 0.965 0.941 3/9_2.1 0.965 0.942 2/9Baseline 0.924 0.895Table 5: Official results 1 of PengYuan@PKUPrecision Words_3.001 _3.1 _2.001 _2.1 baseline?
0.844 0.789 0.789 0.789 0.711?
0.976 0.962 0.969 0.962 0.863??
0.901 0.901 0.901 0.901 0.901?
0.978 0.989 0.978 0.989 0.957?
0.925 0.853  0.864 0.853  0.925??
0.956 0.944 0.956 0.944 0.700??
0.971 0.956 0.956 0.956 0.956?
0.998  0.997 0.997 0.997 0.996?
0.987 0.974 0.974 0.974 0.987?
0.956 0.963 0.971 0.963 0.956?
0.983 0.975  0.969 0.975  0.978?
0.924 0.949 0.937 0.949 0.886??
0.986 0.986 0.986 0.986 0.959?
0.986 0.989 0.989 0.989 0.869?
0.875    0.900 0.875    0.900 0.838?
0.981 0.946 0.953 0.946 0.844Table 6: Official results 2 of PengYuan@PKUMacro Accuracy is the average disambiguationprecision of each target word.
Micro Accuracy isthe disambiguation precision of total instances ofall words.
For task 15 whose instancedistribution of the target words is veryunbalanced in the test dataset, Macro Accuracymaybe a better evaluation indicator.
Our systemsachieved from 1st to 4th position (ranked byMacro Accuracy) out of all nine systems thatparticipated in this task.
Our best system isPengYuan@PKU_3.001 which uses originaltraining data plus extracted 3-gram instances asour training data, P(S) is tuned to 0.5 and?isequal to 0.001.3.2 DiscussionsFrom the official result in Table 5 and Table 6we can see, for this task, our classifier andstrategy of extracting infrequency instances iseffective.
Basically, for each target word, the373performances of our systems are superior to thebaseline.From Table 6, we also see the performances ofour systems are influenced by different ?
anddifferent instance extracting patterns.Comparatively smaller probability ?
ofnonoccurrence features is better.
Using theExtracting 3-gram instances is better than that ofusing 2-gram.
(By using the 3-gram method ofextracting instances, we obtain a better resultthan that of 2-gram.
)Our original idea for the system is two-folds.On one hand, we consider the relieving of datasparseness through more instances extracted by2-gram pattern can achieve a better performancethan that of 3-gram pattern, though the instancesextracted through 2-gram pattern induce morenoise.
On the other hand, we assume that theperformance would be better if we had given alarger probability of nonoccurrence features, forthis strategy favors more infrequent senseinstances.
However the unbalance of sensedistribution in the real test data as is shown inTable 5 went beyond our expectation.
It is veryhard for us to evaluate our system from theviewpoint of smoothness and instance sensedistribution.4 Related WorkTo our knowledge, the methods of auto-acquiring sense-labeled instances include usingparallel corpora like Gale et al (1992) and Ng etal.
(2003), extracting by monosemous relative ofWordNet like Leacock et al (1998), Mihalceaand Moldovan (1999), Agirre and Mart?nez(2004), Mart?nez et al (2006) and PengYuan etal.
(2008).
The method proposed by Mihalceaand Moldovan (2000) is also an effective way.5 Conclusion and Future WorkWe participated in the SemEval-2010 task 15 onInfrequent Sense Identification for MandarinText to Speech Systems.
Official results showour system which extract infrequent senseinstances is effective.For the future studies, we will focus on how toidentify the infrequent sense instances effectivelybased on the plan to change the propositionbetween dominant sense and infrequent sensestep by step.AcknowledgmentsThis work was supported by the project ofNational Natural Science Foundation of China(No.60903063) and China Postdoctoral ScienceFoundation funded project (No.20090450007).ReferencesClaudia Leacock, Martin Chodorow and George A.Miller, Using  Corpus Statistics and WordNetRelations for Sense Identification.
ComputationalLinguistics, 1998, 24(1):147~166David Mart?inez, Eneko Agirre and Xinglong Wang.Word relatives in context for word sensedisambiguation.
Proceedings of the 2006Australasian Language Technology Workshop(ALTW2006), 2006:42~50David Yarowsky.
1993.
One sense per collocation.Proceedings of the ARPA Workshop on HumanLanguage Technology.Eneko Agirre and David Mart?inez.
UnsupervisedWSD based  on  automatically retrieved  examples:The  importance of bias.
Proceedings of theInternational Conference on Empirical Methods  inNatural Language Processing, EMNLP,2004:25~32Hwee Tou Ng, Bin Wang, Yee Seng Chan.
ExploitingParallel Texts for Word Sense Disambiguation: AnEmpirical Study.
Proceeding of the 41st ACL, 455-462, Sappora, Japan.Liu Peng-yuan Zhao Tie-jun Yang Mu-yun Li Zhuang.2008.
Unsupervised Translation DisambiguationBased on Equivalent PseudoTranslation Model.Journal of Electronics & Information Technology.30(7):1690-1695.Rada Mihalcea and Dan I. Moldovan.
1999.
Anautomatic method for generating sense taggedcorpora.
Proceedings of AAAI-99, Orlando, FL,July, pages 461?466.Rada Mihalcea and Dan .I.
Moldovan.
2000.
Aniterative approach to word sense disambiguation.Proceedings of FLAIRS-2000, pages 219?223,Orlando, FL, May.Ted.
Pedersen.
2000.
A Simple Approach to BuildingEnsembles of Na?ve Bayesian Classifiers for WordSense Disambiguation.
Proceedings  of  the  FirstAnnual  Meeting  of  the  North  American  Chapterof  the  Association  for Computational Linguistics,pages 63-69, Seattle, WA, May.Yunfang Wu, Peng Jin, Yangsen Zhang, and ShiwenYu.
2006.
A Chinese corpus with word senseannotation.
Proceedings of ICCPOL-2006.William A. Gale, Kenneth W. Church and DavidYarowsky.
A method for disambiguating wordsenses in a large corpus.
Computers and theHumanities, 26(2):415-539374
