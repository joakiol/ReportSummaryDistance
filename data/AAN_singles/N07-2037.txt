Proceedings of NAACL HLT 2007, Companion Volume, pages 145?148,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsJoint Morphological-Lexical Language Modeling for Machine TranslationRuhi Sarikaya Yonggang DengIBM T.J. Watson Research CenterYorktown Heights, NY 10598sarikaya@us.ibm.com ydeng@us.ibm.comAbstractWe present a joint morphological-lexical languagemodel (JMLLM) for use in statistical machine trans-lation (SMT) of language pairs where one or both ofthe languages are morphologically rich.
The pro-posed JMLLM takes advantage of the rich morphol-ogy to reduce the Out-Of-Vocabulary (OOV) rate,while keeping the predictive power of the wholewords.
It also allows incorporation of additionalavailable semantic, syntactic and linguistic informa-tion about the morphemes and words into the lan-guage model.
Preliminary experiments with anEnglish to Dialectal-Arabic SMT system demon-strate improved translation performance over trigrambased baseline language model.1 IntroductionStatistical machine translation (SMT) methods haveevolved from using the simple word based models(Brown et al, 1993) to phrase based models (Marcu andWong, 2002; Koehn et al, 2004; Och and Ney, 2004).More recently, there is a significant effort focusing onintegrating richer knowledge, such as syntactic parse trees(Huang and Knight, 2006) within the translation processto overcome the limitations of the phrase based models.The SMT has been formulated as a noisy channel modelin which the target language sentence, e is seen as dis-torted by the channel into the foreign language f :)()|(argmax)|(argmax?eeePefPfePe ?
?where P(f | e) is the translation model and P(e) is lan-guage model of the target language.
The overwhelmingproportion of the SMT research has been focusing on im-proving the translation model.
Despite several new studies(Kirchhoff and Yang, 2004; Schwenk et al, 2006), lan-guage modeling for SMT has not been receiving muchattention.
Currently, the state-of-the-art SMT systemshave been using the standard word n-gram models.
Sincen-gram models learn from given data, a severe drop inperformance may be observed if the target domain is notadequately covered in  the training data.
The   coverageproblem is aggravated for morphologically rich lan-guages.
Arabic is such a language where affixes areappended to the beginning or end of a stem to generatenew words that indicate case, gender, tense etc.
associ-ated with the stem.
Hence, it is natural that this leads torapid vocabulary growth, which is accompanied byworse language model probability estimation due todata sparsity and high Out-Of-Vocabulary (OOV) rate.Due to rich morphology, one would suspect thatwords may not be the best lexical units for Arabic, andperhaps morphological units would be a better choice.Recently, there have been a number of new methodsusing the morphological units to represent lexical items(Ghaoui et al, 2005; Xiang et al, 2006; Choueiter et al,2006).
Factored Language Models (FLMs) (Kirchhoffand Yang, 2004) share the same idea to some extent buthere words are decomposed into a number of featuresand the resulting representation is used in a generalizedback-off scheme to improve the robustness of probabil-ity estimates for rarely observed word n-grams.In this study we propose a tree structure called Mor-phological-Lexical Parse Tree (MLPT) to combine theinformation provided by a morphological analyzer withthe lexical information within a single Joint Morpho-logical-Lexical Language Model (JMLLM).
The MLPTallows us to include available syntactic and semanticinformation about the morphological segments1 (i.e.prefix/stem/suffix), words or group of words.
TheJMLLM can also be used to guide the recognition forselecting high probability morphological sentence seg-mentations.The rest of the paper is organized as follows.
Section2 provides a description of the morphological segmenta-tion method.
A short overview of Maximum Entropymodeling is given in Section 3.
The proposed JMLLMis presented in Section 4.
Section 5 introduces the SMTsystem and Section 6 describes the experimental resultsfollowed by the conclusions in Section 7.2 Morphological SegmentationApplying the morphological segmentation to dataimproves  the  coverage  and  reduces the OOV rate.
In1 We use ?Morphological Segment?
and ?Morpheme?
inter-changeably.145this study we use a rule-based morphological segmenta-tion algorithm for Iraqi-Arabic (Afify et.
al., 2006).
Thisalgorithm analyzes a given surface word, and generatesone of the four possible segmentations: {stem, pre-fix+stem, suffix+stem, prefix+stem+suffix}.
Here, stemincludes those words that do not have any affixes.
We usethe longest prefixes (suffixes).
Using finer affixes re-duces the n-gram language model span, and leads to poorperformance for a fixed n-gram size.
Therefore, we prede-fine a set of prefixes and suffixes and perform blind wordsegmentation.
In order to minimize the illegitimate seg-mentations we employ the following algorithm.
Using thegiven set of prefixes and suffixes, a word is first blindlychopped to one of the four segmentations mentionedabove.
This segmentation is accepted if the followingthree rules apply:(1) The resulting stem has more than two characters.
(2) The resulting stem is accepted by the Buckwaltermorphological analyzer (Buckwalter, 2002).
(3) The resulting stem exists in the original dictionary.The first rule eliminates many of the illegitimate segmen-tations.
The second rule ensures that the word is a validArabic stem, given that the Buckwalter morphologicalanalyzer covers all words in the Arabic language.
Unfor-tunately, the fact that the stem is a valid Arabic stem doesnot always imply that the segmentation is valid.
The thirdrule, while still not offering such guarantee, simply pre-fers keeping the word intact if its stem does not occur inthe lexicon.
In our implementation we used the followingset of prefixes and suffixes for dialectal Iraqi:?
Prefix list: {chAl, bhAl, lhAl, whAl, wbAl, wAl, bAl,hAl, EAl, fAl, Al, cd, ll, b, f, c, d, w}.?
Suffix list: {thmA, tynA, hmA, thA, thm, tkm, tnA,tny,whA, whm, wkm, wnA, wny, An, hA, hm, hn, km,kn, nA, ny, tm, wA, wh, wk, wn, yn, tk, th, h, k, t, y}.These affixes are selected based on our knowledge oftheir adequacy for dialectal Iraqi Arabic.
In addition, wefound in preliminary experiments that keeping the top-Nfrequent decomposable words intact led to better per-formance.
A value of N=5000 was experimentally foundto work well in practice.
Using this segmentation methodwill produce prefixes and suffixes on the SMT output thatare glued to the following or previous word to form mean-ingful words.3 Maximum Entropy ModelingThe Maximum Entropy (MaxEnt) method is an effec-tive method to combine multiple information sources(features) in statistical modeling and has been used widelyin many areas of natural language processing (Berger etal.,, 2000).
The MaxEnt modeling produces a probabilitymodel that is as uniform as possible while matching em-pirical feature expectations exactly:????
'),'(),()|(ojhojfieihoifiehoP ?
?which describes the probability of a particular outcome(e.g.
one of the morphemes) given the history (h) orcontext.
Notice that the denominator includes a sumover all possible outcomes, o', which is essentially anormalization factor for probabilities to sum to 1.
Theindicator functions if  or features are ?activated?
whencertain outcomes are generated for certain context.
TheMaxEnt model is trained using the Improved IterativeScaling algorithm.4 Joint Morphological-Lexical LanguageModelingThe purpose of morphological analysis is to split aword into its constituting segments.
Hence, a set ofsegments can form a meaningful lexical unit such as aword.
There may be additional information for words orgroup of words, such as part-of-speech (POS) tags, syn-tactic (from parse tree) and semantic information, ormorpheme and word attributes.
For example, in Arabicand to a certain extent in French, some words can bemasculine/feminine or singular/plural.
All of these in-formation sources can be represented using a -what wecall- Morphological-Lexical Parse Tree (MLPT).MLPT is a tree structured joint representation of lexical,morphological, attribute, syntactic and semantic contentof the sentence.
An example of a MLPT for an Arabicsentence is shown in Fig.
1.
The leaves of the tree aremorphemes that are predicted by the language model.Each morphological segment has one of the three attrib-utes: {prefix, stem, suffix} as generated by the morpho-logical analysis mentioned in Sec.
2.
Each word cantake three sets of attributes: {type, gender, number}.Word type can be considered as POS, but here we con-sider only nouns (N), verbs (V) and the rest are labeledas ?other?
(O).
Gender can be masculine (M) or femi-nine (F).
Number can be singular (S), plural (P) or dou-ble (D) (this is specific to Arabic).
For example, NMPlabel for the first2 word, ???
?, shows that this word is anoun (N), male (M), plural (P).
Using the informationrepresented in MLPT for Arabic language modelingprovides a back-off for smooth probability estimationeven for those words that are not seen before.The JMLLM integrates the local morpheme andword n-grams, morphological dependencies and attrib-ute information associated with morphological segmentsand words, which are all represented in the MLPT usingthe MaxEnt framework.
We  trained JMLLM  for Iraqi-2 In Arabic text is written (read) from right-to-left.146Arabic speech recognition task (Sarikaya et al, 2007),and obtained significant improvements over word andmorpheme based trigram language models.We can construct a single probability model that mod-els the joint probability of all of the available informationsources in the MLPT.
To compute the joint probability ofthe morpheme sequence and its MLPT, we use featuresextracted from MLPT.
Even though the framework isgeneric to jointly represent the information sources in theMLPT, in this study we limit ourselves to using only lexi-cal and morphological content of the sentence, along withthe morphological attributes simply because the lexicalattributes are not available yet and we are in the processof labeling them.
Therefore, the information we used fromMLPT in Fig.
1 uses everything but the second row thatcontains lexical attributes (NFS, VFP, NFS, and NMP).Using the morphological segmentation improves thecoverage, for example, splitting the word, ???????
as ???
(prefix) and ????
(stem) as in Fig.
1, allows us to decodeother combinations of this stem with the prefix and suffixlist provided in Sec.2.
These additional combinationscertainly cover those words that are not seen in the un-segmented training data.The first step in building the MaxEnt model is to rep-resent a MLPT as a sequence of morphological segments,morphological attributes, words, and word attributes usinga bracket notation.
Converting the MLPT into a text se-quence allows us to group the semantically related mor-phological segments and their attributes.
In this notation,each morphological segment is associated (this associa-tion is denoted by ?=") with an attribute (i.e.
pre-fix/stem/suffix) and the lexical items are represented byopening and closing tokens, [WORD and WORD] respec-tively.
The parse tree given in Fig.
1 can be converted intoa token sequence in text format as follows:[!S!
[NMP ???
?=stem NMP] [NFS [???????
?
?=prefix ?????=stem???????]
NFS] [VFP  [?
?????
?=prefix ??
?=stem ?
?=suffix ??????
]VFP]  [NFS [???????
??
?=prefix ??????
?=stem  ???????]
NFS] !S!
]This representation uniquely defines the MLPT given inFig.
1.
Given the bracket notation of the text, JMLLM canbe trained in two ways with varying degrees of ?tightnessof integration?.
A relatively ?loose integration?
involvesusing only the leaves of the MLPT as the model outputand estimating P(M|MLPT), where M is the morphemesequence.
In this case JMLLM predicts only morphemes.A tight integration method would require every token inthe bracket representation to be an outcome of the jointmodel.
In our preliminary experiments we chose theloose integration method, simply because the modeltraining time was significantly faster than that for the tightintegration.
segment.
The JMLLM can employ any typeof questions one can derive from MLPT for predicting thenext morphological segment.
In addition to regular tri-gram questions about previous morphological segments,questions about the attributes of the  previous morpho-Fig 1.
Morphological-Lexical Parse Tree.logical segments, parent lexical item and attributes ofthe parent lexical item can be used.
Obviously jointquestions combining these information sources are alsoused.
Obviously joint questions combining these infor-mation sources are also used.
These questions include(1) previous morpheme 1?im and current active parentword ( iw ) (2) ii wm ,1?
and previous morpheme attribute( 1?ima ).
(3) iii wmama ,, 21 ??
,lexical attribute ( iwa ) and21 , ??
ii mm .The history given in )|( hoP consists of answers tothese questions.
In our experiments, we have not ex-haustively searched for the best feature set but ratherused a small subset of these features which we believedto be helpful in predicting the next morpheme.
The lan-guage model score for a given morpheme using JMLLMis conditioned not only on the previous morphemes butalso on their attributes, and the lexical items and theirattributes.
As such, the language model scores aresmoother compared to n-gram models especially forunseen lexical items.5 Statistical Machine Translation SystemStarting from a collection of parallel sentences, wetrained word alignment models in two translation direc-tions, from English to Iraqi Arabic and from Iraqi Ara-bic to English, and derived two sets of Viterbialignments.
By combining word alignments in two di-rections using heuristics (Och and Ney, 2003), a singleset of static word alignments was then formed.
Allphrase pairs which respect to the word alignmentboundary constraint were identified and pooled togetherto build phrase translation tables with the MaximumLikelihood criterion.
The maximum number of words inArabic phrases was set to 5.Our decoder is the phrase-based multi-stack imple-mentation of log-linear models similar to Pharaoh(Koehn et al 2004).
Like most other MaxEnt-baseddecoders, active features in our decoder include transla-tion models in two directions, lexicon weights in two!S!??????
???????????????
?NMPNFSVFPstem prefix stem???????NFS???????
?????
??
?suffix prefixstemstem prefix147directions, language model, distortion model, and sen-tence length penalty.6 ExperimentsThe parallel corpus has 459K utterance pairs with 90Kwords (50K morphemes).
The Iraqi-Arabic languagemodel training data is slightly larger than the Iraqi-Arabicside of the parallel corpus and it has 2.8M words with98K unique lexical items.
The morphologically analyzedtraining data has 2.6M words with 58K unique vocabularyitems.
A statistical trigram language model using Modi-fied Knesser-Ney smoothing has been built for the mor-phologically segmented data.
The test data consists of2242 utterances (3474 unique words).
The OOV rate forthe unsegmented test data is 8.7%, the correspondingnumber for the morphologically analyzed data is 7.4%.Hence, morphological segmentation reduces the OOVrate by 1.3% (15% relative), which is not as large reduc-tion as compared to training data (about 40% relative re-duction).
We believe this would limit the potentialimprovement we could get from JMLLM, since JMLLMis expected to be more effective compared to word n-gram models, when the OOV rate is significantly reducedafter segmentation.We measure translation performance by the BLEUscore (Papineni et al 2002) with one reference for eachhypothesis.
In order to evaluate the performance of theJMLLM, a translation N-best list (N=10) is generatedusing the baseline Morpheme-trigram language model.First, on a heldout development data all feature weightsincluding the language model weight are optimized tomaximize the BLEU score using the downhill simplexmethod (Och and Hey, 2002).
These weights are fixedwhen the language models are used on the test data.
Thetranslation BLEU (%) scores are given in Table 1.
Thefirst entry (37.59) is the oracle BLEU score for the N-bestlist.
The baseline morpheme-trigram achieved 29.63,word-trigram rescoring improved the BLEU score to29.91.
The JMLLM achieved 30.20 and log-linear inter-polation with the morpheme-trigram improved the BLEUscore to 30.41.7 ConclusionsWe presented a new language modeling technique calledJoint Morphological-Lexical Language Modeling(JMLLM) for use in SMT.
JMLLM allows joint modelingof lexical, morphological and additional informationsources about morphological  segments,  lexical  itemsand sentence.
The translation results demonstrate thatjoint modeling provides encouraging improvement overmorpheme  based language  model.
Our future workwill be directed towards tight integration of all availableTable 1.
SMT N-best list rescoring.LANGUAGE MODELS BLEU (%)N-best Oracle 37.59Morpheme-trigram 29.63Word-trigram 29.91JMLLM 30.20JMLLM + Morpheme-Trigram 30.41information by predicting the entire MLPT (besidesleaves).ReferencesP.
Brown er al.,.
1993.
The mathematics of statistical machine transla-tion.
Computational Linguistics, 19(2):263?311.A.
Berger, S. Della Pietra and V. Della Pietra, "A Maximum EntropyApproach to Natural Language Processing," Computational Lin-guistics, vol.
22, no.
1, March 1996T.
Buckwalter.
2002.
Buckwalter Arabic morphological analyzerversion 1.0, LDC2002L49 and ISBN 1-58563-257-0, 2002.G.
Choueiter, D. Povey, S.F.
Chen, and G. Zweig, 2006.
Morpheme-based language modeling for Arabic LVCSR.
ICASSP?06, Tou-louse, France, 2006.A.
Ghaoui, F. Yvon, C. Mokbel, and G. Chollet, 2005.
On the use ofmorphological constraints in N-gram statistical language model,Eurospeech?05, Lisbon, Portugal, 2005.B.
Huang and K. Knight.
2006.
Relabeling Syntax Trees to ImproveSyntax-Based Machine Translation Quality.
In HLT/NAACL.B.
Xiang, K. Nguyen, L. Nguyen, R. Schwartz, J. Makhoul, 2006.Morphological decomposition for Arabic broadcast news tran-scription?, ICASSP?06,  Toulouse, France, 2006.K.
Kirchhoff and M. Yang.
2005.
Improved language modeling forstatistical machine translation.
In ACL?05 workshop on Buildingand Using Parallel Text, pages 125?128.P.
Koehn, F. J. Och, and D. Marcu.
2004.
Pharaoh: A beam searchdecoder for phrase based statistical machine translation models.
InProc.
of 6th Conf.
of  AMTA.F.
J. Och and H. Ney.
2002.
Discriminative training and maximumentropy models for statistical machine translation.
In ACL, pages295?302, University of Pennsylvania.F.
J. Och and H. Ney.
2003.
A Systematic Comparison of VariousStatistical Alignment Models.
Comp.
Linguistics, 29(1):9--51.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.
BLEU: aMethod for Automatic Evaluation of machine translation.
In ACL02, pages 311?318H.
Schwenk, D. D?echelotte  and J-L. Gauvain.
2006.
Continuousspace language models for statistical machine translation.
InACL/COLING, pages 723?730.M.
Afify, R. Sarikaya, H-K. J. Kuo, L. Besacier and Y. Gao.
2006.
Onthe Use of Morphological Analysis for Dialectal Arabic SpeechRecognition, In Interspeech-2006, Pittsburgh PA.R.
Sarikaya, M .Afify and Y. Gao.
2007.
Joint Morphological-LexicalModeling (JMLLM) for Arabic.
ICASSP 2007, Honolulu Hawaii.148
