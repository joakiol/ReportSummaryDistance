Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134?137,Paris, October 2009. c?2009 Association for Computational LinguisticsScalable Discriminative Parsing for GermanYannick VersleySFB 833Universita?t Tu?bingenversley@sfs.uni-tuebingen.deInes RehbeinDep.
of Computational LinguisticsUniversita?t des Saarlandesrehbein@coli.uni-sb.deAbstractGenerative lexicalized parsing models,which are the mainstay for probabilisticparsing of English, do not perform as wellwhen applied to languages with differ-ent language-specific properties such asfree(r) word order or rich morphology.
ForGerman and other non-English languages,linguistically motivated complex treebanktransformations have been shown to im-prove performance within the frameworkof PCFG parsing, while generative lexical-ized models do not seem to be as easilyadaptable to these languages.In this paper, we show a practical wayto use grammatical functions as first-classcitizens in a discriminative model that al-lows to extend annotated treebank gram-mars with rich feature sets without hav-ing to suffer from sparse data problems.We demonstrate the flexibility of the ap-proach by integrating unsupervised PP at-tachment and POS-based word clustersinto the parser.1 IntroductionTo capture the semantic relations inherent in atext, parsing has to recover both structural infor-mation and grammatical functions, which com-monly coincide in English, but not in freerword order languages such as German.
In-stead one has to make use of morphological fea-tures in addition to exploiting ordering preferencessuch as the (violatable) default ordering of (sub-ject<)dative<accusative.Because of this fact, many successful ap-proaches for German PCFG parsing (Schiehlen,2004; Dubey, 2005; Versley, 2005) use annotatedtreebank grammars where the constituent treesfrom the treebank are enriched with further lin-guistic information that allows an adequate recon-struction of syntactic relationships, suggesting thatprobabilistic context-free grammars are an ade-quate tool for parsing these languages.In the ACL 2008 workshop on Parsing Ger-man (Ku?bler, 2008), Rafferty and Manning (2008)used a lexicalized PCFG parser using markoviza-tion and parent annotation, but no linguistically in-spired transformations; Rafferty and Manning didquite well on constituents, but were not success-ful in reconstructing grammatical functions, withresults considerably worse than for other submis-sions in the shared task.The framework we present in this paper ?
an-notated treebank grammars with a discriminativemodel that allows lexicalization based on gram-matical function assignment, as well as the ad-dition of features based on unsupervised learn-ing, including PP attachment and word clusters ?shows that it is possible to achieve good improve-ments over generative lexicalized models by usingthe additional flexibility gained over standard lex-icalized PCFG models.
Our approach offers moreflexibility than generative PCFG models, whilecomputational costs for development and practi-cal use are still acceptable.
While we only presentresults for German, we are confident that the re-sults carry over to other languages where anno-tated treebank grammars have been used success-fully.2 Parsing German with Morphology andValence InformationAs a base parser, we use BitPar (Schmid, 2004),a fast unlexicalized PCFG parser based on a firstpass where non-probabilistic bottom-up parsingand top-down filtering is carried out efficiently bystoring the chart in bit vectors, and construct theprobabilistic chart only after top-down filtering.We use an annotated treebank PCFG that is de-134rived from the Tiger treebank and largely inspiredby earlier work on annotated treebank grammarsfor German (Schiehlen, 2004; Dubey, 2005; Vers-ley, 2005).Subcategorization With respect to the treebankgrammar, we refine the node labels with linguisti-cally important information that is only implicit inthe treebank but would be tedious (and pointless)to annotate by hand:Firstly, we annotate NPs by case; clause nodes(S and VP) are subcategorized by the clause type(fin,inf,izu,rel), and NPs and PPs with a relativepronoun are marked.
Comparative phrases (e.g.,bigger [than a house], marked as NP in Tiger andTu?Ba-D/Z) are marked by adding a ?CC?
endingto the node label.
Finally, auxiliaries are split ac-cording to their verb lemma into sein (be), haben(have), werden (become).To aid the identification of noun phrase case,we add information related to case/number/gendersyncretism to the preterminal labels of determin-ers, nouns, and adjectives (for details, see Versley,2005) that allows to accurately determine the set ofpossible cases while keeping the size of the tagsetrelatively small .Verb Valence We use information from the lex-icon of the WCDG parser for German (Foth andMenzel, 2006) to mark verbs according to the ar-guments that they can take.
While the WCDGlexicon contains more information, we only en-code the possibility of accusative and dative com-plements, ignoring entries for genitive or clausalcomplements.Markovization with Argument Marking Ithas been noted consistently (Klein and Manning,2003; Schiehlen, 2004) that using markovization- replacing the original treebank rules by an ap-proximation that only considers a limited contextwindow of one or two siblings - improves re-sults at least for a constituency-based evaluation.However, in some cases this simple markoviza-tion scheme leads to undesirable results includ-ing sentences with multiple subjects, as predica-tive arguments also have nominative case.
Toavoid this, we additionally mark which argumentshave already been seen, yielding node labels suchas S fin<VVFIN a<RNP a<sa in the case of apartial constituent for a finite sentence (S fin)expanding to the right (<R) where both subject (s)and accusative object (a) have already been seen.Unknown Words For the base PCFG parse, weuse a decision tree with 43 regular expressions asfeatures, five of which are tailored towards rec-ognizing the past and zu-infinitive form of sep-arable prefix verbs (abarbeiten ?
abgearbeitet,abzuarbeiten), which cannot be recognized byconsidering suffixes only.
The extended part ofspeech tags for verbs (which contain valency in-formation) are interpolated between the distribu-tion at the concrete leaf of the decision tree and theglobal valency distribution for the (coarse) part-of-speech tag.Additionally, we use SMOR (Schmid et al,2004) in conjunction with the verb lexicon anda gazetteer list containing person and locationnames to determine possible fine-grained part-of-speech tags for unknown words.Restoring Grammatical Functions Addingedge labels to the nodes in PCFG parsing easilycreates sparse data problems, as reported byRafferty and Manning (2008), who witness a dropin constituent F-measure (excluding grammaticalfunction labels) when they include function labelsin the symbols of their PCFG.
On the other hand,the informativity of grammatical function labelsfor the contents of the node does not alwaysjustify their cost in terms of data sparseness.Thus, we chose an approach where we includelinguistically relevant information in the nodelabels (see above), and use the finer categoriza-tion to restore the grammatical function labelsautomatically: Using the most frequent functionlabel sequence associated with a rule yields goodresults even in the presence of markovization,where some of the surrounding context is lost.Furthermore, this approach allows us to use thegrammatical function label assignments in thesubsequent discriminative model, thus yieldingtyped dependencies rather than the unlabeleddependencies that are used in the lexicalizationmodel of the Stanford parser.3 Discriminative ParsingGenerative parsing models are based on few dis-tributions that use different feature combinationsbased on smoothing; incorporating additional fea-tures into these is very difficult at best.As a result, the use of external preferences insuch parsers is usually limited to approaches thatreattach dependents in the output of the parserrather than integrating them in the parsing process.135Settings no GFs with GFsRafferty and Manning (2008) 77.40 NA?, training with GFs 72.09 60.48markov[unlex] 74.66 62.47markov+parent[unlex] 73.94 61.63markovGF[unlex] 75.00 63.58markov[lex] 77.68 66.05markovGF[lex] 77.55 66.69markovGF[+pp] 78.43 67.90Table 1: Evaluation results: PARSEVAL F1 onPaGe development setDiscriminative parsing for unification-basedgrammar commonly uses the conditional randomfield formulation introduced by Miyao and Tsu-jii (2002) and Geman and Johnson (2002), whichuses local features to select a parse from a packedforest.
The much larger cost in terms of mem-ory and time compared to generative models hasuntil recently made this approach largely unattrac-tive (but see Finkel et al, 2008, who distributes thelearning process over several powerful machines).An alternative use of discriminative modelshas been to incorporate global features, either byreranking (e.g.
Charniak and Johnson, 2005, orKu?bler et al, 2009 for German) or by beam searchover a pruned parse forest (Huang, 2008).
How-ever, Huang shows that a discriminative model us-ing only local features reaps most of the benefitsof the global model and performs at a similar levelthan earlier reranking-based approaches, pointingto the fact that local ambiguities often result in then-best list not containing the correct parse.The model we propose here extracts a prunedparse forest from a simple unlexicalized parser andthen uses a factored discriminative model to applya rich set of features using the lexicalized parsetree and its typed dependencies.CRF parsing on pruned forests We extract apruned forest that contains exactly those nodes andedges that can occur in trees that have a probabil-ity ?
pbest ?
t, where in practice a threshold oft = 10?3 ensures that no good parse is prunedaway while at the same time, the resulting foresthas only few nodes and edges.For training, we extract an oracle tree, which isselected according to a combination of correct (an-notated grammar) constituents, the absence of in-correct constituents, and the likelihood of the tree,to account for the fact that the forest does not al-fW-w-pos, CW-w-pos word form, clusterf-sp, fS-sp-size node label, node size(1)f-sp-RHS rule expansionLDdir-sp-sd-hsd daughter attachmentLH-sp-sd-hsd-hld head projectionLddir-hsp-hsd dependency (pos-pos)Lddir-hsp-hsd-dist attachment length(1)Ledir-hsp-hsd-hld dependency (pos-lemma)Lfdir-hsp-hlp-hsd dependency (pos-lemma)Lfdir-hsp-hlp-hsd-GF typed dep.
(lemma-pos)LhGF-hcp-hlp-hcd-hld typed dep.
(lemma-lemma)MIpp-prep, MIpp0-prep PP attach (noun)MIppV-prep, MIppV0-prep PP attach (verb)1) node sizes and attachment distances are discretized.dir: one of H(head), L/R(head dep), B/I/E(nonheaded dep)sp/d constituent symbol (parent/dep), hsp/d head cat, hchead cat (coarse), hl head lemmaTable 2: List of Featuresways contain the exact gold tree.
We then usethe AMIS maximum entropy learner of Miyao andTsujii (2002) to learn the discriminative model bycreating a forest from a grammar learned on theremaining 4/5 of the training data.Efficiency Parsing using the discriminativemodel is quite efficient, with a memory con-sumption for the whole system at about 270MB,including the data used to determine the corpusderived features (word clusters, mutual informa-tion statistics, semantic role clusters).
Parsingspeed is at 1.65sec./sentence on a 1.5GHz Pen-tium M, against 1.84sec./sent for BitPar alonewhen not using the tag filter for unknown words.The time needed for learning can be reducedby keeping the pruned parse charts and only re-running the part of lexicalization and discrimina-tive feature extraction; when reusing the old pa-rameters as a starting point for AMIS?
model esti-mation, the turn-around time including feature ex-traction is below two hours.3.1 Clustering for unknown wordsTo improve the behaviour on unknown wordswhere morphological analyzer and regular expres-sions do not yield informative preferences, we ex-ploit a large, part-of-speech-tagged corpus to in-duce clusters which provide robust informationthat is useful even in our case where preterminalsin the PCFG are finer than standard POS tags.The following features were gathered and usedby weighting by the pointwise mutual informationbetween the word and feature occurrences:The context feature retrieves windows of high-frequent words surrounding the word in question136(e.g.
der mit for ?der Mann mit den Blumen?
).The context2 feature retrieves windows of onehigh-frequent word and one part-of-speech tagsurrounding the word in question (e.g.
der NN for?der scho?ne Mann?
).The postag feature simply retrieves the part-of-speech tag that is assigned to the word.The result of using the repeated bisectingk-means implementation of CLUTO (Steinbachet al, 2000) on the resulting features yields syntac-tically sensible clusters containing years, moneysums, last names, or place names.3.2 Unsupervised PP Attachment andSubject-Object preferencesWe used simple part-of-speech tag patterns togather statistics on the association between nounsand immediately following prepositions, as wellas between prepositions and closely followingverbs on the DE-WaC corpus (Baroni and Kilgar-iff, 2006), an 1.7G words sample of the German-language WWW.
The mutual information valuesfor PP attachment are made available to the parseras features that are weighted by the mutual infor-mation value.4 Evaluation and DiscussionTo evaluate our approach, we use the datasetused for the ACL-2008 Parsing German Workshop(Ku?bler, 2008) that contains 26,116 sentences ofthe TIGER treebank (Brants et al, 2002), in a 8:1:1split of training, testing, and evaluation data, andvalidate our approach on the development data,where the results published by Rafferty and Man-ning (2008) provide a useful comparison.
All ourexperiments are done using tags automatically as-signed by the parser, which reaches a tagging ac-curacy of about 97.5% according to the EVALBoutput.We find that our final model, combining aug-menting the treebank labels with lingustic infor-mation in addition to lexicalization and unsuper-vised PP attachment works better than the best-performing models of Rafferty and Manning, witha very large improvement in grammatical func-tions that is only surpassed by the Berkeley Parser(Petrov and Klein, 2008), showing that our combi-nation of annotated treebank grammars with a fac-tored discriminative model not only allows greatcontrol and flexibility for experimenting with theinclusion of novel features, but also yields verygood results compared with the state of the artfor German (see table 1 for results on the Tigertreebank).
Preliminary results on Tu?Ba-D/Z witha subset of the transformations of Versley (2005)show the same tendency as the results for Tiger,with 91.3% for constituents only, and 80.1% in-cluding function labels (compared to 88.9% and77.2% for the Stanford parser).Future work will investigate the impact of in-cluding additional features into the discriminativeparsing model.ReferencesBaroni, M. and Kilgariff, A.
(2006).
Large linguistically-processed web corpora for multiple languages.
In EACL2006.Brants, S., Dipper, S., Hansen, S., Lezius, W., and Smith, G.(2002).
The TIGER treebank.
In Proc.
TLT 2002.Charniak, E. and Johnson, M. (2005).
Coarse-to-fine n-bestparsing and maxent discriminative reranking.
In Proc.ACL 2005.Dubey, A.
(2005).
What to do when lexicalization fails: pars-ing German with suffix analysis and smoothing.
In ACL-2005.Finkel, J. R., Kleeman, A., and Manning, C. D. (2008).
Effi-cient, feature-based, conditional random field parsing.
InACL/HLT-2008.Foth, K. and Menzel, W. (2006).
Hybrid parsing: Using prob-abilistic models as predictors for a symbolic parser.
InACL 2006.Geman, S. and Johnson, M. (2002).
Dynamic programmingfor parsing and estimation of stochastic unification-basedgrammars.
In ACL 2002.Huang, L. (2008).
Forest reranking: Discriminative parsingwith non-local features.
In HLT/ACL 2008.Klein, D. and Manning, C. D. (2003).
Accurate unlexicalizedparsing.
In ACL 2003.Ku?bler, S. (2008).
The PaGe 2008 shared task on parsingGerman.
In Proceedings of the ACL-2008 Workshop onParsing German.Ku?bler, S., Hinrichs, E., Maier, W., and Klett, E. (2009).
Pars-ing coordinations.
In EACL 2009.Miyao, Y. and Tsujii, J.
(2002).
Maximum entropy estimationfor feature forests.
In HLT 2002.Petrov, S. and Klein, D. (2008).
Parsing German with latentvariable grammars.
In Parsing German Workshop at ACL-HLT 2008.Rafferty, A. and Manning, C. D. (2008).
Parsing three Ger-man treebanks: Lexicalized and unlexicalized baselines.In ACL?08 workshop on Parsing German.Schiehlen, M. (2004).
Annotation strategies for probabilisticparsing in German.
In Proc.
Coling 2004.Schmid, H. (2004).
Efficient parsing of highly ambiguouscontext-free grammars with bit vectors.
In Proc.
Coling2004.Schmid, H., Fitschen, A., and Heid, U.
(2004).
SMOR: AGerman computational morphology covering derivation,composition and inflection.
In Proceedings of LREC 2004.Steinbach, M., Karypis, G., and Kumar, V. (2000).
A com-parison of document clustering techniques.
In KDD Work-shop on Text Mining.Versley, Y.
(2005).
Parser evaluation across text types.
InProceedings of the Fourth Workshop on Treebanks andLinguistic Theories (TLT 2005).137
