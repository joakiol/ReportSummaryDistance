Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1607?1615,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsRanking Class Labels Using Query SessionsMarius Pas?caGoogle Inc.1600 Amphitheatre ParkwayMountain View, California 94043mars@google.comAbstractThe role of search queries, as available withinquery sessions or in isolation from one an-other, in examined in the context of rankingthe class labels (e.g., brazilian cities, busi-ness centers, hilly sites) extracted from Webdocuments for various instances (e.g., rio dejaneiro).
The co-occurrence of a class la-bel and an instance, in the same query orwithin the same query session, is used to re-inforce the estimated relevance of the class la-bel for the instance.
Experiments over eval-uation sets of instances associated with Websearch queries illustrate the higher quality ofthe query-based, re-ranked class labels, rel-ative to ranking baselines using document-based counts.1 IntroductionMotivation: The offline acquisition of instances (riode janeiro, porsche cayman) and their correspond-ing class labels (brazilian cities, locations, vehicles,sports cars) from text has been an active area of re-search.
In order to extract fine-grained classes ofinstances, existing methods often apply manually-created (Banko et al, 2007; Talukdar et al, 2008) orautomatically-learned (Snow et al, 2006) extractionpatterns to text within large document collections.In Web search, the relative ranking of documentsreturned in response to a query directly affects theoutcome of the search.
Similarly, the quality ofthe relative ranking among class labels extracted fora given instance influences any applications (e.g.,query refinements or structured extraction) using theextracted data.
But due to noise in Web data andlimitations of extraction techniques, class labels ac-quired for a given instance (e.g., oil shale) may failto properly capture the semantic classes to which theinstance may belong (Kozareva et al, 2008).
In-evitably, some of the extracted class labels will beless useful (e.g., sources, mutual concerns) or incor-rect (e.g., plants for the instance oil shale).
In pre-vious work, the relative ranking of class labels foran instance is determined mostly based on featuresderived from the source Web documents from whichthe data has been extracted, such as variations of thefrequency of co-occurrence or diversity of extractionpatterns producing a given pair (Etzioni et al, 2005).Contributions: This paper explores the role ofWeb search queries, rather than Web documents, ininducing superior ranking among class labels ex-tracted automatically from documents for various in-stances.
It compares two sources of indirect rankingevidence available within anonymized query logs:a) co-occurrence of an instance and its class labelin the same query; and b) co-occurrence of an in-stance and its class label, as separate queries withinthe same query session.
The former source is a noisyattempt to capture queries that narrow the search re-sults to a particular class of the instance (e.g., jaguarcar maker).
In comparison, the latter source nois-ily identifies searches that specialize from a class(e.g., car maker) to an instance (e.g., jaguar) or,conversely, generalize from an instance to a class.To our knowledge, this is the first study comparinginherently-noisy queries and query sessions for thepurpose of ranking of open-domain, labeled class in-stances.1607The remainder of the paper is organized as fol-lows.
Section 2 introduces intuitions behind anapproach using queries for ranking class labels ofvarious instances, and describes associated rankingfunctions.
Sections 3 and 4 describe the experi-mental setting and evaluation results over evaluationsets of instances associated with Web search queries.The results illustrate the higher quality of the query-based, re-ranked lists of class labels, relative to alter-native ranking methods using only document-basedcounts.2 Instance Class Ranking via Query LogsRanking Hypotheses: We take advantage ofanonymized query logs, to induce superior rankingamong the class labels associated with various classinstances within an IsA repository acquired fromWeb documents.
Given a class instance I, the func-tions used for the ranking of its class labels are cho-sen following several observations.?
Hypothesis H1: If C is a prominent class of aninstance I, then C and I are likely to occur in text incontexts that are indicative of an IsA relation.?
Hypothesis H2: If C is a prominent class of aninstance I, and I is ambiguous, then a fraction ofthe queries about I may also refer to and contain C.?
Hypothesis H3: If C is a prominent class of aninstance I, then a fraction of the queries about Imay be followed by queries about C, and vice-versa.Ranking Functions: The ranking functions followdirectly from the above hypotheses.?
Ranking based on H1 (using documents): Thefirst hypothesis H1 is a reformulation of findingsfrom previous work (Etzioni et al, 2005).
In prac-tice, a class label is deemed more relevant for an in-stance if the pair is extracted more frequently and bymultiple patterns, with the scoring formula:ScoreH1(C, I) = Freq(C, I)?
Size({Pattern(C)})2 (1)where Freq(C, I) is the frequency of extraction ofC for the instance I, and Size({Pattern(C)}) is thenumber of unique patterns extracting the class labelC for the instance I.
The patterns are hand-written,following (Hearst, 1992):?[..]
C [such as|including] I [and|,|.
]?,where I is a potential instance (e.g., diderot) and Cis a potential class label (e.g., writers).
The bound-aries are approximated from the part-of-speech tagsof the sentence words, for potential class labels C;and identified by checking that I occurs as an entirequery in query logs, for instances I (Van Durme andPas?ca, 2008).The application of the scoring formula (1) to can-didates extracted from the Web produces a rankedlist of class labels LH1(I).?
Ranking based on H2 (using queries): Intu-itively, Web users searching for information aboutI sometimes add some or all terms of C to a searchquery already containing I, either to further spec-ify their query, or in response to being presentedwith sets of search results spanning several mean-ings of an ambiguous instance.
Examples of suchqueries are happiness emotion and diderot philoso-pher.
Moreover, queries like happiness positive psy-chology and diderot enlightenment may be consid-ered to weakly and partially reinforce the relevanceof the class labels positive emotions and enlighten-ment writers of the instances happiness and diderotrespectively.
In practice, a class label is deemedmore relevant if its individual terms occur in pop-ular queries containing the instance.
More precisely,for each term within any class label from LH1(I),we compute a score TermQueryScore.
The score isthe frequency sum of the term within anonymizedqueries containing the instance I as a prefix, andthe term anywhere else in the queries.
Terms arestemmed before the computation.Each class label C is assigned the geometric meanof the scores of its N terms Ti, after ignoring stopwords:ScoreH2(C, I) = (N?i=1TermQueryScore(Ti))1/N (2)The geometric mean is preferred to the arithmeticmean, because the latter is more strongly affected byoutlier values.
The class labels are ranked accordingto the means, resulting in a ranked list LH2(I).
Incase of ties, LH2(I) keeps the relative ranking fromLH1(I).?
Ranking based on H3 (using query sessions):Given the third hypothesis H3, Web users searchingfor information about I may subsequently search formore general information about one of its classes C.Conversely, users may specialize their search froma class C to one of its instances I.
Examples ofsuch queries are happiness followed later by emo-tions, or diderot followed by philosophers; or emo-1608tions followed later by happiness, or philosophersfollowed by diderot.
In practice, a class label isdeemed more relevant if its individual terms occur aspart of queries that are in the same query session as aquery containing only the instance.
More precisely,for each term within any class label from LH1(I),we compute a score TermSessionScore, equal to thefrequency sum of the anonymized queries from thequery sessions that contain the term and are: a) ei-ther the initial query of the session, with the instanceI being one of the subsequent queries from the samesession; or b) one of the subsequent queries of thesession, with the instance I being the initial queryof the same session.
Before computing the frequen-cies, the class label terms are stemmed.Each class label C is assigned the geometric meanof the scores of its terms, after ignoring stop words:ScoreH3(C, I) = (N?i=1TermSessionScore(Ti))1/N (3)The class labels are ranked according to the geo-metric means, resulting in a ranked list LH3(I).
Incase of ties, LH3(I) preserves the relative rankingfrom LH1(I).Unsupervised Ranking: Given an instance I, theranking hypotheses and corresponding functionsLH1(I), LH2(I) and LH3(I) (or any combinationof them) can be used together to generate a merged,ranked list of class labels per instance I.
The scoreof a class label in the merged list is determined bythe inverse of the average rank in the lists LH1(I)and LH2(I) and LH3(I), computed with the follow-ing formula:ScoreH1+H2+H3(C, I) =N?Ni Rank(C, LHi)(4)where N is the number of input lists of class labels(in this case, 3), and Rank(C, LHi) is the rank of Cin the input list of class labels LHi (LH1, LH2 orLH3).
The rank is set to 1000, if C is not present inthe list LHi.
By using only the relative ranks and notthe absolute scores of the class labels within the in-put lists, the outcome of the merging is less sensitiveto how class labels of a given instance are numeri-cally scored within the input lists.
In case of ties,the scores of the class labels from LH1(I) serve as asecondary ranking criterion.
Thus, every instance Ifrom the IsA repository is associated with a rankedlist of class labels computed according to this rank-ing formula.
Conversely, each class label C fromthe IsA repository is associated with a ranked listof class instances computed with the earlier scoringformula (1) used to generate lists LH1(I).Note that the ranking formula can also consideronly a subset of the available input lists.
For in-stance, ScoreH1+H2 would use only LH1(I) andLH2(I) as input lists; ScoreH1+H3 would use onlyLH1(I) and LH3(I) as input lists; etc.3 Experimental SettingTextual Data Sources: The acquisition of theIsA repository relies on unstructured text availablewithin Web documents and search queries.
Thequeries are fully-anonymized queries in English sub-mitted to Google by Web users in 2009, and areavailable in two collections.
The first collection isa random sample of 50 million unique queries thatare independent from one another.
The second col-lection is a random sample of 5 million query ses-sions.
Each session has an initial query and a se-ries of subsequent queries.
A subsequent query is aquery that has been submitted by the same Web userwithin no longer than a few minutes after the initialquery.
Each subsequent query is accompanied byits frequency of occurrence in the session, with thecorresponding initial query.
The document collec-tion consists of a sample of 100 million documentsin English.Experimental Runs: The experimental runs corre-spond to different methods for extracting and rank-ing pairs of an instance and a class:?
from the repository extracted here, with classlabels of an instance ranked based on the frequencyand the number of extraction patterns (ScoreH1from Equation (1) in Section 2), in run Rd;?
from the repository extracted here, with classlabels of an instance ranked via the rank-basedmerging of: ScoreH1+H2 from Section 2, in runRp, which corresponds to re-ranking using co-occurrence of an instance and its class label inthe same query; ScoreH1+H3 from Section 2, inrun Rs, which corresponds to re-ranking using co-occurrence of an instance and its class label, as sep-arate queries within the same query session; andScoreH1+H2+H3 from Section 2, in run Ru, whichcorresponds to re-ranking using both types of co-occurrences in queries.1609Evaluation Procedure: The manual evaluation ofopen-domain information extraction output is timeconsuming (Banko et al, 2007).
A more practi-cal alternative is an automatic evaluation procedurefor ranked lists of class labels, based on existing re-sources and systems.Assume that there is a gold standard, containinggold class labels that are each associated with a goldset of their instances.
The creation of such gold stan-dards is discussed later.
Based on the gold standard,the ranked lists of class labels available within anIsA repository can be automatically evaluated as fol-lows.
First, for each gold label, the ranked lists ofclass labels of individual gold instances are retrievedfrom the IsA repository.
Second, the individual re-trieved lists are merged into a ranked list of classlabels, associated with the gold label.
The mergedlist can be computed, e.g., using an extension of theScoreH1+H2+H3 formula (Equation (4)) describedearlier in Section 2.
Third, the merged list is com-pared against the gold label, to estimate the accu-racy of the merged list.
Intuitively, a ranked list ofclass labels is a better approximation of a gold label,if class labels situated at better ranks in the list arecloser in meaning to the gold label.Evaluation Metric: Given a gold label and a list ofclass labels, if any, derived from the IsA repository,the rank of the highest class label that matches thegold label determines the score assigned to the goldlabel, in the form of the reciprocal rank of the match.Thus, if the gold label matches a class label at rank1, 2 or 3 in the computed list, the gold label receivesa score of 1, 0.5 or 0.33 respectively.
The score is0 if the gold label does not match any of the top 20class labels.
The overall score over the entire set ofgold labels is the mean reciprocal rank (MRR) scoreover all gold labels from the set.
Two types of MRRscores are automatically computed:?
MRRf considers a gold label and a class labelto match, if they are identical;?
MRRp considers a gold label and a class labelto match, if one or more of their tokens that are notstop words are identical.During matching, all string comparisons are case-insensitive, and all tokens are first converted to theirsingular form (e.g., european countries to europeancountry) using WordNet (Fellbaum, 1998).
Thus, in-surance carriers and insurance companies are con-Query Set: Sample of QueriesQe (807 queries): 2009 movies, amino acids, asiancountries, bank, board games, buildings, capitals,chemical functional groups, clothes, computer lan-guage, dairy farms near modesto ca, disease, egyp-tian pharaohs, eu countries, fetishes, french presidents,german islands, hawaiian islands, illegal drugs, ircclients, lakes, macintosh models, mobile operator in-dia, nba players, nobel prize winners, orchids, photoeditors, programming languages, renaissance artists,roller costers, science fiction tv series, slr cameras,soul singers, states of india, taliban members, thomasedison inventions, u.s. presidents, us president, waterslidesQm (40 queries): actors, actresses, airlines, ameri-can presidents, antibiotics, birds, cars, celebrities, col-ors, computer languages, digital camera, dog breeds,dogs, drugs, elements, endangered animals, europeancountries, flowers, fruits, greek gods, horror movies,idioms, ipods, movies, names, netbooks, operatingsystems, park slope restaurants, planets, presidents,ps3 games, religions, renaissance artists, rock bands,romantic movies, states, universities, university, uscities, vitaminsTable 1: Size and composition of evaluation sets ofqueries associated with non-filtered (Qe) or manually-filtered (Qm) instancessidered to not match in MRRf scores, but match inMRRp scores.
On the other hand, MRRp scores maygive credit to less relevant class labels, such as insur-ance policies for the gold label insurance carriers.Therefore, MRRp is an optimistic, and MRRf is apessimistic estimate of the actual usefulness of thecomputed ranked lists of class labels as approxima-tions of the gold labels.4 EvaluationIsA Repository: The IsA repository, extracted fromthe document collection, covers a total of 4.04 mil-lion instances associated with 7.65 million class la-bels.
The number of class labels available per in-stance and vice-versa follows a long-tail distribu-tion, indicating that 2.12 million of the instanceseach have two or more class labels (with an averageof 19.72 class labels per instance).Evaluation Sets of Queries: Table 1 shows sam-ples of two query sets, introduced in (Pas?ca, 2010)and used in the evaluation.
The first set, denoted Qe,1610Query Set Min Max Avg MedianNumber of Gold Instances:Qe 10 100 70.4 81Qm 8 33 16.9 17Number of Query Tokens:Qe 1 8 2.0 2Qm 1 3 1.4 1Table 2: Number of gold instances (upper part) and num-ber of query tokens (lower part) available per query, overthe evaluation sets of queries associated with non-filteredgold instances (Qe) or manually-filtered gold instances(Qm)is obtained from a random sample of anonymized,class-seeking queries submitted by Web users toGoogle Squared.
The set contains 807 queries, eachassociated with a ranked list of between 10 and 100gold instances automatically extracted by GoogleSquared.Since the gold instances available as input foreach query as part of Qe are automatically extracted,they may or may not be true instances of the respec-tive queries.
As described in (Pas?ca, 2010), the sec-ond evaluation set Qm is a subset of 40 queries fromQe, such that the gold instances available for eachquery in Qm are found to be correct after manualinspection.
The 40 queries from Qm are associatedwith between 8 and 33 human-validated instances.As shown in the upper part of Table 2, the queriesfrom Qe are up to 8 tokens in length, with an averageof 2 tokens per query.
Queries from Qm are com-paratively shorter, both in maximum (3 tokens) andaverage (1.4 tokens) length.
The lower part of Ta-ble 2 shows the number of gold instances availableas input, which average around 70 and 17 per query,for queries from Qe and Qm respectively.
To provideanother view on the distribution of the queries fromevaluation sets, Table 3 lists tokens that are not stopwords, which occur in most queries from Qe.
Com-paratively, few query tokens occur in more than onequery in Qm.Evaluation Procedure: Following the general eval-uation procedure, each query from the sets Qe andQm acts as a gold class label associated with thecorresponding set of instances.
Given a query andits instances I from the evaluation sets Qe or Qm,a merged, ranked lists of class labels is computedout of the ranked lists of class labels available in theQuery Cnt.
Examples of Queries Containingthe TokenTokencountries 22 african countries, eu countries,poor countriescities 21 australian cities, cities in califor-nia, greek citiespresidents 18 american presidents, koreanpresidents, presidents of thesouth korearestaurants 15 atlanta restaurants, nova scotiarestaurants, restaurants 10024companies 14 agriculture companies, gas util-ity companies, retail companiesstates 14 american states, states of india,united states national parksprime 11 australian prime ministers, in-dian prime ministers, prime min-isterscameras 10 cameras, digital cameras olym-pus, nikon camerasmovies 10 2009 movies, movies, romanticmoviesamerican 9 american authors, americanpresident, american revolutionbattlesministers 9 australian prime ministers, in-dian prime ministers, prime min-istersTable 3: Query tokens occurring most frequently inqueries from the Qe evaluation set, along with the number(Cnt) and examples of queries containing the tokensunderlying IsA repository for each instance I. Theevaluation compares the merged lists of class labels,with the corresponding queries from Qe or Qm.Accuracy of Lists of Class Labels: Table 4 summa-rizes results from comparative experiments, quanti-fying a) horizontally, the impact of alternative pa-rameter settings on the computed lists of class la-bels; and b) vertically, the comparative accuracy ofthe experimental runs over the query sets.
The ex-perimental parameters are the number of input in-stances from the evaluation sets that are used for re-trieving class labels, I-per-Q, set to 3, 5, 10; and thenumber of class labels retrieved per input instance,C-per-I, set to 5, 10, 20.Four conclusions can be derived from the results.First, the scores over Qm are higher than those overQe, confirming the intuition that the higher-quality1611AccuracyI-per-Q 3 5 10C-per-I 5 10 20 5 10 20 5 10 20MRRf computed over Qe:Rd 0.186 0.195 0.198 0.198 0.207 0.210 0.204 0.214 0.218Rp 0.202 0.211 0.216 0.232 0.238 0.244 0.245 0.255 0.257Rs 0.258 0.260 0.261 0.278 0.277 0.276 0.279 0.280 0.282Ru 0.234 0.241 0.244 0.260 0.263 0.270 0.274 0.275 0.278MRRp computed over Qe:Rd 0.489 0.495 0.495 0.517 0.528 0.529 0.541 0.553 0.557Rp 0.520 0.531 0.533 0.564 0.573 0.578 0.590 0.601 0.602Rs 0.576 0.584 0.583 0.612 0.616 0.614 0.641 0.636 0.628Ru 0.561 0.570 0.571 0.606 0.614 0.617 0.640 0.641 0.636MRRf computed over Qm:Rd 0.406 0.436 0.442 0.431 0.447 0.466 0.467 0.470 0.501Rp 0.423 0.426 0.429 0.436 0.483 0.508 0.500 0.526 0.530Rs 0.590 0.601 0.594 0.578 0.604 0.595 0.624 0.612 0.624Ru 0.481 0.502 0.508 0.531 0.539 0.545 0.572 0.588 0.575MRRp computed over Qm:Rd 0.667 0.662 0.660 0.675 0.677 0.699 0.702 0.695 0.716Rp 0.711 0.703 0.680 0.734 0.731 0.748 0.733 0.797 0.782Rs 0.841 0.822 0.820 0.835 0.828 0.823 0.850 0.856 0.844Ru 0.800 0.810 0.781 0.795 0.794 0.779 0.806 0.827 0.816Table 4: Accuracy of instance set labeling, as full-match (MRRf ) or partial-match (MRRp) scores over the evaluationsets of queries associated with non-filtered instances (Qe) or manually-filtered instances (Qm), for various experi-mental runs (I-per-Q=number of gold instances available in the input evaluation sets that are used for retrieving classlabels; C-per-I=number of class labels retrieved from IsA repository per input instance)input set of instances available in Qm relative toQe should lead to higher-quality class labels forthe corresponding queries.
Second, when I-per-Qis fixed, increasing C-per-I leads to small, if any,score improvements.
Third, when C-per-I is fixed,even small values of I-per-Q, such as 3 (that is, verysmall sets of instances provided as input) producescores that are competitive with those obtained witha higher value like 10.
This suggests that useful classlabels can be generated even in extreme scenarios,where the number of instances available as input isas small as 3 or 5.
Fourth and most importantly, formost combinations of parameter settings and on bothquery sets, the runs that take advantage of query logs(Rp, Rs, Ru) produce the highest scores.
In particu-lar, when I-per-Q is set to 10 and C-per-I to 20, runRu identifies the original query as an exact matchamong the top three to four class labels returned(score 0.278); and as a partial match among the topone to two class labels returned (score 0.636), as anaverage over the Qe set.
The corresponding MRRfscore of 0.278 over the Qe set obtained with run Ruis 27% higher than with run Rd.In all experiments, the higher scores of Rp, Rs andRu can be attributed to higher-quality lists of classlabels, relative to Rd.
Among combinations of pa-rameter settings described in Table 4, values around10 for I-per-Q and 20 for C-per-I give the highestscores over both Qe and Qm.Among the query-based runs Rp, Rs and Ru, thehighest scores in Table 4 are obtained mostly for runRs.
Thus, between the presence of a class label andan instance either in the same query, or as separatequeries within the same query session, it is the lat-ter that provides a more useful signal during the re-ranking of class labels of each instance.Table 5 illustrates the top class labels from theranked lists generated in run Rs for various queriesfrom both Qe and Qm.
The table suggests that thecomputed class labels are relatively resistant to noiseand variation within the input set of gold instances.For example, the top elements of the lists of class la-1612Query Query Gold Instances Top Labels Generated Using Top 10 Gold In-stancesSet Cnt.
Sample from Top Gold In-stancesactors Qe 100 abe vigoda, ben kingsley, billhickmanactors, stars, favorite actors, celebrities, moviestarsQm 28 al pacino, christopherwalken, danny devitoactors, celebrities, favorite actors, movie stars,starscomputerlanguagesQe 59 acm transactions on math-ematical software, apple-script, clanguages, programming languages, programs,standard programming languages, computer pro-gramming languagesQm 17 applescript, eiffel, haskell languages, programming languages, computerlanguages, modern programming languages,high-level languageseuropeancountriesQe 60 abkhazia, armenia, bosnia &herzegovinacountries, european countries, eu countries, for-eign countries, western countriesQm 19 belgium, finland, greece countries, european countries, eu countries, for-eign countries, western countriesendangeredanimalsQe 98 arkive, arabian oryx,bagheeraspecies, animals, endangered species, animalspecies, endangered animalsQm 21 arabian oryx, blue whale, gi-ant hispaniolan galliwaspanimals, endangered species, species, endan-gered animals, rare animalspark sloperestaurantsQe 100 12th street bar & grill, aji barlounge, anthony?sbusinesses, departmentsQm 18 200 fifth restaurant bar, ap-plewood restaurant, beet thairestaurant(none)renaissanceartistsQe 95 michele da verona, andreasansovino, andrea del sartoartists, famous artists, great artists, renaissanceartists, italian artistsQm 11 botticelli, filippo lippi, gior-gioneartists, famous artists, renaissance artists, greatartists, italian artistsrock bands Qe 65 blood doll, nightmare, rock-away beachsongs, hits, films, novels, famous songsQm 15 arcade fire, faith no more, in-digo girlsbands, rock bands, favorite bands, great bands,groupsTable 5: Examples of gold instances available in the input, and actual ranked lists of class labels produced by run Rs forvarious queries from the evaluation sets of queries associated with non-filtered gold instances (Qe) or manually-filteredgold instances (Qm)bels generated for computer languages are relevantand also quite similar for Qe vs. Qm, although thelist of gold instances in Qe may contain incorrectitems (e.g., acm transactions on mathematical soft-ware).
Similarly, the class labels computed for eu-ropean countries are almost the same for Qe vs. Qm,although the overlap of the respective lists of 10 goldinstances used as input is not large.
The table showsat least one query (park slope restaurants) for whichthe output is less than optimal, either because theclass labels (e.g., businesses) are quite distant se-mantically from the query (for Qe), or because nooutput is produced at all, due to no class labels beingfound in the IsA repository for any of the 10 inputgold instances (for Qm).
For many queries, how-ever, the computed class labels arguably capture themeaning of the original query, although not neces-sarily in the exact same lexical form, and sometimesonly partially.
For example, for the query endan-gered animals, only the fourth class label from Qmidentifies the query exactly.
However, class labelspreceding endangered animals already capture thenotion of animals or species (first and third labels),or that they are endangered (second label).16130.0620.1250.2500.5001.0002.0004.0008.00016.00032.00064.0001 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20(not intop20)Percentageof queriesRankQuery evaluation set: QeFull-matchPartial-match0.0620.1250.2500.5001.0002.0004.0008.00016.00032.00064.0001 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20(not intop20)Percentageof queriesRankQuery evaluation set: QmFull-matchPartial-matchFigure 1: Percentage of queries from the evaluation sets,for which the earliest class labels from the computedranked lists of class labels, which match the queries, oc-cur at various ranks in the ranked lists returned by runRsFigure 1 provides a detailed view on the distribu-tion of queries from the Qe and Qm evaluation sets,for which the class label that matches the query oc-curs at a particular rank in the computed list of classlabels.
In the first graph of Figure 1, for Qe, thequery matches the automatically-generated class la-bel at ranks 1, 2, 3, 4 and 5 for 18.9%, 10.3%, 5.7%,3.7% and 1.2% of the queries respectively, with fullstring matching, i.e., corresponding to MRRf ; andfor 52.6%, 12.4%, 5.3%, 3.7% and 1.7% respec-tively, with partial string matching, corresponding toMRRp.
The second graph confirms that higher MRRscores are obtained for Qm than for Qe.
In particu-lar, the query matches the class label at rank 1 and 2for 50.0% and 17.5% (or a combined 67.5%) of thequeries from Qm, with full string matching; and for52.6% and 12.4% (or a combined 67%), with partialstring matching.Discussion: The quality of lists of items extractedfrom documents can benefit from query-driven rank-ing, particularly for the task of ranking class labelsof instances within IsA repositories.
The use ofqueries for ranking is generally applicable: it canbe seen as a post-processing stage that enhances theranking of the class labels extracted for various in-stances by any method into any IsA repository.Open-domain class labels extracted from text andre-ranked as described in this paper are useful in avariety of applications.
Search tools such as GoogleSquared return a set of instances, in response toclass-seeking queries (e.g., insurance companies).The labeling of the returned set of instances, usingthe re-ranked class labels available per instances, al-lows for the generation of query refinements (e.g.,insurers).
In search over semi-structured data (Ca-farella et al, 2008), the labeling of column cells isuseful to infer the semantics of a table column, whenthe subject row of the table in which the column ap-pears is either absent or difficult to detect.5 Related WorkThe role of anonymized query logs in Web-basedinformation extraction has been explored in taskssuch as class attribute extraction (Pas?ca and VanDurme, 2007), instance set expansion (Pennacchiottiand Pantel, 2009) and extraction of sets of similarentities (Jain and Pennacchiotti, 2010).
Our workcompares the usefulness of queries and query ses-sions for ranking class labels in extracted IsA repos-itories.
It shows that query sessions produce better-ranked class labels than isolated queries do.
A taskcomplementary to class label ranking is entity rank-ing (Billerbeck et al, 2010), also referred to as rank-ing for typed search (Demartini et al, 2009).The choice of search queries and query substitu-tions is often influenced by, and indicative of, vari-ous semantic relations holding among full queries orquery terms (Jones et al, 2006).
Semantic relationsmay be loosely defined, e.g., by exploring the ac-quisition of untyped, similarity-based relations fromquery logs (Baeza-Yates and Tiberi, 2007).
In com-parison, queries are used here to re-rank class labelscapturing a well-defined type of open-domain rela-tions, namely IsA relations.6 ConclusionIn an attempt to bridge the gap between informa-tion stated in documents and information requested1614in search queries, this study shows that inherently-noisy queries are useful in re-ranking class labels ex-tracted from Web documents for various instances,with query sessions leading to higher quality thanisolated queries.
Current work investigates the im-pact of ambiguous input instances (Vyas and Pantel,2009) on the quality of the generated class labels.ReferencesR.
Baeza-Yates and A. Tiberi.
2007.
Extracting semanticrelations from query logs.
In Proceedings of the 13thACM Conference on Knowledge Discovery and DataMining (KDD-07), pages 76?85, San Jose, California.M.
Banko, Michael J Cafarella, S. Soderland, M. Broad-head, and O. Etzioni.
2007.
Open information ex-traction from the Web.
In Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence(IJCAI-07), pages 2670?2676, Hyderabad, India.B.
Billerbeck, G. Demartini, C. Firan, T. Iofciu, andR.
Krestel.
2010.
Ranking entities using Web searchquery logs.
In Proceedings of the 14th EuropeanConference on Research and Advanced Technology forDigital Libraries (ECDL-10), pages 273?281, Glas-gow, Scotland.M.
Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.2008.
WebTables: Exploring the power of tables onthe Web.
In Proceedings of the 34th Conference onVery Large Data Bases (VLDB-08), pages 538?549,Auckland, New Zealand.G.
Demartini, T. Iofciu, and A. de Vries.
2009.
Overviewof the INEX 2009 Entity Ranking track.
In INitiativefor the Evaluation of XML Retrieval Workshop, pages254?264, Brisbane, Australia.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theWeb: an experimental study.
Artificial Intelligence,165(1):91?134.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lexi-cal Database and Some of its Applications.
MIT Press.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14th In-ternational Conference on Computational Linguistics(COLING-92), pages 539?545, Nantes, France.A.
Jain and M. Pennacchiotti.
2010.
Open entity ex-traction from Web search query logs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics (COLING-10), pages 510?518,Beijing, China.R.
Jones, B. Rey, O. Madani, and W. Greiner.
2006.
Gen-erating query substitutions.
In Proceedings of the 15hWorld Wide Web Conference (WWW-06), pages 387?396, Edinburgh, Scotland.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
Semanticclass learning from the Web with hyponym patternlinkage graphs.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics (ACL-08), pages 1048?1056, Columbus, Ohio.M.
Pas?ca and B.
Van Durme.
2007.
What you seekis what you get: Extraction of class attributes fromquery logs.
In Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence (IJCAI-07),pages 2832?2837, Hyderabad, India.M.
Pas?ca.
2010.
The role of queries in ranking la-beled instances extracted from text.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (COLING-10), pages 955?962, Bei-jing, China.M.
Pennacchiotti and P. Pantel.
2009.
Entity extrac-tion via ensemble semantics.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-09), pages 238?247,Singapore.R.
Snow, D. Jurafsky, and A. Ng.
2006.
Semantic tax-onomy induction from heterogenous evidence.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics (COLING-ACL-06), pages 801?808, Sydney, Australia.P.
Talukdar, J. Reisinger, M. Pas?ca, D. Ravichandran,R.
Bhagat, and F. Pereira.
2008.
Weakly-supervisedacquisition of labeled class instances using graph ran-dom walks.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing(EMNLP-08), pages 582?590, Honolulu, Hawaii.B.
Van Durme and M. Pas?ca.
2008.
Finding cars, god-desses and enzymes: Parametrizable acquisition of la-beled instances for open-domain information extrac-tion.
In Proceedings of the 23rd National Confer-ence on Artificial Intelligence (AAAI-08), pages 1243?1248, Chicago, Illinois.V.
Vyas and P. Pantel.
2009.
Semi-automatic entity setrefinement.
In Proceedings of the 2009 Conferenceof the North American Association for ComputationalLinguistics (NAACL-HLT-09), pages 290?298, Boul-der, Colorado.1615
