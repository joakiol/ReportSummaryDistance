Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 785?789,Dublin, Ireland, August 23-24, 2014.UoW: NLP Techniques Developed at the University of Wolverhampton forSemantic Similarity and Textual EntailmentRohit Gupta, Hanna B?echara, Ismail El Maarouf and Constantin Or?asanResearch Group in Computational Linguistics,Research Institute of Information and Language Processing,University of Wolverhampton, UK{R.Gupta, Hanna.Bechara, I.El-Maarouf, C.Orasan}@wlv.ac.ukAbstractThis paper presents the system submit-ted by University of Wolverhampton forSemEval-2014 task 1.
We proposed a ma-chine learning approach which is basedon features extracted using Typed Depen-dencies, Paraphrasing, Machine Transla-tion evaluation metrics, Quality Estima-tion metrics and Corpus Pattern Analysis.Our system performed satisfactorily andobtained 0.711 Pearson correlation for thesemantic relatedness task and 78.52% ac-curacy for the textual entailment task.1 IntroductionThe SemEval task 1 (Marelli et al., 2014a) in-volves two subtasks: predicting the degree of re-latedness between two sentences and detecting theentailment relation holding between them.
Thetask uses SICK dataset (Marelli et al., 2014b),consisting of 10000 pairs, each annotated with re-latedness in meaning and entailment relationshipholding between them.
Similarity measures be-tween sentences are required in a wide variety ofNLP applications.
In applications like Informa-tion Retrieval (IR), measuring similarity is a vi-tal step in order to determine the best result fora related query.
Other applications such as Para-phrasing and Translation Memory (TM) rely onsimilarity measures to weight results.
However,computing semantic similarity between sentencesis a complex and difficult task, due to the fact thatthe same meaning can be expressed in a variety ofways.
For this reason it is necessary to have morethan a surface-form comparison.We present a method based on machine learningwhich exploits available NLP technology.
Our ap-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/proach relies on features inspired by deep seman-tics (such as parsing and paraphrasing), machinetranslation quality estimation, machine translationevaluation and Corpus Pattern Analysis (CPA1).We use the same features to measure both se-mantic relatedness and textual entailment.
Our hy-pothesis is that each feature covers a particular as-pect of implicit similarity and entailment informa-tion contained within the pair of sentences.
Train-ing is performed in a regression framework for se-mantic relatedness and in a classification frame-work for textual entailment.The remainder of the paper is structured as fol-lows.
In Section 2, we review the work relatedto our study and the existing NLP technologiesused to measure sentence similarity.
In Sections 3and 4, we describe our approach and the similaritymeasures we used.
In Section 5, we present the re-sults and an analysis of our runs based on the testand training data provided by the SemEval-2014task.
Finally, our work is summed up in Section 6with perspectives for future work we would like toexplore.2 Related WorkThe areas of semantic relatedness and entailmenthave received extensive interest from the researchcommunity in the last decade.
Earlier work inrelatedness (Banerjee and Pedersen, 2003; Li etal., 2006) exploited WordNet in various ways toextract the semantic relatedness.
Banerjee andPedersen (2003) presented a measure using ex-tended gloss overlap.
This measure takes twoWordNet synsets as input and uses the overlapof their WordNet glosses to compute their degreeof semantic relatedness.
Li et al.
(2006) pre-sented a semantic similarity metric based on thesemantic similarity of words in a sentence.
Re-cently, Wang and Cer (2012) presented an ap-1http://pdev.org.uk785proach that uses probabilistic edit-distance to mea-sure semantic similarity.
The approach uses prob-abilistic finite state and pushdown automata tomodel weighted edit-distance where state transi-tions correspond to edit-operations.
In some as-pects, our work is similar to B?ar et al.
(2012),who presented an approach which combines var-ious text similarity measures using a log-linear re-gression model.Entailment has been modelled using various ap-proaches.
The main approaches are based onlogic inferencing (Moldovan et al., 2003), ma-chine learning (Hickl et al., 2006; Castillo, 2010)and tree edit-distance (Kouylekov and Magnini,2005).
Most of the recent approaches employ var-ious syntactic or tree edit models (Heilman andSmith, 2010; Mai et al., 2011; Rios and Gelbukh,2012; Alabbas and Ramsay, 2013).
Recently, Al-abbas and Ramsay (2013) presented a modifiedtree edit distance approach, which extends treeedit distance to the level of subtrees.
The ap-proach extends Zhang-Shasha?s algorithm (Zhangand Shasha, 1989).3 FeaturesOur system uses the same 31 features for both sub-tasks.
This section explains them and the codewhich implements most of them can be found onGitHub2.3.1 Language Technology FeaturesWe used existing language processing tools to ex-tract features.
Stanford CoreNLP3toolkit provideslemma, parts of speech (POS), named entities, de-pendencies relations of words in each sentence.We calculated Jaccard similarity on surfaceform, lemma, dependencies relations, POS andnamed entities to get the feature values.
The Jac-card similarity computes sentence similarity by di-viding the overlap of words on the total number ofwords of both sentences.Sim(s1, s2) =|s1 ?
s2||s1 ?
s2|(1)where in equation (1), Sim(s1, s2) is the Jaccardsimilarity between sets of words s1 and s2.We used the same toolkit to identify corefer-ence relations and determine clusters of corefer-ential entities.
The coreference feature value was2https://github.com/rohitguptacs/wlvsimilarity3http://nlp.stanford.edu/software/corenlp.shtmlcalculated using clusters of coreferential entities.The intuition is that sentences containing corefer-ential entities should have some semantic related-ness.
In order to extract clusters of coreferentialentities, the pair of sentences was treated as a doc-ument.
The coreference feature value using theseclusters was calculated as follows:V aluecoref=CCTC(2)where CC is the number of clusters formed by theparticipation of entities (at least one entity fromeach sentence of the pair) in both sentences andTC is the total number of clusters.We calculated two separate feature values fordependency relations: the first feature concate-nated the words involved in a dependency relationand the second used grammatical relation tags.
Forexample, for the sentence pair ?the kids are play-ing outdoors?
and ?the students are playing out-doors?
the Jaccard similarity is calculated basedon concatenated words ?kids::the, playing::kids,playing::are, ROOT::playing, playing::outdoors?and ?students::the, playing::students, playing::are,ROOT::playing, playing::outdoors?
to get thevalue for the first feature and ?det, nsubj, aux, root,dobj?
and ?det, nsubj, aux, root, dobj?
to get thevalue for the second feature.These language technology features try to cap-ture the token based similarity and grammaticalsimilarity between a pair of sentences.3.2 Paraphrasing FeaturesWe used the PPDB paraphrase database (Ganitke-vitch et al., 2013) to get the paraphrases.
We usedlexical and phrasal paraphrases of ?L?
size.
Foreach sentence of the pair, we created two sets ofbags of n-grams (1 ?
n ?
length of the sentence).We extended each set with paraphrases for each n-gram available from paraphrase database.
We thencalculated the Jaccard similarity (see Section 3.1)between these extended bag of n-grams to get thefeature value.
This feature capture the cases whereone sentence is a paraphrase of the other.3.3 Negation FeatureOur system does not attempt to model similar-ity with negation, but since negation is an impor-tant feature for contradiction in textual entailment,we designed a non-similarity feature.
The systemchecks for the presence of a negation word such as?no?, ?never?
and ?not?
in the pair of sentences and786returns ?1?
(?0?
otherwise) if both or none of thesentences contain any of these words.3.4 Machine Translation Quality EstimationFeaturesSeventeen of the features consist of MachineTranslation Quality Estimation (QE) features,based on the work of (Specia et al., 2009) and usedas a baseline in recent QE tasks (such as (Callison-Burch et al., 2012)).
We extracted these featuresby treating the first set of sentences as the MachineTranslation (MT) ?source?, and the second set ofsentences as the MT ?target?.
In Machine Trans-lation, these features are used to access the qualityof MT ?target?.
The QE features include shallowsurface features such as the number of punctua-tion marks, the average length of words, the num-ber of words.
Furthermore, these features includen-gram frequencies and language model probabil-ities.
A full list of the QE features is provided inthe documentation of the QE system4(Specia etal., 2009).QE features relate to well-formedness and syn-tax, and are not usually used to compute seman-tic relatedness between sentences.
We have usedthem in the hope that the surface features at leastwill show us some structural similarity betweensentences.3.5 Machine Translation Evaluation FeaturesAdditionally, we used BLEU (Papineni et al.,2002), a very popular machine translation evalu-ation metric, as a feature.
BLEU is based on n-gram counts.
It is meant to capture the similaritybetween translated text and references for machinetranslation evaluation.
The BLEU score over sur-face, lemma and POS was calculated to get threefeature values.
In a pair of sentences, one side wastreated as a translation and another as a reference.We applied it at the sentence level to capture thesimilarity between two sentences.3.6 Corpus Pattern Analysis FeaturesCorpus Pattern Analysis (CPA) (Hanks, 2013) isa procedure in corpus linguistics that associatesword meaning with word use by means of seman-tic patterns.
CPA is a new technique for map-ping meaning onto words in text.
It is currentlybeing used to build a ?Pattern Dictionary of En-glish Verbs?(PDEV5).
It is based on the Theory of4https://github.com/lspecia/quest5http://pdev.org.ukNorms and Exploitations (Hanks, 2013).There are two features extracted from PDEV.They both make use of a derived resource calledthe CPA network (Bradbury and El Maarouf,2013).
The CPA network links verbs accordingto similar semantic patterns (e.g.
both ?pour?
and?trickle?
share an intransitive use where the subjectis ?liquid?
).The first feature value compares the main verbsin both sentences.
When both verbs share a pat-tern, the system returns a value of ?1?
(otherwise?0?).
The second feature extends the CPA networkto compute the probability of a PDEV pattern,given a word.
This probability is computed overthe portion of the British National Corpus which ismanually tagged with PDEV patterns.
The prob-ability of a pattern given each word of a sentenceof the dataset is obtained by the product of thoseprobabilities.
The feature value is the (normalised)number of common patterns from the three mostprobable patterns in each sentence.
These featurestry to capture similarity based on semantic pat-terns.4 Predicting Through Machine Learning4.1 Model DescriptionWe used a support vector machine in order to builda regression model to predict semantic relatednessand a classification model to predict textual entail-ment.
For the actual implementation we used Lib-SVM6(Chang and Lin, 2011).We used a regression model for the related-ness task that estimates a continuous score be-tween 1 and 5 for each sentence.
For the entail-ment task, we trained a classification model whichassigns one of three different labels (ENTAIL-MENT, CONTRADICTION, NEUTRAL) to eachsentence pair.
We trained both systems on the4500 sentence training set, augmented with the500 sentence trial data.
The values of C and ?have been optimised through a grid-search whichuses a 5-fold cross-validation method.The RBF kernel proved to be the best for bothtasks.5 Results and AnalysisWe submitted 4 runs of our system (Run-1 to Run-4).
Run-1 was submitted as primary run.
Run-2,Run-3 and Run-4 systems were identical except6http://www.csie.ntu.edu.tw/ cjlin/libsvm/787Run-1 Run-2 Run-3 Run-4C 8 8 2 2?
0.0441 0.0441 0.125 0.125Pearson 0.7111 0.7166 0.6968 0.6975Table 1: Results: Relatedness.for some parameter differences for SVM train-ing and the replacement of the values which wereoutside the boundaries (1-5).
If relatedness val-ues predicted by the system were less than 1 orgreater than 5, these values were replaced by 1and 5 respectively for Run-1, Run-2 and Run-4and 1.5 and 4.5 respectively for Run-3.
Our pri-mary run also used one extra feature for related-ness, which was obtained by considering entail-ment judgement as a feature.
Our hypothesis wasthat entailment judgement may help in measur-ing relatedness.
In the actual test this feature wasnot helpful and we obtained Pearson correlation of0.711 for the primary run, compared to 0.716 forRun-2.
The details of runs are given in Table 1 and2.After training both models, we ran a featureselection algorithm to determine which featuresyielded the highest accuracy, and therefore had thehighest impact on our system.
Perhaps unsurpris-ingly, the QE features were not very useful in pre-dicting semantic similarity or entailment.
How-ever, despite their focus on fluency rather than se-mantic correctness, the QE features still managedto contribute to some improvements in the textualentailment task (increasing accuracy by 1%), andthe semantic relatedness task (0.027 increase inPearson correlation).In the entailment (classification) task, thestrongest feature proved to be the negation fea-ture with 70% accuracy (on the training set) whentraining on this feature only.
This suggests thatsome measure of negation is crucial in determin-ing whether a sentence contradicts or entails an-other sentence.
Other strong features were lemma,paraphrasing and dependencies.In the relatedness (regression) task, the lemma,surface, paraphrasing, dependencies, PDEV fea-tures were the strongest contributors to accuracy.Run-1 Run-2 Run-3 Run-4C 16 16 8 8?
0.0625 0.0625 0.5 0.5Accuracy 78.526 78.526 78.343 78.343Table 2: Results: Entailment.6 Conclusion and Future WorkWe have presented an efficient approach to calcu-late semantic relatedness and textual entailment.One noticeable point of our approach is that wehave used the same features for both tasks andour system performed well in each of these tasks.Therefore, our system captures reasonably goodmodels to compute semantic relatedness and tex-tual entailment.In the future we would like to explore more fea-tures and particularly those based on tree edit dis-tance, WordNet and PDEV.
Our intuition suggeststhat tree edit distance seems to be more helpful forentailment, whereas WordNet and PDEV seem tobe more helpful for similarity measurement.
Ad-ditionally, we would like to combine our tech-niques for measuring relatedness and entailmentwith MT evaluation techniques.
We would fur-ther like to apply these techniques cross-lingually,moving into other areas like machine translationevaluation and quality estimation.AcknowledgementThe research leading to these results has receivedfunding from the People Programme (Marie CurieActions) of the European Union?s Seventh Frame-work Programme FP7/2007-2013/ under REAgrant agreement no.
317471 and partly supportedby an AHRC grant ?Disambiguating Verbs by Col-location project, AH/J005940/1, 2012-2015?.ReferencesMaytham Alabbas and Allan Ramsay.
2013.
Naturallanguage inference for Arabic using extended treeedit distance with subtrees.
Journal of Artificial In-telligence Research, 48:1?22.Satanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In IJCAI, volume 3, pages 805?810.Daniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In First Joint Conference on788Lexical and Computational Semantics, Associationfor Computational Linguistics, pages 435?440.Jane Bradbury and Isma?
?l El Maarouf.
2013.
Anempirical classification of verbs based on SemanticTypes: the case of the ?poison?
verbs.
In Proceed-ings of the Joint Symposium on Semantic Process-ing.
Textual Inference and Structures in Corpora,pages 70?74.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia, editors.2012.
Proceedings of the Seventh Workshop on Sta-tistical Machine Translation.
Association for Com-putational Linguistics, Montr?eal, Canada, June.Julio J. Castillo.
2010.
Recognizing textual en-tailment: experiments with machine learning al-gorithms and RTE corpora.
Special issue: Natu-ral Language Processings and its Applications, Re-search in Computing Science, 46:155?164.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.Juri Ganitkevitch, Van Durme Benjamin, and ChrisCallison-Burch.
2013.
Ppdb: The paraphrasedatabase.
In Proceedings of NAACL-HLT, pages758?764, Atlanta, Georgia.Patrick Hanks.
2013.
Lexical Analysis: Norms andExploitations.
Mit Press.Michael Heilman and Noah A. Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In The 2010 An-nual Conference of the North American Chapter ofthe ACL, number June, pages 1011?1019.Andrew Hickl, Jeremy Bensley, John Williams, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Rec-ognizing textual entailment with LCC?s GROUND-HOG system.
In Proceedings of the Second PAS-CAL Challenges Workshop.Milen Kouylekov and Bernardo Magnini.
2005.
Rec-ognizing textual entailment with tree edit distancealgorithms.
In Proceedings of the First ChallengeWorkshop Recognising Textual Entailment, pages17?20.Yuhua Li, David McLean, Zuhair A Bandar, James DO?shea, and Keeley Crockett.
2006.
Sentence sim-ilarity based on semantic nets and corpus statistics.Knowledge and Data Engineering, IEEE Transac-tions on, 18(8):1138?1150.Zhewei Mai, Y Zhang, and Donghong Ji.
2011.
Rec-ognizing text entailment via syntactic tree match-ing.
In Proceedings of NTCIR-9 Workshop Meeting,pages 361?364, Tokyo, Japan.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zam-parelli.
2014a.
Semeval-2014 task 1: Evaluation ofcompositional distributional semantic models on fullsentences through semantic relatedness and textualentailment.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014).Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zam-parelli.
2014b.
A sick cure for the evaluation ofcompositional distributional semantic models.
InProceedings of LREC 2014.Dan Moldovan, Christine Clark, Sanda Harabagiu, andSteve Maiorano.
2003.
COGEX : A Logic Proverfor Question Answering.
In Proceedings of HLT-NAACL, number June, pages 87?93.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of theACL, pages 311?318.Miguel Rios and Alexander Gelbukh.
2012.
Recog-nizing Textual Entailment with a Semantic Edit Dis-tance Metric.
In 11th Mexican International Confer-ence on Artificial Intelligence, pages 15?20.
IEEE.Lucia Specia, Marco Turchi, Nicola Cancedda, MarcDymetman, and Nello Cristianini.
2009.
Estimat-ing the sentence-level quality of machine translationsystems.
In 13th Conference of the European Asso-ciation for Machine Translation, pages 28?37.Mengqiu Wang and Daniel Cer.
2012.
Stanford: prob-abilistic edit distance metrics for STS.
In Proceed-ings of the First Joint Conference on Lexical andComputational Semantics, pages 648?654.Kaizhong Zhang and Dennis Shasha.
1989.
SimpleFast Algorithms for the Editing Distance betweenTrees and Related Problems.
SIAM Journal on Com-puting, 18(6):1245?1262.789
