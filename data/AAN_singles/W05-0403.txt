Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 17?23,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsTemporal Feature Modification for Retrospective CategorizationRobert Liebscher and Richard K. BelewDepartment of Cognitive ScienceUniversity of California, San Diego{rliebsch|rik}@cogsci.ucsd.eduAbstractWe show that the intelligent use of one smallpiece of contextual information?a document?spublication date?can improve the performanceof classifiers trained on a text categorizationtask.
We focus on academic research docu-ments, where the date of publication undoubt-edly has an effect on an author?s choice ofwords.
To exploit this contextual feature, wepropose the technique of temporal feature mod-ification, which takes various sources of lexi-cal change into account, including changes interm frequency, associative strength betweenterms and categories, and dynamic categoriza-tion systems.
We present results of classifi-cation experiments using both full text papersand abstracts of conference proceedings, show-ing improved classification accuracy across thewhole collection, with performance increasesof greater than 40% when temporal featuresare exploited.
The technique is fast, classifier-independent, and works well even when mak-ing only a few modifications.1 IntroductionAs they are normally conceived, many tasks relevant toComputational Linguistics (CL), such as text categoriza-tion, clustering, and information retrieval, ignore the con-text in which a document was written, focusing instead onthe lexical content of the document.
Numerous improve-ments have been made in such tasks when context is con-sidered, for example the hyperlink or citation structure ofa document collection (Cohn and Hofmann, 2001; Getooret al, 2001).
In this paper, we aim to show that the intel-ligent use of another dimension of context?a document?spublication date?can improve the performance of classi-fiers trained on a text categorization task.Traditional publications, such as academic papers andpatents, have histories that span centuries.
The WorldWide Web is no longer a new frontier; over a decade of itscontents have been archived (Kahle, 2005); Usenet andother electronic discussion boards have been around forseveral decades.
These forums continue to increase theirpublication rates and show no signs of slowing.
A cur-sory glance at any one of them at two different points intime can reveal widely varying content.For a concrete example, we can ask, ?What is Compu-tational Linguistics about??
Some topics, such as ma-chine translation, lie at the heart of the discipline andwill always be of interest.
Others are ephemeral or havereached theoretical upper bounds on performance.
It isthus more appropriate to ask what CL is about at somepoint in time.
Consider Table 1, which lists the top fiveunigrams that best distinguished the field at different six-year periods, as derived from the odds ratio measure (seeSection 3.2) over the full text of the ACL proceedings.1979-84 1985-90 1991-96 1997-02system phrase discourse wordnatural plan tree corpuslanguage structure algorithm trainingknowledge logical unification modeldatabase interpret plan dataTable 1: ACL?s most characteristic terms for four timeperiods.While these changes are interesting in their own rightfor an historical linguist, we aim to show that they canalso be exploited for practical purposes.
We focus on afairly homogeneous set of academic research documents,where the time of publication undoubtedly has an effectboth on an author?s choice of words and on a field?s defi-nition of underlying topical categories.
A document mustsay something novel while building upon what has al-ready been said.
This dynamic generates a landscapeof changing research language, where authors and dis-ciplines constantly influence and alter the course of oneanother.171.1 MotivationsText Categorization (TC) systems are typically used toclassify a stream of documents soon after they are pro-duced, based upon a set of historical training data.
It iscommon for some TC applications, such as topic tracking(see Section 5.2), to downweight older features, or thefeature vectors of entire documents, while placing moreemphasis on features that have recently shown increasedimportance through changes in frequency and discrimi-native ability.Our task, which we call retrospective categorization,uses historical data in both the training and test sets.
It isretrospective from the viewpoint of a current user brows-ing through previous writings that are categorized withrespect to a ?modern?
interpretation.
Our approach is mo-tivated by three observations concerning lexical changeover time, and our task is to modify features so that a textclassifier can account for all three.First, lexical changes can take place within a category.The text collections used in our experiments are from var-ious conference proceedings of the Association of Com-puting Machinery, which uses a hierarchical classifica-tion system consisting of over 500 labels (see Section2).
As was suggested by the example of Table 1, even ifclassification labels remain constant over time, the termsthat best characterize them can change to reflect evolv-ing ?meanings?.
We can expect that many of the termsmost closely associated with a category like Computa-tional Linguistics cannot be captured properly withoutexplicitly addressing their temporal context.Second, lexical changes can occur between categories.A term that is significant to one category can suddenlyor gradually become of interest to another category.
Thisis especially applicable in news corpora (see examplesin Section 3), but also applies to academic research doc-uments.
Terminological ?migrations?
between topics incomputer science, and across all of science, are common.Third, any coherent document collection on a par-ticular topic is sufficiently dynamic that, over time, itscategorization system must be updated to reflect thechanges in the world on which its texts are based.
Al-though Computational Linguistics predates Artificial In-telligence (Kay, 2003), many now consider the former asubset of the latter.
Within CL, technological and theo-retical developments have continually altered the labelsascribed to particular works.In the ACM?s hierarchical Computing ClassificationSystem (see Section 2.1), several types of transforma-tions are seen in the updates it received in 1983, 1987,1991, and 1998.1 In bifurcations, categories can be splitapart.
With collapses, categories that were formerly morefine-grained, but now do not receive much attention, can1http://acm.org/class/be combined.
Finally, entirely new categories can be in-serted into the hierarchy.2 DataTo make our experiments tractable and easily repeatablefor different parameter combinations, we chose to trainand test on two subsets of the ACM corpus.
One subsetconsists of collections of abstracts from several differentACM conferences.
The other includes the full text col-lection of documents from one conference.2.1 The ACM hierarchyAll classifications were performed with respect to theACM?s Computing Classification System, 1998 version.This, the most recent version of the ACM-CCS, is a hi-erarchic classification scheme that potentially presents awide range of hierarchic classification issues.
Becausethe work reported here is focused on temporal aspects oftext classification, we have adopted a strategy that effec-tively ?flattens?
the hierarchy.
We interpret a documentwhich has a primary2 category at a narrow, low level inthe hierarchy (e.g., H.3.3.CLUSTERING) as also classi-fied at all broader, higher-level categories leading to theroot (H, H.3, H.3.3).
With this construction, the mostrefined categories will have fewer example documents,while broader categories will have more.For each of the corpora considered, a threshold of 50documents was set to guarantee a sufficient number of in-stances to train a classifier.
Narrower branches of the fullACM-CCS tree were truncated if they contained insuf-ficient numbers of examples, and these documents wereassociated with their parent nodes.
For example, if H.3.3contained 20 documents and H.3.4 contained 30, thesewould be ?collapsed?
into the H.3 category.All of our corpora carry publication timestamp infor-mation involving time scales on the order of one to threedecades.
The field of computer science, not surprisingly,has been especially fortunate in that most of its pub-lications have been recorded electronically.
While ob-viously skewed relative to scientific and academic pub-lishing more generally, we nevertheless find significant?micro-cultural?
variation among the different special in-terest groups.2.2 SIGIR full textWe have processed the annual proceedings of the Associ-ation for Computing Machinery?s Special Interest Groupin Information Retrieval (SIGIR) conference from its in-ception in 1978 to 2002.
The collection contains over1,000 documents, most of which are 6-10 page papers,though some are keynote addresses and 2-3 page poster2Many ACM documents also are classified with additional?other?
categories, but these were not used.18Corpus Vocab size No.
docs No.
catsSIGIR 16104 520 17SIGCHI 4524 1910 20SIGPLAN 6744 3123 22DAC 6311 2707 20Table 2: Corpus featuresUnlabeled ExpectedProceedings 18.97% 7.73%Periodicals 19.08% 11.54%No.
docs 24,567 8,703Table 3: Missing classification labels in ACMsummaries.
Every document is tagged with its year ofpublication.
Unfortunately, only about half of the SIGIRdocuments bear category labels.
The majority of theseomissions fall within the 1978-1987 range, leaving us theremaining 15 years to work with.2.3 Conference abstractsWe collected nearly 8,000 abstracts from the Special In-terest Group in Programming Languages (SIGPLAN),the Special Interest Group in Computer-Human Interac-tion (SIGCHI) and the Design Automation Conference(DAC).
Characteristics of these collections, and of the SI-GIR texts, are shown in Table 2.2.4 Missing labels in ACMWe derive the statistics below from the corpus of all doc-uments published by the ACM between 1960 and 2003.The arguments can be applied to any corpus which hascategorized documents, but for which there are classifi-cation gaps in the record.The first column of Table 3 shows that nearly one fifthof all ACM documents, from both conference proceed-ings and periodicals, do not possess category labels.
Wedefine a document?s label as ?expected?
when more thanhalf of the other documents in its publication (one confer-ence proceeding or one issue of a periodical) are labeled,and if there are more than ten total.
The second columnlists the percentage of documents where we expected alabel but did not find one.3 MethodsText categorization (TC) is the problem of assigning doc-uments to one or more pre-defined categories.
As Section1 demonstrated, the terms which best characterize a cate-gory can change through time, so it is not unreasonable toassume that intelligent use of temporal context will proveuseful in TC.Imagine the example of sorting several decades ofarticles from the Los Angeles Times into the cate-gories ENTERTAINMENT, BUSINESS, SPORTS, POL-ITICS, and WEATHER.
Suppose we come across theterm schwarzenegger in a training document.
In the1970s, during his career as a professional bodybuilder,Arnold Schwarzenegger?s name would be a strong indica-tor of a SPORTS document.
During his film career in the1980s-1990s, his name would be most likely to appear inan ENTERTAINMENT document.
After 2003, at the out-set of his term as California?s governor, the POLITICSand BUSINESS categories would be the most likely can-didates.
We refer to schwarzenegger as a temporallyperturbed term, because its distribution across categoriesvaries greatly with time.Documents containing temporally perturbed termshold valuable information, but this is lost in a statisticalanalysis based purely on the average distribution of termsacross categories, irrespective of temporal context.
Thisinformation can be recovered with a technique we calltemporal feature modification (TFM).
We first outline aformal model for its use.3.1 A term generator frameworkOne obvious way to introduce temporal information intothe categorization task is to simply provide the year ofpublication as a new lexical feature.
Preliminary exper-iments (not reported here) showed that this method hadvirtually no effect on classification performance.
Whenthe date features were ?emphasized?
with higher frequen-cies, classification performance declined.Instead, we proceed from the perspective of a simpli-fied language generator model (e.g.
(Blei et al, 2003)).We imagine that the first step in the production of a doc-ument involves an author choosing a category C. Eachterm k (word, bigram, phrase, etc.)
is accorded a uniquegenerator G that determines the distribution of k acrosscategories, and therefore its likelihood to appear in cat-egory C. The model assumes that all authors share thesame generator for each term, and that the generators donot change over time.
We are particularly interested inidentifying temporally perturbed lexical generators thatviolate this assumption.External events at time t can perturb the generator of k,causing Pr(C|k  ) to be different relative to the backgroundPr(C|k) computed over the entire corpus.
If the perturba-tion is significant, we want to separate the instances of kat time t from all other instances.Returning to our earlier example, we would treata generic, atemporal occurrence of schwarzeneg-ger and the pseudo-term ?schwarzenegger+2003?as though they were actually different terms, because theywere produced by two different generators.
We hypoth-esize that separating the analysis of the two can improve19our estimates of the true Pr(C|k), both in 2003 and inother years.3.2 TFM ProcedureThe generator model motivates a procedure we outlinebelow for flagging certain lexemes with explicit temporalinformation that distinguish them so as to contrast themwith those generated by the underlying atemporal alter-natives.
This procedure makes use of the (log) odds ratiofor feature selection:	ffflfiffi	 !#"	$&%('*)",+) " $-%'.! "
+where p is Pr(k|C), the probability that term k ispresent, given category C, and q is Pr(k|!C).The odds ratio between a term and a category is a mea-sure of the associated strength of the two, for it measuresthe likelihood that a term will occur frequently within acategory and (relatively) infrequently outside.
Odds ratiohappens to perform very well in feature selection tests;see (Mladenic, 1998) for details on its use and variations.Ultimately, it is an arbitrary choice and could be replacedby any method that measures term-category strength.The following pseudocode describes the process oftemporal feature modification:VOCABULARY ADDITIONS:for each class C:for each time (year) t:PreModList(C,t,L) = OddsRatio(C,t,L)ModifyList(t) =DecisionRule(PreModList(C,t,L)for each term k in ModifyList(t):Add pseudo-term "k+t" to VocabDOCUMENT MODIFICATIONS:for each document:t = time (year) of docfor each term k:if "k+t" in Vocab:Replace k with "k+t"Classify modified documentPreModList(C,t,L) is a list of the top L terms that, bythe odds ratio measure, are highly associated with cate-gory C at time t. (In our case, time is divided annually,because this is the finest resolution we have for manyof the documents in our corpus.)
We test the hypothe-sis that these come from a perturbed generator at time t,as opposed to the atemporal generator G , by comparingthe odds ratios of term-category pairs in a PreModList attime t with the same pairs across the entire corpus.
Termswhich pass this test are added to the final ModifyList(t)for time t. For the results that we report, DecisionRule isa simple ratio test with threshold factor f. Suppose f is2.0: if the odds ratio between C and k is twice as great attime t as it is atemporally, the decision rule is ?passed?.0 2 4 6 8 10 12 14 16 18 20 22?5051015202530354045Percent terms modifiedPercent accuracyimprovementSIGPLANSIGCHIDACAtemporal baselineFigure 1: Improvement in categorization performancewith TFM, using the best parameter combinations foreach corpus.The generator G is then considered perturbed at time tand k is added to ModifyList(t).
In the training and test-ing phases, the documents are modified so that a term k isreplaced with the pseudo-term "k+t" if it passed the ratiotest.3.3 Text categorization detailsThe TC parameters held constant in our experimentsare: Stoplist, Porter stemming, and Laplacian smoothing.Other parameters were varied: four different classifiers,three unique minimum vocabulary frequencies, unigramsand bigrams, and four threshold factors f. 10-fold crossvalidation was used for parameter selection, and 10% ofthe corpus was held out for testing purposes.
Both ofthese sets were distributed evenly across time.4 ResultsTable 4 shows the parameter combinations, chosen byten-fold cross-validation, that exhibited the greatest in-crease in categorization performance for each corpus.Using these parameters, Figure 1 shows the improve-ment in accuracy for different percentages of terms mod-ified on the test sets.
The average accuracies (acrossall parameter combinations) when no terms are modifiedare less than stellar, ranging from 26.70% (SIGCHI) to37.50% (SIGPLAN), due to the difficulty of the task (20-22 similar categories; each document can only belong toone).
Our aim here, however, is simply to show improve-ment.
A baseline of 0.0 in the plot indicates accuracywithout any temporal modifications.Figure 2 shows the accuracy on an absolute scale whenTFM is applied to the full text SIGIR corpus.
Perfor-mance increased from the atemporal baseline of 28.85%20Corpus Improvement Classifier n-gram size Vocab frequency min.
Ratio threshold fSIGIR 33.32% Naive Bayes Bigram 2 2.0SIGCHI 40.82% TF.IDF Bigram 10 1.0SIGPLAN 18.74% KNN Unigram 10 1.5DAC 20.69% KNN Unigram 2 1.0Table 4: Top parameter combinations for TFM by improvement in classification accuracy.
Vocab frequency min.
isthe minimum number of times a term must appear in the corpus in order to be included.0 0.1 0.2 0.3 0.4 0.5 0.6 0.728303234363840Percent terms modifiedPercent accuracyFigure 2: Absolute categorization performance withTFM for the SIGIR full text corpus.correct to a maximum of 38.46% when only 1.11% of theterms were modified.
The ModifyLists for each categoryand year averaged slightly fewer than two terms each.In most cases, the technique performs best when mak-ing relatively few modifications: the left sides of eachfigure show a rapid performance increase, followed bya gradual decline as more terms are modified.
After re-quiring the one-time computation of odds ratios in thetraining set for each category/year, TFM is very fast andrequires negligible extra storage space.
This is importantwhen computing time is at a premium and enormous cor-pora such as the ACM full text collection are used.
It isalso useful for quickly testing potential enhancements tothe process, some of which are discussed in Section 6.The results indicate that L in PreModList(C,t,L) neednot exceed single digits, and that performance asymp-totes as the number of terms modified increases.
As thishappens, more infrequent terms are judged to have beenproduced by perturbed generators, thus making their truedistributions difficult to compute (for the years in whichthey are not modified) due to an insufficient number ofexamples.4.1 General description of resultsA quantitative average of all results, using all parametercombinations, is not very meaningful, so we provide aqualitative description of the results not shown in Table 4and Figures 1 and 2.
Of the 96 different parameter combi-nations tested on four different corpora, 83.33% resultedin overall increases in performance.
The greatest increasepeaked at 40.82% improvement over baseline (atempo-ral) accuracy, while the greatest decrease dropped perfor-mance by only 8.31%.5 Related WorkThe use of metadata and other complementary (non-content) information to improve text categorization is aninteresting and well-known problem.
The specific use oftemporal information, even if only implicitly, for tasksclosely related to TC has been explored through adaptiveinformation filtering (AIF) and topic detection and track-ing (TDT).5.1 Adaptive Information FilteringThere exists a large body of work on information filter-ing, which ?is concerned with the problem of deliveringuseful information to a user while preventing an overloadof irrelevant information?
(Lam et al, 1996).
Of partic-ular interest here is adaptive information filtering (AIF),which handles the problems of concept drift (a gradualchange in the data set a classifier must learn from) andconcept shift (a more radical change).Klinkenberg and Renz test eight different classifiers ontheir abilities to adapt to changing user preferences fornews documents (Klinkenberg and Renz, 1998).
They trydifferent ?data management techniques?
for the conceptdrift scenario, selectively altering the size of the set ofexamples (the adaptive window) that a classifier trains onusing a heuristic that accounts for the degree of dissimi-larity between the current batch of examples and previousbatches.
Klinkenberg and Joachims later abandon this ap-proach because it relies on ?complicated heuristics?, andinstead concentrate their analysis on support vector ma-chines (Klinkenberg and Joachims, 2000).Stanley uses an innovative approach that eschews theneed for an adaptive window of training examples, and21instead relies on a voting system for decision trees (Stan-ley, 2001).
The weight of each classifier?s vote (classifi-cation) is proportional to its record in predicting classi-fications for previous examples.
He notes that this tech-nique does not rely on decision trees; rather, any combi-nation of classifiers can be inserted into the system.The concept drift and shift scenarios used in the pub-lished literature are often unrealistic and not based uponactual user data.
Topic Detection and Tracking, describedin the following section, must work not with the behaviorof one individual, but with texts that report on real exter-nal events and are not subject to artificial manipulation.This multifaceted, unsupervised character of TDT makesit a more appropriate precursor with which to compareour work.5.2 Topic Detection and TrackingFranz et al note that Topic Detection and Tracking(TDT) is fundamentally different from AIF in that the?adaptive filtering task focuses on performance improve-ments driven by feedback from real-time human rele-vance assessments.
TDT systems, on the other hand, aredesigned to run autonomously without human feedback?
(Franz et al, 2001).
Having roots in information retrieval,text categorization, and information filtering, the initialTDT studies used broadcast news transcripts and writ-ten news corpora to accomplish tasks ranging from newsstory clustering to boundary segmentation.
Of most rel-evance to the present work is the topic tracking task.
Inthis task, given a small number (1-4) of training storiesknown to be about a particular event, the system mustmake a binary decision about whether each story in anincoming stream is about that event.Many TDT systems make use of temporal information,at least implicitly.
Some employ a least recently used(Chen and Ku, 2002) or decay (Allan et al, 2002) func-tion to restrict the lexicon available to the system at anygiven point in time to those terms most likely to be of usein the topic tracking task.There are many projects with a foundation in TDT thatgo beyond the initial tasks and corpora.
For example,TDT-inspired language modeling techniques have beenused to train a system to make intelligent stock tradesbased upon temporal analysis of financial texts (Lavrenkoet al, 2000).
Retrospective timeline generation has alsobecome popular, as exhibited by Google?s Zeitgeist fea-ture and browsers of TDT news corpora (Swan and Allan,2000; Swan and Jensen, 2000).The first five years of TDT research are nicely summa-rized by Allan (Allan, 2002).6 Summary and Future WorkIn this paper, we have demonstrated a feature modifi-cation technique that accounts for three kinds of lexi-cal changes in a set of documents with category labels.Within a category, the distribution of terms can changeto reflect the changing nature of the category.
Terms canalso ?migrate?
between categories.
Finally, the catego-rization system itself can change, leading to necessarylexical changes in the categories that do not find them-selves with altered labels.
Temporal feature modification(TFM) accounts for these changes and improves perfor-mance on the retrospective categorization task as it is ap-plied to subsets of the Association for Computing Ma-chinery?s document collection.While the results presented in this paper indicate thatTFM can improve classification accuracy, we would liketo demonstrate that its mechanism truly incorporateschanges in the lexical content of categories, such as thoseoutlined in Section 1.1.
A simple baseline comparisonwould pit TFM against a procedure in which the corpusis divided into slices temporally, and a classifier is trainedand tested on each slice individually.
Due to changes incommunity interest in certain topics, and in the structureof the hierarchy, some categories are heavily representedin certain (temporal) parts of the corpus and virtually ab-sent elsewhere.
Thus, the chance of finding every cat-egory represented in a single year is very low.
For ourcorpora, this did not even occur once.The ?bare bones?
version of TFM presented here is in-tended as a proof-of-concept.
Many of the parametersand procedures can be set arbitrarily.
For initial featureselection, we used odds ratio because it exhibits goodperformance in TC (Mladenic, 1998), but it could be re-placed by another method such as information gain, mu-tual information, or simple term/category probabilities.The ratio test is not a very sophisticated way to choosewhich terms should be modified, and presently only de-tects the surges in the use of a term, while ignoring the(admittedly rare) declines.In experiments on a Usenet corpus (not reported here)that was more balanced in terms of documents per cate-gory and per year, we found that allowing different termsto ?compete?
for modification was more effective thanthe egalitarian practice of choosing L terms from eachcategory/year.
There is no reason to believe that eachcategory/year is equally likely to contribute temporallyperturbed terms.We would also like to exploit temporal contiguity.
Thepresent implementation treats time slices as independententities, which precludes the possibility of discoveringtemporal trends in the data.
One way to incorporatetrends implicitly is to run a smoothing filter across thetemporally aligned frequencies.
Also, we treat each sliceat annual resolution.
Initial tests show that aggregat-ing two or more years into one slice improves perfor-mance for some corpora, particularly those with tempo-rally sparse data such as DAC.22AcknowledgementsMany thanks to the anonymous reviewers for their helpfulcomments and suggestions.ReferencesJ.
Allan, V. Lavrenko, and R. Swan.
2002.
Explorationswithin topic tracking and detection.
In J. Allan, editor,Topic Detection and Tracking: Event-based Informa-tion Organization, pages 197?224.
Kluwer AcademicPublishers.J.
Allan.
2002.
Introduction to topic detection and track-ing.
In J. Allan, editor, Topic Detection and Track-ing: Event-based Information Organization, pages 1?16.
Kluwer Academic Publishers.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
The Journal of Ma-chine Learning Research, 3:993?1002.H.
Chen and L. Ku.
2002.
An nlp & ir approach totopic detection.
In J. Allan, editor, Topic Detectionand Tracking: Event-based information organization,pages 243?264.
Kluwer Academic Publishers.D.
Cohn and T. Hofmann.
2001.
The missing link: aprobabilistic model of document content and hyperlinkconnectivity.
In Advances in Neural Information Pro-cessing Systems, pages 430?436.
MIT Press.M.
Franz, T. Ward, J.S.
McCarley, and W. Zhu.
2001.Unsupervised and supervised clustering for topictracking.
In Proceedings of the Special Interest Groupin Information Retrieval, pages 310?317.L.
Getoor, E. Segal, B. Taskar, and D. Koller.
2001.Probabilistic models of text and link structure for hy-pertext classification (2001).
In Proceedings of the2001 IJCAI Workshop on Text Learning: Beyond Su-pervision.Brewster Kahle.
2005.
The internet archive.http://www.archive.org/.Martin Kay.
2003.
Introduction.
In Ruslan Mitkov, ed-itor, The Oxford Handbook of Computational Linguis-tics, pages xvii?xx.
Oxford University Press.R.
Klinkenberg and T. Joachims.
2000.
Detecting con-cept drift with support vector machines.
In Proceed-ings of the Seventeenth International Conference onMachine Learning (ICML), page 11.
Morgan Kauf-mann.R.
Klinkenberg and I. Renz.
1998.
Adaptive informationfiltering: Learning in the presence of concept drifts.
InAAAI/ICML workshop on learning for text categoriza-tion.W.
Lam, S. Mukhopadhyay, J. Mostafa, and M. Palakal.1996.
Detection of shifts in user interests for per-sonalized information filtering.
In Proceedings of theSpecial Interest Group in Information Retrieval, pages317?326.V.
Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie,D.
Jensen, and J. Allan.
2000.
Mining of concurrenttext and time series.
In 6th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and DataMining, Text Mining Workshop, pages 37?44.D.
Mladenic.
1998.
Machine Learning on non-homogeneous, distributed text data.
Ph.D. thesis, Uni-versity of Ljubljana, Slovenia.K.O.
Stanley.
2001.
Learning concept drift with a com-mittee of decision trees.
Computer Science Depart-ment, University of Texas-Austin.R.
Swan and J. Allan.
2000.
Automatic generation ofoverview timelines.
In Proceedings of the Special In-terest Group in Information Retrieval, pages 47?55.R.
Swan and D. Jensen.
2000.
Timemines: Construct-ing timelines with statistical models of word usage.In ACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining.23
