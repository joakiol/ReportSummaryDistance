Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 628?637,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsMultilayer Sequence LabelingAi Azuma Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and TechnologyIkoma, Nara 630-0192, Japan{ai-a,matsu}@is.naist.jpAbstractIn this paper, we describe a novel approach tocascaded learning and inference on sequences.We propose a weakly joint learning modelon cascaded inference on sequences, calledmultilayer sequence labeling.
In this model,inference on sequences is modeled as cas-caded decision.
However, the decision on asequence labeling sequel to other decisionsutilizes the features on the preceding resultsas marginalized by the probabilistic modelson them.
It is not novel itself, but our ideacentral to this paper is that the probabilis-tic models on succeeding labeling are viewedas indirectly depending on the probabilisticmodels on preceding analyses.
We also pro-pose two types of efficient dynamic program-ming which are required in the gradient-basedoptimization of an objective function.
Oneof the dynamic programming algorithms re-sembles back propagation algorithm for mul-tilayer feed-forward neural networks.
Theother is a generalized version of the forward-backward algorithm.
We also report experi-ments of cascaded part-of-speech tagging andchunking of English sentences and show ef-fectiveness of the proposed method.1 IntroductionMachine learning approach is widely used to clas-sify instances into discrete categories.
In manytasks, however, some set of inter-related labelsshould be decided simultaneously.
Such tasks arecalled structured prediction.
Sequence labeling isthe simplest subclass of structured prediction prob-lems.
In sequence labeling, the most likely oneamong all the possible label sequences is predictedfor a given input.
Although sequence labeling isthe simplest subclass, a lot of real-world tasks aremodeled as problems of this simplest subclass.
Inaddition, it might offer valuable insight and a toe-hold for more general and complex structured pre-diction problems.
Many models have been proposedfor sequence labeling tasks, such as Hidden MarkovModels (HMM), Conditional Random Fields (CRF)(Lafferty et al, 2001), Max-Margin Markov Net-works (Taskar et al, 2003) and others.
These modelshave been applied to lots of practical tasks in naturallanguage processing (NLP), bioinformatics, speechrecognition, and so on.
And they have shown greatsuccess in recent years.In real-world tasks, it is often needed to cascademultiple predictions.
A cascade of predictions heremeans the situation in which some of predictions aremade based upon the results of other predictions.Sequence labeling is not an exception.
For exam-ple, in NLP, we perform named entity recognition orbase-phrase chunking for given sentences based onpart-of-speech (POS) labels predicted by another se-quence labeler.
Natural languages are especially in-terpreted to have a hierarchy of sequential structureson different levels of abstraction.
Therefore, manytasks in NLP are modeled as a cascade of sequencepredictions.If a prediction is based upon the result of anotherprediction, we call the former upper stage and thelatter lower stage.Methods pursued for a cascade of predictions ?including sequence predictions, of course?, are de-sired to perform certain types of capability.
One de-628sired capability is rich forward information propa-gation, that is, the learning and estimation on eachstage of predictions should utilize rich informa-tion of the results of lower stages whenever pos-sible.
?Rich information?
here includes next bestsand confidence information of the results of lowerstages.
Another is backward information propaga-tion, that is, the rich annotated data on an upper stageshould affect the models on lower stages retroac-tively.Many current systems for a cascade of sequencepredictions adopt a simple 1-best feed-forward ap-proach.
They simply take the most likely output ateach prediction stage and transfer it to the next upperstage.
Such a framework can maximize reusabilityof existing sequence labeling systems.
On the otherhand, it exhibits a strong tendency to propagate er-rors to upper labelers.Typical improvement on the 1-best approach isto keep k-best results in the cascade of predictions.However, the larger k becomes, the more difficult itis to enumerate and maintain the k-best results.
It isparticularly prominent in sequence labeling.The essence of this orientation is that the labeleron an upper stage utilizes the information of all thepossible output candidates on lower stages.
How-ever, the size of the output space can become quitelarge in sequence labeling.
It effectively forbids ex-plicit enumeration of all possible outputs, so it isrequired to represent all the labeling possibilitiescompactly or employ some approximation schemes.Several studies are in this direction.
In the methodproposed in Finkel et al (2006), a cascades of se-quence predictions is viewed as a Bayesian network,and sample sequences are drawn at each stage ac-cording to the output distribution.
The samples arethen used to estimate the entire distribution of thecascade.
In the method proposed in Bunescu (2008),an upper labeler uses the probabilities marginalizedon the parts of the output sequences on lower stagesas weights for the features.
The weighted featuresare integrated in the model of the labeler on theupper stage.
A k-best approach (e.g., (Collins andDuffy, 2002)) and the methods mentioned above areeffective to improve the forward information propa-gation.
However, they can never contribute on back-ward information propagation.To improve the both directions of informationpropagation, Some studies propose the joint learningof multiple sequence labelers.
Sutton et al (2007)proposes the joint learning method in case wheremultiple labels are assigned to each time slice ofthe input sequences.
It enables simultaneous learn-ing and estimation of multiple sequence labelingson the same input sequences, where time slices ofthe outputs of all the out sequences are regularlyaligned.
However, it puts the distribution of statesinto Bayesian networks with cyclic dependencies,and exact inference is not tractable in such a modelin general.
Therefore, it requires some approxi-mate inference algorithms in learning or predictions.Moreover, it only considers the cases where labels ofan input sequence and all output sequences are reg-ularly aligned.
It is not clear how to build a jointlabeling model which handles irregular output labelsequences like semi-Markov models (Sarawagi andCohen, 2005).In this paper, we propose a middle ground fora cascade of sequence predictions.
The proposedmethod adopts the basic idea of Bunescu (2008).
Wefirst assume that the model on all the sequence la-beling stages is probabilistic one.
In modeling of anupper stage, a feature is weighted by the marginalprobability of the fragment of the outputs from alower stage.
However, this is not novel itself be-cause it is just a paraphrase of Bunescu?s core idea.Our intuition behind the proposed method is as fol-lows.
Features integrated in the model on each stageare weighted by the marginal probabilities of thefragments of the outputs on lower stages.
So, ifthe output distributions on lower stages change, themarginal probabilities of any fragments also change,and this in turn can change the value of the featureson the upper stage.
In other words, the features onan upper stage indirectly depend on the models onthe lower stages.
Based on this intuition, the learn-ing procedure of the model on an upper stage canaffect not only direct model parameters, but also theweights of the features by changing the model onthe lower stages.
Supervised learning based on an-notated data on an upper stage may affect the modelor model parameters on the lower stages.
It couldbe said that the information of annotation data onan upper stage is propagated back to the model onlower stages.In the next section, we describe the formal nota-629tion of our model.
In Section 3, we propose an opti-mization procedure according to the intuition notedabove.
In Section 4, we report an experimental resultof our method.
The proposed method shows someimprovements on a real-world task in comparisonwith ordinary methods.2 FormalizationIn this section, we introduce the formal notation ofour model.
Hereafter, for the sake of simplicity, weonly describe the simplest case in which there arejust two stages, one lower stage of sequence labelingnamed L1 and one upper stage of sequence labelingnamed L2.
In L1, the most likely one among a setof possible sequences is predicted for a given inputx.
L2 is also a sequence labeling stage for the sameinput x and the output of L1.
No assumption is madeon the structure of x.
The information of x is totallyencoded in feature functions.
It is only assumed thatthe output spaces of both L1 and L2 are conditionedon the initial input x.First of all, we describe the formalization of theprobabilistic model for L1.
The model for L1 perse is the same as ordinary ones for sequence label-ing.
For a given input x, consider a directed acyclicgraph (DAG) G1 = (V1, E1).
A source of a DAG Gis a node whose in-degree is equal to zero.
A sinkof a DAG G is nodes whose out-degree is equal tozero.
Let src(G), snk(G) denote the set of sourceand sink nodes in G, respectively.
A successful pathof a DAG G is defined as a directed path on G whosestarting node is a source and end node is a sink.
If ydenotes a path on a DAG, let y also denote the set ofall the arcs appearing on y for the sake of shorthand.We denote the set of all the possible successful pathson G1 by Y1.
The space of the output candidates forL1 is exactly equal to Y1.
For the modeling of L1, itis assumed that features of the form f?1,k1,e1,x?
?
R(k1 ?
K1, e1 ?
E1) are allowed to be used.
Here,K1 is the index set of the feature types for L1.
Sucha feature can capture an aspect of the correlation be-tween adjacent nodes.
We call this kind of featuresinput features for L1.
This naming is used to distin-guish them from another kind of features defined onL1, which comes later.
Although features on V1 canbe also defined, they are totally omitted in this paperfor brevity.
Hereafter, if a symbol has subscripts,then missing subscript indicates a set that range overthe omitted subscript.
For example, f?1,e1,x?def?{f?1,k1,e1,x?
}k1?K1 , f?1,k1,x?def?{f?1,k1,e1,x?
}e1?E1 ,f?1,x?def?{f?1,k1,e1,x?
}k1?K1,e1?E1 , and so on.The probabilistic model on L1 forms the log-linearmodel, that is,P1(y1|x;?1)def?
1Z1(x;?1)exp(?1 ?
F?1,y1,x?
)(y1 ?
Y1) ,(1)where ??1,k1?
?
R (k1 ?
K1) is the weight for thefeature of the same index k1, and the k1-th elementof F?1,y1,x?, F?1,k1,y1,x?def?
?e1?y1 f?1,k1,e1,x?.
Dotoperator (?)
denotes the inner product with respect tothe subscripts commonly missing in both operands.Z1 is the partition function for P1, defined asZ1(x;?1)def?
?y1?Y1exp(?1 ?
F?1,y1,x?).
(2)It is worth noting that this formalization subsumesboth directed and undirected linear-chain graphicalmodels, which are the most typical models for se-quence labeling, including HMM and CRF.
That is,if the elements of V1 are aligned into regular timeslices, and the nodes in each time slice are associatedwith possible assignments of labels for that time, weobtain the representation equivalent to the ordinarylinear-chain graphical models, in which all possiblelabel assignments for each state are expanded.
Insuch configuration, all the possible successful pathsdefined in our notation have strict one-to-one corre-spondence to all the possible joint assignments oflabels in linear-chain graphical models.
We pur-posely employ this DAG-based notation because; itis convenient to describe the models and algorithmsfor our purpose, it allows for labels to stay in arbi-trary time as in semi-Markov models, and it is easilyextended to models for a set of trees instead of se-quences by replacing the graph-based notation withhypergraph-based notation.Next, we formalize the probabilistic model on theupper stage L2.
Like L1, consider a DAG G2 =(V2, E2) conditioned on the input x, and the set ofall the possible successful paths on G2, denoted Y2.The space of the output candidates for L2 becomesY2.630The form of the features available in designing theprobabilistic model for L2, denoted by P2, is the keyof this paper.
A feature on an arc e2 ?
E2 can ac-cess local characteristics of the confidence-rated su-perposition of the L1?s outputs, in addition to theinformation of the input x.
To formulate local char-acteristics of the superposition of the L1?s outputs,we first define output features of L1, denoted byh?1,k?1,e1?
?
R (k?1 ?
K?1, e1 ?
E1).
Here, K?1 isthe index set of the output feature types of L1.
Be-fore the output features are integrated into the modelfor L2, they all are confidence-rated with respect toP1, that is, each output feature h?1,k?1,e1?
is numer-ically rated by the estimated probabilities summedover the sequences emitting that feature.
More for-mally, all the L1?s output features are integrated infeatures for P2 in the form of the marginalized out-put features, which are defined as follows;h??1,k?1,e1?(?1)def?
h?1,k?1,e1?P1(e1|x;?1)(k?1 ?
K?1, e1 ?
E1),(3)whereP1(e1|x;?1)def?
?y1?e1P1(y1|x;?1)=?y1?Y1?e1?y1P1(y1|x;?1)(e1 ?
E1) .
(4)Here, the notation?y1?e1 represents the sum-mation over sequences consistent with an arce1 ?
E1, that is, the summation over the set{y1 ?
Y1 | e1 ?
y1}.
?P denotes the indicatorfunction for a predicate P .
The input features for P2on an arc e2 ?
E2 are permitted to arbitrarily com-bine the information of x and the L1?s marginalizedoutput features h?1, in addition to the local charac-teristics of the arc at hand e2.
In summary, an inputfeature for L2 on an arc e2 ?
E2 is of the formf?2,k2,e2,x?(h?1(?1))?
R (k2 ?
K2) , (5)where K2 is the index set of the input feature typesfor L2.
To make the optimization procedure feasible,smoothness condition on any L2?s input feature isassumed with respect to all the L1?s output features,that is, ?f?2,k2,e2,x??h?
?1,k?1,e1?is always guaranteed to exist for?k?1, e1, k2, e2.
For example, additions and mul-tiplications between some elements of h?1(?1) canappear in the definition of L2?s input features.
Forgiven input features f?2,x?
(h?1(?1))and parameters??2,k2?
?
R (k2 ?
K2), the probabilistic model forL2 is defined as follows;P2(y2|x;?1,?2)def?
1Z2(x;?1,?2)exp(?2 ?
F?2,y2,x?
(h?1(?1)))(y2 ?
Y2) ,(6)where F?2,k2,y2,x?
(h?1(?1)) def?
?e2?y2 f?2,k2,e2,x?
(h?1 (?1))and Z2 is the par-tition function of P2, defined byZ2(x;?1,?2)def?
?y2?Y2exp(?2 ?
F?2,y2,x?(h?1(?1))).
(7)The definition of P2 (6) reveals one of the most im-portant points in this paper.
P2 is viewed not onlyas the function of the ordinary direct parameters ?2but also as the function of ?1, which represents theparameters for the L1?s model, through the interme-diate variables h?1.
So optimization procedure on P2may affect the determination of the values not onlyof the direct parameters ?2 but also of the indirectones ?1.If the result of L1 is reduced to the single goldenoutput y?1, i.e.
P1(y1|x) = ?y1=y?1 , the definitionsabove boil down to the formulation of the simple 1-best feed forward architecture.3 Optimization AlgorithmIn this section, we describe optimization procedurefor the model formulated in the previous section.Let D = {?x?, ?G1, y?1?, ?G2, y?2??m}m=1,2,???
,M de-note annotated data for the supervised learning ofthe model.
Here, ?G1, y?1?
is a pair of a DAG andcorrectly annotated successful sequence for L1.
Thesame holds for ?G2, y?2?.
For given D, we can definethe conditional log-likelihood function on L1 and L2respectively, that is,L1 (?1;D)def???x?,y?1?
?Dlog (P1 (y?1|x?
;?1)) ?|?1|2?12(8)631   	   Figure 1: Computation Graph of the Proposed ModelandL2 (?1,?2;D)def???x?,y?2?
?Dlog (P2 (y?2|x?
;?1,?2)) ?|?2|2?22.
(9)Here, ?12, ?22 are the variances of the prior distribu-tions of the parameters.
For the sake of simplicity,we set the prior distribution as the zero-mean uni-variance Gaussian.
To optimize the both probabilis-tic models P1 and P2 jointly, we also define the jointconditional log-likelihood functionL (?1,?2;D)def?
L1 + L2 .
(10)The parameter values to be learned are the ones that(possibly locally) maximize this objective function.Note that this objective function is not guaranteed tobe globally convex.We employ gradient-based parameter optimiza-tion here.
Optimization procedure repeatedlysearches a direction in the parameter space whichis ascendent with respect to the objective function,and updates the parameter values into that directionby small advances.
Many existing optimization rou-tines like steepest descent or conjugation gradient dothat job only by giving the objective value and gra-dients on parameter values to be updated.
So, theoptimization problem here boils down to the calcu-lation of the objective value and gradients on givenparameter values.Before entering the detailed description of the al-gorithm for calculating the objective function andgradients, we note the functional relations amongthe objective function and previously defined vari-ables.
The diagram shown in Figure 1 illustratesthe functional relations among the parameters, inputand output feature functions, models, and objectivefunction.
The variables at the head of a directed ar-row in the figure is directly defined in terms of theones at the tail of the same arrow.
The value of theobjective function on given parameter values can becalculated in order of the arrows shown in the di-agram.
On the other hand, the parameter gradientsare calculated step-by-step in reverse order of the ar-rows.
The functional relations illustrated in the Fig-ure 1 ensure some forms of the chain rule of dif-ferentiation among the variables.
The chain rule isiteratively used to decompose the calculation of thegradients into a divide-and-conquer fashion.
Thesetwo directions of stepwise computation are analo-gous to the forward and back propagation for multi-layer feedforward neural networks, respectively.Algorithm 1 shows the whole picture of thegradient-based optimization procedure for ourmodel.
We first describe the flow to calculate theobjective value for a given parameters ?1 and ?2,which is shown from line 2 through 4 in Algo-rithm 1.
The values of marginalized output featuresh??1,x?
can be calculated by (3).
Because they are thesimple marginals of features, the ordinary forward-backward algorithm (hereafter, abbreviated as ?F-B?)
on G1 offers an efficient way to calculate theirvalues.
Although nothing definite about the formsof the input features for L2 is presented in this pa-per, f?2,x?
can be calculated once the values of h?
?1,x?have been obtained.
Finally, L1, L2 and then L areeasy to calculate because they are no different fromthe ordinary log-likelihood computation.Now we describe the algorithm to calculate theparameter gradients,?L?
?1= ?L1?
?1+ ?L2?
?1, ?L?
?2= ?L2??2.
(11)Line 5 through line 7 in Algorithm 1 describe thegradient computation.
The terms ?L1?
?1 and?L2?
?2 in(11) become the same forms that appear in the ordi-nary CRF optimization, i.e., the difference betweenthe empirical frequencies of the features and themodel expectations of them,?L1?
?1= E?[F?1,y1,x?]?
EP1[F?1,y1,x?]?
|?1|?12,?L2?
?2= E?[F?2,y2,x?]?
EP2[F?2,y2,x?]?
|?2|?22.
(12)These calculations are performed by the ordinary F-B on G1 and G2, respectively.
Using the chain ruleof differentiation derived from the functional rela-tions illustrated in Figure 1, the remaining term ?L2?
?1632Algorithm 1 Gradient-based optimization of the model parametersInput: ?1, ?2Output: argmax?
?1,?2?L1: while ?1 or ?2 changes significantly do2: calculate Z1 by (2), h?1 by (3) with the F-B on G1, and then L1 by (8)3: calculate f?2,x?
according to their definitions4: calculate Z2 by (7) with the F-B on G2, and then L2 by (9) and L by (10)5: calculate ?L1?
?1 and?L2?
?2 by (12) with the F-B on G1 and G2, respectively6: calculate ?L?f?1,x?
by (16) with the F-B on G2,?f?1,x?
?h?1 , and them?L2?h?1 =?L?f?1,x?
??f?1,x?
?h?17: calculate ?L2?
?1 by (18) with Algorithm 28: ??1,?2?
?
update-parameters(?1,?2,L, ?L?
?1 ,?L?
?2)9: end whilein (11) can be decomposed as follows;?L2?
?1= ?L2?f?2,x???f?2,x??
?1= ?L2?f?2,x???f?2,x??h?1?
?h?1??1.
(13)Note that Leibniz?s notation here denotes a Jacobianwith the index sets omitted in the numerator and thedenominator, for example,?f?2,x??h?1def?{?f?2,k2,e2,x??h?1,k?1,e1?
}k2?K2,e2?E2,k?1?K?1,e1?E1(14)And also recall that dot operators here stand for theinner product with respect to the index sets com-monly omitted in both operands, for example,?L2?f2?
?f2?h?1=?k2?K2,e2?E2?L2?f?2,k2,e2,x???f?2,k2,e2,x??h?1.
(15)We describe the manipulation of each factor inthe right side of (13) in turn.
Noting ?f?2,k2,e2,x??f?2,k`2,e`2,x?
=?k2=k`2?e2=e`2 , each element of the first factor of (13)?L2?f?2,x?
can be transformed as follows;?L2?f?2,k2,e2,x?= ??2,k2???x?,y?2?
?D(?e2?y?2 ?
P2(e2|x?;?1,?2)).(16)P2(e2|x?
;?1,?2), the marginal probability on e2, canbe obtained as a by-product of the F-B for (12).As described in the previous section, it is assumedthat the values of the second factor ?f?2,x?
?h?1 is guaran-teed to exists for any given ?1, and the procedure forcalculating them is fixed in advance.
The procedurefor some of concrete features is exemplified in theprevious section.From the definition of h?1 (3), each element of thethird factor of (13) ?h?1?
?1 becomes?h??1,k?1,e1???
?1,k1?= h?1,k?1,e1?CovP1(y1|x)[?e1?y1 , F?1,k1,y1,x?].
(17)There exists efficient dynamic programming to cal-culate the covariance value (17) (without goint intothat detail because it is very similar to the one shownlater in this paper), and of course we can run suchdynamic programming for ?k?1 ?
K?1, e1 ?
E1.However, the size of the Jacobian ?h?1?
?1 is equal to|K?1|?|E1|?|K1|.
Since it is too large in many taskslikely to arise in practice, we should avoid to calcu-late all the elements of this Jacobian in a straight-forward way.
Instead of such naive computation, ifthe values of ?L2?f?2,x?
and?f?2,x?
?h?1 are obtained, then wecan compute ?L2?h?1 =?L2?f?2,x?
??f?2,x?
?h?1 , and from (13)633and (17),?L2?
?1= ?L2?h?1?
?h?1?
?1= EP1(y1|x)[H ??1,y1?F?1,y1,x?]?
EP1(y1|x)[H ??1,y1?]EP1(y1|x)[F?1,y1,x?
],(18)where H ??1,y1?def?
?e1?y1?L2?h??1,e1??
h?1,e1?.
In otherwords, ?L2???1,k1?
becomes the covariance between thek1-th input feature for L1 and the hypothetical fea-ture h??1,e1?def?
?L2?h??1,e1?
?
h?1,e1?.The final problem is to derive an efficient way tocompute the first term of (18).
The second term of(18) can be calculated by the ordinary F-B becauseit consists of the marginals of arc features.
There aretwo derivations of the algorithm for calculating thefirst term.
We describe briefly the both derivations.One is a variant of the F-B on the expectationsemi-ring proposed in Li and Eisner (2009).
First,the F-B is generalized to the expectation semi-ringwith respect to the hypothetical feature h?
?1,e1?, andby summing up the marginals of the feature vectorsf?1,e1,x?
on all the arcs under the distribution of thesemi-ring, then we obtain the expectation of the fea-ture vector f?1,e1,x?
on the semi-ring potential.
Thisexpectation is equal to the first term of (18).
1Another derivation is to apply the automatic dif-ferentiation (AD)(Wengert, 1964; Corliss et al,2002) on the F-B calculating EP1[F?1,y1,x?].
Itexploits the fact that ??
?EP ?1[F?1,y1,x?]
???
?=0isequal to the first term of (18), where ?
?R is a dummy parameter, and P ?1(y1|x)def?1Z1 exp(?1 ?
F?1,y1,x?
+ ?H ??1,y1?).
It is easyto derive the F-B for calculating the valueEP ?1[F?1,y1,x?]
????=0.
AD transforms this F-B intoanother algorithm for calculating the differentiationw.r.t.
?
evaluated at the point ?
= 0.
This trans-formation is achieved in an automatic manner, byreplacing all appearances of ?
in the F-B with a dualnumber ?
+ ?.
The dual number is a variant of thecomplex number, with a kind of the imaginary unit?
with the property ?2 = 0.
Like the usual complex1For the detailed description, see Li and Eisner (2009) andits references.numbers, the arithmetic operations and the exponen-tial function are generalized to the dual numbers,and the ordinary F-B is also generalized to the dualnumbers.
The imaginary part of the resulting valuesis equal to the needed differentiation.
2 Anyway,these two derivations lead to the same algorithm, andthe resulting algorithm is shown as Algorithm 2.The final line in the loop of Algorithm 1 can beimplemented by various optimization routines andline search algorithms.The time and space complexity to compute the ob-jective and gradient values for given parameter vec-tors ?1,?2 is the same as that for that for Bunescu(2008), up to a constant factor.
Because the calcula-tion of the objective function is essentially the sameas that for Bunescu (2008), and in gradient com-putation, the time complexity of Algorithm 1 is thesame as that for the ordinary F-B (up to a constantfactor), and the proposed optimization procedure isonly required to store additional scalar values h?
?1,e1?on each G1?s arc.4 ExperimentWe examined effectiveness of the method proposedin this paper on a real task.
The task is to annotatethe POS tags and to perform base-phrase chunkingon English sentences.Base-phrase chunking is a task to classify con-tinuous subsequences of words into syntactic cat-egories.
This task is performed by annotating achunking label on each word (Ramshaw and Mar-cus, 1995).
The types of chunking label consist of?Begin-Category?, which represents the beginningof a chunk, ?Inside-Category?, which represents theinside of a chunk, and ?Other.?
Usually, POS la-beling runs first before base-phrase chunking is per-formed.
Therefore, this task is a typical interestingcase where a sequence labeling depends on the out-put from other sequence labelers.The data used for our experiment consist of En-glish sentences from the Penn Treebank project(Marcus et al, 1993) consisting of 10948 sentencesand 259104 words.
We divided them into twogroups, training data consisting of 8936 sentencesand 211727 words and test data consisting of 20122For example, Berz (1992) gives a detailed description ofthe reason why the dual number is used for this purpose.634Algorithm 2 Forward-backward Algorithm for Calculating Feature CovariancesInput: f?1,x?, ?e1def?
exp(?1 ?
f?1,e1,x?
), h?e1def?
?L2?h??1,e1?
?
h?1,e1?Output: qk1 = CovP(y1|x)[H ?
?1,y1?, F?1,k1,y1,x?]
(?k1 ?
K1)1: for ?v1 ?
src(G1), ?v1 ?
1, ?
?v1 ?
12: for all v1 ?
V1 in a topological order do3: prev ?
{x ?
V1 | (x, v1) ?
E1}4: ?v1 ??x?prev?
(x,v1)?x, ?
?v1 ??x?prev?(x,v1)(h?
(x,v1)?x + ?
?x)5: end for6: Z1 ?
?x?snk(G1)?x7: for ?v1 ?
snk(G1), ?v1 ?
1, ?
?v1 ?
18: for all v1 ?
V1 in a reverse topological order do9: next ?
{x ?
V1 | (v1, x) ?
E1}10: ?v1 ??x?next?
(v1,x)?x, ?
?v1 ??x?next?(v1,x)(h?
(v1,x)?x + ?
?x)11: end for12: for ?k1 ?
K1, qk1 ?
013: for all (u1, v1) ?
E1 do14: p ?
?(u1,v1)(?u1?
?v1 + ?
?u1?v1)/Z115: for ?k1 ?
K1, qk1 ?
qk1 + pf?1,k1,e1,x?16: end forsentences and 47377 words.
The number of the POSlabel types is equal to 45.
The number of the labeltypes used in base-phrase chunking is equal to 23.We compare the proposed method to two exist-ing sequence labeling methods as baselines.
ThePOS labeler is the same in all the three methodsused in this experiment.
This labeler is a simpleCRF and learned by ordinary optimization proce-dure.
One baseline method is the 1-best pipelinemethod.
A simple CRF model is learned for thechunking labeling, on the input sentences and themost likely POS label sequences predicted by thealready learned POS labeler.
We call this method?CRF + CRF.?
The other baseline method has aCRF model for the chunking labeling, which usesthe marginalized features offered by the POS la-beler.
However, the parameters of the POS labelerare fixed in the training of the chunking model.This method corresponds to the method proposedin Bunescu (2008).
We call this baseline ?CRF +CRF-MF?
(?MF?
for ?marginalized features?).
Theproposed method is the same as ?CRF + CRF-MF?,except that the both labelers are jointly trained by theCRF CRF CRF+ CRF + CRF-MF +CRF-BPPOS labeling 95.6 (95.6) 95.8Base-phrase 92.1 92.7 93.1chunkingTable 2: Experimental result (F-measure)procedure described in Section 3.
We call this pro-posed method ?CRF + CRF-BP?
(?BP?
for ?backpropagation?
).In ?CRF + CRF-BP,?
the objective function forjoint learning (10) is not guaranteed to be convex, sooptimization procedure is sensible to the initial con-figuration of the model parameters.
In this experi-ment, we set the parameter values learned by ?CRF+ CRF-MF?
as the initial values for the training ofthe ?CRF + CRF-BP?
method.
Feature templatesused in this experiment are listed in Table 1.
Al-though we only described the formalization and op-timization procedure of the models with arc features,We use node features in the experiment.Table 2 shows the result of the methods we men-635=== Node feature templates ===Node is sourceNode is sinkInput word on the same time sliceSuffix of input word on the same time slice, n characters (n ?
[1, 2, 3])Initial word character is capitalized?All word characters are capitalized?Input word included in the vocabulary of POS T ?
(T ?
{(All possible POS labels)})Input word contains numbers?POS label?=== Arc feature templates ===Tail node is sourceHead node is sinkCorresponding ordered pair of POS labels?Table 1: List of feature templates.
All node features are combined with the corresponding node label (POS or chunkinglabel) feature.
All arc features are combined with the feature of the corresponding arc label pair.
?
features areinstantiated on each time slice in five character window.
?
features are not used in POS labeler, and marginalized asoutput features for ?CRF + CRF-MF?
and ?CRF + CRF-BP.?tioned.
In Table 2, bold numbers indicate significantimprovement over the baseline models with ?
=0.05.
From Table 2, the proposed method signifi-cantly outperforms two baseline methods on chunk-ing performance.
Although the improvement onPOS labeling performance by the proposed method?CRF + CRF-BP?
is not significant, it might showthat optimization procedure provides some form ofbackward information propagation in comparison to?CRF + CRF-MF.
?5 ConclusionsIn this paper, we adopt the method to weight featureson an upper sequence labeling stage by the marginal-ized probabilities estimated by the model on lowerstages.
We also point out that the model on an upperstage is considered to depend on the model on lowerstages indirectly.
In addition, we propose optimiza-tion procedure that enables the joint optimization ofthe multiple models on the different level of stages.We perform an experiment on a real-world task, andour method significantly outperforms existing meth-ods.We examined the effectiveness of the proposedmethod only on one task in comparison to just a fewexisting methods.
In the future, we hope to compareour method to other competing methods like jointlearning approaches in terms of both accuracy andcomputational efficiency, and perform extensive ex-periments on various tasks.ReferencesM.
Berz.
1992.
Automatic differentiation as nonar-chimedean analysis.
In Computer Arithmetic and En-closure, pages 439?450.R.C.
Bunescu.
2008.
Learning with probabilistic fea-tures for improved pipeline models.
In Proceedings ofthe 2008 Conference on Empirical Methods in NaturalLanguage Processing, pages 670?679.M.
Collins and N. Duffy.
2002.
New ranking algorithmsfor parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In Proceedings ofthe 40th Annual Meeting on Association for Compu-tational Linguistics, pages 263?270.
Association forComputational Linguistics.G.F.
Corliss, C. Faure, and A. Griewank.
2002.
Auto-matic differentiation of algorithms: from simulation tooptimization.
Springer Verlag.J.R.
Finkel, C.D.
Manning, and A.Y.
Ng.
2006.
Solv-ing the problem of cascading errors: Approximatebayesian inference for linguistic annotation pipelines.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 618?626.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-636menting and labeling sequence data.
In Proceedings ofthe Eighteenth International Conference on MachineLearning, pages 282?289.Z.
Li and J. Eisner.
2009.
First-and second-order ex-pectation semirings with applications to minimum-risktraining on translation forests.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing: Volume 1-Volume 1, pages 40?51.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational linguistics,19(2):330.L.A.
Ramshaw and M.P.
Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proceedingsof the Third ACL Workshop on Very Large Corpora,pages 82?94.
Cambridge MA, USA.S.
Sarawagi and W.W. Cohen.
2005.
Semi-markovconditional random fields for information extraction.Advances in Neural Information Processing Systems,17:1185?1192.C.
Sutton, A. McCallum, and K. Rohanimanesh.
2007.Dynamic conditional random fields: Factorized proba-bilistic models for labeling and segmenting sequencedata.
The Journal of Machine Learning Research,8:693?723.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In Advances in Neural InformationProcessing Systems 16.RE Wengert.
1964.
A simple automatic derivativeevaluation program.
Communications of the ACM,7(8):464.637
