Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1807?1816,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsOn- and Off-Topic Classification and Semantic Annotation ofUser-Generated Software RequirementsMarkus Dollmann and Michaela GeierhosHeinz Nixdorf InstituteUniversity of PaderbornFu?rstenallee 11, 33102 Paderborn, Germany{dollmann|geierhos}@hni.upb.deAbstractUsers prefer natural language software re-quirements because of their usability and ac-cessibility.
When they describe their wishesfor software development, they often provideoff-topic information.
We therefore presentREaCT1, an automated approach for identify-ing and semantically annotating the on-topicparts of requirement descriptions.
It is de-signed to support requirement engineers in theelicitation process on detecting and analyzingrequirements in user-generated content.
Sinceno lexical resources with domain-specific in-formation about requirements are available,we created a corpus of requirements writ-ten in controlled language by instructed usersand uncontrolled language by uninstructedusers.
We annotated these requirements re-garding predicate-argument structures, con-ditions, priorities, motivations and semanticroles and used this information to train clas-sifiers for information extraction purposes.REaCT achieves an accuracy of 92% for theon- and off-topic classification task and an F1-measure of 72% for the semantic annotation.1 Introduction?Requirements are what the software product, orhardware product, or service, or whatever you in-tend to build, is meant to do and to be?
(Robert-son and Robertson, 2012).
This intuitive descrip-tion of requirements has one disadvantage.
It is asvague as a requirement that is written by an un-trained user.
More generally, functional require-ments define what a product, system or process, or1Requirements Extraction and Classification Toola part of it is meant to do (Robertson and Robert-son, 2012; Vlas and Robinson, 2011).
Due to its ex-pressiveness, natural language (NL) became a pop-ular medium of communication between users anddevelopers during the requirement elicitation pro-cess (de Almeida Ferreira and da Silva, 2012; Michet al, 2004).
Especially in large ICT projects, re-quirements, wishes, and ideas of up to thousands ofdifferent users have to be grasped (Castro-Herreraet al, 2009).
For this purpose, requirement en-gineers collect their data, look for project-relevantconcepts and summarize the identified technical fea-tures.
However, this hand-crafted aggregation andtranslation process from NL to formal specifica-tions is error-prone (Goldin and Berry, 1994).
Sincepeople are getting tired and unfocused during thismonotonous work, the risk of information loss in-creases.
Hence, this process should be automated asfar as possible to support requirement engineers.In this paper, we introduce our approach to iden-tify and annotate requirements in user-generatedcontent.
We acquired feature requests for opensource software from SourceForge2, specified by(potential) users of the software.
We divided theserequests into off-topic information and (on-topic)requirements to train a binary text classifier.
Thisallows an automated identification of new require-ments in user-generated content.
In addition, we col-lected requirements in controlled language from theNFR corpus3 and from web pages with user-storyexplanations.
We annotated the semantically rele-2https://sourceforge.net3http://openscience.us/repo/requirements/other-requirements/nfr1807vant parts of the acquired requirements for infor-mation extraction purposes.
This will support re-quirements engineers on requirement analysis andenables a further processing such as disambiguationor the resolution of incomplete expressions.This paper is structured as follows: In Section 2,we discuss the notion of requirements.
Then we pro-vide an overview of previous work (Section 3), be-fore we introduce lexical resources necessary for ourmethod (Section 4).
The approach itself is presentedin Section 5 before it is evaluated in Section 6.
Fi-nally, we conclude this work in Section 7.2 The Nature of RequirementsRequirement engineers and software developershave to meet users?
wishes in order to create newsoftware products.
Such descriptions of softwarefunctionalities can be expressed in different ways:For example, by using controlled languages orformal methods, clarity and completeness can beachieved.
But non-experts can hardly apply themand therefore do not belong to the user group.
Forthis reason, users are encouraged to express their in-dividual requirements for the desired software ap-plication in NL in order to improve user accep-tance and satisfaction (Firesmith, 2005).
In gen-eral, software requirements are expressed throughactive verbs such as ?to calculate?
or ?to publish?
(Robertson and Robertson, 2012).
In this work, wedistinguish requirements expressed in NL betweencontrolled and uncontrolled language.A controlled language is a subset of NL, which ischaracterized by a restricted grammar and/or limitedvocabulary (Yue et al, 2010).
Requirements in con-trolled language do not suffer from ambiguity, re-dundancy and complexity (Yue et al, 2010).
That iswhy these recommendations lead to a desirable inputfor text processing.
Robertson and Robertson (2012)therefore recommend specifying each requirementin a single sentence with one verb.
Furthermore,they suggest the following start of record ?The [sys-tem/product/process] shall ...?, which focuses on thefunctionality and keeps the active form of a sen-tence.
An example therefore is ?The system shalldisplay the Events in a graph by time.?
Another typeof controlled requirements are user stories.
They fol-low the form ?As a [role], I want [something] sothat [benefit]?
and describe software functionalitiesfrom the user?s perspective (Cohn, 2004).
Comparedto the previous ones, they do not focus on the tech-nical implementation but concentrate on the goalsand resulting benefits.
An example therefore is ?Asa Creator, I want to upload a video from my localmachine so that any users can view it.
?We also consider uncontrolled language in thiswork because requirements are usually specified byusers that have not been instructed for any typeof formulation.
Requirements in uncontrolled lan-guage do not stick to grammar and/or orthographicrules and may contain abbreviations, acronyms oremoticons.
There is no restriction how to expressoneself.
An example therefore is ?Hello, I wouldlike to suggest the implementation of expiration datefor the master password :)?.In the following, the word ?requirement?
is usedfor a described functionality.
We assume that itstextualization is written within a single English sen-tence.
Requirements are specified in documents likethe Software Requirements Specification (SRS).
Werefer to SRS and other forms (e.g.
e-mails, memosfrom workshops, transcripts of interviews or entriesin bug-trackers) as requirement documentations.3 Previous WorkIt is quite common that requirement engineerselicit requirements together with users in interviews,group meetings, or by using questionnaires (Mich,1996).
Researchers developed (semi-) automatedand collaborative approaches to support requirementengineers in this process (Ankori and Ankori, 2005;Castro-Herrera et al, 2009).
Besides the elicitationin interaction with the users, an identification of re-quirements from existing sources is possible.
Forexample, John and Do?rr (2003) used documenta-tions from related products to derive requirementsfor a new product.
Vlas and Robinson (2011) usedunstructured, informal, NL feature requests fromthe platform SourceForge to collect requirements foropen source software.
They presented a rule-basedmethod to identify and classify requirements accord-ing to the quality criteria of the McCall?s QualityModel (McCall, 1977).
Analogous to their work, wewant to automatically detect requirements in user-generated content.
While they applied a rule-based1808method, we plan to identify requirements in user-generated content with a machine learning approach.Since those approaches automatically identify pat-terns for this classification task, we expect a higherrecall and more reliable results.Goldin and Berry (1994) identified so-called ab-stractions (i.e.
relevant terms and concepts related toa product) of elicited requirements for a better com-prehension of the domain and its restrictions.
Theirtool AbstFinder is based on the idea that the signifi-cance of terms and concepts is related to the numberof mentions in the text.
However, in some cases,there is only a weak correlation between the termfrequencies and their relevance in documents.
Thisproblem can be reduced by a statistical corpus anal-ysis, when the actual term frequency is similar to theexpected (Sawyer et al, 2002; Gacitua et al, 2011).This approach eliminates corpus specific stopwordsand misleading frequent terms.
In our work, we in-tent to perform a content analysis of the previouslydetected requirements.
However, instead of onlyidentifying significant terms and concepts, we cap-ture the semantically relevant parts of requirementssuch as conditions, motivations, roles or actions (cf.Figure 1).In addition to the identification of abstractions,there are methods to transform NL requirements intographical models (e.g.
in Unified Modeling Lan-guage) (Harmain and Gaizauskas, 2003; Ambriolaand Gervasi, 2006; Ko?rner and Gelhausen, 2008).A systematic literature review, done by Yue et al(2010), aims at the modeling of requirements bycomparing transformation techniques in such mod-els.
Unlike those techniques, we aim to keep the ex-pressive aspect of the original textual requirementsand semantically annotate them for filtering pur-poses.
These results can be further used for dif-ferent NLP tasks such as disambiguation, resolu-tion of vagueness or the compensation of under-specification.The semantic annotation task of this work is sim-ilar to semantic role labeling (SLR).
According toJurafsky and Martin (2015), the goal of SLR is un-derstanding events and their participants, especiallybeing able to answer the question who did what towhom (and perhaps also when and where).
In thiswork, we seek to adapt this goal to the requirementsdomain, where we want to answer the question whatactions should be done by which component (andperhaps also who wants to perform that action, arethere any conditions, what is the motivation for per-forming this action and is there a priority assignedto the requirement).4 Gathering and Annotation of Controlledand Uncontrolled RequirementsThere are benchmarks comparing automated meth-ods for requirement engineering (Tichy et al, 2015).However, none of the published datasets is sufficientto train a text classifier, since annotated informationis missing.
For our purposes, we need a data set withannotated predicate-argument structures, conditions,priorities, motivations and semantic roles.
We there-fore created a semantically annotated corpus by us-ing the categories shown in Figure 1, which repre-sent all information bits of a requirement.
Since theapproach should be able to distinguish between (on-topic) requirements and off-topic comments, we ac-quired software domain-specific off-topic sentences,too.Therefore, we acquired requirements in con-trolled language from the system?s and the user?sperspective.
While requirements from the system?sperspective are describing technical software func-tionalities, the requirements from the user?s per-spective express wishes for software, to fulfill userneeds.
For instance, the NFR corpus4 covers the sys-tem?s perspective of controlled requirements spec-ifications.
It consists of 255 functional and 370non-functional requirements whereof we used thefunctional subset to cover the system?s perspective.Since we could not identify any requirement corpusthat describes a software at user?s request, we ac-quired 304 user stories from websites and books thatdescribe how to write user stories.However, these requirements in controlled lan-guage have not the same characteristics as uncon-trolled requirements descriptions.
For the acquisi-tion of uncontrolled requirements, we adapted theidea of Vlas and Robinson (2011) that is based onfeature requests gathered from the open-source soft-ware platform SourceForge5.
These feature requests4https://terapromise.csc.ncsu.edu/repo/requirements/nfr/nfr.arff5https://sourceforge.net1809are created by users that have not been instructed forany type of formulation.
Since these requests do notonly contain requirements, we split them into sen-tences and manually classified them in requirementsand off-topic information.
Here, we consider socialcommunication, descriptions of workflows, descrip-tions of existing software features, feedback, salu-tations, or greetings as off-topic information.
In to-tal, we gathered 200 uncontrolled on-topic sentences(i.e.
requirements) and 492 off-topic ones.Then we analyzed the acquired requirements inorder to identify the different possible semantic cat-egories to annotate their relevant content in our re-quirements corpus (cf.
Figure 1):?
component?
refinement of component?
action?
argument of action?
condition?
priority?
motivation?
role?
object?
refinement of object?
sub-action?
argument of sub-action?
sub-priority?
sub-role?
sub-object?
refinement of sub-objectFigure 1: Semantic categories in our software requirementscorpus used for annotation purposesThe categories component or role, action and ob-ject are usually represented by subject, predicateand object of a sentence.
In general, a descriptionrefers to a component, either to a product or sys-tem itself or to a part of the product/system.
Ac-tions describe what a component should accomplishand affect.
Actions have an effect on Objects.
Theauthors of the requirements can refine the descrip-tion of components and objects, which is coveredby the categories refinement of component and re-finement of object.
For each action, users can set acertain priority, describe their motivation for a spe-cific functionality, state conditions, and/or even de-fine some semantic roles.
Apart from the componentand the object, additional arguments of the action(predicate of a sentence) are annotated with argu-ment of action.
In some cases, requirements containsub-requirements in subordinate clauses.
The anno-tators took this into account when using the prede-fined sub-categories.
An example of an annotatedrequirement is shown in Figure 2.Figure 2: Annotated requirement sampleTwo annotators independently labeled the cate-gories in the requirements.
We define one of theannotation set as gold standard and the other as can-didate set.
We will use the gold standard for trainingand testing purposes in Section 5 and 6 and the can-didate set for calculating an inter-annotator agree-ment.
In total, our gold standard consists of 3,996labeled elements (i.e.
clauses, phrases, and evenmodality).
The frequency distribution is shown inTable 1.Semantic Category CR UR Totalcomponent 241 84 325refinement of component 6 16 22action 526 204 730argument of action 180 104 284condition 94 39 133priority 488 209 697motivation 33 19 52role 406 42 448object 540 195 735refinement of object 155 48 203sub-action 76 40 116argument of sub-action 27 14 41sub-priority 22 16 38sub-role 22 11 33sub-object 78 37 115refinement of sub-object 16 8 24Total 2,910 1,086 3,996Table 1: Number of annotated elements per category in ourgold standard (CR=controlled requirements, UR=uncontrolledrequirements)1810The inter-annotator agreement in multi-token an-notations is commonly evaluated by using F1-score(Chinchor, 1998).
The two annotators achieve anagreement of 80%, whereby the comparison was in-voked from the gold standard.Many information extraction tasks use the IOBencoding6 for annotation purposes.
When using theIOB encoding, the first token of an element is splitinto its head (first token) and its tail (rest of the ele-ment).
That way, its boundaries are labeled with B(begin) and I (inside).
This allows separating suc-cessive elements of the same category.
Thus, we usethe IOB encoding during the annotation step.
How-ever, we want to discuss a drawback of this notation:When applying text classification approaches in in-formation extraction tasks with IOB encoding, thenumber of classes reduplicates and this reduces theamount of training data per class.
During our an-notation process, successive elements of the samesemantic category only occurred in the case of argu-ment of the action and argument of the sub-action.When we disregard the IOB encoding, we can eas-ily split up (sub-)actions by keywords such as ?in?,?by?, ?from?, ?as?, ?on?, ?to?, ?into?, ?for?, and?through?.
So if we use IO encoding, it can be eas-ily transformed to the IOB encoding.
The only dif-ference between IOB and IO encoding is that it doesnot distinguish between the head and tail of an el-ement and therefore does not double the number ofclasses.5 REaCT ?
A Two-Stage ApproachRequirement documentations are the input of oursystem.
Figure 3 illustrates the two-stage approachdivided in two separate classification tasks.
First,we apply an on-/off-topic classification to decidewhether a sentence is a requirement or irrelevant forthe further processing (cf.
Section 5.1).
Then, thepreviously identified requirements were automati-cally annotated (Section 5.2).
As a result, we getfiltered and semantically annotated requirements inXML or JSON.The models for on-/off-topic classification and se-mantic annotation are trained on the gathered re-quirements (cf.
Section 4).
We split up the goldstandard on sentence level in a ratio of 4:1 in a train-6I (inside), O (outside) or B (begin) of an elementFigure 3: Processing pipeline of the two-stage approaching set of 607 requirements and test set of 152 re-quirements.
Furthermore, we used 10-fold cross val-idation on the training set for algorithm configura-tion and feature engineering (cf.
Section 5.1 andSection 5.2).
Finally, our approach is evaluated onthe test set (cf.
Section 6).5.1 On-/Off-Topic Classification TaskUser requirement documentations often contain off-topic information.
Therefore, we present a binarytext classification approach that distinguishes be-tween requirements and off-topic content.
Thus, wetrained different classification algorithms and testedthem using various features and parameter settings.We compared the results to select the best algorithmtogether with its best-suited parameter values andfeatures.5.1.1 FeaturesTo differentiate requirements from off-topic con-tent, the sentences will be transformed in numericalfeature vectors using a bag-of-words approach withdifferent settings7.
The features for the transforma-tion are listed along with their possible parametersettings in Table 2.
We can choose whether the fea-ture should be taken from word or character n-grams(a.1).
For both versions, the unit can range between[n,m] (a.2), which can be specified by parameters.Here, we consider all combinations of n = [1, 5] andm = [1, 5] (where m ?
n).
If the feature should bebuild from word n-grams, stopword detection is pos-sible (a.3).
Additionally, terms can be ignored thatreach a document frequency below or above a given7Parameters; to be chosen during algorithm configuration1811threshold (e.g.
domain-specific stopwords) (a.4 anda.5).
Another threshold can be specified to onlyconsider the top features ordered by term frequency(a.6).
Besides, it is possible to re-weight the unitsin the bag-of-words model in relation to the inversedocument frequency (IDF) (a.7).
Moreover, the fre-quency vector can be reduced to binary values (a.8),so that the bag-of-words model only contains infor-mation about the term occurrence but not about itscalculated frequency.
We also consider the lengthof a sentence as feature (b).
Furthermore, the fre-quency of the part-of-speech (POS) tags (c) and thedependencies between the tokens (d) can be added tothe feature vector8.
These two features are optional(c.1 and d.1).
This set of features covers the domain-specific characteristics and should enable the identi-fication of the requirements.# Feature/Parameter Possible Valuesa Bag of wordsa.1 analyzer word, chara.2 ngram range (1,1),(1,2),...,(5,5)a.3 stop words True, Falsea.4 max df [0,8,1,0]a.5 min df [0.0,0.5]a.6 max features int or Nonea.7 use idf True, Falsea.8 binary True, Falseb Length of the sentencec Dependenciesc.1 use dep True, Falsed Part of speechd.1 use pos True, FalseTable 2: Features for on-/off-topic classification together withtheir corresponding parameters5.1.2 Selected AlgorithmsWe selected the following algorithms from thescikit-learn library9 for binary classification: deci-sion tree (DecisionTreeClassifier), Naive8We use spaCy (https://spacy.io) for POS taggingand dependency parsing9http://scikit-learn.orgBayes (BernoulliNB and MultionmialNB),support vector machine (SVC and NuSVC) as wellas ensemble methods (BaggingClassifier,RandomForestClassifier, ExtraTree-Classifier and AdaBoostClassifier).Finally, after evaluating these algorithms, wechose the best one for the classification task (cf.Section 6).5.2 Semantic Annotation TaskFor each identified requirement, the approach shouldannotate the semantic components (cf.
Figure 1).Here, we use text classification techniques on tokenlevel for information extraction purposes.
The ben-efit is that these techniques can automatically learnrules to classify data from the annotated elements(cf.
Section 4).
Each token will be assigned to oneof the semantic categories presented in Figure 1 orthe additional class O (outside according IOB nota-tion).We decided in favor of IO encoding duringclassification to reduce the drawback described inSection 4.
We finally convert the classification re-sults into the IOB encoding by labeling the head ofeach element as begin and the tail as inside.
By us-ing the keywords listed in Section 4 as separators,we further distinguish the beginning and the innerparts of arguments.5.2.1 FeaturesIn the second classification step, we had to adaptthe features to token level.
The goal of feature en-gineering is to capture the characteristics of the to-kens embedded in their surrounding context.
We di-vided the features in four groups: orthographic andsemantic features of the token, contextual features,and traceable classification results.Orthographic features of a token are its graphe-matic representation (a) and additional flags that de-cide if a token contains a number (b), is capitalized(c), or is somehow uppercased (d) (cf.
Table 3).
Forthe graphematic representation, we can choose be-tween the token or the lemma (a.1).
Another or-thographic feature provides information about thelength of the token (e).
Furthermore, we can usethe pre- and suffix characters of the token as fea-tures (f and g).
Their lengths are configurable (f.1and g.1).1812# Feature/Parameter Possible Valuesa Graphematic representationa.1 use lemma True, Falseb Token contains a numberc Token is capitalizedd Token is somehow uppercasede Length of the tokenf Prefix of the tokenf.1 length prefix [0,5]g Suffix of the tokeng.1 length suffix [0,5]Table 3: Orthographic features for semantic annotationFurthermore, we consider the relevance (h), thePOS tag (i) and the WordNet ID of a token (j) asits semantic features (cf.
Table 4).
By checking thestopword status of a token, we can decide about itsrelevance.
Besides, the POS tag of each token isused as feature.
When applying the POS informa-tion, we can choose between the Universal Tag Set10(consisting of 17 POS tags) and the Penn TreebankTag Set11 (including of 36 POS tags) (i.1).
Anotherboolean feature tells us whether the token appearsin WordNet12.
We use this feature as indicator forcomponent or object identification.# Feature/Parameter Possible Valuesh Relevancei Part-of-speech tagi.1 extended tagset True, Falsej WordNet IDTable 4: Semantic features for semantic annotationAs contextual features, we use sentence length(k), index of the token in the sentence (l), as wellas the tagging and dependency parsing informationof the surrounding tokens (m, n and o) (cf.
Table 5).Thus, the POS tags sequences of the n previous and10http://universaldependencies.org/u/pos/11http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html12https://wordnet.princeton.eduthe next m token are considered, where n and m aredefined during algorithm configuration (l.1 and n.1).Moreover, it can be specified if each POS tag shouldbe stored as a single feature or should be concate-nated (e.g.
NOUN+VERB+NOUN) (l.2 and n.2).# Feature/Parameter Possible Valuesk Sentence lengthl Index of the tokenm Previous part-of-speech tagsl.1 n pos prev [0,15]l.2 conc prev pos True, Falsen Subsequent part-of-speech tagsn.1 n pos succ [0,15]n.2 conc succ pos True, Falseo DependenciesTable 5: Contextual features for semantic annotationThe classification task is carried out from left toright in the sentence.
This enables the considera-tion of previous classification results (cf.
Table 6).We implemented two slightly different variants thatcan be combined on demand: Firstly, we can de-fine a fixed number of previous classification resultsas independent or concatenated features (i.e.
a slid-ing window (p)).
Secondly, the number of token al-ready assigned to a particular class may be a valu-able information (q).
This is especially of interestfor the hierarchical structure of the categories: Forinstance, a sub-object should only occur if an objecthas already been identified.
These two features areoptional (p.1 and q.1).
The size of the sliding win-dow will be specified during algorithm configuration(p.2).# Feature/Parameter Possible Valuesp Sliding windowsp.1 conc prev labels True, Falsep.2 window size [0,10]q Number of previous labels per categoryq.1 use prev labels True, FalseTable 6: Traceable classification results for semantic annotation18135.2.2 Selected AlgorithmsIn addition to the classifiers we already used inthe on-/off-topic classification task, we consideredthree sequential learning algorithms: conditionalrandom fields (FrankWolfeSSVM) from the PyS-truct-library13, multinomial hidden markov model(MultinomialHMM) as well as structured percep-tron from the seqlearn-library14.
We could not esti-mate feasible parameter settings for the NuSVC clas-sifier, so that this classifier was ignored.
We chosethe algorithm with the best results on the test set forannotating the requirements (cf.
Section 6).6 EvaluationAs mentioned in Section 5, the data was separatedin a ratio of 4:1 in a training and a test set.
Wetrained all classifiers on the training set with theirdefined settings from the automated algorithm con-figuration.
Subsequently, we evaluated these classi-fiers on the test set.
Our results are shown in Table 7that lists the accuracy for the best classifier per algo-rithm family of the on-/off-topic classification task.The ExtraTreeClassifier performs best onthe test data with an accuracy of 92%.
The accuracywas calculated with the following formula:accuracy = #true positives+#true negatives#classified requirementsThe ExtraTreeClassifier is an implemen-tation of Extremely Randomized Trees (Geurts et al,2006).
We achieved the best result when using char-acter n-grams as features in the model with a fixedlength of 4.
Thereby, we considered term occurrenceinstead of term frequency and IDF.
Before creatingthe bag-of-words model, the approach removes stop-words.
Furthermore, the frequency of the POS tagsand their dependencies are used as features.
In to-tal, the ExtraTreeClassifier used 167 esti-mators based on entropy in the ensemble (algorithm-specific parameters).13https://pystruct.github.io14https://github.com/larsmans/seqlearnClassifier AccuracyAdaBoostClassifier 0.87ExtraTreeClassifier 0.92MultinomialNB 0.89NuSVC 0.90Table 7: Accuracy of best classifiers per algorithm family in theon-/off-topic classification task after algorithm configurationTable 8 shows the values for precision, recall, andF1 of the ExtraTreeClassifier.
In brief, theintroduced approach detects requirements in user-generated content with an average F1-score of 91%.Class Precision Recall F1off-topic info 0.94 0.85 0.89requirements 0.89 0.96 0.93Avg.
0.92 0.91 0.91Table 8: Evaluation results for the on-/off-topic classificationwith the ExtraTreeClassifierTable 9 provides an overview of the results of thesemantic annotation task.
To determine the F1-score,the agreement of the predicted and the a priori givenannotations is necessary to count an element as truepositive.Again, the ExtraTreeClassifier achievesthe best F1-score of 72%.
We gained the best re-sults by using 171 randomized decision trees basedon entropy (algorithm-specific parameters).
As fea-tures, we took the POS tags from Universal Tag Setfor the twelve previous and the three following to-kens.
Traceable classification results are taken intoaccount by a sliding window of size 1.
Besides, wevalidate if a class label has already been assigned.For each considered token, the four prefix and thetwo suffix characters as well as the graphematic rep-resentation of the token are applied as features.The sequential learning algorithms(FrankWolfeSSVM, MultinomialHMM andStructuredPerceptron) perform worsethan the other classifiers.
We assume that thisis due to the small amount of available trainingdata.
However, the methods depending on de-cision trees, especially the ensemble methods(RandomForestClassifier, Bagging-1814Classifier and ExtraTreeClassifier),perform significantly better.Classifier F1AdaBoostClassifier 0.33ExtraTreeClassifier 0.72FrankWolfeSSVM 0.50MultinomialNB 0.64SVC 0.70Table 9: F1-scores of best classifiers per algorithm family in thesemantic annotation task after algorithm configurationIn Table 10, we provide detailed results achievedwith the ExtraTreeClassifier for the differ-ent semantic categories.
The recognition of mainaspects (component, action and object) reached F1-scores of 73%, 80% and 68%.
The semantic cate-gories, that have only a few training examples, aremore error-prone (e.g.
sub-action or sub-object).Semantic Category Precision Recall F1component 0.71 0.75 0.73ref.
of component 0.17 0.14 0.15action 0.78 0.82 0.80arg.
of action 0.49 0.62 0.54condition 0.88 0.61 0.72priority 0.96 0.96 0.96motivation 0.67 0.29 0.40role 0.93 0.86 0.89object 0.63 0.74 0.68ref.
of object 0.69 0.51 0.59sub-action 0.46 0.44 0.45arg.
of sub-action 0.33 0.29 0.31sub-priority 0.44 0.57 0.50sub-role 0.40 0.80 0.53sub-object 0.35 0.33 0.34ref.
of sub-object 0.67 0.33 0.44Avg.
0.72 0.73 0.73Table 10: Evaluation results for the semantic annotation withthe ExtraTreeClassifier7 Conclusion and Future WorkRequirement engineers and software developershave to meet users?
wishes to create new softwareproducts.
The goal of this work was to develop asystem that can identify and analyze requirementsexpressed in natural language.
These are writtenby users unlimited in their way of expression.
Oursystem REaCT achieves an accuracy of 92% in dis-tinguishing between on- and off-topic informationin the user-generated requirement descriptions.
Thetext classification approach for semantic annotationreaches an F1-score of 72% ?
a satisfying resultcompared to the inter-annotator agreement of 80%.One possibility to improve the quality of the seman-tic annotation is to expand the training set.
Espe-cially the sequential learning techniques need moretraining data.
Besides, this would have a positive im-pact on those semantic categories that only containa small number of annotated elements.Developers and requirement engineers canfacilely identify requirements written by users forproducts in different scenarios by applying ourapproach.
Moreover, the semantic annotationsare useful for further NLP tasks.
User-generatedsoftware requirements adhere to the same qualitystandards as software requirements that are col-lected and revised by experts: They should becomplete, unambiguous and consistent (Hsia et al,1993).
Since there was no assistant system to checkthe quality for many years (Hussain et al, 2007)we plan to extend the provided system in orderto provide some quality analysis of the extractedinformation.
We have already developed conceptsto generate suggestions for non-experts, how tocomplete or clarify their requirement descriptions(Geierhos et al, 2015).
Based on these insights, wewant to implement a system for the resolution ofvagueness and incompleteness of NL requirements.AcknowledgmentsSpecial thanks to our colleagues Frederik S. Ba?umerand David Kopecki for their support during the se-mantic annotation of the requirements.
This workwas partially supported by the German ResearchFoundation (DFG) within the Collaborative Re-search Centre ?On-The-Fly Computing?
(SFB 901).1815ReferencesVincenzo Ambriola and Vincenzo Gervasi.
2006.
Onthe Systematic Analysis of Natural Language Require-ments with CIRCE.
Automated Software Engineering,13(1):107?167.Ronit Ankori and Ronit Ankori.
2005.
Automatic re-quirements elicitation in agile processes.
In Pro-ceedings of the 2005 IEEE International Conferenceon Software - Science, Technology and Engineering,pages 101?109.
IEEE.Carlos Castro-Herrera, Chuan Duan, Jane Cleland-Huang, and Bamshad Mobasher.
2009.
A recom-mender system for requirements elicitation in large-scale software projects.
In Proceedings of the 2009ACM Symposium on Applied Computing, pages 1419?1426.
ACM.Nancy A. Chinchor, editor.
1998.
Proceedings of theSeventh Message Understanding Conference (MUC-7)Named Entity Task Definition, Fairfax, VA.Mike Cohn.
2004.
User Stories Applied: For Agile Soft-ware Development.
Addison Wesley Longman Pub-lishing Co., Redwood City, CA, USA.David de Almeida Ferreira and Alberto Rodriguesda Silva.
2012.
RSLingo: An information extrac-tion approach toward formal requirements specifica-tions.
In Model-Driven Requirements EngineeringWorkshop, pages 39?48.
IEEE.Donald G. Firesmith.
2005.
Are Your RequirementsComplete?
Journal of Object Technology, 4(2):27?43,February.Ricardo Gacitua, Pete Sawyer, and Vincenzo Gervasi.2011.
Relevance-based abstraction identification:technique and evaluation.
Requirements Engineering,16(3):251?265.Michaela Geierhos, Sabine Schulze, and Frederik SimonBa?umer.
2015.
What did you mean?
Facing the Chal-lenges of User-generated Software Requirements.
InProceedings of the 7th International Conference onAgents and Artificial Intelligence, pages 277?283, 10 -12 January.
Lisbon.
ISBN: 978-989-758-073-4.Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006.Extremely randomized trees.
Machine Learning,63(1):3?42.Leah Goldin and Daniel M. Berry.
1994.
AbstFinder,A Prototype Abstraction Finder for Natural LanguageText for Use in Requirements Elicitation: Design,Methodology, and Evaluation.
Automated SoftwareEngineering, 4(4):375?412.H.M.
Harmain and R. Gaizauskas.
2003.
CM-Builder:A Natural Language-Based CASE Tool for Object-Oriented Analysis.
IEEE International Conferenceon Software - Science, Technology & Engineering,10(2):157?181.Pei Hsia, Alan Davis, and David Kung.
1993.
StatusReport: Requirements Engineering.
IEEE Software,10(6):75?79, November.Ishrar Hussain, Olga Ormandjieva, and Leila Kosseim.2007.
Automatic Quality Assessment of SRS Text byMeans of a Decision-Tree-Based Text Classifier.
InProceedings of the 7th International Conference onQuality Software, QSIC ?07, pages 209?218.
IEEE.Isabel John and Jo?rg Do?rr.
2003.
Elicitation of Require-ments from User Documentation.
In Proceedings ofthe 9th International Workshop on Requirements En-gineering: Foundation of Software Quality, pages 17?26.
Springer.Daniel Jurafsky and James H Martin.
2015.
Semanticrole labeling.
In Speech and Language Processing.
3rded.
draft edition.Sven J. Ko?rner and Tom Gelhausen.
2008.
ImprovingAutomatic Model Creation using Ontologies.
In Pro-ceedings of the 20th International Conference on Soft-ware Engineering & Knowledge Engineering, pages691?696.
Knowledge Systems Institute.Jim McCall.
1977.
McCall?s Qual-ity Model.
http://www.sqa.net/softwarequalityattributes.html.Luisa Mich, Mariangela Franch, and Pier Luigi Novi In-verardi.
2004.
Market research for requirements anal-ysis using linguistic tools.
Requirements Engineering,9(2):151?151.Luisa Mich. 1996.
NL-OOPS: from natural languageto object oriented requirements using the natural lan-guage processing system LOLITA.
Natural LanguageEngineering, 2:161?187.James Robertson and Suzanne Robertson.
2012.
Mas-tering the Requirements Process.
Getting Require-ments Right.
Addison-Wesley Publishing, New York,NY, USA.Peter Sawyer, Paul Rayson, and Roger Garside.
2002.REVERE: support for requirements synthesis fromdocuments.
Information Systems Frontiers, 4(3):343?353.Walter F. Tichy, Mathias Landha?u?er, and Sven J. Ko?rner.2015.
nlrpBENCH: A Benchmark for Natural Lan-guage Requirements Processing.
In MultikonferenzSoftware Engineering & Management 2015.
GI.Radu Vlas and William N. Robinson.
2011.
A Rule-Based Natural Language Technique for RequirementsDiscovery and Classification in Open-Source SoftwareDevelopment Projects.
In Proceedings of the 44thHawaii International Conference on System Sciences,pages 1?10.
IEEE.Tao Yue, Lionel C. Briand, and Yvan Labiche.
2010.A systematic review of transformation approaches be-tween user requirements and analysis models.
Re-quirements Engineering, 16(2):75?99.1816
