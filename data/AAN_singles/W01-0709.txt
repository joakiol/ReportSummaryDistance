A Mixture-of-Experts Framework for Text ClassificationAndrew Estabrooks Nathalie JapkowiczIBM Toronto Lab, Office 1B28B, SITE, University of Ottawa,1150 Eglinton Avenue East, 150 Louis Pasteur, P.O.
Box 450 Stn.
A,North York, Ontario, Canada, M3C 1H7 Ottawa, Ontario, Canada K1N 6N5aestabro@ca.ibm.com nat@site.uottawa.caAbstractOne of the particular characteristicsof text classification tasks is that theypresent large class imbalances.
Such aproblem can easily be tackled using re-sampling methods.
However, althoughthese approaches are very simple to im-plement, tuning them most effectivelyis not an easy task.
In particular,it is unclear whether oversampling ismore effective than undersampling andwhich oversampling or undersamplingrate should be used.
This paper presentsa method for combining different ex-pressions of the re-sampling approachin a mixture of experts framework.
Theproposed combination scheme is evalu-ated on a very imbalanced subset of theREUTERS-21578 text collection and isshown to be very effective on this do-main.1 IntroductionA typical use of Machine Learning methods inthe context of Natural Language Processing is inthe domain of text classification.
Unfortunately,several characteristics specific to text data makeits classification a difficult problem to handle.
Inparticular, the data is typically highly dimensionaland it presents a large class imbalance, i.e., there,typically, are very few documents on the topic ofinterest while texts on unrelated subjects abound.Furthermore, although large amounts of texts areavailable on line, little of them are labeled.
Be-cause the class imbalance problem is known tonegatively affect typical classifiers and becauseunlabeled data have no place in conventional su-pervised learning, using off-the-shelf supervisedclassifiers is likely not to be very successful inthe context of text data.
It is, instead, recom-mended to devise a classification method specifi-cally tuned to the text classification problem.The purpose of this study is to target some ofthe characteristics of text data in the hope of im-proving the effectiveness of the classification pro-cess.
The topics of finding a good representa-tion for text data and dealing with its high di-mensionality have been investigated previouslywith, for example, the use of Wordnet [e.g., (Scott& Matwin, 1999)] and Support Vector Machines[e.g., (Joachims, 1998)], respectively.
We will notbe addressing these problems here.
The questionthat we will tackle in this paper, instead, is thatof dealing with the class imbalance, and, in theprocess of doing so, that of finding a way to takeadvantage of the extra, albeit, unlabeled data thatare often left unused in classification studies.1Several approaches have previously been pro-posed to deal with the class imbalance problemincluding a simple and yet quite effective method:re-sampling [e.g., (Lewis & Gale, 1994), (Kubat& Matwin, 1997), (Domingos, 1999)].
This paperdeals with the two different types of re-samplingapproaches: methods that oversample the smallclass in order to make it reach a size close to thatof the larger class and methods that undersamplethe large class in order to make it reach a sizeclose to that of the smaller class.
Because it isunclear whether oversampling is more effectivethan undersampling and which oversampling orundersampling rate should be used, we propose a1Note, however, that unlabeled data is not always left un-used as in the work on co-learning of (Blum & Mitchell,1998).
As discussed below, however, our approach willmake use of the unlabeled data in a different way.method for combining a number of classifiers thatoversample and undersample the data at differ-ent rates in a mixture of experts framework.
Themixture-of-experts is constructed in the context ofa decision tree induction system: C5.0, and all re-sampling is done randomly.
This proposed com-bination scheme is, subsequently, evaluated on aa subset of the REUTERS-21578 text collectionand is shown to be very effective in this case.The remainder of this paper is divided intofour sections.
Section 2 describes an experi-mental study on a series of artificial data setsto explore the effect of oversampling and under-sampling and oversampling or undersampling atdifferent rates.
This study suggests a mixture-of-experts scheme which is described in Section3.
Section 4 discusses the experiment conductedwith that mixture-of-experts scheme on a seriesof text-classification tasks and discusses their re-sults.
Section 5 is the conclusion.2 Experimental StudyWe begin this work by studying the effects ofoversampling versus undersampling and over-sampling or undersampling at different rates.2 Allthe experiments in this part of the paper are con-ducted over artificial data sets defined over the do-main of 4 x 7 DNF expressions, where the firstnumber represents the number of literals presentin each disjunct and the second number representsthe number of disjuncts in each concept.3 Weused an alphabet of size 50.
For each concept,we created a training set containing 240 positiveand 6000 negative examples.
In other words, we2Throughout this work, we consider a fixed imbalanceratio, a fixed number of training examples and a fixed degreeof concept complexity.
A thorough study relating differentdegrees of imbalance ratios, training set sizes and conceptdifficulty was previously reported in (Japkowicz, 2000).3DNF expressions were specifically chosen because oftheir simplicity as well as their similarity to text data whoseclassification accuracy we are ultimately interested in im-proving.
In particular, like in the case of text-classification,DNF concepts of interest are, generally, represented bymuch fewer examples than there are counter-examples ofthese concepts, especially when 1) the concept at hand isfairly specific; 2) the number of disjuncts and literals per dis-junct grows larger; and 3) the values assumed by the literalsare drawn from a large alphabet.
Furthermore, an impor-tant aspect of concept complexity can be expressed in sim-ilar ways in DNF and textual concepts since adding a newsubtopic to a textual concept corresponds to adding a newdisjunct to a DNF concept.0102030405060Error over Positive Data          Error over Negative DataError Rate(%)ImbalancedRe?SamplingDown?SizingFigure 1: Re-Sampling versus Downsizingcreated an imbalance ratio of 1:25 in favor of thenegative class.2.1 Re-Sampling versus DownsizingIn this part of our study, three sets of experimentswere conducted.
First, we trained and tested C5.0on the 4x7 DNF 1:25 imbalanced data sets justmentioned.4 Second, we randomly oversampledthe positive class, until its size reached the sizeof the negative class, i.e., 6000 examples.
Theadded examples were straight copies of the datain the original positive class, with no noise added.Finally, we undersampled the negative class byrandomly eliminating data points from the neg-ative class until it reached the size of the positiveclass or, 240 data points.
Here again, we useda straightforward random approach for selectingthe points to be eliminated.
Each experiment wasrepeated 50 times on different 4x7 DNF conceptsand using different oversampled or removed ex-amples.
After each training session, C5.0 wastested on separate testing sets containing 1,200positive and 1,200 negative examples.
The aver-age accuracy results are reported in Figure 1.
Theleft side of Figure 1 shows the results obtained onthe positive testing set while its right side showsthe results obtained on the negative testing set.As can be expected, the results show that thenumber of false negatives (results over the pos-4(Estabrooks, 2000) reports results on 4 other conceptsizes.
An imbalanced ratio of 1:5 was also tried in prelim-inary experiments and caused a loss of accuracy about aslarge as the 1:25 ratio.
Imbalanced ratios greater than 1:25were not tried on this particular problem since we did notwant to confuse the imbalance problem with the small sam-ple problem.itive class) is a lot higher than the number offalse positives (results over the negative class).
Aswell, the results suggest that both naive oversam-pling and undersampling are helpful for reducingthe error caused by the class imbalance on thisproblem although oversampling appears more ac-curate than undersampling.52.2.
Re-Sampling and Down-Sizing at variousRatesIn order to find out what happens when differentsampling rates are used, we continued using theimbalanced data sets of the previous section, butrather than simply oversampling and undersam-pling them by equalizing the size of the positiveand the negative set, we oversampled and under-sampled them at different rates.
In particular, wedivided the difference between the size of the pos-itive and negative training sets by 10 and used thisvalue as an increment in our oversampling and un-dersampling experiments.
We chose to make the100% oversampling rate correspond to the fullyoversampled data sets of the previous section butto make the 90% undersampled rate correspond tothe fully undersampled data sets of the previoussection.6 For example, data sets with a 10% over-sampling rate contain   fffi	 positive examples and 6,000 negative exam-ples.
Conversely, data sets with a 0% under-sampling rate contain 240 positive examples and6,000 negative ones while data sets with a 10%undersampling rate contain 240 positive examplesandflffi  !#"  negativeexamples.
A 0% oversampling rate and a 90%undersampling rate correspond to the fully imbal-anced data sets designed in the previous sectionwhile a 100% undersampling rate corresponds tothe case where no negative examples are presentin the training set.Once again, and for each oversampling and un-dersampling rate, the rules learned by C5.0 on thetraining sets were tested on testing sets contain-ing 1,200 positive and 1,200 negative examples.5Note that the usefulness of oversampling versus under-sampling is problem dependent.
(Domingos, 1999), for ex-ample, finds that in some experiments, oversampling is moreeffective than undersampling, although in many cases, theopposite can be observed.6This was done so that no classifier was duplicated in ourcombination scheme.
(See Section 3)The results of our experiments are displayed inFigure 2 for the case of oversampling and under-sampling respectively.
They represent the aver-ages of 50 trials.
Again, the results are reportedseparately for the positive and the negative testingsets.0 10 20 30 40 50 60 70 80 90 1000102030405060708090100Downsizing/Oversampling Rate (%)Error Rate(%)Downsize ??
Positive ErrorDownsize ??
Negative ErrorRe?Sample ??
Positive ErrorRe?Sample ??
Negative ErrorCardinalBalancesFigure 2: Oversampling and Downsizing at Dif-ferent RatesThese results suggest that different samplingrates have different effects on the accuracy ofC5.0 on imbalanced data sets for both the over-sampling and the undersampling method.
In par-ticular, the following observation can be made:Oversampling or undersampling until acardinal balance of the two classes isreached is not necessarily the best strat-egy: best accuracies are reached beforethe two sets are cardinally balanced.In more detail, this observation comes from thefact that in both the oversampling and under-sampling curves of figure 2 the optimal accuracyis not obtained when the positive and the neg-ative classes have the same size.
In the over-sampling curves, where class equality is reachedat the 100% oversampling rate, the average er-ror rate obtained on the data sets over the posi-tive class at that point is 35.3% (it is of 0.45%over the negative class) whereas the optimal errorrate is obtained at a sampling rate of 70% (withan error rate of 22.23% over the positive classand of 0.56% over the negative class).
Similarly,although less significantly, in the undersamplingcurves, where class equality is reached at the 90%undersampling rate7, the average error rate ob-7The sharp increase in error rate taking place at the 100%tained at that point is worse than the one obtainedat a sampling rate of 80% since although the errorrate is the same over the positive class (at 38.72%)it went from 1.84% at 90% oversampling over thenegative class to 7.93%.8In general, it is quite likely that the optimalsampling rates can vary in a way that might notbe predictable for various approaches and prob-lems.3 The Mixture-of-Experts SchemeThe results obtained in the previous section sug-gest that it might be useful to combine oversam-pling and undersampling versions of C5.0 sam-pled at different rates.
On the one hand, the com-bination of the oversampling and undersamplingstrategies may be useful given the fact that thetwo approaches are both useful in the presence ofimbalanced data sets (cf.
results of Section 2.1)and may learn a same concept in different ways.9On the other hand, the combination of classifiersusing different oversampling and undersamplingrates may be useful since we may not be able topredict, in advance, which rate is optimal (cf.
re-sults of Section 2.2).We will now describe the combination schemewe designed to deal with the class imbalanceproblem.
This combination scheme will be testedon a subset of the REUTERS-21578 text classifi-cation domain.103.1 ArchitectureA combination scheme for inductive learningconsists of two parts.
On the one hand, we mustdecide which classifiers will be combined and onthe other hand, we must decide how these classi-fiers will be combined.
We begin our discussionwith a description of the architecture of our mix-ture of experts scheme.
This discussion explainsundersampling point is caused by the fact that at this point,no negative examples are present in the training set.8Further results illustrating this point over different con-cept sizes can also be found in (Estabrooks, 2000).9In fact, further results comparing C5.0?s rule sizes ineach case suggest that the two methods, indeed, do tacklethe problem differently [see, (Estabrooks, 2000)].10This combination scheme was first tested on DNF ar-tificial domains and improved classification accuracy by 52to 62% over the positive data and decreased the classifica-tion accuracy by only 7.5 to 13.1% over the negative classas compared to the accuracy of a single C5.0 classifier.
See(Estabrooks, 2000) for more detail.which classifiers are combined and gives a gen-eral idea of how they are combined.
The specificsof our combination scheme are motivated and ex-plained in the subsequent section.In order for a combination method to be effec-tive, it is necessary for the various classifiers thatconstitute the combination to make different de-cisions (Hansen, 1990).
The experiments in Sec-tion 2 of this paper suggest that undersamplingand oversampling at different rates will produceclassifiers able to make different decisions, in-cluding some corresponding to the ?optimal?
un-dersampling or oversampling rates that could nothave been predicted in advance.
This suggestsa 3-level hierarchical combination approach con-sisting of the output level, which combines the re-sults of the oversampling and undersampling ex-perts located at the expert level, which themselveseach combine the results of 10 classifiers locatedat the classifier level and trained on data sets sam-pled at different rates.
In particular, the 10 over-sampling classifiers oversample the data at rates10%, 20%, ... 100% (the positive class is over-sampled until the two classes are of the same size)and the 10 undersampling classifiers undersam-ple the negative class at rate 0%, 10%, ..., 90%(the negative class is undersampled until the twoclasses are of the same size).
Figure 3 illustratesthe architecture of this combination scheme thatwas motivated by (Shimshoni & Intrator, 1998)?sIntegrated Classification Machine.113.2 Detailed Combination SchemeOur combination scheme is based on two differentfacts:Fact #1: Within a single testing set, differenttesting points could be best classified by dif-ferent single classifiers.
(This is a generalfact that can be true for any problem and anyset of classifiers).Fact #2: In class imbalanced domains for whichthe positive training set is small and the neg-ative training set is large, classifiers tend tomake many false-negative errors.
(This is11However, (Shimshoni & Intrator, 1998) is a general ar-chitecture.
It was not tuned to the imbalance problem, nordid it take into consideration the use of oversampling andundersampling to inject principled variance into the differ-ent classifiers.Figure 3: Re-Sampling versus Downsizinga well-known fact often reported in the lit-erature on the class-imbalance problem andwhich was illustrated in Figure 1, above).In order to deal with the first fact, we decidednot to average the outcome of different classi-fiers by letting them vote on a given testing point,but rather to let a single ?good enough?
classi-fier make a decision on that point.
The classi-fier selected for a single data point needs not bethe same as the one selected for a different datapoint.
In general, letting a single, rather than sev-eral classifiers decide on a data point is based onthe assumption that the instance space may be di-vided into non-overlapping areas, each best clas-sified by a different expert.
In such a case, av-eraging the result of different classifiers may notyield the best solution.
We, thus, created a com-bination scheme that allowed single but differentclassifiers to make a decision for each point.Of course, such an approach is dangerous giventhat if the single classifier chosen to make a deci-sion on a data point is not reliable, the result forthis data point has a good chance of being unreli-able as well.
In order to prevent such a problem,we designed an elimination procedure geared atpreventing any unfit classifier present at our ar-chitecture?s classification level from participatingin the decision-making process.
This eliminationprogram relies on our second fact in that it invali-dates any classifier labeling too many examples aspositive.
Since the classifiers of the combinationscheme have a tendency of being naturally biasedtowards classifying the examples as negative, weassume that a classifier making too many positivedecision is probably doing so unreliably.In more detail, our combination scheme con-sists of$ a combination scheme applied to each expertat the expert level$ a combination scheme applied at the outputlevel$ an elimination scheme applied to the classi-fier levelThe expert and output level combinationschemes use the same very simple heuristic: ifone of the non-eliminated classifiers decides thatan example is positive, so does the expert to whichthis classifier belongs.
Similarly, if one of the twoexperts decides (based on its classifiers?
decision)that an example is positive, so does the outputlevel, and thus, the example is classified as pos-itive by the overall system.The elimination scheme used at the classifierlevel uses the following heuristic: the first (mostimbalanced) and the last (most balanced) classi-fiers of each expert are tested on an unlabeleddata set.
The number of positive classificationseach classifier makes on the unlabeled data set isrecorded and averaged and this average is takenas the threshold that none of the expert?s classi-fiers must cross.
In other words, any classifierthat classifies more unlabeled data points as pos-itive than the threshold established for the expertto which this classifier belongs needs to be dis-carded.12It is important to note that, at the expert andoutput level, our combination scheme is heav-ily biased towards the positive under-representedclass.
This was done as a way to compensatefor the natural bias against the positive class em-bodied by the individual classifiers trained on theclass imbalanced domain.
This heavy positivebias, however, is mitigated by our elimination12Because no labels are present, this technique constitutesan educated guess of what an appropriate threshold shouldbe.
This heuristic was tested in (Estabrooks, 2000) on thetext classification task discussed below and was shown toimprove the system (over the combination scheme not usingthis heuristic) by 3.2% when measured according to the %'&measure, 0.36% when measured according to the %)( mea-sure, and 5.73% when measured according to the %ff*,+ - mea-sure.
See the next section, for a definition of the %).
mea-sures, but note that the higher the % .
value, the better.scheme which strenuously eliminates any classi-fier believed to be too biased towards the positiveclass.4 Experiments on a Text ClassificationTaskOur combination scheme was tested on a subsetof the 10 top categories of the REUTERS-21578Data Set.
We first present an overview of the data,followed by the results obtained by our scheme onthese data.4.1 The Reuters-21578 DataThe ten largest categories of the Reuters-21578data set consist of the documents included in theclasses of financial topics listed in Table 1:Class.
Document CountEarn 3987ACQ 2448MoneyFx 801Grain 628Crude 634Trade 551Interest 513Wheat 306Ship 305Corn 254Table 1: The top 10 Reuters-21578 categoriesSeveral typical pre-processing steps were takento prepare the data for classification.
First, thedata was divided according to the ModApte splitwhich consists of considering all labelled docu-ments published before 04/07/87 as training data(9603 documents, altogether) and all labelleddocuments published on or after 04/07/87 as test-ing data (3299 documents altogether).
The unla-belled documents represent 8676 documents andwere used during the classifier elimination step.Second, the documents were transformed intofeature vectors in several steps.
Specifically, allthe punctuation and numbers were removed andthe documents were filtered through a stop wordlist13.
The words in each document were then13The stop word list was obtained at:http://www.dcs.gla.ac.uk/idom/it resources/linguistic utils/stop-words.stemmed using the Lovins stemmer14 and the 500most frequently occurring features were used asthe dictionary for the bag-of-word vectors repre-senting each documents.15 Finally, the data setwas divided into 10 concept learning problemswhere each problem consisted of a positive classcontaining 100 examples sampled from a singletop 10 Reuters topic class and a negative classcontaining the union of all the examples con-tained in the other 9 top 10 Reuters classes.
Di-viding the Reuters multi-class data set into a se-ries of two-class problems is typically done be-cause considering the problem as a straight mul-ticlass classification problem causes difficultiesdue to the high class overlapping rate of the doc-uments, i.e., it is not uncommon for a documentto belong to several classes simultaneously.
Fur-thermore, although the Reuters Data set containsmore than 100 examples in each of its top 10 cat-egories (see Table 1), we found it more realisticto use a restricted number of positive examples.16Having restricted the number of positive exam-ples in each problem, it is interesting to note thatthe class imbalances in these problems is veryhigh since it ranges from an imbalance ratio of1:60 to one of 1:100 in favour of the negativeclass.4.2 ResultsThe results obtained by our scheme on these datawere pitted against those of C5.0 ran with theAda-boost option.17 The results of these exper-14The Lovins stemmer was obtained from:ftp://n106.isitokushima-u.ac.ip/pub/IR/Iterated-Lovins-stemmer15A dictionary of 500 words is smaller than the typicalnumber of words used (see, for example, (Scott & Matwin1999)), however, it was shown that this restricted size didnot affect the results too negatively while it did reduce pro-cessing time quite significantly (see (Estabrooks 2000)).16Indeed, very often in practical situations, we only haveaccess to a small number of articles labeled ?of interest?whereas huge number of documents ?of no interest?
areavailable17Our scheme was compared to C5.0 ran with the Ada-boost option combining 20 classifiers.
This was done inorder to present a fair comparison to our approach whichalso uses 20 classifiers.
It turns out, however, that the Ada-boost option provided only a marginal improvement over us-ing a single version of C5.0 (which itself compares favorablyto state-of-the-art approaches for this problem) (Estabrooks,2000).
Please, note that other experiments using C5.0 withthe Ada-boost option combining fewer or more classifiersshould be attempted as well since 20 classifiers might not beiments are reported in Figure 4 as a function ofthe micro-averaged (over the 10 different classi-fication problems) /10 , /32 and /3465 7 measures.
Inmore detail, the /98 -measure is defined as:/ 8;: 8(<0>=,?
@1?ffA8(?
@<Awhere P represents precision, and R, recall, whichare respectively defined as follows:B C'DFEHG@JIKMLONPLOQGKC'DFEHG@JIKMLONPLOQGK<SRJTVUKG@JIKWLXNPLOQGKY[ZC\D,E]G@JIKWLXNPLOQGKC'DFEHG@JIFKWLONPLOQGK<SRJT^UKGF_GW`TNPLOQGKIn other words, precision corresponds to the pro-portion of examples classified as positive that aretruly positive; recall corresponds to the proportionof truly positive examples that are classified aspositive; the /98 -measure combines the precisionand recall by a ratio specified by a .
If a b ,then precision and recall are considered as beingof equal importance.
If a c  , then recall is con-sidered to be twice as important as precision.
Ifacffde", then precision is considered to be twiceas important as recall.Because 10 different results are obtained foreach value of B and each combination system(1 result per classification problem), these resultshad to be averaged in order to be presented in thegraph of Figure 4.
We used the Micro-averagingtechnique which consists of a straight average ofthe F-Measures obtained in all the problems, byeach combination system, and for each value of B.Using Micro-averaging has the advantage of giv-ing each problem the same weight, independentlyof the number of positive examples they contain.The results in Figure 4 show that our combi-nation scheme is much more effective than Ada-boost on both recall and precision.
Indeed, Ada-boost gets an /0 measure of 52.3% on the dataset while our combination scheme gets an /10measure of 72.25%.
If recall is considered astwice more important than precision, the resultsare even better.
Indeed, the mixture-of-expertsscheme gets an /32 -measure of 75.9% while Ada-boost obtains an / 2 -measure of 48.5%.
On theother hand, if precision is considered as twicemore important than recall, then the combina-tion scheme is still effective, but not as effectiveC5.0-Ada-boost?s optimal number on our problem.00.10.20.30.40.50.60.70.8AdaBoost versus the Mixture?of?Experts SchemeF?MeasureAdaBoostOurSchemeF?1 F?2 F?0.5Figure 4: Average results obtained by Ada-Boostand the Mixture-of-Experts scheme on 10 textclassification problemswith respect to Ada-boost since it brings the /f465 7 -measure on the reduced data set to only 73.61%,whereas Ada-Boost?s performance amounts to64.9%.The generally better performance displayed byour proposed system when evaluated using the/32 -measure and its generally worse performancewhen evaluated using the /3465 7 -measure are notsurprising, since we biased our system so that itclassifies more data points as positive.
In otherwords, it is expected that our system will cor-rectly discover new positive examples that werenot discovered by Ada-Boost, but will incorrectlylabel as positive examples that are not positive.Overall, however, the results of our approach arequite positive with respect to both precision andrecall.
Furthermore, it is important to note thatthis method is not particularly computationallyintensive.
In particular, its computation costs arecomparable to those of commonly used combina-tion methods, such as AdaBoost.5 Conclusion and Future WorkThis paper presented an approach for dealingwith the class-imbalance problem that consistedof combining different expressions of re-samplingbased classifiers in an informed fashion.
In par-ticular, our combination system was built so as tobias the classifiers towards the positive set so ascounteract the negative bias typically developedby classifiers facing a higher proportion of nega-tive than positive examples.
The positive bias weincluded was carefully regulated by an elimina-tion strategy designed to prevent unreliable clas-sifiers to participate in the process.
The techniquewas shown to be very effective on a drasticallyimbalanced version of a subset of the REUTERStext classification task.There are different ways in which this studycould be expanded in the future.
First, our tech-nique was used in the context of a very naive over-sampling and undersampling scheme.
It would beuseful to apply our scheme to more sophisticatedre-sampling approaches such as those of (Lewis &Gale, 1994) and (Kubat & Matwin, 1997).
Sec-ond, it would be interesting to find out whetherour combination approach could also improve oncost-sensitive techniques previously designed.
Fi-nally, we would like to test our technique on otherdomains presenting a large class imbalance.AcknowledgementsWe would like to thank Rob Holte and ChrisDrummond for their valuable comments.
This re-search was funded in part by an NSERC grant.The work described in this paper was conductedat Dalhousie University.ReferencesBlum, A. and Mitchell, T. (1998): Combining Labeledand Unlabeled Data with Co-Training Proceedingsof the 1998 Conference on Computational LearningTheory.Domingos, Pedro (1999): Metacost: A generalmethod for making classifiers cost sensitive, Pro-ceedings of the Fifth International Conference onKnowledge Discovery and Data Mining, 155?164.Estabrooks, Andrew (2000): A Combination Schemefor Inductive Learning from Imbalanced Data Sets,MCS Thesis, Faculty of Computer Science, Dal-housie University.Hansen, L. K. and Salamon, P. (1990): Neural Net-work Ensembles, IEEE Transactions on PatternAnalysis and Machine Intelligence, 12(10), 993?1001.Japkowicz, Nathalie (2000): The Class ImbalanceProblem: Significance and Strategies, Proceedingsof the 2000 International Conference on ArtificialIntelligence (IC-AI?2000), 111?117.Joachims, T. (1998): Text Categorization with SupportVector Machines: Learning with many relevant fea-tures, Proceedings of the 1998 European Confer-ence on Machine Learning.Kubat, Miroslav and Matwin, Stan (1997): Address-ing the Curse of Imbalanced Data Sets: One-SidedSampling, Proceedings of the Fourteenth Interna-tional Conference on Machine Learning, 179?186.Lewis, D. and Gale, W. (1994): Training Text Classi-fiers by Uncertainty Sampling, Proceedings of theSeventh Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval.Scott, Sam and Matwin, Stan (1999): Feature En-gineering for Text Classification, Proceedings ofthe Sixteenth International Conference on MachineLearning, 379?388.Shimshoni, Y. and Intrator, N. (1998): ClassifyingSeismic Signals by Integrating Ensembles of NeuralNetworks, IEEE Transactions On Signal Process-ing, Special issue on NN.
