Two-Phase Shift-Reduce Deterministic Dependency Parser of ChineseMeixun Jin, Mi-Young Kim and Jong-Hyeok LeeDiv.
of Electrical and Computer Engineering,Pohang University of Science and Technology (POSTECH)Advanced Information Technology Research Center (AITrc){Meixunj, colorful, jhlee}@postech.ac.krAbstractIn the Chinese language, a verb mayhave its dependents on its left, right or onboth sides.
The ambiguity resolution ofright-side dependencies is essential for de-pendency parsing of sentences with two ormore verbs.
Previous works on shift-reduce dependency parsers may not guar-antee the connectivity of a dependency treedue to their weakness at resolving theright-side dependencies.
This paper pro-poses a two-phase shift-reduce dependencyparser based on SVM learning.
The left-side dependents and right-side nominal de-pendents are detected in Phase I, and right-side verbal dependents are decided inPhase II.
In experimental evaluation, ourproposed method outperforms previousshift-reduce dependency parsers for theChine language, showing improvement ofdependency accuracy by 10.08%.1 IntroductionDependency parsing describes syntactic struc-ture of a sentence in terms of links between in-dividual words rather than constituency trees.The fundamental relation in dependency parsingis between head and dependent.
Robinson[1]formulates four axioms to the well-formed de-pendency structures, known as single headed,acyclic, connective and projective.In this paper, we present a dependency pars-ing strategy that produces one dependency struc-ture that satisfies all these constraints.This paper is organized as follows.
Relatedworks are introduced in section 2.
In section 3,detailed analysis of the work of Nivre[2] andYamada[3] are given.
Then our parsing strategyis introduced.
In section 4, experiments and re-sults are delivered.
Finally a conclusion will begiven in section 5.2 Overview of Related WorksMost nature language grammars tend to as-sign many possible syntactic structures to thesame input utterance.
A parser should output asingle analysis for each sentence.
The task ofselecting one single analysis for a given sen-tence is known as disambiguation.Some of the parsing strategies first produceall possible trees for a sentence.
The disam-biguation work is done in the end by searchingthe most probable one through parsing tree for-est.
Statistical parsers employ probability as adisambiguation measure and output the tree withthe highest probability[4,5].
However, in thework of Collins [6], 42% of the correct parsetrees were not in the candidate pool of ~30-bestparses.
Disambiguation work by searchingthroughout the parsing tree forest has limitations.The alternative way is to disambiguate at eachparsing step and output the parsing result deter-ministically.
Nivre[2] and Yamada[3] suggest ashift-reduce like dependency parsing strategy.
Insection 3.1 we give a detailed analysis of theirapproach.There are several approaches for dependencyparsing on Chinese text.
Ma[5] and Cheng[18]are examples of these approaches.
The trainingand test set Ma[5] used, are not sufficient toprove the reliability of Ma?s[5] approach.
On theframe of parsing Chinese with CFG, there areseveral approaches to apply the original Englishparsing strategies to Chinese [7,8,9].
The poten-tial purposes of these works are to take advan-tage of state-of-art English parsing strategy andto find a way to apply it to Chinese text.
Due tothe differences between Chinese and English,256the performance of the system on Chinese isabout 10% lower comparing the performance ofthe original system.3 Two-Phase Dependency Parsing3.1  Review of Previous Shift-Reduce DependencyParsersNivre[3] presented a shift-reduce dependencyparsing algorithm which can parse in linear time.The Nivre?s parser was represented by a triples<S, I, A>, where S is a stack, I is a list of (re-maining) input tokens, and A is the set of deter-mined dependency relations.
Nivre defined fourtransitions: Left-Arc, Right-Arc, Reduce, andShift.
If there is a dependency relation betweenthe top word of the stack and the input word,according to the direction of the dependency arc,it can be either Left-Arc or Right-Arc.
Otherwise,the transition can be either shift or reduce.
If thehead of the top word of the stack is already de-termined, then the transition is reduce, otherwiseshift.
The action of each transition is shown inFig.1.
For details, please refer to Nivre[3,10].Fig.2 gives an example1 of parsing a Chinesesentence using Nivre?s algorithm.Nivre?s[3,10] approach has several advan-tages.
First, the dependency structure producedby the algorithm is projective and acyclic[3].Second, the algorithm performs very well fordeciding short-distance dependences.
Third, ateach parsing step, all of the dependency rela-tions on the left side of the input word are de-termined.
Also as the author emphasizes, thetime complexity is linear.However, wrong decision of reduce transition,like early reduce, cause the word at the top ofthe stack loses the chance to be the head of oth-ers.
Some words lose the chance to be the headof other following words.
As a result, the de-pendents of this word will have a wrong head ormay have no head.The parsing steps of a Chinese sentence usingNivre?s[3] algorithm are given in Fig.2.
At step-5 of Fig.2, after reduce, the top of the stack waspopped.
The algorithm doesn?t give a chance forthe word ??
to be the head of other words.Therefore, word ????
cannot have word ????
as its head.
In the final dependency tree ofexample-1 in Fig.2, the arc from ??
to ??
iswrong.
Fig.3 gives the correct dependency tree.Here, ??
is the head of word ?
?.1 All the example sentences are from CTB.If there is a dependency relation between top.stack and inputIf the dependency relation is Left_arcInsert (input, top.stack) pair into set Apop(stack);ElseInsert (top.stack, input) pair  into set Apush(input);ElseIf the head of top of the stack is determinedpop(stack);Elsepush(input);Fig.
1.
Transitions defined  by Nivre[3]??
?
??
??
??
??
?This  province  plan extend  attract merchants  attract investments.The province plans to expand attracting merchants and investments.stack ,  input                      relation set AStep-0:     <nil,??????????
?,{}>Step-1:?S  <??,????????
?,{}>Step-2:?LA <?,????????,{(?,??
)}>Step-3:?LA <??,??????,{(?,??),(??,?
)}>Step-4:?RA <??
??,????,{(?,??),(??,?),(?????
)}>Step-5:?R <??,????,{(?,??),(??,?),(??,??
)}>Step-6:?S <??
??,??,{(?,??),(??,?),(??,??
)}>Step-7:?LA <??,??,{(?,??),(??,?),(??,??),(??,??
)}>Step-8:?LA <??,nil,{(?,??),(??,?),(??,??),(??,??),(?????
)}>The dependency structure of the output:??
?
??
??
??
??
?S:Shift LA:Left-arc RA:Right-arc R:reduceFig.
2.
Example-1: Parsing using Nivre?s algorithm??
?
??
??
??
???Fig.
3.
The correct parse tree of Example-1Fig.4.
gives the parsing step of another example.As the final dependency tree in Fig.4 shows,there is no head for word ??
?After Step-5,the top of the stack is word ?
and input word is?
.
There is no dependency relation betweenthese two words.
Since the head of the word ?is already determined in step-2?the next transi-tion is R(educe).
As a result, word ?
loses thechance to be the head of word ??.
So, there isno head assigned to word ??
in Fig.4.
There-fore, Nivre?s algorithm causes some errors fordetermining the right-side dependents.Yamada?s[4] approach is similar to Nivre?s[3].ReduceshiftRight_arcLeft_arc257Yamada?s algorithm define three actions: left,right and shift, which were similar to those ofNivre?s.
Yamada parsed a sentence by scanningthe sentence word by word from left to right,during the meantime, left or right or shift actionswere decided.
For short dependents, Yamada?salgorithm can cope with it easily.
For long de-pendents, Yamada tried to solve by increasingthe iteration of scanning the sentences.
As Ya-mada pointed out, ?shift?
transition was executedfor two kinds of structure.
This may causewrong decision while deciding the action oftransition.
Yamada tried to resolve it by lookingahead for more information on the right side ofthe target word.??
?
???
?
?
????
?
??
?declare to teachers a  piece      exciting   of  news.Declare a piece of exciting news to teachers.?
?
?Step-2 :?S  <???????????????
?,{}>Step-3 :?RA <??????????????,{(??,?
)}>Step-4 :?RA <?????????????,{(??,?),(?,???
)}>Step-5 :?R <???????????,{(??,?),(?,???
)}>Step-6 :?R <????????????,{(??,?),(?,???)}>?
?
?Step-n:?RA <??,nil,{(??,?),(?,???),(?,?)?(?,????),(??,?),(??,?
)}>The dependency structure of the output:??
?
???
?
?
????
?
???Fig.
4.
Example-2: Parsing with Nivre?s algorithm??
?
???
??
??
??
?
?
?.report _  200       attract  foreign country     investment   of   plan.Report 200 plans in attracting foreign investment.?
?
?step-i : ?RA < ?
?, ????????
?,{( ??,?)}
>Fig.
5.
Example-3: Parsing with Nivre?s algorithmWhen applying to Chinese parsing, the deter-mination of dependency relation between twoverbs is not effective.
In the example-3 of Fig.5,at step-i, the parser decides whether the depend-ency relation between ??
and ??
is eitherLeft-arc or Right-arc.
The actual head of  theverb ??
is ?, which is distant.
By lookingonly two or three right side words ahead, to de-cide the dependency relation between theseverbs at this moment is not reliable.
Yamada?salgorithm is not a clear solution to determine theright side dependents either.3.2 Two-Phase Dependency ParsingFor the head-final languages like Korean orJapanese, Nivre[3] and Yamada?s[4] approachesare efficient.
However, being applied to Chinesetext, the existing methods cannot correctly de-tect various kinds of right-side dependents in-volved in verbs.
All wrong decisions of reducetransition mainly occur if the right dependent ofa verb is also a verb, which may have right-sidedependents.For the correct detection of the right-side de-pendents, we divide the parsing procedure intotwo-phase.
Phase I is to detect the left-side de-pendents and right-side nominal dependents.Although some nominal dependents are right-side, they don?t have dependents on the rightside, and will not cause any ambiguities relatedto right-side dependents.
In Phase II, the detec-tion of right-side verbal dependents, are per-formed.3.2.1 Phase IIn Phase I, we determine the left-side depend-ents and right-side nominal dependents.
We de-fine three transitions for Phase I: Shift, Left-Arc,Right-Arc.
The actions of transition shift andLeft-Arc are the same as Nivre[3] defines.
How-ever, in our method, the transition of Right-Arcdoes not push the input token to the stack.
Theoriginal purpose for pushing input to stack afterright-arc, is to give a chance for the input to bea potential head of the following words.
In Chi-nese, only verbs and prepositions have right-sidedependents.
For other POS categories, the actionof pushing into stack is nonsense.
In case thatthe input word is a preposition, there is no am-biguities we describe.
Only the words belong tovarious verbal categories may cause problems.The method that we use is as follows.
When thetop word of the stack and the next input wordare verbs, like VV, VE, VC or VA2 [11], thedetection of the dependency relation betweenthese two verbs is delayed by transition of shift.To differentiate this shift from original shift, wecall this verbal-shift.
The determination of thedependency relation between these two verbswill be postponed until phase II.
The transitionsare summarized as Fig.6.If there is no more input word, phase I termi-nates.
The output of the phase I is a stack, which2 VV, VE, VC and VA are Penn Chinese Treebank POScategories related to verbs.
For details, please refer to [11].258contains verbs in reverse order of the originalappearance of the verbs in the sentence.
Eachverb in the stack may have their partial depend-ents, which are determined in Phase I.If the action is Verbal-shift: push the input to the stackelse if the action is Shiftpush the input to the stackelse if the action is Left-arcset the dependency relation for two words; popthe top of the stackelse if the action is Right-arcset the dependency relation for two wordsFig.
6.
Types of transitions in the phase IThe type of transition is determined by the topword of the stack, input word and their context.Most of the previous parsing models[4,12,13]use lexical words as features.
Compared to PennEnglish Treebank, the size of Penn ChineseTreebank (version 4.0, abbreviated as CTB) israther small.
Considering the data sparsenessproblem, we use POS tags instead of lexicalwords itself.
As Fig.7.
shows, the window forfeature extraction is the top word of the stack,input word, previous word of the top of thestack, next word of the input.
The left-sidenearest dependent of these is also taken intoconsideration.
Besides, we use two more fea-tures, if_adjoin, and Punc.
The feature vector forPhase I is shown in Fig.7.3.2.2 Phase IIAfter Phase I, only verbs remain in the stack.In Phase II, we determine the right-side verbaldependents.
We take the output stack of Phase Ias input.
Some words in the stack will haveright-side dependents as shown in Fig.8.
ForPhase II, we also define three transitions: shift,left-arc, right-arc.
The operations of these threetransitions are the same as Phase I, but there areno verbal-shifts.
Fig.9 shows the output of PhaseI and parsing at Phase II of example given inFig.8.The window for feature extraction is the sameas that of Phase I.
The right-side nearest de-pendent is newly taken as features for Phase II.The feature vector for Phase II is shown inFig.10.The two-phase parsing will output a projec-tive, acyclic and connective dependency struc-ture.
Nivre[10] said that the time complexity ofhis parser is 2 times the size of the sentence.
Ouralgorithm is 4 times the size of the sentence, sothe time complexity of our parser is still linear tothe size of the sentence.Windows for feature extraction :t.stack :  top word of the stackp.stack:  previous word of top of the stackinput   :  input wordn.input:  next word of the input wordx.pos : POS tag of word xx.left.child : the left-side nearest dependent of word xpunc : the surface form of punctuation between top word of thestack and input word, if there is anyif_adjoin : a binary indicator to show if the top word of thestack and input word are adjoinedThe feature vector for Phase I is :<p.stack.pos t.stack.pos input.pos n.input.pos p.stack.left.child.post.stack.left.child.pos input.left.child.pos punc if_adjoin>Fig.
7.
Feature vector for Phase I?????????????????????????????????????????????????
(The official said that Sichuan will pursue a more open door policy,continuously improve the investment environments and attract morecapitals from overseas, advanced techniques and experiences of ad-ministration.
)The contents of stack after Phase I: <??????????>.
(attract, improve, pursue, said )The dependents  of each verb in the stackFig.
8.
Dependents of each verb after Phase Istep-0      <nil, ??
??
??
??
{}>step-1?S   < ?
?, ??
??
??
{}>step-2?RA  < ?
?, ??
??{(??,??
)}>step-3?RA  < ?
?, ??{(??,??),(??,??
)}>step-4?LA  < nil, ??{(??,??),(??,??),(?,??
)}>step-5 ?S   < ?, nil?{(??,??),(??,??),(?,??)}>Fig.
9.
Example of parsing at Phase IIThe feature vector for Phase II is :<p.stack.pos t.stack.pos input.pos n.input.posp.stack.left.child.pos t.stack.left.child.pos  input.left.child.posp.stack.right.child.pos t.stack.right.child.pos in-put.right.child.pos n.input.right.child.pos punc if_adjoin>Fig.
10.
Feature vector for Phase II.4 Experiments and EvaluationOur parsing procedure is sequentially per-formed from left to right.
The feature vectors for????
??????
?right-side right-side right-sideleft-side left-sideleft-sideleft-side259Phase I and Phase II are used as the input for theparsing model.
The model outputs a parsing ac-tion, left-arc, right-arc or shift.
We use SVM asthe model to obtain a parsing action, and useCTB for training and test the model.4.1 Conversion of Penn Chinese Treebank toDependency TreesAnnotating a Treebank is a tedious task.
Totake the advantage of CTB, we made some heu-ristic rules to convert CTB into dependencyTreebank.
This kind of conversion task has beendone on English Treebank[14,10,4].
We use thedependency formalism as Zhou[15] defined.CTB contains 15,162 newswire sentences (in-cluding titles, fragments and headlines).
Thecontents of CTB are from Xinhua of mainland,information services department of HKSAR andSinorama magazine of Taiwan.
For experiments,12,142 sentences are extracted, excluding all thetitles, headlines and fragments.For the conversion task, we made some heu-ristic rules.
CTB defines total 23 syntacticphrases and verb compounds[11].
A phrase iscomposed of several words accompanied to ahead word.
The head word of each phrase isused as an important resource for PCFG pars-ing[12,13].
According to the position of the headword with respect to other words, a phrase3 canbe categorized into head-final, head-initial orhead-middle set.
Table.1 shows the head-initial,head-final and head-middle groups.For VP, IP and CP, these phrases have a verbas its head word.
So we find a main verb andregard the verb the head word of the phrase.
Ifthe head word for each phrase is determined,other words composing the phrase simply takethe head word of the phrase as its head.
In thecase of BA/LB4, we take a different view fromwhat is done in CTB.
Zhou[15] regards BA/LBas the dependent of the following verb.
We fol-low Zhou?s[15] thought.
For sentences contain-ing BA/LB, we converted them into dependencytrees manually.
With above heuristics, we con-verted the original CTB into dependency Tree-bank.3 We use the label of phrases as CTB has defined.
We ex-clude FRAG, LST, PRN.
For each definition of the phraseplease refer to [11].4 BA, LB are two POS categories of CTB.
For details, see[11].4.2 ExperimentsSVM is one of the binary classifiers based onmaximum margin strategy introduced by Vap-nik[16].
SVM has been used for various NLPtasks, and gives reasonable outputs.
For the ex-periments reported in this paper, we used thesoftware package SVMlight [17].For evaluation matrix, we use DependencyAccuracy and Root Accuracy defined by Ya-mada[4].
An additional evaluation measure,None Head is defined as following.None Head: the proportion of words whosehead is not determined.GROUP PHRASESHead-initial PP; VRD; VPT;Head-final ADJP; ADVP; CLP; DNP; DVP; DP;LCP; NP; QP; VCD; VCP; UCP; VSB;VNV;Head-middleCP; IP; VP;Table 1.
Cluster of CTB syntactic phrasesTable 2.
Comparison of dependency accuracy with Nivre?sWe construct two SVM binary classifiers,Dep vs. N_Dep and LA vs. RA, to output thetransition action of Left-arc, Right-arc or Shift.Dep vs. N_Dep classifier determines if twowords have a dependency relation.
If two wordshave no dependency relation, the transition ac-tion is simply Shift.
If there is a dependency re-lation, the second classifier will decide thedirection of it, and the transition action is eitherLeft-arc or Right-arc.We first train a model along the algorithm ofNivre[10].
The training and test sentences arerandomly selected.
Table.2 shows that 1.53% ofthe words cannot find their head after parsing.This result means that the original Nivre?s algo-rithm cannot guarantee a connective dependencystructure.With our two-phase parsing algorithm, thereis no none head.
Then, the dependency accuracyand root accuracy are increased by 10.08% and13.35% respectively.DependencyaccuracyRoot ac-curacyNoneheadNivre?s algorithm[10] 73.34% 69.98% 1.53%Ours  84.42% 83.33% ----2604.3 Comparison with Related WorksCompared to the original works of Nivre[10]and Yamada[4], the performance of our systemis lower.
We think that is because the target lan-guage is different.AveragesentencelengthDependencyaccuracyRootaccuracyMa[5] 9 80.25% 83.22%Cheng[18] 5.27 94.44% --Ours 34 84.42% 83.33%Table 3 Comparison of the parsing performancesbetween Ma[5], Cheng[18] and oursTable 3 gives the comparison of the perform-ances between Ma[5], Chen[18] and ours.
Thetraining and test domain of Ma[5] is not clear.Cheng[18] used CKIP corpus in his experiments.The average length of sentence in our test set is34, which is much longer than that in Ma[5] andCheng[18].
The performance of our system isstill better than Ma[5] and less than Cheng[8].5 ConclusionTo resolve the right-side long distance de-pendencies, we propose two-phase shift-reduceparsing strategy.
The parsing strategy not onlyguarantees the connectivity of dependency tree,but also improves the parsing performance.
Asthe length of sentences increases, the ambigui-ties for parsing increase drastically.
With ourtwo-phase shift-reduce parsing strategy, the per-formance of syntactic parsing of long sentencesis also reasonable.The motivation of this paper is to design awell-formed dependency parser for Chinese.
Webelieve that there?re rooms to improve the per-formance.
We plan to work further to explorethe optimal features.
We also plan to parse Eng-lish text with our algorithm to see if it can com-pete with the state-of-art dependency parsers onEnglish.
We believe that our parsing strategycan apply to other languages, in which head po-sition is mixed, as Chinese language.
We thinkthat it is the main contribution of our approach.References1.
Robinson, J.J.: Dependency structures andtransformation rules.
Language 46 (1970) 259-2852.
Nivre, J.: An efficient algorithm for projectivedependency parsing.
In Proceedings of IWPT(2003) 149-1603.
Yamada, H. and Matsumoto, Y.: Statistical de-pendency analysis with support vector machines.In Proceedings of IWPT (2003) 195-2064.
Eisner, J.M.
:Three new probabilistic models fordependency parsing: An exploration.
In Proceed-ings of ACL.
( 1996) 340-3455.
Ma,J., Zhang,Y.
and Li,S.
: A statistical depend-ency parser of Chinese under small training data.IJCNLP-04 Workshop : Beyond Shallow Analy-ese-Formalisms and Statistical Modeling for DeepAnalyses (2004)6.
Collins,M.
: Discriminative reranking for naturallanguage parsing.
In proceedings of ICML17.
(2000) 175-1827.
Fung,P., Ngai,G, Yang,Y.S and Chen,B.
: A maxi-mum-entropy Chinese parser augmented by trans-formation-based learning.
ACM transactions onAsian language information processing.
Volume3.
Number 2.
(2004) 159-1688.
Levy,R.
and Manning,C.
: Is it harder to parse Chi-nese, or the Chinese Treebank?
In Proceedings ofACL.
(2003) 439-4469.
Bikel, D.M.
and.Chiang, D.: Two Statistical Pars-ing models applied to the Chinese Treebank.
Inproceedings of  the second Chinese languageprocessing workshop.
(2000)10.Nivre,J, Hall,J and Nilsson,J.
: Deterministic de-pendency parsing of English text.
In Proceedingsof COLING.
(2004) 23?2711.Xue,N and Xia,F.
: The bracketing guidelines forthe Penn Chinese Treebank(3.0).
IRCS Report 00-08, University of Pennsylvania (2000)12.Collins,M.
: Three generative lexicalised modelsfor statistical parsing.
In Proceedings of the 35thAnnual Meeting of the Association for Computa-tional Linguistics, Madrid (1997) 16-2313.Charniak,E.
: A maximum-entropy-inspired parser.In Proceedings of NAACL.
Seattle (2000) 132?139,14.Collins,M.
: A new statistical parser based on bi-gram lexical dependencies.
In Proceedings of theThirty-Fourth Annual Meeting of the Associationfor Computational Linguistics, philladelphia(1996) 184?19115.Zhou,M.
and Huang,C.
: Approach to the Chinesedependency formalism for the tagging of corpus.Journal of Chinese information processing.
(inChinese), Vol.
8(3) (1994) 35-5216.Joachims,T.
: Making large-scale SVM learningpractical.
Advances in Kernel Methods-SupportVector Learning, B.Scholkopf and C.Burges andA.Smola(Eds.
), MIT-Press (1999)17.
Vapnik, V.N.
: The nature of statistical learningtheory.
Springer, New York.
(1995)18.
Cheng, Y.C, Asahara,M and Matsumoto Y.: De-terministic dependency structure analyzer for Chi-nese.
In proceedings of the first IJCNLP(2004)135-140261
