Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 719?724,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTypesetting for Improved Readability usingLexical and Syntactic InformationAhmed Salamaahmedsaa@qatar.cmu.eduKemal OflazerCarnegie Mellon University ?
QatarDoha, Qatarko@cs.cmu.eduSusan Haganhagan@cmu.eduAbstractWe present results from our study of whichuses syntactically and semantically moti-vated information to group segments ofsentences into unbreakable units for thepurpose of typesetting those sentences ina region of a fixed width, using an other-wise standard dynamic programming linebreaking algorithm, to minimize ragged-ness.
In addition to a rule-based base-line segmenter, we use a very modest sizetext, manually annotated with positions ofbreaks, to train a maximum entropy clas-sifier, relying on an extensive set of lexi-cal and syntactic features, which can thenpredict whether or not to break after a cer-tain word position in a sentence.
We alsouse a simple genetic algorithm to searchfor a subset of the features optimizing F1,to arrive at a set of features that deliv-ers 89.2% Precision, 90.2% Recall (89.7%F1) on a test set, improving the rule-basedbaseline by about 11 points and the classi-fier trained on all features by about 1 pointin F1.1 Introduction and MotivationCurrent best practice in typography focuses onseveral interrelated factors (Humar et al 2008;Tinkel, 1996).
These factors include typeface se-lection, the color of the type and its contrast withthe background, the size of the type, the length ofthe lines of type in the body of the text, the mediain which the type will live, the distance betweeneach line of type, and the appearance of the jus-tified or ragged right side edge of the paragraphs,which should maintain either the appearance of astraight line on both sides of the block of type (jus-tified) or create a gentle wave on the ragged rightside edge.This paper addresses one aspect of current ?bestpractice,?
concerning the alignment of text in aparagraph.
While current practice values that gen-tle ?wave,?
which puts the focus on the elegantlook of the overall paragraph, it does so at theexpense of meaning-making features.
Meaning-making features enable typesetting to maintain theintegrity of phrases within sentences, giving thoseinterests equal consideration with the overall lookof the paragraph.
Figure 1 (a) shows a text frag-ment typeset without any regard to natural breakswhile (b) shows an example of a typesetting thatwe would like to get, where many natural breaksare respected.While current practice works well enough fornative speakers, fluency problems for non-nativespeakers lead to uncertainty when the beginningand end of English phrases are interrupted by theneed to move to the next line of the text beforecompleting the phrase.
This pause is a poten-tial problem for readers because they try to inter-pret content words, relate them to their referentsand anticipate the role of the next word, as theyencounter them in the text (Just and Carpenter,1980).
While incorrect anticipation might not beproblematic for native speakers, who can quicklyre-adjust, non-native speakers may find inaccu-rate anticipation more troublesome.
This prob-lem could be more significant because Englishas a second language (ESL) readers are engagednot only in understanding a foreign language, butalso in processing the ?anticipated text?
as theyread a partial phrase, and move to the next linein the text, only to discover that they anticipatedmeaning incorrectly.
Even native speakers withless skill may experience difficulty comprehend-ing text and work with young readers suggests that?
[c]omprehension difficulties may be localized atpoints of high processing demands whether fromsyntax or other sources?
(Perfetti et al 2005).
AsESL readers process a partial phrase, and move to719the next line in the text, instances of incorrectlyanticipated meaning would logically increase pro-cessing demands to a greater degree.
Additionally,as readers make meaning, we assume that theydon?t parse their thoughts using the same phrasaldivisions ?needed to diagram a sentence.?
Our per-spective not only relies on the immediacy assump-tion, but also develops as an outgrowth of otherways that we make meaning outside of the form orfunction rules of grammar.
Specifically, Hallidayand Hasan (1976) found that rules of grammar donot explain how cohesive principals engage read-ers in meaning making across sentences.
In orderto make meaning across sentences, readers mustbe able to refer anaphorically backward to the pre-vious sentence, and cataphorically forward to thenext sentence.
Along similar lines, readers of asingle sentence assume that transitive verbs willinclude a direct object, and will therefore specu-late about what that object might be, and some-times get it wrong.Thus proper typesetting of a segment of textmust explore ways to help readers avoid incor-rect anticipation, while also considering those mo-ments in the text where readers tend to pause inorder to integrate the meaning of a phrase.
Thosedecisions depend on the context.
A phrasal breakbetween a one-word subject and its verb tends tobe more unattractive, because the reader does nothave to make sense of relationships between thenoun/subject and related adjectives before movingon to the verb.
In this case, the reader will be morelikely to anticipate the verb to come.
However,a break between a subject preceded by multipleadjectives and its verb is likely to be more use-ful to a reader (if not ideal), because the relation-ships between the noun and its related adjectivesare more likely to have thematic importance lead-ing to longer gaze time on the relevant words inthe subject phrase (Just and Carpenter, 1980).We are not aware of any prior work for bring-ing computational linguistic techniques to bear onthis problem.
A relatively recent study (Levasseuret al 2006) that accounted only for breaks atcommas and ends of sentences, found that eventhose breaks improved reading fluency.
While theparticipants in that study were younger (7 to 9+years old), the study is relevant because the chal-lenges those young participants face, are facedagain when readers of any age encounter new andcomplicated texts that present words they do notknow, and ideas they have never considered.On the other hand, there is ample work on thebasic algorithm to place a sequence of words in atypesetting area with a certain width, commonlyknown as the optimal line breaking problem (e.g.,Plass (1981), Knuth and Plass (1981)).
This prob-lem is quite well-understood and basic variants areusually studied as an elementary example applica-tion of dynamic programming.In this paper we explore the problem of learn-ing where to break sentences in order to avoid theproblems discussed above.
Once such unbreak-able segments are identified, a simple applicationof the dynamic programming algorithm for opti-mal line breaking, using unbreakable segments as?words?, easily typesets the text to a given widtharea.2 Text BreaksThe rationale for content breaks is linked to our in-terest in preventing inaccurate anticipation, whichis based on the immediacy assumption.
The imme-diacy assumption (Just and Carpenter, 1980) con-siders, among other things, the reader?s interest intrying to relate content words to their referents assoon as possible.
Prior context also encouragesthe reader to anticipate a particular role or casefor the next word, such as agent or the mannerin which something is done.Therefore, in defin-ing our breaks, we consider not only the need tomaintain the syntactic integrity of phrases, suchas the prepositional phrase, but also the semanticintegrity across syntactical divisions.
For exam-ple, semantic integrity is important when transitiveverbs anticipate direct objects.
Strictly speaking,we define a bad break as one that will cause (i)unintended anaphoric collocation, (ii) unintendedcataphoric collocation, or (iii) incorrect anticipa-tion.Using these broad constraints, we derived a setof about 30 rules that define acceptable and non-acceptable breaks, with exceptions based on con-text and other special cases.
Some of the rules arevery simple and are only related to the word posi-tion in the sentence:?
Break at the end of a sentence.?
Keep the first and last words of a sentencewith the rest of it.The rest of the rule set are more complex and de-pend on the structure of the sentence in question,720sanctions and UN charges of gross rights abuses.
Military tensions on theKorean peninsula have risen to their highest level for years, with thecommunist state under the youthful Kim threatening nuclear war in responseto UN sanctions imposed after its third atomic test last month.
It has also(a) Text with standard typesettingfrom US sanctions and UN charges of gross rights abuses.
Military tensionson the Korean peninsula have risen to their highest level for years,with the communist state under the youthful Kim threatening nuclear warin response to UN sanctions imposed after its third atomic test last month.
(b) Text with syntax-directed typesettingFigure 1: Short fragment of text with standard typesetting (a) and with syntax and semantics motivatedtypesetting (b), both in a 75 character width.e.g.:?
Keep a single word subject with the verb.?
Keep an appositive phrase with the noun itrenames.?
Do not break inside a prepositional phrase.?
Keep marooned prepositions with the wordthey modify.?
Keep the verb, the object and the prepositiontogether in a phrasal verb phrase.?
Keep a gerund clause with its adverbial com-plement.There are exceptions to these rules in certain casessuch as overly long phrases.3 Experimental SetupOur data set consists of a modest set of 150 sen-tences (3918 tokens) selected from four differentdocuments and manually annotated by a humanexpert relying on the 30 or so rules.
The annota-tion consists of marking after each token whetherone is allowed to break at that position or not.1We developed three systems for predictingbreaks: a rule-based baseline system, a maximum-entropy classifier that learns to classify breaks us-ing about 100 lexical, syntactic and collocationalfeatures, and a maximum entropy classifier thatuses a subset of these features selected by a sim-ple genetic algorithm in a hill-climbing fashion.We evaluated our classifiers intrinsically using theusual measures:1We expect to make our annotated data available upon thepublication of the paper.?
Precision: Percentage of the breaks positedthat were actually correct breaks in the gold-standard hand-annotated data.
It is possibleto get 100% precision by putting a singlebreak at the end.?
Recall: Percentage of the actual breaks cor-rectly posited.
It is possible to get 100% re-call by positing a break after each token.?
F1: The geometric mean of precision and re-call divided by their average.It should be noted that when a text is typeset intoan area of width of a certain number of characters,an erroneous break need not necessarily lead to anactual break in the final output, that is an error maynot be too bad.
On the other hand, a missed breakwhile not hurting the readability of the text mayactually lead to a long segment that may eventu-ally worsen raggedness in the final typesetting.Baseline Classifier We implemented a subset ofthe rules (those that rely only on lexical and part-of-speech information), as a baseline rule-basedbreak classifier.
The baseline classifier avoidsbreaks:?
after the first word in a sentence, quote orparentheses,?
before the last word in a sentence, quote orparentheses, and?
between a punctuation mark following aword or between two consecutive punctua-tion marks.It posits breaks (i) before a word following apunctuation, and (ii) before prepositions, auxil-iary verbs, coordinating conjunctions, subordinateconjunctions, relative pronouns, relative adverbs,conjunctive adverbs, and correlative conjunctions.721Maximum Entropy Classifier We used theCRF++ Tool2 but with the option to run it onlyas a maximum entropy classifier (Berger et al1996), to train a classifier.
We used a large setof about 100 features grouped into the followingcategories:?
Lexical features: These features include thetoken and the POS tag for the previous, cur-rent and the next word.
We also encodewhether the word is part of a compound nounor a verb, or is an adjective that subcatego-rizes a specific preposition in WordNet, (e.g.,familiar with).?
Constituency structure features: These areunlexicalized features that take into accountin the parse tree, for a word and its previousand next words, the labels of the parent, thegrandparent and their siblings, and number ofsiblings they have.
We also consider the labelof the closest common ancestor for a wordand its next word.?
Dependency structure features: These are un-lexicalized features that essentially capturethe number of dependency relation links thatcross-over a given word boundary.
The moti-vation for these comes from the desire to limitthe amount of information that would need tobe carried over that boundary, assuming thiswould be captured by the number of depen-dency links over the break point.?
Baseline feature: This feature reflectswhether the rule-based baseline break classi-fier posits a break at this point or not.We use the following tools to process the sen-tences to extract some of these features:?
Stanford constituency and dependencyparsers, (De Marneffe et al 2006; Klein andManning, 2002; Klein and Manning, 2003),?
lemmatization tool in NLTK (Bird, 2006),?
WordNet for compound nouns and verbs(Fellbaum, 1998).2Available at http://crfpp.googlecode.com/svn/trunk/doc/index.html.Baseline ME-All ME-GAPrecision 77.9 87.3 89.2Recall 80.4 90.2 90.2F1 79.1 88.8 89.7Table 1: Results from Baseline and Maximum En-tropy break classifiersMaximum Entropy Classifier with GA FeatureSelection We used a genetic algorithm on a de-velopment data set, to select a subset of the fea-tures above.
Basically, we start with a randomlyselected set of features and through mutation andcrossover try to obtain feature combinations thatperform better over the development set in termsof F1 score.
After a few hundred generations ofthis kind of hill-climbing, we get a subset of fea-tures that perform the best.4 ResultsOur current evaluation is only intrinsic in that wemeasure our performance in getting the break andno-break points correctly in a test set.
The resultsare shown in Table 1.
The column ME-All showsthe results for a maximum entropy classifier us-ing all the features and the column ME-GA showsthe results for a maximum entropy classifier usingabout 50 of the about 100 features available, as se-lected by the genetic algorithm.Our best system delivers 89.2% precision and90.2% recall (with 89.7% F1), improving the rule-based baseline by about 11 points and the classifiertrained on all features by about 1 point in F1.After processing our test set with the ME-GAclassifier, we can feed the segments into a stan-dard word-wrapping dynamic programming algo-rithm (along with a maximum width) and obtain atypeset version with minimum raggedness on theright margin.
This algorithm is fast enough to useeven dynamically when resizing a window if thetext is displayed in a browser on a screen.
Fig-ure 1 (b) displays an example of a small fragmentof text typeset using the output of our best breakclassifier.
One can immediately note that this type-setting has more raggedness overall, but avoids thebad breaks in (a).
We are currently in the processof designing a series of experiments for extrinsicevaluation to determine if such typeset text helpscomprehension for secondary language learners.7224.1 Error AnalysisAn analysis of the errors our best classifier makes(which may or may not be translated into an actualerror in the final typesetting) shows that the major-ity of the errors basically can be categorized intothe following groups:?
Incorrect breaks posited for multiword collo-cations (e.g., act* of war,3 rule* of law, farahead* of, raining cats* and dogs, etc.)?
Missed breaks after a verb (e.g., calls | an actof war, proceeded to | implement, etc.)?
Missed breaks before or after prepositions oradverbials (e.g., the day after | the world re-alized, every kind | of interference)We expect to overcome such cases by increasingour training data size significantly by using ourclassifier to break new texts and then have a hu-man annotator to manually correct the breaks.5 Conclusions and Future WorkWe have used syntactically motivated informationto help in typesetting text to facilitate better under-standing of English text especially by secondarylanguage learners, by avoiding breaks which maycause unnecessary anticipation errors.
We havecast this as a classification problem to indicatewhether to break after a certain word or not, bytaking into account a variety of features.
Our bestsystem maximum entropy framework uses about50 such features, which were selected using a ge-netic algorithm and performs significantly betterthan a rule-based break classifier and better than amaximum entropy classifier that uses all availablefeatures.We are currently working on extending thiswork in two main directions: We are designinga set of experiments to extrinsically test whethertypesetting by our system improves reading easeand comprehension.
We are also looking into abreak labeling scheme that is not binary but basedon a notion of ?badness?
?
perhaps quantized into3-4 grades, that would allow flexibility betweenpreventing bad breaks and minimizing raggedness.For instance, breaking a noun-phrase right after aninitial the may be considered very bad.
On theother hand, although it is desirable to keep an ob-ject NP together with the preceding transitive verb,3* indicates a spurious incorrect break, | indicates amissed break.breaking before the object NP, could be OK, if notdoing so causes an inordinate amount of ragged-ness.
Then the final typesetting stage can optimizea combination of raggedness and the total ?bad-ness?
of all the breaks it posits.AcknowledgementsThis publication was made possible by grantNPRP-09-873-1-129 from the Qatar National Re-search Fund (a member of the Qatar Foundation).Susan Hagan acknowledges the generous supportof the Qatar Foundation through Carnegie MellonUniversity?s Seed Research program.
The state-ments made herein are solely the responsibilityof this author(s), and not necessarily those of theQatar Foundation.ReferencesAdam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguis-tics, 22(1):39?71.Steven Bird.
2006.
NLTK: The natural languagetoolkit.
In Proceedings of the COLING/ACL, pages69?72.
Association for Computational Linguistics.Marie-Catherine De Marneffe, Bill MacCartney, andChristopher D Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC, volume 6, pages 449?454.Christiane Fellbaum.
1998.
WordNet: An electroniclexical database.
The MIT Press.M.
A. K. Halliday and R. Hasan.
1976.
Cohesion inEnglish.
Longman, London.I.
Humar, M. Gradisar, and T. Turk.
2008.
The impactof color combinations on the legibility of a web pagetext presented on crt displays.
International Journalof Industrial Ergonomics, 38(11-12):885?899.Marcel A.
Just and Patricia A. Carpenter.
1980.
A the-ory of reading: From eye fixations to comprehen-sion.
Psychological Review, 87:329?354.Dan Klein and Christopher D. Manning.
2002.
Fastexact inference with a factored model for naturallanguage parsing.
Advances in Neural InformationProcessing Systems, 15(2003):3?10.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.723Donald E Knuth and Michael F. Plass.
1981.
Break-ing paragraphs into lines.
Software: Practice andExperience, 11(11):1119?1184.Valerie Marciarille Levasseur, Paul Macaruso,Laura Conway Palumbo, and Donald Shankweiler.2006.
Syntactically cued text facilitates oralreading fluency in developing readers.
AppliedPsycholinguistics, 27(3):423?445.C.
A. Perfetti, N. Landi, and J. Oakhill.
2005.
The ac-quisition of reading comprehension skill.
In M. J.Snowling and C. Hulme, editors, The science ofreading: A handbook, pages 227?247.
Blackwell,Oxford.Michael Frederick Plass.
1981.
Optimal Pagina-tion Techniques for Automatic Typesetting Systems.Ph.D.
thesis, Stanford University.K.
Tinkel.
1996.
Taking it in: What makes type easierto read.
Adobe Magazine, pages 40?50.724
