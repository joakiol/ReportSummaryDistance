Incremental Conceptualization for Language ProductionMarkus Guhe(University of Edinburgh)Mahwah, NJ: Lawrence Erlbaum Associates (distributed by Psychology Press), 2007,xii+260 pp; hardbound, ISBN 978-0-8058-5624-8, $75.00Reviewed byPaul PiwekThe Open UniversityFor the past ten years or more, most work in the field of Natural Language Gener-ation (NLG) has shied away from considerations regarding the processes underlyinghuman language production.
Rather, the focus has been on systems that automaticallyproduce language?usually text?from non-linguistic representations, with the mainobjective being generation of a text that faithfully captures the meaning of those non-linguistic representations (see, e.g., Reiter and Dale?s 2000 textbook on NLG).
Thereis, however, also a different take on NLG ?as not just competent performance by acomputer but the development of a computational theory of the human capacity forlanguage and processes that engage it?
(McDonald 1987, page 642).
Guhe?s researchmonograph, based on his 2003 Ph.D. thesis, is firmly situated in the latter tradition.One of his main goals is to work out a computational architecture for Levelt?s (1989)psycholinguistically motivated model of language production.
According to Levelt?smodel, speaking involves three main activities: conceptualizing (deciding what to say),formulating (deciding how to say it), and articulating (saying it).
Guhe?s book focuseson the mental activity of conceptualizing.Conceptualizing is a recalcitrant object of study, partly because of the problem ofthe ?initial spark?
; the decision to say something appears to be the result of volitionalconscious decisions, which largely elude scientific study.
Guhe avoids this problem byinvestigating conceptualization in settings where the main intention is already fixed:a speaker witnesses several events unfold and is instructed to describe what happens(while it happens).
The research challenge then is to figure out how ?subintentions?
forindividual speech acts come about.
The benefit of using an on-line generation setting isthat it provides information on both what a speaker says at a given point in time andwhat is being reported, that is, the data that drive the speaker?s utterances.The book consists of the usual preface and introduction, followed by four parts(A, B, C, and Results), a list of the book?s theses, and an appendix that includes,among other things, a glossary, bibliography, name index, and subject index.
Part Aof the book is titled ?Conceptualization.?
It starts with an introduction to the field oflanguage production, with particular reference to Levelt?s (1989) model.
The notion ofconceptualization as a ?quasi-module,?
partly using Fodor?s (1983) criteria, is presentedand four subtasks of conceptualization are discussed:1. construction (mainly mapping what is perceived to concepts fromlong-term memory)2. selection (of events that are to be verbalized)3. linearization (ordering selected events appropriate to the goal ofthe discourse)Computational Linguistics Volume 34, Number 14. generation of preverbal messages (mapping the conceptualrepresentations that have been handled so far to semanticcontent that can interface with the linguistic formulator)This chapter also introduces referential nets, the formalism that is used to representconceptual content.Part B (?Incrementality?)
traces the roots of the notion of incrementality in computerscience, and provides an extensive overview of various notions of incrementality.
Guhesettles on a definition of incrementality whose crux is the piecemeal processing ofinformation and production of output before all input has been seen.
He distinguishesbetween incremental processes, algorithms, and models; roughly speaking, incrementalmodels contain a strictly uni-directional cascade of incremental processes that recur-sively call incremental algorithms.
For Guhe, an essential characteristic of incrementalalgorithms is that they use only a local context, as opposed to all available knowl-edge, for their computations.
He also adopts the common distinction between workingmemory and long-term memory.
The former mediates the flow of information betweenincremental processes.
?Increments,?
the small pieces of information that incrementalprocesses operate with, can be read from it and written to it.
It contains ?situation anddiscourse knowledge,?
whereas long-term memory stores static ?encyclopedic knowl-edge.?
This ?blueprint for incrementality?
is accompanied by a useful discussion ofvarious dimensions of incrementality, such as monotonicity, lookahead, feedback, anddiscreteness.Part C focuses on INC, the incremental conceptualizer, which is an implemented?working model?
of the blueprint for incrementality.
INC is offered as a framework,that is, a model which has been fleshed out in detail in some respects and left under-specified in others.
A central role is played by four parameters of INC which influenceits behavior.
For example, two of these concern the storage of event representations in abuffer inworkingmemorywhichmediates the flow of information between incrementalprocesses.
One parameter, length of traverse buffer (LOTB), concerns the size of thisbuffer, whereas the other, latency (LT), determines for how long an element is kept inthe buffer until it is picked up by preverbal message generation.
Small values for LOTBin combination with a large value for LT can lead to the ?forgetting?
of information: Ifthe buffer has filled up and new information is added, the first element on the bufferis discarded and never reaches preverbal message generation.
The book presents someevidence that variation of the parameter settings can account for some of the variationfound among human speakers.
This part of the book concludes with a discussion of theoutput of INC for two domains and output of human speakers for the same domains.It concerns a visual scene, from a bird?s eye perspective, of two moving planes on arunway, and the replay of the drawing of a simple line drawing consisting of eight linesthat represents a crossing.The ?Results?
summarizes the main contributions of the book, makes some com-parisons with Levelt?s (1989) model, and proposes a number of future extensions, suchas the addition of Levelt?s monitor.
The monitor takes as input the output of the speech-comprehension system and uses this to influence the processing of the conceptualizer.Finally, there are a good number of suggestions for further ways to parameterize INC.The book is a rich source of information on language production, both from acomputational and a cognitive point of view.
It includes a good introduction to con-ceptualizing, and provides an insightful discussion of many varieties of incrementality.INC is an excellent starting point for others interested in on-line data-driven generationto both build on and respond to.
The breadth of the work means that one gets a truly130Book Reviewsholistic view of the problem and is given a good impression of the many debates thatcross the boundaries of different disciplines.
In this respect, the book goes against arecent trend in computational linguistics to show less interest in other language-relatedresearch communities (see Reiter 2007).Although the wide scope of this book is in many ways what makes it attractive,it also leads to some of its weaknesses.
In particular, the way INC is presented in thisbroad context did not feel optimal to me.
Although the proper description of INC isdelayed until Part C, there are numerous forward references to INC in the precedingparts.
The reader will find several instances where a certain aspect of conceptualizationor incrementality is discussed with reference to INC, only to find out later that thisparticular feature ?is not implemented yet (apart from a dummy function).?
It wouldhave been fairer to the reader to separate a clear description of the current state of INCfrom the wider discussion surrounding it.
Another presentational issue concerns thetight integration of locality and incrementality in the book?s definitions.
In particular,the virtual identification of incremental algorithms with computation on a local contextmakes one question why the book speaks of incremental rather than local algorithms.A more substantive point relates to Part C on INC.
This part includes the descriptionof two simulations that were run with INC.
Somewhat frustratingly, both descriptionsare incomplete.
For instance, whereas for the first simulation the appendix contains thetexts produced by human participants for the same task, there is no systematic analysisof the structural (dis)similarities between the output of INC and that of the humanspeakers.
For the second simulation, there are some analyses of the similarities betweenthe structure of INC?s and the human speakers?
output, but no transcripts of completehuman outputs are provided.
In both cases, there is also no detail about how long-term memory, referred to as the concept storage (CS), was populated for the relevantdomains, even though the CS must have had a significant influence on the output thatINC produces.This book will be useful to research students and researchers in natural languagegeneration who are interested in the study of generation systems as a computationalmodel of human language production.
Part B of the book, on incrementality, mightalso prove useful to those approaching NLG as an engineering problem.
The mainreason to consult this book is that it brings together in a single place information onconceptualization, incrementality, and various debates in philosophy, cognitive science,and computer science affecting these topics.
INC, the incremental conceptualizer whichis described in part C of the book, presents an ambitious attempt to implement acomputational model of incremental conceptualization.
The verdict on its adequacy isstill out, given the limited empirical evaluation to which it has been subjected thus far.Online generation, a central theme of this book, was adopted in 2007 at the Interna-tional Conference on Intelligent Virtual Agents as a task (automated real-time reportingon simulated horse races) in the GALA competition for Embodied Lifelike Agents.1Work on embodiment and conceptualization, new insights into societal grounding ofconceptual representations (e.g., DeVault, Oved, and Stone 2006), empirical and compu-tational studies on generation (both incremental and non-incremental, from numericaldata; e.g., vanDeemter 2006), and recent experimental techniques for studying languageproduction (see Roelofs 2004 for an overview) give a sense that this book could be partof an exciting revival of cognitively motivated NLG.1 See http://hmi.ewi.utwente.nl/gala/.131Computational Linguistics Volume 34, Number 1Referencesvan Deemter, Kees.
2006.
Generatingreferring expressions that involve gradableproperties.
Computational Linguistics,32(2):195?222.DeVault, David, Iris Oved, and MatthewStone.
2006.
Societal grounding isessential for meaningful language use.
InProceedings of the 21st National Conference onArtificial Intelligence (AAAI-06), Boston,MA, pages 747?754.Fodor, Jerry A.
1983.
The modularity of mind.MIT Press, Cambridge, MA.Levelt, Willem J. M. 1989.
Speaking: FromIntention to Articulation.
MIT Press,Cambridge, MA.McDonald, David.
1987.
Natural languagegeneration.
In Stuart C. Shapiro, editor,Encyclopedia of Artificial Intelligence,Volume 1, John Wiley & Sons, New York,pages 642?654.Reiter, Ehud.
2007.
The shrinkinghorizons of computational linguistics.Computational Linguistics,33(2):283?287.Reiter, Ehud and Robert Dale.
2000.Building Natural Language GenerationSystems.
Cambridge University Press,Cambridge, UK.Roelofs, Ardi.
2004.
The seduced speaker:Modeling of cognitive control.In Anja Belz, Roger Evans, andPaul Piwek, editors, Natural LanguageGeneration, Third InternationalConference, LNCS 3123, Springer,Berlin, pages 1?10.Paul Piwek is lecturer in computing at the Open University.
His research interests are (multimodal)natural language generation and dialogue modeling.
Piwek?s address is: Centre for Research inComputing, The Open University, Walton Hall, Milton Keynes, UK; e-mail: p.piwek@open.ac.uk.132
