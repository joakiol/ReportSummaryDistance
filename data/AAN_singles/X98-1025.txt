SUMMARIZATION: (1) USING MMR FOR DIVERSITY- BASEDRERANKING AND (2) EVALUATING SUMMARIESJade Goldstein and Jaime CarbonellLanguage Techno log ies  InstituteCarnegie Mel lon  Univers i tyPittsburgh, PA  15213 USAjade @ c s.cmu, edu, j gc @ cs.
cmu .eduABSTRACT:This paper 1 develops a method for combining query-relevance with information-novelty in the context oftext retrieval and summarization.
The Maximal MarginalRelevance (MMR) criterion strives to reduceredundancy while maintaining query relevance in re-ranking retrieved documents and in selectingappropriate passages for text summarization.Preliminary results indicate some benefits for MMRdiversity ranking in ad-hoc query and in singledocument summarization.
The latter are borne out bythe trial-run (unofficial) TREC-style evaluation ofsummarization systems.
However, the clearestadvantage is demonstrated in the automated constructionof large document and non-redundant multi-documentsummaries, where MMR results are clearly superior tonon-MMR passage selection.
This paper also discussesour preliminary evaluation of summarization methodsfor single documents.1.
INTRODUCTIONWith the continuing rowth of online information, ithas become increasingly important to provide improvedmechanisms to find information quickly.
ConventionalIR systems rank and assimilate documents based onmaximizing relevance to the user query \[1, 8, 6, 12, 13\].In cases where relevant documents are few, or caseswhere very-high recall is necessary, pure relevanceranking is very appropriate.
But in cases where there isa vast sea of potentially relevant documents, highlyredundant with each other or (in the extreme) containingpartially or fully duplicative information we must utilizemeans beyond pure relevance for document ranking.In order to better illustrate the need to combinerelevance and anti-redundancy, consider a reporter or aThis research was performed as part of CarnegieGroup Inc.'s Tipster III Summarization Project underthe direction of Mark Borger and Alex Kott.student, using a newswire archive collection to researchaccounts of airline disasters.
He composes a well-though-out query including "airline crash", "FAAinvestigation", "passenger deaths", "fire", "airplaneaccidents", and so on.
The IR engine returns a rankedlist of the top 100 documents (more if requested), andthe user examines the top-ranked ocument.
It's aboutthe suspicious TWA-800 crash near Long Island.
Veryrelevant and useful.
The next document is also about"TWA-800", so is the next, and so are the following 30documents.
Relevant?
Yes.
Useful?
Decreasingly so.Most "new" documents merely repeat informationalready contained in previously offered ones, and theuser could have tired long before reaching the first non-TWA-800 air disaster document.
Perfect precision,therefore, may prove insufficient in meeting user needs.A better document ranking method for this user is onewhere each document in the ranked list is selectedaccording to a combined criterion of query relevanceand novelty of information.
The latter measures thedegree of dissimilarity between the document beingconsidered and previously selected ones already in theranked list.
Of course, some users may prefer to drilldown on a narrow topic, and others a panoramicsampling bearing relevance to the query.
Best is a user-tunable method that focuses the search from a narrowbeam to a floodlight.
Maximal Marginal Relevance(MMR) provides precisely such functionality, asdiscussed below.If we consider document summarization by relevant-passage extraction, we must again consider anti-redundancy as well as relevance.
Both query-freesummaries and query-relevant summaries need to avoidredundancy, as it defeats the purpose of summarization.For instance, scholarly articles often state their thesis inthe introduction, elaborate upon it in the body, and"reiterate it in the conclusion.
Including all three inversions in the summary, however, leaves little room forother useful information.
If we move beyond singledocument summarization to document clustersummarization, where the summary must pool passages181from different but possibly overlapping documents,reducing redundancy becomes an even more significantproblem.Automated document summarization dates back toLuhn's work at IBM in the 1950's \[12\], and evolvedthrough several efforts including Tait \[24\] and Paice inthe 1980s \[17, 18\].
Much early work focused on thestructure of the document to select information.
In the1990's everal approaches to summarization blossomed,include trainable methods \[10\], linguistic approaches \[8,15\] and our information-centric method \[2\], the first tofocus on query-relevant summaries and anti-redundancymeasures.
As part of the TIPSTER program \[25\], newinvestigations have started into summary creation usinga variety of strategies.
These new efforts address queryrelevant as well as "generic" summaries and utilize avariety of approaches including using co-referencechains (from the University of Pennsylvania) \[25\], thecombination of statistical and linguistic approaches(Smart and Empire) from SaBir Research, CornellUniversity and GE R&D Labs, topic identification andinterpretation from the ISI, and template basedsummarization from New Mexico State University \[25\].In this paper, we discuss the Maximal MarginalRelevance method (Section 2), its use for documentreranking (Section 3), our approach to query-basedsingle document summarization (Section 4), and ourapproach to long documents (Section 6) and multi-document summarization (Section 6).
We also discussour evaluation efforts of single document summarization(Section 7-8) and our preliminary results (Section 9).2.
MAXIMAL MARGINAL  RELEVANCEMost modern IR search engines produce a ranked listof retrieved ocuments ordered by declining relevanceto the user's query \[1, 18, 21, 26\].
In contrast, wemotivated the need for '"relevant novelty" as apotentially superior criterion.
However, there is noknown way to directly measure new-and-relevantinformation, especially given traditional bag-of-wordsmethods uch as the vector-space model \[19, 21\].
Afirst approximation to measuring relevant novelty is tomeasure relevance and novelty independently andprovide a linear combination as the metric.
We call thelinear combination "marginal relevance" -- i.e.
adocument has high marginal relevance if it is bothrelevant to the query and contains minimal similarity topreviously selected ocuments.
We strive to maximizemarginal relevance in retrieval and summarization,hence we label our method "maximal marginalrelevance" (MMR).The Maximal Marginal Relevance (MMR) metric isdefined as follows:Let C = document collection (or document stream)Let Q = ad-hoc query (or analyst-profile ortopic/category specification)Let R = IR (C, Q, q) - i.e.
the ranked list ofdocuments retrieved by an IR system, given C and Qand a relevance threshold theta, below which it will notretrieve documents.
(q can be degree of match, ornumber of documents).Let S = subset of documents in R already provided tothe user.
(Note that in an IR system without MMR anddynamic reranking, S is typically a proper prefix of listR.)
R~ is the set difference, i.e.
the set of documents inR, not yet offered to the user.defMMR(C,Q,R,S)=Argmax\[X*Sim 1 (Di,Q)-(1-X)Max(Sim2(Di,Dj))\]Di ~R\S Dj eSGiven the above definition, MMR computesincrementally the standard relevance-ranked list whenthe parameter ~=1, and computes a maximal diversityranking among the documents in R when X=0.
Forintermediate values of ~, in the interval \[0,1\], a linearcombination of both criteria is optimized.
Users wishingto sample the information space around the query,should set ~, at a smaller value, and those wishing tofocus in on multiple potentially overlapping orreinforcing relevant documents, hould set ~, to a valuecloser to 1.
For document retrieval, we found that aparticularly effective search strategy (reinforced by theuser study discussed below) is to start with a small L(e.g.
~, = .3) in order to understand the information spacein the region of the query, and then to focus on the mostimportant parts using a reformulated query (possibly viarelevance feedback) and a larger value of ~ (e.g.
~, = .7).Note that the similarity metric Sim 1 used in documentretrieval and relevance ranking between documents andquery could be the same as Sim2 between documents(e.g., both could be cosine similarity), but this need notbe the case.
A more accurate, but computationally morecostly metric could be used when applied only to theelements of the retrieved ocument set R, given that IRI<< ICI, if MMR is applied for re-ranking the top portionof the ranked list produced by a standard IR system.182query : Brazil external debt fiqureAdic le  TitleBRAZIL SEEN AS VANGUARD FOR CHANGING DEBT STRATEGYFUNARO REJECTS UK SUGGESTION OF IMF BRAZIL PLANECONOMIC SPOTLIGHT - BRAZIL DEBT DEADLINES LOOMU.S.
URGED TO STRENGTHEN DEBT STRATEGYU.S, URGES BANKS TO DEVELOP NEW 3RD WLD FINANCEFUNARO'S DEPARTURE COULD LEAD TO BRAZIL DEBT DEALU.S.
OFFICIALS SAY BRAZIL SHOULD DEAL WITH BANKSBRAZIL SEEKS TO REASSURE BANKS ON DEBT SUSPENSIONBRAZIL SEEKS TO REASSURE BANKS ON DEBT SUSPENSIONBRAZIL CRITICISES ADVISORY COMMITTEE STRUCTURELATIN DEBTORS MAKE NEW PUSH FOR DEBT RELIEBRAZIL DEBT SEEN PARTNER TO HARD SELL TACTICSBRAZIL DEBT POSES THORNY ISSUE FOR U.S. BANKSU.S.
URGES BANKS TO WEIGH PHILIPPINE DEBT PLANU.K.
SAYS HAS NO ROLE IN BRAZIL MORATORIUM TALKSTALKING POINT/BANK STOCKSCANADA BANKS COULD SEE PRESSURE ON BRAZIL LOANSTREASURY'S BAKER SAYS BRAZIL NOT tN CRISISBRAZIL'S DEBT CRISIS BECOMING POLITICAL CRISISBAKER AND VOLCKER SAY DEBT STRATEGY WILL WORKI 0.776 761308 13081431 1431104 214950 1042149 13881713 12931388 17131403 501291 13332 129199 9954 1444 541293 3253 691762 1762133 4414 140369 530.37612931308133141388176221496917131041431991291,5444325O140353Table 1: Initial Relevance Ranking (~, = 1) vs. MMR reranking (~L = .7 & X = .3)3.
DOCUMENT REORDERINGWe implemented MMR in two retrieval engines,PURSUIT (an upgraded version of the originalTM retrieval engine inside the Lycos search engine),\[9\] and SMART (the publicly available version of theCornell IR engine) \[1\].
Using the scoring functionsavailable in each system for both Siml and Sim2, weobtained consistent and expected results in thebehavior of the two systems.The results of MMR reranking are shown in Table1.
In this Reuters document collection, article 1403 isa duplicate of 1388.
MMR reranking performs asexpected, for decreasing values of 1, the ranking of1403 drops.
Also as predicted, novel but stillrelevant information as evidenced by document 69starts to increase in ranking.
Relevant, but similar tothe highest ranked documents, such as document1713 drop in ranked ordering.
Document 2149 'sposition varies depending on its similarity topreviously seen information.We also performed a pilot experiment with fiveusers who were undergraduates from variousdisciplines.
The purpose of the study was to find outif they could tell what was the difference between thestandard ranked document order retrieved bySMART and a MMR reranked order with X = ..5.They were asked to perform nine different searchtasks to find information and asked various questionsabout the tasks.
They used two methods to retrievedocuments, known only as R and S. Parallel taskswere constructed so that one set of users wouldperform method R on one task and method S on asimilar task.
Users were not told how the documentswere presented only that either "method R" or"method S" were used and that they needed to be tryto distinguish the differences between methods.
Aftereach task we asked them to record the informationfound.
We also asked them to look at the ranking formethod R and method S and see if they could tell anydifference between the two.
The majority of peoplesaid they preferred the method which gave in theiropinion the most broad and interesting topics.
In thefinal section they were asked to select a searchmethod and use it for a search task.
80% (4 out of 5)chose the method MMR to use.
The person whochose Smart stated it was because "it tends to groupmore like stories together."
The users indicated adifferential preference for MMR in navigation and forlocating the relevant candidate documents morequickly, and pure-relevance ranking when looking atrelated ocuments within that band.
Three of the fiveusers clearly discovered the differential utility ofdiversity search and relevance-only search.
One userexplicitly stated his strategy:"Method R \[relevance only\] groups itemstogether based on similarity and Method S\[MMR re-ranking\] gives a wider array.
I would183use Method S \[MMR re-ranking\] to find a topic... and then use Method R \[relevance-only\] witha specific search from Method S \[MMR re-rankingl to yield a lot of closely related items.
"The initial study was too small to yield statisticallysignificant trends with respect to speed of known-itemretrieval, or recall improvements for broader querytasks.
However, based on our own experience andquestionnaire responses from the five users, weexpect that task demands play a large role withrespect to which method yields better performance.4.
S INGLE DOCUMENT SUMMARIESHuman summarization of documents, sometimescalled "abstraction" is a fixed-length genericsummary, reflecting the key points that the abstractor-- rather than the user -- deems important.
Consider aphysician evaluating a particular chemotherapyregimen who wants to know about its adverse ffectsto elderly female patients.
The retrieval engineproduces several lengthy reports (e.g.
a 300-pageclinical study), whose abstracts do not contain anyhint of whether there is information regarding effectson elderly patients.
A useful summary for thisphysician would contain query-relevant passages (e.g.differential adverse effects on elderly males andfemales, buried in page 211-212 of the clinical study)assembled into a summary.
A different user withdifferent information needs may require a totallydifferent summary of the same document.We developed a minimal-redundancy query-relevant summarizer-by-extraction method, whichdiffers from previous work in summarization \[10, 12,15, 18, 24\] in several dimensions.?
Optional query relevance: as discussed above aquery or a user interest profile (for the vector sumof both, appropriately weighted) is used to selectrelevant passages.
If a generic query-free summaryis desired, the centroid vector of the document iscalculated and passages are selected with theprincipal components of the centroid as the query.?
Variable granularity summarization: Thelength of the summary is under user control.
Briefsummaries are useful for indicative purposes (e.g.whether to read further), and longer ones fordrilling and extracting detailed information.?
Non-redundancy: Information density isenhanced by ensuring a degree of dissimilaritybetween passages contained in the summary.
Thedegree of query-focus vs. diversity sampling isunder user control (the ~, parameter in the MMRformula).Our process for creating single documentsummaries i as follows:1.
Segment a document into passages and index thepassages using the inverted indexing method usedby the IR engine for full documents.
Passagesmay be phrases, sentences, n-sentence chunks, orparagraphs.
For the TIPSTER III evaluation, weused sentences as passages.2.
Within a document, identify the passages relevantto the query.
Use a threshold below which thepassages are discarded.
We used a similaritymetric based on cosine similarity using thetraditional TF-IDF weights.3.
Apply the MMR metric as defined in Section 2 tothe passages (rather than full documents).Depending on the desired length of the summary,select a few or larger number.
If the parameteris not very close to 1, redundant query relevantpassages will tend to be eliminated and otherdifferent, slightly less query relevant passages willbe included.
We allow the user to select thenumber of passages or the percentage of thedocument size (also known as the "compressionratio").4.
Reassemble the selected passages into a summarydocument using one of the following summary-cohesion criteria:?
Document appearance order: Present thesegments according to their order of presentationin the original document.
If the first sentence islonger than a threshold, we automatically includethis sentence in the summary as it tends to set thecontext for the article.
If the user only wants toview a few segments, the first sentence must alsomeet a threshold for sentence rank to beincluded.?
News-story principle: Present he information inMMR-ranked order, i.e., the most relevant andmost diverse information first.
In this manner,the reader gets the maximal information even ifthey stop reading the summary.
This allows thediversity of relevant information to be presentedearlier and topic introduced may be revisitedafter other relevant topics have been introduced.?
Topic-cohesion principle: First group togetherthe document segments by topic clustering (usingsub-document similarity criteria).
Then rank thecentroids of each cluster by MMR (mostimportant first) and present the information, a184topic-coherent cluster at a time, starting with thecluster whose centroid ranks highest.We implemented query-relevant document-appearance-based sequencing of information.
Ourmethod of summarization does not require the moreelaborate language-regeneration needed by KathyMcKeown and her group at Columbia in theirsummarization work \[15\].
As such our method issimpler, faster and more widely applicable, but yieldspotentially less cohesive summaries.
All summaryresults in this paper use the SMART search enginewith stopwords eliminated from the indexed ata andstemming.Query: Delaunay refinement mesh generation finite elementmethod foundations three dimension analysis; ~ = .3\[1\] Delaunay refinement is a technique for generatingunstructured meshes of triangles or tetrahedra suitable foruse in the finite element method or other numerical methodsfor solving partial differential equations.\[5\] The purpose of this thesis is to further this progress bycementing the foundations of two-dimensional Delaunayrefinement, and by extending the technique and its analysisto three dimensions.\[15\] Nevertheless, Delaunay refinement methods fortetrahedral mesh generation have the rare distinction thatthey offer strong theoretical bounds and frequently performwell in practice.\[39\] If one can generate meshes that are completelysatisfying for numedcal techniques like the finite elementmethod, the other applications fall easily in line.\[131\] Our understanding of the relative merit of differentmetrics for measuring element quality, or the effects of smallnumbers of poor quality elements on numedcal solutions, isbased as much on engineedng expedence and rumor as it ison mathematical foundations.\[158\] Delaunay refinement methods are based upon a well-known geometric construction called the Delaunaytriangulation, which is discussed extensively in the meshgeneration chapter.\[201\] I first extend Ruppert's algorithm to three dimensions,and show that the extension generates nicely gradedtetrahedral meshes whose circumradius-to-shortest edgeratios are nearly bounded below two.\[2250\] Refinement Algorithms for Quality Mesh Generation:Delaunay refinement algodthms for mesh generationoperate by maintaining a Delaunay or constrained Delaunaytriangulation, which is refined by inserting carefully placedvertices until the mesh meets constraints on element qualityand size.\[3648\] I do not know to what difference between thealgorithms one should attribute the slightly better bound forDelaunay refinement, nor whether it marks a real differencebetween the algodthms or is an artifact of the differentmethods of analysis.Figure 1: Generic MMR- generated summary ofdissertation.Query: sliver mesh boundary removal small angles; ;L = .7\[1\] Delaunay refinement is a technique for generatingunstructured meshes of tdangles or tetrahedra suitable foruse in the finite element method or other numerical methodsfor solving partial differential equations.\[129\] Hence, many mesh generation algorithms take theapproach of attempting to bound the smallest angle.\[2621\] Because s is locked, inserting a vertex at c will notremove t from the mesh.\[2860\] Of course, one must respect the PSLG; small inputangles cannot be removed.\[3046\] The worst slivers can often be removed by Delaunayrefinement, even if there is no theoretical guarantee.\[3047\] Meshes with bounds on the circumradius-to-shortestedge ratios of their tetrahedra re an excellent starting pointfor mesh smoothing and optimization methods designed toremove slivers and improve the quality of an existing mesh(see smoothing section).\[3686\] If one inserts a vertex at the circumcenter of eachsliver tetrahedron, will the algorithm fail to terminate?\[3702\] A sliver can always be eliminated by splitting it, buthow can one avoid creating new slivers in the process?\[3723\] Unfortunately, my practical success in removingslivers is probably due in part to the severe restdctions oninput angle I have imposed upon Delaunay refinement.\[3724\] Practitioners report that they have the most difficultyremoving slivers at the boundary of a mesh, especially nearsmall angles.Figure 2: Focused-query MMR-generated summaryof dissertation.5.
SUMMARIZ INGDOCUMENTSLONGERThe MMR-passage selection ':'method forsummarization works better for longer documents(which typically contain more inherent passageredundancy across document sections such asabstract, introduction, conclusion, results, etc.).
Todemonstrate the quality of summaries that can beobtained for long documents, we summarized anentire dissertation containing 3,772 sentences with ageneric topic query constructed by expanding thethesis title (Figure 1).
In contrast, Figure 2 shows theresults of a more specialized query with a larger Lvalue to focus summarization less on diversity andmore on topic.The above example demonstrates the utility ofquery relevance in summarization a d the incrementalutility of controlling summary focus via the lambdaparameter.
It also highlights a shortcoming ofsummarization by extraction, namely coping withantecedent references.
Sentence \[2621\] refers tocoefficients "s", "c", and "t," which do not makesense outside the framework that defines them.
Suchreferential problems are ameliorated with increasedpassage length, for instance using paragraphs ratherthan sentences.
However, longer-passage s lection185also implies longer summaries.
Another solutionco-reference r solution \[25\].6.
MULT I -DOCUMENT SUMMARIESisAs discussed earlier, MMR passage selectionworks equally well for summarizing single documentsor clusters of topically related documents.
Ourmethod for multi-document summarization followsthe same basic procedure as that of single documentsummarization (see section 4).
In step 2 (Section 4),we identify the N most relevant passages from each ofthe documents in the collection and use them to formthe passage set to be MMR re-ranked.
N is dependenton the desired resultant length of the summary.
Weused N relevant passages from each documentcollection rather than the top relevant passages in theentire collection so that each article had a chance toprovide a query-relevant contribution.
In the futurewe intend to compare this to using MMR rankingwhere the entire document set is treated as a singledocument.
Steps 2, 3 and 4 are primarily the same.The TIPSTER evaluation corpus provided severalsets of topical clusters to which we applied MMRsummarization.
In one such example on a cluster ofapartheid-related documents, we used the topicdescription as the query (see Figure 3) and N was setto 4 (4 sentences per article were reranked).
The top10 sentences for ~ = 1 (effectively query relevance,but no MMR) and k = .3 (both query relevance andMMR anti-redundancy) are shown in Figures 4 and 5respectively.The summaries clearly demonstrate the need forMMR in passage selection.
The 7~ = 1 case exhibitsconsiderable redundancy, ranging from near-replication in passages \[4\] and \[5\] to redundantcontent in passages \[7\] and \[9\].
Whereas the L = .3case exhibits no such redundancy.
Counting clearlydistinct propositions in both cases yields a 20%greater information content for the MMR case,though both summaries are equivalent in length.Topic:<head> Tipster Topic Description<num> Number: 110<dom> Domain: International Politics<title> Topic: Black Resistance Against the South AfricanGovernment<desc> Description:Document will discuss efforts by the black majority in SouthAfdca to overthrow domination by the white minoritygovernment.<smry> Summary:Document will discuss efforts by the black majority in SouthAfrica to overthrow domination by the white minoritygovernment.<narr> Narrative:A relevant document will discuss any effort by blacks toforce political change in South Africa.
The reported blackchallenge to apartheid may take any form -- military,political, or economic -- but of greatest interest would beinformation on reported activities by armed personnel linkedto the African National Congress (ANC), either in SouthAfrica or in bordering states.<con> Concept(s):1.
African National Congress, ANC, Nelson Mandela, OliverTambo2.
Chief Buthelezi, Inkatha, Zulu3.
terrorist, detainee, subversive, communist4.
Limpopo River, Angola, Botswana, Mozambique, Zambia5.
apartheid, black township, homelands, group areas act,emergency regulationsQuery:Black Resistance Against South Afdcan Government blackmajority South Africa overthrow domination white minoritygovernment blacks force political change South Africa blackchallenge apartheid military political economic activitiesarmed personnel African National Congress (ANC) SouthAfrica bordering states African National Congress ANCNelson Mandela Oliver Tambo Chief Buthelezi Inkatha Zuluterrorist detainee subversive communist Limpopo RiverAngola Botswana Mozambique Zambia apartheid blacktownship homelands group areas act emergency regulationsQuery (short version - no narrative or concepts):Black Resistance South Afncan Government black majoritySouth Afnca overthrow domination white minoritygovernmentFigure 3: Topic and Query for Tipster Topic 110186\[1\] [761\] AP880212-0060 \[15\] ANGOP quoted the Angolanstatement as saying the main causes of conflict in the regionare South Africa's "'illegal occupation" of Namibia, SouthAfrican attacks against its black-ruled neighbors and itsalleged creation of armed groups to carry out "'terroristactivities" in those countries, and the denial of political rightsto the black majodty in South Africa.\[2\] [758\] AP880803-0080 \[25\] Three Canadian anti-apartheidgroups issued a statement urging the government to severdiplomatic and economic links with South Africa and aid theAfrican National Congress, the banned group fighting thewhite-dominated government in South Africa.\[3\] [756\] AP880803-0082 \[25\] Three Canadian anti-apartheidgroups issued a statement urging the government to severdiplomatic and economic links with South Africa and aid theAfrican National Congress, the banned group fighting thewhite-dominated government in South Africa.\[4\] [790\] AP880802-0165 \[27\] South Africa says the ANC,the main black group fighting to overthrow South Africa'swhite government, has seven major military bases inAngola, and the Pretona government wants those basesclosed down.\[5\] [654\] AP880803-0158 \[27\] South Africa says the ANC,the main black group fighting to overthrow South Africa'swhite-led government, has seven major military bases inAngola, and it wants those bases closed down.\[6\] [92\] WSJ910204-0176 \[2\] de Klerk's proposal to repealthe major pillars of apartheid rew a generally positiveresponse from black leaders, but African National Congressleader Nelson Mandela called on the internationalcommunity to continue economic sanctions against SouthAfrica until the government takes further steps.\[7\] [781\] AP880823-0069 \[18\] The ANC is the main guerrillagroup fighting to overthrow the South African governmentand end apartheid, the system of racial segregation in whichSouth Africa's black majority has no vote in national affairs.\[8\] [375\] WSJ890908-0159 \[24\] For everywhere he tums, hehears the same mantra of demands -- release, lift bans,dismantle, negotiate -- be it from local anti-apartheidactivists or from foreign governments: release politicalprisoners, like African National Congress leader NelsonMandela; lift bans on all political organizations, such as theANC, the Pan Africanist Congress and the UnitedDemocratic Front; dismantle all apartheid legislation; andfinally, begin negotiations with leaders of all races.\[9\] [762\] AP880212-0060 \[14\] The African NationalCongress is the main rebel movement fighting South Africa'swhite-led government and SWAPO is a black guerrilla groupfighting for independence for Namibia, which is administeredby South Africa.\[10\] [91\] WSJ910404-0007 \[8\] Under an agreement betweenthe South Afncan government and the Afncan NationalCongress, the major anti-apartheid organization, SouthAfrica's remaining political prisoners are scheduled forrelease by April 30.Fig 4: ~ =l.0 Multi Document Summarization\[Rank\] Document ID \[Sentence Number\] Sentence\[1\] [1\] [761\] AP880212-0060 \[15\] ANGOP quoted theAngolan statement as saying the main causes of conflict inthe region are South Africa's "'illegal occupation" ofNamibia, South African attacks against its black-ruledneighbors and its alleged creation of armed groups to carryout "'terrorist activities" in those countries, and the denial ofpolitical dghts to the black majority in South Afdca.\[2\] [2\] [758\] AP880803-0080 \[25\] Three Canadian anti-apartheid groups issued a statement urging the governmentto sever diplomatic and economic links with South Africaand aid the African National Congress, the banned groupfighting the white-dominated government in South Africa.\[3\] [6\] [92\] WSJ910204-0176 \[2\] de Klerk's proposal torepeal the major pillars of apartheid rew a generallypositive response from black leaders, but African NationalCongress leader Nelson Mandela called on the intemationalcommunity to continue economic sanctions against SouthAfdca until the government takes further steps.\[4\] [8\] [375\] WSJ890908-0159 \[24\] For everywhere heturns, he hears the same mantra of demands -- release, liftbans, dismantle, negotiate -- be it from local anti-apartheidactivists or from foreign governments: release politicalprisoners, like African National Congress leader NelsonMandela; lift bans on all political organizations, such as theANC, the Pan Afncanist Congress and the UnitedDemocratic Front; dismantle all apartheid legislation; andfinally, begin negotiations with leaders of all races.\[5\] [4\] [790\] AP880802-0165 \[27\] South Africa says theANC, the main black group fighting to overthrow SouthAfrica's white government, has seven major military bases inAngola, and the Pretoria government wants those basesclosed down.\[6\] [11\] [334\] AP890703-0114 \[14\] The white delegationchief, Mike Olivier, said the ANC members, includingPresident Oliver Tambo and South African Communist Partyleader Joe Slovo, said some white anti-apartheid membersof Parliament could make a difference, although theorganization believes Parliament as a whole is notrepresentative of South Africans.\[7\] [14\] [788\] WSJ880323-0129 \[11\] These included apicture of Oliver Tambo, the exiled leader of the bannedAfrican National Congress; a story about 250 womenattending an ANC conference in southern Afnca; a report onthe cdsis in black education; and an advertisementsponsored by a Catholic group in West Germany thatquoted a Psalm and called for the abolition of torture inSouth Africa.\[8\] [12\] [303\] AP880621-0089 \[8\] There was no immediatecomment from South Africa, which in the past has stagedcross-border raids on Botswana and other neighboringcountries to attack suspected facilities of the AfdcanNational Congress, which seeks to overthrow South Afdca'swhite-led government.\[9\] [24\] [502\] wsJg00510-0088 \[24\] While the membershipof Inkatha, the religiously and politically conservative groupthat is the ANC's chief rival for power in black South Afdca,is overwhelmingly Zulu, Inkatha's leader, MangosuthoButhelezi, has very seldom appealed to sectional tnballoyalties.\[10\] [16\] [593\] AP890821-0092 \[11\] Besides ending theemergency and lifting bans on anti-apartheid groups andindividual activists, the Harare summit's conditions includedthe removal of all troops from South Afnca's blacktownships, releasing all political prisoners and endingpolitical tdals and executions, and a governmentcommitment to free political discussion.Fig 5: ~ =.3 Multi Document Summarization.\[Rank\] [Previous Rank in X = 1.0 Version\] DocumentID \[Sentence Number\] Sentence187<TITLE>Angola Rejects South African Proposal forPeace Talks</TITLE><TEXT>\[1\] Angola has rejected a South Afncan proposal for aregional peace conference that would include Angolanrebels, Angola's official ANGOP news agency reportedFriday.\[14\] ANGOP quoted the Angolan statement as saying themain causes of conflict in the region are South Africa's"'illegal occupation" of Namibia, South African attacksagainst its black-ruled neighbors and its alleged creation ofarmed groups to carry out "'terronst activities" in thosecountries, and the denial of political rights to the blackmajority in South Africa.</TEXT>Figure 6: Single Document Summary AP880212-0060, 10% of document length.As can be seen from the above summaries, multi-document synthetic summaries require support in theuser interface.
In particular, the following issuesneed to be addressed:?
Attributability: The user needs to be able toaccess easily the source of a given passage.This could be the single document summary(see Figure 6).?
Contextually: The user needs to be able tozoom in on the context surrounding the chosenpassages.?
Redirection: The user should be able tohighlight certain parts of the synthetic summaryand give a command to the system indicatingthat these parts are to be weighted heavily andthat other parts are to be given a lesser weight.7.
EVALUATION OF  S INGLEDOCUMENT SUMMARIZAT IONAn ideal text summary contains the relevantinformation for which the user is looking, excludesextraneous information, provides background to suitthe user's profile, eliminates redundant informationand filters out relevant information that the userknows or has seen.
The first step in building suchsummaries i extracting the relevant pieces of articlesto a user query.
We performed a pilot evaluation inwhich we used a database of assessor marked relevantsentences to examine how well a summarizationsystem could extract the relevant sections ofdocuments.Automatically generating text extraction summariesbased on a query or high frequency words from thetext can produce a reasonable looking summary, yetthis summary can be far from the optimal goal ofquality summaries: readable, useful, intelligible,appropriate length summaries from which theinformation that the user is seeking can be extracted.Jones & Galliers define this type of evaluation asintrinsic (measuring a system's quality) compared toextrinsic (measuring a system's performance in agiven task) \[7\].In the past year, there has been a focus in TIPSTERon both the intrinsic and extrinsic aspects ofsummarization evaluation \[4\].
The evaluationconsisted of three tasks (1) determining documentrelevance to a topic for query-relevant summaries (anindicative summary), (2) determining categorizationfor generic summaries (an indicative summary), (3)establishing whether summaries can answer aspecified set of questions (an informative summary)by comparison to an ideal summary.
In each task, thesummaries are rated in terms of confidence indecision, intelligibility and length.
Jing, Barzilay,McKeown and Elhadad \[6\] performed a pilotexperiment (40 sentences) in which they examinedthe performance (precision-recall) of threesummarization systems (one using notion of numberof sentences, the other two using numbers of words ornumber of clauses).
They compared the performanceof these systems against human ideal summaries andfound that different systems achieved their bestperformances at different lengths (compressionratios).
They also found the same results fordetermining document relevance to a topic (one of theTIPSTER tasks) for query-relevant summaries.Our approach to summarization is different fromColumbia and TIPSTER in that the focus is not on an"ideal human summary" of any particular documentcutoff size.
An ideal summarization system must firstbe able to recognize the relevant sentences (or partsof a document) for a topic or query and then be ableto create a summary from these relevant segments.Although a list of words, an index or table ofcontents, is an appropriate label summary and canindicate relevance, informative summaries need atleast noun-verb phrases.
We choose to use thesentence as our underlying unit and evaluatedsummarization systems for the first stage of summarycreation - coverage of relevant sentences.
Othersystems \[16, 23\] use the paragraph as a summary unit.Since the paragraph consists of more than onesentence and often more than one information unit, itis not as suitable for this type of evaluation, althoughit may be more suitable for a construction unit insummaries due to the additional context that itprovides.
For example., paragraphs will often solveco-reference issues, yet provide additional non-relevant information.
One of the issues in188summarization evaluation is how to score (penalize)extraneous non-useful information contained in asummary.Unlike document information retrieval, textsummarization evaluation has not extensivelyaddressed the performance of different methodologiesby evaluating the effects of different components.Most summarization systems use linguisticknowledge as well as a statistical component \[3, 5,16, 23\].
We applied the monolingual informationretrieval method of query expansion \[20, 27, 28\] tosummarization, using parts of the document to expandour queries.
We also performed compressionexperiments.
We used a modified version of the 11-pt average recall/precision (Section 9.2) to evaluateour results.8.
EXPERIMENT DES IGNFor our pilot experiment, we created two data sets,one based on relevant sentence judgments, the otherbased on model summaries (Section 8.1).
We thendefined a modified version of the 11-point averagerecall precision (Section 8.2) to use as our evaluationmeasure.
We then performed experiments asdescribed in Section 9 to evaluate the effects ofMMR, query expansion, and compression.8.1 Data SetsWe created two data sets for our pilot experiments.For the first { 110 Set} we took 50 documents fromthe TIPSTER evaluation provided set of 200 newsarticles spanning 1988-1991.
All these documentswere on the same topic (see Figure 3).
Threeevaluators ranked each of the sentences in thedocument as relevant, somewhat relevant and notrelevant.
For the purpose of this experiment,somewhat relevant was treated as relevant and thefinal score for the sentence was determined by amajority vote.
The sentences that received thismajority vote were tabulated as a relevant sentence(to the topic).
The document was ranked as relevantor not relevant.
All three assessors had 68%agreement in their relevance judgments.
The querywas extracted from the topic (see Figure 3).The second data set {Model Sutures} was providedas a training set for the Question and Answer portionof the TIPSTER evaluation.
It consisted of "modelsummaries" which contained sentences of an articlethat answered a list of questions.
These modelsentences were used to score the summarizer.
Thequery was extracted from the questions.8.2 Evaluation CodeWe modified the 11-pt recall-precision curves \[21\]commonly used for document information retrieval.Since many documents only have a few relevantsentences, corresponding curves for summarizationhave a lot of intervals with missing data items.
Toremedy this situation, we implemented a step functionfor the precision values.
This allowed the recallintervals that would not naturally be filled to beassigned an actual precision value.
For example, inthe case of two relevant sentences in the document,points 0-5 (the first five intervals) would all have thefirst precision value (naturally occurring at point 5)and points 6-10 (the second value), the second value(naturally occurring at point 10).
We interpolated theresults of each query for the composite graph to formmodified interpolated recall-precision curves.In order to account for the fact that a compressedsummary does not have the opportunity to return thefull set of relevant sentences, we use a normalizedversion of recall and a normalized version of F1 asdefined below.Given:Rel = Number of Relevant Sentences inDocumentRelSum = Number of Relevant Sentences inSummarySentSum = Number of Sentences inSummaryDefinitions:Precision P = RelSum / SentSumRecall R = RelSum / RelF1 = 2P*R / (P + R)NorR = RelSum / rain (Rel, SentSum)NorF1 = 2P*NorR/(P+NorR)9.
EXPERIMENTS AND RESULTSIn this section we describe the experiments weperformed and results obtained in evaluating thediversity gain - MMR (Section 9.1), query expansion(Section 9.2) and compression (Section 9.3).9.1 MMR (Diversity Gain)In order to evaluate what the relevance loss for theMMR diversity gain in single documentsummarization, we created summaries for twodocument length percentages (measured by number ofsentences) and determined how many relevantsentences the summaries contained.189Sentence PrecisionPercentage ofDocument CMULength Relevant10%10%10%10%25%25%25%25%TREC andCMURelevant1.0 0.780.7 0.760.3 0.74Baseline 0.741.0 0.740.7 0.730.3 0.74Baseline 0.600.830.830.790.830.760.740.760.65Table 2: Precision Scorestasknumber of documentssourcerelevant documentsaverage sentences per documentmedian sentences per documentmaximum sentences per documentminimum sentences per documentquery formationstatisticspercent of document lengthsummary includes first sentenceaverage summary size (sentences)median summary size (sentences)I Model SummariesQ&A48provided by Tipsterall22.6195111provided questionsall documents19.4%72%4.34110 Setindicative summaries503 people marked each sentence1525.1235O11topic40 documents24.9%47%, 73% (only relevant docs)6.15Table 3: Data Set Comparison\[ \ [11~'~ i l \ [~ \ ]  I I1 I~ l I ~'~o\] I ~ \[,~t'/~ I l i  I I11\[~l I II I 1;\] III-, \] \[I\] lD  |~  \ [a 'g l  I l l  I l lT l~ I l l  iT:\] I I \ [number of documentsaverage sentences per documentmedian sentences per documentmaximum sentences per documentminimum sentences per documentpercent of document lengthsummary includes first sentenceaverage summary size (sentences)median summary size (sentences)1527.523511536.2%73%10.172523.823441117.7%32%3.74Table 4:110 Set - Relevant vs. Non-Relevant Documents with relevant sentences.The results are given in Table 2 for documentpercentages 0.25 and 0.1.
Two precision scores werecalculated, (1) that of TREC relevance plus at leastone CMU assessor marking the document as relevant(yielding 23 documents) and (2) at least two of thethree CMU assessor marking the document asrelevant (yielding 15 documents).
From these scoreswe can see there is no significant statistical differencebetween the ~,=1, ~,=.7, and 3.=.3 scores.
This is oftenexplained by cases where the L=l article failed topick up a piece of relevant information and thereranking of k=.7 or .3 might or vice versa.
Thebaseline (baseln) contains the first N sentences of thedocument, where N is the number of sentences in thesummary.9.2 Query ExpansionWe expanded the original queries by: (1) addingthe highest ranked sentence of the document (a formof pseudo-relevance feedback), (2) adding the title,and (3) adding the title and the highest rankedsentence.The most significant effects were shown for shortqueries (see Figures 7, 9).
For the longer queries, theeffect was less (see Figures 8, 10).
For 20%document length (characters rounded up to thesentence boundary) adding the highest rankedsentence (prf) and title to the query helpsperformance for the 110 set relevant summaryjudgments (Figures 7, 8).
For 10% document length,190~EO13.> <"OO ?-Figure7:10.8 ~0.60.40.2Query Expansion Effects - 110 Set, SMART weighting Inn, 20% document length, relevant documents onlyI I I Ishort_query -e - -short._query+prf --~--shod_query+title -E~--c~ 4----_______,.
short_query+prf+title ---x ......:..--..::::::~:-,:~---~,,~ ~ -~.... "  first 20% of document --~-.-.........~',~ ".
"~.~ "'::"":::" ............. X..-"~N \ \  "'~ 4--~~- "'" " ...... .
.
.
.
I::~: : :~:-, "'"-X ..................... -X.............~\ ~ ^ ~ ", X.\ \\ \ \  ~ ", "..~'" '" ' " '~"- .  "
-, ~'~ "~ '~.
~ "19" .
.
.
.
.
.
.
.
.
'40 I I I0 0.2 0.4 0.6 0.8Normalized Recall013_> <0E0.80.60.40.2Figure 8: Query Expansion Effects - SMART weighting Inn, 20% document lengthI I I I110 Set relevants docs, query o. .
.
.
.
.
.
.
.
~_ _ 110 Set relevants docs, query+prf+title -4----.
.
.
.
.
.
.
.
.
.
.
,~.,.~ - .
.
.
.
~ 110 Set relevants docs, short query -E]--i:---'.-.:::::~-...~S-.
:.,..~ ~.
.
.
.
.
.
.^  110 Set relevants docs, short query+pff+title --x ......~"--..:~ ..... "-.. v ~ Model Summs query --~---.................... x..................... x : ~i-,,~'~i: L;~.
"'-+ .
.
.
.
.
.
.
.
-+,~, Model_Summs-q ue ry+pff+title -~-.-Q, "<,, ....... ~"  -+,"?% ~'~ ~ "'"''X ............... " "~.
~, ......... .......... - .
, .
[]  .......... .~..:~:,.
\ ?,.
,,,'-~ ",.,,, ~ .......... ",,~:~.
"-,.
~ ?
", "~: "% "% ?"~;(.,.
"",, ~ ' .
.
.
,I- .
.
.
.
.
.
.
.'~:~.,.
"",,, "\......... L:~:::E.E.LS.~.LT_:L:L.::~I I I ?0.2 0.4 0.6 0.8Normalized Recall1911.9 (/)Q.o)"ooe-Figure 9: Query  Expans ion  Effects - 110 Set, SMART weight ing Inn, 10% document  length, re levant documents  onlyI I I Ishort_query oshort_query+prf  -~--,short_query+t i t le  -B - -short query+prf+t i t le  .-x ......first 1~/,~ of document  -~---0.8 .
:'..-..~ ::..-.
::..-..~ T.~..-.
::::..~ :.T.7.::_~-r..-.
r c - - = ~" .
.
- -~  - - + .
.
.
.
.
.
.
.
--w ' .
.
.
.
.
.
.
.
.
.
- ~ ~ ~ ~  .
.
.
.
.
.
.
.
.
.
~c  ....... ; i i  ' ~ -  "R~?
"\k  .
.
.
.
.
.
.
.
E} .
.
.
.
.
.
.
.
.
.
.
E3 -~-'~.
; : - \ -  .13].\ ?
., \ B .
.0.6 ~.
,  ~ ...... ", "-m .
.
.
.
.
.
.
.
.
.
~3 .
.
.
.
.
.
.
.
.
.  '"'
--... ~ ~""~; ,"'""'" - ~%' ' -~  .
.
.
.
~_ .
.
.
.
.
- |o?4 %'" - .
.
"~""" - - -~-  o I" " .~ , ,  .
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.0 I I l I0.20 0.2 0.4 0.6 0.8Normal i zed  Recal lEOE~13_<9.~9E0.80.60.40.2Figure 10: Query  Expans ion  Effects - SMART weight ing Inn, 10% document  length, re levant  documents  only1 I I I I110 Set  re levant  docs,  query  o110 Set  re levant  docs,  query+prf+t i t le  -~-.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
~.~--.--....~,.......
110 Set re levant  docs,  short  query  -B - -F:Z:Z'.
'Z':-~7~TL-:L'~'--:::~:C'7~.~.c_:~,~,__ .
1 10 Set re levant docs,  short query+prf+t i t le  --x ......t .
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
E} .
.
.
.
.
.
.
.
.
.
B.-"~'<'--.~_.
.
.
.
.
.
.
T-tO, Model  Summs query  --A-.-~- .................... -x..................... x ..........
~.~.~......x.~.::"-'-':.
'~ "~- .
.
.
.
.
.
.
.
.
.
~",C,~, Mode l_Summs-query+pr f+t i t le  -~-.- ?
.......... ; ; "  .
.
.
.
.
.
.
.
--'~:~.
"~".C,~.
'k',, ""X~.  "
...............-....'~"~' ' ' ' " ' "E~ ......... ~ ..................... X ....................." '~ .~ ""  " ~'__-.~... = ~ __.
__ ~ .
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
"A .
.
.
.
.
.
.
.
.
~'L ~ ~-.~-.~.~-.~-.-~.~-.~.~.~.~.0 I I i i0 0 .2  0 .4  0 .6  0 .8Normal i zed  Recall192C~ .o(n0_?3r)- loo0.80.60.40.2Figure 11 : Compression Effects - SMART weighting Inn, 110 Set relevant documents onlyi I I Isummary = .1 doc length o-_~......._.....~._.
.
.
.
.
.
summary = .2 doc length -~--.- ~ m ~ , , ~ .
.
summary = .3 doc length -G--.. .
.
.
"~- .
.~<.
.
.~_  summary = .4 doc length --x ......?
.
,  ..:.,~'~ - .
.
.
.
.
.
.
~ summary = .5 doc length -~---'x "--.
?
~x?
~ X ,, ~'x?
,, , , , ; , , ,"%, "% "~.
'~  .
.
.
.
.
_\]2::I I f f0.2 0.4 0.6 0.8Normalized Recall"O._NEOz0.80.60.40.200Figure 12: Compression Effects - SMART weighting InnI I i I110 Set query o110 Set query+prf+title -+---110 Set short query -D--.110 Set short query+prf+title .-x- .....e~.~,~.. 110 Set Baseline, first N sentences of Document -~---- Model Summs query -~-.-of Document -~--- sentences "'~,~.~'~-,, Model Summs Baseline, first N" ' .
.  "
"~:~ .
.
.
.
,~, .
.
.
.
.  "
' "ED .
.
.
.
.
".-.
".. .
.
.
.
.
.
A .
.
.
.
.
.
':L:.~:~" '~  - .
"~.o .
.| I I I0.2 0.4 0.6 0.8Summary Length as Proportion of Document Length193for short queries just adding the title performed betterthan prf and the title (Figures 9,10).
We willdetermine if these results hold over more extensivedata.These results are similar to those obtained fordocument information retrieval \[27\].
Since 72% ofthe first sentences were marked relevant (Table 3),one area we plan to explore is results using the firstsentence in the summary and/or query under specifiedcircumstances, uch as our first sentence heuristics(Section 4).9.3 CompressionAn important evaluation criteria for summarizationis what is the ideal summary output length(compression of the document) and how does itaffects the user's task.
To begin looking at this issue,we evaluated the performance of our system atdifferent summary lengths as a percentage of thedocument length.We used a document compression factor based onthe number of characters in the document.
If thiscutoff fell in the middle of a sentence the rest of thesentence was allowed, thus the output summary endsup being slightly longer than the actually compressionfactor.The data set statistics are shown in Tables 3 and 4.Note that non-relevant documents (Table 4) still havea high percentage of relevant sentences.
Tendocuments in the 110 set were non-relevant and hadno relevant sentences.
We also see that the summarylength or number of relevant sentences chosen perdocument varies significantly.Summaries were compared using the modifiedinterpolated normalized recall-precision curve aspreviously described (Section 8.2).In Figure 11, we examine the effect of compressionon normalized recall and precision and in Figure 12,we show a plot of normalized F1.
This F1 graphindicates that the normalized F1 score is helped byhaving the pseudo-relevance fe dback and title in thequery thereby extracting relevant sentences thatwould otherwise be missed.
As the number ofsentences that are allowed in the summary grows, thedifficulty of finding relevant sentences grows andthus the added prf sentence and title to the query helpto find relevant sentences for their particulardocument.
We need to do more studying on theeffects of query expansion and compression onsummarization, as well as see how our preliminaryresults hold for additional data sets.If we calculate the normalized F1 score for the firstsentence retrieved in the summary, we obtain a scoreof .80 for 110 Set standard query, .67 for 110 Setshort query and .79 for the Model Summaries.
Thisindicates that even for the short query we obtain arelevant sentence two thirds of the time.
However,ideally this first sentence retrieval score would be 1.0and we will explore methods to increase this score aswell as select a "highly relevant" first retrievedsentence for the document.10.
CONCLUSIONWe have shown that MMR ranking provides auseful and beneficial manner of providinginformation to the user by allowing the user tominimize redundancy.
This is especially true in thecase of query-relevant multi-document summarizationin this one data collection.
We are currentlyperforming studies on how this extends to additionaldocument collections.
In the future we will also beinvestigating how to handle co-reference in oursystem as well as analyzing the most suitable ~,par/maeters and clustering the output results.Text Summarization is still in the infant stage in termsof evaluation.
Many monolingual documentinformation retrieval results can be applied to textsummarization, but as of yet, there has been littleevaluation of these techniques.
This pilot experimentshowed many areas that need to be examined infurther detail, including whether the summary selectsthe most relevant sentences in the document andwhether these results generalize to more data sets andother document genres.
We also plan to explorefurther the effects of query expansion using WordNet,as well as the use the first sentence (for news stories)in the query and/or summary.
We also plan to runexperiments fixing the number of sentences for eachdocument as the number of relevant sentences chosenby the assessors as well as a small number, such asthree.
We are currently in the process of building amore extensive sentence relevance database forfurther evaluation.
In this database, we are collectingdata on the user selected most relevant sentence(s) foreach document.
We also plan to explore how to jointhe relevant sections to provide a "good",understandable, readable, relevant, non-redundantsummary.194REFERENCES\[1\].
C. Buckley, Implementation of the SMARTInformation Retrieval System.
Department of ComputerScience Technical Report Comell University, TR 85-686.\[2\].
J.G.
Carbonell, and J. Goldstein, The Use of MMR,Diversity-Based Reranking for Reordering Dotalments andProducing Summaries, InProceedings of SIGIR 98,Melbourne, Australia, 24-28 August 1998, p. 335-336.\[3\] J. Cowie, K. Mahes, S. Nirenbug, R. Zajae, MINDS --Multi-lingual Interactive Document SummariT~rion, AAAIIntelligent Text Summarization Workshop, p. 131-1328,Stanford, CA March 1998\[4\] T.F.
Hand, A Proposal for Task-Based Evaluation ofText Summarization Systems In ACIIIEACIL,-97Summarization Workshop}, 31-36, Madrid., Spain., July1997.\[5\] E. Hovy and C.Y.
Lin, Automated Text Summarizationin SUMMARIST, In ACL/EACL-97 SummarizationWorkshop, 18-24, Madrid, Spain July 1997\[6\] H. Jing, R. Barzilay, K. McKeown, NIl.
Elhadad,Summarization Evaluation Methods E~iments  andAnalysis, AAAI Intelligent Text Summarization Workshop,p.
60-68, Stanford, CA March 1998\[7\].
K.S.
Jones and J.R. Galliers, Evaluation NaturalLanguage Processing Systems: an Analysis and Review.New York: Springer 1996\[8\].
J.L Klavans and J. Shaw, Lexical Semantics inSummarization, In Proceedings of the First AnnualWorkshop of the IFIP Working Group FOR NLP and KR,Nantes, France, April 1995.\[9\].
G. Kowalski, Information Retrieval Systems: Theoryand Implementation, Kluwer Academic Publishers, 1997.\[10\].
J.M.
Kupiec, J. Pedersen, J. and F. Chen, ATrainable Document Summarizer, In Proceedings of the18th Annual Int.
ACM/SIGIR Conference on Researchand Development i  IR, Seattle, WA, July 1995, pp.
68-73.\[11\] D.D.
Lewis, B. Croft, B., and N. Bhandaru,"Language-Oriented Information Retrieval," InternationalJournal oflntelligent Systems, Vol 4 (3), Fall 1989.\[12\] H.P.
Luhn, Automatic Creation of LiteratureAbstracts, IBM Journal, 1958, pp.
159-165.\[13\] M.L Mauldin, Retrieval Performance in FERRET: AConceptual Conference on Research and Development inInformation Retrieval, Proceedings of the 14thInternational Conference on Research and Development iInformation Retrieval, October 1991.\[14\].
M.L.
Mauldin and J.R. Leavitt, Web Agent RelatedResearch at the Center for Machine Translation.
InProceedings of SIGNIDR V, McLean Virginia, August1994.\[15\].
K. McKeown, J. Robin, and K. Kukich, EmpiricallyDesigning and Evaluating aNew Revision-based Model forSummary Generation.
In Information Processing andManagement, 31 (5) 1995.\[16\] M. Mitra, A. Singhal and C. Buckley, Automatic TextSummarization by Paragraph Extraction, In ACL/EACL-97Summarization Workshop, 39-46, Madrid, Spain July 1997.\[17\] C.D.
Paice, Automatic Generation of LiteratureAbstracts - An Approach Based on the Indification of Self-Indicated Phrases, in Information Retrieval Research, R.N.Oddy, S.E.
Robertson, C.J.
van Rijsbergen and P.W.Williams, editors, Butterworths, London, 1981, 172-191.\[18\].
C.D.
Paice, Constructing Literature Abstracts byComputer: Techniques and Prospects, In InformationProcessing and Management, Vol.
26, 1990, pp.
171-186.\[19\] G. Salton G and C. Buckley Improving retrievalperformance by relevance feedback.
Journal of AmericanSociety for Information Sciences, 41:288-297, 1990.\[20\].
G. Salton Automatic Text Processing: TheTransformation, Analysis, and Retrieval of Information byComputer, Addison-Wesley 1989.\[21\].
G. Saiton and M.J. McGill, Introduction to ModernInformation Retrieval, McGraw-Hill, New York, McGraw-Hill Computer Science Series, 1983.\[22\] G. Salton, A. Singhal, M. Mitra,.
and C. Buckley,Automatic Text Structuring and Summarization,Information Processing and Management, 33(2), 193-208,1997.\[23\] T. Strzalkowski, J. Wang, and B.
Wise, A RobustPractical Text Summarization, AAAI Intelligent TextSummarization Workshop, p. 26-3, Stanford, CA March1998.\[24\] J.I.
Tait, Automatic Summarizing of English Texts,PhD dissertation, University of Cambridge, 1983.\[25\] TIPSTER Text Phase II1 18-Month Workshop,Fairfax, VA 4-6 May, 1988,\[26\] C.J.
van Riesburg, Information Retrieval, LondonButterworths 1979.\[27\] J. Xu and B. Croft.
Query expansion using local andglobal document analysis in 19th Ann Int ACM SIGIRConference on Research and Development in InformationRetrieval (SIGIR '96), pages 4-11, 1996.\[28\] E.M. Vorhees.
Using Wordnet o disambiguate wordssenses for text retrieval.
In Proceedings ofACM SIGIRConference ( SIGIR '93)pages, 171-180, 1993.195
