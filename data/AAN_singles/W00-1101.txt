Adapt ing a synonym database to specif ic domainsDav ide  Turcato  Fred Popowich  Jan ine  TooleDan Pass Dev lan  N icho lson  Gordon T i shergavagai Technology Inc.P.O.
374, 3495 Ca~abie Street, Vancouver, British Columbia, V5Z 4R3, CanadaandNatural Language Laboratory, School of Computing Science, Simon Fraser University8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada{turk, popowich ,toole, lass, devl an, gt i sher}@{gavagai, net, as, sfu.
ca}Abst ractThis paper describes a method foradapting ageneral purpose synonymdatabase, like WordNet, to a spe-cific domain, where only a sub-set of the synonymy relations de-fined in the general database hold.The method adopts an eliminativeapproach, based on incrementallypruning the original database.
Themethod is based on a preliminarymanual pruning phase and an algo-rithm for automatically pruning thedatabase.
This method has been im-plemented and used for an Informa-tion Retrieval system in the aviationdomain.1 In t roduct ionSynonyms can be an important resource forInformation Retrieval (IR) applications, andattempts have been made at using them toexpand query terms (Voorhees, 1998).
Inexpanding query terms, overgeneration is asmuch of a problem as incompleteness or lackof synonym resources.
Precision can dramat-ically drop because of false hits due to in-correct synonymy relations.
This problem isparticularly felt when IR is applied to docu-ments in specific technical domains.
In suchcases, the synonymy relations that hold in thespecific domain are only a restricted portionof the synonymy relations holding for a givenlanguage at large.
For instance, a set of syn-onyms like(1) {cocaine, cocain, coke, snow, C}valid for English, would be detrimental in aspecific domain like weather eports, whereboth snow and C (for Celsius) occur very fre-quently, but never as synonyms of each other.We describe a method for creating a do-main specific synonym database from a gen-eral purpose one.
We use WordNet (Fell-baum, 1998) as our initial database, and wedraw evidence from a domain specific corpusabout what synonymy relations hold in thedomain.Our task has obvious relations to wordsense disambiguation (Sanderson, 1997) (Lea-cock et al, 1998), since both tasks are basedon identifying senses of ambiguous words ina text.
However, the two tasks are quite dis-tinct.
In word sense disambiguation, a set ofcandidate senses for a given word is checkedagainst each occurrence of the relevant wordin a text, and a single candidate sense is se-lected for each occurrence ofthe word.
In oursynonym specialization task a set of candidatesenses for a given word is checked against anentire corpus, and a subset of candidate sensesis selected.
Although the latter task could bereduced to the former (by disambiguating alloccurrences of a word in a test and takingthe union of the selected senses), alternativeapproaches could also be used.
In a specificdomain, where words can be expected to bemonosemous to a large extent, synonym prun-ing can be an effective alternative (or a com-plement) to word sense disambiguation.From a different perspective, ourtask is also related to the task of as-signing Subject Field Codes (SFC) toa terminological resource, as done byMagnini and Cavagli~ (2000) for WordNet.Assuming that a specific domain correspondsto a single SFC (or a restricted set of SFCs,at most), the difference between SFC as-signment and our task is that the formerassigns one of many possible values to a givensynset (one of all possible SFCs), while thelatter assigns one of two possible values (thewords belongs or does not belong to the SFCrepresenting the domain).
In other words,SFC assignment is a classification task, whileours can be seen as either a filtering orranking task.Adopting a filtering/ranking perspectivemakes apparent hat the synonym pruningtask can also be seen as an eliminative pro-cess, and as such it can be performed incre-mentally.
In the following section we willshow how such characteristics have been ex-ploited in performing the task.In section 2 we describe the pruningmethodology, while section 3 provides a prac-tical example from a specific domain.
Con-clusions are offered in section 4.2 Methodo logy2.1 Out l ineThe synonym pruning task aims at improv-ing both the accuracy and the speed of a syn-onym database.
In order to set the terms ofthe problem, we find it useful to partition theset of synonymy relations defined in WordNetinto three classes:.
Relations irrelevant to the specific do-main (e.g.
relations involving words thatseldom or never appear in the specific do-main).
Relations that are relevant but incorrectin the specific domain (e.g.
the syn-onymy of two words that do appear in thespecific domain, but are only synonymsin a sense irrelevant o the specific do-main);3.
Relations that are relevant and correct inthe specific domain.The creation of a domain specific databaseaims at removing relations in the first twoclasses (to improve speed and accuracy, re-spectively) and including only relations in thethird class.The overall goal of the described methodis to inspect all synonymy relations in Word-Net and classify each of them into one of thethree aforementioned classes.
We define asynonymy relation as a binary relation be-tween two synonym terms (with respect to?
a particular sense).
Therefore, a WordNetsynset containing n terms defines ~11 k syn-onym relations.
The assignment of a syn-onymy relation to a class is based on evidencedrawn from a domain specific corpus.
We usea tagged and lemmatized corpus for this pur-pose.
Accordingly, all frequencies used in therest of the paper are to be intended as fre-quencies of ( lemma, tag) pairs.The pruning process is carried out in threesteps: (i) manual pruning; (ii) automaticpruning; (iii) optimization.
The first twosteps focus on incrementally eliminating in-correct synonyms, while the third step focuseson removing irrelevant synonyms.
The threesteps are described in the following sections.2.2 Manua l  p run ingDifferent synonymy relations have a differentimpact on the behavior of the application inwhich they are used, depending on how fre-quently each synonymy relation is used.
Rela-tions involving words frequently appearing ineither queries or corpora have a much higherimpact (either positive or negative) than re-lations involving rarely occurring words.
E.g.the synonymy between snow and C has ahigher impact on the weather eport domain(or the aviation domain, discussed in this pa-per) than the synonymy relation between co-caine and coke.
Consequently, the precision ofa synonym database obviously depends muchmore on frequently used relations than onrarely used ones.
Another important consid-eration is that judging the  correctness of agiven synonymy relation in a given domain isoften an elusive issue: besides clearcut cases,there is a large gray area where judgmentsmay not be trivial even for humans evalua-tots.
E.g.
given the following three senses ofthe noun approach(2) a.
{approach, approach path, glidepath, glide slope}(the final path followed by an air-craft as it is landing)b.
{approach, approach shot}(a relatively short golf shot in-tended to put the ball onto theputting green)c. {access, approach}(a way of entering or leaving)it would be easy to judge the first and secondsenses respectively relevant and irrelevant othe aviation domain, but the evaluation of thethird sense would be fuzzier.The combination of the two remarks aboveinduced us to consider a manual pruningphase for the terms of highest 'weight' as agood investment of human effort, in terms ofrate between the achieved increase in preci-sion and the amount of work involved.
Asecond reason for performing an initial man-ual pruning is that its outcome can be usedas a reliable test set against which automaticpruning algorithms can be tested.Based on such considerations, we included amanual phase in the pruning process, consist-ing of two steps: (i) the ranking of synonymyrelations in terms of their weight in the spe-cific domain; (ii) the actual evaluation of thecorrectness of the top ranking synonymy re-lation, by human evaluators.2.2.1 Rank ing  of  synonymy re lat ionsThe goal of ranking synonymy relations isto associate them with a score that estimateshow often a synonymy relation is likely tobe used in the specific domain.
The inputdatabase is sorted by the assigned scores, andthe top ranking words are checked for manualpruning.
Only terms appearing in the domainspecific corpus are considered at this stage.In this way the benefit of manual pruning ismaximized.
Ranking is based on three sortingcriteria, listed below in order of priority.Cr i te r ion  1.
Since a term that does ap-pear in the domain corpus must have at leastone valid sense in the specific domain, wordswith only one sense are not good candidatesfor pruning (under the assumption of com-pleteness of the synonym database).
There-fore .polysemous terms are prioritized overmonosemous terms.Cr i te r ion  2.
The second and third sort-ing criteria axe similar, the only difference be-ing that the second criterion assumes the ex-istence of some inventory of relevant queries(a term list, a collection of previous queries,etc.
), ff such an inventory is not available, thesecond sorting criterion can be omitted.
If theinventory is available, it is used to check whichsynonymy relations are actually to be used inqueries to the domain corpus.
Given a pair(ti,tj) of synonym terms, a score (which wename scoreCQ) is assigned to their synonymyrelation, according to the following formula:(3) scoreCQij =(fcorpusi * fqueryj) +(fcorpusj ?
fqueryi)where fcorpusn and fqueryn are, respec-tively, the frequencies of a term in the domaincorpus and in the inventory of query terms.The above formula aims at estimating howoften a given synonymy relation is likely tobe actually used.
In particular, each half ofthe formula estimates how often a given termin the corpus is likely to be matched as a syn-onym of a given term in a query.
Consider,e.g., the following situation (taken form theaviation domain discussed in section 3.1):(4) fcorpuSsnow = 3042f querysnow = 2fcorpusc = 9168f queryc = 0It is estimated that C would be matched18336 times as a synonym for snow (i.e 9168* 2), while snow would never be matched asa synonym for C, because C never occurs asa query term.
Therefore scoreCQs,~ow,c is18336 (i.e.
18336 + 0).Then, for each polysemous term i andsynset s such that i E s, the following score iscomputed:Table 1: Frequencies of sample synset erms.j fcorpusj fqueryjcocaine 1 0cocain 0 0coke 8 0C 9168 0(5) scorePolyCQ i,~ =E{scoreCQi,~lj ~ s A i ?
j}E.g., i f  ,5' is the synset in (1), thenscorePolyCQs~ow,s is "the sum ofscoreCQsnow,coc~ine, scoreCQsnow,eocain,scoreCQsnow,eoke and scoreCQ,no~o,c.
Giventhe data in Table 1 (taken again from ouraviation domain) the following scoreCQwould result:(6) scoreCQsnow,cocaine -~2scoreCQsnow,cocain = 0scoreCQs~ow,cok~ = 16scoreCQsno~o,c = 18336Therefore, scorePolyCQsnow,s would equal18354.The final score assigned to each polysemousterm tl is the highest scorePolyCQi,s.
Forsnow, which has the following three senses(7) a.
{cocaine, cocaine, coke, C, snow}(a narcotic (alkaloid) extractedfrom coca leaves)b.
{snow}(a layer of snowflakes (white crys-tals of frozen water) covering theground)c. {snow, snowfall}(precipitation falling from cloudsin the form of ice crystals)the highest score would be the one computedabove.Cr i ter ion 3.
The third criterion assignsa score in terms of domain corpus frequencyalone.
It is used to further rank terms thatdo not occur in the query term inventory (orwhen no query term inventory is available).
Itis computed in the same way as the previousscore, with the only difference that a value of1 is conventionally assumed for fquery (thefrequency of a term in the inventory of queryterms).2.2.2 Correctness  evaluat ionAll the synsets containing the top rank-ing terms, according to the hierarchy of crite-ria described above, are manuMly checked forpruning.
For each term, all the synsets con-taining the term are clustered together andpresented to a human operator, who exam-ines each (term, synset) pair and answers thequestion: does the term belong to the synsetin the specific domain?
Evidence about theanswer is drawn from relevant examples auto-matically extracted from the domain specificcorpus.
E.g., following up on our example inthe previous section, the operator would bepresented with the word snow associated witheach of the synsets in (7) and would have toprovide a yes/no answer for each of them.
Inthe specific case, the answer would be likelyto be 'no' for (7a) and 'yes' for (75) and (7c).The evaluator is presented with all thesynsets involving a relevant term (eventhose that did not rank high in terms ofscorePoIyCQ) in order to apply a contrastiveapproach.
It might well be the case that thecorrect sense for a given term is one for whichthe term has no synonyms at all (e.g.
7b inthe example), therefore all synsets for a giventerm need to be presented to the evaiuatorin order to make an informed choice.
Theevaluator provides a yes/no answer for all the(term, synset) he/she is presented with (withsome exceptions, as explained in section 3.1).2.3 Automat ic  p run ingThe automatic pruning task is analogous tomanual pruning in two respects: (i) its in-put is the set of synonymy relations involvingWordNet polysemous words appearing in thedomain specific orpus; (ii) it is performed byexamining all (term, synset) input pairs andanswering the question: does the term belongto the synset in the specific domain?
How-ever, while the manual pruning task was re-garded as a filtering task, where a human eval-4uator assigns a boolean value to each pruningcandidate, the automatic pruning task canbe more conveniently regarded as a rankingtask, where all the pruning candidates are as-signed a score, measuring how appropriate agiven sense is for a given word, in the do-main at hand.
The actual pruning is left asa subsequent step.
Different pruning thresh-olds can be applied to the ranked list, basedon different considerations (e.g.
depending onwhether astronger emphasis i  put on the pre-cision or the recall of the resulting database).The score is based on the frequencies of bothwords in the synset (except the word underconsideration) and words in the sense gloss.We also remove from the gloss all words be-longing to a stoplist (a stoplist provided withWordNet was used for this purpose).
The fol-lowing scoring formula is used:(8) (average_synset_frequeney/synset_cardinality k) .4-(average_gloss_frequency~gloss_cardinality :)Note that the synset cardinality does notinclude the word under consideration, reflect-ing the fact the word's frequency is not usedin calculating the score.
Therefore a synsetonly containing the word under considerationand no synonyms is assigned cardinality 0.The goal is to identify (term, sense) pairsnot pertaining to the domain.
For this rea-son we tend to assign high scores to candi-dates for which we do not have enough evi-dence about their inappropriateness.
This iswhy average frequencies are divided by somefactor which is function of the number of av-eraged frequencies, in order to increase theScores based on little evidence (i.e.
fewer av-eraged numbers).
In the sample applicationdescribed in section 3 the value of k was setto 2.
For analogous reasons, we convention-ally assign a very high score to candidates forwhich we have no evidence (i.e.
no words inboth the synset and the gloss).
If either thesynset or the gloss is empty, we conventionallydouble the score for the gloss or the synset,respectively.
We note at this point that ourfinal ranking list are sorted in reverse orderwith respect o the assigned scores, since weare focusing on removing incorrect items.
Atthe top of the list are the items that receivethe lowest score, i.e.
that are more likely tobe incorrect (term, sense) associations for ourdomain (thus being the best candidates to bepruned out).Table 2 shows the ranking of the sensesfor the word C in the aviation domain.
Inthe table, each term is followed by its corpusfrequency, separated by a slash.
From eachsynset the word C itself has been removed,as well as the gloss words found in the stoplist.
Therefore, the table only contains thewords that contribute to the calculation of thesense's core.
E.g.
the score for the first sensein the list is obtained from the following ex-pression:(9) ((0 + 57)/2/22) +( (8+0+0+ 198+9559+0+1298)/7/72 )The third sense in the list exemplifies thecase of an empty synset (i.e.
a synset orig-inally containing only the word under con-sideration).
In this case the score obtainedfrom the gloss is doubled.
Note that the ob-viously incorrect sense of C as a narcotic isin the middle of the list.
This is due to a tag-ging problem, as the word leaves in the glosswas tagged as verb instead of noun.
Thereforeit was assigned a very high frequency, as theverb leave, unlike the noun leaf, is very com-mon in the aviation domain.
The last sensein the list also requires a brief explanation.The original word in the gloss was 10S.
How-ever, the pre-processor that was used beforetagging the glosses recognized S as an abbre-viation for South and expanded the term ac-cordingly.
It so happens that both words 10and South are very frequent in the aviationcorpus we used, therefore the sense was as-signed a high score.2.4 OptimizationThe aim of this phase is to improve the accessspeed to the synonym database, by removingall information that is not likely to be used.The main idea is to minimize the size of theScoreTable 2: Ranking of synsets containing the word CFrequencies39.3762.75224.28synset:gloss:synset:gloss:synset:gloss:241.69 synset:gloss:585.17 synset:gloss:743.28 synset:gloss:1053.43 synset:gloss:ATOMIC_NUMBEK_6/O, CAKBON/57ABUNDANT/8, NONMETALLIC/O, TETRAVALENT/O, ELEMENT/1980CCUR/9559, ALLOTROPIC/O, FOKM/1298AMPEre-SECOND/O, COULOMB/OUNIT/3378, ELECTRICAL/2373, CHARGE/523, EQUAL/153AMOUNT/1634, CHARGE/523, TKANSFEK/480, CUKKENT/242, 1/37106AMPEre/4, 1/371060GENEKAL-PUKPOSE/O, PROGRAMING/O, LANGUAGE/445, CLOSELY/841ASSOCIATE/543, UNIX/O, OPEKATE/5726, SYSTEM/49863COCAIN/O, COCAINE/i, COKE/8, SNOW/3042NARCOTIC/i, ALKALOID/O, EXTKACT/31, COCA/I, LEAVE/24220LIGHT_SPEED/I, SPEED_OF_LIGHT/OSPEED/14665, LIGHT/22481, TRAVEL/f05, VACUUM/192DEGREE_CELSIUS/24, DEGREEiENTIGRADE/28DEGKEE/43617, CENTIGRADE/34, SCALE/540, TEMPERATURE/2963I00/0, CENTRED/O, CENTUKY/31, HUNDRED/O, ONE_C/OTEN/Z3, 10/16150, SOUTH/12213database in such a way that the database be-havior remains unchanged.
Two operationsare performed at the stage: (i) a simple rel-evance  tes t  to remove irrelevant erms (i.e.terms not pertaining to the domain at hand);(ii) a redundancy check, to remove informa-tion that, although perhaps relevant, does notaffect the database behavior.2.4.1 Re levance  tes tTerms not appearing in the domain cor-pus are considered not relevant o the spe-cific domain and removed from the synonymdatabase.
The rationale underlying this stepis to remove from the synonym database syn-onymy relations that are never going to beused in the specific domain.
In this way the ef-ficiency of the module can be increased, by re-ducing the size of the database and the num-ber of searches performed (synonyms that areknown to never appear are not searched for),without affecting the system's matching at-curacy.
E.g., the synset in (10a) would bereduced to the synset in (10b).
(10) a. AMPERE-SECOND/O, COULOMB/O,C/9168b.
C/91682.4.2 Redundancy  checkThe final step is the removal of redundantsynsets, possibly as a consequence of the pre-vious pruning steps.
Specifically, the follow-ing synsets are removed:?
Synsets containing a single term (al-though the associated sense might be avalid one for that term, in the specificdomain).?
Duplicate synsets, i.e.
identical (in termsof synset elements) to some other synsetnot being removed (the choice of the onlysynset o be preserved is arbitrary).E.g., the synset in (10b) would be finMlyremoved at this stage.3 Sample  app l i ca t ionThe described methodology was applied tothe aviation domain.
We used the AviationSafety Information System (ASRS) corpus(h 'e tp : / /as rs .
a rc .nasa .gov / )  as our avia-tion specific corpus.
The resulting domain-specific database is being used in an IR ap-plication that retrieves documents relevantto user defined queries, expressed as phrasepatterns, and identifies portions of text thatare instances of the relevant phrase patterns.The application makes use of Natural Lan-guage Processing (NLP) techniques (taggingand partial parsing) to annotate documents.User defined queries are matched against suchannotated corpora.
Synonyms are used toexpand occurrences of specific words in suchqueries.
In the following two sections we de-scribe how the pruning process was performedand provide some results.3.1 Adapt ing  Wordnet  to theav iat ion  domainA vocabulary of relevant query terms wasmade available by a user of our IR applica-tion and was used in our ranking of synonymyrelations.
Manual pruning was performed onthe 1000 top ranking terms, with which 6565synsets were associated overall.
The manualpruning task was split between two humanevaluators.
The evaluators were programmersmembers of our staff.
They were English na-tive speakers who had acquaintance with ourIR application and with the goals of the man-ual pruning process, but no specific trainingor background on lexicographic or WordNet-related tasks.
For each of the 1000 terms,the evaluators were provided with a sampleof 100 (at most) sentences where the rele-vant word occurred in the ASRS corpus.
100of the 1000 manually checked clusters (i.e.groups of synsets referring to the same headterm) were submitted to both evaluators (576synsets overall), in order to check the rateof agreement of their evaluations.
The eval-uators were allowed to leave synsets unan-swered, when the synsets only contained thehead term (and at least one other synset inthe cluster had been deemed correct).
Leav-ing out the cases when one or both evalua-tors skipped the answer, there remained 418synsets for which both answered.
There wasagreement in 315 cases (75%) and disagree-ment in 103 cases (25%).
A sample of senseson which the evaluators disagreed is shown in(11).
In each case, the term being evaluatedis the first in the synset.
(11) a.
{about, around}(in the area or vicinity)b.
{accept, admit, take, take on}(admit into a group or commu-nity)c. {accept, consent, go for}(give an affirmative reply to)d. {accept, swallow}(tolerate or accommodate oneselfto)e. {accept, take}(be designed to hold or take)f. {accomplished, effected, estab-lished}(settled securely and uncondi-tionally)g. {acknowledge, know, recognize}(discern)h. {act, cognitive operation, cogni-tive process, operation, process}(the performance of some com-posite cognitive activity)i.
{act, act as, play}(pretend to have certain qualitiesor state of mind)j.
{action, activeness, activity}(the state of being active)k. {action, activity, natural action,natural process}(a process existing in or producedby nature (rather than by the in-tent of human beings))It should be noted that the 'yes' and 'no'answers were not evenly distributed betweenthe evaluators.
In 80% of the cases of dis-agreement, i  was evaluator A answering 'yes'and evaluator B answering 'no'.
This seemsto suggest han one of the reasons for dis-agreement was a different degree of strictnessin evaluating.
Since the evaluators matcheda sense against an entire corpus (representedby a sample of occurrences), one common sit-uation may have been that a sense did oc-cur, but very rarely.
Therefore, the evaluatorsmay have applied different criteria in judginghow many occurrences were needed to deema sense correct.
This discrepancy, of course,may compound with the fact that the differ-ences among WordNet senses can sometimesbe very subtle.Automatic pruning was performed onthe entire WordNet database, regardless ofwhether candidates had already been manu-ally checked or not.
This was done for test-ing purposes, in order to check the results ofautomatic pruning against the test set ob-tained from manual pruning.
Besides asso-ciating ASRS frequencies with all words insynsets and glosses, we also computed fre-quencies for collocations (i.e.
multi-wordterms) appearing in synsets.
The input toautomatic pruning was constituted by 10352polysemous terms appearing at least once inASRS the corpus.
Such terms correspond to37494 (term, synset) pairs.
Therefore, thelatter was the actual number of pruning can-didates that  were ranked.The check of WordNet senses against ASRSsenses was only done unidirectionally, i.e.we only checked whether WordNet senseswere attested in ASRS.
Although it wouldbe interesting to see how often the appropri-ate, domain-specific senses were absent fromWordNet, no check of this kind was done.
Wetook the simplifying assumption that Word-Net be complete, thus aiming at assigning atleast one WordNet sense to each term thatappeared in both WordNet and ASRS.3.2 Resu l tsIn order to test the automatic pruning per-formance, we ran the ranking procedure ona test set taken from the manually checkedfiles.
This file had been set apart and hadnot been used in the preliminary tests on theautomatic pruning algorithm.
The test setincluded 350 clusters, comprising 2300 candi-dates.
1643 candidates were actually assignedan evaluation during manual pruning.
Thesewere used for the test.
We extracted the 1643relevant items from our ranking list, then weincrementally computed precision and recallin terms of the items that had been manuallychecked by our human evaluators.
The re-sults are shown in figure 1.
As an example ofhow this figure can be interpreted, taking intoconsideration the top 20% of the ranking list(along the X axis), an 80% precision (Y axis)means that 80% of the items encountered sofar had been removed in manual pruning; a27% recall (Y axis) means that 27% of theoverall manually removed items have been en-countered so far.The automatic pruning task was intention-ally framed as a ranking problem, in order toleave open the issue of what pruning thresholdwould be optimal.
This same approach wastaken in the IR application in which the prun-ing procedure was embedded.
Users are giventhe option to set their own pruning threshold(depending on whether they focus more onprecision or recall), by setting a value spec-ifying what precision they require.
Pruningis performed on the top section of the rank-ing list that guarantees the required precision,according to the correlation between precisionand amount of pruning shown in figure 1.A second test was designed to checkwhether there is a correlation between thelevels of confidence of automatic and man-ual pruning.
For this purpose we used thefile that had been manually checked by bothhuman evaiuators.
We took into account hecandidates that had been removed by at leastone evaluator: the candidates that were re-moved by both evaluators were deemed tohave a high level of confidence, while thoseremoved by only one evaluator were deemedto have a lower level of confidence.
Then wechecked whether the two classes were equallydistributed in the automatic pruning rankinglist, or whether higher confidence candidatestended to be ranked higher than lower con-fidence ones.
The results are shown in fig-ure 2, where the automatic pruning recall foreach class is shown.
For any given portionof the ranking list higher confidence candi-dates (solid lines) have a significantly higherrecall than lower confidence candidates (dot-Table 3: WordNet optimization results.DB Synsets Word-sensesFull WN 99,642 174,008Reduced WN 9,441 23,368ted line).Finally, table 3 shows the result of applyingthe described optimization techniques alone,i.e.
without any prior pruning, with respectto the ASRS corpus.
The table shows howmany synsets and how many word-senses arecontained in the full Wordnet database and inits optimized version.
Note that such reduc-tion does not involve any loss of accuracy.4 Conc lus ionsThere is a need for automatically or semi-automatically adapting NLP components ospecific domain, if such components are to beeffectively used in IR applications without in-volving labor-intensive manual adaptation.
Akey part of adapting NLP components ospe-cific domains is the adaptation of their lexicaland terminological resources.
It may often bethe case that a consistent section of a generalpurpose terminological resource is irrelevantto a specific domain, thus involving an unnec-essary amount of ambiguity that affects boththe accuracy and efficiency of the overall NLPcomponent.
In this paper we have proposeda method for adapting a general purpose syn-onym database to a specific domain.Evaluating the performance of the pro-posed pruning method is not a straightfor-ward task, since there are no other resultsavailable on a similar task, to the best of ourknowledge.
However, a comparison betweenthe results of manual and automatic pruningprovides ome useful hints.
In particular:?
The discrepancy between the evaluationof human operators hows that the taskis elusive even for humans (the value ofthe agreement evaluation statistic n forour human evaluators was 0.5);?
however, the correlation between thelevel of confidence of human evaluationsand scores assigned by the automaticpruning procedure shows that the auto-matic pruning algorithm captures omesignificant aspect of the problem.Although there is probably room for im-proving the automatic pruning performance,the preliminary results how that the currentapproach is pointing in the right direction.Re ferencesChristiane Fellbaum, editor.
1998.
Wordnet: AnElectronic Lexical Database.
MIT Press Books.Claudia Leacock, Martin Chodorow, andGeorge A. Miller.
1998.
Using corpus tatisticsand WordNet relations for sense identification.Computational Linguistics, 24(1):147-165.Bernardo Magnini and Gabriela Cavaglih.
2000.Integrating Subject Field Codes into WordNet.In Maria Gavrilidou, George Carayannis, StellaMarkantonatou, Stelios Piperidis, and GregoryStainhaouer, editors, Proceedings of the Sec-ond International Conference on Language Re-sources and Evaluation (LREC-PO00), pages1413-1418, Athens, Greece.Mark Sanderson.
1997.
Word Sense Disambigua-tion and Information Retrieval.
Ph.D. thesis,Department ofComputing Science at the Uni-versity of Glasgow, Glasgow G12.
TechnicalReport (TR-1997-7).Ellen M. Voorhees.
1998.
Using WordNet for textretrieval.
In Fellbaum (Fellbaum, 1998), chap-ter 12, pages 285-303.9~2~9100 i I I I959085+?
i75706560550100806040200 I I I I0 20 40 60 80 100Top % of ranking listF igure 1: Precision and recall of automat ic  pruning10~9cg10080604020I I I I t - - ' - /-- _ t / f  j" _-- r l  j --/ J _  ; _- - J "  I I I I0 20 40 60 80 100Top % of ranking listFigure 2: A recall comparison for different confidence rates11
