Improving a general-purpose Statistical Translation Engine byTerminological lexiconsPhilippe LanglaisRALI / DIRO / Universite?
de Montre?alC.P.
6128, succursale Centre-villeMontre?al (Que?bec)Canada, H3C 3J7email:felipe@iro.umontreal.caAbstractThe past decade has witnessed exciting workin the field of Statistical Machine Translation(SMT).
However, accurate evaluation of its po-tential in real-life contexts is still a questionableissue.In this study, we investigate the behavior ofan SMT engine faced with a corpus far differ-ent from the one it has been trained on.
Weshow that terminological databases are obviousresources that should be used to boost the per-formance of a statistical engine.
We proposeand evaluate a way of integrating terminologyinto a SMT engine which yields a significant re-duction in word error rate.1 IntroductionSMT mainly became known to the linguisticcommunity as a result of the seminal work ofBrown et al (1993b).
Since then, many re-searchers have invested effort into designing bet-ter models than the ones proposed in the afore-mentioned article and several new exciting wayshave been suggested to attack the problem.For instance, Vogel et al (1996) succeeded inovercoming the independence assumption madeby IBM models by introducing order-1 HiddenMarkov alignment models.
Och et al (1999) de-scribed an elegant way of integrating automat-ically acquired probabilistic templates into thetranslation process, and Nie?en and Ney (2001)did the same for morphological information.Radically different statistical models havealso been proposed.
(Foster, 2000) investigatedmaximum entropy models as an alternative tothe so-called noisy-channel approach.
Very re-cently, Yamada and Knight (2001) described amodel in which the noisy-channel takes as inputa parsed sentence rather than simple words.While many of these studies include intensiveevaluation sections, it is not always easy to de-termine exactly how well statistical translationcan do on a given task.
We know that on a spe-cific task of spoken language translation, Wang(1998) provided evidence that SMT comparedfavorably to a symbolic translation system; butas mentioned by the author, the comparison wasnot totally fair.We do not know of any studies that describeextensive experiments evaluating the adequacyof SMT in a real translation environment.
Weprefer not to commit ourselves to defining whata real translation task is; instead, we adopt theconservative point of view that a viable transla-tion engine (statistical or not) is one that copeswith texts that may be very different in naturefrom those used to train it.This fairly general definition suggests thatadaptativity is a cornerstone of a successfulSMT engine.
Curiously enough, we are notaware of much work on adaptative SMT, de-spite the tremendous amount of work done onadaptative statistical language modeling.In this paper, we propose to evaluate howa statistical engine behaves when translating avery domain specific text which is far differentfrom the corpus used to trained both our trans-lation and language models.
We first describeour translation engine.
In section 3, we quantifyand analyse the performance deterioration of anSMT engine trained on a broad-based corpus(the Hansard) when used to translate a domainspecific text (in this study, a manual for militarysnipers).
In section 4, We then suggest a sim-ple but natural way of improving a broad-basedSMT engine; that is, by opening the engine toavailable terminological resources.
In section 5,we report on the improvement we observed byimplementing our proposed approach.
Finally,in section 6 we discuss other approaches we feelcan lead to more robust translation.2 Our statistical engine2.1 The statistical modelsIn this study, we built an SMT engine designedto translate from French to English, followingthe noisy-channel paradigm first described by(Brown et al, 1993b).
This engine is based onequation 1, where eI?1 stands for the sequenceof I?
English target words to be found, given aFrench source sentence of J words fJ1 :eI?1 = argmaxI,eI1P (eI1)?
??
?language.
P (fJ1 |eI1)?
??
?translation(1)To train our statistical models, we assembleda bitext composed of 1.6 million pairs of sen-tences that were automatically aligned at thesentence level.
In this experiment, every tokenwas converted into lowercase before training.The language model we used is an interpo-lated trigram we trained on the English sen-tences of our bitext.
The perplexity of the re-sulting model is fairly low ?
65 ?, which actuallyreflects the fact that this corpus contains manyfixed expressions (e.g pursuant to standingorder).The inverted translation model we used isan IBM2-like model: 10 iterations of IBM1-training were run (reducing the perplexity ofthe training corpus from 7776 to 90), followedby 10 iterations of IBM2-training (yielding afinal perplexity of 54).
We further reducedthe number of transfer parameters (originally34 969 331) by applying an algorithm describedin Foster (2000); this algorithm basically filtersin the pairs of words with the best gain, wheregain is defined as the difference in perplexity ?measured on a held-out corpus ?
of a modeltrained with this pair of words and a modeltrained without.
In this experiment, we workedwith a model containing exactly the first gain-ranked million parameters.
It is interesting tonote that by doing this, we not only save mem-ory, and therefore time, but also obtain improv-ments in terms of perplexity and overall perfor-mance1.1On a translation task from French to English on2.2 The search algorithmThe maximum operation in equation 1, alsocalled search or decoding, involves a lengthmodel.
We assume that the length (counted inwords) of French sentences that translate an En-glish sentence of a given length follow a normaldistribution.We extended the decoder described by Nie?enet al (1998) to a trigram language model.
Thebasic idea of this search algorithm is to expandhypotheses along the positions of the targetstring while progressively covering the sourceones.
We refer the reader to the original paperfor the recursion on which it relies, and insteadgive in Figure 1 a sketch of how a translationis built.
An hypothesis h is fully determinedby four parameters: its source (j) and target(i) positions of the last word (e), and its cov-erage (c).
Therefore, the search space can berepresented as a 4-dimension table, each itemof which contains backtracking information (ffor the fertility of e, bj and bw for the sourceposition and the target word we should look atto backtrack) and the hypothesis score (prob).We know that better alignment models havebeen proposed and extensively compared (Ochand Ney, 2000).
We must however pointout that the performance we obtained on thehansard corpus (see Section 3) is comparableto the rates published elsewhere on the samekind of corpus.
In any case, our goal in thisstudy is to compare the behavior of a SMT en-gine in both friendly and adverse situations.
Inour view, the present SMT engine is suitable forsuch a comparative study.2.3 Tuning the decoderThe decoder has been tuned in several ways inorder to reduce its computations without detri-mentally affecting the quality of its output.
Thefirst thing we do when the decoder receives asentence is to compute what we call an activevocabulary ; that is, a collection of words whichare likely to occur in the translation.
This isdone by ranking for each source word the tar-get words according to their non normalizedposterior likelihood (that is argmaxe p(f |e)p(e),where p(e) is given by a unigram target lan-guage model, and p(f |e) is given by the transferHansard sentences, we observed a reduction in word er-ror rate of more than 3% with the reduced model.Input: f1 .
.
.
fj .
.
.
fJInitialize the search space table SpaceSelect a maximum target length: ImaxCompute the active vocabulary// Fill the search table recursively:for all target position i = 1, 2, .
.
.
, Imax doprune(i ?
1);for all alive hyp.
h = Space(i, j, c, e) douv ?
History(h);zones ?
FreeSrcPositions(h);bestWords ?
NBestTgtWords(uv);for all w in bestWords doprob ?
Score(h) + log p(w|uv);setIfBetter(i, j, c, b, prob, 0, j, v);for all free source position d dos ?
prob;for all f ?
[1, fmax] / d + f ?
1 isfree dos+ = log a(i|d, J) + log t(fd|ei);setIfBetter(i, d, c+f, w, s, f, j, w);// Find and return the best hypothesis if anymaxs ?
?
?for all i ?
[1, Imax] dofor all alive hyp.
h = Space(i, j, c, e) dos ?
Score(h) + log p(i|J);if ((c == J) and (s > maxs)) thenmaxs ?
s?maxi, maxj , maxe?
?
?i, j, e?if (maxs!
= ?)
thenReturn Space(maxi, maxj , J, maxe);elseFailureOutput: e1 .
.
.
ei .
.
.
emaxiFigure 1: Sketch of our decoder.FreeSrcPositions returns the source posi-tions not already associated to words of h;NBestTgtWords returns the list of wordsthat are likely to follow the last bigram uvpreceeding e according to the language model;and setIfBetter(i, j, c, e, p, f, bj , bw) is anoperator that memorizes an hypothesis if itsscore (p) is greater than the hypothesis alreadystored in Space(i, j, c, e).
a and t stands for thealignment and transfert distributions used byIBM2 models.probabilities of our inverted translation model)and keeping for each source word at most a tar-get words.Increasing a raises the coverage of the activevocabulary, but also slows down the translationprocess and increases the risk of admitting aword that has nothing to do with the transla-tion.
We have conducted experiments with var-ious a-values, and found that an a-value of 10offers a good compromise.As mentioned in the block diagram, we alsoprune the space to make the search tractable.This is done with relative filtering as well as ab-solute thresholding.
The details of all the filter-ing strategies we implemented are however notrelevant to the present study.3 Performances of our SMT engine3.1 Test corporaIn this section we provide a comparison of thetranslation performances we measured on twocorpora.
The first one (namely, the hansard)is a collection of sentences extracted from a partof the Hansard corpus we did not use for train-ing.
In particular, we did not use any specificstrategy to select these sentences so that theywould be closely related to the ones that wereused for training.Our second corpus (here called sniper) isan excerpt of an army manual on sniper train-ing and deployment that was used in an EAR-LIER study (Macklovitch, 1995).
This corpus ishighly specific to the military domain and wouldcertainly prove difficult for any translation en-gine not specifically tuned to such material.3.2 Overall performanceIn this section, we evaluate the performance ofour engine in terms of sentence- and word- errorrates according to an oracle translation2.
Thefirst rate is the percentage of sentences for whichthe decoder found the exact translation (that is,the one of our oracle), and the word error rateis computed by a Levenstein distance (count-ing the same penalty for both insertion, dele-tion and substitution edition operations).
Werealize that these measures alone are not suffi-cient for a serious evaluation, but we were re-2Both corpora have been published in both Frenchand English, and we took the English part as the goldstandard.luctant in this experiment to resort to manualjudgments, following for instance the protocoldescribed in (Wang, 1998).
Actually a quicklook at the degradation in performance we ob-served on sniper is so clear that we feel thesetwo rates are informative enough !Table 1 summarizes the performance rates wemeasured.
The WER is close to 60% on thehansard corpus and close to 74% on sniper;source sentences in the latter corpus beingslightly longer on average (21 words).
Not asingle sentence was found to be identical to thegold standard translation on the sniper corpus3.corpus nbs |length| SER WERhansard 1038 ?16.2, 7.8?
95.6 59.6sniper 203 ?20.8, 6.8?
100 74.6Table 1: Main characteristics of our test cor-pora and global performance of our statisticaltranslator without any adjustments.
|length|reports the average length (counted in words)of the source sentences and the standard de-viation; nbs is the number of sentences in thecorpus.3.3 Analyzing the performance dropAs expected, the poor performance observed onthe sniper text is mainly due to two reasons:the presence of out of vocabulary (OOV) wordsand the incorrect translations of terminologicalunits.In the sniper corpus, 3.5% of the source to-kens and 6.5% of the target ones are unknownto the statistical models.
44% of the source sen-tences and 77% of the target sentences containat least one unknown word.
In the hansardtext, the OOV rates are much lower: around0.5% of the source and target tokens are un-known and close to 5% of the source and targetsentences contain at least one OOV words.These OOV rates have a clear impact onthe coverage of our active vocabulary.
On thesniper text, 72% of the oracle tokens are in theactive vocabulary (only 0.5% of the target sen-tences are fully covered); whilst on hansard,3The full output of our translation sessionsis available at www-iro.umontreal.ca/?felipe/ResearchOutput/Computerm200286% of the oracle?s tokens are covered (24% ofthe target sentences are fully covered).Another source of disturbance is the presenceof terminological units (TU) within the text totranslate.
Table 2 provides some examples ofmistranslated TU from the sniper text.
Wealso observed that many words within termino-logical units are not even known by the statisti-cal models.
Therefore accounting for terminol-ogy is one of the ways that should be consideredto reduce the impact of OOV words.< source term / oracle / translation><a?me / bore / heart><huile polyvalente / general purpose oil / oilpolyvalente><chambre / chamber / house of common><tireur d?
e?lite / sniper / issuer of elite><la longueur de la crosse / butt length / thelength of the crosse>Table 2: Examples of mistranslated terminolog-ical entries of the sniper corpus.4 Integrating non-probabilisticterminological resourcesUsing terminological resources to improve thequality of an automatic translation engine is notat all a new idea.
However, we know of very fewstudies that actually investigated this avenuein the field of statistical machine translation.Among them, (Brown et al, 1993a) have pro-posed a way to exploit bilingual dictionnaries attraining time.
There may also be cases wheredomain-specific corpora are available which al-low for the training of specialized models thatcan be combined with the general ones.Another approach that would not requiresuch material at training time consists in de-signing an adaptative translation engine.
Forinstance, a cache-based language model couldbe used instead of our static trigram model.However, the design of a truly adaptative trans-lation model remains a more speculative enter-prise.
At the very least, it would require a fairlyprecise location of errors in previously trans-lated sentences; and we know from the AR-CADE campaign on bilingual alignments, thataccurate word alignments are difficult to obtain(Ve?ronis and Langlais, 2000).
This may be evenmore difficult in situations where errors will in-volve OOV words.We investigated a third option, which involvestaking advantage ?
at run time ?
of existing ter-minological resources, such as Termium4.
Asmentioned by Langlais et al (2001), one ofa translator?s first tasks is often terminologicalresearch; and many translation companies em-ploy specialized terminologists.
Actually, asidefrom the infrequent cases where, in a giventhematic context, a word is likely to have aclearly preferred translation (e.g.
bill/facturevs bill/projet de loi), lexicons are often theonly means for a user to influence the transla-tion engine.Merging such lexicons at run time offersa complementary solution to those mentionedabove and it should be a fruitful strategy in sit-uations where terminological resources are notavailable at training time (which may often bethe case).
Unfortunately, integrating termino-logical (or user) lexicons into a probabilistic en-gine is not a straightforward operation, sincewe cannot expect them to come with attachedprobabilities.
Several strategies do come tomind, however.
For instance, we could credit atranslation of a sentence that contains a sourcelexicon entry in cases it contains an authorizedtranslation.
But this strategy may prouve dif-ficult to tune since decoding usually involvesmany filtering strategies.The approach we adopted consists in view-ing a terminological lexicon as a set of con-straints that are employed to reduce the searchspace.
For instance, knowing that sniper is asanctioned translation of tireur d?e?lite, we mayrequire that current hypotheses in the searchspace associate the target word sniper with thethree source French words.In our implementation, we had to slightlymodify the block diagram of Figure 1 in orderto: 1) forbid a given word ei from being asso-ciated with a word belonging to a source ter-minological unit, if it is not sanctioned by thelexicon; and 2) add at any target position anhypothesis linking a target lexicon entry to itssource counterpart.
Whether these hypotheseswill survive intact will depend on constraintsimposed by the maximum operation (of equa-tion 1) over the full translation.The score associated with a target entry ei?i4See http://www.termium.com/site/.when linked to its source counterpart f j?j in thelatter case is given by:?k?[i,i?
]log p(ek|ek?2ek?1) + maxl?[j,j?
]log(a(k|l, J))The rationale behind this equation is thatboth the language (p) and the alignment (a)models have some information that can helpto decide the appropriateness of an extension:the former knows how likely it is that a word(known or not) will follow the current history5;and the latter knows to some extent where thetarget unit should be (regardless of its identity).In the absence of a better mechanism (e.g.
acache-model should be worth a try) We hopethat this will be sufficient to determine the finalposition of the target unit in a given hypothesis.5 ResultsWe considered three terminological lexiconswhose characteristics are summarized in Table3; they essentially differ in terms of numberof entries and therefore coverage of the text totranslate.lexicon nb coverage SER WERsniper-1 33 20/247 99 67.4sniper-2 59 47/299 98 66.2sniper-3 146 132/456 98 64.3Table 3: Translation performance with differ-ent terminological lexicons.
nb is the number ofentries in the lexicon and coverage reports thenumber of different source entries from the lex-icon belonging to the text to translate and thetotal number of their occurrences.The first lexicon (namely sniper-1) containsthe 33 entries used in the study of terminologicalconsistency checking described in (Macklovitch,1995).
The second and third lexicons (namelysniper-2 and sniper-3) contain those entriesplus other ones added manually after an incre-mental inspection of the sniper corpus.As can be observed from Table 3, introduc-ing terminological lexicons into the translationengine does improve performance, measured interms of WER, and this even with lexicons that5Our trigram model has been trained to provide pa-rameters such as p(UNK|ab).Source le tireur d?
e?lite voit simultane?ment les fils croise?s et l?
image ( l?
objectif ) .Target the sniper sees the crosshairs and the image - target - at the same time .without the gunman being same son sit and picture of the hon.
members : agreed .with the sniper simultaneously see the crosshairs and the image (objective .
)Source contro?le de la de?tente .Target exercising trigger control .without the control of de?tente .with control of the trigger .Table 4: Two examples of translation with and without a terminological lexicon; TU appear inbold.cover only a small portion of the text to trans-late.
With the narrow coverage lexicon, we ob-serve an absolute reduction of 7%, and a reduc-tion of 10% with the broader lexicon sniper-3.This suggests that adding more entries into thelexicon is likely to decrease WER.
In anotherstudy (Carl and Langlais, 2002), we investigatedwhether an automatic procedure designed to de-tect term variants could improve these perfor-mances futher.Table 4 provides two examples of translationoutputs, with and without the help of termino-logical units.
The first one clearly shows thatEVEN A few TU (two in this case) may sub-stantially improve the quality of the translationoutput; (the translation produced without thelexicon was particularly poor in this very case.Even though terminological lexicons do im-prove the overall WER figure, a systematic in-spection of the outputs produced with TU re-veals that the translations are still less faithfulto the source text than the translations pro-duced for the hansard text.
OOV words re-main a serious problem.6 DiscussionIn this study, we have shown that translat-ing texts in specific domains with a general[-]purpose statistical engine is difficult.
This sug-gests the need to implementing an adaptativestrategy.
Among the possible scenarios, we haveshown that opening the engine to terminologi-cal resources is a natural and efficient way ofsoftening the decoder.In a similar vein, Marcu (2001) investigatedhow to combine Example Based Machine Trans-lation (EBMT) and SMT approaches.
The au-thor automatically derived from the Hansardcorpus what he calls a translation memory: ac-tually a collection of pairs of source and targetword sequences that are in a translation rela-tion according to the viterbi alignment run withan IBM4 model that was also trained on theHansard corpus.
This collection of phrases wasthen merged with a greedy statistical decoder toimprove the overall performance of the system.What this study suggests is that translationmemories collected from a given corpus can im-prove the performance of a statistical enginetrained on the same corpus, which in itself isan interesting result.
A very similar study butwith weaker results is derscribed in (Langlais etal., 2000), in the framework of the TransTypeproject.
Besides the different metrics the au-thors used, the discrepancy in performance inthese two studies may be explained by the na-ture of the test corpora used.
The test corpusin the latter study was more representative of areal translation task, while the test corpus thatMarcu used was a set of around 500 French sen-tences of no more than 10 words.Our present study is close in spirit to theselast two, except that we do not attack the prob-lem of automatically acquiring bilingual lexi-cons; instead, we consider it a part of the trans-lator?s task to provide such lexicons.
Actually,we feel this may be one of the only ways a userhas of retaining some control over the engine?soutput, a fact that professional translators seemto appreciate (Langlais et al, 2001).As a final remark, we want to stress that wesee the present study as a first step toward theeventual unification of EBMT and SMT, and inthis respect we agree with (Marcu, 2001).
Po-tentially, of course, EBMT can offer much morethan just a simple list of equivalences, like thosewe used in this study.
However, the basic ap-proach we describe here still holds, as long aswe can extend the notion of constraint used inthis study to include non-consecutive sequencesof words.
This is a problem we we plan to in-vestigate in future research.AcknowledgmentsI am indebted to Elliott Macklovitch andGeorge Foster for the fruitful orientation theygave to this work.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vin-cent J. Della Pietra, Meredith J. Goldsmith,Jan Hajic, Robert L. Mercer, and SuryaMohanty.
1993a.
But dictionaries are datatoo.
In Human Language Technology (HLT),pages 202?205, Princeton, NJ, march.Peter F. Brown, Stephen A. Della Pietra, Vin-cent J. Della Pietra, and Robert L. Mer-cer.
1993b.
The mathematics of statisticalmachine translation: Parameter estimation.Computational Linguistics, 19(2):263?311.Michael Carl and Philippe Langlais.
2002.
To-ward an intelligent terminology database asa front-and backend for statistical machinetranslation.
In COMPUTERM 2002, Taipei.George Foster.
2000.
A Maximum Entropy /Minimum Divergence translation model.
InProceedings of the 38th Annual Meeting of theACL, pages 37?44, Hong Kong, October.Philippe Langlais, George Foster, and GuyLapalme.
2000.
Unit completion for acomputer-aided translation typing system.
InProceedings of the 5th Conference on AppliedNatural Language Processing (ANLP), pages135?141, Seattle, Washington, May.Philippe Langlais, George Foster, and Guy La-palme.
2001.
Integrating bilingual lexicons ina probabilistic translation assistant.
In Pro-ceedings of the 8th Machine Translation Sum-mit, pages 197?202, Santiago de Compostela,Galicia, Spain, September.
IAMT.Elliott Macklovitch.
1995.
Can terminologi-cal consistency be validated automatically?
Technical report, CITI/RALI, Montre?al,Canada.Daniel Marcu.
2001.
Towards a unified ap-proach to memory- and statistical-based ma-chine translation.
In Proceedings of the 39thAnnual Meeting of the ACL, pages 378?385,Toulouse, France.Sonja Nie?en and Hermann Ney.
2001.
Towardhierarchical models for statistical machinetranslation of inflected languages.
In Proceed-ings of the Workshop on Data Driven Ma-chine Translation yielded at the 39th AnnualMeeting of the ACL, pages 47?54, Toulouse,France.Sonja Nie?en, Stephan Vogel, Hermann Ney,and Christoph Tillmann.
1998.
A dp basedsearch algorithm for statistical machine trans-lation.
In Proceedings of the 36th AnnualMeeting of the ACL and the 17th COLING,pages 960?966, Montre?al, Canada, August.Franz Joseph Och and Hermann Ney.
2000.A comparison of alignement models for sta-tistical machine translation.
In Proceed-ings of the International Conference onComputational Linguistics (COLING) 2000,pages 1086?1090, Saarbrucken, Luxembourg,Nancy, August.Franz Josef Och, Christoph Tillman, and Her-man Ney.
1999.
Improved alignment modelsfor statistical machine translation.
In Pro-ceedings of the 4nd Conference on Empiri-cal Methods in Natural Language Processing(EMNLP), pages 20?28, College Park, Mary-land.Jean Ve?ronis and Philippe Langlais, 2000.
Eval-uation of parallel text alignment systems: TheARCADE project, volume 13, chapter 19,pages 369?388.
Parallel Text Processing,Kluwer.Stephan Vogel, Hermann Ney, and ChristophTillmann.
1996.
Hmm-based word aligne-ment in statistical translation.
In Proceedingsof the International Conference on Compu-tational Linguistics (COLING) 1996, pages836?841, Copenhagen, Denmark, August.Ye-Yi Wang.
1998.
Grammar Inference andStatistical Machine Translation.
Ph.D. the-sis, CMU-LTI, Carnegie Mellon University.Kenji Yamada and Kevin Knight.
2001.
Asyntax-based statistical translation model.
InProceedings of the 39th Annual Meeting of theACL, pages 531?538, Toulouse, France.
