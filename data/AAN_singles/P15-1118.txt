Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1223?1233,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsParsing Paraphrases with Joint InferenceDo Kook Choe?Brown UniversityProvidence, RIdc65@cs.brown.eduDavid McCloskyIBM ResearchYorktown Heights, NYdmcclosky@us.ibm.comAbstractTreebanks are key resources for develop-ing accurate statistical parsers.
However,building treebanks is expensive and time-consuming for humans.
For domains re-quiring deep subject matter expertise suchas law and medicine, treebanking is evenmore difficult.
To reduce annotation costsfor these domains, we develop methods toimprove cross-domain parsing inferenceusing paraphrases.
Paraphrases are eas-ier to obtain than full syntactic analyses asthey do not require deep linguistic knowl-edge, only linguistic fluency.
A sentenceand its paraphrase may have similar syn-tactic structures, allowing their parses tomutually inform each other.
We presentseveral methods to incorporate paraphraseinformation by jointly parsing a sentencewith its paraphrase.
These methods are ap-plied to state-of-the-art constituency anddependency parsers and provide signif-icant improvements across multiple do-mains.1 IntroductionParsing is the task of reconstructing the syntac-tic structure from surface text.
Many natural lan-guage processing tasks use parse trees as a basisfor deeper analysis.The most effective sources of supervision fortraining statistical parsers are treebanks.
Unfortu-nately, treebanks are expensive, time-consumingto create, and not available for most domains.Compounding the problem, the accuracy of statis-tical parsers degrades as the domain shifts awayfrom the supervised training corpora (Gildea,2001; Bacchiani et al, 2006; McClosky et al,2006b; Surdeanu et al, 2008).
Furthermore, for?Work performed during an IBM internship.domains requiring subject matter experts, e.g., lawand medicine, it may not be feasible to producelarge scale treebanks since subject matter expertsgenerally don?t have the necessary linguistic back-ground.
It is natural to look for resources thatare more easily obtained.
In this work, we ex-plore using paraphrases.
Unlike parse trees, para-phrases can be produced quickly by humans anddon?t require extensive linguistic training.
Whileparaphrases are not parse trees, a sentence and itsparaphrase may have similar syntactic structuresfor portions where they can be aligned.We can improve parsers by jointly parsing asentence with its paraphrase and encouraging cer-tain types of overlaps in their syntactic structures.As a simple example, consider replacing an un-known word in a sentence with a synonym foundin the training data.
This may help disambiguatethe sentence without changing its parse tree.
Moredisruptive forms of paraphrasing (e.g., topicaliza-tion) can also be handled by not requiring strictagreement between the parses.In this paper, we use paraphrases to improveparsing inference within and across domains.We develop methods using dual-decomposition(where the parses of both sentences from a depen-dency parser are encouraged to agree, Section 3.2)and pair-finding (which can be applied to any n-best parser, Section 3.3).
Some paraphrases signif-icantly disrupt syntactic structure.
To counter this,we examine relaxing agreement constraints andbuilding classifiers to predict when joint parsingwon?t be beneficial (Section 3.4).
We show thatparaphrases can be exploited to improve cross-domain parser inference for two state-of-the-artparsers, especially on domains where they performpoorly.2 Related WorkMany constituency parsers can parse Englishnewswire text with high accuracy (Collins, 2000;1223Charniak and Johnson, 2005; Petrov et al, 2006;Socher et al, 2013; Coppola and Steedman,2013).
Likewise, dependency parsers have rapidlyimproved their accuracy on a variety of lan-guages (Eisner, 1996; McDonald et al, 2005;Nivre et al, 2007; Koo and Collins, 2010; Zhangand McDonald, 2014; Lei et al, 2014).
Thereare many approaches tackling the problem of im-proving parsing accuracy both within and acrossdomains, including self-training/uptraining (Mc-Closky et al, 2006b; Petrov et al, 2010), rerank-ing (Collins, 2000; McClosky et al, 2006b), incor-porating word clusters (Koo et al, 2008), modelcombination (Petrov, 2010), automatically weight-ing training data (McClosky et al, 2010), and us-ing n-gram counts from large corpora (Bansal andKlein, 2011).
Using paraphrases falls into thesemi-supervised category.
As we show later, in-corporating paraphrases provides complementarybenefits to self-training.2.1 ParaphrasesWhile paraphrases are difficult to define rigor-ously (Bhagat and Hovy, 2013), we only require aloose definition in this work: a pair of phrases thatmean approximately the same thing.
Paraphrasescan be constructed in various ways: replacingwords with synonyms, reordering clauses, addingrelative clauses, using negation and antonyms, etc.Table 1 lists some example paraphrases.There are a variety of paraphrase resources pro-duced by humans (Dolan and Brockett, 2005) andautomatic methods (Ganitkevitch et al, 2013).Recent works have shown that reliable para-phrases can be crowdsourced at low cost (Ne-gri et al, 2012; Burrows et al, 2013; Tschir-sich and Hintz, 2013).
Paraphrases have beenshown to help summarization (Cohn and Lap-ata, 2013), question answering (Duboue and Chu-Carroll, 2006; Fader et al, 2013), machine trans-lation (Callison-Burch et al, 2006), and seman-tic parsing (Berant and Liang, 2014).
Paraphraseshave been applied to syntactic tasks, such asprepositional phrase attachment and noun com-pounding, where the corpus frequencies of differ-ent syntactic constructions (approximated by websearches) are used to help disambiguate (Nakovand Hearst, 2005).
One method for transformingconstructions is to use paraphrase templates.How did Bob Marley die?What killed Bob Marley?How fast does a cheetah run?What is a cheetah?s top speed?He came home unexpectedly.He wasn?t expected to arrive home like that.They were far off and looked tiny.From so far away, they looked tiny.He turned and bent over the body of the Indian.Turning, he bent over the Indian?s body.No need to dramatize.There is no need to dramatize.Table 1: Example paraphrases from our dataset.2.2 Bilingual ParsingThe closest task to ours is bilingual parsing wheresentences and their translations are parsed simul-taneously (Burkett et al, 2010).
While our meth-ods differ from those used in bilingual parsing, thegeneral ideas are the same.1Translating and para-phrasing are related transformations since both ap-proximately preserve meaning.
While syntax isonly partially preserved across these transforma-tions, the overlapping portions can be leveragedwith joint inference to mutually disambiguate.
Ex-isting bilingual parsing methods typically requireparallel treebanks for training and parallel text atruntime while our methods only require paralleltext at runtime.
Since we do not have a paral-lel paraphrase treebank for training, we cannot di-rectly compare to these methods.3 Jointly Parsing ParaphrasesWith a small number of exceptions, parsers typi-cally assume that the parse of each sentence is in-dependent.
There are good reasons for this inde-pendence assumption: it simplifies parsing infer-ence and oftentimes it is not obvious how to relatemultiple sentences (though see Rush et al (2012)for one approach).
In this section, we present twomethods to jointly parse paraphrases without com-plicating inference steps.
Before going into de-tails, we give a high level picture of how jointlyparsing paraphrases can help in Figure 1.
Withthe baseline parser, the parse tree of the target sen-tence is incorrect but its paraphrase (parsed by thesame parser) is parsed correctly.
We use roughalignments to map words across sentence pairs.1Applying our methods to bilingual parsing is left as fu-ture work.1224Note the similar syntactic relations when they areprojected across the aligned words.Our goal is to encourage an appropriate levelof agreement between the two parses across align-ments.
We start by designing ?hard?
methodswhich require complete agreement between theparses.
However, since parsers are imperfect andalignments approximate, we also develop ?soft?methods which allow for disagreements.
Addi-tionally, we make procedures to decide whether touse the original (non-joint) parse or the new jointparse for each sentence since joint parses may beworse in cases where the sentences are too differ-ent and alignment fails.3.1 ObjectiveIn a typical parsing setting, given a sentence (x)and its paraphrase (y), parsers find a?
(x) and b?
(y)that satisfy the following equation:2a?, b?= argmaxa?T (x),b?T (y)f(a) + f(b)= argmaxa?T (x)f(a) + argmaxb?T (y)f(b)(1)where f is a parse-scoring function and T returnsall possible trees for a sentence.
f can take manyforms, e.g., summing the scores of arcs (Eisner,1996; McDonald et al, 2005) or multiplying prob-abilities together (Charniak and Johnson, 2005).The argmax over a and b of equation (1) is sepa-rable; parsers make two sentence-level decisions.For joint parsing, we modify the objective so thatparsers make one global decision:a?, b?= argmaxa?T (x),b?T (y): c(a,b)=0f(a) + f(b) (2)where c (defined below) measures the syntacticsimilarity between the two trees.
The smallerc(a, b) is, the more similar a and b are.
Intuitively,joint parsers must retrieve the most similar pair oftrees with the highest sum of scores.3.1.1 ConstraintsThe constraint function, c, ties two trees togetherusing alignments as a proxy for semantic informa-tion.
An alignment is a pair of words from sen-tences x and y that approximately mean the samething.
For example, in Figure 1, (helpx, helpy)is one alignment and (pestilencex, diseasey) is2When it is clear from context, we omit x and y to sim-plify notation.Set u0(i, j) = 0 for all i, j ?
Efor k = 1 to K doak= argmaxa?T (x)(f(a)+?i,j?Euk(i, j)a(i, j))bk= argmaxb?T (y)(f(b)?
?i,j?Euk(i, j)b(i, j))v, uk+1= UPDATE(uk, ?k, ak, bk)if v = 0 then return ak, bkreturn aK, bKfunction UPDATE(u, ?, a, b)v = 0, u?
(i, j) = 0 for all i, j ?
Efor i, j ?
E dou?
(i, j) = u(i, j)?
?
(a(i, j)?
b(i, j))if a(i, j) 6= b(i, j) then v = v + 1return v, u?Algorithm 1: Dual decomposition for jointlyparsing paraphrases pseudocode.
E is theset of all possible edges between any pair ofaligned words.
Given ` aligned word pairs,E = {1, .
.
.
, `} ?
{1, .
.
.
, `}.
a(i, j) is one ifthe ith aligned word is the head of jth alignedword, zero otherwise.
u(i, j) is the dual valueof an edge from the ith aligned word to the jthaligned word.
?kis the step size at kth itera-tion.another.
To simplify joint parsing, we assumethe aligned words play the same syntactic roles(which is obviously not always true and shouldbe revisited in future work).
c measures the syn-tactic similarity by computing how many pairs ofalignments have different syntactic head relations.For the two trees in Figure 1, we see two differ-ent relations: (helpx??
dying, help 6y??
dying)and (natives 6x??
dying, nativesy??
dying).
Therest have the same relation so c(a, b) = 2.
Aswe?ll show in Section 5, the constraints definedabove are too restrictive because of this strong as-sumption.
To alleviate the problem, we presentways of appropriately changing constraints later.We now turn to the first method of incorporatingconstraints into joint parsing.3.2 Constraints via Dual DecompositionDual decomposition (Rush and Collins, 2012) iswell-suited for finding the MAP assignment toequation (2).
When the parse-scoring function fincludes an arc-factored component as in McDon-ald et al (2005), it is straightforward to incorpo-1225(target sentence) x: help some natives dying of pestilence(paraphrase) y: help some natives who were dying of diseasewrongrightFigure 1: An illustration of joint parsing a sentence with its paraphrase.
Unaligned words are gray.
Jointparsing encourages structural similarity and allows the parser to correct the incorrect arc.rate constraints as shown in Algorithm 1.
Essen-tially, dual decomposition penalizes relations thatare different in two trees by adding/subtractingdual values to/from arc scores.
When dual de-composition is applied in Figure 1, the arc scoreof (helpx??
dying) decreases and the score for(nativesx??
dying) increases in the second itera-tion, which eventually leads the algorithm to favorthe latter.We relax the constraints by employing soft dualdecomposition (Anzaroot et al, 2014) and replac-ing UPDATE in Algorithm 1 with S-UPDATE fromAlgorithm 2.
The problem with the original con-straints is they force every pair of alignments tohave the same relation even when some alignedwords certainly play different syntactic roles.
Theintroduced slack variable lets some alignmentshave different relations when parsers prefer them.Penalties bounded by the slack tend to help fix in-correct ones and not change correct parses.
In thiswork, we use a single slack variable but it?s possi-ble to have a different slack variable for each typeof dependency relation.33.3 Constraints via Pair-findingOne shortcoming of the dual decomposition ap-proach is that it only applies to parse-scoring func-tions with an arc-factored component.
We intro-duce another method for estimating equation (2)that applies to all n-best parsers.Given the n-best parses of x and the m-bestparses of y, Algorithm 3 scans through n?m pairsof trees and chooses the pair that satisfies equa-tion (2).
If it finds one pair with c(a, b) = 0, then ithas found the answer to the equation.
Otherwise, it3We did pilot experiments with multiple slack variables.Since they showed only small improvements and were harderto tune, we stuck with a single slack variable for remainingexperiments.function S-UPDATE(u, ?, a, b, s)v = 0, u?
(i, j) = 0 for all i, j ?
Efor i, j ?
E dot = max(u(i, j)?
?
(a(i, j)?
b(i, j)), 0)u?
(i, j) = min(t, s)if u?
(i, j) 6= 0, u?
(i, j) 6= s thenv = v + 1return v, u?Algorithm 2: The new UPDATE function ofsoft dual decomposition for joint parsing.
Itprojects all dual values between 0 and s ?
0.s is a slack variable that allows the algorithmto avoid satisfying some constraints.chooses the pair with the smallest c(a, b), breakingties using the scores of the parses (f(a) + f(b)).This algorithm is well suited for finding solutionsto the equation but the solutions are not necessar-ily good trees due to overly hard constraints.The algorithm often finds bad trees far down then-best list because it is mainly interested in re-trieving pairs of trees that satisfy all constraints.Parsers find such pairs with low scores if they areallowed to search through unrestricted space.
Tomitigate the problem, we shrink the search spaceby limiting n. Reducing the search space relies onthe fact that higher ranking trees are more likely tobe correct than the lower ranking ones.
Note thatwe decrease n because we are interested in recov-ering the tree of the target sentence, x. m shouldalso be decreased to improve the parse of its para-phrase, y.3.4 Logistic RegressionOne caveat of the previous two proposed methodsis that they do not know whether the original orjoint parse of x is more accurate.
Sometimes theyincrease agreement between the parses at the cost1226function PAIR-FINDING(a1:n, b1:m)Set a, b = null,min =?,max = ?
?for i = 1 to n dofor j = 1 to m dov = C (ai, bj)sum = f(ai) + f(bj)if v < min thena = ai, b = bjmin = v,max = sumelse if v = min, sum > max thena = ai, b = bjmax = sumreturn a, bfunction C(a, b)v = 0for i, j ?
E doif a(i, j) 6= b(i, j) then v = v + 1return vAlgorithm 3: The pair-finding scheme with aconstraint function, c. a1:nare the n-best treesof x and b1:mare the m-best of y.of accuracy.
To remedy this problem, we use aclassifier (specifically logistic regression) to deter-mine whether a modified tree should be used.
Theclassifier can learn the error patterns produced byeach method.3.4.1 FeaturesClassifier features use many sources of informa-tion: the target sentence x and its paraphrase y,the original and new parses of x (a0and a), andthe alignments between x and y.Crossing Edges How many arcs cross whenalignments are drawn between paraphraseson a plane divided by the length of x. Itroughly measures how many reorderings areneeded to change x to y.Non-projective Edges Whether there are morenon-projective arcs in new parse (a) than theoriginal (a0).Sentence Lengths Whether the length of x issmaller than that of y.
This feature exists be-cause baseline parsers tend to perform betteron shorter sentences.Word Overlaps The number of words in com-mon between x and y normalized by thelength of x.REL REL + RELpREL + RELp+ RELgpREL + RELgpCP CP + CPpCP + CPp+ CPgpCP + CPgpREL + CP REL + CP + CPpREL + CPp+ RELgpTable 2: Feature templates: REL is the dependencyrelation between the word and its parent.
CP isthe coarse part-of-speech tag (first two letters) of aword.
p and gp select the parent and grandparentof the word respectively.Parse Structure Templates The feature genera-tor goes through every word in {a0, a} andsets the appropriate boolean features from Ta-ble 2.
Features are prefixed by whether theycome from a0or a.4 Data and ProgramsThis section describes our paraphrase dataset,parsers, and other tools used in experiments.4.1 Paraphrase DatasetTo evaluate the efficacy of the proposed methodsof jointly parsing paraphrases, we built a corpusof paraphrases where one sentence in a pair ofparaphrases has a gold tree.4We randomly sam-pled 4,000 sentences5from four gold treebanks:Brown, British National Corpus (BNC), Question-Bank6(QB) and Wall Street Journal (section 24)(Francis and Ku?cera, 1989; Foster and van Gen-abith, 2008; Judge et al, 2006; Marcus et al,1993).
A linguist provided a paraphrase for eachsampled sentence according to these instructions:The paraphrases should more or lessconvey the same information as the orig-inal sentence.
That is, the two sentencesshould logically entail each other.
Theparaphrases should generally use mostof the same words (but not necessarilyin the same order).
Active/passive trans-forms, changing words with synonyms,and rephrasings of the same idea are allexamples of transformations that para-phrases can use (others can be used too).4The dataset is available upon request.5We use sentences with 6 to 25 tokens to keep the para-phrasing task in the nontrivial to easy range.6With Stanford?s updates: http://nlp.stanford.edu/data/QuestionBank-Stanford.shtml1227They can be as simple as just chang-ing a single word in some cases (though,ideally, a variety of paraphrasing tech-niques would be used).We also provided 10 pairs of sentences as ex-amples.
We evaluate our methods only on thesampled sentences from the gold corpora becausethe new paraphrases do not include syntactic trees.The data was divided into development and test-ing sets such that development and testing sharethe same distribution over the four corpora.
Para-phrases were tokenized by the BLLIP tokenizer.See Table 3 for statistics of the dataset.74.2 Meteor Word AlignerWe use Meteor, a monolingual word aligner de-veloped by Denkowski and Lavie (2014), to findalignments between paraphrases.
It uses the ex-act matches, stems, synonyms, and paraphrases8to form these alignments.
Because it uses para-phrases, it sometimes aligns multiple words fromsentence x to one or more words from sentence yor vice versa.
We ignore these multiword align-ments because our methods currently only handlesingle word alignments.
In pilot experiments, wealso tried using a simple aligner which requiredexact word matches.
Joint parsing with simpleralignments improved parsing accuracy but not asmuch as Meteor.9Thus, all results in Section 5 useMeteor for word alignment.
On average across thefour corpora, 73% of the tokens are aligned.4.3 ParsersWe use a dependency and constituency parser forour experiments: RBG and BLLIP.
RBG parser(Lei et al, 2014) is a state-of-the-art dependencyparser.10It is a third-order discriminative depen-dency parser with low-rank tensors as part of itsfeatures.
BLLIP (Charniak and Johnson, 2005)is a state-of-the-art constituency parser, which is7The distribution over four corpora is skewed becauseeach corpus has a different number of sentences within lengthconstraints.
Samples are collected uniformly over all sen-tences that satisfy the length criterion.8Here paraphrase means a single/multiword phrase that issemantically similar to another single/multiword.9The pilot was conducted on fewer than 700 sentencepairs before all paraphrases were created.
We give Meteortokenized paraphrases with capitalization.
Maximizing accu-racy rather than coverage worked better in pilot experiments.10http://github.com/taolei87/RBGParser,?master?
version from June 24th, 2014.composed of a generative parser and a discrimina-tive reranker.11To train RBG and BLLIP, we used the standardWSJ training set (sections 2?21, about 40,000 sen-tences).12We also used the self-trained BLLIPparsing model which is trained on an additionaltwo million Gigaword parses generated by theBLLIP parser (McClosky et al, 2006a).4.4 Logistic RegressionWe use the logistic regression implementationfrom Scikit-learn13with hand-crafted featuresfrom Section 3.4.1.
The classifier decides towhether to keep the parse trees from the jointmethod.
When it decides to disregard them, it re-turns the parse from the baseline parser.
We traina separate classifier for each joint method.5 ExperimentsWe ran all tuning and model design experimentson the development set.
For the final evaluation,we tuned parameters on the development set andevaluate them on the test set.
Constituency treeswere converted to basic non-collapsed dependencytrees using Stanford Dependencies (De Marneffeet al, 2006).14We report unlabeled attachmentscores (UAS) for all experiments and labeled at-tachment scores (LAS) as well in final evalua-tion, ignoring punctuation.
Averages are micro-averages across all sentences.5.1 Dual DecompositionSince BLLIP is not arc-factored, these experi-ments only use RBG.
Several parameters need tobe fixed beforehand: the slack constant (s), thelearning rate (?
), and the maximum number of it-erations (K).
We set ?0= 0.1 and ?k=?02twheret is the number of times the dual score has in-creased (Rush et al, 2010).
We choose K = 20.These numbers were chosen from pilot studies.The slack variable (s = 0.5) was tuned with agrid search on values between 0.1 and 1.5 with in-terval 0.1.
We chose a value that generalizes wellacross four corpora as opposed to a value that does11http://github.com/BLLIP/bllip-parser12RBG parser requires predicted POS tags.
We used theStanford tagger (Toutanova et al, 2003) to tag WSJ andparaphrase datasets.
Training data was tagged using 20-foldcross-validation and the paraphrases were tagged by a taggertrained on all of WSJ training.13http://scikit-learn.org14Version 1.3.5, previously numbered as version 2.0.51228Development TestBNC Brown QB WSJ Total BNC Brown QB WSJ TotalSentences 247 558 843 352 2,000 247 558 844 351 2,000Tokens 4,297 7,937 8,391 5,924 26,549 4,120 8,025 8,253 5,990 26,388Tokens?4,372 8,088 8,438 6,122 27,020 4,272 8,281 8,189 6,232 26,974Word types 1,727 2,239 2,261 1,955 6,161 1,710 2,337 2,320 1,970 6,234Word types?1,676 2,241 2,261 1,930 6,017 1,675 2,335 2,248 1,969 6,094OOV 11.2 5.1 5.4 2.4 5.6 11.5 5.1 5.8 2.2 5.7OOV?8.6 4.7 5.4 2.6 5.1 9.3 4.8 6.0 2.4 5.3Tokens/sent.
17.4 14.2 10.0 16.8 13.3 16.7 14.4 7.8 17.1 13.2Avg.
aligned 13.1 10.5 6.9 13.0 9.7 12.6 10.7 6.7 13.0 9.7Table 3: Statistics for the four corpora of the paraphrase dataset.
Most statistics are counted from sen-tences with gold trees, including punctuation.
?
indicates the statistic is from the paraphrased sentences.?Avg.
aligned?
is the average number of aligned tokens from the original sentences using Meteor.
OOVis the percentage of tokens not seen in the WSJ training.Avg BNC Brown QB WSJRBG 86.4 89.2 90.9 75.8 93.7+ Dual 84.7 87.5 87.8 76.0 91.0+ S-Dual 86.8 89.8 90.9 76.5 94.0Table 4: Comparison of hard and soft dual de-composition for joint parsing (development sec-tion, UAS).very well on a single corpus.
As shown in Ta-ble 4, joint parsing with hard dual decompositionperforms worse than independent parsing (RBG).This is expected because hard dual decompositionforces every pair of alignments to form the samerelation even when they should not.
With relaxedconstraints (S-Dual), joint parsing performs sig-nificantly better than independent parsing.
Softdual decomposition improves across all domainsexcept for Brown (where it ties).5.2 Pair-findingThese experiments use the 50-best trees fromBLLIP parser.
When converting to dependencies,some constituency trees map to the same depen-dency tree.
In this case, trees with lower rankingsare dropped.
Like joint parsing with hard dual de-composition, joint parsing with unrestricted pair-finding (n = 50) allows significantly worse parsesto be selected (Table 5).
With small n values,pair-finding improves over the baseline BLLIPparser.15Experiments with self-trained BLLIPexhibit similar results so we use n = 2 for all15Decreasing m did not lead to further improvement andthus we don?t report the results of changing m.n Avg BNC Brown QB WSJ1 89.5 91.1 91.6 83.3 94.22 90.0 91.4 92.3 84.1 94.13 89.8 91.5 92.0 84.2 93.95 89.2 91.9 91.4 83.0 93.210 87.9 90.5 90.3 81.4 92.250 86.3 90.2 88.7 78.6 91.1Table 5: UAS of joint parsing using the pair-finding scheme with various n values on the de-velopment portion.
n = 1 is the baseline BLLIPparser and n > 1 is BLLIP with pair-finding.other experiments.
Interestingly, each corpus hasa different optimal value for n which suggests wemight improve accuracy further if we know the do-main of each sentence.5.3 Logistic RegressionThe classifier is trained on sentences where parsescores (UAS) of the proposed methods are higheror lower than those of the baselines16fromthe development set using leave-one-out cross-validation.
We use random greedy search to selectspecific features from the 15 feature templates de-fined in Section 3.4.1.
Features seen fewer thanthree times in the development are thrown out.Separate regression models are built for three dif-ferent parsers.
The logistic regression classifieruses an L1penalty with regularization parameterC = 1.Logistic regression experiments are reported in16We only use sentences with different scores to limit ceil-ing effects.1229Avg BNC Brown QB WSJRBG 86.4 89.2 90.9 75.8 93.7+ S-Dual 86.8 89.8 90.9 76.5 94.0+ Logit 86.9 89.8 91.1 76.5 94.0BLLIP 89.5 91.1 91.6 83.3 94.2+ Pair 90.0 91.4 92.3 84.1 94.1+ Logit 90.3 91.3 92.1 85.2 94.3BLLIP-ST 90.1 92.7 92.3 84.3 93.8+ Pair 90.7 93.5 92.5 85.6 93.8+ Logit 91.1 93.3 92.6 86.7 93.9Table 6: Effect of using logistic regression ontop of each method (UAS).
Leave-one-out cross-validation is performed on the development data.+X means augmenting the above system with X.Table 6.
All parsers benefit from employing logis-tic regression models on top of paraphrase meth-ods.
BLLIP experiments show a larger improve-ment than RBG.
This may be because BLLIP can-not use soft constraints so its errors are more pro-nounced.5.4 Final EvaluationWe evaluate the three parsers on the test set usingthe tuned parameters and logistic regression mod-els from above.
Joint parsing with paraphrases sig-nificantly improves accuracy for all systems (Ta-ble 7).
Self-trained BLLIP with logistic regres-sion is the most accurate, though RBG with S-Dual provides the most consistent improvements.Joint parsing without logistic regression (RBG+ S-Dual) is more accurate than independent pars-ing (RBG) overall.
With the help of logistic re-gression, the methods do at least as well as theirbaseline counterparts on all domains with the ex-ception of self-trained BLLIP on BNC.
We believethat the drop on BNC is largely due to noise as ourBNC test set is the smallest of the four.
As on de-velopment, logistic regression does not change theaccuracy much over the RBG parser with soft dualdecomposition.Joint parsing provides the largest gains onQuestionBank, the domain with the lowest base-line accuracies.
This fits with our goal of usingparaphrases for domain adaptation ?
parsing withparaphrases helps the most on domains furthestfrom our training data.5.5 Error analysisWe analyzed the errors from RBG and BLLIPalong several dimensions: by dependency label,sentence length, dependency length, alignmentstatus (whether a token was aligned), percentageof tokens aligned in the sentence, and edit distancebetween the sentence pairs.
Most errors are fairlyuniformly distributed across these dimensions andindicate general structural improvements when us-ing paraphrases.
BLLIP saw a 2.2% improvementfor the ROOT relation, though RBG?s improvementhere was more moderate.
For sentence lengths,BLLIP obtains larger boosts for shorter sentenceswhile RBG?s are more uniform.
RBG gets a 1.4%UAS improvement on longer dependencies (6 ormore tokens) while shorter dependencies are moremodestly improved by about 0.3-0.5% UAS.
Sur-prisingly, alignment information provides no sig-nal as to whether accuracy improves.Additionally, we had our annotator label a por-tion of our dataset with the set of paraphrasingoperations employed.17While most paraphrasingoperations generally improved performance un-der joint inference, the largest reliable gains camefrom lexical replacements (e.g., synonyms).6 Conclusions and Future WorkOur methods of incorporating paraphrases im-prove parsing across multiple domains for state-of-the-art constituency and dependency parsers.We leverage the fact that paraphrases often expressthe same semantics with similar syntactic realiza-tions.
These provide benefits even on top of self-training, another domain adaptation technique.Since paraphrases are not available at mosttimes, our methods may seem limited.
However,there are several possible use cases.
The bestcase scenario is when users can be directly askedto rephrase a question and provide a paraphrase.For instance, question answering systems can askusers to rephrase questions when an answer ismarked as wrong by users.
Another option is touse crowdsourcing to quickly create a paraphrasecorpus (Negri et al, 2012; Burrows et al, 2013;Tschirsich and Hintz, 2013).
As part of futurework, we plan to integrate existing larger para-phrase resources, such as WikiAnswers (Fader etal., 2013) and PPDB (Ganitkevitch et al, 2013).WikiAnswers provides rough equivalence classesof questions.
PPDB includes phrasal and syntacticalignments which could supplement our existingalignments or be used as proxies for paraphrases.17See the extended version of this paper for more informa-tion about this task and its results.1230Avg BNC Brown QB WSJRBG 86.7 (81.3) 89.3 (83.7) 90.2 (84.1) 77.0 (71.0) 93.7 (89.9)+ S-Dual 87.3 (81.7) 89.6 (83.8) 90.7 (84.6) 78.1 (71.8) 94.0 (90.2)+ Logit 87.2 (81.6) 89.7 (83.9) 90.6 (84.5) 77.9 (71.7) 93.8 (89.9)BLLIP 89.6 (86.1) 90.6 (87.2) 91.7 (87.9) 83.6 (79.9) 94.3 (91.6)+ Pair 90.1 (86.5) 90.8 (87.3) 92.1 (88.4) 84.7 (80.7) 94.4 (91.6)+ Logit 90.3 (86.8) 90.6 (87.2) 91.9 (88.1) 85.5 (81.7) 94.5 (91.7)BLLIP-ST 90.4 (87.0) 91.8 (88.3) 92.7 (89.0) 84.8 (81.2) 94.3 (91.4)+ Pair 90.5 (87.1) 91.1 (87.6) 92.7 (89.1) 85.5 (81.8) 94.2 (91.4)+ Logit 91.0 (87.6) 91.4 (88.0) 92.9 (89.3) 86.6 (82.9) 94.3 (91.4)Table 7: Final evaluation on testing data.
Numbers are unlabeled attachment score (labeled attachmentscore).
+X indicates extending the above system with X. BLLIP-ST is BLLIP using the self-trainedmodel.
Coloring indicates a significant difference over baseline (p < 0.01).While these resources are noisy, the quantity ofdata may provide additional robustness.
Lastly, in-tegrating our methods with paraphrase detectionor generation systems could help provide para-phrases on demand.There are many other ways to extend this work.Poor alignments are one of the larger sources oferrors and improving alignments could help dra-matically.
One simple extension is to use multipleparaphrases and their alignments instead of justone.
More difficult would be to learn the align-ments jointly while parsing and adaptively learnhow alignments affect syntax.
Our constraints canonly capture certain types of paraphrase transfor-mations currently and should be extended to un-derstand common tree transformations for para-phrases, as in (Heilman and Smith, 2010).
Fi-nally, and perhaps most importantly, our methodsapply only at inference time.
We plan to investi-gate methods which use paraphrases to augmentparsing models created at train time.AcknowledgmentsWe would like to thank Eugene Charniak forthe idea of using paraphrases to improve parsing,our anonymous reviewers for their valuable feed-back, Karen Ingraffea for constructing and classi-fying the paraphrase corpus, Dave Buchanan andWill Headden for last minute paper reading, theDeepQA team at IBM for feedback and supporton the project, and Mohit Bansal and Micha El-sner for helpful discussions.ReferencesSam Anzaroot, Alexandre Passos, David Belanger, andAndrew McCallum.
2014.
Learning soft linearconstraints with application to citation field extrac-tion.
In Proceedings of the 52nd Annual Meetingof the Association for Computational Linguistics,pages 593?602.
Association for Computational Lin-guistics.Michiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochas-tic grammars.
Computer speech & language,20(1):41?68.Mohit Bansal and Dan Klein.
2011.
Web-scale fea-tures for full-scale parsing.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 693?702.
Association for ComputationalLinguistics.Jonathan Berant and Percy Liang.
2014.
Seman-tic parsing via paraphrasing.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics, pages 1415?1425.
Associationfor Computational Linguistics.Rahul Bhagat and Eduard Hovy.
2013.
What is a para-phrase?
Computational Linguistics, 39(3):463?472.David Burkett, John Blitzer, and Dan Klein.
2010.Joint parsing and alignment with weakly synchro-nized grammars.
In Human Language Technolo-gies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 127?135.
Association forComputational Linguistics.Steven Burrows, Martin Potthast, and Benno Stein.2013.
Paraphrase acquisition via crowdsourcing andmachine learning.
ACM Transactions on IntelligentSystems and Technology (TIST), 4(3):43.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine transla-tion using paraphrases.
In Proceedings of the main1231conference on Human Language Technology Con-ference of the North American Chapter of the Asso-ciation of Computational Linguistics, pages 17?24.Association for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics, pages 173?180.
Association for ComputationalLinguistics.Trevor Cohn and Mirella Lapata.
2013.
An ab-stractive approach to sentence compression.
ACMTransactions on Intelligent Systems and Technology,4(3):41.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of theSeventeenth International Conference on MachineLearning, pages 175?182.
ICML.Gregory Coppola and Mark Steedman.
2013.
The ef-fect of higher-order dependency features in discrimi-native phrase-structure parsing.
In ACL, pages 610?616.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D. Manning, et al 2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proceedings of LREC, pages 449?454.Michael Denkowski and Alon Lavie.
2014.
Meteoruniversal: Language specific translation evaluationfor any target language.
In Proceedings of the NinthWorkshop on Statistical Machine Translation, pages376?380.
Association for Computational Linguis-tics.William B. Dolan and Chris Brockett.
2005.
Auto-matically constructing a corpus of sentential para-phrases.
In Proc.
of IWP.Pablo Ariel Duboue and Jennifer Chu-Carroll.
2006.Answering the question you wish they had asked:The impact of paraphrasing for question answering.In Proceedings of the Human Language TechnologyConference of the NAACL, pages 33?36.
Associationfor Computational Linguistics.Jason M. Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: An exploration.
In Pro-ceedings of the 16th conference on Computationallinguistics, pages 340?345.
Association for Compu-tational Linguistics.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-driven learning for open ques-tion answering.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics, pages 1608?1618.
Association for Compu-tational Linguistics.Jennifer Foster and Josef van Genabith.
2008.
Parserevaluation and the BNC: Evaluating 4 constituencyparsers with 3 metrics.
In Proceedings of the SixthInternational Conference on Language Resourcesand Evaluation.
European Language Resources As-sociation.Winthrop Nelson Francis and Henry Ku?cera.
1989.Manual of information to accompany a standardcorpus of present-day edited American English, foruse with digital computers.
Brown University, De-partment of Linguistics.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of NAACL-HLT, pages758?764.
Association for Computational Linguis-tics.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of the 2001 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 167?202.Michael Heilman and Noah A Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 1011?1019.Association for Computational Linguistics.John Judge, Aoife Cahill, and Josef Van Genabith.2006.
QuestionBank: creating a corpus of parse-annotated questions.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and the 44th annual meeting of the Associationfor Computational Linguistics, pages 497?504.
As-sociation for Computational Linguistics.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1?11.
Association forComputational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL: HLT, pages 595?603.
Asso-ciation for Computational Linguistics.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics, pages 1381?1391.
Associationfor Computational Linguistics.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.
Com-putational linguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006a.
Effective self-training for parsing.
InProceedings of the main conference on human lan-guage technology conference of the North AmericanChapter of the Association of Computational Lin-guistics, pages 152?159.
Association for Computa-tional Linguistics.1232David McClosky, Eugene Charniak, and Mark John-son.
2006b.
Reranking and self-training for parseradaptation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Compu-tational Linguistics, pages 337?344.
Association forComputational Linguistics.David McClosky, Eugene Charniak, and Mark John-son.
2010.
Automatic domain adaptation for pars-ing.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 28?36.
Association for Computational Lin-guistics.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, pages 523?530.
Association for Computa-tional Linguistics.Preslav Nakov and Marti Hearst.
2005.
Usingthe web as an implicit training set: Applicationto structural ambiguity resolution.
In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing, pages 835?842, Vancouver,British Columbia, Canada, October.
Association forComputational Linguistics.Matteo Negri, Yashar Mehdad, Alessandro Marchetti,Danilo Giampiccolo, and Luisa Bentivogli.
2012.Chinese whispers: Cooperative paraphrase acquisi-tion.
In LREC, pages 2659?2665.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(02):95?135.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433?440.Association for Computational Linguistics.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, andHiyan Alshawi.
2010.
Uptraining for accurate de-terministic question parsing.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 705?713.
Associationfor Computational Linguistics.Slav Petrov.
2010.
Products of random latent vari-able grammars.
In Human Language Technologies:The 2010 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 19?27.
Association for Computa-tional Linguistics.Alexander M. Rush and Michael Collins.
2012.
Atutorial on dual decomposition and Lagrangian re-laxation for inference in natural language process-ing.
Journal of Artificial Intelligence Research,45(1):305?362.Alexander M. Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On dual decompositionand linear programming relaxations for natural lan-guage processing.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1?11.
Association for Computa-tional Linguistics.Alexander M. Rush, Roi Reichart, Michael Collins,and Amir Globerson.
2012.
Improved parsing andPOS tagging using inter-sentence consistency con-straints.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 1434?1444.
Association for Com-putational Linguistics.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013.
Parsing with composi-tional vector grammars.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics, pages 455?465.
Association forComputational Linguistics.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In Proceedingsof the Twelfth Conference on Computational Natu-ral Language Learning, pages 159?177.
Associationfor Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Martin Tschirsich and Gerold Hintz.
2013.
Leveragingcrowdsourcing for paraphrase recognition.
In Pro-ceedings of the 7th Linguistic Annotation Workshopand Interoperability with Discourse, pages 205?213,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Hao Zhang and Ryan McDonald.
2014.
Enforcingstructural diversity in cube-pruned dependency pars-ing.
In Proceedings of the 52nd Annual Meetingof the Association for Computational Linguistics,pages 656?661.
Association for Computational Lin-guistics.1233
