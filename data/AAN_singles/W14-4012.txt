Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103?111,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsOn the Properties of Neural Machine Translation: Encoder?DecoderApproachesKyunghyun Cho Bart van Merri?enboerUniversit?e de Montr?ealDzmitry Bahdanau?Jacobs University Bremen, GermanyYoshua BengioUniversit?e de Montr?eal, CIFAR Senior FellowAbstractNeural machine translation is a relativelynew approach to statistical machine trans-lation based purely on neural networks.The neural machine translation models of-ten consist of an encoder and a decoder.The encoder extracts a fixed-length repre-sentation from a variable-length input sen-tence, and the decoder generates a correcttranslation from this representation.
In thispaper, we focus on analyzing the proper-ties of the neural machine translation us-ing two models; RNN Encoder?Decoderand a newly proposed gated recursive con-volutional neural network.
We show thatthe neural machine translation performsrelatively well on short sentences withoutunknown words, but its performance de-grades rapidly as the length of the sentenceand the number of unknown words in-crease.
Furthermore, we find that the pro-posed gated recursive convolutional net-work learns a grammatical structure of asentence automatically.1 IntroductionA new approach for statistical machine transla-tion based purely on neural networks has recentlybeen proposed (Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014).
This new approach, whichwe refer to as neural machine translation, is in-spired by the recent trend of deep representationallearning.
All the neural network models used in(Sutskever et al., 2014; Cho et al., 2014) consist ofan encoder and a decoder.
The encoder extracts afixed-length vector representation from a variable-length input sentence, and from this representationthe decoder generates a correct, variable-lengthtarget translation.
?Research done while visiting Universit?e de Montr?ealThe emergence of the neural machine transla-tion is highly significant, both practically and the-oretically.
Neural machine translation models re-quire only a fraction of the memory needed bytraditional statistical machine translation (SMT)models.
The models we trained for this paperrequire only 500MB of memory in total.
Thisstands in stark contrast with existing SMT sys-tems, which often require tens of gigabytes ofmemory.
This makes the neural machine trans-lation appealing in practice.
Furthermore, un-like conventional translation systems, each and ev-ery component of the neural translation model istrained jointly to maximize the translation perfor-mance.As this approach is relatively new, there has notbeen much work on analyzing the properties andbehavior of these models.
For instance: Whatare the properties of sentences on which this ap-proach performs better?
How does the choice ofsource/target vocabulary affect the performance?In which cases does the neural machine translationfail?It is crucial to understand the properties and be-havior of this new neural machine translation ap-proach in order to determine future research di-rections.
Also, understanding the weaknesses andstrengths of neural machine translation might leadto better ways of integrating SMT and neural ma-chine translation systems.In this paper, we analyze two neural machinetranslation models.
One of them is the RNNEncoder?Decoder that was proposed recently in(Cho et al., 2014).
The other model replaces theencoder in the RNN Encoder?Decoder model witha novel neural network, which we call a gatedrecursive convolutional neural network (grConv).We evaluate these two models on the task of trans-lation from French to English.Our analysis shows that the performance ofthe neural machine translation model degrades103quickly as the length of a source sentence in-creases.
Furthermore, we find that the vocabularysize has a high impact on the translation perfor-mance.
Nonetheless, qualitatively we find that theboth models are able to generate correct transla-tions most of the time.
Furthermore, the newlyproposed grConv model is able to learn, withoutsupervision, a kind of syntactic structure over thesource language.2 Neural Networks for Variable-LengthSequencesIn this section, we describe two types of neuralnetworks that are able to process variable-lengthsequences.
These are the recurrent neural net-work and the proposed gated recursive convolu-tional neural network.2.1 Recurrent Neural Network with GatedHidden Neuronszrh h~ x(a) (b)Figure 1: The graphical illustration of (a) the re-current neural network and (b) the hidden unit thatadaptively forgets and remembers.A recurrent neural network (RNN, Fig.
1 (a))works on a variable-length sequence x =(x1,x2, ?
?
?
,xT) by maintaining a hidden state hover time.
At each timestep t, the hidden state h(t)is updated byh(t)= f(h(t?1),xt),where f is an activation function.
Often f is assimple as performing a linear transformation onthe input vectors, summing them, and applying anelement-wise logistic sigmoid function.An RNN can be used effectively to learn a dis-tribution over a variable-length sequence by learn-ing the distribution over the next input p(xt+1|xt, ?
?
?
,x1).
For instance, in the case of a se-quence of 1-of-K vectors, the distribution can belearned by an RNN which has as an outputp(xt,j= 1 | xt?1, .
.
.
,x1) =exp(wjh?t?)?Kj?=1exp(wj?h?t?
),for all possible symbols j = 1, .
.
.
,K, where wjare the rows of a weight matrix W. This results inthe joint distributionp(x) =T?t=1p(xt| xt?1, .
.
.
, x1).Recently, in (Cho et al., 2014) a new activationfunction for RNNs was proposed.
The new activa-tion function augments the usual logistic sigmoidactivation function with two gating units called re-set, r, and update, z, gates.
Each gate depends onthe previous hidden state h(t?1), and the currentinput xtcontrols the flow of information.
This isreminiscent of long short-term memory (LSTM)units (Hochreiter and Schmidhuber, 1997).
Fordetails about this unit, we refer the reader to (Choet al., 2014) and Fig.
1 (b).
For the remainder ofthis paper, we always use this new activation func-tion.2.2 Gated Recursive Convolutional NeuralNetworkBesides RNNs, another natural approach to deal-ing with variable-length sequences is to use a re-cursive convolutional neural network where theparameters at each level are shared through thewhole network (see Fig.
2 (a)).
In this section, weintroduce a binary convolutional neural networkwhose weights are recursively applied to the inputsequence until it outputs a single fixed-length vec-tor.
In addition to a usual convolutional architec-ture, we propose to use the previously mentionedgating mechanism, which allows the recursive net-work to learn the structure of the source sentenceson the fly.Let x = (x1,x2, ?
?
?
,xT) be an input sequence,where xt?
Rd.
The proposed gated recursiveconvolutional neural network (grConv) consists offour weight matrices Wl, Wr, Gland Gr.
Ateach recursion level t ?
[1, T ?
1], the activationof the j-th hidden unit h(t)jis computed byh(t)j= ?c?h(t)j+ ?lh(t?1)j?1+ ?rh(t?1)j, (1)where ?c, ?land ?rare the values of a gater thatsum to 1.
The hidden unit is initialized ash(0)j= Uxj,where U projects the input into a hidden space.104?h~(a) (b) (c) (d)Figure 2: The graphical illustration of (a) the recursive convolutional neural network and (b) the proposedgated unit for the recursive convolutional neural network.
(c?d) The example structures that may belearned with the proposed gated unit.The new activation?h(t)jis computed as usual:?h(t)j= ?
(Wlh(t)j?1+Wrh(t)j),where ?
is an element-wise nonlinearity.The gating coefficients ?
?s are computed by???c?l?r?
?=1Zexp(Glh(t)j?1+Grh(t)j),where Gl,Gr?
R3?dandZ =3?k=1[exp(Glh(t)j?1+Grh(t)j)]k.According to this activation, one can think ofthe activation of a single node at recursion level tas a choice between either a new activation com-puted from both left and right children, the acti-vation from the left child, or the activation fromthe right child.
This choice allows the overallstructure of the recursive convolution to changeadaptively with respect to an input sample.
SeeFig.
2 (b) for an illustration.In this respect, we may even consider the pro-posed grConv as doing a kind of unsupervisedparsing.
If we consider the case where the gat-ing unit makes a hard decision, i.e., ?
follows an1-of-K coding, it is easy to see that the networkadapts to the input and forms a tree-like structure(See Fig.
2 (c?d)).
However, we leave the furtherinvestigation of the structure learned by this modelfor future research.3 Purely Neural Machine Translation3.1 Encoder?Decoder ApproachThe task of translation can be understood from theperspective of machine learning as learning theEconomic growth has slowed down in recent years .La croissance ?conomique a ralenti ces derni?res ann?es .
[z  ,z  , ... ,z  ]1 2 dEncodeDecodeFigure 3: The encoder?decoder architectureconditional distribution p(f | e) of a target sen-tence (translation) f given a source sentence e.Once the conditional distribution is learned by amodel, one can use the model to directly samplea target sentence given a source sentence, eitherby actual sampling or by using a (approximate)search algorithm to find the maximum of the dis-tribution.A number of recent papers have proposed touse neural networks to directly learn the condi-tional distribution from a bilingual, parallel cor-pus (Kalchbrenner and Blunsom, 2013; Cho et al.,2014; Sutskever et al., 2014).
For instance, the au-thors of (Kalchbrenner and Blunsom, 2013) pro-posed an approach involving a convolutional n-gram model to extract a vector of a source sen-tence which is decoded with an inverse convolu-tional n-gram model augmented with an RNN.
In(Sutskever et al., 2014), an RNN with LSTM unitswas used to encode a source sentence and startingfrom the last hidden state, to decode a target sen-tence.
Similarly, the authors of (Cho et al., 2014)proposed to use an RNN to encode and decode apair of source and target phrases.At the core of all these recent works lies anencoder?decoder architecture (see Fig.
3).
Theencoder processes a variable-length input (sourcesentence) and builds a fixed-length vector repre-sentation (denoted as z in Fig.
3).
Conditioned onthe encoded representation, the decoder generates105a variable-length sequence (target sentence).Before (Sutskever et al., 2014) this encoder?decoder approach was used mainly as a part of theexisting statistical machine translation (SMT) sys-tem.
This approach was used to re-rank the n-bestlist generated by the SMT system in (Kalchbren-ner and Blunsom, 2013), and the authors of (Choet al., 2014) used this approach to provide an ad-ditional score for the existing phrase table.In this paper, we concentrate on analyzing thedirect translation performance, as in (Sutskever etal., 2014), with two model configurations.
In bothmodels, we use an RNN with the gated hiddenunit (Cho et al., 2014), as this is one of the onlyoptions that does not require a non-trivial way todetermine the target length.
The first model willuse the same RNN with the gated hidden unit asan encoder, as in (Cho et al., 2014), and the secondone will use the proposed gated recursive convo-lutional neural network (grConv).
We aim to un-derstand the inductive bias of the encoder?decoderapproach on the translation performance measuredby BLEU.4 Experiment Settings4.1 DatasetWe evaluate the encoder?decoder models on thetask of English-to-French translation.
We use thebilingual, parallel corpus which is a set of 348Mselected by the method in (Axelrod et al., 2011)from a combination of Europarl (61M words),news commentary (5.5M), UN (421M) and twocrawled corpora of 90M and 780M words respec-tively.1We did not use separate monolingual data.The performance of the neural machien transla-tion models was measured on the news-test2012,news-test2013 and news-test2014 sets ( 3000 lineseach).
When comparing to the SMT system, weuse news-test2012 and news-test2013 as our de-velopment set for tuning the SMT system, andnews-test2014 as our test set.Among all the sentence pairs in the preparedparallel corpus, for reasons of computational ef-ficiency we only use the pairs where both Englishand French sentences are at most 30 words long totrain neural networks.
Furthermore, we use onlythe 30,000 most frequent words for both Englishand French.
All the other rare words are consid-1All the data can be downloaded from http://www-lium.univ-lemans.fr/?schwenk/cslm_joint_paper/.ered unknown and are mapped to a special token([UNK]).4.2 ModelsWe train two models: The RNN Encoder?Decoder (RNNenc)(Cho et al., 2014) and thenewly proposed gated recursive convolutionalneural network (grConv).
Note that both modelsuse an RNN with gated hidden units as a decoder(see Sec.
2.1).We use minibatch stochastic gradient descentwith AdaDelta (Zeiler, 2012) to train our two mod-els.
We initialize the square weight matrix (transi-tion matrix) as an orthogonal matrix with its spec-tral radius set to 1 in the case of the RNNenc and0.4 in the case of the grConv.
tanh and a rectifier(max(0, x)) are used as the element-wise nonlin-ear functions for the RNNenc and grConv respec-tively.The grConv has 2000 hidden neurons, whereasthe RNNenc has 1000 hidden neurons.
The wordembeddings are 620-dimensional in both cases.2Both models were trained for approximately 110hours, which is equivalent to 296,144 updates and846,322 updates for the grConv and RNNenc, re-spectively.4.2.1 Translation using Beam-SearchWe use a basic form of beam-search to find a trans-lation that maximizes the conditional probabilitygiven by a specific model (in this case, either theRNNenc or the grConv).
At each time step ofthe decoder, we keep the s translation candidateswith the highest log-probability, where s = 10is the beam-width.
During the beam-search, weexclude any hypothesis that includes an unknownword.
For each end-of-sequence symbol that is se-lected among the highest scoring candidates thebeam-width is reduced by one, until the beam-width reaches zero.The beam-search to (approximately) find a se-quence of maximum log-probability under RNNwas proposed and used successfully in (Graves,2012) and (Boulanger-Lewandowski et al., 2013).Recently, the authors of (Sutskever et al., 2014)found this approach to be effective in purely neu-ral machine translation based on LSTM units.2In all cases, we train the whole network including theword embedding matrix.
The embedding dimensionality waschosen to be quite large, as the preliminary experimentswith 155-dimensional embeddings showed rather poor per-formance.106Model Development TestAllRNNenc 13.15 13.92grConv 9.97 9.97Moses 30.64 33.30Moses+RNNenc?31.48 34.64Moses+LSTM?32 35.65NoUNKRNNenc 21.01 23.45grConv 17.19 18.22Moses 32.77 35.63Model Development TestAllRNNenc 19.12 20.99grConv 16.60 17.50Moses 28.92 32.00NoUNKRNNenc 24.73 27.03grConv 21.74 22.94Moses 32.20 35.40(a) All Lengths(b) 10?20 WordsTable 1: BLEU scores computed on the development and test sets.
The top three rows show the scores onall the sentences, and the bottom three rows on the sentences having no unknown words.
(?)
The resultreported in (Cho et al., 2014) where the RNNenc was used to score phrase pairs in the phrase table.
(?
)The result reported in (Sutskever et al., 2014) where an encoder?decoder with LSTM units was used tore-rank the n-best list generated by Moses.When we use the beam-search to find the k besttranslations, we do not use a usual log-probabilitybut one normalized with respect to the length ofthe translation.
This prevents the RNN decoderfrom favoring shorter translations, behavior whichwas observed earlier in, e.g., (Graves, 2013).5 Results and Analysis5.1 Quantitative AnalysisIn this paper, we are interested in the propertiesof the neural machine translation models.
Specif-ically, the translation quality with respect to thelength of source and/or target sentences and withrespect to the number of words unknown to themodel in each source/target sentence.First, we look at how the BLEU score, reflect-ing the translation performance, changes with re-spect to the length of the sentences (see Fig.
4 (a)?(b)).
Clearly, both models perform relatively wellon short sentences, but suffer significantly as thelength of the sentences increases.We observe a similar trend with the number ofunknown words, in Fig.
4 (c).
As expected, theperformance degrades rapidly as the number ofunknown words increases.
This suggests that itwill be an important challenge to increase the sizeof vocabularies used by the neural machine trans-lation system in the future.
Although we onlypresent the result with the RNNenc, we observedsimilar behavior for the grConv as well.In Table 1 (a), we present the translation perfor-mances obtained using the two models along withthe baseline phrase-based SMT system.3Clearlythe phrase-based SMT system still shows the su-perior performance over the proposed purely neu-ral machine translation system, but we can see thatunder certain conditions (no unknown words inboth source and reference sentences), the differ-ence diminishes quite significantly.
Furthermore,if we consider only short sentences (10?20 wordsper sentence), the difference further decreases (seeTable 1 (b).Furthermore, it is possible to use the neural ma-chine translation models together with the existingphrase-based system, which was found recently in(Cho et al., 2014; Sutskever et al., 2014) to im-prove the overall translation performance (see Ta-ble 1 (a)).This analysis suggests that that the current neu-ral translation approach has its weakness in han-dling long sentences.
The most obvious explana-tory hypothesis is that the fixed-length vector rep-resentation does not have enough capacity to en-code a long sentence with complicated structureand meaning.
In order to encode a variable-lengthsequence, a neural network may ?sacrifice?
someof the important topics in the input sentence in or-der to remember others.This is in stark contrast to the conventionalphrase-based machine translation system (Koehnet al., 2003).
As we can see from Fig.
5, theconventional system trained on the same dataset(with additional monolingual data for the languagemodel) tends to get a higher BLEU score on longer3We used Moses as a baseline, trained with additionalmonolingual data for a 4-gram language model.107Source She explained her new position of foreign affairs and security policy representative as a reply to aquestion: ?Who is the European Union?
Which phone number should I call??
; i.e.
as an important stepto unification and better clarity of Union?s policy towards countries such as China or India.Reference Elle a expliqu?e le nouveau poste de la Haute repr?esentante pour les affaires ?etrang`eres et la politique ded?efense dans le cadre d?une r?eponse `a la question: ?Qui est qui `a l?Union europ?eenne??
?A quel num?erode t?el?ephone dois-je appeler?
?, donc comme un pas important vers l?unicit?e et une plus grande lisibilit?ede la politique de l?Union face aux ?etats, comme est la Chine ou bien l?Inde.RNNEnc Elle a d?ecrit sa position en mati`ere de politique ?etrang`ere et de s?ecurit?e ainsi que la politique de l?Unioneurop?eenne en mati`ere de gouvernance et de d?emocratie .grConv Elle a expliqu?e sa nouvelle politique ?etrang`ere et de s?ecurit?e en r?eponse `a un certain nombre de questions: ?Qu?est-ce que l?Union europ?eenne ?
?
.Moses Elle a expliqu?e son nouveau poste des affaires ?etrang`eres et la politique de s?ecurit?e repr?esentant enr?eponse `a une question: ?Qui est l?Union europ?eenne?
Quel num?ero de t?el?ephone dois-je appeler??
;c?est comme une ?etape importante de l?unification et une meilleure lisibilit?e de la politique de l?Union`a des pays comme la Chine ou l?Inde .Source The investigation should be complete by the end of the year when the findings will be presented toDeutsche Bank?s board of managing directors - with recommendations for action.Reference L?examen doit ?etre termin?e d?ici la fin de l?ann?ee, ensuite les r?esultats du conseil d?administration de laDeutsche Bank doivent ?etre pr?esent?es - avec recommandation, d?
habitude.RNNEnc L?
?etude devrait ?etre termin?ee `a la fin de l?
ann?ee, lorsque les conclusions seront pr?esent?ees au conseild?administration de la Deutsche Bank, conseil d?association avec des mesures.grConv L?enqu?ete devrait ?etre termin?ee `a la fin de l?ann?ee o`u les conclusions seront pr?esent?ees par le conseild?administration de la BCE `a la direction des recommandations.Moses L?enqu?ete devrait ?etre termin?e d?ici la fin de l?ann?ee lorsque les r?esultats seront pr?esent?es `a la DeutscheBank conseil des directeurs g?en?eraux - avec des recommandations .Source And there are thorny mechanical questions that must be resolved during that time, like how to bal-ance the state?s mandate of ?adequate access?
to licensed marijuana with its prohibitions on cannabisbusinesses within 1,000 feet of a school, park, playground or child care center.Reference Pendant ce temps, des questions pratiques restent en suspens: comment ?equilibrer le mandat de l?
?etatqui garantit un acc`es appropri?e `a la marijuana agr?e?ee et interdit l?installation de commerces de vente decannabis dans un rayon de 30 km autour d?une ?ecole, d?un parc, d?un terrain de jeu ou d?une cr`eche.RNNEnc Il y a des questions pr?eventives qui se posent quant `a l?
?equilibre des droits de l?enfant dans les limitesd?une entreprise de collecte de sang.grConv De fac?on g?en?erale, il y a des raisons de s?ecurit?e pour que les entreprises aient acc`es `a des milliers decentres de p?eche, d?eau ou de recherche.Moses Et il y a des probl`emes m?ecaniques complexes qui doivent ?etre r?esolues au cours de cette p?eriode,comme la mani`ere d?
?equilibrer le mandat de ?l?acc`es ad?equat?
permis de marijuana avec l?interdictiondu cannabis aux entreprises de 1000 pieds d?une ?ecole de jeu ou de parc, le service de garde.
(a) Long SentencesSource There is still no agreement as to which election rules to follow.Reference Toutefois il n?existe toujours pas d?accord selon quel r`eglement de vote il faut proc?eder.RNNEnc Il n?y a pas encore d?accord sur les r`egles ?electorales.grConv Il n?y a pas encore d?accord sur la question des ?elections `a suivre.Moses Il y a toujours pas d?accord sur l?
?election des r`egles `a suivre.Source Many of these ideas may have been creative, but they didn?t necessarily work.Reference Beaucoup de ces id?ees ?etaient cr?eatives mais elles n?ont pas forc?ement fonctionn?e.RNNEnc Bon nombre de ces id?ees ont peut-?etre ?et?e cr?eatrices, mais elles ne s?appliquaient pas n?ecessairement.grConv Beaucoup de ces id?ees peuvent ?etre cr?eatives, mais elles n?ont pas fonctionn?e.Moses Beaucoup de ces id?ees ont pu ?etre cr?eatif, mais ils n?ont pas n?ecessairement.Source There is a lot of consensus between the Left and the Right on this subject.Reference C?est qu?il y a sur ce sujet un assez large consensus entre gauche et droite.RNNEnc Il existe beaucoup de consensus entre la gauche et le droit `a la question.grConv Il y a un consensus entre la gauche et le droit sur cette question.Moses Il y a beaucoup de consensus entre la gauche et la droite sur ce sujet.Source According to them, one can find any weapon at a low price right now.Reference Selon eux, on peut trouver aujourd?hui `a Moscou n?importe quelle arme pour un prix raisonnable.RNNEnc Selon eux, on peut se trouver de l?arme `a un prix trop bas.grConv En tout cas, ils peuvent trouver une arme `a un prix tr`es bas `a la fois.Moses Selon eux, on trouve une arme `a bas prix pour l?instant.
(b) Short SentencesTable 2: The sample translations along with the source sentences and the reference translations.1080 10 20 30 40 50 60 70 80Sentence length05101520BLEUscoreSource textReference textBoth(a) RNNenc0 10 20 30 40 50 60 70 80Sentence length05101520BLEUscoreSource textReference textBoth(b) grConv0 2 4 6 8 10Max.
number of unknown words1012141618202224BLEUscoreSource textReference textBoth(c) RNNencFigure 4: The BLEU scores achieved by (a) the RNNenc and (b) the grConv for sentences of a givenlength.
The plot is smoothed by taking a window of size 10.
(c) The BLEU scores achieved by the RNNmodel for sentences with less than a given number of unknown words.sentences.In fact, if we limit the lengths of both the sourcesentence and the reference translation to be be-tween 10 and 20 words and use only the sentenceswith no unknown words, the BLEU scores on thetest set are 27.81 and 33.08 for the RNNenc andMoses, respectively.Note that we observed a similar trend evenwhen we used sentences of up to 50 words to trainthese models.5.2 Qualitative AnalysisAlthough BLEU score is used as a de-facto stan-dard metric for evaluating the performance of amachine translation system, it is not the perfectmetric (see, e.g., (Song et al., 2013; Liu et al.,2011)).
Hence, here we present some of the ac-tual translations generated from the two models,RNNenc and grConv.In Table.
2 (a)?
(b), we show the translations ofsome randomly selected sentences from the de-velopment and test sets.
We chose the ones thathave no unknown words.
(a) lists long sentences(longer than 30 words), and (b) short sentences(shorter than 10 words).
We can see that, despitethe difference in the BLEU scores, all three mod-els (RNNenc, grConv and Moses) do a decent jobat translating, especially, short sentences.
Whenthe source sentences are long, however, we no-tice the performance degradation of the neural ma-chine translation models.Additionally, we present here what type ofstructure the proposed gated recursive convolu-tional network learns to represent.
With a samplesentence ?Obama is the President of the UnitedStates?, we present the parsing structure learnedby the grConv encoder and the generated transla-tions, in Fig.
6.
The figure suggests that the gr-0 10 20 30 40 50 60 70 80Sentence length0510152025303540BLEUscoreSource textReference textBothFigure 5: The BLEU scores achieved by an SMTsystem for sentences of a given length.
The plotis smoothed by taking a window of size 10.
Weuse the solid, dotted and dashed lines to show theeffect of different lengths of source, reference orboth of them, respectively.Conv extracts the vector representation of the sen-tence by first merging ?of the United States?
to-gether with ?is the President of?
and finally com-bining this with ?Obama is?
and ?.
?, which iswell correlated with our intuition.
Note, however,that the structure learned by the grConv is differ-ent from existing parsing approaches in the sensethat it returns soft parsing.Despite the lower performance the grConvshowed compared to the RNN Encoder?Decoder,4we find this property of the grConv learning agrammar structure automatically interesting andbelieve further investigation is needed.4However, it should be noted that the number of gradientupdates used to train the grConv was a third of that used totrain the RNNenc.
Longer training may change the result,but for a fair comparison we chose to compare models whichwere trained for an equal amount of time.
Neither model wastrained to convergence.109Obama is the President of the United States .++++++++++++++++++++++++++++++++++++ TranslationsObama est le Pr?esident des?Etats-Unis .
(2.06)Obama est le pr?esident des?Etats-Unis .
(2.09)Obama est le pr?esident des Etats-Unis .
(2.61)Obama est le Pr?esident des Etats-Unis .
(3.33)Barack Obama est le pr?esident des?Etats-Unis .
(4.41)Barack Obama est le Pr?esident des?Etats-Unis .
(4.48)Barack Obama est le pr?esident des Etats-Unis .
(4.54)L?Obama est le Pr?esident des?Etats-Unis .
(4.59)L?Obama est le pr?esident des?Etats-Unis .
(4.67)Obama est pr?esident du Congr`es des?Etats-Unis .
(5.09)(a) (b)Figure 6: (a) The visualization of the grConv structure when the input is ?Obama is the President ofthe United States.?.
Only edges with gating coefficient ?
higher than 0.1 are shown.
(b) The top-10translations generated by the grConv.
The numbers in parentheses are the negative log-probability.6 Conclusion and DiscussionIn this paper, we have investigated the propertyof a recently introduced family of machine trans-lation system based purely on neural networks.We focused on evaluating an encoder?decoder ap-proach, proposed recently in (Kalchbrenner andBlunsom, 2013; Cho et al., 2014; Sutskever et al.,2014), on the task of sentence-to-sentence trans-lation.
Among many possible encoder?decodermodels we specifically chose two models that dif-fer in the choice of the encoder; (1) RNN withgated hidden units and (2) the newly proposedgated recursive convolutional neural network.After training those two models on pairs ofEnglish and French sentences, we analyzed theirperformance using BLEU scores with respect tothe lengths of sentences and the existence of un-known/rare words in sentences.
Our analysis re-vealed that the performance of the neural machinetranslation suffers significantly from the length ofsentences.
However, qualitatively, we found thatthe both models are able to generate correct trans-lations very well.These analyses suggest a number of future re-search directions in machine translation purelybased on neural networks.Firstly, it is important to find a way to scale uptraining a neural network both in terms of com-putation and memory so that much larger vocabu-laries for both source and target languages can beused.
Especially, when it comes to languages withrich morphology, we may be required to come upwith a radically different approach in dealing withwords.Secondly, more research is needed to preventthe neural machine translation system from under-performing with long sentences.
Lastly, we needto explore different neural architectures, especiallyfor the decoder.
Despite the radical difference inthe architecture between RNN and grConv whichwere used as an encoder, both models suffer fromthe curse of sentence length.
This suggests that itmay be due to the lack of representational powerin the decoder.
Further investigation and researchare required.In addition to the property of a general neuralmachine translation system, we observed one in-teresting property of the proposed gated recursiveconvolutional neural network (grConv).
The gr-Conv was found to mimic the grammatical struc-ture of an input sentence without any supervisionon syntactic structure of language.
We believe thisproperty makes it appropriate for natural languageprocessing applications other than machine trans-lation.AcknowledgmentsThe authors would like to acknowledge the sup-port of the following agencies for research fundingand computing support: NSERC, Calcul Qu?ebec,Compute Canada, the Canada Research Chairsand CIFAR.110ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of the ACL Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 355?362.
Association for Compu-tational Linguistics.Nicolas Boulanger-Lewandowski, Yoshua Bengio, andPascal Vincent.
2013.
Audio chord recognition withrecurrent neural networks.
In ISMIR.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014.
Learning phrase representationsusing rnn encoder-decoder for statistical machinetranslation.
In Proceedings of the EmpiricialMethods in Natural Language Processing (EMNLP2014), October.
to appear.Alex Graves.
2012.
Sequence transduction with re-current neural networks.
In Proceedings of the29th International Conference on Machine Learning(ICML 2012).A.
Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv:1308.0850 [cs.NE],August.S.
Hochreiter and J. Schmidhuber.
1997.
Long short-term memory.
Neural Computation, 9(8):1735?1780.Nal Kalchbrenner and Phil Blunsom.
2013.
Two re-current continuous translation models.
In Proceed-ings of the ACL Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages1700?1709.
Association for Computational Linguis-tics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,USA.
Association for Computational Linguistics.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.2011.
Better evaluation metrics lead to better ma-chine translation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 375?384.
Association for Computa-tional Linguistics.Xingyi Song, Trevor Cohn, and Lucia Specia.
2013.BLEU deconstructed: Designing a better MT eval-uation metric.
In Proceedings of the 14th Interna-tional Conference on Intelligent Text Processing andComputational Linguistics (CICLING), March.Ilya Sutskever, Oriol Vinyals, and Quoc Le.
2014.Anonymized.
In Anonymized.Matthew D. Zeiler.
2012.
ADADELTA: an adap-tive learning rate method.
Technical report, arXiv1212.5701.111
