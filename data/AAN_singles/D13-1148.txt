Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1433?1437,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsNaive Bayes Word Sense InductionDo Kook ChoeBrown UniversityProvidence, RIdc65@cs.brown.eduEugene CharniakBrown UniversityProvidence, RIec@cs.brown.eduAbstractWe introduce an extended naive Bayes modelfor word sense induction (WSI) and apply it toa WSI task.
The extended model incorporatesthe idea the words closer to the target word aremore relevant in predicting its sense.
The pro-posed model is very simple yet effective whenevaluated on SemEval-2010 WSI data.1 IntroductionThe task of word sense induction (WSI) is to findclusters of tokens of an ambiguous word in an un-labeled corpus that have the same sense.
For in-stance, given a target word ?crane,?
a good WSI sys-tem should find a cluster of tokens referring to aviancranes and another referring to mechanical cranes.We believe that neighboring words contain enoughinformation that these clusters can be found fromplain texts.WSI is related to word sense disambiguation(WSD).
In a WSD task, a system learns a sense clas-sifier in a supervised manner from a sense-labeledcorpus.
The performance of the learned classifieris measured on some unseen data.
WSD systemsperform better than WSI systems, but building la-beled data can be prohibitively expensive.
In addi-tion, WSD systems are not suitable for newly cre-ated words, new senses of existing words, or domain-specific words.
On the other hand, WSI systems canlearn new senses of words directly from texts becausethese programs do not rely on a predefined set ofsenses.In Section 2 we describe relevant previous work.
InSection 3 and 4 we introduce the naive Bayes modelfor WSI and inference schemes for the model.
In Sec-tion 5 we evaluate the model on SemEval-2010 data.In Section 6 we conclude.2 Related WorkYarowsky (1995) introduces a semi-supervisedbootstrapping algorithm with two assumptionsthat rivals supervised algorithms: one-sense-per-collocation and one-sense-per-discourse.
But thisalgorithm cannot easily be scaled up because forany new ambiguous word humans need to picka few seed words, which initialize the algorithm.In order to automate the semi-supervised system,Eisner and Karakos (2005) propose an unsupervisedbootstrapping algorithm.
Their system tries manydifferent seeds for bootstrapping and chooses the?best?
classifier at the end.
Eisner and Karakos?salgorithm is limited in that their system is designedfor disambiguating words that have only 2 senses.Bayesian WSI systems have been developed byseveral authors.
Brody and Lapata (2009) applyLatent Dirichlet Allocation (LDA) (Blei et al2003) to WSI.
They run a topic modeling algorithmon texts with some fixed number of topics thatcorrespond to senses and induce a cluster by findingtarget words assigned to the same topic.
Theirsystem is evaluated on SemEval-2007 noun data(Agirre and Soroa, 2007).
Lau et al(2012) applya nonparametric model, Hierarchical Dirichlet Pro-cesses (HDP), to SemEval-2010 data (Manandhar etal., 2010).3 ModelFollowing Yarowsky (1995), we assume that a wordin a document has one sense.
Multiple occurrencesof a word in a document refer to the same objector concept.
The naive Bayes model is well suitedfor this one-sense-per-document assumption.
Eachdocument has one topic corresponding to the sense ofthe target word that needs disambiguation.
Contextwords in a document are drawn from the conditionaldistribution of words given the sense.
Context wordsare assumed to be independent from each other given1433the sense, which is far from being true yet effective.3.1 Naive BayesThe naive Bayes model assumes that every word in adocument is generated independently from the con-ditional distribution of words given a sense, p(w|s).The mathematical definition of the naive Bayesmodel is as follows:p(w) =?sp(s,w) =?sp(s)p(w |s)=?sp(s)?wp(w|s), (1)where w is a vector of words in the document.
Withthe model, a new document can be easily labeledusing the following classifier:s?
= argmaxsp(s)?wp(w|s), (2)where s?
is the label of the new document.
In con-trast to LDA-like models, it is easy to constructthe closed form classifier from the model.
The pa-rameters of the model, p(s) and p(w|s), can belearned by maximizing the probability of the corpus,p(d) =?d p(d) =?w p(w) where d is a vector ofdocuments and d = w .3.2 Distance Incorporated Naive BayesIntuitively, context words near a target word aremore indicative of its sense than ones that are far-ther away.
To account for this intuition, we proposea more sophisticated model that uses the distancebetween a context word and a target word.
Beforeintroducing the new model, we define a probabilitydistribution, f(w|s), that incorporates distances asfollows:f(w|s) =p(w|s)l(w)?w?
?W p(w?|s)l(w), (3)where l(w) = 1dist(w)x .
W is a set of types in the cor-pus.
x is a tunable parameter that takes nonnegativereal values.
With the new probability distribution,the model and the classifier become:p(w) =?sp(s)?wf(w|s) (4)s?
= argmaxsp(s)?wf(w|s), (5)where f(w|s) replaces p(w|s).
The naive Bayesmodel is a special case; set x = 0.
The new modelputs more weight on context words that are closeto the target word.
The distribution of words thatare farther away approaches the uniform distribu-tion.
l(w) smoothes the distribution more as x be-comes larger.4 InferenceGiven the generative model, we employ two inferencealgorithms to learn the sense distribution and worddistributions given a sense.
Expectation Maximiza-tion (EM) is a natural choice for the naive Bayes(Dempster et al 1977).
When initialized with ran-dom parameters, EM gets stuck at local maxima.
Toavoid local maxima, we use a Gibbs sampler for theplain naive Bayes to learn parameters that initializeEM.5 Experiments5.1 DataWe evaluate the model on SemEval-2010 WSI taskdata (Manandhar et al 2010).
The task has 100target words, 50 nouns and 50 verbs.
For each targetword, there are training and test documents.
Table1 have details.
The training and test data are plaintexts without sense tags.
For evaluation, the inferredsense labels are compared with human annotations.To tune some parameters we use the trial data ofTraining Testing Senses (#)All 879807 8915 3.79Nouns 716945 5285 4.46Verbs 162862 3630 3.12Table 1: Details of SemEval-2010 dataSemEval-2010.
The trial data consists of trainingand test portions of 4 verbs.
On average there are137 documents for each target word in the trainingpart of the trial data.5.2 TaskParticipants induce clusters from the training dataand use them to label the test data.
Resources otherthan NLP tools for morphology and syntax such aslemmatizer, POS-tagger, and parser are not allowed.Tuning parameters and inducing clusters are onlyallowed during the training phase.
After training,participants submit their sense-labeled test data toorganizers.LDA models are not compatible with the scoringrules for the SemEval-2010 competition, and that isthe work against which we most want to compare.These rules require that training be done strictly be-fore the testing is done.
Note however that LDA re-quires learning the mixture weights of topics for each1434individual document p(topic | document).
These are,of course, learned during training.
But the docu-ments in the testing corpus have never been seenbefore, so clearly their topic mixture weights are notlearned during training, and thus not learned at all.The way to overcome this is by training on bothtrain and test documents, but this is exactly whatSemEval-2010 forbids.5.3 Implementation DetailsThe documents are tokenized and stemmed byStanford tokenizer and stemmer.
Stop words andpunctuation in the training and test data arediscarded.
Words that occur at most 10 times arediscarded from the training data.
Context wordswithin a window of 50 about a target word are usedto construct a bag-of-words.When a target word appears more than oncein a document, the distance between that targetword and a context word is ambiguous.
We definethis distance to be minimum distance between acontext word and an instance of the target word.For example, the word ?chip?
appears 3 times.
For?
?
?
of memory chips .
Currently , chips are pro-duced by shining light through a mask to producean image on the chip , much as ?
?
?Example 1: an excerpt from ?chip?
test dataa context word, e.g., ?shining?
there are three pos-sible distances: 8 away from the first ?chip,?
4 awayfrom the second ?chip?
and 11 away from the last?chip.?
We set the distance of ?shining?
from thetarget to 4.We model each target word individually.
We set ?,a Dirichlet prior for senses, to 0.02 and ?, a Dirichletprior for contextual words, to 0.1 for the Gibbs sam-pler as in Brody and Lapata (2009).
We initializeEM with parameters learned from the sampler.
Werun EM until the likehood changes less than 1%.
Werun the sampler 2000 iterations including 1000 itera-tions of burn-in: 10 samples at an interval of 100 areaveraged.
For comparison, we also evaluate EM withrandom initialization.
All reported scores (describedin Section 5.4) are averaged over ten different runsof the program.15.3.1 Tuning ParametersTwo parameters, the number of senses and x ofthe function l(w), need to be determined before run-ning the program.
To find a good setting we do gridsearch on the trial data with the number of senses1Code used for experiments is available for download athttp://cs.brown.edu/~dc65/.ranging from 2 to 5 and x ranging from 0 to 1.1 withan interval 0.1.
Due to the small size of the trainingportion of the trial data, words that occur once arethrown out in the training portion.
All the other pa-rameters are as described in Section 5.3.
We choose(4, 0.4), which achieves the highest supervised recall.See Table 2 for the performance of the model withvarious parameter settings.
With a fixed value of x,a column is nearly unimodal in the number of sensesand vice versa.
x = 0 is not optimal and there issome noticeable difference between scores with opti-mal x and scores with x = 0.5.4 EvaluationWe compare our system to other WSI systems anddiscuss two metrics for unsupervised evaluation (V-Measure, paired F-Score) and one metric for super-vised evaluation (supervised recall).
We refer to thetrue group of tokens as a gold class and to an inducedgroup of tokens as a cluster.
We refer to the modellearned with the sampler and EM as NB, and to themodel learned with EM only as NB0.5.4.1 Short Descriptions of Other WSISystems Evaluated on SemEval-2010The baseline assigns every instance of a targetword with the most frequent sense (MFS).
UoY runsa clustering algorithm on a graph with words asnodes and co-occurrences between words as edges(Korkontzelos and Manandhar, 2010).
Hermit ap-proximates co-occurrence space with Random Index-ing and applies a hybrid of k-means and HierarchicalAgglomerate Clustering to co-occurrence space (Ju-rgens and Stevens, 2010).
NMFlib factors a matrixusing nonnegative matrix factorization and runs aclustering algorithm on test instances represented byfactors (Van de Cruys et al 2011).5.4.2 V-MeasureV-Measure computes the quality of induced clus-ters as the harmonic mean of two values, homo-geneity and completeness.
Homogeneity measureswhether instances of a cluster belong to a single goldclass.
Completeness measures whether instances of agold class belong to a cluster.
V-Measure is between0 and 1; higher is better.
See Table 3 for details ofV-Measure evaluation (#cl is the number of inducedclusters).With respect to V-Measure, NB performs muchbetter than NB0.
This holds for paired F-Score andsupervised recall evaluations.
The sampler improvesthe log-likelihood of NB by 3.8% on average (4.8%on nouns and 2.9% on verbs).Pedersen (2010) points out that it is possible toincrease the V-Measure of bad models by increasing1435#s \ x 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.12 74.73 74.76 74.41 74.57 74.06 74.07 74.18 74.33 74.14 74.22 74.15 74.523 74.60 74.71 75.21 75.46 75.21 75.57 75.61 75.32 75.53 75.56 74.98 74.794 74.52 75.06 74.97 75.14 76.02 75.51 75.74 75.51 75.59 75.51 75.37 75.355 73.40 73.88 74.93 75.13 74.79 74.68 74.71 74.49 75.11 74.94 74.86 75.25Table 2: Performance of the model with various parameters: supervised recall on the trial data.
The best value fromeach row is bold-faced.
The scores are averaged over 100 runs.VM(%) all nouns verbs #clNB 18.0 23.7 9.9 3.42NB0 14.9 19.0 9.0 3.77Hermit 16.2 16.7 15.6 10.78UoY 15.7 20.6 8.5 11.54NMFlib 11.8 13.5 9.4 4.80MFS 0.0 0.0 0.0 1.00Table 3: Unsupervised evaluation: V-Measurethe number of clusters.
But increasing the numberof clusters harms paired F-Score, which results inbad supervised recalls.
NB attains a very high V-Measure with few induced clusters, which indicatesthat those clusters are high quality.
Other systemsuse more induced clusters but fail to attain the V-Measure of NB.5.4.3 Paired F-ScorePaired F-Score is the harmonic mean of paired re-call and paired precision.
Paired recall is fraction ofpairs belonging to the same gold class that belongto the same cluster.
Paired precision is fraction ofpairs belonging to the same cluster that belong tothe same class.
See Table 4 for details of paired F-Score evaluation.As with V-Measure, it is possible to attain a highpaired F-Score by producing only one cluster.
Thebaseline, MFS, attains 100% paired recall, which to-gether with the poor performance of WSI systemsmakes its paired F-Score difficult to beat.
V-Measureand paired F-Score are meaningful when systemsproduce about the same numbers of clusters as thenumbers of classes and attain high scores on thesemetrics.FS(%) all nouns verbs #clMFS 63.5 57.0 72.7 1.00NB 52.9 52.5 53.5 3.42NB0 46.8 47.4 46.0 3.77UoY 49.8 38.2 66.6 11.54NMFlib 45.3 42.2 49.8 4.80Hermit 26.7 24.4 30.1 10.78Table 4: Unsupervised evaluation: paired F-Score5.4.4 Supervised RecallFor the supervised task, the test data is split intotwo groups: one for mapping clusters to classes andthe other for standard WSD evaluation.
2 differ-ent split schemes (80% mapping, 20% evaluation and60% mapping, 40% evaluation) are evaluated.
5 ran-dom splits are averaged for each split scheme.
Map-ping is induced automatically by the program pro-vided by organizers.
See Table 5 for details of super-vised recall evaluation (#s is the average number ofclasses mapped from clusters).2SR(%) all nouns verbs #sNB 65.4 62.6 69.5 1.72NB0 63.5 59.8 69.0 1.76NMFlib 62.6 57.3 70.2 1.82UoY 62.4 59.4 66.8 1.51MFS 58.7 53.2 66.6 1.00Hermit 58.3 53.6 65.3 2.06Table 5: Supervised evaluation: supervised recall, 80%mapping and 20% evaluationOverall our system performs better than other sys-tems with respect to supervised recall.
When a sys-tem has higher V-Measure and paired F-Score onnouns than another system, it achieves a higher su-pervised recall on nouns too.
However, this behav-ior is not observed on verbs.
For example, NB hashigher V-Measure and paired F-Score on verbs thanNMFlib but NB attains a lower supervised recall onverbs than NMFlib.
It is difficult to see which verbsclusters are better than some other clusters.6 ConclusionOf the four SemEval-2010 evaluation metrics, andrestricting ourselves to systems obeying the evalua-tion conditions for that competition, our new modelachieves new best results on three.
The exception ispaired F-Score.
As we note earlier, this metric tendsto assign very high scores when every word receivesonly one sense, and our model is bested by the base-line system that does exactly that.260-40 split is omitted here due to almost identical result.1436If we loosen possible comparison systems, theLDA/HDP model of Lau et al(2012) achieves supe-rior numbers to ours for the two supervised metrics,but at the expense of requiring LDA type processingon the test data, something that the SemEval or-ganizers ruled out, presumably with the reasonableidea that such processing would not be feasible inthe real world.
More generally, their system assignsmany senses (about 10) to each word, and thus no-doubt does poorly on the paired F-Score (they do notreport results on V-Measure and paired F-Score).ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007 task02: Evaluating word sense induction and discrimina-tion systems.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations, pages 7?12.
Asso-ciation for Computational Linguistics.David M Blei, Andrew Y Ng, and Michael I Jordan.
2003.Latent dirichlet alcation.
the Journal of machineLearning research, 3:993?1022.Samuel Brody and Mirella Lapata.
2009.
Bayesian wordsense induction.
In Proceedings of the 12th Confer-ence of the European Chapter of the Association forComputational Linguistics, EACL ?09, pages 103?111,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Arthur P Dempster, Nan M Laird, and Donald B Ru-bin.
1977.
Maximum likelihood from incomplete datavia the em algorithm.
Journal of the Royal StatisticalSociety.
Series B (Methodological), pages 1?38.Jason Eisner and Damianos Karakos.
2005.
Bootstrap-ping without the boot.
In Proceedings of the conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing, HLT ?05, pages395?402, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.David Jurgens and Keith Stevens.
2010.
Hermit: Flex-ible clustering for the semeval-2 wsi task.
In Pro-ceedings of the 5th international workshop on semanticevaluation, pages 359?362.
Association for Computa-tional Linguistics.Ioannis Korkontzelos and Suresh Manandhar.
2010.
Uoy:Graphs of unambiguous vertices for word sense induc-tion and disambiguation.
In Proceedings of the 5thinternational workshop on semantic evaluation, pages355?358.
Association for Computational Linguistics.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012.
Word sense induc-tion for novel sense detection.
In Proceedings of the13th Conference of the European Chapter of the Asso-ciation for Computational Linguistics, pages 591?601.Association for Computational Linguistics.Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dli-gach, and Sameer S Pradhan.
2010.
Semeval-2010task 14: Word sense induction & disambiguation.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, pages 63?68.
Association for Com-putational Linguistics.Ted Pedersen.
2010.
Duluth-wsi: Senseclusters appliedto the sense induction task of semeval-2.
In Proceed-ings of the 5th international workshop on semanticevaluation, pages 363?366.
Association for Computa-tional Linguistics.Tim Van de Cruys, Marianna Apidianaki, et al2011.Latent semantic word sense induction and disambigua-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies (ACL/HLT), pages 1476?1485.David Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceedingsof the 33rd annual meeting on Association for Compu-tational Linguistics, ACL ?95, pages 189?196, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.1437
