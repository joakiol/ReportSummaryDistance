Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 129?132,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005PORTAGE: A Phrase-based Machine Translation SystemFatiha Sadat+, Howard Johnson++, Akakpo Agbago+, George Foster+,Roland Kuhn+, Joel Martin++ and Aaron Tikuisis?+ NRC Institute for InformationTechnology101 St-Jean-Bosco StreetGatineau, QC K1A 0R6, Canada++ NRC Institute for InformationTechnology1200 Montreal RoadOttawa, ON K1A 0R6, Canada?University of Waterloo200 University Avenue W.,Waterloo, Ontario, Canadafirstname.lastname@cnrc-nrc.gc.ca aptikuis@uwaterloo.caAbstractThis paper describes the participation ofthe Portage team at NRC Canada in theshared task1 of ACL 2005 Workshop onBuilding and Using Parallel Texts.
We dis-cuss Portage, a statistical phrase-basedmachine translation system, and presentexperimental results on the four languagepairs of the shared task.
First, we focus onthe French-English task using multiple re-sources and techniques.
Then we describeour contribution on the Finnish-English,Spanish-English and German-English lan-guage pairs using the provided data for theshared task.1 IntroductionThe rapid growth of the Internet has led to a rapidgrowth in the need for information exchange amongdifferent languages.
Machine Translation (MT) andrelated technologies have become essential to theinformation flow between speakers of different lan-guages on the Internet.
Statistical Machine Transla-tion (SMT), a data-driven approach to producingtranslation systems, is becoming a practical solutionto the longstanding goal of cheap natural languageprocessing.In this paper, we describe Portage, a statisticalphrase-based machine translation system, which weevaluated on all different language pairs that wereprovided for the shared task.
As Portage is a very1 http://www.statmt.org/wpt05/mt-shared-task/new system, our main goal in participating in theworkshop was to test it out on different languagepairs, and to establish baseline performance for thepurpose of comparison against other systems andagainst future improvements.
To do this, we used afairly standard configuration for phrase-based SMT,described in the next section.Of the language pairs in the shared task, French-English is particularly interesting to us in light ofCanada?s demographics and policy of official bilin-gualism.
We therefore divided our participation intotwo parts: one stream for French-English and an-other for Finnish-, German-, and Spanish-English.For the French-English stream, we tested the use ofadditional data resources along with hand-codedrules for translating numbers and dates.
For theother streams, we used only the provided resourcesin a purely statistical framework (although we alsoinvestigated several automatic methods of copingwith Finnish morphology).The remainder of the paper is organized as fol-lows.
Section 2 describes the architecture of thePortage system, including its hand-coded rules forFrench-English.
Experimental results for the fourpairs of languages are reported in Section 3.
Section4 concludes and gives pointers to future work.2 PortagePortage operates in three main phases: preprocess-ing of raw data into tokens, with translation sugges-tions for some words or phrases generated by rules;decoding to produce one or more translation hy-potheses; and error-driven rescoring to choose thebest final hypothesis.
(A fourth postprocessingphase was not needed for the shared task.
)1292.1 PreprocessingPreprocessing is a necessary first step in order toconvert raw texts in both source and target lan-guages into a format suitable for both model train-ing and decoding (Foster et al, 2003).
For thesupplied Europarl corpora, we relied on the existingsegmentation and tokenization, except for French,which we manipulated slightly to bring into linewith our existing conventions (e.g., converting l ?an  into l?
an).
For the Hansard corpus used tosupplement our French-English resources (de-scribed in section 3 below), we used our ownalignment based on Moore?s algorithm (Moore,2002), segmentation, and tokenization procedures.Languages with rich morphology are often prob-lematic for statistical machine translation becausethe available data lacks instances of all possibleforms of a word to efficiently train a translation sys-tem.
In a language like German, new words can beformed by compounding (writing two or morewords together without a space or a hyphen in be-tween).
Segmentation is a crucial step in preproc-essing languages such as German and Finnish texts.In addition to these simple operations, we alsodeveloped a rule-based component to detect num-bers and dates in the source text and identify theirtranslation in the target text.
This component wasdeveloped on the Hansard corpus, and applied to theFrench-English texts (i.e.
Europarl and Hansard), onthe development data in both languages, and on thetest data.2.2 DecodingDecoding is the central phase in SMT, involving asearch for the hypotheses t that have highest prob-abilities of being translations of the current sourcesentence s according to a model for P(t|s).
Ourmodel for P(t|s) is a log-linear combination of fourmain components: one or more trigram languagemodels, one or more phrase translation models, adistortion model, and a word-length feature.
Thetrigram language model is implemented in theSRILM toolkit (Stolcke, 2002).
The phrase-basedtranslation model is similar to the one described in(Koehn, 2004), and relies on symmetrized IBMmodel 2 word-alignments for phrase pair induction.The distortion model is also very similar toKoehn?s, with the exception of a final cost to ac-count for sentence endings.sTo set weights on the components of the log-linear model, we implemented Och?s algorithm(Och, 2003).
This essentially involves generating,in an iterative process, a set of nbest translation hy-potheses that are representative of the entire searchspace for a given set of source sentences.
Once thisis accomplished, a variant of Powell?s algorithm isused to find weights that optimize BLEU score(Papineni et al 2002) over these hypotheses, com-pared to reference translations.
Unfortunately, ourimplementation of this algorithm converged onlyvery slowly to a satisfactory final nbest list, so weused two different ad hoc strategies for settingweights: choosing the best values encountered dur-ing, with the exception of ach as the ability to decode eitherw ards.transla-rent language pairs of thesha d thared t-the iterations of Och?s algorithm (French-English), and a grid search (all other languages).To perform the actual translation, we used ourdecoder, Canoe, which implements a dynamic-programming beam search algorithm based on thatof Pharaoh (Koehn, 2004).
Canoe is input-outputcompatible with Pharaohfew extensions suback ards or forw2.3 RescoringTo improve raw output from Canoe, we used arescoring strategy: have Canoe generate a list ofnbest translations rather than just one, then reorderthe list using a model trained with Och?s method tooptimize BLEU score.
This is identical to the finalpass of the algorithm described in the previous sec-tion, except for the use of a more powerful log-linear model than would have been feasible to useinside the decoder.
In addition to the four basic fea-tures of the initial model, our rescoring model in-cluded IBM2 model probabilities in both directions(i.e., P(s|t) and P(t|s)); and an IBM1-based featuredesigned to detect whether any words in one lan-guage seemed to be left without satisfactorytions in the other language.
This missing-wordfeature was also applied in both directions.3 Experiments on the Shared TaskWe conducted experiments and evaluations onPortage using the differe ask.
The training data was provided for theask as follows:Training data of 688,031 sentences inFrench and English.
A similarly sized cor-130pus is provided for Finnish, Spanish andGerman with matched English translations.orpus was used to generate bothlane translations into English, wasPortage for a comparative study ex-ploiting and combining different resources andtec3.
arl corpus4.rd corpora as training data andt  mod estp icipation at th h-English tas 9.53.od D  Decoding+Rescoring- Development test data of 2,000 sentences inthe four languages.In addition to the provided data, a set of6,056,014 sentences extracted from Hansard corpus,the official record of Canada?s parliamentary de-bates, was used in both French and English lan-guages.
This cguage and translation models for use in decodingand rescoring.The development test data was split into twoparts: The first part that includes 1,000 sentences ineach language with reference translations into Eng-lish served in the optimization of weights for boththe decoding and rescoring models.
In this study,number of n-best lists was set to 1,000.
The secondpart, which includes 1,000 sentences in each lan-guage with referencused in the evaluation of the performance of thetranslation models.3.1 Experiments on the French-English TaskOur goal for this language pair was to conduct ex-periments onhniques:1.
Method E is based on the Europarl corpusas training data,2.
Method E-H is based on both Europarl andHansard corpora as training data,Method E-p is based on the Europas training data and parsing numbers anddates in the preprocessing phase,Method E-H-p is based on both Europarland Hansaparsing numbers and date in the preprocess-ing phase.Results are shown in Table 1 for the French-English task.
The first column of Table 1 indicatesthe method, the second column gives results fordecoding with Canoe only, and the third column fordecoding and rescoring with Canoe.
For comparisonbetween the four methods, there was an improve-ment in terms of BLEU scores when using two lan-guage models and two translation models generatedfrom Europarl and Hansard corpora; however, pars-ing numbers and dates had a negative impact on theranslation els.
The b  BLEU score for ourart e Frenc k was 2Meth ecodingE 27.71 29.22E-H 28.71 29.53E-p 26.45 28.21E-H-p 28.29 28.56Taedfof increased trade within Northmerica but also functions as a good counterpointfor French-English.ble 1.
BLEU scores for the French-English testsentencesA noteworthy feature of these results is that theimprovement given by the out-of-domain Hansardcorpus was very slight.
Although we suspect thatsomewhat better performance could have beenachieved by better weight optimization, this resultclearly underscores the importance of matchingtraining and test domains.
A related point is that ournumber and date translation rules actually caused aperformance drop due to the fact that they were op-timized for typographical conventions prevalent inHansard, which are quite different from those usedin Europarl.Our best result ranked third in the sharedWPT05 French-English task , with a difference of0.74 in terms of BLEU score from the first rankparticipant, and a difference of 0.67 in terms oBLEU score from the second ranked participant.3.2 Experiments on other Pairs of LanguagesThe WPT05 workshop provides a good opportunityto achieve our benchmarking goals with corporathat provide challenging difficulties.
German andFinnish are languages that make considerable use ofcompounding.
Finnish, in addition, has a particu-larly complex morphology that is organized onprinciples that are quite different from any in Eng-lish.
This results in much longer word forms each ofwhich occurs very infrequently.Our original intent was to propose a number of pos-sible statistical approaches to analyzing and split-ting these word forms and improving our results.Since none of these yielded results as good as thebaseline, we will continue this work until we under-stand what is really needed.
We also care verymuch about translating between French and Englishin Canada and plan to spend a lot of extra effort ondifficulties that occur in this case.
Translation be-tween Spanish and English is also becoming moremportant as a result iA131Language Pair Decoding+RescoringFinnish-English 20.95German-English 23.21Spanish English 29.08Taand 1.56 inm ores, respectively, compared tothe first ranked participant.lation, greater use of morphologicalRFrMeeting of the Association for ComputationalFrStatistical Machine Transla-GeidKeeKial Meeting of the Association for Com-Mne Trans-Ocof the 40th Annual Meet-FrProceedings ofPh parl: A multilingual corpusforPationModels.
In Proceedings of the Association for Ma-chine Translation in the Americas AMTA 2004.ble 2 BLEU scores for the Finnish-English, Ger-man-English and Spanish-English test sentencesTo establish our baseline, the only preprocessingwe did was lowercasing (using the provided tokeni-zation).
Canoe was run without any special settings,although weights for distortion, word penalty, lan-guage model, and translation model were optimizedusing a grid search, as described above.
Rescoringwas also done, and usually resulted in at least anextra BLEU point.Our final results are shown in Table 2.
Ranks atthe shared WPT05 Finnish-, German-, and Spanish-English tasks were assigned as second, third andfourth, with differences of 1.06, 1.87ter s of BLEU sc4 ConclusionWe have reported on our participation in the sharedtask of the ACL 2005 Workshop on Building andUsing Parallel Texts, conducting evaluations ofPortage, our statistical machine translation system,on all four language pairs.
Our best BLEU scoresfor the French-, Finnish-, German-, and Spanish-English at this stage were 29.5, 20.95, 23.21 and29.08, respectively.
In total, eleven teams took partat the shared task and most of them submitted re-sults for all pairs of languages.
Our results distin-guished the NRC team at the third, second, thirdand fourth ranks with slight differences with thefirst ranked participants.A major goal of this work was to evaluate Port-age at its first stage of implementation on differentpairs of languages.
This evaluation has served toidentify some problems with our system in the areasof weight optimization and number and date rules.It has also indicated the limits of using out-of-domain corpora, and the difficulty of morphologi-cally complex languages like Finnish.Current and planned future work includes theexploitation of comparable corpora for statisticamachine translknowledge, and better features for nbest rescoring.eferencesAndreas Stolcke.
2002.
SRILM - an Extensible LanguageModeling Toolkit.
In ICSLP-2002, 901-904.anz Josef Och, Hermann Ney.
2000.
Improved Statisti-cal Alignment Models.
In Proceedings of the 38th An-nualLinguistics, Hong Kong, China, October 2000, 440-447.anz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, Dragomir Radev.
2004.
A Smor-gasbord of Features fortion.
In Proceeding of the HLT/NAACL 2004,Boston, MA, May 2004.orge Foster, Simona Gandrabur, Philippe Langlais,Pierre Plamondon, Graham Russell and Michel Si-mard.
2003.
Statistical Machine Translation: RapDevelopment with Limited Resources.
In Proceedingsof MT Summit IX 2003, New Orleans, September.vin Knight, Ishwar Chander, Matthew Haines, Va-sileios Hatzivassiloglou, Eduard Hovy, Masayo Iida,Steve K. Luk, Richard Whitney, and Kenji Yamada.1995.
Filling Knowledge Gaps in a Broad-CoveragMT System.
In Proceedings of the International JointConference on Artificial Intelligence (IJCAI), 1995.shore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedings ofthe 40th Annuputational Linguistics ACL, Philadelphia, July 2002,pp.
311-318.oore, Robert.
2002.
Fast and Accurate SentenceAlignment of Bilingual Corpora.
In Machine Transla-tion: From Research to Real Users (Proceedings of the5th Conference of the Association for Machilation in the Americas, Tiburon, California), Springer-Verlag, Heidelberg, Germany, pp.
135-244.h, F. J. and H. Ney.
2002.
Discriminative Trainingand Maximum Entropy Models for Statistical MachineTranslation.
In Proceedingsing of the Association for Computational Linguistics,Philadelphia, pp.
295?302.anz Josef Och, 2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
Inthe 41st Annual Meeting of the Association for Com-putational Linguistics, Sapporo, July.ilipp Koehn.
2002.
Euroevaluation of machine translation.
Ms., University ofSouthern California.hilipp Koehn.
2004.
Pharaoh: a Beam Search Decoderfor Phrase-based Statistical Machine Transl132
