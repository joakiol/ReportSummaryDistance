Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 31?39,Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational LinguisticsLearning English Light Verb Constructions: Contextual or StatisticalYuancheng TuDepartment of LinguisticsUniversity of Illinoisytu@illinois.eduDan RothDepartment of Computer ScienceUniversity of Illinoisdanr@illinois.eduAbstractIn this paper, we investigate a supervised ma-chine learning framework for automaticallylearning of English Light Verb Constructions(LVCs).
Our system achieves an 86.3% accu-racy with a baseline (chance) performance of52.2% when trained with groups of either con-textual or statistical features.
In addition, wepresent an in-depth analysis of these contex-tual and statistical features and show that thesystem trained by these two types of cosmet-ically different features reaches similar per-formance empirically.
However, in the situa-tion where the surface structures of candidateLVCs are identical, the system trained withcontextual features which contain informationon surrounding words performs 16.7% better.In this study, we also construct a balancedbenchmark dataset with 2,162 sentences fromBNC for English LVCs.
And this data set ispublicly available and is also a useful com-putational resource for research on MWEs ingeneral.1 IntroductionMulti-Word Expressions (MWEs) refer to varioustypes of linguistic units or expressions, includingidioms, noun compounds, named entities, complexverb phrases and any other habitual collocations.MWEs pose a particular challenge in empirical Nat-ural Language Processing (NLP) because they al-ways have idiosyncratic interpretations which can-not be formulated by directly aggregating the se-mantics of their constituents (Sag et al, 2002).The study in this paper focuses on one specialtype of MWEs, i.e., the Light Verb Constructions(LVCs), formed from a commonly used verb andusually a noun phrase (NP) in its direct object po-sition, such as have a look and make an offer inEnglish.
These complex verb predicates do not fallclearly into the discrete binary distinction of com-positional or non-compositional expressions.
In-stead, they stand somewhat in between and are typ-ically semi-compositional.
For example, considerthe following three candidate LVCs: take a wallet,take a walk and take a while.
These three complexverb predicates are cosmetically very similar.
Buta closer look at their semantics reveals significantdifferences and each of them represents a differentclass of MWEs.
The first expression, take a walletis a literal combination of a verb and its object noun.The last expression take a while is an idiom and itsmeaning cost a long time to do something, cannotbe derived by direct integration of the literal mean-ing of its components.
Only the second expression,take a walk is an LVC whose meaning mainly de-rives from one of its components, namely its nounobject (walk) while the meaning of its main verb issomewhat bleached (Butt, 2003; Kearns, 2002) andtherefore light (Jespersen, 1965).LVCs have already been identified as one of themajor sources of problems in various NLP applica-tions, such as automatic word alignment (Samardz?ic?and Merlo, 2010) and semantic annotation transfer-ence (Burchardt et al, 2009), and machine transla-tion.
These problems provide empirical grounds fordistinguishing between the bleached and full mean-ing of a verb within a given sentence, a task that isoften difficult on the basis of surface structures sincethey always exhibit identical surface properties.
Forexample, consider the following sentences:311.
He had a look of childish bewilderment on hisface.2.
I?ve arranged for you to have a look at his filein our library.In sentence 1, the verb have in the phrase have alook has its full fledged meaning ?possess, own?
andtherefore it is literal instead of light.
However, insentence 2, have a look only means look and themeaning of the verb have is impoverished and is thuslight.In this paper, we propose an in-depth case studyon LVC recognition, in which we investigate ma-chine learning techniques for automatically identi-fying the impoverished meaning of a verb given asentence.
Unlike the earlier work that has viewed allverbs as possible light verbs (Tan et al, 2006), Wefocus on a half dozen of broadly documented andmost frequently used English light verbs among thesmall set of them in English.We construct a token-based data set with a totalof 2, 162 sentences extracted from British NationalCorpus (BNC)1 and build a learner with L2-lossSVM.
Our system achieves a 86.3% accuracy witha baseline (chance) performance of 52.2%.
We alsoextract automatically two groups of features, statis-tical and contextual features and present a detailedablation analysis of the interaction of these features.Interestingly, the results show that the system per-forms similarly when trained independently with ei-ther groups of these features.
And the integrationof these two types of features does not improve theperformance.
However, when tested with all sen-tences with the candidate LVCs whose surface struc-tures are identical in both negative and positive ex-amples, for example, the aforementioned sentence 1(negative) and 2 (positive) with the candidate LVC?have a look?, the system trained with contextualfeatures which include information on surroundingwords performs more robust and significantly better.This analysis contributes significantly to the under-standing of the functionality of both contextual andstatistical features and provides empirical evidenceto guide the usage of them in NLP applications.In the rest of the paper, we first present some re-lated work on LVCs in Sec.
2.
Then we describe our1http://www.natcorp.ox.ac.uk/XMLedition/model including the learning algorithm and statisti-cal and contextual features in Sec.
3.
We present ourexperiments and analysis in Sec.
4 and conclude ourpaper in Sec.
5.2 Related WorkLVCs have been well-studied in linguistics sinceearly days (Jespersen, 1965; Butt, 2003; Kearns,2002).
Recent computational research on LVCsmainly focuses on type-based classification, i.e., sta-tistically aggregated properties of LVCs.
For exam-ple, many works are about direct measuring of thecompositionality (Venkatapathy and Joshi, 2005),compatibility (Barrett and Davis, 2003), acceptabil-ity (North, 2005) and productivity (Stevenson et al,2004) of LVCs.
Other works, if related to token-based identification, i.e., identifying idiomatic ex-pressions within context, only consider LVCs as onesmall subtype of other idiomatic expressions (Cooket al, 2007; Fazly and Stevenson, 2006).Previous computational works on token-basedidentification differs from our work in one key as-pect.
Our work builds a learning system which sys-tematically incorporates both informative statisticalmeasures and specific local contexts and does in-depth analysis on both of them while many previ-ous works, either totally rely on or only emphasizeon one of them.
For example, the method usedin (Katz and Giesbrecht, 2006) relies primarily onlocal co-occurrence lexicon to construct feature vec-tors for each target token.
On the other hand, someother works (Fazly and Stevenson, 2007; Fazly andStevenson, 2006; Stevenson et al, 2004), argue thatlinguistic properties, such as canonical syntactic pat-terns of specific types of idioms, are more informa-tive than local context.Tan et.al.
(Tan et al, 2006) propose a learning ap-proach to identify token-based LVCs.
The method isonly similar to ours in that it is a supervised frame-work.
Our model uses a different data set annotatedfrom BNC and the data set is larger and more bal-anced compared to the previous data set from WSJ.In addition, previous work assumes all verbs as po-tential LVCs while we intentionally exclude thoseverbs which linguistically never tested as light verbs,such as buy and sell in English and only focus ona half dozen of broadly documented English light32verbs, such as have, take, give, do, get and make.The lack of common benchmark data sets forevaluation in MWE research unfortunately makesmany works incomparable with the earlier ones.
Thedata set we construct in this study hopefully canserve as a common test bed for research in LVCsor MWEs in general.3 Learning English LVCsIn this study, we formulate the context sensitive En-glish LVC identification task as a supervised binaryclassification problem.
For each target LVC candi-date within a sentence, the classifier decides if it isa true LVC.
Formally, given a set of n labeled ex-amples {xi, yi}ni=1, we learn a function f : X ?
Ywhere Y ?
{?1, 1}.
The learning algorithm we useis the classic soft-margin SVM with L2-loss whichis among the best ?off-the-shelf?
supervised learn-ing algorithms and in our experiments the algorithmindeed gives us the best performance with the short-est training time.
The algorithm is implemented us-ing a modeling language called Learning Based Java(LBJ) (Rizzolo and Roth, 2010) via the LIBSVMJava API (Chang and Lin, 2001).Previous research has suggested that both localcontextual and statistical measures are informativein determining the class of an MWE token.
How-ever, it is not clear to what degree these two typesof information overlap or interact.
Do they containsimilar knowledge or the knowledge they providefor LVC learning is different?
Formulating a clas-sification framework for identification enables us tointegrate all contextual and statistical measures eas-ily through features and test their effectiveness andinteraction systematically.We focus on two types of features: contextual andstatistical features, and analyze in-depth their inter-action and effectiveness within the learning frame-work.
Statistical features in this study are numericalfeatures which are computed globally via other bigcorpora rather than the training and testing data usedin the system.
For example, the Cpmi and Deverbalv/n Ratio (details in sec.
3.1) are generated from thestatistics of Google n-gram and BNC corpus respec-tively.
Since the phrase size feature is numerical andthe selection of the candidate LVCs in the data setuses the canonical length information2, we includeit into the statistical category.
Contextual featuresare defined in a broader sense and consist of all localfeatures which are generated directly from the inputsentences, such as word features within or aroundthe candidate phrases.
We describe the details of theused contextual features in sec.
3.2.Our experiments show that arbitrarily combiningstatistic features within our current learning systemdoes not improve the performance.
Instead, we pro-vide systematic analysis for these features and ex-plore some interesting empirical observations aboutthem within our learning framework.3.1 Statistical FeaturesCpmi: Collocational point-wise mutual informationis calculated from Google n-gram dataset whose n-gram counts are generated from approximately onetrillion words of text from publicly accessible Webpages.
We use this big data set to overcome the datasparseness problem.Previous works (Stevenson et al, 2004; Cook etal., 2007) show that one canonical surface syntac-tic structure for LVCs is V + a/an Noun.
For ex-ample, in the LVC take a walk, ?take?
is the verb(V) and ?walk?
is the deverbal noun.
The typicaldeterminer in between is the indefinite article ?a?.It is also observed that when the indefinite articlechanges to definite, such as ?the?, ?this?
or ?that?,a phrase is less acceptable to be a true LVC.
There-fore, the direct collocational pmi between the verband the noun is derived to incorporate this intuitionas shown in the following3:Cpmi = 2I(v, aN) ?
I(v, theN)Within this formula, I(v, aN) is the point-wise mu-tual information between ?v?, the verb, and ?aN?,the phrase such as ?a walk?
in the aforementionedexample.
Similar definition applies to I(v, theN).PMI of a pair of elements is calculated as (Church etal., 1991):I(x, y) = log Nx+yf(x, y)f(x, ?
)f(?, y)2We set an empirical length constraint to the maximal lengthof the noun phrase object when generating the candidates fromBNC corpus.3The formula is directly from (Stevenson et al, 2004).33Nx+y is the total number of verb and a/the nounpairs in the corpus.
In our case, all trigram countswith this pattern in N-gram data set.
f(x, y) is thefrequency of x and y co-occurring as a v-a/theN pairwhere f(x, ?)
and f(?, y) are the frequency wheneither of x and y occurs independent of each otherin the corpus.
Notice these counts are not easilyavailable directly from search engines since manysearch engines treat articles such as ?a?
or ?the?
asstop words and remove them from the search query4.Deverbal v/n Ratio: the second statistical featurewe use is related to the verb and noun usage ratio ofthe noun object within a candidate LVC.
The intu-ition here is that the noun object of a candidate LVChas a strong tendency to be used as a verb or relatedto a verb via derivational morphology.
For exam-ple, in the candidate phrase ?have a look?, ?look?can directly be used as a verb while in the phrase?make a transmission?, ?transmission?
is derivation-ally related to the verb ?transmit?.
We use fre-quency counts gathered from British National Cor-pus (BNC) and then calculate the ratio since BNCencodes the lexeme for each word and is also taggedwith parts of speech.
In addition, it is a large corpuswith 100 million words, thus, an ideal corpus to cal-culate the verb-noun usage for each candidate wordin the object position.Two other lexical resources, WordNet (Fellbaum,1998) and NomLex (Meyers et al, 1998), are usedto identify words which can directly be used as anoun and a verb and those that are derivational re-lated.
Specifically, WordNet is used to identify thewords which can be used as both a noun and a verband NomLex is used to recognize those derivation-ally related words.
And the verb usage counts ofthese nouns are the frequencies of their correspond-ing derivational verbs.
For example, for the word?transmission?, its verb usage frequency is the countin BNC with its derivationally related verb ?trans-mit?.Phrase Size: the third statistical feature is the ac-tual size of the candidate LVC phrase.
Many modi-fiers can be inserted inside the candidate phrases togenerate new candidates.
For example, ?take a look?can be expanded to ?take a close look?, ?take an ex-4Some search engines accept ?quotation strategy?
to retainstop words in the query.tremely close look?
and the expansion is in theoryinfinite.
The hypothesis behind this feature is thatregular usage of LVCs tends to be short.
For exam-ple, it is observed that the canonical length in En-glish is from 2 to 6.3.2 Contextual FeaturesAll features generated directly from the input sen-tences are categorized into this group.
They con-sists of features derived directly from the candidatephrases themselves as well as their surrounding con-texts.Noun Object: this is the noun head of the objectnoun phrase within the candidate LVC phrase.
Forexample, for a verb phrase ?take a quick look?, itsnoun head ?look?
is the active Noun Object feature.In our data set, there are 777 distinctive such nouns.LV-NounObj: this is the bigram of the light verband the head of the noun phrase.
This feature en-codes the collocation information between the can-didate light verb and the head noun of its object.Levin?s Class: it is observed that members withincertain groups of verb classes are legitimate candi-dates to form acceptable LVCs (Fazly et al, 2005).For example, many sound emission verbs accord-ing to Levin (Levin, 1993), such as clap, whis-tle, and plop, can be used to generate legitimateLVCs.
Phrases such as make a clap/plop/whistle areall highly acceptable LVCs by humans even thoughsome of them, such as make a plop rarely occurwithin corpora.
We formulate a vector for all the256 Levin?s verb classes and turn the correspond-ing class-bits on when the verb usage of the headnoun in a candidate LVC belongs to these classes.We add one extra class, other, to be mapped to thoseverbs which are not included in any one of these 256Levin?s verb classes.Other Features: we construct other local con-textual features, for example, the part of speech ofthe word immediately before the light verb (titledposBefore) and after the whole phrase (posAfter).We also encode the determiner within all candidateLVCs as another lexical feature (Determiner).
Weexamine many other combinations of these contex-tual features.
However, only those features that con-tribute positively to achieve the highest performanceof the classifier are listed for detailed analysis in thenext section.344 Experiments and AnalysisIn this section, we report in detail our experimentalsettings and provide in-depth analysis on the inter-actions among features.
First, we present our mo-tivation and methodology to generate the new dataset.
Then we describe our experimental results andanalysis.4.1 Data Preparation and AnnotationThe data set is generated from BNC, a balanced syn-chronic corpus containing 100 million words col-lected from various sources of British English.
Webegin our sentence selection process with the ex-amination of a handful of previously investigatedverbs (Fazly and Stevenson, 2007; Butt, 2003).Among them, we pick the 6 most frequently usedEnglish light verbs: do, get, give, have, make andtake.To identify potential LVCs within sentences, wefirst extract all sentences where one or more of thesix verbs occur from BNC (XML Edition) and thenparse these sentences with Charniak?s parser (Char-niak and Johnson, 2005).
We focus on the ?verb+ noun object?
pattern and choose all the sentenceswhich have a direct NP object for the target verbs.We then collect a total of 207, 789 sentences.We observe that within all these chosen sentences,the distribution of true LVCs is still low.
We there-fore use three resources to filter out trivial nega-tive examples.
Firstly, We use WordNet (Fellbaum,1998) to identify the head noun in the object positionwhich can be used as both a noun and a verb.
Then,we use frequency counts gathered from BNC to fil-ter out candidates whose verb usage is smaller thantheir noun usage.
Finally, we use NomLex (Meyerset al, 1998) to recognize those head words in theobject position whose noun forms and verb formsare derivationally related, such as transmission andtransmit.
We keep all candidates whose object headnouns are derivationlly related to a verb accordingto a gold-standard word list we extract from Nom-Lex5.
With this pipeline method, we filter out ap-proximately 55% potential negative examples.
Thisleaves us with 92, 415 sentences which we sampleabout 4% randomly to present to annotators.
Thisfiltering method successfully improves the recall of5We do not count those nouns ending with er and istthe positive examples and ensures us a corpus withbalanced examples.A website6 is set up for annotators to annotate thedata.
Each potential LVC is presented to the anno-tator in a sentence.
The annotator is asked to decidewhether this phrase within the given sentence is anLVC and to choose an answer from one of these fouroptions: Yes, No, Not Sure, and Idiom.Detailed annotation instructions and LVC exam-ples are given on the annotation website.
When fac-ing difficult examples, the annotators are instructedto follow a general ?replacing?
principle, i.e, if thecandidate light verb within the sentence can be re-placed by the verb usage of its direct object nounand the meaning of the sentence does not change,that verb is regarded as a light verb and the candidateis an LVC.
Each example is annotated by two anno-tators and We only accept examples where both an-notators agree on positive or negative.
We generate atotal of 1, 039 positive examples and 1, 123 negativeexamples.
Among all these positive examples, thereare 760 distinctive LVC phrases and 911 distinctiveverb phrases with the pattern ?verb + noun object?among negative examples.
The generated data settherefore gives the classifier the 52.2% chance base-line if the classifier always votes the majority classin the data set.4.2 Evaluation MetricsFor each experiment, we evaluate the performancewith three sets of metrics.
We first report the stan-dard accuracy on the test data set.
Since accuracyis argued not to be a sufficient measure of the eval-uation of a binary classifier (Fazly et al, 2009) andsome previous works also report F1 values for thepositive classes, we therefore choose to report theprecision, recall and F1 value for both positive andnegative classes.True Class+ -Predicted Class + tp fp- fn tnTable 1: Confusion matrix to define true positive (tp),true negative (tn), false positive (fp) and false negative(fn).6http://cogcomp.cs.illinois.edu/?ytu/test/LVCmain.html35Based on the classic confusion matrix as shown inTable 1, we calculate the precision and recall for thepositive class in equation 1:P+ = tptp + fp R+ = tptp + fn (1)And similarly, we use equation 2 for negative class.And the F1 value is the harmonic mean of the preci-sion and recall of each class.P?
= tntn + fn R?
= tntn + fp (2)4.3 Experiments with Contextual FeaturesIn our experiments, We aim to build a high perfor-mance LVC classifier as well as to analyze the in-teraction between contextual and statistical features.We randomly sample 90% sentences for training andthe rest for testing.
Our chance baseline is 52.2%,which is the percentage of our majority class in thedata set.
As shown in Table 2, the classifier reachesan 86.3% accuracy using all contextual features de-scribed in previous section 3.2.
Interestingly, we ob-serve that adding other statistical features actuallyhurts the performance.
The classifier can effectivelylearn when trained with discrete contextual features.Label Precision Recall F1+ 86.486 84.211 85.333- 86.154 88.189 87.160Accuracy 86.307Chance Baseline 52.2Table 2: By using all our contextual features, our classi-fier achieves overall 86.307% accuracy.In order to examine the effectiveness of each indi-vidual feature, we conduct an ablation analysis andexperiment to use only one of them each time.
It isshown in Table 3 that LV-NounObj is found to be themost effective contextural feature since it boosts thebaseline system up the most, an significant increaseof 31.6%.We then start from this most effective feature, LV-NounObj and add one feature each step to observethe change of the system accuracy.
The results arelisted in Table 4.
Other significant features are fea-tures within the candidate LVCs themselves such asDeterminer, Noun Object and Levin?s Class relatedFeatures Accuracy Diff(%)Baseline (chance) 52.2LV-NounObj 83.817 +31.6Noun Object 79.253 +27.1Determiner 72.614 +20.4Levin?s Class 69.295 +17.1posBefore 53.112 +0.9posAfter 51.037 -1.1Table 3: Using only one feature each time.
LV-NounObjis the most effective feature.
Performance gain is associ-ated with a plus sign and otherwise a negative sign.to the object noun.
This observation agrees with pre-vious research that the acceptance of LVCs is closelycorrelated to the linguistic properties of their compo-nents.
The part of speech of the word after the phraseseems to have negative effect on the performance.However, experiments show that without this fea-ture, the overall performance decreases.Features Accuracy Diff(%)Baseline (chance) 52.2+ LV-NounObj 83.817 +31.6+ Noun Object 84.232 +0.4+ Levin?s Class 84.647 +0.4+ posBefore 84.647 0.0+ posAfter 83.817 -0.8+ Determiner 86.307 +2.5Table 4: Ablation analysis for contextual features.
Eachfeature is added incrementally at each step.
Performancegain is associated with a plus sign otherwise a negativesign.4.4 Experiments with Statistical FeaturesWhen using statistical features, instead of directlyusing the value, we discretize each value to a binaryfeature.
On the one hand, our experiments show thatthis way of transformation achieves the best perfor-mance.
On the other hand, the transformation playsan analogical role as a kernel function which mapsone dimensional non-linear separable examples intoan infinite or high dimensional space to render thedata linearly separable.In these experiments, we use only numerical fea-tures described in section 3.1.
And it is interestingto observe that those features achieve very similar36Label Precision Recall F1+ 86.481 85.088 86.463- 86.719 87.402 87.059Accuracy 86.307Table 5: Best performance achieved with statistical fea-tures.
Comparing to Table 2, the performance is similarto that trained with all contextual features.performance as the contextual features as shown inTable 5.To validate that the similar performance is notincidental.
We then separate our data into 10-foldtraining and testing sets and learn independentlyfrom each fold of these ten split.
Figure 1, whichshows the comparison of accuracies for each datafold, indicates the comparable results for each foldof the data.
Therefore, we conclude that the similareffect achieved by training with these two groups offeatures is not accidental.50607080901000 1 2 3 4 5 6 7 8 9AccuracyTen folds in the Data SetAccuracy of each fold using statistic or contextual featuresContextual FeaturesStatistic FeaturesFigure 1: Classifier Accuracy of each fold of all 10 foldtesting data, trained with groups of statistical features andcontextual features separately.
The similar height of eachhistogram indicates the similar performance over eachdata separation and the similarity is not incidental.We also conduct an ablation analysis with statis-tical features.
Similar to the ablation analyses forcontextual features, we first find that the most ef-fective statistical feature is Cpmi, the collocationalbased point-wise mutual information.
Then we addone feature at each step and show the increasingperformance in Table 6.
Cpmi is shown to be agood indicator for LVCs and this observation agreeswith many previous works on the effectiveness ofFeatures Accuracy Diff(%)BaseLine (chance) 52.2+ Cpmi 83.402 +31.2+ Deverbal v/n Ratio 85.892 +2.5+ Phrase Size 86.307 +0.4Table 6: Ablation analysis for statistical features.
Eachfeature is added incrementally at each step.
Performancegain is associated with a plus sign.point-wise mutual information in MWE identifica-tion tasks.4.5 Interaction between Contextual andStatistical FeaturesExperiments from our previous sections show thattwo types of features which are cosmetically differ-ent actually achieve similar performance.
In the ex-periments described in this section, we intend to dofurther analysis to identify further the relations be-tween them.4.5.1 Situation when they are similarOur ablation analysis shows that Cpmi and LV-NounObj features are the most two effective featuressince they boost the baseline performance up morethan 30%.
We then train the classifier with them to-gether and observe that the classifier exhibits sim-ilar performance as the one trained with them in-dependently as shown in Table 7.
This result indi-cates that these two types of features actually pro-vide similar knowledge to the system and thereforecombining them together does not provide any addi-tional new information.
This observation also agreeswith the intuition that point-wise mutual informa-tion basically provides information on word collo-cations (Church and Hanks, 1990).Feature Accuracy F1+ F1-LV-NounObj 83.817 82.028 85.283Cpmi 83.402 81.481 84.962Cpmi+LV-NounObj 83.817 82.028 85.283Table 7: The classifier achieves similar performancetrained jointly with Cpmi and LV-NounObj features, com-paring with the performance trained independently.374.5.2 Situation when they are differentToken-based LVC identification is a difficult taskon the basis of surface structures since they alwaysexhibit identical surface properties.
However, can-didate LVCs with identical surface structures in bothpositive and negative examples provide an ideal testbed for the functionality of local contextual features.For example, consider again these two aforemen-tioned sentences which are repeated here for refer-ence:1.
He had a look of childish bewilderment on hisface.2.
I?ve arranged for you to have a look at his filein our library.The system trained only with statistic features can-not distinguish these two examples since their type-based statistical features are exactly the same.
How-ever, the classifier trained with local contextual fea-tures is expected to perform better since it containsfeature information from surrounding words.
Toverify our hypothesis, we extract all examples inour data set which have this property and then se-lect same number of positive and negative examplesfrom them to formulate our test set.
We then trainout classifier with the rest of the data, independentlywith contextual features and statistical features.
Asshown in Table 8, the experiment results validateour hypothesis and show that the classifier trainedwith contextual features performs significantly bet-ter than the one trained with statistical features.
Theoverall lower system results also indicate that indeedthe test set with all ambiguous examples is a muchharder test set.One final observation is the extremely low F1value for negative class and relatively good perfor-mance for positive class when trained with only sta-tistical features.
This may be explained by the factthat statistical features have stronger bias towardpredicting examples as positive and can be used asan unsupervised metric to acquire real LVCs in cor-pora.5 Conclusion and Further ResearchIn this paper, we propose an in-depth case study onLVC recognition, in which we build a supervisedlearning system for automatically identifying LVCsClassifier Accuracy F1+ F1-Contextual 68.519 75.362 56.410Statistical 51.852 88.976 27.778Diff (%) +16.7 -13.6 +28.3Table 8: Classifier trained with local contextual featuresis more robust and significantly better than the one trainedwith statistical features when the test data set consists ofall ambiguous examples.in context.
Our learning system achieves an 86.3%accuracy with a baseline (chance) performance of52.2% when trained with groups of either contex-tual or statistical features.
In addition, we exploit indetail the interaction of these two groups of contex-tual and statistical features and show that the systemtrained with these two types of cosmetically differ-ent features actually reaches similar performance inour learning framework.
However, when it comes tothe situation where the surface structures of candi-date LVCs are identical, the system trained with con-textual features which include information on sur-rounding words provides better and more robust per-formance.In this study, we also construct a balanced bench-mark dataset with 2,162 sentences from BNC fortoken-based classification of English LVCs.
Andthis data set is publicly available and is also a use-ful computational resource for research on MWEs ingeneral.There are many aspects for further research of thecurrent study.
One direction for further improve-ment would be to include more long-distance fea-tures, such as parse tree path, to test the sensitivity ofthe LVC classifier to those features and to examinemore extensively the combination of the contextualand statistical features.
Another direction would beto adapt our system to other MWE types and to testif the analysis on contextual and statistical featuresin this study also applies to other MWEs.AcknowledgmentsThe authors would like to thank all annotators whoannotated the data via the web interface and fourannonymous reviewers for their valuable comments.The research in this paper was supported by the Mul-timodal Information Access & Synthesis Center at38UIUC, part of CCICADA, a DHS Science and Tech-nology Center of Excellence.ReferencesL.
Barrett and A. Davis.
2003.
Diagnostics for determingcompatibility in english support verb nominalizationpairs.
In Proceedings of CICLing-2003, pages 85?90.A.
Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado,and M. Pinkal.
2009.
Using framenet for seman-tic analysis of german: annotation, representationand automation.
In Hans Boas, editor, MultilingualFrameNets in Computational Lexicography: methodsand applications, pages 209?244.
Mouton de Gruyter.M.
Butt.
2003.
The light verb jungle.
In Harvard Work-ing Paper in Linguistics, volume 9, pages 1?49.C.
Chang and C. Lin, 2001.
LIBSVM: a libraryfor support vector machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InProceedings of ACL-2005.K.
Church and P. Hanks.
1990.
Word association norms,mutual information, and lexicography.
ComputationalLinguistics, 16(1), March.K.
Church, W. Gale, P. Hanks, and D. Hindle.
1991.
Us-ing statistics in lexical analysis.
In Lexical Acquisi-tion: Exploiting On-Line Resources to Build a Lexi-con, pages 115?164.
Erlbaum.P.
Cook, A. Fazly, and S. Stevenson.
2007.
Pulling theirweight: Exploiting syntactic forms for the automaticidentification of idiomatic expressions in context.
InProceedings of the Workshop on A Broader Perspec-tive on Multiword Expressions, pages 41?48, Prague,Czech Republic, June.
Association for ComputationalLinguistics.A.
Fazly and S. Stevenson.
2006.
Automatically con-structing a lexicon of verb phrase idiomatic combina-tions.
In Proceedings of EACL-2006.A.
Fazly and S. Stevenson.
2007.
Distinguishing sub-types of multiword expressions using linguistically-motivated statistical measures.
In Proceedings of theWorkshop on A Broader Perspective on Multiword Ex-pressions, pages 9?16, Prague, Czech Republic, June.A.
Fazly, R. North, and S. Stevenson.
2005.
Auto-matically distinguishing literal and figurative usagesof highly polysemous verbs.
In Proceedings of theACL-SIGLEX Workshop on Deep Lexical Acquisition,pages 38?47, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.A.
Fazly, P. Cook, and S. Stevenson.
2009.
Unsupervisedtype and token identification of idiomatic expression.Comutational Linguistics.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press.O.
Jespersen.
1965.
A Modern English Grammar on His-torical Principles, Part VI, Morphology.
Aeorge Allenand Unwin Ltd.G.
Katz and E. Giesbrecht.
2006.
Automatic identi-fication of non-compositional multi-word expressionsusing latent semantic analysis.
In Proceedings of theWorkshop on Multiword Expressions: Identifying andExploiting Underlying Properties, pages 12?19.K.
Kearns.
2002.
Light verbs in english.
Inhttp://www.ling.canterbury.ac.nz/documents.B.
Levin.
1993.
English Verb Classes and Alternations,A Preliminary Investigation.
University of ChicagoPress.A.
Meyers, C. Macleod, R. Yangarber, R. Grishman,L.
Barrett, and R. Reeves.
1998.
Using nomlex toproduce nominalization patterns for information ex-traction.
In Proceedings of COLING-ACL98 Work-shop:the Computational Treatment of Nominals.R.
North.
2005.
Computational measures of the ac-ceptability of light verb constructions.
University ofToronto, Master Thesis.N.
Rizzolo and D. Roth.
2010.
Learning based java forrapid development of nlp systems.
In Proceedings ofthe International Conference on Language Resourcesand Evaluation (LREC).I.
Sag, T. Baldwin, F. Bond, and A. Copestake.
2002.Multiword expressions: A pain in the neck for nlp.
InIn Proc.
of the 3rd International Conference on Intel-ligent Text Processing and Computational Linguistics(CICLing-2002, pages 1?15.T.
Samardz?ic?
and P. Merlo.
2010.
Cross-lingual vari-ation of light verb constructions: Using parallel cor-pora and automatic alignment for linguistic research.In Proceedings of the 2010 Workshop on NLP and Lin-guistics: Finding the Common Ground, pages 52?60,Uppsala, Sweden, July.S.
Stevenson, A. Fazly, and R. North.
2004.
Statisticalmeasures of the semi-productivity of light verb con-structions.
In Proceedings of ACL-04 workshop onMultiword Expressions: Integrating Processing, pages1?8.Y.
Tan, M. Kan, and H. Cui.
2006.
Extending corpus-based identification of light verb constructions usinga supervised learning framework.
In Proceedings ofEACL-06 workshop on Multi-word-expressions in amultilingual context, pages 49?56.S.
Venkatapathy and A. Joshi.
2005.
Measuring the rel-ative compositionality of verb-noun (v-n) collocationsby integrating features.
In Proceedings of HLT andEMNLP05, pages 899?906.39
