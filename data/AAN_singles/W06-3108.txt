Proceedings of the Workshop on Statistical Machine Translation, pages 55?63,New York City, June 2006. c?2006 Association for Computational LinguisticsDiscriminative Reordering Models for Statistical Machine TranslationRichard Zens and Hermann NeyHuman Language Technology and Pattern RecognitionLehrstuhl fu?r Informatik 6 ?
Computer Science DepartmentRWTH Aachen University, D-52056 Aachen, Germany{zens,ney}@cs.rwth-aachen.deAbstractWe present discriminative reorderingmodels for phrase-based statistical ma-chine translation.
The models are trainedusing the maximum entropy principle.We use several types of features: based onwords, based on word classes, based onthe local context.
We evaluate the overallperformance of the reordering models aswell as the contribution of the individualfeature types on a word-aligned corpus.Additionally, we show improved transla-tion performance using these reorderingmodels compared to a state-of-the-artbaseline system.1 IntroductionIn recent evaluations, phrase-based statistical ma-chine translation systems have achieved good per-formance.
Still the fluency of the machine transla-tion output leaves much to desire.
One reason isthat most phrase-based systems use a very simple re-ordering model.
Usually, the costs for phrase move-ments are linear in the distance, e.g.
see (Och et al,1999; Koehn, 2004; Zens et al, 2005).Recently, in (Tillmann and Zhang, 2005) and in(Koehn et al, 2005), a reordering model has beendescribed that tries to predict the orientation of aphrase, i.e.
it answers the question ?should the nextphrase be to the left or to the right of the currentphrase??
This phrase orientation probability is con-ditioned on the current source and target phrase andrelative frequencies are used to estimate the proba-bilities.We adopt the idea of predicting the orientation,but we propose to use a maximum-entropy basedmodel.
The relative-frequency based approach maysuffer from the data sparseness problem, becausemost of the phrases occur only once in the trainingcorpus.
Our approach circumvents this problem byusing a combination of phrase-level and word-levelfeatures and by using word-classes or part-of-speechinformation.
Maximum entropy is a suitable frame-work for combining these different features with awell-defined training criterion.In (Koehn et al, 2005) several variants of the ori-entation model have been tried.
It turned out that fordifferent tasks, different models show the best per-formance.
Here, we let the maximum entropy train-ing decide which features are important and whichfeatures can be neglected.
We will see that addi-tional features do not hurt performance and can besafely added to the model.The remaining part is structured as follows: firstwe will describe the related work in Section 2 andgive a brief description of the baseline system inSection 3.
Then, we will present the discriminativereordering model in Section 4.
Afterwards, we willevaluate the performance of this new model in Sec-tion 5.
This evaluation consists of two parts: first wewill evaluate the prediction capabilities of the modelon a word-aligned corpus and second we will showimproved translation quality compared to the base-line system.
Finally, we will conclude in Section 6.2 Related WorkAs already mentioned in Section 1, many currentphrase-based statistical machine translation systemsuse a very simple reordering model: the costs55for phrase movements are linear in the distance.This approach is also used in the publicly availablePharaoh decoder (Koehn, 2004).
The idea of pre-dicting the orientation is adopted from (Tillmannand Zhang, 2005) and (Koehn et al, 2005).
Here,we use the maximum entropy principle to combinea variety of different features.A reordering model in the framework of weightedfinite state transducers is described in (Kumar andByrne, 2005).
There, the movements are defined atthe phrase level, but the window for reordering isvery limited.
The parameters are estimated using anEM-style method.None of these methods try to generalize from thewords or phrases by using word classes or part-of-speech information.The approach presented here has some resem-blance to the bracketing transduction grammars(BTG) of (Wu, 1997), which have been applied toa phrase-based machine translation system in (Zenset al, 2004).
The difference is that, here, we donot constrain the phrase reordering.
Neverthelessthe inverted/monotone concatenation of phrases inthe BTG framework is similar to the left/right phraseorientation used here.3 Baseline SystemIn statistical machine translation, we are given asource language sentence fJ1 = f1 .
.
.
fj .
.
.
fJ ,which is to be translated into a target language sen-tence eI1 = e1 .
.
.
ei .
.
.
eI .
Among all possible tar-get language sentences, we will choose the sentencewith the highest probability:e?I?1 = argmaxI,eI1{Pr(eI1|fJ1 )} (1)The posterior probability Pr(eI1|fJ1 ) is modeled di-rectly using a log-linear combination of severalmodels (Och and Ney, 2002):Pr(eI1|fJ1 ) =exp(?Mm=1 ?mhm(eI1, fJ1 ))?I?,e?I?1exp(?Mm=1 ?mhm(e?I?1 , fJ1 ))(2)The denominator represents a normalization factorthat depends only on the source sentence fJ1 .
There-fore, we can omit it during the search process.
As adecision rule, we obtain:e?I?1 = argmaxI,eI1{ M?m=1?mhm(eI1, fJ1 )}(3)This approach is a generalization of the source-channel approach (Brown et al, 1990).
It has theadvantage that additional models h(?)
can be eas-ily integrated into the overall system.
The modelscaling factors ?M1 are trained with respect to the fi-nal translation quality measured by an error criterion(Och, 2003).We use a state-of-the-art phrase-based translationsystem (Zens and Ney, 2004; Zens et al, 2005) in-cluding the following models: an n-gram languagemodel, a phrase translation model and a word-basedlexicon model.
The latter two models are used forboth directions: p(f |e) and p(e|f).
Additionally,we use a word penalty and a phrase penalty.
Thereordering model of the baseline system is distance-based, i.e.
it assigns costs based on the distance fromthe end position of a phrase to the start position ofthe next phrase.
This very simple reordering modelis widely used, for instance in (Och et al, 1999;Koehn, 2004; Zens et al, 2005).4 The Reordering Model4.1 IdeaIn this section, we will describe the proposed dis-criminative reordering model.To make use of word level information, we needthe word alignment within the phrase pairs.
This canbe easily stored during the extraction of the phrasepairs from the bilingual training corpus.
If there aremultiple possible alignments for a phrase pair, weuse the most frequent one.The notation is introduced using the illustration inFigure 1.
There is an example of a left and a rightphrase orientation.
We assume that we have alreadyproduced the three-word phrase in the lower part.Now, the model has to predict if the start positionof the next phrase j?
is to the left or to the right ofthe current phrase.
The reordering model is appliedonly at the phrase boundaries.
We assume that thereordering within the phrases is correct.In the remaining part of this section, we will de-scribe the details of this reordering model.
Theclasses our model predicts will be defined in Sec-tion 4.2.
Then, the feature functions will be defined56targetpositionssource positionsj j?itargetpositionssource positionsiright phrase orientationleft phrase orientationjj?Figure 1: Illustration of the phrase orientation.in Section 4.3.
The training criterion and the train-ing events of the maximum entropy model will bedescribed in Section 4.4.4.2 Class DefinitionIdeally, this model predicts the start position of thenext phrase.
But as predicting the exact position israther difficult, we group the possible start positionsinto classes.
In the simplest case, we use only twoclasses.
One class for the positions to the left andone class for the positions to the right.
As a refine-ment, we can use four classes instead of two: 1) oneposition to the left, 2) more than one positions to theleft, 3) one position to the right, 4) more than onepositions to the right.In general, we use a parameter D to specify 2 ?
Dclasses of the types:?
exactly d positions to the left, d = 1, ...,D ?
1?
at least D positions to the left?
exactly d positions to the right, d = 1, ...,D?1?
at least D positions to the rightLet cj,j?
denote the orientation class for a move-ment from source position j to source position j?
asillustrated in Figure 1.
In the case of two orientationclasses, cj,j?
is defined as:cj,j?
={left, if j?
< jright, if j?
> j (4)Then, the reordering model has the formp(cj,j?|fJ1 , eI1, i, j)A well-founded framework for directly modeling theprobability p(cj,j?|fJ1 , eI1, i, j) is maximum entropy(Berger et al, 1996).
In this framework, we have aset of N feature functions hn(fJ1 , eI1, i, j, cj,j?
), n =1, .
.
.
,N .
Each feature function hn is weighted witha factor ?n.
The resulting model is:p?N1 (cj,j?|fJ1 , eI1, i, j)=exp( N?n=1?nhn(fJ1 , eI1, i, j, cj,j?
))?c?exp( N?n=1?nhn(fJ1 , eI1, i, j, c?))
(5)The functional form is identical to Equation 2,but here we will use a large number of binaryfeatures, whereas in Equation 2 usually only avery small number of real-valued features is used.More precisely, the resulting reordering modelp?N1 (cj,j?|fJ1 , eI1, i, j) is used as an additional com-ponent in the log-linear combination of Equation 2.4.3 Feature DefinitionThe feature functions of the reordering model de-pend on the last alignment link (j, i) of a phrase.Note that the source position j is not necessarily the57end position of the source phrase.
We use the sourceposition j which is aligned to the last word of thetarget phrase in target position i.
The illustration inFigure 1 contains such an example.To introduce generalization capabilities, some ofthe features will depend on word classes or part-of-speech information.
Let F J1 denote the wordclass sequence that corresponds to the source lan-guage sentence fJ1 and let EI1 denote the target wordclass sequence that corresponds to the target lan-guage sentence eI1.
Then, the feature functions areof the form hn(fJ1 , eI1, F J1 , EI1 , i, j, j?).
We considerthe following binary features:1. source words within a window around the cur-rent source position jhf,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?)
(6)= ?
(fj+d, f) ?
?
(c, cj,j?)2.
target words within a window around the cur-rent target position ihe,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?)
(7)= ?
(ei+d, e) ?
?
(c, cj,j?)3.
word classes or part-of-speech within a windowaround the current source position jhF,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?)
(8)= ?
(Fj+d, F ) ?
?
(c, cj,j?)4.
word classes or part-of-speech within a windowaround the current target position ihE,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?)
(9)= ?
(Ei+d, E) ?
?
(c, cj,j?
)Here, ?
(?, ?)
denotes the Kronecker-function.
In theexperiments, we will use d ?
{?1, 0, 1}.
Manyother feature functions are imaginable, e.g.
combi-nations of the described feature functions, n-gramor multi-word features, joint source and target lan-guage feature functions.4.4 TrainingAs training criterion, we use the maximum classposterior probability.
This corresponds to maximiz-ing the likelihood of the maximum entropy model.Since the optimization criterion is convex, there isonly a single optimum and no convergence problemsoccur.
To train the model parameters ?N1 , we use theGeneralized Iterative Scaling (GIS) algorithm (Dar-roch and Ratcliff, 1972).In practice, the training procedure tends to resultin an overfitted model.
To avoid overfitting, (Chenand Rosenfeld, 1999) have suggested a smoothingmethod where a Gaussian prior distribution of theparameters is assumed.This method tried to avoid very large lambda val-ues and prevents features that occur only once for aspecific class from getting a value of infinity.We train IBM Model 4 with GIZA++ (Och andNey, 2003) in both translation directions.
Then thealignments are symmetrized using a refined heuris-tic as described in (Och and Ney, 2003).
This word-aligned bilingual corpus is used to train the reorder-ing model parameters, i.e.
the feature weights ?N1 .Each alignment link defines an event for the max-imum entropy training.
An exception are the one-to-many alignments, i.e.
one source word is alignedto multiple target words.
In this case, only the top-most alignment link is considered because the otherones cannot occur at a phrase boundary.
Many-to-one and many-to-many alignments are handled in asimilar way.5 Experimental Results5.1 StatisticsThe experiments were carried out on the BasicTravel Expression Corpus (BTEC) task (Takezawaet al, 2002).
This is a multilingual speech cor-pus which contains tourism-related sentences sim-ilar to those that are found in phrase books.
Weuse the Arabic-English, the Chinese-English and theJapanese-English data.
The corpus statistics areshown in Table 1.As the BTEC is a rather clean corpus, the prepro-cessing consisted mainly of tokenization, i.e., sep-arating punctuation marks from words.
Addition-ally, we replaced contractions such as it?s or I?m inthe English corpus and we removed the case infor-mation.
For Arabic, we removed the diacritics andwe split common prefixes: Al, w, f, b, l. Therewas no special preprocessing for the Chinese and theJapanese training corpora.To train and evaluate the reordering model, we58Table 1: Corpus statistics after preprocessing for the BTEC task.Arabic Chinese Japanese EnglishTrain Sentences 20 000Running Words 180 075 176 199 198 453 189 927Vocabulary 15 371 8 687 9 277 6 870C-Star?03 Sentences 506Running Words 3 552 3 630 4 130 3 823Table 2: Statistics of the training and test word align-ment links.Ara-Eng Chi-Eng Jap-EngTraining 144K 140K 119KTest 16.2K 15.7K 13.2Kuse the word aligned bilingual training corpus.
Forevaluating the classification power of the reorderingmodel, we partition the corpus into a training partand a test part.
In our experiments, we use about10% of the corpus for testing and the remainingpart for training the feature weights of the reorder-ing model with the GIS algorithm using YASMET(Och, 2001).
The statistics of the training and testalignment links is shown in Table 2.
The numberof training events ranges from 119K for Japanese-English to 144K for Arabic-English.The word classes for the class-based features aretrained using the mkcls tool (Och, 1999).
In theexperiments, we use 50 word classes.
Alternatively,one could use part-of-speech information for thispurpose.Additional experiments were carried out on thelarge data track of the Chinese-English NIST task.The corpus statistics of the bilingual training cor-pus are shown in Table 3.
The language model wastrained on the English part of the bilingual train-ing corpus and additional monolingual English datafrom the GigaWord corpus.
The total amount of lan-guage model training data was about 600M runningwords.
We use a fourgram language model withmodified Kneser-Ney smoothing as implemented inthe SRILM toolkit (Stolcke, 2002).
For the four En-glish reference translations of the evaluation sets, theaccumulated statistics are presented.Table 3: Chinese-English NIST task: corpus statis-tics for the bilingual training data and the NIST eval-uation sets of the years 2002 to 2005.Chinese EnglishTrain Sentence Pairs 7MRunning Words 199M 213MVocabulary Size 223K 351KDictionary Entry Pairs 82KEval 2002 Sentences 878 3 512Running Words 25K 105K2003 Sentences 919 3 676Running Words 26K 122K2004 Sentences 1788 7 152Running Words 52K 245K2005 Sentences 1082 4 328Running Words 33K 148K5.2 Classification ResultsIn this section, we present the classification resultsfor the three language pairs.
In Table 4, we presentthe classification results for two orientation classes.As baseline we always choose the most frequentorientation class.
For Arabic-English, the baselineis with 6.3% already very low.
This means that theword order in Arabic is very similar to the word or-der in English.
For Chinese-English, the baselineis with 12.7% about twice as large.
The most dif-ferences in word order occur for Japanese-English.This seems to be reasonable as Japanese has usu-ally a different sentence structure, subject-object-verb compared to subject-verb-object in English.For each language pair, we present results for sev-eral combination of features.
The three columns perlanguage pair indicate if the features are based on thewords (column label ?Words?
), on the word classes(column label ?Classes?)
or on both (column label59Table 4: Classification error rates [%] using two orientation classes.Arabic-English Chinese-English Japanese-EnglishBaseline 6.3 12.7 26.2Lang.
Window Words Classes W+C Words Classes W+C Words Classes W+CTgt d = 0 4.7 5.3 4.4 9.3 10.4 8.9 13.6 15.1 13.4d ?
{0, 1} 4.5 5.0 4.3 8.9 9.9 8.6 13.7 14.9 13.4d ?
{?1, 0, 1} 4.5 4.9 4.3 8.6 9.5 8.3 13.5 14.6 13.3Src d = 0 5.6 5.0 3.9 7.9 8.3 7.2 12.2 11.8 11.0d ?
{0, 1} 3.2 3.0 2.6 4.7 4.7 4.2 10.1 9.7 9.4d ?
{?1, 0, 1} 2.9 2.5 2.3 3.9 3.5 3.3 9.0 8.0 7.8Src d = 0 4.3 3.9 3.7 7.1 7.8 6.5 10.8 10.9 9.8+ d ?
{0, 1} 2.9 2.6 2.5 4.6 4.5 4.1 9.3 9.1 8.6Tgt d ?
{?1, 0, 1} 2.8 2.1 2.1 3.9 3.4 3.3 8.7 7.7 7.7?W+C?).
We also distinguish if the features dependon the target sentence (?Tgt?
), on the source sentence(?Src?)
or on both (?Src+Tgt?
).For Arabic-English, using features based only onwords of the target sentence the classification er-ror rate can be reduced to 4.5%.
If the features arebased only on the source sentence words, a classifi-cation error rate of 2.9% is reached.
Combining thefeatures based on source and target sentence words,a classification error rate of 2.8% can be achieved.Adding the features based on word classes, the clas-sification error rate can be further improved to 2.1%.For the other language pairs, the results are similarexcept that the absolute values of the classificationerror rates are higher.We observe the following:?
The features based on the source sentence per-form better than features based on the targetsentence.?
Combining source and target sentence featuresperforms best.?
Increasing the window always helps, i.e.
addi-tional context information is useful.?
Often the word-class based features outperformthe word-based features.?
Combining word-based and word-class basedfeatures performs best.?
In general, adding features does not hurt theperformance.These are desirable properties of an appropriatereordering model.
The main point is that these arefulfilled not only on the training data, but on unseentest data.
There seems to be no overfitting problem.In Table 5, we present the results for four orien-tation classes.
The final error rates are a factor 2-4larger than for two orientation classes.
Despite thatwe observe the same tendencies as for two orien-tation classes.
Again, using more features alwayshelps to improve the performance.5.3 Translation ResultsFor the translation experiments on the BTEC task,we report the two accuracy measures BLEU (Pap-ineni et al, 2002) and NIST (Doddington, 2002) aswell as the two error rates: word error rate (WER)and position-independent word error rate (PER).These criteria are computed with respect to 16 refer-ences.In Table 6, we show the translation results forthe BTEC task.
In these experiments, the reorder-ing model uses two orientation classes, i.e.
it pre-dicts either a left or a right orientation.
The fea-tures for the maximum-entropy based reorderingmodel are based on the source and target languagewords within a window of one.
The word-classbased features are not used for the translation ex-periments.
The maximum-entropy based reorderingmodel achieves small but consistent improvementfor all the evaluation criteria.
Note that the baselinesystem, i.e.
using the distance-based reordering, wasamong the best systems in the IWSLT 2005 evalua-60Table 5: Classification error rates [%] using four orientation classes.Arabic-English Chinese-English Japanese-EnglishBaseline 31.4 44.9 59.0Lang.
Window Words Classes W+C Words Classes W+C Words Classes W+CTgt d = 0 24.5 27.7 24.2 30.0 34.4 29.7 28.9 31.4 28.7d ?
{0, 1} 23.9 27.2 23.7 29.2 32.9 28.9 28.7 30.6 28.3d ?
{?1, 0, 1} 22.1 25.3 21.9 27.6 31.4 27.4 28.3 30.1 28.2Src d = 0 22.1 23.2 20.4 25.9 27.7 20.4 24.1 24.9 22.3d ?
{0, 1} 11.9 12.0 10.8 14.0 14.9 13.2 18.6 19.5 17.7d ?
{?1, 0, 1} 10.1 8.7 8.0 11.4 11.1 10.5 15.6 15.6 14.5Src d = 0 20.9 21.8 19.6 24.1 26.8 19.6 22.3 23.4 21.1+ d ?
{0, 1} 11.8 11.5 10.6 13.5 14.5 12.8 18.6 18.8 17.1Tgt d ?
{?1, 0, 1} 9.6 7.7 7.6 11.3 10.1 10.1 15.6 15.2 14.2Table 6: Translation Results for the BTEC task.Language Pair Reordering WER [%] PER [%] NIST BLEU [%]Arabic-English Distance-based 24.1 20.9 10.0 63.8Max-Ent based 23.6 20.7 10.1 64.8Chinese-English Distance-based 50.4 43.0 7.67 44.4Max-Ent based 49.3 42.4 7.36 45.8Japanese-English Distance-based 32.1 25.2 8.96 56.2Max-Ent based 31.2 25.2 9.00 56.8tion campaign (Eck and Hori, 2005).Some translation examples are presented in Ta-ble 7.
We observe that the system using themaximum-entropy based reordering model producesmore fluent translations.Additional translation experiments were carriedout on the large data track of the Chinese-EnglishNIST task.
For this task, we use only the BLEUand NIST scores.
Both scores are computed case-insensitive with respect to four reference translationsusing the mteval-v11b tool1.For the NIST task, we use the BLEU score as pri-mary criterion which is optimized on the NIST 2002evaluation set using the Downhill Simplex algorithm(Press et al, 2002).
Note that only the eight or ninemodel scaling factors of Equation 2 are optimizedusing the Downhill Simplex algorithm.
The featureweights of the reordering model are trained usingthe GIS algorithm as described in Section 4.4.
Weuse a state-of-the-art baseline system which wouldhave obtained a good rank in the last NIST evalua-1http://www.nist.gov/speech/tests/mt/resources/scoring.htmtion (NIST, 2005).The translation results for the NIST task are pre-sented in Table 8.
We observe consistent improve-ments of the BLEU score on all evaluation sets.
Theoverall improvement due to reordering ranges from1.2% to 2.0% absolute.
The contribution of themaximum-entropy based reordering model to thisimprovement is in the range of 25% to 58%, e.g.
forthe NIST 2003 evaluation set about 58% of the im-provement using reordering can be attributed to themaximum-entropy based reordering model.We also measured the classification performancefor the NIST task.
The general tendencies are iden-tical to the BTEC task.6 ConclusionsWe have presented a novel discriminative reorder-ing model for statistical machine translation.
Thismodel is trained on the word aligned bilingual cor-pus using the maximum entropy principle.
Severaltypes of features have been used:?
based on the source and target sentence61Table 7: Translation examples for the BTEC task.System TranslationDistance-based I would like to check out time one day before.Max-Ent based I would like to check out one day before the time.Reference I would like to check out one day earlier.Distance-based I hate pepper green.Max-Ent based I hate the green pepper.Reference I hate green peppers.Distance-based Is there a subway map where?Max-Ent based Where is the subway route map?Reference Where do they have a subway map?Table 8: Translation results for several evaluation sets of the Chinese-English NIST task.Evaluation set 2002 (dev) 2003 2004 2005Reordering NIST BLEU[%] NIST BLEU[%] NIST BLEU[%] NIST BLEU[%]None 8.96 33.5 8.67 32.7 8.76 32.0 8.62 30.8Distance-based 9.19 34.6 8.85 33.2 9.05 33.2 8.79 31.6Max-Ent based 9.24 35.5 8.87 33.9 9.04 33.6 8.78 32.1?
based on words and word classes?
using local context informationWe have evaluated the performance of the re-ordering model on a held-out word-aligned corpus.We have shown that the model is able to predict theorientation very well, e.g.
for Arabic-English theclassification error rate is only 2.1%.We presented improved translation results forthree language pairs on the BTEC task and for thelarge data track of the Chinese-English NIST task.In none of the cases additional features have hurtthe classification performance on the held-out testcorpus.
This is a strong evidence that the maximumentropy framework is suitable for this task.Another advantage of our approach is the gener-alization capability via the use of word classes orpart-of-speech information.
Furthermore, additionalfeatures can be easily integrated into the maximumentropy framework.So far, the word classes were not used for thetranslation experiments.
As the word classes helpfor the classification task, we might expect furtherimprovements of the translation results.
Using part-of-speech information instead (or in addition) to theautomatically computed word classes might also bebeneficial.
More fine-tuning of the reordering modeltoward translation quality might also result in im-provements.
As already mentioned in Section 4.3, aricher feature set could be helpful.AcknowledgmentsThis material is partly based upon work supportedby the Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023,and was partly funded by the European Union un-der the integrated project TC-STAR (Technologyand Corpora for Speech to Speech Translation, IST-2002-FP6-506738, http://www.tc-star.org).ReferencesA.
L. Berger, S. A. Della Pietra, and V. J. DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?72, March.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85,June.S.
F. Chen and R. Rosenfeld.
1999.
A gaussian priorfor smoothing maximum entropy models.
TechnicalReport CMUCS-99-108, Carnegie Mellon University,Pittsburgh, PA.62J.
N. Darroch and D. Ratcliff.
1972.
Generalized itera-tive scaling for log-linear models.
Annals of Mathe-matical Statistics, 43:1470?1480.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proc.
ARPA Workshop on Human LanguageTechnology.M.
Eck and C. Hori.
2005.
Overview of the IWSLT 2005evaluation campaign.
In Proc.
International Workshopon Spoken Language Translation (IWSLT), Pittsburgh,PA, October.P.
Koehn, A. Axelrod, A.
B. Mayne, C. Callison-Burch,M.
Osborne, and D. Talbot.
2005.
Edinburgh sys-tem description for the 2005 IWSLT speech translationevaluation.
In Proc.
International Workshop on Spo-ken Language Translation (IWSLT), Pittsburgh, PA,October.P.
Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.In Proc.
6th Conf.
of the Assoc.
for Machine Transla-tion in the Americas (AMTA), pages 115?124, Wash-ington DC, September/October.S.
Kumar and W. Byrne.
2005.
Local phrase reorder-ing models for statistical machine translation.
InProc.
of the Human Language Technology Conf./Conf.on Empirical Methods in Natural Language Pro-cessing (HLT/EMNLP), pages 161?168, Vancouver,Canada, October.NIST.
2005.
NIST 2005 machinetranslation evaluation official results.http://www.nist.gov/speech/tests/mt/mt05eval official results release20050801 v3.html, August.F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of the 40th Annual Meeting ofthe Assoc.
for Computational Linguistics (ACL), pages295?302, Philadelphia, PA, July.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51, March.F.
J. Och, C. Tillmann, and H. Ney.
1999.
Improvedalignment models for statistical machine translation.In Proc.
Joint SIGDAT Conf.
on Empirical Methodsin Natural Language Processing and Very Large Cor-pora, pages 20?28, University of Maryland, CollegePark, MD, June.F.
J. Och.
1999.
An efficient method for determiningbilingual word classes.
In Proc.
9th Conf.
of the Europ.Chapter of the Assoc.
for Computational Linguistics(EACL), pages 71?76, Bergen, Norway, June.F.
J. Och.
2001.
YASMET: Toolkit for conditional maxi-mum entropy models.
http://www-i6.informatik.rwth-aachen.de/web/Software/YASMET.html.F.
J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
of the 41th AnnualMeeting of the Assoc.
for Computational Linguistics(ACL), pages 160?167, Sapporo, Japan, July.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proc.
of the 40th Annual Meeting ofthe Assoc.
for Computational Linguistics (ACL), pages311?318, Philadelphia, PA, July.W.
H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.Flannery.
2002.
Numerical Recipes in C++.
Cam-bridge University Press, Cambridge, UK.A.
Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In Proc.
Int.
Conf.
on Spoken LanguageProcessing (ICSLP), volume 2, pages 901?904, Den-ver, CO.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, andS.
Yamamoto.
2002.
Toward a broad-coverage bilin-gual corpus for speech translation of travel conver-sations in the real world.
In Proc.
of the Third Int.Conf.
on Language Resources and Evaluation (LREC),pages 147?152, Las Palmas, Spain, May.C.
Tillmann and T. Zhang.
2005.
A localized predictionmodel for statistical machine translation.
In Proc.
ofthe 43rd Annual Meeting of the Assoc.
for Computa-tional Linguistics (ACL), pages 557?564, Ann Arbor,MI, June.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?403, September.R.
Zens and H. Ney.
2004.
Improvements in phrase-based statistical machine translation.
In Proc.
HumanLanguage Technology Conf.
/ North American Chap-ter of the Assoc.
for Computational Linguistics AnnualMeeting (HLT-NAACL), pages 257?264, Boston, MA,May.R.
Zens, H. Ney, T. Watanabe, and E. Sumita.
2004.Reordering constraints for phrase-based statistical ma-chine translation.
In Proc.
20th Int.
Conf.
on Computa-tional Linguistics (COLING), pages 205?211, Geneva,Switzerland, August.R.
Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,J.
Xu, Y. Zhang, and H. Ney.
2005.
The RWTHphrase-based statistical machine translation system.
InProc.
International Workshop on Spoken LanguageTranslation (IWSLT), pages 155?162, Pittsburgh, PA,October.63
