Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 531?538, Vancouver, October 2005. c?2005 Association for Computational LinguisticsMaking Computers Laugh:Investigations in Automatic Humor RecognitionRada MihalceaDepartment of Computer ScienceUniversity of North TexasDenton, TX, 76203, USArada@cs.unt.eduCarlo StrapparavaIstituto per la Ricerca Scientifica e TecnologicaITC ?
irstI-38050, Povo, Trento, Italystrappa@itc.itAbstractHumor is one of the most interesting andpuzzling aspects of human behavior.
De-spite the attention it has received in fieldssuch as philosophy, linguistics, and psy-chology, there have been only few at-tempts to create computational models forhumor recognition or generation.
In thispaper, we bring empirical evidence thatcomputational approaches can be success-fully applied to the task of humor recogni-tion.
Through experiments performed onvery large data sets, we show that auto-matic classification techniques can be ef-fectively used to distinguish between hu-morous and non-humorous texts, with sig-nificant improvements observed over apri-ori known baselines.1 Introduction... pleasure has probably been the main goal all along.
But I hesitateto admit it, because computer scientists want to maintain their imageas hard-working individuals who deserve high salaries.
Sooner orlater society will realize that certain kinds of hard work are in factadmirable even though they are more fun than just about anythingelse.
(Knuth, 1993)Humor is an essential element in personal com-munication.
While it is merely considered a wayto induce amusement, humor also has a positive ef-fect on the mental state of those using it and has theability to improve their activity.
Therefore computa-tional humor deserves particular attention, as it hasthe potential of changing computers into a creativeand motivational tool for human activity (Stock etal., 2002; Nijholt et al, 2003).Previous work in computational humor has fo-cused mainly on the task of humor generation (Stockand Strapparava, 2003; Binsted and Ritchie, 1997),and very few attempts have been made to developsystems for automatic humor recognition (Taylorand Mazlack, 2004).
This is not surprising, since,from a computational perspective, humor recogni-tion appears to be significantly more subtle and dif-ficult than humor generation.In this paper, we explore the applicability ofcomputational approaches to the recognition of ver-bally expressed humor.
In particular, we investigatewhether automatic classification techniques are a vi-able approach to distinguish between humorous andnon-humorous text, and we bring empirical evidencein support of this hypothesis through experimentsperformed on very large data sets.Since a deep comprehension of humor in all ofits aspects is probably too ambitious and beyondthe existing computational capabilities, we choseto restrict our investigation to the type of humorfound in one-liners.
A one-liner is a short sen-tence with comic effects and an interesting linguisticstructure: simple syntax, deliberate use of rhetoricdevices (e.g.
alliteration, rhyme), and frequent useof creative language constructions meant to attractthe readers attention.
While longer jokes can havea relatively complex narrative structure, a one-linermust produce the humorous effect ?in one shot?,with very few words.
These characteristics makethis type of humor particularly suitable for use in anautomatic learning setting, as the humor-producingfeatures are guaranteed to be present in the first (andonly) sentence.We attempt to formulate the humor-recognition531problem as a traditional classification task, and feedpositive (humorous) and negative (non-humorous)examples to an automatic classifier.
The humor-ous data set consists of one-liners collected fromthe Web using an automatic bootstrapping process.The non-humorous data is selected such that itis structurally and stylistically similar to the one-liners.
Specifically, we use three different nega-tive data sets: (1) Reuters news titles; (2) proverbs;and (3) sentences from the British National Corpus(BNC).
The classification results are encouraging,with accuracy figures ranging from 79.15% (One-liners/BNC) to 96.95% (One-liners/Reuters).
Re-gardless of the non-humorous data set playing therole of negative examples, the performance of theautomatically learned humor-recognizer is alwayssignificantly better than apriori known baselines.The remainder of the paper is organized as fol-lows.
We first describe the humorous and non-humorous data sets, and provide details on the Web-based bootstrapping process used to build a verylarge collection of one-liners.
We then show experi-mental results obtained on these data sets using sev-eral heuristics and two different text classifiers.
Fi-nally, we conclude with a discussion and directionsfor future work.2 Humorous and Non-humorous Data SetsTo test our hypothesis that automatic classificationtechniques represent a viable approach to humorrecognition, we needed in the first place a data setconsisting of both humorous (positive) and non-humorous (negative) examples.
Such data sets canbe used to automatically learn computational mod-els for humor recognition, and at the same time eval-uate the performance of such models.2.1 Humorous DataFor reasons outlined earlier, we restrict our attentionto one-liners, short humorous sentences that have thecharacteristic of producing a comic effect in veryfew words (usually 15 or less).
The one-liners hu-mor style is illustrated in Table 1, which shows threeexamples of such one-sentence jokes.It is well-known that large amounts of trainingdata have the potential of improving the accuracy ofthe learning process, and at the same time provideinsights into how increasingly larger data sets canaffect the classification precision.
The manual con-enumerations matchingstylistic constraint (2)?yesyesseed one?linersautomatically identifiedone?linersWeb searchwebpages matchingthematic constraint (1)?candidatewebpagesFigure 1: Web-based bootstrapping of one-liners.struction of a very large one-liner data set may behowever problematic, since most Web sites or mail-ing lists that make available such jokes do not usu-ally list more than 50?100 one-liners.
To tackle thisproblem, we implemented a Web-based bootstrap-ping algorithm able to automatically collect a largenumber of one-liners starting with a short seed list,consisting of a few one-liners manually identified.The bootstrapping process is illustrated in Figure1.
Starting with the seed set, the algorithm auto-matically identifies a list of webpages that include atleast one of the seed one-liners, via a simple searchperformed with a Web search engine.
Next, the web-pages found in this way are HTML parsed, and ad-ditional one-liners are automatically identified andadded to the seed set.
The process is repeated sev-eral times, until enough one-liners are collected.An important aspect of any bootstrapping algo-rithm is the set of constraints used to steer the pro-cess and prevent as much as possible the addition ofnoisy entries.
Our algorithm uses: (1) a thematicconstraint applied to the theme of each webpage;and (2) a structural constraint, exploiting HTML an-notations indicating text of similar genre.The first constraint is implemented using a setof keywords of which at least one has to appearin the URL of a retrieved webpage, thus poten-tially limiting the content of the webpage to atheme related to that keyword.
The set of key-words used in the current implementation consistsof six words that explicitly indicate humor-relatedcontent: oneliner, one-liner, humor, humour, joke,532One-linersTake my advice; I don?t use it anyway.I get enough exercise just pushing my luck.Beauty is in the eye of the beer holder.Reuters titlesTrocadero expects tripling of revenues.Silver fixes at two-month high, but gold lags.Oil prices slip as refiners shop for bargains.BNC sentencesThey were like spirits, and I loved them.I wonder if there is some contradiction here.The train arrives three minutes early.ProverbsCreativity is more important than knowledge.Beauty is in the eye of the beholder.I believe no tales from an enemy?s tongue.Table 1: Sample examples of one-liners, Reuters ti-tles, BNC sentences, and proverbs.funny.
For example, http://www.berro.com/Jokesor http://www.mutedfaith.com/funny/life.htm are theURLs of two webpages that satisfy this constraint.The second constraint is designed to exploit theHTML structure of webpages, in an attempt to iden-tify enumerations of texts that include the seed one-liner.
This is based on the hypothesis that enumer-ations typically include texts of similar genre, andthus a list including the seed one-liner is likely toinclude additional one-line jokes.
For instance, if aseed one-liner is found in a webpage preceded by theHTML tag <li> (i.e.
?list item?
), other lines foundin the same enumeration preceded by the same tagare also likely to be one-liners.Two iterations of the bootstrapping process,started with a small seed set of ten one-liners, re-sulted in a large set of about 24,000 one-liners.After removing the duplicates using a measure ofstring similarity based on the longest common sub-sequence metric, we were left with a final set ofapproximately 16,000 one-liners, which are used inthe humor-recognition experiments.
Note that sincethe collection process is automatic, noisy entries arealso possible.
Manual verification of a randomly se-lected sample of 200 one-liners indicates an averageof 9% potential noise in the data set, which is withinreasonable limits, as it does not appear to signifi-cantly impact the quality of the learning.2.2 Non-humorous DataTo construct the set of negative examples re-quired by the humor-recognition models, we triedto identify collections of sentences that were non-humorous, but similar in structure and compositionto the one-liners.
We do not want the automatic clas-sifiers to learn to distinguish between humorous andnon-humorous examples based simply on text lengthor obvious vocabulary differences.
Instead, we seekto enforce the classifiers to identify humor-specificfeatures, by supplying them with negative examplessimilar in most of their aspects to the positive exam-ples, but different in their comic effect.We tested three different sets of negative exam-ples, with three examples from each data set illus-trated in Table 1.
All non-humorous examples areenforced to follow the same length restriction as theone-liners, i.e.
one sentence with an average lengthof 10?15 words.1.
Reuters titles, extracted from news articles pub-lished in the Reuters newswire over a period ofone year (8/20/1996 ?
8/19/1997) (Lewis et al,2004).
The titles consist of short sentences withsimple syntax, and are often phrased to catchthe readers attention (an effect similar to theone rendered by one-liners).2.
Proverbs extracted from an online proverb col-lection.
Proverbs are sayings that transmit, usu-ally in one short sentence, important facts orexperiences that are considered true by manypeople.
Their property of being condensed, butmemorable sayings make them very similar tothe one-liners.
In fact, some one-liners attemptto reproduce proverbs, with a comic effect, asin e.g.
?Beauty is in the eye of the beer holder?,derived from ?Beauty is in the eye of the be-holder?.3.
British National Corpus (BNC) sentences, ex-tracted from BNC ?
a balanced corpus coveringdifferent styles, genres and domains.
The sen-tences were selected such that they were similarin content with the one-liners: we used an in-formation retrieval system implementing a vec-torial model to identify the BNC sentence mostsimilar to each of the 16,000 one-liners1 .
Un-like the Reuters titles or the proverbs, the BNCsentences have typically no added creativity.However, we decided to add this set of negativeexamples to our experimental setting, in order1The sentence most similar to a one-liner is identified byrunning the one-liner against an index built for all BNC sen-tences with a length of 10?15 words.
We use a tf.idf weightingscheme and a cosine similarity measure, as implemented in theSmart system (ftp.cs.cornell.edu/pub/smart)533to observe the level of difficulty of a humor-recognition task when performed with respectto simple text.To summarize, the humor recognition experimentsrely on data sets consisting of humorous (positive)and non-humorous (negative) examples.
The posi-tive examples consist of 16,000 one-liners automat-ically collected using a Web-based bootstrappingprocess.
The negative examples are drawn from: (1)Reuters titles; (2) Proverbs; and (3) BNC sentences.3 Automatic Humor RecognitionWe experiment with automatic classification tech-niques using: (a) heuristics based on humor-specificstylistic features (alliteration, antonymy, slang); (b)content-based features, within a learning frameworkformulated as a typical text classification task; and(c) combined stylistic and content-based features,integrated in a stacked machine learning framework.3.1 Humor-Specific Stylistic FeaturesLinguistic theories of humor (Attardo, 1994) havesuggested many stylistic features that characterizehumorous texts.
We tried to identify a set of fea-tures that were both significant and feasible to im-plement using existing machine readable resources.Specifically, we focus on alliteration, antonymy, andadult slang, which were previously suggested as po-tentially good indicators of humor (Ruch, 2002; Bu-caria, 2004).Alliteration.
Some studies on humor appreciation(Ruch, 2002) show that structural and phonetic prop-erties of jokes are at least as important as their con-tent.
In fact one-liners often rely on the reader?sawareness of attention-catching sounds, through lin-guistic phenomena such as alliteration, word repeti-tion and rhyme, which produce a comic effect even ifthe jokes are not necessarily meant to be read aloud.Note that similar rhetorical devices play an impor-tant role in wordplay jokes, and are often used innewspaper headlines and in advertisement.
The fol-lowing one-liners are examples of jokes that includeone or more alliteration chains:Veni, Vidi, Visa: I came, I saw, I did a little shopping.Infants don?t enjoy infancy like adults do adultery.To extract this feature, we identify and count thenumber of alliteration/rhyme chains in each exam-ple in our data set.
The chains are automatically ex-tracted using an index created on top of the CMUpronunciation dictionary2 .Antonymy.
Humor often relies on some type ofincongruity, opposition or other forms of apparentcontradiction.
While an accurate identification ofall these properties is probably difficult to accom-plish, it is relatively easy to identify the presence ofantonyms in a sentence.
For instance, the comic ef-fect produced by the following one-liners is partlydue to the presence of antonyms:A clean desk is a sign of a cluttered desk drawer.Always try to be modest and be proud of it!The lexical resource we use to identify antonymsis WORDNET (Miller, 1995), and in particular theantonymy relation among nouns, verbs, adjectivesand adverbs.
For adjectives we also consider an in-direct antonymy via the similar-to relation amongadjective synsets.
Despite the relatively large num-ber of antonymy relations defined in WORDNET,its coverage is far from complete, and thus theantonymy feature cannot always be identified.
Adeeper semantic analysis of the text, such as wordsense disambiguation or domain disambiguation,could probably help detecting other types of seman-tic opposition, and we plan to exploit these tech-niques in future work.Adult slang.
Humor based on adult slang is verypopular.
Therefore, a possible feature for humor-recognition is the detection of sexual-oriented lexi-con in the sentence.
The following represent exam-ples of one-liners that include such slang:The sex was so good that even the neighbors had a cigarette.Artificial Insemination: procreation without recreation.To form a lexicon required for the identification ofthis feature, we extract from WORDNET DOMAINS3all the synsets labeled with the domain SEXUALITY.The list is further processed by removing all wordswith high polysemy (?
4).
Next, we check for thepresence of the words in this lexicon in each sen-tence in the corpus, and annotate them accordingly.Note that, as in the case of antonymy, WORDNETcoverage is not complete, and the adult slang fea-ture cannot always be identified.Finally, in some cases, all three features (alliteration,2Available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict3WORDNET DOMAINS assigns each synset in WORDNETwith one or more ?domain?
labels, such as SPORT, MEDICINE,ECONOMY.
See http://wndomains.itc.it.534antonymy, adult slang) are present in the same sen-tence, as for instance the following one-liner:Behind every greatal manant is a greatal womanant, andbehind every greatal womanant is some guy staring at herbehindsl!3.2 Content-based LearningIn addition to stylistic features, we also experi-mented with content-based features, through ex-periments where the humor-recognition task is for-mulated as a traditional text classification problem.Specifically, we compare results obtained with twofrequently used text classifiers, Na?
?ve Bayes andSupport Vector Machines, selected based on theirperformance in previously reported work, and fortheir diversity of learning methodologies.Na?
?ve Bayes.
The main idea in a Na?
?ve Bayes textclassifier is to estimate the probability of a categorygiven a document using joint probabilities of wordsand documents.
Na?
?ve Bayes classifiers assumeword independence, but despite this simplification,they perform well on text classification.
While thereare several versions of Na?
?ve Bayes classifiers (vari-ations of multinomial and multivariate Bernoulli),we use the multinomial model, previously shown tobe more effective (McCallum and Nigam, 1998).Support Vector Machines.
Support Vector Ma-chines (SVM) are binary classifiers that seek to findthe hyperplane that best separates a set of posi-tive examples from a set of negative examples, withmaximum margin.
Applications of SVM classifiersto text categorization led to some of the best resultsreported in the literature (Joachims, 1998).4 Experimental ResultsSeveral experiments were conducted to gain insightsinto various aspects related to an automatic hu-mor recognition task: classification accuracy usingstylistic and content-based features, learning rates,impact of the type of negative data, impact of theclassification methodology.All evaluations are performed using stratified ten-fold cross validations, for accurate estimates.
Thebaseline for all the experiments is 50%, which rep-resents the classification accuracy obtained if a labelof ?humorous?
(or ?non-humorous?)
would be as-signed by default to all the examples in the data set.Experiments with uneven class distributions werealso performed, and are reported in section 4.4.4.1 Heuristics using Humor-specific FeaturesIn a first set of experiments, we evaluated the classi-fication accuracy using stylistic humor-specific fea-tures: alliteration, antonymy, and adult slang.
Theseare numerical features that act as heuristics, and theonly parameter required for their application is athreshold indicating the minimum value admitted fora statement to be classified as humorous (or non-humorous).
These thresholds are learned automat-ically using a decision tree applied on a small subsetof humorous/non-humorous examples (1000 exam-ples).
The evaluation is performed on the remaining15,000 examples, with results shown in Table 24.One-liners One-liners One-linersHeuristic Reuters BNC ProverbsAlliteration 74.31% 59.34% 53.30%Antonymy 55.65% 51.40% 50.51%Adult slang 52.74% 52.39% 50.74%ALL 76.73% 60.63% 53.71%Table 2: Humor-recognition accuracy using allitera-tion, antonymy, and adult slang.Considering the fact that these features representstylistic indicators, the style of Reuters titles turnsout to be the most different with respect to one-liners, while the style of proverbs is the most sim-ilar.
Note that for all data sets the alliteration featureappears to be the most useful indicator of humor,which is in agreement with previous linguistic find-ings (Ruch, 2002).4.2 Text Classification with Content FeaturesThe second set of experiments was concerned withthe evaluation of content-based features for humorrecognition.
Table 3 shows results obtained usingthe three different sets of negative examples, withthe Na?
?ve Bayes and SVM text classifiers.
Learningcurves are plotted in Figure 2.One-liners One-liners One-linersClassifier Reuters BNC ProverbsNa ?
?ve Bayes 96.67% 73.22% 84.81%SVM 96.09% 77.51% 84.48%Table 3: Humor-recognition accuracy using Na?
?veBayes and SVM text classifiers.4We also experimented with decision trees learned from alarger number of examples, but the results were similar, whichconfirms our hypothesis that these features are heuristics, ratherthan learnable properties that improve their accuracy with addi-tional training data.5354050607080901000  20  40  60  80  100Classificationaccuracy(%)Fraction of data (%)Classification learning curvesNaive BayesSVM4050607080901000  20  40  60  80  100Classificationaccuracy(%)Fraction of data (%)Classification learning curvesNaive BayesSVM4050607080901000  20  40  60  80  100Classificationaccuracy(%)Fraction of data (%)Classification learning curvesNaive BayesSVM(a) (b) (c)Figure 2: Learning curves for humor-recognition using text classification techniques, with respect to threedifferent sets of negative examples: (a) Reuters; (b) BNC; (c) Proverbs.Once again, the content of Reuters titles appearsto be the most different with respect to one-liners,while the BNC sentences represent the most simi-lar data set.
This suggests that joke content tends tobe very similar to regular text, although a reasonablyaccurate distinction can still be made using text clas-sification techniques.
Interestingly, proverbs can bedistinguished from one-liners using content-basedfeatures, which indicates that despite their stylisticsimilarity (see Table 2), proverbs and one-liners dealwith different topics.4.3 Combining Stylistic and Content FeaturesEncouraged by the results obtained in the firsttwo experiments, we designed a third experimentthat attempts to jointly exploit stylistic and con-tent features for humor recognition.
The featurecombination is performed using a stacked learner,which takes the output of the text classifier, joins itwith the three humor-specific features (alliteration,antonymy, adult slang), and feeds the newly createdfeature vectors to a machine learning tool.
Giventhe relatively large gap between the performanceachieved with content-based features (text classifi-cation) and stylistic features (humor-specific heuris-tics), we decided to implement the second learningstage in the stacked learner using a memory basedlearning system, so that low-performance featuresare not eliminated in the favor of the more accu-rate ones5.
We use the Timbl memory based learner(Daelemans et al, 2001), and evaluate the classifica-tion using a stratified ten-fold cross validation.
Table5Using a decision tree learner in a similar stacked learningexperiment resulted into a flat tree that takes a classification de-cision based exclusively on the content feature, ignoring com-pletely the remaining stylistic features.4 shows the results obtained in this experiment, forthe three different data sets.One-liners One-liners One-linersReuters BNC Proverbs96.95% 79.15% 84.82%Table 4: Humor-recognition accuracy for combinedlearning based on stylistic and content features.Combining classifiers results in a statistically sig-nificant improvement (p < 0.0005, paired t-test)with respect to the best individual classifier for theOne-liners/Reuters and One-liners/BNC data sets,with relative error rate reductions of 8.9% and 7.3%respectively.
No improvement is observed for theOne-liners/Proverbs data set, which is not surpris-ing since, as shown in Table 2, proverbs and one-liners cannot be clearly differentiated using stylisticfeatures, and thus the addition of these features tocontent-based features is not likely to result in animprovement.4.4 DiscussionThe results obtained in the automatic classificationexperiments reveal the fact that computational ap-proaches represent a viable solution for the task ofhumor-recognition, and good performance can beachieved using classification techniques based onstylistic and content features.Despite our initial intuition that one-liners aremost similar to other creative texts (e.g.
Reuters ti-tles, or the sometimes almost identical proverbs),and thus the learning task would be more difficult inrelation to these data sets, comparative experimentalresults show that in fact it is more difficult to distin-guish humor with respect to regular text (e.g.
BNC536sentences).
Note however that even in this case thecombined classifier leads to a classification accuracythat improves significantly over the apriori knownbaseline.An examination of the content-based featureslearned during the classification process reveals in-teresting aspects of the humorous texts.
For in-stance, one-liners seem to constantly make referenceto human-related scenarios, through the frequent useof words such as man, woman, person, you, I. Simi-larly, humorous texts seem to often include negativeword forms, such as the negative verb forms doesn?t,isn?t, don?t, or negative adjectives like wrong or bad.A more extensive analysis of content-based humor-specific features is likely to reveal additional humor-specific content features, which could also be used instudies of humor generation.In addition to the three negative data sets, we alsoperformed an experiment using a corpus of arbitrarysentences randomly drawn from the three negativesets.
The humor recognition with respect to this neg-ative mixed data set resulted in 63.76% accuracy forstylistic features, 77.82% for content-based featuresusing Na?
?ve Bayes and 79.23% using SVM.
Thesefigures are comparable to those reported in Tables 2and 3 for One-liners/BNC, which suggests that theexperimental results reported in the previous sec-tions do not reflect a bias introduced by the negativedata sets, since similar results are obtained when thehumor recognition is performed with respect to ar-bitrary negative examples.As indicated in section 2.2, the negative exam-ples were selected structurally and stylistically sim-ilar to the one-liners, making the humor recognitiontask more difficult than in a real setting.
Nonethe-less, we also performed a set of experiments wherewe made the task even harder, using uneven classdistributions.
For each of the three types of nega-tive examples, we constructed a data set using 75%non-humorous examples and 25% humorous exam-ples.
Although the baseline in this case is higher(75%), the automatic classification techniques forhumor-recognition still improve over this baseline.The stylistic features lead to a classification accu-racy of 87.49% (One-liners/Reuters), 77.62% (One-liners/BNC), and 76.20% (One-liners/Proverbs),and the content-based features used in a Na?
?veBayes classifier result in accuracy figures of 96.19%(One-liners/Reuters), 81.56% (One-liners/BNC),and 87.86% (One-liners/Proverbs).Finally, in addition to classification accuracy, wewere also interested in the variation of classifica-tion performance with respect to data size, whichis an aspect particularly relevant for directing fu-ture research.
Depending on the shape of the learn-ing curves, one could decide to concentrate futurework either on the acquisition of larger data sets, ortoward the identification of more sophisticated fea-tures.
Figure 2 shows that regardless of the type ofnegative data, there is significant learning only un-til about 60% of the data (i.e.
about 10,000 positiveexamples, and the same number of negative exam-ples).
The rather steep ascent of the curve, especiallyin the first part of the learning, suggests that humor-ous and non-humorous texts represent well distin-guishable types of data.
An interesting effect canbe noticed toward the end of the learning, where forboth classifiers the curve becomes completely flat(One-liners/Reuters, One-liners/Proverbs), or it evenhas a slight drop (One-liners/BNC).
This is probablydue to the presence of noise in the data set, whichstarts to become visible for very large data sets6.This plateau is also suggesting that more data is notlikely to help improve the quality of an automatichumor-recognizer, and more sophisticated featuresare probably required.5 Related WorkWhile humor is relatively well studied in scientificfields such as linguistics (Attardo, 1994) and psy-chology (Freud, 1905; Ruch, 2002), to date thereis only a limited number of research contributionsmade toward the construction of computational hu-mour prototypes.One of the first attempts is perhaps the work de-scribed in (Binsted and Ritchie, 1997), where a for-mal model of semantic and syntactic regularities wasdevised, underlying some of the simplest types ofpuns (punning riddles).
The model was then ex-ploited in a system called JAPE that was able to au-tomatically generate amusing puns.Another humor-generation project was the HA-HAcronym project (Stock and Strapparava, 2003),whose goal was to develop a system able to au-tomatically generate humorous versions of existing6We also like to think of this behavior as if the computeris losing its sense of humor after an overwhelming number ofjokes, in a way similar to humans when they get bored and stopappreciating humor after hearing too many jokes.537acronyms, or to produce a new amusing acronymconstrained to be a valid vocabulary word, startingwith concepts provided by the user.
The comic ef-fect was achieved mainly by exploiting incongruitytheories (e.g.
finding a religious variation for a tech-nical acronym).Another related work, devoted this time to theproblem of humor comprehension, is the study re-ported in (Taylor and Mazlack, 2004), focused ona very restricted type of wordplays, namely the?Knock-Knock?
jokes.
The goal of the study wasto evaluate to what extent wordplay can be automati-cally identified in ?Knock-Knock?
jokes, and if suchjokes can be reliably recognized from other non-humorous text.
The algorithm was based on auto-matically extracted structural patterns and on heuris-tics heavily based on the peculiar structure of thisparticular type of jokes.
While the wordplay recog-nition gave satisfactory results, the identification ofjokes containing such wordplays turned out to besignificantly more difficult.6 ConclusionA conclusion is simply the place where you got tired of thinking.
(anonymous one-liner)The creative genres of natural language have beentraditionally considered outside the scope of anycomputational modeling.
In particular humor, be-cause of its puzzling nature, has received little atten-tion from computational linguists.
However, giventhe importance of humor in our everyday life, andthe increasing importance of computers in our workand entertainment, we believe that studies related tocomputational humor will become increasingly im-portant.In this paper, we showed that automatic classifi-cation techniques can be successfully applied to thetask of humor-recognition.
Experimental results ob-tained on very large data sets showed that computa-tional approaches can be efficiently used to distin-guish between humorous and non-humorous texts,with significant improvements observed over aprioriknown baselines.
To our knowledge, this is the firstresult of this kind reported in the literature, as weare not aware of any previous work investigating theinteraction between humor and techniques for auto-matic classification.Finally, through the analysis of learning curvesplotting the classification performance with respectto data size, we showed that the accuracy of the au-tomatic humor-recognizer stops improving after acertain number of examples.
Given that automatichumor-recognition is a rather understudied problem,we believe that this is an important result, as it pro-vides insights into potentially productive directionsfor future work.
The flattened shape of the curvestoward the end of the learning process suggests thatrather than focusing on gathering more data, fu-ture work should concentrate on identifying moresophisticated humor-specific features, e.g.
semanticoppositions, ambiguity, and others.
We plan to ad-dress these aspects in future work.ReferencesS.
Attardo.
1994.
Linguistic Theory of Humor.
Mouton deGruyter, Berlin.K.
Binsted and G. Ritchie.
1997.
Computational rules for pun-ning riddles.
Humor, 10(1).C.
Bucaria.
2004.
Lexical and syntactic ambiguity as a sourceof humor.
Humor, 17(3).W.
Daelemans, J. Zavrel, K. van der Sloot, and A. van denBosch.
2001.
Timbl: Tilburg memory based learner, ver-sion 4.0, reference guide.
Technical report, University ofAntwerp.S.
Freud.
1905.
Der Witz und Seine Beziehung zum Unbe-wussten.
Deutike, Vienna.T.
Joachims.
1998.
Text categorization with Support VectorMachines: learning with many relevant features.
In Pro-ceedings of the European Conference on Machine Learning.D.E.
Knuth.
1993.
The Stanford Graph Base: A Platform forcombinatorial computing.
ACM Press.D.
Lewis, Y. Yang, T. Rose, and F. Li.
2004.
RCV1: A newbenchmark collection for text categorization research.
TheJournal of Machine Learning Research, 5:361?397.A.
McCallum and K. Nigam.
1998.
A comparison of eventmodels for Naive Bayes text classification.
In Proceedingsof AAAI-98 Workshop on Learning for Text Categorization.G.
Miller.
1995.
Wordnet: A lexical database.
Communicationof the ACM, 38(11):39?41.A.
Nijholt, O.
Stock, A. Dix, and J. Morkes, editors.
2003.
Pro-ceedings of CHI-2003 workshop: Humor Modeling in theInterface, Fort Lauderdale, Florida.W.
Ruch.
2002.
Computers with a personality?
lessons to belearned from studies of the psychology of humor.
In Pro-ceedings of the The April Fools Day Workshop on Computa-tional Humour.O.
Stock and C. Strapparava.
2003.
Getting serious about thedevelopment of computational humour.
In Proceedings ofthe 8th International Joint Conference on Artificial Intelli-gence (IJCAI-03), Acapulco, Mexico.O.
Stock, C. Strapparava, and A. Nijholt, editors.
2002.
Pro-ceedings of the The April Fools Day Workshop on Computa-tional Humour, Trento.J.
Taylor and L. Mazlack.
2004.
Computationally recognizingwordplay in jokes.
In Proceedings of CogSci 2004, Chicago.538
