Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 258?267,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsFaster and Smaller N -Gram Language ModelsAdam Pauls Dan KleinComputer Science DivisionUniversity of California, Berkeley{adpauls,klein}@cs.berkeley.eduAbstractN -gram language models are a major resourcebottleneck in machine translation.
In this pa-per, we present several language model imple-mentations that are both highly compact andfast to query.
Our fastest implementation isas fast as the widely used SRILM while re-quiring only 25% of the storage.
Our mostcompact representation can store all 4 billionn-grams and associated counts for the Googlen-gram corpus in 23 bits per n-gram, the mostcompact lossless representation to date, andeven more compact than recent lossy compres-sion techniques.
We also discuss techniquesfor improving query speed during decoding,including a simple but novel language modelcaching technique that improves the queryspeed of our language models (and SRILM)by up to 300%.1 IntroductionFor modern statistical machine translation systems,language models must be both fast and compact.The largest language models (LMs) can contain asmany as several hundred billion n-grams (Brantset al, 2007), so storage is a challenge.
At thesame time, decoding a single sentence can trig-ger hundreds of thousands of queries to the lan-guage model, so speed is also critical.
As al-ways, trade-offs exist between time, space, and ac-curacy, with many recent papers considering small-but-approximate noisy LMs (Chazelle et al, 2004;Guthrie and Hepple, 2010) or small-but-slow com-pressed LMs (Germann et al, 2009).In this paper, we present several lossless meth-ods for compactly but efficiently storing large LMsin memory.
As in much previous work (Whittakerand Raj, 2001; Hsu and Glass, 2008), our meth-ods are conceptually based on tabular trie encodingswherein each n-gram key is stored as the concatena-tion of one word (here, the last) and an offset encod-ing the remaining words (here, the context).
Afterpresenting a bit-conscious basic system that typifiessuch approaches, we improve on it in several ways.First, we show how the last word of each entry canbe implicitly encoded, almost entirely eliminatingits storage requirements.
Second, we show that thedeltas between adjacent entries can be efficiently en-coded with simple variable-length encodings.
Third,we investigate block-based schemes that minimizethe amount of compressed-stream scanning duringlookup.To speed up our language models, we present twoapproaches.
The first is a front-end cache.
Cachingitself is certainly not new to language modeling, butbecause well-tuned LMs are essentially lookup ta-bles to begin with, naive cache designs only speedup slower systems.
We present a direct-addressingcache with a fast key identity check that speeds upour systems (or existing fast systems like the widely-used, speed-focused SRILM) by up to 300%.Our second speed-up comes from a more funda-mental change to the language modeling interface.Where classic LMs take word tuples and producecounts or probabilities, we propose an LM that takesa word-and-context encoding (so the context neednot be re-looked up) and returns both the probabil-ity and also the context encoding for the suffix of theoriginal query.
This setup substantially acceleratesthe scrolling queries issued by decoders, and alsoexploits language model state equivalence (Li andKhudanpur, 2008).Overall, we are able to store the 4 billion n-gramsof the Google Web1T (Brants and Franz, 2006) cor-258pus, with associated counts, in 10 GB of memory,which is smaller than state-of-the-art lossy languagemodel implementations (Guthrie and Hepple, 2010),and significantly smaller than the best publishedlossless implementation (Germann et al, 2009).
Weare also able to simultaneously outperform SRILMin both total size and speed.
Our LM toolkit, whichis implemented in Java and compatible with the stan-dard ARPA file formats, is available on the web.12 PreliminariesOur goal in this paper is to provide data structuresthat map n-gram keys to values, i.e.
probabilitiesor counts.
Maps are fundamental data structuresand generic implementations of mapping data struc-tures are readily available.
However, because of thesheer number of keys and values needed for n-gramlanguage modeling, generic implementations do notwork efficiently ?out of the box.?
In this section,we will review existing techniques for encoding thekeys and values of an n-gram language model, tak-ing care to account for every bit of memory requiredby each implementation.To provide absolute numbers for the storage re-quirements of different implementations, we willuse the Google Web1T corpus as a benchmark.
Thiscorpus, which is on the large end of corpora typicallyemployed in language modeling, is a collection ofnearly 4 billion n-grams extracted from over a tril-lion tokens of English text, and has a vocabulary ofabout 13.5 million words.2.1 Encoding ValuesIn the Web1T corpus, the most frequent n-gramoccurs about 95 billion times.
Storing this countexplicitly would require 37 bits, but, as noted byGuthrie and Hepple (2010), the corpus contains onlyabout 770 000 unique counts, so we can enumerateall counts using only 20 bits, and separately store anarray called the value rank array which converts therank encoding of a count back to its raw count.
Theadditional array is small, requiring only about 3MB,but we save 17 bits per n-gram, reducing value stor-age from around 16GB to about 9GB for Web1T.We can rank encode probabilities and back-offs inthe same way, allowing us to be agnostic to whether1http://code.google.com/p/berkeleylm/we encode counts, probabilities and/or back-offweights in our model.
In general, the number of bitsper value required to encode all value ranks for agiven language model will vary ?
we will refer tothis variable as v .2.2 Trie-Based Language ModelsThe data structure of choice for the majority ofmodern language model implementations is a trie(Fredkin, 1960).
Tries or variants thereof areimplemented in many LM tool kits, includingSRILM (Stolcke, 2002), IRSTLM (Federico andCettolo, 2007), CMU SLM (Whittaker and Raj,2001), and MIT LM (Hsu and Glass, 2008).
Triesrepresent collections of n-grams using a tree.
Eachnode in the tree encodes a word, and paths in thetree correspond to n-grams in the collection.
Triesensure that each n-gram prefix is represented onlyonce, and are very efficient when n-grams sharecommon prefixes.
Values can also be stored in a trieby placing them in the appropriate nodes.Conceptually, trie nodes can be implemented asrecords that contain two entries: one for the wordin the node, and one for either a pointer to the par-ent of the node or a list of pointers to children.
Ata low level, however, naive implementations of triescan waste significant amounts of space.
For exam-ple, the implementation used in SRILM represents atrie node as a C struct containing a 32-bit integerrepresenting the word, a 64-bit memory2 pointer tothe list of children, and a 32-bit floating point num-ber representing the value stored at a node.
The totalstorage for a node alone is 16 bytes, with additionaloverhead required to store the list of children.
Intotal, the most compact implementation in SRILMuses 33 bytes per n-gram of storage, which wouldrequire around 116 GB of memory to store Web1T.While it is simple to implement a trie node in this(already wasteful) way in programming languagesthat offer low-level access to memory allocation likeC/C++, the situation is even worse in higher levelprogramming languages.
In Java, for example, C-style structs are not available, and records aremost naturally implemented as objects that carry anadditional 64 bits of overhead.2While 32-bit architectures are still in use today, their lim-ited address space is insufficient for modern language modelsand we will assume all machines use a 64-bit architecture.259Despite its relatively large storage requirements,the implementation employed by SRILM is stillwidely in use today, largely because of its speed ?
toour knowledge, SRILM is the fastest freely availablelanguage model implementation.
We will show thatwe can achieve access speeds comparable to SRILMbut using only 25% of the storage.2.3 Implicit TriesA more compact implementation of a trie is de-scribed in Whittaker and Raj (2001).
In their imple-mentation, nodes in a trie are represented implicitlyas entries in an array.
Each entry encodes a wordwith enough bits to index all words in the languagemodel (24 bits for Web1T), a quantized value, anda 32-bit3 offset that encodes the contiguous blockof the array containing the children of the node.Note that 32 bits is sufficient to index all n-grams inWeb1T; for larger corpora, we can always increasethe size of the offset.Effectively, this representation replaces system-level memory pointers with offsets that act as logicalpointers that can reference other entries in the array,rather than arbitrary bytes in RAM.
This represen-tation saves space because offsets require fewer bitsthan memory pointers, but more importantly, it per-mits straightforward implementation in any higher-level language that provides access to arrays of inte-gers.42.4 Encoding n-gramsHsu and Glass (2008) describe a variant of the im-plicit tries of Whittaker and Raj (2001) in whicheach node in the trie stores the prefix (i.e.
parent).This representation has the property that we can re-fer to each n-gram wn1 by its last word wn and theoffset c(wn?11 ) of its prefix wn?11 , often called thecontext.
At a low-level, we can efficiently encodethis pair (wn, c(wn?11 )) as a single 64-bit integer,where the first 24 bits refer to wn and the last 40 bits3The implementation described in the paper represents each32-bit integer compactly using only 16 bits, but this represen-tation is quite inefficient, because determining the full 32-bitoffset requires a binary search in a look up table.4Typically, programming languages only provide supportfor arrays of bytes, not bits, but it is of course possible to simu-late arrays with arbitrary numbers of bits using byte arrays andbit manipulation.encode c(wn?11 ).
We will refer to this encoding as acontext encoding.Note that typically, n-grams are encoded in triesin the reverse direction (first-rest instead of last-rest), which enables a more efficient computation ofback-offs.
In our implementations, we found that thespeed improvement from switching to a first-rest en-coding and implementing more efficient queries wasmodest.
However, as we will see in Section 4.2, thelast-rest encoding allows us to exploit the scrollingnature of queries issued by decoders, which resultsin speedups that far outweigh those achieved by re-versing the trie.3 Language Model ImplementationsIn the previous section, we reviewed well-knowntechniques in language model implementation.
Inthis section, we combine these techniques to buildsimple data structures in ways that are to our knowl-edge novel, producing language models with state-of-the-art memory requirements and speed.
We willalso show that our data structures can be very effec-tively compressed by implicitly encoding the wordwn, and further compressed by applying a variable-length encoding on context deltas.3.1 Sorted ArrayA standard way to implement a map is to store anarray of key/value pairs, sorted according to the key.Lookup is carried out by performing binary searchon a key.
For an n-gram language model, we can ap-ply this implementation with a slight modification:we need n sorted arrays, one for each n-gram order.We construct keys (wn, c(wn?11 )) using the contextencoding described in the previous section, wherethe context offsets c refer to entries in the sorted ar-ray of (n ?
1)-grams.
This data structure is showngraphically in Figure 1.Because our keys are sorted according to theircontext-encoded representation, we cannot straight-forwardly answer queries about an n-gram w with-out first determining its context encoding.
We cando this efficiently by building up the encoding in-crementally: we start with the context offset of theunigram w1, which is simply its integer representa-tion, and use that to form the context encoding of thebigram w21 = (w2, c(w1)).
We can find the offset of260?ran?w c15180053w c24bits40bits64 bits?cat?15176585?dog?68796879687968796879688068806880687900004598?slept?00004588000045680000453000004502000046680000466900004568000045776879 000044981517658315176585151765931517661315179801151800511517658915176591?had??the??left?1933............
?slept?193319331933193319331933193519351935..val val valvbits..?dog?3-grams2-grams 1-gramswFigure 1: Our SORTED implementation of a trie.
The dotted paths correspond to ?the cat slept?, ?the cat ran?, and ?thedog ran?.
Each node in the trie is an entry in an array with 3 parts: w represents the word at the node; val representsthe (rank encoded) value; and c is an offset in the array of n ?
1 grams that represents the parent (prefix) of a node.Words are represented as offsets in the unigram array.the bigram using binary search, and form the contextencoding of the trigram, and so on.
Note, however,that if our queries arrive in context-encoded form,queries are faster since they involve only one binarysearch in the appropriate array.
We will return to thislater in Section 4.2This implementation, SORTED, uses 64 bits forthe integer-encoded keys and v bits for the values.Lookup is linear in the length of the key and log-arithmic in the number of n-grams.
For Web1T(v = 20), the total storage is 10.5 bytes/n-gram orabout 37GB.3.2 Hash TableHash tables are another standard way to implementassociative arrays.
To enable the use of our contextencoding, we require an implementation in whichwe can refer to entries in the hash table via arrayoffsets.
For this reason, we use an open address hashmap that uses linear probing for collision resolution.As in the sorted array implementation, in order toinsert an n-gram wn1 into the hash table, we mustform its context encoding incrementally from theoffset of w1.
However, unlike the sorted array im-plementation, at query time, we only need to beable to check equality between the query key wn1 =(wn, c(wn?11 )) and a key w?n1 = (w?n, c(w?n?11 )) inthe table.
Equality can easily be checked by firstchecking if wn = w?n, then recursively checkingequality between wn?11 and w?n?11 , though again,equality is even faster if the query is already context-encoded.This HASH data structure also uses 64 bits forinteger-encoded keys and v bits for values.
How-ever, to avoid excessive hash collisions, we also al-locate additional empty space according to a user-defined parameter that trades off speed and time ?we used about 40% extra space in our experiments.For Web1T, the total storage for this implementationis 15 bytes/n-gram or about 53 GB total.Look up in a hash map is linear in the length ofan n-gram and constant with respect to the number261of n-grams.
Unlike the sorted array implementa-tion, the hash table implementation also permits ef-ficient insertion and deletion, making it suitable forstream-based language models (Levenberg and Os-borne, 2009).3.3 Implicitly Encoding wnThe context encoding we have used thus far stillwastes space.
This is perhaps most evident in thesorted array representation (see Figure 1): all n-grams ending with a particular word wi are storedcontiguously.
We can exploit this redundancy bystoring only the context offsets in the main array,using as many bits as needed to encode all contextoffsets (32 bits for Web1T).
In auxiliary arrays, onefor each n-gram order, we store the beginning andend of the range of the trie array in which all (wi, c)keys are stored for each wi.
These auxiliary arraysare negligibly small ?
we only need to store 2n off-sets for each word.The same trick can be applied in the hash tableimplementation.
We allocate contiguous blocks ofthe main array for n-grams which all share the samelast word wi, and distribute keys within those rangesusing the hashing function.This representation reduces memory usage forkeys from 64 bits to 32 bits, reducing overall storagefor Web1T to 6.5 bytes/n-gram for the sorted imple-mentation and 9.1 bytes for the hashed implementa-tion, or about 23GB and 32GB in total.
It also in-creases query speed in the sorted array case, since tofind (wi, c), we only need to search the range of thearray over which wi applies.
Because this implicitencoding reduces memory usage without a perfor-mance cost, we will assume its use for the rest ofthis paper.3.4 A Compressed Implementation3.4.1 Variable-Length CodingThe distribution of value ranks in language mod-eling is Zipfian, with far more n-grams having lowcounts than high counts.
If we ensure that the valuerank array sorts raw values by descending order offrequency, then we expect that small ranks will oc-cur much more frequently than large ones, which wecan exploit with a variable-length encoding.To compress n-grams, we can exploit the contextencoding of our keys.
In Figure 2, we show a portionw c val1933 15176585 31933 15176587 21933 15176593 11933 15176613 81933 15179801 11935 15176585 2981935 15176589 11933 15176585 563097887 956 3 0 +0 +2 2 +0 +5 1 +0 +40 8!w !cval1933 15176585 3+0 +2 1+0 +5 1+0 +40 8+0 +188 1+2 15176585 298+0 +4 1|!w| |!c||val|24 40 32 3 32 3 32 9 62 12 34 36 152 6 3.  .
.
(a) Context-Encoding (b) Context Deltas (c) Bits Required(d) Compressed ArrayNumberof bitsin thisblockValue rankfor headerkeyHeaderkeyLogicaloffset ofthis blockTrue ifall !w inblock are0Figure 2: Compression using variable-length encoding.
(a) A snippet of an (uncompressed) context-encoded ar-ray.
(b) The context and word deltas.
(c) The numberof bits required to encode the context and word deltas aswell as the value ranks.
Word deltas use variable-lengthblock coding with k = 1, while context deltas and valueranks use k = 2.
(d) A snippet of the compressed encod-ing array.
The header is outlined in bold.of the key array used in our sorted array implemen-tation.
While we have already exploited the fact thatthe 24 word bits repeat in the previous section, wenote here that consecutive context offsets tend to bequite close together.
We found that for 5-grams, themedian difference between consecutive offsets wasabout 50, and 90% of offset deltas were smaller than10000.
By using a variable-length encoding to rep-resent these deltas, we should require far fewer than32 bits to encode context offsets.We used a very simple variable-length coding toencode offset deltas, word deltas, and value ranks.Our encoding, which is referred to as ?variable-length block coding?
in Boldi and Vigna (2005),works as follows: we pick a (configurable) radixr = 2k.
To encode a number m, we determine thenumber of digits d required to express m in base r.We write d in unary, i.e.
d ?
1 zeroes followed bya one.
We then write the d digits of m in base r,each of which requires k bits.
For example, usingk = 2, we would encode the decimal number 7 as010111.
We can choose k separately for deltas andvalue indices, and also tune these parameters to agiven language model.We found this encoding outperformed otherstandard prefix codes, including Golombcodes (Golomb, 1966; Church et al, 2007)262and Elias ?
and ?
codes.
We also experimentedwith the ?
codes of Boldi and Vigna (2005), whichmodify variable-length block codes so that theyare optimal for certain power law distributions.We found that ?
codes performed no better thanvariable-length block codes and were slightly morecomplex.
Finally, we found that Huffman codesoutperformed our encoding slightly, but came at amuch higher computational cost.3.4.2 Block CompressionWe could in principle compress the entire array ofkey/value pairs with the encoding described above,but this would render binary search in the array im-possible: we cannot jump to the mid-point of the ar-ray since in order to determine what key lies at a par-ticular point in the compressed bit stream, we wouldneed to know the entire history of offset deltas.Instead, we employ block compression, a tech-nique also used by Harb et al (2009) for smallerlanguage models.
In particular, we compress thekey/value array in blocks of 128 bytes.
At the be-ginning of the block, we write out a header consist-ing of: an explicit 64-bit key that begins the block;a 32-bit integer representing the offset of the headerkey in the uncompressed array;5 the number of bitsof compressed data in the block; and the variable-length encoding of the value rank of the header key.The remainder of the block is filled with as manycompressed key/value pairs as possible.
Once theblock is full, we start a new block.
See Figure 2 fora depiction.When we encode an offset delta, we store the deltaof the word portion of the key separately from thedelta of the context offset.
When an entire blockshares the same word portion of the key, we set asingle bit in the header that indicates that we do notencode any word deltas.To find a key in this compressed array, we firstperform binary search over the header blocks (whichare predictably located every 128 bytes), followedby a linear search within a compressed block.Using k = 6 for encoding offset deltas and k = 5for encoding value ranks, this COMPRESSED im-plementation stores Web1T in less than 3 bytes pern-gram, or about 10.2GB in total.
This is about5We need this because n-grams refer to their contexts usingarray offsets.6GB less than the storage required by Germann etal.
(2009), which is the best published lossless com-pression to date.4 Speeding up DecodingIn the previous section, we provided compact andefficient implementations of associative arrays thatallow us to query a value for an arbitrary n-gram.However, decoders do not issue language model re-quests at random.
In this section, we show that lan-guage model requests issued by a standard decoderexhibit two patterns we can exploit: they are highlyrepetitive, and also exhibit a scrolling effect.4.1 Exploiting Repetitive QueriesIn a simple experiment, we recorded all of thelanguage model queries issued by the Joshua de-coder (Li et al, 2009) on a 100 sentence test set.Of the 31 million queries, only about 1 million wereunique.
Therefore, we expect that keeping the re-sults of language model queries in a cache should beeffective at reducing overall language model latency.To this end, we added a very simple cache toour language model.
Our cache uses an array ofkey/value pairs with size fixed to 2b ?
1 for someinteger b (we used 24).
We use a b-bit hash func-tion to compute the address in an array where wewill always place a given n-gram and its fully com-puted language model score.
Querying the cache isstraightforward: we check the address of a key givenby its b-bit hash.
If the key located in the cache ar-ray matches the query key, then we return the valuestored in the cache.
Otherwise, we fetch the lan-guage model probability from the language modeland place the new key and value in the cache, evict-ing the old key in the process.
This scheme is oftencalled a direct-mapped cache because each key hasexactly one possible address.Caching n-grams in this way reduces overall la-tency for two reasons: first, lookup in the cache isextremely fast, requiring only a single evaluation ofthe hash function, one memory lookup to find thecache key, and one equality check on the key.
Incontrast, even our fastest (HASH) implementationmay have to perform multiple memory lookups andequality checks in order to resolve collisions.
Sec-ond, when calculating the probability for an n-gram263the cat + fell downthe cat fellcat fell down18569876 fell35764106 downLM0.76LM0.12LMLM0.760.12?the cat?
?cat fell?3576410ExplicitRepresentationContextEncodingFigure 3: Queries issued when scoring trigrams that arecreated when a state with LM context ?the cat?
combineswith ?fell down?.
In the standard explicit representationof an n-gram as list of words, queries are issued atom-ically to the language model.
When using a context-encoding, a query from the n-gram ?the cat fell?
returnsthe context offset of ?cat fell?, which speeds up the queryof ?cat fell down?.not in the language model, language models withback-off schemes must in general perform multiplequeries to fetch the necessary back-off information.Our cache retains the full result of these calculationsand thus saves additional computation.Federico and Cettolo (2007) also employ a cachein their language model implementation, thoughbased on traditional hash table cache with linearprobing.
Unlike our cache, which is of fixed size,their cache must be cleared after decoding a sen-tence.
We would not expect a large performance in-crease from such a cache for our faster models sinceour HASH implementation is already a hash tablewith linear probing.
We found in our experimentsthat a cache using linear probing provided marginalperformance increases of about 40%, largely be-cause of cached back-off computation, while oursimpler cache increases performance by about 300%even over our HASH LM implementation.
More tim-ing results are presented in Section 5.4.2 Exploiting Scrolling QueriesDecoders with integrated language models (Och andNey, 2004; Chiang, 2005) score partial translationhypotheses in an incremental way.
Each partial hy-pothesis maintains a language model context con-sisting of at most n ?
1 target-side words.
Whenwe combine two language model contexts, we createseveral new n-grams of length of n, each of whichgenerate a query to the language model.
These newWMT2010Order #n-grams1gm 4,366,3952gm 61,865,5883gm 123,158,7614gm 217,869,9815gm 269,614,330Total 676,875,055WEB1TOrder #n-grams1gm 13,588,3912gm 314,843,4013gm 977,069,9024gm 1,313,818,3545gm 1,176,470,663Total 3,795,790,711Table 1: Sizes of the two language models used in ourexperiments.n-grams exhibit a scrolling effect, shown in Fig-ure 3: the n ?
1 suffix words of one n-gram formthe n?
1 prefix words of the next.As discussed in Section 3, our LM implementa-tions can answer queries about context-encoded n-grams faster than explicitly encoded n-grams.
Withthis in mind, we augment the values stored in ourlanguage model so that for a key (wn, c(wn?11 )),we store the offset of the suffix c(wn2 ) as well asthe normal counts/probabilities.
Then, rather thanrepresent the LM context in the decoder as an ex-plicit list of words, we can simply store context off-sets.
When we query the language model, we getback both a language model score and context offsetc(w?n?11 ), where w?n?11 is the the longest suffix ofwn?11 contained in the language model.
We can thenquickly form the context encoding of the next queryby simply concatenating the new word with the off-set c(w?n?11 ) returned from the previous query.In addition to speeding up language modelqueries, this approach also automatically supports anequivalence of LM states (Li and Khudanpur, 2008):in standard back-off schemes, whenever we computethe probability for an n-gram (wn, c(wn?11 )) whenwn?11 is not in the language model, the result will bethe same as the result of the query (wn, c(w?n?11 ).
Itis therefore only necessary to store as much of thecontext as the language model contains instead ofall n ?
1 words in the context.
If a decoder main-tains LM states using the context offsets returnedby our language model, then the decoder will au-tomatically exploit this equivalence and the size ofthe search space will be reduced.
This same effect isexploited explicitly by some decoders (Li and Khu-danpur, 2008).264WMT2010LM Type bytes/ bytes/ bytes/ Totalkey value n-gram SizeSRILM-H ?
?
42.2 26.6GSRILM-S ?
?
33.5 21.1GHASH 5.6 6.0 11.6 7.5GSORTED 4.0 4.5 8.5 5.5GTPT ?
?
7.5??
4.7G?
?COMPRESSED 2.1 3.8 5.9 3.7GTable 2: Memory usages of several language model im-plementations on the WMT2010 language model.
A??
indicates that the storage in bytes per n-gram is re-ported for a different language model of comparable size,and the total size is thus a rough projection.5 Experiments5.1 DataTo test our LM implementations, we performedexperiments with two different language models.Our first language model, WMT2010, was a 5-gram Kneser-Ney language model which storesprobability/back-off pairs as values.
We trained thislanguage model on the English side of all French-English corpora provided6 for use in the WMT 2010workshop, about 2 billion tokens in total.
This datawas tokenized using the tokenizer.perl scriptprovided with the data.
We trained the languagemodel using SRILM.
We also extracted a count-based language model, WEB1T, from the Web1Tcorpus (Brants and Franz, 2006).
Since this data isprovided as a collection of 1- to 5-grams and asso-ciated counts, we used this data without further pre-processing.
The make up of these language modelsis shown in Table 1.5.2 Compression ExperimentsWe tested our three implementations (HASH,SORTED, and COMPRESSED) on the WMT2010language model.
For this language model, there areabout 80 million unique probability/back-off pairs,so v ?
36.
Note that here v includes both thecost per key of storing the value rank as well as the(amortized) cost of storing two 32 bit floating pointnumbers (probability and back-off) for each uniquevalue.
The results are shown in Table 2.6www.statmt.org/wmt10/translation-task.htmlWEB1TLM Type bytes/ bytes/ bytes/ Totalkey value n-gram SizeGzip ?
?
7.0 24.7GT-MPHR?
?
?
3.0 10.5GCOMPRESSED 1.3 1.6 2.9 10.2GTable 3: Memory usages of several language model im-plementations on the WEB1T.
A ?
indicates lossy com-pression.We compare against three baselines.
The first two,SRILM-H and SRILM-S, refer to the hash table-and sorted array-based trie implementations pro-vided by SRILM.
The third baseline is the Tightly-Packed Trie (TPT) implementation of Germann etal.
(2009).
Because this implementation is not freelyavailable, we use their published memory usage inbytes per n-gram on a language model of similarsize and project total usage.The memory usage of all of our models is con-siderably smaller than SRILM ?
our HASH imple-mentation is about 25% the size of SRILM-H, andour SORTED implementation is about 25% the sizeof SRILM-S. Our COMPRESSED implementationis also smaller than the state-of-the-art compressedTPT implementation.In Table 3, we show the results of our COM-PRESSED implementation on WEB1T and againsttwo baselines.
The first is compression of the ASCIItext count files using gzip, and the second is theTiered Minimal Perfect Hash (T-MPHR) of Guthrieand Hepple (2010).
The latter is a lossy compres-sion technique based on Bloomier filters (Chazelleet al, 2004) and additional variable-length encod-ing that achieves the best published compression ofWEB1T to date.
Our COMPRESSED implementa-tion is even smaller than T-MPHR, despite using alossless compression technique.
Note that since T-MPHR uses a lossy encoding, it is possible to re-duce the storage requirements arbitrarily at the costof additional errors in the model.
We quote here thestorage required when keys7 are encoded using 12-bit hash codes, which gives a false positive rate ofabout 2?12 =0.02%.7Guthrie and Hepple (2010) also report additional savingsby quantizing values, though we could perform the same quan-tization in our storage scheme.265LM Type No Cache Cache SizeCOMPRESSED 9264?73ns 565?7ns 3.7GSORTED 1405?50ns 243?4ns 5.5GHASH 495?10ns 179?6ns 7.5GSRILM-H 428?5ns 159?4ns 26.6GHASH+SCROLL 323?5ns 139?6ns 10.5GTable 4: Raw query speeds of various language modelimplementations.
Times were averaged over 3 runs onthe same machine.
For HASH+SCROLL, all queries wereissued to the decoder in context-encoded form, whichspeeds up queries that exhibit scrolling behaviour.
Notethat memory usage is higher than for HASH because westore suffix offsets along with the values for an n-gram.LM Type No Cache Cache SizeCOMPRESSED 9880?82s 1547?7s 3.7GSRILM-H 1120?26s 938?11s 26.6GHASH 1146?8s 943?16s 7.5GTable 5: Full decoding times for various language modelimplementations.
Our HASH LM is as fast as SRILMwhile using 25% of the memory.
Our caching also re-duces total decoding time by about 20% for our fastestmodels and speeds up COMPRESSED by a factor of 6.Times were averaged over 3 runs on the same machine.5.3 Timing ExperimentsWe first measured pure query speed by logging allLM queries issued by a decoder and measuringthe time required to query those n-grams in isola-tion.
We used the the Joshua decoder8 with theWMT2010 model to generate queries for the first100 sentences of the French 2008 News test set.
Thisproduced about 30 million queries.
We measured thetime9 required to perform each query in order withand without our direct-mapped caching, not includ-ing any time spent on file I/O.The results are shown in Table 4.
As expected,HASH is the fastest of our implementations, andcomparable10 in speed to SRILM-H, but using sig-8We used a grammar trained on all French-English dataprovided for WMT 2010 using the make scripts providedat http://sourceforge.net/projects/joshua/files/joshua/1.3/wmt2010-experiment.tgz/download9All experiments were performed on an Amazon EC2 High-Memory Quadruple Extra Large instance, with an Intel XeonX5550 CPU running at 2.67GHz and 8 MB of cache.10Because we implemented our LMs in Java, we issuedqueries to SRILM via Java Native Interface (JNI) calls, whichintroduces a performance overhead.
When called natively, wefound that SRILM was about 200 ns/query faster.
Unfortu-nificantly less space.
SORTED is slower but ofcourse more memory efficient, and COMPRESSEDis the slowest but also the most compact repre-sentation.
In HASH+SCROLL, we issued queriesto the language model using the context encoding,which speeds up queries substantially.
Finally, wenote that our direct-mapped cache is very effective.The query speed of all models is boosted substan-tially.
In particular, our COMPRESSED implementa-tion with caching is nearly as fast as SRILM-H with-out caching, and even the already fast HASH imple-mentation is 300% faster in raw query speed withcaching enabled.We also measured the effect of LM performanceon overall decoder performance.
We modifiedJoshua to optionally use our LM implementationsduring decoding, and measured the time requiredto decode all 2051 sentences of the 2008 Newstest set.
The results are shown in Table 5.
With-out caching, SRILM-H and HASH were comparablein speed, while COMPRESSED introduces a perfor-mance penalty.
With caching enabled, overall de-coder speed is improved for both HASH and SRILM-H, while the COMPRESSED implementation is onlyabout 50% slower that the others.6 ConclusionWe have presented several language model imple-mentations which are state-of-the-art in both sizeand speed.
Our experiments have demonstrated im-provements in query speed over SRILM and com-pression rates against state-of-the-art lossy compres-sion.
We have also described a simple caching tech-nique which leads to performance increases in over-all decoding time.AcknowledgementsThis work was supported by a Google Fellowship for thefirst author and by BBN under DARPA contract HR0011-06-C-0022.
We would like to thank David Chiang, ZhifeiLi, and the anonymous reviewers for their helpful com-ments.nately, it is not completely fair to compare our LMs against ei-ther of these numbers: although the JNI overhead slows downSRILM, implementing our LMs in Java instead of C++ slowsdown our LMs.
In the tables, we quote times which includethe JNI overhead, since this reflects the true cost to a decoderwritten in Java (e.g.
Joshua).266ReferencesPaolo Boldi and Sebastiano Vigna.
2005.
Codes for theworld wide web.
Internet Mathematics, 2.Thorsten Brants and Alex Franz.
2006.
Google web1t5-gram corpus, version 1.
In Linguistic Data Consor-tium, Philadelphia, Catalog Number LDC2006T13.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing.Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, andAyellet Tal.
2004.
The Bloomier filter: an efficientdata structure for static support lookup tables.
In Pro-ceedings of the fifteenth annual ACM-SIAM sympo-sium on Discrete algorithms.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In The Annual Con-ference of the Association for Computational Linguis-tics.Kenneth Church, Ted Hart, and Jianfeng Gao.
2007.Compressing trigram language models with golombcoding.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.Marcello Federico and Mauro Cettolo.
2007.
Efficienthandling of n-gram language models for statistical ma-chine translation.
In Proceedings of the Second Work-shop on Statistical Machine Translation.Edward Fredkin.
1960.
Trie memory.
Communicationsof the ACM, 3:490?499, September.Ulrich Germann, Eric Joanis, and Samuel Larkin.
2009.Tightly packed tries: how to fit large models into mem-ory, and make them load fast, too.
In Proceedings ofthe Workshop on Software Engineering, Testing, andQuality Assurance for Natural Language Processing.S.
W. Golomb.
1966.
Run-length encodings.
IEEETransactions on Information Theory, 12.David Guthrie and Mark Hepple.
2010.
Storing the webin memory: space efficient language models with con-stant time retrieval.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing.Boulos Harb, Ciprian Chelba, Jeffrey Dean, and SanjayGhemawat.
2009.
Back-off language model compres-sion.
In Proceedings of Interspeech.Bo-June Hsu and James Glass.
2008.
Iterative languagemodel estimation: Efficient data structure and algo-rithms.
In Proceedings of Interspeech.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for smt.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
In Pro-ceedings of the Second Workshop on Syntax and Struc-ture in Statistical Translation.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenN.
G. Thornton, Jonathan Weese, and Omar F. Zaidan.2009.
Joshua: an open source toolkit for parsing-based machine translation.
In Proceedings of theFourth Workshop on Statistical Machine Translation.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computationl Linguistics, 30:417?449, Decem-ber.Andreas Stolcke.
2002.
SRILM: An extensible languagemodeling toolkit.
In Proceedings of Interspeech.E.
W. D. Whittaker and B. Raj.
2001.
Quantization-based language model compression.
In Proceedingsof Eurospeech.267
