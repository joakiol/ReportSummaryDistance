Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33?40Manchester, August 2008Mixed-Source Multi-Document Speech-to-Text SummarizationRicardo RibeiroINESC ID Lisboa/ISCTE/ISTSpoken Language Systems LabRua Alves Redol, 91000-029 Lisboa, Portugalrdmr@l2f.inesc-id.ptDavid Martins de MatosINESC ID Lisboa/ISTSpoken Language Systems LabRua Alves Redol, 91000-029 Lisboa, Portugaldavid@l2f.inesc-id.ptAbstractSpeech-to-text summarization systemsusually take as input the output of anautomatic speech recognition (ASR)system that is affected by issues likespeech recognition errors, disfluencies, ordifficulties in the accurate identificationof sentence boundaries.
We propose theinclusion of related, solid backgroundinformation to cope with the difficultiesof summarizing spoken language and theuse of multi-document summarizationtechniques in single document speech-to-text summarization.
In this work, weexplore the possibilities offered by pho-netic information to select the backgroundinformation and conduct a perceptualevaluation to better assess the relevance ofthe inclusion of that information.
Resultsshow that summaries generated usingthis approach are considerably better thanthose produced by an up-to-date latentsemantic analysis (LSA) summarizationmethod and suggest that humans prefersummaries restricted to the informationconveyed in the input source.1 IntroductionNews have been the subject of summarizationfor a long time, demonstrating the importanceof both the subject and the process.
Systemslike NewsInEssence (Radev et al, 2005), News-blaster (McKeown et al, 2002), or even Googlec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.News substantiate this relevance that is also sup-ported by the spoken language scenario, wheremost speech summarization systems concentrateon broadcast news (McKeown et al, 2005).
Nev-ertheless, although the pioneering efforts on sum-marization go back to the work of Luhn (1958)and Edmundson (1969), it is only after the re-naissance of summarization as a research area ofgreat activity?following up on the Dagstuhl Sem-inar (Endres-Niggemeyer et al, 1995)?that thefirst multi-document news summarization system,SUMMONS (McKeown and Radev, 1995), makesits breakthrough (Radev et al, 2005; Sp?arck Jones,2007).
In what concerns speech summarization,the state of affairs is more problematic: news sum-marization systems appeared later and still focusonly on single document summarization (McKe-own et al, 2005).
In fact, while text summarizationhas attained some degree of success (Hovy, 2003;McKeown et al, 2005; Sp?arck Jones, 2007) due tothe considerable body of work, speech summariza-tion still requires further research, both in speechand text analysis, in order to overcome the specificchallenges of the task (McKeown et al, 2005; Fu-rui, 2007).
Issues like speech recognition errors,disfluencies, and difficulties in accurately identi-fying sentence boundaries must be taken into ac-count when summarizing spoken language.
How-ever, if on the one hand, recognition errors seemnot to have a considerable impact on the summa-rization task (Murray et al, 2006; Murray et al,2005), on the other hand, spoken language summa-rization systems often explore ways of minimizingthat impact (Zechner and Waibel, 2000; Hori et al,2003; Kikuchi et al, 2003).We argue that by including related solid back-ground information from a different source lessprone to this kind of errors (e.g., a textual source)33in the summarization process, we are able to re-duce the influence of recognition errors on the re-sulting summary.
To support this argument, we de-veloped a new approach to speech-to-text summa-rization that combines information from multipleinformation sources to produce a summary drivenby the spoken language document to be summa-rized.
The idea mimics the natural human behav-ior, in which information acquired from differentsources is used to build a better understanding ofa given topic (Wan et al, 2007).
Furthermore, webuild on the conjecture that this background infor-mation is often used by humans to overcome per-ception difficulties.
In that sense, one of our goalsis also to understand what is expected in a sum-mary: a comprehensive, shorter, text that addressesthe same subject of the input source to be summa-rized (possibly introducing new information); or atext restricted to the information conveyed in theinput source.This work explores the use of phonetic domaininformation to overcome speech recognition errorsand disfluencies.
Instead of using the traditionaloutput of the ASR module, we use the phonetictransliteration of the output and compare it to thephonetic transliteration of solid background infor-mation.
This enables the use of text, related to theinput source, free from the common speech recog-nition issues, in further processing.We use broadcast news as a case study andnews stories from online newspapers provide thebackground information.
Media monitoring sys-tems, used to transcribe and disseminate news,provide an adequate framework to test the pro-posed method.This document is organized as follows: sec-tion 2 briefly introduces the related work; section3 presents a characterization of the speech-to-textsummarization problem and how we propose toaddress it; section 4 explicits our use of phoneticdomain information, given the previously definedcontext; the next section describes the case study,including the experimental set up and results; con-clusions close the document.2 Related WorkMcKeown et al (2005) depict spoken languagesummarization as a much harder task than textsummarization.
In fact, the previously enumeratedproblems that make speech summarization sucha difficult task constrain the applicability of textsummarization techniques to speech summariza-tion (although in the presence of planned speech,as it partly happens in the broadcast news domain,that portability is more feasible (Christensen et al,2003)).
On the other hand, speech offers possibili-ties like the use of prosody and speaker identifica-tion to ascertain relevant content.Furui (2007) identifies three main approachesto speech summarization: sentence extraction-based methods, sentence compaction-based meth-ods, and combinations of both.Sentence extractive methods comprehend, es-sentially, methods like LSA (Gong and Liu,2001), Maximal Marginal Relevance (Carbonelland Goldstein, 1998), and feature-based meth-ods (Edmundson, 1969).
Feature-based methodscombine several types of features: current workuses lexical, acoustic/prosodic, structural, and dis-course features to summarize documents from do-mains like broadcast news or meetings (Maskeyand Hirschberg, 2005; Murray et al, 2006; Ribeiroand de Matos, 2007).
Even so, spoken languagesummarization is still quite distant from text sum-marization in what concerns the use of discoursefeatures, and shallow approaches is what can befound in state-of-the-art work such as the one pre-sented by Maskey and Hirschberg (2005) or Mur-ray et al (2006).
Sentence compaction methodsare based on word removal from the transcription,with recognition confidence scores playing a ma-jor role (Hori et al, 2003).
A combination of thesetwo types of methods was developed by Kikuchiet al (2003), where summarization is performedin two steps: first, sentence extraction is donethrough feature combination; second, compactionis done by scoring the words in each sentence andthen a dynamic programming technique is appliedto select the words that will remain in the sentenceto be included in the summary.3 Problem CharacterizationSummarization can be seen as a reductive transfor-mation ?
that, given an input source I , produces asummary S:S = ?
(I),where len(S) < len(I) and inf (S) is as closeas possible of inf (I); len() is the length of thegiven input and inf () is the information conveyedby its argument.The problem is that in order to compute S, weare not using I , but?I , a noisy representation of I .34Thus, we are computing?S, which is a summaryaffected by the noise present in?I:?S = ?
(?I).This means thatinf (?S) ?
inf (S) ?
inf (I), whereaslen(?S) ?
len(S) < len(I).Our argument is that using a similar reductivetransformation ?, where solid background infor-mation B is also given as input, it is possible tocompute a summary?S:?S = ?
(?I,B), such thatinf (?S) ?
(inf (?S) ?
inf (S)) ?
inf (I), withlen(?S) ?
len(?S) ?
len(S) < len(I).As seen in section 2, the most common methodto perform these transformations is by selectingsentences (or extracts) from the corresponding in-put sources.Thus, let the input source representation?I becomposed by a sequence of extracts ei,?I = e1, e2, .
.
.
, enand the background information be defined as asequence of sentencesB = s1, s2, .
.
.
, sm.The proposed method consists of selecting sen-tences siform the background information B suchthatsim(si, ej) < ?
?
0 ?
i ?
m ?
0 ?
j ?
n,with sim() being a similarity function and ?
anadequate threshold.
The difficulty lies in definingthe function and the threshold.4 Working in the phonetic domainThe approach we introduce minimizes the effectsof recognition errors through the selection, frompreviously determined background knowledge, ofsentence-like units close to the ones of the newsstory transcription.
In order to select sentence-likeunits, while diminishing recognition problems, wecompute the similarity between them at the pho-netic level.
The estimation of the threshold isbased on the distance, measured in the phoneticFeature ValuesType vowel, consonantVowel length short, long, diphthong,schwaVowel height high, mid, lowVowel frontness front mid backLip rounding yes, noConsonant type stop, fricative, affricative,nasal, liquidPlace of articulation labial, alveolar, palatal,labio-dental, dental, velarConsonant voicing yes, noTable 1: Phone features.domain, between the output of the ASR and itshand-corrected version.The selection of sentences from the backgroundinformation is based on the alignment cost of thephonetic transcriptions of sentences from the inputsource and sentence from the background informa-tion.
Sentences from the background informationwith alignment costs below the estimated thresholdare selected to be used in summary generation.4.1 Similarity Between SegmentsThere are several ways to compute phonetic simi-larity.
Kessler (2005) states that phonetic distancecan be seen as, among other things, differencesbetween acoustic properties of the speech stream,differences in the articulatory positions during pro-duction, or as the perceptual distance between iso-lated sounds.
Choosing a way to calculate phoneticdistance is a complex process.The phone similarity function used in this pro-cess is based on a model of phone production,where the phone features correspond to the articu-latory positions during production: the greater thematching between phone features, the smaller thedistance between phones.
The phone features usedare described in table 1.The computation of the similarity betweensentence-like units is based on the alignment ofthe phonetic transcriptions of the given segments.The generation of the possible alignments and theselection of the best alignment is done throughthe use of Weighted Finite-State Transducers (WF-STs) (Mohri, 1997; Paulo and Oliveira, 2002).354.2 Threshold Estimation ProcessTo estimate the threshold to be used in the sentenceselection process, we use the algorithm presentedin figure 1.
The procedure consists of comparingautomatic transcriptions and their hand-correctedversions: the output is the average difference be-tween the submitted inputs.PhonetictransliterationPhonetictransliterationSentence segmentedASR outputManual transcriptionProjection of thesentences ofthe ASR ouputover the manualtranscriptionSentence segmentedManual transcriptionSentence-by-sentencedistancecalculationFigure 1: Threshold estimation process.The idea is that the phonetic distance betweenthe automatic transcription and its hand-correctedversion would be similar to the phonetic distancebetween the automatic transcription and the back-ground information.
Even though this heuristicmay appear naif, we believe it is adequate as arough approach, considering the target material(broadcast news).5 A Case Study Using Broadcast News5.1 Media Monitoring SystemSSNT (Amaral et al, 2007) is a system for selec-tive dissemination of multimedia contents, work-ing primarily with Portuguese broadcast news ser-vices.
The system is based on an ASR mod-ule, that generates the transcriptions used bythe topic segmentation, topic indexing, and ti-tle&summarization modules.
User profiles enablethe system to deliver e-mails containing relevantnews stories.
These messages contain the nameof the news service, a generated title, a summary,a link to the corresponding video segment, and aclassification according to a thesaurus used by thebroadcasting company.Preceding the speech recognition module, an au-dio preprocessing module, based on Multi-layerPerceptrons, classifies the audio in accordance toseveral criteria: speech/non-speech, speaker seg-mentation and clustering, gender, and backgroundconditions.The ASR module, based on a hybrid speechrecognition system that combines Hidden MarkovModels with Multi-layer Perceptrons, with an av-erage word error rate of 24% (Amaral et al, 2007),greatly influences the performance of the subse-quent modules.The topic segmentation and topic indexingmodules were developed by Amaral and Tran-coso (2004).
Topic segmentation is based on clus-tering and groups transcribed segments into sto-ries.
The algorithm relies on a heuristic derivedfrom the structure of the news services: each storystarts with a segment spoken by the anchor.
Thismodule achieved an F -measure of 68% (Amaralet al, 2007).
The main problem identified by theauthors was boundary deletion: a problem whichimpacts the summarization task.
Topic indexing isbased on a hierarchically organized thematic the-saurus provided by the broadcasting company.
Thehierarchy has 22 thematic areas on the first level,for which the module achieved a correctness of91.4% (Amaral et al, 2006; Amaral et al, 2007).Batista et al (2007) inserted a module for re-covering punctuation marks, based on maximumentropy models, after the ASR module.
The punc-tuation marks addressed were the ?full stop?
and?comma?, which provide the sentence units nec-essary for use in the title&summarization mod-ule.
This module achieved an F -measure of 56%and SER (Slot Error Rate, the measure commonlyused to evaluate this kind of task) of 0.74.Currently, the title&summarization module pro-duces a summary composed by the first n sen-tences, as detected by the previous module, of eachnews story and a title (the first sentence).5.2 CorporaTwo corpora were used in this experiment: abroadcast news corpus, the subject of our summa-rization efforts; and a written newspaper corpus,used to select the background information.36Corpus Stories SUs Tokens Durationtrain 184 2661 57063 5htest 26 627 7360 1hTable 2: Broadcast news corpus composition.The broadcast news corpus is composed by 6Portuguese news programs, and exists in two ver-sions: an automatically processed one, and a hand-corrected one.
Its composition (number of stories,number of sentence-like units (SUs), number of to-kens, and duration) is detailed in table 2.
To es-timate the threshold used for the selection of thebackground information, 5 news programs wereused.
The last one was used for evaluation.The written newspaper corpus consists of theonline version a Portuguese newspaper, down-loaded daily from the Internet.
In this experiment,three editions of the newspaper were used, corre-sponding to the day and the two previous days ofthe news program to be summarized.
The corpusis composed by 135 articles, 1418 sentence-likeunits, and 43102 tokens.5.3 The Summarization ProcessThe summarization process we implemented ischaracterized by the use of LSA to compute therelevance of the extracts (sentence-like units) ofthe given input source.LSA is based on the singular vector decomposi-tion (SVD) of the term-sentence frequency m?
nmatrix, M .
U is an m ?
n matrix of left singularvectors; ?
is the n?
n diagonal matrix of singularvalues; and, V is the n?n matrix of right singularvectors (only possible if m ?
n):M = U?VTThe idea behind the method is that the decom-position captures the underlying topics of the doc-ument by means of co-occurrence of terms (the la-tent semantic analysis), and identifies the best rep-resentative sentence-like units of each topic.
Sum-mary creation can be done by picking the best rep-resentatives of the most relevant topics accordingto a defined strategy.For this summarization process, we imple-mented a module following the original ideas ofGong and Liu (2001) and the ones of Murray, Re-nals, and Carletta (2005) for solving dimensional-ity problems, and using, for matrix operations, theGNU Scientific Library1.5.4 Experimental ResultsOur main objective was to understand if it is pos-sible to select relevant information from back-ground information that could improve the qualityof speech-to-text summaries.
To assess the valid-ity of this hypothesis, five different processes ofgenerating a summary were considered.
To bet-ter analyze the influence of the background in-formation, all automatic summarization methodsare based on the up-to-date LSA method previ-ously described: one taking as input only the newsstory to be summarized (Simple) and used as base-line; other taking as input only the selected back-ground information (Background only); and, thelast one, using both the news story and the back-ground information (Background + News).
Theother two processes were human: extractive (usingonly the news story) and abstractive (understand-ing the news story and condensing it by meansof paraphrase).
Since the abstractive summarieshad already been created, summary size was de-termined by their size (which means creating sum-maries using a compression rate of around 10% ofthe original size).As mentioned before, the whole summariza-tion process begins with the selection of the back-ground information.
Using the threshold estimatedas described in section 4.2 and the method de-scribed in section 4.1 to compute similarity be-tween sentence-like units, no background informa-tion was selected for 11 of the 26 news stories ofthe test corpus.
For the remaining 15 news sto-ries, summaries were generated using the three au-tomatic summarization strategies described before.In what concerns the evaluation process, al-though ROUGE (Lin, 2004) is the most commonevaluation metric for the automatic evaluation ofsummarization, since our approach might intro-duce in the summary information that it is notpresent in the original input source, we found that ahuman evaluation was more adequate to assess therelevance of that additional information.
A percep-tual evaluation is also adequate to assess the per-ceive quality of the summaries and a better indica-tor of the what is expected to be in a summary.We asked an heterogeneous group of sixteenpeople to evaluate the summaries created for the15 news stories for which background information1http://www.gnu.org/software/gsl/370 20 40 60 80 100 120Simple (News only)Background onlyBackground + NewsHuman ExtractiveHuman Abstractive ns00ns01ns02ns03ns04ns05ns06ns07ns08ns09ns10ns11ns12ns13ns14Figure 2: Overall results for each summary cre-ation method (nsnn identifies a news story).was selected.
Each evaluator was given, for eachstory, the news story itself (without background in-formation) and five summaries, corresponding tothe five different methods presented before.
Theevaluation procedure consisted in identifying thebest summary and in the classification of eachsummary (1?5, 5 is better) according to its contentand readability (which covers issues like grammat-icality, existence of redundant information, or en-tity references (Nenkova, 2006)).0%10%20%30%40%50%60%70%80%90%100%ns00 ns01 ns02 ns03 ns04 ns05 ns06 ns07 ns08 ns09 ns10 ns11 ns12 ns13 ns14Simple (News only)Background onlyBackground + NewsHuman ExtractiveHuman AbstractiveFigure 3: Relative results for each news story(nsnn identifies a news story; stack order is inverseof legend order).Surprisingly enough (see figures 2 and 3), ingeneral, the extractive human summaries were pre-ferred over the abstractive ones.
Moreover, thesummaries generated automatically using back-ground information (exclusively or not) were alsoselected as best summary (over the human createdones) a non-negligible number of times.
The poor-est performance was attained, as expected, by thesimple LSA summarizer, only preferred on twonews stories for which all summaries were verysimilar.
The results of the two approaches usingbackground information were very close, a resultthat can be explained by the fact the summariesgenerated by these two approaches were equal for11 of the 15 news stories (in the remaining 4, theaverage distribution was 31.25% from the newsstory versus 68.75% from the background infor-mation).Figure 4 further discriminates the results interms of content and readability.0.000.501.001.502.002.503.003.504.004.505.00Simple (Newsonly)BackgroundonlyBackground +NewsHumanExtractiveHumanAbstractivecontent readabilityFigure 4: Average of the content and readabilityscores for each summary creation method.Regarding content, the results suggest that thechoice of the best summary is highly correlatedwith its content, as the average content scoresmimic the overall ones of figure 2.
In what con-cerns readability, the summaries generated usingbackground information achieved the best results.The reasons underlying these results are that thenewspaper writing is naturally better planned thanspeech and that speech transcriptions are affectedby the several problems described before (and theoriginal motivation for the work), hence the ideaof using them as background information.
How-ever, what is odd is that the result obtained bythe human abstractive summary creation methodis worse than the ones obtained by automaticgeneration using background information, whichcould suffer from coherence and cohesion prob-lems.
One possible explanation is that the humanabstractive summaries tend to mix both informa-38tive and indicative styles of summary.0.000.200.400.600.801.001.20Simple (Newsonly)BackgroundonlyBackground +NewsHumanExtractiveHumanAbstractivecontent readabilityFigure 5: Standard deviation of the content andreadability scores.Figure 5 presents the standard deviation for con-tent and readability scores: concerning content,automatically generated summaries using back-ground information achieved the highest standarddeviation scores (see also figure 6 for a samplestory).
That is in part supported by some commen-taries made by the human evaluators on whethera summary should contain information that is notpresent in the input source.
This aspect and the ob-tained results, suggest that this issue should be fur-ther analyzed, possibly using an extrinsic evalua-tion setup.
On the other hand, readability standarddeviation scores show that there is a considerableagreement in what concerns this criterion.0.000.501.001.502.002.503.003.504.004.505.00Simple (Newsonly)BackgroundonlyBackground +NewsHumanExtractiveHumanAbstractiveContent (avg) Readability (avg)Content (stdev) Readability (stdev)Figure 6: Average and standard deviation of thecontent and readability scores for one news story.6 ConclusionsWe present a new approach to speech summariza-tion that goes in the direction of the integration oftext and speech analysis, as suggested by McKe-own et al (2005).
The main idea is the inclusionof related, solid background information to copewith the difficulties of summarizing spoken lan-guage and the use of multi-document summariza-tion techniques in single document speech-to-textsummarization.
In this work, we explore the pos-sibilities offered by phonetic information to selectthe background information and conducted a per-ceptual evaluation to assess the relevance of the in-clusion of that information.The results obtained show that the human eval-uators preferred human extractive summaries overhuman abstractive summaries.
Moreover, simpleLSA summaries attained the poorest results both interms of content and readability, while human ex-tractive summaries achieved the best performancein what concerns content, and a considerably bet-ter performance than simple LSA in what concernsreadability.
This suggests that it is sill relevant topursue new methods for relevance estimation.
Onthe other hand, automatically generated summariesusing background information were significantlybetter than simple LSA.
This indicates that back-ground information is a viable way to increase thequality of automatic summarization systems.ReferencesAmaral, R. and I. Trancoso.
2004.
Improving the TopicIndexation and Segmentation Modules of a MediaWatch System.
In Proceedings of INTERSPEECH2004 - ICSLP, pages 1609?1612.
ISCA.Amaral, R., H. Meinedo, D. Caseiro, I. Trancoso, andJ.
P. Neto.
2006.
Automatic vs. Manual Topic Seg-mentation and Indexation in Broadcast News.
InProc.
of the IV Jornadas en Tecnologia del Habla.Amaral, R., H. Meinedo, D. Caseiro, I. Trancoso, andJ.
P. Neto.
2007.
A Prototype System for SelectiveDissemination of Broadcast News in European Por-tuguese.
EURASIP Journal on Advances in SignalProcessing, 2007.Batista, F., D. Caseiro, N. J. Mamede, and I. Tran-coso.
2007.
Recovering Punctuation Marks for Au-tomatic Speech Recognition.
In Proceedings of IN-TERSPEECH 2007, pages 2153?2156.
ISCA.Carbonell, J. and J. Goldstein.
1998.
The Use of MMR,Diversity-Based Reranking for Reordering Docu-ments and Producing Summaries.
In SIGIR 1998:Proceedings of the 21stAnnual International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 335?336.
ACM.Christensen, H., Y. Gotoh, B. Kolluru, and S. Renals.2003.
Are Extractive Text Summarisation Tech-niques Portable To Broadcast News?
In Proceedings39of the IEEE Workshop on Automatic Speech Recog-nition and Understanding, pages 489?494.
IEEE.Edmundson, H. P. 1969.
New methods in automaticabstracting.
Journal of the Association for Comput-ing Machinery, 16(2):264?285.Endres-Niggemeyer, B., J. R. Hobbs, and K. Sp?arckJones, editors.
1995.
Summarizing Textfor Intelligent Communication?Dagstuhl-Seminar-Report 79.
IBFI.Furui, S. 2007.
Recent Advances in Automatic SpeechSummarization.
In Proceedings of the 8thConfer-ence on Recherche d?Information Assist?ee par Or-dinateur (RIAO).
Centre des Hautes?Etudes Interna-tionales d?Informatique Documentaire.Gong, Y. and X. Liu.
2001.
Generic Text Summariza-tion Using Relevance Measure and Latent SemanticAnalysis.
In SIGIR 2001: Proceedings of the 24stAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval,pages 19?25.
ACM.Hori, T., C. Hori, and Y. Minami.
2003.
Speech Sum-marization using Weighted Finite-State Transducers.In Proceedings of the 8thEUROSPEECH - INTER-SPEECH 2003, pages 2817?2820.
ISCA.Hovy, E., 2003.
The Oxford Handbook of Compu-tational Linguistics, chapter Text Summarization,pages 583?598.
Oxford University Press.Kessler, B.
2005.
Phonetic comparison algo-rithms.
Transactions of the Philological Society,103(2):243?260.Kikuchi, T., S. Furui, and C. Hori.
2003.
Two-stage Automatic Speech Summarization by Sen-tence Extraction and Compaction.
In Proceedingsof the ISCA & IEEE Workshop on SpontaneousSpeech Processing and Recognition (SSPR-2003),pages 207?210.
ISCA.Lin, C. 2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81.
ACL.Luhn, H. P. 1958.
The Automatic Creation of Litera-ture Abstracts.
IBM Journal of Research and Devel-opment, 2(2):159?165.Maskey, S. and J. Hirschberg.
2005.
Comparing Lexi-cal, Acoustic/Prosodic, Strucural and Discourse Fea-tures for Speech Summarization.
In Proceedingsof the 9thEUROSPEECH - INTERSPEECH 2005,pages 621?624.
ISCA.McKeown, K. R. and D. Radev.
1995.
GeneratingSummaries of Multiple News Articles.
In SIGIR1995: Proceedings of the 18thAnnual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, pages 74?82.
ACM.McKeown, K. R., R. Barzilay, D. Evans, V. Hatzi-vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,B.
Schiffman, and S. Sigelman.
2002.
Track-ing and Summarizing News on a Daily Basis withColumbia?s Newsblaster.
In Proc.
of the 2ndInter-national Conference on Human Language Technol-ogy Research, pages 280?285.
Morgan Kaufmann.McKeown, K. R., J. Hirschberg, M. Galley, andS.
Maskey.
2005.
From Text to Speech Summa-rization.
In 2005 IEEE International Conference onAcoustics, Speech, and Signal Processing.
Proceed-ings, volume V, pages 997?1000.
IEEE.Mohri, M. 1997.
Finite-State Transducers in Languageand Speech Processing.
Computational Linguistics,23(2):269?311.Murray, G., S. Renals, and J. Carletta.
2005.
ExtractiveSummarization of Meeting Records.
In Proceedingsof the 9thEUROSPEECH - INTERSPEECH 2005,pages 593?596.
ISCA.Murray, G., S. Renals, J. Carletta, and J. Moore.2006.
Incorporating Speaker and Discourse Featuresinto Speech Summarization.
In Proceedings of theHLT/NAACL, pages 367?374.
ACL.Nenkova, A.
2006.
Summarization Evaluation for Textand Speech: Issues and Approaches.
In Proceedingsof INTERSPEECH 2006 - ICSLP, pages 1527?1530.ISCA.Paulo, S. and L. C. Oliveira.
2002.
Multilevel Annota-tion Of Speech Signals Using Weighted Finite StateTransducers.
In Proc.
of the 2002 IEEE Workshopon Speech Synthesis, pages 111?114.
IEEE.Radev, D., J. Otterbacher, A. Winkel, and S. Blair-Goldensohn.
2005.
NewsInEssence: SummarizingOnline News Topics.
Communications of the ACM,48(10):95?98.Ribeiro, R. and D. M. de Matos.
2007.
Extractive Sum-marization of Broadcast News: Comparing Strate-gies for European Portuguese.
In Text, Speech andDialogue ?
10thInternational Conference.
Proceed-ings, volume 4629 of Lecture Notes in Computer Sci-ence (Subseries LNAI), pages 115?122.
Springer.Sp?arck Jones, K. 2007.
Automatic summarising: Thestate of the art.
Information Processing and Man-agement, 43:1449?1481.Wan, X., J. Yang, and J. Xiao.
2007.
CollabSum: Ex-ploiting Multiple Document Clustering for Collabo-rative Single Document Summarizations.
In SIGIR2007: Proc.
of the 30thAnnual International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 143?150.
ACM.Zechner, K. and A. Waibel.
2000.
Minimizing WordError Rate in Textual Summaries of Spoken Lan-guage.
In Proceedings of the 1stconference of theNorth American chapter of the ACL, pages 186?193.Morgan Kaufmann.40
