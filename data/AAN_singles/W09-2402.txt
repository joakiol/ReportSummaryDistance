Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2?9,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMaking Sense of Word Sense VariationRebecca J. Passonneau and Ansaf Salleb-AouissiCenter for Computational Learning SystemsColumbia UniversityNew York, NY, USA(becky@cs|ansaf@ccls).columbia.eduNancy IdeDepartment of Computer ScienceVassar CollegePoughkeepsie, NY, USAide@cs.vassar.eduAbstractWe present a pilot study of word-sense an-notation using multiple annotators, relativelypolysemous words, and a heterogenous cor-pus.
Annotators selected senses for words incontext, using an annotation interface that pre-sented WordNet senses.
Interannotator agree-ment (IA) results show that annotators agreewell or not, depending primarily on the indi-vidual words and their general usage proper-ties.
Our focus is on identifying systematicdifferences across words and annotators thatcan account for IA variation.
We identify threelexical use factors: semantic specificity of thecontext, sense concreteness, and similarity ofsenses.
We discuss systematic differences insense selection across annotators, and presentthe use of association rules to mine the datafor systematic differences across annotators.1 IntroductionOur goal is to grapple seriously with the naturalsense variation arising from individual differences inword usage.
It has been widely observed that usagefeatures such as vocabulary and syntax vary acrosscorpora of different genres and registers (Biber,1995), and that serve different functions (Kittredgeet al, 1991).
Still, we are far from able to pre-dict specific morphosyntactic and lexical variationsacross corpora (Kilgarriff, 2001), much less quan-tify them in a way that makes it possible to applythe same analysis tools (taggers, parsers) without re-training.
In comparison to morphosyntactic proper-ties of language, word and phrasal meaning is fluid,and to some degree, generative (Pustejovsky, 1991;Nunberg, 1979).
Based on our initial observationsfrom a word sense annotation task for relatively pol-ysemous words, carried out by multiple annotatorson a heterogeneous corpus, we hypothesize that dif-ferent words lead to greater or lesser interannota-tor agreement (IA) for reasons that in the long runshould be explicitly modelled in order for NaturalLanguage Processing (NLP) applications to handleusage differences more robustly.
This pilot study isa step in that direction.We present related work in the next section, thendescribe the annotation task in the following one.
InSection 4, we present examples of variation in agree-ment on a matched subset of words.
In Section 5we discuss why we believe the observed variationdepends on the words and present three lexical usefactors we hypothesize to lead to greater or lesserIA.
In Section 6, we use association rules to mineour data for systematic differences among annota-tors, thus to explain the variations in IA.
We con-clude with a summary of our findings goals.2 Related WorkThere has been a decade-long community-wide ef-fort to evaluate word sense disambiguation (WSD)systems across languages in the four Senseval ef-forts (1998, 2001, 2004, and 2007, cf.
(Kilgarriff,1998; Pedersen, 2002a; Pedersen, 2002b; Palmeret al, 2005)), with a corollary effort to investi-gate the issues pertaining to preparation of man-ually annotated gold standard corpora tagged forword senses (Palmer et al, 2005).
Differences in IAand system performance across part-of-speech havebeen examined, as in (Ng et al, 1999; Palmer et al,2Word POS No.
senses No.
occurrencesfair Adj 10 463long Adj 9 2706quiet Adj 6 244land Noun 11 1288time Noun 10 21790work Noun 7 5780know Verb 11 10334say Verb 11 20372show Verb 12 11877tell Verb 8 4799Table 1: Ten Words2005).
Pedersen (Pedersen, 2002a) examines varia-tion across individual words in evaluating WSD sys-tems, but does not attempt to explain it.Factors that have been proposed as affectinghuman or system sense disambiguation includewhether annotators are allowed to assign multilabels(Veronis, 1998; Ide et al, 2002; Passonneau et al,2006), the number or granularity of senses (Ng et al,1999), merging of related senses (Snow et al, 2007),sense similarity (Chugur et al, 2002), sense perplex-ity (Diab, 2004), entropy (Diab, 2004; Palmer etal., 2005), and in psycholinguistic experiments, re-actions times required to distinguish senses (Kleinand Murphy, 2002; Ide and Wilks, 2006).With respect to using multiple annotators, Snowet al included disambiguation of the wordpresident?a relatively non-polysemous word withthree senses?in a set of tasks given to Amazon Me-chanical Turkers, aimed at determining how to com-bine data from multiple non-experts for machinelearning tasks.
The word sense task comprised 177sentences taken from the SemEval Word Sense Dis-ambiguation Lexical Sample task.
Majority votingamong three annotators achieve 99% accuracy.3 The Annotation TaskThe Manually Annotated Sub-Corpus (MASC)project is creating a small, representative corpusof American English written and spoken textsdrawn from the Open American National Cor-pus (OANC).1 The MASC corpus includes hand-validated or manually produced annotations for a va-riety of linguistic phenomena.
One of the goals of1http://www.anc.orgFigure 1: MASC word sense annotation toolthe project is to support efforts to harmonize Word-Net (Miller et al, 1993) and FrameNet (Ruppen-hofer et al, 2006), in order to bring the sense distinc-tions each makes into better alignment.
As a start-ing sample, we chose ten fairly frequent, moderatelypolysemous words for sense tagging, targeting inparticular words that do not yet exist in FrameNet, aswell as words with different numbers of senses in thetwo resources.
The ten words with part of speech,number of senses, and occurrences in the OANCare shown in Table 1.
One thousand occurrencesof each word , including all occurrences appear-ing in the MASC subset and others semi-randomly2chosen from the remainder of the 15 million wordOANC, were annotated by at least one annotator ofsix undergraduate annotators at Vassar College andColumbia University.Fifty occurrences of each word in context weresense-tagged by all six annotators for the in-depthstudy of inter-annotator agreement (IA) reportedhere.
We have just finished collecting annotationsof fifty new occurrences.
All annotations are pro-2The occurrences were drawn equally from each of thegenre-specific portions of the OANC.3duced using the custom-built interface to WordNetshown in Figure 1: the sentence context is at the topwith the word in boldface (fair), a comment regionbelow that allows the annotator to keep notes, anda scrollable area below that shows three of the tenWordNet senses for ?fair.
?4 Observation: Varying Agreement,depending on Lexical ItemsWe expected to find varying levels of interannotatoragreement (IA) among all six annotators, depend-ing on obvious grouping factors such as the part ofspeech, or the number of senses per word.
We dofind widely varying levels of agreement, but as de-scribed here, most of the variation does not dependon these a priori factors.
Inherent usage propertiesof the words themselves, and systematic patterns ofvariation across annotators, seem to be the primaryfactors, with a secondary effect of part of speech.In previous work (Passonneau, 2004), we havediscussed why we use Krippendorff?s ?
(Krippen-dorff, 1980), and for purposes of comparison wealso report Cohen?s ?
; note the similarity in values3.As with the various agreement coefficients that fac-tor out the agreement that would occur by chance,values range from 1 for perfect agreement and -1for perfect opposition, to 0 for chance agreement.While there are no hard and fast criteria for whatconstitutes good IA, Landis and Koch (Landis andKoch, 1977) consider values between 0.40 and 0.60to represent moderately good agreement, and valuesabove 0.60 as quite good; Krippendorff (Krippen-dorff, 1980) considers values above 0.67 moderatelygood, and values above 0.80 as quite good.
(cf.
(Art-stein and Poesio, 2008) for discussion of agreementmeasurement for computational linguistic tasks.
)Table 2 shows IA for a pair of adjectives, nounsand verbs from our sample for which the IA scoresare at the extremes (high and low) in each pair: theaverage delta is 0.24.
Note that the agreement de-creases as part-of-speech varies from adjectives tonouns to verbs, but for all three parts-of-speech,there is a wide spread of values.
It is striking, giventhat the same annotators did all words, that one ineach pair has relatively better agreement.3?
handles multiple annotators; Arstein and Poesio (Artsteinand Poesio, 2008) propose an extension of ?
(?3) we use here.POS Word ?
?
No.
senses Usedadj long 0.6664 0.6665 9 8fair 0.3546 0.3593 10 5noun work 0.5359 0.5358 7 7land 0.2627 0.2671 11 8verb tell 0.4152 0.4165 8 8show 0.2636 0.2696 12 11Table 2: Varying interannotator agreement across wordsThe average of the agreement values shown inTable 2 (?=0.4164; ?=0.4191) is somewhat higherthan the average 0.317 found for 191 words anno-tated for WordNet senses in (Ng et al, 1999), butlower than their recomputed ?
of 0.85 for verbs, af-ter they reanalyzed the data to merge senses for 42of the verbs.
It is widely recognized that achievinghigh ?
scores (or percent agreement between anno-tators, cf.
(Palmer et al, 2005)) is difficult for wordsense annotation.Given that the same annotators have higher IA onsome words, and lower on others, we hypothesizethat it is the word usages themselves that lead to thehigh deltas in IA for each part-of-speech pair.
Wediscuss the impact of three factors on the observedvariations in agreement:1.
Greater specificity in the contexts of use leads tohigher agreement2.
More concrete senses give rise to higher agreement3.
A sense inventory with closely related senses(e.g., relatively lower average inter-sense similarityscores) gives rise to lower agreement5 Explanatory FactorsFirst we list factors that can not explain the variationin Table 2.
Then we turn to examples illustratingfactors that can, based on a manual search for exam-ples of two types: examples where most annotatorsagreed on a single sense, and examples where twoor three senses were agreed upon by multiple anno-tators.
Later we how how we use association rulesto detect these two types of cases automatically.
Forthese examples, the WordNet sense number is shown(e.g., WN S1) with an abbreviated gloss, followedby the number of annotators who chose it.45.1 Ruled Out FactorsIt appears that neither annotator expertise, a word?spart of speech, the number of senses in WordNet,the number of senses annotators find in the corpus,nor the nature of the distribution across senses, canaccount for the variation in IA in Table 2.
All sixannotators used the same annotation tool, the sameguidelines, and had already become experienced inthe word sense annotation task.The six annotators all exhibit roughly the sameperformance.
We measure an individual annotator?sperformance by computing the average pairwise IA(IA2).
For every annotator Ai, we first compute thepairwise agreement of Ai with every other annota-tor, then average.
This gives us a measure for com-paring individual annotators with each other: an-notators that have a higher IA2 have more agree-ment, on average, with other annotators.
Note thatwe get the same ranking of individuals when foreach annotator, we calculate how much the agree-ment among the five remaining annotators improvesover the agreement among all six annotators.
Ifagreement improves relatively more when annota-tor Ai is dropped, then Ai agrees less well with theother five annotators.
While both approaches givethe same ranking among annotators, IA2 also pro-vides a number that has an interpretable value.On a word-by-word basis, some annotators dobetter than others.
For example, for long, the bestannotator (A) has IA2=0.79, and the worst (F) has0.44.
However, across ten words annotated by allsix, the average of their IA2 is 0.39 with a standarddeviation of 0.037.
F at 0.32 is an outlier; apart fromF, annotators have similar IA across words.Table 2 lists the distribution of available sensesin WordNet for the four words (column 4), and thenumber of senses used (column 5).
The words workand tell have relatively fewer senses (seven and eig-ith) compared with nine through twelve for the otherwords.
However, neither the number (or proportion)of senses used by annotators, nor the distributionacross senses, has a significant correlation with IA,as given by Pearson?s correlation test.5.2 Lexical Use FactorsUnderspecified contexts lead to ambiguous wordmeanings, a factor that has been recognized as be-ing associated with polysemous contexts (Palmer etal., 2005).
We find that the converse is also true:relatively specific contexts reduce ambiguity.The word long seems to engender the greatest IAprimarily because the contexts are concrete and spe-cific, with a secondary effect that adjectives havehigher IA overall than the other parts of speech.
Sen-tences such as (1.
), where a specific unit of temporalor spatial measurement is mentioned (months), re-strict the sense to extent in space or time.1.
For 18 long months Michael could not find a job.WN S1.
temporal extent [N=6 of 6]In the few cases where annotators disagree onlong, the context is less specific or less concrete.
Inexample (2.
), long is predicated of the word chap-ter, which has non-concrete senses that exemplifya certain type of productive polysemy (Pustejovsky,1991).
It can be taken to refer to a physical object(a specific set of pages in an actual book), or a con-ceptual object (the abstract literary work).
The ad-jective inherits this polysemy.
The three annotatorswho agree on sense two (spatial extent) might havethe physical object sense in mind; the two who selectsense one (temporal extent) possibly took the pointof view of the reader who requires a long time toread the chapter.2.
After I had submitted the manuscript my editor atSimon Schuster had suggested a number of cuts tostreamline what was already a long and involvedchapter on Brians ideas.WN S2.spatial extent [N=3 of 6],WN S1.temporal extent [N=2 of 6],WN S9.more than normal or necessary [N=1 of 6]Several of the senses of work are concrete, andquite distinct: sense seven, ?an artist?s or writer?soutput?
; sense three, ?the occupation you are paidfor?
; sense five, ?unit of force in physics?
; sensesix, ?the place where one works.?
These are thesenses most often selected by a majority of annota-tors.
Senses one and two, which are closely related,are the two senses most often selected by differentannotators for the same instance.
They also repre-sent examples of productive polysemy, here betweenan activity sense (sense one) and a product-of-the-activity sense (sense two).
Example (3) shows a sen-5tence where the verb perform restricts the meaningto the activity sense, which all annotators selected.3.
The work performed by Rustom and colleaguessuggests that cell protrusions are a general mech-anism for cell-to-cell communication and that in-formation exchange is occurring through the directmembrane continuity of connected cells indepen-dently of exo- and endocytosis.WN S1.activity of making something [N=6 of 6]In sentence (4.
), four annotators selected senseone (activity) and two selected sense two (result):4.
A close friend is a plastic surgeon who did someminor OK semi-major facial work on me in the past.WN S1.activity directed toward making something[N=4 of 6],WN S2.product of the effort of a person or thing[N=2 of 6]For the word fair, if five or six annotators agree,often they have selected sense one?
?free of fa-voritism or bias?
?as in example (5).
However, thissense is often selected along with sense two?
?not ex-cessive or extreme?as in example (6).
Both sensesare relatively abstract.5.
By insisting that everything Microsoft has done isfair competition they risk the possibility that thepublic if it accepts the judges finding to the con-trary will conclude that Microsoft doesn?t know thedifference.WN S1.free of favoritism/bias [N=6 of 6]6.
I I think that?s true I can remember times my parentswould say well what do you think would be a fairpunishment.WN S1.free of favoritism/bias [N=3 of 6],WN S2.not excessive or extreme [N=3 of 6]Example (7) illustrates a case where all annota-tors agreed on a sense for land.
The named entityIndia restricts the meaning to sense five, ?territoryoccupied by a nation.?
Apart from a few such casesof high consensus, land seems to have low agree-ment due to senses being so closely related they canbe merged.
Senses one and seven both have to dowith property (cf.
example (8))., senses three andfive with geopolitical senses, and senses two andfour with the earth?s surface or soil.
If these threepairs of senses are merged into three senses, the IAgoes up from 0.2627 to 0.3677.7.
India is exhilarating exhausting and infuriating aland where you?ll find the practicalities of daily lifeoverlay the mysteries that popular myth attaches toIndia.WN S5.territory occupied by a nation [N=6 of 6]8. uh the Seattle area we lived outside outside of thecity in the country and uh we have five acres of landup against a hillside where i grew up and so we didhave a garden about a one a half acre gardenWN S4.solid part of the earth?s surface [N=1 of 6],WN S1.location of real estate [N=2 of 6],WN S7.extensive landed property [N=3 of 6]Examples for tell and show exhibit the same trendin which agreement is greater when the sense ismore specific or concrete, which we illustrate brieflywith show.
Example (9) describes a specific work ofart, an El Greco painting, and agreement is universalamong the six annotators on sense 5.
In contrast, ex-ample (10) shows a fifty-fifty split among annotatorsfor a sentence with a very specific context, an ex-periment regarding delivery of a DNA solution, butwhere the sense is abstract rather than concrete: theargument of show is an abstract proposition, namelya conclusion is drawn regarding what the experimentdemonstrates, rather than a concrete result such as aspecific measurement, or statistical outcome.
Sensetwo in fact contains the word ?experiment?
that oc-curs in (9), which presumably biases the choice ofsense two.
Impressionistically, senses two and threeappear to be quite similar.9.
El Greco shows St. Augustine and St. Stephen,in splendid ecclesiastical garb, lifting the count?sbody.WN S5.show in, or as in, a picture, N=6 of 610.
These experiments show that low-volume jetinjection specifically targeted delivery of a DNAsolution to the skin and that the injection paths didnot reach into the underlying tissue.WN S2.establish the validity of something, as byan example, explanation or experiment, N=3 of 6WN S3.provide evidence for, N=3 of 665.3 Quantifying Sense SimilarityApplication of an inter-sense similarity measure(ISM) proposed in (Ide, 2006) to the sense invento-ries for each of the six words supports the observa-tion that words with very similar senses have lowerIA scores.
ISM is computed for each pair in a givenword?s sense inventory, using a variant of the leskmeasure (Banerjee and Pedersen, 2002).
Agglom-erative clustering may then be applied to the result-ing similarity matrix to reveal the overall pattern ofinter-sense relations.ISMs for senses pairs of long, fair, work, land,tell, and show range from 0 to 1.44.4 We computea confusion threshhold CT based on the ISMs for all250 sense pairs asCT = ?A + 2?Awhere A is the sum of the ISMs for the six words?
250sense pairs.Table 3 shows the ISM statistics for the six words.
Thevalues show that the ISMs for work and long are signifi-cantly lower than for land and fair.
The ISMs for the twoverbs in the study, show and tell, are distributed acrossnearly the same range (0 - 1.38 and 0 - 1.22, respec-tively), despite substantially lower IA scores for show.However, the ISMs for three of show?s sense pairs arewell above CT , vs. one for tell, suggesting that in addi-tion to the range of ISMs for a given word?s senses, thenumber of sense pairs with high similarity contributes tolow IA.
Overall, the correlation between the percentageof ISMs above CT for the words in this study and theirIA scores is .8, which supports this claim.POS Word Max Mean Std.
Dev > CTadj long .71 .28 .18 0fair 1.25 .28 .34 5noun work .63 .22 .16 0land 1.44 .17 .29 3verb tell 1.22 .15 .25 1show 1.38 .18 .27 3Table 3: ISM statistics6 Association RulesAssociation rules express relations among instancesbased on their attributes.
Here the attributes of interest are4Note that because the scores are based on overlaps amongWordNet relations, glosses, examples, etc., there is no pre-defined ceiling value for the ISMs.
For the words in this study,we compute a ceiling value by taking the maximum of the ISMsfor each of the 57 senses with itself, 4.85 in this case.the annotators who choose one sense versus those whochoose another.
Mining association rules to find strongrelations has been studied in many domains (see for in-stance (Agrawal et al, 1993; Zaki et al, 1997; Salleb-Aouissi et al, 2007)).
Here we illustrate how associationrules can be used to mine relations such as systematic dif-ferences in word sense choices across annotators.An association rule is an expression C1 ?
C2, whereC1 and C2 express conditions on features describing theinstances in a dataset.
The strength of the rules is usuallyevaluated by means of measures such as Support (Supp)and Confidence (Conf).
Where C, C1 and C2 express con-ditions on attributes:?
Supp(C) is the fraction of instances satisfying C?
Supp(C1 ?
C2) = Supp(C1 ?
C2)?
Conf(C1 ?
C2) = Supp(C1 ?
C2)/Supp(C1)Given two thresholds MinSupp (for minimum support)and MinConf (for minimum confidence), a rule is strongwhen its support is greater than MinSupp and its confi-dence greater than MinConf.
Discovering strong rules isusually a two-step process of retrieving instances aboveMinSupp, then from these retrieving instances aboveMinConf.The types of association rules to mine can includeany attributes in either the left hand side or the righthand side of rules.
In our data, the attributes consistof the word sense assigned by annotators, the annota-tors, and the instances (words).
In order to find rulesthat relate annotators to each other, the dataset must bepre-processed to produce flat (two-dimensional) tables.Here we focus on annotators to get a flat table in whicheach line corresponds to an annotator/sense combination:Annotator Sense.
We denote the six annotators as A1through A6, and word senses by WordNet sense number.Here are 15 unique pairs of annotators, so one wayto look at where agreements occur is to determine howmany of these pairs choose the same sense with non-negligible support and confidence.
Tell has much bet-ter IA than show, but less than long and work.
Wewould expect association rules among many pairs ofannotators for some but not all of its senses.
Wefind 11 pairs of rules of the form Ai Tell:Sense1 ?Aj Tell:Sense1, Aj Tell:Sense1 ?
Ai Tell:Sense1,indicating a bi-directional relationship between pairs ofannotators choosing the same sense, with support rang-ing from 14% to 44% and confidence ranging from 37%to 96%.
This indicates good support and confidence formany possible pairsOur interest here is primarily in mining for systematicdisagreements thus we now turn to pairs of rules wherein one rule, an attribute Annotator Sensei occurs in theleft hand side, and a distinct attributeAnnotator Sensejoccurs in the right.
Again, we are especially interested in7i j Supp(%) Confi(%) Confj(%)Ai fair.S1 ?
Aj fair.S2A3 A6 20 100 32.3A5 A6 20 100 31.2A1 A2 16 80 40Ai show.S2 ?
Aj show.S3A1 A3 32 84.2 69.6A5 A3 24 63.2 80.0A4 A3 22 91.7 57.9A4 A6 14 58.3 46.7A4 A2 12 60.0 50.0A5 A2 12 60.0 40.0Ai show.S5 ?
Aj show.S10A1 A6 12 85.7 40.0A5 A2 10 83.3 50.0A4 A2 10 83.3 30.5A4 A6 10 71.4 38.5A3 A2 8 66.7 40.0A3 A6 8 57.1 40.0A5 A6 8 57.1 40.0Table 4: Association Rules for Systematic Disagreementsbi-directional cases where there is a corresponding rulewith the left and right hand clauses reversed.
Table 4shows some general classes of disagreement rules using acompact representation with a bidirectional arrow, alongwith a table of variables for the different pairs of annota-tors associated with different levels of support and confi-dence.For fair, Table 4 summarizes three pairs of rules withgood support (16-20% of all instances) in which one an-notator chooses sense 1 of fair and another chooses sense2: A3 and A5 choose sense 1 where A6 chooses sense 2,and A1 chooses sense 1 where A2 chooses sense 2.
Theconfidence varies for each rule, thus in 100% of caseswhere A6 selects sense 2 of fair, A3 selects sense 1, butin only 32.3% of cases is the converse true.
Example (6)where half the annotators picked sense 1 of fair and halfpicked sense 2 falls into the set of instances covered bythese rules.
The rules indicate this is not isolated, butrather part of a systematic pattern of usage.The word land had the lowest interannotator agree-ment among the six annotators, with eight of elevensenses were used overall (cf.
Table 2).
Here we did notfind pairs of rules in which distinct Annotator Senseattributes that occur in the left and right sides of one ruleoccur in the right and left sides of another rule.
For show,Table 4 illustrates two systematic divisions amonggroups of annotators.
With rather good support rang-ing from 12% to 32%, senses 2 and 3 exhibit a system-atic difference: annotators A1, A4 and A5 select sense2 where annotators A3, A3 and A6 select sense 3.
Sim-ilarly, senses 5 and 10 exhibit a systematic difference:with a more modest support of 8% to 12%, annotatorsA1, A3, A4 and A5 select sense 5 where annotators A2and A6 select sense 10.7 ConclusionWe have performed a sense assignment experimentamong multiple annotators for word occurrences drawnfrom a broad range of genres, rather than the domain-specific data utilized in many studies.
The selected wordswere all moderately polysemous.
Based on the results,we identify several factors that distinguish words withhigh vs. low interannotator agreement scores.
We alsoshow the use of association rules to mine the data forsystematic annotator differences.
Where relevant, the re-sults can be used to merge senses, as done in much pre-vious work, or to identify internal structure within a setof senses, such as a word-based sense-hierarchy.
In ourfuture work, we want to develop the use of associationrules in several ways.
First, we hope to fully automatedthe process of finding systematic patterns of differenceacross annotators.
Second, we hope to extend their useto mining associations among the representations of in-stances in order to further investigate the lexical use fac-tors discussed here.AcknowledgmentsThis work was supported in part by National ScienceFoundation grant CRI-0708952.ReferencesRakesh Agrawal, Tomasz Imielinski, and Arun N.Swami.
1993.
Mining association rules between setsof items in large databases.
In Peter Buneman andSushil Jajodia, editors, Proceedings of the 1993 ACMSIGMOD International Conference on Management ofData, Washington, D.C., May 26-28, 1993, pages 207?216.
ACM Press.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Satanjeev Banerjee and Ted Pedersen.
2002.
An adaptedLesk algorithm for word sense disambiguation usingWordNet.
In Proceedings of the third InternationalConference on Intelligent Text Processing and Com-putational Linguistics (CICLing-2002), pages 136?45,Mexico City, Mexico.Douglas Biber.
1995.
Dimensions of register variation :a cross-linguistic comparison.
Cambridge UniversityPress, Cambridge.8Irina Chugur, Julio Gonzalo, and Felisa Verdejo.
2002.Polysemy and sense proximity in the senseval-2 testsuite.
In Proceedings of the SIGLEX/SENSEVALWorkshop on Word Sense Disambiguation: Re-cent Successes and Future Directions, pages 32?39,Philadelphia.Mona Diab.
2004.
Relieving the data acquisition bottle-neck in word sense disambiguation.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, pages 303?311.Nancy Ide and Yorick Wilks.
2006.
Making sense aboutsense.
In E. Agirre and P. Edmonds, editors, WordSense Disambiguation: Algorithms and Applications,pages 47?74, Dordrecht, The Netherlands.
Springer.Nancy Ide, Tomaz Erjavec, and Dan Tufis.
2002.
Sensediscrimination with parallel corpora.
In Proceedingsof ACL?02 Workshop on Word Sense Disambiguation:Recent Successes and Future Directions, pages 54?60,Philadelphia.Nancy Ide.
2006.
Making senses: Bootstrapping sense-tagged lists of semantically-related words.
In Alexan-der Gelbukh, editor, Computational Linguistics andIntelligent Text, pages 13?27, Dordrecht, The Nether-lands.
Springer.Adam Kilgarriff.
1998.
SENSEVAL: An exercise inevaluating word sense disambiguation programs.
InProceedings of the First International Conference onLanguage Resources and Evaluation (LREC), pages581?588, Granada.Adam Kilgarriff.
2001.
Comparing corpora.
Interna-tional Journal of Corpus Linguistics, 6:1?37.Richard Kittredge, Tanya Korelsky, and Owen Rambow.1991.
On the need for domain communication knowl-edge.
Computational Intelligence, 7(4):305?314.Devra Klein and Gregory Murphy.
2002.
Paper has beenmy ruin: Conceptual relations of polysemous words.Journal of Memory and Language, 47:548?70.Klaus Krippendorff.
1980.
Content analysis: An intro-duction to its methodology.
Sage Publications, Bev-erly Hills, CA.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33(1):159?174.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1993.
In-troduction to WordNet: An on-line lexical database(revised).
Technical Report Cognitive Science Labo-ratory (CSL) Report 43, Princeton University, Prince-ton.
Revised March 1993.Hwee Tou Ng, Chung Yong Lim, and Shou King Foo.1999.
A case study on inter-annotator agreement forword sense disambiguation.
In SIGLEX Workshop OnStandardizing Lexical Resources.Geoffrey Nunberg.
1979.
The non-uniqueness of seman-tic solutions: Polysemy.
Linguistics and Philosophy,3:143?184.Martha Palmer, Hoa Trang Dang, and Christiane Fell-baum.
2005.
Making fin-egrained and coarse-grainedsense distinctions.
Journal of Natural Language Engi-neering, 13.2:137?163.Rebecca J. Passonneau, Nizar Habash, and Owen Ram-bow.
2006.
Inter-annotator agreement on a multilin-gual semantic annotation task.
In Proceedings of theInternational Conference on Language Resources andEvaluation (LREC), pages 1951?1956, Genoa, Italy.Rebecca J. Passonneau.
2004.
Computing reliability forcoreference annotation.
In Proceedings of the Interna-tional Conference on Language Resources and Evalu-ation (LREC), Portugal.Ted Pedersen.
2002a.
Assessing system agreementand instance difficulty in the lexical sample tasks ofSenseval-2.
In Proceedings of the ACL-02 Workshopon Word Sense Disambiguation: Recent Successes andFuture Directions, pages 40?46.Ted Pedersen.
2002b.
Evaluating the effectiveness ofensembles of decision trees in disambiguating SEN-SEVAL lexical samples.
In Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: RecentSuccesses and Future Directions, pages 81?87.James Pustejovsky.
1991.
The generative lexicon.
Com-putational Linguitics, 17(4):409?441.Josef Ruppenhofer, Michael Ellsworth, MiriamR.
L. Petruck, Christopher R. Johnson, andJan Scheffczyk.
2006.
Framenet ii: Ex-tended theory and practice.
Available fromhttp://framenet.icsi.berkeley.edu/index.php.Ansaf Salleb-Aouissi, Christel Vrain, and Cyril Nortet.2007.
Quantminer: A genetic algorithm for miningquantitative association rules.
In IJCAI, pages 1035?1040.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2007.Learning to merge word senses.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 1005?1014, Prague.Jean Veronis.
1998.
A study of polysemy judgementsand inter-annotator agreement.
In SENSEVAL Work-shop, pages Sussex, England.Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mit-sunori Ogihara, and Wei Li.
1997.
New algorithmsfor fast discovery of association rules.
In KDD, pages283?286.9
