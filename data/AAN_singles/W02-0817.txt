Building a Sense Tagged Corpus withOpen Mind Word ExpertTimothy ChklovskiArtificial Intelligence LaboratoryMassachusetts Institute of Technologytimc@mit.eduRada MihalceaDepartment of Computer ScienceUniversity of Texas at Dallasrada@utdallas.eduAbstractOpen Mind Word Expert is an imple-mented active learning system for col-lecting word sense tagging from thegeneral public over the Web.
It is avail-able at http://teach-computers.org.
Weexpect the system to yield a large vol-ume of high-quality training data at amuch lower cost than the traditionalmethod of hiring lexicographers.
Wethus propose a Senseval-3 lexical sam-ple activity where the training data iscollected via Open Mind Word Expert.If successful, the collection process canbe extended to create the definitive cor-pus of word sense information.1 IntroductionMost of the efforts in the Word Sense Disam-biguation (WSD) field have concentrated on su-pervised learning algorithms.
These methods usu-ally achieve the best performance at the cost oflow recall.
The main weakness of these meth-ods is the lack of widely available semanticallytagged corpora and the strong dependence of dis-ambiguation accuracy on the size of the trainingcorpus.
The tagging process is usually done bytrained lexicographers, and consequently is quiteexpensive, limiting the size of such corpora to ahandful of tagged texts.This paper introduces Open Mind Word Ex-pert, a Web-based system that aims at creatinglarge sense tagged corpora with the help of Webusers.
The system has an active learning compo-nent, used for selecting the most difficult exam-ples, which are then presented to the human tag-gers.
We expect that the system will yield moretraining data of comparable quality and at a sig-nificantly lower cost than the traditional methodof hiring lexicographers.Open Mind Word Expert is a newly born projectthat follows the Open Mind initiative (Stork,1999).
The basic idea behind Open Mind is touse the information and knowledge that may becollected from the existing millions of Web users,to the end of creating more intelligent software.This idea has been used in Open Mind CommonSense, which acquires commonsense knowledgefrom people.
A knowledge base of about 400,000facts has been built by learning facts from 8,000Web users, over a one year period (Singh, 2002).If Open Mind Word Expert experiences a similarlearning rate, we expect to shortly obtain a cor-pus that exceeds the size of all previously taggeddata.
During the first fifty days of activity, we col-lected about 26,000 tagged examples without sig-nificant efforts for publicizing the site.
We expectthis rate to gradually increase as the site becomesmore widely known and receives more traffic.2 Sense Tagged CorporaThe availability of large amounts of semanti-cally tagged data is crucial for creating successfulWSD systems.
Yet, as of today, only few sensetagged corpora are publicly available.One of the first large scale hand tagging effortsis reported in (Miller et al, 1993), where a subsetof the Brown corpus was tagged with WordNetJuly 2002, pp.
116-122.
Association for Computational Linguistics.Disambiguation: Recent Successes and Future Directions, Philadelphia,Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sensesenses.
The corpus includes a total of 234,136tagged word occurrences, out of which 186,575are polysemous.
There are 88,058 noun occur-rences of which 70,214 are polysemous.The next significant hand tagging task was re-ported in (Bruce and Wiebe, 1994), where 2,476usages of interest were manually assigned withsense tags from the Longman Dictionary of Con-temporary English (LDOCE).
This corpus wasused in various experiments, with classificationaccuracies ranging from 75% to 90%, dependingon the algorithm and features employed.The high accuracy of the LEXAS system (Ngand Lee, 1996) is due in part to the use of largecorpora.
For this system, 192,800 word occur-rences have been manually tagged with sensesfrom WordNet.
The set of tagged words consistsof the 191 most frequently occurring nouns andverbs.
The authors mention that approximatelyone man-year of effort was spent in tagging thedata set.Lately, the SENSEVAL competitions provide agood environment for the development of su-pervised WSD systems, making freely availablelarge amounts of sense tagged data for about100 words.
During SENSEVAL-1 (Kilgarriff andPalmer, 2000), data for 35 words was made avail-able adding up to about 20,000 examples taggedwith respect to the Hector dictionary.
The sizeof the tagged corpus increased with SENSEVAL-2(Kilgarriff, 2001), when 13,000 additional exam-ples were released for 73 polysemous words.
Thistime, the semantic annotations were performedwith respect to WordNet.Additionally, (Kilgarriff, 1998) mentions theHector corpus, which comprises about 300 wordtypes with 300-1000 tagged instances for eachword, selected from a 17 million word corpus.Sense tagged corpora have thus been central toaccurate WSD systems.
Estimations made in (Ng,1997) indicated that a high accuracy domain inde-pendent system for WSD would probably need acorpus of about 3.2 million sense tagged words.At a throughput of one word per minute (Ed-monds, 2000), this would require about 27 man-years of human annotation effort.With Open Mind Word Expert we aim at creat-ing a very large sense tagged corpus, by makinguse of the incredible resource of knowledge con-stituted by the millions of Web users, combinedwith techniques for active learning.3 Open Mind Word ExpertOpen Mind Word Expert is a Web-based interfacewhere users can tag words with their WordNetsenses.
Tagging is organized by word.
That is,for each ambiguous word for which we want tobuild a sense tagged corpus, users are presentedwith a set of natural language (English) sentencesthat include an instance of the ambiguous word.Initially, example sentences are extracted froma large textual corpus.
If other training data isnot available, a number of these sentences are pre-sented to the users for tagging in Stage 1.
Next,this tagged collection is used as training data, andactive learning is used to identify in the remainingcorpus the examples that are ?hard to tag?.
Theseare the examples that are presented to the users fortagging in Stage 2.
For all tagging, users are askedto select the sense they find to be the most appro-priate in the given sentence, from a drop-downlist that contains all WordNet senses, plus twoadditional choices, ?unclear?
and ?none of theabove?.
The results of any automatic classifica-tion or the classification submitted by other usersare not presented so as to not bias the contrib-utor?s decisions.
Based on early feedback fromboth researchers and contributors, a future versionof Open Mind Word Expert may allow contribu-tors to specify more than one sense for any word.A prototype of the system has been imple-mented and is available at http://www.teach-computers.org.
Figure 1 shows a screen shot fromthe system interface, illustrating the screen pre-sented to users when tagging the noun ?child?.3.1 DataThe starting corpus we use is formed by a mixof three different sources of data, namely thePenn Treebank corpus (Marcus et al, 1993), theLos Angeles Times collection, as provided duringTREC conferences1 , and Open Mind CommonSense2, a collection of about 400,000 common-sense assertions in English as contributed by vol-unteers over the Web.
A mix of several sources,each covering a different spectrum of usage, is1http://trec.nist.gov2http://commonsense.media.mit.eduFigure 1: Screen shot from Open Mind Word Expertused to increase the coverage of word senses andwriting styles.
While the first two sources are wellknown to the NLP community, the Open MindCommon Sense constitutes a fairly new textualcorpus.
It consists mostly of simple single sen-tences.
These sentences tend to be explanationsand assertions similar to glosses of a dictionary,but phrased in a more common language and withmany sentences per sense.
For example, the col-lection includes such assertions as ?keys are usedto unlock doors?, and ?pressing a typewriter keymakes a letter?.
We believe these sentences maybe a relatively clean source of keywords that canaid in disambiguation.
For details on the data andhow it has been collected, see (Singh, 2002).3.2 Active LearningTo minimize the amount of human annotation ef-fort needed to build a tagged corpus for a givenambiguous word, Open Mind Word Expert in-cludes an active learning component that has therole of selecting for annotation only those exam-ples that are the most informative.According to (Dagan et al, 1995), there are twomain types of active learning.
The first one usesmemberships queries, in which the learner con-structs examples and asks a user to label them.
Innatural language processing tasks, this approachis not always applicable, since it is hard andnot always possible to construct meaningful un-labeled examples for training.
Instead, a secondtype of active learning can be applied to thesetasks, which is selective sampling.
In this case,several classifiers examine the unlabeled data andidentify only those examples that are the most in-formative, that is the examples where a certainlevel of disagreement is measured among the clas-sifiers.We use a simplified form of active learningwith selective sampling, where the instances to betagged are selected as those instances where thereis a disagreement between the labels assigned bytwo different classifiers.
The two classifiers aretrained on a relatively small corpus of tagged data,which is formed either with (1) Senseval trainingexamples, in the case of Senseval words, or (2)examples obtained with the Open Mind Word Ex-pert system itself, when no other training data isavailable.The first classifier is a Semantic Tagger withActive Feature Selection (STAFS).
This system(previously known as SMUls) is one of the topranked systems in the English lexical sample taskat SENSEVAL-2.
The system consists of an in-stance based learning algorithm improved witha scheme for automatic feature selection.
It re-lies on the fact that different sets of featureshave different effects depending on the ambigu-ous word considered.
Rather than creating a gen-eral learning model for all polysemous words,STAFS builds a separate feature space for eachindividual word.
The features are selected from apool of eighteen different features that have beenpreviously acknowledged as good indicators ofword sense, including: part of speech of the am-biguous word itself, surrounding words and theirparts of speech, keywords in context, noun be-fore and after, verb before and after, and others.An iterative forward search algorithm identifiesat each step the feature that leads to the highestcross-validation precision computed on the train-ing data.
More details on this system can be foundin (Mihalcea, 2002b).The second classifier is a COnstraint-BAsedLanguage Tagger (COBALT).
The system treatsevery training example as a set of soft constraintson the sense of the word of interest.
WordNetglosses, hyponyms, hyponym glosses and otherWordNet data is also used to create soft con-straints.
Currently, only ?keywords in context?type of constraint is implemented, with weightsaccounting for the distance from the target word.The tagging is performed by finding the sense thatminimizes the violation of constraints in the in-stance being tagged.
COBALT generates confi-dences in its tagging of a given instance based onhow much the constraints were satisfied and vio-lated for that instance.Both taggers use WordNet 1.7 dictionaryglosses and relations.
The performance of the twosystems and their level of agreement were eval-uated on the Senseval noun data set.
The twosystems agreed in their classification decision in54.96% of the cases.
This low agreement levelis a good indication that the two approaches arefairly orthogonal, and therefore we may hope forhigh disambiguation precision on the agreementPrecisionSystem (fine grained) (coarse grained)STAFS 69.5% 76.6%COBALT 59.2% 66.8%STAFS   COBALT 82.5% 86.3%STAFS - STAFS   COBALT 52.4% 63.3%COBALT - STAFS   COBALT 30.09% 42.07%Table 1: Disambiguation precision for the two in-dividual classifiers and their agreement and dis-agreement setsset.
Indeed, the tagging accuracy measured onthe set where both COBALT and STAFS assignthe same label is 82.5%, a figure that is closeto the 85.5% inter-annotator agreement measuredfor the SENSEVAL-2 nouns (Kilgarriff, 2002).Table 1 lists the precision for the agreementand disagreement sets of the two taggers.
Thelow precision on the instances in the disagreementset justifies referring to these as ?hard to tag?.
InOpen Mind Word Expert, these are the instancesthat are presented to the users for tagging in theactive learning stage.3.3 Ensuring QualityCollecting from the general public holds thepromise of providing much data at low cost.
Italso makes attending to two aspects of data col-lection more important: (1) ensuring contributionquality, and (2) making the contribution processengaging to the contributors.We have several steps already implemented andhave additional steps we propose to ensure qual-ity.First, redundant tagging is collected for eachitem.
Open Mind Word Expert currently uses thefollowing rules in presenting items to volunteercontributors: Two tags per item.
Once an item has twotags associated with it, it is not presented forfurther tagging. One tag per item per contributor.
We allowcontributors to submit tagging either anony-mously or having logged in.
Anonymouscontributors are not shown any items alreadytagged by contributors (anonymous or not)from the same IP address.
Logged in con-tributors are not shown items they have al-ready tagged.Second, inaccurate sessions will be discarded.This can be accomplished in two ways, roughlyby checking agreement and precision: Using redundancy of tags collected for eachitem, any given session (a tagging done allin one sitting) will be checked for agreementwith tagging of the same items collected out-side of this session. If necessary, the precision of a given contrib-utor with respect to a preexisting gold stan-dard (such as SemCor or Senseval trainingdata) can be estimated directly by presentingthe contributor with examples from the goldstandard.
This will be implemented if thereare indications of need for this in the pilot;it will help screen out contributors who, forexample, always select the first sense (andare in high agreement with other contribu-tors who do the same).In all, automatic assessment of the quality oftagging seems possible, and, based on the ex-perience of prior volunteer contribution projects(Singh, 2002), the rate of maliciously misleadingor incorrect contributions is surprisingly low.Additionally, the tagging quality will be esti-mated by comparing the agreement level amongWeb contributors with the agreement level thatwas already measured in previous sense taggingprojects.
An analysis of the semantic annotationtask performed by novice taggers as part of theSemCor project (Fellbaum et al, 1997) revealedan agreement of about 82.5% among novice tag-gers, and 75.2% among novice taggers and lexi-cographers.Moreover, since we plan to use paid, trainedtaggers to create a separate test corpus for eachof the words tagged with Open Mind Word Ex-pert, these same paid taggers could also validatea small percentage of the training data for whichno gold standard exists.3.4 Engaging the ContributorsWe believe that making the contribution processas engaging and as ?game-like?
for the contrib-utors as possible is crucial to collecting a largevolume of data.
With that goal, Open Mind WordExpert tracks, for each contributor, the number ofitems tagged for each topic.
When tagging items,a contributor is shown the number of items (forthis word) she has tagged and the record numberof items tagged (for this word) by a single user.If the contributor sets a record, it is recognizedwith a congratulatory message on the contributionscreen, and the user is placed in the Hall of Famefor the site.
Also, the user can always access areal-time graph summarizing, by topic, their con-tribution versus the current record for that topic.Interestingly, it seems that relatively sim-ple word games can enjoy tremendoususer acceptance.
For example, WordZap(http://wordzap.com), a game that pits playersagainst each other or against a computer to be thefirst to make seven words from several presentedletters (with some additional rules), has beendownloaded by well over a million users, andthe reviewers describe the game as ?addictive?.If sense tagging can enjoy a fraction of suchpopularity, very large tagged corpora will begenerated.Additionally, NLP instructors can use the siteas an aid in teaching lexical semantics.
An in-structor can create an ?activity code?, and then,for users who have opted in as participants of thatactivity (by entering the activity code when cre-ating their profiles), access the amount tagged byeach participant, and the percentage agreement ofthe tagging of each contributor who opted in forthis activity.
Hence, instructors can assign OpenMind Word Expert tagging as part of a homeworkassignment or a test.Also, assuming there is a test set of alreadytagged examples for a given ambiguous word, wemay add the capability of showing the increasein disambiguation precision on the test set, as itresults from the samples that a user is currentlytagging.4 Proposed Task for SENSEVAL-3The Open Mind Word Expert system will be usedto build large sense tagged corpora for some ofthe most frequent ambiguous words in English.The tagging will be collected over the Web fromvolunteer contributors.
We propose to organize atask in SENSEVAL-3 where systems will disam-biguate words using the corpus created with thissystem.We will initially select a set of 100 nouns,and collect for each of them   taggedsamples (Edmonds, 2000), where  is the num-ber of senses of the noun.
It is worth mention-ing that, unlike previous SENSEVAL evaluations,where multi-word expressions were consideredas possible senses for an constituent ambiguousword, we filter these expressions apriori with anautomatic tool for collocation extraction.
There-fore, the examples we collect refer only to singleambiguous words, and hence we expect a lowerinter-tagger agreement rate and lower WSD tag-ging precision when only single words are used,since usually multi-word expressions are not am-biguous and they constitute some of the ?easycases?
when doing sense tagging.These initial set of tagged examples will thenbe used to train the two classifiers described inSection 3.2, and annotate an additional set of examples.
From these, the users will bepresented only with those examples where thereis a disagreement between the labels assigned bythe two classifiers.
The final corpus for each am-biguous word will be created with (1) the originalset of tagged examples, plus (2) theexamples selected by the active learning compo-nent, sense tagged by users.Words will be selected based on their frequen-cies, as computed on SemCor.
Once the tag-ging process of the initial set of 100 words iscompleted, additional nouns will be incremen-tally added to the Open Mind Word Expert inter-face.
As we go along, words with other parts ofspeech will be considered as well.To enable comparison with Senseval-2, the setof words will also include the 29 nouns used inthe Senseval-2 lexical sample tasks.
This wouldallow us to assess how much the collected datahelps on the Senseval-2 task.As shown in Section 3.3, redundant tags will becollected for each item, and overall quality will beassessed.
Moreover, starting with the initial set of examples labeled for each word, wewill create confusion matrices that will indicatethe similarity between word senses, and help uscreate the sense mappings for the coarse grainedevaluations.One of the next steps we plan to take is to re-place the ?two tags per item?
scheme with the?tag until at least two tags agree?
scheme pro-posed and used during the SENSEVAL-2 tagging(Kilgarriff, 2002).
Additionally, the set of mean-ings that constitute the possible choices for a cer-tain ambiguous example will be enriched withgroups of similar meanings, which will be de-termined either based on some apriori providedsense mappings (if any available) or based on theconfusion matrices mentioned above.For each word with sense tagged data createdwith Open Mind Word Expert, a test corpus willbe built by trained human taggers, starting withexamples extracted from the corpus mentioned inSection 3.1.
This process will be set up indepen-dently of the Open Mind Word Expert Web in-terface.
The test corpus will be released duringSENSEVAL-3.5 Conclusions and future workOpen Mind Word Expert pursues the poten-tial of creating a large tagged corpus.
WSDcan also benefit in other ways from the OpenMind approach.
We are considering using aAutoASC/GenCor type of approach to generatesense tagged data with a bootstrapping algorithm(Mihalcea, 2002a).
Web contributors can helpthis process by creating the initial set of seeds,and exercising control over the quality of theautomatically generated seeds.AcknowledgmentsWe would like to thank the Open Mind Word Ex-pert contributors who are making all this workpossible.
We are also grateful to Adam Kilgar-riff for valuable suggestions and interesting dis-cussions, to Randall Davis and to the anonymousreviewers for useful comments on an earlier ver-sion of this paper, and to all the Open Mind WordExpert users who have emailed us with their feed-back and suggestions, helping us improve this ac-tivity.ReferencesR.
Bruce and J. Wiebe.
1994.
Word sense disam-biguation using decomposable models.
In Proceed-ings of the 32nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL-94), pages139?146, LasCruces, NM, June.I.
Dagan, , and S.P.
Engelson.
1995.
Committee-based sampling for training probabilistic classifiers.In International Conference on Machine Learning,pages 150?157.P.
Edmonds.
2000.
Designing a task forSenseval-2, May.
Available online athttp://www.itri.bton.ac.uk/events/senseval.C.
Fellbaum, J. Grabowski, and S. Landes.
1997.Analysis of a hand-tagging task.
In Proceedingsof ANLP-97 Workshop on Tagging Text with Lexi-cal Semantics: Why, What, and How?, WashingtonD.C.A.
Kilgarriff and M. Palmer, editors.
2000.
Com-puter and the Humanities.
Special issue: SENSE-VAL.
Evaluating Word Sense Disambiguation pro-grams, volume 34, April.A.
Kilgarriff.
1998.
Gold standard datasets for eval-uating word sense disambiguation programs.
Com-puter Speech and Language, 12(4):453?472.A.
Kilgarriff, editor.
2001.
SENSEVAL-2, Toulouse,France, November.A.
Kilgarriff.
2002.
English lexical sample task de-scription.
In Proceedings of Senseval-2, ACL Work-shop.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: the penn treebank.
Computational Linguis-tics, 19(2):313?330.R.
Mihalcea.
2002a.
Bootstrapping large sense taggedcorpora.
In Proceedings of the Third InternationalConference on Language Resources and EvaluationLREC 2002, Canary Islands, Spain, May.
(to ap-pear).R.
Mihalcea.
2002b.
Instance based learning withautomatic feature selection applied to Word SenseDisambiguation.
In Proceedings of the 19th Inter-national Conference on Computational Linguistics(COLING-ACL 2002), Taipei, Taiwan, August.
(toappear).G.
Miller, C. Leacock, T. Randee, and R. Bunker.1993.
A semantic concordance.
In Proceedingsof the 3rd DARPA Workshop on Human LanguageTechnology, pages 303?308, Plainsboro, New Jer-sey.H.T.
Ng and H.B.
Lee.
1996.
Integrating multipleknowledge sources to disambiguate word sense: Anexamplar-based approach.
In Proceedings of the34th Annual Meeting of the Association for Com-putational Linguistics (ACL-96), Santa Cruz.H.T.
Ng.
1997.
Getting serious about word sense dis-ambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why, What, and How?, pages 1?7, Washington.P.
Singh.
2002.
The public acquisition of common-sense knowledge.
In Proceedings of AAAI SpringSymposium: Acquiring (and Using) Linguistic (andWorld) Knowledge for Information Access., PaloAlto, CA.
AAAI.D.
Stork.
1999.
The Open Mind initiative.
IEEE Ex-pert Systems and Their Applications, 14(3):19?20.
