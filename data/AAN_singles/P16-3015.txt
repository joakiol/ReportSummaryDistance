Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ?
Student Research Workshop, pages 100?106,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsFrom Extractive to Abstractive Summarization: A JourneyParth MehtaInformation Retrieval and Language Processing LabDhirubhai Ambani Institute of Information and Communication TechnologyGandhinagar, Indiaparth me@daiict.ac.inAbstractThe availability of large document-summary corpora have opened up newpossibilities for using statistical textgeneration techniques for abstractivesummarization.
Progress in Extractivetext summarization has become stagnantfor a while now and in this work wecompare the two possible alternates toit.
We present an argument in favor ofabstractive summarization compared toan ensemble of extractive techniques.Further we explore the possibility ofusing statistical machine translation as agenerative text summarization techniqueand present possible research questions inthis direction.
We also report our initialfindings and future direction of research.1 Motivation for proposed researchExtractive techniques of text summarization havelong been the primary focus of research comparedto abstractive techniques.
But recent reports tendto suggest that advances in extractive text summa-rization have slowed down in the past few years(Nenkova and McKeown, 2012).
Only marginalimprovements are being reported over previoustechniques, and more often than not these seemto be a result of variation in the parameters usedduring evaluation using ROUGE, and some timesdue to other factors like a better redundancy re-moval module (generally used after the sentencesare ranked according to their importance) ratherthan the actual algorithm.
Overall it seems thatthe current state of the art techniques for extractivesummarization have more or less achieved theirpeak performance and only some small improve-ments can be further achieved.
In such a scenariothere seem to be two possible directions of fur-ther research.
One approach that could be usedis making an ensemble of these techniques whichmight prove to be better than the individual meth-ods.
The other option is to focus on abstractivetechniques instead.A large number of extractive summarizationtechniques have been developed in the past decadeespecially after the advent of conferences likeDocument Understanding Conference (DUC)1and Text Analysis Conference (TAC)2.
But veryfew inquiries have been made as to how thesediffer from each other and what are the salientfeatures on some which are absent in others.
(Hong et al, 2014) is first such attempt to com-pare summaries beyond merely comparing theROUGE(Lin, 2004) scores.
They show that manysystems, although having a similar ROUGE scoreindeed have very different content and have lit-tle overlap among themselves.
This difference, atleast theoretically, opens up a possibility of com-bining these summaries at various levels, like fus-ing rank lists(Wang and Li, 2012), choosing thebest combination of sentences from several sum-maries(Hong et al, 2015) or using learning-to-rank techniques to generate rank lists of sentencesand then choosing the top-k sentences as a sum-mary, to get a better result.
In the next sectionwe report our initial experiments and show that ameaningful ensemble of these techniques can helpin improving the coverage of existing techniques.But such a scenario is not always guaranteed, asshown in the next section, and given that such fu-sion techniques do have a upper bound to the ex-tent to which they can improve the summarizationperformance as shown by (Hong et al, 2015), anensemble approach would be of limited interest.Keeping this in mind we plan to focus on1duc.nist.gov2www.nist.gov/tac100both approaches for abstractive text summariza-tion, those that depend on initial extractive sum-mary and those that do not (text generation ap-proach).
Also availability of large document-summary corpora, as we discuss in section 3, hasopened up new possibilities for applying statisticaltext generation approaches to summarization.
Inthe next section we present a brief overview of theinitial experiments that we have performed withan ensemble of extractive techniques.
In section 3we then propose further research directions usingthe generative approach towards text summariza-tion.
In the final section we present some prelim-inary results of summarizing documents using amachine translation system.2 Fusion of Summarization systemsIn this section we report some of our experimentswith fusion techniques for combining extractivesummarization systems.
For the first experimentwe consider five basic techniques mentioned in(Hong et al, 2014) for the simple reason that theyare tested extensively and are simple yet effective.These systems include LexRank, the much pop-ular graph based summarization technique(Erkanand Radev, 2004), and Greedy-KL(Haghighi andVanderwende, 2009), which iteratively choosesthe sentence that has least KL-divergence com-pared to the remaining document.
Other systemsare FreqSum, a word frequency based system, andTsSum, which uses topic signatures computed bycomparing the documents to a background corpus.A Centroid based technique finds the sentencesmost similar to the document based on cosine sim-ilarity.
We also combine the rank lists from thesesystems using the Borda count3and ReciprocalRank Methods.System Rouge-1 Avg-RankCentroid 0.3641 1.94FreqSum 0.3531 1.48Greedy-KL 0.3798 2.2LexRank 0.3595 1.72TsSum 0.3587 1.88BC 0.3621 2.5RR 0.3633 2.46Table 1: Effect of FusionWe evaluated the techniques based on ROUGE-3https://en.wikipedia.org/wiki/Borda_count1, ROUGE-2 and ROUGE-4 Recall (Lin, 2004)using the parameters mentioned in (Hong et al,2014).
We report only ROUGE-1 results due tospace constraints.
We also computed Average-Rank for each system.
Average-Rank indicatesthe average number of systems that the given sys-tem outperformed.
The higher the average-rankthe more consistent a given system.
When systemsare ranked based on ROUGE-1 metric, both Bordaand Reciprocal Rank perform better than four ofthe five systems but couldn?t beat the Greedy-KL method.
Both combination techniques outper-formed all five methods when systems are rankedbased on ROUGE-2 and ROUGE-4.
Even in casewhere Borda and Reciprocal Rank did outperformall the other systems, the increase in ROUGEscores were negligible.
These results are con-trary to what has been reported previously (Wangand Li, 2012) as neither of the fusion techniquesperformed significantly better than the candidatesystems.
The only noticeable improvement in allcases was in the Average-Rank.
The combinedsystems were more consistent than the individualsystems.
These results indicate that Fusion can atleast help us in improving the consistency of themeta-system.One clear trend we observed was that not allcombinations performed poorly, and summariesfrom certain techniques when fused together per-formed well (on both ROUGE score and consis-tency).
To further investigate this issue we con-ducted another experiment where we try to makean informed fusion of various extractive tech-niques.Due to space constraints we report results onlyon two families of summarization techniques: oneis a graph based iterative method as suggested in(Erkan and Radev, 2004) and (Mihalcea and Ta-rau, 2004) and the other is the ?Greedy approach?where we greedily add a sentence that is most sim-ilar to the entire document, remove the sentencefrom the document and repeat the process untilwe have the desired number of sentences.
Wethen chose three commonly used sentence similar-ity measures: Cosine similarity, Word overlap andKL-Divergence.
Several other similar approachesare possible, for example TsSum and FreqSumare related in the sense that each method rates asentence based on the average number of impor-tant words in it, the difference being in the wayin which importance of the word is computed.101We perform this experiment in a very constrainedmanner and leave it to the future experimentingwith other such possible combinations.Graph Greedy BordaCosine 0.3473 0.3313 0.3370Word Overlap 0.3139 0.3229 0.3039KLD 0.3248 0.3429 0.3121Borda 0.3638 0.3515 -Table 2: Effect of ?Informed?
FusionWe generate summaries using all the possible6 combinations of two approaches and three sen-tence similarity metrics.
We then combine thesummaries resulting from a particular sentencesimilarity metric or from a particular sentenceranking algorithm.
The results in table 2 showthat techniques that have a similar ranking algo-rithm but use different sentence similarity metrics,when combined produce an aggregate summarywhose coverage is much better than the originalsummary.
The aggregate summaries from the sys-tems that have different ranking algorithm but thesame sentence similarity measure do not beat thebest performing system.
Figures in bold indicatethe maximum score for that particular approach.We have tested this for several other ranking algo-rithms like centroid based and LSA based and sen-tence similarity measures.
The hypothesis holds inmost cases.
We consider this experiment to be in-dicative of a future direction of research and donot consider it in any way to be conclusive.
Butit definitely indicates the difficulties that might beencountered when attempting to fuse summariesfrom different sources compared to the limited im-provement in the coverage (ROUGE scores).
Thiscombined with availability of a larger training setof document-summary pairs, which enables us touse several text generation approaches, is our prin-ciple motivation behind the proposed research.3 Abstractive SummarizationAbstractive Summarization covers techniqueswhich can generate summaries by rewriting thecontent in a given text, rather than simply extract-ing important sentences from it.
But most of thecurrent abstractive summarization techniques stilluse sentence extraction as a first step for abstractgeneration.
In most cases, extractive summariesreach their limitation primarily because only a partof every sentence selected is informative and theother part is redundant.
Abstractive techniquestry to tackle this issue by either dropping the re-dundant part altogether or fusing two similar sen-tences in such a way as to maximize the informa-tion content and minimize the sentence lengths.We discuss some experiments we plan to do inthis direction.
An alternative to this technique iswhat is known as the Generative approach for textsummarization.
These techniques extract concepts(instead of sentences or phrases) from the giventext and generate new sentences using those con-cepts and the relationships between them.
We pro-pose a novel approach of using statistical machinetranslation for document summarization.
We dis-cuss the possibilities of exploiting Statistical ma-chine translation techniques, which in themselvesare generative techniques and have a sound math-ematical formulation, for translating a text in Doc-ument Language to Summary Language.
In thissection we highlight the research questions we aretrying to address and issues that we might face indoing so.
We also mention another approach wewould like to explore which uses topic modelingfor generating summaries.3.1 Sentence FusionMost abstractive summarization techniques relyon sentence fusion to remove redundancy andcreate a new concise sentence.
Graph basedtechniques similar to (Ganesan et al, 2010) and(Banerjee et al, 2015) have become very popu-lar recently.
These techniques rely on extractivesummarization to get important sentences, clus-ter lexically similar sentences together, create aword graph from this cluster and try to generate anew meaningful sentence by selecting a best suitedpath from this word graph.
Several factors like thelinguistic quality of the sentence, informativeness,length of the sentence are considered when select-ing an appropriate path form the word graph.Informativeness of the selected path can be de-fined in several ways, and the choice defines howgood my summary would be (at least when usingROUGE as a evaluation measure).
In one of ourexperiments we changed the informativeness cri-teria from TextRank scores of words as used in theoriginal approach in (Banerjee et al, 2015) to Log-Likelihood ratio of the words compared to a largebackground corpus as suggested in (Lin and Hovy,2000).
We observed that changing measure of in-formativeness produces a dramatic change in the102quality of the summaries.
We would like to con-tinue working in this direction.3.2 Summarization as a SMT problemThe idea is to model the text summarization prob-lem as a Statistical Machine Translation (SMT)problem of translating text written in a Docu-ment Language to that in a Summary Language.Machine translation techniques have well definedand well accepted generative models which havebeen researched extensively over more than twodecades.
At least on the surface, the idea of model-ing a text summarization problem as that of trans-lation between two pairs of texts might enable usto leverage this progress in the field of SMT andextend it to abstractive text summarization, albeitwith several modifications.
We expect this areato be our primary research focus.
While a simi-lar approach has been used in the case of QuestionAnswering (Zhang et al, 2014), to the best of ourknowledge it has not yet been used for DocumentSummarization.While the idea seems very intuitive and appeal-ing, there are several roadblocks to it.
The firstand perhaps the biggest issue has been the lack ofavailability of a large training corpus.
Tradition-ally SMT systems have depended on large vol-umes of parallel texts that are used to learn thephrase level alignment between sentences fromtwo languages and the probability with which aparticular phrase in the source language might betranslated to another in the target language.
TheText Summarization community on the other handhas relied on more linguistic approaches or sta-tistical approaches which use limited amount oftraining data.
Most of the evaluation benchmarkdatasets generated by conferences like DUC orTAC are limited to less than a hundred Document-Summary pairs and the focus has mainly beenon short summaries of very few sentences.
Thismakes the available data too small (especiallywhen considering the number of sentences).We hope to solve this problem partially usingthe Supreme Court Judgments dataset released bythe organizers of Information Access in Legal Do-main Track4at FIRE 2014.
The dataset has 1500Judgments with a corresponding summary knownas a headnote, manually written by legal experts.The organizers released another dataset of addi-4http://www.isical.ac.in/?fire/2014/legal.htmltional 10,000 judgment-headnote pairs from theSupreme court of India spread over four decades,that are noisy and need to be curated.
The averagejudgment length is 150 sentences while a head-note is 30 sentence long on an average.
Using thiswe can create a parallel corpus of approximately45,000 sentences using the clean data, and an ad-ditional 300,000 sentences after curating the entiredataset.
This is comparable to the size of standarddatasets used for training SMT systems.Given this data is only semi-parallel and alignedat document level and not at sentence level, thenext issue is extracting pairs of source sentenceand target sentence.
The exception being that boththe source sentence and target sentence can actu-ally be several sentences instead of a single sen-tence, the possibility being higher in case of thesource than the target.
This might seem to be aclassic example of the problem of extracting par-allel sentences from a comparable corpus.
Butthere are several important differences, the biggestone being that it is almost guaranteed that severalsentences from the text written in Document Lan-guage will map to a single sentence in the Sum-mary Language.
This itself makes this task morechallenging compared to the already daunting taskof finding parallel sentences in a comparable cor-pora.
Another notable difference is that unlike incase of SMT, the headnotes (or the Summary Lan-guage) are influenced a lot by the stylistic qual-ity of its author.
The nature of headnotes seemsto vary to a large extent over the period of fourdecades, and we are in the process of trying to fig-ure out how this affects the sentence alignment aswell as the overall translation process.
The othermajor difference can actually be used as leverageto improve the quality of sentence level alignment.The headnotes tend to follow a general format, inthe sense that there are certain points about theCourt judgment that should always occur in theheadnote and certain phrases or certain types ofsentences are always bound to be excluded.
Howto leverage this information is one of the researchquestions we plan to address in the proposed work.Another issue that we plan to address in par-ticular is how to handle the mismatch betweenlengths of a sentence (i.e.
multiple sentences con-sidered to be a single sentence) in the DocumentLanguage when compared to the Summary Lan-guage.
Currently two different languages do varyin the average sentence lengths, for example Ger-103man sentences are in general longer than English.But in our case the ratio of sentence lengths wouldbe almost 3:1 with the Document Language be-ing much longer than their Summary Languagecounterparts.
While most current translation mod-els do have a provision for a penalty on sentencelengths which can make the target sentence longeror shorter, the real challenge lies in finding phraselevel alignments when either the source sentenceor the target sentence is too long compared to theother.
This leads to a large number of phrases hav-ing no alignment at all which is not a commonphenomenon in cases of SMT.In effect we propose to address the followingresearch questions:?
Exploring the major challenges that onemight face when modeling Summarization asa Machine translation problem ??
How to create a sentence aligned parallel cor-pus from a given document and its handwrit-ten summary ??
How to handle the disparity in lengths of sen-tence of Document Language and SummaryLanguage ??
How to reduce the sparsity in training datacreated due to the stylistic differences presentwithin the Documents and Summaries ?3.3 Topic model based sentence generationThe graph based approaches of sentence fusionmentioned above assumes availability of a num-ber of similar sentences from which a word graphcan be formed.
It might not always be easy to getsuch similar sentences, especially in case of sin-gle document summarization.
We wish to explorethe possibility of using topic modeling to extractinformative phrases and entities and then use stan-dard sentence generation techniques to generaterepresentative sentences.4 Preliminary experimentWe would like to conclude by reporting results ofa very preliminary experiment wherein we usedsimple cosine similarity to align sentences be-tween the original Judgments and the manuallygenerated headnotes (summaries).
For a smalltraining set of 1000 document-summary pairs, wecompute the cosine similarity of each sentence inthe judgment to each sentence in the correspond-ing headnote.
Sentences in the judgment whichdo not have a cosine similarity of at least 0.5 withany sentence in the headnote are considered tohave no alignment at all.
The remaining sentencesare aligned to a single best matching sentence inthe headnote.
Hence each sentence in the judg-ment is aligned to exactly one or zero sentences inthe headnote, while each sentence in the headnotecan have a many to one mapping.
All the judg-ment sentences aligned to the same headnote sen-tence are combined to form a single sentence, thusforming a parallel corpus between Judgment Lan-guage and Headnote Language.
Further we usedthe Moses5machine translation toolkit to gener-ate a translation model with the source languageas the judgment (or the document Language) andthe target language as the headnote (or summarylanguage).
Since we have not yet used the entiretraining data, the results in the current experimentwere not very impressive.
But there are certain ex-amples worth reporting, where good results wereindeed obtained.4.1 Example TranslationOriginal: There can in my opinion be no escapefrom the conclusion that section 12 of the Act bywhich a most important protection or safeguardconferred on the subject by the Constitution hasbeen taken away is not a valid provision since itcontravenes the very provision in the Constitu-tion under which the Parliament derived its com-petence to enact it.Translation: There can be no escape from theconclusion that section 12 of the Act by whichsafeguard conferred on the subject by the Consti-tution has been taken away is not valid since it con-travened the very provision in the Constitution un-der which the Parliament derived its competenceto enact it.The highlighted parts in the original sentenceare the ones that have been changed in the cor-responding translation.
We can attribute the ex-clusion of ?in my opinion?
solely to the languagemodel of the Summary Language.
Since the sum-maries are in third person while many statementsin the original judgment would be in first person,such a phrase which is common in the Judgmentwill never occur in the headnote.
Similarly theheadnotes are usually written in past tense and thatmight account for changing ?contravenes?
to ?con-travened?.
We are not sure what the reasons mightbe behind the other changes.
We plan to do an5www.statmt.org/moses104exhaustive error analysis on the results of this ex-periment, which will provide further insights andideas.
We have reported some more examples inthe appendix section.Although not all translations are linguisticallycorrect and many of them don?t make much sense,we believe that by using a larger training cor-pus (which we are currently curating) and a bet-ter technique for creating a sentence aligned cor-pus the results can be significantly improved.
Alsocurrently the target sentences are not much shorterthan their source, and we need to further work onthat issue.
Overall the idea of using SMT for doc-ument summarization seems to be promising andworth pursuing.ReferencesSiddhartha Banerjee, Prasenjit Mitra, and KazunariSugiyama.
2015.
Multi-document abstractive sum-marization using ilp based multi-sentence compres-sion.
In 24th International Joint Conference on Ar-tificial Intelligence (IJCAI).G?unes Erkan and Dragomir R Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
Journal of Artificial IntelligenceResearch, pages 457?479.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of the 23rd international conference oncomputational linguistics, pages 340?348.
Associa-tion for Computational Linguistics.Aria Haghighi and Lucy Vanderwende.
2009.
Ex-ploring content models for multi-document summa-rization.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 362?370.
Association forComputational Linguistics.Kai Hong, John M Conroy, Benoit Favre, AlexKulesza, Hui Lin, and Ani Nenkova.
2014.
A repos-itory of state of the art and competitive baseline sum-maries for generic news summarization.
In LREC.Kai Hong, Mitchell Marcus, and Ani Nenkova.
2015.System combination for multi-document summa-rization.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Process-ing, pages 107?117.
Association for ComputationalLinguistics.Chin-Yew Lin and Eduard Hovy.
2000.
The automatedacquisition of topic signatures for text summariza-tion.
In Proceedings of the 18th conference on Com-putational linguistics-Volume 1, pages 495?501.
As-sociation for Computational Linguistics.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Text summariza-tion branches out: Proceedings of the ACL-04 work-shop, volume 8.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into texts.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.
Association for ComputationalLinguistics.Ani Nenkova and Kathleen McKeown.
2012.
A surveyof text summarization techniques.
In Mining TextData, pages 43?76.
Springer.Dingding Wang and Tao Li.
2012.
Weighted con-sensus multi-document summarization.
InformationProcessing & Management, 48(3):513?523.Kai Zhang, Wei Wu, Haocheng Wu, Zhoujun Li, andMing Zhou.
2014.
Question retrieval with highquality answers in community question answering.In Proceedings of the 23rd ACM International Con-ference on Conference on Information and Knowl-edge Management, pages 371?380.
ACM.105A Additional Examples?
The underlined parts in the original sentence are the ones that are correctly omitted in the targetsentence.
The striked out part in the original sentences are wrongly missing in the translation,affecting the comprehensibility of the sentence.?
The striked out parts in the Translation are the ones that are misplaced in the sentence.
Boldfacedparts in the Translation are the ones newly introduced.?
The boldfaced parts in the Expected Translations are the corrections that are made compared to theactual translation.Original:The Act provides for levy of two kinds of taxes called the general tax and the special tax by the twocharging sections 5 and 10 respectively.
Seervai attempted to make out that the provisions of thecharging sections 5 and 10 fixing Rs.
30000 and Rs.
5000 as the minimum taxable turnover for generaltax and special tax respectively were found discriminatory and void under article 14 read with article13 of the Constitution and he gave us several tables of figures showing how the imposition of the taxactually works out in practice in hypothetical cases.Translation:The Act provides for the levy of the general tax and special tax by the two charging sections 5 and 10respectively.
that the provisions of the charging sections 5 and 10 fixing Rs.
30000 and Rs.
5000 as theminimum taxable turnover for general tax and special tax respectively are discriminatory and void underart of the Constitution and he gave the several tables of figures showing how the imposition of the taxactually works.Expected Tranlsation:The Act provides for the levy of the general tax and special tax by the two charging sections 5 and 10respectively.
Seervai attempted to make out that the provisions of the charging sections 5 and 10 fixingRs.
30000 and Rs.
5000 as the minimum taxable turnover for general tax and special tax respectivelyare discriminatory and void under article 14 read with article 13 of the Constitution and he gave theseveral tables of figures showing how the imposition of the tax actually works.Original:The learned trial magistrate believed the prosecution evidence rejected the pleas raised by the defe-nce convicted the appellants of the charge framed and sentenced them to undergo simple imprisonmentfor two months each.
The appellate court confirmed the conviction of the appellants but reduced theirsentence from simple imprisonment for two months to a fine of Rs.
50 or in default simple imprisonmentfor one month each.Translation:The Magistrate found the appellants of the charge framed and sentenced them to undergo simple im-prisonment for two months guilty.
confirmed the conviction of the appellants but reduced their sentencefrom simple imprisonment for two months to a fine of Rs.
50 or in default simple imprisonment for onemonth each.Expected Tranlsation:The Magistrate found the appellants guilty of the charge framed and sentenced them to undergo simpleimprisonment for two months.
The appellate court confirmed the conviction of the appellants butreduced their sentence from simple imprisonment for two months to a fine of Rs.
50 or in default simpleimprisonment for one month each.106
