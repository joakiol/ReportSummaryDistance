A Report of Recent Progress in Transformation-BasedError-Driven Learning*Eric BrillABSTRACTMost recent research in trainable part of speech taggers hasexplored stochastic tagging.
While these taggers obtain highaccuracy, linguistic information is captured indirectly, typi-cally in tens of thousands of lexical and contextual probabili-ties.
In \[Brill 92\], a trainable rule-based tagger was describedthat obtained performance comparable to that of stochas-tic taggers, but captured relevant linguistic information ina sma\]_l number of simple non-stochastic rules.
In this pa-per, we describe a number of extensions to this rule-basedtagger.
First, we describe a method for expressing lexical re-lations in tagging that stochastic taggers are currently unableto express.
Next, we show a rule-based approach to taggingunknown words.
Finally, we show how the tagger can beextended into a k-best tagger, where multiple tags can beassigned to words in some cases of uncertainty.Spoken Language Systems GroupLaboratory for Computer ScienceMassachusetts Institute of TechnologyCambridge, Massachusetts 02139/that achieves performance comparable to that of stochas-tic taggers.
Training this tagger is fully automated, butunlike trainable stochastic taggers, linguistic informationis encoded directly in a set of simple non-stochastic rules.In this paper, we describe some extensions to this rule-based tagger.
These include a rule-based approach to:lexicalizing the tagger, tagging unknown words, and as-signing the k-best tags to a word.
All of these extensions,as well as the original tagger, are based upon a learn-ing paradigm called transformation-based error-drivenlearning.
This learning paradigm has shown promise ina number of other areas of natural language processing,and we hope that the extensions to transformation-basedlearning described in this paper can carry over to otherdomains of application as well.
21.
INTRODUCTIONWhen automated part of speech tagging was initially ex-plored \[Klein and Simmons 63, Harris 62\], people manu-ally engineered rules for tagging, sometimes with the aidof a corpus.
As large corpora became available, it be-came clear that simple Markov-model based stochastictaggers that were automatical ly trained could achievehigh rates of tagging accuracy \[Jelinek 85\].
Thesestochastic taggers have a number of advantages over themanually built taggers, including obviating the need forlaborious manual rule construction, and possibly captur-ing useful information that may not have been noticed bythe human engineer.
However, stochastic taggers havethe disadvantage that linguistic information is only cap-tured indirectly, in large tables of statistics.
Almost allrecent work in developing automatically trained part ofspeech taggers has been on further exploring Markov-model based tagging \[Jetinek 85, Church 88, DeRose 88,DeMarcken 90, Merialdo 91, Cutting et al 92,Kupiec 92, Charniak et al 93, Weischedel et al 93\].
1In \[Brill 92\], a trainable rule-based tagger is described*This research was supported by ARPA under contract N00014-89-J-1332, monitored through the Office of Naval Research.1Markov-modelbased taggers assign a sentence the tag sequence that maximizesProb(word\[tag) * Prob(taglprevious n tags).2562.
TRANSFORMATION-BASEDERROR-DRIVEN LEARNINGTransformation-based error-driven learning has been ap-plied to a number of natural language problems, includ-ing part of speech tagging, prepositional phrase attach-ment disambiguation, and syntactic parsing \[Brill 92,Brill 93, Brill 93a\].
A similar approach is being exploredfor machine translation \[Su et al 92\].
Figure 1 illus-trates the learning process.
First, unannotated text ispassed through the initial-state annotator.
The initial-state annotator can range in complexity from assign-ing random structure to assigning the output of a so-phisticated manually created annotator.
Once text hasbeen passed through the initial-state annotator, it is thencompared to the truth, 3 and transformations are learnedthat can be applied to the output of the initial stateannotator to make it better resemble the truth.In all of the applications described in this paper, thefollowing greedy search is applied: at each iteration oflearning, the transformation is found whose applicationresuits in the highest score; that transformation is thenadded to the ordered transformation list and the trainingcorpus is updated by applying the learned transforma-tion.
To define a specific application of transformation-2The programs described in this paper are freely available.3As specified in a manually annotated corpus.UNANNOTATEDTExT ISTATEANNO~TAJD I TRUTHRULESFigure 1: Transformation-Based Error-Driven Learning.based learning, one must specify the following: (1) thestart state annotator, (2) the space of transformationsthe learner is allowed to examine, and (3) the scoringfunction for comparing the corpus to the lrulh and choos-ing a transformation.Once an ordered list of transformations is learned, newtext can be annotated by first applying the initial stateannotator to it and then applying each of the learnedtransformations, in order.3.
AN EARL IER ATTEMPTThe original tranformation-based tagger \[Brill 92\] worksas follows.
The start state annotator assigns each wordits most likely tag as indicated in the training corpus.The most likely tag for unknown words is guessed basedon a number of features, such as whether the word iscapitalized, and what the last three letters of the wordare.
The allowable transformation templates are:Change tag a to tag b when:1.
The preceding (following) word is tagged z.2.
The word two before (after) is tagged z.3.
One of the two preceding (following) words is tagged2'.4.
One of the three preceding (following) words istagged z.5.
The  preceding word is tagged z and the followingword is tagged w.6.
The preceding (following)word is tagged z and theword two before (after) is tagged w.where a,b,z and w are variables over the set of parts ofspeech.
To learn a transformation, the learner in essenceapplies every possible transformation, a counts the num-ber of tagging errors after that transformation is applied,and chooses that transformation resulting in the great-est error reduction.
5 Learning stops when no transfor-mations can be found whose application reduces errorsbeyond some prespecified threshold.
An example of atransformation that was learned is: change the taggingof a word from noun to verb  if the previous word istagged as a modal .
Once the system is trained, a newsentence is tagged by applying the start state annotatorand then applying each transformation, in turn, to thesentence.4.
LEX ICAL IZ ING THE TAGGERNo relationships between words are directly captured instochastic taggers.
In the Markov model, state tran-sition probabilities (P(Tagi\]Tagi-z...Tagi_,~)) expressthe likelihood of a tag immediately following n othertags, and emit probabilities (P(WordjlTagi)) expressthe likelihood of a word given a tag.
Many useful rela-tionships, such as that between a word and the previousword, or between a tag and the following word, are notdirectly captured by Markov-model based taggers.
Thesame is true of the earlier transformation-based tagger,where transformation templates did not make referenceto words.To remedy this problem, the transformation-based tag-ger was extended by adding contextual transformationsthat could make reference to words as well as part ofspeech tags.
The transformation templates that wereadded are:Change tag a to tag b when:1.
The preceding (following) word is w.2.
The word two before (after) is w.3.
One of the two preceding (following) words is w.4.
The current word is w and the preceding (following)word is x.4 All possible instantiations of transformation templates.5The search is data-d.riven~ so only a very small percentage ofpossible transformations eed be examined.2575.
The current word is w and the preceding (following)word is tagged z.where w and x are variables over all words in the trainingcorpus, and z is a variable over all parts of speech.Below we list two lexicalized transformations that werelearned.
6Change the tag:Training ://: of RulesCorpus or Context.Method Size (Words) Probs.Stochastic 64 K 6,170Stochastic 1 Million 10,000Rule-Basedw/o Lex.
Rules 600 K 219Rule-BasedWith Lex.
Rules 600 K 267AcE.
(%)96.396.796.997.2(12) From prepos i t ion  to adverb  if the word two po-sitions to the right is as.
(16) From non-3rd  person  s ingu lar  p resent  verb  tobase  fo rm verb  if one of the previous two words is n~t.7The Penn Treebank tagging style manual specifies thatin the collocation as .
.
.
as, the first as is tagged as an ad-verb and the second is tagged as a preposition.
Since asis most frequently tagged as a preposition in the trainingcorpus, the start state tagger will mistag the phrase as~all as as:as /prepos i t ion  tall/adjective as/prepositionThe first lexicalized transformation corrects this mistag-ging.
Note that a stochastic tagger trained on ourtraining set would not correctly tag the first occurrenceof as.
Although adverbs are more likely than prepo-sitions to follow some verb form tags, the fact thatP(aslprcposition ) is much greater than P(as\[adverb),and P(adjectiveIpreposition ) is much greater thanP(adjective\]adverb) lead to as being incorrectly taggedas a preposition by a stochastic tagger.
A trigram tag-ger will correctly tag this collocation in some instances,due to the fact that P(preposition\[adverb adjective) isgreater than P(prepositionlpreposition adjective), butthe outcome will be highly dependent upon the contextin which this collocation appears.The second transformation arises from the fact thatwhen a verb appears in a context such as We do n'~__ or We did n't usually ___, the verb is in base form.A stochastic tr igram tagger would have to capture thislinguistic information indirectly from frequency countsof all trigrams of the form: s* ADVERB PRESENT_VERB* ADVERB BASE_VERB6All experiments were run on the Penn Treebank tagged WallStreet Journal  corpus, version 0.5 \[Marcus et al 93\].7In the Penn Treebank, n'$ is treated as a separate token, sodon't becomes do/VB-NON3rd-SING n'~/ADVERB.SWhere a star can match any part  of speech tag.Table 1: Comparison of Tagging Accuracy With No Un-known WordsADVERB * PRESENT_VERBADVERB * BASE_VERBand from the fact that P(n ' t lADVERB ) is fairly high.In \[Weischedel t al.
93\], results are given when train-ing and testing a Markov-model based tagger on thePenn Treebank Tagged Wall Street Journal Corpus.They cite results making the closed vocabulary assump-tion that all possible tags for all words in the testset are known.
When training contextual probabil-ities on 1 million words, an accuracy of 96.7% wasachieved.
Accuracy dropped to 96.3% when contextualprobabilities were trained on 64,000 words.
We trainedthe transformation-based tagger on 600,000 words fromthe same corpus, making the same closed vocabularyassumption, 9 and achieved an accuracy of 97.2% on aseparate 150,000 word test set.
The transformation-based learner achieved better performance, despite thefact that contextual information was captured in only267 simple nonstochastic rules, as opposed to 10,000 con-textual probabilities that were learned by the stochas-tic tagger.
To see whether lexicalized transformationswere contributing to the accuracy rate, we ran the ex-act same test using the tagger trained using the earliertransformation template set, which contained no trans-formations making reference to words.
Accuracy of thattagger was 96.9%.
Disallowing lexicalized transforma-tions resulted in an 11% increase in the error rate.
Theseresults are summarized in table 1.9In both  \[Weischedel t al.
93\] and here, the test set was incor-porated into the lexicon, but  was not used in learning contextualinformation.
Testing with no unknown words might seem llke anunrealistic test.
We have done so for three reasons (We show re-sults when unknown words are included later in the paper):  (1) toallow for a comparison with previously quoted results, (2) to iso-late known word accuracy from unknown word accuracy, and (3)in some systems, such as a closed vocabulary speech recognitionsystem, the assumpt ion that all words are known is valid.258When transformations are allowed to make reference towords and word pairs, some relevant information is prob-ably missed due to sparse data.
we  are currently explor-ing the possibility of incorporating word classes into therule-based learner in hopes of overcoming this problem.The idea is quite simple.
Given a source of word classinformation, such as WordNet \[Miller 90\], the learner isextended such that  a rule is allowed to make referenceto parts of speech, words, and word classes, allowing forrules such as Change the tag from X to Y if the followingword belongs to word class Z.
This approach as alreadybeen successfully applied to a system for prepositionalphrase disambiguation \[Brill 93a\].5.
UNKNOWN WORDSIn addition to not being lexicalized, another problemwith the original transformation-based tagger was its rel-atively low accuracy at tagging unknown words3 ?
Inthe start state annotator  for tagging, words are assignedtheir most likely tag, estimated from a training corpus.In khe original formulation of the rule-based tagger, arather ad-hoc algorithm was used to guess the most likelytag for words not appearing in the training corpus.
Totry to improve upon unknown word tagging accuracy,we built a transformation-based l arner to learn rules formore accurately guessing the most likely tag for wordsnot seen in the training corpus.
I f  the most likely tag forunknown words can be assigned with high accuracy, thenthe contexual rules can be used to improve accuracy, asdescribed above.In the transformation-based unknown-word tagger, thestart state annotator  naively labels the most likely tagfor unknown words as proper noun if capitalized andcommon noun otherwise, lz...Adding the character string x as a suffix results ina word (Izl <= 4).Adding the character string x as a prefix results ina word (1 :1 <= 4).Word W ever appears immediately to the left (right)of the word.8.
Character Z appears in the word.An unannotated text can be used to check the condi-tions in all of the above transformation templates.
An-notated text is necessary in training to measure the ef-fect of transformations on tagging accuracy.
Below arethe first 10 transformation learned for tagging unknownwords in the Wall Street Journal corpus:Change tag:1.
From common noun to p lu ra l  common noun ifthe word has suffix -s t22.
From common noun to number  if the word hascharacter .3.
From common noun to ad jec t ive  if the word hascharacter -4.
From common noun to past  par t i c ip le  verb  ifthe word has suffix -ed5.
From common noun to gerund  or  p resent  par -t i c ip le  verb  if the word has suffix - ing6.
To ad jec t ive  if adding the suffix - ly  results in awordBelow we list the set of allowable transformations: 7.
To adverb  if the word has suffix - lyChange the guess of the most-likely tag of a word (fromX) to Y if:1.
Deleting the prefix x, Ixl <=4, results in a word (xis any string of length 1 to 4).2.
The first (1,2,3,4) characters of the word are x.3.
Deleting the suffix x, Ix I <= 4, results in a word.'4.
The last (1,2,3,4) characters of the word are x.10 This section describes work done in part while the author wasat the University of Pennsylvania.l l If we change the tagger to tag all unknown words as commonnouns, then a number of rules are learned of the form: change tagto proper noun if the prefix is "E", since the learner is notprovided with the concept of upper case in its set of transformationtemplates.8.
From common noun to number  if the word $ everappears immediately to the left9.
From common noun to ad jec t ive  if the word hassuffix -al10.
From noun to base  fo rm verb  if the word wou ldever appears immediately to the left.Keep in mind that  no specific affixes are prespecified.A transformation can make reference to any string ofcharacters up to a bounded length.
So while the firstrule specifies the English suffix "s", the rule learner also12Note that this transformation will result in the mistaggingof mistress.
The 17th learned rule fixes this problem.
This rulestates: change a tag from plural common noun to singularcommon noun if the word has suffix ss.259considered such nonsensical rules as: change a tag toadjective if the word has suffix "xhqr'.
Also, absolutelyno English-specific information eed be prespecified inthe learner.
13We then ran the following experiment using 1.1 millionwords of the Penn Treebank Tagged Wall Street JournalCorpus.
The first 950,000 words were used for trainingand the next 150,000 words were used for testing.
An-notations of the test corpus were not used in any wayto train the system.
From the 950,000 word trainingcorpus, 350,000 words were used to learn rules for tag-ging unknown words, and 600,000 words were used tolearn contextual rules.
148 rules were learned for taggingunknown words, and 267 contextual tagging rules werelearned.
Unknown word accuracy on the test corpus was85.0%, and overall tagging accuracy on the test corpuswas 96.5%.
To our knowledge, this is the highest over-all tagging accuracy ever quoted on the Penn TreebankCorpus when making the open vocabulary assumption.In \[Weischedel t al.
93\], a statistical approach to tag-ging unknown words is shown.
In this approach, a num-ber of suffixes and important features are prespecified.Then, for unknown words:P(WIT) = p(unknown wordlT) *p(Capitalize-featurelT ) * p(suffixes, hyphenationIT)Using this equation for unknown word emit probabil-ities within the stochastic tagger, an accuracy of 85%was obtained on the Wall Street Journal corpus.
Thisportion of the stochastic model has over 1,000 parame-ters, with 108 possible unique emit probabilities, as op-posed to only 148 simple rules that are learned and usedin the rule-based approach.
We have obtained compa-rable performance on unknown words, while capturingthe information in a much more concise and perspicuousmanner, and without prespecifying any language-specificor corpus-specific nformation.6.
K -BEST  TAGSThere are certain circumstances where one is will-ing to relax the one tag per word requirement in or-der to increase the probability that the correct tagwill be assigned to each word.
In \[DeMarcken 90,Weischedel et al 93\], k-best tags are assigned withina stochastic tagger by returning all tags within somethreshold of probability of being correct for a particularword.We can modify the transformation-based tagger to re-turn multiple tags for a word by making a simple mod-Z3This learner  has  also been appl ied to tagging Old Engl ish.
See\[Srin 93a\].of Rules Accuracy Avg.
-~ of tags per word0 96.5 1.0050 96.9 1.02100 97.4 1.04150 97.9 1.10200 98.4 1.19250 99.1 1.50Table 2: Results from k-best tagging.ification to the contextual transformations describedabove.
The initial-state annotator is the tagging out-put of the transformation-based tagger described above.The allowable transformation templates are the sameas the contextual transformation templates listed above,but with the action change tag X to tag Y modified toadd tag X to tag Y or add tag X to word W. Insteadof changing the tagging of a word, transformations owadd alternative taggings to a word.When allowing more than one tag per word, there is atrade-off between accuracy and the average number oftags for each word.
Ideally, we would like to achieve aslarge an increase in accuracy with as few extra tags aspossible.
Therefore, in training we find transformationsthat maximize precisely this function.In table 2 we present results from first using the one-tag-per-word transformation-based tagger described in theprevious ection and then applying the k-best tag trans-formations.
These transformations were learned from aseparate 240,000 word corpus.
147.
CONCLUSIONSIn this paper, we have described a number of extensionsto previous work in rule-based part of speech tagging,including the ability to make use of lexical relationshipspreviously unused in tagging, a new method for taggingunknown words, and a way to increase accuracy by re-turning more than one tag per word in some instances.We have demonstrated that the rule-based approach ob-tains performance comparable to that of stochastic tag-gets on unknown word tagging and better performanceon known word tagging, despite the fact that the rule-based tagger captures linguistic information in a smallnumber of simple non-stochastic rules, as opposed to?
14Unfortunately,  it is difficult to f ind results  to compare thesek-best  tag results  to.
In \[DeMarcken 90\], the  test set is inc luded inthe t ra in ing set, and  so it is difficult to know how this  sys tem woulddo on fresh text .
In \[Weischedel t al.
93\], a k-best  tag exper imentwas run  on the Wall Street Journa l  corpus.
They  quote the averagenumber  of tags per  word for various thresho ld  sett ings,  but  do notprovide accuracy results.260large numbers of lexical and contextual probabilities.Recently, we have begun to explore the possibility of ex-tending these techniques to both learning pronunciationnetworks for speech recognition and to learning map-pings between sentences and semantic representations.References\[Brill 92\] E. Brill 1992.
A simple rule-based part of speechtagger.
In Proceedings of the Third Conference on Ap-plied Natural Language Processing, Trento, Italy.\[Brill 93\] E. Brill 1993.
Automatic grammar induction andparsing free text: a transformation-based approach.
InProceedings of the 31st Meeting of the Association ofComputational Linguistics, Columbus, Ohio.\[Brill 93a\] E. Brill 1993.
A corpus-based approach to lan-guage learning.
Ph.D. Dissertation, Department ofComputer and Information Science, University of Penn-sylvania.\[Charniak et al 93\] E. Charniak, C. Hendrickson, N. Jacob-son, and M. Perkowitz.
1993.
Equations for part-of-speech tagging.
In Proceedings of Conference of theAmerican Association for Artificial Intelligence (AAAI),?
Washington, D.C.\[Church 88\] K. Church.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.
In Pro-ceedings of the Second Conference on Applied NaturalLanguage Processing, Austin, Texas.\[Cutting et al 92\] D. Cutting, J. Kupiec, J. Pedersen, andP.
Sibun.
1992.
A practical part-of-speech tagger InProceedings of the Third Conference on Applied NaturalLanguage Processing, Trento, Italy.\[DeRose 88\] S. DeRose 1988.
Grammatical category dis-ambiguation by statistical optimization.
ComputationalLinguistics, Volume 14.\[DeMarcken 90\] C. DeMarcken.
1990.
Parsing the LOB cor-pus.
In Proceedings ofthe 1990 Conference ofthe Asso-ciation for Computational Linguistics.\[Harris 62\] Z. Harris.
1962.
String Analysis of LanguageStructure, Mouton and Co., The Hague.\[Klein and Simmons 63\] S. Klein and R. Simmons.
1963.
Acomputational pproach to grammatical coding of En-glish words.
JACM, Volume 10.\[Jelinek 85\] F. Jelinek.
1985.
Markov source modeling oftext generation.
In Impact of Processing Techniques onCommunication.
3.
Skwirzinski, ed., Dordrecht.\[Kupiec 92\] J. Kupiec.
1992.
Robust part-of-speech taggingusing a hidden Markov model.
Computer Speech andLanguage.\[Marcus et al 93\] M. Marcus, B. Santorini, and M.Marcinkiewicz.
1993.
Building a large annotated corpusof English: the Penn Treebank.
Computational Linguis-tics, Volume.
19.\[Merialdo 91\] B. Merialdo.
1991.
Tagging text with a prob-abilistic model.
In 1EEE International Conference onAcoustics, Speech and Signal Processing.\[Mi//er 90J G. Miller.
1990.
WordNet: an on-line lexicaldatabase.
International Journal of Lexicography.261\[Suet al 92\] K. Su, M. Wu, and J. Chang.
1992.
Anew quantitative quality measure, for machine transla-tion Systems.
In Proceedings of COLING-92, Nantes,France.\[Weischedel t al.
93\] R. Weischedel, M. Meteer, R.Schwartz, L. Ramshaw, and J. Palmucci.
1993.
Copingwith ambiguity and unknown words through probabilis-tic models.
Computational Linguistics, Volume 19.
