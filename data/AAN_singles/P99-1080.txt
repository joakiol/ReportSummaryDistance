A Pylonic Decision-Tree Language Model with Optimal QuestionSelectionAdr ian  CorduneanuUniversity of Toronto73 Saint George St #299Toronto, Ontario, M5S 2E5, Canadag7adrian@cdf.toronto.eduAbst rac tThis paper discusses a decision-tree approach tothe problem of assigning probabilities to wordsfollowing a given text.
In contrast with previ-ous decision-tree language model attempts, analgorithm for selecting nearly optimal questionsis considered.
The model is to be tested on astandard task, The Wall Street Journal, allow-ing a fair comparison with the well-known tri-gram model.1 In t roduct ionIn many applications such as automatic speechrecognition, machine translation, spelling cor-rection, etc., a statistical language model (LM)is needed to assign ~probabilities to sentences.This probability assignment may be used, e.g.,to choose one of many transcriptions hypoth-esized by the recognizer or to make deci-sions about capitalization.
Without any lossof generality, we consider models that oper-ate left-to-right on the sentences, assigning aprobability to the next word given its wordhistory.
Specifically, we consider statisticalLM's which compute probabilities of the typeP{wn \]Wl, W2,..-, Wn--1}, where wi denotes thei-th word in the text.Even for a small vocabulary, the space ofword histories is so large that any attempt oestimate the conditional probabilities for eachdistinct history from raw frequencies i  infea-sible.
To make the problem manageable, onepartitions the word histories into some classesC(wl ,w2, .
.
.
,Wn-1) ,  and identifies the wordprobabilities with P{wn \[ C(wl ,  w2,.
.
.
, Wn-1)}.Such probabilities are easier to estimate as eachclass gets significantly more counts from a train-ing corpus.
With this setup, building a languagemodel becomes a classification problem: groupthe word histories into a small number of classes606while preserving their predictive power.Currently, popular N-gram models classifythe word histories by their last N - 1 words.N varies from 2 to 4 and the trigram modelP{wn \[Wn-2, wn-1} is commonly used.
Al-though these simple models perform surpris-ingly well, there is much room for improvement.The approach used in this paper is to classifythe histories by means of a decision tree: to clus-ter word histories Wl,W2,... ,wn-1 for whichthe distributions of the following word Wn ina training corpus are similar.
The decision treeis pylonic in the sense that histories at differentnodes in the tree may be recombined in a newnode to increase the complexity of questions andavoid data fragmentation.The method has been tried before (Bahl et al,1989) and had promising results.
In the workpresented here we made two major changes tothe previous attempts: we have used an opti-mal tree growing algorithm (Chou, 1991) notknown at the time of publication of (Bahl etal., 1989), and we have replaced the ad-hoc clus-tering of vocabulary items used by Bahl with adata-driven clustering scheme proposed in (Lu-cassen and Mercer, 1984).2 Descr ip t ion  o f  the  Mode l2.1 The  Decis ion-Tree Classif ierThe purpose of the decision-tree classifier is tocluster the word history wl, w2, .
.
.
,  Wn-1 into amanageable number of classes Ci, and to esti-mate for each class the next word conditionaldistribution P{wn \[C i}.
The classifier, togetherwith the collection of conditional probabilities,is the resultant LM.The general methodology of decision treeconstruction is well known (e.g., see (Jelinek,1998)).
The following issues need to be ad-dressed for our specific application.?
A tree growing criterion, often called themeasure of purity;?
A set of permitted questions (partitions) tobe considered at each node;?
A stopping rule, which decides the numberof distinct classes.These are discussed below.
Once the tree hasbeen grown, we address one other issue: theestimation of the language model at each leaf ofthe resulting tree classifier.2.1.1 The Tree  Growing  Cr i te r ionWe view the training corpus as a set of orderedpairs of the following word wn and its word his-tory (wi,w2,... ,wn- i ) .
We seek a classifica-tion of the space of all histories (not just thoseseen in the corpus) such that a good conditionalprobability P{wn I C(wi ,  w2,.
.
.
, Wn- i )} can beestimated for each class of histories.
Since sev-eral vocabulary items may potentially followany history, perfect "classification" or predic-tion of the word that follows a history is outof the question, and the classifier must parti-tion the space of all word histories maximizingthe probability P{wn I C(wi ,  w2, .
.
.
, Wn-i)} as"signed to the pairs in the corpus.We seek a history classification such thatC(wi ,w2, .
.
.
,Wn- i )  is as informative as pos-sible about the distribution of the next word.Thus, from an information theoretical point ofview, a natural cost function for choosing ques-tions is the empirical conditional entropy of thetraining data with respect o the tree:H = - Z I c,)log f (w  I C,).w iEach question in the tree is chosen so as tominimize the conditional entropy, or, equiva-lently, to maximize the mutual information be-tween the class of a history and the predictedword.2.1.2 The Set of Quest ions andDecision Py lonsAlthough a tree with general questions can rep-resent any classification of the histories, somerestrictions must be made in order to make theselection of an optimal question computation-ally feasible.
We consider elementary questionsof the type w-k E S, where W-k refers to thek-th position before the word to be predicted,607y/ n( Dnyes noFigure 1: The structure of a pylonand S is a subset of the vocabulary.
However,this kind of elementary question is rather sim-plistic, as one node in the tree cannot refer totwo different history positions.
A conjunction ofelementary questions can still be implementedover a few nodes, but similar histories becomeunnecessarily fragmented.
Therefore a node inthe tree is not implemented asa single elemen-tary question, but as a modified ecision tree initself, called a pylon (Bahl et al, 1989).
Thetopology of the pylon as in Figure 1 allows usto combine answers from elementary questionswithout increasing the number of classes.
A py-lon may be of any size, and it is grown as astandard ecision tree.2.1.3 Quest ion Selection With in  thePy lonFor each leaf node and position k the problemis to find the subset S of the vocabulary thatminimizes the entropy of the split W-k E S.The best question over all k's will eventuallybe selected.
We will use a greedy optimizationalgorithm developed by Chou (1991).
Given apartition P = {81,/32,...,/3k} of the vocabu-lary, the method finds a subset S of P for whichthe reduction of entropy after the split is nearlyoptimal.The algorithm is initialized with a randompartition S t2 S of P. At each iteration everyatom 3 is examined and redistributed into a newpartition S'U S', according to the following rule:place j3 into S' whenl(wlw-kcf~) < Ew f (w lw-k  e 3) log I(w w_heS) --E,o f (wlw_  3) log f(wlW-kEC3)where the f 's are word frequencies computedrelative to the given leaf.
This selection crite-rion ensures a decreasing empirical entropy ofthe tree.
The iteration stops when S = S' andIf questions on the same level in the pylon areconstructed independently with the Chou algo-ritm, the overall entropy may increase.
That iswhy nodes whose children are merged must bejointly optimized.
In order to reduce complex-ity, questions on the same level in the pylon areasked with respect o the same position in thehistory.The Chou algorithm is not accurate when thetraining data is sparse.
For instance, when nohistory at the leaf has w-k E /3, the atom isinvariantly placed in S'.
Because such a choiceof a question is not based on evidence, it is notexpected to generalize to unseen data.
As thetree is growing, data is fragmented among theleaves, and this issue becomes unavoidable.
Todeal with this problem, we choose the atomicpartition P so that  each atom gets a historycount above a threshold.The choice of such an atomic partition is acomplex problem, as words composing an atommust have similar predictive power.
Our ap-proach is to consider a hierarchical c assificationof the words, and prune it to a level at whicheach atom gets sufficient history counts.
Theword hierarchy is generated from training datawith an information theoretical algorithm (Lu-cassen and Mercer, 1984) detailed in section 2.2.2.1.4 The  Stopping RuleA common problem of all decision trees is thelack of a clear rule for when to stop growingnew nodes.
The split of a node always bringsa reduction in the estimated entropy, but thatmight not hold for the true entropy.
We use asimplified version of cross-validation (Breimanet al, 1984), to test for the significance of thereduction in entropy.
If the entropy on a heldout data set is not reduced, or the reductionon the held out text is less than 10% of theentropy reduction on the training text, the leafis not split, because the reduction in entropyhas failed to generalize to the unseen data.2.1.5 Est imat ing the Language Modelat Each LeafOnce an equivalence classification of all histo-ries is constructed, additional training data isused to estimate the conditional probabilitiesrequired for each node, as described in (Bahl etal., 1989).
Smoothing as well as interpolationwith a standard trigram model eliminates thezero probabilities.2.2 The  Hierarchical  Classif ication ofWordsThe goal is to build a binary tree with the wordsof the vocabulary as leaves, such that similarwords correspond to closely related leaves.
Apartition of the vocabulary can be derived fromsuch a hierarchy by taking a cut through thetree to obtain a set of subtrees.
The reason forkeeping a hierarchy instead of a fixed partitionof the vocabulary is to be able to dynamicallyadjust the partition to accommodate for train-ing data fragmentation.The hierarchical classification of words wasbuilt with an entirely data-driven method.
Themotivation is that even though an expert couldexhibit some strong classes by looking at partsof speech and synonyms, it is hard to produce afull hierarchy of a large vocabulary.
Perhaps acombination of the expert and data-driven ap-proaches would give the best result.
Neverthe-less, the algorithm that has been used in deriv-ing the hierarchy can be initialized with classesbased on parts of speech or meaning, thus tak-ing account of prior expert information.The approach is to construct he tree back-wards.
Starting with single-word classes, eachiteration consists of merging the two classesmost similar in predicting the word that followsthem.
The process continues until the entire vo-cabulary is in one class.
The binary tree is thenobtained from the sequence of merge operations.To quantify the predictive power of a parti-tion P = {j3z,/32,...,/3k} of the vocabulary welook at the conditional entropy of the vocabu-lary with respect o class of the previous word:H(w I P) = EZeP p(/3)H(w \[ w-1 ?/3) =- E epp(/3) E evp(wl )logp(w I/3)At each iteration we merge the two classesthat minimize H(w I P') - H(w I P), where P' isthe partition after the merge.
In information-theoretical terms we seek the merge that bringsthe least reduction in the information providedby P about the distribution of the current word.608IRAN'SUNION'SIRAQ'SINVESTORS'BANKS'PEOPLE'SFARMERTEACHERWORKERDRIVERWRITERSPECIAL ISTEXPERTTRADERPLUMMETEDPLUNGEDSOAREDTUMBLEDSURGEDRALLIEDFALLINGFALLSRISENFALLENMYSELFH IMSELFOURSELVESTHEMSELVESCONSIDERABLYS IGNIF ICANTLYSUBSTANTIALLYSOMEWHATSLIGHTLYFigure 2: Sample classes from a 1000-elementpartition of a 5000-word vocabulary (each col-umn is a different class)The algorithm produced satisfactory resultson a 5000-word vocabulary.
One can see fromthe sample classes that the automatic buildingof the hierarchy accounts both for similarity inmeaning and of parts of speech.the vocabulary is significantly larger, makingimpossible the estimation of N-gram models forN > 3.
However, we expect that due to thegood smoothing of the trigram probabilities acombination of the decision-tree and N-grammodels will give the best results.4 SummaryIn this paper we have developed a decision-treemethod for building a language model that pre-dicts words given their previous history.
Wehave described a powerful question search algo-rithm, that guarantees the local optimality ofthe selection, and which has not been appliedbefore to word language models.
We expectthat the model will perform significantly betterthan the standard N-gram approach.5 AcknowledgmentsI would like to thank Prof.Frederick Jelinek and Sanjeev Khu-dampur from Center for Language and Speech Processing,Johns Hopkins University, for their help related to this workand for providing the computer resources.
I also wish to thankProf.Graeme Hirst from University of Toronto for his usefuladvice in all the stages of this project.3 Evaluat ion of the Mode lThe decision tree is being trained and testedon the Wall Street Journal corpus from 1987 to1989 containing 45 million words.
The data isdivided into 15 million words for growing thenodes, 15 million for cross-validation, 10 mil-lion for estimating probabilities, and 5 millionfor testing.
To compare the results with othersimilar attempts (Bahl et al, 1989), the vocab-ulary consists of only the 5000 most frequentwords and a special "unknown" word that re-places all the others.
The model tries to predictthe word following a 20-word history.At the time this paper was written, the im-plementation of the presented algorithms wasnearly complete and preliminary results on theperformance of the decision tree were expectedsoon.
The evaluation criterion to be used isthe perplexity of the test data with respect othe tree.
A comparison with the perplexityof a standard back-off trigram model will in-dicate which model performs better.
Althoughdecision-tree l tter language models are inferiorto their N-gram counterparts (Potamianos andJelinek, 1998), the situation should be reversedfor word language models.
In the case of wordsReferencesL.
R. Bahl, P. F. Brown, P. V. de Souza, andR.
L. Mercer.
1989.
A tree-based statisticallanguage model for natural language speechrecognition.
IEEE Transactions on Acous-tics, Speech, and Signal Processing, 37:1001-1008.L.
Breiman, J. Friedman, R. Olshen, andC.
Stone.
1984.
Classification and regressiontrees.
Wadsworth and Brooks, Pacific Grove.P.
A. Chou.
1991.
Optimal partitioning forclassification and regression trees.
IEEETransactions on Pattern Analysis and Ma-chine Intelligence, 13:340-354.F.
Jelinek.
1998.
Statistical methods \]or speechrecognition.
The MIT Press, Cambridge.J.
M. Lucassen and R. L. Mercer.
1984.
Aninformation theoretic approach to the auto-matic determination of phonemic baseforms.In Proceedings of the 1984 International Con--ference on Acoustics, Speech, and Signal Pro-cessing, volume III, pages 42.5.1-42.5.4.G.
Potamianos and F. Jelinek.
1998.
A studyof n-gram and decision tree letter languagemodeling methods.
Speech Communication,24:171-192.609
