RESEARCH IN  CONTINUOUS SPEECHRECOGNIT ION AT DRAGON SYSTEMS UNDERTHE DARPA SLS PROGRAMJanet Baker, Larry Gillick, and Robert RothDragon Systems, Inc.320 Nevada St.Newton, MA 02160PROJECT GOALSThe primary long term goal of the speech research atDragon Systems is to develop algorithms that allow usto achieve high performance large vocabulary continuousspeech recognition.
At the same time, we are concernedto keep the computational demands of our algorithms asmodest as possible, so that the results of our research canbe incorporated into products that will run on modestlypriced personal computers.RECENT RESULTSIn the past year (1991) Dragon has greatly modifiedits signal processing and its modeling, with a focus onachieving more accurate speech recognition performance.At the beginning of the year, Dragon was using 8 spec-tral parameter signal processing and was training up Re-source Management speakers via adaptation of a refer-ence speaker's models.
Each PIC (phoneme-in-context)was modeled as a linear sequence of unimodal outputdistributions (PELs), with the sequence of PELs beingchosen once and for all, based on one reference speaker.The next step in our research was to allow the sequenceof PELs to be determined in a speaker dependent way:we called this process "respelling the PICs".
Dragonthen went on to experiment with several new signal pro-cessing representations involving the use of 32 parame-ters.
To make use of the new signal processing param-eters (which were a superset of our original 8), it wasnecessary to have an automatic way of generating a newset of possible output distributions (PELs).
PreviouslyPELs had been generated through a sort of human-aidedclustering algorithm - a spectrogram labeler initializeda new PEL when he observed an acoustic event that hedeemed to be new.
An automatic lustering algorithmwas developed as an alternative to this labor intensivetask.
This new process was called "rePELing."
Based onthese changes and numerous more minor ones, we wereable to lower our word error rate (without rapid match)*This work was sponsored by the Defense Advanced ResearchProjects Agency and was monitored by the Space and Naval War-fare Systems Command under contract N00039-86-C-0307.on the RM1 development test data from 5.1% last Febru-ary to 2.3% by August (using respelling, rePELing, andinverse Fourier transform based cepstral and differencecepstral parameters).We decided at this point to address the limitations in-herent in using unimodal representations for our outputdistributions, and we embarked on a project to developa new set of training programs that would be far moreflexible and would allow us to more accurately model the~true variability of speech.
Thus Dragon spent the lastthird of 1991 implementing a Baum-Welch training algo-rithm that estimates tied mixture output distributionsfor each state of each PIC (for an arbitrary choice of in-dependent streams of parameters).
The code supportsBayesian smoothing of the mixture weights, which is akey element of the algorithm since there is rarely enoughdata for the MLE alone to be an adequate stimator.
Wehave debugged our system by focusing on the special caseof 32 independent s reams, with equally spaced univari-ate basis distributions for each parameter.
We have alsosimultaneously developed a new training strategy for ourrapid match models, based on the idea of manufactur-ing the rapid match model for a word from the HiddenMarkov Model for the word.PLANS FOR THE COMING YEARWe plan to continue working with our new training pro-grams for HMMs and Rapid Match, with a strong focuson building speaker independent models.
We will beexploring a variety of strategies for choosing basis dis-tributions and for choosing streams.
So far we have nottapped the ability of tied mixtures to model the statisti-cal dependence among the parameters.
We also plan towork on reducing the memory requirements for our tiedmixture models by clustering the PICs and the PELs,and to generally improve the computational efficiencyof our implementations.
Another major project for thecoming year will be the development of a user interface(with error correction facilities) and the addition of thecapability for adding words to the vocabulary on the fly.471
