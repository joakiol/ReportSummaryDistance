Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 272?280,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsPredicting Risk from Financial Reports with RegressionShimon KoganMcCombs School of BusinessUniversity of Texas at AustinAustin, TX 78712, USAshimon.kogan@mccombs.utexas.eduDimitry LevinMellon College of ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAdimitrylevin@gmail.comBryan R. RoutledgeTepper School of BusinessCarnegie Mellon UniversityPittsburgh, PA 15213, USAroutledge@cmu.eduJacob S. SagiOwen Graduate School of ManagementVanderbilt UniversityNashville, TN 37203, USAJacob.Sagi@Owen.Vanderbilt.eduNoah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAnasmith@cs.cmu.eduAbstractWe address a text regression problem: given apiece of text, predict a real-world continuousquantity associated with the text?s meaning.
Inthis work, the text is an SEC-mandated finan-cial report published annually by a publicly-traded company, and the quantity to be pre-dicted is volatility of stock returns, an empiri-cal measure of financial risk.
We apply well-known regression techniques to a large cor-pus of freely available financial reports, con-structing regression models of volatility forthe period following a report.
Our models ri-val past volatility (a strong baseline) in pre-dicting the target variable, and a single modelthat uses both can significantly outperformpast volatility.
Interestingly, our approach ismore accurate for reports after the passage ofthe Sarbanes-Oxley Act of 2002, giving someevidence for the success of that legislation inmaking financial reports more informative.1 IntroductionWe consider a text regression problem: given a pieceof text, predict a R-valued quantity associated withthat text.
Specifically, we use a company?s annualfinancial report to predict the financial risk of invest-ment in that company, as measured empirically by aquantity known as stock return volatility.Predicting financial risk is of clear interest toanyone who invests money in stocks and centralto modern portfolio choice.
Financial reports area government-mandated artifact of the financialworld that?one might hypothesize?contain a largeamount of information about companies and theirvalue.
Indeed, it is an important question whethermandated disclosures are informative, since they aremeant to protect investors but are costly to produce.The intrinsic properties of the problem are attrac-tive as a test-bed for NLP research.
First, there isno controversy about the usefulness or existentialreality of the output variable (volatility).
Statisti-cal NLP often deals in the prediction of variablesranging from text categories to linguistic structuresto novel utterances.
While many of these targets areuncontroversially useful, they often suffer from eval-uation difficulties and disagreement among annota-tors.
The output variable in this work is a statisticsummarizing facts about the real world; it is not sub-ject to any kind of human expertise, knowledge, orintuition.
Hence this prediction task provides a new,objective test-bed for any kind of linguistic analysis.Second, many NLP problems rely on costly anno-tated resources (e.g., treebanks or aligned bilingualcorpora).
Because the text and historical financialdata used in this work are freely available (by law)and are generated as a by-product of the American272economy, old and new data can be obtained by any-one with relatively little effort.In this paper, we demonstrate that predicting fi-nancial volatility automatically from a financial re-port is a novel, challenging, and easily evaluated nat-ural language understanding task.
We show that avery simple representation of the text (essentially,bags of unigrams and bigrams) can rival and, incombination, improve over a strong baseline thatdoes not use the text.
Analysis of the learned modelsprovides insights about what can make this problemmore or less difficult, and suggests that disclosure-related legislation led to more transparent reporting.2 Stock Return VolatilityVolatility is often used in finance as a measure ofrisk.
It is measured as the standard deviation ofa stock?s returns over a finite period of time.
Astock will have high volatility when its price fluctu-ates widely and low volatility when its price remainsmore or less constant.Let rt = PtPt?1 ?
1 be the return on a given stockbetween the close of trading day t ?
1 and day t,where Pt is the (dividend-adjusted) closing stockprice at date t. The measured volatility over the timeperiod from day t?
?
to day t is equal to the samples.d.:v[t?
?,t] =?????
?i=0(rt?i ?
r?)2/?
(1)where r?
is the sample mean of rt over the period.
Inthis work, the above estimate will be treated as thetrue output variable on training and testing data.It is important to note that predicting volatility isnot the same as predicting returns or value.
Ratherthan trying to predict how well a stock will perform,we are trying to predict how stable its price will beover a future time period.
It is, by now, receivedwisdom in the field of economics that predicting astock?s performance, based on easily accessible pub-lic information, is difficult.
This is an attribute ofwell-functioning (or ?efficient?)
markets and a cor-nerstone of the so-called ?efficient market hypoth-esis?
(Fama, 1970).
By contrast, the idea that onecan predict a stock?s level of risk using public in-formation is uncontroversial and a basic assumptionmade by many economically sound pricing mod-els.
A large body of research in finance suggeststhat the two types of quantities are very different:while predictability of returns could be easily tradedaway by the virtue of buying/selling stocks that areunder- or over-valued (Fama, 1970), similar tradesare much more costly to implement with respect topredictability of volatility (Dumas et al, 2007).
Byfocusing on volatility prediction, we avoid takinga stance on whether or not the United States stockmarket is informationally efficient.3 Problem FormulationGiven a text document d, we seek to predict thevalue of a continuous variable v. We do this via aparameterized function f :v?
= f(d;w) (2)where w ?
Rd are the parameters or weights.
Ourapproach is to learn a human-interpretable w froma collection of N training examples {?di, vi?
}Ni=1,where each di is a document and each vi ?
R.Support vector regression (Drucker et al, 1997)is a well-known method for training a regressionmodel.
SVR is trained by solving the following op-timization problem:minw?Rd12?w?2+CNN?i=1max(0,??
?vi ?
f(di;w)????
)?
??
?-insensitive loss function(3)where C is a regularization constant and  controlsthe training error.1 The training algorithm findsweights w that define a function f minimizing the(regularized) empirical risk.Let h be a function from documents into somevector-space representation?
Rd.
In SVR, the func-tion f takes the form:f(d;w) = h(d)>w =N?i=1?iK(d,di) (4)where Equation 4 re-parameterizes f in terms of akernel function K with ?dual?
weights ?i.
K can1Given the embedding h of documents in Rd,  definesa ?slab?
(region between two parallel hyperplanes, some-times called the ?-tube?)
in Rd+1 through which each?h(di), f(di;w)?
must pass in order to have zero loss.273year words documents words/doc.1996 5.5M 1,408 3,8931997 9.3M 2,260 4,1321998 11.8M 2,462 4,8081999 14.5M 2,524 5,7432000 13.4M 2,425 5,5412001 15.4M 2,596 5,9282002 22.7M 2,846 7,9832003 35.3M 3,612 9,7802004 38.9M 3,559 10,9362005 41.9M 3,474 12,0652006 38.8M 3,308 11,736total 247.7M 26,806 9,240Table 1: Dimensions of the dataset used in this paper,after filtering and tokenization.
The near doubling in av-erage document size during 2002?3 is possibly due to thepassage of the Sarbanes-Oxley Act of 2002 in the wakeof Enron?s accounting scandal (and numerous others).be seen as a similarity function between two docu-ments.
At test time, a new example is compared to asubset of the training examples (those with ?i 6= 0);typically with SVR this set is sparse.
With the linearkernel, the primal and dual weights relate linearly:w =N?i=1?ih(di) (5)The full details of SVR and its implementation arebeyond the scope of this paper; interested readers arereferred to Scho?lkopf and Smola (2002).
SVMlight(Joachims, 1999) is a freely available implementa-tion of SVR training that we used in our experi-ments.24 DatasetIn the United States, the Securities Exchange Com-mission mandates that all publicly-traded corpora-tions produce annual reports known as ?Form 10-K.?
The report typically includes information aboutthe history and organization of the company, equityand subsidiaries, as well as financial information.These reports are available to the public and pub-lished on the SEC?s web site.3 The structure of the10-K is specified in detail in the legislation.
We havecollected 54,379 reports published over the period2Available at http://svmlight.joachims.org.3http://www.sec.gov/edgar.shtml1996?2006 from 10,492 different companies.
Eachreport comes with a date of publication, which is im-portant for tying the text back to the financial vari-ables we seek to predict.From the perspective of predicting future events,one section of the 10-K is of special interest: Section7, known as ?management?s discussion and anal-ysis of financial conditions and results of opera-tions?
(MD&A), and in particular Subsection 7A,?quantitative and qualitative disclosures about mar-ket risk.?
Because Section 7 is where the most im-portant forward-looking content is most likely tobe found, we filter other sections from the reports.The filtering is done automatically using a short,hand-written Perl script that seeks strings looselymatching the Section 7, 7A, and 8 headers, finds thelongest reasonable ?Section 7?
match (in words) ofmore than 1,000 whitespace-delineated tokens.Section 7 typically begins with an introductionlike this (from ABC?s 1998 Form 10-K, before to-kenization for readability; boldface added):The following discussion and analysis ofABC?s consolidated financial condition andconsolidated results of operation should beread in conjunction with ABC?s Consoli-dated Financial Statements and Notes theretoincluded elsewhere herein.
This discus-sion contains certain forward-looking state-ments which involve risks and uncertain-ties.
ABC?s actual results could differ mate-rially from the results expressed in, or impliedby, such statements.
See ?Regarding Forward-Looking Statements.
?Not all of the documents downloaded pass the fil-ter at all, and for the present work we have only useddocuments that do pass the filter.
(One reason for thefailure of the filter is that many 10-K reports includeSection 7 ?by reference,?
so the text is not directlyincluded in the document.
)In addition to the reports, we used the Centerfor Research in Security Prices (CRSP) US StocksDatabase to obtain the price return series along withother firm characteristics.4 We proceeded to calcu-late two volatilities for each firm/report observation:the twelve months prior to the report (v(?12)) andthe twelve months after the report (v(+12)).4The text and volatility data are publicly available at http://www.ark.cs.cmu.edu/10K.274Tokenization was applied to the text, includingpunctuation removal, downcasing, collapsing alldigit sequences,5 and heuristic removal of remnantmarkup.
Table 1 gives statistics on the corporaused in this research; this is a subset of the cor-pus for which there is no missing volatility informa-tion.
The drastic increase in length during the 2002?2003 period might be explained by the passage bythe US Congress of the Sarbanes-Oxley Act of 2002(and related SEC and exchange rules), which im-posed revised standards on reporting practices ofpublicly-traded companies in the US.5 Baselines and Evaluation MethodVolatility displays an effect known as autoregressiveconditional heteroscedasticity (Engle, 1982).
Thismeans that the variance in a stock?s return tends tochange gradually.
Large changes in price are pre-saged by other changes, and periods of stability tendto continue.
Volatility is, generally speaking, notconstant, yet prior volatility (e.g., v(?12)) is a verygood predictor of future volatility (e.g., v(+12)).
Atthe granularity of a year, which we consider herebecause the 10-K reports are annual, there are noexisting models of volatility that are widely agreedto be significantly more accurate than our histor-ical volatility baseline.
We tested a state-of-the-art model known as GARCH(1, 1) (Engle, 1982;Bollerslev, 1986) and found that it was no strongerthan our historical volatility baseline on this sample.Throughout this paper, we will report perfor-mance using the mean squared error between thepredicted and true log-volatilities:6MSE = 1N ?N ??i=1(log(vi)?
log(v?i))2 (6)where N ?
is the size of the test set, given in Table 1.6 ExperimentsIn our experiments, we vary h (the function thatmaps inputs to a vector space) and the subset of the5While numerical information is surely informative aboutrisk, recall that our goal is to find indicators of risk expressed inthe text; automatic predictors of risk from numerical data woulduse financial data streams directly, not text reports.6We work in the log domain because it is standard in finance,due to the dynamic range of actual volatilities; the distributionover log v across companies tends to have a bell shape.data used for training.
We will always report perfor-mance over test sets consisting of one year?s worthof data (the subcorpora described in Table 1).
Inthis work, we focus on predicting the volatility overthe year following the report (v(+12)).
In all experi-ments,  = 0.1 and C is set using the default choiceof SVMlight , which is the inverse of the average ofh(d)>h(d) over the training data.76.1 Feature RepresentationWe first consider how to represent the 10-K reports.We adopt various document representations, all us-ing word features.
Let M be the vocabulary sizederived from the training data.8 Let freq(xj ;d) de-note the number of occurrences of the jth word inthe vocabulary in document d.?
TF: hj(d) = 1|d| freq(xj ;d), ?j ?
{1, ...,M}.?
TFIDF: hj(d) = 1|d| freq(xj ;d)?
log(N/|{d :freq(xj ;d) > 0}|), where N is the number ofdocuments in the training set.
This is the classic?TFIDF?
score.?
LOG1P: hj(d) = log(1 + freq(xj ;d)).
Ratherthan normalizing word frequencies as for TF,this score dampens them with a logarithm.
Wealso include a variant of LOG1P where termsare the union of unigrams and bigrams.Note that each of these preserves sparsity; whenfreq(xj ;d) = 0, hj(d) = 0 in all cases.For interpretability of results, we use a linear ker-nel.
The usual bias weight b is included.
We foundit convenient to work in the logarithmic domain forthe predicted variable, predicting log v instead of v,since volatility is always nonnegative.
In this setting,the predicted volatility takes the form:log v?
= b+M?j=1wjhj(d) (7)Because the goal of this work is to explore how textmight be used to predict volatility, we also wish7These values were selected after preliminary and cursoryexploration with 1996?2000 as training data and 2001 as thetest set.
While the effects of  and C were not large, furtherimprovements may be possible with more careful tuning.8Preliminary experiments that filtered common or rarewords showed a negligible or deleterious effect on performance.275features 2001 2002 2003 2004 2005 2006 micro-ave.history v(?12) (baseline) 0.1747 0.1600 0.1873 0.1442 0.1365 0.1463 0.1576v(?12) (SVR with bias) 0.2433 0.4323 0.1869 0.2717 0.3184 5.6778 1.2061v(?12) (SVR without bias) 0.2053 0.1653 0.2051 0.1337 0.1405 0.1517 0.1655wordsTF 0.2219 0.2571 0.2588 0.2134 0.1850 0.1862 0.2197TFIDF 0.2033 0.2118 0.2178 0.1660 0.1544 0.1599 0.1842LOG1P 0.2107 0.2214 0.2040 0.1693 0.1581 0.1715 0.1873LOG1P, bigrams 0.1968 0.2015 ?0.1729 0.1500 0.1394 0.1532 0.1667bothTF+ 0.1885 0.1616 0.1925 ?0.1230 ?0.1272 ?0.1402 ?0.1541TFIDF+ 0.1919 0.1618 0.1965 ?0.1246 ?0.1276 ?0.1403 ?0.1557LOG1P+ 0.1846 0.1764 ?0.1671 ?0.1309 ?0.1319 0.1458 ?0.1542LOG1P+, bigrams 0.1852 0.1792 ?0.1599 ?0.1352 ?0.1307 0.1448 ?0.1538Table 2: MSE (Eq.
6) of different models on test data predictions.
Lower values are better.
Boldface denotesimprovements over the baseline, and ?
denotes significance compared to the baseline under a permutation test (p <0.05).to see whether text adds information beyond whatcan be predicted using historical volatility alone (thebaseline, v(?12)).
We therefore consider modelsaugmented with an additional feature, defined ashM+1 = log v(?12).
Since this is historical informa-tion, it is always available when the 10-K report ispublished.
These models are denoted TF+, TFIDF+,and LOG1P+.The performance of these models, compared tothe baseline from Section 5, is shown in Table 2.We used as training examples all reports from thefive-year period preceding the test year (so six ex-periments on six different training and test sets areshown in the figure).
We also trained SVR modelson the single feature v(?12), with and without biasweights (b in Eq.
7); these are usually worse andnever signficantly better than the baseline.Strikingly, the models that use only the text topredict volatility come very close to the historicalbaseline in some years.
That a text-only method(LOG1P with bigrams) for predicting future riskcomes within 5% of the error of a strong baseline(2003?6) shows promise for the overall approach.A combined model improves substantially over thebaseline in four out of six years (2003?6), and thisdifference is usually robust to the representationused.
Table 3 shows the most strongly weightedterms in each of the text-only LOG1P models (in-cluding bigrams).
These weights are recovered us-ing the relationship expressed in Eq.
5.6.2 Training Data EffectsIt is well known that more training data tend to im-prove the performance of a statistical method; how-ever, the standard assumption is that the trainingdata are drawn from the same distribution as the testdata.
In this work, where we seek to predict thefuture based on data from past, that assumption isobviously violated.
It is therefore an open questionwhether more data (i.e., looking farther into the past)is helpful for predicting volatility, or whether it isbetter to use only the most recent data.Table 4 shows how performance varies when one,two, or five years of historical training data are used,averaged across test years.
In most cases, usingmore training data (from a longer historical period)is helpful, but not always.
One interesting trend,not shown in the aggregate statistics of Table 4,is that recency of the training set affected perfor-mance much more strongly in earlier train/test splits(2001?3) than later ones (2004?6).
This experimentleads us to conclude that temporal changes in fi-nancial reporting make training data selection non-trivial.
Changes in the macro economy and spe-cific businesses make older reports less relevant forprediction.
For example, regulatory changes likeSarbanes-Oxley, variations in the business cycle,and technological innovation like the Internet influ-ence both the volatility and the 10-K text.6.3 Effects of Sarbanes-OxleyWe noted earlier that the passage of the Sarbanes-Oxley Act of 2002, which sought to reform financialreporting, had a clear effect on the lengths of the10-K reports in our collection.
But are the reportsmore informative?
This question is important, be-cause producing reports is costly; we present an em-pirical argument based on our models that the legis-2761996?20001997?20011998?20021999?20032000?20042001?2005netloss0.026year#0.028loss0.023loss0.026loss0.025loss0.026highv?year#0.024netloss0.023netloss0.020netloss0.020netloss0.017netloss0.018?loss0.020expenses0.020expenses0.017expenses0.017year#0.016goingconcern0.014expenses0.019loss0.020year#0.015goingconcern0.015expenses0.015expenses0.014covenants0.017experienced0.015obligations0.015year#0.015goingconcern0.014agoing0.014diluted0.014of$#0.015financing0.014financing0.014agoing0.013personnel0.013convertible0.014covenants0.015convertible0.014agoing0.014administrative0.013financing0.013date0.014additional0.014additional0.014additional0.013personnel0.013administrative0.012longterm-0.014mergeragreement-0.015unsecured-0.012distributions-0.012distributions-0.011policies-0.011rates-0.015dividends-0.015earnings-0.012annual-0.012insurance-0.011bythe-0.011dividend-0.015unsecured-0.017distributions-0.012dividend-0.012criticalaccounting-0.012earnings-0.011unsecured-0.015dividend-0.017dividends-0.015dividends-0.012lowerinterest-0.012dividends-0.012mergeragreement-0.017properties-0.018income-0.016rates-0.013dividends-0.013unsecured-0.012properties-0.018netincome-0.019properties-0.016properties-0.015properties-0.014properties-0.013income-0.021income-0.021netincome-0.019rate-0.019rate-0.017rate-0.014?rate-0.022rate-0.025rate-0.022netincome-0.023netincome-0.021netincome-0.018lowv?Table3:Moststrongly-weightedtermsinmodelslearnedfromvarioustimeperiods(LOG1Pmodelwithunigramsandbigrams).?#?denotesanydigitsequence.
features 1 2 5TF+ 0.1509 0.1450 0.1541TFIDF+ 0.1512 0.1455 0.1557LOG1P+ 0.1621 0.1611 0.1542LOG1P+, bigrams 0.1617 0.1588 0.1538Table 4: MSE of volatility predictions using reports fromvarying historical windows (1, 2, and 5 years), micro-averaged across six train/test scenarios.
Boldface marksbest in a row.
The historical baseline achieves 0.1576MSE (see Table 2).lation has actually been beneficial.Our experimental results in Section 6.1, in whichvolatility in the years 2004?2006 was more accu-rately predicted from the text than in 2001?2002,suggest that the Sarbanes-Oxley Act led to more in-formative reports.
We compared the learned weights(LOG1P+, unigrams) between the six overlappingfive-year windows ending in 2000?2005; measuredin L1 distance, these were, in consecutive order,?52.2, 59.9, 60.7, 55.3, 52.3?
; the biggest differ-ences came between 2001 and 2002 and between2002 and 2003.
(Firms are most likely to have be-gun compliance with the new law in 2003 or 2004.
)The same pattern held when only words appearingin all five models were considered.
Variation in therecency/training set size tradeoff (?6.2), particularlyduring 2002?3, also suggests that there were sub-stantial changes in the reports during that time.6.4 Qualitative EvaluationOne of the advantages of a linear model is that wecan explore what each model discovers about dif-ferent unigram and bigram terms.
Some manuallyselected examples of terms whose learned weights(w) show interesting variation patterns over time areshown in Figure 1, alongside term frequency pat-terns, for the text-only LOG1P model (with bigrams).These examples were suggested by experts in fi-nance from terms with weights that were both largeand variable (across training sets).A particularly interesting case, in light ofSarbanes-Oxley, is the term accounting policies.Sarbanes-Oxley mandated greater discussion of ac-counting policy in the 10-K MD&A section.
Be-fore 2002 this term indicates high volatility, per-haps due to complicated off-balance sheet transac-tions or unusual accounting policies.
Starting in2002, explicit mention of accounting policies indi-27700.20.40.60.8ave.termfrequency-0.015-0.010-0.00500.005waccounting policiesestimates-0.010-0.00500.005wreitmortgages-0.010-0.00500.0050.01096-00 97-01 98-02 99-03 00-04 01-05whigher marginlower margin00.050.100.150.20ave.termfrequency02468ave.termfrequencyFigure 1: Left:learned weights forselected terms acrossmodels trained ondata from differenttime periods (x-axis).These weights arefrom the LOG1P(unigrams andbigrams) modelstrained on five-yearperiods, the samemodels whoseextreme weights aresummarized inTab.
3.
Note that allweights are within0?
0.026.
Right: theterms?
averagefrequencies (bydocument) over thesame periods.cates lower volatility.
The frequency of the termalso increases drastically over the same period, sug-gesting that the earlier weights may have been in-flated.
A more striking example is estimates, whichaverages one occurrence per document even in the1996?2000 period, experiences the same term fre-quency explosion, and goes through a similar weightchange, from strongly indicating high volatility tostrongly indicating low volatility.As a second example, consider the terms mort-gages and reit (Real Estate Investment Trust, a taxdesignation for businesses that invest in real estate).Given the importance of the housing and mortgagemarket over the past few years, it is interesting tonote that the weight on both of these terms increasesover the period from a strong low volatility term to aweak indicator of high volatility.
It will be interest-ing to see how the dramatic decline in housing pricesin late 2007, and the fallout created in credit marketsin 2008, is reflected in future models.Finally, notice that high margin and low mar-gin, whose frequency patterns are fairly flat ?switchplaces,?
over the sample: first indicating high andlow volatility, respectively, then low and high.
Thereis no a priori reason to expect high or low marginswould be associated with high or low stock volatil-ity.
However, this is an interesting example wherebigrams are helpful (the word margin by itself isuninformative) and indicates that predicting risk ishighly time-dependent.6.5 DelistingAn interesting but relatively infrequent phenomenonis the delisting of a company, i.e., when it ceases tobe traded on a particular exchange due to dissolutionafter bankruptcy, a merger, or violation of exchangerules.
The relationship between volatility and delist-ing has been studied by Merton (1974), among oth-ers.
Our dataset includes a small number of caseswhere the volatility figures for the period followingthe publication of a 10-K report are unavailable be-cause the company was delisted.
Learning to predictdelisting is extremely difficult because fewer than4% of the 2001?6 10-K reports precede delisting.Using the LOG1P representation, we built a lin-ear SVM classifier for each year in 2001?6 (trainedon the five preceding years?
data) to predict whethera company will be delisted following its 10-K re-port.
Performance for various precision measures isshown in Table 5.
Notably, for 2001?4 we achieve278precision (%) at ... ?01 ?02 ?03 ?04 ?05 ?06recall = 10% 80 93 79 100 47 21n = 5 100 100 40 100 60 80n = 10 80 90 70 90 60 70n = 100 38 48 53 29 24 20oracle F1 (%) 35 42 44 36 31 166 bulletin, creditors, dip, otc5 court4 chapter, debtors, filing, prepetition3 bankruptcy2 concern, confirmation, going, liquidation1 debtorinpossession, delisted, nasdaq, petitionTable 5: Left: precision of delisting predictions.
The ?oracle F1?
row shows the maximal F1 score obtained for anyn.
Right: Words most strongly predicting delisting of a company.
The number is how many of the six years (2001?6)the word is among the ten most strongly weighted.
There were no clear patterns across years for words predicting thata company would not be delisted.
The word otc refers to ?over-the-counter?
trading, a high-risk market.above 75% precision at 10% recall.
Our best (or-acle) F1 scores occur in 2002 and 2003, suggestingagain a difference in reports around Sarbanes-Oxley.Table 5 shows words associated with delisting.7 Related WorkIn NLP, regression is not widely used, since mostnatural language-related data are discrete.
Regres-sion methods were pioneered by Yang and Chute(1992) and Yang and Chute (1993) for informationretrieval purposes, but the predicted continuous vari-able was not an end in itself in that work.
Bleiand McAuliffe (2007) used latent ?topic?
variablesto predict movie reviews and popularity from text.Lavrenko et al (2000b) and Lavrenko et al (2000a)modeled influences between text and time series fi-nancial data (stock prices) using language models.Farther afield, Albrecht and Hwa (2007) used SVRto train machine translation evaluation metrics tomatch human evaluation scores and compared tech-niques using correlation.
Regression has also beenused to order sentences in extractive summarization(Biadsy et al, 2008).While much of the information relevant for in-vestors is communicated through text (rather thannumbers), only recently is this link explored.
Somepapers relate news articles to earning forecasts, stockreturns, volatility, and volume (Koppel and Shtrim-berg, 2004; Tetlock, 2007; Tetlock et al, 2008; Gaa,2007; Engelberg, 2007).
Das and Chen (2001) andAntweiler and Frank (2004) ask whether messagesposted on message boards can help explain stockperformance, while Li (2005) measures the associ-ation between frequency of words associated withrisk and subsequent stock returns.
Weiss-Hanley andHoberg (2008) study initial public offering disclo-sures using word statistics.
Many researchers havefocused the related problem of predicting sentimentand opinion in text (Pang et al, 2002; Wiebe andRiloff, 2005), sometimes connected to extrinsic val-ues like prediction markets (Lerman et al, 2008).In contrast to text regression, text classificationcomprises a widely studied set of problems involv-ing the prediction of categorial variables related totext.
Applications have included the categorizationof documents by topic (Joachims, 1998), language(Cavnar and Trenkle, 1994), genre (Karlgren andCutting, 1994), author (Bosch and Smith, 1998),sentiment (Pang et al, 2002), and desirability (Sa-hami et al, 1998).
Text categorization has served asa test application for nearly every machine learningtechnique for discrete classification.8 ConclusionWe have introduced and motivated a new kind oftask for NLP: text regression, in which text is usedto make predictions about measurable phenomenain the real world.
We applied the technique to pre-dicting financial volatility from companies?
10-K re-ports, and found text regression model predictionsto correlate with true volatility nearly as well as his-torical volatility, and a combined model to performeven better.
Further, improvements in accuracy andchanges in models after the passage of the Sarbanes-Oxley Act suggest that financial reporting reformhas had interesting and measurable effects.AcknowledgmentsThe authors are grateful to Jamie Callan, Chester Spatt,Anthony Tomasic, Yiming Yang, and Stanley Zin forhelpful discussions, and to the anonymous reviewers foruseful feedback.
This research was supported by grantsfrom the Institute for Quantitative Research in Finanaceand from the Center for Analytical Research in Technol-ogy at the Tepper School of Business, Carnegie MellonUniversity.279ReferencesJ.
S. Albrecht and R. Hwa.
2007.
Regression forsentence-level MT evaluation with pseudo references.In Proc.
of ACL.W.
Antweiler and M. Z. Frank.
2004.
Is all that talkjust noise?
the information content of internet stockmessage boards.
Journal of Finance, 59:1259?1294.F.
Biadsy, J. Hirschberg, and E. Filatova.
2008.
Anunsupervised approach to biography production usingWikipedia.
In Proc.
of ACL.D.
M. Blei and J. D. McAuliffe.
2007.
Supervised topicmodels.
In Advances in NIPS 21.T.
Bollerslev.
1986.
Generalized autoregressive con-ditional heteroskedasticity.
Journal of Econometrics,31:307?327.R.
Bosch and J. Smith.
1998.
Separating hyperplanesand the authorship of the disputed Federalist papers.American Mathematical Monthly, 105(7):601?608.W.
B. Cavnar and J. M. Trenkle.
1994. n-gram-basedtext categorization.
In Proc.
of SDAIR.S.
Das and M. Chen.
2001.
Yahoo for Amazon: Ex-tracting market sentiment from stock mesage boards.In Proc.
of Asia Pacific Finance Association AnnualConference.H.
Drucker, C. J. C. Burges, L. Kaufman, A. Smola, andV.
Vapnik.
1997.
Support vector regression machines.In Advances in NIPS 9.B.
Dumas, A. Kurshev, and R. Uppal.
2007.
Equilibriumportfolio strategies in the presence of sentiment riskand excess volatility.
Swiss Finance Institute ResearchPaper No.
07-37.J.
Engelberg.
2007.
Costly information processing: Ev-idence from earnings announcements.
Working paper,Northwestern University.R.
F. Engle.
1982.
Autoregressive conditional het-eroscedasticity with estimates of variance of unitedkingdom inflation.
Econometrica, 50:987?1008.E.
F. Fama.
1970.
Efficient capital markets: A reviewof theory and empirical work.
Journal of Finance,25(2):383?417.C.
Gaa.
2007.
Media coverage, investor inattention, andthe market?s reaction to news.
Working paper, Univer-sity of British Columbia.T.
Joachims.
1998.
Text categorization with support vec-tor machines: Learning with many relevant features.In Proc.
of ECML.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In Advances in Kernel Methods - SupportVector Learning.
MIT Press.J.
Karlgren and D. Cutting.
1994.
Recognizing text gen-res with simple metrics using discriminant analysis.
InProc.
of COLING.M.
Koppel and I. Shtrimberg.
2004.
Good news or badnews?
let the market decide.
In AAAI Spring Sympo-sium on Exploring Attitude and Affect in Text: Theo-ries and Applications.V.
Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie,D.
Jensen, and J. Allan.
2000a.
Language models forfinancial news recommendation.
In Proc.
of CIKM.V.
Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie,D.
Jensen, and J. Allan.
2000b.
Mining of concurrenttext and time series.
In Proc.
of KDD.K.
Lerman, A. Gilder, M. Dredze, and F. Pereira.
2008.Reading the markets: Forecasting public opinion ofpolitical candidates by news analysis.
In COLING.F.
Li.
2005.
Do stock market investors understand therisk sentiment of corporate annual reports?
WorkingPaper, University of Michigan.R.
Merton.
1974.
On the pricing of corporate debt: Therisk structure of interest rates.
Journal of Finance,29:449?470.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learningtechniques.
In Proc.
of EMNLP.M.
Sahami, S. Dumais, D. Heckerman, and E. Horvitz.1998.
A Bayesian approach to filtering junk email.
InProc.
of AAAI Workshop on Learning for Text Catego-rization.B.
Scho?lkopf and A. J. Smola.
2002.
Learning with Ker-nels: Support Vector Machines, Regularization, Opti-mization, and Beyond.
MIT Press.P.
C. Tetlock, M. Saar-Tsechansky, and S. Macskassy.2008.
More than words: Quantifying language tomeasure firms?
fundamentals.
Journal of Finance,63(3):1437?1467.P.
C. Tetlock.
2007.
Giving content to investor senti-ment: The role of media in the stock market.
Journalof Finance, 62(3):1139?1168.K.
Weiss-Hanley and G. Hoberg.
2008.
Strategic disclo-sure and the pricing of initial public offerings.
Work-ing paper.J.
Wiebe and E. Riloff.
2005.
Creating subjective andobjective sentence classifiers from unannotated texts.In CICLing.Y.
Yang and C. G. Chute.
1992.
A linear least squares fitmapping method for information retrieval from naturallanguage texts.
In Proc.
of COLING.Y.
Yang and C. G. Chute.
1993.
An application of leastsquares fit mapping to text information retrieval.
InProc.
of SIGIR.280
