Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 11?19,Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational LinguisticsUsing character overlap to improve language transformationSander WubbenTilburg UniversityP.O.
Box 901355000 LE TilburgThe Netherlandss.wubben@uvt.nlEmiel KrahmerTilburg UniversityP.O.
Box 901355000 LE TilburgThe Netherlandse.j.krahmer@uvt.nlAntal van den BoschRadboud University NijmegenP.O.
Box 91036500 HD NijmegenThe Netherlandsa.vandenbosch@let.ru.nlAbstractLanguage transformation can be definedas translating between diachronically dis-tinct language variants.
We investigate thetransformation of Middle Dutch into Mod-ern Dutch by means of machine transla-tion.
We demonstrate that by using char-acter overlap the performance of the ma-chine translation process can be improvedfor this task.1 IntroductionIn this paper we aim to develop a system to para-phrase between diachronically distinct languagevariants.
For research into history, historical lin-guistics and diachronic language change, histori-cal texts are of great value.
Specifically from ear-lier periods, texts are often the only forms of in-formation that have been preserved.
One prob-lem that arises when studying these texts is thedifference between the language the text is writ-ten in and the modern variant that the researcherswho want to study the texts know and speak them-selves.
It takes a great deal of deciphering and in-terpretation to be able to grasp these texts.
Our aimis to facilitate scholars such as historians who donot possess extensive knowledge of Middle Dutchwho are studying medieval texts.
We do this by at-tempting to generate literal translations of the sen-tences in the text into modern language.
In par-ticular we focus on the task of translating MiddleDutch to modern Dutch.
The transformation be-tween language variants, either synchronically ordiachronically, can be seen as a paraphrase and atranslation task, as it is often impossible to catego-rize two languages as either variants or differentlanguages.We define Middle Dutch as a collection ofclosely related West Germanic dialects that werespoken and written between 1150 and 1500 in thearea that is now defined as the Netherlands andparts of Belgium.
One of the factors that makeMiddle Dutch difficult to read is the fact that atthe time no overarching standard language existed.Modern Dutch is defined as Dutch as spoken from1500.
The variant we investigate is contempo-rary Dutch.
An important difference with regularparaphrasing is the amount of parallel data avail-able.
The amount of parallel data for the vari-ant pair Middle Dutch - Modern Dutch is sev-eral orders of magnitude smaller than bilingualparallel corpora typically used in machine trans-lation (Koehn, 2005) or monolingual parallel cor-pora used for paraphrase generation by machinetranslation (Wubben et al 2010).We do expect many etymologically relatedwords to show a certain amount of characteroverlap between the Middle and Modern variants.An example of the data is given below, from thework ?Van den vos Reynaerde?
(?About Reynardthe Fox?
), part of the Comburg manuscript thatwas written between 1380-1425.
Here, the firsttext is the original text, the second text is amodern translation in Dutch by Walter Verniersand a translation in English is added below thatfor clarity.
?Doe al dat hof versamet was,Was daer niemen, sonder die das,Hine hadde te claghene over Reynaerde,Den fellen metten grijsen baerde.?
?Toen iedereen verzameld was,was er niemand -behalve de das-die niet kwam klagen over Reynaert,die deugniet met zijn grijze baard.?
?When everyone was gathered,there was noone -except the badger-who did not complain about Reynaert,that rascal with his grey beard.
?11We can observe that although the two Dutchtexts are quite different, there is a large amountof character overlap in the words that are used.Our aim is to use this character overlap to compen-sate for the lower amount of data that is available.We compare three different approaches to trans-late Middle Dutch into Modern Dutch: a standardPhrase-Based machine translation (PBMT) ap-proach, a PBMT approach with additional prepro-cessing based on Needleman-Wunsch sequencealignment, and a character bigram based PBMTapproach.
The PBMT approach with preprocess-ing identifies likely translations based on characteroverlap and adds them as a dictionary to improvethe statistical alignment process.
The PBMT ap-proach based on character bigrams rather thantranslating words, transliterates character bigramsand in this way improves the transformation pro-cess.
We demonstrate that these two approachesoutperform standard PBMT in this task, and thatthe PBMT transliteration approach based on char-acter bigrams performs best.2 Related workLanguage transformation by machine translationwithin a language is a task that has not been stud-ied extensively before.
Related work is the studyby Xu et al(2012).
They evaluate paraphrase sys-tems that attempt to paraphrase a specific style ofwriting into another style.
The plays of WilliamShakespeare and the modern translations of theseworks are used in this study.
They show that theirmodels outperform baselines based on dictionar-ies and out-of-domain parallel text.
Their workdiffers from our work in that they target writingin a specific literary style and we are interested intranslating between diachronic variants of a lan-guage.Work that is slightly comparable is the work byZeldes (2007), who extrapolates correspondencesin a small parallel corpus taken from the Modernand Middle Polish Bible.
The correspondences areextracted using machine translation with the aimof deriving historical grammar and lexical items.A larger amount of work has been published aboutspelling normalization of historical texts.
Baronand Rayson (2008) developed tools for research inEarly Modern English.
Their tool, VARD 2, findscandidate modern form replacements for spellingvariants in historical texts.
It makes use of adictionary and a list of spelling rules.
By plug-ging in other dictionaries and spelling rules, thetool can be adapted for other tasks.
Kestemont etal.
(2010) describe a machine learning approachto normalize the spelling in Middle Dutch Textfrom the 12th century.
They do this by convertingthe historical spelling variants to single canonical(present-day) lemmas.
Memory-based learning isused to learn intra-lemma spelling variation.
Al-though these approaches normalize the text, theydo not provide a translation.More work has been done in the area of trans-lating between closely related languages and deal-ing with data sparsity that occurs within these lan-guage pairs (Hajic?
et al 2000; Van Huyssteenand Pilon, 2009).
Koehn et al(2003) have shownthat there is a direct negative correlation betweenthe size of the vocabulary of a language and theaccuracy of the translation.
Alignment modelsare directly affected by data sparsity.
Uncommonwords are more likely to be aligned incorrectly toother words or, even worse, to large segments ofwords (Och and Ney, 2003).
Out of vocabulary(OOV) words also pose a problem in the trans-lation process, as systems are unable to providetranslations for these words.
A standard heuristicis to project them into the translated sentence un-translated.Various solutions to data sparsity have beenstudied, among them the use of part-of-speechtags, suffixes and word stems to normalizewords (Popovic and Ney, 2004; De Gispert andMarino, 2006), the treatment of compound wordsin translation (Koehn and Knight, 2003), translit-eration of names and named entities, and advancedmodels that combine transliteration and transla-tion (Kondrak et al 2003; Finch et al 2012)or learn unknown words by analogical reason-ing (Langlais and Patry, 2007).Vilar et al(2007) investigate a way to han-dle data sparsity in machine translation betweenclosely related languages by translating betweencharacters as opposed to words.
The words in theparallel sentences are segmented into characters.Spaces between words are marked with a specialcharacter.
The sequences of characters are thenused to train a standard machine translation modeland a language model with n-grams up to n = 16.They apply their system to the translation betweenthe related languages Spanish and Catalan, andfind that a word based system outperforms their12letter-based system.
However, a combined sys-tem performs marginally better in terms of BLEUscores.Tiedemann (2009) shows that combiningcharacter-based translation with phrase-basedtranslation improves machine translation qualityin terms of BLEU and NIST scores when trans-lating between Swedish and Norwegian if theOOV-words are translated beforehand with thecharacter-based model.Nakov and Tiedemann (2012) investigate theuse of character-level models in the translation be-tween Macedonian and Bulgarian movie subtitles.Their aim is to translate between the resource poorlanguage Macedonian to the related language Bul-garian, in order to use Bulgarian as a pivot in or-der to translate to other languages such as English.Their research shows that using character bigramsshows improvement over a word-based baseline.It seems clear that character overlap can be usedto improve translation quality in related languages.We therefore use character overlap in languagetransformation between two diachronic variants ofa language.3 This studyIn this study we investigate the task of translatingfrom Middle Dutch to Modern Dutch.
Similarlyto resource poor languages, one of the problemsthat are apparent is the small amount of parallelMiddle Dutch - Modern Dutch data that is avail-able.
To combat the data sparseness we aim to usethe character overlap that exists between the Mid-dle Dutch words and their Modern Dutch counter-parts.
Examples of overlap in some of the wordsgiven in the example can be viewed in Table1.
Weare interested in the question how we can use thisoverlap to improve the performance of the transla-tion model.
We consider three approaches: (A)Perform normal PBMT without any preprocess-ing, (B) Apply a preprocessing step in which wepinpoint words and phrases that can be alignedbased on character overlap and (C) perform ma-chine translation not to words but to character bi-grams in order to make use of the character over-lap.We will first discuss the PBMT baseline, fol-lowed by the PBMT + overlap system and thecharacter bigram PBMT transliteration system inSection 4.
We then describe the experiment withhuman judges in Section 6, and its results in Sec-tion 7.
We close with a discussion of our results inSection 8.Middle Dutch Modern Dutchversamet verzameldwas wasniemen niemanddie dedas dasclaghene klagenover overReynaerde Reynaertmetten met zijngrijsen grijzebaerde baardTable 1: Examples of character overlap in wordsfrom a fragment of ?Van den vos Reynaerde?4 Language transformation Models4.1 PBMT baselineFor our baseline we use the Moses software totrain a phrase based machine translation (PBMT)model (Koehn et al 2007).
In general, a statisticalmachine translation model normally finds a besttranslation E?
of a text in language F for a textin language E by combining a translation modelP (F |E) with a language model P (E):E?
= arg maxE?E?P (F |E)P (E)In phrase-based machine translation the sen-tence F is segmented into a sequence of I phrasesduring decoding.
Each source phrase is then trans-lated into a target phrase to form sentence E.Phrases may be reordered.The GIZA++ statistical alignment package(Och and Ney, 2003) is used to perform the wordalignments, which are later combined into phrasealignments in the Moses pipeline to build thelanguage transformation model.
GIZA++ imple-ments IBM Models 1 to 5 and an HMM wordalignment model to find statistically motivatedalignments between words.
We first tokenize ourdata.
We then lowercase all data and use all sen-tences from the Modern Dutch part of the cor-pus to train an n-gram language model with theSRILM toolkit (Stolcke, 2002).
Then we run theGIZA++ aligner using the training pairs of sen-tences in Middle Dutch and Modern Dutch.
We13execute GIZA++ with standard settings and weoptimize using minimum error rate training withBLEU scores.
The Moses decoder is used to gen-erate the translations.4.2 PBMT with overlap-based alignmentBefore using the Moses pipeline we perform apreprocessing alignment step based on characteroverlap.
Word and phrase pairs that exhibit a largeamount of character overlap are added to the par-allel corpus that GIZA++ is trained on.
Every timewe find a phrase or word pair with large overlap itis added to the corpus.
This helps bias the align-ment procedure towards aligning similar wordsand reduces alignment errors.
To perform the pre-processing step we use the Needleman-Wunschalgorithm (Needleman and Wunsch, 1970).
TheNeedleman-Wunsch algorithm is a dynamic pro-gramming algorithm that performs a global align-ment on two sequences.
Sequence alignment isa method to find commonalities in two (or more)sequences of some items or characters.
One of-ten used example is the comparison of sequencesof DNA to find evolutionary differences and sim-ilarities.
Sequence alignment is also used in lin-guistics, where it is applied to finding the longestcommon substring or the differences or similari-ties between strings.The Needleman-Wunsch algorithm is a se-quence alignment algorithm that optimizes a scorefunction to find an optimal alignment of a pair ofsequences.
Each possible alignment is scored ac-cording to the score function, where the alignmentgiving the highest similarity score is the optimalalignment of a pair of sequences.
If more thanone alignment yields the highest score, there aremultiple optimal solutions.
The algorithm usesan iterative matrix to calculate the optimal solu-tion.
All possible pairs of characters containingone character from each sequence are plotted ina 2-dimensional matrix.
Then, all possible align-ments between those characters can be representedby pathways through the matrix.
Insertions anddeletions are allowed, but can be penalized bymeans of a gap penalty in the alignment.The first step is to initialize the matrix and fill inthe gap scores in the top row and leftmost column.In our case we heuristically set the values of thescores to +1 for matches, -2 for mismatches and-1 for gaps after evaluating on our developmentset.
After initialization, we can label the cellsin the matrix C(i, j) where i = 1, 2, ..., N andj = 1, 2, ...,M , the score of any cell C(i, j) isthen the maximum of:qdiag = C(i?
1, j ?
1) + s(i, j)qdown = C(i?
1, j) + gqright = C(i, j ?
1) + gHere, s(i, j) is the substitution score for let-ters i and j, and g is the gap penalty score.
If iand j match, the substitution score is in fact thematching score.
The table is filled this way recur-sively, filling each cell with the maximum score ofthe three possible options (diagonally, down andright).
After this is done, an optimal path canbe found by performing a traceback, starting inthe lower right corner of the table and ending inthe upper left corner, by visiting cells horizontally,vertically or diagonally, but only those cells withthe highest score.
After this process we end upwith an alignment.We use the Needleman-Wunsch algorithm tofind an optimal alignment of the Middle Dutch- Modern Dutch sentence pairs.
We regard eachline as a sentence.
In case of rhyming text, afrequent phenomenon in Middle Dutch text, linesare usually parts of sentences.
We then considereach line a string, and we try to align as manycharacters and whitespaces to their equivalents inthe parallel line.
We split the aligned sentencesin each position where two whitespaces align andwe consider the resulting aligned words or phrasesas alignments.
For each aligned word or phrasepair we calculate the Jaccard coefficient and if thatis equal or higher than a threshold we add thealigned words or phrases to the training material.We heuristically set this threshold to 0.5.
By usingthis method we already can find many-to-one andone-to-many alignments.
In this way we help theGIZA++ alignment process by biasing it towardsaligning words and phrases that show overlap.
Ta-ble 2 illustrates this process for two lines.4.3 Character bigram transliterationAnother somewhat novel approach we proposefor Language Transformation is Character-basedtransliteration.
To circumvent the problem of14Middle Dutch: hine hadde+ ++te claghene over Reynaerde ,Modern Dutch: di+e ++niet kwam klag+en+ over Reynaer+t ,Jaccard 0.4 0.14 0 0.63 1 0.70 1Middle Dutch: +den fe++llen met++ten grijsen baerde .Modern Dutch: die+ deugniet met zijn grijze+ baard+ .Jaccard 0.50 0.09 0.50 0.71 0.8 1Table 2: Alignment of lines with Jaccard scores for the aligned phrases.
A + indicates a gap introducedby the Needleman Wunsch alignment.OOV-words and use the benefits of character over-lap more directly in the MT system, we builda translation model based on character bigrams,similar to (Nakov and Tiedemann, 2012).
Wherethey use this approach to translate between closelyrelated languages, we use it to translate betweendiachronic variants of a language.
The sentencesin the parallel corpus are broken into charac-ter bigrams, with a special character representingwhitespaces.
These bigrams are used to train thetranslation model and the language model.
An ex-ample of the segmentation process is displayed inTable 3.
We train an SRILM language model onthe character bigrams and model sequences of upto 10 bigrams.
We then run the standard Mosespipeline, using GIZA++ with standard settings togenerate the phrase-table and we use the Mosesdecoder to decode the bigram sequences.
A num-ber of sample entries are shown in Table 4.
As afinal step, we recombine the bigrams into words.The different sizes of the Phrase-table for the dif-ferent approaches can be observed in Table 5.original segmentedHine #H Hi in ne e#hadde #h ha ad dd de e#te #t te e#claghene #c cl la ag gh he en ne e#over #o ov ve er r#Reynaerde #R Re ey yn na ae er rd de e#, #, ,#Table 3: Input and output of the character bigramsegmenter5 Data SetOur training data consists of various Middle Dutchliterary works with their modern Dutch transla-tion.
A breakdown of the different works is in#d da at t# en n# #d da aa ar#d da at t# et t# #s st#d da at t# et t# #s#d da at t# ie et t# #s st#d da at t# ie et t# #s#d da at t# la an n##d da at t# le et t##d da at t# n# #d da aa ar ro#d da at t# n# #d da aa ar#d da at t# n##d da at t# rd da at t##d da at ts s# #d da at t##d da at ts si i# #h he eb bb be en#d da at ts #d da at t##d da at ts #w wa at t##d da at tt tu #w wa at t# #jTable 4: Example entries from the character bi-gram Phrase-table, without scores.system linesPBMT 20,092PBMT + overlap 27,885character bigram transliteration 93,594Table 5: Phrase-table sizes of the different modelsTable 6.
All works are from the Digital Library ofDutch Literature1.
?Middle Dutch?
is a very broaddefinition.
It encompasses all Dutch language spo-ken and written between 1150 and 1500 in theNetherlands and parts of Belgium.
Works stem-ming from different centuries, regions and writerscan differ greatly in their orthography and spellingconventions.
No variant of Dutch was consideredstandard or the norm; Middle Dutch can be con-sidered a collection of related lects (regiolects, di-alects).
This only adds to the problem of data1http://www.dbnl.org15sparsity.
Our test set consists of a selection ofsentences from the Middle Dutch work Beatrijs,a Maria legend written around 1374 by an anony-mous author.source text lines date of originVan den vos Reynaerde 3428 around 1260Sint Brandaan 2312 12th centuryGruuthuuse gedichten 224 around 1400?t Prieel van Trojen 104 13th centuryVarious poems 42 12th-14th cent.Table 6: Middle Dutch works in the training setMiddle Si seide: ?Ic vergheeft u dan.Dutch Ghi sijt mijn troest voer alle manModern Ze zei: ?ik vergeef het je dan.Dutch Je bent voor mij de enige manPBMT Ze zei : ?
Ik vergheeft u dan .Gij ze alles in mijn enige voor al manPBMT + Ze zei : ?
Ik vergheeft u dan .Overlap dat ze mijn troest voor al manChar.
Bigram Ze zeide : ?
Ik vergeeft u dan .PBMT Gij zijt mijn troost voor alle manMiddle Dat si daer snachts mochte bliven.Dutch ?Ic mocht u qualijc verdriven,?Modern omdat ze nu niet verder kon reizen.Dutch ?Ik kan u echt de deur niet wijzen,?PBMT dat ze daar snachts kon bliven .?
Ik kon u qualijc verdriven , ?PBMT + dat ze daar s nachts kon blijven .Overlap ?
Ik kon u qualijc verdriven , ?Char.
Bigram dat zij daar snachts mocht blijven .PBMT ?
Ik mocht u kwalijk verdrijven ,Table 7: Example output6 ExperimentIn order to evaluate the systems, we ran an exper-iment to collect human rankings of the output ofthe systems.
We also performed automatic evalu-ation.6.1 MaterialsBecause of the nature of our data, in which sen-tences often span multiple lines, it is hard to eval-uate the output on the level of separate lines.
Wetherefore choose to evaluate pairs of lines.
We ran-domly choose a line, and check if it is part of asensible sentence that can be understood withoutmore context.
If that is the case, we select it to in-clude in our test set.
In this way we select 25 pairsof lines.
We evaluate the translations produced bythe three different systems for these sentences.
Ex-amples of the selected sentences and the generatedcorresponding output are displayed in Table 7.6.2 ParticipantsThe participants in this evaluation study were 22volunteers.
All participants were native speakersof Dutch, and participated through an online in-terface.
All participants were adults, and 12 weremale and 10 female.
In addition to the 22 partici-pants, one expert in the field of Middle Dutch alsoperformed the experiment, in order to be able tocompare the judgements of the laymen and the ex-pert.6.3 ProcedureThe participants were asked to rank three differentautomatic literal translations of Middle Dutch text.For reference, they were also shown a modern (of-ten not literal) translation of the text by Dutch au-thor Willem Wilmink.
The order of items to judgewas randomized for each participant, as well as theorder of the output of the systems for each sen-tence.
The criterium for ranking was the extent towhich the sentences could be deciphered and un-derstood.
The participants were asked to alwaysprovide a ranking and were not allowed to assignthe same rank to multiple sentences.
In this way,each participant provided 25 rankings where eachpair of sentences received a distinct rank.
The pairwith rank 1 is considered best and the pair with 3is considered worst.system mean rank 95 % c. i.PBMT 2.44 (0.03) 2.38 - 2.51PBMT + Overlap 2.00 (0.03) 1.94 - 2.06char.
bigram PBMT 1.56 (0.03) 1.50 - 1.62Table 8: Mean scores assigned by human subjects,with the standard error between brackets and thelower and upper bound of the 95 % confidence in-terval7 Results7.1 Human judgementsIn this section we report on results of the exper-iment with judges ranking the output of the sys-tems.
To test for significance of the differencein the ranking of the different systems we ran re-peated measures analyses of variance with system(PBMT, PBMT + Overlap, character bigram MT)as the independent variable, and the ranking of theoutput as the dependent variable.
Mauchly?s testfor sphericity was used to test for homogeneity of16PBMT PBMT +overlapchar.bigramPBMTX22.05 2.59 1.36 16.636**2.77 1.82 1.41 21.545**2.50 1.27 2.23 18.273**1.95 1.45 2.59 14.273**2.18 2.36 1.45 10.182**2.45 2.00 1.55 9.091*2.91 1.77 1.32 29.545**2.18 2.27 1.55 6.903*2.14 2.00 1.86 0.8182.27 1.73 2.00 3.2732.68 1.68 1.64 15.364**2.82 1.95 1.23 27.909**2.68 2.09 1.23 23.545**1.95 2.55 1.50 12.091**2.77 1.86 1.36 22.455**2.32 2.23 1.45 9.909**2.86 1.91 1.23 29.727**2.18 1.09 2.73 30.545**2.05 2.09 1.86 0.6362.73 2.18 1.09 30.545**2.41 2.27 1.32 15.545**2.68 2.18 1.14 27.364**1.82 2.95 1.23 33.909**2.73 1.95 1.32 21.909**2.91 1.77 1.32 29.545**Table 9: Results of the Friedman test on each ofthe 25 sentences.
Results marked * are significantat p < 0.05 and results marked ** are significantat p < 0.01variance, but was not significant, so no correctionshad to be applied.
Planned pairwise comparisonswere made with the Bonferroni method.
The meanranking can be found in Table 8 together with thestandard deviation and 95 % confidence interval.We find that participants ranked the three systemsdifferently, F (2, 42) = 135, 604, p < .001, ?2p =.866.
All pairwise comparisons are significant atp < .001.
The character bigram model receivesthe best mean rank (1.56), then the PBMT + Over-lap system (2.00) and the standard PBMT systemis ranked lowest (2.44).
We used a Friedman testto detect differences across multiple rankings.
Weran the test on each of the 25 K-related samples,and found that for 13 sentences the ranking pro-vided by the test subjects was equal to the meanranking: the PBMT system ranked lowest, then thePBMT + Overlap system and the character bigramsystem scored highest for each of these cases atp < .005.
These results are detailed in Table 9.When comparing the judgements of the partici-pants with the judgements of an expert, we finda significant medium Pearson correlation of .65(p < .001) between the judgements of the expertand the mean of the judgements of the participants.This indicates that the judgements of the laymenare indeed useful.7.2 Automatic judgementsIn order to attempt to measure the quality of thetransformations made by the different systems au-tomatically, we measured NIST scores by compar-ing the output of the systems to the reference trans-lation.
We do realize that the reference translationis in fact a literary interpretation and not a literaltranslation, making automatic assessment harder.Having said that, we still hope to find some effectby using these automatic measures.
We only re-port NIST scores here, because BLEU turned outto be very uninformative.
In many cases sentenceswould receive a BLEU score of 0.
Mauchly?s testfor sphericity was used to test for homogeneity ofvariance for the NIST scores, and was not signifi-cant.
We ran a repeated measures test with plannedpairwise comparisons made with the Bonferronimethod.
We found that the NIST scores for the dif-ferent systems differed significantly (F (2, 48) =6.404, p < .005, ?2p = .211).
The average NISTscores with standard error and the lower and up-per bound of the 95 % confidence interval can beseen in Table 10.
The character bigram translit-eration model scores highest with 2.43, followedby the PBMT + Overlap model with a score of2.30 and finally the MT model scores lowest witha NIST score of 1.95.
We find that the scoresfor the PBMT model differ significantly from thePBMT + Overlap model (p < .01) and the charac-ter bigram PBMT model (p < .05), but the scoresfor the PBMT + Overlap and the character bigramPBMT model do not differ significantly.
Whenwe compare the automatic scores to the human as-signed ranks we find no significant Pearson corre-lation.8 ConclusionIn this paper we have described two modificationsof the standard PBMT framework to improve thetransformation of Middle Dutch to Modern Dutch17system mean NIST 95 % c. i.PBMT 1.96 (0.18) 1.58 - 2.33PBMT + overlap 2.30 (0.21) 1.87 - 2.72char.
bigram PBMT 2.43 (0.20) 2.01 - 2.84Table 10: Mean NIST scores, with the standarderror between brackets and the lower and upperbound of the 95 % confidence intervalby using character overlap in the two variants.
Wedescribed one approach that helps the alignmentprocess by adding words that exhibit a certainamount of character overlap to the parallel data.We also described another approach that translatessequences of character bigrams instead of words.Reviewing the results we conclude that the use ofcharacter overlap between diachronic variants of alanguage is beneficial in the translation process.More specifically, the model that uses characterbigrams in translation instead of words is rankedbest.
Also ranked significantly better than a stan-dard Phrase Based machine translation approachis the approach using the Needleman-Wunsch al-gorithm to align sentences and identify words orphrases that exhibit a significant amount of char-acter overlap to help the GIZA++ statistical align-ment process towards aligning the correct wordsand phrases.
We have seen that one issue thatis encountered when considering the task of lan-guage transformation from Middle Dutch to Mod-ern Dutch is data sparseness.
The number of linesused to train on amounts to a few thousand, andnot millions as is more common in SMT.
It istherefore crucial to use the inherent character over-lap in this task to compensate for the lack of dataand to make more informed decisions.
The char-acter bigram approach is able to generate a trans-lation for out of vocabulary words, which is alsoa solution to the data sparseness problem.
Onearea where the character bigram model often fails,is translating Middle Dutch words into ModernDutch words that are significantly different.
Oneexample can be seen in Table 7, where ?mocht?is translated by the PBMT and PBMT + Overlapsystems to ?kon?
and left the same by the charac-ter bigram transliteration model.
This is probablydue to the fact that ?mocht?
still exists in Dutch,but is not as common as ?kon?
(meaning ?could?
).another issue to consider is the fact that the char-acter bigram model learns character mappings thatare occurring trough out the language.
One exam-ple is the disappearing silent ?h?
after a ?g?.
Thisoften leads to transliterated words of which thespelling is only partially correct.
Apparently thehuman judges rate these ?half words?
higher thancompletely wrong words, but automatic measuressuch as NIST are insensitive to this.We have also reported the NIST scores for theoutput of the standard PBMT approach and thetwo proposed variants.
We see that the NISTscores show a similar patterns as the human judge-ments: the PBMT + Overlap and character bigramPBMT systems both achieve significantly higherNIST scores than the normal PBMT system.
How-ever, the PBMT + Overlap and character bigramPBMT models do not differ significantly in scores.It is interesting that we find no significant corre-lation between human and automatic judgements,leading us to believe that automatic judgementsare not viable in this particular scenario.
This isperhaps due to the fact that the reference transla-tions the automatic measures rely on are in thiscase not literal translations but rather more looselytranslated literary interpretations of the source textin modern Dutch.
The fact that both versions arewritten on rhyme only worsens this problem, asthe author of the Modern Dutch version is oftenvery creative.We think techniques such as the ones describedhere can be of great benefit to laymen wishing toinvestigate works that are not written in contem-porary language, resulting in improved access tothese older works.
Our character bigram translit-eration model may also play some role as a com-putational model in the study of the evolution oforthography in language variants, as it often willgenerate words that are strictly speaking not cor-rect, but do resemble Modern Dutch in some way.Automatic evaluation is another topic for futurework.
It would be interesting to see if an automaticmeasure operating on the character level correlatesbetter with human judgements.ReferencesAlistair Baron and Paul Rayson.
2008.
VARD 2: Atool for dealing with spelling variation in historicalcorpora.
In Proceedings of the Postgraduate Confer-ence in Corpus Linguistics, Birmingham, UK.
AstonUniversity.A.
De Gispert and J.
B. Marino.
2006.
Linguisticknowledge in statistical phrase-based word align-ment.
Natural Language Engineering, 12(1):91?108.18Andrew Finch, Paul Dixon, and Eiichiro Sumita.
2012.Rescoring a phrase-based machine transliterationsystem with recurrent neural network language mod-els.
In Proceedings of the 4th Named Entity Work-shop (NEWS) 2012, pages 47?51, Jeju, Korea, July.Association for Computational Linguistics.Jan Hajic?, Jan Hric, and Vladislav Kubon?.
2000.
Ma-chine translation of very close languages.
In Pro-ceedings of the sixth conference on Applied naturallanguage processing, pages 7?12.
Association forComputational Linguistics.Mike Kestemont, Walter Daelemans, and GuyDe Pauw.
2010.
Weigh your words?memory-based lemmatization for Middle Dutch.
Literaryand Linguistic Computing, 25(3):287?301, Septem-ber.Philipp Koehn and Kevin Knight.
2003.
Feature-richstatistical translation of noun phrases.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 311?318, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Philip Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.
Association for Computa-tional Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens,Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In ACL.
The As-sociation for Computer Linguistics.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT summit, volume 5.Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.2003.
Cognates can improve statistical translationmodels.
In HLT-NAACL.Philippe Langlais and Alexandre Patry.
2007.
Trans-lating unknown words by analogical learning.
InEMNLP-CoNLL, pages 877?886.
ACL.Preslav Nakov and Jo?rg Tiedemann.
2012.
Combin-ing word-level and character-level models for ma-chine translation between closely-related languages.In Proceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 301?305, Jeju Island, Korea,July.
Association for Computational Linguistics.S.
B. Needleman and C. D. Wunsch.
1970.
A generalmethod applicable to the search for similarities inthe amino acid sequence of two proteins.
Journal ofmolecular biology, 48(3):443?453, March.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Maja Popovic and Hermann Ney.
2004.
Towards theuse of word stems and suffixes for statistical ma-chine translation.
In LREC.
European Language Re-sources Association.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In In Proc.
Int.
Conf.
onSpoken Language Processing, pages 901?904, Den-ver, Colorado.Jo?rg Tiedemann.
2009.
Character-based PSMT forclosely related languages.
In Llu?
?s Marque?s andHarold Somers, editors, Proceedings of 13th An-nual Conference of the European Association forMachine Translation (EAMT?09), pages 12 ?
19,Barcelona, Spain, May.Gerhard B Van Huyssteen and Sule?ne Pilon.
2009.Rule-based conversion of closely-related languages:a dutch-to-afrikaans convertor.
20th Annual Sympo-sium of the Pattern Recognition Association of SouthAfrica, December.David Vilar, Jan-Thorsten Peter, and Hermann Ney.2007.
Can we translate letters?
In Second Work-shop on Statistical Machine Translation, pages 33?39, Prague, Czech Republic, jun.
Association forComputational Linguistics.Sander Wubben, Antal van den Bosch, and EmielKrahmer.
2010.
Paraphrase generation as mono-lingual translation: Data and evaluation.
InB.
Mac Namee J. Kelleher and I. van der Sluis, ed-itors, Proceedings of the 10th International Work-shop on Natural Language Generation (INLG2010), pages 203?207, Dublin.Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, andColin Cherry.
2012.
Paraphrasing for style.
InCOLING, pages 2899?2914.Amir Zeldes.
2007.
Machine translation between lan-guage stages: Extracting historical grammar from aparallel diachronic corpus of Polish.
In MatthewDavies, Paul Rayson, Susan Hunston, and PernillaDanielsson, editors, Proceedings of the Corpus Lin-guistics Conference CL2007.
University of Birming-ham.19
