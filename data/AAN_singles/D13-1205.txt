Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1996?2006,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsCross-Lingual Discriminative Learning of Sequence Modelswith Posterior RegularizationKuzman GanchevGoogle Research76 9th AvenueNew York, NY 10011kuzman@google.comDipanjan DasGoogle Research76 9th AvenueNew York, NY 10011dipanjand@google.comAbstractWe present a framework for cross-lingualtransfer of sequence information from aresource-rich source language to a resource-impoverished target language that incorporatessoft constraints via posterior regularization.
Tothis end, we use automatically word alignedbitext between the source and target languagepair, and learn a discriminative conditional ran-dom field model on the target side.
Our poste-rior regularization constraints are derived fromsimple intuitions about the task at hand andfrom cross-lingual alignment information.
Weshow improvements over strong baselines fortwo tasks: part-of-speech tagging and named-entity segmentation.1 IntroductionSupervised systems for NLP tasks are available fora handful of languages.
These systems achieve highaccuracy for many applications; a variety of robustalgorithms to train them from labeled data have beendeveloped.
Here, we focus on learning sequence mod-els for the languages that lack annotated resources.For a given resource-poor target language of inter-est, we assume that parallel data with a resource-richsource language exists.
With the help of this bitextand a supervised system in the source language, weinfer constraints over the label distribution in the tar-get language, and train a discriminative model usingposterior regularization (Ganchev et al 2010).Cross-lingual learning of structured predictionmodels via parallel data has been applied for severalnatural language processing problems, including part-of-speech (POS) tagging (Yarowsky and Ngai, 2001),syntactic parsing (Hwa et al 2005) and named-entityrecognition (Kim et al 2012).
These methods areuseful in several ways.
First, they help in fast proto-typing of natural language systems for new languagesthat do not boast human annotations.
Second, theoutput of such systems could be used to bootstrapmore extensive human annotation projects (Vlachos,2006).
Finally, they are significantly more accuratethan purely unsupervised systems (McDonald et al2011; Das and Petrov, 2011).Recently, Ta?ckstro?m et al(2013) presented a tech-nique for coupling token constraints derived from pro-jected cross-lingual information and type constraintsderived from noisy tag dictionaries to learn POS tag-gers.
Although this technique resulted in state-of-the-art weakly supervised taggers, the authors used aheuristic to combine the aforementioned two sourcesof constraints: the dictionary constraints pruned thetagger?s search space, and the intersected token-levelprojections were treated as hard observations.
Onthe other hand, Ganchev et al(2009) presented aframework for learning weakly-supervised systems(in their case, dependency parsers) that incorporatedalignment-based information too, but used the cross-lingual information only as soft constraints, via poste-rior regularization.
The advantage of this frameworklay in the fact that the projections were only trustedto a certain degree, determined by a strength hyper-parameter, which unfortunately the authors did nothave an elegant way to tune.
In this paper, we ex-ploit the better aspects of these two lines of work:first, we extend the framework of Ta?ckstro?m et alby treating the alignment-based projections only assoft constraints (see ?3.4); second, we choose theconstraint strength by utilizing the tag ambiguity oftokens for a given resource-poor language (see ?6.1).Other than validating our framework on part-of-speech tagging, we experiment on named-entity seg-mentation in a cross-lingual framework.
For this1996task, we present a novel method to perform high-precision phrase-level entity transfer (?5.2.2); wealso provide ways to balance precision and recallwith posterior regularization (?6.2) by incorporatingintuitive soft constraints during learning.
We mea-sure performance on standard benchmark datasets forboth of these tasks, and report improvements overstate-of-the-art baselines.2 Prior WorkCross-lingual projection methods can be classifiedby their use of two very broad ideas.
The first ideautilizes parallel data to create full or partial annota-tions in the low-resource language and trains fromthis data.
This was popularized by Yarowsky andNgai (2001) who applied this to POS tagging andshallow parsing.
It was later applied to parsing (Hwaet al 2005) and named entity recognition (Kim etal., 2012).
The second idea, first proposed by Ze-man and Resnik (2008) and applied more broadlyby McDonald et al(2011), is to train a model ona resource-rich language and apply it to a resource-poor language directly.
The disparity between thelanguages is mitigated by the choice of features.
Inaddition to cross-lingual projection, purely unsuper-vised methods have been explored but with limitedsuccess (Christodoulopoulos et al 2010).
Here, weresort to cross-lingual projection and incorporate thefirst idea; we also follow Li et al(2012) and useWiktionary to further constrain the POS tagging task.Our learning setup is similar to that of Ganchev etal.
(2009), who also use posterior regularization butfocus on dependency parsing alone.
Our work differswith respect to the tasks, the learning algorithm andalso in that we use corpus-wide constraints, whileGanchev et aluse one constraint per sentence.
Forthe part-of-speech tagging task, our approach is sim-ilar to that of Ta?ckstro?m et al(2013), who use analmost identical learning setup but only make use ofhard constraints.
By relaxing these constraints, weallow the model to identify and ignore inconsistentlylabeled parts of sentences, and achieve better resultsusing identical training and test data.3 ApproachWe give an overview of our approach, and present thedetails of our model used for cross-lingual learning.Algorithm 1 Cross-Lingual Learning with PosteriorRegularizationRequire: Parallel source and target language dataDe and Df , source language model (M)e, task-specific target language constraints C.Ensure: ?f , a set of target language parameters.1: De?f ?
word-align-bitext(De,Df )2: D?e ?
label-supervised(De)3: D?f ?
project-and-filter-labels(De?f , D?e)4: ?f ?
learn-posterior-constrained(D?f , C)5: Return ?f3.1 General OverviewThe general overview of our framework is providedin Algorithm 1.
The process of learning parame-ters for a target language for a given task involvesfour subtasks.
First, we run word alignment over alarge corpus of parallel data between the resource-rich source language and the resource-impoverishedtarget language (see ?4.3).
In the second step, weuse a supervised model to label the source side ofthe parallel data (see ?5.1.1 and ?5.2.1).
The thirdstep involves a task-specific word-alignment filter-ing step; this step involves heuristics for which weuse cues from prior state-of-the-art (Das and Petrov,2011; Ta?ckstro?m et al 2013, see ?5.1.2) and alsointroduce some novel ones for the NE segmentationproblem (see ?5.2.2).
In the fourth step, we train alinear chain conditional random field (Lafferty et al2001, CRF henceforth) using posterior regularization.In the next subsection, we turn to a brief summary ofthis final step of estimating parameters of a discrimi-native model with posterior regularization.3.2 Learning with Posterior RegularizationIn this work, we utilize discriminative CRF mod-els, and use posterior regularization (PR) to optimizetheir parameters.
As a framework, posterior regular-ization is described in detail by Ganchev et al(2010).However in our work, we adopt a different optimiza-tion technique; in what follows, we summarize theoptimization algorithm in the context of CRF models.Let x be an input sentence with a set of possiblelabelings Y(x) and let y ?
Y(x) be a particular la-beling for sentence x.
We use bold capital lettersX = {x1 .
.
.xn} and Y = {y1 .
.
.yn} to denote1997a corpus of sentences and labelings for the corpusrespectively.
A CRF models the probability distri-bution over possible labels for a sentence p?(y|x)as:p?
(y | x) ?
exp(?
?
f(x,y)) (1)where ?
are the model parameters and f(.)
is a fea-ture function.
The model examines sentences in iso-lation, and the probability of a particular labeling fora corpus is defined as a product over the individualsentences:p?
(Y | X) =?(x,y)?(X,Y)p?
(y | x).
(2)Traditionally, CRF models have been trained to op-timize the regularized log-likelihood of the trainingdatamax?L(?)
= max?log(p?
(Y | X))?
?
||?|| (3)In our setting, we do not have a fully labeled cor-pus, but we have constraints on the distribution oflabels.
For example, we may know that a particulartoken could be labeled only by a label inventory li-censed by a dictionary, or that a labeling projectedfrom a source language is usually (but not always)correct.
We define these constraints in terms of fea-ture expectations.
Let q(Y) be a distribution over allpossible labelings of our corpus Y(X).
Let Q be aset of distributions defined by:Q = {q(Y) : Eq[?
(X,Y)] ?
b}, (4)where ?
is a constraint feature function and b is a vec-tor of non-negative values that serve as upper boundsto the expectations of every constraint feature.
Thevector b is used to encode our prior knowledge aboutdesirable distributions q(Y).
Note that the constraintfeatures ?
are not related to the model features f .
Themodel features, together with the model parameters ?define the CRF model; the model features need to becomputed at inference time for prediction.
By con-trast, the constraint features and their correspondingconstraint values are used to define our training ob-jective function (and are only used during learning).The PR objective with no labeled data is defined withrespect to Q as:PR: max?JQ(?)
=max??KL(Q?p?
(Y | X))?
?
||?|| (5)where KL(Q||p) = minq?QKL(q||p) is theKL-divergence (Kullback and Leibler, 1951)from a set to a point.
Note that as we add moreconstraints, Q becomes a smaller set.
In thelimit, Q = {q(Y) : q(Y?)
= 1} contains just onedistribution concentrated on a single labeling Y?.In this limit, posterior regularization degeneratesinto the convex log-likelihood objective normallyused for supervised data JQ(?)
= L(?).
However,in the general case, the PR objective JQ is notnecessarily convex.
Prior work, including that ofGanchev et alpropose an algorithm similar toExpectation-Maximization (Dempster et al 1977,EM henceforth) to optimize JQ, but we follow Lianget al(2009) in using a schochastic update-basedalgorithm described below.Note: To make it easier to reason about constraintvalues b, we scale constraint features ?
(X,Y) to liein [0, 1] by computing maxY ?
(X,Y) for the corpusto which ?
is applied.3.3 OptimizationThe optimization procedure proposed byGanchev et alis similar to the EM algorithm,and computes the minimization minq?QKL(q||p)at each step, using its dual form; this minimization isconvex, so there is no duality gap.
They show thatthe optimal primal variables q?
(Y) are related to theoptimal dual variables ??
by:q?
(Y) =p?(Y|X)e?????(X,Y)Z(??).
(6)where Z(??)
is the normalizer.
The dual problem isgiven by:max?
?0?b ?
??
logZ(?).
(7)Substituting Eq.
7 into the objective in Eq.
5, we getthe saddle-point problem:max?min?
?0b ?
?+ log?Yp?(Y|X)e?????(X,Y)?
?
||?|| .
(8)To optimize the above objective function, we need tocompute partial derivatives with respect to both ?
and?.
First, to compute the partial derivatives of Eq.
81998with respect to ?, we need to find expectations of themodel features f given the current distribution p?
andthe constraint distribution q.
To perform tractableinference, a linear-chain CRF model assumes thatthe feature function factorizes according to smallerparts; in particular the factorization uses the follow-ing structure:f(x,y) =?if(x, yi, yi?1) (9)where i ranges over the tokens in the sentence.
Thisfactorization allows us to efficiently compute expec-tations over the labels yi and label-pairs (yi, yi+1).To compute the partial gradient of Eq.
8 with respectto ?, we need to find the expectations of the con-straint features ?.
In order to be tractable here too,we ensure that ?
also factorize according to the samestructure as f .
Therefore, the gradient computationw.r.t.
?
turns out to be straightforward.For all the experiments in this paper, we optimizeEq.
8 using stochastic projected gradient.
For eachtraining sentence, we compute the gradient of ?
and?
with respect to Eq.
8, take a gradient step in eachone, and truncate the negative entries in ?
to zero.We use a step size of 1 for all experiments.13.4 Relationship with Ta?ckstro?m et al(2013)In this subsection, we focus briefly on the relationshipbetween this work and the work of Ta?ckstro?m et al(2013), who focused on constrained learning of POStaggers.
Ta?ckstro?m et aldefine constrained latticesand train by optimizing marginal conditional log-likelihood.
In our notation, they define their objectiveas:max?log?Y?Y?(X)p?(Y|X)?
????
(10)where Y?
(X) are the constrained lattices of label se-quences that agree with both a dictionary and cross-lingually projected POS tags for each sentence ofthe training corpus.
Let us define a constraint fea-ture ?
(X,Y) which counts the number of tags in Ywhich are outside the constraint set Y?
(X) and require?
(X,Y) ?
0.
Note that,arg minqKL(q||p?
(Y|X)) s. t.
?
(X,Y) ?
01Note that we did not implement regularization of ?
in thestochastic optimizer, hence our PR objective (Eq.
8) was unregu-larized; however, the baseline models use `2 regularization.gives the same distribution as Eq.
10.
Given thisequivalence, it is easy to see that the gradient ofEq.
5 with respect to ?
is the same as that of Eq.
10.By using such constrained lattices, Ta?ckstro?m et alavoid maintaining a parameter for the constraint, butlose the ability to relax the constraint value and al-low some probability mass outside the pruned lat-tice.
Their paper also differs from ours in that theyuse L-BFGS (Liu and Nocedal, 1989), while we usean online optimization procedure.
Since the objec-tives are non-convex, the two optimization techniquescould lead to different local optima even when theconstraint is not relaxed (b = 0).4 Tasks and DataIn this section, we focus on the nature of the two tasksthat we attempt to solve, describe the source languagedatasets we use to train our supervised models fortransfer, the target language datasets on which weevaluate our models and the parallel data we use forcross-lingual transfer.4.1 Part-of-Speech TaggingFirst, we focus on the task of part-of-speech tagging.Following previous work on cross-lingual POS tag-ging (Das and Petrov, 2011; Ta?ckstro?m et al 2013),we adopt the POS tags of Petrov et al(2012), ver-sion 1.03;2 we use the October 2012 version of Wik-tionary3 as our tag dictionary.After pruning the search space with the dictionary,we place soft constraints derived by projecting POStags across word alignments.
The alignments are fil-tered for confidence (see ?5.1.2), but we also filterany projected tags that are not licensed by the dictio-nary.
The example in Figure 1 illustrates why thisdictionary filtering step is important.
Consider theEnglish-Spanish phrase pair from Figure 1, whichwe observed in our training data.
Our supervised tag-ger correctly tags Asian with the ADJ tag as shownin the figure.
Asian is aligned to the Spanish wordAsia, which should be tagged NOUN.
Because theSpanish Wiktionary only allows the NOUN tag forAsia, we do not project the ADJ tag from the Englishword Asian.
By contrast, we do project the NOUNtag from the English word sponges to the Spanish2http://code.google.com/p/universal-pos-tags3http://meta.wikimedia.org/wiki/Wiktionary1999of    [ Asian ]        spongesde   las   esponjas   de   AsiaADPADJ NOUNMISCFigure 1: An English (top) ?
Spanish (bottom) phrase pairfrom our parallel data.
The correct POS tags and NERannotations are shown for the English phrase.
Word align-ments are shown as links between English and Spanishwords.word esponjas because this tag is in our dictionaryfor the latter word.For all our POS experiments, we evaluate on sev-enteen target languages.
Fifteen of these languageswere part of the experiments conducted by Ta?ckstro?met al(2013); we add Arabic and Hungarian to the set.The first column of Table 1 lists all seventeen lan-guages using their two-letter abbreviation codes fromthe ISO 639-1 standard.
The evaluation datasets cor-respond to the test sets from the CoNLL shared taskson dependency parsing (Buchholz and Marsi, 2006;Nivre et al 2007).
For French we use the treebankof Abeille?
et al(2003).
English serves as our sourcelanguage and we use the Penn Treebank (Marcus etal., 1993, with tags mapped to the universal tags) totrain our supervised source-side model.4.2 Named-Entity SegmentationSecond, we investigate the task of named-entity seg-mentation.
The goal of this task is to identify theboundaries of named-entities for a given languagewithout classifying them by type.
This is the un-labeled version of named-entity recognition, and ismore amenable to cross-lingual supervision.
To un-derstand why that is, consider again the examplefrom Figure 1.
The English supervised NE taggercorrectly identifies Asian as a named entity of typeMISC (miscellaneous).
The word-alignments sug-gest we should transfer this annotation to the Spanishword Asia which is also an entity.
However, thisshould be labeled LOC (location) according to theCoNLL annotation guidelines (Tjong Kim Sang andDe Meulder, 2003).
Because syntactic variationsof this kind are common, it makes cross-lingual de-tection of NE boundaries as well as types hard.4 Inthis paper, we focus on named-entity segmentationalone, consider the full NER task out of scope.
Weuse English as a source language and train a super-vised English named-entity tagger with the labels inplace, using the CoNLL 2003 shared task data (TjongKim Sang and De Meulder, 2003).
We project thespans using the maximal-span heuristic (Yarowskyand Ngai, 2001).
We project into Dutch, German andSpanish and evaluate on the standard CoNLL 2002and 2003 shared task data sets (Tjong Kim Sang,2002; Tjong Kim Sang and De Meulder, 2003).4.3 Parallel DataFor both tasks we use parallel data gathered automat-ically from the web using the method of Uszkoreitet al(2010), as well as data from Europarl (Koehn,2005) and the UN parallel corpus (UN, 2006), forlanguages covered by the latter two corpora.
Theparallel sentences are word aligned with the alignerof DeNero and Macherey (2011).
The size of theparallel corpus is larger than we need for our tasks,so we follow Ta?ckstro?m et al(2013) in sampling500k tokens for POS tagging and 10k sentences fornamed-entity segmentation (see ?5.1.2 and ?5.2.2).5 Experimental DetailsIn this section, we provide details about task-specific implementations of the supervised source-side model and the word-alignment filtering tech-niques (steps 2 and 3 in Algorithm 1 respectively);we also briefly describe the setup of the cross-lingualexperiments for each task.5.1 Part-of-Speech TaggingWe first focus on the experimental setup for the POStagging task.
When describing feature sets we refer tofeatures conjoined with just a single tag as emissionfeatures and with consecutive tag pairs as transitionfeatures.4We tried using English and German gazetteers from theCoNLL 2002 and 2003 shared tasks as a label dictionary similarto the way we use Wiktionary for POS tagging.
This did not workwell because the CoNLL gazetteers do not have good coverageon our parallel datasets, which we use for training.20005.1.1 Supervised Source-Side ModelWe tag the English side of our parallel data witha supervised first-order linear-chain CRF POS tag-ger.
We use standard features for tagging.
Our emis-sion features are a bias feature, the current word,its suffixes up to length 3, its capitalization shape,whether it contains a hyphen, digit or punctuationand its cluster identity.
Our transition features are abias feature and the cluster identities of each wordin the transition.
For the cluster-based features, weuse monolingual word clusters induced with the ex-change algorithm of Uszkoreit and Brants (2008),which implements the same objective as Brown et al(1992); these clusters have shown improvements forsequence labeling tasks (Turian et al 2010; Ta?ck-stro?m et al 2012).
We set the number of clusters to256 for both the source side tagger and all the otherlanguages.
On Section 23 of the WSJ section of thePenn Treebank, the source side tagger achieves anaccuracy of 96.2%.5.1.2 Word Alignment FilteringFollowing Ta?ckstro?m et al(2013), we tag the En-glish side of our parallel data using the source-sidePOS tagger, intersect the word alignments and filteralignments with confidence below 0.95.
We sam-ple 500,000 tokens of target side sentences for eachlanguage, and use this as training data for learningweakly-supervised taggers.5.1.3 Setup for Cross-Lingual ExperimentsFollowing Ta?ckstro?m et al(2013) we use a re-duced feature set for the cross-lingual models.
Theemission features are the same as the supervisedmodel but without the punctuation feature,5 and weuse only the bias transition feature.
Because thislimits the ability of the model to use context, wealso experiment with an extended feature set thathas transition features for the clusters of each wordin the transition, and their suffixes up to length 3.We refer to the extended-feature models as ?BASE+?and ?PR+?
to distinguish them from the models withfewer features, labeled ?BASE?
and ?PR?.We train BASE and BASE+ using L-BFGS withan `2 regularization weight of 1 for 100 iterations toreproduce the setup used by Ta?ckstro?m et al(2013).5The dictionary licenses punctuations, only by the ?.?
tag.We have only one constraint feature in our poste-rior regularization models that fires for the unprunedprojected tags on words xi.
This feature controlshow often our model trusts a projected tag; we ex-plain how its strength is chosen in ?6.1.
The PR andPR+ models are trained using the stochastic gradientmethod described in ?3.3.5.2 Named-Entity SegmentationIn this subsection, we turn to the experimental detailsof the named-entity segmentation system.5.2.1 Supervised Source-Side ModelTo train our supervised source-side NER model,we implemented a linear-chain first order CRF model.Our feature set was inspired by the model of Kazamaand Torisawa (2007, ?6.1); we used all the local fea-tures from their model except the gazetteer features,and added cluster emission features for offsets in therange [-2, 2] and transition features for offsets in therange [-1, 1] as well as a sentence-start feature.
Weuse automatic POS tags for all the experiments.We use a BIO encoding of the four NER labels(PER, LOC, ORG and MISC).
We also experi-mented with omitting the NE labels from the tagger,still with a BIO encoding for segments, but the resultswere worse on average than what we report in Table 2.We train the source-side model on the CoNLL 2003English training set with log-loss using L-BFGS for100 iterations with `2 regularization weight of 0.1.The model gets 90.9% and 87.5% labeled F1 on theCoNLL development and test sets respectively.65.2.2 Word-Alignment FilteringProjecting named entities across languages canbe error prone for several reasons.
Mistakes intro-duced by the automatic word aligner is one of them.Word alignment errors are particularly problematicfor entity mentions because of the garbage collectoreffect (Brown et al 1993); due to differences in theword order between languages, a few alignment er-rors can result in many errors in the other language.Additionally, entities can occur on just one side ofthe bitext.7 Another source of error is the automatic6These performance values would place us among the topthree competitors of the CoNLL 2003 shared task.7For example, ?It?s all Greek to me.?
in one language and ?Idon?t understand it.?
in another.2001labeling on the source side, which is inaccurate if theparallel corpus is out of domain.
To mitigate theseerrors, we aggressively filter the training data for thistask.
We discard sentence pairs where more than30% of the source language tokens are unaligned,where any source entities are unaligned or whereany source entities are more than 4 tokens long.
Wealso compute a confidence score over entity anno-tations as the minimum posterior over the tags thatcomprise the entity and discard sentence pairs thathave an entity with confidence below 0.9.
Finally,we discard any sentences that contain no projectedentities.
These filtering steps allow us to keep 7.4%,9.7% and 10.4% of the aligned sentence pairs for Ger-man, Spanish and Dutch, respectively, resulting invery high-precision named-entity projections (see Ta-ble 2).
For comparison, we also perform experimentswithout this filtering step.5.2.3 Setup for Cross-Lingual ExperimentsWe use a CRF with the same feature set and BIOencoding for the cross-lingual models as the source-side NER model.
We compare our approach (?PR?in Table 2) to a baseline (?BASE?
in Table 2) whichtreats the projected annotations as fully observed.The PR model treats the projected NE spans of asentence as observed, and allows all labels on theremaining tokens.
Since the ?O?
tag is never seen, anunconstrained model would learn to never predict it.We add two features that fire when the current wordis tagged ?O?
: a bias feature and a feature that fireswhen the automatic POS tag is a proper noun.
We setupQ so the desired expectations are at least 0.98 andat most 0.1 for these constraint features respectively.6 ResultsIn this section, we turn to our experimental results;first, we focus on POS tagging and then turn to theNE segmentation task.6.1 Part-of-Speech TaggingConstraint Strength: As discussed in ?4.1, it isimportant to filter out projected annotations not li-censed by Wiktionary.
Thus, the quality of weakly-supervised POS taggers learned from projectionsis closely correlated with the coverage of the Wik-tionary.
To quantify the effect of Wiktionary cover-age, we counted the expected number of possible tags0.70.80.910.1  0.2  0.3  0.4optimal b1/TpTFigure 2: Correlation between optimal constraint value band dictionary pruning efficiency.
Each blue square is alanguage, the green line is a linear approximation of thedata.per token (TpT) for our unlabeled corpora.
Specif-ically, for each token, we counted the number oftags licensed by the dictionary, or all tags for wordforms not in the dictionary.
For each language, wealso ran our system with constraint strengths in {0.7,0.75, 0.8, 0.85, 0.9, 0.92, 0.95, 0.98, 1.00}, and com-puted the optimal constraint strength from this set.We found that the best constraint strength is closelycorrelated with the average number of tags availablefor each token.
Figure 2 shows the best constraintstrength as a function of the inverse of the number ofunpruned tags per token.
As observed in the figure,the relationship between the optimal strength and1/TpT is roughly linear.
Figure 2 also shows a linearapproximation to the data plotted.
When applyingthis technique to a new language, we would not beable to estimate the optimal constraint strength, butwe could use the linear approximation and knowl-edge of 1/TpT to estimate it.
For our experimentsbelow, we perform this estimation for each languageusing the linear approximation computed from theremaining languages.Results: The results for our part-of-speech taggingexperiments are in Table 1.
We compare our resultsto BASE, which corresponds to reruns of the bestmodel of Ta?ckstro?m et al(2013, Column 9 of Ta-ble 2), and closely aligns with the numbers reportedby the authors.
We see in Table 1 that for both fea-ture sets (i.e., with and without the ?+?
extension),our estimated constraint strength is usually betterthan using a constraint strength of 1.
The resultsin the column labeled PR are better than BASE for12 out of 17 languages, and the results for PR+ are2002BASE BASE+ PR PR+ar 37.84 44.96 ?
49.04?
50.10?bg 88.04 87.93 88.02 88.42?cs 79.67 80.01 ?
80.20?
80.68?da 88.14 87.92 88.24?
87.90de 90.32 89.97 90.41?
90.29el 90.03 89.03 90.63?
90.24?es 86.99 86.81 87.20?
87.21?fr 87.07 87.53 ?
87.44?
87.48?hu 82.05 82.05 82.14?
83.13?it 89.48 89.89 ?
89.52 89.72?ja 80.63 78.54 80.02 79.68nl 85.89 85.77 85.59 85.98?pt 90.93 91.60 ?
91.48?
91.56?sl 82.46 82.08 83.16?
83.49?sv 89.06 88.72 89.25?
88.77tr 64.39 65.74 ?
63.88 66.47?zh 73.98 72.82 74.51?
68.43Avg 81.59 81.85 ?
82.40?
82.33?-zh-ar 85.01 84.91 85.15?
85.40?Table 1: POS tagging results.
BASE represents the bestmodel of Ta?ckstro?m et al(2013).
PR is a system withthe same features but with relaxed constraints.
BASE+and PR+ add additional model features (see ?5.2.3).
?
in-dicates improvements over the previous state of the art(BASE), and bold values indicate the best score for a lan-guage.
?Avg?
indicates averaged results for all 17 lan-guages, while ?-zh-ar?
shows averaged results withoutChinese and Arabic.better than BASE+ for 13 out of 17 languages.
Ad-ditionally, adding features does not tend to help thebaseline model to a large extent (the wins are for6 languages), but does tend to help the PR model(for 11 languages); however, there is a large drop inperformance for Chinese.Error Analysis: Here, we analyze the nature ofimprovements that the PR models get.
For the lan-guages where PR results in large improvements, itstems from the ability to allow the sentential con-text to sometimes override the tag projected via theparallel data.
For example, the Czech word se caneither be a reflexive pronoun (such as ourselves inEnglish) or translate to the preposition with.
Thepronominal sense comprises about 95% of occur-rences in the Czech annotations, but it would notappear in an English translation.
For example, thephrase ?pod?
?vali jsme se?
translates to ?we looked?,and the word jsme would typically be aligned to we;se, which serves as a reflexive pronoun here, remainsunaligned.
Consequently, in our data, over 7000 oc-currences of se appear, but only 17 instances have atag projection that is not filtered by Wiktionary.
Sincethe remaining are tagged with the preposition tag, thehard-constrained baseline always tags se as a prepo-sition.
By contrast, the soft-constrained PR modelpredicts the pronominal sense in cases where the con-text is most indicative of a pronoun ?
38% of the time.It still mistags many of the pronominal cases wherethe contextual evidence is not strong enough.
We getvery similar behavior with the Hungarian word hogywhich can translate to the conjunction that (as in ?Isee that you are here?)
or the adverb how.We found that the drastic drop in performance forChinese under the PR+ model is due to the possessivemarker ???
which serves exclusively as a particlein the test data.
Wiktionary also allows the noun andadverb tags.
The adverbial use is actually a differenttoken (??
?
really, truly) containing the samecharacter.
Because the cross-lingual training data isbased on machine-learned alignments, 99.4% of thetraining examples of?
have no annotations, and only0.6% have the particle annotation projected from theEnglish ?s possessive marker.
If we remove the nounand adverb senses from the Wiktionary performanceof PR+ improves to 72.87%.
Alternatively, we couldadd another constraint to prefer closed-class wordsover open-class words when both are licensed by thedictionary.
When we add such a constraint to Chinesewith a constraint value of 0.95, we recover most of theloss (68.43?
72.94); however, we do not report thisspecific change to the Chinese experimental setup inTable 1 to maintain generality.6.2 Named-Entity SegmentationResults: Table 2 shows the results for the named en-tity segmentation experiments.
First, we observe thatthe word alignment filtering step (?5.2.2) improvesresults for all three languages by significant margins,for both the BASE and PR models.
Both with andwithout filtering, we observe that the baseline mod-els are very strongly biased towards precision.
Thefiltering step tends to help with recall more than pre-cison for both models.
By having a soft constraint viaPR and allowing some segmentations to fall outsideof the transferred one, we get an increase in recall,2003No Filtering Filtering (?5.2.2)Lang Metric BASE PR BASE PRPrec 74.29 73.85 75.36 76.47de Recall 41.69 54.50 54.71 64.61F1 53.41 62.71 63.39 70.04Prec 74.53 62.10 82.50 70.22es Recall 56.39 78.33 67.27 81.10F1 64.20 69.28 74.11 75.27Prec 81.90 75.12 86.39 76.09nl Recall 50.54 76.11 65.45 79.11F1 62.51 75.61 74.47 77.57Above: dev, below: testPrec 73.23 71.67 69.90 70.94de Recall 39.70 51.81 52.52 61.42F1 51.49 60.14 59.97 65.84Prec 75.38 65.40 83.50 73.68es Recall 56.00 80.30 67.55 83.31F1 64.26 72.09 74.68 78.20Prec 79.45 73.55 86.01 77.05nl Recall 47.45 75.37 65.16 80.11F1 59.42 74.45 74.14 78.55Table 2: Result for the named-entity segmentation exper-iments.
The highest score in each category is shown inbold.
Note that ?No Filtering?
still discards sentences withno projected entities.and in turn an improved F1 score.
On average thePR model improves F-score by 3.6% on the develop-ment set and 4.6% on the test set over the baseline(when filtering is used).
Note that because we focuson named entity segmentation, our results are notdirectly comparable to those of Ta?ckstro?m (2012),who train a de-lexicalized named entity recognizeron one language and apply it to other languages.Error Analysis: In order to get a sense for the typesof errors made by the baseline which are correctedby the PR model, we collected statistics about themost frequent errors in the segments extracted bythe baseline and by our model.
We divided the er-rors into missing segments, extraneous segments andoverlapping segments.From Table 2, it is clear that the most commonerrors for the baseline models are missing entities.From our analysis of the CoNLL development data,we found that the entities that occur with little context(such as the location and publisher of an item) at theonset of news articles are most frequently missed.
ForGerman, dpa (Deutsche Presse-Agentur) and Reuterare the two most common missing segmentations;the Spanish counterparts are Gobierno (Government)and Barcelona, while for Dutch they are De Morgenand Brussel.
While filtering parallel sentences andusing a soft constraint both increase recall, even ourstrongest model does not get enough information topredict these entities, and they continue to be majorsources of error.
By contrast, the names mentionedin context are the ones that are most frequently addedto the analysis when PR is used.
In a sense thisis desirable, since a machine-learned named-entitysegmentation system is most useful for the long tailof entity mentions.If we filter the training data and use the PR modelto further increase recall, precision errors tend tobecome relatively more frequent (this trend is ob-servable in Table 2).
For German, the most frequentprecision error is Mark referring to the DeutscheMark.
For Spanish, the most frequent precision er-rors are due to boundary errors.
The Spanish an-notation guidelines include enclosing quotes as partof the entity name, and failing to include them ac-counts for just under 1% of the precision errors ofthe PR system that uses filtering.
The second mostfrequent error is failing to segment Inter de Mila?n.The model segments out either Inter or Mila?n or bothby themselves depending on context.7 ConclusionsIn this paper, we presented a framework for cross-lingual transfer of sequence information from aresource-rich source language to a resource-poor tar-get language.
Our framework incorporates soft con-straints while training with projected informationvia posterior regularization.
We presented the effi-cacy of our framework on two very useful naturallanguage tasks: POS tagging and named-entity seg-mentation.
The soft constraints used in our workmodel intuitions about a given task.
For the POStagging problem, we designed constraints that alsoincorporate projected token-level information, andpresented a principled method for choosing the extentto which this information should be trusted withinthe PR framework.
This approach generalizes thestate of the art in cross-lingual projection work inthe context of POS tagging, and improves upon it.2004Across seventeen languages, our models outperformthe previous state of the art by an average of 0.8%(greater than 4% error reduction), and outperformsit on twelve out of seventeen languages.
For named-entity segmentation, our model results in 3.6% and4.6% absolute improvements in F1-score on our de-velopment and test sets respectively, when averagedacross three languages.AcknowledgmentsWe would like to thank Ryan McDonald, FernandoPereira, Slav Petrov and Oscar Ta?ckstro?m for numer-ous discussions on this topic and providing detailedfeedback on early drafts of this paper.
We are alsograteful to the four anonymous reviewers for theirvaluable comments.ReferencesAnne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.2003.
Building a Treebank for French.
In A. Abeille?,editor, Treebanks: Building and Using Parsed Corpora,chapter 10.
Kluwer.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18:467?479.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, Meredith J. Goldsmith, Jan Hajic,Robert L. Mercer, and Surya Mohanty.
1993.
Butdictionaries are data too.
In Proceedings of the Work-shop on Human Language Technology.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedPOS induction: How far have we come?
In Proceed-ings of EMNLP.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of ACL-HLT.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the em algorithm.
Journal of the Royal StatisticalSociety, Series B, 39(1):1?38.John DeNero and Klaus Macherey.
2011.
Model-basedaligner combination using dual decomposition.
In Pro-ceedings of ACL-HLT.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.2009.
Dependency grammar induction via bitext pro-jection constraints.
In Proceedings of ACL-IJCNLP.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of Machine Learn-ing Research, 11:2001?2049.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(03):311?325.Jun?ichi Kazama and Kentaro Torisawa.
2007.
A newperceptron algorithm for sequence labeling with non-local features.
In Proceedings of EMNLP.Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012.Multilingual named entity recognition using paralleldata and metadata from wikipedia.
In Proceedings ofACL.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of MTSummit.Solomon Kullback and Richard A. Leibler.
1951.
Oninformation and sufficiency.
Annals of MathematicalStatistics, 22:49?86.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
InProceedings of ICML.Shen Li, Joa?o Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In Proceedings ofEMNLP-CoNLL.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning from measurements in exponential families.In Proceedings of ICML.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45:503?528.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated corpusof English: the Penn treebank.
Computational Linguis-tics, 19(2):313?330.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of EMNLP.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of EMNLP-CoNLL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofLREC.Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.2012.
Cross-lingual word clusters for direct transfer oflinguistic structure.
In Proceedings of NAACL-HLT.2005Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-Donald, and Joakim Nivre.
2013.
Token and type con-straints for cross-lingual part-of-speech tagging.
Trans-actions of the Association for Computational Linguis-tics, 1:1?12.Oscar Ta?ckstro?m.
2012.
Nudging the envelope of directtransfer methods for multilingual named entity recogni-tion.
In Proceedings of the NAACL-HLT Workshop onthe Induction of Linguistic Structure.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the CoNLL-2003 shared task: language-independent named entity recognition.
In Proceedingsof CoNLL.Erik F. Tjong Kim Sang.
2002.
Introduction tothe CoNLL-2002 shared task: Language-independentnamed entity recognition.
In Proceedings of CoNLL.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceedingsof ACL.UN.
2006.
ODS UN parallel corpus.Jakob Uszkoreit and Thorsten Brants.
2008.
Distributedword clustering for large scale class-based languagemodeling in machine translation.
In Proceedings ofACL-HLT.Jakob Uszkoreit, Jay Ponte, Ashok Popat, and MosheDubiner.
2010.
Large scale parallel document miningfor machine translation.
In Proceedings of COLING.Andreas Vlachos.
2006.
Active annotation.
Proceedingsof EACL.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedings ofNAACL.Daniel Zeman and Philip Resnik.
2008.
Cross-languageparser adaptation between related languages.
In Pro-ceedings of IJCNLP Workshop: NLP for Less Privi-leged Languages.2006
