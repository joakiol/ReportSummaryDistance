Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 641?648Manchester, August 2008Parsing the SYNTAGRUS Treebank of RussianJoakim NivreV?axj?o University andUppsala Universityjoakim.nivre@vxu.seIgor M. BoguslavskyUniversidad Polit?ecnicade MadridDepartamento deInteligencia Artificialigor@opera.dia.fi.upm.esLeonid L. IomdinRussian Academyof SciencesInstitute for InformationTransmission Problemsiomdin@iitp.ruAbstractWe present the first results on parsing theSYNTAGRUS treebank of Russian with adata-driven dependency parser, achievinga labeled attachment score of over 82%and an unlabeled attachment score of 89%.A feature analysis shows that high parsingaccuracy is crucially dependent on the useof both lexical and morphological features.We conjecture that the latter result can begeneralized to richly inflected languages ingeneral, provided that sufficient amountsof training data are available.1 IntroductionDependency-based syntactic parsing has becomeincreasingly popular in computational linguisticsin recent years.
One of the reasons for the growinginterest is apparently the belief that dependency-based representations should be more suitable forlanguages that exhibit free or flexible word orderand where most of the clues to syntactic structureare found in lexical and morphological features,rather than in syntactic categories and word orderconfigurations.
Some support for this view can befound in the results from the CoNLL shared taskson dependency parsing in 2006 and 2007, wherea variety of data-driven methods for dependencyparsing have been applied with encouraging resultsto languages of great typological diversity (Buch-holz and Marsi, 2006; Nivre et al, 2007a).However, there are still important differences inparsing accuracy for different language types.
For?
Joakim Nivre, Igor M. Boguslavsky, and LeonidL.
Iomdin, 2008.
Licensed under the Creative Com-mons Attribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.example, Nivre et al (2007a) observe that the lan-guages included in the 2007 CoNLL shared taskcan be divided into three distinct groups with re-spect to top accuracy scores, with relatively lowaccuracy for richly inflected languages like Arabicand Basque, medium accuracy for agglutinatinglanguages like Hungarian and Turkish, and highaccuracy for more configurational languages likeEnglish and Chinese.
A complicating factor in thiskind of comparison is the fact that the syntactic an-notation in treebanks varies across languages, insuch a way that it is very difficult to tease apart theimpact on parsing accuracy of linguistic structure,on the one hand, and linguistic annotation, on theother.
It is also worth noting that the majority ofthe data sets used in the CoNLL shared tasks arenot derived from treebanks with genuine depen-dency annotation, but have been obtained throughconversion from other kinds of annotation.
Andthe data sets that do come with original depen-dency annotation are generally fairly small, withless than 100,000 words available for training, thenotable exception of course being the Prague De-pendency Treebank of Czech (Haji?c et al, 2001),which is one of the largest and most widely usedtreebanks in the field.This paper contributes to the growing litera-ture on dependency parsing for typologically di-verse languages by presenting the first results onparsing the Russian treebank SYNTAGRUS (Bo-guslavsky et al, 2000; Boguslavsky et al, 2002).There are several factors that make this treebankan interesting resource in this context.
First ofall, it contains a genuine dependency annotation,theoretically grounded in the long tradition of de-pendency grammar for Slavic languages, repre-sented by the work of Tesni`ere (1959) andMel?
?cuk(1988), among others.
Secondly, with close to641500,000 tokens, the treebank is larger than mostother available dependency treebanks and providesa good basis for experimental investigations us-ing data-driven methods.
Thirdly, the Russian lan-guage, which has not been included in previous ex-perimental evaluations such as the CoNLL sharedtasks, is a richly inflected language with free wordorder and thus representative of the class of lan-guages that tend to pose problems for the currentlyavailable parsing models.
Taken together, thesefactors imply that experiments using the SYNTA-GRUS treebank may be able to shed further lighton the complex interplay between language type,annotation scheme, and training set size, as deter-minants of parsing accuracy for data-driven depen-dency parsers.The experimental parsing results presented inthis paper have been obtained using MaltParser,a freely available system for data-driven depen-dency parsing with state-of-the-art accuracy formost languages in previous evaluations (Buchholzand Marsi, 2006; Nivre et al, 2007a; Nivre et al,2007b).
Besides establishing a first benchmark forthe SYNTAGRUS treebank, we analyze the influ-ence of different kinds of features on parsing ac-curacy, showing conclusively that both lexical andmorphological features are crucial for obtaininggood parsing accuracy.
All results are based on in-put with gold standard annotations, which meansthat the results can be seen to establish an upperbound on what can be achieved when parsing rawtext.
However, this also means that results arecomparable to those from the CoNLL shared tasks,which have been obtained under the same condi-tions.The rest of the paper is structured as follows.Section 2 introduces the SYNTAGRUS treebank,section 3 describes the MaltParser system used inthe experiments, and section 4 presents experimen-tal results and analysis.
Section 5 contains conclu-sions and future work.2 The SYNTAGRUS TreebankThe Russian dependency treebank, SYNTAGRUS,is being developed by the Computational Linguis-tics Laboratory, Institute of Information Trans-mission Problems, Russian Academy of Sciences.Currently the treebank contains over 32,000 sen-tences (roughly 460,000 words) belonging to textsfrom a variety of genres (contemporary fiction,popular science, newspaper and journal articlesdated between 1960 and 2008, texts of onlinenews, etc.)
and it is growing steadily.
It is an inte-gral but fully autonomous part of the Russian Na-tional Corpus developed in a nationwide researchproject and can be freely consulted on the Web(http://www.ruscorpora.ru/).Since Russian is a language with relatively freeword order, SYNTAGRUS adopted a dependency-based annotation scheme, in a way parallel to thePrague Dependency Treebank (Haji?c et al, 2001).The treebank is so far the only corpus of Russiansupplied with comprehensive morphological anno-tation and syntactic annotation in the form of acomplete dependency tree provided for every sen-tence.Figure 1 shows the dependency tree for thesentence Naibol~xee vozmuwenie uqastnikovmitinga vyzval prodolawi$is rost cenna benzin, ustanavlivaemyh neftnymi kom-panimi (It was the continuing growth of petrolprices set by oil companies that caused the greatestindignation of the participants of the meeting).
Inthe dependency tree, nodes represent words (lem-mas), annotated with parts of speech and morpho-logical features, while arcs are labeled with syntac-tic dependency types.
There are over 65 distinctdependency labels in the treebank, half of whichare taken from Mel?
?cuk?s Meaning?Text Theory(Mel?
?cuk, 1988).
Dependency types that are usedin figure 1 include:1. predik (predicative), which, prototypically,represents the relation between the verbalpredicate as head and its subject as depen-dent;2.
1-kompl (first complement), which denotesthe relation between a predicate word as headand its direct complement as dependent;3. agent (agentive), which introduces the rela-tion between a predicate word (verbal nounor verb in the passive voice) as head and itsagent in the instrumental case as dependent;4. kvaziagent (quasi-agentive), which relatesany predicate noun as head with its first syn-tactic actant as dependent, if the latter isnot eligible for being qualified as the noun?sagent;5. opred (modifier), which connects a nounhead with an adjective/participle dependent ifthe latter serves as an adjectival modifier tothe noun;642Figure 1: A syntactically annotated sentence from the SYNTAGRUS treebank.6.
predl (prepositional), which accounts for therelation between a preposition as head and anoun as dependent.Dependency trees in SYNTAGRUS may containnon-projective dependencies.
Normally, one tokencorresponds to one node in the dependency tree.There are however a noticeable number of excep-tions, the most important of which are the follow-ing:1. compound words like ptidestitany$i(fifty-storied), where one token correspondsto two or more nodes;2. so-called phantom nodes for the representa-tion of hard cases of ellipsis, which do notcorrespond to any particular token in the sen-tence; for example,  kupil rubaxku, a ongalstuk (I bought a shirt and he a tie), whichis expanded into  kupil rubaxku, a onkupilPHANTOMgalstuk (I bought a shirt andhe boughtPHANTOMa tie);3. multiword expressions like po kra$ine$i mere(at least), where several tokens correspond toone node.Syntactic annotation is performed semi-automatically: sentences are first processedby the rule-based Russian parser of an advancedNLP system, ETAP-3 (Apresian et al, 2003) andthen edited manually by linguists who handleerrors of the parser as well as cases of ambiguitythat cannot be reliably resolved without extra-linguistic knowledge.
The parser processes rawsentences without prior part-of-speech tagging.Morphological annotation in SYNTAGRUS isbased on a comprehensive morphological dictio-nary of Russian that counts about 130,000 entries(over 4 million word forms).
The ETAP-3 mor-phological analyzer uses the dictionary to producemorphological annotation of words belonging tothe corpus, including lemma, part-of-speech tagand additional morphological features dependenton the part of speech: animacy, gender, number,case, degree of comparison, short form (of adjec-tives and participles), representation (of verbs), as-pect, tense, mood, person, voice, composite form,and attenuation.Statistics for the version of SYNTAGRUS usedfor the experiments described in this paper are asfollows:?
32,242 sentences, belonging to the fictiongenre (9.8%), texts of online news (12.4%),newspaper and journal articles (77.8%);?
461,297 tokens, including expressions withnon-alphabetical symbols (e.g., 10, 1.200,$333, +70C, #) but excluding punctuation;?
31,683 distinct word types, of which 635 witha frequency greater than 100, 5041 greaterthan 10, and 18231 greater than 1;?
3,414 sentences (10.3%) with non-projective643POS DEP MOR LEM LEXTOP + + + + +TOP?1 +HEAD(TOP) + +LDEP(TOP) +RDEP(TOP) +NEXT + + + +NEXT+1 + + + +NEXT+2 +NEXT+3 +LDEP(NEXT) +Table 1: History-based features (TOP = token ontop of stack; NEXT = next token in input buffer;HEAD(w) = head of w; LDEP(w) = leftmost depen-dent of w; RDEP(w) = leftmost dependent of w).dependencies and 3,934 non-projective de-pendency arcs in total;?
478 sentences (1.5%) containing phantomnodes and 631 phantom nodes in total.3 MaltParserMaltParser (Nivre et al, 2007b) is a language-independent system for data-driven dependencyparsing, based on a transition-based parsing model(McDonald and Nivre, 2007).
More precisely, theapproach is based on four essential components:?
A transition-based deterministic algorithmfor building labeled projective dependencygraphs in linear time (Nivre, 2003).?
History-based feature models for predictingthe next parser action (Black et al, 1992;Magerman, 1995; Ratnaparkhi, 1997).?
Discriminative classifiers for mapping histo-ries to parser actions (Kudo and Matsumoto,2002; Yamada and Matsumoto, 2003).?
Pseudo-projective parsing for recovering non-projective structures (Nivre and Nilsson,2005).In the following subsections, we briefly describeeach of these four components in turn.3.1 Parsing AlgorithmThe parser uses the deterministic algorithm for la-beled dependency parsing first proposed by Nivre(2003).
The algorithm builds a labeled dependencygraph in one left-to-right pass over the input, us-ing a stack to store partially processed tokens andadding arcs using four elementary actions (whereTOP is the token on top of the stack and NEXT isthe next token):?
Shift: Push NEXT onto the stack.?
Reduce: Pop the stack.?
Right-Arc(r): Add an arc labeled r from TOPto NEXT; push NEXT onto the stack.?
Left-Arc(r): Add an arc labeled r from NEXTto TOP; pop the stack.Parser actions are predicted using a history-basedfeature model (section 3.2) and SVM classifiers(section 3.3).
Although the parser only derivesprojective graphs, the fact that these graphs arelabeled allows non-projective dependencies to becaptured using the pseudo-projective approach ofNivre and Nilsson (2005) (section 3.4).3.2 History-Based Feature ModelsHistory-based parsing models rely on features ofthe derivation history to predict the next parser ac-tion (Black et al, 1992).
The features used areall symbolic and defined in terms of five differentnode attributes:?
POS = part of speech (atomic)?
DEP = dependency type?
MOR = morphological features (set)?
LEM = lemma?
LEX = word formFeatures of the type DEP have a special status inthat they are extracted during parsing from the par-tially built dependency graph and are updated dy-namically during parsing.
The other four featuretypes (LEX, LEM, POS, and MOR) are given as partof the input to the parser and remain static duringthe processing of a sentence.
Of these four fea-ture types, all except LEX presupposes that the in-put has been preprocessed by a lemmatizer, taggerand morphological analyzer, respectively, but forthe experiments reported below we use gold stan-dard annotation from the treebank.In order to study the influence of different fea-tures, we have experimented with different combi-nations of the five feature types, where the base-line model contains only POS and DEP features,while more complex models add MOR, LEM, andLEX features in different combinations.
The exact644features included for each feature type are shownin table 1, where rows denote tokens in a parserconfiguration (defined relative to the stack, the re-maining input, and the partially built dependencygraph), and where columns correspond to featuretypes.
The selection of features in each group wastuned on a development set as described in sec-tion 4.3.3 Discriminative ClassifiersWe use support vector machines (Vapnik, 1995) topredict the next parser action from a feature vectorrepresenting the history.
More specifically, we useLIBSVM (Chang and Lin, 2001) with a quadratickernel K(xi, xj) = (?xTixj+ r)2and the built-in one-versus-all strategy for multi-class classifica-tion.
Symbolic features are converted to numericalfeatures using the standard technique of binariza-tion, and we split the set values of MOR featuresinto their atomic components.
In order to speedup training, we also divide the training data intosmaller bins according to the feature POS of NEXT,and train separate classifiers on each bin.3.4 Pseudo-Projective ParsingPseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non-projective structures in a projective data-drivenparser.
We projectivize training data by a minimaltransformation, lifting non-projective arcs one stepat a time, and extending the arc label of lifted arcsusing the encoding scheme called HEAD by Nivreand Nilsson (2005), which means that a lifted arcis assigned the label r?h, where r is the originallabel and h is the label of the original head in thenon-projective dependency graph.Non-projective dependencies can be recoveredby an inverse transformation applied to the depen-dency graph output by the parser, using a left-to-right, top-down, breadth-first search, guided by theextended arc labels r?h assigned by the parser.4 ExperimentsIn this section we describe the first experiments onparsing the SYNTAGRUS treebank using a data-driven parser.
The experimental setup is describedin section 4.1, while the experimental results arepresented and discussed in section 4.2.4.1 Experimental SetupAll experiments have been performed on the ver-sion of SYNTAGRUS described in section 2, con-Model Count LAS UASBase = POS + DEP 46506 60.2 76.0B1 = Base + MOR 46506 73.0 84.5B2 = Base + LEM 46506 75.5 84.6B3 = Base + LEX 46506 74.5 84.6BM1 = B1 + LEM 46506 82.3 89.0BM2 = B1 + LEX 46506 81.0 88.8All = B1 + LEM + LEX 46506 82.3 89.1Table 2: Parsing accuracy for different featuremodels on the final test set (Count = Number oftokens in the test set, LAS = Labeled attachmentscore, UAS = Unlabeled attachment score).verted to the CoNLL data format (Buchholz andMarsi, 2006).1The available data were dividedinto 80% for training, 10% for development, and10% for final testing, using a pseudo-randomizedsplit.
The development set was used for tuningparameters of the parsing algorithm and pseudo-projective parsing technique, and for feature selec-tion within the feature groups not included in thebaseline model (i.e., MOR, LEM, and LEX).
Thetest set was used for evaluating the finally selectedmodels once.The evaluation metrics used are labeled attach-ment score (LAS) ?
the percentage of tokens thatare assigned the correct head and dependency type?
and unlabeled attachment score (UAS) ?
the per-centage of tokens that are assigned the correct head(regardless of dependency type).
In addition, wepresent precision and recall for non-projective de-pendencies.
Punctuation tokens are excluded in allscores, but phantom tokens are included.
We useMcNemar?s test for statistical significance.4.2 Results and DiscussionTable 2 gives the parsing accuracy for different fea-ture models on the held-out test set, measured aslabeled attachment score (LAS) and unlabeled at-tachment score (UAS).
With respect to LAS, thereare statistically significant differences between allmodels except BM1 and All (p < 0.01).
With re-spect to UAS, there are statistically significant dif-ferences between four groups, such that {Base} <{B1, B2, B3} < {BM2} < {BM1, All}, but there1Since SYNTAGRUS only distinguishes ten different partsof speech (not counting morphological features), the fieldsCPOSTAG and POSTAG in the CoNLL format ?
for coarse-grained and fine-grained parts of speech ?
were given thesame content.645are no differences within these groups.2Looking at the results for different models, wesee that while the baseline model (Base) achievesa modest 60.2% LAS and 76.0% UAS, the addi-tion of only one additional feature group (B1?B3)boosts unlabeled accuracy by close to ten percent-age points and labeled accuracy by up to fifteenpercentage points.
Somewhat surprisingly, the dif-ferences between models B1?B3 are very small,and only differences with respect to LAS are statis-tically significant, which may be taken to suggestthat morphological and lexical features capture thesame type of information.
However, this hypothe-sis is clearly refuted by the results for models BM1and BM2, where the addition of lexical features ontop of morphological features gives a further gainin LAS of eight to ten percentage points (and overfour percentage points for UAS).Comparing the use of raw word forms (LEX) andlemmas (LEM) as lexical features, we see a slightadvantage for the latter, at least for labeled accu-racy.
However, it must be remembered that the ex-periments are based on gold standard input anno-tation, which probably leads to an overestimationof the value of LEM features.
Finally, it is worthnoting that including both LEX and LEM featuresdoes not result in a significant improvement overthe model with only LEM features, which may be asign of saturation, although this may again changein the presence of noisy LEM features.The experimental results show conclusively thatboth morphological and lexical features are crucialfor achieving high parsing accuracy.
It may seemthat they are most important for labeled accuracy,where the gain in absolute percentage points is thegreatest with respect to the baseline, but it mustbe remembered that the unlabeled scores start at ahigher level, thus leaving less room for improve-ment.
In fact, the total error reduction from Baseto All is over 50% for both LAS and UAS.Table 3 gives a more detailed picture of parsingperformance for the best model (All), by breakingdown both LAS and UAS by the part-of-speech tagof the dependent.
We note that accuracy is higherthan average for nouns (S), adjectives (A), parti-cles (PART), and reasonably good for verbs (V).For prepositions (PR), conjunctions (CONJ), andadverbs (ADV), accuracy is considerably lower,which may be attributed to attachment ambigui-2For the difference BM2 < BM1, 0.01 < p < 0.05; forall other differences, p < 0.01.Part of Speech Count LAS UASS (noun) 7303 86.7 93.3A (adjective) 7024 92.8 94.2V (verb) 6946 81.9 85.8PR (preposition) 5302 60.0 79.0CONJ (conjunction) 2998 76.1 80.7ADV (adverb) 2855 72.3 83.3PART (particle) 1833 88.1 89.6NUM (numeral) 807 88.7 93.6NID (foreign word) 142 76.5 91.5COM (compound) 32 93.8 96.9P (proposition word) 7 57.1 85.7INTJ (interjection) 5 0.0 20.0Table 3: Accuracy by part of speech on the finaltest set for All features (Count = Number of tokensin the test set, LAS = Labeled attachment score,UAS = Unlabeled attachment score).ties.
It is also worth noting that both prepositionsand adverbs have considerably higher UAS thanLAS (almost twenty percentage points for prepo-sitions), which shows that even when they are at-tached correctly they are are often mislabeled.
Theremaining parts of speech are too infrequent towarrant any conclusions.Looking specifically at non-projective depen-dencies, we find that the best model has a la-beled precision of 68.8 and a labeled recall of 31.4.The corresponding unlabeled figures are 73.3 and33.4.3This confirms the results of previous studiesshowing that the pseudo-projective parsing tech-nique used by MaltParser tends to give high pre-cision ?
given that non-projective dependenciesare among the most difficult to parse correctly ?but rather low recall (McDonald and Nivre, 2007).It is also worth mentioning that phantom tokens,i.e., empty tokens inserted for the analysis of cer-tain elliptical constructions (see section 2), havea labeled precision of 82.4 and a labeled recallof 82.8 (89.2 and 89.6 unlabeled), which is veryclose to the average accuracy, despite being veryinfrequent.
However, it must be remembered thatthese tokens were given as part of the input inthese experiments.
In order to correctly analysethese tokens and their dependencies when pars-ing raw text, they would have to be recovered ina pre-processing phase along the lines of Dienes3The precision is the percentage of non-projective depen-dencies predicted by the parser that were correct, while therecall is the percentage of true non-projective dependenciesthat were correctly predicted by the parser.646and Dubey (2003).Summing up, the main result of the experimen-tal evaluation is that both morphological and lex-ical features are crucial for attaining high accu-racy when training and evaluating on the repre-sentations found in the SYNTAGRUS treebank ofRussian.
With regard to morphological featuresthis is in line with a number of recent studiesshowing the importance of morphology for pars-ing languages with less rigid word order, includ-ing work on Spanish (Cowan and Collins, 2005),Hebrew (Tsarfaty, 2006; Tsarfaty and Sima?an,2007), Turkish (Eryigit et al, 2006), and Swedish(?vrelid and Nivre, 2007).With regard to lexical features, the situation ismore complex in that there are a number of stud-ies questioning the usefulness of lexical featuresin statistical parsing and arguing that equivalentor better results can be achieved with unlexical-ized models provided that linguistic categories canbe split flexibly into more fine-grained categories,either using hand-crafted splits, as in the seminalwork of Klein and Manning (2003), or using hid-den variables and unsupervised learning, as in themore recent work by Petrov et al (2006), amongothers.
There are even studies showing that lexi-calization can be harmful when parsing richly in-flected languages like German (Dubey and Keller,2003) and Turkish (Eryi?git and Oflazer, 2006).However, it is worth noting that most of theseresults have been obtained either for models ofconstituency-based parsing or for models of de-pendency parsing suffering from sparse data.4Inthe experiments presented here, we have useda transition-based model for dependency parsingthat has much fewer parameters than state-of-the-art probabilistic models for constituency parsing.Moreover, we have been able to use a relativelylarge training set, thereby minimizing the effect ofsparseness for lexical features.
We therefore con-jecture that the beneficial effect of lexical featureson parsing accuracy will generalize to other richlyinflected languages when similar conditions hold.As far as we know, these are the first results fora large-scale data-driven parser for Russian.
Theredo exist several rule-based parsers for Russian,such as the ETAP-3 parser (Apresian et al, 2003)and a Link Grammar parser,5as well as a prototypeof a hybrid system based on the ETAP-3 parser en-4The latter case applies to the probabilistic model of de-pendency parsing explored by Eryi?git and Oflazer (2006).5http://sz.ru/parser/riched with statistics extracted from SYNTAGRUS(Boguslavsky et al, 2003; Chardin, 2004), but dif-ferences in both input format and output repre-sentations make it difficult to compare the perfor-mance directly.5 ConclusionWe have presented the first results on parsing theSYNTAGRUS treebank of Russian using a data-driven dependency parser.
Besides establishinga first benchmark for the SYNTAGRUS treebank,we have analyzed the influence of different kindsof features on parsing accuracy, showing conclu-sively that both lexical and morphological featuresare crucial for obtaining good parsing accuracy.We hypothesize that this result can be generalizedto other richly inflected languages, provided thatsufficient amounts of data are available.Future work includes a deeper analysis of the in-fluence of individual features, both morphologicaland lexical, as well as an evaluation of the parserunder more realistic conditions without gold stan-dard annotation in the input.
This will require notonly automatic morphological analysis and disam-biguation but also a mechanism for inserting so-called phantom tokens in elliptical constructions.AcknowledgmentsWe want to thank Ivan Chardin for initiating thiscollaboration and Jens Nilsson for converting theSYNTAGRUS data to the CoNLL format.
We aregrateful to the Russian Foundation of Basic Re-search for partial support of this research (grant no.07-06-00339).ReferencesApresian, Ju., I. Boguslavsky, L. Iomdin, A. Lazursky,V.
Sannikov, V. Sizov, and L. Tsinman.
2003.ETAP-3 linguistic processor: A full-fledged NLPimplementation of the MTT.
In Proceedings ofthe First International Conference on Meaning-TextTheory, 279?288.Black, E., F. Jelinek, J. D. Lafferty, D. M. Mager-man, R. L. Mercer, and S. Roukos.
1992.
To-wards history-based grammars: Using richer modelsfor probabilistic parsing.
In Proceedings of the 5thDARPA Speech and Natural Language Workshop,31?37.Boguslavsky, I., S. Grigorieva, N. Grigoriev, L. Krei-dlin, and N. Frid.
2000.
Dependency treebank forRussian: Concept, tools, types of information.
InProceedings of COLING, 987?991.647Boguslavsky, I., I. Chardin, S. Grigorieva, N. Grigoriev,L.
Iomdin, L. Kreidlin, and N. Frid.
2002.
Devel-opment of a dependency treebank for Russian andits possible applications in NLP.
In Proceedings ofLREC, page 852856.Boguslavsky, I. M., L. L. Iomdin, V. S. Sizov, andI.
Chardin.
2003.
Parsing with a treebank.
In Pro-ceedings of the Conference on Cognitive Modelingin Linguistics [In Russian].Buchholz, S. and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Pro-ceedings of CoNLL, 149?164.Chang, C.-C. and C.-J.
Lin, 2001.
LIBSVM: A Libraryfor Support Vector Machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.Chardin, Ivan.
2004.
Dependency Treebanks and TheirUse in Parsing.
Ph.D. thesis, Russian Academy ofScience [In Russian].Cowan, B. and M. Collins.
2005.
Morphology andreranking for the statistical parsing of spanish.
InProceedings of HLT/EMNLP, 795?802.Dienes, P. and A. Dubey.
2003.
Deep syntactic pro-cessing by combining shallow methods.
In Proceed-ings of ACL, 431?438.Dubey, A. and F. Keller.
2003.
Probabilistic parsingfor German using sister-head dependencies.
In Pro-ceedings of ACL, 96?103.Eryi?git, G. and K. Oflazer.
2006.
Statistical depen-dency parsing of Turkish.
In Proceedings of EACL,89?96.Eryigit, G., J. Nivre, and K. Oflazer.
2006.
The in-cremental use of morphological information and lex-icalization in data-driven dependency parsing.
InProceedings of the 21st International Conferenceon the Computer Processing of Oriental Languages,498?507.Haji?c, J., B. Vidova Hladka, J. Panevov?a, E. Haji?cov?a,P.
Sgall, and P. Pajas.
2001.
Prague DependencyTreebank 1.0.
LDC, 2001T10.Klein, D. and C. D. Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of ACL, 423?430.Kudo, T. and Y. Matsumoto.
2002.
Japanese depen-dency analysis using cascaded chunking.
In Pro-ceedings of CoNLL, 63?69.Magerman, D. M. 1995.
Statistical decision-tree mod-els for parsing.
In Proceedings of ACL, 276?283.McDonald, R. and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.
InProceedings of EMNLP-CoNLL, 122?131.Mel?
?cuk, I.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.Nivre, J. and J. Nilsson.
2005.
Pseudo-projective de-pendency parsing.
In Proceedings of ACL, 99?106.Nivre, J., J.
Hall, S. K?ubler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007a.
The CoNLL 2007shared task on dependency parsing.
In Proceedingsof the CoNLL Shared Task of EMNLP-CoNLL 2007,915?932.Nivre, J., J.
Hall, J. Nilsson, A. Chanev, G. Eryi?git, S.K?ubler, S. Marinov, and E. Marsi.
2007b.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13:95?135.Nivre, J.
2003.
An efficient algorithm for projectivedependency parsing.
In Proceedings of IWPT, 149?160.
?vrelid, L. and J. Nivre.
2007.
When word order andpart-of-speech tags are not enough ?
swedish depen-dency parsing with rich linguistic features.
In Pro-ceedings of RANLP, 447?451.Petrov, S., L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable treeannotation.
In Proceedings of COLING/ACL, 433?440.Ratnaparkhi, A.
1997.
A linear observed time statis-tical parser based on maximum entropy models.
InProceedings of EMNLP, 1?10.Tesni`ere, L.
1959.?El?ements de syntaxe structurale.Editions Klincksieck.Tsarfaty, R. and K. Sima?an.
2007.
Three-dimensionalparametrization for parsing morphologically richlanguages.
In Proceedings of IWPT, 156?167.Tsarfaty, R. 2006.
Integrated morphological andsyntactic disambiguation for modern hebrew.
InProceedings of the COLING/ACL 2006 Student Re-search Workshop, 49?54.Vapnik, V. N. 1995.
The Nature of Statistical LearningTheory.
Springer.Yamada, H. and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProceedings of IWPT, 195?206.648
