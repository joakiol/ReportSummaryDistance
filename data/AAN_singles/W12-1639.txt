Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 280?289,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsA Regression-based Approach to Modeling Addressee BackchannelsAllison Terrell, Bilge MutluDepartment of Computer Sciences, University of Wisconsin?Madison1210 West Dayton Street, Madison, WI 53705, USA{aterrell,bilge}@cs.wisc.eduAbstractDuring conversations, addressees produceconversational acts?verbal and nonverbalbackchannels?that facilitate turn-taking,acknowledge speakership, and communicatecommon ground without disrupting thespeaker?s speech.
These acts play a key rolein achieving fluent conversations.
Therefore,gaining a deeper understanding of how theseacts interact with speaker behaviors in shap-ing conversations might offer key insightsinto the design of technologies such ascomputer-mediated communication systemsand embodied conversational agents.
In thispaper, we explore how a regression-basedapproach might offer such insights into mod-eling predictive relationships between speakerbehaviors and addressee backchannels ina storytelling scenario.
Our results revealspeaker eye contact as a significant predictorof verbal, nonverbal, and bimodal backchan-nels and utterance boundaries as predictors ofnonverbal and bimodal backchannels.1 IntroductionConversations involve a dynamic shifting of speak-ership, one party playing the role of the ?speaker?and the other(s) the role of the ?addressee?
at anygiven moment (Goodwin, 1981; Levinson, 1988;Clark, 1996).
In these roles, while speakers pro-duce the majority of the conversational content, ad-dressees play a major role in facilitating speakershipby performing backchannels?verbal and nonverbalacts such as ?uh huh?
and head nods that indicate theaddressee?s understanding and involvement and ac-knowledge that the speaker has and may continue toNods*.923p < .001*.297p = .011*.618p = .009*OverlappingBimodalBackchannelsR2=.961Speech* Gaze*ConjunctionsGesturesPitchFigure 1: A mapping of the predictive relationships be-tween speaker behaviors and overlapping bimodal ad-dressee backchannels.
?
coefficients show the relativeimportance of significant predictors of backchannel be-haviors.have the floor (Yngve, 1970; Drummond and Hop-per, 1993).Backchannels serve as a mechanism for coopera-tion between speakers and addressees to achieve ef-ficient communication (Brunner, 1979; Grice, 1989)and to establish rapport (Drolet and Morris, 2000).The design of conversational technologies suchas computer-mediated communication systems willhave to facilitate the use of backchannel mechanismsto help their users achieve efficient conversations.Similarly, embodied conversational agents will haveto use these mechanisms to achieve efficient inter-actions with their users.
However, these develop-ments require a deeper understanding of backchan-nel behavior and models of the relationship betweenbackchannel acts and speaker behaviors.280Research across many communities includingdiscourse processes, dialog systems, and human-computer interaction has explored the use ofbackchannels in conversations and sought to modelthe relationships between backchannel acts andother conversational processes using techniques thatrange from contingency analyses (Truong et al,2011) to model training (Morency et al, 2010).
Inthis paper, we propose a complementary, regression-based approach to untangle the predictive rela-tionships between speaker behaviors and addresseebackchannels.
This approach provides us with anunderstanding of what speaker behaviors are signifi-cant predictors of addressee backchannels and of therelative contributions of each behavior in these pre-dictions.
The resulting models inform us of whatspeaker behaviors are important to support in in-teractive systems and communication technologiesto facilitate addressee backchannels and comple-ment finer-granulated analyses of specific backchan-nel mechanisms.We contextualize our exploration in a storytellingscenario, which requires addressees to rely on andfrequently use backchannels to participate in the dis-course while maintaining consistency in conversa-tional roles, using a multimodal data corpus col-lected from 24 dyads.
Our analysis includes verbaland nonverbal backchannels, focusing on continuersand assessments in the verbal channel and head nodsin the nonverbal channel.
In the remainder of the pa-per, we review related work, describe our methodol-ogy, present our results, and discuss our findings andtheir implications for future research and the designof communication and interactive technologies.2 BackgroundConversations involve a cooperative process inwhich interlocutors manage the floor, negotiateturns, and provide feedback with the aid of subtlelinguistic and extralinguistic cues?backchannels?that might not significantly contribute to the sub-stance of the conversation (Yngve, 1970; Brunner,1979; Grice, 1989; Drummond and Hopper, 1993).These backchannels allow parties, particularly ad-dressees, to exchange information on their inten-tions and statuses and to participate in the conver-sation without disrupting ongoing speech (Morrisand Desebrock, 1977; White, 1989).
Backchan-nels differ from ?backchannel inviting cues,?
whichmight indicate what might be an appropriate timefor a backchannel (Gravano and Hirschberg, 2011).While backchannels are produced universally, in-dividual characteristics such as gender (Helweg-Larsen et al, 2004) and cultural background (White,1989; Ward and Tsukahara, 2000) significantlyshape their production and interpretation.2.1 Backchannel CuesResearchers have sought to distinguish and cate-gorize the wide range of backchannels based onhow they are expressed by addressees (Jenkins andParra, 2003) and how they contribute to the conver-sation(Young and Lee, 2004).
The majority of re-search on backchannels considers verbal or linguis-tic cues and offers several categorizations.
One ofthese categorizations distinguishes continuers fromassessments (Young and Lee, 2004).
Continuers areshort, nondescript verbal segments such as ?uh huh?and ?yeah?
that prompt the speaker to continue talk-ing, while assessments are longer verbal segmentssuch as ?oh, wow?
and ?really??
that offer commen-tary or request clarification on the speaker?s state-ments.Another classification of verbal backchannels dis-tinguishes among non-lexical, phrasal, and substan-tive backchannels (Iwasaki, 1997; Young and Lee,2004).
Non-lexical backchannels include vocaliza-tions such as ?hmm?
or ?uh huh?
that offer littleor no meaning but indicate the addressee?s engage-ment in the conversation.
Phrasal backchannels in-volve simple, well-established expressions such as?Really??
or ?Are you serious??
that indicate ac-knowledgment.
Finally, substantive backchannelsinvolve the addressee taking the floor for brief pe-riods and include repetitions, summary statements,clarifying questions about the speaker?s speech, re-pair, and collaborative completions.Research on backchannels also describes nonver-bal or extralinguistic cues such as smiling (Brunner,1979) and gaze (Rosenfeld and Hancks, 1980) ascommon backchannel behaviors that indicate agree-ment, understanding, or engagement in the conver-sation (Jenkins and Parra, 2003).
Nodding is a par-ticularly common nonverbal backchannel behaviorthat plays a range of roles from indicating agree-281ment to conveying sympathy and understanding withthe speaker?s perspective (Stivers, 2008).
While ver-bal and nonverbal backchannels play similar com-municative roles, the specific context of the conver-sation, such as whether the conversation involves anegotiation or a discussion, shapes how participantsperform and interpret the two forms of backchannels(Jenkins and Parra, 2003).
Addressees often displayboth verbal and nonverbal backchannels (Truong etal., 2011), such as concurrently nodding and saying?yeah?
to express agreement.2.2 Modeling BackchannelsResearch on conversational backchannels involves awide range of modeling approaches including rule-based models (Duncan, 1972), contingency analysis(Truong et al, 2011), and trained models (Morencyet al, 2010) across a wide range of conversa-tional contexts from telephone conversations (Wardand Tsukahara, 2000) to face-to-face interactions(Truong et al, 2011).
Rule-based models capturerelationships between backchannels and other con-versational behaviors based on prototypical exam-ples of commonly observed behaviors.
Contingencyanalysis offers a quantitative basis for modelingthese relationships through pairwise analyses of co-occurrences.
Finally, statistical learning techniquesallow researchers to train machine learning algo-rithms, such as Support Vector Machines (SVM) andHidden Markov models (HMM), on data that cap-ture these relationships in order to estimate the tim-ing of backchannels.2.3 Regression-based ModelingWhile it remains unexplored in the context of mod-eling backchannel behaviors, regression-based ap-proaches are commonly used in modeling complexrelationships among many variables.
In the contextof modeling discourse and dialog, frameworks suchas PARADISE (PARAdigm for DIalogue SystemEvaluation) build on regression-based approaches toidentify predictive relationships between several el-ements of dialog and objective or subjective out-comes of the dialog (Walker et al, 1997).
Re-searchers have used these frameworks to evaluatethe effectiveness of spoken dialog in interactive sys-tems (Foster et al, 2009; Peltason et al, 2012).3 MethodDue to the broad range of verbal and nonverbalbackchannels, we chose to focus on a limited subsetof verbal and nonverbal cues, including continuersand assessments as verbal backchannels and headnods as nonverbal backchannels.
Although there arenumerous possible speaker behaviors, which maypredict backchannels, we focused on six cues basedon previous research: (1) speaker?s gaze (directedtoward the addressee), (2) nods, (3) gestures, (4)speech (whether the speaker is speaking or not), (5)conjunctions in the speaker?s speech, and (6) pitchvariance in the speaker?s speech.
These six predic-tors were then used to build models for five depen-dent variables: (1) nonverbal backchannels, (2) ver-bal backchannels, (3) concurrent verbal and nonver-bal backchannels (e.g., a nod and an ?OK?
startingsimultaneously), (4) overlapping verbal and nonver-bal backchannels (e.g., a nod followed by an ?OK?towards the end of the nod), and (5) independent bi-modal backchannels (the presence of either verbal ornonverbal backchannels).
We modeled the relation-ships between these predictors and dependent vari-ables using stepwise regression.3.1 Participants and Data CorpusA total of 48 subjects from the University ofWisconsin?Madison participated in this study.
Theystudied a diverse set of fields and were aged be-tween 18 and 28.
All participants were native En-glish speakers.
We assigned participants into dyadsand conversational roles following a fully stratifieddesign to control for the effects of gender compo-sition of the dyads.
We discarded data from onedyad, because the participants did not conform tothe conversational roles that they were asked to fol-low.
With this omission, our final dataset consistedof 23 dyads.Our experimental setup followed common con-ventions of face-to-face conversations.
Two partici-pants unfamiliar with one another were seated acrossfrom each other at a ?social distance?
of five feet(Hall, 1963).
An illustration of our experimentalsetup can be seen in Figure 2.
The data collectionequipment consisted of three high-definition videocameras at 1080p resolution and 30p frame rate,two high-fidelity lapel microphones, and an omni-2825 feetSpeakerAddresseeInteractioncamera &microphoneSpeaker cameraAddressee cameraSpeakermicrophoneAddresseemicrophoneSpeakerParticipantsAddresseeFigure 2: The experimental setup (left) shows the place-ment of the participants at a ?social distance?
and of theequipment for capturing data.
The snapshots (right) showthe vantage point from each of the three cameras.directional microphone.
Two of the video cameraswere positioned across from each participant, cap-turing their upper torso from a direct frontal angle,while the lapel microphones captured their speech.The third camera and the omni-directional micro-phone recorded the speech and nonverbal behaviorsof both participants from a side angle.
The finaldatas corpus consisted of 1 hour and 31 minutes ofaudio and video.
The average video length was 3minutes and 57 seconds.3.2 ProcedureThe experimental task involved partaking in a sto-rytelling scenario that aimed to elicit a wide rangeof behavioral and interactional mechanisms.
In thisscenario, one of the participants took on the role ofthe speaker and narrated the plot of their favoritemovie to the second participant who took on the roleof the addressee.
We expected this scenario to pro-vide us with a rich context to observe backchannels.Participants were first given a brief description ofthe experiment and asked to review and sign a con-sent form.
The experimenter then seated the partic-ipants, assigned them conversational roles, and setup the data collection equipment.
Participants firstperformed an acclimation task (getting to know oneanother) that was not considered part of the exper-imental task.
The participants then performed thestorytelling scenario.
Following the experiment, theexperimenter debriefed the participants.
Participantswere paid $10 for their time.3.3 MeasurementsBased on a preliminary analysis of our data, weidentified five forms of addressee backchannels asdependent variables: (1) nonverbal backchannels,(2) verbal backchannels, (3) concurrent verbal andnonverbal backchannels (e.g., a nod and an ?OK?starting simultaneously), (4) overlapping verbal andnonverbal backchannels (e.g., a nod followed by an?OK?
towards the end of the nod), and (5) indepen-dent bimodal backchannels (either verbal or nonver-bal backchannels).Our independent variables consisted of speakerbehaviors that previous research suggested as likelypredictors of addressee backchannels and that a real-time interactive system might be able to capture andinterpret.
These variables included visible and au-dible features from the speaker?s movements andspeech, such as the presence or absence of speechand pitch variability, and specific linguistic featuresthat might signal discourse structure, such as con-junctions.
Drawing on these considerations, ouranalysis included speaker?s gaze (directed towardthe addressee), nods, gestures, speech (whether thespeaker is speaking or not), conjunctions in thespeaker?s speech, and pitch variance measurementsof the speaker?s speech.In our measurement of pitch, we sought to cap-ture computationally feasible, high-level intona-tional characteristics of the speech by calculating thevariability in pitch in the entire conversation.
Lowpitch variability indicated more monotonous speak-ers, whereas high pitch variability represented moreexpressive speech.
This measure was calculated byfinding the average pitch of the speaker throughoutthe conversation and aggregating the difference be-tween the average pitch and the pitch value at eachframe, as expressed below:pitch variance =n?i=0|pitch?
pitchi|Here, the number of measurements in the conver-sation is represented by n; each individual measure-ment is represented by i; the speaker?s average pitchin the entire conversation is represented by pitch;and the pitch value at each individual measurementis represented by pitchi.The data was labeled using a combination of man-ual and computational techniques.
All speaker and283Measure (y) Function (?0 + ?1x1 + .
.
.
+ ?nxn+ e) R2 SignificanceNonverbal backchannels .138 + .635 ?
N (gaze) + .374 ?
N (speech) + .089 .911 gazespeechp < .001p = .008Verbal backchannels .034 + .875 ?
N (gaze) + .067 .977 gaze p < .001Concurrentbimodal backchannels.019 + .471 ?
N (gaze) + .822 ?
N (speech) + .059 .940 gazespeechp < .001p < .001Overlappingbimodal backchannels.013 + .923 ?
N (gaze) + .618 ?
N (nods) + .297 ?
N (speech) + .061 .966 gazenodsspeechp < .001p = .009p = .011Independentbimodal backchannels.134 + .483 ?
N (gaze) + .212 ?
N (pitch) + .074 .896 gazepitchp < .001p = .014Figure 3: The final models for each dependent variable after elimination in the stepwise regression analysis includingonly the significant predictors.
Gaze was a significant predictor in all five models.
Speech was significant in threemodels.
Pitch variability and nods each significantly predicted one type of backchannel.addressee utterances were transcribed using Praat.Speech and conjunctions measurements were drawnfrom this transcription.
Only pauses that were longerthan 500 milliseconds were considered as absenceof speech; speech segments that were separated byshorter pauses were combined into a single segment.The pitch variability was automatically extracted us-ing Praat.
A primary coder labeled 100% of theremaining attributes (addressee nods, speaker nods,speaker gestures, and speaker gaze).
To evaluatereliability, a second coder labeled 10% of a ran-domly sampled subset of the data.
The inter-raterreliability showed substantial agreement for all at-tributes; addressee nods (94% agreement, Cohen?s?
= 0.72), speaker nods (92% agreement, Cohen?s?= 0.71), speaker gesture (87% agreement, Cohen?s?
= 0.67), and speaker gaze (96% agreement, Co-hen?s ?
= 0.75).All variables except pitch variability were binary:0 for not occurring and 1 for occurring of events.Pitch variability was a normalized continuous vari-able that varied between 0 and 1.
We consideredvariables as co-occurring when they overlapped witheach other within a window that spanned 200 mil-liseconds before the onset and after the end of eachvariable, following criteria from previous research(Truong et al, 2011).
The data corpus included mea-surements of all variables every 33.3 milliseconds.The data corpus for each dependent variable in-cluded aggregate counts of measurements for allvariables for each video.
The aggregate counts foreach video were normalized by dividing them by thelength of the video in seconds.
Finally, each vari-able across all videos were normalized to vary be-tween 0 (least frequent) and 1 (most frequent).
Theresulting data corpus included five data tables of size23x7 (data from 23 dyads on seven variables?thedependent variable and six predictors) for five typesof backchannel behaviors.3.4 AnalysisOur analysis followed a stepwise multiple linearregression to model the relationships between ourpredictors and dependent variables.
Each analysisstarted with the following linear form:y = (?0 +?1x1 +?2x2 + .
.
.+?nxn)+eHere, ?0 is a constant, whereas ?1 .
.
.
?n are coef-ficient weights for each of n predictors.
The valuesof each predictor for each measurement are repre-sented by x1 .
.
.xn.
The error term for the model is e,which is assumed to be mean zero and independentand identically distributed (i.i.d.
).Our use of stepwise regression followed a back-ward elimination algorithm in which the final modelis constructed by gradually excluding predictors thatdo not sufficiently contribute to the model.
For thepurposes of our study, we excluded any predictorwith a p-value above .25.
The final model is com-prised of predictors left which are statistically sig-284PredictorNodsGazeGesturesSpeechConjunctionsPitch?.297.621?.134.388.129.012p.542< .001.275.043.779.148Nonverbal backchannels(R2 = .912)Verbal backchannels(R2 = .968)Concurrentbimodal backchannels(R2 = .938)Overlappingbimodal backchannels(R2 = .962)?.209.952?.033.067.323.003p.242< .001.834.644.372.721?.332.574.052.631n/a?.001p.227.001.735.005n/a.761?.579.848.235.205?.191.009p.015< .001.113.091.622.655Independentbimodal backchannels(R2 = .889)?.530.542?.066.072.397.402p.205.001.489.775.325.118Figure 4: The results of the model for each dependent variable before elimination in the stepwise regression analysis.nificant (p < .050).
The ?
coefficients in the modelprovide the relative contribution of each indepen-dent variable in predicting the dependent variable.Our analysis considered the number of addresseebackchannels that occurred in each dyad as the met-ric of success.3.5 ResultsIn all five of our models, the independent variablesaccounted for a significant proportion of variance inthe dependent variables, varying between 89.9% and96.6%.
These results are summarized in Figure 3.In the first model, speaker behaviors accountedfor a significant portion of addressee nonverbalbackchannels, R2 = .911,F(2,20) = 113.6, p <.001.
Speaker gaze and speech significantly pre-dicted these backchannels, ?
= .635, t(21) = 6.02,p < .001 and ?
= .374, t(21) = 2.90, p = .008,respectively.
Gaze also significantly predicted ad-dressee verbal backchannels, ?
= .875, t(22) =27.24, p < .001, and explained a significant por-tion of the variance in them, R2 = .977,F(1,21) =702.5, p < .001.Results from the third model showed that gazeand speech explained a significant proportion ofthe variance in concurrent bimodal backchannels,R2 = .940,F(2,20) = 172.3, p < .001, and sig-nificantly predicted these backchannels, ?
= .471,t(21) = 3.98, p < .001 and ?
= .822, t(21) =7.92, p < .001, respectively.
In the fourth model,speaker behaviors explained a significant proportionof the variance in overlapping bimodal backchan-nels, R2 = .966,F(3,19) = 180, p < .001.
Speakergaze, speech, and nods were significant predictorsof these backchannels, ?
= .923, t(20) = 12.3, p <.001, ?
= .297, t(20) = 2.80, p = .011, ?
= .618,t(20) = 2.93, p = .009, respectively.Finally, results from the fifth model showed thatspeaker behaviors explained a significant proportionof the variance in independent bimodal backchan-nels, R2 = .896,F(2,20) = 94.63, p < .001.
Thespeaker?s gaze and the variability in the pitch of thespeaker?s speech significantly predicted these ad-dressee behaviors, ?
= .483, t(21) = 6.74, p < .001and ?
= .212, t(21) = 2.83, p = .014, respectively.4 DiscussionThe results of our statistical analysis show key rela-tionships between speaker behaviors and addresseebackchannels, reaffirming findings from previousstudies and revealing new relationships.
The para-graphs below provide a discussion of these find-ings and support them with examples of addresseebackchannels that we frequently observed in ourdata.
These examples are illustrated in Appendix Ain three episodes of interaction.
We also discuss theimplications of our approach for modeling conver-sational mechanisms.Our results are summarized in Figures 3 and 4,which show our final models after elimination andthe models before elimination, respectively.
Theresults in Figure 3, consistent with previous work(Bavelas et al, 2002), highlight the importance ofgaze in eliciting addressee backchannels.
Gaze is285included in all five of our models and is consistentlythe most important predictor of addressee backchan-nels in four of our five models.
In AppendixA, all six instances of the addressee backchannelsacross three illustrated episodes occur either whenthe speaker is looking toward the addressee or al-most concurrently with the speaker shifting gazeaway from the addressee.The results also show speech to be a signifi-cant predictor of addressee backchannels.
Three ofour models included speech as a predictor, whichsuggests that more frequent pauses in speech pro-vides the addressee with more opportunities to pro-vide backchannels; that frequent pauses prompt ad-dressees to provide more backchannels to facilitatethe continuation of the speaker?s speech; and/or thatthe addressees produce more backchannels, becausespeakers present more information.
Four instancesof backchannels shown in Appendix A occur imme-diately after an utterance has ended, which exem-plify pauses as opportune moments for the addresseeto produce backchannels.The significance of pitch variability in predictingindependent bimodal backchannels offers a differentperspective on the relationship between attributes ofspeaker pitch and addressee backchannels than pre-vious research does.
Although previous work sug-gested that pitch attributes do not have a significantrelationship with addressee backchannels in face-to-face conversations (Truong et al, 2011), pitch vari-ability significantly predicted independent bimodalbackchannels in our models.
We speculate that pitchvariability captures the speaker?s overall ability toengage their addressees in their speech and, thus,predicts addressee backchannels.
However, our re-sults show that this predictive relationship only ex-ists with independent bimodal backchannels and notwith verbal or nonverbal backchannels.
This dis-crepancy might be a result of variability across indi-viduals in their preferences to use verbal and nonver-bal backchannels, which is not captured by our mod-els for these individual backchannels but is capturedby the model that considers either type of backchan-nels.Speech did not significantly predict the ad-dressee?s verbal or independent bimodal backchan-nels, while it predicted nonverbal and concurrentand overlapping backchannels.
This finding sug-gests that frequent pauses in speech elicit primarilynonverbal backchannels and elicit verbal backchan-nels only in the presence of nonverbal backchan-nels.
A possible explanation of this finding is thataddressees might prefer nonverbal backchannels toverbal backchannels when they wish to facilitate thecontinuation of speech.A key contribution of our work is an explorationof the relationship between verbal and nonverbalbackchannels by modeling the concurrent onsets andoverlaps between these backchannels.
These modelsindicate that gaze and speech are significant predic-tors of concurrent onsets and overlaps in verbal andnonverbal backchannels and that speaker nods alsosignificantly predict overlaps.Our analysis also identified overlapping bimodalbackchannels as a new form of backchannel behav-ior that has not been considered by previous research(Truong et al, 2011).
These backchannels involvethe addressee producing a nonverbal backchanneltowards the end of the speaker?s speech and thenproducing a verbal backchannel when the speakerhad stopped talking.
We speculate that this behav-ior allows the addressee to express agreement duringthe speaker?s speech using nonverbal backchannelswithout disrupting the speech and reassert agree-ment using verbal backchannels when the speaker?sutterance is completed.
Episode B in AppendixA illustrates an instance of overlapping bimodalbackchannels.A final contribution of this work is an illustrationof the use of a regression-based approach in model-ing predictive relationships between speaker behav-iors and addressee backchannels.
This approach al-lowed us to explore the relationships among manyaspects of speaker and addressee behavior and toquantify the relative significance of each aspect ofthe speaker?s behaviors in predicting addressee be-haviors.
Our results confirmed findings from previ-ous research and produced new findings, revealingnovel relationships between these behaviors.
Theserelationships will serve as a basis for future researchto create more nuanced models of speaker and ad-dressee behavior.
They will also inform the designof future communication technologies and interac-tive systems that incorporate mechanisms to supportthe communication of key predictors of addresseebackchannels.286While the primary goal of our study was to betterunderstand relationships among conversational be-haviors, our models might also serve as coarse esti-mation models.
The models shown in Figure 3 mightbe used to estimate y?
?how frequently addresseebackchannels should appear?using the predictorcoefficients ?
and values for known speaker behav-iors x.
These estimations might complement finer-granulated models of backchannel mechanisms ingenerating opportune backchannel behaviors for ar-tificial agents and predict when these backchannelsmight occur in human-computer interaction scenar-ios.4.1 LimitationsOur work also has a number of limitations.
First,because our approach uses aggregate counts of be-haviors from the entire interaction, it does not ac-count for the temporal relationships among thesevariables.
Therefore, the insights offered by ourapproach are limited to high-level conclusions onthe relationships between these behaviors and illus-trations of these relationships in example episodesof interaction.
Future work should include com-plementary modeling techniques to build finer-granulated models of backchannel mechanisms.Although participants in each conversation wereexplicitly assigned to one of the roles of speaker andaddressee, we did not specifically tell addressees notto speak, which led to a greater amount of variabilityin their participation in the conversation, some offer-ing up their opinions or asking questions throughoutthe speaker?s story and others limiting their behav-iors to a small number of backchannels.
While thisvariability enabled more natural conversations, thislack of control might have limited the power of ourstatistical models.In this paper, we focused on a set of high-levelpredictors that allow for real-time capture and inter-pretation, ignoring underlying conversational mech-anisms such as repair, which might also serve as sig-nificant predictors of backchannels.
The relation-ships between these mechanisms and backchannelbehavior would be a fruitful area of exploration forfuture research.Finally, the generalizability of our results suffersfrom the limited extent of the conversational con-text and participation structure of our experimentalsetup.
Future work should seek to extend this ex-ploration to a broader set of conversational settings,such as interview and discussion scenarios, and par-ticipation structures, such as multi-party conversa-tions.5 ConclusionBackchannels are essential behaviors for achievingfluent and effective conversations.
Gaining a deeperunderstanding of how these behaviors shape conver-sations might offer key insights into the design oftechnologies such as computer-mediated communi-cation systems and embodied conversational agents.In an exploratory study, we used a stepwise regres-sion approach to model the relationships betweenvarious types of addressee backchannels and speakerbehaviors in a storytelling scenario.
We found thatgaze significantly predicted all types of backchannelbehaviors including verbal, nonverbal, and bimodalbackchannels.
Our results also showed that speech,speaker nods, and pitch variability predicted sometypes of backchannel behaviors.
While these re-sults have some limitations due to our methodolog-ical choices, they suggest directions for future workand offer preliminary insights toward a deeper un-derstanding of backchannel behaviors and how in-teractive systems and communication technologiesmight be designed to support these behaviors.AcknowledgmentsWe would like to thank Faisal Khan for his helpin data collection and processing.
This work wassupported by National Science Foundation award1149970.287ReferencesJ.B.
Bavelas, L. Coates, and T. Johnson.
2002.
Listenerresponses as a collaborative process: The role of gaze.Journal of Communication, 52(3):566?580.L.J.
Brunner.
1979.
Smiles can be back channels.
Jour-nal of Personality and Social Psychology, 37(5):728.H.H.
Clark.
1996.
Using language.
Cambridge Univer-sity Press.A.L.
Drolet and M.W.
Morris.
2000.
Rapport in conflictresolution: Accounting for how face-to-face contactfosters mutual cooperation in mixed-motive conflicts.Journal of Experimental Social Psychology, 36(1):26?50.K.
Drummond and R. Hopper.
1993.
Back channels re-visited: Acknowledgment tokens and speakership in-cipiency.
Research on Language and Social Interac-tion, 26(2):157?177.S.
Duncan.
1972.
Some signals and rules for takingspeaking turns in conversations.
Journal of person-ality and social psychology, 23(2):283.M.E.
Foster, M. Giuliani, and A. Knoll.
2009.
Compar-ing objective and subjective measures of usability in ahuman-robot dialogue system.
In Proc ACL/AFNLP,volume 2, pages 879?887.
Association for Computa-tional Linguistics.C.
Goodwin.
1981.
Conversational organization: Inter-action between speakers and hearers.
Academic PressNew York.A.
Gravano and J. Hirschberg.
2011.
Turn-taking cuesin task-oriented dialogue.
Computer Speech & Lan-guage, 25(3):601?634.P.
Grice.
1989.
Studies in the Way of Words.
HarvardUniversity Press.E.T.
Hall.
1963.
A system for the notation of proxemicbehavior.
American anthropologist, 65(5):1003?1026.M.
Helweg-Larsen, S.J.
Cunningham, A. Carrico, andA.M.
Pergram.
2004.
To nod or not to nod: An obser-vational study of nonverbal communication and statusin female and male college students.
Psychology ofWomen Quarterly, 28(4):358?361.S.
Iwasaki.
1997.
The northridge earthquake conver-sations: The floor structure and the ?loop?
sequencein japanese conversation.
Journal of Pragmatics,28(6):661?693.S.
Jenkins and I. Parra.
2003.
Multiple layers of meaningin an oral proficiency test: The complementary rolesof nonverbal, paralinguistic, and verbal behaviors inassessment decisions.
The Modern Language Journal,87(1):90?107.S.C.
Levinson, 1988.
Putting linguistics on a proper foot-ing: Explorations in Goffman?s concepts of participa-tion., pages 161?227.
Oxford, England: Polity Press.L.P.
Morency, I. de Kok, and J. Gratch.
2010.
A prob-abilistic multimodal approach for predicting listenerbackchannels.
Autonomous Agents and Multi-AgentSystems, 20(1):70?84.D.
Morris and G. Desebrock.
1977.
Manwatching: Afield guide to human behaviour.
HN Abrams NewYork, NY.J.
Peltason, N. Riether, B. Wrede, and I. Lu?tkebohle.2012.
Talking with robots about objects: a system-level evaluation in hri.
In Proceedings of the sev-enth annual ACM/IEEE international conference onHuman-Robot Interaction, HRI ?12, pages 479?486,New York, NY, USA.
ACM.H.M.
Rosenfeld and M. Hancks, 1980.
The nonverbalcontext of verbal listener responses, pages 193?206.The hague: Mouton Publishers.T.
Stivers.
2008.
Stance, alignment, and affiliation dur-ing storytelling: When nodding is a token of affilia-tion.
Research on Language and Social Interaction,41(1):31?57.K.P.
Truong, R.W.
Poppe, I.A.
de Kok, and D.K.J.Heylen.
2011.
A multimodal analysis of vocal andvisual backchannels in spontaneous dialogs.
In ProcInterspeech, pages 2973?2976.
International SpeechCommunication Association.M.A.
Walker, D.J.
Litman, C.A.
Kamm, and A. Abella.1997.
Paradise: A framework for evaluating spokendialogue agents.
In Proc EACL, pages 271?280.
As-sociation for Computational Linguistics.N.
Ward and W. Tsukahara.
2000.
Prosodic fea-tures which cue back-channel responses in english andjapanese.
Journal of Pragmatics, 32(8):1177?1207.S.
White.
1989.
Backchannels across cultures: Astudy of americans and japanese.
Language in soci-ety, 18(1):59?76.V.H.
Yngve.
1970.
On getting a word in edgewise.
InSixth Regional Meeting of the Chicago Linguistic So-ciety, volume 6, pages 657?677.R.F.
Young and J. Lee.
2004.
Identifying units in inter-action: Reactive tokens in korean and english conver-sations.
Journal of Sociolinguistics, 8(3):380?407.288Appendix A. Contextual ExamplesBelow are three example episodes drawn from our data.
Each episode displays all occurrences of all thepredictors we measured in real time.
All six instances of backchannels highlight the importance of thespeaker?s gaze and speech in eliciting addressee backchannels.Nonverbal backchannelVerbal backchannelAddresseeSpeechSpeakerPitchConjunctionswas one of  the owners of  the farm or like the daughter of  the owner of  the farmGazeNodsGesturesorBackchannelBackchannel1 2Speaker SpeakerSpeakerNonverbal backchannelVerbal backchannelAddresseeSpeechSpeakerPitchConjunctionsthat guy definitely got screwed over in the court scene like right awayGazeNodsGesturesBackchannelBackchannelyeah34SpeakerNonverbal backchannelVerbal backchannelBackchannel BackchannelAddresseeSpeechSpeakerPitchConjunctionsit was a good movie, like Johnny Depp was really goodGazeNodsGestures5 6AddresseeSpeaker AddresseeSpeakerEpisode AEpisode BEpisode C289
