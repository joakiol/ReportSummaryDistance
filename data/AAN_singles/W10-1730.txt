Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201?206,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsMaximum Entropy Translation Modelin Dependency-Based MT FrameworkDavid Marec?ek, Martin Popel, Zdene?k Z?abokrtsky?Charles University in Prague, Institute of Formal and Applied LinguisticsMalostranske?
na?m.
25, Praha 1, CZ-118 00, Czech Republic{marecek,popel,zabokrtsky}@ufal.mff.cuni.czAbstractMaximum Entropy Principle has beenused successfully in various NLP tasks.
Inthis paper we propose a forward transla-tion model consisting of a set of maxi-mum entropy classifiers: a separate clas-sifier is trained for each (sufficiently fre-quent) source-side lemma.
In this waythe estimates of translation probabilitiescan be sensitive to a large number of fea-tures derived from the source sentence (in-cluding non-local features, features mak-ing use of sentence syntactic structure,etc.).
When integrated into English-to-Czech dependency-based translation sce-nario implemented in the TectoMT frame-work, the new translation model signif-icantly outperforms the baseline model(MLE) in terms of BLEU.
The perfor-mance is further boosted in a configurationinspired by Hidden Tree Markov Mod-els which combines the maximum entropytranslation model with the target-languagedependency tree model.1 IntroductionThe principle of maximum entropy states that,given known constraints, the probability distri-bution which best represents the current state ofknowledge is the one with the largest entropy.Maximum entropy models based on this princi-ple have been widely used in Natural LanguageProcessing, e.g.
for tagging (Ratnaparkhi, 1996),parsing (Charniak, 2000), and named entity recog-nition (Bender et al, 2003).
Maximum entropymodels have the following formp(y|x) =1Z(x)exp?i?ifi(x, y)where fi is a feature function, ?i is its weight, andZ(x) is the normalizing factorZ(x) =?yexp?i?ifi(x, y)In statistical machine translation (SMT), trans-lation model (TM) p(t|s) is the probability that thestring t from the target language is the translationof the string s from the source language.
Typicalapproach in SMT is to use backward translationmodel p(s|t) according to Bayes?
rule and noisy-channel model.
However, in this paper we dealonly with the forward (direct) model.1The idea of using maximum entropy for con-structing forward translation models is not new.
Itnaturally allows to make use of various featurespotentially important for correct choice of target-language expressions.
Let us adopt a motivat-ing example of such a feature from (Berger et al,1996) (which contains the first usage of maxenttranslation model we are aware of): ?If house ap-pears within the next three words (e.g., the phrasesin the house and in the red house), then dans mightbe a more likely [French] translation [of in].
?Incorporating non-local features extracted fromthe source sentence into the standard noisy-channel model in which only the backward trans-lation model is available, is not possible.
Thisdrawback of the noisy-channel approach is typi-cally compensated by using large target-languagen-gram models, which can ?
in a result ?
play arole similar to that of a more elaborate (more con-text sensitive) forward translation model.
How-ever, we expect that it would be more beneficial toexploit both the parallel data and the monolingualdata in a more balance fashion, rather than extractonly a reduced amount of information from theparallel data and compensate it by large languagemodel on the target side.1A backward translation model is used only for pruningtraining data in this paper.201A deeper discussion on the potential advantagesof maximum entropy approach over the noisy-channel approach can be found in (Foster, 2000)and (Och and Ney, 2002), in which another suc-cessful applications of maxent translation modelsare shown.
Log-linear translation models (insteadof MLE) with rich feature sets are used also in(Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap-ineni et al, 1997).What makes our approach different from thepreviously published works is that1.
we show how the maximum entropy trans-lation model can be used in a dependencyframework; we use deep-syntactic depen-dency trees (as defined in the Prague Depen-dency Treebank (Hajic?
et al, 2006)) as thetransfer layer,2.
we combine the maximum entropy transla-tion model with target-language dependencytree model and use tree-modified Viterbisearch for finding the optimal lemmas label-ing of the target-tree nodes.The rest of the paper is structured as follows.
InSection 2 we give a brief overview of the trans-lation framework TectoMT in which the experi-ments are implemented.
In Section 3 we describehow our translation models are constructed.
Sec-tion 4 summarizes the experimental results, andSection 5 contains a summary.2 Translation frameworkWe use tectogrammatical (deep-syntactic) layer oflanguage representation as the transfer layer in thepresented MT experiments.
Tectogrammatics wasintroduced in (Sgall, 1967) and further elaboratedwithin the Prague Dependency Treebank project(Hajic?
et al, 2006).
On this layer, each sentenceis represented as a tectogrammatical tree, whosemain properties (from the MT viewpoint) are fol-lowing: (1) nodes represent autosemantic words,(2) edges represent semantic dependencies (a nodeis an argument or a modifier of its parent), (3) thereare no functional words (prepositions, auxiliarywords) in the tree, and the autosemantic words ap-pear only in their base forms (lemmas).
Morpho-logically indispensable categories (such as numberwith nouns or tense with verbs, but not numberwith verbs as it is only imposed by agreement) arestored in separate node attributes (grammatemes).The intuition behind the decision to use tec-togrammatics for MT is the following: we be-lieve that (1) tectogrammatics largely abstractsfrom language-specific means (inflection, agglu-tination, functional words etc.)
of expressingnon-lexical meanings and thus tectogrammaticaltrees are supposed to be highly similar across lan-guages,2 (2) it enables a natural transfer factor-ization,3 (3) and local tree contexts in tectogram-matical trees carry more information (especiallyfor lexical choice) than local linear contexts in theoriginal sentences.4In order to facilitate transfer of sentence ?syn-tactization?, we work with tectogrammatical nodesenhanced with the formeme attribute (Z?abokrtsky?et al, 2008), which captures the surface mor-phosyntactic form of a given tectogrammaticalnode in a compact fashion.
For example, thevalue n:pr?ed+4 is used to label semantic nounsthat should appear in an accusative form in aprepositional group with the preposition pr?ed inCzech.
For English we use formemes such asn:subj (semantic noun (SN) in subject position),n:for+X (SN with preposition for), n:X+ago (SNwith postposition ago), n:poss (possessive form ofSN), v:because+fin (semantic verb (SV) as a sub-ordinating finite clause introduced by because),v:without+ger (SV as a gerund after without), adj:attr(semantic adjective (SA) in attributive position),adj:compl (SA in complement position).We have implemented our experiments in theTectoMT software framework, which already of-fers tool chains for analysis and synthesis of Czechand English sentences (Z?abokrtsky?
et al, 2008).The translation scenario proceeds as follows.1.
The input English text is segmented into sen-tences and tokens.2.
The tokens are lemmatized and tagged withPenn Treebank tags using the Morce tagger(Spoustova?
et al, 2007).2This claim is supported by error analysis of output oftectogrammatics-based MT system presented in (Popel andZ?abok/rtsky?, 2009), which shows that only 8 % of translationerrors are caused by the (obviously too strong) assumptionthat the tectogrammatical tree of a sentence and the tree rep-resenting its translation are isomorphic.3Morphological categories can be translated almost inde-pendently from lemmas, which makes parallel training data?denser?, especially when translating from/to a language withrich inflection such as Czech.4Recall the house-is-somewhere-around feature in the in-troduction; again, the fact that we know the dominating (ordependent) word should allow to construct a more compacttranslation model, compared to n-gram models.202Figure 1: Intermediate sentence representations when translating the English sentence ?However, thisvery week, he tried to find refuge in Brazil.
?, leading to the Czech translation ?Pr?esto se tento pra?ve?ty?den snaz?il naj?
?t u?toc?is?te?
v Braz??lii.?.3.
Then the Maximum Spanning Tree parser(McDonald et al, 2005) is applied and asurface-syntax dependency tree (analyticaltree in the PDT terminology) is created foreach sentence (Figure 1a).4.
This tree is converted to a tectogrammaticaltree (Figure 1b).
Each autosemantic wordwith its associated functional words is col-lapsed into a single tectogrammatical node,labeled with lemma, formeme, and seman-tically indispensable morphologically cate-gories; coreference is also resolved.
Collaps-ing edges are depicted by wider lines in theFigure 1a.5.
The transfer phase follows, whose most dif-ficult part consists in labeling the tree withtarget-side lemmas and formemes5 (changesof tree topology are required relatively infre-quently).
See Figure 1c.6.
Finally, surface sentence shape (Figure 1d) issynthesized from the tectogrammatical tree,which is basically a reverse operation for the5In this paper we focus on using maximum entropyfor translating lemmas, but it can be used for translatingformemes as well.tectogrammatical analysis: adding punctua-tion and functional words, spreading mor-phological categories according to grammat-ical agreement, performing inflection (usingCzech morphology database (Hajic?, 2004)),arranging word order etc.3 Training the two modelsIn this section we describe two translation mod-els used in the experiments: a baseline translationmodel based on maximum likelihood estimates(3.2), and a maximum entropy based model (3.3).Both models are trained using the same data (3.1).In addition, we describe a target-language treemodel (3.4), which can be combined with boththe translation models using the Hidden TreeMarkov Model approach and tree-modified Viterbisearch, similarly to the approach of (Z?abokrtsky?and Popel, 2009).3.1 Data preprocessing common for bothmodelsWe used Czech-English parallel corpus CzEng 0.9(Bojar and Z?abokrtsky?, 2009) for training thetranslation models.
CzEng 0.9 contains about8 million sentence pairs, and also their tectogram-matical analyses and node-wise alignment.203We used only trees from training sections (about80 % of the whole data), which contain around 30million pairs of aligned tectogrammatical nodes.From each pair of aligned tectogrammaticalnodes, we extracted triples containing the source(English) lemma, the target (Czech) lemma, andthe feature vector.In order to reduce noise in the training data,we pruned the data in two ways.
First, we dis-regarded all triples whose lemma pair did not oc-cur at least twice in the whole data.
Second,we computed forward and backward maximumlikelihood (ML) translation models (target lemmagiven source lemma and vice versa) and deletedall triples whose probability according to one ofthe two models was lower than the threshold 0.01.Then the forward ML translation model wasreestimated using only the remaining data.For a given pair of aligned nodes, the featurevector was of course derived only from the source-side node or from the tree which it belongs to.
Asalready mentioned in the introduction, the advan-tage of the maximum entropy approach is that arich and diverse set of features can be used, with-out limiting oneself to linearly local context.
Thefollowing features (or, better to say, feature tem-plates, as each categorical feature is in fact con-verted to a number of 0-1 features) were used:?
formeme and morphological categories of thegiven node,?
lemma, formeme and morphological cate-gories of the governing node,?
lemmas and formemes of all child nodes,?
lemmas and formemes of the nearest linearlypreceding and following nodes.3.2 Baseline translation modelThe baseline TM is basically the ML translationmodel resulting from the previous section, lin-early interpolated with several translation modelsmaking use of regular word-formative derivations,which can be helpful for translating some less fre-quent (but regularly derived) lemmas.
For exam-ple, one of the derivation-based models estimatesthe probability p(zaj?
?mave?|interestingly) (possiblyunseen pair of deadjectival adverbs) by the valueof p(zaj??mavy?|interesting).
More detailed descrip-tion of these models goes beyond the scope of thispaper; their weights in the interpolation are verysmall anyway.3.3 MaxEnt translation modelThe MaxEnt TM was created as follows:1. training triples (source lemma, target lemma,feature vector) were disregarded if the sourcelemma was not seen at least 50 times (onlythe baseline model will be used for such lem-mas),2. the remaining triples were grouped by the En-glish lemma (over 16 000 groups),3. due to computational issues, the maximumnumber of triples in a group was reduced to1000 by random selection,4.
a separate maximum entropy classifierwas trained for each group (i.e., oneclassifier per source-side lemma) usingAI::MaxEntropy Perl module,65.
due to the more aggressive pruning of thetraining data, coverage of this model issmaller than that of the baseline model; in or-der not to loose the coverage, the two mod-els were combined using linear interpolation(1:1).Selected properties of the maximum entropytranslation model (before the linear interpolationwith the baseline model) are shown in Figure 2.We increased the size of the training data from10 000 training triples up to 31 million and eval-uated three relative quantities characterizing thetranslation models:?
coverage - relative frequency of source lem-mas for which the translation model offers atleast one translation,?
first - relative frequency of source lemmas forwhich the target lemmas offered as the firstby the model (argmax) are the correct ones,?
oracle - relative frequency of source lemmasfor which the correct target lemma is amongthe lemmas offered by the translation model.As mentioned in Section 3.1, there are contextfeatures making use both of local linear contextand local tree context.
After training the MaxEntmodel, there are about 4.5 million features withnon-zero weight, out of which 1.1 million features6http://search.cpan.org/perldoc?AI::MaxEntropy204Figure 2: Three measures characterizing the Max-Ent translation model performance, depending onthe training data size.
Evaluated on aligned nodepairs from the dtest portion of CzEng 0.9.are derived from the linear context and 2.4 millionfeatures are derived from the tree context.
Thisshows that the MaxEnt translation model employsthe dependency structure intensively.A preliminary analysis of feature weights seemsto support our intuition that the linear contextis preferred especially in the case of more sta-ble collocations.
For example, the most impor-tant features for translating the lemma bare arebased on the lemma of the following noun: tar-get lemma bosy?
(barefooted) is preferred if the fol-lowing noun on the source side is foot, while holy?
(naked, unprotected) is preferred if hand follows.The contribution of dependency-based featurescan be illustrated on translating the word drop.The greatest weight for choosing kapka (a droplet)as the translation is assigned to the feature captur-ing the presence of a node with formeme n:of+Xamong the node?s children.
The greatest weightsin favor of odhodit (throw aside) are assigned tofeatures capturing the presence of words such asgun or weapon, while the greatest weights in favorof klesnout (to come down) are assigned to fea-tures saying that there is the lemma percent or thepercent sign among the children.Of course, the lexical choice is influenced alsoby the governing lemmas, as can be illustratedwith the word native.
One can find a high-value feature for rodily?
(native-born) saying thatthe source-side parent is speaker; similarly formater?sky?
(mother) with governing tongue, androdny?
(home) with land.Linear and tree features are occasionally usedsimultaneously: there are high-valued positiveconfiguration BLEU NISTbaseline TM 10.44 4.795MaxEnt TM 11.77 5.135baseline TM + TreeLM 11.77 5.038MaxEnt TM + TreeLM 12.58 5.250Table 1: BLEU and NIST evaluation of four con-figurations of our MT system; the WMT 2010 testset was used.weights for translating order as objednat (reserve,give an order for st.) assigned both to tree-basedfeatures saying that there are words such as pizza,meal or goods and to linear features saying that thevery following word is some or two.3.4 Target-language tree modelAlthough the MaxEnt TM captures some contex-tual dependencies that are covered by languagemodels in the standard noisy-channel SMT, it maystill be beneficial to exploit target-language mod-els, because these can be trained on huge mono-lingual corpora.
We use a target-language depen-dency tree model differing from standard n-grammodel in two aspects:?
it uses tree context instead of linear context,?
it predicts tectogrammatical attributes (lem-mas and formemes) instead of word forms.In particular, our target-language tree model(TreeLM) predicts the probability of node?slemma and formeme given its parent?s lemma andformeme.
The optimal (lemma and formeme) la-beling is found by tree-modified Viterbi search;for details see (Z?abokrtsky?
and Popel, 2009).4 ExperimentsWhen included into the above described transla-tion scenario, the MaxEnt TM outperforms thebaseline TM, be it used together with or with-out TreeLM.
The results are summarized in Ta-ble 1.
The improvement is statistically signif-icant according to paired bootstrap resamplingtest (Koehn, 2004).
In the configuration withoutTreeLM the improvement is greater (1.33 BLEU)than with TreeLM (0.81 BLEU), which confirmsour hypothesis that MaxEnt TM captures some ofthe contextual dependencies resolved otherwise bylanguage models.2055 ConclusionsWe have introduced a maximum entropy transla-tion model in dependency-based MT which en-ables exploiting a large number of feature func-tions in order to obtain more accurate translations.The BLEU evaluation proved significant improve-ment over the baseline solution based on the trans-lation model with maximum likelihood estimates.However, the performance of this system still be-low the state of the art (which is around BLEU 16for the English-to-Czech direction).AcknowledgmentsThis research was supported by the grantsMSM0021620838, MS?MT C?R LC536, FP7-ICT-2009-4-247762 (Faust), FP7-ICT-2007-3-231720(EuroMatrix Plus), GA201/09/H057, and GAUK116310.
We thank two anonymous reviewers forhelpful comments.ReferencesOliver Bender, Franz Josef Och, and Hermann Ney.2003.
Maximum entropy models for named entityrecognition.
In Proceedings of CoNLL 2003, pages148?151.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tional linguistics, 22(1):39?71.Ondr?ej Bojar and Zdene?k Z?abokrtsky?.
2009.
CzEng0.9, Building a Large Czech-English Automatic Par-allel Treebank.
The Prague Bulletin of Mathemati-cal Linguistics, 92:63?83.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st NorthAmerican chapter of the ACL conference, pages132?139, San Francisco, USA.George Foster.
2000.
A maximum entropy/minimumdivergence translation model.
In Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, pages 45?52, Morristown, USA.Association for Computational Linguistics.Kevin Gimpel and Noah A. Smith.
2009.
Feature-rich translation by quasi-synchronous lattice pars-ing.
In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing,pages 219?228, Morristown, USA.
Association forComputational Linguistics.Jan Hajic?
et al 2006.
Prague Dependency Treebank2.0.
CD-ROM, Linguistic Data Consortium, LDCCatalog No.
: LDC2006T01, Philadelphia.Jan Hajic?.
2004.
Disambiguation of Rich Inflection ?Computational Morphology of Czech.
Charles Uni-versity ?
The Karolinum Press, Prague.Abraham Ittycheriah and Salim Roukos.
2007.
Directtranslation model 2.
In Candace L. Sidner, TanjaSchultz, Matthew Stone, and ChengXiang Zhai, edi-tors, HLT-NAACL, pages 57?64.
The Association forComputational Linguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, volume 4, pages 388?395.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceed-ings of HLT / EMNLP, pages 523?530, Vancouver,Canada.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of ACL,pages 295?302.Kishore A. Papineni, Salim Roukos, and Todd R.Ward.
1997.
Feature-based language understand-ing.
In European Conference on Speech Commu-nication and Technology (EUROSPEECH), pages1435?1438, Rhodes, Greece, September.Martin Popel and Zdene?k Z?abok/rtsky?.
2009.Improving English-Czech Tectogrammatical MT.The Prague Bulletin of Mathematical Linguistics,(92):1?20.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In In Proceedingsof EMNLP?96, pages 133?142.Petr Sgall.
1967.
Generativn??
popis jazyka a c?eska?deklinace.
Academia, Prague.Drahom?
?ra Spoustova?, Jan Hajic?, Jan Votrubec, PavelKrbec, and Pavel Kve?ton?.
2007.
The Best of TwoWorlds: Cooperation of Statistical and Rule-BasedTaggers for Czech.
In Proceedings of the Work-shop on Balto-Slavonic Natural Language Process-ing, ACL 2007, pages 67?74, Praha.Zdene?k Z?abokrtsky?
and Martin Popel.
2009.
Hiddenmarkov tree model in dependency-based machinetranslation.
In Proceedings of the ACL-IJCNLP2009 Conference Short Papers, pages 145?148, Sun-tec, Singapore.Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas.
2008.TectoMT: Highly Modular MT System with Tec-togrammatics Used as Transfer Layer.
In Proceed-ings of the 3rd Workshop on Statistical MachineTranslation, ACL, pages 167?170.206
