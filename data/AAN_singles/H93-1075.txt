A S IMULAT ION-BASED RESEARCH STRATEGYFOR DES IGNING COMPLEX NL  SYSTEMS*Sharon Oviatt, Philip Cohen, Michelle Wang 8J Jeremy Gaston tABSTRACTBasic research is critically needed to guide the developmentof a new generation of multimodal and multilingual NL sys-tems.
This paper summarizes the goals, capabilities, com-puting environment, and performance characteristics of anew semi-automatic simulation technique.
This techniquehas been designed to support a wide spectrum of empiri-cal studies on highly interactive speech, writing, and multi-modal systems incorporating pen and voice.
Initial studiesusing this technique have provided information on people'slanguage, performance, and preferential use of these com-munication modalities, either alone or in multimodal combi-nation.
One aim of this research as been to explore howthe selection of input modality and presentation format canbe used to reduce difficult sources of linguistic variability inpeople's peech and writing, such that more robust systemprocessing results.
The development of interface techniquesfor channeling users' language will be important to the abilityof complex NL systems to function successfully in actual fielduse, as well as to the overall commercialization f this tech-nology.
Future extensions of the present simulation researchalso are discussed.1.
INTRODUCTIONBasic research is critically needed to guide the develop:ment of a new generation of complex natural languagesystems that are still in the planning stages, such as onesthat support multimodal, multilingual, or multiparty ex-changes across a variety of intended applications.
In thecase of planned multimodal systems, for example, thepotential exists to support more robust, productive, andflexible human-computer interaction than that affordedby current unimodal ones \[3\].
However, since multimodalsystems are relatively complex, the problem of how todesign optimal configurations i unlikely to be solvedthrough simple intuition alone.
Advance empirical work*This research was supported in part by Grant No.
IFti-9213472from the National Science Foundation to the first authors, as wellas additional funding and equipment donat ions from ATR Interns-tional, Apple Computer, USWest, and ~Vacom Inc. Any opinions,findings, or conclusions expressed in this paper are those of theauthors, and do not necessarily reflect he views of our sponsors.t Michelle Wang is afl'diated with the Computer Science Depart-ment and Jeremy Gaston with the Symbolic Systems Program atStanford University.Computer  D ia logue LaboratoryA.I .
Center ,  SRI  In ternat iona l333 Ravenswood AvenueMen lo  Park ,  Cal i forn ia ,  U.S.A.  94025with human subjects will be needed to generate a factualbasis for designing multimodal systems that can actuallydeliver performance superior to unimodal ones.In particular, there is a special need for both method-ological tools and research results based on high-qualitysimulations of proposed complex NL systems.
Suchsimulations can reveal specific information about peo-ple's language, task performance, and preferential useof different types of systems, so that they can be de-signed to handle expected input.
Likewise, simulationresearch provides a relatively affordable and nimble wayto compare the specific advantages and disadvantages ofalternative architectures, uch that more strategic de-signs can be developed in support of particular applica-tions.
In the longer term, conclusions based on a seriesof related simulation studies also can provide a broaderand more principled perspective on the best applicationprospects for emerging technologies such as speech, pen,and multimodal systems incoporating them.In part for these reasons, simulation studies of spokenlanguage systems have become common in the past fewyears, and have begun to contribute to our understand-ing of human speech to computers \[1, 5, 6, 7, 8, 17\].
How-ever, spoken language simulations typically have beenslow and cumbersome.
There is concern that delayed re-sponding may systematically distort the data that thesesimulation studies were designed to collect, especially fora modality like speech from which people expect speed\[6, 10, 15\].
Unlike research on spoken language systems,there currently is very little literature on handwritingand pen systems.
In particular, no simulation studieshave been reported on: (1) interactive handwriting 1 \[6\],(2) comparing interactive speech versus handwriting asalternative ways to interact with a system, or (3) ex-amining the combined use of speech and handwriting tosimulated multimodal systems of different types.
Po-tential advantages of a combined pen/vo i~ system havebeen outlined previously \[4, 12\].
High quality simulation1Although we are familiar with nonlnteractive writing fromeveryday activities like personal notetaking, very llttle-is knownabout interactive writing and pen use as a modality of human-computer interaction.370research on these topics will be especially important othe successful design of mobile computing technology,much of which will emphasize communications and bekeyboardless.The simulation technique developed for this researchaims to: (1) support a very rapid exchange with sim-ulated speech, pen, and pen/voice systems, such thatresponse delays are less than 1 second and interactionscan be subject-paced, (2) provide a tool for investigat-ing interactive handwriting and other pen functional-ity, and (3) devise a technique appropriate for compar-ing people's use of speech and writing, such that dif-ferences between these communication modalities andtheir related technologies can be better understood.
To-ward these ends, an adaptable simulation method wasdesigned that supports a wide range of studies investi-gating how people speak, write, or use both pen andvoice when interacting with a system to complete qual-itatively different asks (e.g., verbal/temporal, compu-tational/numeric, graphic/cartographic).
The methodalso supports examination of different issues in spoken,written, and combined pen/voice interactions (e.g., typ-ical error patterns and resolution strategies).In developing this simulation, an emphasis was placed onproviding automated support for streamlining the simu-lation to the extent needed to create facile, subject-pacedinteractions with clear feedback, and to have compara-ble specifications for the different modalities.
Responsespeed was achieved in part by using scenarios with cor-rect solutions, and by preloading information.
This en-abled the assistant o click on predefined fields in orderto respond quickly.
In addition, the simulated systemwas based on a conversational model that provides ana-logues of human backchannel and propositional confir-mations.
Initial tasks involving service transactions em-bedded propositional-level confirmations in a compacttransaction "receipt," an approach that contributed tothe simulation's clarity and speed.
Finally, emphasis wasplaced on automating features to reduce attentional de-mand on the simulation assistant, which also contributedto the fast pace and low rate of technical errors in thepresent simulation.2.
S IMULAT ION METHODBasic simulation features for the studies completed todate are summarized below, and have been detailed else-where \[16\], although some adaptations to these specifica-tions are in progress to accommodate planned research.2 .1 .
P rocedure  and  Ins t ruct ionsVolunteer participants coming into the Computer Di-alogue Laboratory at SRI are told that the researchproject aims to develop and test a new pen/voice systemfor use on future portable devices.
To date, subjects haveincluded a broad spectrum of white-collar professionals,excluding computer scientists.
All participants o farhave believed that the "system" was a fully functionalone.
Following each session, they are debriefed aboutthe nature and rationale for conducting a simulation.During the study, subjects receive written instructionsabout how to enter information on an LCD tablet whenwriting, when speaking, and when free to use bothmodalities.
When writing, they are told to handwriteinformation with the electronic stylus directly onto ac-tive areas on the tablet.
They are free to print or writecursive.
When speaking, subjects are instructed to tapand hold the stylus on active areas as they speak into themicrophone.
During free choice, people are completelyfree to use either modality in any way they wish.
Partic-ipants also receive written instructions about how to usethe system to complete realistic tasks, which currentlyfocus on the broad class of service-oriented transactions(e.g., car rental reservations, personal banking, real es-tate selection).
Then they practice several scenarios us-ing spoken and written input until the system and thetasks are completely clear.People are encouraged to speak and write naturally.They are asked to complete the tasks according to in-structions, while working at their own pace.
Other thanproviding motivation to complete the tasks and speci-fying the input modality, an effort is made not to in-fluence the specific manner in which subjects expressthemselves.
They are encouraged to focus on complet-ing the tasks and are told that, if their input cannot beprocessed for any reason, this will be clear immediatelysince the system will respond with ???
to prompt themto try again.
Subjects are told how to remove or replaceinformation as needed.
Otherwise, they are told thatinput will be confirmed by the system on a transactionreceipt, which they can monitor to check that their re-quests are being met (see next section for details).
Ofcourse, participants' input actually is received by an in-formed assistant, who performs the role of interpretingand responding as the system would.The simulation assistant is instructed to respond as accu-rately and rapidly as possible to any spoken or writteninformation corresponding to predefined receipt fields.Essentially, the assistant tracks the subject's input, click-ing with a mouse on predefined fields on a Sun SPARC-station to send confirmations back to the subject.
Undersome circumstances, the assistant is instructed to senda ???
prompt instead of a confirmation.
For exam-ple, subjects receive ???
feedback when input is judgedto be inaudible or illegible, when the subject forgets to371supply task-critical information, or when input clearly isinappropriate, ambiguous, or underspecified.
In general,however, the assistant is instructed to use ???
feedbacksparingly in order to minimize intervention with people'snatural tendencies to speak or write.
If the subject com-mits a procedural error, such as forgetting to click beforeentering speech or attempting to enter information us-ing the wrong modality, then the assistant is instructednot to respond until the subject recovers and correctlyengages the system.
The assistant's task is sufficientlyautomated that he or she is free to focus attention onmonitoring the accuracy of incoming information, andon maintaining sufficient vigilance to respond promptlywith confirmations.2.2.
P resentat ion  FormatFor studies completed to date, two different promptingtechniques have been used to guide subjects' spoken andwritten input--  one unconstrained and one forms-based.In the relatively unconstrained presentation format, sub-jects must take the initiative to ask questions or stateneeds in one general workspace area.
No specific systemprompts direct their input.
They simply continue pro-viding information until their transaction receipt is com-pleted, correctly reflecting their requests.
In this case,guidance is provided primarily by the task itself and thereceipt.
When the presentation format is a form, labeledfields are used to elicit specific task content, for example:Car  p ickup locationl I.
In this case,the interaction is more system-guided, and linguistic andlayout cues are used to channel the content and order ofpeople's language as they work.For other studies in which people work with visual in-formation (e.g., graphic/cartographic tasks), differentgraphic dimensions of presentation format are manip-ulated.
In all studies, the goal is to examine the impactof presentation format on people's language and perfor-mance as they either speak or write to a simulated sys-tem.
As a more specific aim, assessments are being con-ducted of the extent o which different formats naturallyconstrain linguistic variability, resulting in opportunitiesfor more robust natural anguage processing.2.3.
Conversat iona l  FeedbackWith respect o system feedback, a conversational modelof human-computer interaction was adopted.
As a re-sult, analogues are provided of human backchannel andpropositional-level confirmations.
These confirmationsfunction the same for different input modalities and pre-sentation formats.
With respect o backchannel signals,subjects receive *** immediately following spoken in-put, and an electronic ink trace following written input.These confirmations are presented in the tablet's activearea or a narrow "confirmation panel" just below it.
Sub-jects are told that this feedback indicates that their inputhas been legible/audible and processable by the system,and that they should continue.In addition to this backchannel-level signal, subjects aretold to verify that their requests are being met success-fully by checking the content of the receipt at the bot-tom of the tablet.
This receipt is designed to confirmall task-critical information supplied during the interac-tion, thereby providing propositional confirmations.
Itremains visible throughout the transaction, and is com-pleted gradually as the interaction proceeds.
Althoughthe receipt varies for different asks, its form and contentremains the same for different modalities and presenta-tion formats.Apart from confirmation feedback, the simulation alsoresponds to people's questions and commands by trans-mitting textual and tabular feedback.
For example, if asubject selects the car model that he or she wants andthen says, "Do you have infant seats?"
or "Show me thecar options," a brief table would be displayed in whichavailable items like infant seats and car phones are listedalong with their cost.2 .4 .
Automated  FeaturesTo simplify and speed up system responding, the cor-rect receipt information associated with each task ispreloaded for the set of tasks that a subject is to re-ceive.
A series of preprogrammed dependency relationsbetween specified task-critical information and associ-ated receipt fields is used to support the automation ofpropositional confirmations.
As mentioned earlier, withthis arrangement the assistant simply needs to click oncertain predefined fields to send appropriate acknowledg-ments automatically as the subject gradually suppliesrelevant information.
Of course, if the subject makes aperformance error, the assistant must manually type andconfirm the error that occurs.
In such cases, however,canonical answers are maintained so that they can beconfirmed quickly when people self-correct, which theytend to do over 50% of the time.
The automated simula-tion strategy described above works well when researchcan take advantage of task scenarios that entail a limitedset of correct answers.An additional automated feature of the present simu-lation technique is a "random error generator," whichis designed to ensure that subjects encounter at least aminimal level of simulated system errors, in part to sup-port the credibility of the simulation.
In this research, ifsubjects do not receive at least one ???
response from372the system during a set of two tasks, then the simula-tion generates one.
This results in a minimum baselinerate of one simulated error per 33 items of informationsupplied, or 3%, which in this research as been consid-ered a relatively error-free nvironment.
The simulatederrors are distributed randomly across all task-criticalinformation supplied for the set of tasks.2.5.
Per fo rmance  Character i s t i csThe described method for organizing simulated responsefeedback was responsible in part for the fast pace of thepresent simulation.
In studies conducted to date, re-sponse delays during the simulation have averaged 0.4second between a subject's input and visible confirma-tion on the tablet receipt, with less than a 1-second elayin all conditions.
The rate of technical errors in exe-cuting the assistant's role according to instructions hasbeen low, averaging 0.05 such errors per task.
Further-more, any major error by the assistant would result indiscarding that subject's data, which currently has beenaveraging 6% of subjects tested.
The present simulationalso appears to be adequately credible, since no partici-pants to date have doubted that it was a fully functionalsystem.
As a result, no data has been discarded for thisreason.2.6.
S imula t ion  Env i ronmentThe computing equipment that supports this simulationtechnique includes two Sun workstations, one a SPARC-station 2, that are linked via ethernet.
A Wacom HD-648A integral transparent digitizing tablet/LCD displayis interfaced to the SPARC 2 through aVigra S-bus VGAcard.
An accompanying cordless digitizing pen is usedfor writing, clicking to speak, pointing, or otherwise op-erating the tablet.
A Crown PCC 160 microphone trans-mits spoken input from the subject to the simulationassistant, who listens through a pair of stereo speakersfrom a remote location.
The assistant also views an im-age of the subject working at the tablet, along with animage of all visible input and feedback occurring on thetablet.The user interface is based on the X-windows system,employing MIT Athena widgets.
X-windows is used forits ability to display results on multiple screens, includ-ing the subject's tablet and the assistant's workstation,and because the resulting program runs on equipmentfrom several manufacturers.
Two aspects of the systemarchitecture are designed for rapid interface adaptability.First, Widget Creation Language (WCL) enables non-programmers to alter the user interface layout.
Second,a simple textual language and interpreter were created toenable declarative specification of widget behavior andinterrelations.
Some widget behavior also is written inthe C programming language.Various modifications tothe standard X-windows opera-tion have been deployed to ensure adequate r al-time re-sponding needed for acceptable handwriting quality andspeed.
To avoid objectionable ag in the system's elec-tronic ink echo, a high-performance workstation (i.e.,Sun SPARCstation 2) is used to process the subject'sinput.2.7.
Data  CaptureWith respect to data collection, all human-computerinteractions are videotaped for subsequent analysis.The recording is a side-by-side split-screen image, cre-ated using a JVC KM-1200U special-effects generator.Videotaping is conducted unobtrusively with a remotegenlocked Panasonic WV-D5000 videocamera filmingthrough aone-way mirror.
Data capture includes aclose-up of the subject working at the LCD tablet, and a real-time record of interactions on the tablet, including thesubject's input, simulated feedback, and the graduallycompleted receipt.
This image is recorded internallyfrom the assistant's workstation, is processed througha Lyon Lamb scan converter, and then is merged usingthe special-effects generator and preserved on videotapefor later analysis.
In addition to being transmitted tothe simulation assistant, he subject's peech is recordedand stored in analog form on a timecoded videotape, andlater is transcribed for data analysis.
All handwritteninput is recorded on-line during real-time tablet interac-tions, which then is preserved on videotape and availablefor hardcopy printout.3.
RESEARCH DES IGNIn studies conducted at SRI to date, the experimen-tal design usually has been a completely-crossed fac-toriM with repeated measures, or a within-subjects de-sign.
Primary factors of interest have included: (1) com-munication modality (speech-only, pen-only, combinedpen/voice), and (2) presentation format (form-based,unconstrained).
In a typical study, each subject com-pletes a series of 12 tasks, two representing each of thesix main conditions.
The order of presenting conditionsis counterbalanced across ubjects.This generM design has been selected for its relative f-ficiency and power and, in particular, for its ability tocontrol linguistic variability due to individual differences.In brief, for example, this design permits comparing howthe same person completing the same tasks displays onetype of language and performance while speaking, butthen switches this language and performance when writ-ing.3734.
SAMPLE RESULTSThe variability inherent in people's language, whetherspoken or written, poses a substantial challenge to thesuccessful design of future NL systems.
One aspect ofthis research as been a comprehensive assessment ofthe linguistic variability evident in people's peech andwriting at various levels of processing, including acous-tic, lexical, syntactic, and semantic.
Full reports of theseresults are forthcoming \[11, 14\].
Special emphasis hasbeen placed on identifying problematic sources of vari-ability for system processing, as well as an explanation ofthe circumstances and apparent reasons for their occur-rence.
In connection with these analyses, one goal of thisresearclh program has been to identify specific interfacetechniques that may naturally channel users' languagein ways that reduce or eliminate difficult sources of vari-ability, so that more robust system processing can beachieved.
In particular, the impact of selecting a par-ticular input modality or presentation format is beingexamined, so that future system designers will have theoption of choosing a particular modality or format be-cause doing so will minimize xpected performance fail-ures of their planned NL systems.To briefly illustrate the research theme of reducing lin-guistic variability through selection of modality and for-mat, the results of an analysis related to syntactic ambi-guity are summarized.
Two indices of relative ambiguitywere measured for all phrasal and sentential utterancesthat people spoke to an unconstrained format (SNF),wrote in an unconstrained.format (WNF), spoke to aform (SF), or wrote in a form (WF).
Two different es-timates of parse ambiguity were computed to check forconvergence of results.
First, utterances produced underthe different simulation conditions were parsed using DI-ALOGIC \[9\], a robust text processing system developedat SRI that employs a broad coverage grammar.
Sec-ond, a summary was computed of the number of canoni-cal parses produced by DIALOGIC, through a mappingof each DIALOGIC parse to an emerging national stan-dard parse tree representation called PARSEVAL form 2\[2\].
The average number of DIALOGIC and PARSEVALparses generated per utterance for the different simula-tion conditions i summarized in Table 1, along with thepercentage of all utterances in each condition that werephrases or sentences and therefore appropriate for pars-ing.None of the subjects produced phrases or sentences whenwriting to a form, so none of the simple utterances fromPAFtSEVAL form is designed to reflect agreement among com-putational linguists imply on the major constituent bracketlngs,so  PARSEVAL identification of syntactic structures should tendto  represent the commonalities among many different systems.COND.
DIALOGICSNF 20.9WNF 10.7SFWF6.3PARSEVAL7.24.42.8UTTERANCESPARSED36%18%8%0%Table 1: Average number of DIALOGIC and PARSE-VAL parses per utterance as a function of modality andformat.this condition were appropriate for parsing.
The percent-age of phrase and sentential utterances available for pars-ing was greater for unconstrained than form-based input,and greater for spoken than written input.
Comparisonof both parse metrics for unconstrained and form-basedspeech revealed that using a form significantly reducedthe average number of parses per utterance, t (paired)= 2.50 (df = 5), p < .03, one-tailed (DIALOGIC), andt (paired) = 2.35 (df = 5), p < .04, one-tailed (PARSE-VAL).
When comparisons were made of the same sub-jects accomplishing the same tasks, the parse ambiguityof utterances in the unconstrained format averaged 232%higher for DIALOGIC and 157% higher for PARSEVALthan when communicating to a form.
However, com-parison of both parse metrics for speech and writing inan unconstrained format did not confirm that use of thewritten modality reduced the average number of parsesper utterance, t (paired) = 1.18 (df = 14), p > .10, one-tailed (DIALOGIC), and t < 1 (PARSEVAL).
That is,reliable reduction of parse ambiguity was obtained onlythrough manipulation of the presentation format.This pattern of results suggests that selection of pre-sentation format can have a substantial impact on theease of natural anguage processing, with direct implica-tions for improved system robustness.
In addition, post-experimental interviews indicated that participants pre-ferred form-based interactions over unconstrained onesby a factor of 2-to-1 in the present asks.
In particular,both the guidance and assurance of completeness a so-ciated with a form were considered esirable.
This indi-cates that the a priori assumption that any type of con-straint will be viewed by people as unacceptable or un-natural clearly is not always valid.
Furthermore, such apresumption may simply bias system development awayfrom good prospects for shorter-term gain.
The applica-tion of this kind of interface knowledge will be importantto the successful performance and commercialization ffuture natural anguage technology.3745.
FUTURE DIRECT IONSThe long-term goal of the present research method is tosupport a wide spectrum of advance mpirical studies oninteractive speech, pen, and pen/voice systems under dif-ferent circumstances of theoretical and commercial inter-est.
Future extensions of the present simulation researchare under way to examine issues relevant to multilin-gual and other multiparty applications \[13\].
In addition,a taxonomy of tasks is being developed in order to es-tablish a more analytical basis for distinguishing whenfindings do or do not generalize to qualitatively differentdomains, such that future work need not approach eachnew application as an unknown entity.
Efforts also areunder way to define the important dimensions of systeminteractivity, such as feedback characteristics and errorresolution strategies, as well as their impact on human-computer interaction.
Finally, in addition to providingproactive guidance for system design, a further aim ofthis simulation research is to yield better informationabout the range of preferred metrics for conducting per-formance assessments of future NL systems, includingtheir accuracy, efficiency, learnability, flexibility, ease ofuse, expressive power, and breadth of utility.6.
ACKNOWLEDGMENTSThanks to John Dowding, Dan Wilk, Martin Fong, andMichael Frank for invaluable programming assistanceduring the design and adaptation of the simulation.
Spe-cial thanks also to Dan Wilk and Martin Fong for actingas the simulation assistant during experimental studies,to Zak Zaidman for general experimental ssistance, andto John Bear, Jerry Hobbs, and Mabry Tyson for assist-ing with the preparation of DIALOGIC and PARSEVALparses.
Finally, thanks to the many volunteers who sogenerously offered their time to participate in this re-search.References1.
F. Andry, E. Bilange, F. Charpentier, K. Choukri,M.
Ponamal6, and S. Soudoplatoff.
Computerised sim-ulation tools for the design of an oral dialogue system.In Selected Publications, 1988-1990, SUNDIAL Project(Esprit P2218).
Commission of the European Commu-nities, 1990.2.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos, B. San-torini, and T. Strzalkowski.
A procedure for quantita-tively comparing the syntactic overage of English gram-mars.
In Proceedings of the DARPA Speech and NaturalLanguage Workshop, pages 306-311.
Morgan Kaufmann,Inc., February 1991.3.
R. Cole, L. Hirschman, L. Atlas, M. Beckman, A. Bier-man, M. Bush, J. Cohen, O. Garcia, B. Hanson,H.
Hermansky, S. Levinson, K. McKeown, N. Morgan,D.
Novick, M. Ostendorf, S. Oviatt, P. Price, H. Silver-man, J. Spitz, A. Waibel, C. Weinstein, S. Zahorain, andV.
Zue.
NSF workshop on spoken language understand-ing.
Technical Report CS/E 92-014, Oregon GraduateInstitute, September 1992.4.
H. D. Crane.
Writing and talking to computers.
Busi-ness Intelligence Program Report D91-1557, SRI Inter-national, Menlo Park, California, July 1991.5.
N. DahlbKck, A. JSnsson, and L. Ahrenberg.
Wiz-ard of Oz studies - -  why and how.
In L. Ahrenberg,N.
Dahlbgck, and A. JSnsson, editors, Proceedings fromthe Workshop on Empirical Models and Methodology forNatural Language Dialogue Systems, Trento, Italy, April1992.
Association for Computational Linguistics, ThirdConference on Applied Natural Language Processing.6.
N. M. Fraser and G. N. Gilbert.
Simulating speech sys-tems.
Computer Speech and Language, 5(1):81-99, 1991.7.
M. Guyomard and J. Siroux.
Experimentation i  thespecification of an oral dialogue.
In H. Niemann,M.
Lung, and G. Sagerer, editors, Recent Advances inSpeech Understanding and Dialog Systems.
Springer Vet-lag, Berlin, B. R. D., 1988.
NATO ASI Series, vol.
46.8.
C. T. Hemphill, J. J. Godfrey, and G. R. Doddington.The ATIS spoken language systems pilot corpus.
InProceedings o/the 8rd Darpa Workshop on Speech andNatural Language, pages 96-101, San Mateo, California,1990.
Morgan Kanfmann Publishers, Inc.9.
J. R. Hobbs, D. E. Appelt, J.
Bear, M. Tyson, andD.
Magerman.
Robust processing of real-world natural-language texts.
In P. S. Jacobs, editor, Text-Based In.telligent Systems: Current Research and Practice in In-\]ormation Extraction and Retrieval.
Lawrence ErlbaumAssociates, Publishers, Hillsdale, New Jersey, 1992.10.
A. F. Newell, J. L. Arnott, K. Carter, and G. Cruick-shank.
Listening typewriter simulation studies.
Inter-national Journal o/Man-machine Studies, 33(1):1-19,1990.11.
S. L. Oviatt.
Writing and talking to future interactivesystems, manuscript in preparation.12.
S. L. Oviatt.
Pen/voice: Complementary multimodalcommunication.
In Proceedings of Speech Tech '9~, pages238-241, New York, February 1992.13.
S. L. Oviatt.
Toward multimodal support for interpretedtelephone dialogues.
In M. M. Taylor, F. Nrel, andD.
G. Bouwhuis, editors, Structure o/Multimodal Di-alogue.
Elsevier Science Publishers B. V., Amsterdam,Netherlands, in press.14.
S. L. Oviatt and P. R. Cohen.
Interface techniques forenhancing robust performance of speech and handwrit-ing systems, manuscript in preparation.15.
S. L. Oviatt and P. R. Cohen.
Discourse structureand performance efficiency in interactive and noninterac-tive spoken modalities.
Computer Speech and Language,5(4):297-326, 1991a.16.
S. L. Oviatt, P. R. Cohen, M. W. Fong, and M. P. Frank.A rapid semi-automatic simulation technique for inves-tigating interactive speech and handwriting.
In Pro-ceedings of the 1992 International Conference on SpokenLanguage Processing, Banff, Canada, October 1992.17.
E. Zoltan-Ford.
How to get people to say and typewhat computers can understand.
International Journalof Man-Machine Studies, 34:527-547, 1991.375
