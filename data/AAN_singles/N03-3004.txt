Discriminating Among Word Senses Using McQuitty?s Similarity AnalysisAmruta PurandareDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812pura0010@d.umn.eduAbstractThis paper presents an unsupervised methodfor discriminating among the senses of a giventarget word based on the context in which it oc-curs.
Instances of a word that occur in similarcontexts are grouped together via McQuitty?sSimilarity Analysis, an agglomerative cluster-ing algorithm.
The context in which a targetword occurs is represented by surface lexicalfeatures such as unigrams, bigrams, and sec-ond order co-occurrences.
This paper summa-rizes our approach, and describes the results ofa preliminary evaluation we have carried outusing data from the SENSEVAL-2 English lexi-cal sample and the line corpus.1 IntroductionWord sense discrimination is the process of grouping orclustering together instances of written text that includesimilar usages of a given target word.
The instances thatform a particular cluster will have used the target word insimilar contexts and are therefore presumed to represent arelated meaning.
This view follows from the strong con-textual hypothesis of (Miller and Charles, 1991), whichstates that two words are semantically similar to the ex-tent that their contextual representations are similar.Discrimination is distinct from the more commonproblem of word sense disambiguation in at least tworespects.
First, the number of possible senses a targetword may have is usually not known in discrimination,while disambiguation is often viewed as a classificationproblem where a word is assigned to one of several pre?existing possible senses.
Second, discrimination utilizesfeatures and information that can be easily extracted fromraw corpora, whereas disambiguation often relies on su-pervised learning from sense?tagged training examples.However, the creation of sense?tagged data is time con-suming and results in a knowledge acquisition bottleneckthat severely limits the portability and scalability of tech-niques that employ it.
Discrimination does not sufferfrom this problem since there is no expensive preprocess-ing, nor are any external knowledge sources or manuallyannotated data required.The objective of this research is to extend previouswork in discrimination by (Pedersen and Bruce, 1997),who developed an approach using agglomerative cluster-ing.
Their work relied on McQuitty?s Similarity Anal-ysis using localized contextual features.
While the ap-proach in this paper also adopts McQuitty?s method, itis distinct in that it uses a larger number of features thatoccur both locally and globally in the instance being dis-criminated.
It also incorporates several ideas from laterwork by (Schu?tze, 1998), including the reliance on a sep-arate ?training?
corpus of raw text from which to iden-tify contextual features, and the use of second order co?occurrences (socs) as feature for discrimination.Our near term objectives for this research include de-termining to what extent different types of features im-pact the accuracy of unsupervised discrimination.
Weare also interested in assessing how different measuresof similarity such as the matching coefficient or the co-sine affect overall performance.
Once we have refinedour clustering techniques, we will incorporate them into amethod that automatically assigns sense labels to discov-ered clusters by using information from a machine read-able dictionary.This paper continues with a more detailed discussionof the previous work that forms the foundation for our re-search.
We then present an overview of the features usedto represent the context of a target word, and go on to de-scribe an experimental evaluation using the SENSEVAL-2lexical sample data.
We close with a discussion of our re-sults, a summary of related work, and an outline of ourfuture directions.Edmonton, May-June 2003Student Research Workshop , pp.
19-24Proceedings of HLT-NAACL 20032 Previous WorkThe work in this paper builds upon two previous ap-proaches to word sense discrimination, those of (Peder-sen and Bruce, 1997) and (Schu?tze, 1998).
Pedersen andBruce developed a method based on agglomerative clus-tering using McQuitty?s Similarity Analysis (McQuitty,1966), where the context of a target word is representedusing localized contextual features such as collocationsand part of speech tags that occur within one or two po-sitions of the target word.
Pedersen and Bruce demon-strated that despite it?s simplicity, McQuitty?s methodwas more accurate than Ward?s Method of MinimumVariance and the EM Algorithm for word sense discrimi-nation.McQuitty?s method starts by assuming that each in-stance is a separate cluster.
It merges together the pairof clusters that have the highest average similarity value.This continues until a specified number of clusters isfound, or until the similarity measure between every pairof clusters is less than a predefined cutoff.
Pedersen andBruce used a relatively small number of features, and em-ployed the matching coefficient as the similarity measure.Since we use a much larger number of features, we are ex-perimenting with the cosine measure, which scales simi-larity based on the number of non?zero features in eachinstance.By way of contrast, (Schu?tze, 1998) performs discrim-ination through the use of two different kinds of contextvectors.
The first is a word vector that is based on co?occurrence counts from a separate training corpus.
Eachword in this corpus is represented by a vector made up ofthe words it co-occurs with.
Then, each instance in a testor evaluation corpus is represented by a vector that is theaverage of all the vectors of all the words that make upthat instance.
The context in which a target word occursis thereby represented by second order co?occurrences,which are words which co?occur with the co?occurrencesof the target word.
Discrimination is carried out by clus-tering instance vectors using the EM Algorithm.The approach described in this paper proceeds as fol-lows.
Surface lexical features are identified in a trainingcorpus, which is made up of instances that consists of asentence containing a given target word, plus one or twosentences to the left or right of it.
Similarly defined in-stances in the test data are converted into vectors basedon this feature set, and a similarity matrix is constructedusing either the matching coefficient or the cosine.
There-after McQuitty?s Similarity Analysis is used to group to-gether instances based on the similarity of their context,and these are evaluated relative to a manually createdgold standard.3 Discrimination FeaturesWe carry out discrimination based on surface lexical fea-tures that require little or no preprocessing to identify.They consist of unigrams, bigrams, and second order co?occurrences.Unigrams are single words that occur in the same con-text as a target word.
Bag?of?words feature sets madeup of unigrams have had a long history of success in textclassification and word sense disambiguation (Mooney,1996), and we believe that despite creating quite a bit ofnoise can provide useful information for discrimination.Bigrams are pairs of words which occur together inthe same context as the target word.
They may includethe target word, or they may not.
We specify a win-dow of size five for bigrams, meaning that there may beup to three intervening words between the first and lastword that make up the bigram.
As such we are definingbigrams to be non?consecutive word sequences, whichcould also be considered a kind of co?occurrence feature.Bigrams have recently been shown to be very successfulfeatures in supervised word sense disambiguation (Peder-sen, 2001).
We believe this is because they capture mid-dle distance co?occurrence relations between words thatoccur in the context of the target word.Second order co?occurrences are words that occur withco-occurrences of the target word.
For example, supposethat line is the target word.
Given telephone line and tele-phone bill, bill would be considered a second order co?occurrence of line since it occurs with telephone, a firstorder co?occurrence of line.We define a window size of five in identifying sec-ond order co?occurrences, meaning that the first orderco?occurrence must be within five positions of the tar-get word, and the second order co?occurrence must bewithin five positions of the first order co?occurrence.
Weonly select those second order co?occurrences which co?occur more than once with the first order co-occurrenceswhich in turn co-occur more than once with the targetword within the specified window.We employ a stop list to remove high frequency non?content words from all of these features.
Unigrams thatare included in the stop list are not used as features.
A bi-gram is rejected if any word composing it is a stop word.Second order co?occurrences that are stop words or thosethat co?occur with stop words are excluded from the fea-ture set.After the features have been identified in the trainingdata, all of the instances in the test data are convertedinto binary feature vectors 	that repre-sent whether the features found in the training data haveoccurred in a particular test instance.
In order to clus-ter these instances, we measure the pair?wise similaritiesbetween them using matching and cosine coefficients.These values are formatted in a    similarity ma-trix such that cell   contains the similarity measurebetween instancesand.
This information serves as theinput to the clustering algorithm that groups together themost similar instances.4 Experimental MethodologyWe evaluate our method using two well known sources ofsense?tagged text.
In supervised learning sense?taggedtext is used to induce a classifier that is then applied toheld out test data.
However, our approach is purely un-supervised and we only use the sense tags to carry out anautomatic evaluation of the discovered clusters.
We fol-low Schu?tze?s strategy and use a ?training?
corpus onlyto extract features and ignore the sense tags.In particular, we use subsets of the line data (Leacocket al, 1993) and the English lexical sample data from theSENSEVAL-2 comparative exercise among word sensedisambiguation systems (Edmonds and Cotton, 2001).The line data contains 4,146 instances, where eachconsists of two to three sentences where a single oc-currence of line has been manually tagged with one ofsix possible senses.
We randomly select 100 instancesof each sense for test data, and 200 instances of eachsense for training.
This gives a total of 600 evaluationinstances, and 1200 training instances.
This is done totest the quality of our discrimination method when sensesare uniformly distributed and where no particular sense isdominant.The standard distribution of the SENSEVAL-2 dataconsists of 8,611 training instances and 4,328 test in-stances.
Each instance is made up of two to three sen-tences where a single target word has been manuallytagged with a sense (or senses) appropriate for that con-text.
There are 73 distinct target words found in thisdata; 29 nouns, 29 verbs, and 15 adjectives.
Most ofthese words have less than 100 test instances, and ap-proximately twice that number of training examples.
Ingeneral these are relatively small samples for an unsu-pervised approach, but we are developing techniques toincrease the amount of training data for this corpus auto-matically.We filter the SENSEVAL-2 data in three different waysto prepare it for processing and evaluation.
First, we in-sure that it only includes instances whose actual sense isamong the top five most frequent senses as observed inthe training data for that word.
We believe that this is anaggressive number of senses for a discrimination systemto attempt, considering that (Pedersen and Bruce, 1997)experimented with 2 and 3 senses, and (Schu?tze, 1998)made binary distinctions.Second, instances may have been assigned more thanone correct sense by the human annotator.
In order tosimplify the evaluation process, we eliminate all but themost frequent of multiple correct answers.Third, the SENSEVAL-2 data identifies target wordsthat are proper nouns.
We have elected not to use that in-formation and have removed these P tags from the data.After carrying out these preprocessing steps, the numberof training and test instances is 7,476 and 3,733.5 Evaluation TechniqueWe specify an upper limit on the number of senses thatMcQuitty?s algorithm can discover.
In these experimentsthis value is five for the SENSEVAL-2 data, and six forline.
In future experiments we will specify even highervalues, so that the algorithm is forced to create largernumber of clusters with very few instances when the ac-tual number of senses is smaller than the given cutoff.About a third of the words in the SENSEVAL-2 data havefewer than 5 senses, so even now the clustering algorithmis not always told the correct number of clusters it shouldfind.Once the clusters are formed, we access the actual cor-rect sense of each instance as found in the sense?taggedtext.
This information is never utilized prior to evalua-tion.
We use the sense?tagged text as a gold standard bywhich we can evaluate the discovered sense clusters.
Weassign sense tags to clusters such that the resulting accu-racy is maximized.For example, suppose that five clusters (C1 ?
C5) havebeen discovered for a word with 100 instances, and thatthe number of instances in each cluster is 25, 20, 10, 25,and 20.
Suppose that there are five actual senses (S1 ?S5), and the number of instances for each sense is 20, 20,20, 20, and 20.
Figure 1 shows the resulting confusionmatrix if the senses are assigned to clusters in numericorder.
After this assignment is made, the accuracy of theclustering can be determined by finding the sum of thediagonal, and dividing by the total number of instances,which in this case leads to accuracy of 10% (10/100).However, clearly there are assignments of senses to clus-ters that would lead to better results.Thus, the problem of assigning senses to clusters be-comes one of reordering the columns of the confusionsuch that the diagonal sum is maximized.
This corre-sponds to several well known problems, among them theAssignment Problem in Operations Research, and deter-mining the maximal matching of a bipartite graph.
Figure2 shows the maximally accurate assignment of senses toclusters, which leads to accuracy of 70% (70/100).During evaluation we assign one cluster to at most onesense, and vice versa.
When the number of discoveredclusters is the same as the number of senses, then thereis a 1 to 1 mapping between them.
When the numberof clusters is greater than the number of actual senses,then some clusters will be left unassigned.
And when theS1 S2 S3 S4 S5C1: 5 20 0 0 0 25C2: 10 0 5 0 5 20C3: 0 0 0 0 10 10C4: 0 0 15 5 5 25C5: 5 0 0 15 0 2020 20 20 20 20 100Figure 1: Numeric AssignmentS2 S1 S5 S3 S4C1: 20 5 0 0 0 25C2: 0 10 5 5 0 20C3: 0 0 10 0 0 10C4: 0 0 5 15 5 25C5: 0 5 0 0 15 2020 20 20 20 20 100Figure 2: Maximally Accurate Assignmentnumber of senses is greater than the number of clusters,some senses will not be assigned to any cluster.We determine the precision and recall based on thismaximally accurate assignment of sense tags to clusters.Precision is defined as the number of instances that areclustered correctly divided by the number of instancesclustered, while recall is the number of instances clus-tered correctly over the total number of instances.To be clear, we do not believe that word sense discrim-ination must be carried out relative to a pre?existing setof senses.
In fact, one of the great advantages of an un-supervised approach is that it need not be relative to anyparticular set of senses.
We carry out this evaluation tech-nique in order to improve the performance of our cluster-ing algorithm, which we will then apply on text wheresense?tagged data is not available.An alternative means of evaluation is to have a hu-man inspect the discovered clusters and judge them basedon the semantic coherence of the instances that populateeach cluster, but this is a more time consuming and sub-jective method of evaluation that we will pursue in future.6 Experimental ResultsFor each word in the SENSEVAL-2 data and line, we con-ducted various experiments, each of which uses a differ-ent combination of measure of similarity and features.Features are identified from the training data.
Our fea-tures consist of unigrams, bigrams, or second order co?occurrences.
We employ each of these three types of fea-tures separately, and we also create a mixed set that is theunion of all three sets.
We convert each evaluation in-stance into a feature vector, and then convert those into asimilarity matrix using either the matching coefficient orthe cosine.Table 1 contains overall precision and recall for thenouns, verbs, and adjectives overall in the SENSEVAL-2 data, and for line.
The SENSEVAL-2 values are de-rived from 29 nouns, 28 verbs, and 15 adjectives fromthe SENSEVAL-2 data.
The first column lists the part ofspeech, the second shows the feature, the third lists themeasure of similarity, the fourth and the fifth show pre-cision and recall, the sixth shows the percentage of themajority sense, and the final column shows the numberof words in the given part of speech that gave accuracygreater than the percentage of the majority sense.
Thevalue of the majority sense is derived from the sense?tagged data we use in evaluation, but this is not infor-mation that we would presume to have available duringactual clustering.Table 1: Experimental Resultspos feat meas prec rec maj   majnoun soc cos 0.49 0.48 0.57 6/29mat 0.54 0.52 0.57 7/29big cos 0.53 0.50 0.57 5/29mat 0.52 0.49 0.57 3/29uni cos 0.50 0.49 0.57 7/29mat 0.52 0.50 0.57 8/29mix cos 0.50 0.48 0.57 6/29mat 0.54 0.51 0.57 5/29verb soc cos 0.51 0.49 0.51 11/28mat 0.50 0.47 0.51 6/28big cos 0.54 0.45 0.51 5/28mat 0.53 0.43 0.51 5/28uni cos 0.42 0.41 0.51 13/28mat 0.43 0.41 0.51 9/28mix cos 0.43 0.41 0.51 12/28mat 0.42 0.41 0.51 7/28adj soc cos 0.59 0.54 0.64 1/15mat 0.59 0.55 0.64 1/15big cos 0.56 0.51 0.64 0/15mat 0.55 0.50 0.64 0/15uni cos 0.55 0.50 0.64 1/15mat 0.58 0.53 0.64 0/15mix cos 0.50 0.44 0.64 0/15mat 0.59 0.54 0.64 2/15line soc cos 0.25 0.25 0.17 1/1mat 0.23 0.23 0.17 1/1big cos 0.19 0.18 0.17 1/1mat 0.18 0.17 0.17 1/1uni cos 0.21 0.21 0.17 1/1mat 0.20 0.20 0.17 1/1mix cos 0.21 0.21 0.17 1/1mat 0.20 0.20 0.17 1/1For the SENSEVAL-2 data, on average the precisionand recall of the clustering as determined by our evalu-ation method is less than that of the majority sense, re-gardless of which features or measure are used.
How-ever, for nouns and verbs, a relatively significant num-ber of individual words have precision and recall valueshigher than that of the majority sense.
The adjectives arean exception to this, where words are very rarely dis-ambiguated more accurately than the percentage of themajority sense.
However, many of the adjectives havevery high frequency majority senses, which makes thisa difficult standard for an unsupervised method to reach.When examining the distribution of instances in clusters,we find that the algorithm tends to seek more balanceddistributions, and is unlikely to create a single long clus-ter that would result in high accuracy for a word whosetrue distribution of senses is heavily skewed towards asingle sense.We also note that the precision and recall of the clus-tering of the line data is generally better than that of themajority sense regardless of the features or measures em-ployed.
We believe there are two explanations for this.First, the number of training instances for the line data issignificantly higher (1200) than that of the SENSEVAL-2words, which typically have 100?200 training instancesper word.
The number and quality of features identifiedimproves considerably with an increase in the amount oftraining data.
Thus, the amount of training data avail-able for feature identification is critically important.
Webelieve that the SENSEVAL-2 data could be augmentedwith training data taken from the World Wide Web, andwe plan to pursue such approaches and see if our perfor-mance on the evaluation data improves as a result.At this point we do not observe a clear advantage tousing the cosine measure or matching coefficient.
Thissurprises us somewhat, as the number of features em-ployed is generally in the thousands, and the number ofnon?zero features can be quite large.
It would seem thatsimply counting the number of matching features wouldbe inferior to the cosine measure, but this is not the case.This remains an interesting issue that we will continue toexplore, with these and other measures of similarity.Finally, there is not a single feature that does best inall parts of speech.
Second order co?occurrences seem todo well with nouns and adjectives, while bigrams resultin accurate clusters for verbs.
We also note that secondorder co?occurrences do well with the line data.
As yetwe have drawn no conclusions from these results, but itis clearly a vital issue to investigate further.7 Related WorkUnsupervised approaches to word sense discriminationhave been somewhat less common in the computationallinguistics literature, at least when compared to super-vised approaches to word sense disambiguation.There is a body of work at the intersection of super-vised and unsupervised approaches, which involves usinga small amount of training data in order to automaticallycreate more training data, in effect bootstrapping from thesmall sample of sense?tagged data.
The best example ofsuch an approach is (Yarowsky, 1995), who proposes amethod that automatically identifies collocations that areindicative of the sense of a word, and uses those to itera-tively label more examples.While our focus has been on Pedersen and Bruce, andon Schu?tze, there has been other work in purely unsuper-vised approaches to word sense discrimination.
(Fukumoto and Suzuki, 1999) describe a method fordiscriminating among verb senses based on determiningwhich nouns co?occur with the target verb.
Collocationsare extracted which are indicative of the sense of a verbbased on a similarity measure they derive.
(Pantel and Lin, 2002) introduce a method known asCommittee Based Clustering that discovers word senses.The words in the corpus are clustered based on their dis-tributional similarity under the assumption that semanti-cally similar words will have similar distributional char-acteristics.
In particular, they use Pointwise Mutual In-formation to find how close a word is to its context andthen determine how similar the contexts are using the co-sine coefficient.8 Future WorkOur long term goal is to develop a method that will as-sign sense labels to clusters using information found inmachine readable dictionaries.
This is an important prob-lem because clusters as found in discrimination have nosense tag or label attached to them.
While there are cer-tainly applications for unlabeled sense clusters, havingsome indication of the sense of the cluster would bringdiscrimination and disambiguation closer together.
Wewill treat glosses as found in a dictionary as vectors thatwe project into the same space that is populated by in-stances as we have already described.
A cluster could beassigned the sense of the gloss whose vector it was mostclosely located to.This idea is based loosely on work by (Niwa and Nitta,1994), who compare word co?occurrence vectors derivedfrom large corpora of text with co?occurrence vectorsbased on the definitions or glosses of words in a ma-chine readable dictionary.
A co?occurrence vector indi-cates how often words are used with each other in a largecorpora or in dictionary definitions.
These vectors can beprojected into a high dimensional space and used to mea-sure the distance between concepts or words.
Niwa andNitta show that while the co?occurrence data from a dic-tionary has different characteristics that a co?occurrencevector derived from a corpus, both provide useful infor-mation about how to categorize a word based on its mean-ing.
Our future work will mostly attempt to merge clus-ters found from corpora with meanings in dictionarieswhere presentation techniques like co?occurrence vectorscould be useful.There are a number of smaller issues that we are inves-tigating.
We are also exploring a number of other typesof features, as well as varying the formulation of the fea-tures we are currently using.
We have already conducteda number of experiments that vary the window sizes em-ployed with bigrams and second order co?occurrences,and will continue in this vein.
We are also consideringthe use of other measures of similarity beyond the match-ing coefficient and the cosine.
We do not stem the train-ing data prior to feature identification, nor do or employfuzzy matching techniques when converting evaluationinstances into feature vectors.
However, we believe bothmight lead to increased numbers of useful features beingidentified.9 ConclusionsWe have presented an unsupervised method of wordsense discrimination that employs a range of surface lexi-cal features, and relies on similarity based clustering.
Wehave evaluated this method in an extensive experimentthat shows that our method can achieve precision and re-call higher than the majority sense of a word for a reason-ably large number of cases.
We believe that increases inthe amount of training data employed in this method willyield to considerably improved results, and have outlinedour plans to address this and several other issues.10 AcknowledgmentsThis research is being conducted as a part of my M.S.
the-sis in Computer Science at the University of Minnesota,Duluth.
I am grateful to my thesis advisor, Dr. Ted Ped-ersen, for his help and guidance.I have been fully supported by a National ScienceFoundation Faculty Early CAREER Development Award(#0092784) during the 2002?2003 academic year.I would like to thank the Director of Computer ScienceGraduate Studies, Dr. Carolyn Crouch, and the AssociateVice Chancellor, Dr. Stephen Hedman, for their supportin providing a travel award to attend the Student ResearchWorkshop at HLT-NAACL 2003.ReferencesP.
Edmonds and S. Cotton, editors.
2001.
Proceedingsof the Senseval?2 Workshop.
Association for Compu-tational Linguistics, Toulouse, France.F.
Fukumoto and Y. Suzuki.
1999.
Word sense disam-biguation in untagged text based on term weight learn-ing.
In Proceedings of the Ninth Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 209?216, Bergen.C.
Leacock, G. Towell, and E. Voorhees.
1993.
Corpus-based statistical sense resolution.
In Proceedings ofthe ARPA Workshop on Human Language Technology,pages 260?265, March.L.
McQuitty.
1966.
Similarity analysis by reciprocalpairs for discrete and continuous data.
Educationaland Psychological Measurement, 26:825?831.G.A.
Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.R.
Mooney.
1996.
Comparative experiments on disam-biguating word senses: An illustration of the role ofbias in machine learning.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 82?91, May.Y.
Niwa and Y. Nitta.
1994.
Co-occurrence vectors fromcorpora versus distance vectors from dictionaries.
InProceedings of the Fifteenth International Conferenceon Computational Linguistics, pages 304?309, Kyoto,Japan.P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In Proceedings of ACM SIGKDD Confer-ence on Knowledge Discovery and Data Mining-2002.T.
Pedersen and R. Bruce.
1997.
Distinguishing wordsenses in untagged text.
In Proceedings of the Sec-ond Conference on Empirical Methods in Natural Lan-guage Processing, pages 197?207, Providence, RI,August.T.
Pedersen.
2001.
A decision tree of bigrams is an ac-curate predictor of word sense.
In Proceedings of theSecond Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics,pages 79?86, Pittsburgh, July.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting of the Associationfor Computational Linguistics, pages 189?196, Cam-bridge, MA.
