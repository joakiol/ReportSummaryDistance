Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1415?1425,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSemantic Parsing via ParaphrasingJonathan BerantStanford Universityjoberant@stanford.eduPercy LiangStanford Universitypliang@cs.stanford.eduAbstractA central challenge in semantic parsing ishandling the myriad ways in which knowl-edge base predicates can be expressed.Traditionally, semantic parsers are trainedprimarily from text paired with knowledgebase information.
Our goal is to exploitthe much larger amounts of raw text nottied to any knowledge base.
In this pa-per, we turn semantic parsing on its head.Given an input utterance, we first use asimple method to deterministically gener-ate a set of candidate logical forms witha canonical realization in natural languagefor each.
Then, we use a paraphrase modelto choose the realization that best para-phrases the input, and output the corre-sponding logical form.
We present twosimple paraphrase models, an associationmodel and a vector space model, and trainthem jointly from question-answer pairs.Our system PARASEMPRE improves state-of-the-art accuracies on two recently re-leased question-answering datasets.1 IntroductionWe consider the semantic parsing problem of map-ping natural language utterances into logical formsto be executed on a knowledge base (KB) (Zelleand Mooney, 1996; Zettlemoyer and Collins,2005; Wong and Mooney, 2007; Kwiatkowskiet al, 2010).
Scaling semantic parsers to largeknowledge bases has attracted substantial atten-tion recently (Cai and Yates, 2013; Berant et al,2013; Kwiatkowski et al, 2013), since it drivesapplications such as question answering (QA) andinformation extraction (IE).Semantic parsers need to somehow associatenatural language phrases with logical predicates,e.g., they must learn that the constructions ?WhatWhat party did Clay establish?paraphrase modelWhat political party founded by Henry Clay?
... What event involved the people Henry Clay?Type.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClayWhig PartyFigure 1: Semantic parsing via paraphrasing: For eachcandidate logical form (in red), we generate canonical utter-ances (in purple).
The model is trained to paraphrase the in-put utterance (in green) into the canonical utterances associ-ated with the correct denotation (in blue).does X do for a living?
?, ?What is X?s profes-sion?
?, and ?Who is X?
?, should all map to thelogical predicate Profession.
To learn these map-pings, traditional semantic parsers use data whichpairs natural language with the KB.
However, thisleaves untapped a vast amount of text not relatedto the KB.
For instance, the utterances ?Where isACL in 2014??
and ?What is the location of ACL2014??
cannot be used in traditional semanticparsing methods, since the KB does not containan entity ACL2014, but this pair clearly containsvaluable linguistic information.
As another refer-ence point, out of 500,000 relations extracted bythe ReVerb Open IE system (Fader et al, 2011),only about 10,000 can be aligned to Freebase (Be-rant et al, 2013).In this paper, we present a novel approach forsemantic parsing based on paraphrasing that canexploit large amounts of text not covered by theKB (Figure 1).
Our approach targets factoid ques-tions with a modest amount of compositionality.Given an input utterance, we first use a simple de-terministic procedure to construct a manageableset of candidate logical forms (ideally, we wouldgenerate canonical utterances for all possible logi-cal forms, but this is intractable).
Next, we heuris-1415utteranceunderspecifiedlogicalformcanonicalutterancelogicalformontologymatchingparaphrasedirect(traditional)(Kwiatkowski et al 2013)(this work)Figure 2: The main challenge in semantic parsing is cop-ing with the mismatch between language and the KB.
(a)Traditionally, semantic parsing maps utterances directly tological forms.
(b) Kwiatkowski et al (2013) map the utter-ance to an underspecified logical form, and perform ontologymatching to handle the mismatch.
(c) We approach the prob-lem in the other direction, generating canonical utterances forlogical forms, and use paraphrase models to handle the mis-match.tically generate canonical utterances for each log-ical form based on the text descriptions of predi-cates from the KB.
Finally, we choose the canoni-cal utterance that best paraphrases the input utter-ance, and thereby the logical form that generatedit.
We use two complementary paraphrase mod-els: an association model based on aligned phrasepairs extracted from a monolingual parallel cor-pus, and a vector space model, which representseach utterance as a vector and learns a similarityscore between them.
The entire system is trainedjointly from question-answer pairs only.Our work relates to recent lines of researchin semantic parsing and question answering.Kwiatkowski et al (2013) first maps utterances toa domain-independent intermediate logical form,and then performs ontology matching to producethe final logical form.
In some sense, we ap-proach the problem from the opposite end, usingan intermediate utterance, which allows us to em-ploy paraphrasing methods (Figure 2).
Fader etal.
(2013) presented a QA system that maps ques-tions onto simple queries against Open IE extrac-tions, by learning paraphrases from a large mono-lingual parallel corpus, and performing a singleparaphrasing step.
We adopt the idea of usingparaphrasing for QA, but suggest a more generalparaphrase model and work against a formal KB(Freebase).We apply our semantic parser on two datasets:WEBQUESTIONS (Berant et al, 2013), whichcontains 5,810 question-answer pairs withcommon questions asked by web users; andFREE917 (Cai and Yates, 2013), which has917 questions manually authored by annota-tors.
On WEBQUESTIONS, we obtain a relativeimprovement of 12% in accuracy over thestate-of-the-art, and on FREE917 we match thecurrent best performing system.
The sourcecode of our system PARASEMPRE is releasedat http://www-nlp.stanford.edu/software/sempre/.2 SetupOur task is as follows: Given (i) a knowledgebase K, and (ii) a training set of question-answerpairs {(xi, yi)}ni=1, output a semantic parser thatmaps new questions x to answers y via latent log-ical forms z.
Let E denote a set of entities (e.g.,BillGates), and let P denote a set of properties(e.g., PlaceOfBirth).
A knowledge base K is aset of assertions (e1, p, e2) ?
E ?
P ?
E (e.g.,(BillGates, PlaceOfBirth, Seattle)).
We usethe Freebase KB (Google, 2013), which has 41Mentities, 19K properties, and 596M assertions.To query the KB, we use a logical languagecalled simple ?-DCS.
In simple ?-DCS, anentity (e.g., Seattle) is a unary predicate(i.e., a subset of E) denoting a singleton setcontaining that entity.
A property (which is abinary predicate) can be joined with a unarypredicate; e.g., Founded.Microsoft denotesthe entities that are Microsoft founders.
InPlaceOfBirth.Seattle u Founded.Microsoft,an intersection operator allows us to denotethe set of Seattle-born Microsoft founders.A reverse operator reverses the order of ar-guments: R[PlaceOfBirth].BillGatesdenotes Bill Gates?s birthplace (in con-trast to PlaceOfBirth.Seattle).
Lastly,count(Founded.Microsoft) denotes set cardinal-ity, in this case, the number of Microsoft founders.The denotation of a logical form z with respect toa KB K is given by JzKK.
For a formal descriptionof simple ?-DCS, see Liang (2013) and Berant etal.
(2013).3 Model overviewWe now present the general framework for seman-tic parsing via paraphrasing, including the modeland the learning algorithm.
In Sections 4 and 5,we provide the details of our implementation.Canonical utterance construction Given an ut-terance x and the KB, we construct a set of candi-1416date logical forms Zx, and then for each z ?
Zxgenerate a small set of canonical natural languageutterances Cz.
Our goal at this point is only to gen-erate a manageable set of logical forms containingthe correct one, and then generate an appropriatecanonical utterance from it.
This strategy is feasi-ble in factoid QA where compositionality is low,and so the size of Zxis limited (Section 4).Paraphrasing We score the canonical utter-ances in Czwith respect to the input utterance xusing a paraphrase model, which offers two ad-vantages.
First, the paraphrase model is decoupledfrom the KB, so we can train it from large text cor-pora.
Second, natural language utterances often donot express predicates explicitly, e.g., the question?What is Italy?s money??
expresses the binarypredicate CurrencyOf with a possessive construc-tion.
Paraphrasing methods are well-suited forhandling such text-to-text gaps.
Our frameworkaccommodates any paraphrasing method, and inthis paper we propose an association model thatlearns to associate natural language phrases thatco-occur frequently in a monolingual parallel cor-pus, combined with a vector space model, whichlearns to score the similarity between vector rep-resentations of natural language utterances (Sec-tion 5).Model We define a discriminative log-linearmodel that places a probability distribution overpairs of logical forms and canonical utterances(c, z), given an utterance x:p?
(c, z | x) =exp{?
(x, c, z)>?}?z??Zx,c??Czexp{?
(x, c?, z?)>?
},where ?
?
Rbis the vector of parameters to belearned, and ?
(x, c, z) is a feature vector extractedfrom the input utterance x, the canonical utterancec, and the logical form z.
Note that the candidateset of logical forms Zxand canonical utterancesCxare constructed during the canonical utteranceconstruction phase.The model score decomposes into two terms:?
(x, c, z)>?
= ?pr(x, c)>?pr+ ?lf(x, z)>?lf,where the parameters ?prdefine the paraphrasemodel (Section 5), which is based on features ex-tracted from text only (the input and canonical ut-terance).
The parameters ?lfcorrespond to seman-tic parsing features based on the logical form andinput utterance, and are briefly described in thissection.Many existing paraphrase models introduce la-tent variables to describe the derivation of c fromx, e.g., with transformations (Heilman and Smith,2010; Stern and Dagan, 2011) or alignments(Haghighi et al, 2005; Das and Smith, 2009;Chang et al, 2010).
However, we opt for a sim-pler paraphrase model without latent variables inthe interest of efficiency.Logical form features The parameters ?lfcorre-spond to the following features adopted from Be-rant et al (2013).
For a logical form z, we extractthe size of its denotation JzKK.
We also add all bi-nary predicates in z as features.
Moreover, we ex-tract a popularity feature for predicates based onthe number of instances they have in K. For Free-base entities, we extract a popularity feature basedon the entity frequency in an entity linked subsetof Reverb (Lin et al, 2012).
Lastly, Freebase for-mulas have types (see Section 4), and we conjointhe type of z with the first word of x, to capture thecorrelation between a word (e.g., ?where?)
withthe Freebase type (e.g., Location).Learning As our training data consists ofquestion-answer pairs (xi, yi), we maximize thelog-likelihood of the correct answer.
The proba-bility of an answer y is obtained by marginaliz-ing over canonical utterances c and logical formsz whose denotation is y.
Formally, our objectivefunction O(?)
is as follows:O(?)
=n?i=1log p?
(yi| xi)?
????1,p?
(y | x) =?z?Zx:y=JzKK?c?Czp?
(c, z | x).The strength ?
of the L1regularizer is set basedon cross-validation.
We optimize the objective byinitializing the parameters ?
to zero and runningAdaGrad (Duchi et al, 2010).
We approximatethe set of pairs of logical forms and canonical ut-terances with a beam of size 2,000.4 Canonical utterance constructionWe construct canonical utterances in two steps.Given an input utterance x, we first construct aset of logical forms Zx, and then generate canon-ical utterances from each z ?
Zx.
Both steps areperformed with a small and simple set of deter-ministic rules, which suffices for our datasets, as1417they consist of factoid questions with a modestamount of compositional structure.
We describethese rules below for completeness.
Due to its so-porific effect though, we advise the reader to skimit quickly.Candidate logical forms We consider logicalforms defined by a set of templates, summarizedin Table 1.
The basic template is a join of a bi-nary and an entity, where a binary can either beone property p.e (#1 in the table) or two proper-ties p1.p2.e (#2).
To handle cases of events in-volving multiple arguments (e.g., ?Who did BradPitt play in Troy??
), we introduce the templatep.
(p1.e1u p2.e2) (#3), where the main event ismodified by more than one entity.
Logical formscan be further modified by a unary ?filter?, e.g.,the answer to ?What composers spoke French?
?is a set of composers, i.e., a subset of all people(#4).
Lastly, we handle aggregation formulas forutterances such as ?How many teams are in theNCAA??
(#5).To construct candidate logical forms Zxfor agiven utterance x, our strategy is to find an en-tity in x and grow the logical form from that en-tity.
As we show later, this procedure actually pro-duces a set with better coverage than construct-ing logical forms recursively from spans of x, asis done in traditional semantic parsing.
Specifi-cally, for every span of x, we take at most 10 en-tities whose Freebase descriptions approximatelymatch the span.
Then, we join each entity e withall type-compatible1binaries b, and add these log-ical forms to Zx(#1 and #2).To construct logical forms with multiple en-tities (#3) we do the following: For any logicalform z = p.p1.e1, where p1has type signa-ture (t1, ?
), we look for other entities e2thatwere matched in x.
Then, we add the logicalform p.(p1.e1u p2.e2), if there exists a binaryp2with a compatible type signature (t1, t2),where t2is one of e2?s types.
For example, forthe logical form Character.Actor.BradPitt,if we match the entity Troy in x, we obtainCharacter.
(Actor.BradPitt u Film.Troy).We further modify logical forms by intersectingwith a unary filter (#4): given a formula z withsome Freebase type (e.g., People), we look atall Freebase sub-types t (e.g., Composer), and1Entities in Freebase are associated with a set of types,and properties have a type signature (t1, t2) We use thesetypes to compute an expected type t for any logical form z.check whether one of their Freebase descriptions(e.g., ?composer?)
appears in x.
If so, weadd the formula Type.t u z to Zx.
Finally, wecheck whether x is an aggregation formula byidentifying whether it starts with phrases such as?how many?
or ?number of?
(#5).On WEBQUESTIONS, this results in 645 for-mulas per utterance on average.
Clearly, we canincrease the expressivity of this step by expand-ing the template set.
For example, we could han-dle superlative utterances (?What NBA player istallest??)
by adding a template with an argmaxoperator.Utterance generation While mapping generallanguage utterances to logical forms is hard, weobserve that it is much easier to generate a canoni-cal natural language utterances of our choice givena logical form.
Table 2 summarizes the rules usedto generate canonical utterances from the templatep.e.
Questions begin with a question word, are fol-lowed by the Freebase description of the expectedanswer type (d(t)), and followed by Freebase de-scriptions of the entity (d(e)) and binary (d(p)).To fill in auxiliary verbs, determiners, and prepo-sitions, we parse the description d(p) into one ofNP, VP, PP, or NP VP.
This determines the gen-eration rule to be used.Each Freebase property p has an explicit prop-erty p?equivalent to the reverse R[p] (e.g.,ContainedBy and R[Contains]).
For each logicalform z, we also generate using equivalent logicalforms where p is replaced with R[p?].
Reversedformulas have different generation rules, since en-tities in these formulas are in the subject positionrather than object position.We generate the description d(t) from the Free-base description of the type of z (this handles #4).For the template p1.p2.e (#2), we have a similarset of rules, which depends on the syntax of d(p1)and d(p2) and is omitted for brevity.
The tem-plate p.(p1.e1u p2.e2) (#3) is generated by ap-pending the prepositional phrase in d(e2), e.g,?What character is the character of Brad Pitt inTroy??.
Lastly, we choose the question phrase?How many?
for aggregation formulas (#5), and?What?
for all other formulas.We also generate canonical utterances usingan alignment lexicon, released by Berant et al(2013), which maps text phrases to Freebase bi-nary predicates.
For a binary predicate b mappedfrom text phrase d(b), we generate the utterance1418# Template Example Question1 p.e Directed.TopGun Who directed Top Gun?2 p1.p2.e Employment.EmployerOf.SteveBalmer Where does Steve Balmer work?3 p.(p1.e1u p2.e2) Character.
(Actor.BradPitt u Film.Troy) Who did Brad Pitt play in Troy?4 Type.t u z Type.Composer u SpeakerOf.French What composers spoke French?5 count(z) count(BoatDesigner.NatHerreshoff) How many ships were designed byNat Herreshoff?Table 1: Logical form templates, where p, p1, p2are Freebase properties, e, e1, e2are Freebase entities, t is a Freebase type,and z is a logical form.d(p) Categ.
Rule Examplep.e NP WH d(t) has d(e) as NP ?
What election contest has George Bush as winner?VP WH d(t) (AUX) VP d(e) ?
What radio station serves area New-York?PP WH d(t) PP d(e) ?
What beer from region Argentina?NP VP WH d(t) VP the NP d(e) ?
What mass transportation system served the area Berlin?R(p).e NP WH d(t) is the NP of d(e) ?
What location is the place of birth of Elvis Presley?VP WH d(t) AUX d(e) VP ?
What film is Brazil featured in?PP WH d(t) d(e) PP ?
What destination Spanish steps near travel destination?NP VP WH NP is VP by d(e) ?
What structure is designed by Herod?Table 2: Generation rules for templates of the form p.e and R[p].e based on the syntactic category of the property description.Freebase descriptions for the type, entity, and property are denoted by d(t), d(e) and d(p) respectively.
The surface form of theauxiliary AUX is determined by the POS tag of the verb inside the VP tree.WH d(t) d(b) d(e) ?.
On the WEBQUESTIONSdataset, we generate an average of 1,423 canonicalutterances c per input utterance x.
In Section 6,we show that an even simpler method of gener-ating canonical utterances by concatenating Free-base descriptions hurts accuracy by only a modestamount.5 ParaphrasingOnce the candidate set of logical forms paired withcanonical utterances is constructed, our problemis reduced to scoring pairs (c, z) based on a para-phrase model.
The NLP paraphrase literature isvast and ranges from simple methods employingsurface features (Wan et al, 2006), through vec-tor space models (Socher et al, 2011), to latentvariable models (Das and Smith, 2009; Wang andManning, 2010; Stern and Dagan, 2011).In this paper, we focus on two paraphrase mod-els that emphasize simplicity and efficiency.
Thisis important since for each question-answer pair,we consider thousands of canonical utterances aspotential paraphrases.
In contrast, traditional para-phrase detection (Dolan et al, 2004) and Recog-nizing Textual Entailment (RTE) tasks (Dagan etal., 2013) consider examples consisting of only asingle pair of candidate paraphrases.Our paraphrase model decomposes into an as-sociation model and a vector space model:?pr(x, c)>?pr= ?as(x, c)>?as+ ?vs(x, c)>?vs.x : What type of music did Richard Wagner playc : What is the musical genres of Richard WagnerFigure 3: Token associations extracted for a paraphrasepair.
Blue and dashed (red and solid) indicate positive (neg-ative) score.
Line width is proportional to the absolute valueof the score.5.1 Association modelThe goal of the association model is to deter-mine whether x and c contain phrases that arelikely to be paraphrases.
Given an utterance x =?x0, x1, .., xn?1?, we denote by xi:jthe span fromtoken i to token j.
For each pair of utterances(x, c), we go through all spans of x and c andidentify a set of pairs of potential paraphrases(xi:j, ci?:j?
), which we call associations.
(We willdescribe how associations are identified shortly.
)We then define features on each association; theweighted combination of these features yields ascore.
In this light, associations can be viewedas soft paraphrase rules.
Figure 3 presents exam-ples of associations extracted from a paraphrasepair and visualizes the learned scores.
We can seethat our model learns a positive score for associ-ating ?type?
with ?genres?, and a negative scorefor associating ?is?
with ?play?.We define associations in x and c primarily bylooking up phrase pairs in a phrase table con-structed using the PARALEX corpus (Fader et al,2013).
PARALEX is a large monolingual parallel1419Category DescriptionAssoc.
lemma(xi:j) ?
lemma(ci?:j?
)pos(xi:j) ?
pos(ci?:j?
)lemma(xi:j) = lemma(ci?:j?
)?pos(xi:j) = pos(ci?:j?
)?lemma(xi:j) and lemma(ci?:j?)
are synonyms?lemma(xi:j) and lemma(ci?:j?)
are derivations?Deletions Deleted lemma and POS tagTable 3: Full feature set in the association model.
xi:jandci?
:j?denote spans from x and c. pos(xi:j) and lemma(xi:j)denote the POS tag and lemma sequence of xi:j.corpora, containing 18 million pairs of questionparaphrases from wikianswers.com, whichwere tagged as having the same meaning by users.PARALEX is suitable for our needs since it fo-cuses on question paraphrases.
For example, thephrase ?do for a living?
occurs mostly in ques-tions, and we can extract associations for thisphrase from PARALEX.
Paraphrase pairs in PAR-ALEX are word-aligned using standard machinetranslation methods.
We use the word alignmentsto construct a phrase table by applying the con-sistent phrase pair heuristic (Och and Ney, 2004)to all 5-grams.
This results in a phrase table withapproximately 1.3 million phrase pairs.
We let Adenote this set of mined candidate associations.For a pair (x, c), we also consider as candidateassociations the set B (represented implicitly),which contains token pairs (xi, ci?)
such that xiand ci?share the same lemma, the same POS tag,or are linked through a derivation link on WordNet(Fellbaum, 1998).
This allows us to learn para-phrases for words that appear in our datasets butare not covered by the phrase table, and to han-dle nominalizations for phrase pairs such as ?Whodesigned the game of life??
and ?What game de-signer is the designer of the game of life?
?.Our model goes over all possible spans of xand c and constructs all possible associations fromA and B.
This results in many poor associations(e.g., ?play?
and ?the?
), but as illustrated in Fig-ure 3, we learn weights that discriminate goodfrom bad associations.
Table 3 specifies the fullset of features.
Note that unlike standard para-phrase detection and RTE systems, we use lexi-calized features, firing approximately 400,000 fea-tures on WEBQUESTIONS.
By extracting POSfeatures, we obtain soft syntactic rules, e.g., thefeature ?JJ N ?
N?
indicates that omitting ad-jectives before nouns is possible.
Once associa-tions are constructed, we mark tokens in x and cthat were not part of any association, and extractdeletion features for their lemmas and POS tags.Thus, we learn that deleting pronouns is accept-able, while deleting nouns is not.To summarize, the association model linksphrases of two utterances in multiple overlappingways.
During training, the model learns whichassociations are characteristic of paraphrases andwhich are not.5.2 Vector space modelThe association model relies on having a good setof candidate associations, but mining associationssuffers from coverage issues.
We now introducea vector space (VS) model, which assigns a vec-tor representation for each utterance, and learns ascoring function that ranks paraphrase candidates.We start by constructing vector representationsof words.
We run the WORD2VEC tool (Mikolov etal., 2013) on lower-cased Wikipedia text (1.59 bil-lion tokens), using the CBOW model with a win-dow of 5 and hierarchical softmax.
We also ex-periment with publicly released word embeddings(Huang et al, 2012), which were trained usingboth local and global context.
Both result in k-dimensional vectors (k = 50).
Next, we constructa vector vx?
Rkfor each utterance x by simplyaveraging the vectors of all content words (nouns,verbs, and adjectives) in x.We can now estimate a paraphrase score for twoutterances x and c via a weighted combination ofthe components of the vector representations:v>xWvc=k?i,j=1wijvx,ivc,jwhere W ?
Rk?kis a parameter matrix.
In termsof our earlier notation, we have ?vs= vec(W ) and?vs(x, c) = vec(vxv>c), where vec(?)
unrolls a ma-trix into a vector.
In Section 6, we experiment withW equal to the identity matrix, constraining W tobe diagonal, and learning a full W matrix.The VS model can identify correct paraphrasesin cases where it is hard to directly associatephrases from x and c. For example, the answerto ?Where is made Kia car??
(from WEBQUES-TIONS), is given by the canonical utterance ?Whatcity is Kia motors a headquarters of??.
The as-sociation model does not associate ?made?
and?headquarters?, but the VS model is able to de-termine that these utterances are semantically re-lated.
In other cases, the VS model cannot distin-guish correct paraphrases from incorrect ones.
For1420Dataset # examples # word typesFREE917 917 2,036WEBQUESTIONS 5,810 4,525Table 4: Statistics on WEBQUESTIONS and FREE917.example, the association model identifies that theparaphrase for ?What type of music did RichardWagner Play??
is ?What is the musical genresof Richard Wagner?
?, by relating phrases such as?type of music?
and ?musical genres?.
The VSmodel ranks the canonical utterance ?What com-position has Richard Wagner as lyricist??
higher,as this utterance is also in the music domain.
Thus,we combine the two models to benefit from theircomplementary nature.In summary, while the association model alignsparticular phrases to one another, the vector spacemodel provides a soft vector-based representationfor utterances.6 Empirical evaluationIn this section, we evaluate our system on WE-BQUESTIONS and FREE917.
After describing thesetup (Section 6.1), we present our main empiricalresults and analyze the components of the system(Section 6.2).6.1 SetupWe use the WEBQUESTIONS dataset (Berant etal., 2013), which contains 5,810 question-answerpairs.
This dataset was created by crawlingquestions through the Google Suggest API, andthen obtaining answers using Amazon Mechani-cal Turk.
We use the original train-test split, anddivide the training set into 3 random 80%?20%splits for development.
This dataset is character-ized by questions that are commonly asked on theweb (and are not necessarily grammatical), suchas ?What character did Natalie Portman play inStar Wars??
and ?What kind of money to take toBahamas?
?.The FREE917 dataset contains 917 questions,authored by two annotators and annotated withlogical forms.
This dataset contains questions onrarer topics (for example, ?What is the enginein a 2010 Ferrari California??
and ?What wasthe cover price of the X-men Issue 1??
), but thephrasing of questions tends to be more rigid com-pared to WEBQUESTIONS.
Table 4 provides somestatistics on the two datasets.
Following Cai andYates (2013), we hold out 30% of the data for thefinal test, and perform 3 random 80%-20% splitsof the training set for development.
Since we trainfrom question-answer pairs, we collect answers byexecuting the gold logical forms against Freebase.We execute ?-DCS queries by converting theminto SPARQL and executing them against a copyof Freebase using the Virtuoso database engine.We evaluate our system with accuracy, that is, theproportion of questions we answer correctly.
Werun all questions through the Stanford CoreNLPpipeline (Toutanova and Manning, 2003; Finkel etal., 2005; Klein and Manning, 2003).We tuned the L1regularization strength, devel-oped features, and ran analysis experiments on thedevelopment set (averaging across random splits).On WEBQUESTIONS, without L1regularization,the number of non-zero features was 360K; L1regularization brings it down to 17K.6.2 ResultsWe compare our system to Cai and Yates (2013)(CY13), Berant et al (2013) (BCFL13), andKwiatkowski et al (2013) (KCAZ13).
ForBCFL13, we obtained results using the SEMPREpackage2and running Berant et al (2013)?s sys-tem on the datasets.Table 5 presents results on the test set.
Weachieve a substantial relative improvement of 12%in accuracy on WEBQUESTIONS, and match thebest results on FREE917.
Interestingly, our systemgets an oracle accuracy of 63% on WEBQUES-TIONS compared to 48% obtained by BCFL13,where the oracle accuracy is the fraction of ques-tions for which at least one logical form in thecandidate set produced by the system is correct.This demonstrates that our method for construct-ing candidate logical forms is reasonable.
To fur-ther examine this, we ran BCFL13 on the devel-opment set, allowing it to use only predicates fromlogical forms suggested by our logical form con-struction step.
This improved oracle accuracy onthe development set to 64.5%, but accuracy was32.2%.
This shows that the improvement in accu-racy should not be attributed only to better logicalform generation, but also to the paraphrase model.We now perform more extensive analysis of oursystem?s components and compare it to variousbaselines.Component ablation We ablate the associationmodel, the VS model, and the entire paraphrase2http://www-nlp.stanford.edu/software/sempre/1421FREE917 WEBQUESTIONSCY13 59.0 ?BCFL13 62.0 35.7KCAZ13 68.0 ?This work 68.5 39.9Table 5: Results on the test set.FREE917 WEBQUESTIONSOur system 73.9 41.2?VSM 71.0 40.5?ASSOCIATION 52.7 35.3?PARAPHRASE 31.8 21.3SIMPLEGEN 73.4 40.4Full matrix 52.7 35.3Diagonal 50.4 30.6Identity 50.7 30.4JACCARD 69.7 31.3EDIT 40.8 24.8WDDC06 71.0 29.8Table 6: Results for ablations and baselines on develop-ment set.model (using only logical form features).
Table 5shows that our full system obtains highest accu-racy, and that removing the association model re-sults in a much larger degradation compared to re-moving the VS model.Utterance generation Our system generatesrelatively natural utterances from logical forms us-ing simple rules based on Freebase descriptions(Section 4).
We now consider simply concate-nating Freebase descriptions.
For example, thelogical form R[PlaceOfBirth].ElvisPresleywould generate the utterance ?What location ElvisPresley place of birth??.
Row SIMPLEGEN in Ta-ble 6 demonstrates that we still get good results inthis setup.
This is expected given that our para-phrase models are not sensitive to the syntacticstructure of the generated utterance.VS model Our system learns parameters for afull W matrix.
We now examine results whenlearning parameters for a full matrix W , a diago-nal matrix W , and when setting W to be the iden-tity matrix.
Table 6 (third section) illustrates thatlearning a full matrix substantially improves accu-racy.
Figure 4 gives an example for a correct para-phrase pair, where the full matrix model booststhe overall model score.
Note that the full ma-trix assigns a high score for the phrases ?officiallanguage?
and ?speak?
compared to the simplermodels, but other pairs are less interpretable.Baselines We also compared our system to thefollowing implemented baselines:Full do people czech republic speakoffical 0.7 8.09 15.34 21.62 24.44language 3.86 -3.13 7.81 2.58 14.74czech 0.67 16.55 2.76republic -8.71 12.47 -10.75Diagonal do people czech republic speakoffical 2.31 -0.72 1.88 0.27 -0.49language 0.27 4.72 11.51 12.33 11czech 1.4 8.13 5.21republic -0.16 6.72 9.69Identity do people czech republic speakoffical 2.26 -1.41 0.89 0.07 -0.58language 0.62 4.19 11.91 10.78 12.7czech 2.88 7.31 5.42republic -1.82 4.34 9.44Figure 4: Values of the paraphrase score v>xiWvci?for allcontent word tokens xiand ci?, where W is an arbitrary fullmatrix, a diagonal matrix, or the identity matrix.
We omitscores for the words ?czech?
and ?republic?
since they ap-pear in all canonical utterances for this example.?
JACCARD: We compute the Jaccard scorebetween the tokens of x and c and define?pr(x, c) to be this single feature.?
EDIT: We compute the token edit distancebetween x and c and define ?pr(x, c) to bethis single feature.?
WDDC06: We re-implement 13 featuresfrom Wan et al (2006), who obtained close tostate-of-the-art performance on the MicrosoftResearch paraphrase corpus.3Table 6 demonstrates that we improve perfor-mance over all baselines.
Interestingly, JACCARDand WDDC06 obtain reasonable performanceon FREE917 but perform much worse on WE-BQUESTIONS.
We surmise this is because ques-tions in FREE917 were generated by annotatorsprompted by Freebase facts, whereas questionsin WEBQUESTIONS originated independently ofFreebase.
Thus, word choice in FREE917 is of-ten close to the generated Freebase descriptions,allowing simple baselines to perform well.Error analysis We sampled examples from thedevelopment set to examine the main reasonsPARASEMPRE makes errors.
We notice that inmany cases the paraphrase model can be furtherimproved.
For example, PARASEMPRE suggests3We implement all features that do not require depen-dency parsing.1422that the best paraphrase for ?What company didHenry Ford work for??
is ?What written worknovel by Henry Ford??
rather than ?The em-ployer of Henry Ford?, due to the exact matchof the word ?work?.
Another example is thequestion ?Where is the Nascar hall of fame?
?,where PARASEMPRE suggests that ?What hall offame discipline has Nascar hall of fame as hallsof fame??
is the best canonical utterance.
Thisis because our simple model allows to associate?hall of fame?
with the canonical utterance threetimes.
Entity recognition also accounts for manyerrors, e.g., the entity chosen in ?where was thegallipoli campaign waged??
is Galipoli and notGalipoliCampaign.
Last, PARASEMPRE does nothandle temporal information, which causes errorsin questions like ?Where did Harriet Tubman liveafter the civil war?
?7 DiscussionIn this work, we approach the problem of seman-tic parsing from a paraphrasing viewpoint.
Afundamental motivation and long standing goalof the paraphrasing and RTE communities hasbeen to cast various semantic applications as para-phrasing/textual entailment (Dagan et al, 2013).While it has been shown that paraphrasing meth-ods are useful for question answering (Harabagiuand Hickl, 2006) and relation extraction (Romanoet al, 2006), this is, to the best of our knowledge,the first paper to perform semantic parsing throughparaphrasing.
Our paraphrase model emphasizessimplicity and efficiency, but the framework is ag-nostic to the internals of the paraphrase method.On the semantic parsing side, our work is mostrelated to Kwiatkowski et al (2013).
The mainchallenge in semantic parsing is coping with themismatch between language and the KB.
In bothKwiatkowski et al (2013) and this work, an inter-mediate representation is employed to handle themismatch, but while they use a logical represen-tation, we opt for a text-based one.
Our choiceallows us to benefit from the parallel monolingualcorpus PARALEX and from word vectors trainedon Wikipedia.
We believe that our approach isparticularly suitable for scenarios such as factoidquestion answering, where the space of logicalforms is somewhat constrained and a few gener-ation rules suffice to reduce the problem to para-phrasing.Our work is also related to Fader et al (2013),who presented a paraphrase-driven question an-swering system.
One can view this work as ageneralization of Fader et al along three dimen-sions.
First, Fader et al use a KB over natu-ral language extractions rather than a formal KBand so querying the KB does not require a gener-ation step ?
they paraphrase questions to KB en-tries directly.
Second, they suggest a particularparaphrasing method that maps a test question to aquestion for which the answer is already known ina single step.
We propose a general paraphrasingframework and instantiate it with two paraphrasemodels.
Lastly, Fader et al handle queries withonly one property and entity whereas we general-ize to more types of logical forms.Since our generated questions are passed toa paraphrase model, we took a very simple ap-proach, mostly ensuring that we preserved the se-mantics of the utterance without striving for themost fluent realization.
Research on generation(Dale et al, 2003; Reiter et al, 2005; Turner etal., 2009; Piwek and Boyer, 2012) typically fo-cuses on generating natural utterances for humanconsumption, where fluency is important.In conclusion, the main contribution of this pa-per is a novel approach for semantic parsing basedon a simple generation procedure and a paraphrasemodel.
We achieve state-of-the-art results on tworecently released datasets.
We believe that our ap-proach opens a window of opportunity for learn-ing semantic parsers from raw text not necessarilyrelated to the target KB.
With more sophisticatedgeneration and paraphrase, we hope to tackle com-positionally richer utterances.AcknowledgmentsWe thank Kai Sheng Tai for performing the er-ror analysis.
Stanford University gratefully ac-knowledges the support of the Defense AdvancedResearch Projects Agency (DARPA) Deep Ex-ploration and Filtering of Text (DEFT) Programunder Air Force Research Laboratory (AFRL)contract no.
FA8750-13-2-0040.
Any opinions,findings, and conclusion or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the view of the DARPA,AFRL, or the US government.
The second authoris supported by a Google Faculty Research Award.1423ReferencesJ.
Berant, A. Chou, R. Frostig, and P. Liang.
2013.Semantic parsing on Freebase from question-answerpairs.
In Empirical Methods in Natural LanguageProcessing (EMNLP).Q.
Cai and A. Yates.
2013.
Large-scale semantic pars-ing via schema matching and lexicon extension.
InAssociation for Computational Linguistics (ACL).M.
Chang, D. Goldwasser, D. Roth, and V. Srikumar.2010.
Discriminative learning over constrained la-tent representations.
In North American Associationfor Computational Linguistics (NAACL).I.
Dagan, D. Roth, M. Sammons, and F. M. Zanzotto.2013.
Recognizing Textual Entailment: Models andApplications.
Morgan and Claypool Publishers.R.
Dale, S. Geldof, and J. Prost.
2003.
Coral: usingnatural language generation for navigational assis-tance.
In Australasian computer science conference,pages 35?44.D.
Das and N. A. Smith.
2009.
Paraphrase identifica-tion as probabilistic quasi-synchronous recognition.In Association for Computational Linguistics (ACL),pages 468?476.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsuper-vised construction of large paraphrase corpora: Ex-ploiting massively parallel news sources.
In Inter-national Conference on Computational Linguistics(COLING).J.
Duchi, E. Hazan, and Y.
Singer.
2010.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
In Conference on Learning Theory(COLT).A.
Fader, S. Soderland, and O. Etzioni.
2011.
Identi-fying relations for open information extraction.
InEmpirical Methods in Natural Language Processing(EMNLP).A.
Fader, L. Zettlemoyer, and O. Etzioni.
2013.Paraphrase-driven learning for open question an-swering.
In Association for Computational Linguis-tics (ACL).C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.J.
R. Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by Gibbs sampling.
In Associ-ation for Computational Linguistics (ACL), pages363?370.Google.
2013.
Freebase data dumps (2013-06-09).
https://developers.google.com/freebase/data.A.
Haghighi, A. Y. Ng, and C. D. Manning.
2005.Robust textual inference via graph matching.
InEmpirical Methods in Natural Language Processing(EMNLP).S.
Harabagiu and A. Hickl.
2006.
Methods for usingtextual entailment in open-domain question answer-ing.
In Association for Computational Linguistics(ACL).M.
Heilman and N. A. Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Human Lan-guage Technology and North American Associationfor Computational Linguistics (HLT/NAACL), pages1011?1019.E.
H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.2012.
Improving word representations via globalcontext and multiple word prototypes.
In Associa-tion for Computational Linguistics (ACL).D.
Klein and C. Manning.
2003.
Accurate unlexical-ized parsing.
In Association for Computational Lin-guistics (ACL), pages 423?430.T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2010.
Inducing probabilistic CCGgrammars from logical form with higher-order uni-fication.
In Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1223?1233.T.
Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.2013.
Scaling semantic parsers with on-the-fly on-tology matching.
In Empirical Methods in NaturalLanguage Processing (EMNLP).P.
Liang.
2013.
Lambda dependency-based composi-tional semantics.
Technical report, ArXiv.T.
Lin, Mausam, and O. Etzioni.
2012.
Entity linkingat web scale.
In Knowledge Extraction Workshop(AKBC-WEKEX).T.
Mikolov, K. Chen, G. Corrado, and Jeffrey.
2013.Efficient estimation of word representations in vec-tor space.
Technical report, ArXiv.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30:417?449.P.
Piwek and K. E. Boyer.
2012.
Varieties of questiongeneration: Introduction to this special issue.
Dia-logue and Discourse, 3:1?9.E.
Reiter, S. Sripada, J.
Hunter, J. Yu, and I. Davy.2005.
Choosing words in computer-generatedweather forecasts.
Artificial Intelligence, 167:137?169.L.
Romano, M. kouylekov, I. Szpektor, I. Dagan,and A. Lavelli.
2006.
Investigating a genericparaphrase-based approach for relation extraction.In Proceedings of ECAL.R.
Socher, E. H. Huang, J. Pennin, C. D. Manning, andA.
Ng.
2011.
Dynamic pooling and unfolding re-cursive autoencoders for paraphrase detection.
InAdvances in Neural Information Processing Systems(NIPS), pages 801?809.1424A.
Stern and I. Dagan.
2011.
A confidence modelfor syntactically-motivated entailment proofs.
InRecent Advances in Natural Language Processing,pages 455?462.K.
Toutanova and C. D. Manning.
2003.
Feature-rich part-of-speech tagging with a cyclic depen-dency network.
In Human Language Technologyand North American Association for ComputationalLinguistics (HLT/NAACL).R.
Turner, Y. Sripada, and E. Reiter.
2009.
Generatingapproximate geographic descriptions.
In EuropeanWorkshop on Natural Language Generation, pages42?49.S.
Wan, M. Dras, R. Dale, and C. Paris.
2006.
Usingdependency-based features to take the ?para-farce?out of paraphrase.
In Australasian Language Tech-nology Workshop.M.
Wang and C. D. Manning.
2010.
Probabilistic tree-edit models with structured latent variables for tex-tual entailment and question answering.
In The In-ternational Conference on Computational Linguis-tics, pages 1164?1172.Y.
W. Wong and R. J. Mooney.
2007.
Learningsynchronous grammars for semantic parsing withlambda calculus.
In Association for ComputationalLinguistics (ACL), pages 960?967.M.
Zelle and R. J. Mooney.
1996.
Learning to parsedatabase queries using inductive logic proramming.In Association for the Advancement of Artificial In-telligence (AAAI), pages 1050?1055.L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Un-certainty in Artificial Intelligence (UAI), pages 658?666.1425
