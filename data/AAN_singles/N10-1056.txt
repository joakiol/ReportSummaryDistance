Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365?368,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsFor the sake of simplicity:Unsupervised extraction of lexical simplifications from WikipediaMark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil and Lillian Leemy89@cornell.edu, bopang@yahoo-inc.com, cristian@cs.cornell.edu, llee@cs.cornell.eduAbstractWe report on work in progress on extract-ing lexical simplifications (e.g., ?collaborate??
?work together?
), focusing on utilizingedit histories in Simple English Wikipedia forthis task.
We consider two main approaches:(1) deriving simplification probabilities via anedit model that accounts for a mixture of dif-ferent operations, and (2) using metadata tofocus on edits that are more likely to be sim-plification operations.
We find our methodsto outperform a reasonable baseline and yieldmany high-quality lexical simplifications notincluded in an independently-created manu-ally prepared list.1 IntroductionNothing is more simple than greatness; indeed, to besimple is to be great.
?Emerson, Literary EthicsStyle is an important aspect of information pre-sentation; indeed, different contexts call for differ-ent styles.
Here, we consider an important dimen-sion of style, namely, simplicity.
Systems that canrewrite text into simpler versions promise to makeinformation available to a broader audience, such asnon-native speakers, children, laypeople, and so on.One major effort to produce such text is theSimple English Wikipedia (henceforth SimpleEW)1,a sort of spin-off of the well-known EnglishWikipedia (henceforth ComplexEW) where hu-man editors enforce simplicity of language throughrewriting.
The crux of our proposal is to learn lexicalsimplifications from SimpleEW edit histories, thusleveraging the efforts of the 18K pseudonymous in-dividuals who work on SimpleEW.
Importantly, notall the changes on SimpleEW are simplifications; wethus also make use of ComplexEW edits to filter outnon-simplifications.Related work and related problems Previouswork usually involves general syntactic-level trans-1http://simple.wikipedia.orgformation rules [1, 9, 10].2 In contrast, we exploredata-driven methods to learn lexical simplifications(e.g., ?collaborate?
?
?work together?
), which arehighly specific to the lexical items involved and thuscannot be captured by a few general rules.Simplification is strongly related to but distinctfrom paraphrasing and machine translation (MT).While it can be considered a directional form ofthe former, it differs in spirit because simplificationmust trade off meaning preservation (central to para-phrasing) against complexity reduction (not a con-sideration in paraphrasing).
Simplification can alsobe considered to be a form of MT in which the two?languages?
in question are highly related.
How-ever, note that ComplexEW and SimpleEW do nottogether constitute a clean parallel corpus, but ratheran extremely noisy comparable corpus.
For ex-ample, Complex/Simple same-topic document pairsare often written completely independently of eachother, and even when it is possible to get goodsentence alignments between them, the sentencepairs may reflect operations other than simplifica-tion, such as corrections, additions, or edit spam.Our work joins others in using Wikipedia revi-sions to learn interesting types of directional lexicalrelations, e.g, ?eggcorns?3 [7] and entailments [8].2 MethodAs mentioned above, a key idea in our work is toutilize SimpleEW edits.
The primary difficulty inworking with these modifications is that they includenot only simplifications but also edits that serveother functions, such as spam removal or correctionof grammar or factual content (?fixes?).
We describetwo main approaches to this problem: a probabilis-tic model that captures this mixture of different editoperations (?2.1), and the use of metadata to filterout undesirable revisions (?2.2).2One exception [5] changes verb tense and replaces pro-nouns.
Other lexical-level work focuses on medical text [4, 2],or uses frequency-filtered WordNet synonyms [3].3A type of lexical corruption, e.g., ?acorn??
?eggcorn?.3652.1 Edit modelWe say that the kth article in a Wikipedia corre-sponds to (among other things) a title or topic (e.g.,?Cat?)
and a sequence ~dk of article versions causedby successive edits.
For a given lexical item orphrase A, we write A ?
~dk if there is any versionin ~dk that contains A.
From each ~dk we extract acollection ek = (ek,1, ek,2, .
.
.
, ek,nk) of lexical editinstances, repeats allowed, where ek,i = A ?
ameans that phrase A in one version was changed toa in the next, A 6= a; e.g., ?stands for?
?
?is thesame as?.
(We defer detailed description of how weextract lexical edit instances from data to ?3.1.)
Wedenote the collection of ~dk in ComplexEW and Sim-pleEW as C and S, respectively.There are at least four possible edit operations: fix(o1), simplify (o2), no-op (o3), or spam (o4).
How-ever, for this initial work we assume P (o4) = 0.4Let P (oi | A) be the probability that oi is appliedto A, and P (a | A, oi) be the probability of A ?
agiven that the operation is oi.
The key quantities ofinterest are P (o2 | A) in S, which is the probabilitythatA should be simplified, and P (a | A, o2), whichyields proper simplifications of A.
We start with anequation that models the probability that a phrase Ais edited into a:P (a | A) =?oi?
?P (oi | A)P (a | A, oi), (1)where ?
is the set of edit operations.
This involvesthe desired parameters, which we solve for by esti-mating the others from data, as described next.Estimation Note that P (a | A, o3) = 0 if A 6= a.Thus, if we have estimates for o1-related probabili-ties, we can derive o2-related probabilities via Equa-tion 1.
To begin with, we make the working as-sumption that occurrences of simplification in Com-plexEW are negligible in comparison to fixes.
Sincewe are also currently ignoring edit spam, we thusassume that only o1 edits occur in ComplexEW.5Let fC(A) be the fraction of ~dk in Ccontaining A in which A is modified:fC(A) =|{~dk?C|?a,i such that ek,i=A?a}||{~dk?C|A?~dk}|.4Spam/vandalism detection is a direction for future work.5This assumption also provides useful constraints to EM,which we plan to apply in the future, by reducing the number ofparameter settings yielding the same likelihood.We similarly define fS(A) on ~dk in S. Note that wecount topics (version sequences), not individual ver-sions: if A appears at some point and is not editeduntil 50 revisions later, we should not concludethat A is unlikely to be rewritten; for example, theintervening revisions could all be minor additions,or part of an edit war.If we assume that the probability of any particularfix operation being applied in SimpleEW is propor-tional to that in ComplexEW?
e.g., the SimpleEWfix rate might be dampened because already-editedComplexEW articles are copied over ?
we have6P?
(o1 | A) = ?fC(A)where 0 ?
?
?
1.
Note that in SimpleEW,P (o1 ?
o2 | A) = P (o1 | A) + P (o2 | A),where P (o1 ?
o2 | A) is the probability that A ischanged to a different word in SimpleEW, which weestimate as P?
(o1 ?
o2 | A) = fS(A).
We then setP?
(o2 | A) = max (0, fS(A)?
?fC(A)).Next, under our working assumption, we estimatethe probability of A being changed to a as a fixby the proportion of ComplexEW edit instances thatrewrite A to a:P?
(a | A, o1) =|{(k, i) pairs | ek,i = A?
a ?
~dk ?
C}|?a?
|{(k, i) pairs | ek,i = A?
a?
?
~dk ?
C}|.A natural estimate for the conditional probabilityof A being rewritten to a under any operation typeis based on observations of A ?
a in SimpleEW,since that is the corpus wherein both operations areassumed to occur:P?
(a | A) =|{(k, i) pairs | ek,i = A?
a ?
~dk ?
S}|?a?
|{(k, i) pairs | ek,i = A?
a?
?
~dk ?
S}|.Thus, from (1) we get that for A 6= a:P?
(a | A,o2) =P?
(a | A)?
P?
(o1 | A)P?
(a | A,o1)P?
(o2 | A).2.2 Metadata-based methodsWiki editors have the option of associating a com-ment with each revision, and such comments some-times indicate the intent of the revision.
We there-fore sought to use comments to identify ?trusted?6Throughout, ?hats?
denote estimates.366revisions wherein the extracted lexical edit instances(see ?3.1) would be likely to be simplifications.Let ~rk = (r1k, .
.
.
, rik, .
.
.)
be the sequence of revi-sions for the kth article in SimpleEW, where rik is theset of lexical edit instances (A ?
a) extracted fromthe ith modification of the document.
Let cik be thecomment that accompanies rik, and conversely, letR(Set) = {rik|cik ?
Set}.We start with a seed set of trusted comments,Seed.
To initialize it, we manually inspected a smallsample of the 700K+ SimpleEW revisions that bearcomments, and found that comments containing aword matching the regular expression *simpl* (e.g,?simplify?)
seem promising.
We thus set Seed :={ ?
simpl?}
(abusing notation).The SIMPL method Given a set of trusted revi-sions TRev (in our case TRev = R(Seed)), wescore each A ?
a ?
TRev by the point-wise mu-tual information (PMI) between A and a.7 We writeRANK(TRev) to denote the PMI-based ranking ofA?
a ?
TRev, and use SIMPL to denote our mostbasic ranking method, RANK(R(Seed)).Two ideas for bootstrapping We also consideredbootstrapping as a way to be able to utilize revisionswhose comments are not in the initial Seed set.Our first idea was to iteratively expand the setof trusted comments to include those that most of-ten accompany already highly ranked simplifica-tions.
Unfortunately, our initial implementations in-volved many parameters (upper and lower comment-frequency thresholds, number of highly ranked sim-plifications to consider, number of comments to addper iteration), making it relatively difficult to tune;we thus omit its results.Our second idea was to iteratively expand theset of trusted revisions, adding those that containalready highly ranked simplifications.
While ourinitial implementation had fewer parameters thanthe method sketched above, it tended to terminatequickly, so that not many new simplifications werefound; so, again, we do not report results here.An important direction for future work is to differ-entially weight the edit instances within a revision,as opposed to placing equal trust in all of them; this7PMI seemed to outperform raw frequency and conditionalprobability.could prevent our bootstrapping methods from giv-ing common fixes (e.g., ?a??
?the?)
high scores.3 Evaluation83.1 DataWe obtained the revision histories of both Sim-pleEW (November 2009 snapshot) and ComplexEW(January 2008 snapshot).
In total, ?1.5M revisionsfor 81733 SimpleEW articles were processed (only30% involved textual changes).
For ComplexEW,we processed ?16M revisions for 19407 articles.Extracting lexical edit instances.
For each ar-ticle, we aligned sentences in each pair of adja-cent versions using tf-idf scores in a way simi-lar to Nelken and Shieber [6] (this produced sat-isfying results because revisions tended to repre-sent small changes).
From the aligned sentencepairs, we obtained the aforementioned lexical editinstances A ?
a.
Since the focus of our studywas not word alignment, we used a simple methodthat identified the longest differing segments (basedon word boundaries) between each sentence, exceptthat to prevent the extraction of entire (highly non-matching) sentences, we filtered out A ?
a pairs ifeither A or a contained more than five words.3.2 Comparison pointsBaselines RANDOM returns lexical edit instancesdrawn uniformly at random from among those ex-tracted from SimpleEW.
FREQUENT returns themost frequent lexical edit instances extracted fromSimpleEW.Dictionary of simplifications The SimpleEW ed-itor ?Spencerk?
(Spencer Kelly) has assembled a listof simple words and simplifications using a combi-nation of dictionaries and manual effort9.
He pro-vides a list of 17,900 simple words ?
words that donot need further simplification ?
and a list of 2000transformation pairs.
We did not use Spencerk?s setas the gold standard because many transformationswe found to be reasonable were not on his list.
In-stead, we measured our agreement with the list oftransformations he assembled (SPLIST).8Results at http://www.cs.cornell.edu/home/llee/data/simple9http://www.spencerwaterbed.com/soft/simple/about.html3673.3 Preliminary resultsThe top 100 pairs from each system (edit model10and SIMPL and the two baselines) plus 100 ran-domly selected pairs from SPLIST were mixed andall presented in random order to three native Englishspeakers and three non-native English speakers (allnon-authors).
Each pair was presented in randomorientation (i.e., either as A ?
a or as a ?
A),and the labels included ?simpler?, ?more complex?,?equal?, ?unrelated?, and ???
(?hard to judge?).
Thefirst two labels correspond to simplifications for theorientations A ?
a and a ?
A, respectively.
Col-lapsing the 5 labels into ?simplification?, ?not a sim-plification?, and ???
yields reasonable agreementamong the 3 native speakers (?
= 0.69; 75.3% of thetime all three agreed on the same label).
While wepostulated that non-native speakers11 might be moresensitive to what was simpler, we note that they dis-agreed more than the native speakers (?
= 0.49) andreported having to consult a dictionary.
The native-speaker majority label was used in our evaluations.Here are the results; ?-x-y?
means that x and y arethe number of instances discarded from the precisioncalculation for having no majority label or majoritylabel ??
?, respectively:Method Prec@100 # of pairsSPLIST 86% (-0-0) 2000Edit model 77% (-0-1) 1079SIMPL 66% (-0-0) 2970FREQUENT 17% (-1-7) -RANDOM 17% (-1-4) -Both baselines yielded very low precisions ?clearly not all (frequent) edits in SimpleEW weresimplifications.
Furthermore, the edit model yieldedhigher precision than SIMPL for the top 100 pairs.
(Note that we only examined one simplification perA for those A where P?
(o2 | A) was well-defined;thus ?# of pairs?
does not directly reflect the fullpotential recall that either method can achieve.
)Both, however, produced many high-quality pairs(62% and 71% of the correct pairs) not included inSPLIST.
We also found the pairs produced by thesetwo systems to be complementary to each other.
We10We only considered those A such that freq(A ?
?)
>1 ?
freq(A) > 100 on both SimpleEW and ComplexEW.
Thefinal top 100 A ?
a pairs were those with As with the highestP (o2 | A).
We set ?
= 1.11Native languages: Russian; Russian; Russian and Kazakh.believe that these two approaches provide a goodstarting point for further explorations.Finally, some examples of simplifications foundby our methods: ?stands for?
?
?is the sameas?, ?indigenous?
?
?native?, ?permitted?
?
?al-lowed?, ?concealed?
?
?hidden?, ?collapsed?
?
?fell down?, ?annually??
?every year?.3.4 Future workFurther evaluation could include comparison withmachine-translation and paraphrasing algorithms.
Itwould be interesting to use our proposed estimatesas initialization for EM-style iterative re-estimation.Another idea would be to estimate simplification pri-ors based on a model of inherent lexical complexity;some possible starting points are number of sylla-bles (which is used in various readability formulae)or word length.Acknowledgments We first wish to thank Ainur Yessenalinafor initial investigations and helpful comments.
We arealso thankful to R. Barzilay, T. Bruce, C. Callison-Burch, J.Cantwell, M. Dredze, C. Napoles, E. Gabrilovich, & the review-ers for helpful comments; W. Arms and L. Walle for access tothe Cornell Hadoop cluster; J. Cantwell for access to computa-tional resources; R. Hwa & A. Owens for annotation software;M. Ulinski for preliminary explorations; J. Cantwell, M. Ott, J.Silverstein, J. Yatskar, Y. Yatskar, & A. Yessenalina for annota-tions.
Supported by NSF grant IIS-0910664.References[1] R. Chandrasekar, B. Srinivas.
Automatic induction of rulesfor text simplification.
Knowledge-Based Systems, 1997.
[2] L. Dele?ger, P. Zweigenbaum.
Extracting lay paraphrasesof specialized expressions from monolingual comparablemedical corpora.
Workshop on Building and Using Com-parable Corpora, 2009.
[3] S. Devlin, J. Tait.
The use of a psycholinguistic database inthe simplification of text for aphasic readers.
In LinguisticDatabases, 1998.
[4] N. Elhadad, K. Sutaria.
Mining a lexicon of technical termsand lay equivalents.
Workshop on BioNLP, 2007.
[5] B. Beigman Klebanov, K. Knight, D. Marcu.
Text simplifi-cation for information-seeking applications.
OTM Confer-ences, 2004.
[6] R. Nelken, S. M. Shieber.
Towards robust context-sensitivesentence alignment for monolingual corpora.
EACL, 2006.
[7] R. Nelken, E. Yamangil.
Mining Wikipedia?s article re-vision history for training computational linguistics algo-rithms.
WikiAI, 2008.
[8] E. Shnarch, L. Barak, I. Dagan.
Extracting lexical referencerules from Wikipedia.
ACL, 2009.
[9] A. Siddharthan, A. Nenkova, K. McKeown.
Syntacticsimplification for improving content selection in multi-document summarization.
COLING, 2004.
[10] D. Vickrey, D. Koller.
Sentence simplification for seman-tic role labeling/ ACL, 2008.368
