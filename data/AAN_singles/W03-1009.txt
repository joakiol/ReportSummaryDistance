Variation of Entropy and Parse Trees of Sentences as a Function of theSentence NumberDmitriy Genzel and Eugene CharniakBrown Laboratory for Linguistic Information ProcessingDepartment of Computer ScienceBrown UniversityProvidence, RI, USA, 02912{dg,ec}@cs.brown.eduAbstractIn this paper we explore the variation ofsentences as a function of the sentencenumber.
We demonstrate that while theentropy of the sentence increases with thesentence number, it decreases at the para-graph boundaries in accordance with theEntropy Rate Constancy principle (intro-duced in related work).
We also demon-strate that the principle holds for differ-ent genres and languages and explore therole of genre informativeness.
We investi-gate potential causes of entropy variationby looking at the tree depth, the branch-ing factor, the size of constituents, and theoccurrence of gapping.1 Introduction and Related WorkIn many natural language processing applications,such as parsing or language modeling, sentences aretreated as natural self-contained units.
Yet it is well-known that for interpreting the sentences the dis-course context is often very important.
The latersentences in the discourse contain references to theentities in the preceding sentences, and this fact isoften useful, e.g., in caching for language model-ing (Goodman, 2001).
The indirect influence of thecontext, however, can be observed even when a sen-tence is taken as a stand-alone unit, i.e., without itscontext.
It is possible to distinguish between a setof earlier sentences and a set of later sentences with-out any direct comparison by computing certain lo-cal statistics of individual sentences, such as theirentropy (Genzel and Charniak, 2002).
In this workwe provide additional evidence for this hypothesisand investigate other sentence statistics.1.1 Entropy Rate ConstancyEntropy, as a measure of information, is often usedin the communication theory.
If humans haveevolved to communicate in the most efficient way(some evidence for that is provided by Plotkin andNowak (2000)), then they would communicate insuch a way that the entropy rate would be constant,namely, equal to the channel capacity (Shannon,1948).In our previous work (Genzel and Charniak,2002) we propose that entropy rate is indeed con-stant in human communications.
When read in con-text, each sentence would appear to contain roughlythe same amount of information, per word, whetherit is the first sentence or the tenth one.
Thus thetenth sentence, when taken out of context, must ap-pear significantly more informative (and thereforeharder to process), since it implicitly assumes thatthe reader already knows all the information in thepreceding nine sentences.
Indeed, the greater thesentence number, the harder to process the sentencemust appear, though for large sentence numbers thiswould be very difficult to detect.
This makes intu-itive sense: out-of-context sentences are harder tounderstand than in-context ones, and first sentencescan never be out of context.
It is also demonstratedempirically through estimating entropy rate of vari-ous sentences.In the first part of the present paper (Sections 2and 3) we extend and further verify these results.
Inthe second part (Section 4), we investigate the poten-tial reasons underlying this variation in complexityby looking at the parse trees of the sentences.
Wealso discuss how genre and style affect the strengthof this effect.1.2 Limitations of Preceding WorkIn our previous work we demonstrate that the wordentropy rate increases with the sentence number; wedo it by estimating entropy of Wall Street Journalarticles in Penn Treebank in three different ways.
Itmay be the case, however, that this effect is corpus-and language-specific.
To show that the EntropyRate Constancy Principle is universal, we need toconfirm it for different genres and different lan-guages.
We will address this issue in Section 3.Furthermore, if the principle is correct, it shouldalso apply to the sentences numbered from the be-ginning of a paragraph, rather than from the begin-ning of the article, since in either case there is a shiftof topic.
We will discuss this in Section 2.2 Within-Paragraph Effects2.1 Implications of Entropy Rate ConstancyPrincipleWe have previously demonstrated (see Genzel andCharniak (2002) for detailed derivation) that theconditional entropy of the ith word in the sentence(Xi), given its local context Li (the preceding wordsin the same sentence) and global context Ci (thewords in all preceding sentences) can be representedasH(Xi|Ci, Li) = H(Xi|Li)?
I(Xi, Ci|Li)where H(Xi|Li) is the conditional entropy of theith word given local context, and I(Xi, Ci|Li) isthe conditional mutual information between the ithword and out-of-sentence context, given the localcontext.
Since Ci increases with the sentence num-ber, we will assume that, normally, it will providemore and more information with each sentence.
Thiswould cause the second term on the right to increasewith the sentence number, and since H(Xi|Ci, Li)must remain constant (by our assumption), the firstterm should increase with sentence number, and ithad been shown to do so (Genzel and Charniak,2002).Our assumption about the increase of the mutualinformation term is, however, likely to break at theparagraph boundary.
If there is a topic shift at theboundary, the context probably provides more infor-mation to the preceding sentence, than it does to thenew one.
Hence, the second term will decrease, andso must the first one.In the next section we will verify this experimen-tally.2.2 Experimental SetupWe use the Wall Street Journal text (years 1987-1989) as our corpus.
We take all articles that con-tain ten or more sentences, and extract the first tensentences.
Then we:1.
Group extracted sentences according to theirsentence number into ten sets of 49559 sen-tences each.2.
Separate each set into two subsets, paragraph-starting and non-paragraph-starting sentences1.3.
Combine first 45000 sentences from each setinto the training set and keep all remaining dataas 10 testing sets (19 testing subsets).We use a simple smoothed trigram language model:P (xi|x1 .
.
.
xi?1) ?
P (xi|xi?2xi?1)= ?1P?
(xi|xi?2xi?1)+ ?2P?
(xi|xi?1)+ (1?
?1 ?
?2)P?
(xi)where ?1 and ?2 are the smoothing coefficients2,and P?
is a maximum likelihood estimate of the cor-responding probability, e.g.,P?
(xi|xi?2xi?1) =C(xi?2xi?1xi)C(xi?2xi?1)where C(xi .
.
.
xj) is the number of times this se-quence appears in the training data.We then evaluate the resulting model on each ofthe testing sets, computing per-word entropy of theset:H?
(X) =1|X|?xi?XlogP (xi|xi?2xi?1)1First sentences are, of course, all paragraph-starting.2We have arbitrarily chosen the smoothing coefficients to be0.5 and 0.3, correspondingly.1 2 3 4 5 6 7 8 9 105.85.96.06.16.26.36.46.56.6Sentence numberEntropy(bits)all sentencesparagraph?startingnon?paragraph?startingFigure 1: Entropy vs.
Sentence number2.3 Results and DiscussionAs outlined above, we have ten testing sets, one foreach sentence number; each set (except for the first)is split into two subsets: sentences that start a para-graph, and sentences that do not.
The results for fullsets, paragraph-starting subsets, and non-paragraph-starting subsets are presented in Figure 1.First, we can see that the the entropy for fullsets (solid line) is generally increasing.
This re-sult corresponds to the previously discussed effectof entropy increasing with the sentence number.
Wealso see that for all sentence numbers the paragraph-starting sentences have lower entropy than the non-paragraph-starting ones, which is what we intendedto demonstrate.
In such a way, the paragraph-starting sentences are similar to the first sentences,which makes intuitive sense.All the lines roughly show that entropy increaseswith the sentence number, but the behavior at thesecond and the third sentences is somewhat strange.We do not yet have a good explanation of this phe-nomenon, except to point out that paragraphs thatstart at the second or third sentences are probablynot ?normal?
because they most likely do not indi-cate a topic shift.
Another possible explanation isthat this effect is an artifact of the corpus used.We have also tried to group sentences based ontheir sentence number within paragraph, but wereunable to observe a significant effect.
This may bedue to the decrease of this effect in the later sen-tences of large articles, or perhaps due to the relativeweakness of the effect3.3 Different Genres and Languages3.1 Experiments on Fiction3.1.1 IntroductionAll the work on this problem so far has focusedon the Wall Street Journal articles.
The results arethus naturally suspect; perhaps the observed effectis simply an artifact of the journalistic writing style.To address this criticism, we need to perform com-parable experiments on another genre.Wall Street Journal is a fairly prototypical exam-ple of a news article, or, more generally, a writingwith a primarily informative purpose.
One obviouscounterpart of such a genre is fiction4.
Another al-ternative might be to use transcripts of spoken dia-logue.Unfortunately, works of fiction, are either non-homogeneous (collections of works) or relativelyshort with relatively long subdivisions (chapters).This is crucial, since in the sentence number experi-ments we obtain one data point per article, thereforeit is impossible to use book chapters in place of arti-cles.3.1.2 Experimental Setup and ResultsFor our experiments we use War and Peace (Tol-stoy, 1869), since it is rather large and publicly avail-able.
It contains only about 365 rather long chap-ters5.
Unlike WSJ articles, each chapter is not writ-ten on a single topic, but usually has multiple topicshifts.
These shifts, however, are marked only asparagraph breaks.
We, therefore, have to assumethat each paragraph break represents a topic shift,3We combine into one set very heterogeneous data: both 1stand 51st sentence might be in the same set, if they both starta paragraph.
The experiment in Section 2.2 groups only theparagraph-starting sentences with the same sentence number.4We use prose rather than poetry, which presumably iseven less informative, because poetry often has superficial con-straints (meter); also, it is hard to find a large homogeneouspoetry collection.5For comparison, Penn Treebank contains over 2400 (muchshorter) WSJ articles.1 2 3 4 58.058.18.158.28.258.3Sentence number since beginning of paragraphEntropyin bitsReal runControl runsFigure 2: War and Peace: Englishand treat each paragraph as being an equivalent of aWSJ article, even though this is obviously subopti-mal.The experimental setup is very similar to the oneused in Section 2.2.
We use roughly half of the datafor training purposes and split the rest into testingsets, one per each sentence number, counted fromthe beginning of a paragraph.We then evaluate the results using the samemethod as in Section 2.2.
We expect that the en-tropy would increase with the sentence number, justas in the case of the sentences numbered from thearticle boundary.
This effect is present, but is notvery pronounced.
To make sure that it is statisticallysignificant, we also do 1000 control runs for com-parison, with paragraph breaks inserted randomly atthe appropriate rate.
The results (including 3 ran-dom runs) can be seen in Figure 2.
To make sureour results are significant we compare the correla-tion coefficient between entropy and sentence num-ber to ones from simulated runs, and find them to besignificant (P=0.016).It is fairly clear that the variation, especially be-tween the first and the later sentences, is greaterthan it would be expected for a purely random oc-currence.
We will see further evidence for this in thenext section.3.2 Experiments on Other LanguagesTo further verify that this effect is significant anduniversal, it is necessary to do similar experimentsin other languages.
Luckily, War and Peace is alsodigitally available in other languages, of which wepick Russian and Spanish for our experiments.We follow the same experimental procedure as inSection 3.1.2 and obtain the results for Russian (Fig-ure 3(a)) and Spanish (Figure 3(b)).
We see that re-sults are very similar to the ones we obtained forEnglish.
The results are again significant for bothRussian (P=0.004) and Spanish (P=0.028).3.3 Influence of Genre on the Strength of theEffectWe have established that entropy increases with thesentence number in the works of fiction.
We ob-serve, however, that the effect is smaller than re-ported in our previous work (Genzel and Charniak,2002) for Wall Street Journal articles.
This is to beexpected, since business and news writing tends tobe more structured and informative in nature, grad-ually introducing the reader to the topic.
Context,therefore, plays greater role in this style of writing.To further investigate the influence of genre andstyle on the strength of the effect we perform exper-iments on data from British National Corpus (Leech,1992) which is marked by genre.For each genre, we extract first ten sentences ofeach genre subdivision of ten or more sentences.90% of this data is used as training data and 10%as testing data.
Testing data is separated into tensets: all the first sentences, all the second sentences,and so on.
We then use a trigram model trained onthe training data set to find the average per-word en-tropy for each set.
We obtain ten numbers, whichin general tend to increase with the sentence num-ber.
To find the degree to which they increase, wecompute the correlation coefficient between the en-tropy estimates and the sentence numbers.
We reportthese coefficients for some genres in Table 1.
To en-sure reliability of results we performed the describedprocess 400 times for each genre, sampling differenttesting sets.The results are very interesting and strongly sup-port our assumption that informative and struc-tured (and perhaps better-written) genres will have1 2 3 4 59.29.39.49.59.69.79.8Sentence number since beginning of paragraphEntropyin bitsReal runControl runs(a) Russian1 2 3 4 58.28.258.38.358.48.458.58.558.68.65Entropyin bitsSentence number since beginning of paragraphReal runControl runs(b) SpanishFigure 3: War and Peacestronger correlations between entropy and sentencenumber.
There is only one genre, tabloid newspa-pers6, that has negative correlation.
The four gen-res with the smallest correlation are all quite non-informative: tabloids, popular magazines, advertise-ments7 and poetry.
Academic writing has highercorrelation coefficients than non-academic.
Also,humanities and social sciences writing is probablymore structured and better stylistically than scienceand engineering writing.
At the bottom of the tablewe have genres which tend to be produced by pro-fessional writers (biography), are very informative(TV news feed) or persuasive and rhetorical (parlia-mentary proceedings).3.4 ConclusionsWe have demonstrated that paragraph boundaries of-ten cause the entropy to decrease, which seems tosupport the Entropy Rate Constancy principle.
Theeffects are not very large, perhaps due to the fact6Perhaps, in this case the readers are only expected to lookat the headlines.7Advertisements could be called informative, but they tendto be sets of loosely related sentences describing various fea-tures, often in no particular order.that each new paragraph does not necessarily rep-resent a shift of topic.
This is especially true in amedium like the Wall Street Journal, where articlesare very focused and tend to stay on one topic.
Infiction, paragraphs are often used to mark a topicshift, but probably only a small proportion of para-graph breaks in fact represents topic shifts.
We alsoobserved that more informative and structured writ-ing is subject to stronger effect than speculative andimaginative one, but the effect is present in almostall writing.In the next section we will discuss the potentialcauses of the entropy results presented both in thepreceding and this work.4 Investigating Non-Lexical CausesIn our previous work we discuss potential causesof the entropy increase.
We find that both lexical(which words are used) and non-lexical (how thewords are used) causes are present.
In this sectionwe will discuss possible non-lexical causes.We know that some non-lexical causes arepresent.
The most natural way to find these causes isto examine the parse trees of the sentences.
There-fore, we collect a number of statistics on the parseBNC genre Corr.
coef.Tabloid newspapers ?0.342?
0.014Popular magazines 0.073?
0.016Print advertisements 0.175?
0.015Fiction: poetry 0.261?
0.013Religious texts 0.328?
0.012Newspapers: commerce/finance 0.365?
0.013Non-acad: natural sciences 0.371?
0.012Official documents 0.391?
0.012Fiction: prose 0.409?
0.011Non-acad: medicine 0.411?
0.013Newspapers: sports 0.433?
0.047Acad: natural sciences 0.445?
0.010Non-acad: tech, engineering 0.478?
0.011Non-acad: politics, law, educ.
0.512?
0.004Acad: medicine 0.517?
0.007Acad: tech, engineering 0.521?
0.010Newspapers: news reportage 0.541?
0.009Non-acad: social sciences 0.541?
0.008Non-acad: humanities 0.598?
0.007Acad: politics, laws, educ.
0.619?
0.006Newspapers: miscellaneous 0.622?
0.009Acad: humanities 0.676?
0.007Commerce/finance, economics 0.678?
0.007Acad: social sciences 0.688?
0.004Parliamentary proceedings 0.774?
0.002TV news script 0.850?
0.002Biographies 0.894?
0.001Table 1: Correlation coefficient for different genrestrees and investigate if any statistics show a signifi-cant change with the sentence number.4.1 Experimental SetupWe use the whole Penn Treebank corpus (Marcus etal., 1993) as our data set.
This corpus contains about50000 parsed sentences.Many of the statistics we wish to compute are verysensitive to the length of the sentence.
For example,the depth of the tree is almost linearly related to thesentence length.
This is important because the aver-age length of the sentence varies with the sentencenumber.
To make sure we exclude the effect of thesentence length, we need to normalize for it.We proceed in the following way.
Let T be the setof trees, and f : T ?
R be some statistic of a tree.Let l(t) be the length of the underlying sentence for0 2 4 6 8 100.9850.990.99511.0051.011.015Bucket number (for sentence number)AdjustedtreedepthFigure 4: Tree Depthtree t. Let L(n) = {t|l(t) = n} be the set of trees ofsize n. Let Lf (n) be defined as 1|L(n)|?t?L(n) f(t),the average value of the statistic f on all sentencesof length n. We then define the sentence-length-adjusted statistic, for all t, asf ?
(t) =f(t)Lf (l(t))The average value of the adjusted statistic is nowequal to 1, and it is independent of the sentencelength.We can now report the average value of eachstatistic for each sentence number, as we have donebefore, but instead we will group the sentence num-bers into a small number of ?buckets?
of exponen-tially increasing length8.
We do so to capture thebehavior for all the sentence numbers, and not justfor the first ten (as before), as well as to lump to-gether sentences with similar sentence numbers, forwhich we do not expect much variation.4.2 Tree DepthThe first statistic we consider is also the most nat-ural: tree depth.
The results can be seen in Figure4.In the first part of the graph we observe an in-crease in tree depth, which is consistent with the in-creasing complexity of the sentences.
In the later8For sentence number n we compute the bucket number asblog1.5 nc0 2 4 6 8 100.960.9811.021.041.061.081.11.121.14Bucket number (for sentence number)AdjustedbranchingfactorBranching factorNPs onlyBase NPs onlyFigure 5: Branching factorsentences, the depth decreases slightly, but still staysabove the depth of the first few sentences.4.3 Branching Factor and NP SizeAnother statistic we investigate is the averagebranching factor, defined as the average number ofchildren of all non-leaf nodes in the tree.
It doesnot appear to be directly correlated with the sentencelength, but we normalize it to make sure it is on thesame scale, so we can compare the strength of re-sulting effect.Again, we expect lower entropy to correspond toflatter trees, which corresponds to large branchingfactor.
Therefore we expect the branching factor todecrease with the sentence number, which is indeedwhat we observe (Figure 5, solid line).Each non-leaf node contributes to the averagebranching factor.
It is likely, however, that thebranching factor changes with the sentence num-ber for certain types of nodes only.
The most obvi-ous contributors for this effect seem to be NP (nounphrase) nodes.
Indeed, one is likely to use severalwords to refer to an object for the first time, but onlya few words (even one, e.g., a pronoun) when refer-ring to it later.
We verify this intuitive suggestion,by computing the branching factor for NP, VP (verbphrase) and PP (prepositional phrase) nodes.
OnlyNP nodes show the effect, and it is much stronger(Figure 5, dashed line) than the effect for the branch-0 2 4 6 8 100.980.9911.011.021.031.041.05Bucket number (for sentence number)AdjustedbranchingfactorBranching factorBranching factor w/o base NPsFigure 6: Branching Factor without Base NPsing factor.Furthermore, it is natural to expect that most ofthis effect arises from base NPs, which are definedas the NP nodes whose children are all leaf nodes.Indeed, base NPs show a slightly more pronouncedeffect, at least with regard to the first sentence (Fig-ure 5, dotted line).4.4 Further InvestigationsWe need to determine whether we have accountedfor all of the branching factor effect, by proposingthat it is simply due to decrease in the size of the baseNPs.
To check, we compute the average branchingfactor, excluding base NP nodes.By comparing the solid line in Figure 6 (the origi-nal average branching factor result) with the dashedline (base NPs excluded), you can see that base NPsaccount for most, though not all of the effect.
Itseems, then, that this problem requires further in-vestigation.4.5 GappingAnother potential reason for the increase in the sen-tence complexity might be the increase in the use ofgapping.
We investigate whether the number of theellipsis constructions varies with the sentence num-ber.
We again use Penn Treebank for this experi-0 2 4 6 8 100.40.50.60.70.80.911.11.21.3Bucket number (for sentence number)AdjustednumberofgapsFigure 7: Number of ellipsis nodesment9.As we can see from Figure 7, there is indeed a sig-nificant increase in the use of ellipsis as the sentencenumber increases, which presumably makes the sen-tences more complex.
Only about 1.5% of all thesentences, however, have gaps.5 Future Work and ConclusionsWe have discovered a number of interesting factsabout the variation of sentences with the sentencenumber.
It has been previously known that the com-plexity of the sentences increases with the sentencenumber.
We have shown here that the complexitytends to decrease at the paragraph breaks in accor-dance with the Entropy Rate Constancy principle.We have verified that entropy also increases with thesentence number outside of Wall Street Journal do-main by testing it on a work of fiction.
We have alsoverified that it holds for languages other than En-glish.
We have found that the strength of the effectdepends on the informativeness of a genre.We also looked at the various statistics that showa significant change with the sentence number, suchas the tree depth, the branching factor, the size ofnoun phrases, and the occurrence of gapping.Unfortunately, we have been unable to apply theseresults successfully to any practical problem so far,9Ellipsis nodes in Penn Treebank are marked with *?
* .See Bies et al (1995) for details.primarily because the effects are significant on av-erage and not in any individual instances.
Findingapplications of these results is the most importantdirection for future research.Also, since this paper essentially makes state-ments about human processing, it would be very ap-propriate to to verify the Entropy Rate Constancyprinciple by doing reading time experiments on hu-man subjects.6 AcknowledgmentsWe would like to acknowledge the members of theBrown Laboratory for Linguistic Information Pro-cessing and particularly Mark Johnson for manyuseful discussions.
This research has been supportedin part by NSF grants IIS 0085940, IIS 0112435, andDGE 9870676.ReferencesA.
Bies, M. Ferguson, K. Katz, and R. MacIntyre, 1995.Bracketing Guidelines for Treebank II Style Penn Tree-bank Project.
Penn Treebank Project, University ofPennsylvania.D.
Genzel and E. Charniak.
2002.
Entropy rate con-stancy in text.
In Proceedings of ACL?2002, Philadel-phia.J.
T. Goodman.
2001.
A bit of progress in language mod-eling.
Computer Speech and Language, 15:403?434.G.
Leech.
1992.
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: the Penn treebank.
Computational Linguistics,19:313?330.J.
B. Plotkin and M. A. Nowak.
2000.
Language evo-lution and information theory.
Journal of TheoreticalBiology, pages 147?159.C.
E. Shannon.
1948.
A mathematical theory of commu-nication.
The Bell System Technical Journal, 27:379?423, 623?656, July, October.L.
Tolstoy.
1869.
War and Peace.
Available online,in 4 languages (Russian, English, Spanish, Italian):http://www.magister.msk.ru/library/tolstoy/wp/wp00.htm.
