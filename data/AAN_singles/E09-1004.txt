Proceedings of the 12th Conference of the European Chapter of the ACL, pages 24?32,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsContextual Phrase-Level Polarity Analysis using Lexical Affect Scoringand Syntactic N-gramsApoorv AgarwalDepartment of Computer ScienceColumbia UniversityNew York, USAaa2644@columbia.eduFadi BiadsyDepartment of Computer ScienceColumbia UniversityNew York, USAfadi@cs.columbia.eduKathleen R. MckeownDepartment of Computer ScienceColumbia UniversityNew York, USAkathy@cs.columbia.eduAbstractWe present a classifier to predict con-textual polarity of subjective phrases ina sentence.
Our approach features lexi-cal scoring derived from the Dictionary ofAffect in Language (DAL) and extendedthrough WordNet, allowing us to automat-ically score the vast majority of words inour input avoiding the need for manual la-beling.
We augment lexical scoring withn-gram analysis to capture the effect ofcontext.
We combine DAL scores withsyntactic constituents and then extract n-grams of constituents from all sentences.We also use the polarity of all syntacticconstituents within the sentence as fea-tures.
Our results show significant im-provement over a majority class baselineas well as a more difficult baseline consist-ing of lexical n-grams.1 IntroductionSentiment analysis is a much-researched area thatdeals with identification of positive, negative andneutral opinions in text.
The task has evolved fromdocument level analysis to sentence and phrasallevel analysis.
Whereas the former is suitable forclassifying news (e.g., editorials vs. reports) intopositive and negative, the latter is essential forquestion-answering and recommendation systems.A recommendation system, for example, must beable to recommend restaurants (or movies, books,etc.)
based on a variety of features such as food,service or ambience.
Any single review sentencemay contain both positive and negative opinions,evaluating different features of a restaurant.
Con-sider the following sentence (1) where the writerexpresses opposing sentiments towards food andservice of a restaurant.
In tasks such as this, there-fore, it is important that sentiment analysis be doneat the phrase level.
(1) The Taj has great food but I found their ser-vice to be lacking.Subjective phrases in a sentence are carriers ofsentiments in which an experiencer expresses anattitude, often towards a target.
These subjectivephrases may express neutral or polar attitudes de-pending on the context of the sentence in whichthey appear.
Context is mainly determined by con-tent and structure of the sentence.
For example, inthe following sentence (2), the underlined subjec-tive phrase seems to be negative, but in the largercontext of the sentence, it is positive.1(2) The robber entered the store but his effortswere crushed when the police arrived on time.Our task is to predict contextual polarity of sub-jective phrases in a sentence.
A traditional ap-proach to this problem is to use a prior polaritylexicon of words to first set priors on target phrasesand then make use of the syntactic and semanticinformation in and around the sentence to makethe final prediction.
As in earlier approaches, wealso use a lexicon to set priors, but we explorenew uses of a Dictionary of Affect in Language(DAL) (Whissel, 1989) extended using WordNet(Fellbaum, 1998).
We augment this approach withn-gram analysis to capture the effect of context.We present a system for classification of neutralversus positive versus negative and positive versusnegative polarity (as is also done by (Wilson et al,2005)).
Our approach is novel in the use of fol-lowing features:?
Lexical scores derived from DAL and ex-tended through WordNet: The Dictionaryof Affect has been widely used to aid in in-terpretation of emotion in speech (Hirschberg1We assign polarity to phrases based on Wiebe (Wiebe etal., 2005); the polarity of all examples shown here is drawnfrom annnotations in the MPQA corpus.
Clearly the assign-ment of polarity chosen in this corpus depends on generalcultural norms.24et al, 2005).
It contains numeric scores as-signed along axes of pleasantness, activenessand concreteness.
We introduce a method forsetting numerical priors on words using thesethree axes, which we refer to as a ?scoringscheme?
throughout the paper.
This schemehas high coverage of the phrases for classi-fication and requires no manual interventionwhen tagging words with prior polarities.?
N-gram Analysis: exploiting automaticallyderived polarity of syntactic constituentsWe compute polarity for each syntactic con-stituent in the input phrase using lexical af-fect scores for its words and extract n-gramsover these constituents.
N-grams of syntacticconstituents tagged with polarity provide pat-terns that improve prediction of polarity forthe subjective phrase.?
Polarity of Surrounding Constituents: Weuse the computed polarity of syntactic con-stituents surrounding the phrase we want toclassify.
These features help to capture theeffect of context on the polarity of the sub-jective phrase.We show that classification of subjectivephrases using our approach yields better accuracythan two baselines, a majority class baseline and amore difficult baseline of lexical n-gram features.We also provide an analysis of how the differ-ent component DAL scores contribute to our re-sults through the introduction of a ?norm?
thatcombines the component scores, separating polarwords that are less subjective (e.g., Christmas ,murder) from neutral words that are more subjec-tive (e.g., most, lack).Section 2 presents an overview of previouswork, focusing on phrasal level sentiment analy-sis.
Section 3 describes the corpus and the goldstandard we used for our experiments.
In sec-tion 4, we give a brief description of DAL, dis-cussing its utility and previous uses for emotionand for sentiment analysis.
Section 5 presents, indetail, our polarity classification framework.
Herewe describe our scoring scheme and the featureswe extract from sentences for classification tasks.Experimental set-up and results are presented inSection 6.
We conclude with Section 7 where wealso look at future directions for this research.2 Literature SurveyThe task of sentiment analysis has evolved fromdocument level analysis (e.g., (Turney., 2002);(Pang and Lee, 2004)) to sentence level analy-sis (e.g., (Hu and Liu., 2004); (Kim and Hovy.,2004); (Yu and Hatzivassiloglou, 2003)).
Theseresearchers first set priors on words using a priorpolarity lexicon.
When classifying sentiment atthe sentence level, other types of clues are alsoused, including averaging of word polarities ormodels for learning sentence sentiment.Research on contextual phrasal level sentimentanalysis was pioneered by Nasukawa and Yi(2003), who used manually developed patterns toidentify sentiment.
Their approach had high preci-sion, but low recall.
Wilson et al, (2005) also ex-plore contextual phrasal level sentiment analysis,using a machine learning approach that is closer tothe one we present.
Both of these researchers alsofollow the traditional approach and first set priorson words using a prior polarity lexicon.
Wilsonet al (2005) use a lexicon of over 8000 subjec-tivity clues, gathered from three sources ((Riloffand Wiebe, 2003); (Hatzivassiloglou and McKe-own, 1997) and The General Inquirer2).
Wordsthat were not tagged as positive or negative weremanually labeled.
Yi et al (2003) acquired wordsfrom GI, DAL and WordNet.
From DAL, onlywords whose pleasantness score is one standarddeviation away from the mean were used.
Na-sukawa as well as other researchers (Kamps andMarx, 2002)) also manually tag words with priorpolarities.
All of these researchers use categoricaltags for prior lexical polarity; in contrast, we usequantitative scores, making it possible to use themin computation of scores for the full phrase.While Wilson et al (2005) aim at phrasal levelanalysis, their system actually only gives ?eachclue instance its own label?
[p. 350].
Their goldstandard is also at the clue level and assigns avalue based on the clue?s appearance in differentexpressions (e.g., if a clue appears in a mixture ofnegative and neutral expressions, its class is neg-ative).
They note that they do not determine sub-jective expression boundaries and for this reason,they classify at the word level.
This approach isquite different from ours, as we compute the po-larity of the full phrase.
The average length ofthe subjective phrases in the corpus was 2.7 words,with a standard deviation of 2.3.
Like Wilson et al2http://www.wjh.harvard.edu/ inquirer25(2005) we do not attempt to determine the bound-ary of subjective expressions; we use the labeledboundaries in the corpus.3 CorpusWe used the Multi-Perspective Question-Answering (MPQA version 1.2) Opinion corpus(Wiebe et al, 2005) for our experiments.
Weextracted a total of 17,243 subjective phrasesannotated for contextual polarity from the corpusof 535 documents (11,114 sentences).
Thesesubjective phrases are either ?direct subjective?or ?expressive subjective?.
?Direct subjective?expressions are explicit mentions of a private state(Quirk et al, 1985) and are much easier to clas-sify.
?Expressive subjective?
phrases are indirector implicit mentions of private states and thereforeare harder to classify.
Approximately one third ofthe phrases we extracted were direct subjectivewith non-neutral expressive intensity whereas therest of the phrases were expressive subjective.
Interms of polarity, there were 2779 positive, 6471negative and 7993 neutral expressions.
Our GoldStandard is the manual annotation tag given tophrases in the corpus.4 DALDAL is an English language dictionary built tomeasure emotional meaning of texts.
The samplesemployed to build the dictionary were gatheredfrom different sources such as interviews, adoles-cents?
descriptions of their emotions and univer-sity students?
essays.
Thus, the 8742 word dictio-nary is broad and avoids bias from any one par-ticular source.
Each word is given three kinds ofscores (pleasantness ?
also called evaluation, ee,activeness, aa and imagery, ii) on a scale of 1 (low)to 3 (high).
Pleasantness is a measure of polarity.For example, in Table 1, affection is given a pleas-antness score of 2.77 which is closer to 3.0 andis thus a highly positive word.
Likewise, active-ness is a measure of the activation or arousal levelof a word, which is apparent from the activenessscores of slug and energetic in the table.
The thirdscore, imagery, is a measure of the ease with whicha word forms a mental picture.
For example, af-fect cannot be imagined easily and therefore has ascore closer to 1, as opposed to flower which is avery concrete and therefore has an imagery scoreof 3.A notable feature of the dictionary is that it hasdifferent scores for various inflectional forms of aword ( affect and affection) and thus, morphologi-cal parsing, and the possibility of resulting errors,is avoided.
Moreover, Cowie et al, (2001) showedthat the three scores are uncorrelated; this impliesthat each of the three scores provide complemen-tary information.Word ee aa iiAffect 1.75 1.85 1.60Affection 2.77 2.25 2.00Slug 1.00 1.18 2.40Energetic 2.25 3.00 3.00Flower 2.75 1.07 3.00Table 1: DAL scores for wordsThe dictionary has previously been used for de-tecting deceptive speech (Hirschberg et al, 2005)and recognizing emotion in speech (Athanaselis etal., 2006).5 The Polarity Classification FrameworkIn this section, we present our polarity classifi-cation framework.
The system takes a sentencemarked with a subjective phrase and identifies themost likely contextual polarity of this phrase.
Weuse a logistic regression classifier, implementedin Weka, to perform two types of classification:Three way (positive, negative, vs. neutral) andbinary (positive vs. negative).
The features weuse for classification can be broadly divided intothree categories: I.
Prior polarity features com-puted from DAL and augmented using WordNet(Section 5.1).
II.
lexical features including POSand word n-gram features (Section 5.3), and III.the combination of DAL scores and syntactic fea-tures to allow both n-gram analysis and polarityfeatures of neighbors (Section 5.4).5.1 Scoring based on DAL and WordNetDAL is used to assign three prior polarity scoresto each word in a sentence.
If a word is found inDAL, scores of pleasantness (ee), activeness (aa),and imagery (ii) are assigned to it.
Otherwise, alist of the word?s synonyms and antonyms is cre-ated using WordNet.
This list is sequentially tra-versed until a match is found in DAL or the listends, in which case no scores are assigned.
Forexample, astounded, a word absent in DAL, wasscored by using its synonym amazed.
Similarly,in-humane was scored using the reverse polarity of26its antonym humane, present in DAL.
These scoresare Z-Normalized using the mean and standard de-viation measures given in the dictionary?s manual(Whissel, 1989).
It should be noted that in our cur-rent implementation all function words are givenzero scores since they typically do not demonstrateany polarity.
The next step is to boost these nor-malized scores depending on how far they lie fromthe mean.
The reason for doing this is to be ableto differentiate between phrases like ?fairly decentadvice?
and ?excellent advice?.
Without boosting,the pleasantness scores of both phrases are almostthe same.
To boost the score, we multiply it bythe number of standard deviations it lies from themean.After the assignment of scores to individualwords, we handle local negations in a sentence byusing a simple finite state machine with two states:RETAIN and INVERT.
In the INVERT state, thesign of the pleasantness score of the current wordis inverted, while in the RETAIN state the sign ofthe score stays the same.
Initially, the first word ina given sentence is fed to the RETAIN state.
Whena negation (e.g., not, no, never, cannot, didn?t)is encountered, the state changes to the INVERTstate.
While in the INVERT state, if ?but?
is en-countered, it switches back to the RETAIN state.In this machine we also take care of ?not only?which serves as an intensifier rather than nega-tion (Wilson et al, 2005).
To handle phrases like?no better than evil?
and ?could not be clearer?,we also switch states from INVERT to RETAINwhen a comparative degree adjective is found after?not?.
For example, the words in phrase in Table(2) are given positive pleasantness scores labeledwith positive prior polarity.Phrase has no greater desirePOS VBZ DT JJR NN(ee) 0 0 3.37 0.68State RETAIN INVERT RETAIN RETAINTable 2: Example of scoring scheme using DALWe observed that roughly 74% of the contentwords in the corpus were directly found in DAL.Synonyms of around 22% of the words in the cor-pus were found to exist in DAL.
Antonyms ofonly 1% of the words in the corpus were found inDAL.
Our system failed to find prior semantic ori-entations of roughly 3% of the total words in thecorpus.
These were rarely occurring words likeapartheid, apocalyptic and ulterior.
We assignedzero scores for these words.In our system, we assign three DAL scores, us-ing the above scheme, for the subjective phrasein a given sentence.
The features are (1) ?ee, themean of the pleasantness scores of the words in thephrase, (2) ?aa, the mean of the activeness scoresof the words in the phrase, and similarly (3) ?ii,the mean of the imagery scores.5.2 NormWe gave each phrase another score, which we callthe norm, that is a combination of the three scoresfrom DAL.
Cowie et al (2001) suggest a mecha-nism of mapping emotional states to a 2-D contin-uous space using an Activation-Evaluation space(AE) representation.
This representation makesuse of the pleasantness and activeness scores fromDAL and divides the space into four quadrants:?delightful?, ?angry?, ?serene?, and ?depressed?.Whissel (2008), observes that tragedies, whichare easily imaginable in general, have higher im-agery scores than comedies.
Drawing on these ap-proaches and our intuition that neutral expressionstend to be more subjective, we define the norm inthe following equation (1).norm =?ee2 + aa2ii(1)Words of interest to us may fall into the follow-ing four broad categories:1.
High AE score and high imagery: Theseare words that are highly polar and less sub-jective (e.g., angel and lively).2.
Low AE score and low imagery: These arehighly subjective neutral words (e.g., gener-ally and ordinary).3.
High AE score and low imagery: These arewords that are both highly polar and subjec-tive (e.g., succeed and good).4.
Low AE score and high imagery: These arewords that are neutral and easily imaginable(e.g., car and door).It is important to differentiate between thesecategories of words, because highly subjectivewords may change orientation depending on con-text; less subjective words tend to retain their priororientation.
For instance, in the example sentencefrom Wilson et al(2005)., the underlined phrase27seems negative, but in the context it is positive.Since a subjective word like succeed depends on?what?
one succeeds in, it may change its polar-ity accordingly.
In contrast, less subjective words,like angel, do not depend on the context in whichthey are used; they evoke the same connotation astheir prior polarity.
(3) They haven?t succeeded and will never succeedin breaking the will of this valiant people.As another example, AE space scores of good-ies and good turn out to be the same.
What differ-entiates one from the another is the imagery score,which is higher for the former.
Therefore, value ofthe norm is lower for goodies than for good.
Un-surprisingly, this feature always appears in the top10 features when the classification task containsneutral expressions as one of the classes.5.3 Lexical FeaturesWe extract two types of lexical features, part ofspeech (POS) tags and n-gram word features.
Wecount the number of occurrences of each POS inthe subjective phrase and represent each POS asan integer in our feature vector.3 For each subjec-tive phrase, we also extract a subset of unigram,bigrams, and trigrams of words (selected automat-ically, see Section 6).
We represent each n-gramfeature as a binary feature.
These types of featureswere used to approximate standard n-gram lan-guage modeling (LM).
In fact, we did experimentwith a standard trigram LM, but found that it didnot improve performance.
In particular, we trainedtwo LMs, one on the polar subjective phrases andanother on the neutral subjective phrases.
Given asentence, we computed two perplexities of the twoLMs on the subjective phrase in the sentence andadded them as features in our feature vectors.
Thisprocedure provided us with significant improve-ment over a chance baseline but did not outper-form our current system.
We speculate that thiswas caused by the split of training data into twoparts, one for training the LMs and another fortraining the classifier.
The resulting small quantityof training data may be the reason for bad perfor-mance.
Therefore, we decided to back off to onlybinary n-gram features as part of our feature vec-tor.3We use the Stanford Tagger to assign parts of speech tagsto sentences.
(Toutanova and Manning, 2000)5.4 Syntactic FeaturesIn this section, we show how we can combine theDAL scores with syntactic constituents.
This pro-cess involves two steps.
First, we chunk eachsentence to its syntactic constituents (NP, VP,PP, JJP, and Other) using a CRF Chunker.4 Ifthe marked-up subjective phrase does not containcomplete chunks (i.e., it partially overlaps withother chunks), we expand the subjective phrase toinclude the chunks that it overlaps with.
We termthis expanded phrase as the target phrase, see Fig-ure 1.Second, each chunk in a sentence is then as-signed a 2-D AE space score as defined by Cowieet al, (2001) by adding the individual AE spacescores of all the words in the chunk and then nor-malizing it by the number of words.
At this point,we are only concerned with the polarity of thechunk (i.e., whether it is positive or negative orneutral) and imagery will not help in this task; theAE space score is determined from pleasantnessand activeness alone.
A threshold, determinedempirically by analyzing the distributions of posi-tive (pos), negative (neg) and neutral (neu) expres-sions, is used to define ranges for these classes ofexpressions.
This enables us to assign each chunka prior semantic polarity.
Having the semantic ori-entation (positive, negative, neutral) and phrasaltags, the sentence is then converted to a sequenceof encodings [Phrasal ?
Tag]polarity.
We markeach phrase that we want to classify as a ?target?
todifferentiate it from the other chunks and attach itsencoding.
As mentioned, if the target phrase par-tially overlaps with chunks, it is simply expandedto subsume the chunks.
This encoding is illus-trated in Figure 1.After these two steps, we extract a set of fea-tures that are used in classifying the target phrase.These include n-grams of chunks from the allsentences, minimum and maximum pleasantnessscores from the chunks in the target phrase itself,and the syntactic categories that occur in the con-text of the target phrase.
In the remainder of thissection, we describe how these features are ex-tracted.We extract unigrams, bigrams and trigrams ofchunks from all the sentences.
For example, wemay extract a bigram from Figure 1 of [V P ]neufollowed by [PP ]targetneg .
Similar to the lexical4Xuan-Hieu Phan, ?CRFChunker: CRF English PhraseChunker?, http://crfchunker.sourceforge.net/, 2006.28??
???????
??
??
??
??
????????????????????????????????
??????????????????????
?
?
?
?
?Figure 1: Converting a sentence with a subjective phrase to a sequence of chunks with their types and polaritiesn-grams, for the sentence containing the targetphrase, we add binary values in our feature vec-tor such that the value is 1 if the sentence containsthat chunk n-gram.We also include two features related to the tar-get phrase.
The target phrase often consists ofmany chunks.
To detect if a chunk of the targetphrase is highly polar, minimum and maximumpleasantness scores over all the chunks in the tar-get phrase are noted.In addition, we add features which attempt tocapture contextual information using the prior se-mantic polarity assigned to each chunk both withinthe target phrase itself and within the context of thetarget phrase.
In cases where the target phrase isin the beginning of the sentence or at the end, wesimply assign zero scores.
Then we compute thefrequency of each syntactic type (i.e., NP, VP, PP,JJP) and polarity (i.e., positive, negative, neutral)to the left of the target, to the right of the targetand for the target.
This additional set of contextualfeatures yields 36 features in total: three polari-ties: {positive, negative, neutral} * three contexts:{left, target, right} * four chunk syntactic types:{NP, VP, PP, JJP}.The full set of features captures different typesof information.
N-grams look for certain patternsthat may be specific to either polar or neutral senti-ments.
Minimum and maximum scores capture in-formation about the target phrase standalone.
Thelast set of features incorporate information aboutthe neighbors of the target phrase.
We performedfeature selection on this full set of n-gram relatedfeatures and thus, a small subset of these n-gramrelated features, selected automatically (see sec-tion 6) were used in the experiments.6 Experiments and ResultsSubjective phrases from the MPQA corpus wereused in 10-fold cross-validation experiments.
TheMPQA corpus includes gold standard tags for eachFeature Types Accuracy Pos.
* Neg.
* Neu.
*Chance baseline 33.33% - - -N-gram baseline 59.05% 0.602 0.578 0.592DAL scores only 59.66% 0.635 0.635 0.539+ POS 60.55% 0.621 0.542 0.655+ Chunks 64.72% 0.681 0.665 0.596+ N-gram (all) 67.51% 0.703 0.688 0.632All (unbalanced) 70.76% 0.582 0.716 0.739Table 3: Results of 3 way classification (Positive, Negative,and Neutral).
In the unbalanced case, majority class baselineis 46.3% (*F-Measure).Feature Types Accuracy Pos.
* Neg.
*Chance baseline 50% - -N-gram baseline 73.21% 0.736 0.728DAL scores only 77.02% 0.763 0.728+ POS 79.02% 0.788 0.792+ Chunks 80.72% 0.807 0.807+ N-gram (all) 82.32% 0.802 0.823All (unbalanced) 84.08% 0.716 0.889Table 4: Positive vs.
Negative classification results.
Baselineis the majority class.
In the unbalanced case, majority classbaseline is 69.74%.
(* F-Measure)phrase.
A logistic classifier was used for two po-larity classification tasks, positive versus negativeversus neutral and positive versus negative.
Wereport accuracy, and F-measure for both balancedand unbalanced data.6.1 Positive versus Negative versus NeutralTable 3 shows results for a 3-way classifier.
Forthe balanced data-set, each class has 2799 in-stances and hence the chance baseline is 33%.
Forthe unbalanced data-set, there are 2799 instancesof positive, 6471 instances of negative and 7993instances of neutral phrases and thus the baselineis about 46%.
Results show that the accuracy in-creases as more features are added.
It may beseen from the table that prior polarity scores donot do well alone, but when used in conjunctionwith other features they play an important rolein achieving an accuracy much higher than bothbaselines (chance and lexical n-grams).
To re-29Figure 2: (a) An example sentence with three annotated subjective phrases in the same sentence.
(b) Part of the sentence withthe target phrase (B) and their chunks with prior polarities.confirm if prior polarity scores add value, we ex-perimented by using all features except the priorpolarity scores and noticed a drop in accuracy byabout 4%.
This was found to be true for theother classification task as well.
The table showsthat parts of speech and lexical n-grams are goodfeatures.
A significant improvement in accuracy(over 4%, p-value = 4.2e-15) is observed whenchunk features (i.e., n-grams of constituents andpolarity of neighboring constituents) are used inconjunction with prior polarity scores and part ofspeech features.5 This improvement may be ex-plained by the following observation.
The bi-gram ?
[Other]targetneu [NP ]neu?
was selected as atop feature by the Chi-square feature selector.
Sowere unigrams, [Other]targetneu and [Other]targetneg .We thus learned n-gram patterns that are char-acteristic of neutral expressions (the just men-tioned bigram and the first of the unigrams) aswell as a pattern found mostly in negative ex-pressions (the latter unigram).
It was surpris-ing to find another top chunk feature, the bigram?
[Other]targetneu [NP ]neg?
(i.e., a neutral chunk ofsyntactic type ?Other?
preceding a negative nounphrase), present in neutral expressions six timesmore than in polar expressions.
An instance wherethese chunk features could have been responsi-ble for the correct prediction of a target phrase isshown in Figure 2.
Figure 2(a) shows an exam-ple sentence from the MPQA corpus, which hasthree annotated subjective phrases.
The manuallylabeled polarity of phrases (A) and (C) is negativeand that of (B) is neutral.
Figure 2(b) shows the5We use the binomial test procedure to test statistical sig-nificance throughout the paper.relevant chunk bigram which is used to predict thecontextual polarity of the target phrase (B).It was interesting to see that the top 10 featuresconsisted of all categories (i.e., prior DAL scores,lexical n-grams and POS, and syntactic) of fea-tures.
In this and the other experiment, pleasant-ness, activation and the norm were among the top5 features.
We ran a significance test to show theimportance of the norm feature in our classifica-tion task and observed that it exerted a significantincrease in accuracy (2.26%, p-value = 1.45e-5).6.2 Positive versus NegativeTable 4 shows results for positive versus negativeclassification.
We show results for both balancedand unbalanced data-sets.
For balanced, there are2779 instances of each class.
For the unbalanceddata-set, there are 2779 instances of positive and6471 instances of neutral, thus our chance base-line is around 70%.
As in the earlier classification,accuracy and F-measure increase as we add fea-tures.
While the increase of adding the chunk fea-tures, for example, is not as great as in the previousclassification, it is nonetheless significant (p-value= 0.0018) in this classification task.
The smallerincrease lends support to our hypothesis that po-lar expressions tend to be less subjective and thusare less likely to be affected by contextual polar-ity.
Another thing that supports our hypothesis thatneutral expressions are more subjective is the factthat the rank of imagery (ii), dropped significantlyin this classification task as compared to the previ-ous classification task.
This implies that imageryhas a much lesser role to play when we are dealingwith non-neutral expressions.307 Conclusion and Future WorkWe present new features (DAL scores, normscores computed using DAL, n-gram over chunkswith polarity) for phrasal level sentiment analysis.They work well and help in achieving high accu-racy in a three-way classification of positive, neg-ative and neutral expressions.
We do not requireany manual intervention during feature selection,and thus our system is fully automated.
We alsointroduced a 3-D representation that maps differ-ent classes to spatial coordinates.It may seem to be a limitation of our system thatit requires accurate expression boundaries.
How-ever, this is not true for the following two reasons:first, Wiebe et al, (2005) declare that while mark-ing the span of subjective expressions and handannotating the MPQA corpus, the annotators werenot trained to mark accurate expression bound-aries.
The only constraint was that the subjectiveexpression should be within the mark-ups for allannotators.
Second, we expanded the marked sub-jective phrase to subsume neighboring phrases atthe time of chunking.A limitation of our scoring scheme is that itdoes not handle polysemy, since words in DALare not provided with their parts of speech.
Statis-tics show, however, that most words occurred withprimarily one part of speech only.
For example,?will?
occurred as modal 1272 times in the corpus,whereas it appeared 34 times as a noun.
The caseis similar for ?like?
and ?just?, which mostly occuras a preposition and an adverb, respectively.
Also,in our state machine, we haven?t accounted for theimpact of connectives such as ?but?
or ?although?
;we propose drawing on work in argumentative ori-entation to do so ((Anscombre and Ducrot, 1983);(Elhadad and McKeown, 1990)).For future work, it would be interesting to dosubjectivity and intensity classification using thesame scheme and features.
Particularly, for thetask of subjectivity analysis, we speculate that theimagery score might be useful for tagging chunkswith ?subjective?
and ?objective?
instead of posi-tive, negative, and neutral.AcknowledgmentsThis work was supported by the National ScienceFoundation under the KDD program.
Any opin-ions, ndings, and conclusions or recommendationsexpressed in this paper are those of the authors anddo not necessarily reect the views of the NationalScience Foundation.
score.We would like to thank Julia Hirschberg for use-ful discussion.
We would also like to acknowledgeNarayanan Venkiteswaran for implementing partsof the system and Amal El Masri, Ashleigh Whiteand Oliver Elliot for their useful comments.ReferencesJ.C.
Anscombre and O. Ducrot.
1983.
Philosophie etlangage.
l?argumentation clans la langue.
Bruxelles:Pierre Mardaga.T.
Athanaselis, S. Bakamidis, , and L. Dologlou.
2006.Automatic recognition of emotionally colouredspeech.
In Proceedings of World Academy of Sci-ence, Engineering and Technology, volume 12, ISSN1307-6884.R.
Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Vot-sis, S. Kollias, and W. Fellenz et al 2001.
Emo-tion recognition in human-computer interaction.
InIEEE Signal Processing Magazine, 1, 32-80.M.
Elhadad and K. R. McKeown.
1990.
Generatingconnectives.
In Proceedings of the 13th conferenceon Computational linguistics, pages 97?101, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.C.
Fellbaum.
1998.
Wordnet, an electronic lexicaldatabase.
In MIT press.V.
Hatzivassiloglou and K. McKeown.
1997.
Predict-ing the semantic orientation of adjectives.
In Pro-ceedings of ACL.J.
Hirschberg, S. Benus, J.M.
Brenier, F. Enos, andS.
Friedman.
2005.
Distinguishing deceptive fromnon-deceptive speech.
In Proceedings of Inter-speech, 1833-1836.M.
Hu and B. Liu.
2004.
Mining and summarizingcustomer reviews.
In Proceedings of KDD.J.
Kamps and M. Marx.
2002.
Words with attitude.
In1st International WordNet Conference.S.
M. Kim and E. Hovy.
2004.
Determining the senti-ment of opinions.
In In Coling.T.
Nasukawa and J. Yi.
2003.
Sentiment analysis:Capturing favorability using natural language pro-cessing.
In Proceedings of K-CAP.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity analysis usingsubjectivity summarization based on minimum cuts.In Proceedings of ACL.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.1985.
A comprehensive grammar of the english lan-guage.
Longman, New York.31E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In Proceedings ofEMNLP.K.
Toutanova and C. D. Manning.
2000.
Enrichingthe knowledge sources used in a maximum entropypart-of-speech tagger.
In Proceedings of the JointSIGDAT Conference on Empirical Methods in Nat-ural Language Processing and Very Large Corpora(EMNLP/VLC-2000), pp.
63-70.P.
Turney.
2002.
Thumbs up or thumbs down?
seman-tic orientation applied to unsupervised classificationof reviews.
In Proceedings of ACL.C.
M. Whissel.
1989.
The dictionary of affect in lan-guage.
In R. Plutchik and H. Kellerman, editors,Emotion: theory research and experience, volume 4,Acad.
Press., London.C.
M. Whissell.
2008.
A psychological investiga-tion of the use of shakespeare=s emotional language:The case of his roman tragedies.
In Edwin MellenPress., Lewiston, NY.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.In Language Resources and Evaluation, volume 39,issue 2-3, pp.
165-210.T.
Wilson, J. Wiebe, and P. Hoffman.
2005.
Recog-nizing contextual polarity in phrase level sentimentanalysis.
In Proceedings of ACL.J.
Yi, T. Nasukawa, R. Bunescu, and W. Niblack.
2003.Sentiment analyzer: Extracting sentiments about agiven topic using natural language processing tech-niques.
In Proceedings of IEEE ICDM.H.
Yu and V. Hatzivassiloglou.
2003.
Towards an-swering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of EMNLP.32
