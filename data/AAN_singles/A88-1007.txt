IMPROVED PORTABIL ITY  AND PARSING THROUGHINTERACTIVE  ACQUIS IT ION OF SEMANTIC  INFORMATION tFrancois-Michel Lang and Lynette HirschmanPaoli Research Center, UNISYSP.
O.
Box 517, Paoli, PA 19301ABSTRACTThis paper presents SPQR (Selectional Pat-tern Queries and Responses), a module of thePUNDIT text-processing system designed to facili-tate the acquisition of domain-specific semanticinformation, and to improve the accuracy andefficiency of the parser.
SPQR operates byinteractively and incrementally collecting informa-tion about the semantic acceptabil ity of certainlexical co-occurrence patterns (e.g., subject-verb-object) found in partial ly constructed parses.
Themodule has proved to be a valuable tool for port-ing PUNDIT to new domains and acquiring essen-tial semantic information about the domains.Preliminary results also indicate that SPQRcauses a threefold reduction in the number ofparses found, and about a 40~ reduction in totalparsing time.1.
INTRODUCTIONA major concern in designing a natural-language system is portability: It is advantageousto design a system in such a way that it can beported to new domains with a minimum of effort.The level of effort required for such a port is con-siderably simplified if the system features a highdegree of modularity.
For example, if thedomain-independent and domain-specific com-ponents of a system are clearly factored, only thedomain-specific knowledge bases need be changedwhen porting to a new domain.
Even if a systemdemonstrates such separation, however, the prob-lem remains of acquiring this domain-specifictThis work has been supported in part by DARPA undercontract N00014-85-C-0012, administered by the Office of Na-val Research, and in part by National Science Foundationcontract DCR-85-02205, as well as by Independent R~D fund-ing from System Development Corporation, now part of UnisysCorporation.knowledge.One obvious benefit of acquiring domain-specific semantic information is rejecting parsesgenerated by the syntactic component which aresemantically anomalous.
Using domain knowledgeto rule out semantically anomalous parses is espe-cially important when parsing with large, broad-coverage grammars such as ours: Our Prologimplementation of Restriction Grammar~-Iirschman1982,Hirschman1985\] includes about100 grammar rules and 75 restrictions, and isbased on Sager's Linguistic String Grammar\[Sager1981\].
It also includes a full treatment ofsentential fragments and telegraphic messagestyle.
As a result of this extended coverage, manysentences receive numerous yntactic analyses.
Amajority of these analyses, however, are incorrectbecause they violate some semantic onstraint.Let us take as an example the sentence Highlsbe oil temperatsre belle~ed contribstor tosnlt failure.
Two of the parses for this sentencecould be paraphrased as:(1) The high lube oil temperature believed thecontributor to the unit failure.
(2) The high lube oil temperature was believedto be a contributor to the unit failure.but our knowledge of the domain (and commonsense) tells us that  the first parse is wrong, sincetemperatures cannot hold beliefs.It is only because of this semantic informa-tion that we know that parse (2) is correct, andthat parse (1) is not, since we cannot rule outparse (1) on syntactic grounds alone.
In fact, ourgrammar generates the incorrect parse before thecorrect one, since it produces full assertion parsesbefore fragment parses.
If the syntactic com-ponent has access to semantic knowledge,49however, many incorrect parses such as (1) willnever be generated.How then can we collect the necessarysemantic information about a domain?
One tradi-tional approach involves analysing a corpus oftexts by hand, or perhaps even simply relying onone's intuitive knowledge of the domain in orderto gather information about what relations canhold among domain entities.
Several obviousdrawbacks to these approaches are that  they aretime-consuming, error-prone, and incomplete.
Amore robust approach would be to use (semi-)automated tools designed to collect such informa-tion by cataloguing selectional patterns found incorrect parses of sentences.However, our reasoning appears circular:The desired domain-specific information can onlybe obtained from analyses of correctly parsed sen-tences, but our goal is to restrict the parser tothese correct analyses precisely by using thisdomain knowledge.
In the example above, weneed the semantic knowledge to rule out the firstparse; but it is only by knowing that this parse issemantically anomalous that  we can obtain theselectional information about the domain.One way to avoid this circularity is tobootstrap into a state of increasingly completedomain knowledge.
We have implemented inSPQR such a bootstrapping process by incremen-tally collecting and ~toring domaln-specific datagathered through interaction with the user.
Thedata are in the form of selectlonal constraintsexpressed as allowable and unallowable syntacticco-occurrence patterns.
All the data collectedwhile parsing a set of sentences can then be usedto help guide the parser to correct analyses and todecrease the search space traversed uring futureparsing.
As the system's semantic knowledgebecomes increasingly rich, we can expect it todemonstrate some measure of learning, since it willproduce fewer incorrect analyses and present fewerqueries to the user about the validity of syntacticpatterns.A number of systems have been developed toassist the user in acquiring domaln-speclficknowledge, including TELI ~allard1986\], TEAM\[Cross1083\], KLAUS ~-Iendrix1980\], ASK \[Thomp-son1983\] and \[Thompson1985\], TQA paN-erau1985\] and IRACQ ~V\[oser1984,Ayuso1987\].Related work has also been reported in \[Tom-ita1984\], as well as in \[Grishman1986\] and~-Iirschman1986a\].
The work described here differsfrom these previous efforts in several ways: 1?
Since PUNDIT is not a natural- languageinterface or a database front end, but rathera full text-processing system, sentencesanalysed by PUNDIT are taken from cor-puses of naturally-occurring texts.
Thesemantic information gathered is thereforeempirically or statistically based, and notderived from sentences generated by a user.?
The ellcltation of information from the userfollows a highly structured, data-drivenapproach, yielding results which should bemore reproducible and consistent amongusers.?
IV\[any systems have a clearly definedknowledge-acquisltlon phase which must becompleted before the system can beeffectively used or tested.
We have choseninstead to adopt a paradigm of incrementalknowledge acquisition.Our incremental approach is based on theassumption that gathering complete knowledgeabout domain is an unattainable ideal, especiallyfor a system which performs in-depth analysis oftexts written in technical sublanguages: Even ifone could somehow be assured of acquiring all con-ceivable knowledge about a domain, the system'somniscience would be transient, since the technicalfields themselves are constantly changing, andthus require modifications to one's knowledge base.An incremental acquisition method thereforeallows us to start  from an essentially emptyknowledge base.
Each sentence parsed can addinforr~.atlon about the domain, and the systemthereby effectively bootstraps itself until itsknowledge about the selectional patterns in adomain approaches completeness.In this paper we present SPQR, the com-ponent of the PUNDIT 2 text-understanding systemwhich is designed to acquire domain-specific selec-tional information ~ang1987\].
We present in Sec-tion 2 the methodology we have adopted to collectand use selectional patterns, and then give in Sec-tion 3 some examples of the operation of our1See \[Ballard1986\] for a detailed and informative com-parison of TELI, TEAM, IRACQ, T(~A, and ASK.=PUNDIT (Prolog UNDerstands Integrated Text) is im-plemented in Quintus Prolog, and has been described in\[Hirschman1985\] and \[Hirschman1988b\] (syntax), \[Palmer19861(semantics), \[Dah119861 (discourse), and \[Passonneau1988\] (tem-poral analysis).50module.
We conclude by presenting some experi-mental results and discussing some future plans toextend the module.SPQR has been used in analysing texts inthree domains: casualty reports (CASREPs) deal-ing with mechanical failures of starting aireompreuorJ (SACs are a component of a ship'sengine), queries to a Navy ships database, andNavy sighting messages (RAINFORMs).2.
METHODOLOGYThe essential feature of our parser whichfacilitates the collecting of syntactic patterns isthe INTERMEDIATE SYNTACTIC REPRESENTATION(ISR) produced by the syntactic analyser.
TheISR is the result of regularizing the surface syntac-tic structure into a canonical form of operatorsand arguments.
Since there are only a limitednumber of structures which can appear in an ISR,we have been able to write a program to analyzethe ISR and examine the syntactic patterns asthey are generated.\ [past , repa i r ,\ [ tpos ( the) ,\[nvar(\[engineer,singular,_\])\]\],\[tpos(the),\[nvar(\[sac,singular,_\])\],adj(\[pastpart,break\])\]\]Figure 1: ISR for the sentenceThe engi,eer epaired the broke, sacA brief note about the implementation: Sincethe ISR is represented as a Prolog llst, the pro-gram which analyzes it was written as a definite-clause grammar and has the flavor of a smallparser.
As a sample ISR, we present in Figure 1the regularized representation of the obvious parsefor the sentence The englneer epaired the bro-ke~ mac (pretty-printed for clarity).
At the toplevel, the ISR consists of the main verb (precededby its tense operators), followed by its subject andobject.
The ISR of a noun phrase contains firstthe determiner (labelled TPOS), then the headnoun, (the label NVAR stands for "noun or vari-ant"), and finally any nominal modifiers.
Notethat part of the regularisation performed by theISR is morphological, since the actual lexical itemsappearing in the ISR are represented by their rootforms.
Hence broken in the input sentence is reg-ularized to break in the ISR, and rep41red in theinput sentence appears in the ISR simply asrepair.SPQR is invoked by two restrictions whichare called after the BNF grammar has assembleda complete NP (and constructed the ISR for thatNP), and after it has assembled a complete sen-tence (and constructed its ISR).
The programoperates by presenting to the user a syntactic pat-tern (either a head-modifier pattern or apredicate-argument pattern) found in the ISR, andquerying hlm/her about the acceptabil ity of thatpattern.For each of the basic types of patterns whichthe program currently generates, the chart inTable 1 shows that pattern's components, anexample of that pattern, and a sentence in whichthe pattern occurs.
When presented with a syn-tactic pattern such as those in the chart in Table1, the user can respond to the query in one of twoways, depending on the semantic compatibil ity ofthe predicate and arguments (e.g., in the case ofan SVO pattern) or of the head and modifiers (e.g.,for an ADJ pattern) contained in the pattern.
Ifthe pattern describes a relationship that can besaid to hold among domain entitles (i.e., if thepattern occurs in the sublanguage), the useraccepts the pattern, thereby classifying it as good.The analysis of the ISR and the parsing of thesentence are then allowed to continue.
If, how-ever, the pattern describes a relationship amongdomain entities that is not consistent with theuser's domain knowledge or with his/her prag-matic knowledge (i.e., if the pattern cannot ordoes not occur in the sublanguage) the user rejectsit, thereby classifying it as bad, and signalling anincorrect parse.
This response causes the restric-tion which checks selection to fail, and as a result,the parse under construction is immediately failed,and the parser backtracks.As the user classifies these co-occurrence pat-terns into good patterne and bad patterns, theyare stored in a pattern database which is con-suited before any query to the user is made.
Thus,once a pattern has been classified as good or bad,the user is not asked to classify it again.
If a pat-tern previously classified as bad by the user isencountered in the course of analyzing the ISR,SPQR consults the database, recognizes that thepattern is bad, and automatically fails the parsebeing assembled.
Similarly, if a pattern previously51TABLE 1: Select ional  Pat te rnsPATTERN COMPONENTS EXAMPLE(1) SVO subject, main verb, object inspection reveal particle\ [NSPECTION o~ iube oll ~lter REYEALED metal PARTICLES.
(2) ADJ adjective, head* normal pressureTroxblesbootlng re~ealed NORMAL sac lxbe oil PRESSURE.
(3) ADV head, adverb decrease rapidlySac air pressure DECREASED RAP IDLY  to 5.7~ psi.
(4) CONJ conjunct1, conjunction, conjunct2 pressure and temperatureTroublemhooting rewealed normal PRESSURE AND TEMPERATURE.
(5) NOUN-NOUN noun modifier, headVAL V~ PARTS ezeeesi, e/lr ?orroded.
(6) PREP head, prep, objectDIHENGA GED immedlatel I AFTER ALARM.
(7) PREDN noun, predicate nominalAlarm CAPABIL ITY  is a NECESSITY.valve partdisengage after alarmcapability necessity*We use "head" throughout the chart to denote the head of a construction i  which a modifierappears.
The head can simply be thought of as that word which the modifier modifies.recorded as good is encountered, SPQR will recog-nine that the pattern is good simply by consultingthe database, and allow the parsing to proceed.The selectional mechanism as described sofar deals only with lexical patterns (i.e., patternsinvolving specific lexical items appearing in thelexicon).
However, we have implemented amethod of generalising these patterns by usinginformation taken from the domain isa(generallzation/specialization) hierarchy to con-struct semantic class patterns from the lexicalpatterns.
After deciding whether a given patternis good or bad, the user is asked if the relationdescribed by the pattern can be generalized.
Inpresenting this second query, SPQR shows the userall the super-concepts of each word appearing inthe pattern, and asks for the most general super-concept(s), if any, for which the relation holds.Let us take as an example the noun-nounpattern generated by the compound nominal ollpressure.
~Vhlle parsing a sentence containingthis expression, the user would accept the noun-noun pattern \[oil, presexre\].
The program willthen show the user in hierarchically ascendingorder all the generalizations for oil (fllld,ph~aieal_obieet, and root_eo~eept), and all thegeneralizations for premesre (mcalar_qxa~tlty,obiect_property, abatract_obyect, and againroot_?o~cept).
The user can then identify whichof those super-concepts of oll and preuxre canform a semantically acceptable compound nomi-nal.
In this case, the correct generalization wouldbe \[fluid, aealar_qxantity\], because?
The fluids in the domain are oil, air, andwater; the scalar quantities are pressure andtemperature; and it is consistent with thedomain to speak of the pressure and thetemperature of oil, air, and water.?
We cannot generalize higher than fluid sinceit would be semantically anomalous to speakof "physical_object pressure" for everyphyaieal_ob~eet in the domain (e.g., one52would not speak of eonneetlng_pin pres-sure or gearboz pressure).?
We cannot generalize higher than pressuresince aKape is also an objeet_propertT, andit would be infelicitous to speak of ollsKape.As with the lexical-level patterns, the user'sgeneralizations are stored for reference in evaluat-ing patterns generated by other sentences.
Theobvious advantage of storing not just lexical pat-terns but also semantic patterns is the broadercoverage of the latter: Knowing that  the semanticclass pattern ~nld, pressure\] is semanticallyacceptable provides much more information thanknowing only that  the lexical pattern loll, pres-sure\] is good.g.
SOME (S IMPL IF IED)  E IAMPLESAs we mentioned earlier, multiple syntacticanalyses which can only be disamhiguated byusing semantic information abound in our corpusesbecause of the telegraphic and fragmentary natureof our texts.
This ambiguity has two principalcauses:(1) A sentence which parses correctly as a frag-ment can often be parsed as a full assertionas well.
(2) Determiners are often omitted from our sen-tences, thus making it difficult to establishNP boundaries.Since such syntactical ly degenerate sentenceswill generally contain fewer syntactic markersthan full, non-telegraphic English sentences, theyare characterized by correspondingly greaterambiguity.
We now present an example of the useof selection to rule out a semantically anomalousassertion parse in favor of a correct fragmentreading, s Consider the sentence Loss of secondinstalled sac.
In the correct analysis, the sen-tence is parsed is a noun string fragment; however,another reading is available in which the sentenceis analyzed as a full assertion, with loss of secondas the subject, installed as main verb, and sac asdirect object.
A paraphrase of this parse might beThe loss of a second installed the sac.
But thisanalysis is semantically completely anomalous forseveral reasons, but most notably because ittin this simplified explanation, we present only the svopattern.
In actual parsing of this sentence, however, addition-al patterns would be generated from the NP level.makes no sense to say that  the loss of a secondcan cause a sac (or anything else) to be installed.Since our parser tries assertion parses before frag-ment parses, the incorrect reading of this sentenceis produced first.
In generating the assertionparse, the parser encounters the svo  pattern \[loss,install, sae\], and queries the user as follows:-SVO-  pat tern  : l oss  ins ta l l  sacThis query asks if a loss can install a sac in thisdomain, or if a domain expert would ever speak ofa loss installing a sac.
Since it is nonsensical tospeak of a loss installing a sac, the correctresponse to SPQR's query in this case is to rejectthe pattern,  causing the assertion parse to failafter the module elicits the appropr iate generali-zations of the pattern.In order to generalize the pattern,  the user isshown all the the super-ordinates of toes and sac,and asked to generalize the anomalous SVO pat-tern lion, install, sae\].
The super-concepts ofloss are /allure, problem, scent, abstract_object,and root_concept.
The super-concepts of sac areunit, meehanical_dewiee, system_component,pAysleal_objeet, and root_concept.
Since noth-ing that  is an abstract  object can install anythingat all, the correct generalization would be\[abstract_object, install, root_concept\].
Sincethe user's response to the original prompt labelledthe pattern as bad, the assertion parse under con-struction then fails, and the parser backtracks.An especially convoluted example ofassertion-fragment ambiguity is found in the sen-tence Ezperieneed frequent /oases of pressurefollowing clutch engage command.
In thecorrect reading (which is again a fragment), thesubject is elided, the main verb is effiperlenesd,and the direct object is the frequent losses ofpressure (in this parse, following eluteA engagecommand functions as a sentence adjunct, withclutch engage command as a compound nominal).However, in another reading generated by ourparser, the subject is experienced frequent lossesof pressure following clutch, the main verb isengage, and command is the direct object.
Thisreading would fail selection at the SVO level (if notsooner) because the svo pattern \[loss, engage,53TABLE 2: S ta t i s t i ca l  Summary  o f  31 SentencesPARSING INFORMATION# of sentences receivinga correct parse# of sentences receivinga correct F IRST parse# of sentences receivingmore than one parseWITH SPQR3130W/OUT SPQR29178 22average # of parses foundper sentence 1.45 4.66average correct parse number* 1.10 2.4519.80 24.4838.67 51.74average search focusto reach correct parseaverage search focusin generating all parsesaverage time taken (seconds)to reach correct parse taverage time taken {seconds)in generating all parses t35.92 56.1881.63 125.94SEARCH FOCUS RATIO TO CORRECT PARSE = 0.80{with SPQR / without SPQR)SEARCH FOCUS RATIO TO ALL PARSES = 0.75(with SPQR / without SPQR)T IMING RATIO TO CORRECT PARSE = 0.64T IMING RATIO TO COMPLET ION = 0.85NEW CORRECT PARSES FOUND USING SPQR = 2NEW CORRECT F IRST PARSES FOUND USING SPQR = 13*That is, which parse, on the average, was the correct one.tSPQR has not yet been optimized.command\] is anomalous for two reasons: The sub-ject of engage cannot be an abstract  concept suchas loam, and the object of enrage must be amachine part .4.
EXPERIMENTAL  RESULTSThe experimental results we present here arebased on a sample of 31 sentences from one of ourCASREP corpuses, each of which was parsed withand without invoking SPQR.
We compare resultsobtained without using the selectional module toresults obtained with the parser set to query theuser about selectional patterns (starting from anempty pattern database).
The chart in Table 2summarizes the results for the 31 sentences.54One of the statistics presented in Table 2 isthe SEARCH FOCUS, which is a measure of theefficiency of the parser in either reaching thecorrect parse of a sentence, or generating all possi-ble parses.
It is equal to the ratio of the number ofnodes attached to the parse tree in the course ofparsing (and possibly detached upon backtrack-ing), 4 to the number of nodes in the completed,correct parse tree.
Thus a search focus of 1.0 inreaching the correct parse would indicate that  forevery (branching) grammar  rule tried, the firstoption was the correct one, or, in other words,that  the parser had never backtracked.The first line of the Table 2 chart deservessome explanation.
One might wonder how amechanism designed in part  to rule oat parsescan actually prodsce  a correct analysis for a sen-tence where none had been available without themodule.
The explanation is the COMMITTED DIS-JUNCTION mechanism we have implemented in ourparser in order to reduce the (often spurious)ambiguity caused by allowing both full sententialand fragmentary readings.
This pruning of thesearch space is most apparent  when the parser isturned loose and set to generate all possibleparses, as it was when we gathered the statisticssummarized above.
Recall that  our parser triesfull assertion parses before fragment parses.
Theeffect of the COMMITTED DISJUNCTION mechanismis to commit the parser to produce only assertionparses (and no fragment parses) if an assertionparse is found.
Fragment parses are tried only ifno assertion parse is available.
Thus no fragmentreading will ever be generated for a sentencewhich can be analysed as both an assertion and afragment.
This has proved to be the correctbehavior in a major i ty of the texts we haveanalysed.
However, a fragment which can also beanalysed as an assertion will never receive acorrect parse unless all assertion parses can beblocked using selection.
Thus it is possible forselection to make available a correct syntacticanalysis where none would be available withoutselection.5.
FUTURE PLANSOur ult imate goal is to integrate SPQR withthe domain model and the semantic componentthat  maps syntactic constituents into predicates4Another way to interpret this figure is that itrepresents the number of grammar rules tried.and associated thematic roles.
At present, thesecomponents are developed independently.
Ouraim is to link these components in order to main-tain consistency and faci l itate updating the sys-tem.
For example, if semantic rules exist to fillthematic roles of a given predicate, we should beable to derlwe a set of "surface" selectional pat-terns consistent with the underlying semantics.Similarly, given a set of selectional patterns, weshould be able to suggest a (set of) semanticrule(s) consistent with the observed selection.
Inaddition, if a word encountered in parsing is notrepresented in the domain model, it should be pos-sible to suggest where the word should fit in themodel, based on similarity to previously observedpatterns.
If, for example, in the CASREP domain,we encounter a sentence such as The widgetbroke, but widgets do not appear  in our domainmodel, the system would check for any patterns ofthe form iX, break\]; if it finds such a pattern,  e.g.,\[machine_part, break\], the system can then sug-gest that  widget be classified as a machine part  inthe domain model.
If the user concurs, widgetwould then automatical ly be entered into themodel.In addition to the above work, which isalready underway, we plan to improve the userinterface, to measure the rate at which selectlonalpatterns are acquired, and to investigate the useof selectio~al patterns in developing a weightingalgorithm based on frequency of occurrence in thedomain.5.1.
The  User  In ter faeeIn the current implementation, the questionswhich the program asks the user are phrased interms of grammatica l  categories, and are thustailored to users who know what is meant by suchterms as "svo"  and "noun-noun compounds".
Asa result, only linguists can be reasonably expectedto make sense of the questions and provide mean-ingful answers.
Our intended users, however, arenot linguists, but rather domain experts who willknow what can and cannot be said in the sub-language, but who cannot be expected to reason interms of grammatica l  categories.
Deciding how tophrase questions designed to elicit the desiredinformation is a difficult problem.
Our firstat tempt will be to paraphrase the pattern.
E.g.,for the svo pattern \[Iou t instal\[, aac\], the queryto the user would be something like "Can a lossinstall a sac?
".SSWe are also examining what kind ofknowledge the user must draw upon in order toanswer the system's questions.
Users' answers areusually based on a combination of commonsenseknowledge (e.g., losses cannot install things) anddomain-specific information.
In certain cases,however, the user can be called upon to make finelinguistic distinctions.
For example, in the sen-tence Sac disengaged immediately after alarm,does the adverb immediately modify the verbdimengaged, or the prepositional phrase afteralarm?
Most users, and even trained linguistsfamiliar with the domain, find it difficult to pro-vide definitive answers to such questions, becausethere is often no definitively correct answer.
Inthis case, the adverbial attachment would seem tobe genuinely ambiguous.
It would be helpful torecognise patterns which a user cannot be reason-ably expected to pass judgment on, and not gen-erate queries about these, perhaps allowing themto succeed by default.8.2 .
Measur ing  the  System's  Learn ingAs more sentences are parsed and more pat-terns are classified, we can expect the system togrow "smarter" in the sense that it will ask theuser increasingly fewer questions.
Eventually, thesystem should reach a state of reasonably com-plete domain knowledge, at which time fewunknown patterns would be encountered, and theuser would almost never be queried.
We do notknow how many sentences SPQR would have toexamine before attaining such a plateau, but anestimate would be in the range of 500 to 1000\[Grishman1986\].
We plan to measure the decreasein the frequency of queries to the user as a func-tion of the number of sentences parsed and thenumber of patterns collected in order to evaluatethe system's learning.
This will enable us todetermine the feasibility of using this technique tobootstrap into a new domain.8.3 .
Pre ference-Based  Pars ingA long-term goal is to implement a parsingalgorithm based on preference rather than on thecurrent success/failure paradigm.
This wouldallow the system to use statistical information onthe frequency of observed patterns as one factor inweighting.
Frequently occurring patterns wouldbe assigned greater weight than unknown pat-terns, and bad patterns would detract from theoverall weighting.
This would allow the system tomake intelligent "guesses" about parsing withoutconstantly querying the user.ACKNOWLEDGMENTSWe would like to thank Marcia Linebargerfor her helpful comments and insightful sugges-tions.REFERENCES\[Ayuso1987\]Damaris M. Ayuso, Varda Shaked, andRalph M. Weischedel, An Environmentfor Acquiring Semantic Information.
InProeeedlnfe of the ?Jtk Annual Meet-ing of the Auoeiat lon for Computa-tional Linguimtiee, Stanford, Califor-nia, July 1987, pp.
32-40.~allard198e\]Bruce W. Ballard and Douglas E. Stum-berger, Semantic Acquisition in TELI:A Transportable, User-Customised Na-tural Language Processor.
In Proeeed-inge of the ?~tlt Annual Meeting ofthe Asmoeiation for Compst-tionalLinf,tlmtlem, New York, N'Y, July 1986,pp.
20-29.~Dah11986\]Deborah A. Dahl, Focusing and Refer-ence Resolution in PUNDIT.
InProeeedlnfe of the Fifth NationalConference on Artificial Intelligence,Philadelphia, PA, 1986, pp.
1083-1088.~)amerau1985\]Fred J. Damerau, Problems and SomeSolutions in Customization of NaturalLanguage Database Front En~s.
ACMTransaetlone on O~ee InformationBy, terns 3(2), April 1985, pp.
165-184.\[Grishman1988\]Ralph Grishman, Lynette Hirschman,and Ngo Thanh Nhan, Discovery Pro-cedures for Sublanguage SelectionalPatterns: InitialExperiments.
Compwtational Linglls-tics 12(3), 1986, pp.
205-215.56\[Gross1983\]Barbara J.
Gross, TEAM: A Transport-able Natural-Language InterfaceSystem.
In Proceedings of the Confer-ence on Applied Natural LanguageProeesslng, Santa Monies, CA, Febru-ary 1983, pp.
39-45.~-Iendrix1980\]Gary G. Hendrix, Mediating the Viewsof Databases and Database Users.
InProeeedlngs of the Workshop on DataAbstraction, Databases, and Concep-tual Modelling, Pingree Park, CO,June 1980, pp.
131-132.~-Iirschman1982\]Lynette Hirschman and Karl Puder,Restriction Grammar in Prolog.
InProeecdlngs of the First InternationalLogic Programming Conference, M.Van Caneghem (ed.
), Association pour laDiffusion et le Developpement deProlog,Marseille, 1982, pp.
85-90.~Iixschman1985\]Lynette Hirschman and Karl Puder,Restriction Grammar: A PrologImplementation.
I  Logic Program-ming and its Applications, D.I-LD.Warren and M. VanCaneghem (ed.
),1985.~Iirschman1986a\]Lynette Hirschman, Discovering Sub-language Structures.
In Sublanguage:Deseriptio~ and Processing, R. Kit-tredge and R. Grishman (ed.
), LawrenceErlbaum Assoc., Hilisdale, N J, 1986.~Iirschman1986b\]Lynette Hirschman, Conjunction inMeta-Restriction Grammar.
Journal ofLogic Programming 4, 1988, pp.
299-328.~ang1987\]Francois-Michel Lang, A User's Guide tothe Selection Module, Logic-Based Sys-tems Technical Memo No.
88, PaoliResearch Center, Unisys, Paoli, PA, Oc-tober, 1987.\[~VIoser 1984\]M. G. Moser, Domain Dependent Seman-tic Acquisition.
In Proceedings of theFirst Conference on ArtlJlelal Intelll-genes Applications, IEEE ComputerSociety, Denver, CO, December, 1984,pp.
13-18.~almer1986\]Martha S. Palmer, Deborah A. Dahl,Rebecca J.
~assonneau\] Schiffman,Lynette HIirschman, Marcia Linebarger,and John Dowding, Recovering ImplicitInformation.
In Proceedings of tke?~th Annual Meeting of the Auoeia-tion for Computational Lingulstles,Columbia University, New York, August1986.~assonneau1986\]Rebecca J. Passonneau, A Computation-al Model of the Semantics of Tense andAspect, Logic-Based Systems TechnicalMemo No.
43, Paoli Research Center,Unisys, Paoli, PA, November, 1986.\[Sager1981\]Naomi Sager, Natural Language Infor-mation Processing: A ComputerGrammar of English and ItsApplications.
Addlson-Wesley, Reading,Mass., 1981.\[Thompson1983\]Bosena H. Thompson and Frederick B.Thompson, Introducing ASK, A SimpleKnowledgeable System.
In Proceedingsof tke Conference on Applied NaturalLanguage Processing, Santa Monies,CA, February 1983, pp.
17-24.\[Thompson1985\]Bosena H. Thompson and Frederick B.Thompson, ASK is Transportable inHalf a Dosen Ways.
ACM Transac-tions on Oee Information Sys-tems 3(2), April 1985, pp.
185-203.\[Tomita1984\]Masaru Tomita, Disambiguating Gram-matically Ambiguous Sentences byAsking.
ln Proceedings of the ?
?ndAnnual Meeting of the Associationfor Computational Linguistles, Stan-ford, CA, July 1984, pp.
476-480.57
