Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 921?928Manchester, August 2008Class-Driven Attribute ExtractionBenjamin Van Durme, Ting Qian and Lenhart SchubertDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627, USAAbstractWe report on the large-scale acquisitionof class attributes with and without theuse of lists of representative instances, aswell as the discovery of unary attributes,such as typically expressed in Englishthrough prenominal adjectival modifica-tion.
Our method employs a system basedon compositional language processing, asapplied to the British National Corpus.
Ex-perimental results suggest that document-based, open class attribute extraction canproduce results of comparable quality asthose obtained using web query logs, indi-cating the utility of exploiting explicit oc-currences of class labels in text.1 IntroductionRecent work on the task of acquiring attributesfor concept classes has focused on the use of pre-compiled lists of class representative instances,where attributes recognized as applying to multi-ple instances of the same class are inferred as be-ing likely to apply to most, or all, members ofthat class.
For example, the class US Presidentmight be represented as a list containing the en-tries Bill Clinton, George Bush, Jimmy Carter, etc.Phrases such as Bill Clinton?s chief of staff ..., orsearch queries such as chief of staff bush, provideevidence that the class US President has as an at-tribute chief of staff.Usually the focus of such systems has been onon binary attributes, such as the example chief ofstaff, while less attention has been paid to unaryc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.class attributes such as illegal for the class Drug,or warm-blooded for the class Animal.1Theseattributes are most typically expressed in Englishthrough prenominal adjectival modification, withthe nominal serving as a class designator.
Whenattribute extraction is based entirely on instancesand not the class labels themselves, this form ofmodification goes undiscovered.In what follows we explore both the impact ofgazetteers in attribute extraction as well as theacquisition and filtering of unary class attributes,through a process based on logical form genera-tion from syntactic parses derived from the BritishNational Corpus.2 Extraction FrameworkExtraction was performed using a modified ver-sion of the KNEXT system, a knowledge acquisi-tion framework constructed for large scale genera-tion of abstracted logical forms through composi-tional linguistic analysis.
The following providesan overview of KNEXT and its target knowledgerepresentation, Episodic Logic.2.1 Episodic LogicAutomatically acquiring general world knowledgefrom text is not a task that provides an immedi-ate solution to any real world problem.2Rather,the motivation for acquiring large stores of back-ground knowledge is to enable research withinother areas of artificial intelligence, e.g., the con-struction of systems that can engage in dialoguesabout everyday topics in unrestricted English, use1Almuhareb and Poesio (2004) treat unary attributes asvalues of binary attributes; e.g., illegal might be the value ofa legality attribute.
But for many unary attributes, this is astretch.2Unless one regularly needs reminding of facts such as, AWOMAN MAY BOIL A GOAT.921(Some e0:[e0 at-about Now0][(Many.det x :[x ((attr athletic.a) (plur youngster.n))][x want.v(Ka(become.v(plur((attr professional.a) athlete.n))))])** e0])Figure 1: Example EL formula; square brackets indicatea sentential infix syntax of form [subject pred object ...], Kareifies action predicates, and attr ?raises?
adjectival predicatesto predicate modifiers; e0 is the situation characterized by thesentence.common sense in answering questions or solv-ing problems, pursue intrinsic goals independently,and show awareness of their own characteristics,biography, and cognitive capacities and limita-tions.
An important challenge in the pursuit ofthese long-range goals is the design and implemen-tation of a knowledge representation that is as ex-pressively rich as natural language and facilitateslanguage understanding and commonsense reason-ing.Episodic Logic (EL) (Schubert and Hwang,2000), is a superset of FOL augmented with cer-tain semantic features common to all human lan-guages: generalized quantification, intensionality,uncertainty, modification and reification of predi-cates and propositions, and event characterizationby sentences.
An implementation of EL exists asthe EPILOG system (Schaeffer et al, 1993), whichsupports both forward and backward inference,along with various specialized routines for dealingwith, e.g., color, time, class subsumption, etc.
EPI-LOG is under current development as a platform forstudying a notion of explicit self-awareness as de-fined by Schubert (2005).As an indication of EL?s NL-like syntax, figure1 contains the output of EPILOG?s parser/logical-form generator for the sentence, Many athleticyoungsters want to become professional athletes.2.2 KNEXTIf ?deep?
language understanding and common-sense reasoning involve items as complex andstructured as seen in figure 1, then automatedknowledge acquisition cannot simply be a mat-ter of accumulating rough associations betweenword strings, along the lines ?
(Youngster) (wantbecome) (professional athlete)?.
Rather, acquiredknowledge needs to conform with a systematic,highly expressive KR syntax such as EL.The KNEXT project is aimed at extracting suchstructured knowledge from text.
One of the majorobstacles is that the bulk of commonsense knowl-edge on which people rely is not explicitly writtendown ?
precisely because it is common.
Even itwere written down, most of it could not be reliablyinterpreted, because reliable interpretation of lan-guage is itself dependent on commonsense knowl-edge (among other things).In view of these difficulties, KNEXT has initiallyfocused on attempting to abstract world knowl-edge ?factoids?
from texts, based on the logicalforms derived from parsed sentences.
The idea isthat nominal pre- and post-modifiers, along withsubject-verb-object relations, captured in logicalforms similar to that in figure 1, give a glimpseof the common properties and relationships in theworld ?
even if the source sentences describe in-vented situations.
For example, the following wereextracted by KNEXT, then automatically verbal-ized back into English for ease of readability:?
SOME NUMBER OF YOUNGSTERS MAY WANTTO BECOME ATHLETES.?
YOUNGSTERS CAN BE ATHLETIC.?
ATHLETES CAN BE PROFESSIONAL.2.3 Attribute Extraction via KNEXTIn order to study the contribution of lists of in-stances (i.e., generalized gazetteers) to the task ofattribute extraction, the version of KNEXT as pre-sented by Schubert (2002) was modified to provideoutput of a form similar to that of the extractionwork of Pas?ca and Van Durme (2007).KNEXT?s abstracted, propositional output wasautomatically verbalized into English, with any re-sultant statements of the form, A(N) X MAY HAVEA(N) Y, taken to suggest that the class X has as anattribute the property Y.KNEXT was designed from the beginning tomake use of gazetteers if available, where aphrase such as Bill Clinton vetoed the bill sup-ports the (verbalized) proposition A PRESIDENTMAY VETO A BILL.
just as would The presidentvetoed the bill.
We instrumented the system torecord which propositions did or did not requiregazetteers in their construction, allowing for a nu-merical breakdown of the respective contributionsof known instances of a class, versus the class labelitself.922Pas?ca and Van Durme (2007) described the re-sults of an informal survey asking participants toenumerate what they felt to be important attributesfor a small set of example classes.
Some of theseresultant attributes were not of the form targetedby the authors?
system.
For example, nonprofitwas given as an important potential attribute forthe class Company, as well as legal for the classDrug.
These attributes correspond to unary predi-cates as compared to the targeted binary predicatesunderlying such attributes as cost(X,Y) for the classDrug.We extracted such unary attributes by focusingon verbalizations of the form, A(N) X CAN BE Yas in AN ANIMAL CAN BE WARM-BLOODED.3 Experimental Setting3.1 Corpus ProcessingInitial reports on the use of KNEXT were focusedon the processing of manually created parse trees,on a corpus of limited size (the Brown corpus ofKucera and Francis (1967)).
Since that time thesystem has been modified into a fully automaticextraction system, making use of syntactic parsetrees generated by parsers trained on the PennTreebank.For our studies here, the parser employed wasthat of Collins (1997) applied to the sentencesof the British National Corpus (BNC Consortium,2001).
Our choice of the BNC was motivated byits breadth of genre, its substantial size (100 mil-lion words) and its familiarity (and accessibility)to the community.3.2 GazetteersKNEXT?s gazetteers were used as-is, and weredefined based on a variety of sources: miscella-neous publicly available lists, as well as manualenumeration.
The classes covered can be seen inthe Results section in table 2, where the minimum,maximum and mean size were 2, 249, and 41, re-spectively.3.3 Filtering out Non-predicative AdjectivesBeyond the pre-existing KNEXT framework, ad-ditional processing was introduced for the extrac-tion of unary attributes in order to filter out vacu-ous or unsupported propositions derived from non-compositional phrases.This filtering was performed through the cre-ation of three lists: a whitelist of accepted pred-icative adjectives; a graylist containing such adjec-tives that are meaningful as unary predicates onlywhen applied to plural nouns; and a blacklist de-rived from Wikipedia topic titles, representing lex-icalized, non-compositional phrases.Whitelist The creation of the whitelist began withcalculating part-of-speech (POS) tagged bigramcounts using the Brown corpus.
The advantageof using a POS-tagged bigram model lies in thesaliency of phrase structures, which enabled fre-quency calculations for both attributive and pred-icative uses of a given adjective.
Attributive countswere based on instances when an adjective appearsin the pre-nominal position and modifies anothernoun.
Predicative counts were derived by sum-ming over occurrences of a given adjective afterall possible copulas.
These counts were used tocompute a p/a ratio - the quotient of predicativecount over attributive count - for each word classi-fied by WordNet (Fellbaum, 1998) as a having anadjectival use.
After manual inspection, two cut-off points were chosen at ratios of .06 and 0, asseen in table 1.Words not appearing in the Brown corpus (i.e.having 0 count for both uses), were sampled andinspected, with the decision made to place themajority within the whitelist, excluding just thosewith suffixes including -al, -c, -an, -st, -ion, -th, -o,-ese, -er, -on, -i, -x, -v, and -ing.This process resulted in a combined whitelist of14,249 (usually) predicative adjectives.p/a ratio (r) Cut-off decisionr ?
.06 keep the adjective*0 < r < .06 remove the adjective*otherwise keep the adjective*Table 1: Cut-off decision given the p/a ratio of anadjective.
*Note: except for hand-selected cases.Graylist We manually constructed a short list (cur-rently 33 words) containing adjectives that are gen-erally inappropriate as whitelist entries, but couldbe acceptable when applied to plurals.
For exam-ple, the verbalized proposition OBJECTS CAN BESIMILAR was deemed acceptable, while a state-ment such as AN OBJECT CAN BE SIMILAR iserroneous because of a missing argument.Blacklist From an exhaustive set of Wikipediatopic titles was derived a blacklist consisting of en-tries that had to satisfy four criteria: 1) no morethan three words in length; 2) has no closed-class923words, such as prepositions or adverbs; 3) mustbegin with an adjective and end with a noun (de-termined by WordNet); and 4) does not containany numerical characters or miscellaneous sym-bols that are usually not meaningful in English.Therefore, each title in the resultant list is liter-ally a short noun phrase with adjectives as pre-modifiers.
It was observed that in these encyclope-dia titles, the role of adjectives is predominantly torestrict the scope of the object that is being named(e.g.
CRIMINAL LAW), rather than to describeits attributions or features (e.g.
DARK EYES).More often than not, only cases similar to the sec-ond example can be safely verbalized as X CANBE Y from a noun phrase Y X, with Y being thepre-nominal adjective.We further refined this list by examining trigramfrequencies as reported in the web-derived n-gramcollection of Brants and Franz (2006).
For each ti-tle of the form (Adj N) ..., we gathered trigram fre-quencies for adverbial modifications such as (veryAdj N) ..., and (truly Adj N) ....
Intuitively, high rel-ative frequency of such modification with respectto the non-modified bigram supports removal ofthe given title from the blacklist.Trigram counts were collected using the modi-fiers: absolutely, almost, entirely, highly, nearly,perfectly, truly and very.
These counts weresummed for a given title then divided by the afore-mentioned bigram score.
Upon sampled inspec-tion, all three-word titles were kept on the black-list, along with any two-word title with a resultantratio less than 0.028.
For example, the titles HardyFish, Young Galaxy, and Sad Book were removed,while Common Cause, Bouncy Ball, and Heavy Oilwere retained.4 ResultsFrom the parsed BNC, 6,205,877 propositionswere extracted, giving an average of 1.396 propo-sitions per input sentence.3These results werethen used to explore the necessity of gazetteers,and the potential for extracting unary attributes.Quality judgements were performed using a 5point scale as seen in figure 2.3These approximately six million verbalized propo-sitions, along with their underlying logical form andrespective source sentence(s), may be browsed in-teractively through an online browser available at:http://www.cs.rochester.edu/u/vandurme/epikTHE STATEMENT ABOVE IS A REASONABLYCLEAR, ENTIRELY PLAUSIBLE GENERALCLAIM AND SEEMS NEITHER TOO SPECIFICNOR TOO GENERAL OR VAGUE TO BE USEFUL:1.
I agree.2.
I lean towards agreement.3.
I?m not sure.4.
I lean towards disagreement.5.
I disagree.Figure 2: Instructions for scaled judging.4.1 Necessity of GazetteersFrom the total set of extracted propositions,638,809 could be verbalized as statements of theform X MAY HAVE Y.
There were 71,531 uniqueclasses (X) for which at least a single candidate at-tribute (Y) was extracted, with 9,743 of those hav-ing at least a single such attribute that was sup-ported by a minimum of two distinct sentences.Table 2 gives the number of attributes extractedfor the given classes when using only gazetteers,when using only the given names as class labels,and when using both together.
While instance-based extraction generated more unique attributes,there were still a significant number of results de-rived based exclusively on class labels.
Further,as can be seen for cases such as Artist, class-driven extraction provided a large number of at-tribute candidates not observed when relying onlyon gazetteers (701 total candidate attributes weregathered based on the union of 441 and 303 can-didates respectively extracted with, and without agazetteer for Artist).We note that this volume measure is potentiallybiased against class-driven extraction, as no ef-fort was made to pick an optimal label for a givengazetteer, (the original hand-specified class labelswere retained).
For example, one might expect thelabel Drink to generate more, yet still appropriate,propositions than Beverage, Actor and/or Actressas compared to Show Biz Star, or the semanticallysimilar Book versus Literary Work.
This is sug-gested by the entries in the table based on usingsupertypes of the given class, as well as in figure3, which favorably compares top attributes discov-ered for select classes against those reported else-where in the literature.Table 3 gives the assessed quality for the top tenattributes extracted for five of the classes in table2.
As can be seen, class-driven extraction can pro-duce attributes of quality assessed at par with at-tributes extracted using only gazetteers.924BasicFood ReligionK (Food): quality, part, taste, value, portion.. K: basis, influence, name, truths, symbols, principles,D: species, pounds, cup, kinds, lbs, bowl.. strength, practice, origin, adherent, god, defence..Q: nutritional value, health benefits, glycemic index, D: teachings, practice, beliefs, religion spread,varieties, nutrition facts, calories.. principles, emergence, doctrines..Q: basic beliefs, teachings, holy book, practices, rise,branches, spread, sects..HeavenlyBody PainterKG(Planet): surface, orbit, bars, history, atmosphere.. KG(Artist) : works, life, career, painting, impression,K (Planet): surface, history, future, orbit, mass, field.. drawings, paintings, studio, exhibition..K (Star): surface, mass, field, regions.. K (Artist): works, impression, career, life, studio..D: observations, spectrum, planet, spectra, conjunction, K (Painter) : works, life, wife, eye..transit, temple, surface..
Q?
: paintings, works, portrait, death, style, artwork,Q: atmosphere, surface, gravity, diameter, mass, bibliography, bio, autobiography, childhood..rotation, revolution, moons, radius..Figure 3: Qualitative comparison of top extracted attributes; KGis KNEXT using gazetteers, K (class) is KNEXT for a classlabel similar to the heading, D and Q are document- and query-based results as reported in (Pas?ca et al, 2007), Q?
is query-basedresults reported in (Pas?ca and Van Durme, 2007).The noticeable drop in quality for the classPlanet when only using gazetteers (3.2 meanjudged acceptability) highlights the recurringproblem of word sense ambiguity in extraction.The names of Roman deities, such as Mars or Mer-cury, are used to refer to a number of conceptu-ally distinct items, such as planets within our so-lar system.
Two of the attributes judged as poorquality for this class were bars and customers, re-spectively derived from the noun phrases: (NP(NNP Mars) (NNS bars)), and (NP (NNP Mer-cury) (NNS customers)).
Note that in both casesthe underlying extraction is correctly performed;the error comes from abstracting to the wrongclass.
These NPs may arguably support the ver-balized propositions, e.g.
: A CANDY-COMPANYMAY HAVE BARS, and A CAR-COMPANYMAY HAVE CUSTOMERS.These examples point to additional areas forimprovement beyond sense disambiguation: non-compositional phrase filtering for all NPs, ratherthan just in the cases of adjectival modification(Mars bar is a Wikipedia topic); and relative dis-counting of patterns used in the extraction pro-cess4.
This later technique is commonly used inspecialized extraction systems, such as constructedby Snow et al (2005) who fit a logistic regressionmodel for hypernym (X is-a Y) classification basedon WordNet, and Girju et al (2003) who trained aclassifier to look specifically for part-whole rela-tions.4For example, (NP (NNP X) (NNS Y)) may be more se-mantically ambiguous than, e.g., the possessive construction(NP (NP (NNP X) (POS ?s)) (NP (NNS Y))).4.2 Unary AttributesTable 4 shows how filtering non-compositionalphrases from CAN BE propositions affects extrac-tion volume.
Table 5 shows the difference betweensuch post-filtered propositions and those that weredeleted.
As our filter lists were not built fully au-tomatically, evaluation was performed exclusivelyby an author with negligible direct involvement inthe lists?
creation (so-as to minimize judgementbias).As examples, the top ten unary attributes forselect classes are given in table 6, which the au-thors believe to be high quality on average, withsome bad entries present.
Attributes such aspre-raphaelite for Painter are considered obscure,while those such as favourite for Animal are con-sidered unlikely to be useful as a unary predicate.The importance of class-driven extraction can beseen in results such as those given for the class Ap-ple.
Even if it were the case that gazetteer-basedextraction could deliver perfect results for thoseclasses whose instances occasionally appear ex-plicitly in text, there are a number of classes forwhich such instances are entirely lacking.
For ex-ample, there are many instances of the class Com-pany which have been individually named and ap-pear in text with some frequency, e.g., Microsoft,Walmart, or Boeing.
However, despite the manyreal-world instantiations of the class Apple, thisdoes not translate into a list of individually namedmembers in text.5If our goal is to acquire at-tributes for as many classes as possible, our results5Instances of Apple are referred to directly as such; ?anapple.
?925Class Both Gaz.
Class Lbl.Continent 777 698 96Country 7,285 5,993 1,696US State 1,289 1,286 609*US City 2,216 2,120 813*World City 4,780 4,747 813*Beverage 53 53 0Tycoon 19 10 10TV Network 71 71 0Artist 706 441 303Medicine 29 2 27Weekday 1,234 1,232 2Month 2,282 1,875 474Dictator 533 509 28Conqueror 103 84 19Philosopher 672 649 37Conductor 118 74 45Singer 220 179 49Band 349 58 303King 811 208 664Queen 541 17 532Religious Leader 127 127 0Adventurer 32 27 5Planet 289 163 141Criminal/Outlaw 30 30 6/4*Service Agency 85 83 2Architect 72 67 63Show Biz Star 82 82 0Film Maker 42 33 9Composer 722 651 98Humanitarian 5 5 0Pope 235 123 113River 402 168 253Company 3,968 1,553 2,941Deity 1,037 1,027 19Scientist 798 750 60Religious Holiday 594 593 65*Civic Holiday 3 3 65*Military Commander 71 71 26*Intl Political Entity 673 673 0Sports Celebrity 45 45 0Activist Organization 63 63 0Martial Art 3 3 0Government Agency 295 294 2Criminal Organization 0 0 0US President 596 596 1,421*Political Leader 568 568 170*Supreme Court Justice 0 0 18*Emperor 436 211 259Fictitious Character 227 227 180*Literary Work 9 9 0Engineer/Inventor 10 10 73/13*Famous Lawyer 0 0 72*Writer 1,116 957 236TOTAL 35,723 29,518 8,506Table 2: Extraction volume with and without usinggazetteers.
*Note: When results are zero after gaz.
omission,values are reported for super-types, such as Holiday for thesub-type Civic Holiday, or City for US City.
A/B scores re-ported for each class used separately, e.g., Engineer/Inventor.Class Both Gazetteer Class LabelKing 1.2 1.9 1.3Composer 1.5 1.5 2.1River 1.9 1.9 1.5Continent 1.5 1.9 2.0Planet 1.9 3.2 1.6Table 3: Average judged acceptability for the top ten at-tributes extracted for the given classes when using/not-usinggazetteer information.Collection Size% ofOriginal CAN BEOriginal total 6,204,184 100 -Filtered total 5,382,282 87 -Original CAN BE 2,895,325 46 100Filtered CAN BE 2,073,417 33 72Whitelist 812,146 15 28Blacklist 19,786 1< 1Table 4: Impact of filtering on volume.
For example, thosepropositions removed because of the whitelist comprised 15%of the total propositions extracted, or 28% of those specifi-cally verbalized as X CAN BE Y.indicate the benefits of exploiting the explicit ap-pearance of class labels in text.5 Related WorkPas?ca and Van Durme (2007) presented an ap-proach to attribute extraction based on the useof search engine query logs, a previously unex-plored source of data within information extrac-tion.
Results confirmed the intuition that a sig-nificant number of high quality, characteristic at-tributes for many classes may be derived basedon the relative frequency with which anonymoususers request particular pieces of information forknown instances of a concept class.
Pas?ca et al(2007) compared the quality of shallow attributeextraction techniques as applied to documents ver-sus search engine query logs, concluding that suchmethods are more applicable to query logs than todocuments.
We note that while search queries doseem ideally suited for extracting class attributes,existing large-scale collections of query logs areproprietary and thus unavailable to the general re-search community.
At least until such a resourcebecomes available, it is of interest to the commu-nity that (qualitatively) similar extraction resultsmay be achieved exclusively using publicly avail-able document collections.Alternative approaches to harvesting large-scaleknowledge repositories based on logical forms in-clude that reported by Suchanek et al (2007).The authors used non-linguistic information avail-9261 10 100 1,000Filtered 3.18 3.60 2.74 2.76Blacklist 3.88 4.00 4.08 4.06Whitelist 3.78 3.76 3.74 3.80Table 5: Mean evaluated acceptability for 50 unary at-tributes randomly sampled from each of the given levels ofsupport (attribute occurred once, less than 10 times, less than100 times, ...).
Filtered refers to the final ?clean?
results,Blacklist and Whitelist refer to propositions deleted due tothe given list.Painterfamous, romantic, distinguished, celebrated,well-known, pre-raphaelite, flemish, dutch,abstractAnimaldead, trapped, dangerous, unfortunate, intact,hungry, wounded, tropical, sick, favouriteDrugdangerous, powerful, addictive, safe, illegal,experimental, effective, prescribed, harmful,hallucinatoryApplered, juicy, fresh, bad, substantive, stuffed,shiny, ripe, green, bakedEarthquakedisastrous, violent, underwater, prolonged,powerful, popular, monstrous, fatal, famous,epicTable 6: Top ten unary attributes for select classes, gatheredexclusively without the use of gazetteers.able via Wikipedia to populate a KB based ona variant of the logic underlying the Web On-tology Language (OWL).
Results were limited to14 predefined relation types, such as diedInYearand politicianOf, with membership of instanceswithin particular concept classes inferred based onWikipedia?s category pages.
Authors report 5 mil-lion so-called ontological facts being extracted.Almuhareb and Poesio (2004) performed at-tribute extraction on webtext using simple extrac-tion patterns (e.g., ?the * of the C [is|was]?, and?
[a|an|the] * C [is|was]?, which respectively matchThe color of the rose was red and A red rose was...), and showed that such attributes could improveconcept clustering.
Subsequently they tested analternative approach to the same problem using adependency parser, extracting syntactic relationssuch as (ncmod, rose, red) and (ncsubj, grow, rose)(Almuhareb and Poesio, ).
They concluded thatsyntactic information is relatively expensive to de-rive, and serves primarily to alleviate data spar-sity problems (by capturing dependencies betweenpotentially widely separated words) that may nolonger be an issue given the scale of the Web.
Wetake a different view, first because attribute extrac-tion is an offline task for which a 60% overheadcost (reported by the authors) is not a major is-sue, but more importantly because we regard ap-proaches that process language compositionally asultimately necessary for deeper meaning represen-tation and language understanding.Following intuitions similar to those laid out bySchubert (2002), Banko et al (2007) presentedTextRunner, the latest in a series of ever more so-phisticated general information extraction systems(Cafarella et al, 2005; Etzioni et al, 2004).
Theauthors constructed a non-parser based extractorfor open domain text designed to efficiently pro-cess web-sized datasets.
Results are in the form ofbracketed text sequences that hint at a sentence?sunderlying semantics.
For example, (BletchleyPark) was location of (Station X).Cimiano et al (2005) performed a limited formof class-driven extraction in order to induce classhierarchies via the methods of Formal ConceptAnalysis (FCA).
For example, a car is both drive-able and rentable based on its occurrence in objectposition of the relevant verbs.
A bike shares theseproperties with car, as well as having the propertyrideable, leading to these classes being near in theresultant automatically constructed taxonomy.
Ex-periments were performed on limited domains forwhich pre-existing ontologies existed for measur-ing performance (tourism and finance).Lin (1999) gave a corpus-based method for find-ing various types of non-compositional phrases,including the sort discussed in this paper.
Identi-fication was based on mutual information statisticsconditioned on a given syntactic context (such asour targeted prenominal adjectival modification).If the mutual information of, e.g., white house,shows strong differences from that for construc-tions with similar components, e.g., red house, andwhite barn, then the given phrase was determinedto be non-compositional.
The use of this method tosupplement that explored here is a matter of cur-rent investigation.
Early results confirm our in-tuition regarding the correlation between such au-tomatically discovered non-compositional phrasesand Wikipedia topic titles, where high scoringphrases not already in our list tend to suggest miss-Yescooking pot, magic flute, runny nose, skimmed milk,acquired dyslexia, charged particles, earned incomeNocausal connectives, golden oldies, ruling junta,graduated pension, unsung heroes, viral rnaTable 7: Example high-scoring phrases as ranked by Lin?smetric when applied to KNEXT logical forms, along withwhether there is, at the time of this writing, an associatedWikipedia entry.927A CAR MAY HAVE A ...back, boot, side, driver, front, roof, seat, end, interior,owner, door, control, value, bonnet, wheel, window,engine, headlights..A CAR CAN BE ...black, parked, red, white, armoured, nice, hired, bloody,open, beautiful, wrecked, unmarked, secondhand,powerful, brand-new, out-of use, damaged, heavy, dark,competitive, broken-down..A CAR MAY BE ...
IN SOME WAYparked, stolen, driven, damaged, serviced, stopped,lost, clamped, overturned, locked, involved in an accident,found, turned, transported..Table 8: Top attributes extracted for the class Car, whereMAY BE relational properties (akin to those used by Cimi-ano et al (2005)) are similarly acquired via verbalization ofabstracted logical forms.ing entries in the encyclopedia (see table 7).
Theability to perform such ?missing topic discovery?should be of interest to those within the emergingcommunity of Wikipedia-focused AI researchers.6 ConclusionWe have shown that an open knowledge extractionsystem can effectively yield class attributes, evenwhen named instances of the class are unavailableor scarce (as a final example see table 8).
We stud-ied the quantitative contributions of instances (asgiven in KNEXT gazetteers) and explicitly occur-ring class nominals to the discovery of attributes,and found both to be important.
We paid partic-ular attention to the acquisition of unary class at-tributes, for which access to class labels is of par-ticular importance because of their typical mannerof expression in text.Acknowledgements The authors are grateful toDaniel Gildea for contributing a parsed version ofthe BNC.
This work was supported by NSF grantsIIS-0328849 and IIS-0535105.ReferencesAlmuhareb, Abdulrahman and Massimo Poesio.
Finding con-cept attributes in the web using a parser.
In ProceedingsCorpus Linguistics Conference.Almuhareb, Abdulrahman and Massimo Poesio.
2004.Attribute-based and value-based clustering: an evaluation.In Proceedings of EMNLP.Banko, Michele, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open Infor-mation Extraction from the Web.
In Proceedings of IJCAI.BNC Consortium.
2001.
The British National Corpus, ver-sion 2 (BNC World).
Distributed by Oxford UniversityComputing Services.Brants, Thorsten and Alex Franz.
2006.
Web 1T 5-gram Ver-sion 1.
Distributed by the Linguistic Data Consortium.Cafarella, Michael J., Doug Downey, Stephen Soderland, andOren Etzioni.
2005.
KnowItNow: Fast, Scalable Infor-mation Extraction from the Web.
In Proceedings of HLT-EMNLP.Cimiano, Philipp, Andreas Hotho, and Steffen Stabb.
2005.Learning concept hierarchies from text corpora using for-mal concept analysis.
Journal of Artificial Inteligence Re-search.Collins, Michael.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proceedings of ACL.Etzioni, Oren, Michael Cafarella, Doug Downey, StanleyKok, AnaMaria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2004.
Web-scaleInformation Extraction in KnowItAll.
In Proceedings ofWWW.Fellbaum, Christiane.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Girju, R., A. Badulescu, and D. Moldovan.
2003.
Learningsemantic constraints for the automatic discovery of part-whole relations.
In Proceedings of HLT-NAACL.Kucera, H. and W. N. Francis.
1967.
Computational Anal-ysis of Present-Day American English.
Brown UniversityPress, Providence, RI.Lin, Dekang.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL.Pas?ca, Marius and Benjamin Van Durme.
2007.
What youseek is what you get: Extraction of class attributes fromquery logs.
In Proceedings of IJCAI.Pas?ca, Marius, Benjamin Van Durme, and Nikesh Garera.2007.
The role of documents vs. queries in extracting classattributes from text.
In Proceedings of CIKM.Schaeffer, S.A., C.H.
Hwang, J. de Haan, and L.K.
Schubert.1993.
EPILOG, the computational system for episodiclogic: User?s guide.
Technical report, Dept.
of Comput-ing Science, Univ.
of Alberta, August.Schubert, Lenhart K. and Chung Hee Hwang.
2000.
Episodiclogic meets little red riding hood: A comprehensive, natu-ral representation for language understanding.
In Iwanska,L.
and S.C. Shapiro, editors, Natural Language Processingand Knowledge Representation: Language for Knowledgeand Knowledge for Language.
MIT/AAAI Press.Schubert, Lenhart K. 2002.
Can we derive general worldknowledge from texts?
In Proceedings of HLT.Schubert, Lenhart K. 2005.
Some Knowledge Representa-tion and Reasoning Requirements for Self-awareness.
InProc.
AAAI Spring Symposium on Metacognition in Com-putation.Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernym dis-covery.
In Proceedings of NIPS 17.Suchanek, Fabian M., Gjergji Kasneci, and Gerhard Weikum.2007.
YAGO: A core of semantic knowledge unifyingWordNet and Wikipedia.
In Proceedings of WWW.928
