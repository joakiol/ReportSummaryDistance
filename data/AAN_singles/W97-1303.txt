Factors in anaphora resolution:they are not the only things that matter.A case study based on two different approachesRuslan MitkovSchool of Languages and European StudiesUniversity of WolverhamptonStafford StreetWolverhampton WV 1 1 SBUnited KingdomR.
M i tkov@wlv .
ac.
ukABSTRACTThe paper discusses the significance of factors inanaphora resolution and on the basis of a compara-tive study argues that what matters is not only agood set of reliable factors but also the strategy fortheir application.
The objective of the study was tofind out how well the same set of factors workedwithin two different computational strategies.
Tothis end, we tuned two anaphora resolution ap-proaches to use the same core set of factors.
Thefirst approach uses constraints o discount implau-sible candidates and then consults preferences torank order the most likely candidate.
The secondemploys only preferences and does not discard anycandidate but assumes initially that the candidateexamined is the antecedent; on the basis of uncer-tainty reasoning formula this hypothesis i eitherrejected or accepted.The last section of the paper addresses some re-lated unresolved issues which need further re-search.I.
Approaches and factors in anaphoraresolutionApproaches to anaphora resolution usually rely ona set of "anaphora resolution factors".
Factors usedfrequently in the resolution process include genderand number agreement, c-command constraints,semantic consistency, syntactic parallelism, se-mantic parallelism, salience, proximity etc.
Thesefactors can be "eliminating" i.e.
discounting cer-tain noun phrases from the set of possible candi-dates (such as gender and number constraints, c-command constraints, semantic consistency) or"preferential", giving more preference to certaincandidates and less to others (such as parallelism,salience).
Computational linguistics literature usesdiverse terminology for these - for example E.Rich and S. LuperFoy (\[Rich & LuperFoy 88\])refer to the "eliminating" factors as "constraints",and to the preferential ones as "proposers",whereas Carbonell and Brown (\[Carbonell &Brown 88\] use the terms "constraints" and"preferences".
Other authors argue that all factorsshould be regarded as preferential, giving higherpreference to more restrictive factors and lower -to less "absolute" ones, calling them simply"factors" (\[PreuB et al 94\]), "attributes" (\[P6rez94\]) or "symptoms" (\[Mitkov 95\]).The impact of  different factors and/or their co-ordination have already been described in the lit-erature (e.g.
\[Carter 90\], \[Dagan et al 91\]).
In hiswork David Carter argues that a flexible controlstructure based on numerical scores assigned topreferences allows greater co-operation betweenfactors as opposed to a more limited depth-firstarchitecture.
His discussion is grounded in com-parisons between two different implemented sys-tems - SPAR (\[Carter 87\]) and the SRI Core Lan-guage Engine (\[Alshawi 90\]).
I. Dagan, J. Just-eson, Sh.
Lappin, H. Leass and A. Ribak (\[Daganet al 91\] attempt o determine the relative impor-tance of distinct informational factors by compar-ing a syntactically-based salience algorithm forpronominal anaphora resolution (RAP) (\[Lappin &Leass 94\]) with a procedure for reevaluating thedecisions of the algorithm on the basis of statisti-cally modelled lexical semantic/pragmatic prefer-ences (\[Dagan 92\]).
Their results suggest thatsyntactically measured salience preferences aredominant in anaphora resolution.While a number of approaches use a similar setof  factors, the "computational strategies" for theapplication of  these factors may differ (by"computational strategy" we mean here the wayantecedents are computed, tracked down, i.e.
thealgorithm, formula for assigning antecedents andnot computational issues related to programminglanguages, complexity etc.).
Some approachesincorporate a traditional model which discountsunlikely candidates until a minimal set of plausiblecandidates i obtained (then make use of center orfocus, for instance), whereas others compute themost likely candidate on the basis of statistical orAI techniques/models.
This observation led us toterm the approaches to anaphora resolution"traditional knowledge-based" and "alternative"14(\[Mitkov 96\]).
In the experiment ~ described below,we have kept the set of factors constant and soughtto explore which of two approaches, different interms of "computational strategy" (\[Mitkov 94a\],\[Mitkov 95\]) was the more successful.In the first of the two approaches, constraintsrule out impossible candidates and those left arefurther evaluated according to various preferencesand heuristics but above all the "opinion" of thediscourse module which strongly suggests thecenter of the previous clause as the most likelyantecedent.
The second approach regards all can-didates as equal to start with and seeks to collectevidence about how plausible ach candidate is onthe basis of the presence/absence of certain symp-toms (influence/non-influence of certain factors).All factors (symptoms) are unconditional prefer-ences (i.e.
there are no "absolute", "ruling out"factors) and are assigned numerical values.
Candi-dates are proposed or rejected as antecedents by anuncertainty reasoning hypothesis verification for-mula.
From the results obtained, we shall see thatsome of our conclusions coincide with Carter's.Further, we shall see that to achieve improvedperformance, a compromise, two-engine approachincorporating both strategies i an even better op-tion.The results of this study have an implication forbuilding a practical anaphora resolution system:what matters is not only the careful selection offactors, but also the choice of approach (e.g.
tradi-tional or statistic, AI etc.)
or combination of ap-proaches.2.
Comparing two different approaches usingthe same factorsBefore discussing the results of our comparativestudy, we shall briefly outline the approacheswhich served as a basis for the experiment.2.1 The integrated anaphora resolutionapproach (\[Mitkov 94a\])The Integrated Approach (IA) relies on both con-straints and preferences, with constraints discount-ing implausible candidates, and preferencesworking towards the selection of the most likelyantecedent.
The IA is built on modules consistingof different ypes of rule-based knowledge - syn-tactic, semantic, domain, discourse and heuristic(\[Mitkov 94a\]).The syntactic module, for example, knows thatthe anaphor and antecedent must agree in number,gender and person.
It checks whether the c-command constraints hold and establishes disjointreference.
In cases of syntactic parallelism, it pre-fers the noun phrase with the same syntactic role~The idea for this study was suggested by Allan Ramseyas the anaphor as the most probable antecedent.
Itknows when cataphora is possible and can indicatesyntactically topicalised noun phrases, which aremore likely to be antecedents han non-topicalisedones .The semantic module checks for semantic on-sistency between the anaphor and the possibleantecedent.
I  filters out semantically incompatiblecandidates following verb semantics or animacy ofthe candidate.
In cases of semantic parallelism, itprefers the noun phrase which has the same se-mantic role as the anaphor as the most likely an-tecedent.The syntactic and semantic modules are en-hanced by a discourse module which plays a veryimportant role because it keeps a track of the cen-ters of each discourse segment (it is the centerwhich is, in most cases, the most probable candi-date for an antecedent).
Based on empirical studiesfrom the sublanguage of computer science, wehave developed a statistical approach to determinethe probability of a noun (verb) phrase to be thecenter of a sentence.
Unlike other approachesknown to us, our method is able to propose thecenter with a high probability in every discoursesentence, including the first.
The approach uses aninference ngine based on Bayes' formula whichdraws an inference in the light of some new pieceof evidence.
This formula calculates the new prob-ability, given the old probability plus some newpiece of evidence (\[Mitkov 94b\]).The domain knowledge module is a smallknowledge base of the concepts of the domainconsidered, while the heuristic knowledge moduleis a set of useful rules (e.g.
the antecedent is likelyto be located in the current sentence or in the pre-vious one) which can forestall certain impracticalsearch procedures.The referential expression filter plays an im-portant role in filtering out expressions where 'it' isnot anaphoric (e.g.
"it is important", "it is neces-sary", "it should be pointed out" etc.
).The IA operates as follows.
Syntax and semanticconstraints (agreement, configurational, semanticconsistency) reduce the set of all candidates to theset of possible ones.
If the latter consists of morethan one noun phrase, then preferences are acti-vated.
Highest preference (score) is given to nounphrases which are the center of the previousclause, but syntactic parallelism, semantic parallel-ism and referential distance also contribute(though less significantly) to the overall score.2.2 The uncertainty reasoning approach(\[Mitkov 95\]).The Uncertainty Reasoning Approach (URA) usesAI uncertainty reasoning techniques.
Uncertaintyreasoning was selected as an alternative because:15?
In Natural Language Understanding, the pro-gram is likely to estimate the antecedent of ananaphor on the basis of incomplete infor-mation: even if information about constraintsand preferences is available, one can to as-sume that a Natural Language Understandingprogram is not able to understand the inputcompletely ;?
The necessary initial constraint and preferencescores are determined by humans; thereforethe scores are originally subjective and shouldbe regarded as uncertain facts.The uncertainty reasoning approach makes useof "standard" anaphor esolution "symptoms" suchas agreement, c-command constraints, parallelism,topicalisation, verb-case roles, but also of furthersymptoms based on empirical evidence, such assubject preference, domain concept preference,object preference, section head preference, reit-eration preference, definiteness preference, mainclause preference tc.
Note that this strategy doesnot regard factors as absolute constraints; allsymptoms are in practice preferences with numeri-cal values assigned.More specifically, the presence/absence of acertain symptom corresponds to an appropriatescore - certainty factor (CF) which is attached to it.For instance, the presence of a certain symptom sassigns CFsp r (0<CFspr<l), whereas the absencecorresponds to CFsa b (-l<CFsab_<0).
For easierreference and brevity, we associate with thesymptom s only the certainty factor CF s which weregard as a two-value function (CF s ~ {CFsp r,CFsab})-The antecedent searching procedure mploys anuncertainty reasoning strategy: the search for anantecedent is regarded as an affirmation (or rejec-tion) of the hypothesis that a certain noun phrase isthe correct antecedent.
The certainty factor (CF)serves as a quantitative approximation of the hy-pothesis.
The presence/absence of each symptom scauses recalculation (increase or decrease) of theglobal hypothesis certainty factor CFhy p until:CFhyp > CFthreshold for affirmation or CFhy p <CFmin for rejection of the hypothesis.
Hypothesisverification operates from right to left: first theclosest to the anaphor noun phrase is tried.
If thisnoun phrase does not survive the hypothesis ofbeing the antecedent, he next rightmost is triedand so on.We use a hypothesis verification formula for re-calculation of the hypothesis on the basis of pres-ence (in our case also of absence) of certainsymptoms.
Our formula is a modified version ofvan Melle's formula in (\[Buchanan & Shortliffe841).16CFhy p (s, CFol d) =CF s + CFol d - CF s * CFol d ~ CF s >0, CFol d >0 or(CF s + CFold)/\[l-min (ICFsl, ICFoldl)\] ?
:~ CFs>0,CFol d <0 or CFs>O, CFol d <0 or- CFhy p (s, CFol d) ~ CF s <0, CFol d <0where CFhy p (s, CFold) is the hypothesis certaintyfactor, contributed by the presence/absence ofsymptom s and the current (old) hypothesis cer-tainty factor CFol d. As an illustration, suppose acertain NP has reached a CF=0.5 after testing thepresence of some symptoms (e.g.
syntactic agree-ment) and that the symptom s with CF=0.45 holds.Then CFhyp (s, CFold) = 0.5+0.45-0.5*0.45=0.7252.3.
The same set of factors but differentcomputat ional  strategiesThe objective of the study was to compare the IAand the URA with both using the same repertoireof factors to see what was the imPact of the differ-ent computational strategies.2.3.1 Factors usedWe used the same set of factors in both ap-proaches - the factors selected were deemed to bea "core set" from the point of view of both ap-proaches.
The factors used in our experimentwere:?
Gender agreement?
Number agreementAnaphors and their antecedents mustagree in number and gender.Syntactic parallelismPreference is given to antecedents withthe same syntactic function as the ana-phor.The programmer, combined successfully Prolog.
1 jwith C, but he i had combined itj with Pascal lasttime.The programmer i combined successfully Prologwith Cj, but he i had combined Pascal with itj lasttime.TopicalisationTopicalised structures are given prefer-ential treatment as possible antecedents.It was Ruslan.
who convinced me to go to Madrid.
!Why did he i do it?Semantic onsistencyIf satisfied by the anaphor, semanticconsistency constraints must be satisfiedalso by its antecedent.Vincent removed the diskette from the computer iand then disconnected it i.Vincent removed the diskette i from the computerand then copied it i.Semantic parallelismThose antecedents are favoured whichhave the same semantic role as the ana-phor.Vincent gave the diskette to Sody i. Kim also gavehim i a letter.Vincent i gave the diskette to Sody.
He i also gaveKim a letter.SubjectsFrom the list of potential candidates thesubject of  the previous sentence (clause)is the preferred antecedent; the secondpreferred antecedent is the direct object.Domain conceptsThe NP representing a domain concept ispreferred to NPs which are not domainconcepts.The last two preferences can be illustrated by theexample:When the Prolog system i finds a solution to aquery, it i will print the values given to variablesused in the query.Object preference indicated by verbsIf the verb is a member of the Verb_set ={discuss, present, illustrate, summarise,examine, describe, define, show, check,develop, review, report, outline, consider,investigate, explore, assess, analyse,synthesise, study, survey, deal, cover},then consider the object as the preferredantecedent.Object preference indicated by nounsIf the subject is "chapter", "section","table", "document", "paper" etc.
or apersonal pronoun 'T', "we", "you", thenconsider the object as the preferred an-tecedent.This table shows a minimal configurationi; ti doesnot leave much room for additional applications orother software for which you may require addi-tional swap space.RepetitionRepeated NPs are considered to be thepreferred candidate for antecedent.?
Heading17If an NP occurs in the head of the section,part of which is the current sentence, thenconsider it as the candidate likeliest to bethe antecedent.System programsSystem programs i such as the supervisor and thelanguage translator should not have to be translatedevery time they i are used, otherwise this would re-sult in a serious increase in the time spent in proc-essing a user's program.
System programs i are usu-ally written in the assembly version of the machinelanguage and are translated once into the machinecode itself.
From then on they i can be loaded intomemory in machine code without he need for anyintermediate ranslation phase.DistanceCandidates from the previous clause orsentence are preferred.The objective of our study was to use the same setof factors.
As listed above, number ,and genderagreement, as well as semantic onsistency, wereused as constraints by the IA whereas the remain-ing were used as preferences; the URA used allfactors as preferences.
Note also that the factors"subject", "repetition", "head", "verb", "object"and "distance" were used as "anaphora resolutionsymptoms" (preferences) in the URA, whereasthey played the role of center tracking preferencesin the IA.
In both approaches these factors wereduly "consulted" and taken into consideration.2.3.2 EvaluationThe evaluation was conducted on the basis of amanually annotated test corpus from the sublan-guage of Computer Science.
We selected 133paragraphs containing the anaphor "it" (altogether512 occurrences of "it") and tested both ap-proaches tuned to activate only the core set offactors described.Our preliminary results showed a success rate of83% for the IR as opposed to 82% for the URAwith CFthreshol d 0.7.
Out of the 17% uncorrectlysolved anaphors by the IR, 5% were solved cor-rectly by the URA.
Out of the 18% uncorrectlysolved anaphors by the URA, 4% were solvedcorrectly by the IR.
With a higher threshold of 0.8,however, the URA went down to a level of accu-racy of 71%.
The lower success rates (as com-pared to \[Mitkov 95\]) are due to the fact that bothapproaches are restricted to the "core set of fac-tors" and thus cannot draw on others which theywould normally have at their disposal (e.g.
c-command constraints were not included in theexperimental core set).
In particular, when thenumber of symptoms is reduced, the URA cannotbenefit from all its sources of evidence and thushigh thresholds cannot realistically be reached.2.3.3 Discussion of the resultsnouns) where the anaphor may be plural and theantecedent singular or vice versa.In terms of performance it looks like the IA has aslight edge over the URA.
However, such a sug-gestion may be misleading because it turned outthat the URA was in general "safer".
Our studyprompts the following conclusions.
(i) In most cases both approaches were correctThis applies to the majority of cases.
One exampleis:Installing the battery in your Portable StyleWriterIn most cases a Print dialogue box appears, withoptions for printing your document.
The dialoguebox i may not look exactly like the window shownhere, but it i will have the options hown in this one.The IR conciudes that "dialogue box" is the ante-cedent mainly on the basis of assigning "dialoguebox" as center of the preceding clause.
It is evidentthat syntactic or semantic constraints cannot bevery helpful here.
The URA reaches confidencefactor 0.9227 which is sufficient for accepting thehypothesis.
(ii) When information is insufficient, the URA isless "decisive"As an illustration, consider the test text:Why C++ is better than C?Because C++ i is based on C, it i retains much of thatlanguage, including a rich operator set, nearly or-thogonal design, terseness and extendibility.The URA reaches confidence factor 0.893367.
Itcannot arrive at a confidence factor above 0.9 be-cause the number of indicative symptoms is insuf-ficient.
In this case, the URA works towards thehypothesis on the basis of the following symptomsonly: number, gender, semantic onsistency, syn-tactic parallelism, subjecthood.Our evaluation also showed that(iii) The IA is more decisive but could be "iffy"(iv) When information is ample, the URA is more"confident"(v) The URA is better in cases of gender andnumber discrepancy between anaphor andantecedentBecause the IA followed the traditional rule thatanaphor and antecedent must agree in gender andnumber, its initial version did not capture a num-ber of exceptions (e.g.
in the case of collectiveComputer memory, also known as primary storage,is closely associated with the central processingunit i but not actually part of it i.
Memory holds thedata k after it k is input to the system but before it k isprocessed.One way of coping with such "irregularities" is todraw up a comprehensive list of all such discrep-ancies.
However, it would be more natural to use apreferences-only approach which assigns prefer-ences and in which ruling out on the basis of non-agreement can be overturned by the joint influenceof other preferences.
(vi) The IA is better in cases where "it" occursfrequently and refers to different antecedentsThe Central Processing Unit and Memory: DataManipulationComputer memory, also known as primary storage,is closely associated with the central processingunit i but not actually part of it i.
Memory k holds thedataj after itj is input to the system but before itj isprocessed.
Itk also holds the data after itj has beenprocessed but before itj has been released to theoutput device.
(vii)Both approaches lose on performance becauseof the lack of corpus-based collocation infor-mationNeither approach relies on collocation patternswhich is seen as a disadvantage in cases wheresyntactical and semantic constraints/preferencesare not able to discriminate between more thanone candidate.These results inspired us to venture towards atwo-engine strategy which would combine thebenefits of the "speed" of the IR and the "safety"of the URA.2.4 The two-engine strategyTwo engines are better than one: a combined strat-egy which incorporates the advantages of each ofthese approaches, generates more power and con-fidence in the search for the antecedent.The two-engine strategy evaluates each candi-date for anaphor from the point of view of both theIA and the URA.
If opinions coincide, the evaluat-ing process is stopped earlier than would be thecase if only one engine were acting.
This alsomakes the searching process shorter: our testsshow that the integrated approach engine needsabout 90% of the search it would make when op-erating on its own; similarly, the uncertainty rea-soning engine does only 67% of the search itwould do when operating as a separate system.
In18addition, the results from using both approachesare more accurate (see the figure below).This combined strategy enables the system toconsider all the symptoms in a consistent way; itdoes not regard any symptom as absolute or un-conditional.
This "attitude" is particularly appro-priate for symptoms like 'gender' or 'number'(which could be regarded as absolute in some lan-guages but 'conditional' in other) 2.Additional reasons for selecting a two-engineapproach are the following:?
two independent judgements, if confirmed,lend more credibility to the selected an-tecedent?
using two approaches means complementar-ity: e.g.
the "conditionality" of gender is bettercaptured by uncertainty reasoning; in addi-tion, in sentences with more than one pro-noun, center tracking alone (and therefore theintegrated approach) is not very helpful fordetermining the corresponding antecedents?
though the URA may be considered morestable in such situations, it is comparativelyslow: if intermediate results obtained by bothengines are reported to be close, it couldadopt a lower hypothesis threshold (thusspeeding up the decision process)We have implemented the two-engine model as aprogram and the following table shows its successrate.
Five documents erved as inputs, each texttaken from a computer science book.
The docu-ments ranged from 3000 to 5000 words and wereestimated to contain a comparatively high numberof pronouns (it was not always easy to find textsabundant in pronominal anaphors).
These docu-ments were different from the corpus initially usedfor the development of various 'resolution rules'and were hand-annotated (syntactic and semanticroles).
Other versions of these documents, whichcontained anaphoric references marked by a hu-man expert, were used as an evaluation corpus.We tested on these inputs (i) the integrated ap-proach, (ii) the uncertainty reasoning approachand (iii) the two-engine approach.
Note that thetwo-engine version did not work on a "core set" offactors only, but benefited from the full range of"constraints" and "preferences" used by the IA andthe complete list of "symptoms" utilised by theURA.
The results show an improvement when theIA and the URA were combined into a two-enginestrategy:2 Often in English singular pronouns (e.g.
some singularpronouns denoting acollective notion) may be referredto by plural pronoun and vice-versa; In German, there isno absolute gender agreement: "M~idchen" (girl) isneuter, but one can refer to "M~idchen" by a femalepronoun (sic).Document 1Document 2Document 3Document 4Document 5Integrated?
approach89,190,691,785,988,6 " .Uncertaintyreasoning?
87,3?
91,690,4: :.
83,3 .. I89,2 .ITwo-enginestrategy91,793,193,888,493,73.
Factors in anaphora resolution: furtherissues that need attentionIn this section we address four questions that re-main unresolved or debatable: (i) how dependentare factors?
(ii) are preferences better than con-straints?
(iii) do factors hold good for all genres?and (iv) which is the best order to apply the fac-tors?3.1 Dependence and mutual  dependence offactorsWhile it is vital to arrive at a comprehensive list ofcontributory factors (or a core set of maximallycontributory factors), it is worthwhile to considernot only the impact of each factor on the resolu-tion process but also the impact of factors on otherfactors.
A key issue which needs further attentionis the "(mutual) dependence" of factors.In order to clarify the notion of (mutual) de-pendence, it would be helpful to view the "factors"as "symptoms", "indicators" i.e.
as "present" or"absent" in a certain discourse situation.
For in-stance, if "gender agreement" holds between acandidate for an anaphor and the anaphor itself,we say that the symptom or indicator "genderagreement" is present.
Similarly, if the candidate isin a subject position, we say that the symptom"subjecthood" is present.We define dependence/mutual dependence offactors in the following way.
Given the factors xand y, we say that factor y is dependent on factor xto the extent hat the presence of x implies y. Twofactors will be termed mutually dependent if eachdepends on the other.The phenomenon of (mutual) dependence hasnot yet been fully investigated, but we feel that itcan play an important role in the process ofanaphora resolution, especially in algorithmsbased on the ranking of preferences.
Informationon the degree of dependence would be especiallywelcome in a comprehensive probabilistic modeland will undoubtedly lead to more precise results.Our preliminary (and insufficient) observationssuggest hat there are more preferences which aredependent, than there are constraints.
The prefer-ences "object preference indicated by verbs" and"object preference indicated by nouns" (see sec-19tion 2.3.1) are a good example of mutual depend-ence.
Indeed, I had difficulties finding a discoursesituation in which those two factors did not occurtogether.
In a simple scoring formula it might bewiser to take only one of them into account; in amore sophisticated probabilistic model what weneed is sufficient empirical evidence on the degreeof this dependence in order to incorporate it in themodel.
In addition, the preference "lexical reitera-tion" is dependent (though to a lower degree) onthe preference "section heading" (this dependencedoes not seem to hold in the reverse direction, sothese two factors are not mutually dependent).Finally, it seems that "syntactic parallelism" and"semantic parallelism" are not completely inde-pendent.As far as constraints are concerned, those thatwe looked at (gender and number agreement, c-command constraints, semantic consistency), donot appear to be dependent at least for English.We have attempted to correct the mutual de-pendence between "object preference indicated byverbs" and "object preference indicated by nouns"by giving the latter symptom a lower numericalvalue.
However, more exact data on the degree ofdependence are needed and have to be captured inan appropriate probabilistic model.
An investiga-tion into the (mutual) dependence of factors on thebasis of large annotated corpora is one of our pri-ority research objectives.A simple, safe alternative would be to use a coreset restricted to "independent factors" but thiswould mean a compromise on performance sincethe benefit from some additional (though not in-dependent) factors would be lost.3.2 What is better: preferences or constraints?This is a another question which does not have anunambiguous answer.
Preferences may be safer inthat they, as opposed to constraints, may not ruleout a situation not modelled by the resolution en-gine.
On the other hand, as shown in our experi-ment, constraints could make the procedure fasterand more accurate.3.3.
Do the factors hold good for all genres?Perhaps we can speak of less "general" factors andmore "general" factors.
The factors "object prefer-ence indicated by verbs", "object preference indi-cated by nouns" and "section heading" appear tobe more "genre specific".
Their role, however,should not be underestimated - we have foundthem very useful in the textbook genre whichspans way beyond the sublanguage of ComputerScience.
In our experiments, we gave the factor"object preference indicated verbs" highly prefer-ential treatment.
As an illustration, the RAP algo-rithm has been reported (\[Dagan et al9l\] as hav-20ing difficulty in identifying the antecedent of "it"in the sentenceThe utility (CDVU) shows you a LIST4250,LIST38PP, or LIST3820 file i on your terminal fora format similar to that in which it i will be printed.where it pointed out "utility" as the most salientcandidate for the anaphor "it".
Both IA and URA,however, would correctly identify "file" as theantecedent because the "object preference indi-cated by verbs" (and "object preference indicatedby nouns") factors would regard "file" as highlysalient and would considerably raise its score.3.4 Order of constraints and priority ofpreferencesDoes order of constraints matter?
Since "absolute"constraints have to be met, not complying withany of them means discounting candidates.
There-fore, in our opinion, the order in which the con-straints are applied does not matter.In a system which incorporates both constraintsand preferences, it would be natural to start withconstraints and then to switch to preferences.
Wefear, however, that unless we have a comprehen-sive list of exceptions, simply discounting candi-dates on the basis of gender and number agree-ment in English could be risky (we are referring tothe number of cases where collective nouns maybe referred to by plural pronouns 3 and cases whereplural nouns may be referred to by a singular pro-noun4).
Therefore, unless we have such a compre-hensive list, our personal inclination would be torely on a preferences-based architecture.As far as preferences are concerned, it would benatural to start with the more "contributory" fac-tors in terms of numerical value.
In our experi-ments so far we have tried both descending(starting first the higher value factors) and ascend-ing orders of application.
We did not find any es-sential difference in the final result.
However, thesearching process in the second option was, asexpected, longer.4.
ConclusionThe paper demonstrates, on the basis of a com-parative study, that an anaphora resolution systemneeds not only a good set of contributory factorsbut also a clear strategy for their application.
Theresults of the study have implications for the de-velopment of anaphora resolution systems, sug-3For instance, nouns such as "government","parliament", police" "team" etc.
are usually referred toby "they4See section 2.3.3, the examples which follow conclu-sions (v) and (vi)gesting careful selection of both factors and com-putational strategies, or combination of them.AcknowledgementsMany thanks to Allan Ramsey for suggesting theidea of the comparative study.
I am also indebtedto Chris Paice and to the 3 referees for their usefulcomments.References\[Alshawi 90\] H. Alshawi - Resolving quasi logicalforms.
Computational Linguistics, 16:3, 1990\[Buchanan & Shortliffe 84\] B. Buchanan, FEd.
Shortliffe- Rule-based expert systems.
Addison-Wesley, 1984\[Carbonell & Brown 88\] J. Carbonell, R. Brown -Anaphora resolution: a multi-strategy approach.Proceedings of the 12. International Conference onComputational Linguistics COLING'88, Budapest,August, 1988\[Carter 87\] D. Carter - Interpreting anaphora in naturallanguage texts.
Chichester: Ellis Horwood, 1987\[Carter 90\] David M. Carter - Control issues in anaphorresolution.
Journal of Semantics, 7 1990\[Dagan 92\] I. Dagan - Multilingual statistical ap-proaches for natural language disambiguation (inHebrew).
PhD dissertation.
Technion-Israel Instituteof Technology, Haifa\[Dagan et al 91\] Ido Dagan, John Justeson, ShalomLappin, Hergert Leass and Amnon Ribak - Syntaxand lexical statistics in anaphora resolution.
AppliedArtificial Intelligence, 9  1995\[Lappin & Leass 94\] Sh.
Lappin, H. Leass - An algo-rithm for pronominal anaphora resolution.
Compu-tational Linguistics, 20(4), 1994\[Mitkov 94a\] Mitkov R. - An integrated model foranaphora resolution.
Proceedings of the 15th Inter-national Conference on Computational LinguisticsCOLING'94, Kyoto, Japan, 5-9 August 1994\[Mitkov 94b\] Mitkov R. - A new approach for trackingcenter.
In Proceedings of the International Confer-ence "New Methods in Language Processing",UMIST, Manchester, UK, 13-16 September 1994\[Mitkov 95\] R. Mitkov - An uncertainy reasoning ap-proach to anaphora resolution.
Proceedings of theNatural Language Pacific Rim Symposium, 4-7 De-cember 1995, Seoul, Korea\[Mitkov 96\] Mitkov R. - Anaphor esolution in NaturalLanguage Processing and Machine Translation.Proceedings of the International Colloquium on Dis-course Anaphora nd Anaphora Resolution.
Lancas-ter, 17-19 July 1996 (keynote speech)\[Rico P6rez 94\] C. Rico P6rez - Resolucidn de laan6fora discursiva mediante una estrategia de in-spiraci6n vectoral.
Proceedings of the SEPLN'94conference, Cordoba 20-22 July 1994\[Preul3 94 et al Preul3 S., Schmitz B., Hauenschild C.,Umbach C. - Anaphora Resolution bz MachineTranslation.
In W. Ramm (ed): Studies in MachineTranslation and Natural Language Processing, Vol-ume 6 "Text and content in Machine Translation:Aspects of discourse representation a d discourseprocessing", Office for Official Publications of theEuropean Community, Luxembourg, 1994\[Rich & LuperFoy 88\] E. Rich, S. LuperFoy - An archi-tecture for anaphora resolution.
Proceedings of theSecond Conference on Applied Natural LanguageProcessing, Austin, Texas, 9-12 February 198821
