Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 180?188,Sydney, July 2006. c?2006 Association for Computational LinguisticsSemantic Role Labeling via Instance-Based LearningChi-San Althon LinDepartment of Computer ScienceWaikato UniversityHamilton, New Zealandcl123@cs.waikato.ac.nzTony C. SmithDepartment of Computer ScienceWaikato UniversityHamilton, New Zealandtcs@cs.waikato.ac.nzAbstractThis paper demonstrates two methods toimprove the performance of instance-based learning (IBL) algorithms for theproblem of Semantic Role Labeling(SRL).
Two IBL algorithms are utilized:k-Nearest Neighbor (kNN), and PriorityMaximum Likelihood (PML) with amodified back-off combination method.The experimental data are the WSJ23 andBrown Corpus test sets from the CoNLL-2005 Shared Task.
It is shown that ap-plying the Tree-Based Predicate-Argument Recognition Algorithm(PARA) to the data as a preprocessingstage allows kNN and PML to deliver F1:68.61 and 71.02 respectively on theWSJ23, and F1: 56.96 and 60.55 on theBrown Corpus; an increase of 8.28 in F1measurement over the most recent pub-lished PML results for this problem(Palmer et al, 2005).
Training times forIBL algorithms are very much faster thanfor other widely used techniques for SRL(e.g.
parsing, support vector machines,perceptrons, etc); and the feature reduc-tion effects of PARA yield testing andprocessing speeds of around 1.0 secondper sentence for kNN and 0.9 second persentence for PML respectively, suggest-ing that IBL could be a more practicalway to perform SRL for NLP applica-tions where it is employed; such as real-time Machine Translation or AutomaticSpeech Recognition.1 IntroductionThe proceedings from CoNLL2004 andCoNLL2005 detail a wide variety of approachesto Semantic Role Labeling (SRL).
Many re-search efforts utilize machine learning (ML) ap-proaches; such as support vector machines (Mo-schitti et al, 2004; Pradhan et al, 2004), percep-trons (Carreras et al, 2004), the SNoW learningarchitecture (Punyakanok et al, 2004), EM-based clustering (Baldewein et al, 2004), trans-formation-based learning (Higgins, 2004), mem-ory-based learning (Kouchnir, 2004), and induc-tive learning (Surdeanu et al, 2003).
This papercompares two instance-based learning ap-proaches, kNN and PML.
The PML methodused here utilizes a modification of the backofflattice method used by Gildea & Jurafsky (2002)to use a set of basic features?specifically, thefeatures employed for learning in this paper arePredicate (pr), Voice (vo), Phrase Type (pt), Dis-tance (di), Head Word (hw), Path (pa), Preposi-tion in a PP (pp), and an ?Actor?
heuristic.The general approach presented here is anexample of memory-based learning.
Manyexisting SRL systems are also memory-based(Bosch et al, 2004;Kouchnir, 2004),implemented using TilMBL software(http://ilk.kub.nl/software.html) with advancedmethods such as Feature Weighting, and so forth.This paper measures the performance of kNNand PML for comparison in terms of accuracyand processing speed, both against each otherand against previously published results.2 Related WorkFeaturesMost of the systems outlined in CoNLL2004 andCoNLL2005 utilize as many as 30 features forlearning approaches to SRL.
The research pre-sented here uses only seven of these:180Figure 1.Illustration of path ?NP?S?VP?VBD?from a constituent ?The officer?
to the predicate ?came?.Predicate?
the given predicate lemmaVoice ?
whether the predicate is realized as anactive or passive construction (Pradhan et al,2004, claim approximately 11% of the sentences inPropBank use a passive instantiation)Phrase Type ?
the syntactic category (NP, PP, S,etc.)
of the phrase corresponding to the semanticargumentDistance ?the relative displacement from thepredicate, measured in intervening constituents(negative if the constituent appears prior to thepredicate, positive if it appears after it)Head Word ?
the syntactic head of the phrase,calculated by finding the last noun of a NounPhrasePath?
the syntactic path through the parse tree,from the parse constituent to the predicate beingclassified (for example, in Figure 1, the path fromArg0 ?
?The officer?
to the predicate ?came?, isrepresented with the string NP?S?VP?VBD?represent upward and downward movements in thetree respectively)Preposition?
the preposition of an argument in aPP, such as ?during?, ?at?, ?with?, etc (for exam-ple, in Figure 1, the preposition for the PP withArgm-Loc label is ?to?
).In addition, an actor heuristic is adopted: wherean instance can be labeled as A0 (actor) only ifthe argument is a subject before the predicate inactive voice, or if the preposition ?by?
appearsprior to this argument but after the predicate in apassive voice sentence.
For example, if there is aset of labels, A0 (subject or actor) V (active) A0(non actor), then the latter ?A0?
after V isskipped and labeled to another suitable role bythis heuristic; such as the label with the secondhighest probability for this argument accordingto the PML estimate, or with the second shortestdistance estimate by kNN.2.1 k Nearest Neighbour (kNN) AlgorithmOne instance-based learning algorithm is k-Nearest Neighbour (kNN), which is suitablewhen 1) instances can be mapped topoints/classifications in n-dimensional featuredimension, 2) fewer than 20 features are utilized,and 3) training data is sufficiently abundant.One advantage of kNN is that training is veryfast; one disadvantage is it is generally slow attesting.
The implementation of kNN is describedas following1.
Instance base:All the training data is stored in a formatsimilar to Bosch et al, (2004)?specifically,?Role, Predicate, Voice, Phrase type, Dis-tance, Head Word, Path?.
As an example in-stance, the second argument of a predicate?take?
in the training data is stored as:A0 take active NP ?1 classics NP?S?VP?VBDThis format maps each argument to six fea-ture dimensions + one classification.2.
Distance metric (Euclidean distance) is de-fined as:D(xi, xj) = ??
(ar(xi))-ar(xj))2where r=1 to n (n = number of different clas-sifications), and ar(x) is the r-th feature of in-stance x.
If instances xi and xj  are identical,then D(xi , xj )=0 otherwise D(xi , xj ) repre-sents the vector distance between xi and xj .3.
Classification functionGiven a query/test instance xq to be classified,let x1, ... xk denote the k instances from thetraining data that are nearest to xq.
The clas-sification function isF^(xq) <- argmax??
(v,f(xi))where i =1 to k,  v =1 to m (m = size of train-ing data), ?
(a,b)=1 if a=b, 0 otherwise; andv denotes a semantic role for each instanceof training data.Computational complexity for kNN is linear,such that TkNN -> O( m * n ), which is propor-tional to the product of the number of features (m)and the number of training instances (n).2.2 Priority Maximum Likelihood (PML)EstimationGildea & Jurafsky (2002), Gildea & Hocken-maier (2003) and Palmer et al, (2005) use a sta-tistical approach based on Maximum Likelihoodmethod for SRL, with different backoff combina-Predicate Arg0Argm-LOC181P(r | hw, pt, pre ,pp) P(r | pt, pa, pr, pp) P(r | pt, di, vo, pr, pp)P(r | hw, pr, pp) P(r | pt, pr, pp)P(r | pr, pp)    LocalGlobalP(r | hw, pp)   P(r | pt, di, vo, pp)tion methods in which selected probabilities arecombined with linear interpolation.
The prob-ability estimation or Maximum Likelihood isbased on the number of known features available.If the full feature set is selected the probability iscalculated byP (r | pr, vo, pt, di, hw, pa, pp) =# (r, pr, vo, pt, di, hw, pa, pp)  /# (pr, vo, pt, di, hw, pa, pp)Gildea & Jurafsky (2002) claims ?there is atrade-off between more-specific distributions,which have higher accuracy but lower coverage,and less-specific distributions, which have loweraccuracy but higher coverage?
and that the se-lection of feature subsets is exponential; and thatselection of combinations of different featuresubsets is doubly exponential, which is NP-complete.
Gildea & Jurafsky (2002) propose thebackoff combination in a linear interpolation forboth coverage and precision.
Following theirlead, the research presented here uses PriorityMaximum Likelihood Estimation modified fromthe backoff combination as follows:P?
( r | pr, vo, pt, di, hw, pa, pp) =?1*P(r | pr, pp) +?2*P(r | pt, pr, pp) +?3*P(r | pt, pa, pr, pp) + ?4*P(r | pt, di,vo, pp) + ?5*P(r | pt, di, vo, pr, pp) +?6*P(r | hw, pp) + ?7*P(r | hw, pr, pp)+ ?8*P(r | hw, pt, pr, pp)where ?i?i = 1.Figure 2 depicts a graphic organization of thepriority combination with more-specific distribu-tion toward the top, similar to Palmer et al (2005)but adding another preposition feature.
Thebackoff lattice is consulted to calculate probabili-ties for whichever subset of features is availableto combine.
As Gildea & Jurasksy (2002) state,?the less-specific distributions were used onlywhen no data were present for any more-specificdistribution.
Thus, the distributions selected arearranged in a cut across the lattice representingthe most-specific distributions for which data areavailable.
?Figure 2.
Combination of Priority Estimation forPML system originated from Gildea et al, (2002)The classification decision is made by the fol-lowing calculation for each argument in a sen-tence: argmaxr1 .. n P(r1?n | f1,..n) This approach isdescribed in more detail in Gildea and Jurasky(2002).The computational complexity of PML is hard tocalculate due to the many different distributionsat each priority level.
In Figure 2, the two calcu-lations P(r | hw, pp), and P(r | pt, di, vo, pp) be-long to the global search, while the rest belong toa local search which can reduce the computa-tional complexity.
Examination of the details ofexecution time (described in the results sectionof this paper) show that a plot of the executiontime exhibits logarithmic characteristics, imply-ing that the computational complexity for PMLis log-linear, such that TPML -> O( m * log n )where m denotes the size of features and n de-notes the size of training data.2.3 Predicate-Argument Recognition Algo-rithm (PARA)Lin & Smith (2005; 2006) describe a tree-basedpredicate-argument recognition algorithm(PARA).
PARA simply finds all boundaries forgiven predicates by browsing input parse-trees,such as given by Charniak?s parser or hand-corrected parses.
There are three major types ofphrases including given predicates, which are VP,NP, and PP.
Boundaries can be recognizedwithin boundary areas or from the top levels ofclauses (as in Xue & Palmer, 2004).
Figure 3shows the basic algorithm of PARA, and moredetails can be found in Lin & Smith (2006).
Thebest state-of-the-art ML technique using thesame syntactic information (Moschitti, 2005)only just outperforms a preliminary version ofPARA in F1 from 80.72 to 81.52 for boundaryrecognition tasks.
But PARA is much faster thanall other existing techniques, and is thereforeused for preprocessing in this study to minimizequery time when applying instance-based learn-ing to SRL.
The computational complexity ofPARA is constant.3 System ArchitectureThere are two stages to this system: the buildingstage (comparable to training for inductive sys-tems) and testing (or classification).
The build-ing stage shown in Figure 4 just stores all featurerepresentations of training instances in memorywithout any calculations.
All instances arestored in memory in the format described earlier,denoting {Role (r), Predicate (pr), Voice (vo),182Phrase Type (pt), Path (pa), Distance (di), HeadWord (hw), Preposition in a PP (pp) }.
Figure 5characterizes the testing stage, where new in-stances are classified by matching their featurerepresentation to all instances in memory in or-der to find the most similar instances.
There aretwo tasks during the testing stage: ArgumentIdentification (or Boundary recognition) per-formed by PARA, and Argument Classification(or Role Labeling) performed using either kNNor PML.
This approach is thus a ?lazy learning?strategy applied to SRL because no calculationsoccur during the building stage.4 Data, Evaluation, and ParsersThe research outlined here uses the dataset re-leased by the CoNLL-05 Shared Task(http://www.lsi.upc.edu/~srlconll/soft.html).
Itincludes several Wall Street Journal sectionswith parse-trees from both Charniak?s (2000)parser and Collins?
(1999) parser.
These sectionsare also part of the PropBank corpus(http://www.cis.upenn.edu/~treebank).
WSJ sec-tions 20 and 21 (with Charniak?s parses) wereused as test data.
PARA operates directly on theparse tree.
Evaluation is carried out using preci-sion, recall and F1 measures of assignment-accuracy of predicated arguments.
Precision (p)is the proportion of arguments predicated by thesystem that are correct.
Recall (r) is the propor-tion of correct arguments in the dataset that arepredicated by the system.Finally, the F1 measure computes the harmonicmean of precision and recall, such that F1 =2*p*r/ (p+r), and is the most commonly used primarymeasure when comparing different SRL systems.For consistency, the performance of PARA forboundary recognition is tested using the officialevaluation script from CoNLL 2005, srl-eval.pl(http://www.lsi.upc.edu/~srlconll/soft.html) in allexperiments presented in this paper.
Related sta-tistics of training data and testing data are out-lined in Table 1.
The average number of predi-cates in a sentence for WSJ02-21 is 2.27, andeach predicate comes with an average of 2.64arguments.Create_Boundary(predicate, tree)If the phrase type of the predicate == VP- find the boundary area ( the closest S clause)- find NP before predicate- If there is no NP, then find the closest NP from Ancestors.- find if WHNP in it?s siblings of the boundary area,if found  // for what, which, that , who,?-  if the word of the first WP?s family is ?what?
then- add WHNP to boundary listelse // not what, such as who which,?- find the closest NP from Ancestors- add the NP to the boundary list and addthis WHNP to boundary list as reference of NP-  add valid boundaries of the rest of constituents to boundary list.If phrase type of the predicate  ==NP- find the boundary area ( the NP clause)- find RB(POS) before predicate and add to boundary list.- Add this predicate to boundary list.- Add the rest of word group after the predicate and before the end of the NP clause as awhole boundary to boundary list.If phrase type of the predicate  ==PP- find the boundary area ( the PP clause)- find the closet NP from Ancestors if the lemma of the predicate is ?include?, and addthis NP to boundary list.
(special for PropBank)- Add this predicate to boundary list.-Add the rest of children of this predicate to boundary list or add one closest NP outside the boundaryarea to boundary list if there is no child after this predicate.Figure 3.
Outline of the Predicate Argument Recognition Algorithm (PARA)183Figure 4.
Illustration of System Architecture forthe building stageFigure 5.
Illustration of System Architecture forthe testing stage5 Experiments and ResultsExperimental results were obtained for part ofthe Brown corpus (the part provided by CoNLL-2005) and for Wall Street Journal (WSJ)Sections 21, 23, and 24 using different trainingdata sets (WSJ 21, WSJ 15 to 18, and WSJ 02 to21) shown in Table 1.
There are two tasks, Roleclassification with known arguments as input,and Boundary recognition & Role classificationwith gold (hand-corrected) parses or auto(Charniak?s) parses.
In addition, execution speed,the learning curve, and some further results forexploration of kNN and PML are also includedbelow.5.1 WSJ 24 with known argumentsTable 2 shows the results from kNN and PMLwith known boundaries/arguments (i.e.
the sys-tems are given the correct arguments for roleclassification).
All training datasets (WSJ02-21)include Charniak?s parse trees.
The table showsthat PML achieves F1: 2.69 better than kNN.5.2 Features & Heuristic on WSJ 24 withknown argumentsTable 3 shows the contribution of each featureand the actor heuristic by excluding one featureor heuristic.
It indicates that Head Word, Prepo-sition, and Distance are the three features thatcontribute most to system accuracy, and the addi-tional Actor heuristic is fourth.
Path, Phrase typeand Voice are the three features contibuting theleast for both classification algorithms.W02-21 W15-18 W21 W23 W24 BrownSent 39,832 8,936 1,671 2,416 1,346 426Tok 950,028 211,727 40,039 56,684 32,853 7,159Pred 90,750 19,098 3,627 5,267 3,248 804Verb 3,101 1,838 855 982 860 351Args 239,858 50,182 9,598 14,077 8,346 2,177Table 1.
Counts on the data sets used in this pa-per from CoNLL 2005 Shared TaskKnown Boundary on WSJ 24Algorithm P R F1 LacckNN 83.71 83.73 83.72 85.03PML 86.29 86.52 86.41 87.20Table 2.
Illustration of results by kNN (k=1)and PML on WSJ Section 24 with known argu-ments5.3 Learning CurveTable 4 shows that performance improves asmore training data is provided; and that PMLoutperforms kNN by about F1:2.8 on average forWSJ 24 for the three different training sets,mainly because the backoff lattice improves bothrecall and precision.
The table shows that it isnot always beneficial to include all features forlabeling all roles.
While P(r | hw, pt, pre, pp) ismainly for adjunctive roles (e.g.
AM-TMP), P(r |pt, di, vo, pr, pp) is mainly for core roles (e.g.
A0).5.4 Performance of Execution TimeBuilding (or training) time is about 2.5 minutesfor both PML and kNN, whereas it takes any-where from about 10 hours to 60 hours for otherML-based architectures (according to the datapresented by McCracken http://www.lsi.upc.es/~srlconll/st05/slides/mccracken.pdf).
Table 5shows average execution time (in seconds) persentence for the two algorithms.
PML runsfaster than kNN when all 20 training datasets areused (i.e.
WSJ 02 to 21).
A graphic illustrationof execution speed is shown in Figure 6.
Thesimulation formulas for PML and kNN are ?y =0.1734Ln(x) - 0.9046?
and ?y = 2.441*10-5 x +0.0129?
respectively.
?x?
denotes numbers oftraining sentences, and ?y?
denotes second persentence related to ?x?
training sentences.
Theexecution time for PML is about 8 times longerthan kNN for 1.7k training sentences, but PMLultimately runs faster than kNN on all 39.8Ktraining sentences (and, extrapolating from thegraph in Figure 6, on any larger datasets).
ThusPML seems generally more suitable for largetraining data.Input Instanceretriever InstanceBaseInput PARAInstanceBaseRoleClassifierOutput184Training sets  KNN PMLWSJ 21  0.050 0.396WSJ 15 - 18  0.241 0.687WSJ 02 - 21  1.000 0.941Table 5.
Illustration of results for executiontime by kNN and PML on WSJ 24 with knownarguments00.10.20.30.40.50.60.70.80.911.1Figure 6.
Curve of execution time for kNN (k=1)and PML on WSJ 24 with known arguments5.5 WSJ 24 with Gold parses and PARATable 6 shows performance for both systemswhen gold (hand-corrected) parses are suppliedand PARA preprocessing is employed.
Com-pared to the results in Table 4, the performanceon the combined training sets (WSJ 02 to 21)drops F1:9.24 and Lacc (label accuracy):2.4 forkNN; and drops F1:8.02 and Lacc:0.66 for PMLrespectively.
This may indicate that PML ismore error tolerant in labeling accuracy.
How-ever, both systems perform worse due largely toan idiosyncratic problem in the PARA-preprocessor when dealing with hand-correctedparses?ultimately due to a particular parsingerror.5.6 WSJ 24 with Charniak?s parses andPARATable 7 shows the performance of both systemsusing auto-parsing (i.e.
Charniak?s parser) andPARA argument recognition.
Compared to theresults in Table 4, the performance on all trainingsets (WSJ 02 to 21) drops F1:17.25 andLacc:0.65 for kNN, and F1:16.78 and Lacc:-0.78(i.e.
increasing Lacc) for PML respectively.Both systems drop a lot in F1 due to errorscaused by the auto-parser (in particular errorsrelating to punctuation), whose effects are subse-quently exacerbated by PARA.
Even so, the la-bel accuracy (Lacc) is more or less similar be-cause the training dataset are parsed byCharniak?s parser instead of gold parses.5.7 WSJ 23 with Charniak?s parses andPARATable 8 shows the results for WSJ 23, where theperformance of PML exceeds kNN by aboutF1:3.8.
WSJ 23 is used as a comparison datasetin SRL.
More comparisons with other systemsare shown in Table 12.5.8 Brown corpus with Charniak?s parsesand PARATable 9 shows the results when moving to a dif-ferent language domain?the Brown corpus.Both systems drop a lot in F1 .
Compared to WSJ23, MPL drops 10.47 in F1 and kNN, 11.65 in F1.These drops are caused partially by PARA, andpartially by classifiers.
PARA in Lin & Smith(2006) drops about 3.1 in F1 when moving to theBrown Corpus; but more research is required touncover the cause.5.9 Further results on kNN with all trainingdataTable 10 shows different results for various val-ues of k in kNN.
Both systems, GP (gold-parse)& PARA and CP (Charniak?s parse) & PARA,perform best (as measured by F1) when K is setas one.
But when the system is labeling a knownargument, selection of k=5 is better in terms ofboth F1 and Label accuracy (Lacc).5.10 Further results on PML with all train-ing dataTable 11 shows results for PML with differentmethods of calculating probabilities.
?L+G?means the basic probability distribution (fromFigure 2).
?L only?
and ?G only?
mean all prob-ability is calculated only as either ?local?
or?global?, respectively.
?L>>G?
means thatprobabilities are calculated globally only whenthe local probability is zero.
?L only?
is the fast-est approach, and ?G only?
the slowest (aboutfive seconds per sentence).
Both are poor in per-formance.
?L+G?
has the best result and?L>>G?
is rated as intermediate in performanceand execution time.5.11 Comparison with other systemsTable 12 shows results from other existing sys-tems.
In the second row (PARA+PML) istrained on all datasets (WSJ 02 to 21) for the?BR+RL?
task (to recognize argument bounda-ries and label arguments) on the test data WSJ 23,with an improvement of F1:8.28 in comparison tothe result of Palmer et al, (2005) given in the185first row.
The basic kNN in the fourth row,trained by four datasets (WSJ 15 to 18 in CoNLL2004) for the RL?
task (to label arguments bygiving the known arguments) on the test dataWSJ 21, increases F1:6.68 compared to the resultof Kouchnir (2004) in the third row.
Executiontime for our own re-implementation of Palmer(2005) is about 3.785 sec per sentence.
Instead ofcalculating each node in a parse tree like thePalmer (2005) model, PARA+PML can only fo-cus on essential nodes from the output of PARA,which helps to reduce the execution time as0.941 second per sentence.
Execution time byPalmer (2005) is about 4 times longer thanPARA+PML on the same machine (n.b.
execu-tion times are for a computer running Linux on aP4 2.6GHz CPU with 1G MBRAM).More details from different systems and combi-nations of systems are described in the proceed-ings of CoNLL-2005.kNN   k=1   PMLP R F1 P R F1ALL 83.71 83.73 83.72 86.29 86.52 86.41- Voice 81.69 81.60 81.64 85.64 85.90 85.77- Phrase Type 82.79 82.79 82.79 85.68 85.96 85.82- Distance 76.53 76.42 76.47 83.76 83.97 83.86- Head Word 78.26 78.05 78.15 81.84 81.96 81.90- Path 83.67 83.63 83.65 85.44 85.72 85.58- Preposition 79.40 79.29 79.33 82.02 82.12 82.07- Actor 80.38 80.64 80.51 84.74 85.01 84.81Table 3.
Illustration of contribution for each feature and the Actor heuristic by kNN (k=1) and PMLon WSJ 24 with known argumentskNN   k=1      PMLTraining sets  P R F1 Lacc  P R F1 LaccWSJ 21  76.76 77.02 76.89 78.03  79.20 79.26 79.23 80.40WSJ 15 - 18  80.40 80.18 80.29 81.85  83.61 83.70 83.66 84.61WSJ 02 - 21  83.71 83.73 83.72 85.03  86.29 86.52 86.41 87.20Table 4.
Illustration of results with different training datasets by kNN (k=1) and PML on WSJ 24with known argumentskNN   k=1      PMLTraining sets  P R F1 Lacc  P R F1 LaccWSJ 21  67.96 67.90 67.93 75.61  70.51 70.57 70.54 78.17WSJ 15 - 18  72.42 72.25 72.34 80.66  75.64 75.62 75.63 83.55WSJ 02 - 21  74.48 74.48 74.48 82.63  78.39 78.40 78.39 86.54Table 6.
Illustration of results with different training datasets by kNN (k=1) and PML on WSJ 24with gold (Hand corrected) parses and PARAkNN   k=1      PMLTraining sets  P R F1 Lacc  P R F1 LaccWSJ 21  61.05 60.90 60.98 77.45  63.75 63.43 63.59 80.70WSJ 15 - 18  64.66 64.11 64.38 82.13  67.55 67.15 67.35 85.23WSJ 02 - 21  66.62 66.32 66.47 84.38  69.81 69.45 69.63 87.98Table 7.
Illustration of results with different training datasets by kNN (k=1) and PML on WSJ 24with Charniak?s parses and PARAkNN   k=1      PMLTraining sets  P R F1 Lacc  P R F1 LaccWSJ 21  62.87 62.55 62.71 78.85  64.94 64.49 64.71 81.31WSJ 15 - 18  66.66 65.96 66.31 83.60  69.05 68.52 68.79 86.14WSJ 02 - 21  68.92 68.31 68.61 86.20  71.24 70.79 71.02 88.77Table 8.
Illustration of results with different training datasets by kNN (k=1) and PML on WSJ 23with Charniak?s parses and PARA186kNN   k=1      PMLTraining sets  P R F1 Lacc  P R F1 LaccWSJ 21  52.56 51.40 51.97 67.70  55.17 53.88 54.52 70.15WSJ 15 - 18  55.58 54.20 54.88 71.56  59.10 57.56 58.32 75.53WSJ 02 - 21  57.71 56.22 56.96 74.14  61.26 59.85 60.55 78.26Table 9.
Illustration of results with different training datasets by kNN (k=1) and PML on Brown Cor-pus with Charniak?s parses and PARAKnown boundary  GP & PARA  CP & PARAK F1 Lacc  F1 Lacc  F1 Lacc1 83.72 85.03  74.48 82.63  66.47 84.383 83.67 85.13  74.33 82.70  65.94 84.035 83.89 85.16  74.14 82.28  65.89 83.817 83.27 84.66  73.43 81.59  65.52 83.549 82.86 84.25  73.00 81.22  65.13 82.99Table 10.
Illustration of results by kNN with different K values on WSJ 24 with known arguments,Gold (Hand-corrected) parses & PARA and Charniak?s parses & PARAKnown boundary on WSJ 24Method P R F1 Lacc T (Sec/Sen)L+G 86.29 86.52 86.41 87.20 0.941L only 80.78 80.73 80.76 81.70 0.027G only 75.60 76.35 75.97 77.52 5.094L>>G 82.44 82.42 82.43 83.29 0.128Table 11.
Illustration of results by PML with different methods on WSJ 24 with known argumentsSystem Train Test Tasks P R F1 Lacc TPalmer (2005) W02-21 W23 BR+RL 68.60 57.80 62.74 81.70 3.785PARA+PML W02-21 W23 BR+RL 71.24 70.79 71.02 88.77 0.941Kouchnir (2004) W15-18 W21 RL 75.71 74.60 75.15kNN W15-18 W21 RL 81.86 81.79 81.83 83.57 0.242Table 12.
Illustration of results for different tasks by different systems and training datasets on differ-ent testing datasets6 Summary and RemarksThis paper has shown that basic syntactic infor-mation is useful for Semantic role labeling usinginstance-based learning techniques.
Specifically,the following have been demonstrated:1.
It is possible to achieve acceptable F1scores with considerably faster executiontimes (compared to Gildea & Jurasky, 2002)for the Semantic role labeling problem us-ing the Priority Maximum Likelihood in-stance-based learning algorithm and theTree-based Predicate-Argument Algorithm(PARA) as a preprocessing step, withoutany training given a state-of-the-art parsersuch as Charniak?s parser.
The overall per-formance on WSJ 23 dataset is 71.02 in F1score.
Performance drops to 60.55 for theBrown corpus, but this appears to be simi-lar to performance drops experienced byother systems reported in CoNLL-2005.2.
F1 performance is better for PML than forkNN, where the computational complexityfor PML is O( m * log n ) as opposed toO( m * n ) for kNN, where m denotes thenumber of features and n denotes the num-ber of training instances.3.
Execution time for the instance-basedlearning presented here is about four timesfaster for SRL than the comparable ap-proach used by Palmer, (2005).
That is,PARA plays an important role reducing theoverhead during classification when usinginstance-based learning.4.
Using PARA, and other modifications suchas the preposition feature and Actor heuris-tic, improves the accuracy of both kNN andPML in comparison to similar approaches.1875.
The best system developed for this paper(PML & PARA) is still outperformed bysome of the best systems from CoNLL-2005 when it comes to accuracy, but it ismuch simpler and is many orders-of-magnitude faster at delivering acceptableperformance.With the latest revised and optimized PML, theperformance on WSJ 23 is 71.22 in F1, and thespeed is 0.623 second per sentence with 3.0GCPU and 1 G RAM.
Koomen et al (2006), withmore than 25 features, achieved the best resultsreported in CoNLL2005 on WSJ 24; but PML?sperformance (using PARA as a preprocessor, andseven features) achieves an F1 measure 5.10 lessthan Kooman?s system (74.76) on WSJ 24 utilis-ing Charniak-1 parses, and 4.07 less when usingKooman?s test result (WSJ 23) as known-boundary input.
In this experiment, with the Ac-tor heuristic, PML delivers better accuracy forA0 (89.96%) than Kooman?s (88.22%), but therecall (83.53%) is 4.35 % lower than Kooman?s(87.88%).
There are some spaces to improvePML such as low accuracy on AM-MOD, andAM-NEG, and duplicate core roles, and forth.Future work will investigate using more features,new heuristics and/or other ML approaches toimprove the performance of instance-basedlearning algorithms at the SRL task.ReferencesBaldewein, U, Erk, K, Pad?, S. and Prescher, D.(2004).
Semantic role labelling with similarity-based generalization using EM-based clustering InProceedings of Senseval-3 pp.
64-68Bosch, A. V. D., Canisius, S., Daelemans, W., andSang, E. T. K. (2004).
Memory-based semanticrole labeling: Optimizing features, algorithm andoutput.
In Proceeding of CoNLL?2004 Shared Task.Carreras, X., M?rquez, L. and Chrupa?a, G. (2004).Hierarchical Recognition of Propositional Argu-ments with Perceptrons.
In Proceeding ofCoNLL?2004 Shared Task.Charniak, E. (2000).
A Maximum-Entropy-InspiredParser.
In Proceedings of NAACL-2000.Collins, M. (1999).
Head-Driven Statistical Modelsfor Natural Language Parsing.
PhD Dissertation,University of Pennsylvania.Gildea, D. and Jurafsky, D. (2002).
Automatic Label-ing of Semantic Roles.
Computational Linguistics,28(3):245-288.Gildea, D. and Hockenmaier, J.
(2003).
IdentifyingSemantic Roles Using Combinatory CategorialGrammar .
In Proceedings of EMNLP-2003, Sap-poro, Japan.Higgins, D. (2004).
A transformation-based approachto argument labeling.
In Proceeding ofCoNLL?2004 Shared Task.Kouchnir, B.
(2004).
A Memory-Based Approach forSemantic Role Labeling.
In Proceeding ofCoNLL?2004 Shared Task.Kooman, P., Punyakanok, V., Roth, D., and Yih, W.(2005).
Generalized Inference with Multiple Se-mantic Role Labeling Systems.
In Proceedings ofCoNLL-2005.Lin, C.S.
A. and Smith, T. C. (2005).
Semantic rolelabeling via Consensus in Pattern-matching.
InProceedings of CoNLL-2005.Lin, C.S.
A. and Smith, T. C. (2006).
A Tree-basedAlgorithm for Predicate-Argument Recognition.
InBulletin of Association for Computing MachineryNew Zealand (ACM_NZ), volumn 2, issue 1.Moschitti, A., Giuglea, A. M., Coppola, B., and Basili,R.
(2005).
Semantic role labeling using supportvector machines.
In Proceedings of CoNLL-2005.Palmer, M., Gildea, D., and Kingsbury, P., (2005).The Propostin Bank: An Annotated Corpus of Se-mantic Roles.
In Proceedings of ACL: Volume 31,Number 1. p72-105.Pradhan, S., Ward, W., Hacioglu, K., Martin, J. H.,Jurafsky, D. (2004).
Shallow Semantic Parsing us-ing Support Vector Machines, in Proceedings ofthe Human Language Technology Confer-ence/North American chapter of the Associationfor Computational Linguistics annual meeting(HLT/NAACL-2004), Boston, MA.Punyakanok, V., Roth, D., Yih, W., and Zimak, D.(2004).
Semantic Role Labeling via Integer LinearProgramming Inference .
In Proceedings of.
the In-ternational Conference on Computational Linguis-tics (COLING),2004.Surdeanu, M., Harabagiu, S., Williams, J., andAarseth, P. (2003).
Using Predicate-ArgumentStructures for Information Extraction.
In Proceed-ings of ACL 2003, Sapporo, Japan.188
