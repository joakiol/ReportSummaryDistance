Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 401?406,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsTerminal-Aware Synchronous BinarizationLicheng Fang, Tagyoung Chung and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractWe present an SCFG binarization algorithmthat combines the strengths of early termi-nal matching on the source language side andearly language model integration on the tar-get language side.
We also examine how dif-ferent strategies of target-side terminal attach-ment during binarization can significantly af-fect translation quality.1 IntroductionSynchronous context-free grammars (SCFG) are be-hind most syntax-based machine translation mod-els.
Efficient machine translation decoding with anSCFG requires converting the grammar into a bina-rized form, either explicitly, as in synchronous bina-rization (Zhang et al, 2006), where virtual nontermi-nals are generated for binarization, or implicitly, asin Earley parsing (Earley, 1970), where dotted itemsare used.Given a source-side binarized SCFG with termi-nal set T and nonterminal set N , the time complex-ity of decoding a sentence of length n with a m-gramlanguage model is (Venugopal et al, 2007):O(n3(|N | ?
|T |2(m?1))K)where K is the maximum number of right-hand-sidenonterminals.
SCFG binarization serves two impor-tant goals:?
Parsing complexity for unbinarized SCFGgrows exponentially with the number of non-terminals on the right-hand side of grammarrules.
Binarization ensures cubic time decod-ing in terms of input sentence length.?
In machine translation, integrating languagemodel states as early as possible is essential toreducing search errors.
Synchronous binariza-tion (Zhang et al, 2006) enables the decoder toincorporate language model scores as soon as abinarized rule is applied.In this paper, we examine a CYK-like syn-chronous binarization algorithm that integrates anovel criterion in a unified semiring parsing frame-work.
The criterion we present has explicit consider-ation of source-side terminals.
In general, terminalsin a rule have a lower probability of being matchedgiven a sentence, and therefore have the effect of?anchoring?
a rule and limiting its possible applica-tion points.
Hopkins and Langmead (2010) formal-ized this concept as the scope of a rule.
A rule ofscope of k can be parsed in O(nk).
The scope of arule can be calculated by counting the number of ad-jacent nonterminal pairs and boundary nonterminals.For example,A?
w1BCw2Dhas scope two.
Building on the concept of scope,we define a cost function that estimates the expectednumber of hyperedges to be built when a particularbinarization tree is applied to unseen data.
This ef-fectively puts hard-to-match derivations at the bot-tom of the binarization tree, which enables the de-coder to decide early on whether an unbinarized rulecan be built or not.We also investigate a better way to handle target-side terminals during binarization.
In theory, differ-ent strategies should produce equivalent translationresults.
However, because decoding always involves4010200040006000800010000120001400016000180001 2 3 4 5 6 7Number of rulesNumber of right-hand-side nonterminalsTotalBinarizableMonotonicFigure 1: Rule Statisticspruning, we show that different strategies do have asignificant effect in translation quality.Other works investigating alternative binarizationmethods mostly focus on the effect of nonterminalsharing.
Xiao et al (2009) also proposed a CYK-like algorithm for synchronous binarization.
Appar-ently the lack of virtual nonterminal sharing in theirdecoder caused heavy competition between virtualnonterminals, and they created a cost function to?diversify?
binarization trees, which is equivalent tominimizing nonterminal sharing.DeNero et al (2009b) used a greedy method tomaximize virtual nonterminal sharing on the sourceside during the -LM parsing phase.
They show thateffective source-side binarization can improve the ef-ficiency of parsing SCFG.
However, their methodworks only on the source side, and synchronous bina-rization is put off to the +LM decoding phase (DeN-ero et al, 2009a).Although these ideas all lead to faster decodingand reduced search errors, there can be conflicts inthe constraints each of them has on the form of rulesand accommodating all of them can be a challenge.In this paper, we present a cubic time algorithm tofind the best binarization tree, given the conflictingconstraints.2 The Binarization AlgorithmAn SCFG rule is synchronously binarizable if whensimultaneously binarizing source and target sides,virtual nonterminals created by binarizations alwayshave contiguous spans on both sides (Huang, 2007).Algorithm 1 The CYK binarization algorithm.CYK-BINARIZE(X ?
?
?, ??
)for i = 0 .
.
.
|?| ?
1 doT [i, i + 1]?
cinit(i)for s = 2 .
.
.
|?| dofor i = 0 .
.
.
|?|-1 doj ?
i + sfor k = i + 1 .
.
.
j ?
1 dot?
T [i, k] + T [k, j] + c(?i, k, j?
)T [i, j]?
min(T [i, j], t)Even with the synchronous binarization constraint,many possible binarizations exist.
Analysis of ourChinese-English parallel corpus has shown that themajority of synchronously binarizable rules with ar-ity smaller than 4 are monotonic, i.e., the target-sidenonterminal permutation is either strictly increasingor decreasing (See Figure 1).
For monotonic rules,any source-side binarization is also a permissiblesynchronous binarization.The binarization problem can be formulated as asemiring parsing (Goodman, 1999) problem.
Wedefine a cost function that considers different bina-rization criteria.
A CYK-like algorithm can be usedto find the best binarization tree according to thecost function.
Consider an SCFG rule X ?
?
?, ?
?,where ?
and ?
stand for the source side and the tar-get side.
Let B(?)
be the set of all possible bina-rization trees for ?.
With the cost function c definedover hyperedges in a binarization tree t, the optimalbinarization tree t?
ist?
= argmint?B(?
)?h?tc(h)where c(h) is the cost of a hyperedge h in t.The optimization problem can be solved by Al-gorithm 1.
?i, k, j?
denotes a hyperedge h that con-nects the spans (i, k) and (k, j) to the span (i, j).cinit is the initialization for the cost function c. Wecan recover the optimal source-side binarization treeby augmenting the algorithm with back pointers.Binarized rules are generated by iterating over thenodes in the optimal binarization tree, while attach-ing unaligned target-side terminals.
At each treenode, we generate a virtual nonterminal symbol byconcatenating the source span it dominates.We define the cost function c(h) to be atuple of component cost functions: c(h) =402(c1(h), c2(h), ...).
When two costs a and b are com-pared, the components are compared piecewise, i.e.c < c?
?
c1 < c?1 ?
(c1 = c?1 ?
c2 < c?2) ?
.
.
.If the (min,+) operators on each component costsatisfy the semiring properties, the cost tuple is alsoa semiring.
Next, we describe our cost functions andhow we handle target-side terminals.2.1 Synchronous Binarization as a CostWe use a binary cost b to indicate whether a binariza-tion tree is a permissible synchronous binarization.Given a hyperedge ?i, k, j?, we say k is a permissiblesplit of the span (i, j) if and only if the spans (i, k)and (k, j) are both synchronously binarizable andthe span (i, j) covers a consecutive sequence of non-terminals on the target side.
A span is synchronouslybinarizable if and only if the span is of length one,or a permissible split of the span exists.
The cost bis defined as:b(?i, k, j?)
={T if k is a permissible split of (i, j)F otherwisebinit(i) = TUnder this configuration, the semiring operators(min,+) defined for the cost b are (?,?).
Using b asthe first cost function in the cost function tuple guar-antees that we will find a tree that is a synchronouslybinarized if one exists.2.2 Early Source-Side Terminal MatchingWhen a rule is being applied while parsing a sen-tence, terminals in the rule have less chance of be-ing matched.
We can exploit this fact by taking ter-minals into account during binarization and placingterminals lower in the binarization tree.
Consider thefollowing SCFG rule:VP ?
PP??
JJ NN,propose a JJ NN PPThe synchronous binarization algorithm of Zhang etal.
(2006) binarizes the rule1 by finding the right-most binarizable points on the source side:1We follow Wu (1997) and use square brackets for straightrules and pointed brackets for inverted rules.
We also markbrackets with indices to represent virtual nonterminals.VP ?
PP [??
[JJ NN]1]2,[[propose a JJ NN]1]2 PPThe source side of the first binarized rule ?
[]1 ?
JJNN, propose a JJ NN?
contains a very frequent non-terminal sequence ?JJ NN?.
If one were to parsewith the binarized rule, and if the virtual nontermi-nal []1 has been built, the parser needs to continuefollowing the binarization tree in order to determinewhether the original rule would be matched.
Further-more, having two consecutive nonterminals adds tocomplexity since the parser needs to test each splitpoint.The following binarization is equally valid but in-tegrates terminals early:VP ?
PP [[??
JJ]1 NN]2,[[propose a JJ]1 NN]2 PPHere, the first binarized rule ?
[]1 ?
??
JJ, pro-pose a JJ?
anchors on a terminal and enables earlierpruning of the original rule.We formulate this intuition by asking the ques-tion: given a source-side string ?, what binarizationtree, on average, builds the smallest number of hy-peredges when the rule is applied?
This is realizedby defining a cost function e which estimates theprobability of a hyperedge ?i, k, j?
being built.
Weuse a simple model: assume each terminal or non-terminal in ?
is matched independently with a fixedprobability, then a hyperedge ?i, k, j?
is derived ifand only if all symbols in the source span (i, j) arematched.
The cost e is thus defined as2e(?i, k, j?)
=?i?`<jp(?`)einit(i) = 0For terminals, p(?`) can be estimated by countingthe source side of the training corpus.
For nontermi-nals, we simply assume p(?`) = 1.With the hyperedge cost e, the cost of a binariza-tion tree t is?h?t e(h), i.e., the expected number ofhyperedges to be built when a particular binarizationof a rule is applied to unseen data.3 The operators2In this definition, k does not appear on the right-hand sideof the equation because all edges leading to the same span sharethe same cost value.3Although this cost function is defined as an expectation, itdoes not form an expectation semiring (Eisner, 2001) because403for the cost e are the usual (min,+) operators onreal numbers.2.3 Maximizing Nonterminal SharingDuring binarization, newly created virtual nontermi-nals are named according to the symbols (terminalsand nonterminals) that they generate.
For example, anew virtual nonterminal covering two nonterminalsNP and VP is named NP+VP.
To achieve maximumvirtual nonterminal sharing, we also define a costfunction n to count the number new nonterminalsgenerated by a binarization tree.
We keep track ofall the nonterminals that have been generated whenbinarizing a rule set.
When the i?th rule is beingbinarized, a nonterminal is considered new if it ispreviously unseen in binarizing rules 1 to i?1.
Thisgreedy approach is similar to that of DeNero et al(2009b).
The cost function is thus defined as:n(?i, k, j?)
={1 if the VT for span (i, j) is new0 otherwiseninit(i) = 0The semiring operators for this cost are also(min,+) on real numbers.2.4 Late Target-Side Terminal AttachmentOnce the optimal source-side binarization tree isfound, we have a good deal of freedom to attachtarget-side terminals to adjacent nonterminals, aslong as the bracketing of nonterminals is not vio-lated.
The following example is taken from Zhanget al (2006):ADJP ?
RB??
PP?
NN,RB responsible for the NN PPWith the source-side binarization fixed, we can pro-duce distinct binarized rules by choosing differentways of attaching target-side terminals:ADJP ?
[RB??
]1 ?
[PP?
]3 NN ?2,[RB]1 ?
resp.
for the NN [PP]3 ?2ADJP ?
[RB??
]1 ?
[PP?
]3 NN ?2,[RB]1 resp.
for the ?
NN [PP]3 ?2The first binarization is generated by attaching thetarget-side terminals as low as possible in a post-it is defined as an expectation over input strings, instead of anexpectation over trees.order traversal of the binarization tree.
The conven-tional wisdom is that early consideration of target-side terminals promotes early language model scoreintegration (Huang et al, 2009).
The second bina-rization, on the contrary, attaches the target-side ter-minals as high as possible in the binarization tree.We argue that this late target-side terminal attach-ment is in fact better for two reasons.First, as in the example above, compare the fol-lowing two rules resulting from early attachment oftarget terminals and late attachment of target termi-nals:?
?2 ?
[]3 NN, resp.
for the NN []3?
?2 ?
[]3 NN, NN []3The former has a much smaller chance of sharingthe same target side with other binarized rules be-cause on the target side, many nonterminals will beattached without any lexical evidence.
We are morelikely to have a smaller set of rules with the latterbinarization.Second, with the presence of pruning, dynamicprogramming states that are generated by rules withmany target-side terminals are disadvantaged whencompeting with others in the same bin because ofthe language model score.
As a result, these wouldbe discarded earlier, even if the original unbinarizedrule has a high probability.
Consequently, we losethe benefit of using larger rules, which have morecontextual information.
We show in our experimentthat late target side terminal attachment significantlyoutperforms early target side terminal attachment.Although the problem can be alleviated by pre-computing a language model score for the originalunbinarized rule and applying the heuristic to its bi-narized rules, this still grants no benefit over late ter-minal attachment.
We show in our experiment thatlate target-side terminal attachment significantly out-performs early target side terminal attachment.3 Experiments3.1 SetupWe test our binarization algorithm on an Chinese-English translation task.
We extract a GHKM gram-mar (Galley et al, 2004) from a parallel corpus withthe parsed English side with some modification so404-395-390-385-380-375-370-365-360-35510  100ModelScore(log-probability)Seconds / Sentence (log scale)(b,n)-early(b,n)-late(b,e,n)-early(b,e,n)-lateFigure 2: Model Scores vs. Decoding Time17.51818.51919.52020.510  100BLEUSeconds / Sentence (log scale)(b,n)-early(b,n)-late(b,e,n)-early(b,e,n)-lateFigure 3: BLEU Scores vs Decoding Timeas not to extract unary rules (Chung et al, 2011).The corpus consists of 250K sentence pairs, whichis 6.3M words on the English side.
A 392-sentencetest set was to evaluate different binarizations.Decoding is performed by a general CYK SCFGdecoder developed in-house and a trigram languagemodel is used.
The decoder runs the CYK algorithmwith cube-pruning (Chiang, 2007).
In all our exper-iments, we discard unbinarizable rules, which havebeen shown by Zhang et al (2006) to have no signif-icant effect on translation accuracy.3.2 ResultsWe first discuss effects of maximizing nonterminalsharing.
Having nonterminal sharing maximizationas a part of the cost function for binarization didyield slightly smaller grammars.
However, we couldnot discern any noticeable difference or trend interms of BLEU score, decoding speed, or modelscore when comparing translation results that usedgrammars that employed nonterminal sharing max-imization and ones that did not.
In the rest of thissection, all the results we discuss use nonterminalsharing maximization as a part of the cost function.We then compare the effects of early target-sideterminal attachment and late attachment.
Figure 2shows model scores of each decoder run with vary-ing bin sizes, and Figure 3 shows BLEU scoresfor corresponding runs of the experiments.
(b,n)-early is conventional synchronous binarization withearly target-side terminal attachment and nontermi-nal sharing maximization, (b,n)-late is the same set-ting with late target-side terminal attachment.
Thetuples represent cost functions that are discussed inSection 2.
The figures clearly show that late attach-ment of target-side terminals is better.
AlthoughFigure 3 does not show perfect correlation with Fig-ure 2, it exhibits the same trend.
The same goes for(b,e,n)-early and (b,e,n)-late.Finally, we examine the effect of including thesource-side terminal-aware cost function, denoted?e?
in our cost tuples.
Comparing (b,e,n)-late with(b,n)-late, we see that terminal-aware binarizationgives better model scores and BLEU scores.
Thetrend is the same when one compares (b,e,n)-earlyand (b,n)-early.4 ConclusionWe examined binarizing synchronous context-freegrammars within a semiring parsing framework.
Weproposed binarization methods that explicitly taketerminals into consideration.
We have found that al-though binarized rules are already scope 3, we canstill do better by putting infrequent derivations aslow as possible in a binarization tree to promoteearly pruning.
We have also found that attachingtarget side terminals as late as possible promotessmarter pruning of rules thereby improving modelscore and translation quality at decoding time.
Im-provements we discuss in this paper result in bettersearch, and hence better translation.Acknowledgments We thank Hao Zhang for use-ful discussions and the anonymous reviewers fortheir helpful comments.
This work was supportedby NSF grants IIS-0546554 and IIS-0910611.405ReferencesDavid Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Tagyoung Chung, Licheng Fang, and Daniel Gildea.2011.
Issues concerning decoding with synchronouscontext-free grammar.
In Proceedings of the ACL2011 Conference Short Papers, Portland, Oregon, June.Association for Computational Linguistics.J.
DeNero, A. Pauls, and D. Klein.
2009a.
Asynchronousbinarization for synchronous grammars.
In Proceed-ings of the ACL-IJCNLP 2009 Conference Short Pa-pers, pages 141?144.
Association for ComputationalLinguistics.John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.2009b.
Efficient parsing for transducer grammars.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 227?235, Boulder, Colorado, June.
Associationfor Computational Linguistics.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 6(8):451?455.J.
Eisner.
2001.
Expectation semirings: Flexible EMfor learning finite-state transducers.
In Proceedings ofthe ESSLLI workshop on finite-state methods in NLP.Citeseer.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of the 2004 Meeting of the North Americanchapter of the Association for Computational Linguis-tics (NAACL-04), pages 273?280.Joshua Goodman.
1999.
Semiring parsing.
Computa-tional Linguistics, 25(4):573?605.Mark Hopkins and Greg Langmead.
2010.
SCFG decod-ing without binarization.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 646?655, Cambridge, MA,October.
Association for Computational Linguistics.Liang Huang, Hao Zhang, Daniel Gildea, and KevinKnight.
2009.
Binarization of synchronouscontext-free grammars.
Computational Linguistics,35(4):559?595.Liang Huang.
2007.
Binarization, synchronous bina-rization, and target-side binarization.
In Proceedingsof the NAACL/AMTA Workshop on Syntax and Struc-ture in Statistical Translation (SSST), pages 33?40,Rochester, NY.Ashish Venugopal, Andreas Zollmann, and Stephan Vo-gel.
2007.
An efficient two-pass approach tosynchronous-CFG driven statistical MT.
In NAACL07,Rochester, NY, April.Dekai Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Compu-tational Linguistics, 23(3):377?403.T.
Xiao, M. Li, D. Zhang, J. Zhu, and M. Zhou.
2009.Better synchronous binarization for machine transla-tion.
In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing: Vol-ume 1-Volume 1, pages 362?370.
Association for Com-putational Linguistics.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of the 2006 Meeting of theNorth American chapter of the Association for Compu-tational Linguistics (NAACL-06), pages 256?263, NewYork, NY.406
