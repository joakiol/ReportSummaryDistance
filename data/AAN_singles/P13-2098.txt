Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 549?555,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsReranking with Linguistic and Semantic Featuresfor Arabic Optical Character RecognitionNadi Tomeh, Nizar Habash, Ryan Roth, Noura FarraCenter for Computational Learning Systems, Columbia University{nadi,habash,ryanr,noura}@ccls.columbia.eduPradeep Dasigi Mona DiabSafaba Translation Solutions The George Washington Universitypradeep@safaba.com mtdiab@gwu.eduAbstractOptical Character Recognition (OCR) sys-tems for Arabic rely on information con-tained in the scanned images to recognizesequences of characters and on languagemodels to emphasize fluency.
In this paperwe incorporate linguistically and seman-tically motivated features to an existingOCR system.
To do so we follow an n-bestlist reranking approach that exploits recentadvances in learning to rank techniques.We achieve 10.1% and 11.4% reduction inrecognition word error rate (WER) relativeto a standard baseline system on typewrit-ten and handwritten Arabic respectively.1 IntroductionOptical Character Recognition (OCR) is the taskof converting scanned images of handwritten,typewritten or printed text into machine-encodedtext.
Arabic OCR is a challenging problem dueto Arabic?s connected letter forms, consonantaldiacritics and rich morphology (Habash, 2010).Therefore only a few OCR systems have been de-veloped (Ma?rgner and Abed, 2009).
The BBNByblos OCR system (Natajan et al, 2002; Prasadet al, 2008; Saleem et al, 2009), which we usein this paper, relies on a hidden Markov model(HMM) to recover the sequence of characters fromthe image, and uses an n-gram language model(LM) to emphasize the fluency of the output.
Foran input image, the OCR decoder generates an n-best list of hypotheses each of which is associatedwith HMM and LM scores.In addition to fluency as evaluated by LMs,other information potentially helps in discrimi-nating good from bad hypotheses.
For example,Habash and Roth (2011) use a variety of linguistic(morphological and syntactic) and non-linguisticfeatures to automatically identify errors in OCRhypotheses.
Another example presented by De-vlin et al (2012) shows that using a statistical ma-chine translation system to assess the difficulty oftranslating an Arabic OCR hypothesis into Englishgives valuable feedback on OCR quality.
There-fore, combining additional information with theLMs could reduce recognition errors.
However,direct integration of such information in the de-coder is difficult.A straightforward alternative which we advo-cate in this paper is to use the available informa-tion to rerank the hypotheses in the n-best lists.The new top ranked hypothesis is considered asthe new output of the system.
We propose com-bining LMs with linguistically and semanticallymotivated features using learning to rank meth-ods.
Discriminative reranking allows each hypoth-esis to be represented as an arbitrary set of featureswithout the need to explicitly model their interac-tions.
Therefore, the system benefits from globaland potentially complex features which are notavailable to the baseline OCR decoder.
This ap-proach has successfully been applied in numerousNatural Language Processing (NLP) tasks includ-ing syntactic parsing (Collins and Koo, 2005), se-mantic parsing (Ge and Mooney, 2006), machinetranslation (Shen et al, 2004), spoken languageunderstanding (Dinarelli et al, 2012), etc.
Fur-thermore, we propose to combine several rankingmethods into an ensemble which learns from theirpredictions to further reduce recognition errors.We describe our features and reranking ap-proach in ?2, and we present our experiments andresults in ?3.2 Discriminative Reranking for OCREach hypothesis in an n-best list {hi}ni=1 is repre-sented by a d-dimensional feature vector xi ?
Rd.Each xi is associated with a loss li to generate alabeled n-best list H = {(xi, li)}ni=1.
The loss iscomputed as the Word Error Rate (WER) of the549hypotheses compared to a reference transcription.For supervised training we use a set of n-best listsH = {H(k)}Mk=1.2.1 Learning to rank approachesMajor approaches to learning to rank can be di-vided into pointwise score regression, pairwisepreference satisfaction, and listwise structuredlearning.
See Liu (2009) for a survey.
In thispaper, we explore all of the following learning torank approaches.Pointwise In the pointwise approach, the rank-ing problem is formulated as a regression, or ordi-nal classification, for which any existing methodcan be applied.
Each hypothesis constitutes alearning instance.
In this category we use a regres-sion method called Multiple Additive RegressionTrees (MART) (Friedman, 2000) as implementedin RankLib.1 The major problem with pointwiseapproaches is that the structure of the list of hy-potheses is ignored.Pairwise The pairwise approach takes pairs ofhypotheses as instances in learning, and formal-izes the ranking problem as a pairwise classifica-tion or pairwise regression.
We use several meth-ods from this category.RankSVM (Joachims, 2002) is a method basedon Support Vector Machines (SVMs) for whichwe use only linear kernels to keep complexity low.Exact optimization of the RankSVM objective canbe computationally expensive as the number ofhypothesis pairs can be very large.
Approximatestochastic training strategies reduces complexityand produce comparable performance.
There-fore, in addition to RankSVM, we use stochas-tic sub-gradient descent (SGDSVM), Pegasos (Pe-gasosSVM) and Passive-Aggressive Perceptron(PAPSVM) as implemented in Sculley (2009).2RankBoost (Freund et al, 2003) is a pairwiseboosting approach implemented in RankLib.
Ituses a linear combination of weak rankers, each ofwhich is a binary function associated with a singlefeature.
This function is 1 when the feature valueexceeds some threshold and 0 otherwise.RankMIRA is a ranking method presented in (LeRoux et al, 2012).3 It uses a weighted linearcombination of features which assigns the highest1http://people.cs.umass.edu/?vdang/ranklib.html2http://code.google.com/p/sofia-ml3https://github.com/jihelhere/adMIRAblescore to the hypotheses with the lowest loss.
Dur-ing training, the weights are updated according tothe Margin-Infused Relaxed Algorithm (MIRA),whenever the highest scoring hypothesis differsfrom the hypothesis with the lowest error rate.In pairwise approaches, the group structure ofthe n-best list is still ignored.
Additionally, thenumber of training pairs generated from an n-bestlist depends on its size, which could result in train-ing a model biased toward larger hypothesis lists(Cao et al, 2006).Listwise The listwise approach takes n-best listsas instances in both learning and prediction.
Thegroup structure is considered explicitly and rank-ing evaluation measures can be directly optimized.The listwise methods we use are implemented inRankLib.AdaRank (Xu and Li, 2007) is a boosting ap-proach, similar to RankBoost, except that it opti-mizes an arbitrary ranking metric, for which weuse Mean Average Precision (MAP).Coordinate Ascent (CA) uses a listwise linearmodel whose weights are learned by a coordinateascent method to optimize a ranking metric (Met-zler and Bruce Croft, 2007).
As with AdaRank weuse MAP.ListNet (Cao et al, 2007) uses a neural networkmodel whose parameters are learned by gradientdescent method to optimize a listwise loss basedon a probabilistic model of permutations.2.2 Ensemble rerankingIn addition to the above mentioned approaches,we couple simple feature selection and rerankingmodels combination via a straightforward ensem-ble learning method similar to stacked general-ization (Wolpert, 1992) and Combiner (Chan andStolfo, 1993).
Our goal is to generate an overallmeta-ranker that outperforms all base-rankers bylearning from their predictions how they correlatewith each other.To obtain the base-rankers, we train each of theranking models of ?2.1 using all the features of?2.3 and also using each feature family added tothe baseline features separately.
Then, we use thebest model for each ranking approach to make pre-dictions on a held-out data set of n-best lists.
Wecan think of each base-ranker as computing onefeature for each hypothesis.
Hence, the scoresgenerated by all the rankers for a given hypothe-sis constitute its feature vector.The held-out n-best lists and the predictions of550the base-rankers represent the training data for themeta-ranker.
We choose RankSVM4 as the meta-ranker since it performed well as a base-ranker.2.3 FeaturesOur features fall into five families.Base features include the HMM and LM scoresproduced by the OCR system.
These features areused by the baseline system5 as well as by the var-ious reranking methods.Simple features (?simple?)
include the baselinerank of the hypothesis and a 0-to-1 range normal-ized version of it.
We also use a hypothesis confi-dence feature which corresponds to the average ofthe confidence of individual words in the hypoth-esis; ?confidence?
for a given word is computedas the fraction of hypotheses in the n-best listthat contain the word (Habash and Roth, 2011).The more consensus words a hypothesis contains,the higher its assigned confidence.
We also usethe average word length and the number of con-tent words (normalized by the hypothesis length).We define ?content words?
as non-punctuation andnon-digit words.
Additionally, we use a set of bi-nary features indicating if the hypothesis containsa sequence of duplicated characters, a date-like se-quence and an occurrence of a specific characterclass (punctuation, alphabetic and digit).Word LM features (?LM-word?)
include thelog probabilities of the hypothesis obtained us-ing n-gram LMs with n ?
{1, .
.
.
, 5}.
SeparateLMs are trained on the Arabic Gigaword 3 corpus(Graff, 2007), and on the reference transcriptionsof the training data (see ?3.1).
The LM modelsare built using the SRI Language Modeling Toolkit(Stolcke, 2002).Linguistic LM features (?LM-MADA?)
aresimilar to the word LM features except that theyare computed using the part-of-speech and thelemma of the words instead of the actual words.6Semantic coherence feature (?SemCoh?)
ismotivated by the fact that semantic informationcan be very useful in modeling the fluency ofphrases, and can augment the information pro-vided by n-gram LMs.
In modeling contextual4RankSVM has also been shown to be a good choice forthe meta-learner in general stacking ensemble learning (Tanget al, 2010).5The baseline ranking is simply based on the sum of thelogs of the HMM and LM scores.6The part-of-speech and the lemmas are obtained usingMADA 3.0, a tool for Arabic morphological analysis anddisambiguation (Habash and Rambow, 2005; Habash et al,2009).lexical semantic information, simple bag-of-wordsmodels usually have a lot of noise; while moresophisticated models considering positional infor-mation have sparsity issues.
To strike a balancebetween these two extremes, we introduce a novelmodel of semantic coherence that is based on ameasure of semantic relatedness between pairs ofwords.
We model semantic relatedness betweentwo words using the Information Content (IC) ofthe pair in a method similar to the one used by Lin(1997) and Lin (1998).IC(w1,d, w2) = logf(w1, d, w2)f(?,d, ?
)f(w1, d, ?
)f(?,d, w2)Here, d can generally represent some form of re-lation between w1 and w2.
Whereas Lin (1997)and Lin (1998) used dependency relation betweenwords, we use distance.
Given a sentence, the dis-tance between w1 and w2 is one plus the numberof words that are seen after w1 and before w2 inthat sentence.
Hence, f(w1, d, w2) is the numberof times w1 occurs before w2 at a distance d inall the sentences in a corpus.
?
is a placeholderfor any word, i.e., f(?, d, ?)
is the frequency of allword pairs occurring at distance d. The distancesare directional and not absolute values.
A simi-lar measure of relatedness was also used by Kolb(2009).We estimate the frequencies from the ArabicGigaword.
We set the window size to 3 and cal-culate IC values of all pairs of words occurring atdistance within the window size.
Since the dis-tances are directional, it has to be noted that givena word, its relations with three words before it andthree words after it are modeled.
During testing,for each phrase in our test set, we measure se-mantic relatedness of pairs of words using the ICvalues estimated from the Arabic Gigaword, andnormalize their sum by the number of pairs in thephrase to obtain a measure of Semantic Coherence(SC) of the phrase.
That is,SC(p) = 1m ?
?1?d?W1?i+d<nIC(wi,d, wi+d)where p is the phrase being evaluated, n is thenumber of words in it, d is the distance betweenwords, W is the window size (set to 3), and m isthe number of all possible wi, wi+d pairs in thephrase given these conditions.551print hand|H?| n |h| |H?| n |h|Hb 1,560 62 9 2,295 225 8Hm 1,000 76 9 1,000 225 9Ht 1,000 64 9 1,000 227 9Table 1: Data sets statistics.
|H?| refers to thenumber of n-best lists, n is the average size of thelists, and |h| is the average length of a hypothesis.print handBaseline 13.8% 35%Oracle 9.8% 20.9%Best result 12.4% 30.9%Table 2: WER for baseline, oracle and bestreranked hypotheses.3 Experiments3.1 Data and baselinesWe used two data sets derived from high-resolution image scans of typewritten and hand-written Arabic text along with ground truth tran-scriptions.7 The BBN Byblos system was thenused to process these scanned images into se-quences of segments (sentence fragments) andgenerate a ranked n-best list of hypotheses foreach segment (Natajan et al, 2002; Prasad et al,2008; Saleem et al, 2009).
We divided each of thetypewritten data set (?print?)
and handwritten dataset (?hand?)
into three disjoint parts: a training setfor the base-rankersHb, a training set for the meta-ranker Hm and a test set Ht.
Table 1 presentssome statistics about these data sets.
Our base-line is based on the sum of the logs of the HMMand LM scores.
Table 2 presents the WER for ourbaseline hypothesis, the best hypothesis in the list(our oracle) and our best reranking results whichwe describe in details in ?3.2.For LM training we used 220M words fromArabic Gigaword 3, and 2.4M words from each?print?
and ?hand?
ground truth annotations.Effect of n-best training size onWER The sizeof the training n-best lists is crucial to the learningof the ranking model.
In particular, it determinesthe number of training instances per list.
To deter-mine the optimal n to use for the rest of this pa-per, we conducted the following experiment aimsto understand the effect of the size of n-best lists7The Anfal data set discussed here was collected by theLinguistic Data Consortium.30.53131.53232.53333.5341212.51313.51414.5155 15 25 35 45 55Size of each training n-best listWER printhandFigure 1: Effect of the size of training n-best listson WER.
The horizontal axis represents the max-imum size of the n-best lists and the vertical axisrepresents WER, left is ?print?
and right is ?hand?.on the reranking performance for one of our bestreranking models, namely RankSVM.
We trainedeach model with different sizes of n-best, varyingfrom n = 5 to n = 60 for ?print?
data, and be-tween n = 5 and n = 150 for ?hand?
data.
Thetop n hypotheses according to the baseline are se-lected for each n. Figure 1 plots WER as a func-tion of the size of the training list n for both ?print?and ?hand?
data.The lowest WER scores are achieved for n =10 and n = 15 for both ?print?
and ?hand?
data.We note that a small number of hypotheses per listis sufficient for RankSVM to obtain a good per-formance, but also increasing n further seems toincrease the error rate.
For the rest of this paperwe use the top 10-best hypotheses per segment.3.2 Reranking resultsThe reranking results for ?print?
and ?hand?
arepresented in Table 3.
The results are presentedas the difference in WER from the baseline WER.See the caption in Table 3 for more information.For ?print?, the pairwise approaches clearly out-perform the listwise approaches and achieve thelowest WER of 12.4% (10.1% WER reduction rel-ative to the baseline) with 7 different combinationsof rankers and feature families.
While both ap-proaches do not minimize WER directly, the pair-wise methods have the advantage of using objec-tives that are simpler to optimize, and they aretrained on much larger number of examples whichmay explain their superiority.
RankBoost, how-ever, is less competitive with a performance closerto that of listwise approaches.
All the methodsimproved over the baseline with any feature fam-ily, except for the pointwise approach which did552Pointwise Listwise PairwiseFeatures MARTAdaRankListNetCA RankBoostRankSVMSGDSVMRankMIRAPega.SVMPAPSVMPrintBase 1.1 -0.4 -1.0 -1.0 -1.0 -1.1 -1.2 -1.2 -1.3 -1.3+simple -0.1 0.0 -0.1 -0.2 0.0 -0.1 0.1 0.0 0.1 0.0+LM-word -1.0 -0.2 0.1 -0.1 -0.1 -0.3 -0.2 -0.1 0.0 -0.1+LM-MADA 0.0 -0.3 0.1 -0.2 -0.1 0.0 -0.1 -0.2 -0.1 -0.1+SemCoh 0.0 -0.4 0.0 -0.2 -0.1 -0.1 0.0 -0.1 0.0 0.1+All 0.6 0.1 0.0 0.1 0.0 0.1 0.2 0.2 0.2 0.0HandBase 4.2 -3.1 -3.2 -3.4 -2.9 -3.2 -3.5 -3.8 -3.6 -3.8+simple 0.3 -0.1 0.1 0.2 0.1 -0.1 0.2 -0.2 0.1 0.2+LM-word 0.4 -0.1 0.1 0.8 -0.2 -0.7 -0.2 -0.1 0.0 0.1+LM-MADA 0.0 -0.5 0.1 0.0 0.1 -0.4 -0.1 0.3 -0.2 0.1+SemCoh 0.0 -0.1 0.0 -0.4 0.0 -0.2 -0.3 -0.2 -0.2 0.0+All 0.2 0.4 0.0 0.4 0.2 0.4 0.2 0.1 0.2 0.0Table 3: Reranking results for the ?print?
and ?hand?
data sets; the ?print?
baseline WER is 13.9% and the ?hand?
baselineWER is 35.0%.
The ?Base?
numbers represent the difference in WER between the corresponding ranker using ?Base?
featuresonly and the baseline, which uses the same ?Base?
features.
The ?+features?
numbers represent additional gain (relative to?Base?)
obtained by adding the corresponding feature family.
The ?+All?
numbers represent the gain of using all features,relative to the best single-family system.
The actual WER of a ranker can be obtained by summing the baseline WER and thecorresponding ?Base?
and ?+features?
scores.
Bolded values are the best performers overall.worse than the baseline.
When combined withthe ?Base?
features, ?LM-words?
lead to improve-ments with 8 out of 10 rankers, and proved to bethe most helpful among feature families.
?LM-MADA?
follows with improvements with 7 out of10 rankers.
The lowest WER is achieved usingone of these two LM-based families.
Combiningall feature families did not help and in many casesresulted in a higher WER than the best family.Similar improvements are observed for ?hand?.The lowest achieved WER is 31% (11.4% WERreduction relative to the baseline).
Here also,the pointwise method increased the WER by 12%relative to the baseline (as opposed to 7% for?print?).
Again, the listwise approaches are over-all less effective than their pairwise counterparts,except for RankBoost which resulted in the small-est WER reduction among all rankers.
The twobest rankers correspond to RankMIRA with the?simple?
and the ?SemCoh?
features.
The ?Sem-Coh?
feature resulted in improvements for 6 out ofthe 10 rankers, and thus was the best single featureon average for the ?hand?
data set.
As observedwith ?print?
data, combining all the features doesnot lead to the best performance.In an additional experiment, we selected thebest model for each ranking method and combinedthem to build an ensemble as described in ?2.2.For ?hand?, the ensemble slightly outperformedall the individual rankers and achieved the lowestWER of 30.9%.
However, for the ?print?
data, theensemble failed to improve over the base-rankersand resulted in a WER of 12.4%.The best overall results are presented in Table 2.Our best results reduce the distance to the oracletop line by 35% for ?print?
and 29% for ?hand?.4 ConclusionWe presented a set of experiments on incorporat-ing features into an existing OCR system via n-best list reranking.
We compared several learn-ing to rank techniques and combined them us-ing an ensemble technique.
We obtained 10.1%and 11.4% reduction in WER relative to the base-line for ?print?
and ?hand?
data respectively.
Ourbest systems used pairwise reranking which out-performed the other methods, and used the MADAbased features for ?print?
and our novel semanticcoherence feature for ?hand?.AcknowledgmentWe would like to thank Rohit Prasad and MatinKamali for providing the data and helpful dis-cussions.
This work was funded under DARPAproject number HR0011-08-C-0004.
Any opin-ions, findings and conclusions or recommenda-tions expressed in this paper are those of the au-thors and do not necessarily reflect the views ofDARPA.
The last two authors, Dasigi and Diab,worked on this project while at Columbia Univer-sity.553ReferencesYunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, YalouHuang, and Hsiao-Wuen Hon.
2006.
Adaptingranking SVM to document retrieval.
In Proceedingsof the 29th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, SIGIR ?06, pages 186?193.Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, andHang Li.
2007.
Learning to rank: from pairwiseapproach to listwise approach.
In Proceedings of the24th international conference on Machine learning,ICML ?07, pages 129?136.Philip K. Chan and Salvatore J. Stolfo.
1993.
Exper-iments on multistrategy learning by meta-learning.In Proceedings of the second international confer-ence on Information and knowledge management,CIKM ?93, pages 314?323.Michael Collins and Terry Koo.
2005.
DiscriminativeReranking for Natural Language Parsing.
Comput.Linguist., 31(1):25?70, March.Jacob Devlin, Matin Kamali, Krishna Subramanian,Rohit Prasad, and Prem Natarajan.
2012.
Statisti-cal Machine Translation as a Language Model forHandwriting Recognition.
In ICFHR, pages 291?296.Marco Dinarelli, Alessandro Moschitti, and GiuseppeRiccardi.
2012.
Discriminative Reranking forSpoken Language Understanding.
IEEE Transac-tions on Audio, Speech & Language Processing,20(2):526?539.Yoav Freund, Raj Iyer, Robert E. Schapire, and YoramSinger.
2003.
An efficient boosting algorithmfor combining preferences.
J. Mach.
Learn.
Res.,4:933?969, December.Jerome H. Friedman.
2000.
Greedy Function Approx-imation: A Gradient Boosting Machine.
Annals ofStatistics, 29:1189?1232.Ruifang Ge and Raymond J. Mooney.
2006.
Discrimi-native Reranking for Semantic Parsing.
In ACL.David Graff.
2007.
Arabic Gigaword 3, LDC Cat-alog No.
: LDC2003T40.
Linguistic Data Consor-tium, University of Pennsylvania.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and Morphologi-cal Disambiguation in One Fell Swoop.
In Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL?05), pages 573?580, Ann Arbor, Michigan, June.Nizar Habash and Ryan M. Roth.
2011.
Using deepmorphology to improve automatic error detection inArabic handwriting recognition.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies - Volume 1, HLT ?11, pages 875?884.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.MADA+TOKAN: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, POStagging, stemming and lemmatization.
In KhalidChoukri and Bente Maegaard, editors, Proceedingsof the Second International Conference on ArabicLanguage Resources and Tools.
The MEDAR Con-sortium, April.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Thorsten Joachims.
2002.
Optimizing search en-gines using clickthrough data.
In Proceedings ofthe eighth ACM SIGKDD international conferenceon Knowledge discovery and data mining, KDD ?02,pages 133?142.Peter Kolb.
2009.
Experiments on the difference be-tween semantic similarity and relatedness.
In Pro-ceedings of the 17th Nordic Conference of Computa-tional Linguistics, NEALT Proceedings Series Vol.4.Joseph Le Roux, Benoit Favre, Alexis Nasr, andSeyed Abolghasem Mirroshandel.
2012.
Gener-ative Constituent Parsing and Discriminative De-pendency Reranking: Experiments on English andFrench.
In SP-SEM-MRL 2012.Dekang Lin.
1997.
Using syntactic dependency aslocal context to resolve word sense ambiguity.
InProceedings of the eighth conference on Europeanchapter of the Association for Computational Lin-guistics, EACL ?97, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics - Volume 2, ACL ?98.Tie-Yan Liu.
2009.
Learning to Rank for Informa-tion Retrieval.
Now Publishers Inc., Hanover, MA,USA.Volker Ma?rgner and Haikal El Abed.
2009.
Ara-bic Word and Text Recognition - Current Develop-ments.
In Khalid Choukri and Bente Maegaard, ed-itors, Proceedings of the Second International Con-ference on Arabic Language Resources and Tools,Cairo, Egypt, April.
The MEDAR Consortium.Donald Metzler and W. Bruce Croft.
2007.
Linearfeature-based models for information retrieval.
Inf.Retr., 10(3):257?274, June.Premkumar Natajan, Zhidong Lu, Richard Schwartz,Issam Bazzi, and John Makhoul.
2002.
Hid-den Markov models.
chapter Multilingual machineprinted OCR, pages 43?63.
World Scientific Pub-lishing Co., Inc., River Edge, NJ, USA.554Rohit Prasad, Shirin Saleem, Matin Kamali, Ralf Meer-meier, and Premkumar Natarajan.
2008.
Improve-ments in hidden Markov model based Arabic OCR.In Proceedings of International Conference on Pat-tern Recognition (ICPR), pages 1?4.Shirin Saleem, Huaigu Cao, Krishna Subramanian,Matin Kamali, Rohit Prasad, and Prem Natarajan.2009.
Improvements in BBN?s HMM-Based Of-fline Arabic Handwriting Recognition System.
InProceedings of the 2009 10th International Confer-ence on Document Analysis and Recognition, IC-DAR ?09, pages 773?777.D.
Sculley.
2009.
Large scale learning to rank.
InNIPS 2009 Workshop on Advances in Ranking.Libin Shen, Anoop Sarkar, and Franz Josef Och.2004.
Discriminative Reranking for Machine Trans-lation.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceed-ings, pages 177?184, Boston, Massachusetts, USA,May.Andreas Stolcke.
2002.
SRILM - an Extensible Lan-guage Modeling Toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Pro-cessing (ICSLP), volume 2, pages 901?904, Denver,CO.Buzhou Tang, Qingcai Chen, Xuan Wang, and Xiao-long Wang.
2010.
Reranking for stacking ensem-ble learning.
In Proceedings of the 17th interna-tional conference on Neural information processing:theory and algorithms - Volume Part I, ICONIP?10,pages 575?584.David H. Wolpert.
1992.
Original Contribution:Stacked generalization.
Neural Netw., 5(2):241?259, February.Jun Xu and Hang Li.
2007.
AdaRank: a boosting al-gorithm for information retrieval.
In Proceedings ofthe 30th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, SIGIR ?07, pages 391?398.555
