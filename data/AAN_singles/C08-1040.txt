Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 313?320Manchester, August 2008Tracking the Dynamic Evolution of Participant Salience in a DiscussionAhmed HassanUniversity of Michiganhassanam@umich.eduAnthony FaderUniversity of Michiganafader@umich.eduMichael H. CrespinUniversity of Georgiacrespin@uga.eduKevin M. QuinnHarvard Universitykquinn@fsa.harvard.eduBurt L. MonroePennsylvania State Universityburtmonroe@psu.eduMichael ColaresiMichigan State Universitycolaresi@msu.eduDragomir R. RadevUniversity of Michiganradev@umich.eduAbstractWe introduce a technique for analyzing thetemporal evolution of the salience of par-ticipants in a discussion.
Our method candynamically track how the relative impor-tance of speakers evolve over time usinggraph based techniques.
Speaker salienceis computed based on the eigenvector cen-trality in a graph representation of partici-pants in a discussion.
Two participants in adiscussion are linked with an edge if theyuse similar rhetoric.
The method is dy-namic in the sense that the graph evolvesover time to capture the evolution inher-ent to the participants salience.
We usedour method to track the salience of mem-bers of the US Senate using data from theUS Congressional Record.
Our analysisinvestigated how the salience of speakerschanges over time.
Our results show thatthe scores can capture speaker centralityin topics as well as events that result inchange of salience or influence among dif-ferent participants.1 IntroductionThere are several sources of data that recordspeeches or participations in debates or discus-sions among a group of speakers or participants.Those include parliamentary records, blogs, andnews groups.
This data represents a very importantand unexploited source of information that con-tains several trends and ideas.
In any debate ordiscussion, there are certain types of persons whoc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.influence other people and pass information or ad-vice to them.
Those persons are often regardedas experts in the field or simply influential peo-ple and they tend to affect the ideas and rhetoricof other participants.
This effect can be trackeddown by tracking the similarity between differentspeeches.
We can then imagine a debate with manypeople arguing about many different things as anetwork of speeches or participations interactingwith each other.
We can then try to identify themost salient or important participants by identify-ing the most central speeches in this network andassociating them with their speakers.
When wehave a large dataset of debates and conversationsthat expand over a long period of time, the salienceof participants becomes a dynamic property thatchanges over time.
To capture this dynamic natureof the process, the graph of speeches must evolveover time such that we have a different graph ateach instance of time that reflects the interactionof speeches at this instant.We apply our method to the US CongressionalRecord.
The US Congressional Record documentseverything said and done in the US CongressHouse and Senate.
The speeches in this data setare made by a large number of people over a longperiod of time.
Using political speeches as testdata for the proposed method adds an extra layerof meaning onto the measure of speakers salience.Speaker salience of the Congress members can re-flect the importance or influence in the US leg-islative process.
The way salience scores evolveover time can answer several interesting issues likehow the influence of the speakers vary with major-ity status and change of party control.
It can alsostudy the dynamics of the relative distribution ofattention to each topic area in different time peri-ods.313The rest of this paper will proceed as follows.Section 2 reviews some related work.
In Section 3,we describe how the data can be clustered into dif-ferent topic clusters.
In Section 4, we describeour method for computing the salience of differentparticipant in a discussion, we also describe howto the network of speakers varies over time.
Sec-tion 5 describes the experimental setup.
Finally,we present the conclusions in Section 6.2 Related WorkSeveral methods have been proposed for identify-ing the most central nodes in a network.
Degreecentrality, closeness, and betweenness (Newman,2003) are among the most known methods formeasuring centrality of nodes in a network.
Eigen-vector centrality is another powerful method thatthat has been applied to several types of networks.For example it has been used to measure cen-trality in hyperlinked web pages networks (Brinand Page, 1998; Kleinberg, 1998), lexical net-works (Erkan and Radev, 2004; Mihalcea and Ta-rau, 2004; Kurland and Lee, 2005; Kurland andLee, 2006), and semantic networks (Mihalcea etal., 2004).The interest of applying natural language pro-cessing techniques in the area of political sciencehas been recently increasing.
(Quinn et al, 2006) introduce a multinomialmixture model to cluster political speeches intotopics or related categories.
In (Porter et al, 2005),a network analysis of the members and committeesof the US House of Representatives is performed.The authors prove that there are connections link-ing some political positions to certain committees.This suggests that there are factors affecting com-mittee membership and that they are not deter-mined at random.
In (Thomas et al, 2006), the au-thors try to automatically classify speeches, fromthe US Congress debates, as supporting or oppos-ing a given topic by taking advantage of the votingrecords of the speakers.
(Fader et al, 2007) in-troduce MavenRank , which is a method based onlexical centrality that identifies the most influen-tial members of the US Senate.
It computes a sin-gle salience score for each speaker that is constantover time.In this paper, we introduce a new method fortracking the evolution of the salience of partici-pants in a discussion over time.
Our method isbased on the ones described in (Erkan and Radev,2004; Mihalcea and Tarau, 2004; Fader et al,2007), The objective of this paper is to dynami-cally rank speakers or participants in a discussion.The proposed method is dynamic in the sense thatthe computed importance varies over time.3 Topic ClustersBefore applying the proposed method to a dataset with speeches in multiple topics, we first needto divide the speech documents into topic clus-ters.
We used the model described in (Quinn et al,2006) for this purpose.
The model presented in thispaper assumes that the probabilities of a documentbelonging to a certain topic varies smoothly overtime and the words within a given document haveexactly the same probability of being drawn froma particular topic (Quinn et al, 2006).
These twoproperties make the model different than standardmixture models (McLachlan and Peel, 2000) andthe latent Dirichlet alocation model of (Blei et al,2003).
The model of (Quinn et al, 2006) is mostclosely related to the model of (Blei and Lafferty,2006), who present a generalization of the modelused by (Quinn et al, 2006).The output from the topic model is a D ?
Kmatrix Z where D is the number of speeches , Kis the number of topics and the element zdkrepre-sents the probability of the dth speech being gen-erated by topic k. We then assign each speech dto the kth cluster where k = argmaxjzdj.
If themaximum value is not unique, one of the clustershaving the maximum value is arbitrary selected.4 Speaker CentralityIn this section we describe how to build a networkof speeches and use it to identify speaker centrality.We also describe how to generate different projec-tions of the network at different times, and howto use those projection to get dynamic saliencescores.4.1 Computing Speaker SalienceThe method we used is similar to the methods de-scribed in (Erkan and Radev, 2004; Mihalcea andTarau, 2004; Kurland and Lee, 2005), which wereoriginally used for ranking sentences and docu-ments in extractive summarization and informationretrieval systems.A collection of speeches can be represented asa network where similar speeches are linked toeach other.
The proposed method is based on314the premise that important speeches tend to belexically similar to other important speeches, andimportant speeches tend to belong to importantspeakers.
Hence given a collection of speeches anda similarity measure, we can build a network anddefine the centrality score of a speech recursivelyin terms of the scores of other similar speeches.Later, we can compute the salience of a speakeras the sum of the centrality measure of all hisspeeches.To measure the similarity between twospeeches, we use the bag-of-words model to repre-sent each sentence as an N-dimensional vector oftf-idf scores, where N is the number of all possiblewords in the target language.
The similaritybetween two speeches is then computed using thecosine similarity between the two vectors.A vector of term frequencies is used to representeach speech.
Those term frequencies are weightedaccording to the relative importance of the giventerm in the cluster.The vectors representing speeches contain termfrequencies (or tf), which are weighted accordingto their inverse document frequencies to accountfor the relative importance of the given term in thecluster.
The inverse document frequency of a termw is given by (Sparck-Jones, 1972)idf(w) = log(Nnw)(1)where nwis the number of speeches in the clus-ter containing the term w, and N is the number ofdocuments in the cluster.
We calculated idf valuesspecific to each topic, rather than to all speeches.We preferred to use topic-specific idf values be-cause the relative importance of words may varyfrom one topic to the other.The tf-idf cosine similarity measure is computedas the cosine of the angle between the tf-idf vec-tors.
It is defined as follows:Pw?u,vtfu(w) tfv(w) idf(w)2?Pw?u(tfu(w) idf(w))2?Pw?v(tfv(w) idf(w))2, (2)The choice of tf-idf scores to measure speechsimilarity is an arbitrary choice.
Some other possi-ble similarity measures are edit distance, languagemodels (Kurland and Lee, 2005), or generationprobabilities (Erkan, 2006).The recursive definition of the score of anyspeech s in the speeches network is given byp(s) =?t?adj[s]p(t)deg(t)(3)where deg(t) is the degree of node t, and adj[s] isthe set of all speeches adjacent to s in the network.This can be rewritten in matrix notation as:p = pB (4)where p = (p(s1), p(s2), .
.
.
, p(sN)) and the ma-trix B is the row normalized similarity matrix ofthe graphB(i, j) =S(i, j)?kS(i, k)(5)where S(i, j) = sim(si, sj).
Equation (4) showsthat the vector of salience scores p is the left eigen-vector of B with eigenvalue 1.The matrix B can be thought of as a stochasticmatrix that acts as transition matrix of a Markovchain.
An element X(i, j) of a stochastic matrixspecifies the transition probability from state i tostate j in the corresponding Markov chain.
Andthe whole process can be seen as a Markovian ran-dom walk on the speeches graph.
To help the ran-dom walker escape from periodic or disconnectedcomponents, (Brin and Page, 1998) suggests re-serving a small escape probability at each nodethat represents a chance of jumping to any nodein the graph, making the Markov chain irreducibleand aperiodic, which guarantees the existence ofthe eigenvector.Equation (4) can then be rewritten, assuming auniform escape probability, as:p = p[dU+ (1 ?
d)B] (6)where N is the total number of nodes, U is asquare matrix with U(i, j) = 1/N for all i, j, andd is the escape probability chosen in the interval[0.1, 0.2] (Brin and Page, 1998).4.2 Dynamic Salience ScoresWe use the time stamps associated with the data tocompute dynamic salience scores pT(u) that iden-tify central speakers at some time T .
To do this,we create a speech graph that evolves over time.Let T be the current date and let u and v be twospeech documents that occur on days tuand tv.Our goal is to discount the lexical similarity of uand v based on how far apart they are.
One wayto do this is by defining a new similarity measures(u, v;T ) as:s(u, v;T ) = tf-idf-cosine(u, v) ?
f(u, v;T ) (7)315where f(u, v;T ) is a function taking values in[0, 1].If f(u, v;T ) = 1 for all u, v, and T , then time isignored when calculating similarity and pT(u) =p(u).
On the other hand, suppose we letf(u, v;T ) ={1 if tu= tv= T ,0 else.
(8)This removes all edges that link a speech, occur-ring at some time T , to all other speeches occur-ring at some time other than T and the ranking al-gorithm will be run on what is essentially the sub-graph of documents restricted to time T (althoughthe isolated speech documents will receive smallnon-zero scores because of the escape probabilityfrom Section 4.1).
These two cases act as the ex-treme boundaries of possible functions f : in thefirst case time difference has no effect on documentsimilarity, while in the second case two documentsmust occur on the same day to be similar.We use the following time weight functions inour experiments.
In each case, we assume that thespeeches represented by speech documents u and vhave already occurred, that is, tu, tv?
T .
We willuse the convention that f(u, v;T ) = 0 if tu> Tor tv> T for all time weight functions, whichcaptures the idea that speeches that have not yetoccurred have no influence on the graph at time T .Also defineage(u, v;T ) = T ?
min{tu, tv} (9)which gives the age of the oldest speech documentfrom the pair u, v at time T .?
Exponential: Given a parameter a > 0, definefexp,a(u, v;T ) = e?a age(u,v;T ).
(10)This function will decrease the impact of sim-ilarity as time increases in an exponentialfashion.
a is a parameter that controls howfast this happens, where a larger value of amakes earlier speeches have a small impacton current scores and a smaller value of ameans that earlier speeches will have a largerimpact on current scores.?
Linear: Given b > 0, defineflin,d(u, v;T ) =????
?1 ?1bage(u, v;T )if age(u, v;T ) ?
b0 if age(u, v;T ) > b(11)Figure 1: The Dynamic boundary cases for Sena-tor Santorum.This function gives speech documents thatoccur at time T full weight and then decreasestheir weight linearly towards time T + b,where it becomes 0.?
Boundary: Given d ?
0, definefbnd,d(u, v;T ) ={1 if age(u, v;T ) ?
d0 if age(u, v;T ) > d(12)This function gives speech documents occur-ring within d days of T the regular tf-idf sim-ilarity score, but sets the similarity of speechdocuments occurring outside of d days to 0.The case when d = 0 is one of the boundarycases explained above.Figure 1 gives an example of different timeweighting functions for Senator Rick Santorum(R - Pennsylvania) on topic 22 (Abortion) during1997, the first session of the 105th Congress.
Thedashed line shows the case when time has no ef-fect on similarity (his score is constant over time),while the solid line shows the case where onlyspeeches on the current day are considered simi-lar (his score spikes only on days where he speaksand is near zero otherwise).
The dotted line showsthe case when the influence of older speeches de-creases exponentially, which is more dynamic thanthe first case but smoother than the second case.5 Experiments and Results5.1 DataWe used the United States Congressional Speechcorpus (Monroe et al, 2006) in our experiment.316This corpus is in XML formatted version of theelectronic United States Congressional Recordfrom the Library of Congress1.
The CongressionalRecord is a verbatim transcript of the speechesmade in the US House of Representatives and Sen-ate and includes tens of thousands of speeches peryear (Monroe et al, 2006).
The data we used coverthe period from January 2001 to January 2003.5.2 Experimental SetupWe used results from (Quinn et al, 2006) to gettopic clusters from the data, as described in Sec-tion 3.
The total number of topics was 42.
Theaverage sized topic cluster had several hundredspeech documents (Quinn et al, 2006).We set up a pipeline using a Perl implementa-tion of the proposed method We ran it on the topicclusters and ranked the speakers based on the cen-trality scores of their speeches.
The graph nodeswere speech documents.
A speaker?s score wasdetermined by the average of the scores of thespeeches given by that speaker.
After comparingthe different time weighting function as shown inFigure 1, we decided to use the exponential timeweight function for all the experiments discussedbelow.
Exponential time weighting function de-creases the impact of similarity as time increasesin an exponential fashion.
It also allows us to con-trol the rate of decay using the parameter a.5.3 BaselineWe compare the performance of our system toa simple baseline that calculates the salience ofa speaker as a weighted count of the number oftimes he has spoken.
The baseline gives highweight to recent speeches .
The weight decreasesas the speeches gets older.
The salience score of aspeaker is calculate as follows:BS(i) =?d?d0?d?
Sid(13)Where BS(i) is the baseline score of speaker i,?
is the discounting factor, d0is the current date,and Sidis the number of speeches made by speakeri at date d. We used ?
= 0.9 for all our experi-ments.5.4 ResultsOne way to evaluate the dynamic salience scores,is to look at changes when party control of the1http://thomas.loc.govchamber switches.
Similar to (Hartog and Mon-roe, 2004), we exploit the party switch made bySenator Jim Jeffords of Vermont and the result-ing change in majority control of the Senate dur-ing the 107th Congress as a quasi-experimentaldesign.
In short, Jeffords announced his switchon May 24, 2001 from Republican to Independentstatus, effective June 6, 2001.
Jeffords stated thathe would vote with the Democrats to organize theSenate, giving the Democrats a one-seat advantageand change control of the Senate from the Repub-licans back to the Democrats.
This change of ma-jority status during the 107th Congress allows usto ignore many of the factors that could potentiallyinfluence dynamic salience scores at the start of anew congress.On average, we expect committee chairs or amember of the majority party to be the most im-portant speaker on each topic followed by rankingmembers or a member of the minority party.
Ifour measure is capturing dynamics in the central-ity of Senators, we expect Republicans to be morecentral before the Jeffords switch and Democratsbecoming central soon afterwards, assuming thetopic is being discussed on the Senate floor.
Weshow that the proposed technique captures severalinteresting events in the data and also show that thebaseline explained above fails to capture the sameset of events.Figure 2(a) shows the dynamic salience scoresover time for Senator John McCain (R - Arizona)and Senator Carl Levin (D - Michigan) on topic5 (Armed Forces 2) for the 107th Senate.
Mc-Cain was the most salient speaker for this topicuntil June 2001.
Soon after the change in major-ity status a switch happened and Levin, the newchair of Senate Armed Services, replaced McCainas the most salient speaker.
On the other hand,Figure 2(b) shows the baseline scores for the sametopic and same speakers.
We notice here that thebaseline failed to capture the switch of saliencenear June 2001.We can also observe similar behavior in Fig-ure 3(a).
This figure shows how Senate MajorityLeader Trent Lott (R - Mississippi) was the mostsalient speaker on topic 35 (Procedural Legisla-tion) until July 2001.
Topic 35 does not map toa specific committee but rather is related to ma-neuvering bills through the legislative process onthe floor, a job generally delegated to members inthe Senate leadership.
Just after his party gained31700.10.20.30.40.50.60.70.8 Jan01Mar01May01Jul01Sep01Nov01Jan02Mar02May02Jul02Sep02Nov02Jan03LexRankTimeDynamic Lexrank, Senate 107 Armed Forces 2 (Infrastructure)?
?, Exponential, a=0.02, th=MCCAINLEVINCARL(a) Dynamic Lexrank0246810 Jan01Mar01May01Jul01Sep01Nov01Jan02Mar02May02Jul02Sep02Nov02Jan03LexRankTimeBaseline, Senate 107ArmedForces2 (Infrastructure)MCCAINLEVINCARL(b) BaselineFigure 2: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2).00.10.20.30.40.50.60.70.8 Jan01Mar01May01Jul01Sep01Nov01Jan02Mar02May02Jul02Sep02Nov02Jan03LexRankTimeDynamic Lexrank, Senate 107 Procedural 4(Legislaton 2)?
?, Exponential, a=0.02, th= REID LOTT(a) Dynamic Lexrank05101520 Jan01Mar01May01Jul01Sep01Nov01Jan02Mar02May02Jul02Sep02Nov02Jan03LexRankTimeBaseline, Senate 107Procedural 4 (Legislaton2)?
?REID LOTT(b) BaselineFigure 3: The Switch of Speakers Salience near Jun 2001 for Topic 35(Procedural Legislation).majority status, Senator Harry Reid (D - Nevada)became the most salient speaker for this topic.
Thisis consistent with Reid?s switch from Assistant mi-nority Leader to Assistant majority Leader.
Againthe baseline scores for the same topic and speakersin Figure 3(b) fails to capture the switch.An even more interesting test would be to checkwhether the Democrats in general become morecentral than Republicans after the Jeffords switch.Figure 4(a) shows the normalized sum of thescores of all the Democrats and all the Republicanson topic 5 (Armed Forces 2) for the 107th Senate.The figure shows how the Republicans were mostsalient until soon after the Jeffords switch when theDemocrats regained the majority and became moresalient.
We even discovered similar behavior whenwe studied how the average salience of Democratsand Republicans change across all topics.
This isshown in Figure 5(a) where we can see that theRepublicans were more salient on average for alltopics until June 2001.
Soon after the change inmajority status, Democrats became more central.Figures 4(b) and 5(b) show the same results usingthe baseline system.
We notice that the number ofspeeches made by the Democrats and the Repub-licans is very similar in most of the times.
Evenwhen one of the parties has more speeches thanthe other, it does not quite reflect the salience ofthe speakers or the parties in general.An alternative approach to evaluate the dynamicscores is to exploit the cyclical nature of the leg-islative process as some bills are re-authorized ona fairly regular time schedule.
For example, thefarm bill comes due about every five years.
As anew topic is coming up for debate, we expect thesaliency scores for relevant legislators to increase.Figure 6 shows the dynamic scores of SenatorThomas Harkin (D - Iowa), and Senator Richard31800.20.40.60.8 1 Jan01Mar01May01Jul01Sep01Nov01Jan02Mar02May02Jul02Sep02Nov02Jan03LexRankTimeDynamic Lexrank, Senate 107 Armed Forces 2?
?Republicans Democrats(a) Dynamic Lexrank051015202530 Jan01Mar01May01Jul01Sep01Nov01Jan02Mar02May02Jul02Sep02Nov02Jan03LexRankTimeBaseline, Senate 107ArmedForces2 (Infrastructure)DemocratesRepublicans(b) BaselineFigure 4: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2), Republicans vsDemocrats.101214161820 Jan01Mar01May01Jul01Sep01Nov01Jan02Mar02May02Jul02Sep02Nov02Jan03LexRankTimeDynamic Lexrank, Senate 107Republicans Democrats(a) Dynamic Lexrank0100200300400500600700 Jan01Mar01May01Jul01Sep01Nov01Jan02Mar02May02Jul02Sep02Nov02Jan03LexRankTimeBaseline, Senate 107DemocratesRepublicans(b) BaselineFigure 5: The Switch of Speakers Salience near Jun 2001 for All Topics, Republicans vs Democrats.Lugar (R - Indiana) during the 107th senate ontopic 24 (Agriculture).
The two senators wereidentified, by the proposed method, as the mostsalient speakers for this topic, as expected, sincethey both served as chairmen of the Senate Com-mittee on Agriculture, Nutrition, and Forestrywhen their party was in the majority during the107th Senate.
This committee was in charge ofshepherding the Farm Bill through the Senate.
Thescores of both senators on the agriculture topic sig-nificantly increased starting late 2001 until June2002.
The debate began on the bill starting inSeptember of 2001 and it was not passed until May2002.6 ConclusionWe presented a graph based method for analyz-ing the temporal evolution of the salience of par-ticipants in a discussion.
We used this method totrack the evolution of salience of speakers in theUS Congressional Record.
We showed that theway salience scores evolve over time can answerseveral interesting issues.
We tracked how the in-fluence of the speakers vary with majority statusand change of party control.
We also show howa baseline system that depends on the number ofspeeches fails to capture the interesting events cap-tured by the proposed system.
We also studied thedynamics of the relative distribution of attention toeach topic area in different time periods and cap-tured the cyclical nature of the legislative processas some bills are re-authorized on a fairly regulartime schedule.31900.10.20.30.40.5 Jan01Mar01May01Jul01Sep01Nov01Jan02Mar02May02Jul02Sep02Nov02Jan03LexRankTimeDynamic Lexrank, Senate 107 Agriculture?
?, Exponential,a=0.02, th= LUGAR HARKINFigure 6: The Farm Bill Discussions on the Rela-tive Distribution of Attention to Topic 24 (Agricul-ture).AcknowledgmentsThis paper is based upon work supported bythe National Science Foundation under Grant No.0527513, ?DHB: The dynamics of Political Rep-resentation and Political Rhetoric?.
Any opinions,findings, and conclusions or recommendations ex-pressed in this paper are those of the authors anddo not necessarily reflect the views of the NationalScience Foundation.ReferencesBlei, David and John Lafferty.
2006.
Dynamic topicmodels.
In ICML 2006.Blei, David, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Brin, Sergey and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual Web search engine.CNIS, 30(1?7):107?117.Erkan, G?unes?
and Dragomir Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR).Erkan, Gunes.
2006.
Language model-based documentclustering using random walks.
In HLT/NAACL2006, pages 479?486.
Association for Computa-tional Linguistics.Fader, Anthony, Dragomir Radev, Michael Crespin,Burt Monroe, Kevin Quinn, and Michael Colaresi.2007.
Mavenrank: Identifying influential membersof the us senate using lexical centrality.
In EMNLP2007.Hartog, Chris Den and Nathan Monroe.
2004.
Thevalue of majority status: The effect of jeffords?sswitch on asset prices of republican and democraticfirms.
Legislative Studies Quarterly, 33:63?84.Kleinberg, Jon.
1998.
Authoritative sources in a hyper-linked environment.
In the ACM-SIAM Symposiumon Discrete Algorithms, pages 668?677.Kurland, Oren and Lillian Lee.
2005.
PageRank with-out hyperlinks: Structural re-ranking using links in-duced by language models.
In SIGIR 2005, pages306?313.Kurland, Oren and Lillian Lee.
2006.
Respect my au-thority!
HITS without hyperlinks, utilizing cluster-based language models.
In SIGIR 2006, pages 83?90.McLachlan, Geoffrey and David Peel.
2000.
FiniteMixture Models.
New York: Wiley.Mihalcea, Rada and Paul Tarau.
2004.
TextRank:Bringing order into texts.
In EMNLP 2004.Mihalcea, Rada, Paul Tarau, and Elizabeth Figa.
2004.Pagerank on semantic networks, with applicationto word sense disambiguation.
In COLING 2004,pages 1126?1132.Monroe, Burt, Cheryl Monroe, Kevin Quinn, DragomirRadev, Michael Crespin, Michael Colaresi, AnthonyFader, Jacob Balazer, and Steven Abney.
2006.United states congressional speech corpus.
Depart-ment of Political Science, The Pennsylvania StateUniversity.Newman, Mark.
2003.
A measure of betweennesscentrality based on random walks.
Technical Reportcond-mat/0309045, Arxiv.org.Porter, Mason, Peter Mucha, Miark Newman, andCasey Warmbrand.
2005.
A network analysis ofcommittees in the U.S. House of Representatives.PNAS, 102(20).Quinn, Kevin, Burt Monroe, Michael Colaresi, MichaelCrespin, and Dragomir Radev.
2006.
An automatedmethod of topic-coding legislative speech over timewith application to the 105th-108th U.S. senate.
InMidwest Political Science Association Meeting.Sparck-Jones, Karen.
1972.
A statistical interpretationof term specificity and its application in retrieval.Journal of Documentation, 28(1):11?20.Thomas, Matt, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or opposition fromCongressional floor-debate transcripts.
In EMNLP2006, pages 327?335.320
