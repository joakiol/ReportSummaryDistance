Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 703?711,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsThe impact of language models and loss functions on repair disfluencydetectionSimon Zwarts and Mark JohnsonCentre for Language TechnologyMacquarie University{simon.zwarts|mark.johnson|}@mq.edu.auAbstractUnrehearsed spoken language often containsdisfluencies.
In order to correctly inter-pret a spoken utterance, any such disfluen-cies must be identified and removed or other-wise dealt with.
Operating on transcripts ofspeech which contain disfluencies, we studythe effect of language model and loss func-tion on the performance of a linear rerankerthat rescores the 25-best output of a noisy-channel model.
We show that language mod-els trained on large amounts of non-speechdata improve performance more than a lan-guage model trained on a more modest amountof speech data, and that optimising f-scorerather than log loss improves disfluency detec-tion performance.Our approach uses a log-linear reranker, oper-ating on the top n analyses of a noisy chan-nel model.
We use large language models,introduce new features into this reranker andexamine different optimisation strategies.
Weobtain a disfluency detection f-scores of 0.838which improves upon the current state-of-the-art.1 IntroductionMost spontaneous speech contains disfluencies suchas partial words, filled pauses (e.g., ?uh?, ?um?,?huh?
), explicit editing terms (e.g., ?I mean?
), par-enthetical asides and repairs.
Of these, repairspose particularly difficult problems for parsing andrelated Natural Language Processing (NLP) tasks.This paper presents a model of disfluency detec-tion based on the noisy channel framework, whichspecifically targets the repair disfluencies.
By com-bining language models and using an appropriateloss function in a log-linear reranker we are able toachieve f-scores which are higher than previously re-ported.Often in natural language processing algorithms,more data is more important than better algorithms(Brill and Banko, 2001).
It is this insight that drivesthe first part of the work described in this paper.
Thispaper investigates how we can use language modelstrained on large corpora to increase repair detectionaccuracy performance.There are three main innovations in this paper.First, we investigate the use of a variety of languagemodels trained from text or speech corpora of vari-ous genres and sizes.
The largest available languagemodels are based on written text: we investigate theeffect of written text language models as opposed tolanguage models based on speech transcripts.
Sec-ond, we develop a new set of reranker features ex-plicitly designed to capture important properties ofspeech repairs.
Many of these features are lexicallygrounded and provide a large performance increase.Third, we utilise a loss function, approximate ex-pected f-score, that explicitly targets the asymmetricevaluation metrics used in the disfluency detectiontask.
We explain how to optimise this loss func-tion, and show that this leads to a marked improve-ment in disfluency detection.
This is consistent withJansche (2005) and Smith and Eisner (2006), whoobserved similar improvements when using approx-imate f-score loss for other problems.
Similarly weintroduce a loss function based on the edit-f-score inour domain.703Together, these three improvements are enough toboost detection performance to a higher f-score thanpreviously reported in literature.
Zhang et al (2006)investigate the use of ?ultra large feature spaces?
asan aid for disfluency detection.
Using over 19 mil-lion features, they report a final f-score in this task of0.820.
Operating on the same body of text (Switch-board), our work leads to an f-score of 0.838, this isa 9% relative improvement in residual f-score.The remainder of this paper is structured as fol-lows.
First in Section 2 we describe related work.Then in Section 3 we present some background ondisfluencies and their structure.
Section 4 describesappropriate evaluation techniques.
In Section 5 wedescribe the noisy channel model we are using.
Thenext three sections describe the new additions: Sec-tion 6 describe the corpora used for language mod-els, Section 7 describes features used in the log-linear model employed by the reranker and Section 8describes appropriate loss functions which are criti-cal for our approach.
We evaluate the new model inSection 9.
Section 10 draws up a conclusion.2 Related workA number of different techniques have been pro-posed for automatic disfluency detection.
Schuleret al (2010) propose a Hierarchical Hidden MarkovModel approach; this is a statistical approach whichbuilds up a syntactic analysis of the sentence andmarks those subtrees which it considers to be madeup of disfluent material.
Although they are inter-ested not only in disfluency but also a syntactic anal-ysis of the utterance, including the disfluencies be-ing analysed, their model?s final f-score for disflu-ency detection is lower than that of other models.Snover et al (2004) investigate the use of purelylexical features combined with part-of-speech tagsto detect disfluencies.
This approach is compared toapproaches which use primarily prosodic cues, andappears to perform equally well.
However, the au-thors note that this model finds it difficult to identifydisfluencies which by themselves are very fluent.
Aswe will see later, the individual components of a dis-fluency do not have to be disfluent by themselves.This can occur when a speaker edits her speech formeaning-related reasons, rather than errors that arisefrom performance.
The edit repairs which are the fo-cus of our work typically have this characteristic.Noisy channel models have done well on the dis-fluency detection task in the past; the work of John-son and Charniak (2004) first explores such an ap-proach.
Johnson et al (2004) adds some hand-written rules to the noisy channel model and use amaximum entropy approach, providing results com-parable to Zhang et al (2006), which are state-of-theart results.Kahn et al (2005) investigated the role ofprosodic cues in disfluency detection, although themain focus of their work was accurately recoveringand parsing a fluent version of the sentence.
Theyreport a 0.782 f-score for disfluency detection.3 Speech DisfluenciesWe follow the definitions of Shriberg (1994) regard-ing speech disfluencies.
She identifies and definesthree distinct parts of a speech disfluency, referredto as the reparandum, the interregnum and the re-pair.
Consider the following utterance:I want a flightreparandum?
??
?to Boston,uh, I mean?
??
?interregnumto Denver?
??
?repairon Friday(1)The reparandum to Boston is the part of the utterancethat is ?edited out?
; the interregnum uh, I mean is afilled pause, which need not always be present; andthe repair to Denver replaces the reparandum.Shriberg and Stolcke (1998) studied the locationand distribution of repairs in the Switchboard cor-pus (Godfrey and Holliman, 1997), the primary cor-pus for speech disfluency research, but did not pro-pose an actual model of repairs.
They found that theoverall distribution of speech disfluencies in a largecorpus can be fit well by a model that uses only in-formation on a very local level.
Our model, as ex-plained in section 5, follows from this observation.As our domain of interest we use the Switchboardcorpus.
This is a large corpus consisting of tran-scribed telephone conversations between two part-ners.
In the Treebank III (Marcus et al, 1999) cor-pus there is annotation available for the Switchboardcorpus, which annotates which parts of utterancesare in a reparandum, interregnum or repair.7044 Evaluation metrics for disfluencydetection systemsDisfluency detection systems like the one describedhere identify a subset of the word tokens in eachtranscribed utterance as ?edited?
or disfluent.
Per-haps the simplest way to evaluate such systems isto calculate the accuracy of labelling they produce,i.e., the fraction of words that are correctly labelled(i.e., either ?edited?
or ?not edited?).
However,as Charniak and Johnson (2001) observe, becauseonly 5.9% of words in the Switchboard corpus are?edited?, the trivial baseline classifier which assignsall words the ?not edited?
label achieves a labellingaccuracy of 94.1%.Because the labelling accuracy of the trivial base-line classifier is so high, it is standard to use a dif-ferent evaluation metric that focuses more on the de-tection of ?edited?
words.
We follow Charniak andJohnson (2001) and report the f-score of our disflu-ency detection system.
The f-score f is:f =2cg + e(2)where g is the number of ?edited?
words in the goldtest corpus, e is the number of ?edited?
words pro-posed by the system on that corpus, and c is the num-ber of the ?edited?
words proposed by the systemthat are in fact correct.
A perfect classifier whichcorrectly labels every word achieves an f-score of1, while the trivial baseline classifiers which labelevery word as ?edited?
or ?not edited?
respectivelyachieve a very low f-score.Informally, the f-score metric focuses more onthe ?edited?
words than it does on the ?not edited?words.
As we will see in section 8, this has implica-tions for the choice of loss function used to train theclassifier.5 Noisy Channel ModelFollowing Johnson and Charniak (2004), we use anoisy channel model to propose a 25-best list ofpossible speech disfluency analyses.
The choice ofthis model is driven by the observation that the re-pairs frequently seem to be a ?rough copy?
of thereparandum, often incorporating the same or verysimilar words in roughly the same word order.
Thatis, they seem to involve ?crossed?
dependencies be-tween the reparandum and the repair.
Example (3)shows the crossing dependencies.
As this exam-ple also shows, the repair often contains many ofthe same words that appear in the reparandum.
Infact, in our Switchboard training corpus we foundthat 62reparandum also appeared in the associatedrepair,to Boston uh, I mean, to Denver?
??
?reparandum?
??
?interregnum?
??
?repair(3)5.1 Informal DescriptionGiven an observed sentence Y we wish to find themost likely source sentence X?
, whereX?
= argmaxXP (Y |X)P (X) (4)In our model the unobserved X is a substring of thecomplete utterance Y .Noisy-channel models are used in a similar wayin statistical speech recognition and machine trans-lation.
The language model assigns a probabilityP (X) to the string X , which is a substring of theobserved utterance Y .
The channel model P (Y |X)generates the utterance Y , which is a potentially dis-fluent version of the source sentence X .
A repaircan potentially begin before any word of X .
Whena repair has begun, the channel model incrementallyprocesses the succeeding words from the start of therepair.
Before each succeeding word either the re-pair can end or else a sequence of words can be in-serted in the reparandum.
At the end of each re-pair, a (possibly null) interregnum is appended to thereparandum.We will look at these two components in the nexttwo Sections in more detail.5.2 Language ModelInformally, the task of language model componentof the noisy channel model is to assess fluency ofthe sentence with disfluency removed.
Ideally wewould like to have a model which assigns a veryhigh probability to disfluency-free utterances and alower probability to utterances still containing dis-fluencies.
For computational complexity reasons, asdescribed in the next section, inside the noisy chan-nel model we use a bigram language model.
This705bigram language model is trained on the fluent ver-sion of the Switchboard corpus (training section).We realise that a bigram model might not be ableto capture more complex language behaviour.
Thismotivates our investigation of a range of additionallanguage models, which are used to define featuresused in the log-linear reranker as described below.5.3 Channel ModelThe intuition motivating the channel model designis that the words inserted into the reparandum arevery closely related to those in the repair.
Indeed,in our training data we find that 62% of the wordsin the reparandum are exact copies of words in therepair; this identity is strong evidence of a repair.The channel model is designed so that exact copyreparandum words will have high probability.Because these repair structures can involve an un-bounded number of crossed dependencies, they can-not be described by a context-free or finite-stategrammar.
This motivates the use of a more expres-sive formalism to describe these repair structures.We assume that X is a substring of Y , i.e., that thesource sentence can be obtained by deleting wordsfrom Y , so for a fixed observed utterance Y thereare only a finite number of possible source sen-tences.
However, the number of possible source sen-tences, X , grows exponentially with the length of Y ,so exhaustive search is infeasible.
Tree AdjoiningGrammars (TAG) provide a systematic way of for-malising the channel model, and their polynomial-time dynamic programming parsing algorithms canbe used to search for likely repairs, at least whenused with simple language models like a bigramlanguage model.
In this paper we first identify the25 most likely analyses of each sentence using theTAG channel model together with a bigram lan-guage model.Further details of the noisy channel model can befound in Johnson and Charniak (2004).5.4 RerankerTo improve performance over the standard noisychannel model we use a reranker, as previously sug-gest by Johnson and Charniak (2004).
We rerank a25-best list of analyses.
This choice is motivated byan oracle experiment we performed, probing for thelocation of the best analysis in a 100-best list.
Thisexperiment shows that in 99.5% of the cases the bestanalysis is located within the first 25, and indicatesthat an f-score of 0.958 should be achievable as theupper bound on a model using the first 25 best anal-yses.
We therefore use the top 25 analyses from thenoisy channel model in the remainder of this paperand use a reranker to choose the most suitable can-didate among these.6 Corpora for language modellingWe would like to use additional data to modelthe fluent part of spoken language.
However, theSwitchboard corpus is one of the largest widely-available disfluency-annotated speech corpora.
It isreasonable to believe that for effective disfluency de-tection Switchboard is not large enough and moretext can provide better analyses.
Schwartz et al(1994), although not focusing on disfluency detec-tion, show that using written language data for mod-elling spoken language can improve performance.We turn to three other bodies of text and investi-gate the use of these corpora for our task, disfluencydetection.
We will describe these corpora in detailhere.The predictions made by several language modelsare likely to be strongly correlated, even if the lan-guage models are trained on different corpora.
Thismotivates the choice for log-linear learners, whichare built to handle features which are not necessar-ily independent.
We incorporate information fromthe external language models by defining a rerankerfeature for each external language model.
The valueof this feature is the log probability assigned by thelanguage model to the candidate underlying fluentsubstring XFor each of our corpora (including Switchboard)we built a 4-gram language model with Kneser-Neysmoothing (Kneser and Ney, 1995).
For each analy-sis we calculate the probability under that languagemodel for the candidate underlying fluent substringX .
We use this log probability as a feature in thereranker.
We use the SRILM toolkit (Stolcke, 2002)both for estimating the model from the training cor-pus as well as for computing the probabilities of theunderlying fluent sentences X of the different anal-ysis.As previously described, Switchboard is our pri-706mary corpus for our model.
The language modelpart of the noisy channel model already uses a bi-gram language model based on Switchboard, but inthe reranker we would like to also use 4-grams forreranking.
Directly using Switchboard to build a 4-gram language model is slightly problematic.
Whenwe use the training data of Switchboard both for lan-guage fluency prediction and the same training dataalso for the loss function, the reranker will overesti-mate the weight associated with the feature derivedfrom the Switchboard language model, since the flu-ent sentence itself is part of the language modeltraining data.
We solve this by dividing the Switch-board training data into 20 folds.
For each fold weuse the 19 other folds to construct a language modeland then score the utterance in this fold with thatlanguage model.The largest widely-available corpus for languagemodelling is the Web 1T 5-gram corpus (Brants andFranz, 2006).
This data set, collected by GoogleInc., contains English word n-grams and their ob-served frequency counts.
Frequency counts are pro-duced from this billion-token corpus of web text.Because of the noise1 present in this corpus there isan ongoing debate in the scientific community of theuse of this corpus for serious language modelling.The Gigaword Corpus (Graff and Cieri, 2003)is a large body of newswire text.
The corpus con-tains 1.6 ?
109 tokens, however fluent newswire textis not necessarily of the same domain as disfluencyremoved speech.The Fisher corpora Part I (David et al, 2004) andPart II (David et al, 2005) are large bodies of tran-scribed text.
Unlike Switchboard there is no disflu-ency annotation available for Fisher.
Together thetwo Fisher corpora consist of 2.2 ?
107 tokens.7 FeaturesThe log-linear reranker, which rescores the 25-bestlists produced by the noisy-channel model, canalso include additional features besides the noisy-channel log probabilities.
As we show below, theseadditional features can make a substantial improve-ment to disfluency detection performance.
Ourreranker incorporates two kinds of features.
The first1We do not mean speech disfluencies here, but noise in web-text; web-text is often poorly written and unedited text.are log-probabilities of various scores computed bythe noisy-channel model and the external languagemodels.
We only include features which occur atleast 5 times in our training data.The noisy channel and language model featuresconsist of:1.
LMP: 4 features indicating the probabilities ofthe underlying fluent sentences under the lan-guage models, as discussed in the previous sec-tion.2.
NCLogP: The Log Probability of the entirenoisy channel model.
Since by itself the noisychannel model is already doing a very good job,we do not want this information to be lost.3.
LogFom: This feature is the log of the ?fig-ure of merit?
used to guide search in the noisychannel model when it is producing the 25-bestlist for the reranker.
The log figure of merit isthe sum of the log language model probabilityand the log channel model probability plus 1.5times the number of edits in the sentence.
Thisfeature is redundant, i.e., it is a linear combina-tion of other features available to the rerankermodel: we include it here so the reranker hasdirect access to all of the features used by thenoisy channel model.4.
NCTransOdd: We include as a feature parts ofthe noisy channel model itself, i.e.
the channelmodel probability.
We do this so that the taskto choosing appropriate weights of the channelmodel and language model can be moved fromthe noisy channel model to the log-linear opti-misation algorithm.The boolean indicator features consist of the fol-lowing 3 groups of features operating on words andtheir edit status; the latter indicated by one of threepossible flags: when the word is not part of a dis-fluency or E when it is part of the reparandum or Iwhen it is part of the interregnum.1.
CopyFlags X Y: When there is an exact copyin the input text of length X (1 ?
X ?
3) andthe gap between the copies is Y (0 ?
Y ?
3)this feature is the sequence of flags covering thetwo copies.
Example: CopyFlags 1 0 (E707) records a feature when two identical wordsare present, directly consecutive and the firstone is part of a disfluency (Edited) while thesecond one is not.
There are 745 different in-stances of these features.2.
WordsFlags L n R: This feature records theimmediate area around an n-gram (n ?
3).L denotes how many flags to the left and R(0 ?
R ?
1) how many to the right are includesin this feature (Both L and R range over 0 and1).
Example: WordsFlags 1 1 0 (need) is a feature that fires when a fluent word isfollowed by the word ?need?
(one flag to theleft, none to the right).
There are 256808 ofthese features present.3.
SentenceEdgeFlags B L: This feature indi-cates the location of a disfluency in an ut-terance.
The Boolean B indicates whetherthis features records sentence initial or sen-tence final behaviour, L (1 ?
L ?
3)records the length of the flags.
ExampleSentenceEdgeFlags 1 1 (I) is a fea-ture recording whether a sentence ends on aninterregnum.
There are 22 of these featurespresent.We give the following analysis as an example:but E but that does n?t workThe language model features are the probabilitycalculated over the fluent part.
NCLogP, Log-Fom and NCTransOdd are present with their asso-ciated value.
The following binary flags are present:CopyFlags 1 0 (E )WordsFlags:0:1:0 (but E)WordsFlags:0:1:0 (but )WordsFlags:1:1:0 (E but )WordsFlags:1:1:0 ( that )WordsFlags:0:2:0 (but E but ) etc.2SentenceEdgeFlags:0:1 (E)SentenceEdgeFlags:0:2 (E )SentenceEdgeFlags:0:3 (E )These three kinds of boolean indicator features to-gether constitute the extended feature set.2An exhaustive list here would be too verbose.8 Loss functions for reranker trainingWe formalise the reranker training procedure as fol-lows.
We are given a training corpus T containinginformation about n possibly disfluent sentences.For the ith sentence T specifies the sequence ofwords xi, a set Yi of 25-best candidate ?edited?
la-bellings produced by the noisy channel model, aswell as the correct ?edited?
labelling y?i ?
Yi.3We are also given a vector f = (f1, .
.
.
, fm)of feature functions, where each fj maps a wordsequence x and an ?edit?
labelling y for x to areal value fj(x, y).
Abusing notation somewhat,we write f(x, y) = (f1(x, y), .
.
.
, fm(x, y)).
Weinterpret a vector w = (w1, .
.
.
, wm) of featureweights as defining a conditional probability distri-bution over a candidate set Y of ?edited?
labellingsfor a string x as follows:Pw(y | x,Y) =exp(w ?
f(x, y))?y?
?Y exp(w ?
f(x, y?
))We estimate the feature weights w from the train-ing data T by finding a feature weight vector w?
thatoptimises a regularised objective function:w?
= argminwLT (w) + ?m?j=1w2jHere ?
is the regulariser weight and LT is a lossfunction.
We investigate two different loss functionsin this paper.
LogLoss is the negative log conditionallikelihood of the training data:LogLossT (w) =m?i=1?
log P(y?i | xi,Yi)Optimising LogLoss finds the w?
that define (regu-larised) conditional Maximum Entropy models.It turns out that optimising LogLoss yields sub-optimal weight vectors w?
here.
LogLoss is a sym-metric loss function (i.e., each mistake is equallyweighted), while our f-score evaluation metricweights ?edited?
labels more highly, as explainedin section 4.
Because our data is so skewed (i.e.,?edited?
words are comparatively infrequent), we3In the situation where the true ?edited?
labelling does notappear in the 25-best list Yi produced by the noisy-channelmodel, we choose y?i to be a labelling in Yi closest to the truelabelling.708can improve performance by using an asymmetricloss function.Inspired by our evaluation metric, we devised anapproximate expected f-score loss function FLoss .FLossT (w) = 1 ?2Ew[c]g + Ew[e]This approximation assumes that the expectationsapproximately distribute over the division: see Jan-sche (2005) and Smith and Eisner (2006) for otherapproximations to expected f-score and methods foroptimising them.
We experimented with other asym-metric loss functions (e.g., the expected error rate)and found that they gave very similar results.An advantage of FLoss is that it and its deriva-tives with respect to w (which are required fornumerical optimisation) are easy to calculate ex-actly.
For example, the expected number of correct?edited?
words is:Ew[c] =n?i=1Ew[cy?i | Yi], where:Ew[cy?i | Yi] =?y?Yicy?i (y) Pw(y | xi,Yi)and cy?
(y) is the number of correct ?edited?
labelsin y given the gold labelling y?.
The derivatives ofFLoss are:?FLossT?wj(w) =1g + Ew[e](FLossT (w)?Ew[e]?wj?
2?Ew[c]?wj)where:?Ew[c]?wj=n?i=1?Ew[cy?i | xi,Yi]?wj?Ew[cy?
| x,Y]?wj=Ew[fjcy?
| x,Y] ?
Ew[fj | x,Y] Ew[cy?
| x,Y].
?E[e]/?wj is given by a similar formula.9 ResultsWe follow Charniak and Johnson (2001) and splitthe corpus into main training data, held-out train-ing data and test data as follows: main training con-sisted of all sw[23]?.dps files, held-out training con-sisted of all sw4[5-9]?.dps files and test consisted ofall sw4[0-1]?.dps files.
However, we follow (John-son and Charniak, 2004) in deleting all partial wordsand punctuation from the training and test data (theyargued that this is more realistic in a speech process-ing application).Table 1 shows the results for the different modelson held-out data.
To avoid over-fitting on the testdata, we present the f-scores over held-out trainingdata instead of test data.
We used the held-out datato select the best-performing set of reranker features,which consisted of features for all of the languagemodels plus the extended (i.e., indicator) features,and used this model to analyse the test data.
The f-score of this model on test data was 0.838.
In thistable, the set of Extended Features is defined as allthe boolean features as described in Section 7.We first observe that adding different external lan-guage models does increase the final score.
Thedifference between the external language models isrelatively small, although the differences in choiceare several orders of magnitude.
Despite the pu-tative noise in the corpus, a language model builton Google?s Web1T data seems to perform verywell.
Only the model where Switchboard 4-gramsare used scores slightly lower, we explain this be-cause the internal bigram model of the noisy chan-nel model is already trained on Switchboard and sothis model adds less new information to the rerankerthan the other models do.Including additional features to describe the prob-lem space is very productive.
Indeed the best per-forming model is the model which has all extendedfeatures and all language model features.
The dif-ferences among the different language models whenextended features are present are relatively small.We assume that much of the information expressedin the language models overlaps with the lexical fea-tures.We find that using a loss function related to ourevaluation metric, rather than optimising LogLoss ,consistently improves edit-word f-score.
The stan-dard LogLoss function, which estimates the ?max-imum entropy?
model, consistently performs worsethan the loss function minimising expected errors.The best performing model (Base + Ext.
Feat.+ All LM, using expected f-score loss) scores an f-score of 0.838 on test data.
The results as indicatedby the f-score outperform state-of-the-art models re-709Model F-scoreBase (noisy channel, no reranking) 0.756Model log loss expected f-score lossBase + Switchboard 0.776 0.791Base + Fisher 0.771 0.797Base + Gigaword 0.777 0.797Base + Web1T 0.781 0.798Base + Ext.
Feat.
0.824 0.827Base + Ext.
Feat.
+ Switchboard 0.827 0.828Base + Ext.
Feat.
+ Fisher 0.841 0.856Base + Ext.
Feat.
+ Gigaword 0.843 0.852Base + Ext.
Feat.
+ Web1T 0.843 0.850Base + Ext.
Feat.
+ All LM 0.841 0.857Table 1: Edited word detection f-score on held-out data for a variety of language models and loss functionsported in literature operating on identical data, eventhough we use vastly less features than other do.10 Conclusion and Future workWe have described a disfluency detection algorithmwhich we believe improves upon current state-of-the-art competitors.
This model is based on a noisychannel model which scores putative analyses witha language model; its channel model is inspired bythe observation that reparandum and repair are of-ten very similar.
As Johnson and Charniak (2004)noted, although this model performs well, a log-linear reranker can be used to increase performance.We built language models from a variety ofspeech and non-speech corpora, and examine the ef-fect they have on disfluency detection.
We use lan-guage models derived from different larger corporaeffectively in a maximum reranker setting.
We showthat the actual choice for a language model seemsto be less relevant and newswire text can be usedequally well for modelling fluent speech.We describe different features to improve disflu-ency detection even further.
Especially these fea-tures seem to boost performance significantly.Finally we investigate the effect of different lossfunctions.
We observe that using a loss function di-rectly optimising our interest yields a performanceincrease which is at least at large as the effect of us-ing very large language models.We obtained an f-score which outperforms othermodels reported in literature operating on identicaldata, even though we use vastly fewer features thanothers do.AcknowledgementsThis work was supported was supported under Aus-tralian Research Council?s Discovery Projects fund-ing scheme (project number DP110102593) andby the Australian Research Council as part of theThinking Head Project the Thinking Head Project,ARC/NHMRC Special Research Initiative Grant #TS0669874.
We thank the anonymous reviewers fortheir helpful comments.ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.
Published by Linguistic Data Consortium,Philadelphia.Erik Brill and Michele Banko.
2001.
Mitigating thePaucity-of-Data Problem: Exploring the Effect ofTraining Corpus Size on Classifier Performance forNatural Language Processing.
In Proceedings of theFirst International Conference on Human LanguageTechnology Research.Eugene Charniak and Mark Johnson.
2001.
Edit detec-tion and parsing for transcribed speech.
In Proceed-ings of the 2nd Meeting of the North American Chap-ter of the Association for Computational Linguistics,pages 118?126.Christopher Cieri David, David Miller, and KevinWalker.
2004.
Fisher English Training Speech Part1 Transcripts.
Published by Linguistic Data Consor-tium, Philadelphia.710Christopher Cieri David, David Miller, and KevinWalker.
2005.
Fisher English Training Speech Part2 Transcripts.
Published by Linguistic Data Consor-tium, Philadelphia.John J. Godfrey and Edward Holliman.
1997.Switchboard-1 Release 2.
Published by LinguisticData Consortium, Philadelphia.David Graff and Christopher Cieri.
2003.
English gi-gaword.
Published by Linguistic Data Consortium,Philadelphia.Martin Jansche.
2005.
Maximum Expected F-MeasureTraining of Logistic Regression Models.
In Proceed-ings of Human Language Technology Conference andConference on Empirical Methods in Natural Lan-guage Processing, pages 692?699, Vancouver, BritishColumbia, Canada, October.
Association for Compu-tational Linguistics.Mark Johnson and Eugene Charniak.
2004.
A TAG-based noisy channel model of speech repairs.
In Pro-ceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics, pages 33?39.Mark Johnson, Eugene Charniak, and Matthew Lease.2004.
An Improved Model for Recognizing Disfluen-cies in Conversational Speech.
In Proceedings of theRich Transcription Fall Workshop.Jeremy G. Kahn, Matthew Lease, Eugene Charniak,Mark Johnson, and Mari Ostendorf.
2005.
EffectiveUse of Prosody in Parsing Conversational Speech.
InProceedings of Human Language Technology Confer-ence and Conference on Empirical Methods in Natu-ral Language Processing, pages 233?240, Vancouver,British Columbia, Canada.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech, and Signal Processing, pages 181?184.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.Published by Linguistic Data Consortium, Philadel-phia.William Schuler, Samir AbdelRahman, Tim Miller, andLane Schwartz.
2010.
Broad-Coverage Parsing us-ing Human-Like Memory Constraints.
ComputationalLinguistics, 36(1):1?30.Richard Schwartz, Long Nguyen, Francis Kubala,George Chou, George Zavaliagkos, and JohnMakhoul.
1994.
On Using Written LanguageTraining Data for Spoken Language Modeling.
InProceedings of the Human Language TechnologyWorkshop, pages 94?98.Elizabeth Shriberg and Andreas Stolcke.
1998.
Howfar do speakers back up in repairs?
A quantitativemodel.
In Proceedings of the International Confer-ence on Spoken Language Processing, pages 2183?2186.Elizabeth Shriberg.
1994.
Preliminaries to a Theory ofSpeech Disuencies.
Ph.D. thesis, University of Cali-fornia, Berkeley.David A. Smith and Jason Eisner.
2006.
Minimum RiskAnnealing for Training Log-Linear Models.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and the 44th annual meeting ofthe Association for Computational Linguistics, pages787?794.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2004.
A Lexically-Driven Algorithm for DisfluencyDetection.
In Proceedings of Human Language Tech-nologies and North American Association for Compu-tational Linguistics, pages 157?160.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 901?904.Qi Zhang, Fuliang Weng, and Zhe Feng.
2006.
A pro-gressive feature selection algorithm for ultra large fea-ture spaces.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 561?568.711
