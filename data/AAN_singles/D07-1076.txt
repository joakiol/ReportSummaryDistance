Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
728?736, Prague, June 2007. c?2007 Association for Computational LinguisticsTree Kernel-based Relation Extractionwith Context-Sensitive Structured Parse Tree InformationGuoDong ZHOU12     Min ZHANG 2     Dong Hong JI 2     QiaoMing ZHU 11School of Computer Science & Technology         2  Institute for Infocomm ResearchSoochow Univ.
Heng Mui Keng TerraceSuzhou, China 215006                                           Singapore 119613Email: {gdzhou,qmzhu}@suda.edu.cn      Email: {zhougd, mzhang, dhji}@i2r.a-star.edu.sgAbstractThis paper proposes a tree kernel with context-sensitive structured parse tree information for re-lation extraction.
It resolves two critical problemsin previous tree kernels for relation extraction intwo ways.
First, it automatically determines a dy-namic context-sensitive tree span for relation ex-traction by extending the widely-used ShortestPath-enclosed Tree (SPT) to include necessarycontext information outside SPT.
Second, it pro-poses a context-sensitive convolution tree kernel,which enumerates both context-free and context-sensitive sub-trees by considering their  ancestornode paths as their contexts.
Moreover, this paperevaluates the complementary nature between ourtree kernel and a state-of-the-art linear kernel.Evaluation on the ACE RDC corpora shows thatour dynamic context-sensitive tree span is muchmore suitable for relation extraction than SPT andour tree kernel outperforms the state-of-the-artCollins and Duffy?s convolution tree kernel.
Italso shows that our tree kernel achieves much bet-ter performance than the state-of-the-art linearkernels .
Finally, it shows that feature-based andtree kernel-based methods much complement eachother and the composite kernel can well integrateboth flat and structured features.1 IntroductionRelation extraction is to find various predefined se-mantic relations between pairs of entities in text.
Theresearch in relation extraction has been promoted bythe Message Understanding Conferences (MUCs)(MUC, 1987-1998) and the NIST Automatic ContentExtraction (ACE) program (ACE, 2002-2005).
Ac-cording to the ACE Program, an entity is an object ora set of objects in the world and a relation is an ex-plicitly or implicitly stated relationship among enti-ties.
For example, the sentence ?Bill Gates is thechairman and chief software architect of MicrosoftCorporation.?
conveys the ACE-style relation?EMPLOYMENT.exec?
between the entities ?BillGates?
(person name) and ?Microsoft Corporation?
(organization name).
Extraction of semantic relationsbetween entities can be very useful in many applica-tions such as question answering, e.g.
to answer thequery ?Who is the president of the United States?
?,and information  retrieval, e.g.
to expand the query?George W. Bush?
with ?the president of the UnitedStates?
via his relationship with ?the United States?.Many researches have been done in relation extrac-tion.
Among them, feature-based methods (Kamb-hatla 2004; Zhou et al, 2005) achieve certain successby employing a large amount of diverse linguisticfeatures, varying from lexical knowledge, entity-related information to syntactic parse trees, depend-ency trees and semantic information.
However, it isdifficult for them to effectively capture structuredparse tree information (Zhou et al2005), which iscritical for further performance improvement in rela-tion extraction.As an alternative to feature-based methods, treekernel-based methods provide an elegant solution toexplore implicitly structured features by directlycomputing the similarity between two trees.
Althoughearlier researches (Zelenko et al2003; Culotta andSorensen 2004; Bunescu and Mooney 2005a) onlyachieve success on simple tasks and fail on complextasks, such as the ACE RDC task, tree kernel-basedmethods achieve much progress recently.
As thestate-of-the-art, Zhang et al(2006) applied the convo-lution tree kernel (Collins and Duffy 2001) andachieved comparable performance with a state-of-the-art linear kernel (Zhou et al2005) on the 5 relationtypes in the ACE RDC 2003 corpus.However, there are two problems in Collins andDuffy?s convolution tree kernel for relation extraction.The first is that the sub-trees enumerated in the treekernel computation are context-free.
That is, eachsub-tree enumerated in the tree kernel computation728does not consider the context information outside thesub-tree.
The second is to decide a proper tree span inrelation extraction.
Zhang et al(2006) explored fivetree spans in relation extraction and it was  a bit sur-prising to find that the Shortest Path-enclosed Tree(SPT, i.e.
the sub-tree enclosed by the shortest pathlinking two involved entities in the parse tree) per-formed best.
This is contrast to our intuition.
For ex-ample, ?got married?
is critical to determine therelationship between ?John?
and ?Mary?
in the sen-tence ?John and Mary got married?
?
as shown inFigure 1(e).
It is obvious that the information con-tained in SPT (?John and Marry?)
is not enough todetermine their relationship.This paper proposes a context-sensitive convolu-tion tree kernel for relation extraction to resolve theabove two problems.
It first automatically determinesa dynamic context-sensitive tree span for relation ex-traction by extending the Shortest Path-enclosed Tree(SPT) to include necessary context information out-side SPT.
Then it proposes a context-sensitive convo-lution tree kernel, whic h not only enumerates context-free sub-trees but also context-sensitive sub-trees byconsidering their ancestor node paths as their contexts.Moreover, this paper evaluates the complementarynature of different linear kernels and tree kernels via acomposite kernel.The layout of this paper is as follows.
In Section 2,we review related work in more details.
Then, thedynamic context-sensitive tree span and the context-sensitive convolution tree kernel are proposed in Sec-tion 3 while Section 4 shows the experimental results.Finally, we conclude our work in Sec tion 5.2 Related WorkThe relation extraction task was first introduced aspart of the Template Element task in MUC6 and thenformulated as the Template Relation task in MUC7.Since then, many methods, such as feature-based(Kambhatla 2004; Zhou et al2005, 2006), tree ker-nel-based (Zelenko et al2003; Culotta and Sorensen2004; Bunescu and Mooney 2005a; Zhang et al2006)and composite kernel-based (Zhao and Gris hman2005; Zhang et al2006), have been proposed in lit-erature.For the feature-based methods, Kambhatla (2004)employed Maximum Entropy models to combine di-verse lexical, syntactic and semantic features in rela-tion extraction, and achieved the F-measure of 52.8on the 24 relation subtypes in the ACE RDC 2003corpus.
Zhou et al(2005) further systematically ex-plored diverse features through a linear kernel andSupport Vector Machines, and achieved the F-measures of 68.0 and 55.5 on the 5 relation types andthe 24 relation subtypes in the ACE RDC 2003 cor-pus respectively.
One problem with the feature-basedmethods is that they need extensive feature engineer-ing.
Another problem is that, although they can ex-plore some structured information in the parse tree(e.g.
Kambhatla (2004) used the non-terminal pathconnecting the given two entities in a parse tree whileZhou et al (2005) introduced additional chunkingfeatures to enhance the performance), it is found dif-ficult to well preserve structured information in theparse trees using the feature-based methods.
Zhou etal (2006) further improved the performance by ex-ploring the commonality among related classes in aclass hierarchy using hierarchical learning strategy.As an alternative to the feature-based methods, thekernel-based methods (Haussler, 1999) have beenproposed to implicitly explore various features in ahigh dimensional space by employing a kernel to cal-culate the similarity between two objects directly.
Inparticular, the kernel-based methods could be veryeffective at reducing the burden of feature engineer-ing for structured objects in NLP researches, e.g.
thetree structure in relation extraction.Zelenko et al (2003) proposed a kernel betweentwo parse trees, which recursively matches nodesfrom roots to leaves in a top-down manner.
For eachpair of matched nodes, a subsequence kernel on theirchild nodes is invoked.
They achieved quite successon two simple relation extraction tasks.
Culotta andSorensen (2004) extended this work to estimate simi-larity between augmented dependency trees andachieved the F-measure of 45.8 on the 5 relationtypes in the ACE RDC 2003 corpus.
One problemwith the above two tree kernels is that matched nodesmust be at the same height and have the same path tothe root node.
Bunescu and Mooney (2005a) pro-posed a shortest path dependency tree kernel, whichjust sums up the number of common word classesat each position in the two paths, and achieved theF-measure of 52.5 on the 5 relation types in the ACERDC 2003 corpus.
They argued that the informationto model a relationship between two entities can betypically captured by the shortest path between themin the dependency graph.
While the shortest pathmay not be able to well preserve structured de-pendency tree information, another problem withtheir kernel is that the two paths should have samelength.
This makes it suffer from the similar behaviorwith that of Culotta and Sorensen (2004): high preci-sion but very low recall.As the state-of-the-art tree kernel-based method,Zhang et al(2006) explored various structured feature729spaces and used the convolution tree kernel overparse trees (Collins and Duffy 2001) to model syntac-tic structured information for relation extraction.They achieved the F-measures of 61.9 and 63.6 on the5 relation types of the ACE RDC 2003 corpus and the7 relation types of the ACE RDC 2004 corpus respec-tively without entity-related information while the F-measure on the 5 relation types in the ACE RDC2003 corpus reached 68.7 when entity-related infor-mation was included in the parse tree.
One problemwith Collins and Duffy?s convolution tree kernel isthat the sub-trees involved in the tree kernel computa-tion are context-free, that is, they do not consider theinformation outside the sub-trees.
This is differentfrom the tree kernel in Culota and Sorensen (2004),where the sub-trees involved in the tree kernel com-putation are context-sensitive (that is, with the pathfrom the tree root node to the sub-tree root node inconsideration).
Zhang et al(2006) also showed thatthe widely-used Shortest Path-enclosed Tree (SPT)performed best.
One problem with SPT is that it failsto capture the contextual information outside theshortest path, which is important for relation extrac-tion in many cases.
Our random selection of 100 pos i-tive training instances from the ACE RDC 2003training corpus shows that ~25% of the cases needcontextual information outside the shortest path.Among other kernels, Bunescu and Mooney (2005b)proposed a subsequence kernel and applied it in pro-tein interaction and ACE relation extraction tasks.In order to integrate the advantages of feature-based and tree kernel-based methods, some research-ers have turned to composite kernel-based methods.Zhao and Grishman (2005) defined several feature-based composite kernels to integrate diverse featuresfor relation extraction and achieved the F-measure of70.4 on the 7 relation types of the ACE RDC 2004corpus.
Zhang et al(2006) proposed two compositekernels to integrate a linear kernel and Collins andDuffy?s convolution tree kernel.
It achieved the F-measure of 70.9/57.2 on the 5 relation types/24 rela-tion subtypes in the ACE RDC 2003 corpus and theF-measure of 72.1/63.6 on the 7 relation types/23relation subtypes in the ACE RDC 2004 corpus.The above discussion suggests that structured in-formation in the parse tree may not be fully utilized inthe previous works, regardless of feature-based, treekernel-based or composite kernel-based methods.Compared with the previous works, this paper pro-poses a dynamic context-sensitive tree span trying tocover necessary structured information and a context-sensitive convolution tree kernel considering bothcontext-free and context-sensitive sub-trees.
Further-more, a composite kernel is applied to combine ourtree kernel and a state-of-the-art linear kernel for in-tegrating both flat and structured features in relationextraction as well as validating their complementarynature.3 Context Sensitive Convolution TreeKernel for Relation ExtractionIn this section, we first propose an algorithm to dy-namically determine a proper context-sensitive treespan and then a context-sensitive convolution treekernel for relation extraction.3.1 Dynamic Context-Sensitive Tree Span inRelation ExtractionA relation instance between two entities is encaps u-lated by a parse tree.
Thus, it is critical to understandwhich portion of a parse tree is important in the treekernel calculation.
Zhang et al(2006) systematicallyexplored seven different tree spans, including theShortest Path-enclosed Tree (SPT) and a Context-Sensitive Path-enclosed Tree1 (CSPT), and found thatSPT per formed best.
That is, SPT even outperformsCSPT.
This is contrary to our intuition.
For example,?got married?
is critical to determine the relationshipbetween ?John?
and ?Mary?
in the sentence ?Johnand Mary got married?
?
as shown in Figure 1(e),and the information contained in SPT (?John andMary?)
is not enough to determine their relationship.Obviously, context-sensitive tree spans should havethe potential for better performance.
One problemwith the context-sensitive tree span explored in Zhanget al(2006) is that it only considers the availability ofentities?
siblings and fails to consider following twofactors:1) Whether is the information contained in SPTenough to determine the relationship betweentwo entities?
It depends.
In the embedded cases,SPT is enough.
For example, ?John?s wife?
isenough to determine the relationship between?John?
and ?John?s wife?
in the sentence ?John?swife got a good job?
?
as shown in Figure 1(a) .However, SPT is not enough in the coordinatedcases, e.g.
to determine the relationship between?John?
and ?Mary?
in the sentence ?John andMary got married?
?
as shown in Figure 1(e).1 CSPT means SPT extending with the 1st left sibling ofthe node of entity 1 and the 1st right sibling of the nodeof entity 2.
In the case of no available  sibling, it movesto the parent of current node and repeat the same proc-ess until a sibling is available or the root is reached.7302) How can we extend SPT to include necessarycontext information if there is no enough infor-mation in SPT for relation extraction?To answer the above two questions, we randomlychose 100 positive instances from the ACE RDC2003 training data and studied their necessary treespans.
It was observed that we can classify them into5 categories: 1) embedded (37 instances), where oneentity is embedded in another entity, e.g.
?John?
and?John?s wife?
as shown in Figure 1(a); 2) PP-linked(21 instances), where one entity is linked to anotherentity via PP attachment, e.g.
?CEO?
and ?Microsoft?in the sentence ?CEO of Microsoft announced ?
?
asshown in Figure 1(b); 3) semi-structured (15 in-stances), where the sentence consists of a sequence ofnoun phrases (including the two given entities), e.g.?Jane?
and ?ABC news?
in the sentence ?Jane, ABCnews, California.?
as shown in Figure 1(c); 4) de-scriptive (7 instances), e.g.
the citizenship between?his mother?
and ?Lebanese?
in the sentence ?hismother Lebanese landed at ??
as shown in Figure1(d); 5) predicate-linked and others (19 instances,including coordinated cases), where the predicateinformation is necessary to determine the relationshipbetween two entities, e.g.
?John?
and ?Mary?
in thesentence ?John and Mary got married??
as shown inFigure 1(e);Based on the above observations, we implement analgorithm to determine the necessary tree span for therelation extract task.
The idea behind the algorithm isthat the necessary tree span for a relation should bedetermined dynamically according to its tree spancategory and context.
Given a parsed tree and twoentities in consideration, it first determin es the treespan category and then extends the tree span accord-ingly.
By default, we adopt the Shortest Path-enclosed Tree (SPT) as our tree span.
We only ex-pand the tree span when the tree span belongs to the?predicate-linked?
category.
This is based on our ob-servation that the tree spans belonging to the ?predi-cate-linked?
category vary much syntactically andmajority (~70%) of them need information outsideSPT while it is quite safe (>90%) to use SPT as thetree span for the remaining categories.
In our algo-rithm, the expansion is done by first moving up untila predicate-headed phrase is found and then movingdown along the predicated-headed path to the predi-cate terminal node.
Figure 1(e) shows an example forthe ?predicate-linked?
category where the lines witharrows indicate the expansion path.e) predicate-linked: SPT and the dynamic context-sensitive tree spanFigure 1: Different tree span categories with SPT (dotted circle) and an ex-ample of the dynamic context-sensitive tree span (solid circle)Figure 2: Examples of context-free and context-sensitive sub-trees related with Figure 1(b).Note: the bold node is the rootfor a sub-tree.A problem with our algorithm is how to deter-mine whether an entity pair belongs to the ?predi-cate-linked?
category.
In this paper, a simplemethod is applied by regarding the ?predicate-linked?
category as the default category.
That is,those entity pairs, which do not belong to the fourwell defined and easily detected categories (i.e.embedded, PP-liked, semi-structured and descrip-tive), are classified into the ?predicate-linked?
cate-gory.His mother Lebanese  landedPRP$ NNP VBD INNP-E1-PER NP-E2-GPE PPSd)  descriptiveNPNNat?VPJane ABC news ,NNP , NNP NNS , NNP .NP NP-E1-PER NP-E2-ORGNPc) semi-structuredCalifornia .
.,,,NP(NN)of MicrosoftIN NNPNP-E2-ORGPP(IN)-subrootb) context -sensitiveNP(NN)of MicrosoftIN NNPNP-E2-ORGS(VBD)PP(IN)-subrootc) context -sensitivePP(IN)-subtootNP-E2-ORGof MicrosoftIN NNPa) context -free?NPJohn and Mary  gotNNP CC NNP VBDmarriedNP-E1-PER NP-E2-PER VPSVPVBN ?John and Mary  gotNNP CC NNP VBDmarriedNP-E1-PER NP-E2-PER VPNP VP?NPCEO of Microsoft announcedNN IN NNP VBD ?NP-E1-PER NP-E2-ORGVPSb)  PP -linkedPP?John ?s wife found a  jobNNP POS NN VBD DT JJ NNNP NP-E1-PERNP-E2-PER VPSa) embeddedgood731Since ?predicate -linked?
instances only occupy~20% of cases, this explains why SPT performsbetter than the Context-Sensitive Path-enclosedTree (CSPT) as described in Zhang et al(2006):consistently adopting CSPT may introduce toomuch noise/unnecessary information in the treekernel.3.2 Context-Sensitive Convolution Tree KernelGiven any tree span, e.g.
the dynamic context-sensitive tree span in the last subsection, we nowstudy how to measure the similarity between twotrees, using a convolution tree kernel.A convolutionkernel (Haussler D., 1999) aims to capture structuredinformation in terms of substructures .
As a special-ized convolution kernel, Collins and Duffy?s convolu-tion tree kernel ),( 21 TTKC  (?C?
for convolution)counts the number of common sub-trees (sub-structures) as the syntactic structure similarity be-tween two parse trees T1 and T2 (Collins and Duffy2001):??
?D=2211 ,2121 ),(),(NnNnC nnTTK    (1)where Nj is the set of nodes in tree Tj , and 1 2( , )n nDevaluates the common sub-trees rooted at n1 and n2 2and is computed recursively as follows:1) If the context-free productions (Context-FreeGrammar(CFG) rules) at 1n  and 2n  are different,1 2( , ) 0n nD = ; Otherwise go to 2.2) If both 1n  and 2n  are POS tags, 1 2( , ) 1n n lD = ?
;Otherwise go to 3.3)  Calculate 1 2( , )n nD recursively as:?=D+=D)(#121211)),(),,((1(),(nchkknchknchnn l  (2)where )(# nch is the number of children of node n ,),( knch  is the k th child of node n  andl (0< l <1) isthe decay factor in order to make the kernel value lessvariable with respect to different sub-tree sizes.This convolution tree kernel has been successfullyapplied by Zhang et al(2006) in relation extraction.However, there is one problem with this tree kernel:the sub-trees involved in the tree kernel computationare context-free (That is, they do not consider theinformation outside the sub-trees).
This is contrast to2 That is, each node n encodes the identity of a sub-tree rooted at n and, if there are two nodes in thetree with the same label, the summation will go overboth of them.the tree kernel proposed in Culota and Sorensen(2004) which is context-sensitive, that is, it considersthe path from the tree root node to the sub-tree rootnode.
In order to integrate the advantages of both treekernels and resolve the problem in Collins andDuffy?s convolution tree kernel, this paper proposes acontext-sensitive convolution tree kernel.
It works bytaking ancestral information (i.e.
the root node path)of sub-trees into consideration:?
?= ?
?D=mi NnNniiCiiiinnTTK1 ]2[]2[],1[]1[111111])2[],1[(])2[],1[(  (3)Where?
][1 jN i is the set of root node paths with length iin tree T[j] while the maximal length of a rootnode path is defined by m.?
])[...(][ 211 jnnnjn ii = is a root node path withlength i in tree T[j] , which takes into account thei-1 ancestral nodes in2 [j] of 1n [j] in T[j].
Here,][1 jn k+  is the parent of ][ jn k and ][1 jn  is theroot node of a context-free sub-tree in T[j].
Forbetter differentiation, the label of each ancestralnode in in1 [j] is augmented with the POS tag ofits head word.?
])2[],1[( 11 ii nnD  measures the common context-sensitive sub-trees rooted at root node paths]1[1in  and ]2[1in3.
In our tree kernel, a sub-treebecomes context-sensitive with its dependence onthe root node path instead of the root node itself.Figure 2 shows a few examples of context-sensitive sub-trees with comparison to context-free sub-trees.Similar to Collins and Duffy (2001),   our tree ker-nel computes ])2[],1[( 11 ii nnD recursively as follows:1) If the context-sensitive productions (Context-Sensitive Grammar (CSG) rules with root nodepaths as their left hand sides) rooted at ]1[1in  and]2[1in  are different, return ])2[],1[( 11ii nnD =0;Otherwise go to Step 2.2) If both ]1[1n  and ]2[1n  are POS tags,l=D ])2[],1[( 11 ii nn ; Otherwise go to Step 3.3 That is, each root node path in1  encodes the identityof a context-sensitive sub-tree rooted at in1  and, ifthere are two root node paths in the tree with thesame label sequence, the summation will go overboth of them.7323) Calculate ])2[],1[( 11 ii nnD  recursively as:?=D+=D])1[(#111111))],2[(),],1[((1(])2[],1[(inchkiiiiknchknchnnl(4)where ])],[( 1 kjnch i  is the kth context-sensitivechild of the context-sensitive sub-tree rooted at][1 jn i  with ])[(# 1 jnch i the number of the con-text-sensitive children.
Here, l (0< l <1) is thedecay factor in order to make the kernel valueless variable with respect to different sizes of thecontext-sensitive sub-trees.It is worth comparing our tree kernel with previoustree kernels.
Obviously, our tree kernel is an exten-sion of Collins and Duffy?s convolution tree kernel,which is a special case of our tree kernel (if m=1 inEquation (3)).
Our tree kernel not only counts theoccurrence of each context-free sub-tree, which doesnot consider its ancestors, but also counts the occur-rence of each context-sensitive sub-tree, which con-siders its ancestors.
As a result, our tree kernel is notlimited by the constraints in previous tree kernels (asdiscussed in Section 2), such as Collins and Duffy(2001), Zhang et al(2006), Culotta and Sorensen(2004) and Bunescu and Mooney (2005a).
Finally,let?s study the computational issue with our tree ker-nel.
Although our tree kernel takes the context-sensitive sub-trees into consideration, it only slightlyincreases the computational burden, compared withCollins and Duffy?s convolution tree kernel.
This isdue to that 0])2[],1[( 11 =D nn  holds for the major-ity of context-free sub-tree pairs (Collins and Duffy2001) and that computation for context-sensitive sub-tree pairs is necessary only when0])2[],1[( 11 ?D nn  and the context-sensitive sub-tree pairs have the same root node path(i.e.
]2[]1[ 11 ii nn =  in Equation (3)).4 ExperimentationThis paper uses the ACE RDC 2003 and 2004 cor-pora provided by LDC in all our experiments.4.1 Experimental SettingThe ACE RDC corpora are gathered from variousnewspapers, newswire and broadcasts.
In the 2003corpus , the training set consists of 674 documents and9683 positive relation instances w hile the test set con-sists of 97 documents and 1386 positive relation in-stances.
The 2003 corpus defines 5 entity types, 5major relation types and 24 relation subtypes.
All thereported performances in this paper on the ACE RDC2003 corpus are evaluated on the test data.
The 2004corpus  contains 451 documents and 5702 positiverelation instances.
It redefines 7 entity types, 7 majorrelation types and 23 relation subtypes.
For compari-son, we use the same setting as Zhang et al(2006) byapplying a 5-fold cross-validation on a subset of the2004 data, containing 348 documents and 4400 rela-tion instances.
That is, all the reported performancesin this paper on the ACE RDC 2004 corpus are evalu-ated using 5-fold cross validation on the entire corpus .Both corpora are parsed using Charniak?s parser(Charniak, 2001) with the boundaries of all the entitymentions kept 4 .
We iterate over all pairs of entitymentions occurring in the same sentence to generatepotential relation instances5.
In our experimentation,SVM (SVMLight, Joachims(1998)) is selected as ourclassifier.
For efficiency, we apply the one vs. othersstrategy, which builds K classifiers so as to separateone class from all others.
The training parameters arechosen using cross-validation on the ACE RDC 2003training data.
In particular, l  in our tree kernel isfine-tuned to 0.5.
This suggests that about 50% dis-count is done as our tree kernel moves down onelevel in computing ])2[],1[( 11 ii nnD .4.2 Experimental ResultsFirst, we systematically evaluate the context-sensitiveconvolution tree kernel and the dynamic context-sensitive tree span proposed in this paper.Then, we evaluate the complementary nature be-tween our tree kernel and a state-of-the-art linear ker-nel via a composite kernel.
Generally differentfeature-based methods and tree kernel-based methodshave their own merits.
It is usually easy to build asystem using a feature-based method and achieve thestate-of-the-art performance, while tree kernel-basedmethods  hold the potential for further performanceimprovement.
Therefore, it is always a good idea tointegrate them via a composite kernel.4 This can be done by first representing all entity men-tions with their head words and then restoring all theentity mentions after parsing.
Moreover, please notethat the final performance of relation extraction maychange much with different range of parsing errors.We will study this issue in the near future.5 In this paper, we only measure the performance of rela-tion extraction on ?true?
mentions with ?true?
chain-ing of co-reference (i.e.
as annotated by LDCannotators ).
Moreover, we only model explicit relations andexplicitly model the argument order of the two mentions in-volved.733Finally, we compare our system with the state-of-the-art systems in the literature.Context-Sensitive Convolution Tree KernelIn this paper, the m parameter of our context-sensitiveconvolution tree kernel as shown in Equation (3)indicates the maximal length of root node paths and isoptimized to 3 using 5-fold cross validation on theACE RDC 2003 training data.
Table 1 compares theimpact of different m in context-sensitive convolutiontree kernels using the Shortest Path-enclosed Tree(SPT) (as described in Zhang et al(2006)) on themajor relation types of the ACE RDC 2003 and 2004corpora, in details.
It also shows that our tree kernelachieves best performance on the test data using SPTwith m = 3, which outperforms the one with m = 1 by~2.3 in F-measure.
This suggests the parent andgrandparent nodes of a sub-tree  contains muchinformation for relation extraction while consideringmore ancestral nodes may not help.
This may be dueto that, although our experimentation on thetraining data indicates that  more than 80% (onaverage) of subtrees has a root node path longerthan 3 (since most of the subtrees are deep from theroot node and more than 90% of the parsed trees inthe training data are deeper than 6 levels),including a root node path longer than 3 may bevulnerable to the full parsing errors and havenegative impact.
Table 1 also evaluates the impact ofentity-related information in our tree kernel byattaching entity type information (e.g.
?PER?
in theentity node 1 of Figure 1(b)) into both entity nodes.It shows that such information can significantlyimprove the performance by ~6.0 in F-measure.
In allthe following experiments, we will apply our treekernel with m=3 and entity-related information bydefault.Table 2 compares the dynamic context-sensitivetree span with SPT using our tree kernel.
It shows thatthe dynamic tree span can futher improve theperformance by ~1.2 in F-measure6.
This suggests theusefulness of extending the tree span beyond SPT forthe ?predicate-linked?
tree span category.
In thefuture work, we will further explore expanding thedynamic tree span beyond SPT for the remaining treespan categories.6 Significance test shows that the dynamic tree span per-forms s tatistically significantly better than SPT with p-values smaller than 0.05.m P(%) R(%) F1 72.3(72.7)  56.6(53.8) 63.5(61.8)2 74.9(75.2)  57.9(54.7) 65.3(63.5)3 75.7(76.1)  58.3(55.1) 65.9(64.0)4 76.0(75.9)  58.3(55.3) 66.0(63.9)a) without entity-related informationm P(%) R(%) F1 77.2(76.9)  63.5(60.8) 69.7(67.9)2 79.1(78.6)  65.0(62.2) 71.3(69.4)3 79.6(79.4)  65.6(62.5) 71.9(69.9)4 79.4(79.1)  65.6(62.3) 71.8(69.7)b) with entity-related informationTable 1: Evaluation of context-sensitive convolutiontree kernels using SPT on the major relation types ofthe ACE RDC 2003 (inside the parentheses) and 2004(outside the parentheses) corpora.Tree Span P(%) R(%) FShortest Path-enclosed Tree79.6(79.4)65.6(62.5)71.9(69.9)Dynamic Context-Sensitive Tee81.1(80.1)66.7(63.8)73.2(71.0)Table 2: Comparison of dynamic context-sensitivetree span with SPT using our context-sensitiveconvolution tree kernel on the major relation types ofthe ACE RDC 2003 (inside the parentheses) and 2004(outside the parentheses) corpora.
18% of positiveinstances in the ACE RDC 2003 test data belong tothe predicate-linked category.Composite KernelIn this paper, a composite kernel via polynomial in-terpolation, as described Zhang et al(2006), is ap-plied to integrate the proposed context-sensitiveconvolution tree kernel with a state-of-the-art linearkernel (Zhou et al2005) 7:),()1(),(),(1 ???-+???=??
CPL KKK aa  (5)Here, ),( ?
?LK  and ),( ?
?CK  indicates the normal-ized linear kernel and context-sensitive convolutiontree kernel respectively while  ( , )pK ?
?
is the poly-nomial expansion of ( , )K ?
?
with degree d=2, i.e.2( , ) ( ( , ) 1)pK K?
?
?
?= +  and a  is the coefficient (a  isset to 0.3 using cross-validation).7 Here, we use the same set of flat features (i.e.
word,entity type, mention level, overlap, base phrase chunk-ing, dependency tree, parse tree and semantic informa-tion) as Zhou et al(2005).734Table 3 evaluates the performance of thecomposite kernel.
It shows that the composite kernelmuch further improves the performance beyond thatof either the state-of-the-art linear kernel or our treekernel and achieves the F-measures of 74.1 and 75.8on the major relation types of the ACE RDC 2003and 2004 corpora respectively.
This suggests that ourtree kernel and the state-of-the-art linear kernel arequite complementary, and that our composite kernelcan effectively integrate both flat and structuredfeatures.System P(%) R(%) FLinear Kernel 78.2 (77.2)63.4(60.7)70.1(68.0)Context-Sensitive Con-volution Tree Kernel81.1(80.1)66.7(63.8)73.2(71.0)Composite Kernel 82.2 (80.8)70.2(68.4)75.8(74.1)Table 3: Performance of the compos ite kernel viapolynomial interpolation on the major relation typesof the ACE RDC 2003 (inside the parentheses) and2004 (outside the parentheses) corporaComparison with Other SystemsACE RDC 2003 P(%) R(%) FOurs:composite kernel80.8(65.2)68.4(54.9)74.1(59.6)Zhang et al(2006):composite kernel77.3(64.9)65.6(51.2)70.9(57.2)Ours: context-sensitiveconvolution tree kernel80.1(63.4)63.8(51.9)71.0(57.1)Zhang et al(2006):convolution tree kernel76.1(62.4)62.6(48.5)68.7(54.6)Bunescu et al(2005):shortest pathdependency kernel65.5(-)43.8(-)52.5(-)Culotta et al(2004):dependency kernel67.1(-)35.0(-)45.8(-)Zhou et al (2005):feature-based77.2(63.1)60.7(49.5)68.0(55.5)Kambhatla (2004):feature-based-(63.5)-(45.2)-(52.8)Table 4: Comparison of difference systems on theACE RDC 2003 corpus over both 5 types (outside theparentheses) and 24 subtypes (inside the parentheses)ACE RDC 2004 P(%) R(%) FOurs:composite kernel82.2(70.3)70.2(62.2)75.8(66.0)Zhang et al(2006):composite kernel76.1(68.6)68.4(59.3)72.1(63.6)Zhao et al(2005):8composite kernel69.2(-)70.5(-)70.4(-)Ours: context-sensitiveconvolution tree kernel81.1(68.8)66.7(60.3)73.2(64.3)Zhang et al(2006):convolution tree kernel72.5(-)56.7(-)63.6(-)Table 5: Comparison of difference systems on theACE RDC 2004 corpus over both 7 types (outside theparentheses) and 23 subtypes (inside the parentheses)Finally, Tables 4 and 5 compare our system withother state-of-the-art systems9 on the ACE RDC 2003and 2004 corpora, respectively.
They show that ourtree kernel-based system outperforms previous treekernel-based systems.
This is largely due to the con-text-sensitive nature of our tree kernel which resolvesthe limitations of the previous tree kernels.
They alsoshow that our tree kernel-based system outperformsthe state-of-the-art feature-based system.
This provesthe great potential inherent in the parse tree structurefor relation extraction and our tree kernel takes a bigstride towards the right direction.
Finally, they alsoshow that our composite kernel-based system outper-forms other composite kernel-based systems.5 ConclusionStructured parse tree information holds great potentialfor relation extraction.
This paper proposes a context-sensitive convolution tree kernel to resolve two criti-cal problems in previous tree kernels for relation ex-traction by first automatically determining a dynamiccontext-sensitive tree span and then applying a con-text-sensitive convolution tree kernel.
Moreover, thispaper evaluates the complementary nature betweenour tree kernel and a state-of-the-art linear kernel.Evaluation on the ACE RDC corpora shows that ourdynamic context-sensitive tree span is much moresuitable for relation extraction than the widely -usedShortest Path-enclosed Tree and our tree kernel out-performs the state-of-the-art Collins and Duffy?s con-volution tree kernel.
It also shows that feature-based8 There might be some typing errors for the performancereported in Zhao and Grishman(2005) since P, R and Fdo not match.9 All the state-of-the-art systems apply the entity-relatedinformation.
It is not supervising: our experimentsshow that using the entity-related information gives alarge performance improvement.735and tree kernel-based methods well complement eachother and the composite kernel can effectively inte-grate both flat and structured features.To our knowledge, this is the first research to dem-onstrate that, without extensive feature engineer ing,an individual tree kernel can achieve much better per-formance than the state-of-the-art linear kernel in re-lation extraction.
This shows the great potential ofstructured parse tree information for relation extrac-tion and our tree kernel takes a big stride towards theright direction.For the future work, we will focus on improvingthe context-sensitive convolution tree kernel by ex-ploring more useful context information.
Moreover,we will explore more entity-related information in theparse tree.
Our preliminary work of including the en-tity type information significantly improves the per-formance.
Finally, we will study how to resolve thedata imbalance and sparseness issues from the learn-ing algorithm viewpoint.AcknowledgementThis research is supported by Project 60673041 underthe National Natural Science Foundation of Chinaand Project 2006AA01Z147 under the ?863?
NationalHigh-Tech Research and Development of China.
Wewould also like to thank the critical and insightfulcomments from the four anonymous reviewers.ReferencesACE.
(2000-2005).
Automatic Content Extraction.http://www.ldc.upenn.edu/Projects/ACE/Bunescu R. & Mooney R.J. (2005a).
A shortest pathdependency kernel for relation extraction.HLT/EMNLP?2005 : 724-731.
6-8 Oct 2005.
Van-cover, B.C.Bunescu R. & Mooney R.J. (2005b).
Subsequence Ker-nels for Relation Extraction  NIPS?2005.
Vancouver,BC, December 2005Charniak E. (2001).
Immediate-head Parsing for Lan-guage Models.
ACL?2001: 129-137.
Toulouse, FranceCollins M. and Duffy N. (2001).
Convolution Ke rnelsfor Natural Language.
NIPS?2001: 625-632.
Ca m-bridge, MACulotta A. and Sorensen J.
(2004).
Dependency treekernels for relation extraction.
ACL?2004 .
423-429.21-26 July 2004.
Ba rcelona, Spain.Haussler D. (1999).
Convolution Kernels on DiscreteStructures.
Technical Report UCS-CRL-99-10, Uni-versity of California, Santa CruzJoachims T. (1998).
Text Categorization with Su pportVector Machine: learning with many relevant fea-tures.
ECML-1998 : 137-142.
Chemnitz, GermanyKambhatla N. (2004).
Combining lexical, syntactic andsemantic features with Maximum Entropy models forextracting relations.
ACL?2004(Poster).
178-181.
21-26 July 2004.
Barcelona, Spain.MUC.
(1987-1998).
The NIST MUC website: http://www.itl.nist.gov/iaui/894.02/related_projects/muc/Zelenko D., Aone C. and Richardella.
(2003).
Kernelmethods for relation extraction.
Journal of MachineLearning Research.
3(Feb):1083-1106.Zhang M., Zhang J., Su J. and Zhou G.D. (2006).
AComposite Kernel to Extract Relations between Enti-ties with both Flat and Structured Features .
COLING-ACL-2006: 825-832.
Sydney, AustraliaZhao S.B.
and Grishman R. (2005).
Extracting relationswith integrated information using kernel methods.ACL?2005: 419-426.
Univ of Michigan-Ann Arbor,USA,  25-30 June 2005.Zhou G.D., Su J. Zhang J. and Zhang M. (2005).
Ex-ploring various knowledge in relation extraction.ACL?2005.
427-434.
25-30 June, Ann Arbor, Mich-gan, USA.Zhou G.D., Su J. and Zhang M. (2006).
Modeling com-monality among related classes in relation extraction,COLING-ACL?2006: 121-128.
Sydney, Australia.736
