Proceedings of NAACL-HLT 2013, pages 746?751,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsLinguistic Regularities in Continuous Space Word RepresentationsTomas Mikolov?
, Wen-tau Yih, Geoffrey ZweigMicrosoft ResearchRedmond, WA 98052AbstractContinuous space language models have re-cently demonstrated outstanding results acrossa variety of tasks.
In this paper, we ex-amine the vector-space word representationsthat are implicitly learned by the input-layerweights.
We find that these representationsare surprisingly good at capturing syntacticand semantic regularities in language, andthat each relationship is characterized by arelation-specific vector offset.
This allowsvector-oriented reasoning based on the offsetsbetween words.
For example, the male/femalerelationship is automatically learned, and withthe induced vector representations, ?King -Man + Woman?
results in a vector very closeto ?Queen.?
We demonstrate that the wordvectors capture syntactic regularities by meansof syntactic analogy questions (provided withthis paper), and are able to correctly answeralmost 40% of the questions.
We demonstratethat the word vectors capture semantic regu-larities by using the vector offset method toanswer SemEval-2012 Task 2 questions.
Re-markably, this method outperforms the bestprevious systems.1 IntroductionA defining feature of neural network language mod-els is their representation of words as high dimen-sional real valued vectors.
In these models (Ben-gio et al 2003; Schwenk, 2007; Mikolov et al2010), words are converted via a learned lookup-table into real valued vectors which are used as the?Currently at Google, Inc.inputs to a neural network.
As pointed out by theoriginal proposers, one of the main advantages ofthese models is that the distributed representationachieves a level of generalization that is not possi-ble with classical n-gram language models; whereasa n-gram model works in terms of discrete units thathave no inherent relationship to one another, a con-tinuous space model works in terms of word vectorswhere similar words are likely to have similar vec-tors.
Thus, when the model parameters are adjustedin response to a particular word or word-sequence,the improvements will carry over to occurrences ofsimilar words and sequences.By training a neural network language model, oneobtains not just the model itself, but also the learnedword representations, which may be used for other,potentially unrelated, tasks.
This has been used togood effect, for example in (Collobert and Weston,2008; Turian et al 2010) where induced word rep-resentations are used with sophisticated classifiers toimprove performance in many NLP tasks.In this work, we find that the learned word repre-sentations in fact capture meaningful syntactic andsemantic regularities in a very simple way.
Specif-ically, the regularities are observed as constant vec-tor offsets between pairs of words sharing a par-ticular relationship.
For example, if we denote thevector for word i as xi, and focus on the singu-lar/plural relation, we observe that xapple?xapples ?xcar?xcars, xfamily?xfamilies ?
xcar?xcars, andso on.
Perhaps more surprisingly, we find that thisis also the case for a variety of semantic relations, asmeasured by the SemEval 2012 task of measuringrelation similarity.746The remainder of this paper is organized as fol-lows.
In Section 2, we discuss related work; Section3 describes the recurrent neural network languagemodel we used to obtain word vectors; Section 4 dis-cusses the test sets; Section 5 describes our proposedvector offset method; Section 6 summarizes our ex-periments, and we conclude in Section 7.2 Related WorkDistributed word representations have a long his-tory, with early proposals including (Hinton, 1986;Pollack, 1990; Elman, 1991; Deerwester et al1990).
More recently, neural network languagemodels have been proposed for the classical lan-guage modeling task of predicting a probability dis-tribution over the ?next?
word, given some preced-ing words.
These models were first studied in thecontext of feed-forward networks (Bengio et al2003; Bengio et al 2006), and later in the con-text of recurrent neural network models (Mikolov etal., 2010; Mikolov et al 2011b).
This early workdemonstrated outstanding performance in terms ofword-prediction, but also the need for more compu-tationally efficient models.
This has been addressedby subsequent work using hierarchical prediction(Morin and Bengio, 2005; Mnih and Hinton, 2009;Le et al 2011; Mikolov et al 2011b; Mikolov etal., 2011a).
Also of note, the use of distributedtopic representations has been studied in (Hintonand Salakhutdinov, 2006; Hinton and Salakhutdi-nov, 2010), and (Bordes et al 2012) presents a se-mantically driven method for obtaining word repre-sentations.3 Recurrent Neural Network ModelThe word representations we study are learned by arecurrent neural network language model (Mikolovet al 2010), as illustrated in Figure 1.
This architec-ture consists of an input layer, a hidden layer with re-current connections, plus the corresponding weightmatrices.
The input vector w(t) represents inputword at time t encoded using 1-of-N coding, and theoutput layer y(t) produces a probability distributionover words.
The hidden layer s(t) maintains a rep-resentation of the sentence history.
The input vectorw(t) and the output vector y(t) have dimensional-ity of the vocabulary.
The values in the hidden andFigure 1: Recurrent Neural Network Language Model.output layers are computed as follows:s(t) = f (Uw(t) +Ws(t?1)) (1)y(t) = g (Vs(t)) , (2)wheref(z) =11 + e?z, g(zm) =ezm?k ezk.
(3)In this framework, the word representations arefound in the columns of U, with each column rep-resenting a word.
The RNN is trained with back-propagation to maximize the data log-likelihood un-der the model.
The model itself has no knowledgeof syntax or morphology or semantics.
Remark-ably, training such a purely lexical model to max-imize likelihood will induce word representationswith striking syntactic and semantic properties.4 Measuring Linguistic Regularity4.1 A Syntactic Test SetTo understand better the syntactic regularities whichare inherent in the learned representation, we createda test set of analogy questions of the form ?a is to bas c is to ?
testing base/comparative/superlativeforms of adjectives; singular/plural forms of com-mon nouns; possessive/non-possessive forms ofcommon nouns; and base, past and 3rd personpresent tense forms of verbs.
More precisely, wetagged 267M words of newspaper text with Penn747Category Relation Patterns Tested # Questions ExampleAdjectives Base/Comparative JJ/JJR, JJR/JJ 1000 good:better rough:Adjectives Base/Superlative JJ/JJS, JJS/JJ 1000 good:best rough:Adjectives Comparative/SuperlativeJJS/JJR, JJR/JJS 1000 better:best rougher:Nouns Singular/Plural NN/NNS,NNS/NN1000 year:years law:Nouns Non-possessive/PossessiveNN/NN POS,NN POS/NN1000 city:city?s bank:Verbs Base/Past VB/VBD,VBD/VB1000 see:saw return:Verbs Base/3rd PersonSingular PresentVB/VBZ, VBZ/VB 1000 see:sees return:Verbs Past/3rd PersonSingular PresentVBD/VBZ,VBZ/VBD1000 saw:sees returned:Table 1: Test set patterns.
For a given pattern and word-pair, both orderings occur in the test set.
For example, if?see:saw return: ?
occurs, so will ?saw:see returned: ?.Treebank POS tags (Marcus et al 1993).
We thenselected 100 of the most frequent comparative adjec-tives (words labeled JJR); 100 of the most frequentplural nouns (NNS); 100 of the most frequent pos-sessive nouns (NN POS); and 100 of the most fre-quent base form verbs (VB).
We then systematicallygenerated analogy questions by randomly matchingeach of the 100 words with 5 other words from thesame category, and creating variants as indicated inTable 1.
The total test set size is 8000.
The test setis available online.
14.2 A Semantic Test SetIn addition to syntactic analogy questions, we usedthe SemEval-2012 Task 2, Measuring Relation Sim-ilarity (Jurgens et al 2012), to estimate the extentto which RNNLM word vectors contain semanticinformation.
The dataset contains 79 fine-grainedword relations, where 10 are used for training and69 testing.
Each relation is exemplified by 3 or4 gold word pairs.
Given a group of word pairsthat supposedly have the same relation, the task isto order the target pairs according to the degree towhich this relation holds.
This can be viewed as an-other analogy problem.
For example, take the Class-Inclusion:Singular Collective relation with the pro-1http://research.microsoft.com/en-us/projects/rnn/default.aspxtotypical word pair clothing:shirt.
To measure thedegree that a target word pair dish:bowl has the samerelation, we form the analogy ?clothing is to shirt asdish is to bowl,?
and ask how valid it is.5 The Vector Offset MethodAs we have seen, both the syntactic and semantictasks have been formulated as analogy questions.We have found that a simple vector offset methodbased on cosine distance is remarkably effective insolving these questions.
In this method, we assumerelationships are present as vector offsets, so that inthe embedding space, all pairs of words sharing aparticular relation are related by the same constantoffset.
This is illustrated in Figure 2.In this model, to answer the analogy question a:bc:d where d is unknown, we find the embeddingvectors xa, xb, xc (all normalized to unit norm), andcompute y = xb ?
xa + xc.
y is the continuousspace representation of the word we expect to be thebest answer.
Of course, no word might exist at thatexact position, so we then search for the word whoseembedding vector has the greatest cosine similarityto y and output it:w?
= argmaxwxwy?xw?
?y?When d is given, as in our semantic test set, wesimply use cos(xb ?
xa + xc, xd) for the words748Figure 2: Left panel shows vector offsets for three wordpairs illustrating the gender relation.
Right panel showsa different projection, and the singular/plural relation fortwo words.
In high-dimensional space, multiple relationscan be embedded for a single word.provided.
We have explored several related meth-ods and found that the proposed method performswell for both syntactic and semantic relations.
Wenote that this measure is qualitatively similar to rela-tional similarity model of (Turney, 2012), which pre-dicts similarity between members of the word pairs(xb, xd), (xc, xd) and dis-similarity for (xa, xd).6 Experimental ResultsTo evaluate the vector offset method, we usedvectors generated by the RNN toolkit of Mikolov(2012).
Vectors of dimensionality 80, 320, and 640were generated, along with a composite of severalsystems, with total dimensionality 1600.
The sys-tems were trained with 320M words of BroadcastNews data as described in (Mikolov et al 2011a),and had an 82k vocabulary.
Table 2 shows resultsfor both RNNLM and LSA vectors on the syntactictask.
LSA was trained on the same data as the RNN.We see that the RNN vectors capture significantlymore syntactic regularity than the LSA vectors, anddo remarkably well in an absolute sense, answeringmore than one in three questions correctly.
2In Table 3 we compare the RNN vectors withthose based on the methods of Collobert and We-ston (2008) and Mnih and Hinton (2009), as imple-mented by (Turian et al 2010) and available online3 Since different words are present in these datasets,we computed the intersection of the vocabularies ofthe RNN vectors and the new vectors, and restrictedthe test set and word vectors to those.
This resultedin a 36k word vocabulary, and a test set with 66322Guessing gets a small fraction of a percent.3http://metaoptimize.com/projects/wordreprs/Method Adjectives Nouns Verbs AllLSA-80 9.2 11.1 17.4 12.8LSA-320 11.3 18.1 20.7 16.5LSA-640 9.6 10.1 13.8 11.3RNN-80 9.3 5.2 30.4 16.2RNN-320 18.2 19.0 45.0 28.5RNN-640 21.0 25.2 54.8 34.7RNN-1600 23.9 29.2 62.2 39.6Table 2: Results for identifying syntactic regularities fordifferent word representations.
Percent correct.Method Adjectives Nouns Verbs AllRNN-80 10.1 8.1 30.4 19.0CW-50 1.1 2.4 8.1 4.5CW-100 1.3 4.1 8.6 5.0HLBL-50 4.4 5.4 23.1 13.0HLBL-100 7.6 13.2 30.2 18.7Table 3: Comparison of RNN vectors with Turian?s Col-lobert and Weston based vectors and the HierarchicalLog-Bilinear model of Mnih and Hinton.
Percent correct.questions.
Turian?s Collobert andWeston based vec-tors do poorly on this task, whereas the HierarchicalLog-Bilinear Model vectors of (Mnih and Hinton,2009) do essentially as well as the RNN vectors.These representations were trained on 37M wordsof data and this may indicate a greater robustness ofthe HLBL method.We conducted similar experiments with the se-mantic test set.
For each target word pair in a rela-tion category, the model measures its relational sim-ilarity to each of the prototypical word pairs, andthen uses the average as the final score.
The resultsare evaluated using the two standard metrics definedin the task, Spearman?s rank correlation coefficient?
and MaxDiff accuracy.
In both cases, larger val-ues are better.
To compare to previous systems, wereport the average over all 69 relations in the test set.From Table 4, we see that as with the syntac-tic regularity study, the RNN-based representationsperform best.
In this case, however, Turian?s CWvectors are comparable in performance to the HLBLvectors.
With the RNN vectors, the performance im-proves as the number of dimensions increases.
Sur-prisingly, we found that even though the RNN vec-749Method Spearman?s ?
MaxDiff Acc.LSA-640 0.149 0.364RNN-80 0.211 0.389RNN-320 0.259 0.408RNN-640 0.270 0.416RNN-1600 0.275 0.418CW-50 0.159 0.363CW-100 0.154 0.363HLBL-50 0.149 0.363HLBL-100 0.146 0.362UTD-NB 0.230 0.395Table 4: Results in measuring relation similaritytors are not trained or tuned specifically for this task,the model achieves better results (RNN-320, RNN-640 & RNN-1600) than the previously best perform-ing system, UTD-NB (Rink and Harabagiu, 2012).7 ConclusionWe have presented a generally applicable vector off-set method for identifying linguistic regularities incontinuous space word representations.
We haveshown that the word representations learned by aRNNLM do an especially good job in capturingthese regularities.
We present a new dataset for mea-suring syntactic performance, and achieve almost40% correct.
We also evaluate semantic general-ization on the SemEval 2012 task, and outperformthe previous state-of-the-art.
Surprisingly, both re-sults are the byproducts of an unsupervised maxi-mum likelihood training criterion that simply oper-ates on a large amount of text data.ReferencesY.
Bengio, R. Ducharme, Vincent, P., and C. Jauvin.2003.
A neural probabilistic language model.
Jour-nal of Machine Learning Reseach, 3(6).Y.
Bengio, H. Schwenk, J.S.
Sene?cal, F. Morin, and J.L.Gauvain.
2006.
Neural probabilistic language models.Innovations in Machine Learning, pages 137?186.A.
Bordes, X. Glorot, J. Weston, and Y. Bengio.
2012.Joint learning of words and meaning representationsfor open-text semantic parsing.
In Proceedings of 15thInternational Conference on Artificial Intelligence andStatistics.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: Deep neural networkswith multitask learning.
In Proceedings of the 25thinternational conference on Machine learning, pages160?167.
ACM.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for Informa-tion Science, 41(96).J.L.
Elman.
1991.
Distributed representations, simple re-current networks, and grammatical structure.
Machinelearning, 7(2):195?225.G.E.
Hinton and R.R.
Salakhutdinov.
2006.
Reducingthe dimensionality of data with neural networks.
Sci-ence, 313(5786):504?507.G.
Hinton and R. Salakhutdinov.
2010.
Discovering bi-nary codes for documents by learning deep generativemodels.
Topics in Cognitive Science, 3(1):74?91.G.E.
Hinton.
1986.
Learning distributed representationsof concepts.
In Proceedings of the eighth annual con-ference of the cognitive science society, pages 1?12.Amherst, MA.David Jurgens, Saif Mohammad, Peter Turney, and KeithHolyoak.
2012.
Semeval-2012 task 2: Measuring de-grees of relational similarity.
In *SEM 2012: The FirstJoint Conference on Lexical and Computational Se-mantics (SemEval 2012), pages 356?364.
Associationfor Computational Linguistics.Hai-Son Le, I. Oparin, A. Allauzen, J.-L. Gauvain, andF.
Yvon.
2011.
Structured output layer neural networklanguage model.
In Proceedings of ICASSP 2011.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: the penn treebank.
Computational Lin-guistics, 19(2):313?330.Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-jeev Khudanpur.
2010.
Recurrent neural networkbased language model.
In Proceedings of Interspeech2010.Tomas Mikolov, Anoop Deoras, Daniel Povey, LukasBurget, and Jan Cernocky.
2011a.
Strategies forTraining Large Scale Neural Network Language Mod-els.
In Proceedings of ASRU 2011.Tomas Mikolov, Stefan Kombrink, Lukas Burget, JanCernocky, and Sanjeev Khudanpur.
2011b.
Ex-tensions of recurrent neural network based languagemodel.
In Proceedings of ICASSP 2011.Tomas Mikolov.
2012.
RNN toolkit.A.
Mnih and G.E.
Hinton.
2009.
A scalable hierarchicaldistributed language model.
Advances in neural infor-mation processing systems, 21:1081?1088.F.
Morin and Y. Bengio.
2005.
Hierarchical probabilisticneural network language model.
In Proceedings of the750international workshop on artificial intelligence andstatistics, pages 246?252.J.B.
Pollack.
1990.
Recursive distributed representa-tions.
Artificial Intelligence, 46(1):77?105.Bryan Rink and Sanda Harabagiu.
2012.
UTD: Deter-mining relational similarity using lexical patterns.
In*SEM 2012: The First Joint Conference on Lexicaland Computational Semantics (SemEval 2012), pages413?418.
Association for Computational Linguistics.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21(3):492?
518.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proceedings of Association forComputational Linguistics (ACL 2010).P.D.
Turney.
2012.
Domain and function: A dual-spacemodel of semantic relations and compositions.
Jour-nal of Artificial Intelligence Research, 44:533?585.751
