Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 133?138,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCreating Interactive Macaronic Interfaces for Language LearningAdithya Renduchintala and Rebecca Knowles and Philipp Koehn and Jason EisnerDepartment of Computer ScienceJohns Hopkins University{adi.r,rknowles,phi,eisner}@jhu.eduAbstractWe present a prototype of a novel tech-nology for second language instruction.Our learn-by-reading approach lets a hu-man learner acquire new words and con-structions by encountering them in con-text.
To facilitate reading comprehen-sion, our technology presents mixed na-tive language (L1) and second language(L2) sentences to a learner and allowsthem to interact with the sentences tomake the sentences easier (more L1-like)or harder (more L2-like) to read.
Eventu-ally, our system should continuously tracka learner?s knowledge and learning styleby modeling their interactions, includingperformance on a pop quiz feature.
Thiswill allow our system to generate person-alized mixed-language texts for learners.1 IntroductionGrowing interest in self-directed language learn-ing methods like Duolingo (von Ahn, 2013), alongwith recent advances in machine translation andthe widespread ease of access to a variety of textsin a large number of languages, has given rise toa number of web-based tools related to languagelearning (ranging from dictionary apps to moreinteractive tools like Alpheios (Nelson, 2007) orLingua.ly (2013)).
Most of these either focus onvocabulary learning or require hand-curated les-son plans.
We present a prototype of a systemfor learning to read in a foreign language, whichpresents learners with text consisting of a mix oftheir native language (L1) and the language theyare interested in learning (L2).
We refer to sen-tences containing a mix of L1 and L2 text as mac-aronic1sentences.
Along the continuum from1The term ?macaronic?
traditionally refers to a mash-up of languages, often intended to be humorous.
We usethis term, rather than ?code-switching,?
since code-switchingfully L1 to fully L2 text are sentences with anycombination of L1 and L2 vocabulary, syntax, and(potentially) morphology.Proponents of language acquisition through ex-tensive reading, such as Krashen (1989), arguethat much of language acquisition takes placethrough incidental learning?when a learner is ex-posed to novel vocabulary or structures and mustfind a way to understand them in order to compre-hend the text.
The trouble is that learning by read-ing already requires considerable L2 fluency.
Tobootstrap, we propose making L2 sentences moreaccessible to early learners by shifting these sen-tences along the macaronic spectrum towards L1,stopping at the ?zone of proximal development?(Vygotski?
?, 2012) where the learner is able to com-prehend the text but only by stretching their L2capacity.
We aim in the future to customize maca-ronic sentences to each individual learner.A reasonable concern is whether exposure tomacaronic language might actually harm acquisi-tion of correct L2 (even though our interface usescolor and font to mark the L1 ?intrusions?
into theL2 sentence).
As some reassurance, our approachis analogous to the well-established paradigm ofinventive spelling (or ?invented spelling?
),2inwhich early writers are encouraged to write intheir native language without concern for cor-rect spelling, in part so they can more fullyand happily engage with the writing challengeof composing longer and more authentic texts(Clarke, 1988).
We also observe that simulta-neous dual language acquisition?from multilin-gual and code-switched language?is common foryoung children in many countries, who employcode-switching in a socially appropriate way andas ?a resource .
.
.
to fill gaps in their developingrequires the speaker/writer to be fluent in both languages.Code-switching is governed by syntactic and pragmatic con-siderations, rather than by pedagogical or humorous ones.2Spelling, like L2, is a type of linguistic knowledge thatis acquired after L1 fluency and largely through incidentallearning (Krashen, 1993).133languages?
(Genesee, 2009).
Still, it remains anopen question whether older students can success-fully unlearn initial habits and move toward an in-creasingly complete and correct L2 model.We envision our technology being used along-side traditional classroom L2 instruction?thesame instructional mix that leads parents to ac-cept inventive spelling (Gentry, 2000).
Traditionalgrammar-based instruction and assessment, whichuse ?toy?
sentences in pure L2, should provide fur-ther scaffolding for our users to acquire languageby reading more advanced (but macaronic) text.We provide details of the current user interfaceand discuss how content for our system can be au-tomatically generated using existing statistical ma-chine translation (SMT) methods, enabling learn-ers or teachers to choose their own texts to read.Our prototype is currently running on http://www.clsp.jhu.edu:3030/ with samplecontent.
Our interface lets the user navigatethrough the spectrum from L2 to L1, going beyondthe single-word or single-phrase translations of-fered by other online tools such as Swych (2015),or dictionary-like browser plugins.Finally, we discuss plans to extend this proto-type and to integrate it with a continuously adapt-ing user model.
To this end, our companion pa-per (Renduchintala et al, 2016) develops an initialmodel of macaronic sentence comprehension bynovice L2 learners, using data collected from hu-man subjects via Amazon?s Mechanical Turk ser-vice.
In another paper (Knowles et al, 2016), wecarry out a controlled study of comprehension ofindividual L2 words in isolation and in L1 context.2 Macaronic InterfaceFor the purposes of this demo we assume a na-tive English speaker (L1=English) who is learn-ing German (L2=German).
However, our exist-ing interface can accommodate any pair of lan-guages whose writing systems share directional-ity.3The primary goal of the interface is to em-power a learner to translate and reorder parts of aconfusing foreign language sentence.
These trans-lations and reorderings serve to make the Germansentence more English-like.
The interface alsopermits reverse transformations, letting the curi-ous learner ?peek ahead?
at how specific Englishwords and constructions would surface in German.3We also assume that the text is segmented into words.
(a) Initial sentence state.
(b) Mouse hovered under Preis.
(c) Preis translated to prize.
(d) Mouse hovered above prize.
Clicking above will revertthe sentence back to the initial state 1a.
(e) Sentence with 2 different words translated into EnglishFigure 1: Actions that translate words.Using these fundamental interactions as build-ing blocks, we create an interactive framework fora language learner to explore this continuum of?English-like?
to ?foreign-like?
sentences.
By re-peated interaction with new content and exposureto recurring vocabulary items and linguistic pat-terns, we believe a learner can pick up vocabularyand other linguistic rules of the foreign language.2.1 TranslationThe basic interface idea is that a line of macaronictext is equipped with hidden interlinear annota-tions.
Notionally, English translations lurk belowthe macaronic text, and German ones above.The Translation interaction allows the learnerto change the text in the macaronic sentence fromone language to another.
Consider a macaronicsentence that is completely in the foreign state(i.e.,, entirely in German), as shown in Fig.
1a.Hovering on or under a German word shows a pre-view of a translation (Fig.
1b).
Clicking on thepreview will cause the translation to ?rise up?
andreplace the German word (Fig.
1c).To translate in the reverse direction, the user canhover and click above an English word (Fig.
1d).Since the same mechanism applies to all thewords in the sentence, a learner can manipulatetranslations for each word independently.
For ex-ample, Fig.
1e shows two words in English.The version of our prototype displayed in Fig-ure 1 blurs the preview tokens when a learner ishovering above or below a word.
This blurredpreview acts as a visual indication of a potentialchange to the sentence state (if clicked) but it also134(a)(b)(c)(d)Figure 2: Actions that reorder phrases.gives the learner a chance to think about what thetranslation might be, based on visual clues such aslength and shape of the blurred text.2.2 ReorderingWhen the learner hovers slightly below the wordsnach Georg B?uchner a Reordering arrow isdisplayed (as shown in Figure 2).
The arrow is anindicator of reordering.
In this example, the Ger-man past participle benannt appears at the endof the sentence (the conjugated form of the verb isist benannt, or is named); this is the gram-matically correct location for the participle in Ger-man, while the English form should appear earlierin the equivalent English sentence.Similar to the translation actions, reorderingactions also have a directional attribute.
Figure2b shows a German-to-English direction arrow.When the learner clicks the arrow, the interface re-arranges all the words involved in the reordering.The new word positions are shown in 2c.
Onceagain, the user can undo: hovering just abovenach Georg B?uchner now shows a gray ar-row, which if clicked returns the phrase to its Ger-man word order (shown in 2d).German phrases that are not in original Germanorder are highlighted as a warning (Figure 2c).2.3 ?Pop Quiz?
FeatureSo far, we have described the system?s standardresponses to a learner?s actions.
We now add oc-casional ?pop quizzes.?
When a learner hovers be-low a German word (s0in Figure 3) and clicks theblurry English text, the system can either revealthe translation of the German word (state s2) as de-s0s1s3s4s5s2s6bcceeacFigure 3: State diagram of learner interaction (edges) andsystem?s response(vertices).
Edges can be traversed by click-ing (c), hovering above (a), hovering below (b) or the enter(e) key.
Unmarked edges indicate an automatic transition.scribed in section 2.1 or quiz the learner (state s3).We implement the quiz by presenting a text inputbox to the learner: here the learner is expected totype what they believe the German word means.Once a guess is typed, the system indicates if theguess is correct (s4) or incorrect(s5) by flashinggreen or red highlights in the text box.
The boxthen disappears (after 700ms) and the system au-tomatically proceeds to the reveal state s2.
As thisimposes a high cognitive load and increases the in-teraction complexity (typing vs. clicking), we in-tend to use the pop quiz infrequently.The pop quiz serves two vital functions.
First,it further incentivizes the user to retain learned vo-cabulary.
Second, it allows the system to update itsmodel of the user?s current L2 lexicon, macaroniccomprehension, and learning style; this is work inprogress (see section 4.2).2.4 Interaction ConsistencyAgain, we regard the macaronic sentence as a kindof interlinear text, written between two mostly in-visible sentences: German above and English be-low.
In general, hovering above the macaronicsentence will reveal German words or word or-ders, which fall down into the macaronic sentenceupon clicking.
Hovering below will reveal Englishtranslations, which rise up upon clicking.The words in the macaronic sentence are col-ored according to their language.
We want theuser to become accustomed to reading German, sothe German words are in plain black text by de-135Action Direction Trigger Preview Preview Color Confirm ResultTranslationE-to-G Hover above EnglishBlurry Germantranslation aboveGray BlurClick onBlurry Texttranslation replacesEnglish word(s)G-to-EHover under GermantokenBlurry Englishtranslation belowBlue BlurClick onBlurry Texttranslation replacesGerman word(s)ReorderingE-to-G Hover above tokenArrow abovereordering tokensGray Arrow Click on Arrow tokens reorderG-to-E Hover under tokenArrow belowreordering tokensBlue Arrow Click on Arrow tokens reorderTable 1: Summary of learner triggered interactions in the Macaronic Interface.fault, while the English words use a marked colorand font (italic blue).
Reordering arrows also fol-low the same color scheme: arrows that will makethe macaronic sentence more ?German-like?
aregray, while arrows that make the sentence more?English-like?
are blue.
The summary of interac-tions is shown in Table 1.3 Constructing Macaronic TranslationsIn this section, we describe the details of the un-derlying data structures needed to allow all the in-teractions mentioned in the previous section.
Akey requirement in the design of the data struc-ture was to support orthogonal actions in each sen-tence.
Making all translation and reordering ac-tions independent of one another creates a largespace of macaronic states for a learner to explore.At present, the input to our macaronic inter-face is bitext with word-to-word alignments pro-vided by a phrase-based SMT system (or, if de-sired, by hand).
We employ Moses (Koehn et al,2007) to translate German sentences and gener-ate phrase alignments.
News articles written insimple German from nachrichtenleicht.de (Deutschlandfunk, 2016) were translated aftertraining the SMT system on the WMT15 German-English corpus (Bojar et al, 2015).We convert the word alignments into ?mini-mal alignments?
that are either one-to-one, one-to-many or many-to-one.4This step ensures con-sistent reversibility of actions and prevents largephrases from being translated with a single click.5The resulting bipartite graph can be regarded as4For each many-to-many alignment returned by the SMTsystem, we remove alignment edges (lowest probability first)until the alignment is no longer many-to-many.
Then wegreedily add edges from unaligned tokens (highest probabil-ity first), subject to not creating many-to-many alignmentsand subject to minimizing the number of crossing edges, un-til all tokens are aligned.5Preliminary experiments showed that allowing largephrases to translate with one click resulted in abrupt jumpsin the visualization, which users found hard to follow.Figure 4: The dotted lines show word-to-word alignmentsbetween the German sentence f0, f1, .
.
.
, f7and its Englishtranslation e0, e1, .
.
.
, e6.
The figure highlights 3 of the 7units: u2, u3, u4.Figure 5: A possible state of the sentence, which renders asubset of the tokens (shown in black).
The rendering order(section 3.2) is not shown but is also part of the state.
Thestring displayed in this case is ?Und danach they runnoch einen Marathon.?
(assuming no reordering).a collection of connected components, or units(Fig.
4).63.1 Translation MechanismIn a given state of the macaronic sentence, eachunit is displayed in either English or German.
Atranslation action toggles the display language ofthe unit, leaving it in place.
For example, in Fig-ure 5, where the macaronic sentence is currentlydisplaying f4f5= noch einen, a translationaction will replace this with e4= a.3.2 Reordering MechanismA reordering action changes the unit orderof the current macaronic sentence.
The out-6In the sections below, we gloss over cases where a unit isdiscontiguous (in one language).
Such units are handled spe-cially (we omit details for reasons of space).
If a unit wouldfall outside the bounds of what our special handling can han-dle, we fuse it with another unit.136put string ?Und danach they run nocheinen Marathon.?
is obtained from Figure5 only if unit u2(as labeled in Figure 4) is ren-dered (in its current language) to the left of unitu3, which we write as u2< u3.
In this case, it ispossible for the user to change the order of theseunits, because u3< u2in German.
Table 2 showsthe 8 possible combinations of ordering and trans-lation choices for this pair of units.String Rendered Unit Ordering.
.
.they run.
.
.
{u2} < {u3}.
.
.they laufen.
.
.. .
.sie run.
.
.. .
.sie laufen.
.
.. .
.run they.
.
.
{u2} > {u3}.
.
.run sie.
.
.. .
.laufen they.
.
.. .
.laufen sie.
.
.Table 2: Generating reordered strings using units.The space of possible orderings for a sentencepair is defined by a bracketing ITG tree (Wu,1997), which transforms the German ordering ofthe units into the English ordering by a collec-tion of nested binary swaps of subsequences.7Theordering state of the macaronic sentence is givenby the subset of these swaps that have been per-formed.
A reordering action toggles one of theswaps in this collection.Since we have a parser for German (Raffertyand Manning, 2008), we take care to select anITG tree that is ?compatible?
with the Germansentence?s dependency structure, in the followingsense: if the ITG tree combines two spans A andB, then there are not dependencies from words inA to words in B and vice-versa.4 Discussion and Future Work4.1 Machine Translation ChallengesWhen the English version of the sentence is pro-duced by an MT system, it may suffer from MTerrors and/or poor alignments.Even with correct MT, a given syntactic con-struction may be handled inconsistently on differ-ent occasions, depending on the particular wordsinvolved (as these affect what phrasal alignmentis found and how we convert it to a minimal align-ment).
Syntax-based MT could be used to design amore consistent interface that is also more closelytied to classroom L2 lessons.7Occasionally no such ITG tree exists, in which case wefuse units as needed until one does.Cross-linguistic divergences in the expressionof information (Dorr, 1994) could be confusing.For example, when moving through macaronicspace from Kaffee gef?allt Menschen(coffee pleases humans) to its translation humanslike coffee, it may not be clear to thelearner that the reordering is triggered by thefact that like is not a literal translation ofgef?allt.
One way to improve this might be tohave the system pass smoothly through a rangeof intermediate translations from word-by-wordglosses to idiomatic phrasal translations, ratherthan always directly translating idioms.
We mightalso see benefit in guiding our gradual translationswith cognates (for example, rather than translatedirectly from the German M?ohre to the Englishcarrot, we might offer the cognate Karotteas an intermediate step).We also plan to transition through wordsthat are macaronic at the sub-word level.
Forexample, hovering over the unfamiliar Ger-man word gesprochen might decompose itinto ge-sprochen; then clicking on one ofthose morphemes might yield ge-talk orsprech-ed before reaching talked.
Thiscould guide learners towards an understanding ofGerman tense marking and stem changes.4.2 User Adaptation and EvaluationWe would prefer to show the learner a macaronicsentence that provides just enough clues for thelearner to be able to comprehend it, while stillpushing them to figure out new vocabulary or newstructures.
Thus, we plan to situate this interfacein a framework that continuously adapts as theuser progresses.
As the user learns new vocabu-lary, the system will automatically present themwith more challenging sentences (containing lessL1).
In (Renduchintala et al, 2016) we show thatwe can predict a novice learner?s guesses of L2word meanings in macaronic sentences using afew simple features.
We will subsequently trackthe user?s learning by observing their mouse ac-tions and ?pop quiz?
responses (section 2).While we have had users interact with our sys-tem in order to collect data about novice learn-ers?
guesses, we are working toward an evaluationwhere our system is used to supplement classroominstruction for real foreign-language students.1375 ConclusionIn this work we present a prototype of an inter-active interface for learning to read in a foreignlanguage.
We expose the learner to L2 vocabularyand constructions in contexts that are comprehen-sible because they have been partially translatedinto the learner?s native language, using statisticalMT.
Using MT affords flexibility: learners or in-structors can choose which texts to read, and learn-ers or the system can control which parts of a sen-tence are translated.We are working towards integrating models oflearner understanding (Renduchintala et al, 2016;Knowles et al, 2016) to produce personalizedmacaronic texts that give each learner just the rightamount of challenge and support.
In the long term,we would like to extend the approach to allowusers also to produce macaronic language, draw-ing on techniques from grammatical error correc-tion or computer-aided translation to help themgradually remove L1 features from their writing(or speech) and make it more L2-like.AcknowledgmentsThis material is based upon work supported by aseed grant from the Science of Learning Instituteat Johns Hopkins University, and also by a Na-tional Science Foundation Graduate Research Fel-lowship (Grant No.
DGE-1232825) to the secondauthor.
We would like to thank Chadia Abras foruseful discussions.Supplemental Material?
A video demonstration can be found here:https://youtu.be/d5lxyeHIDWI?
A live sample version is here: http://www.clsp.jhu.edu:3030/signinReferencesOnd?rej Bojar, Rajen Chatterjee, Christian Federmann,Barry Haddow, Matthias Huck, Chris Hokamp,Philipp Koehn, Varvara Logacheva, Christof Monz,Matteo Negri, Matt Post, Carolina Scarton, LuciaSpecia, and Marco Turchi.
2015.
Findings of the2015 Workshop on Statistical Machine Translation.In Proceedings of the Tenth Workshop on StatisticalMachine Translation, pages 1?46.Linda K. Clarke.
1988.
Invented versus traditionalspelling in first graders?
writings: Effects on learn-ing to spell and read.
Research in the Teaching ofEnglish, pages 281?309, October.Deutschlandfunk.
2016. nachrichtenleicht.
http://www.nachrichtenleicht.de/.
Accessed:2015-09-30.Bonnie J. Dorr.
1994.
Machine translation diver-gences: A formal description and proposed solution.Computational Linguistics, 20(4):597?633, Decem-ber.Fred H. Genesee.
2009.
Early childhood bilingualism:Perils and possibilities.
Journal of Applied Researchon Learning, 2(Article 2):1?21, April.J.
Richard Gentry.
2000.
A retrospective on inventedspelling and a look forward.
The Reading Teacher,54(3):318?332, November.Rebecca Knowles, Adithya Renduchintala, PhilippKoehn, and Jason Eisner.
2016.
Analyzing learnerunderstanding of novel L2 vocabulary.
To appear.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of ACL: Interactive Poster and Demonstra-tion Sessions, pages 177?180.Stephen Krashen.
1989.
We acquire vocabulary andspelling by reading: Additional evidence for theinput hypothesis.
The Modern Language Journal,73(4):440?464.S.
Krashen.
1993.
How well do people spell?
ReadingImprovement, 30(1).Lingua.ly.
2013.
Lingua.ly.
https://lingua.ly/.
Accessed: 2016-04-04.Mark Nelson.
2007.
The Alpheios project.
http://alpheios.net/.
Accessed: 2016-04-05.Anna N Rafferty and Christopher D Manning.
2008.Parsing three German treebanks: Lexicalized andunlexicalized baselines.
In Proceedings of the Work-shop on Parsing German, pages 40?46.
Associationfor Computational Linguistics.Adithya Renduchintala, Rebecca Knowles, PhilippKoehn, and Jason Eisner.
2016.
User modeling inlanguage learning with macaronic texts.
In Proceed-ings of ACL.Swych.
2015.
Swych.
http://swych.it/.
Ac-cessed: 2016-04-05.Luis von Ahn.
2013.
Duolingo: Learn a language forfree while helping to translate the web.
In Proceed-ings of the 2013 International Conference on Intel-ligent User Interfaces, pages 1?2.Lev Vygotski??.
2012.
Thought and Language (Revisedand Expanded Edition).
MIT press.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.138
