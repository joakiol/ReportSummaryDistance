Proceedings of NAACL-HLT 2013, pages 958?968,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsGrouping Language Model Boundary Words to Speed K?Best Extractionfrom HypergraphsKenneth Heafield?,?
Philipp Koehn?
Alon Lavie??
School of InformaticsUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9AB, UKpkoehn@inf.ed.ac.uk?
Language Technologies InstituteCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213, USA{heafield,alavie}@cs.cmu.eduAbstractWe propose a new algorithm to approximatelyextract top-scoring hypotheses from a hyper-graph when the score includes an N?gramlanguage model.
In the popular cube prun-ing algorithm, every hypothesis is annotatedwith boundary words and permitted to recom-bine only if all boundary words are equal.However, many hypotheses share some, butnot all, boundary words.
We use these com-mon boundary words to group hypotheses anddo so recursively, resulting in a tree of hy-potheses.
This tree forms the basis for ournew search algorithm that iteratively refinesgroups of boundary words on demand.
Ma-chine translation experiments show our algo-rithm makes translation 1.50 to 3.51 times asfast as with cube pruning in common cases.1 IntroductionThis work presents a new algorithm to search apacked data structure for high-scoring hypothe-ses when the score includes an N?gram languagemodel.
Many natural language processing systemshave this sort of problem e.g.
hypergraph searchin hierarchical and syntactic machine translation(Mi et al 2008; Klein and Manning, 2001), lat-tice rescoring in speech recognition, and confusionnetwork decoding in optical character recognition(Tong and Evans, 1996).
Large language modelshave been shown to improve quality, especially inmachine translation (Brants et al 2007; Koehn andHaddow, 2012).
However, language models makesearch computationally expensive because they ex-amine surface words without regard to the structureat North Koreain North Koreawith North Koreawith the DPRKat??
?North Koreainwith{the DPRKFigure 1: Hypotheses are grouped by common prefixesand suffixes.of the packed search space.
Prior work, includingcube pruning (Chiang, 2007), has largely treated thelanguage model as a black box.
Our new searchalgorithm groups hypotheses by common prefixesand suffixes, exploiting the tendency of the languagemodel to score these hypotheses similarly.
An exam-ple is shown in Figure 1.
The result is a substantialimprovement over the time-accuracy trade-off pre-sented by cube pruning.The search spaces mentioned in the previous para-graph are special cases of a directed acyclic hyper-graph.
As used here, the difference from a nor-mal graph is that an edge can go from one vertexto any number of vertices; this number is the arityof the edge.
Lattices and confusion networks arehypergraphs in which every edge happens to havearity one.
We experiment with parsing-based ma-chine translation, where edges represent grammarrules that may have any number of non-terminals,including zero.Hypotheses are paths in the hypergraph scored bya linear combination of features.
Many features areadditive: they can be expressed as weights on edgesthat sum to form hypothesis features.
However, logprobability from anN?gram language model is non-958additive because it examines surface strings acrossedge and vertex boundaries.
Non-additivity makessearch difficult because locally optimal hypothesesmay not be globally optimal.In order to properly compute the language modelscore, each hypothesis is annotated with its bound-ary words, collectively referred to as its state (Liand Khudanpur, 2008).
Hypotheses with equal statemay be recombined, so a straightforward dynamicprogramming approach (Bar-Hillel et al 1964) sim-ply treats state as an additional dimension in the dy-namic programming table.
However, this approachquickly becomes intractable for large language mod-els where the number of states is too large.Beam search (Chiang, 2005; Lowerre, 1976) ap-proximates the straightforward algorithm by remem-bering a beam of up to k hypotheses1 in each vertex.It visits each vertex in bottom-up order, each timecalling a beam filling algorithm to select k hypothe-ses.
The parameter k is a time-accuracy trade-off:larger k increases both CPU time and accuracy.We contribute a new beam filling algorithm thatimproves the time-accuracy trade-off over the popu-lar cube pruning algorithm (Chiang, 2007) discussedin ?2.3.
The algorithm is based on the observationthat competing hypotheses come from the same im-put, so their language model states are often similar.Grouping hypotheses by these similar words enablesour algorithm to reason over multiple hypotheses atonce.
The algorithm is fully described in ?3.2 Related Work2.1 Alternatives to Bottom-Up SearchBeam search visits each vertex in the hypergraphin bottom-up (topological) order.
The hypergraphcan also be searched in left-to-right order (Watanabeet al 2006; Huang and Mi, 2010).
Alternatively,hypotheses can be generated on demand with cubegrowing (Huang and Chiang, 2007), though we notethat it showed little improvement in Moses (Xu andKoehn, 2012).
All of these options are compatiblewith our algorithm.
However, we only experimentwith bottom-up beam search.1We use K to denote the number of fully-formed hypothesesrequested by the user and k to denote beam size.2.2 Exhaustive Beam FillingOriginally, beam search was used with an exhaustivebeam filling algorithm (Chiang, 2005).
It generatesevery possible hypothesis (subject to the beams inprevious vertices), selects the top k by score, anddiscards the remaining hypotheses.
This is expen-sive: just one edge of arity a encodes O(1 + ak)hypotheses and each edge is evaluated exhaustively.In the worst case, our algorithm is exhaustive andgenerates the same number of hypotheses as beamsearch; in practice, we are concerned with the aver-age case.2.3 Baseline: Cube PruningCube pruning (Chiang, 2007) is a fast approximatebeam filling algorithm and our baseline.
It choosesk hypotheses by popping them off the top of a prior-ity queue.
Initially, the queue is populated with hy-potheses made from the best (highest-scoring) parts.These parts are an edge and a hypothesis from eachvertex referenced by the edge.
When a hypothesisis popped, several next-best alternatives are pushed.These alternatives substitute the next-best edge or anext-best hypothesis from one of the vertices.Our work follows a similar pattern of popping onequeue entry then pushing multiple entries.
However,our queue entries are a group of hypotheses whilecube pruning?s entries are a single hypothesis.Hypotheses are usually fully scored before beingplaced in the priority queue.
An alternative priori-tizes hypotheses by their additive score.
The addi-tive score is the edge?s score plus the score of eachcomponent hypothesis, ignoring the non-additive as-pect of the language model.
When the additive scoreis used, the language model is only called k times,once for each hypothesis popped from the queue.Cube pruning can produce duplicate queue en-tries.
Gesmundo and Henderson (2010) modified thealgorithm prevent duplicates instead of using a hashtable.
We include their work in the experiments.Hopkins and Langmead (2009) characterizedcube pruning as A* search (Hart et al 1968) with aninadmissible heuristic.
Their analysis showed deepand unbalanced search trees.
Our work can be inter-preted as a partial rebalancing of these search trees.9592.4 Exact AlgorithmsA number of exact search algorithms have been de-veloped.
We are not aware of an exact algorithm thattractably scales to the size of hypergraphs and lan-guage models used in many modern machine trans-lation systems (Callison-Burch et al 2012).The hypergraph and language model can be com-piled into an integer linear program.
The best hy-pothesis can then be recovered by taking the dualand solving by Lagrangian relaxation (Rush andCollins, 2011).
However, that work only dealt withlanguage models up to order three.Iglesias et al(2011) represent the search spaceas a recursive transition network and the languagemodel as a weighted finite state transducer.
Usingstandard finite state algorithms, they intersect thetwo automatons then exactly search for the highest-scoring paths.
However, the intersected automatonis too large.
The authors suggested removing lowprobability entries from the language model, but thisform of pruning negatively impacts translation qual-ity (Moore and Quirk, 2009; Chelba et al 2010).Their work bears some similarity to our algorithmin that partially overlapping state will be collapsedand efficiently handled together.
However, the keyadvatage to our approach is that groups have a scorethat can be used for pruning before the group is ex-panded, enabling pruning without first constructingthe intersected automaton.2.5 Coarse-to-FineCoarse-to-fine (Petrov et al 2008) performs mul-tiple pruning passes, each time with more detail.Search is a subroutine of coarse-to-fine and our workis inside search, so the two are compatible.
There areseveral forms of coarse-to-fine search; the closest toour work increases the language model order eachiteration.
However, by operating inside search, ouralgorithm is able to handle hypotheses at differentlevels of refinement and use scores to choose whereto further refine hypotheses.
Coarse-to-fine decod-ing cannot do this because it determines the level ofrefinement before calling search.3 Our New Beam Filling AlgorithmIn our algorithm, the primary idea is to group hy-potheses with similar language model state.
Thefollowing sections formalize what these groups are(partial state), that the groups have a recursive struc-ture (state tree), how groups are split (bread crumbs),using groups with hypergraph edges (partial edge),prioritizing search (scoring) and best-first search(priority queue).3.1 Partial StateAn N?gram language model (with order N ) com-putes the probability of a word given the N ?
1 pre-ceding words.
The left state of a hypothesis is thefirst N ?
1 words, which have insufficient contextto be scored.
Right state is the last N ?
1 words;these might become context for another hypothesis.Collectively, they are known as state.
State mini-mization (Li and Khudanpur, 2008) may reduce thesize of state due to backoff in the language model.For example, the hypothesis ?the few nations thathave diplomatic relations with North Korea?
mighthave left state ?the few?
and right state ?Korea?after state minimization determined that ?North?could be elided.
Collectively, the state is denoted(the few a  ` Korea).
The diamond  is a stand-infor elided words.
Terminators a and ` indicate whenleft and right state are exhausted, respectively2.Our algorithm is based on partial state.
Par-tial state is simply state with more inner wordselided.
For example, (the  Korea) is a partial statefor (the few a  ` Korea).
Terminators a and ` canbe elided just like words.
Empty state is denotedusing the customary symbol for empty string, .
Forexample, (  ) is the empty partial state.
The termi-nators serve to distinguish a completed state (whichmay be short due to state minimization) from an in-complete partial state.3.2 State TreeStates (the few a  ` Korea) and (the a  ` Korea)have words in common, so the partial state(the  Korea) can be used to reason over both ofthem.
Generalizing this notion to the set of hypothe-ses in a beam, we build a state tree.
The root ofthe tree is the empty partial state (  ) that reasons2A corner case arises for hypotheses with less than N ?
1words.
For these hypotheses, we still attempt state minimiza-tion and, if successful, the state is treated normally.
If stateminimization fails, a flag is set in the state.
For purposes of thestate tree, the flag acts like a different terminator symbol.960(  )(a  ) (a  Korea) (a a  Korea)(a a  ` Korea)(a a  in Korea) (a a  ` in Korea)(some  ) (some  DPRK) (some a  DPRK) (some a  ` DPRK)(the  ) (the  Korea)(the a  Korea) (the a  ` Korea)(the few  Korea) (the few  ` Korea) (the few a  ` Korea)Figure 2: A state tree containing five states: (the few a  ` Korea), (the a  ` Korea), (some a  ` DPRK),(a a  ` in Korea), and (a a  ` Korea).
Nodes of the tree are partial states.
The branching order is the first word,the last word, the second word, and so on.
If the left or right state is exhausted, then branching continues with theremaining state.
For purposes of branching, termination symbols a and ` act like normal words.
(  )(a a  Korea)(a a  ` Korea)(a a  ` in Korea)(some a  ` DPRK)(the  Korea)(the a  ` Korea)(the few a  ` Korea)Figure 3: The optimized version of Figure 2.
Nodesimmediately reveal the longest shared prefix and suffixamong hypotheses below them.over all hypotheses.
From the root, the tree branchesby the first word of state, the last word, the secondword, the second-to-last word, and so on.
If left orright state is exhausted, then branching continues us-ing the remaining state.
The branching order priori-tizes the outermost words because these can be usedto update the language model probability.
The deci-sion to start with left state is arbitrary.
An exampletree is shown in Figure 2.As an optimization, each node determines thelongest shared prefix and suffix of the hypothesesbelow it.
The node reports these words immedi-ately, rendering some other nodes redundant.
Thismakes our algorithm faster because it will then onlyencounter nodes when there is a branching decisionto be made.
The original tree is shown in Figure 2and the optimized version is shown in Figure 3.
Asa side effect of branching by left state first, the al-gorithm did not notice that states (the  Korea) and(  )[1+](a a  Korea)(a a  ` Korea)(a a  ` in Korea)(some a  ` DPRK)(the  Korea)(the a  ` Korea)(the few a  ` Korea)(the  Korea)[0+](the a  ` Korea)(the few a  ` Korea)Figure 4: Visiting the root node partitions the tree intobest child (the  Korea)[0+] and bread crumb (  )[1+].The data structure remains intact for use elsewhere.
(a a  Korea) both end with Korea.
We designed thetree building algorithm for speed and plan to exper-iment with alternatives as future work.The state tree is built lazily.
A node initially holdsa flat array of all the hypotheses below it.
When itschildren are first needed, the hypotheses are groupedby the branching word and an array of child nodesis built.
In turn, these newly created children eachinitially hold an array of hypotheses.
CPU time issaved because nodes containing low-scoring nodesmay never construct their children.Each node has a score.
For leaves, this score iscopied from the underlying hypothesis (or best hy-pothesis if some other feature prevented recombina-tion).
The score of an internal node is the maximumscore of its children.
As an example, the root node?sscore is the same as the highest-scoring hypothesisin the tree.
Children are sorted by score.9613.3 Bread CrumbsThe state tree is explored in a best-first manner.Specifically, when the algorithm visits a node, itconsiders that node?s best child.
The best child re-veals more words, so the score may go up or downwhen the language model is consulted.
Therefore,simply following best children may lead to a poorhypothesis.
Some backtracking mechanism is re-quired, for which we use bread crumbs.
Visiting anode results in two items: the best child and a breadcrumb.
The bread crumb encodes the node that wasvisited and how many children have already beenconsidered.
Figure 4 shows an example.More formally, each node has an array of chil-dren sorted by score, so it suffices for the breadcrumb to keep an index in this array.
An in-dex of zero denotes that no child has been vis-ited.
Continuing the example from Figure 3,(  )[0+] denotes the root partial state with chil-dren starting at index 0 (i.e.
all of them).
Visit-ing (  )[0+] yields best child (the  Korea)[0+]and bread crumb (  )[1+].
Later, the search al-gorithm may return to (  )[1+], yielding bestchild (some a  ` DPRK)[0+] and bread crumb(  )[2+].
If there is no remaining sibling, visit-ing yields only the best child.The index serves to restrict the array of childrento those with that index or above.
Formally, let dmap from a node or bread crumb to the set of leavesdescended from it.
The descendants of a node n arethose of its childrend(n) =|n|?1?i=0d(n[i])where unionsq takes the union of disjoint sets and n[i] isthe ith child.
In a bread crumb with index c, only de-scendents by the remaining children are consideredd(n[c+]) =|n|?1?i=cd(n[i])It follows that the set of descendants is partitionedinto two disjoint setsd(n[c+]) = d(n[c])?d(n[c+ 1+])3.4 Partial EdgeThe beam filling algorithm is tasked with selectinghypotheses given a number of hypergraph edges.Hypergraph edges are strings comprised of wordsand references to vertices (in parsing, terminals andnon-terminals).
A hypergraph edge is converted to apartial edge by replacing each vertex reference withthe root node from that vertex.
For example, the hy-pergraph edge ?is v .?
referencing vertex v becomespartial edge ?is (  )[0+] .
?Partial edges allow our algorithm to reason overa large set of hypotheses at once.
Visiting apartial edge divides that set into two as follows.A heuristic chooses one of the non-leaf nodes tovisit.
Currently, this heuristic picks the node withthe fewest words revealed.
As a tie breaker, itchooses the leftmost node.
The chosen node isvisited (partitioned), yielding the best child andbread crumb as described in the previous section.These are substituted into separate copies of the par-tial edge.
Continuing our example with the vertexshown in Figure 3, ?is (  )[0+] .?
partitions into?is (the  Korea)[0+] .?
and ?is (  )[1+] .
?3.5 ScoringEvery partial edge has a score that determines itssearch priority.
Initially, this score is the sum of theedge?s score and the scores of each bread crumb (de-fined below).
As words are revealed, the score isupdated to account for new language model context.Each edge score includes a log language modelprobability and possibly additive features.
When-ever there is insufficient context to compute the lan-guage model probability of a word, an estimate r isused.
For example, edge ?is v .?
incorporates esti-matelog r(is)r(.
)into its score.
The same applies to hypotheses:(the few a  ` Korea) includes estimatelog r(the)r(few | the)because the words in left state are those with insuf-ficient context.In common practice (Chiang, 2007; Hoang et al2009; Dyer et al 2010), the estimate is taken fromthe language model: r = p. However, queryingthe language model with incomplete context leads962Kneser-Ney smoothing (Kneser and Ney, 1995) toassume that backoff has occurred.
An alternative isto use average-case rest costs explicitly stored in thelanguage model (Heafield et al 2012).
Both optionsare used in the experiments3.The score of a bread crumb is the maximum scoreof its descendants as defined in ?3.3.
For example,the bread crumb (  )[1+] has a lower score than(  )[0+] because the best child (the  Korea)[0+]and its descendants no longer contribute to the max-imum.The score of partial edge ?is (  )[0+] .?
isthe sum of scores from its two parts: edge?is v .?
and bread crumb (  )[0+].
Theedge?s score includes estimated log probabilitylog r(is)r(.)
as explained earlier.
The bread crumb?sscore comes from its highest-scoring descendent(the few a  ` Korea) and therefore includes esti-mate log r(the)r(few | the).Estimates are updated as words are revealed.Continuing the example, ?is (  )[0+] .?
has bestchild ?is (the  Korea)[0+] .?
In this best child, theestimate r(.)
is updated to r(.
| Korea).
Similarly,r(the) is replaced with r(the | is).
Updates exam-ine only words that have been revealed: r(few | the)remains unrevised.Updates are computed efficiently by using point-ers (Heafield et al 2011) with KenLM.
To summa-rize, the language model computesr(wn|wn?11 )r(wn|wn?1i )in a single call.
In the popular reverse trie data struc-ture, the language model visits wni while retrievingwn1 , so the cost is the same as a single query.
More-over, when the language model earlier provided es-timate r(wn|wn?1i ), it also returned a data-structurepointer t(wni ).
Pointers are retained in hypotheses,edges, and partial edges for each word with an esti-mated probability.
When context is revealed, our al-gorithm queries the language model with new con-text wi?11 and pointer t(wni ).
The language modeluses this pointer to immediately retrieve denomina-tor r(wn|wn?1i ) and as a starting point to retrieve nu-merator r(wn|wn?11 ).
It can therefore avoid looking3We also tested upper bounds (Huang et al 2012; Carter etal., 2012) but the result is still approximate due to beam pruningand initial experiments showed degraded performance.up r(wn), r(wn|wn?1), .
.
.
, r(wn|wn?1i+1 ) as wouldnormally be required with a reverse trie.3.6 Priority QueueOur beam filling algorithm is controlled by a priorityqueue containing partial edges.
The queue is popu-lated by converting all outgoing hypergraph edgesinto partial edges and pushing them onto the queue.After this initialization, the algorithm loops.
Eachiteration begins by popping the top-scoring partialedge off the queue.
If all nodes are leaves, then thepartial edge is converted to a hypothesis and placedin the beam.
Otherwise, the partial edge is parti-tioned as described in ?3.3.
The two resulting partialedges are pushed onto the queue.
Looping continueswith the next iteration until the queue is empty or thebeam is full.
After the loop terminates, the beam isgiven to the root node of the state tree; other nodeswill be built lazily as described in ?3.2.Overall, the algorithm visits hypergraph verticesin bottom-up order.
Our beam filling algorithm runsin each vertex, making use of state trees in verticesbelow.
The top of the tree contains full hypotheses.If a K-best list is desired, packing and extractionworks the same way as with cube pruning.4 ExperimentsPerformance is measured by translating the 3003-sentence German-English test set from the 2011Workshop on Machine Translation (Callison-Burchet al 2011).
Two translation models were built, onehierarchical (Chiang, 2007) and one with target syn-tax.
The target-syntax system is based on Englishparses from the Collins (1999) parser.
Both weretrained on Europarl (Koehn, 2005).
The languagemodel interpolates models built on Europarl, newscommentary, and news data provided by the evalua-tion.
Interpolation weights were tuned on the 2010test set.
Language models were built with SRILM(Stolcke, 2002), modified Kneser-Ney smoothing(Kneser and Ney, 1995; Chen and Goodman, 1998),default pruning, and order 5.
Feature weights weretuned with MERT (Och, 2003), beam size 1000,100-best output, and cube pruning.
Systems werebuilt with the Moses (Hoang et al 2009) pipeline.Measurements were collected by running the de-coder on all 3003 sentences.
For consistency, all963-101.6-101.5-101.40 1 2AveragemodelscoreCPU seconds/sentenceThis workAdditive cube pruningCube pruningFigure 5: Hierarchial system in Moses with our algo-rithm, cube pruning with additive scores, and cube prun-ing with full scores (?2.3).
The two baselines overlap.relevant files were forced into the operating systemdisk cache before each run.
CPU time is the to-tal user and system time taken by the decoder mi-nus loading time.
Loading time was measured byrunning the decoder with empty input.
In partic-ular, CPU time includes the cost of parsing.
Ourtest system has 32 cores and 64 GB of RAM; norun came close to running out of memory.
Whilemulti-threaded experiments showed improvementsas well, we only report single-threaded results to re-duce noise and to compare with cdec (Dyer et al2010).
Decoders were compiled with the optimiza-tion settings suggested in their documentation.Search accuracy is measured by average modelscore; higher is better.
Only relative comparisonsare meaningful because model scores have arbitraryscale and include constant factors.
Beam sizes startat 5 and rise until a time limit determined by runningthe slowest algorithm with beam size 1000.4.1 Comparison Inside MosesFigure 5 shows Moses performance with this workand with cube pruning.
These results used the hi-erarchical system with common-practice estimates(?3.5).
The two cube pruning variants are explainedin ?2.3.
Briefly, the queue can be prioritized using-101.6-101.5-101.40 1 2AveragemodelscoreCPU seconds/sentenceThis workGesmundo 1Gesmundo 2Cube pruningFigure 6: Hierarchical system in cdec with our algorithm,similarly-performing variants of cube pruning defined inGesmundo and Henderson (2010), and the default.additive or full scores.
Performance with additivescores is roughly the same as using full scores withhalf the beam size.Our algorithm is faster for every beam size tested.It is also more accurate than additive cube pruningwith the same beam size.
However, when comparedwith full scores cube pruning, it is less accurate forbeam sizes below 300.
This makes sense becauseour algorithm starts with additive estimates and iter-atively refines them by calling the language model.Moreover, when beams are small, there are fewerchances to group hypotheses.
With beams largerthan 300, our algorithm can group more hypotheses,overtaking both forms of cube pruning.Accuracy improvements can be interpreted asspeed improvements by asking how much time eachalgorithm takes to achieve a set level of accuracy.By this metric, our algorithm is 2.04 to 3.37 times asfast as both baselines.4.2 Comparison Inside cdecWe also implemented our algorithm in cdec (Dyeret al 2010).
Figure 6 compares with two enhancedversions of cube pruning (Gesmundo and Hender-son, 2010) and the cdec baseline.
The model scores964-101.6-101.5-101.40 1 2AveragemodelscoreCPU seconds/sentenceRest+This workThis workRest+Cube pruningCube pruning21.421.621.8220 1 2UncasedBLEUCPU seconds/sentenceRest+This workThis workRest+Cube pruningCube pruningFigure 7: Effect of rest costs on our algorithm and on cube pruning in Moses.
Noisy BLEU scores reflect model errors.are comparable with Moses4.Measuring at equal accuracy, our algorithmmakes cdec 1.56 to 2.24 times as fast as the bestbaseline.
At first, this seems to suggest that cdec isfaster.
In fact, the opposite is true: comparing Fig-ures 5 and 6 reveals that cdec has a higher parsingcost than Moses5, thereby biasing the speed ratio to-wards 1.
In subsequent experiments, we use Mosesbecause it more accurately reflects search costs.4.3 Average-Case Rest CostsPrevious experiments used the common-practiceprobability estimate described in ?3.5.
Figure 7shows the impact of average-case rest costs on ouralgorithm and on cube pruning in Moses.
We alsolooked at uncased BLEU (Papineni et al 2002)scores, finding that our algorithm attains near-peakBLEU in less time.
The relationship between modelscore and BLEU is noisy due to model errors.4The glue rule builds hypotheses left-to-right.
In Moses,glued hypotheses start with <s> and thus have empty left state.In cdec, sentence boundary tokens are normally added last, sointermediate hypotheses have spurious left state.
Running cdecwith the Moses glue rule led to improved time-accuracy perfor-mance.
The improved version is used in all results reported.
Weaccounted for constant-factor differences in feature definitioni.e.
whether <s> is part of the word count.5In-memory phrase tables were used with both decoders.The on-disk phrase table makes Moses slower than cdec.Average-case rest costs impact our algorithmmore than they impact cube pruning.
For small beamsizes, our algorithm becomes more accurate, mostlyeliminating the disadvantage reported in ?4.1.
Com-pared to the common-practice estimate with beamsize 1000, rest costs made our algorithm 1.62 timesas fast and cube pruning 1.22 times as fast.Table 1 compares our best result with the bestbaseline: our algorithm and cube pruning, both withrest costs inside Moses.
In this scenario, our algo-rithm is 2.59 to 3.51 times as fast as cube pruning.4.4 Target-SyntaxWe took the best baseline and best result from previ-ous experiments (Moses with rest costs) and ran thetarget-syntax system.
Results are shown in Figure8.
Parsing and search are far more expensive.
Forbeam size 5, our algorithm attains equivalent accu-racy 1.16 times as fast.
Above 5, our algorithm is1.50 to 2.00 times as fast as cube pruning.
More-over, our algorithm took less time with beam size6900 than cube pruning took with beam size 1000.A small bump in model score occurs around 15seconds.
This is due to translating ?durchzoge-nen?
as ?criss-crossed?
instead of passing it through,which incurs a severe penalty (-100).
The only rulecapable of doing so translates ?X durchzogenen?
as?criss-crossed PP?
; a direct translation rule was not965-105-104.8-104.6-104.4-104.20 10 20AveragemodelscoreCPU seconds/sentenceRest+This workRest+Cube pruning2121.221.40 10 20UncasedBLEUCPU seconds/sentenceRest+This workRest+Cube pruningFigure 8: Performance of Moses with the target-syntax system.extracted due to reordering.
An appropriate prepo-sitional phrase (PP) was pruned with smaller beamsizes because it is disfluent.4.5 MemoryPeak virtual memory usage was measured beforeeach process terminated.
Compared with cube prun-ing at a beam size of 1000, our algorithm uses 160MB more RAM in Moses and 298 MB less RAM incdec.
The differences are smaller with lower beamsizes and minor relative to 12-13 GB total size, mostof which is the phrase table and language model.Rest+This work Rest+Cube pruningk CPU Model BLEU CPU Model BLEU5 0.068 -1.698 21.59 0.243 -1.667 21.7510 0.076 -1.593 21.89 0.255 -1.592 21.9750 0.125 -1.463 22.07 0.353 -1.480 22.0475 0.157 -1.446 22.06 0.408 -1.462 22.05100 0.176 -1.436 22.03 0.496 -1.451 22.05500 0.589 -1.408 22.00 1.356 -1.415 22.00750 0.861 -1.405 21.96 1.937 -1.409 21.981000 1.099 -1.403 21.97 2.502 -1.407 21.98Table 1: Numerical results from the hierarchical systemfor select beam sizes k comparing our best result with thebest baseline, both in Moses with rest costs enabled.
Toconserve space, model scores are shown with 100 added.5 ConclusionWe have described a new search algorithm thatachieves equivalent accuracy 1.16 to 3.51 times asfast as cube pruning, including two implementationsand four variants.
The algorithm is based on group-ing similar language model feature states togetherand dynamically expanding these groups.
In do-ing so, it exploits the language model?s ability toestimate with incomplete information.
Our imple-mentation is available under the LGPL as a stand-alone from http://kheafield.com/code/and distributed with Moses and cdec.AcknowledgementsThis research work was supported in part by the Na-tional Science Foundation under grant IIS-0713402,by a NPRP grant (NPRP 09-1140-1-177) from theQatar National Research Fund (a member of theQatar Foundation), and by computing resources pro-vided by the NSF-sponsored XSEDE program undergrant TG-CCR110017.
The statements made hereinare solely the responsibility of the authors.
The re-search leading to these results has received fundingfrom the European Union Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreements287576 (CASMACAT), 287658 (EU BRIDGE),287688 (MateCat), and 288769 (ACCEPT).966ReferencesYehoshua Bar-Hillel, Micha Perles, and Eli Shamir.1964.
On Formal Properties of Simple Phrase Struc-ture Grammars.
Hebrew University Students?
Press.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational LanguageLearning, pages 858?867, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion, pages 22?64, Edinburgh, Scotland, July.
Associ-ation for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical machinetranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 10?51, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Simon Carter, Marc Dymetman, and GuillaumeBouchard.
2012.
Exact sampling and decoding inhigh-order hidden Markov models.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1125?1134, JejuIsland, Korea, July.Ciprian Chelba, Thorsten Brants, Will Neveitt, and PengXu.
2010.
Study on interaction between entropy prun-ing and Kneser-Ney smoothing.
In Proceedings of In-terspeech, pages 2242?2245.Stanley Chen and Joshua Goodman.
1998.
An empiricalstudy of smoothing techniques for language modeling.Technical Report TR-10-98, Harvard University, Au-gust.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics, pages 263?270, Ann Arbor, Michi-gan, June.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33:201?228, June.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec:A decoder, alignment, and learning framework forfinite-state and context-free translation models.
InProceedings of the ACL 2010 System Demonstrations,ACLDemos ?10, pages 7?12.Andrea Gesmundo and James Henderson.
2010.
Fastercube pruning.
In Proceedings of the InternationalWorkshop on Spoken Language Translation (IWSLT),pages 267?274.Peter Hart, Nils Nilsson, and Bertram Raphael.
1968.
Aformal basis for the heuristic determination of mini-mum cost paths.
IEEE Transactions on Systems Sci-ence and Cybernetics, 4(2):100?107, July.Kenneth Heafield, Hieu Hoang, Philipp Koehn, TetsuoKiso, and Marcello Federico.
2011.
Left languagemodel state for syntactic machine translation.
In Pro-ceedings of the International Workshop on SpokenLanguage Translation, San Francisco, CA, USA, De-cember.Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012.Language model rest costs and space-efficient storage.In Proceedings of the 2012 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning, Jeju Is-land, Korea.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.A unified framework for phrase-based, hierarchical,and syntax-based statistical machine translation.
InProceedings of the International Workshop on SpokenLanguage Translation, pages 152?159, Tokyo, Japan.Mark Hopkins and Greg Langmead.
2009.
Cube pruningas heuristic search.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 62?71, Singapore, August.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics, Prague, CzechRepublic.Liang Huang and Haitao Mi.
2010.
Efficient incrementaldecoding for tree-to-string translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 273?283, Cambridge,MA, October.Zhiheng Huang, Yi Chang, Bo Long, Jean-Francois Cre-spo, Anlei Dong, Sathiya Keerthi, and Su-Lin Wu.2012.
Iterative Viterbi A* algorithm for k-best se-quential decoding.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 1125?1134, Jeju Island, Korea,July.Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`de Gispert, and Michael Riley.
2011.
Hierarchicalphrase-based translation representations.
In Proceed-ings of the 2011 Conference on Empirical Methods in967Natural Language Processing, pages 1373?1383, Ed-inburgh, Scotland, UK, July.
Association for Compu-tational Linguistics.Dan Klein and Christopher D. Manning.
2001.
Parsingand hypergraphs.
In Proceedings of the Seventh Inter-national Workshop on Parsing Technologies, Beijing,China, October.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing, pages 181?184.Philipp Koehn and Barry Haddow.
2012.
Towardseffective use of training data in statistical machinetranslation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 317?321,Montre?al, Canada, June.
Association for Computa-tional Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of MTSummit.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
In Pro-ceedings of the Second ACL Workshop on Syntax andStructure in Statistical Translation (SSST-2), pages10?18, Columbus, Ohio, June.Bruce Lowerre.
1976.
The Harpy Speech RecognitionSystem.
Ph.D. thesis, Carnegie Mellon University.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08: HLT,pages 192?199, Columbus, Ohio, June.Robert C. Moore and Chris Quirk.
2009.
Less is more:Significance-based n-gram selection for smaller, betterlanguage models.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 746?755, August.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL ?03: Pro-ceedings of the 41st Annual Meeting on Associationfor Computational Linguistics, pages 160?167, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevalution of machine translation.
In Proceedings 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, PA, July.Slav Petrov, Aria Haghighi, and Dan Klein.
2008.Coarse-to-fine syntactic machine translation using lan-guage projections.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 108?116, Honolulu, HI, USA, October.Alexander Rush and Michael Collins.
2011.
Exactdecoding of syntactic translation models through la-grangian relaxation.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics, pages 72?82, Portland, Oregon, USA,June.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the Seventh Inter-national Conference on Spoken Language Processing,pages 901?904.Xiang Tong and David A. Evans.
1996.
A statisticalapproach to automatic OCR error correction in con-text.
In Proceedings of the Fourth Workshop on VeryLarge Corpora, pages 88?100, Copenhagen, Den-mark, April.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the ACL, pages 777?784, Sydney, Australia, July.Wenduan Xu and Philipp Koehn.
2012.
Extending hierodecoding in Moses with cube growing.
The PragueBulletin of Mathematical Linguistics, 98:133?142.968
