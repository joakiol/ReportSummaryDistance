The Syntax and Semantics of Prepositions inthe Task of Automatic Interpretation ofNominal Phrases and Compounds:A Cross-Linguistic StudyRoxana Girju?University of Illinois atUrbana-ChampaignIn this article we explore the syntactic and semantic properties of prepositions in the contextof the semantic interpretation of nominal phrases and compounds.
We investigate the problembased on cross-linguistic evidence from a set of six languages: English, Spanish, Italian, French,Portuguese, and Romanian.
The focus on English and Romance languages is well motivated.Most of the time, English nominal phrases and compounds translate into constructions of theform N P N in Romance languages, where the P (preposition) may vary in ways that correlatewith the semantics.
Thus, we present empirical observations on the distribution of nominalphrases and compounds and the distribution of their meanings on two different corpora, basedon two state-of-the-art classification tag sets: Lauer?s set of eight prepositions and our list of 22semantic relations.
A mapping between the two tag sets is also provided.
Furthermore, given atraining set of English nominal phrases and compounds along with their translations in the fiveRomance languages, our algorithm automatically learns classification rules and applies themto unseen test instances for semantic interpretation.
Experimental results are compared againsttwo state-of-the-art models reported in the literature.1.
IntroductionPrepositions are an important and frequently used category in both English and Ro-mance languages.
In a corpus study of one million English words, Fang (2000) showsthat one in ten words is a preposition.
Moreover, about 10% of the 175 most frequentwords in a corpus of 20 million Spanish words were found to be prepositions (Almelaet al 2005).
Studies on language acquisition (Romaine 1995; Celce-Murcia and Larsen-Freeman 1999) have shown that the acquisition and understanding of prepositions inlanguages such as English and Romance is a difficult task for native speakers, andeven more difficult for second language learners.
For example, together with articles,prepositions represent the primary source of grammatical errors for learners of Englishas a foreign language (Gocsik 2004).?
Linguistics and Computer Science Departments, University of Illinois at Urbana-Champaign, Urbana, IL61801.
E-mail: girju@illinois.edu.Submission received: 1 August 2006; revised submission received: 20 January 2008; accepted for publication:17 March 2008.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 2Although the complexity of preposition usage has been argued for and documentedby various scholars in linguistics, psycholinguistics, and computational linguistics,very few studies have been done on the function of prepositions in natural languageprocessing (NLP) applications.
The reason is that prepositions are probably the mostpolysemous category and thus, their linguistic realizations are difficult to predict andtheir cross-linguistic regularities difficult to identify (Saint-Dizier 2005a).In this article we investigate the role of prepositions in the task of automatic seman-tic interpretation of English nominal phrases and compounds.
The problem is simple todefine: Given a compositional noun phrase (the meaning of the phrase derives from themeaning of the constituents) constructed out of a pair of nouns, N1 N2, one representingthe head and the other the modifier, determine the semantic relationship between thetwo nouns.
For example, the noun?noun compound family estate encodes a POSSESSIONrelation, while the nominal phrase the faces of the children refers to PART-WHOLE.
Theproblem, although simple to state, is difficult for automatic semantic interpretation.The reason is that the meaning of these constructions is most of the time implicit (itcannot be easily recovered from morphological analysis).
Interpreting nominal phrasesand compounds correctly requires various types of information, from world knowledgeto lexico-syntactic and discourse information.This article focuses on nominal phrases of the type N P N and noun compounds(N N) and investigates the problem based on cross-linguistic evidence from a set of sixlanguages: English, Spanish, Italian, French, Portuguese, and Romanian.
The choice ofthese constructions is empirically motivated.
In a study of 6,200 (Europarl1) and 2,100(CLUVI2) English token nominal phrase and compound instances randomly chosenfrom two English?Romance parallel text collections of different genres, we show thatover 80% of their Romance noun phrase translations are encoded by N P N and N Nconstructions.
For instance, beer glass, an English compound of the form N1 N2, trans-lates into N2 P N1 instances in Romance: tarro de cerveza (?glass of beer?)
in Spanish,bicchiere da birra (?glass for beer?)
in Italian, verre a` bie`re (?glass at/to beer?)
in French, copode cerveja (?glass of beer?)
in Portuguese, and pahar de bere (?glass of beer?)
in Romanian.In this article, in addition to the sense translation (in italics), when relevant we alsoprovide the word-by-word gloss (in ?parentheses?).
Moreover, we use N1, N2 to denotethe two lexical nouns that encode a semantic relation (whereN1 is the syntactic modifierand N2 is the syntactic head), and Arg1, Arg2 to denote the semantic arguments of therelation encoded by the two nouns.
For example, beer glass encodes a PURPOSE relationwhere Arg1 (beer) is the purpose of Arg2 (?glass?
; thus ?glass (used) for beer?
).We argue here that the syntactic directionality given by the head-modifier relation(N1 N2 in noun compounds and N2 P N1 in nominal phrases) is not always the sameas the semantic directionality given by the semantic argument frame of the semanticrelation.
Otherwise said, N1 does not always map to Arg1 and N2 to Arg2 for any givenrelation.Languages choose different nominal phrases and compounds to encode relation-ships between nouns.
For example, English nominal phrases and compounds of the1 http://www.isi.edu/koehn/europarl/.This corpus contains over 20 million words in eleven official languages of the European Union coveringthe proceedings of the European Parliament from 1996 to 2001.2 CLUVI - Linguistic Corpus of the University of Vigo Parallel Corpus 2.1; http://sli.uvigo.es/CLUVI/.CLUVI is an open text repository of parallel corpora of contemporary oral and written texts in some ofthe Romance languages (such as Galician, French, Spanish, and Portuguese) and Basque parallel textcollections.186Girju The Syntax and Semantics of Prepositionsform N1 N2 (e.g., wood stove) and N2 P1 N1 (e.g., book on the table) usually translatein Romance languages as N2 P2 N1 (e.g., four a` bois in French ?
?stove at/to wood?,and livre sur la table ?
?book on the table?).
Romance languages have very few N Ncompounds and they are of limited semantic categories, such as TYPE (e.g., legge quadroin Italian ?
?law framework?
?
translates as framework law).
Besides the unproductiveN N and the productive N P N phrases, Romanian also uses another productive con-struction: the genitive-marked noun?noun compounds (e.g., frumuset?ea fetei ?
beauty-the girl-GEN ?
translated as the beauty of the girl).
Whereas English N N compoundsare right-headed (e.g., framework/Modifier law/Head), Romance compounds are left-headed (e.g., legge/Head quadro/Modifier).
Moreover, the Romance preposition used inthe translations of English nominal phrase instances of the type N P N is one that comesclosest to having overlapping semantic range as intended in the English instance, butmay not be the exact counterpart for the whole semantic range.
For example, Committeeon Culture translates as Comisio?n de la Cultura (Spanish) (?Committee of the Culture?
),Commission de la Culture (French) (?Committee of the Culture?
), Commissione per la Cul-tura (Italian) (?Committee for the Culture?
), Comissa?o para Cultura (Portuguese) (?Com-mittee for Culture?
), and Comitet pentru Cultura?
(Romanian) (?Committee for Culture?
).Even those Romance prepositions that are spelled ?de?
are pronounced differently indifferent Romance languages.Thus, the focus on nominal phrases and compounds in English and Romance lan-guages is also motivated linguistically.
The extension of this task to natural languagesother than English brings forth both new insights and new challenges.
The Romanceprepositions used in the translations of English nominal phrases and compounds, mayvary in ways that correlate with the semantics.
Thus, Romance language prepositionswill give us another source of evidence for disambiguating the semantic relations inEnglish nominal phrases and compounds.
We argue that, in languages with multiplesyntactic options such as English (N N and N P N) and Romanian (N N, genitive-marked N N, and N P N), the choice between such constructions in context is governedin part by semantic factors.
For example, the set of semantic relations that can beencoded by pairs of nouns such as tea?cup and sailor?suit varies with the syntacticconstruction used.
In English, while the noun?noun compounds tea cup and sailor suitencode only PURPOSE, the N P N constructions cup of tea and suit of the sailor encodeCONTENT-CONTAINER (a subtype of LOCATION) and MEASURE relations and POSSES-SION, respectively.
Similarly, in Romanian both tea cup and cup of tea translate only asthe N P N instance ceas?ca?
de ceai (?cup of tea?
), while sailor suit translates as costum demarinar (?suit of sailor?)
and the suit of the sailor as the genitive-marked N N costumulmarinarului (?suit-the sailor-GEN?).
Thus, we study the distribution of semantic relationsacross different nominal phrases and compounds in one language and across all sixlanguages, and analyze the resulting similarities and differences.
This distribution isevaluated over the two different corpora based on two state-of-the-art classification tagsets: Lauer?s set of eight prepositions (Lauer 1995) and our list of 22 semantic relations.A mapping between the two tag sets is also provided.In order to test their contribution to the task of semantic interpretation, preposi-tions and other linguistic clues are employed as features in a supervised, knowledge-intensive model.
Furthermore, given a training set of English nominal phrases andcompounds along with their translations in the five Romance languages, our algo-rithm automatically learns classification rules and applies them to unseen test instancesfor semantic interpretation.
As training and test data we used 3,124 Europarl and2,023 CLUVI token instances.
These instances were annotated with semantic relationsand analyzed for inter-annotator agreement.
The results are compared against two187Computational Linguistics Volume 35, Number 2state-of-the-art approaches: a supervised machine learning model, semantic scattering(Moldovan and Badulescu 2005), and a Web-based unsupervised model (Lapata andKeller 2005).
Moreover, we show that the Romanian linguistic features contribute moresubstantially to the overall performance than the features obtained for the other Ro-mance languages.
This is explained by the fact that the choice of the linguistic construc-tions (either genitive-marked N N or N P N) in Romanian is highly correlated with theirmeaning.The article is organized as follows.
Section 2 presents a summary of related work.In Section 3 we describe the general approach to the interpretation of nominal phrasesand compounds and list the syntactic and semantic interpretation categories usedalong with observations regarding their distribution in the two different cross-linguisticcorpora.
Sections 4 and 5 present a learning model and experimental results.
Section 6presents linguistic observations on the behavior of English and Romanian N N andN P N constructions.
Finally, in Section 7 we provide an error analysis and in Section 8we offer some discussion and conclusions.2.
Previous Work2.1 Noun Phrase Semantic InterpretationThe semantic interpretation of nominal phrases and compounds in particular and nounphrases (NPs) in general has been a long-term research topic in linguistics, computa-tional linguistics,3 and artificial intelligence.Noun?noun compounds in linguisticsEarly studies in linguistics (Lees 1963) classified noun?noun compounds on purelygrammatical criteria using a transformational approach, criteria which failed to accountfor the large variety of constraints needed to interpret these constructions.
Later on, Levi(1978) attempted to give a tight account of noun?noun interpretation, distinguishingtwo types of noun?noun compounds: (a) compounds interpreted as involving one ofnine predicates (CAUSE, HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT) (e.g., oniontears encodes CAUSE) and (b) those involving nominalizations, namely, compoundswhose heads are nouns derived from a verb, and whose modifiers are interpreted asarguments of the related verb (e.g., a music lover loves music).
Levi?s theory was castin terms of the more general theory of Generative Semantics.
In that theory it wasassumed that the interpretation of compounds was available because the examples werederived from underlying relative clauses that had the same meanings.
Thus, honey bee,expressing the relation MAKE, was taken to be derived from a headed relative a beethat makes honey.
Levi was committed to the view that a very limited set of predicatesconstituted all of the relations that could hold between nouns in simple noun?nouncompounds.
This reductionist approach has been criticized in studies of language useby psycholinguists (Gleitman and Gleitman 1970; Downing 1977) who claim that noun?noun compounds, which are frequent in languages like English, encode in principle an3 In the past few years at many workshops, tutorials, and competitions this research topic has receivedconsiderable interest from the computational linguistics community: the Workshops on MultiwordExpressions at ACL 2003, ACL 2004 and COLING/ACL 2006; the Computational Lexical SemanticsWorkshop at ACL 2004; the Tutorial on Knowledge Discovery from Text at ACL 2003; the Shared Task onSemantic Role Labeling at CONLL 2004 and 2005 and at SemEval 2007.188Girju The Syntax and Semantics of Prepositionsunbounded number of possible relations.
One such example is apple juice seat?
?a seatin front of which an apple juice [is] placed?
(Downing 1977, page 818)?which can onlybe interpreted in the current discourse context.In this article we tackle the problem using a unified framework.
Although weagree with Downing (1977) that pragmatics plays an important factor in noun?nouninterpretation, a large variety of noun?noun meanings can be captured with a well-chosen set of semantic relations.
Our proposed semantic classification set differs fromthat of Levi (1978) in the sense that it contains more homogenous categories.
Levi?scategories, instead, are more heterogeneous, including both prepositions and verbs,some of which are too general (e.g., the prepositions for, in and the verb to have), andthus, too ambiguous.
Moreover, in our approach to automatic semantic interpretationwe focus on both N N and N P N constructions and exploit a set of five Romancelanguages.Noun?noun compounds in computational linguisticsThe automatic interpretation of nominal phrases and compounds is a difficult taskfor both unsupervised and supervised approaches.
Currently, the best-performingnoun?noun interpretation methods in computational linguistics focus mostly on twoor three-word noun?noun compounds and rely either on ad hoc, domain-specific,hand-coded semantic taxonomies, or statistical models on large collections of unlabeleddata.
Recent results have shown that symbolic noun?noun compound interpretationsystems using machine learning techniques coupled with a large lexical hierarchyperform with very good accuracy, but they are most of the time tailored to a specificdomain (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002), or are generalpurpose (Turney 2006) but rely on semantic similarity metrics on WordNet (Fellbaum1998).
On the other hand, the majority of corpus statistics approaches to noun?nouncompound interpretation collect statistics on the occurrence frequency of the nounconstituents and use them in a probabilistic model (Lauer 1995).
The problem is thatmost noun?noun compounds are rare and thus, statistics on such infrequent instanceslead in general to unreliable estimates of probabilities.
More recently, Lapata and Keller(2005) showed that simple unsupervised models applied to the noun?noun compoundinterpretation task perform significantly better when the n-gram frequencies areobtained from the Web (55.71% accuracy4), rather than from a large standard corpus.Nakov and Hearst (2005) improve over Lapata and Keller?s method through the use ofsurface features and paraphrases only for the task of noun?noun compound bracketing(syntactic parsing of three-word noun compounds) without their interpretation.Other researchers (Pantel and Ravichandran 2004; Pantel and Pennacchiotti 2006;Pennacchiotti and Pantel 2006) use clustering techniques coupled with syntacticdependency features to identify IS-A relations in large text collections.
Kim and Baldwin(2005) propose a general-purpose method that computes the lexical similarity of unseennoun?noun compounds with those found in training.
More recently Kim and Baldwin(2006) developed an automatic method for interpreting noun?noun compounds basedon a set of 20 semantic relations.
The relations are detected based on a fixed set ofconstructions involving the constituent nouns and a set of seed verbs denoting thesemantic relation (e.g., to own denotes POSSESSION).
Then all noun?noun instances4 These results were obtained on AltaVista on a general and abstract set of eight prepositions (Lauer 1995)as semantic classification categories: of, for, with, in, on, at, about, and from.189Computational Linguistics Volume 35, Number 2in transitive sentential contexts (i.e., those sentences containing a transitive verb) aremapped onto the selected set of constructions based on lexical similarity over the verbs.However, although the Web-based solution might overcome the data sparsity prob-lem, current probabilistic models are limited because they do not take full advantage ofthe structure and the meaning of language.From a cross-linguistic perspective, there hasn?t been much work on the automaticinterpretation of nominal phrases and compounds.
Busa and Johnston (1996), Johnstonand Busa (1996), and Calzolari et al (2002), for example, focus on the differencesbetween English and Italian noun?noun compounds.
In their work they argue that acomputational approach to the cross-linguistic interpretation of these compounds has torely on a rich lexical representation model, such as those provided by FrameNet frames(Baker, Fillmore, and Lowe 1998) and qualia structure (Pustejovsky 1995).
In the qualiastructure representation, for example, the meaning of a lexical concept, such as themodifier in a noun?noun compound, is defined in terms of four elements representingconcept attributes along with their use and purpose.
Thus, qualia structure providesa relational structure that enables the compositional interpretation of the modifier inrelation to the head noun.
Two implementations of such representations are providedby the SIMPLE Project ontology (Lenci et al 2000) and the OMB ontology (Pustejovskyet al 2006).
The SIMPLE ontology, for example, is developed for 12 European languagesand defines entry words that are mapped onto high-level concepts in EuroWordNet(Vossen 1998), a version of WordNet developed for European languages.In this article, we use a supervised semantic interpretation model employing richlinguistic features generated from corpus evidence coupled with word sense disam-biguation and WordNet concept structure information.
The results obtained are com-pared against two state-of-the-art approaches: a supervised machine learning model,semantic scattering (Moldovan and Badulescu 2005), and a Web-based unsupervisedmodel (Lapata and Keller 2005).
In this research we do not consider extra cross-linguisticinformation, such as semantic classes of Romance nouns (those provided by IS-A re-lations; e.g., cat belongs to the class of animals) made available, for example, by theSIMPLE ontology.
However, such resources can be added at any time to further improvethe performance of noun?noun interpretation systems.2.2 Semantics of PrepositionsAlthough prepositions have been studied intensively in linguistics (Herskovits 1987;Zelinski-Wibbelt 1993; Linstromberg 1997; Tyler and Evans 2003; Evans and Chilton2009, among others), they have only recently started to receive more attention in thecomputational linguistics community.5 Moreover, the findings from these broad stud-ies have not yet been fully integrated into NLP applications.
For example, althoughinformation retrieval, and even question answering systems, would benefit from theincorporation of prepositions into their NLP techniques, they often discard them as stopwords.5 The first Workshop on the Syntax and Semantics of Prepositions, Toulouse, France, 2003; the secondACL-SIGSEM Workshop on The Linguistic Dimensions of Prepositions and their Use in ComputationalLinguistics Formalisms and Applications, Colchester, UK, 2005; the third ACL-SIGSEM Workshop onPrepositions, Trento, Italy, 2006.190Girju The Syntax and Semantics of PrepositionsPrepositions in linguisticsConsiderable effort has been allocated to the investigation of spatial prepositions mainlybased on a cognitive approach, not only in English (Herskovits 1987; Linstromberg 1997;Tyler and Evans 2003; Evans and Chilton 2009), but also in many of the Indo-Europeanlanguages (Casadei 1991; Vandeloise 1993; Cadiot 1997; Melis 2002; Luraghi 2003).
Thesestudies provide a detailed analysis of such prepositions trying to give a methodologi-cal motivated account for the range of their polysemy.
These works identify specialconstraints on various prepositional patterns, such as semantic restrictions on the nounphrases occurring as complements of the preposition.
For example, in prepositionalphrase constructions such as in NP, the head noun can be a container (in a cup), ageometrical area (in a region), a geo-political area (in Paris), an atmospheric condition(in the rain), and so on.
These selectional restrictions imposed by the preposition on thenoun phrases it combines with are presented in various formats from lists (Herskovits1987; Linstromberg 1997) to semantic networks of cluster senses (Tyler and Evans 2003).In this article we also focus on the polysemy of such prepositions, but we identify the se-lectional restrictions automatically based on a specialization procedure on the WordNetIS-A hierarchy.
However, unlike Herskovits, we do not consider pragmatic issues suchas relevance and tolerance.
These account for the difference that pragmatic motivationsand context dependency make to how expressions are understood.
Relevance has to dowith communicative goals and choice of means and is evident, for example, in instancessuch as cat on the mat which is still relevant even when only the paws and not thewhole cat are on the mat.
Tolerance occurs in situations in which a book, for example,is described as on the table even though a set of files are placed between it and thetable.The use of spatial prepositions can also trigger various inferences.
For example,the man at his desk (cf.
Herskovits 1987) implies, besides a LOCATION relation, that theman is using the desk, thus an INSTRUMENT relation.
Other inferences are more subtle,involving spatial reasoning about the actions that can be performed on the argumentsof the preposition.
One such instance is infant in a playpen (cf.
Tyler and Evans 2003),where the movement of the playpen involves the movement of the infant.
In order toidentify such inferences the automatic interpretation system has to rely on pragmaticknowledge.
In this research we do not deal with such inference issues, rather we identifythe meaning of N P N constructions based on the local context of the sentence.Prepositions in computational linguisticsIn order to incorporate prepositions into various resources and applications, it is neces-sary to perform first a systematic investigation of their syntax and semantics.
Variousresearchers (Dorr 1993; Litkowski and Hargraves 2005; Saint-Dizier 2005b; Lersundiand Aggire 2006) have already provided inventories of preposition senses in Englishand other languages.
Others have focused on the analysis of verb particles (Baldwin2006a, 2006b; Villavicencio 2006), the distributional similarity (Baldwin 2005) and thesemantics of prepositions (Kordoni 2005) in a multilingual context, and the meaningof prepositions in applications such as prepositional phrase attachment (O?Hara andWiebe 2003; Kordoni 2006; Volk 2006).Moreover, although there is a large amount of work in linguistics and computa-tional linguistics relating to contrastive analysis of prepositions (Busa and Johnston(1996); Johnston and Busa (1996); Jensen and Nilsson (2005); Kordoni (2005), inter alia),to our knowledge, there have not been any attempts to provide an investigation of theprepositions?
role in the task of automatic noun phrase interpretation in a large cross-linguistic English?Romance framework.191Computational Linguistics Volume 35, Number 23.
Linguistic Considerations of Nominal Phrases and CompoundsThe meaning of nominal phrases and compounds can be compositional (e.g., spoonhandle?PART?WHOLE, kiss in the morning?TEMPORAL), or idiosyncratic, when themeaning is a matter of convention (e.g., soap opera, sea lion).
These constructions canalso encode metaphorical names (e.g., ladyfinger), proper names (e.g., John Doe), anddvandva compounds6 in which neither noun is the head (e.g., player?coach).Moreover, they can also be classified into synthetic (verbal, e.g., truck driver) androot (non-verbal, e.g., tea cup) constructions.7 It is widely held (Levi 1978; Selkirk 1982b)that the modified noun of a synthetic noun?noun compound, for example, may beassociated with a theta-role of the compound?s head noun, which is derived from averb.
For instance, in truck driver, the noun truck satisfies the THEME relation associatedwith the direct object in the corresponding argument structure of the verb to drive.In this article we address English?Romance compositional nominal phrases andcompounds of the type N N (noun?noun compounds which can be either genitive-marked or not genitive-marked) and N P N, and disregard metaphorical names, propernames, and dvandva structures.
In the following we present two state-of-the-art se-mantic classification sets used in automatic noun?noun interpretation and analyze theirdistribution in two different corpora.3.1 Lists of Semantic Classification RelationsAlthough researchers (Jespersen 1954; Downing 1977) argued that noun?noun com-pounds, and noun phrases in general, encode an infinite set of semantic relations,many agree (Levi 1978; Finin 1980) there is a limited number of relations that occurwith high frequency in these constructions.
However, the number and the level ofabstraction of these frequently used semantic categories are not agreed upon.
They canvary from a few prepositions (Lauer 1995) to hundreds and even thousands of morespecific semantic relations (Finin 1980).
The more abstract the category, the more nounphrases are covered, but also the larger the variation as to which category a phraseshould be assigned.
Lauer, for example, classifies the relation between the head and themodifier nouns in a noun?noun compound by making use of a set of eight frequentlyused prepositions: of, for, with, in, on, at, about, and from.
However, according to thisclassification, the noun?noun compound love story, for instance, can be classified bothas story of love and story about love.
The main problem with these abstract categoriesis that much of the meaning of individual compounds is lost, and sometimes there isno way to decide whether a form is derived from one category or another.
On the otherhand, lists of very specific semantic relations are difficult to build as they usually containa very large number of predicates, such as the list of all possible verbs that can link thenoun constituents.
Finin, for example, uses semantic categories such as dissolved in tobuild interpretations of compounds such as salt water and sugar water.In this article we experiment with two sets of semantic classification categoriesdefined at different levels of abstraction.
The first is a core set of 22 semantic relations(SRs), a set which was identified by us from the linguistics literature and from variousexperiments after many iterations over a period of time (Moldovan and Girju 2003).6 The term dvandva comes from Sanskrit, translates literally as ?two-and-two?
and means ?pair?.7 In the linguistic literature the words ?synthetic?
and ?root?
have been coined for noun?noun compounds.Because these terms apply also to nominal phrases, we use them in relation to these constructions as well.192Girju The Syntax and Semantics of PrepositionsMoldovan and Girju proved empirically that this set is encoded by noun?noun pairs innoun phrases; the set is a subset of their larger list of 35 semantic relations used in a largeset of semantics tasks.
This list, presented in Table 1 along with examples and semanticargument frames, is general enough to cover a large majority of text semantics whilekeeping the semantic relations to a manageable number.
A semantic argument frame isdefined for each semantic relation and indicates the position of each semantic argumentin the underlying relation.
For example, ?Arg2 is part of (whole)Arg1?
identifies the part(Arg2) and the whole (Arg1) entities in this relation.
This representation is importantbecause it allows us to distinguish between different arrangements of the argumentsfor given relation instances.
For example, most of the time, in N N compounds Arg1precedes Arg2, whereas in N P N constructions the position is reversed (Arg2 P Arg1).However, this is not always the case as shown by N N instances such as ham/Arg2sandwich/Arg1 and spoon/Arg1 handle/Arg2, both encoding PART?WHOLE.
More detailson subtypes of PART?WHOLE relations are presented in Section 6.2.
A special relationhere is KINSHIP, which is encoded only by N P N constructions and whose argumentorder is irrelevant.
Thus, the labeling of the semantic arguments for each relation asArg1 and Arg2 is just a matter of convention and they were introduced to provide aconsistent guide to the annotators to easily test the goodness-of-fit of the relations.
Theexamples in column 4 are presented with their WordNet senses identified in contextfrom the CLUVI and Europarl text collections, where the specific sense is representedas the sense number preceded by a ?#?
sign.The second set is Lauer?s list of eight prepositions (exemplified in Table 2) and canbe applied only to noun?noun compounds, because in N P N instances the prepositionis explicit.
We selected these two state-of-the-art sets as they are of different size andcontain semantic classification categories at different levels of abstraction.
Lauer?s list ismore abstract and thus capable of encoding a large number of noun?noun compoundinstances found in a corpus (e.g., many N1 N2 instances can be paraphrased as N2 ofTable 1The set of 22 semantic relations along with examples interpreted in context and the semanticargument frame.SemanticNo.
relations Default argument frame Examples1 POSSESSION Arg1 POSSESSES Arg2 family#2/Arg1 estate#2/Arg22 KINSHIP Arg1 IS IN KINSHIP REL.
WITH Arg2 the sister#1/Arg2 of the boy#1/Arg13 PROPERTY Arg2 IS PROPERTY OF Arg1 lubricant#1/Arg1 viscosity#1/Arg24 AGENT Arg1 IS AGENT OF Arg2 investigation#2/Arg2 of the police#1/Arg15 TEMPORAL Arg1 IS TEMPORAL LOCATION OF Arg2 morning#1/Arg1 news#3/Arg26 DEPICTION-DEPICTED Arg2 DEPICTS Arg1 a picture#1Arg2 of my nice#1/Arg17 PART-WHOLE Arg2 IS PART OF (whole) Arg1 faces#1/Arg2 of children#1/Arg18 HYPERNYMY (IS-A) Arg1 IS A Arg2 daisy#1/Arg1 flower#1/Arg29 CAUSE Arg1 CAUSES Arg2 scream#1/Arg2 of pain#1/Arg110 MAKE/PRODUCE Arg1 PRODUCES Arg2 chocolate#2/Arg2 factory#1/Arg111 INSTRUMENT Arg1 IS INSTRUMENT OF Arg2 laser#1/Arg1 treatment#1/Arg212 LOCATION Arg2 IS LOCATED IN Arg1 castle#1/Arg2 in the desert#1/Arg113 PURPOSE Arg1 IS PURPOSE OF Arg2 cough#1/Arg1 syrup#1/Arg214 SOURCE Arg1 IS SOURCE OF Arg2 grapefruit#2/Arg1 oil#3/Arg215 TOPIC Arg1 IS TOPIC OF Arg2 weather#1/Arg1 report#2/Arg216 MANNER Arg1 IS MANNER OF Arg2 performance#3/Arg2 with passion#1/Arg117 MEANS Arg1 IS MEANS OF Arg2 bus#1/Arg1 service#1/Arg218 EXPERIENCER Arg1 IS EXPERIENCER OF Arg2 the fear#1/Arg2 of the girl#1/Arg119 MEASURE Arg2 IS MEASURE OF Arg1 inches#1/Arg2 of snow#2/Arg120 TYPE Arg2 IS A TYPE OF Arg1 framework#1/Arg1 law#2/Arg221 THEME Arg1 IS THEME OF Arg2 acquisition#1/Arg2 of stock#1/Arg122 BENEFICIARY Arg1 IS BENEFICIARY OF Arg2 reward#1/Arg2 for the finder#1/Arg1OTHERS cry of death193Computational Linguistics Volume 35, Number 2Table 2Lauer?s set of prepositions along with examples interpreted in context.No.
Preposition Examples1 of sea bottom (bottom of the sea)2 for leisure boat (boat for leisure)3 with spoon feeding (feeding with a spoon)4 in London house (house in London)5 on Saturday snowstorm (snowstorm on Saturday)6 at night flight (flight at night)7 about war story (story about war)8 from almond butter (butter from almonds)N1), whereas our list contains finer grained semantic categories (e.g., only some N1 N2instances encode a CAUSE relation).In the next section, we present the coverage of these semantic lists on two differentcorpora, how well they solve the interpretation problem of noun phrases, and themapping from one list to another.3.2 Corpus AnalysisFor a better understanding of the semantic relations encoded by N N and N P Ninstances, we analyzed the semantic behavior of these constructions on two largecross-linguistic corpora of examples.
Our intention is to answer questions like:(1) What syntactic constructions are used to translate the English instances to the targetRomance languages and vice versa?
(cross-linguistic syntactic mapping)(2) What semantic relations do these constructions encode?
(cross-linguistic semanticmapping)(3)What is the corpus distribution of the semantic relations per each syntactic construction?
(4) What is the role of English and Romance prepositions in the semantic interpretation ofnominal phrases and compounds?For questions (1) and (2), we expand the work of Selkirk (1982b), Grimshaw(1990), Giorgi and Longobardi (1991), and Alexiadou, Haegeman, and Stavrou (2007)on the syntax of noun phrases in English and Romance languages by providingcross-linguistic empirical evidence for in-context instances on two different corporabased on the set of 22 semantic tags.
Following a configurational approach, Giorgi andLongobardi, for example, focus only on synthetic nominal phrases, such as the captureof the soldier (THEME), where the noun capture is derived through nominalizationfrom the verb to capture.
Besides synthetic constructions, we also consider root nominalphrases and compounds, such as family estate (POSSESSION).The dataIn order to perform empirical investigations of the semantics of nominal phrases andcompounds, and to train and test a learning model for the interpretation of noun?noun194Girju The Syntax and Semantics of Prepositionsinstances encoded by these constructions, we collected data from two text collectionswith different distributions and of different genres, Europarl and CLUVI.The Europarl data were assembled by combining four of the bilingual sentence-aligned corpora made public as part of the freely available Europarl corpus.
Specif-ically, the Spanish?English, Italian?English, French?English and Portuguese?Englishcorpora were automatically aligned based on exact matches of English translations.8Then, only those English sentences which appeared verbatim in all four language pairswere considered.
The resulting English corpus contained 10,000 sentences which weresyntactically parsed using Charniak?s parser (Charniak 2000).
From these we extracted6,200 token instances of N N (49.62%) and N P N (50.38%) constructions.CLUVI (Linguistic Corpus of the University of Vigo) is an open text repository ofparallel corpora of contemporary oral and written languages, a resource that besidesGalician also contains literary text collections in other Romance languages.
Because thecollection provides translations into only two of the Romance languages consideredhere, Spanish and Portuguese, we focused only on the English?Portuguese and English?Spanish literary parallel texts from the works of Agatha Christie, James Joyce, and H. G.Wells, among others.
Using the CLUVI search interfaces we created a sentence-alignedparallel corpus of 4,800 unique English?Portuguese?Spanish sentences.
The Englishversion was syntactically parsed using Charniak?s parser (Charniak 2000) after whicheach N N and N P N instance was manually mapped to the corresponding translations.The resulting corpus contains 2,310 English token instances with a distribution of25.97% N N and 74.03% N P N.Corpus annotation and inter-annotator agreementFor each corpus, each nominal phrase and compound instance was presented separatelyto two experienced annotators9 in a Web interface in context along with the Englishsentence and its translations.
Because the corpora do not cover some of the languages(Romanian in Europarl, and Romanian, Italian, and French in CLUVI), three othernative speakers of these languages who were fluent in English provided the translations,which were added to the list.
The two computational semantics annotators had to tageach English constituent noun with its corresponding WordNet sense.10 If the word wasnot found in WordNet the instance was not considered.
The annotators were also askedto identify the translation phrases, tag each instance with the corresponding semanticrelation, and identify the semantic arguments Arg1 and Arg2 in the semantic argumentframe of the corresponding relation.
Whenever the annotators found an example encod-ing a semantic relation or a preposition paraphrase other than those provided, or if theydid not know what interpretation to give, they had to tag it as OTHER-SR (e.g., melodyof the pearl: here the context of the sentence did not indicate the association between thetwo nouns; cry of death: the cry announcing death), and OTHER-PP (e.g., box by the wall,searches after knowledge) respectively.Tagging each noun constituent with the corresponding WordNet sense in context isimportant not only as a feature employed in the training models, but also as guidancefor the annotators to select the right semantic relation.
For instance, in the follow-ing sentences, daisy flower expresses a PART?WHOLE relation in Example (1) and an8 This version of the Europarl text collection does not include Romanian.9 The annotators have extensive expertise in computational semantics and are fluent in at least three of theRomance languages considered for this task.10 We used version 2.1 of WordNet.195Computational Linguistics Volume 35, Number 2IS-A relation in Example (2) depending on the sense of the noun flower (cf.
WordNet2.1: flower#2 is a ?reproductive organ of angiosperm plants especially one havingshowy or colorful parts,?
whereas flower#1 is ?a plant cultivated for its blooms orblossoms?).
(1) Usually, more than one daisy#1 flower#2 grows on top of a single stem.
(2) Try them with orange or yellow flowers of red-hot poker, solidago, or other latedaisy#1 flowers#1, such as rudbeckias and heliopsis.In cases where noun senses were not enough for relation selection, the annotatorshad to rely on a larger context provided by the sentence and its translations.Moreover, because the order of the semantic arguments in a nominal phrase ornoun?noun compound is not fixed (Girju et al 2005), the annotators were presentedwith the semantic argument frame for each of the 22 semantic relations and wereasked to tag the instances accordingly.
For example, in PART?WHOLE instances suchas chair/Arg1 arm/Arg2 the part arm follows the whole chair, whereas in spoon/Arg1handle/Arg2 the order is reversed.
In the annotation process the translators also usedthe five corresponding translations as additional information in selecting the semanticrelation.
For instance, the context provided by the Europarl English sentence in Exam-ple (3) does not give enough information for the disambiguation of the English nominalphrase judgment of the presidency, where the modifier noun presidency can be eitherAGENT or THEME in relation to the nominalized noun head judgment.
The annotatorshad to rely on the Romance translations in order to identify the correct meaning incontext (THEME): valoracio?n sobre la Presidencia (Sp.
?
Spanish), avis sur la pre?sidence(Fr.
?
French), giudizio sulla Presidenza (It.
?
Italian), veredicto sobre a Preside?ncia (Port.
?Portuguese), evaluarea Pres?endint?iei (Ro.
?
Romanian).Most of the time, one instance was tagged with one semantic relation, and onepreposition paraphrase (in case of noun?noun compounds), but there were also situa-tions in which an example could belong to more than one category in the same context.For example, Texas city is tagged as PART?WHOLE, but also as a LOCATION relation usingthe 22-SR classification set, and as of, from, in based on the 8-PP set (e.g., city of Texas,city from Texas, and city in Texas).
Overall, 8.2% CLUVI and 4.8% Europarl instanceswere tagged with more than one semantic relation, and almost half of the noun?nouncompound instances were tagged with more than one preposition.
(3) En.
: If you do , the final judgment of the Spanish presidency will be even morepositive than it has been so far.Sp.
: Si se hace, la valoracio?n sobre la Presidencia espan?ola del Consejosera?
au?n mucho ma?s positiva de lo que es hasta ahora.Fr.
: Si cela arrive, notre avis sur la pre?sidence espagnole du Conseil seraencore beaucoup plus positif que ce n?est de?ja` le cas.It.
: Se ci riuscira`, il nostro giudizio sulla Presidenza spagnola sara` ancorapiu` positivo di quanto non sia stato finora.Port.
: Se isso acontecer, o nosso veredicto sobre a Preside?ncia espanhola sera?ainda muito mais positivo do que o actual.Ro.
: Daca?
are loc, evaluarea Pres?edint?iei spaniole va fi ??nca?
mai positivadeca?t pa?na?
acum.196Girju The Syntax and Semantics of PrepositionsThus, the corpus instances used in the corpus analysis phase have the followingformat: ?NPEn; NPEs; NPIt; NPFr; NPPort; NPRo; target?.
The word target is one of the23 (22 + OTHER-SR) semantic relations and one of the eight prepositions considered fornoun compound instances, and one of the 23 semantic relations for N P N instances.
Forexample, ?development cooperation; cooperacio?n para el desarrollo; cooperazione allo sviluppo;coope?ration au de?veloppement; cooperac?a?o para o desenvolvimento; cooperare de dezvoltare;PURPOSE / FOR?.Inter-annotator agreement was measured using kappa, one of the most frequentlyused measures of inter-annotator agreement for classification tasks: K = Pr(A)?Pr(E)1?Pr(E) ,where Pr(A) is the proportion of times the annotators agree and Pr(E) is the probabilityof agreement by chance.
The K coefficient is 1 if there is a total agreement amongthe annotators, and 0 if there is no agreement other than that expected to occur bychance.The kappa values along with percentage agreements obtained on each corpus areshown in Table 3.
We also computed the number of instances that were tagged withOTHER by both annotators for each semantic relation and preposition paraphrase, overthe number of examples classified in that category by at least one of the judges.
Forthe instances that encoded more than one classification category, the agreement wasmeasured on the first relation on which the annotators agreed.The agreement obtained for the Europarl corpus is higher than that for CLUVIon both classification sets.
Overall, the K coefficient shows a fair to good level ofagreement for the corpus data on the set of 22 relations, with a higher agreement for thepreposition paraphrases.
However, according to Artstein (2007), kappa values can dropsignificantly if the frequency distribution of the annotation categories in the text corpusis skewed.
This is the case here, as will be shown in the next section.
Thus, for a betterunderstanding of the annotation results we also computed the percentage agreement,which is indicated for each classification set in parentheses in Table 3.7.8% of Europarl and 5.7% of CLUVI instances that could not be tagged withLauer?s prepositions were included in the OTHER-PP category.
From these, 2.1% and2.3%, respectively, could be paraphrased with prepositions other than those consideredby Lauer (e.g., bus service: service by bus), and 5.7% and 3.4%, respectively, could not beparaphrased with prepositions (e.g., daisy flower).In the next section we discuss the distribution of the syntactic and semantic inter-pretation categories on the two different cross-linguistic corpora.Table 3The inter-annotator agreement on the annotation of the nominal phrases and compounds in thetwo corpora.
For the instances that encoded more than one classification category, the agreementwas measured on the first relation on which the annotators agreed.
N/A = not applicable.Kappa Agreement(% agreement)Corpus Classification tag sets N N N P N OTHEREuroparl 8 PPs 0.80 (85.4%) N/A 91%22 SRs 0.61 (76.1%) 0.67 (80.8%) 78%CLUVI 8 PPs 0.77 (84.7%) N/A 86%22 SRs 0.56 (73.8%) 0.58 (75.1%) 69%197Computational Linguistics Volume 35, Number 23.3 Distribution of Syntactic Constructions and Semantic RelationsA.
Cross-linguistic distribution and mapping of nominal phrases and compoundsTable 4 shows the distribution of various syntactic constructions used for thetranslation of the 6,200 (3,076 N N and 3,124 N P N) Europarl and 2,310 (600 N Nand 1,710 N P N) CLUVI English token instances in each of the five target languagesconsidered.
The data show that N N and N P N constructions cover over 83% of thetranslation patterns for both text corpora.
However, whereas the distribution of bothconstructions is balanced in the Europarl corpus (about 45%, with the exception ofRomanian for which N P N constructions are less frequent), in CLUVI the N P Nconstructions occur in more than 85% of the cases (again, with the exception of Ro-manian where they represent about 56% of the data).
The high percentage obtained forN P N instances in CLUVI is explained by the fact that Romance languages have veryfew N N compounds which are of limited semantic types, such as TYPE.
Moreover, itis interesting to note here that some of the English instances are translated into bothnoun?noun (N N) and noun?adjective (N A) compounds in the target languages.
Forexample, love affair translates into either the N A construction enredo amoroso (Spanish),aventure amoureuse (French), relazione amorosa (Italian), relac?ao amorosa (Portuguese),and aventura?
amoroasa?
(Romanian), or using the more common N de N pattern aventurade amor (Spanish), aventure d?amour (French), storia d?amore (Italian), estoria de amor(Portuguese), and aventura?
de dragoste (Romanian).
There are also instances whichtranslate as one word in the target language, shown in Table 4, column 6.
For example,Table 4The distribution of syntactic constructions used in the translation of 6,200 Europarl and 2,310English NN and N P N instances.
N A = noun?adjective; pph = other syntactic paraphrase.Syntactic distributionCorpus Language N N N P N N A word pph TotalFrench 2,747 2,896 372 37 148(44.31%) (46.71%) (5.99%) (0.6%) (2.39%)Italian 2,896 2,413 520 111 260(46.71%) (38.92%) (8.38%) (1.8%) (4.19%)Europarl Spanish 2,896 2,487 483 36 298(46.71%) (40.12%) (7.79%) (0.58%) (4.80%) 6,200Portuguese 2,858 2,301 594 75 372(46.1%) (37.11%) (9.58%) (1.21%) (6%)Romanian 4,010 1,596 297 74 223(64.68%) (25.74%) (4.79%) (1.19%) (3.6%)French 32 1,967 94 154 63(1.39%) (85.15%) (4.07%) (6.66%) (2.73%)Italian 25 2,046 75 113 51(1.08%) (88.57%) (3.25%) (4.89%) (2.21%)CLUVI Spanish 25 1,959 107 163 56(1.08%) (84.81%) (4.63%) (7.06%) (2.42%) 2,310Portuguese 25 1,990 163 88 44(1.08%) (86.15%) (7.05%) (3.81%) (1.91%)Romanian 758 1,295 88 125 44(32.81%) (56.06%) (3.81%) (5.41%) (1.91%)198Girju The Syntax and Semantics of Prepositionsankle boot is translated into bottine in French and stivaletto in Italian.
The rest of the datais encoded by other syntactic paraphrases, as shown in Table 4, column 7.
For example,bomb site is translated into Italian as luogo dove e` esplosa la bomba (?the place wherethe bomb has exploded?).
Moreover, Table 5 shows the distribution of the prepositionspresent in the N P N translations.Table 5The distribution of N P N constructions used in the translation of the English noun phraseinstances on both text corpora.
The preposition a is used to denote a, ad, and de to denote simpleand articulated prepositions (de, di, du, de la, della, degli, d?, etc.
).Corpus Language N P N distribution TotalEnglish of (81.15%); for (3.27%); in (4.61%); on (2.43%); 3,124at (1.22%); from (0.67%); with (2.85%);by (1.5%); against (0.42%); through (0.29%);under (0.42%); after (0.38%); before (0.85%)French de (75.69%); a` (2.93%); pour (6.42%); par (1.42%); 2,896en (1.62%); avec (1.6%) ; devant (1.6%);apre`s (1.21%); dans (2.11%); sur (2.6%);contre (0.4%); avant (0.4%)Italian de (71.78%); a (7%); su (1.29%); a (3.11%); 2,413da (6.59%); per (6.22%); via (0.79%); in (0.79%);con (1.41%); contra (0.62%); davanti (0.2%);dopo (0.2%)Europarl Spanish de (83.39%); a (1.81%); en (1.41%); para (3.5%); 2,487por (2.61%); con (3.18%); sobre (3.3%);contra (0.4%); en materia de (0.4%)Portuguese de (78.4%); a (0.8%); em (0.8%); para (3.5%); 2,301por (1.6%); com (0.8%); sobre (1.3%);antes de (0.4%)Romanian de (82.2%); ?
?nainte de (1.82%); cu (1.82%); pentru (4.51%); 1,596despre (1.63%); la (0.38%); datorita?
(0.38%);pe (6.08%); pe calea (0.37%); ?
?n (0.81%)English of (83.80%); for (1.17%); in (5.90%); on (2.40%); 1,710at (0.76%); with (1.99%); against (1.17%);through (0.41%); over (0.41%); above (0.41%);beside (0.41%); about (0.41%); behind (0.76%)French de (82.33%); a` (6.2%); pour (1.42%); en (1.8%); 1,967sur (7.02%); contre (0.41%); pre`s de (0.41%);a` cote?
de (0.41%)Italian de (75.42%); a (8.07%); su (1.32%); 2,046da (6.6%); per (6.21%); in (0.78%); con (0.4%);contra (0.4%); sopra (0.2%); accanto a (0.2%);dietro de (0.2%); via (0.2%)CLUVI Spanish de (85.96%); a (2.81%); en (3.89%); para (0.71%); 1,959por (1.74%); con (2.1%); sobre (1.38%);contra (0.36%); detra?s (0.71%); encima (0.36%)Portuguese de (78.4%); a (0.8%); em (0.82%); para (3.5%); 1,990por (1.6%); com (0.8%); sobre (1.3%);acima de (0.4%)Romanian de (85.21%); cu (1.82%); pentru (4.5%); 1,295la (0.4%); datorita?
(0.4%); pe (5.08%);despre (1.58%); ?
?n (0.79%); la?nga ( 0.2%)199Computational Linguistics Volume 35, Number 2For the purposes of this research, from the 6,200 Europarl and 2,310 CLUVIinstances, we selected those which had all the translations encoded only by N N andN P N constructions.
Columns 3 and 4 in Table 4 show the number of N N and N P Ntranslation instances in each Romance language.
Out of these, we considered only3,124 Europarl and 2,023 CLUVI token instances representing the examples encodedby N N and N P N in all languages considered, after inter-annotator agreement.B.
Cross-linguistic distribution of semantic relations and their mapping to nominalphrases and compoundsA closer look at the N N and N P N translation instances in Table 4 shows thattheir syntactic distribution is influenced by the text genre and the semantics of theinstances.
For example, in Europarl most of the N N instances were naming noun?noun compounds referring to entities such as member states and framework law whichwere repeated in many sentences.
Many of them encoded TYPE relations (e.g., memberstate, framework law) which, most of the time, are encoded by N N patterns in the targetlanguages (stato membro and legge quadro in Italian, respectively).
In the CLUVI corpus,on the other hand, the N N Romance translations represented only 1% of the data.
Anotable exception here is Romanian (64.68% of Europarl and 32.8% of CLUVI).
This isexplained by the fact that, in Romanian, many noun phrases are represented as genitive-marked noun compounds (N1 N2).
In Romanian the genitive case is realized either asa suffix attached to the modifier noun N2 or as one of the genitival articles a/al/ale.
Ifthe modifier noun N2 is determined by an indefinite article then the genitive mark isapplied to the article, not to the noun, for example o fata?
?
unei fete (?a girl ?
of/to agirl?)
and un ba?iat ?
unui ba?iat (?a boy ?
of/to a boy?).
Similarly, if the modifier noun isdetermined by the definite article (which is enclitic in Romanian), the genitive mark isadded at the end of the noun together with the article.
For example, fata?fetei (the girl ?girl-GEN), cartea?ca?rt?ii (the book ?
book-GEN).
Thus, the noun phrase the beauty of thegirl, for instance, is translated as frumuset?ea fetei (?beauty-the girl-GEN?
), and the beauty ofa girl as frumuset?ea unei fete (?beauty-the of/to a girl?
).In general, in Romanian the choice between the N de N and the genitive-markedN N constructions depends on the specificity of the instance.
Some noun?noun instancesrefer to a specific entity (existential interpretation), in which case the constructionpreferred is the genitive-marked N N, or they can refer in general to the category ofthose entities (generic interpretation),11 thus using N de N. For example, the instancethe bite of the scorpion (AGENT) translates into mus?ca?tura scorpionului (?bite-the scorpion-GEN?
), whereas a scorpion bite (AGENT) translates into mus?ca?tura?
de scorpion (?bite ofscorpion?
).Many semantic relations that allow both the generic and the existential interpre-tations can be encoded by both N P N and genitive-marked N N constructions asshown by the example above.
However, there are situations when the generic andthe existential interpretations change the meaning of the noun?noun pair.
One suchexample is the suit of the sailor (POSSESSION) translated as costumul marinarului (?suit-the sailor-GEN?
), and sailor suit (PURPOSE) translated as costum de marinar (?suit ofsailor?
).11 The words existential and generic are borrowed here from the vast linguistic literature on definite andindefinite descriptions.
Here, nouns such as firemen can have different readings in various contexts:Firemen are available (existential reading), vs. Firemen are altruistic (generic reading).200Girju The Syntax and Semantics of PrepositionsAt the other extreme there are relations which prefer either the generic or theexistential interpretation.
For example, some POSSESSION-encoding instances such asthe budget of the University translate as ?bugetul Universita?t?ii?
(budget-the University-GEN)and not as ?bugetul de Universitate?
(budget-the of University).
Other relations such asPURPOSE and SOURCE identify generic instances.
For example, (a) olive oil (SOURCE)translates as ?ulei de ma?sline?
(oil of olive), and not as ?uleiul ma?slinei?
(oil-the olive-GEN), and (b) the milk glass (PURPOSE) translates as ?paharul de lapte?
(glass-the of milk)and not as ?paharul laptelui?
(glass-the milk-GEN).
Other examples include CAUSE andTOPIC.
This observation is very valuable for the interpretation of nominal phrases andcompounds and is used in the learning model to discriminate among the possibleinterpretations.Tables 6 and 7 show the semantic distribution of the instances on both text corpora.This distribution is represented both in number of tokens (the total number of instancesper relation) and types (the unique number of instances per relation).
In Europarl,the most frequently occurring relations are TYPE and THEME that together representabout 50% of the data with an equal distribution.
The next most frequent relationsare TOPIC, PURPOSE, AGENT, and PROPERTY with an average coverage of about 8%.Moreover, eight relations of the 22-SR set (KINSHIP, DEPICTION, CAUSE, INSTRUMENT,SOURCE, MANNER, MEASURE, and BENEFICIARY) did not occur in this corpus.
The9.61% of the OTHER-SR relation represents the ratio of those instances that did notencode any of the 22 semantic relations.
It is interesting to note here the large differencebetween the number of types versus tokens for the TYPE relation in Europarl.
This isaccounted for by various N N instances such as member states that repeat across thecorpus.This semantic distribution contrasts with the one in CLUVI.
Here, the most fre-quent relation by far is PART?WHOLE (40.53%), followed by LOCATION (8.95%), AGENT(6.23%), and IS-A (5.93%).
The missing relations are KINSHIP, MANNER and BENEFI-CIARY.
A larger percentage of OTHER-SR instances (12.95%) did not encode any of the22 semantic relations.
Moreover, in CLUVI 256 instances were tagged with more thanone semantic relation with the following distribution: 46.8% MEASURE/PART?WHOLE(e.g., a couple of cigarettes), 28.2% PART?WHOLE/LOCATION (e.g., bottom of the sea), 10.9%MEASURE/LOCATION (e.g., cup of chocolate), 8.2% PURPOSE/LOCATION (e.g., waste gar-den), and 5.9% THEME/MAKE-PRODUCE (e.g., makers of songs).
In Europarl, on the otherhand, there were only 97 such cases: 81.4% THEME/MAKE-PRODUCE (e.g., bus manufac-turers) and 18.6% MEASURE/PART?WHOLE (e.g., number of states).One way to study the contribution of both the English and Romance prepositionsto the interpretation task is to look at their distribution over the set of semantic relationson two reasonably large text corpora of different genres.
Of course, this approach doesnot provide an analysis that generates an exhaustive generalization over the propertiesof the language.
However, as Tables 6 and 7 show, there are dependencies between thestructure of the Romance language translations and the semantic relations encoded bythe nominal phrases and compounds, although the most frequently occurring preposi-tions are de and its English equivalent of.
Here we use the preposition de to represent aset of translation equivalents in Romance languages (e.g., the Italian counterpart is di).These prepositions are semantically underspecified, encoding a large set of semanticrelations.
The many-to-many mappings of the prepositions to the semantic classesadds to the complexity of the interpretation task.
For example, in the Europarl corpusLOCATION is encoded in French by de, sur, devant, and a` pre`s de, while TOPIC is encodedin English by of, for, on, about and noun compounds, and in Spanish by de, sobre, enmateria de.201Computational Linguistics Volume 35, Number 2Table 6Mapping between the set of 22 semantic classification categories and the set of English andRomance syntactic constructions on the Europarl corpus.
The preposition de is used here todenote simple and articulated prepositions (de, di, du, de la, della, degli, d?, etc.).
Also, the dash ??
?refers to noun?noun compounds where there is no connecting preposition.
The mapping wasobtained on the 3,124 Europarl instance corpus.
En.
= English; Sp.
= Spanish; It.
= Italian;Fr.
= French; Port.
= Portuguese; Ro.
= Romanian.TotalToken TypeNr.
SRs En.
Sp.
It.
Fr.
Port.
Ro.
[%] [%] Example1 POSSESSION of, ?
de, ?
de, ?
de de, ?
2.85 2.4 Union resources?resursele uniunii?
(Ro.
)(resource-the union-GEN)2 KINSHIP 0 03 PROPERTY of, for, de de de de de, ?
6.05 6.05 traffic densityin, ?
?densita` del traffico?
(It.
)(density of traffic)4 AGENT of, for, de de,?
de de de 7.47 7.08 request of a memberin, by, ?
?richiesta di uno membro?
(It.
)(request of a member)5 TEMPORAL of, in, de, con de de, de de, 0.04 0.04 year before the constitutionon, at, ?
a avant acima de ?
?nainte de ?an?o anterior a la constitucio?n?
(Sp.
)(the year previous of the)constitution6 DEPICTION 0 07 PART?WHOLE of, in, de, con de, a, de, a` de, de 3.20 2.75 Union citizenwith, ?
a, ?
con ?citoyen de l?
Union?
(Fr.
)(citizen of the Union)8 IS?A of, ?
de, ?
de, ?
de, ?
?
?
0.8 0.8 process of decay(HYPERNYMY) with ?proces de descompunere?
(Ro.
)(process of decay)9 CAUSE 0 010 MAKE/ of, for, ?
de de de de de 1.43 1.43 paper plantPRODUCE in, from ?fa?brica de papel?
(Sp.
)(plant of paper)11 INSTRUMENT 0 012 LOCATION of, in, de, en, de, su, de, sur, de de, pe, 2.14 2.14 place of the meetingon, ?
sobre a, in a`, pre`s de, la, ?
?n ?lieu de la re?union?
(Fr.
)at devant (place of the meeting)13 PURPOSE of, ?
de, por, de, da, contre, a`, de, a de, 7.48 7.23 building stonefor para, per, a, de, ?
pentru ?pedras de construc?a?o?
(Port.
)contra ?
pour (stones of building)14 SOURCE 0 015 TOPIC of, for, de, sobre, de, a, de de, de, 11.03 11.03 policy on asylumon, ?
en materia su sobre despre ?pol?
?tica en materia de asilo?
(Sp.
)about de (policy in regard to asylum)16 MANNER 0 017 MEANS by por, en, per, in, en, a`, por pe, cu, 0.07 0.07 travel by trainde, ?
a, via par pe calea ?calatorie cu tenul?
(Ro.
)(travel with train-the)18 EXPERIENCER of,?
de de de de de, ?
0.04 0.04 suffering of the peoplein ?sofrimento das pessoas?
(Port.
)in (suffering of the people)19 MEASURE 0 020 TYPE ?
?
?
?
?
?
24.47 1.7 framework law?legge quadro?
(It.
)(law framework)21 THEME of, for, de de, a de de de 23.13 19.2 conflict preventionin, ?
?prevenire de conflict?
(Ro.
)(prevention of conflict)22 BENEFICIARY 0 023 OTHER?SR of, by de a, de de, a` de, a, de, 9.61 8.13 tobacco addictioncom pentru ?adiccio?n a tabaco?
(Sp.
)(addiction to tobacco)Total no.
of examples 3,124 2,190Moreover, in the Europarl corpus, 31.64% of the instances are synthetic phrases en-coding AGENT, MEANS, LOCATION, THEME, and EXPERIENCER.
Out of these instances,98.7% use the preposition of and its Romance equivalent de.
In the CLUVI corpus,14.1% of the examples were verbal, from which the preposition of/de has a coverageof 77.66%.Based on the literature on prepositions (Lyons 1986; Barker 1998; Ionin,Matushansky, and Ruys 2006) and our own observations, the preposition of/de in bothroot and synthetic nominal phrases may have a functional or a semantic role, actingas a linking device with no apparent semantic content, or with a meaning of its own.Thus, for the interpretation of these constructions a system must rely on the meaning ofpreposition and the meaning of the two constituent nouns in particular, and on context202Girju The Syntax and Semantics of PrepositionsTable 7Mapping between the set of 22 semantic classification categories and the set of English andRomance syntactic constructions on the CLUVI corpus.
The preposition de is used here to denotesimple and articulated prepositions (de, di, du, de la, della, degli, d?, etc.).
Also, the dash ???
refersto noun?noun compounds where there is no connecting preposition.
The mapping was obtainedon the 2,023 CLUVI instance corpus.
En.
= English; Sp.
= Spanish; It.
= Italian; Fr.
= French;Port.
= Portuguese; Ro.
= Romanian.TotalToken TypeNr.
SRs En.
Sp.
It.
Fr.
Port.
Ro.
[%] [%] Example1 POSSESSION of, ?
de, ?
de, ?
de de de, ?
1.35 1.21 police car?coche de polizia?
(Sp.
)(car of police)2 KINSHIP 0 03 PROPERTY of, for, de de de de de, ?
2.97 2.76 beauty of the buildingsin, ?
?belleza de los edificios?
(Sp.
)(beauty of the buildings)4 AGENT of, for, de de, ?
de ?
de, ?
6.23 5.78 return of the familyin, by, ?
?regresso da fam??lia?
(Port.
)(return of the family)5 TEMPORAL of, in, de, con de de de de 2.97 2.97 spring rainon, at, ?
?pluie de printemps?
(Fr.
)(rain of spring)6 DEPICTION?
of de de de de de 0.3 0.3 picture of a girlDEPICTED ?retrato de uma rapariga?
(Port.
)(picture of a girl)7 PART?WHOLE of, in, de, con de, a, de, a` de 40.53 34.35 ruins of granitewith, ?
?
com de, ?
?ruinas de granito?
(Sp.
)(ruins of granite)8 IS?A of, ?
de, ?
de, ?
de, ?
?
de 5.93 5.4 sensation of fear(HYPERNYMY) with ?sensac?a?o de medo?
(Port.
)(sensation of fear)9 CAUSE from, ?
de de, da de de de, 2.72 2.72 cries of delightdatorita?
?cri de joie?
(Fr.
)(cries of delight)10 MAKE/ of, for, de de de de de 0.29 0.29 noise of the machineryPRODUCE in, from, ?
?ruido de la maquinaria?
(Sp.
)(noise of the machinery)11 INSTRUMENT for, with de, ?
de, a, de, a` de de, cu 0.29 0.29 a finger scratchcon ?o zga?rietura?
de unghie?
(Ro.
)(a scratch of finger)12 LOCATION of, in, de, en, de, su, de, sur, de, em de, pe, la, 8.65 8.01 book on the tableon, at, ?
sobre, a, in, a`, pre`s de, acima de ?
?n, la?nga ?livre sur la table?
(Fr.
)dietro de, a` cote?
de (book on the table)accanto a,sopra13 PURPOSE of, ?
de, por, de, da, contre, a, de de, 4.45 4.45 nail brushfor para, per, a, ?
de, ?
pentru ?spazzolino per le unghie?
(It.
)contra contra pour (brush for the nails)14 SOURCE of, from de de de de de 0.94 0.15 oil of cloves?o?leo de cravinho?
(Port.
)(oil of cloves)15 TOPIC of, for, on, de, de, a, de de, de, 0.79 0.79 love storyabout, ?
sobre su sobre despre ?histoire d?amour?
(Fr.
)(story of love)16 MANNER 0 017 MEANS of, by por via a` por pe 0.15 0.15 travel by car?ca?la?torie cu mas?ina?
(Ro.
)(travel by car)18 EXPERIENCER of, in, ?
de de de de de, ?
0.64 0.64 the agony of the prisoners?l?agonia dei prigionieri?
(It.
)(the agony of.the prisoners)19 MEASURE of por de a` de de, 3.81 2.72 a cup of sugarpentru ?o ceas?ca?
de zaha?r?
(Ro.
)(a cup of sugar)20 TYPE 0 021 THEME for, ?
de de, a de de de, a, ?
4.05 3.94 lack of intelligenceof, in ?manque d?intelligence?
(Fr.
)(lack of intelligence)22 BENEFICIARY 0 023 OTHER?SR of, by de de, a de de, a, de 12.95 8.81 cry of death?cri de mort?
(Fr.
)(cry of death)Total no.
of examples 2,023 1,734in general.
Because the two corpora used in this paper contain both root and syntheticinstances, we employed two semantic resources for this task: WordNet noun semanticclasses and a collection of verb classes in English that correspond to special types ofnominalizations.
These resources are defined in Section 4.2.
Moreover, in Section 6we present a detailed linguistic analysis of the prepositions of in English and de inRomance languages, and show how their selection correlates with the meaning of theconstruction.203Computational Linguistics Volume 35, Number 24.
Model4.1 Mathematical FormulationGiven the syntactic constructions considered, the goal is to develop a procedure forthe automatic annotation of the semantic relations they encode.
The semantic relationsderive from various lexical and semantic features of each instance.The semantic classification of instances of nominal phrases and compounds can beformulated as a learning problem, and thus benefits from the theoretical foundationand experience gained with various learning paradigms.
The task is a multi-class clas-sification problem since the output can be one of the semantic relations in the set.
Wecast this as a supervised learning problem where input/output pairs are available astraining data.An important first step is to map the characteristics of each instance (i.e., list ofproperties that describe the instance, usually not numerical) into feature vectors.
Let usdefine xi as the feature vector of an instance i and let X be the space of all instances; thatis, xi ?
X.The multi-class classification is performed by a function that maps the feature spaceX into a semantic space S, f :X ?
S, where S is the set of semantic relations from Table 1,namely, rj ?
S, where rj is a semantic relation.Let T be the training set of examples or instances T = (x1r1 .. xlrl) ?
(X x S)l wherel is the number of examples x each accompanied by its semantic relation label r. Theproblem is to decide which semantic relation to assign to a new, unseen example xl+1.In order to classify a given set of examples (members of X), one needs some kind ofmeasure of the similarity (or the difference) between any two given members of X.Thus, the system receives as input an English nominal phrase and compoundinstances along with their translations in the Romance languages, plus a set of extra-linguistic features.
The output is a set of learning rules that classify the data based onthe set of 22 semantic target categories.
The learning procedure is supervised and takesinto consideration the cross-linguistic lexico-syntactic information gathered for eachinstance.4.2 Feature SpaceThe set of features allows a supervised machine learning algorithm to induce a functionthat can be applied to accurately classify unseen instances.
Based on the study of theinstances and their semantic distribution presented in Section 3, we have identified andexperimented with the following features presented subsequently for each language in-volved.
Features F1?F5 have been employed by us in our previous research (Moldovanet al 2004; Girju et al 2005; Girju, Badulescu, and Moldovan 2006).
All the other featuresare novel.A.
English featuresF1 and F2.
Semantic class of noun specifies the WordNet sense of the head noun (F1), andthe modifier noun (F2) and implicitly points to all its hypernyms.
The semantics of theinstances of nominal phrases and compounds is heavily influenced by the meaning ofthe noun constituents.
One such example is family#2 car#1, which encodes a POSSESSIONrelation.
The hypernyms of the head noun car#1 are: {motor vehicle}, {self-propelled204Girju The Syntax and Semantics of Prepositionsvehicle} ... {entity} (cf.
WordNet 2.1).
These features will help generalize over the se-mantic classes of the two nouns in the instance corpus.F3 and F4.
WordNet derivationally related form specifies if the head noun (F3), and themodifier noun (F4) are related to a corresponding verb in WordNet.
WordNet containsinformation about nouns derived from verbs (e.g., statement derived from to state; cryfrom to cry; death from to die).F5.
Prepositional cues link the two nouns in a nominal phrase.
These can be either simpleor complex prepositions such as of or according to.
In case of N N instances (e.g., memberstate), this feature is ??
?.F6 and F7.
Type of nominalized noun indicates the specific class of nouns the head (F6) ormodifier (F7) belongs to depending on the verb from which it derives.
First, we check ifthe noun is a nominalization or not.
For English we used the NomLex-Plus dictionary ofnominalizations (Meyers at al.
2004) to map nouns to corresponding verbs.12 One suchexample is the destruction of the city, where destruction is a nominalization.
F6 and F7 mayoverlap with features F3 and F4 which are used in case the noun to be checked has noentry in the NomLex-Plus dictionary.These features are of particular importance because they impose some constraintson the possible set of relations the instance can encode.
They take the following values:a) active form nouns, b) unaccusative nouns, c) unergative nouns, and d) inherentlypassive nouns.
We present them in more detail subsequently.a.
Active form nouns are derived through nominalization from psych verbs and rep-resent states of emotion, such as love, fear, desire, and so forth.
They have an intrinsicactive voice predicate?argument structure and, thus, resist passivisation.
For example,we can say the desire of Anna, but not the desire by Anna.
This is also explained bythe fact that in English the AGENT or EXPERIENCER relations are mostly expressedby the clitic genitive ?s (e.g., Anna?s desire) and less or never by N P N constructions.Citing Anderson (1983), Giorgi and Longobardi (1991) mention that with such nounsthat resist passivisation, the preposition introducing the internal argument, even ifit is of, has always a semantic content, and is not a bare case-marker realizing thegenitive case.
Moreover, they argue that the meaning of these nouns might patterndifferently in different languages.
Consider for example the Italian sentences (4) and (5)below and their English equivalents (see Giorgi and Longobardi 1991, pages 121?122).
In English the instance Anna?s desire identifies the subject of desire (and thusencodes an EXPERIENCER relation), whereas in Italian it can identify either the subject(EXPERIENCER) as in Example (4), or the object of desire (THEME) as in Example (5),the disambiguation being done at the discourse level.
In Example (6) the prenominalconstruction il suo desiderio encodes only EXPERIENCER.
(4) Il desiderio di Anna fu esaudito.
(EXPERIENCER)(The desire of Anna was fulfilled.
)?Anna?s desire was fulfilled.?
(5) Il desiderio di Anna lo portera` alla rovina.
(THEME)(The desire of Anna him will ruin.
)?The desire for Anna will ruin him.
?12 NomLex-Plus is a hand-coded database of 5,000 verb nominalizations, de-adjectival, and de-adverbialnouns including the corresponding subcategorization frames (verb-argument structure information).205Computational Linguistics Volume 35, Number 2(6) Il suo desiderio fu esaudito.
(EXPERIENCER)(The her desire was fulfilled.
)?Her desire was fulfilled.
?However, our observations on the Romanian training instances in Europarl and CLUVI(captured by features F12 and F13 below) indicate that the choice of syntactic construc-tions can help in the disambiguation of instances that include such active nouns.
Thus,whereas genitive-marked N N compounds identify only the subject (thus encodingEXPERIENCER), the N de/pentru N constructions identify only the object (thus encodingTHEME).
Such examples are dorint?a Anei (?desire-the Anna-GEN?
?
Anna?s desire) (EX-PERIENCER) and dorint?a de/pentru Ana (?desire-the of/for Anna?
?
the desire for Anna)(THEME).Another example is the love of children and not the love by the children, where childrenare the recipients of love, not its experiencers.
In Italian the instance translates as l?amoreper i bambini (?the love for the children?
), whereas in Romanian it translates as dragosteapentru copii (?love-the for children?).
These nouns mark their internal argument throughof in English and most of the time require prepositions such as for in Romance languagesand vice versa.b.
Unaccusative nouns are derived from ergative verbs that take only internal ar-guments (e.g., those that indicate an object and not a subject grammatical role).
Forexample, the transitive verb to disband allows the subject to be deleted as in the followingsentences:(7) The lead singer disbanded the group in 1991.
(8) The group disbanded.Thus, the corresponding unaccusative nominalization of to disband, the disbandment ofthe group, encodes THEME and not AGENT.c.
Unergative nouns are derived from intransitive verbs.
They can take only AGENTsemantic relations.
One such case is exemplified in the instance l?arrivo della cavalleria inItalian which translates in English as the arrival of the cavalry and in Romanian as sorireacavaleriei (?arrival-the cavalry-GEN?).d.
Inherently passive nouns.
These nouns, like the verbs they are derived from, assumean implicit AGENT relation and, being transitive, associate to their internal argumentthe THEME relation.
One such example is the capture of the soldier which translates inItalian as la cattura del soldato (?the capture of the soldier?
), la capture du soldat in French(?the capture of soldier?
), and la captura de soldado in Spanish and Portuguese (?thecapture of soldier?
), where the nominalization capture (cattura, capture, captura in Italian,French, and Spanish and Portuguese respectively) is derived from the verb to capture.Here, whereas English and Italian, Spanish, Portuguese, and French use the N of/de Nconstruction (as shown in Examples (9) and (10) for English and Italian), Romanianuses genitive-marked noun compounds.
In Romanian, however, nominalizations areformed through suffixation, where a suffix is added to the root of the verb it comes from.Different suffixes attached to the same verb may lead, however, to more than one nom-inalization, producing different meanings.
The verb to capture (a captura in Romanian),for example, can result through suffixation in two nominalizations: capturare (with the206Girju The Syntax and Semantics of Prepositionsinfinitive suffix -are and encoding an implicit AGENT relation) and captura?
(throughzero derivation and encoding an implicit THEME relation) (Cornilescu 2001).
Thus, thenoun phrase capturarea soldatului (?capture-the soldier-GEN?)
encodes a THEME relation,while captura soldatului (?capture-the soldier-GEN?)
encodes an AGENT relation.
In all theRomance languages with the exception of Romanian, this construction is ambiguous,unless the AGENT is explicitly stated or inferred as shown in Example (9) for Italian.The same ambiguity might occur sometimes in English, with the difference that besidesthe of-genitive, English also uses the s-genitive: the soldier?s capture (AGENT is preferredif the context doesn?t mention otherwise), the soldier?s capture by the enemy (THEME), thecapture of the soldier (THEME is preferred if the context doesn?t mention otherwise), thecapture of the soldier by the enemy (THEME).
(9) La cattura del soldato (da parte del nemigo) e` cominciata come un atto terroristico.
(THEME)?The capture of the soldier (by the enemy) has started as a terrorist act.?
(10) La sua cattura e` cominciata come un atto terroristico.
(THEME)?His capture has started as a terrorist act.
?These nouns have a different behavior than that of active form nouns.
As shownpreviously, the object of inherently passive nouns can move to the subject position asin the soldier?s capture by the enemy, whereas it cannot do so for active form nouns (e.g.,*Anna?s desire by John).
Similarly, in Italian, although active form nouns allow only thesubject reading in prenominal constructions (e.g., il suo desiderio ?
?her desire?
), inher-ently passive nouns allow only the object reading (e.g., la sua cattura ?
?his capture?
).For Romanian, the nominalization suffixes were identified based on the morpho-logical patterns presented in Cornilescu (2001).We assembled a list of about 3,000 nouns that belong to classes a?d using the infor-mation on subcategorization frames and thematic roles of the verbs in VerbNet (Kipper,Dang, and Palmer 2000).
VerbNet is a database which encodes rich lexical informationfor a large number of English verbs in the form of subcategorization information,selectional restrictions, thematic roles for each argument of the verb, and alternations(the syntactic constructions in which the verb participates).B.
Romance featuresF8, F9, F10, F11, and F12.
Prepositional cues that link the two nouns are extracted fromeach translation of the English instance: F8 (Sp.
), F9 (Fr.
), F10 (It.
), F11 (Port.
), and F12(Ro.).
These can be either simple or complex prepositions (e.g., de, in materia de [Sp.])
inall five Romance languages, or the Romanian genitival article a/al/ale.
For N N instances,this feature is ??
?.F13.Noun inflection is defined only for Romanian and shows if the modifier noun in N Ninstances is not inflected or is inflected and modifies the head noun which is or is nota nominalization.
This feature is used to help differentiate between instances encodedby genitive-marked N N constructions and noun?noun compounds, when the choiceof syntactic construction reflects different semantic content.
Two such examples are thenoun?noun compound lege cadru (law framework) (TYPE) which translates as framework207Computational Linguistics Volume 35, Number 2law and the genitive-marked N N instance frumuset?ea fetei (?beauty-the girl-GEN?)
(PROP-ERTY) meaning the beauty of the girl.
It also covers examples such as capturarea soldatului(?capture-the soldier-GEN?
), where the modifier soldatului is inflected and the head nouncapturarea is a nominalization derived through infinitive suffixation.In the following Example we present the feature vector for the instance the captureof the soldiers.
(11) The instance the capture of the soldiers has the following Romance translations:?capture#4/Arg2 of soldiers#1/Arg1; captura de soldados; capture du soldats; cattura dei soldati;captura dos soldados; capturarea soldat?ilor; THEME?.Its corresponding feature vector is:?entity#1/Arg2; entity#1/Arg1; capture; ?
; of; inherently passive noun; ?
; de; de; de; de; ?
;mod-inflected-inf-nom; THEME?,where mod-inflected-inf-nom indicates that the noun modifier soldat?ilor in the Ro-manian translation capturarea soldat?ilor (?capture-the soldiers-GEN?)
is inflected and thatthe head noun capturarea is an infinitive nominalization.4.3 Learning ModelsSeveral learning models can be used to provide the discriminating function f. Wehave experimented with the support vector machines model and compared the resultsagainst two state-of-the-art models: semantic scattering, a supervised model describedin Moldovan et al (2004), Girju et al (2005), and Moldovan and Badulescu (2005), andLapata and Keller?s Web-based unsupervised model (Lapata and Keller 2005).Each model was trained and tested on the Europarl and CLUVI corpora using a7:3 training?testing ratio.
All the test nouns were tagged with the corresponding sensein context using a state-of-the-art WSD tool (Mihalcea and Faruque 2004).
The defaultsemantic argument frame for each relation was used in the automatic identification ofthe argument positions.A.
Support vector machinesSupport vector machines (SVMs) are a set of related supervised learning methods usedfor creating a learning function from a set of labeled training instances.
The functioncan be either a classification function, where the output is binary (is the instance ofcategory X?
), or it can be a general regression function.
For classification, SVMs operateby finding a hypersurface in the space of possible inputs.
This hypersurface will attemptto split the positive examples from the negative examples.
The split will be chosento have the largest distance from the hypersurface to the nearest of the positive andnegative examples.
Intuitively, this makes the classification correct for testing data thatis similar but not identical to the training data.In order to achieve classification in n semantic classes, n > 2, we built a binaryclassifier for each pair of classes (a total of C2n classifiers), and then we used a votingprocedure to establish the class of a new example.
For the experiments with semanticrelations, the simplest voting scheme has been chosen; each binary classifier has onevote, which is assigned to the class it chooses when it is run.
Then the class with thelargest number of votes is considered to be the answer.
The software used in theseexperiments is the package LIBSVM (http://www.csie.ntu.edu.tw/?cjlin/libsvm/)which implements an SVM model.
We tested with the radial-based kernel.208Girju The Syntax and Semantics of PrepositionsAfter the initial instances in the training and testing corpora were expanded withthe corresponding features, we had to prepare them for the SVM model.
The set-upprocedure is now described.Corpus set-up for the SVM model:The processing method consists of a set of iterative procedures of specialization ofthe examples on the WordNet IS-A hierarchy.
Thus, after a set of necessary specializa-tion iterations, the method produces specialized examples which through supervisedmachine learning are transformed into sets of semantic rules for the semantic interpre-tation of nominal phrases and compounds.
The specialization procedure is describedsubsequently.Initially, the training corpus consists of examples that follow the format exemplifiedat the end of Section 4.2 (Example [11]).
Note that for the English instances, each nounconstituent was expanded with the corresponding WordNet top semantic class.
At thispoint, the generalized training corpus contains two types of examples: unambiguousand ambiguous.
The second situation occurs when the training corpus classifies thesame noun?noun pair into more than one semantic category.
For example, both rela-tionships chocolate#2 cake#3 (PART?WHOLE) and chocolate#2 article#1 (TOPIC) are mappedinto the more general type ?entity#1, entity#1, PART?WHOLE/TOPIC?.13 We recursivelyspecialize these examples to eliminate the ambiguity.
By specialization, the semanticclass is replaced with the corresponding hyponym for that particular sense, that is,the concept immediately below in the hierarchy.
These steps are repeated until thereare no more ambiguous examples.
For this example, the specialization stops at thefirst hyponym of entity: physical entity (for cake) and abstract entity (for article).
For theunambiguous examples in the generalized training corpus (those that are classifiedwith a single semantic relation), constraints are determined using cross-validation onthe SVM model.B.
Semantic scatteringThe semantic scattering (SS) model was initially tested on the classification of genitiveconstructions, but it is also applicable to nominal phrases and compounds (Moldovanet al 2004).
SS is a supervised model which, like the SVM model described previously,relies on WordNet?s IS-A semantic hierarchy to learn a function which separates positiveand negative examples.
Essentially, it consists of using a training data set to establish aboundary G?
on WordNet noun hierarchies such that each feature pair of noun?nounsenses fij on this boundary maps uniquely into one of a predefined list of semanticrelations.
The algorithm starts with the most general boundary corresponding to theentity WordNet noun hierarchy and then specializes it based on the training data untila good approximation is reached.14 Any feature pair above the boundary maps intomore than one semantic relation.
Due to the specialization property on noun hierarchies,feature pairs below the boundary also map into only one semantic relation.
For any newpair of noun?noun senses, the model finds the closest boundary pair which maps to onesemantic relation.The authors define with SCm = { f mi } and SCh = { f hj } the sets of semantic class featuresfor modifier noun and, respectively, head noun.
A pair of<modifier, head> nouns maps13 The specialization procedure applies only to features 1 and 2.14 Moldovan et al (2004) used a list of 35 semantic relations ?
actually only 22 of them proved to be encodedby nominal phrases and compounds.209Computational Linguistics Volume 35, Number 2uniquely into a semantic class feature pair ?
f mi , fhj ?
(henceforth fij).
The probability of asemantic relation r given the feature pair fij, P(r| fij ) =n(r,fij )n( fij )is defined as the ratio betweenthe number of occurrences of a relation r in the presence of the feature pair fij over thenumber of occurrences of the feature pair fij in the corpus.
The most probable semanticrelation r?
isr?
= arg maxr?RP(r| fij) = arg maxr?RP( fij|r)P(r) (1)From the training corpus, one can measure the quantities n(r, fij) and n( fij).
Depend-ing on the level of abstraction of fij two cases are possible:Case 1.
The feature pair fij is specific enough such that there is only one semanticrelation r for which P(r| fij) = 1 and 0 for all the other semantic relations.Case 2.
The feature pair fij is general enough such that there are at least two semanticrelations for which P(r| fij) = 0.
In this case Equation (1) is used to find the most appro-priate r?.DefinitionA boundary G?
in the WordNet noun hierarchies is a set of synset pairs such that:a) for any feature pair on the boundary, denoted f G?ij ?
G?, f G?ij maps uniquely intoonly one relation r, andb) for any f uij  fG?ij , fuij maps into more than one relation r, andc) for any f lij ?
fG?ij , flij maps uniquely into a semantic relation r.Here relations  and ?
mean ?semantically more general?
and ?semantically morespecific?, respectively.As proven by observation, there are more concept pairs under the boundary G?than above it, that is, | {f lij} |  | {fuij } |.Boundary Detection AlgorithmStep 1.
Create an initial boundary.The initial boundary denoted G1 is formed from combinations of the entity#1 ?entity#1 noun class pairs.
For each training example a corresponding feature fij isfirst determined, after which it is replaced with the most general correspondingfeature consisting of top WordNet hierarchy concepts.
For example, both instancesfamily#2 estate#2 (POSSESSION) and the sister#1 of the boy#1 (KINSHIP) are mapped intoentity#1 ?
entity#1.
At this level, the noun?noun feature encodes a number of semanticrelations.
For each feature, one can determine the most probable relation using Equa-tion (1).
For instance, the feature entity#1 ?
entity#1 can be encoded by any of the 23relations.The next step is to construct a lower boundary by specializing the semantic classesof the ambiguous features.
A feature fij is ambiguous if it corresponds to more thanone relation and its most relevant relation has a conditional probability less than 0.9.210Girju The Syntax and Semantics of PrepositionsTo eliminate irrelevant specializations, the algorithm specializes only the ambiguousclasses that occur in more than 1% of the training examples.The specialization procedure consists of first identifying the features fij to whichcorrespond more than one semantic relation, then replacing these features with theirhyponym synsets.
Thus one feature breaks into several new specialized features.For example, the feature entity#1 ?
entity#1 generated through generalization forthe examples family#2 estate#2 and the sister#1 of the boy#1 is specialized now askin group#1 ?
real property#1 and female sibling#1 ?
male person#1 corresponding to thedirect hyponyms of the nouns in these instances.
The net effect is that the semanticrelations that were attached to fij will be ?scattered?
across the new specialized featureswhich form the second boundary.
The probability of the semantic relations that areencoded by these specialized features is recalculated again using Equation (1).
Thenumber of relations encoded by each of this boundary?s features is less than the one forthe features defining the previous boundary.
This process continues until each featurehas only one semantic relation attached.
Each iteration creates a new boundary.Step 2.
Test the new boundary.The new boundary is more specific than the previous boundary and it is closer to theideal boundary.
One does not know how well it behaves on unseen examples, but thegoal is to find a boundary that classifies these instances with high accuracy.
Thus,the boundary is first tested on only 10% of the annotated examples (different fromthe 10% of the examples used for testing).
If the accuracy is larger than the previousboundary?s accuracy, the algorithm is converging toward the best approximation of theboundary and thus it repeats Step 2 for the new boundary.
If the accuracy is lower thanthe previous boundary?s accuracy, the new boundary is too specific and the previousboundary is a better approximation of the ideal boundary.C.
Lapata and Keller?s Web-based unsupervised modelLauer (1995) was the first to devise and test an unsupervised probabilistic model fornoun?noun compound interpretation on Grolier?s Encyclopedia, an eight million wordcorpus, based on a set of eight preposition paraphrases.
His probabilistic model com-putes the probability of a preposition p given a noun?noun pair n1 ?
n2 and findsthe most likely preposition paraphrase p?
= argmaxpP(p|n1,n2).
However, as Lauernoticed, this model requires a very large training corpus to estimate these proba-bilities.
More recently, Lapata and Keller (2005) replicated the model using the Webas training corpus and showed that the best performance was obtained with the trigrammodel f (n1, p,n2).
In their approach, they used as the count for a given trigram the num-ber of pages returned by using the trigram as a query.
These co-occurrence frequencieswere estimated using inflected queries which are obtained by expanding a noun?nouncompound into all its morphological forms; then searching for N P N instances, for eachof the eight prepositions P in Lauer?s list.
All queries are performed as exact matchesusing quotation marks.
For example, for the test noun?noun compound instancewar sto-ries, all possible combinations of definite/indefinite articles and singular/plural nounforms are tried resulting in the queries story about war, a/the story about war, story abouta/the war, stories about war, stories about the wars, story about wars, story about the wars,and so on.
These forms are then submitted as literal queries, and the resulting hits aresummed up.
The query, and thus the preposition, with the largest number of hits isselected as the correct semantic interpretation category.211Computational Linguistics Volume 35, Number 2For the Europarl and CLUVI test sets, we replicated Lapata and Keller?s (2005) ex-periments using Google.15 We formed inflected queries with the patterns they proposedand searched the Web.5.
Experimental ResultsWe performed various experiments on both the Europarl and CLUVI testing corporausing seven sets of supervised models.
Table 8 shows the results obtained against SSand Lapata and Keller?s model on both corpora and the contribution of the featuresexemplified in seven versions of the SVM model.
Supervised models 1 and 2 are definedonly for the English features.
Here, features F1 and F2 measure the contribution of theWordNet IS-A lexical hierarchy specialization.
However, supervised model 1, which isalso the baseline, does not differentiate between unambiguous and ambiguous trainingexamples and thus does not specialize those that are ambiguous.
These models showthe difference between SS and SVM and the contribution of the other English features,such as preposition and nominalization (F1?F7).The table shows that overall the performance is better for the Europarl corpusthan for CLUVI.
For the supervised models 1 and 2, SS [F1 + F2] gives better re-sults than SVM [F1 + F2].
The inclusion of the other English features (SVM [F1?F7])adds more than 10% accuracy (with a higher increase in Europarl) for the supervisedmodel 1.The results obtained are presented using the standard measure of accuracy (thenumber of correctly labeled instances over the number of instances in the test set).5.1 The Contribution of Romance Linguistic FeaturesOur intuition is that the more information we use from other languages for the interpre-tation of an English instance, the better the results.
Thus, we wanted to see the impactof each Romance language on the overall performance.
Supervised model 3 shows theresults obtained for English and the Romance language that contributed the least to theperformance (English and Spanish for the entire English feature subset F1?F8).
Here wecomputed the performance on all five English?Romance language combinations andchose the Romance language that provided the best result.
Thus, supervised models 3through 7 add Spanish, French, Portuguese, Italian, and Romanian in this order andshow the contribution of each Romance preposition and all English features.The language ranking in Table 8 shows that Romance languages considered herehave a different contribution to the overall performance.
Whereas the addition ofPortuguese in CLUVI decreases the performance, in Europarl it increases it, if onlyby a few points.
However, a closer analysis of the data shows that this is mostly dueto the distribution of the corpus instances.
For example, French, Italian, Spanish, andPortuguese are consistent in the choice of preposition (e.g., if the preposition de [of ] isused in French, then the corresponding preposition is used in the other four languagetranslations).
A notable exception here is Romanian which provides two possible con-structions with almost equal distribution: the N P N and the genitive-marked N N. Thetable shows (in the increase in performance between supervised models 6 and 7) that15 As Google limits the number of queries to 1,000 per day per computer, we repeated the experiment using10 computers for a number of days.
Although Keller and Lapata used AltaVista for the interpretation oftwo noun?noun compounds, they showed that there is almost no difference between the correlationsachieved using Google and AltaVista counts.212Girju The Syntax and Semantics of PrepositionsTable 8The performance obtained by five versions of the cross-linguistic SVM model compared againstthe baseline, an English SVM model, and the SS model.
The results obtained are presentedusing the standard measure of accuracy (number of correctly labeled instances over the numberof instances in the test set).Results [%]CLUVI EuroparlLearning models 8-PP 22-SR 8-PP 22-SRSupervised model 1: Baseline SS (F1+F2) 42.01 46.03 35.8 36.2(English nominal features only) SVM (F1+F2) 34.17 38.11 30.02 33.01(no WordNet specialization) SVM (F1-F7) ?
50.1 ?
43.33Supervised model 2 SS (F1+F2) 55.20 61.02 54.12 57.01(English features) SVM (F1+F2) 41.8 46.18 41.03 41.3SVM (F1-F7) ?
61.04 ?
67.63Supervised model 3 SVM (F1-F7+F8) ?
63.11 ?
68.04(English and Spanish features)Supervised model 4 SVM ?
65.81 ?
69.58(English, Spanish, and French (F1-F7+F8+F9)features)Supervised model 5 SVM ?
64.31 ?
69.92(English, Spanish, French, (F1-F7+F8+F9)and Portuguese features) (+F11)Supervised model 6 SVM ?
66.05 ?
71.25(English, Spanish, French, (F1-F7+F8+F9+Portuguese, and Italian features) F10+F11)Supervised model 7 (SVM) ?
72.82 ?
76.34(English and all Romance features: F1?F13)Lapata and Keller?s Web-based 41.10 ?
42.12 ?unsupervised model (English)this choice is not random, but influenced by the meaning of the instances (features F12,F13).
This observation is also supported by the contribution of each feature to the overallperformance.
For example, in Europarl, the WordNet verb and nominalization featuresof the head noun (F3, F6) have a contribution of 5.12%, whereas for the modifier nounsthey decrease by about 2.7%.
The English preposition (F5) contributes 6.11% (Europarl)and 4.82% (CLUVI) to the overall performance.The most frequently occurring preposition in both corpora is the underspecifiedpreposition de (of ), encoding almost all of the 22 semantic relations.
The many-to-many mappings of the preposition to the semantic classes adds to the complexityof the interpretation task.
A closer look at the Europarl and CLUVI data shows thatLauer?s set of eight prepositions represents 88.2% (Europarl) and 91.8% (CLUVI) of theN P N instances.
From these, the most frequent preposition is of with a coverage of 79%(Europarl) and 88% (CLUVI).
Because the polysemy of this preposition is very high, wewanted to analyze its behavior on the set of most representative semantic relations inboth corpora.
Moreover, we wanted to see what prepositions were used to translate theEnglish nominal phrase and compound instances in the target Romance languages, andthus to capture the semantic (ir)regularities among these languages in the two corporaand their contribution to the semantic interpretation task.For most of the N P N instances, we noticed consistent behavior of the targetRomance languages in terms of the prepositions used.
This behavior can be classified213Computational Linguistics Volume 35, Number 2roughly in four categories exemplified subsequently: Example (12) shows a combinationof the preposition of/de and more specific prepositions; Example (13) shows differentprepositions than the one corresponding to the English equivalent in the instance; andExamples (14) and (15) show corresponding translations of the equivalent preposi-tion in English in all Romance languages with variations in Romanian (e.g., de for of,para/pour/par/pentru for for).
(12) Committee on Culture (En.)
?
Comisio?n de la Cultura (Sp.)
?
commission de laculture (Fr.)
?
commissione per la cultura (It.)
?
Comissa?o para Cultura (Port.)
?comitet pentru cultura?
(Ro.)
(PURPOSE)(13) the supervision of the administration (En.)
?
control sobre la administracio?n(Sp.)
?
contro?le sur l?administration (Fr.)
?
controllo sull?amministrazione (It.)
?controlo sobre administrac?a?o (Port.)
?
controlul asupra administrat?iei (Ro.
)(THEME)(14) lack of protection (En.)
?
falta de proteccio?n (Sp.)
?
manque de protection (Fr.)
?mancanza di tutela (It.)
?
falta de protecc?a?o (Port.)
?
lipsa?
de protect?ie (Ro.
)(THEME)(15) the cry of a man (En.)
?
el llanto de un hombre (Sp.)
?
un cri d?homme (Fr.)
?l?urlo di un uomo (It.)
?
o choro de um be?bado (Port.)
?
striga?tul unui om (Ro.
)(AGENT)Because the last three categories are the most frequent in both corpora, we analyzedtheir instances.
Most of the time Spanish, French, Italian, and Portuguese make use ofspecific prepositions such as those in Examples (12) and (13) to encode some semanticrelations such as PURPOSE and LOCATION, but rely on N de N constructions for almostall the other relations.
English and Romanian, however, can choose between N N andN P N constructions.
In the next section we present in more detail an analysis of thesemantic correlations between English and Romanian nominal phrases and compoundsand their role in the semantic interpretation task.6.
Linguistic ObservationsIn this section we present some linguistic observations derived from the analysis ofthe system?s performance on the CLUVI and Europarl corpora.
More specifically, wepresent different types of ambiguity that can occur in the interpretation of nominalphrases and compounds when using more abstract interpretation categories such asLauer?s eight prepositions.
We also show that the choice of syntactic constructionsin English and Romanian can help in the identification of the correct position of thesemantic arguments in test instances.6.1 Observations on Lapata and Keller?s Unsupervised ModelIn this section we show some of the limitations of the unsupervised probabilistic ap-proaches that rely on more abstract interpretation categories, such as Lauer?s set ofeight prepositions.
For this, we used Lapata and Keller?s approach, a state-of-the-artknowledge-poor Web-based unsupervised probabilistic model which provided a per-formance of 42.12% on Europarl and 41.10% on CLUVI.
We manually checked the first214Girju The Syntax and Semantics of PrepositionsTable 9Experimental results with Lapata and Keller?s Web-based unsupervised interpretation model ondifferent types of test sets from the Europarl corpus.Noun?noun compound Accuracytest set Ambiguity of noun constituents [%]Set#1 one part of speech, one WordNet sense 35.28%Set#2 multiple parts of speech, one WordNet sense 31.22%Set#3 one part of speech, multiple WordNet senses 50.63%Set#4 multiple parts of speech, multiple WordNet senses 43.25%five entries of the pages returned by Google for each most frequent N P N paraphrasefor 100 CLUVI and Europarl instances and noticed that about 35% of them were wrongdue to syntactic (e.g., part of speech) and/or semantic ambiguities.
For example, baby crygenerated instances such as ?it will make moms cry with the baby,?
where cry is a verb,not a noun.
This shows that many of the NP instances selected by Google as matchingthe N P N query are incorrect, and thus the number of hits returned for the query is over-estimated.
Thus, because we wanted to measure the impact of various types of noun?noun compound ambiguities on the interpretation performance, we further tested theprobabilistic Web-based model on four distinct test sets selected from Europarl, eachcontaining 30 noun?noun compounds encoding different types of ambiguity: In Set#1the noun constituents had only one part of speech and one WordNet sense; in Set#2 thenouns had at least two possible parts of speech and were semantically unambiguous; inSet#3 the nouns were ambiguous only semantically; and in Set#4 they were ambiguousboth syntactically and semantically.
Table 9 shows that for Set#1, the model obtained anaccuracy of 35.28%, while for more semantically ambiguous compounds it obtained anaverage accuracy of about 48% (50.63% [Set#3] and 43.25% [Set#4].
This shows that formore syntactically ambiguous instances, the Web-based probabilistic model introducesa significant number of false positives, thus decreasing the accuracy (cf.
sets #1 vs. #2and #3 vs. #4).Moreover, further analyses of the results obtained with Lapata and Keller?s modelshowed that about 30% of the noun?noun compounds in sets #3 and #4 were ambiguouswith at least two possible readings.
For example, paper bag can be interpreted out-of-context both as bag of paper (bag made of paper?STUFF?OBJECT, a subtype ofPART?WHOLE) and as bag for papers (bag used for storing papers?PURPOSE).
Simi-larly, gingerbread bowl can be correctly paraphrased both as bowl of/with gingerbread(CONTENT?CONTAINER) and as bowl of gingerbread (bowl made of gingerbread?STUFF?OBJECT).
The following two examples show the two readings of the noun?noun com-pound gingerbread bowl as found on Google:(16) Stir a bowl of gingerbread,Smooth and spicy and brown,Roll it with a rolling pin,Up and up and down,...1616 An excerpt from the ?Gingerbread Man?
song.215Computational Linguistics Volume 35, Number 2(17) The gingerbread will take the shape of the glass bowl.
Let it cool for a fewminutes and then carefully loosen the foil and remove the gingerbread fromthe glass.
And voila`: your bowl of gingerbread.These ambiguities partially explain why the accuracy values obtained for sets #3and #4 are higher then the ones obtained for the other two sets.
The semantic ambiguityalso explains why the accuracy obtained for set #2 is higher than that for set #4.
For thesesets of examples the syntactic ambiguity affected the accuracy much less than the se-mantic ambiguity (that is, more N P N combinations were possible due to various nounsenses).
This shows one more time that a large number of noun?noun compounds arecovered by more abstract categories, such as prepositions.
Moreover, these categoriesalso allow for a large variation as to which category a compound should be assigned.6.2 Observations on the Symmetry of Semantic Relations: A Study on Englishand RomanianNominal phrases and compounds in English, nominal phrases in the Romance lan-guages considered here, and genitive-marked noun?noun compounds in Romanianhave an inherent directionality imposed by their fixed syntactic structure.
For example,in English noun?noun compounds the syntactic head always follows the syntacticmodifier, whereas in English and Romance nominal phrases the order is reversed.
Twosuch examples are tea/Modifier cup/Head and glass/Head of wine/Modifier.The directionality of semantic relations (i.e., the order of the semantic arguments)however, is not fixed and thus it is not always the same as the inherent direc-tionality imposed by the syntactic structure.
Two such examples are ham/Modifier/Arg2 sandwich/Head/Arg1 and spoon/Modifier/Arg1 handle/Head/Arg2.
Althoughboth instances encode a PART?WHOLE relation (Arg1 is the semantic argument iden-tifying the whole and Arg2 is the semantic argument identifying the part), their se-mantic arguments are not listed in the same order (Arg1 Arg2 for spoon handle andArg2 Arg1 for ham sandwich).
For a better understanding of this phenomenon, weperformed a more thorough analysis of the training instances in both CLUVI andEuroparl.
Because the choice of syntactic constructions in context is governed in partby semantic factors, we focused on English and Romanian because they are the onlylanguages from the set considered here with two productive syntactic options: N Nand N P N (English) and genitive-marked N N and N P N (Romanian).
Thus, wegrouped the English?Romanian parallel instances per each semantic relation and eachsyntactic construction and checked if the relation was symmetric or not, according tothe following definition.DefinitionWe say that a semantic relation is symmetric relative to a particular syntacticconstruction if there is at least one relation instance whose arguments are in a differentorder than the order indicated by the relation?s default argument frame for thatconstruction.For example, PART?WHOLE is symmetric with regard to nominal phrases becausethe semantic arguments of the instance the building/Arg1 with parapets/Arg2 are in adifferent order than the one imposed by the relation?s default argument frame (Arg2P Arg1) for nominal phrases (cf.
Table 1).216Girju The Syntax and Semantics of PrepositionsBecause the relation distribution is skewed in both corpora, we focused only onthose relations encoded by at least 50 instances in both Europarl and CLUVI.
Forexample, in English the POSSESSION relation is symmetric when encoded by N P N andnoun?noun compounds.
For instance, we can say the girl with three dogs and the resourcesof the Union, but also family estate and land proprietor.
The findings are summarized andpresented in Table 10 along with examples.
Some relations such as IS-A, PURPOSE, andMEASURE cannot be encoded by genitive-marked noun?noun compounds in Romanian(indicated by ???
in the table).
A checkmark symbol indicates if the relation is symmetric(??)
or not (?x?)
for a particular syntactic construction.
It is interesting to note that notall the relations are symmetric and this behavior varies from one syntactic constructionto another and from one language to another.
Although some relations such as AGENTand THEME are not symmetric, others such as TEMPORAL, PART?WHOLE, and LOCATIONare symmetric irrespective of the syntactic construction used.Symmetric relations pose important challenges to the automatic interpretation ofnominal phrases and compounds because the system has to know which of the nounsis the semantic modifier and which is the semantic head.
In this research, the orderof the semantic arguments has been manually identified and marked in the trainingcorpora.
However, this information is not provided for unseen test instances.
So far,in our experiments with the test data the system used the order indicated by thedefault argument frames.
Another solution is to build argument frames for clusters ofprepositions which impose a particular order of the arguments in N P N constructions.For example, in the N2 P N1 phrases the books on the table (LOCATION) and relaxationduring the summer (TEMPORAL), the semantic content of the prepositions on and duringidentifies the position of the physical and temporal location (e.g., that N1 is the timeor location).
This approach works most of the time for relations such as LOCATIONand TEMPORAL because in both English and Romance languages they rely mostly onprepositions indicating location and time and less on underspecified prepositions suchas of or de.
However, a closer look at these relations shows that some of the noun?nounpairs that encode them are not symmetric and this is true for both English and Romance.For instance, cut on the chin and house in the city cannot be reversed as chin P cut orcity P house.
One notable exception here is indicated by examples such as box of/withmatches ?
matches in/inside the box and vessels of/with blood ?
blood in vessels17 encodingCONTENT?CONTAINER.
Another special case is when P1 and P2 are location antonyms(e.g., the book under the folder and the folder on the book).
However, even here symmetryis not always possible, being influenced by pragmatic factors (Herskovits 1987) (e.g.,we can say the vase on the table, but not the table under the vase?this has to do with thedifference in size of the objects indicated by the head and modifier nouns.
Thus, a largerobject cannot be said to be placed under a smaller one).It is important to stress here the fact that our definition of symmetry of semanticrelations does not focus in particular on the symmetry of an instance noun?noun pairthat encodes the relation, although it doesn?t exclude such a case.
We call this lexicalsymmetry and define it here.DefinitionWe say that a noun?noun pair (N1 ?
N2) is symmetric relative to a particular syntacticconstruction and the semantic relation it encodes in that construction if the order of thenouns in the construction can be changed provided the semantic relation is preserved.17 Here the noun vessels refers to a type of container.217Computational Linguistics Volume 35, Number 2Table 10A summary of the symmetry properties of a set of the 12 most frequent semantic relations inCLUVI and Europarl.
???
means the semantic relation is not encoded by the syntacticconstruction, ??
and ?x?
symbols indicate whether the relation is or is not symmetric.SymmetryEnglish RomanianSemantic genitive-markedNo.
relations N N N P N N N N P N Examples1 POSSESSION    x En.
: family#2/Arg1 estate#2/Arg2 vs.land#1/Arg2 proprietor#1/Arg1Ro.
: terenul/Arg2 proprietarului/Arg1(land-the owner-GEN)(?the owner?s land?
)proprietarul/Arg1 magazinului/Arg2(owner-the store-GEN)(?the owner of the store?
)2 PROPERTY x  x  En.
: calm#1/Arg2 of evening#1/Arg1 vs.spots#4/Arg1 of color#1/Arg2Ro.
: pete/Arg1 de culoare/Arg2(?spots of color?
)miros/Arg2 de camfor/Arg1(?odour of camphor?
)3 AGENT x x x x En.
: the investigation#2/Arg2 of the police#1/Arg1Ro.
: investigat?ia/Arg2 polit?iei/Arg1(investigation-the police-GEN)4 TEMPORAL     En.
: news#3/Arg2 in the morning#1/Arg1 vs.the evening#1/Arg1 of her arrival#2/Arg2Ro.
: placinte/Arg2 de dimineat?a?/Arg1(cakes of morning)(?morning cakes?)
vs.ani/Arg1 de subjugare/Arg2(?years of subjugation?
)5 PART?WHOLE     En: faces#1/Arg2 of children#1/Arg1 vs.the shell#5Arg2 of the egg#2/Arg1Ro: fet?ele/Arg2 copiilor/Arg1(faces-the children-GEN)(?the faces of the children?)
vs.coaja?/Arg1 de ou/Arg2(shell of egg)(?egg shell?
)6 HYPERNYMY x x ?
x En.
: daisy#1/Arg1 flower#1/Arg2(is-a) Ro.
: meci/Arg2 de fotbal/Arg1(match of football)(?football match?
)7 LOCATION     En.
: castle#2/Arg2 in the desert#1/Arg1 vs.point#2/Arg1 of arrival#1/Arg2Ro.
: castel/Arg2 in des?ert/Arg1(castle in desert)(?castle in the desert?)
vs.punct/Arg1 de sosire/Arg2(?point of arrival?
)8 PURPOSE x x ?
x En.
: war#1/Arg1 canoe#1/Arg2Ro.
: piroga?/Arg2 de ra?zboi/Arg1(canoe of war)9 TOPIC x x x x En.
: war#1/Arg1 movie#1/Arg2Ro.
: film/Arg2 despre ra?zboi/Arg1(?movie about war?
)10 MEASURE ?
x ?
x En.
: inches#1/Arg2 of snow#2/Arg1Ro.
: inci/Arg2 de zapada?/Arg1 (inches of snow)11 TYPE x  x x En.
: framework#1/Arg1 law#2/Arg2Ro.
: lege/Arg2 cadru/Arg1 (law framework)12 THEME x x x x En.
: examination#1/Arg2 of machinery#1/Arg1Ro.
: verificarea/Arg2 mas?inii/Arg1(examination-the machinery-GEN)(?the examination of the machinery?
)218Girju The Syntax and Semantics of PrepositionsFor instance, the pair building?parapets in the nominal phrases the building/Arg1 withparapets/Arg2 and the parapets/Arg2 of the building/Arg1 encodes a PART?WHOLE relation.Here, both the noun?noun pair and the semantic relation are symmetric relative toN P N. However, the situation is different for instances such as the book/Arg2 underthe folder/Arg1 and the folder/Arg2 on the book/Arg1, both encoding LOCATION.
Here, thebook?folder pair is symmetric in N P N constructions (in the first instance the book is thesyntactic head and the folder is the modifier, whereas in the second instance the orderis reversed).
However, the LOCATION relation they encode is not symmetric (in bothinstances, the order of the semantic arguments matches the default argument frame forLOCATION).
It is interesting to notice here that these two location instances are actuallyparaphrases of one another.
This can be explained by the fact that both the book and thefolder can act as a location with respect to the other, and that the prepositions under andon are location antonyms.
In comparison, the building with parapets is not a paraphraseof the parapets of the building.
Here, the nouns building and parapets cannot act as awhole/part with respect to each other (e.g., the only possible whole here is the nounbuilding, and the only possible part here is the noun parapets).
This is because parts andwholes have an inherent semantic directionality imposed by the inclusion operation onthe set of things representing parts and wholes, respectively.In this research we consider the identification and extraction of semantic relationsin nominal phrases and compounds, but we do not focus in particular on the acquisitionof paraphrases in these constructions.
Our goal is to build an accurate semantic parserwhich will automatically annotate instances of nominal phrases and compounds withsemantic relations in context.
This approach promises to be very useful in applicationsthat require semantic inference, such as textual entailment (Tatu and Moldovan 2005).However, a thorough analysis of the semantics of nominal phrases and compoundsshould focus on both semantic relations and paraphrases.
We leave this topic for futureresearch.Because we wanted to study in more detail the directionality of semantic relations,we focused on PART?WHOLE.
These relations, and most of the semantic relations con-sidered here, are encoded mostly by N of/de N constructions, genitive-marked N N(Romanian), and noun?noun compounds (English) and thus, the task of argument orderidentification becomes more challenging.
For the purpose of this research we decidedto take a closer look at the PART?WHOLE relation in both CLUVI and Europarl wheretogether it accounted for 920 token and 636 type instances.
We show subsequently adetailed analysis of the symmetry property on a classification of PART?WHOLE relationsstarting with a set of five PART?WHOLE subtypes identified by Winston, Chaffin, andHermann (1987):18 (1) Component?Integral object, (2) Member?Collection, (3) Portion?Mass,(4) Stuff?Object, and (5) Place?Area.
(1) Component?Integral objectThis is a relation between components and the objects to which they belong.
Integralobjects have a structure with their components being separable and having a functionalrelation with their wholes.
This type of PART?WHOLE relation can be encoded by N of Nand less often by N N constructions.
Moreover, here the existential interpretation ispreferred over the generic one.
Such examples are the leg of the table and the tableleg which translate in Romanian as piciorul mesei (?leg-the table-GEN?).
In Romanian a18 Winston, Chaffin, and Hermann (1987) identified six subtypes of PART?WHOLE relations, one of which,(Feature?Activity), is not presented here because it is not frequently encoded by N N and N P Nconstructions.219Computational Linguistics Volume 35, Number 2generic interpretation is also possible, but with change of construction and most of thetime of semantic relation (e.g., picior de masa?
?
?leg of table?
encoding PURPOSE19).This relation subtype is symmetric in English for both N N and N P N con-structions.
In Romanian, however, it is symmetric only when encoded by N P N.Moreover, it is interesting to note that Modifier/Arg1 Head/Arg2 noun?noun com-pound instances translate as genitive noun?noun compounds in Romanian, whereasModifier/Arg2 Head/Arg1 instances translate as N P N, with P different from of.
Forexample, chair/Arg1 arm/Arg2 and ham/Arg2 sandwich/Arg1 translate in Romanian asHead/Arg2 Modifier/Arg1 ?
brat?ul scaunului (?arm-the chair-GEN?)
and Head/Arg1 PModifier/Arg2 ?
sandwich cu s?unca?
(?sandwich with ham?
).For N P N instances in Romanian and English both Arg1 P1 Arg2 and Arg2 P2 Arg1argument orderings are possible, but with a different choice of preposition (with P1different from of/de).
For example, one can say the parapets/Arg2 of the building/Arg1,but also the building/Arg1 with parapets/Arg2.
A closer look at such instances shows thatsymmetry is possible when the modifier (in this case the part) is not a mandatory partof the whole, but an optional part with special features (e.g., color, shape).
For example,the car with the door is less preferred than the car with the red door which differentiates thecar from other types of cars.
(2) Stuff?ObjectThis category encodes the relations between an object and the material of which itis partly or entirely made.
The parts are not similar to the wholes which they compose,cannot be separated from the whole, and have no functional role.
The relation canbe encoded by both N of N and N N English and Romanian patterns and the choicebetween existential and generic interpretations correlates with the relation symmetry.For N N constructions this relation subtype is not symmetric, while for N P N itis symmetric only in English.
Such examples are brush/Arg2 hut/Arg1 in English, andmetalul/Arg2 scaunului/Arg1 (?metal-the seat-GEN?
?
the metal of the seat) and scaun de metal(?chair of metal?
?
metal chair) in Romanian.N P N instances can only be encoded by of in English or de/din (of/from) in Ro-manian.
If the position of the arguments is Arg1 of Arg2 and Arg2 is an indefinite nounindicating the part then the instance interpretation is generic.
For example, seat of metaltranslates as scaun de/din metal (?chair of/from metal?)
in Romanian.
It is important tonote here the possible choice of the preposition from in Romanian, a preposition whichis rarely used in English for this type of relation.When the position of the arguments changes (e.g., Arg2 of Arg1), the same preposi-tion of is used and the semantic relation is still STUFF?OBJECT, but the instance is morespecific having an existential interpretation.
For instance, the metal of the seat translates inRomanian as metalul scaunului (?metal-the seat-GEN?)
and not as metalul de scaun (?metal-the of seat?).
(3) Portion?MassAccording to Selkirk (1982a), Ionin, Matushansky, and Ruys (2006), and our ownobservations on the CLUVI and Europarl data, this type of PART?WHOLE relation canbe further classified into mass, measure, and fraction partitives.
Here the parts areseparable and similar to each other and to the whole they are part of.
An example of amass partitive is half/Arg2 of the cake/Arg1 which translates in Romanian as juma?tate/Arg219 This reading is possible if the leg is separated from the table.220Girju The Syntax and Semantics of Prepositionsde/din prajitura?/Arg1 (?half of/from cake?).
Note that here the noun cake is indefinite inRomanian, and thus the instance is generic.
An existential interpretation is possiblewhen the noun is modified by a possessive (e.g., half of your cake).Measure partitives are also called vague PART?WHOLE relations (Selkirk 1982b)because they can express both PART?WHOLE and MEASURE depending on the context.They are encoded by N1 of N2 constructions, where N2 is indefinite, and can indi-cate both existential and generic interpretations.
Two such examples are bottles/Arg1 ofwine/Arg2 and cup/Arg1 of sugar/Arg2.
In Romanian, the preposition used is either de(of ), or cu (with).
For example, sticle/Arg1 de/cu vin/Arg2 (?bottles of/with wine?)
andceas?ca?/Arg1 de/cu zaha?r/Arg2 (?cup of/with sugar?
).Fraction partitives indicate fractions of wholes, such as three quarters/Arg2 of apie/Arg1 (trei pa?trimi/Arg2 de pla?cinta?/Arg1 [Romanian]?
[?three quarters of pie?])
and onethird/Arg2 of the nation/Arg1 (o treime/Arg2 din populat?ia/Arg1 [Romanian]?
[?one third frompopulation-the?]
and not o treime de populat?ia ?
[?a third of population-the?]).
Here again,we notice the choice of the Romanian preposition din and not de when the second nounis definite.
The preposition from indicates the idea of separation of the part from thewhole, an idea which characterizes PART?WHOLE relations.Portion?Mass relations cannot be encoded by N N structures in either English orRomanian and they are not symmetric in N P N constructions.
(4) Member?CollectionThis subtype represents membership in a collection.
Members are parts, but maynot play any functional role with respect to their whole.
That is, compared withComponent?Integral instances such as the knob of the door, where the knob is a roundhandle one turns in order to open a door, in an example like bunch of cats, the cats don?tplay any functional role to the whole bunch.This subtype can be further classified into a basic subtype (e.g., the member of theteam), count partitives (e.g., two of these people), fraction count partitives (e.g., twoout of three workers), and vague measure partitives (e.g., a number/lot/bunch of cats).Although the basic Member?Collection partitives are symmetric for N N (Romanianonly) and N P N (English and Romanian), the other subtypes can be encoded only byN P N constructions and are not symmetric in English or in Romanian.
For example,the children/Arg2 of the group/Arg1 and children/Arg2 group/Arg1 translate as copiii/Arg2din grup/Arg1 (?children-the from group?)
and as grup/Arg1 de copii/Arg2 (?group ofchildren?
).The second and the third subtypes translate in Romanian as doi/Arg2 din aces?tioameni/Arg1 (?two from these people?)
and doi/Arg2 din trei lucra?tori/Arg1 (?two from threeworkers?
), by always using the preposition din ( from) instead of de (of ).
On the otherhand, vague measure partitives translate as un numa?r/Arg1 de pisici/Arg2 (?a number ofcats?)
and not as un numa?r din pisici (?a number from cats?).
Although all these subtypesneed to have a plural modifier noun and are not symmetric, count partitives alwayshave an existential interpretation, whereas fraction count and vague measure partitiveshave a generic meaning.
(5) Location?AreaThis subtype captures the relation between areas and special places and locationswithin them.
The parts are similar to their wholes, but they are not separable fromthem.
Thus, this relation overlaps with the LOCATION relation.
One such example is thesurface/Arg2 of the water/Arg1.
Both nouns can be either definite or indefinite and the rela-tion is not symmetric when the part is a relational noun (e.g., surface, end).
In Romanian,221Computational Linguistics Volume 35, Number 2both N de N and genitive-marked N N constructions are possible: suprafat?a/Arg2apei/Arg1 (?surface-the water-GEN?)
and suprafat?a?/Arg2 de apa?/Arg1 (?surface of water?
).The relation is symmetric only for N P N in both English and Romanian.Table 11 summarizes the symmetry properties of all five PART?WHOLE subtypesaccompanied by examples.Thus, features such as the semantic classes of the two nouns (F1, F2), and the syntac-tic constructions in English and Romanian?more specifically, the preposition featuresfor English (F5) and Romanian (F12) and the inflection feature for Romanian (F13)?can be used to train a classifier for the identification of the argument order in nominalphrases and compounds encoding different subtypes of PART?WHOLE relations.
Forexample, the argument order for Portion?Mass instances can be easily identified if it isdetermined that they are encoded byN2 of/de N1 in English and Romanian and the headnoun N2 is identified as a fraction in the WordNet IS-A hierarchy, thus representing Arg2(the part).
It is interesting to note here that all the other Member?Collection subtypes withthe exception of the basic one are also encoded only by N of/de N, but here the orderis reversed in both English and Romanian (N1 of/de N2), where the head noun N1, ifidentified as a collection concept in WordNet, represents the whole concept (Arg1).This approach can also be applied to other symmetric relations by classifying theminto more specific subtypes for argument order identification.
Thus, local classifierscan be trained for each subtype on features such as those mentioned herein and testedon unseen instances.
However, for training this procedure requires a sufficiently largenumber of examples for each subtype of the semantic relation considered.Table 11A summary of the symmetry properties of the five subtypes of PART?WHOLE semantic relation inCLUVI and Europarl.
???
means the semantic relation is not encoded by the syntacticconstruction, ??
and ?x?
symbols indicate whether the relation is or is not symmetric.SymmetryEnglish RomanianSemantic genitive-markedNo.
relations N N N P N N N N P N Examples1 Component ?
  x  En.
: chair#1/Arg1 arm#5/Arg2 vs.Integral obj.
Arg2 Arg1 ham#1/Arg2 sandwich#1/Arg1Ro: ?brat?ul/Arg2 scaunului/Arg1?
(arm-the chair-GEN) vs.?sandwhich/Arg1 cu s?unca?/Arg2?
(sandwich with ham)2 Stuff ?
x  x x En.
: dress#1/Arg1 of silk#1/Arg2 vs.Object Arg2 Arg1 Arg2 Arg1 Arg1 de Arg2 the silk#1/Arg2 of the dress#1/Arg1Ro.
: ?rochie/Arg1 de ma?tase/Arg2?
(dress of silk) vs.?ma?tasea/Arg2 rochiei/Arg1?
(silk-the dress-GEN)3 Portion ?
?
x ?
x En.
: half#1/Arg2 of the cake#3/Arg1 vs.Mass Arg2 of Arg1 Arg2 de Arg1 Ro: ?juma?tate/Arg2 de/din prajitura?/Arg1?
(half of/from cake)4 Member ?
?
x ?
x En.
: a bunch#1/Arg1 of cats#1/Arg2Collection Arg1 of Arg2 Arg1 de Arg2 Ro.
: ?o gra?mada?/Arg1 de pisici/Arg2?
(count, (a bunch of cats)fraction count,and vague measurepartitives)Member ?
x    En.
: president#4/Arg2 of the committee#1/Arg1 vs.Collection Arg1 Arg2 committee#1/Arg1 of idiots#1/Arg2(basic Ro.
: copiii/Arg2 din grup/Arg1partitive) (children-the from group)(?the children from the group?
)grup/Arg1 de copii/Arg2(?group of children?
)5 Location ?
x  x  En.
: the swamps#1/Arg2 of the land#7/Arg1 vs.Area Arg1 Arg2 Arg2 Arg1 the land#7/Arg1 with swamps#1/Arg2Ro.
: oaza?
?
?n des?ert(oasis in desert) vs.des?ert cu oaza?
?
?n (desert with oasis)222Girju The Syntax and Semantics of PrepositionsThis analysis shows that the choice of lexico-syntactic structures in both English andRomanian correlates with the meaning of the instances encoded by such structures.
Inthe next section we present a list of errors and situations that, currently, our system failsto recognize, and suggest possible improvements.7.
Error Analysis and Suggested ImprovementsA major part of the difficulty of interpreting nominal phrases and compounds stemsfrom multiple sources of ambiguity.
These factors range from syntactic analysis, tosemantic, pragmatic, and contextual information and translation issues.
In this sectionwe show various sources of error we found in our experiments and present somepossible improvements.A.
Error analysisTwo basic factors are wrong part-of-speech and word sense disambiguation tags.
Thus,if the syntactic tagger and WSD system fail to annotate the nouns with the correct senses,the system can generate wrong semantic classes which will lead to wrong conclusions.Moreover, there were also instances for which the nouns or the corresponding senses ofthese nouns were not found in WordNet.
There were 42.21% WSD and 6.7% POS taggingerrors in Europarl and 54.8% and 7.32% in CLUVI.
Additionally, 6.9% (Europarl) and4.6% (CLUVI) instances had missing senses.There are also cases when local contextual information such as word sense disam-biguation is not enough for relation detection and when access to a larger discoursecontext is needed.
Various researchers (Spa?rck Jones 1983; Lascarides and Copestake1998; Lapata 2002) have shown that the interpretation of noun?noun compounds, forexample, may be influenced by discourse and pragmatic knowledge.
This context maybe identified at the level of local nominal phrases and compounds or sentences or at thedocument and even collection level.
For example, a noun?noun compound modifiedby a relative clause might be disambiguated in the context of another argument of thesame verb in the clause, which can limit the number of possible semantic relations.
Forinstance, the interpretation of the instance museum book in the subject position in thefollowing examples is influenced by another argument of the verbs bought in Exam-ple (18), and informed in Example (19):(18) the [museum book]TOPIC John bought in the bookshop at the museum(19) the [museum book]LOCATION that informed John about the ancient artPrepositions such as spatial ones are also amenable to visual interpretations dueto their usage in various visual contexts.
For example, the instance nails in the box (cf.Herskovits 1987) indicates two possible arrangements of the nails: either held by thebox, or hammered into it.
We cannot capture these subtleties with the current procedureeven if they are mentioned in the context of the sentence or discourse.B.
Suggested improvementsIn this article we investigated the contribution of English and Romance prepositions tothe task of interpreting nominal phrases and compounds, both as features employedin a learning model and as classification categories.
An interesting extension of thisapproach would be to look into more detail at the functional?semantic aspect of theseprepositions and to define various tests that would classify them as pure functionalcomponents with no semantic content or semantic devices with their own meaning.223Computational Linguistics Volume 35, Number 2Moreover, our experiments focused on the detection of semantic relations encodedby N N and N P N patterns.
A more general approach would extend the investigationto adjective?noun constructions in English and Romance languages as well.Another direction for future work is the study of the semantic (ir)regularities amongEnglish and Romance nominal phrases and compounds in both directions.
Such ananalysis might be also useful for machine translation, especially when translating into alanguage with multiple choices of syntactic constructions.
One such example is tarrode cerveza (?glass of beer?)
in Spanish which can be translated as either glass of beer(MEASURE) or beer glass (PURPOSE) in English.
The current machine translation languagemodels do not differentiate between such options, choosing the most frequent instancein a large training corpus.The drawback of the approach presented in this article, as for other very preciselearning methods, is the need for a large number of training examples.
If a certainclass of negative or positive examples is not seen in the training data (and thereforeit is not captured by the classification rules), the system cannot classify its instances.Thus, the larger and more diverse the training data, the better the classification rules.Moreover, each cross-linguistic study requires translated data, which is not easy toobtain in electronic form, especially for most of the world?s languages.
However, moreand more parallel corpora in various languages are expected to be forthcoming.8.
Discussion and ConclusionsIn this article we investigated the contribution of English and Romance prepositions tothe interpretation of N N and N P N instances and presented a supervised, knowledge-intensive interpretation model.Our approach to the interpretation of nominal phrases and compounds is novelin several ways.
We investigated the problem based on cross-linguistic evidence froma set of six languages: English, Spanish, Italian, French, Portuguese, and Romanian.Thus, we presented empirical observations on the distribution of nominal phrases andcompounds and the distribution of their meanings on two different corpora, basedon two state-of-the-art classification tag sets: Lauer?s set of eight prepositions (Lauer1995) and our list of 22 semantic relations.
A mapping between the two tag sets wasalso provided.
A supervised learning model employing various linguistic features wassuccessfully compared against two state-of-the-art models reported in the literature.It is also important to mention here the linguistic implications of this work.
Wehope that the corpus investigations presented in this article provide new insight for themachine translation and multilingual question answering communities.
The translationof nominal phrase and compound instances from one language to another is highlycorrelated with the structure of each language, or set of languages.
In this article wemeasured the contribution of a set of five Romance languages to the task of semantic in-terpretation of English nominal phrases and compounds.
More specifically, we showedthat the Romanian linguistic features contribute more substantially to the overall per-formance than the features obtained for the other Romance languages.
The choice ofthe Romanian linguistic constructions (either N N or N P N) is highly correlated withtheir meaning.
This distinct behavior of Romanian constructions is also explained bythe Slavic and Balkanic influences.
An interesting future research direction would be toconsider other Indo- and non Indo-European languages and measure their contributionto the task of interpreting nominal phrases and compounds in particular, and nounphrases in general.224Girju The Syntax and Semantics of PrepositionsAcknowledgmentsWe would like to thank all our annotatorswithout whom this research would not havebeen possible: Silvia Kunitz (Italian) andFlorence Mathieu-Conner (French).
We alsothank Richard Sproat, Tania Ionin, and BrianDrexler for their suggestions on variousversions of the article.
And last but not least,we also would like to thank the reviewers fortheir very useful comments.ReferencesAlexiadou, Artemis, Liliane Haegeman, andMelita Stavrou.
2007.
Noun Phrases in theGenerative Perspective.
Mouton de Gruyter,Berlin.Almela, Ramo?n, Pascual Cantos, AquilinoSa?nchez, Ramo?n Sarmiento, and Moise?sAlmela.
2005.
Frequencias del Espan?ol.Dicctionario de estudios le?xicos y morfolo?gicos.Ed.
Universitas, Madrid.Anderson, Mona.
1983.
Prenominal genitiveNPs.
The Linguistic Review, 3:1?24.Artstein, Ron.
2007.
Quality Control of CorpusAnnotation Through Reliability Measures.Association for Computational LinguisticsConference (ACL), Prague, CzechRepublic.Baker, Collin, Charles Fillmore, and JohnLowe.
1998.
The Berkeley FrameNetProject.
In Proceedings of the 36th AnnualMeeting of the Association for ComputationalLinguistics and 17th International Conferenceon Computational Linguistics (COLING-ACL1998), pages 86?90, Montreal.Baldwin, Timothy.
2005.
Distributionalsimilarity and collocational prepositionalphrases.
In Patrick Saint-Dizier, editor,Syntax and Semantics of Prepositions,pages 197?210, Kluwer, Dordrecht.Baldwin, Timothy.
2006a.
Automaticidentification of English verb particleconstructions using linguistic features.
InThird ACL-SIGSEMWorkshop onPrepositions, pages 65?72, Trento, Italy.Baldwin, Timothy.
2006b.
Representingand Modeling the Lexical Semantics ofEnglish Verb Particle Constructions.In The European Association forComputational Linguistics (EACL), theACL-SIGSEMWorkshop on Prepositions,Trento.Barker, Chris.
1998.
Partitives, doublegenitives and anti-uniqueness.Natural Language and Linguistic Theory,16:679?717.Busa, Federica and Michael Johnston.
1996.Cross-linguistic semantics for complexnominals in the generative lexicon.
In AISBWorkshop on Multilinguality in the Lexicon,Sussex.Cadiot, Piere.
1997.
Les pre?positions abstraitesen franc?ais.
Armand Colin, Paris.Calzolari, Nicoletta, Charles J. Fillmore,Ralph Grishman, Nancy Ide, AlessandroLenci, Catherine MacLeod, and AntonioZampolli.
2002.
Towards best practice formultiword expressions in computationallexicons.
In The International Conference onLanguage Resources and Evaluation LREC,pages 1934?1940, Las Palmas.Casadei, Federica.
1991.
Le locuzionipreposizionali.
Struttura lessicale e gradidi lessicalizzazione.
Lingua e Stile, XXXVI:43?80.Celce-Murcia, Marianne and DianeLarsen-Freeman.
1999.
The grammar book,2nd edition.
Heinle and Heinle, Boston,MA.Charniak, Eugene.
2000.
Amaximum-entropy-inspired parser.
In The1st Conference of the North American Chapterof the Association for ComputationalLinguistics (NAACL), pages 132?139,Seattle, WA.Cornilescu, Alexandra.
2001.
Romaniannominalizations: Case and aspectualstructure.
Journal of Linguistics, 37:467?501.Dorr, Bonnie.
1993.
Machine Translation: AView from the Lexicon.
MIT Press,Cambridge, MA.Downing, Pamela.
1977.
On the creation anduse of English compound nouns.
Language,53:810?842.Evans, Vyvyan and Paul Chilton, editors.2009.
Language, Cognition and Space: TheState of the Art and New Directions.Advances in Cognitive Linguistics.Equinox Publishing Company, London.Fang, Alex C. 2000.
A lexicalist approachtowards the automatic determination forthe syntactic functions of prepositionalphrases.
Natural Language Engineering,6:183?201.Fellbaum, Christiane.
1998.
WordNet?AnElectronic Lexical Database.
MIT Press,Cambridge, MA.Finin, Timothy W. 1980.
The SemanticInterpretation of Compound Nominals.Ph.D.
thesis, University of Illinois atUrbana-Champaign, Urbana-Champaign,IL.Giorgi, Alessandra and GiuseppeLongobardi.
1991.
The syntax of nounphrases.
Cambridge University Press,London.Girju, Roxana, Alexandra Badulescu, andDan Moldovan.
2006.
Automatic discovery225Computational Linguistics Volume 35, Number 2of part-whole relations.
ComputationalLinguistics, 32(1):83?135.Girju, Roxana, Dan Moldovan, Marta Tatu,and Daniel Antohe.
2005.
On theSemantics of Noun Compounds.
ComputerSpeech and Language, Special Issue onMultiword Expressions, 19(4):479?496.Gleitman, Lila R. and Henry Gleitman.
1970.Phrase and Paraphrase: Some Innovative Usesof Language.
Norton, New York.Gocsik, Karen.
2004.
English as a SecondLanguage.
Dartmouth College Press,Hanover, NH.Grimshaw, Jane.
1990.
Argument Structure.MIT Press, Cambridge, MA.Herskovits, Annette.
1987.
Language andSpatial Cognition: An InterdisciplinaryStudy of the Prepositions in English.Cambridge University Press, Cambridge,MA.Ionin, Tania, Ora Matushansky, and EddyRuys.
2006.
Parts of speech: Toward aunified semantics for partitives.
InConference of the North East LinguisticSociety (NELS), pages 357?370,Amherst, MA.Jensen, Per Anker and Jo?rgen F. Nilsson.2005.
Ontology-based semantics forprepositions.
In Patrick Saint-Dizier,editor, Syntax and Semantics of Prepositions,volume 29 of Text, Speech and LanguageTechnology.
Springer, Dordrecht.Jespersen, Otto.
1954.
AModern EnglishGrammar on Historical Principles.
GeorgeAllen & Unwin Ltd., Heidelberg andLondon.Johnston, Michael and Federica Busa.
1996.Qualia structure and the compositionalinterpretation of compounds.
In EvelyneViegas, editor, Breadth and Depth ofSemantics Lexicon, pages 77?88, KluwerAcademic, Dordrecht.Kim, Su Nam and Timothy Baldwin.
2005.Automatic interpretation of nouncompounds using WordNet similarity.In The International Joint Conference onNatural Language Processing (IJCNLP),pages 945?956, Jeju.Kim, Su Nam and Timothy Baldwin.
2006.Interpreting semantic relations in nouncompounds via verb semantics.
In TheInternational Conference on ComputationalLinguistics / the Association forComputational Linguistics (COLING/ACL) -Main Conference Poster Sessions,pages 491?498, Sydney.Kipper, Karin, Hoa Trang Dang, and MarthaPalmer.
2000.
Class-based constructionof a verb lexicon.
In The NationalConference on Artificial Intelligence (AAAI),pages 691?696, Austin, TX.Kordoni, Valia.
2005.
Prepositionalarguments in a multilingual context.In Patrick Saint-Dizier, editor,Syntax and Semantics of Prepositions,volume 29 of Text, Speech and LanguageTechnology.
Springer, Dordrecht,pages 307?330.Kordoni, Valia.
2006.
PPs as verbalarguments: From a computationalsemantics perspective.
In The EuropeanAssociation for Computational Linguistics(EACL), the ACL-SIGSEMWorkshop onPrepositions, Trento, Italy.Lapata, Mirella.
2002.
The Disambiguation ofnominalisations.
Computational Linguistics,28(3):357?388.Lapata, Mirella and Frank Keller.
2005.Web-based models for natural languageprocessing.
ACM Transactions on Speech andLanguage Processing, 2:1?31.Lascarides, Alex and Ann Copestake.
1998.Pragmatics and word meaning.
Journal ofLinguistics, 34:387?414.Lauer, Mark.
1995.
Corpus statistics meet thenoun compound: Some empirical results.In The Association for ComputationalLinguistics Conference (ACL), pages 47?54,Cambridge, MA.Lees, Robert B.
1963.
The Grammar of EnglishNominalisations.
Mouton, The Hague.Lenci, Alessandro, Nuria Bel, Federica Busa,Nicoletta Calzolari, Elisabetta Gola,Monica Monachini, Antoine Ogonowski,Ivonne Peters, Wim Peters, Nilda Ruimy,Marta Villegas, and Antonio Zampolli.2000.
SIMPLE: A general framework forthe development of multilingual lexicons.International Journal of Lexicography,13:249?263.Lersundi, Mikel and Eneko Aggire.
2006.Multilingual inventory of interpretationsfor postpositions and prepositions.
InPatrick Saint-Dizier, editor, Syntax andSemantics of Prepositions, volume 29 of Text,Speech and Language Technology.
Springer,Dordrecht, pages 69?82.Levi, Judith.
1978.
The Syntax and Semanticsof Complex Nominals.
Academic Press,New York.Linstromberg, Seth.
1997.
English PrepositionsExplained.
John Benjamins Publishing Co.,Amsterdam/Philadelphia.Litkowski, Kenneth C. and Orin Hargraves.2005.
The Preposition Project.
In TheACL-SIGSEMWorkshop on the LinguisticDimensions of Prepositions and their Use inComputational Linguistics Formalisms and226Girju The Syntax and Semantics of PrepositionsApplications, pages 171?179, Colchester,UK.Luraghi, Silvia.
2003.
Prepositions in Greek andIndo-European.
Benjamins, Amsterdam.Lyons, Christopher.
1986.
The syntax ofEnglish genitive constructions.
Journal ofLinguistics, 22:123?143.Melis, Ludo.
2002.
Les pre?positions du franc?ais.L?essentiel franc?ais.
Ophrys, Paris/Gap.Meyers, A., R. Reeves, Catherine Maclead,Rachel Szekely, Veronika Zielinsk, BrianYoung, and R. Grishman.
2004.
Thecross-breeding of dictionaries.
InProceedings of the 5th InternationalConference on Language Resources andEvaluation (LREC-2004), pages 1095?1098,Lisbon.Mihalcea, Rada and Ehsanul Faruque.
2004.SenseLearner: Minimally supervised wordsense disambiguation for all words inopen text.
In Senseval-3: Third InternationalWorkshop on the Evaluation of Systems for theSemantic Analysis of Text, pages 155?158,Barcelona.Moldovan, Dan and Adriana Badulescu.2005.
A semantic scattering model for theautomatic interpretation of genitives.
InProceedings of Human Language TechnologyConference and Conference on EmpiricalMethods in Natural Language Processing,pages 891?898, Vancouver.Moldovan, Dan, Adriana Badulescu, MartaTatu, Daniel Antohe, and Roxana Girju.2004.
Models for the semanticclassification of noun phrases.
In TheHuman Language Technology Conference /North American Association forComputational Linguistics Conference(HLT/NAACL), Workshop on ComputationalLexical Semantics, pages 60?67,Boston, MA.Moldovan, Dan and Roxana Girju.
2003.Proceedings of the Tutorial on KnowledgeDiscovery from Text.
Association forComputational Linguistics, Sapporo,Japan.Nakov, Preslav and Marti Hearst.
2005.Search engine statistics beyond then-gram: Application to noun compoundbracketing.
In The 9th Conference onComputational Natural Language Learning,pages 835?842, Vancouver.O?Hara, Tom and Janyce Wiebe.
2003.Preposition semantic classification viaTreebank and FrameNet.
In Conference onComputational Natural Language Learning(CoNLL), pages 79?86, Edmonton.Pantel, Patrick and Marco Pennacchiotti.2006.
Espresso: Leveraging genericpatterns for automatically harvestingsemantic relations.
In The InternationalComputational Linguistics Conference /Association for Computational Linguistics(COLING/ACL), pages 113?120, Sydney.Pantel, Patrick and Deepak Ravichandran.2004.
Automatically labeling semanticclasses.
In The Human Language TechnologyConference of the North American Chapterof the Association for ComputationalLinguistics (HLT/NAACL), pages 321?328,Boston, MA.Pennacchiotti, Marco and Patrick Pantel.2006.
Ontologizing semantic relations.
InThe International Computational LinguisticsConference / Association for ComputationalLinguistics (COLING/ACL), pages 793?800,Sydney.Pustejovsky, James.
1995.
The GenerativeLexicon.
MIT Press, Cambridge, MA.Pustejovsky, James, Catherine Havasi, RoserSauri, Patrick Hanks, Jessica Littman,Anna Rumshisky, Jose Castano, and MarcVerhagen.
2006.
Towards a generativelexical resource: The brandeis semanticontology.
In The International Conference onLanguage Resources and Evaluation (LREC),pages 385?388, Genoa.Romaine, Suzanne.
1995.
Bilingualism.Blackwell, Oxford.Rosario, Barbara and Marti Hearst.
2001.Classifying the semantic relations in nouncompounds.
In Conference on EmpiricalMethods in Natural Language Processing,pages 82?90, Pittsburgh, PA.Rosario, Barbara, Marti Hearst, and CharlesFillmore.
2002.
The descent of hierarchy,and selection in relational semantics.
InThe 40th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 247?254, Philadelphia, PA.Saint-Dizier, Patrick.
2005a.
PrepNet: Aframework for describing prepositions:Preliminary investigation results.
In The6th International Workshop on ComputationalSemantics, pages 25?34, Tilburg.Saint-Dizier, Patrick, editor.
2005b.
Syntaxand Semantics of Prepositions.
Springer,Dordrecht.Selkirk, Elisabeth.
1982a.
Some remarks onnoun phrase structure.
In Peter W.Culicover, Thomas Wasow, and AdrianAkmajian, editors, Formal Syntax.Academic Press, London.Selkirk, Elisabeth.
1982b.
Syntax of Words.MIT Press, Cambridge, MA.Spa?rck Jones, Karen.
1983.
Compoundnoun interpretation problems.
InFrank Fallside and William A. Woods,227Computational Linguistics Volume 35, Number 2editors, Computer Speech Processing.Prentice-Hall, Englewood Cliffs, NJ,pages 363?381.Tatu, Marta and Dan Moldovan.
2005.
Asemantic approach to recognizing textualentailmant.
In Proceedings of HumanLanguage Technology Conference andConference on Empirical Methods in NaturalLanguage Proceesing (HLT/EMNLP 2005),pages 371?378, Vancouver.Turney, Peter.
2006.
Expressing implicitsemantic relations without supervision.
InThe Computational Linguistics Conference /Association for Computational LinguisticsConference (COLING/ACL), pages 313?320,Sydney.Tyler, Andrea and Vyvyan Evans.
2003.The Semantics of English Prepositions:Spatial Sciences, Embodied Meaning, andCognition.
Cambridge University Press,Cambridge, MA.Vandeloise, Claude, editor.
1993.
La couleurdes pre?positions, volume 110.
Larousse,Paris.Villavicencio, Aline.
2006.
Verb-particleconstructions in the World Wide Web.
InPatrick Saint-Dizier, editor, Syntax andSemantics of Prepositions, volume 29 of Text,Speech and Language Technology.
Springer,Dordrecht, pages 115?130.Volk, Martin.
2006.
How bad is the problemof PP-attachment?
A comparison ofEnglish, German and Swedish.
In TheEuropean Association for ComputationalLinguistics (EACL), the ACL-SIGSEMWorkshop on Prepositions, pages 81?88,Trento.Vossen, Peter.
1998.
EuroWordNet: AMultilingual Database with LexicalSemantic Networks.
Kluwer AcademicPublishers, Verlag.Winston, Morton, Roger Chaffin, andDouglas Hermann.
1987.
A taxonomy ofpart-whole relations.
Cognitive Science,11:417?444.Zelinski-Wibbelt, Cornelia, editor.
1993.
TheSemantics of Prepositions.
Mouton deGruyter, Berlin.228This article has been cited by:1.
Timothy Baldwin, Valia Kordoni, Aline Villavicencio.
2009.
Prepositions in Applications:A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey andIntroduction to the Special Issue.
Computational Linguistics 35:2, 119-149.
[Citation] [PDF][PDF Plus]
